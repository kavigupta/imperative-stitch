{"setup.py": "#!/usr/bin/env python\nimport codecs\nimport os.path\nimport re\nimport sys\n\nfrom setuptools import setup, find_packages\n\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read(*parts):\n    return codecs.open(os.path.join(here, *parts), 'r').read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\",\n                              version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(\"Unable to find version string.\")\n\n\ninstall_requires = [\n    'botocore==1.34.132',\n    'docutils>=0.10,<0.17',\n    's3transfer>=0.10.0,<0.11.0',\n    'PyYAML>=3.10,<6.1',\n    'colorama>=0.2.5,<0.4.7',\n    'rsa>=3.1.2,<4.8',\n]\n\n\nsetup_options = dict(\n    name='awscli',\n    version=find_version(\"awscli\", \"__init__.py\"),\n    description='Universal Command Line Environment for AWS.',\n    long_description=read('README.rst'),\n    author='Amazon Web Services',\n    url='http://aws.amazon.com/cli/',\n    scripts=['bin/aws', 'bin/aws.cmd',\n             'bin/aws_completer', 'bin/aws_zsh_completer.sh',\n             'bin/aws_bash_completer'],\n    packages=find_packages(exclude=['tests*']),\n    include_package_data=True,\n    install_requires=install_requires,\n    extras_require={},\n    license=\"Apache License 2.0\",\n    python_requires=\">= 3.8\",\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'Intended Audience :: System Administrators',\n        'Natural Language :: English',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3 :: Only',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n    ],\n    project_urls={\n        'Source': 'https://github.com/aws/aws-cli',\n        'Reference': 'https://docs.aws.amazon.com/cli/latest/reference/',\n        'Changelog': 'https://github.com/aws/aws-cli/blob/develop/CHANGELOG.rst',\n    },\n)\n\n\nif 'py2exe' in sys.argv:\n    # This will actually give us a py2exe command.\n    import py2exe\n    # And we have some py2exe specific options.\n    setup_options['options'] = {\n        'py2exe': {\n            'optimize': 0,\n            'skip_archive': True,\n            'dll_excludes': ['crypt32.dll'],\n            'packages': ['docutils', 'urllib', 'httplib', 'HTMLParser',\n                         'awscli', 'ConfigParser', 'xml.etree', 'pipes'],\n        }\n    }\n    setup_options['console'] = ['bin/aws']\n\n\nsetup(**setup_options)\n", "awscli/formatter.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n\n#     http://aws.amazon.com/apache2.0/\n\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nfrom botocore.compat import json\n\nfrom botocore.utils import set_value_from_jmespath\nfrom botocore.paginate import PageIterator\n\nfrom awscli.table import MultiTable, Styler, ColorizedStyler\nfrom awscli import text\nfrom awscli import compat\nfrom awscli.utils import json_encoder\n\n\nLOG = logging.getLogger(__name__)\n\n\ndef is_response_paginated(response):\n    return isinstance(response, PageIterator)\n\n\nclass Formatter(object):\n    def __init__(self, args):\n        self._args = args\n\n    def _remove_request_id(self, response_data):\n        if 'ResponseMetadata' in response_data:\n            if 'RequestId' in response_data['ResponseMetadata']:\n                request_id = response_data['ResponseMetadata']['RequestId']\n                LOG.debug('RequestId: %s', request_id)\n            del response_data['ResponseMetadata']\n\n    def _get_default_stream(self):\n        return compat.get_stdout_text_writer()\n\n    def _flush_stream(self, stream):\n        try:\n            stream.flush()\n        except IOError:\n            pass\n\n\nclass FullyBufferedFormatter(Formatter):\n    def __call__(self, command_name, response, stream=None):\n        if stream is None:\n            # Retrieve stdout on invocation instead of at import time\n            # so that if anything wraps stdout we'll pick up those changes\n            # (specifically colorama on windows wraps stdout).\n            stream = self._get_default_stream()\n        # I think the interfaces between non-paginated\n        # and paginated responses can still be cleaned up.\n        if is_response_paginated(response):\n            response_data = response.build_full_result()\n        else:\n            response_data = response\n        self._remove_request_id(response_data)\n        if self._args.query is not None:\n            response_data = self._args.query.search(response_data)\n        try:\n            self._format_response(command_name, response_data, stream)\n        except IOError as e:\n            # If the reading end of our stdout stream has closed the file\n            # we can just exit.\n            pass\n        finally:\n            # flush is needed to avoid the \"close failed in file object\n            # destructor\" in python2.x (see http://bugs.python.org/issue11380).\n            self._flush_stream(stream)\n\n\nclass JSONFormatter(FullyBufferedFormatter):\n\n    def _format_response(self, command_name, response, stream):\n        # For operations that have no response body (e.g. s3 put-object)\n        # the response will be an empty string.  We don't want to print\n        # that out to the user but other \"falsey\" values like an empty\n        # dictionary should be printed.\n        if response != {}:\n            json.dump(response, stream, indent=4, default=json_encoder,\n                    ensure_ascii=False)\n            stream.write('\\n')\n\n\nclass TableFormatter(FullyBufferedFormatter):\n    \"\"\"Pretty print a table from a given response.\n\n    The table formatter is able to take any generic response\n    and generate a pretty printed table.  It does this without\n    using the output definition from the model.\n\n    \"\"\"\n    def __init__(self, args, table=None):\n        super(TableFormatter, self).__init__(args)\n        if args.color == 'auto':\n            self.table = MultiTable(initial_section=False,\n                                    column_separator='|')\n        elif args.color == 'off':\n            styler = Styler()\n            self.table = MultiTable(initial_section=False,\n                                    column_separator='|', styler=styler)\n        elif args.color == 'on':\n            styler = ColorizedStyler()\n            self.table = MultiTable(initial_section=False,\n                                    column_separator='|', styler=styler)\n        else:\n            raise ValueError(\"Unknown color option: %s\" % args.color)\n\n    def _format_response(self, command_name, response, stream):\n        if self._build_table(command_name, response):\n            try:\n                self.table.render(stream)\n            except IOError:\n                # If they're piping stdout to another process which exits before\n                # we're done writing all of our output, we'll get an error about a\n                # closed pipe which we can safely ignore.\n                pass\n\n    def _build_table(self, title, current, indent_level=0):\n        if not current:\n            return False\n        if title is not None:\n            self.table.new_section(title, indent_level=indent_level)\n        if isinstance(current, list):\n            if isinstance(current[0], dict):\n                self._build_sub_table_from_list(current, indent_level, title)\n            else:\n                for item in current:\n                    if self._scalar_type(item):\n                        self.table.add_row([item])\n                    elif all(self._scalar_type(el) for el in item):\n                        self.table.add_row(item)\n                    else:\n                        self._build_table(title=None, current=item)\n        if isinstance(current, dict):\n            # Render a single row section with keys as header\n            # and the row as the values, unless the value\n            # is a list.\n            self._build_sub_table_from_dict(current, indent_level)\n        return True\n\n    def _build_sub_table_from_dict(self, current, indent_level):\n        # Render a single row section with keys as header\n        # and the row as the values, unless the value\n        # is a list.\n        headers, more = self._group_scalar_keys(current)\n        if len(headers) == 1:\n            # Special casing if a dict has a single scalar key/value pair.\n            self.table.add_row([headers[0], current[headers[0]]])\n        elif headers:\n            self.table.add_row_header(headers)\n            self.table.add_row([current[k] for k in headers])\n        for remaining in more:\n            self._build_table(remaining, current[remaining],\n                              indent_level=indent_level + 1)\n\n    def _build_sub_table_from_list(self, current, indent_level, title):\n        headers, more = self._group_scalar_keys_from_list(current)\n        self.table.add_row_header(headers)\n        first = True\n        for element in current:\n            if not first and more:\n                self.table.new_section(title,\n                                       indent_level=indent_level)\n                self.table.add_row_header(headers)\n            first = False\n            # Use .get() to account for the fact that sometimes an element\n            # may not have all the keys from the header.\n            self.table.add_row([element.get(header, '') for header in headers])\n            for remaining in more:\n                # Some of the non scalar attributes may not necessarily\n                # be in every single element of the list, so we need to\n                # check this condition before recursing.\n                if remaining in element:\n                    self._build_table(remaining, element[remaining],\n                                    indent_level=indent_level + 1)\n\n    def _scalar_type(self, element):\n        return not isinstance(element, (list, dict))\n\n    def _group_scalar_keys_from_list(self, list_of_dicts):\n        # We want to make sure we catch all the keys in the list of dicts.\n        # Most of the time each list element has the same keys, but sometimes\n        # a list element will have keys not defined in other elements.\n        headers = set()\n        more = set()\n        for item in list_of_dicts:\n            current_headers, current_more = self._group_scalar_keys(item)\n            headers.update(current_headers)\n            more.update(current_more)\n        headers = list(sorted(headers))\n        more = list(sorted(more))\n        return headers, more\n\n    def _group_scalar_keys(self, current):\n        # Given a dict, separate the keys into those whose values are\n        # scalar, and those whose values aren't.  Return two lists,\n        # one is the scalar value keys, the second is the remaining keys.\n        more = []\n        headers = []\n        for element in current:\n            if self._scalar_type(current[element]):\n                headers.append(element)\n            else:\n                more.append(element)\n        headers.sort()\n        more.sort()\n        return headers, more\n\n\nclass TextFormatter(Formatter):\n\n    def __call__(self, command_name, response, stream=None):\n        if stream is None:\n            stream = self._get_default_stream()\n        try:\n            if is_response_paginated(response):\n                result_keys = response.result_keys\n                for i, page in enumerate(response):\n                    if i > 0:\n                        current = {}\n                    else:\n                        current = response.non_aggregate_part\n\n                    for result_key in result_keys:\n                        data = result_key.search(page)\n                        set_value_from_jmespath(\n                            current,\n                            result_key.expression,\n                            data\n                        )\n                    self._format_response(current, stream)\n                if response.resume_token:\n                    # Tell the user about the next token so they can continue\n                    # if they want.\n                    self._format_response(\n                        {'NextToken': {'NextToken': response.resume_token}},\n                        stream)\n            else:\n                self._remove_request_id(response)\n                self._format_response(response, stream)\n        finally:\n            # flush is needed to avoid the \"close failed in file object\n            # destructor\" in python2.x (see http://bugs.python.org/issue11380).\n            self._flush_stream(stream)\n\n    def _format_response(self, response, stream):\n        if self._args.query is not None:\n            expression = self._args.query\n            response = expression.search(response)\n        text.format_text(response, stream)\n\n\ndef get_formatter(format_type, args):\n    if format_type == 'json':\n        return JSONFormatter(args)\n    elif format_type == 'text':\n        return TextFormatter(args)\n    elif format_type == 'table':\n        return TableFormatter(args)\n    raise ValueError(\"Unknown output type: %s\" % format_type)\n", "awscli/plugin.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nfrom botocore.hooks import HierarchicalEmitter\n\nlog = logging.getLogger('awscli.plugin')\n\nBUILTIN_PLUGINS = {'__builtin__': 'awscli.handlers'}\n\n\ndef load_plugins(plugin_mapping, event_hooks=None, include_builtins=True):\n    \"\"\"\n\n    :type plugin_mapping: dict\n    :param plugin_mapping: A dict of plugin name to import path,\n        e.g. ``{\"plugingName\": \"package.modulefoo\"}``.\n\n    :type event_hooks: ``EventHooks``\n    :param event_hooks: Event hook emitter.  If one if not provided,\n        an emitter will be created and returned.  Otherwise, the\n        passed in ``event_hooks`` will be used to initialize plugins.\n\n    :type include_builtins: bool\n    :param include_builtins: If True, the builtin awscli plugins (specified in\n        ``BUILTIN_PLUGINS``) will be included in the list of plugins to load.\n\n    :rtype: HierarchicalEmitter\n    :return: An event emitter object.\n\n    \"\"\"\n    if include_builtins:\n        plugin_mapping.update(BUILTIN_PLUGINS)\n    modules = _import_plugins(plugin_mapping)\n    if event_hooks is None:\n        event_hooks = HierarchicalEmitter()\n    for name, plugin in zip(plugin_mapping.keys(), modules):\n        log.debug(\"Initializing plugin %s: %s\", name, plugin)\n        plugin.awscli_initialize(event_hooks)\n    return event_hooks\n\n\ndef _import_plugins(plugin_names):\n    plugins = []\n    for name, path in plugin_names.items():\n        log.debug(\"Importing plugin %s: %s\", name, path)\n        if '.' not in path:\n            plugins.append(__import__(path))\n        else:\n            package, module = path.rsplit('.', 1)\n            module = __import__(path, fromlist=[module])\n            plugins.append(module)\n    return plugins\n", "awscli/table.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n\n#     http://aws.amazon.com/apache2.0/\n\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport sys\nimport struct\nimport unicodedata\n\nimport colorama\n\nfrom awscli.utils import is_a_tty\n\n\n# `autoreset` allows us to not have to sent reset sequences for every\n# string. `strip` lets us preserve color when redirecting.\nCOLORAMA_KWARGS = {\n    'autoreset': True,\n    'strip': False,\n}\n\n\ndef get_text_length(text):\n    # `len(unichar)` measures the number of characters, so we use\n    # `unicodedata.east_asian_width` to measure the length of characters.\n    # Following responses are considered to be full-width length.\n    # * A(Ambiguous)\n    # * F(Fullwidth)\n    # * W(Wide)\n    text = str(text)\n    return sum(2 if unicodedata.east_asian_width(char) in 'WFA' else 1\n               for char in text)\n\n\ndef determine_terminal_width(default_width=80):\n    # If we can't detect the terminal width, the default_width is returned.\n    try:\n        from termios import TIOCGWINSZ\n        from fcntl import ioctl\n    except ImportError:\n        return default_width\n    try:\n        height, width = struct.unpack('hhhh', ioctl(sys.stdout,\n                                                    TIOCGWINSZ, '\\000' * 8))[0:2]\n    except Exception:\n        return default_width\n    else:\n        return width\n\n\ndef center_text(text, length=80, left_edge='|', right_edge='|',\n                text_length=None):\n    \"\"\"Center text with specified edge chars.\n\n    You can pass in the length of the text as an arg, otherwise it is computed\n    automatically for you.  This can allow you to center a string not based\n    on it's literal length (useful if you're using ANSI codes).\n    \"\"\"\n    # postcondition: get_text_length(returned_text) == length\n    if text_length is None:\n        text_length = get_text_length(text)\n    output = []\n    char_start = (length // 2) - (text_length // 2) - 1\n    output.append(left_edge + ' ' * char_start + text)\n    length_so_far = get_text_length(left_edge) + char_start + text_length\n    right_side_spaces = length - get_text_length(right_edge) - length_so_far\n    output.append(' ' * right_side_spaces)\n    output.append(right_edge)\n    final = ''.join(output)\n    return final\n\n\ndef align_left(text, length, left_edge='|', right_edge='|', text_length=None,\n               left_padding=2):\n    \"\"\"Left align text.\"\"\"\n    # postcondition: get_text_length(returned_text) == length\n    if text_length is None:\n        text_length = get_text_length(text)\n    computed_length = (\n        text_length + left_padding + \\\n        get_text_length(left_edge) + get_text_length(right_edge))\n    if length - computed_length >= 0:\n        padding = left_padding\n    else:\n        padding = 0\n    output = []\n    length_so_far = 0\n    output.append(left_edge)\n    length_so_far += len(left_edge)\n    output.append(' ' * padding)\n    length_so_far += padding\n    output.append(text)\n    length_so_far += text_length\n    output.append(' ' * (length - length_so_far - len(right_edge)))\n    output.append(right_edge)\n    return ''.join(output)\n\n\ndef convert_to_vertical_table(sections):\n    # Any section that only has a single row is\n    # inverted, so:\n    # header1 | header2 | header3\n    # val1    | val2    | val2\n    #\n    # becomes:\n    #\n    # header1 | val1\n    # header2 | val2\n    # header3 | val3\n    for i, section in enumerate(sections):\n        if len(section.rows) == 1 and section.headers:\n            headers = section.headers\n            new_section = Section()\n            new_section.title = section.title\n            new_section.indent_level = section.indent_level\n            for header, element in zip(headers, section.rows[0]):\n                new_section.add_row([header, element])\n            sections[i] = new_section\n\n\nclass IndentedStream(object):\n    def __init__(self, stream, indent_level, left_indent_char='|',\n                 right_indent_char='|'):\n        self._stream = stream\n        self._indent_level = indent_level\n        self._left_indent_char = left_indent_char\n        self._right_indent_char = right_indent_char\n\n    def write(self, text):\n        self._stream.write(self._left_indent_char * self._indent_level)\n        if text.endswith('\\n'):\n            self._stream.write(text[:-1])\n            self._stream.write(self._right_indent_char * self._indent_level)\n            self._stream.write('\\n')\n        else:\n            self._stream.write(text)\n\n    def __getattr__(self, attr):\n        return getattr(self._stream, attr)\n\n\nclass Styler(object):\n    def style_title(self, text):\n        return text\n\n    def style_header_column(self, text):\n        return text\n\n    def style_row_element(self, text):\n        return text\n\n    def style_indentation_char(self, text):\n        return text\n\n\nclass ColorizedStyler(Styler):\n    def __init__(self):\n        colorama.init(**COLORAMA_KWARGS)\n\n    def style_title(self, text):\n        # Originally bold + underline\n        return text\n        #return colorama.Style.BOLD + text + colorama.Style.RESET_ALL\n\n    def style_header_column(self, text):\n        # Originally underline\n        return text\n\n    def style_row_element(self, text):\n        return (colorama.Style.BRIGHT + colorama.Fore.BLUE +\n                text + colorama.Style.RESET_ALL)\n\n    def style_indentation_char(self, text):\n        return (colorama.Style.DIM + colorama.Fore.YELLOW +\n                text + colorama.Style.RESET_ALL)\n\n\nclass MultiTable(object):\n    def __init__(self, terminal_width=None, initial_section=True,\n                 column_separator='|', terminal=None,\n                 styler=None, auto_reformat=True):\n        self._auto_reformat = auto_reformat\n        if initial_section:\n            self._current_section = Section()\n            self._sections = [self._current_section]\n        else:\n            self._current_section = None\n            self._sections = []\n        if styler is None:\n            # Move out to factory.\n            if is_a_tty():\n                self._styler = ColorizedStyler()\n            else:\n                self._styler = Styler()\n        else:\n            self._styler = styler\n        self._rendering_index = 0\n        self._column_separator = column_separator\n        if terminal_width is None:\n            self._terminal_width = determine_terminal_width()\n\n    def add_title(self, title):\n        self._current_section.add_title(title)\n\n    def add_row_header(self, headers):\n        self._current_section.add_header(headers)\n\n    def add_row(self, row_elements):\n        self._current_section.add_row(row_elements)\n\n    def new_section(self, title, indent_level=0):\n        self._current_section = Section()\n        self._sections.append(self._current_section)\n        self._current_section.add_title(title)\n        self._current_section.indent_level = indent_level\n\n    def render(self, stream):\n        max_width = self._calculate_max_width()\n        should_convert_table = self._determine_conversion_needed(max_width)\n        if should_convert_table:\n            convert_to_vertical_table(self._sections)\n            max_width = self._calculate_max_width()\n        stream.write('-' * max_width + '\\n')\n        for section in self._sections:\n            self._render_section(section, max_width, stream)\n\n    def _determine_conversion_needed(self, max_width):\n        # If we don't know the width of the controlling terminal,\n        # then we don't try to resize the table.\n        if max_width > self._terminal_width:\n            return self._auto_reformat\n\n    def _calculate_max_width(self):\n        max_width = max(s.total_width(padding=4, with_border=True,\n                                      outer_padding=s.indent_level)\n                        for s in self._sections)\n        return max_width\n\n    def _render_section(self, section, max_width, stream):\n        stream = IndentedStream(stream, section.indent_level,\n                                self._styler.style_indentation_char('|'),\n                                self._styler.style_indentation_char('|'))\n        max_width -= (section.indent_level * 2)\n        self._render_title(section, max_width, stream)\n        self._render_column_titles(section, max_width, stream)\n        self._render_rows(section, max_width, stream)\n\n    def _render_title(self, section, max_width, stream):\n        # The title consists of:\n        # title        :  |   This is the title      |\n        # bottom_border:  ----------------------------\n        if section.title:\n            title = self._styler.style_title(section.title)\n            stream.write(center_text(title, max_width, '|', '|',\n                                     get_text_length(section.title)) + '\\n')\n            if not section.headers and not section.rows:\n                stream.write('+%s+' % ('-' * (max_width - 2)) + '\\n')\n\n    def _render_column_titles(self, section, max_width, stream):\n        if not section.headers:\n            return\n        # In order to render the column titles we need to know\n        # the width of each of the columns.\n        widths = section.calculate_column_widths(padding=4,\n                                                 max_width=max_width)\n        # TODO: Built a list instead of +=, it's more efficient.\n        current = ''\n        length_so_far = 0\n        # The first cell needs both left and right edges '|  foo  |'\n        # while subsequent cells only need right edges '  foo  |'.\n        first = True\n        for width, header in zip(widths, section.headers):\n            stylized_header = self._styler.style_header_column(header)\n            if first:\n                left_edge = '|'\n                first = False\n            else:\n                left_edge = ''\n            current += center_text(text=stylized_header, length=width,\n                                   left_edge=left_edge, right_edge='|',\n                                   text_length=get_text_length(header))\n            length_so_far += width\n        self._write_line_break(stream, widths)\n        stream.write(current + '\\n')\n\n    def _write_line_break(self, stream, widths):\n        # Write out something like:\n        # +-------+---------+---------+\n        parts = []\n        first = True\n        for width in widths:\n            if first:\n                parts.append('+%s+' % ('-' * (width - 2)))\n                first = False\n            else:\n                parts.append('%s+' % ('-' * (width - 1)))\n        parts.append('\\n')\n        stream.write(''.join(parts))\n\n    def _render_rows(self, section, max_width, stream):\n        if not section.rows:\n            return\n        widths = section.calculate_column_widths(padding=4,\n                                                 max_width=max_width)\n        if not widths:\n            return\n        self._write_line_break(stream, widths)\n        for row in section.rows:\n            # TODO: Built the string in a list then join instead of using +=,\n            # it's more efficient.\n            current = ''\n            length_so_far = 0\n            first = True\n            for width, element in zip(widths, row):\n                if first:\n                    left_edge = '|'\n                    first = False\n                else:\n                    left_edge = ''\n                stylized = self._styler.style_row_element(element)\n                current += align_left(text=stylized, length=width,\n                                      left_edge=left_edge,\n                                      right_edge=self._column_separator,\n                                      text_length=get_text_length(element))\n                length_so_far += width\n            stream.write(current + '\\n')\n        self._write_line_break(stream, widths)\n\n\nclass Section(object):\n    def __init__(self):\n        self.title = ''\n        self.headers = []\n        self.rows = []\n        self.indent_level = 0\n        self._num_cols = None\n        self._max_widths = []\n\n    def __repr__(self):\n        return (\"Section(title=%s, headers=%s, indent_level=%s, num_rows=%s)\" %\n                (self.title, self.headers, self.indent_level, len(self.rows)))\n\n    def calculate_column_widths(self, padding=0, max_width=None):\n        # postcondition: sum(widths) == max_width\n        unscaled_widths = [w + padding for w in self._max_widths]\n        if max_width is None:\n            return unscaled_widths\n        if not unscaled_widths:\n            return unscaled_widths\n        else:\n            # Compute scale factor for max_width.\n            scale_factor = max_width / float(sum(unscaled_widths))\n            scaled = [int(round(scale_factor * w)) for w in unscaled_widths]\n            # Once we've scaled the columns, we may be slightly over/under\n            # the amount we need so we have to adjust the columns.\n            off_by = sum(scaled) - max_width\n            while off_by != 0:\n                iter_order = range(len(scaled))\n                if off_by < 0:\n                    iter_order = reversed(iter_order)\n                for i in iter_order:\n                    if off_by > 0:\n                        scaled[i] -= 1\n                        off_by -= 1\n                    else:\n                        scaled[i] += 1\n                        off_by += 1\n                    if off_by == 0:\n                        break\n            return scaled\n\n    def total_width(self, padding=0, with_border=False, outer_padding=0):\n        total = 0\n        # One char on each side == 2 chars total to the width.\n        border_padding = 2\n        for w in self.calculate_column_widths():\n            total += w + padding\n        if with_border:\n            total += border_padding\n        total += outer_padding + outer_padding\n        return max(get_text_length(self.title) + border_padding + outer_padding +\n                   outer_padding, total)\n\n    def add_title(self, title):\n        self.title = title\n\n    def add_header(self, headers):\n        self._update_max_widths(headers)\n        if self._num_cols is None:\n            self._num_cols = len(headers)\n        self.headers = self._format_headers(headers)\n\n    def _format_headers(self, headers):\n        return headers\n\n    def add_row(self, row):\n        if self._num_cols is None:\n            self._num_cols = len(row)\n        if len(row) != self._num_cols:\n            raise ValueError(\"Row should have %s elements, instead \"\n                             \"it has %s\" % (self._num_cols, len(row)))\n        row = self._format_row(row)\n        self.rows.append(row)\n        self._update_max_widths(row)\n\n    def _format_row(self, row):\n        return [str(r) for r in row]\n\n    def _update_max_widths(self, row):\n        if not self._max_widths:\n            self._max_widths = [get_text_length(el) for el in row]\n        else:\n            for i, el in enumerate(row):\n                self._max_widths[i] = max(get_text_length(el), self._max_widths[i])\n", "awscli/topictags.py": "# Copyright (c) 2015 Amazon.com, Inc. or its affiliates.  All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nimport os\nimport json\nimport docutils.core\n\n\nclass TopicTagDB(object):\n    \"\"\"This class acts like a database for the tags of all available topics.\n\n    A tag is an element in a topic reStructured text file that contains\n    information about a topic. Information can range from titles to even\n    related CLI commands. Here are all of the currently supported tags:\n\n    Tag                 Meaning                         Required?\n    ---                 -------                         ---------\n    :title:             The title of the topic          Yes\n    :description:       Sentence description of topic   Yes\n    :category:          Category topic falls under      Yes\n    :related topic:     A related topic                 No\n    :related command:   A related command               No\n\n    To see examples of how to specify tags, look in the directory\n    awscli/topics. Note that tags can have multiple values by delimiting\n    values with commas. All tags must be on their own line in the file.\n\n    This class can load a JSON index representing all topics and their tags,\n    scan all of the topics and store the values of their tags, retrieve the\n    tag value for a particular topic, query for all the topics with a specific\n    tag and/or value, and save the loaded data back out to a JSON index.\n\n    The structure of the database can be viewed as a python dictionary:\n\n    {'topic-name-1': {\n        'title': ['My First Topic Title'],\n        'description': ['This describes my first topic'],\n        'category': ['General Topics', 'S3'],\n        'related command': ['aws s3'],\n        'related topic': ['topic-name-2']\n     },\n     'topic-name-2': { .....\n    }\n\n    The keys of the dictionary are the CLI command names of the topics. These\n    names are based off the name of the reStructed text file that corresponds\n    to the topic. The value of these keys are dictionaries of tags, where the\n    tags are keys and their value is a list of values for that tag. Note\n    that all tag values for a specific tag of a specific topic are unique.\n    \"\"\"\n\n    VALID_TAGS = ['category', 'description', 'title', 'related topic',\n                  'related command']\n\n    # The default directory to look for topics.\n    TOPIC_DIR = os.path.join(\n        os.path.dirname(\n            os.path.abspath(__file__)), 'topics')\n\n    # The default JSON index to load.\n    JSON_INDEX = os.path.join(TOPIC_DIR, 'topic-tags.json')\n\n    def __init__(self, tag_dictionary=None, index_file=JSON_INDEX,\n                 topic_dir=TOPIC_DIR):\n        \"\"\"\n        :param index_file: The path to a specific JSON index to load.\n            If nothing is specified it will default to the default JSON\n            index at ``JSON_INDEX``.\n\n        :param topic_dir: The path to the directory where to retrieve\n            the topic source files. Note that if you store your index\n            in this directory, you must supply the full path to the json\n            index to the ``file_index`` argument as it may not be ignored when\n            listing topic source files. If nothing is specified it will\n            default to the default directory at ``TOPIC_DIR``.\n        \"\"\"\n        self._tag_dictionary = tag_dictionary\n        if self._tag_dictionary is None:\n            self._tag_dictionary = {}\n\n        self._index_file = index_file\n        self._topic_dir = topic_dir\n\n    @property\n    def index_file(self):\n        return self._index_file\n\n    @index_file.setter\n    def index_file(self, value):\n        self._index_file = value\n\n    @property\n    def topic_dir(self):\n        return self._topic_dir\n\n    @topic_dir.setter\n    def topic_dir(self, value):\n        self._topic_dir = value\n\n    @property\n    def valid_tags(self):\n        return self.VALID_TAGS\n\n    def load_json_index(self):\n        \"\"\"Loads a JSON file into the tag dictionary.\"\"\"\n        with open(self.index_file, 'r') as f:\n            self._tag_dictionary = json.load(f)\n\n    def save_to_json_index(self):\n        \"\"\"Writes the loaded data back out to the JSON index.\"\"\"\n        with open(self.index_file, 'w') as f:\n            f.write(json.dumps(self._tag_dictionary, indent=4, sort_keys=True))\n\n    def get_all_topic_names(self):\n        \"\"\"Retrieves all of the topic names of the loaded JSON index\"\"\"\n        return list(self._tag_dictionary)\n\n    def get_all_topic_src_files(self):\n        \"\"\"Retrieves the file paths of all the topics in directory\"\"\"\n        topic_full_paths = []\n        topic_names = os.listdir(self.topic_dir)\n        for topic_name in topic_names:\n            # Do not try to load hidden files.\n            if not topic_name.startswith('.'):\n                topic_full_path = os.path.join(self.topic_dir, topic_name)\n                # Ignore the JSON Index as it is stored with topic files.\n                if topic_full_path != self.index_file:\n                    topic_full_paths.append(topic_full_path)\n        return topic_full_paths\n\n    def scan(self, topic_files):\n        \"\"\"Scan in the tags of a list of topics into memory.\n\n        Note that if there are existing values in an entry in the database\n        of tags, they will not be overwritten. Any new values will be\n        appended to original values.\n\n        :param topic_files: A list of paths to topics to scan into memory.\n        \"\"\"\n        for topic_file in topic_files:\n            with open(topic_file, 'r') as f:\n                # Parse out the name of the topic\n                topic_name = self._find_topic_name(topic_file)\n                # Add the topic to the dictionary if it does not exist\n                self._add_topic_name_to_dict(topic_name)\n                topic_content = f.read()\n                # Record the tags and the values\n                self._add_tag_and_values_from_content(\n                    topic_name, topic_content)\n\n    def _find_topic_name(self, topic_src_file):\n        # Get the name of each of these files\n        topic_name_with_ext = os.path.basename(topic_src_file)\n        # Strip of the .rst extension from the files\n        return topic_name_with_ext[:-4]\n\n    def _add_tag_and_values_from_content(self, topic_name, content):\n        # Retrieves tags and values and adds from content of topic file\n        # to the dictionary.\n        doctree = docutils.core.publish_doctree(content).asdom()\n        fields = doctree.getElementsByTagName('field')\n        for field in fields:\n            field_name = field.getElementsByTagName('field_name')[0]\n            field_body = field.getElementsByTagName('field_body')[0]\n            # Get the tag.\n            tag = field_name.firstChild.nodeValue\n            if tag in self.VALID_TAGS:\n                # Get the value of the tag.\n                values = field_body.childNodes[0].firstChild.nodeValue\n                # Separate values into a list by splitting at commas\n                tag_values = values.split(',')\n                # Strip the white space around each of these values.\n                for i in range(len(tag_values)):\n                    tag_values[i] = tag_values[i].strip()\n                self._add_tag_to_dict(topic_name, tag, tag_values)\n            else:\n                raise ValueError(\n                    \"Tag %s found under topic %s is not supported.\"\n                    % (tag, topic_name)\n                )\n\n    def _add_topic_name_to_dict(self, topic_name):\n        # This method adds a topic name to the dictionary if it does not\n        # already exist\n\n        # Check if the topic is in the topic tag dictionary\n        if self._tag_dictionary.get(topic_name, None) is None:\n            self._tag_dictionary[topic_name] = {}\n\n    def _add_tag_to_dict(self, topic_name, tag, values):\n        # This method adds a tag to the dictionary given its tag and value\n        # If there are existing values associated to the tag it will add\n        # only values that previously did not exist in the list.\n\n        # Add topic to the topic tag dictionary if needed.\n        self._add_topic_name_to_dict(topic_name)\n        # Get all of a topics tags\n        topic_tags = self._tag_dictionary[topic_name]\n        self._add_key_values(topic_tags, tag, values)\n\n    def _add_key_values(self, dictionary, key, values):\n        # This method adds a value to a dictionary given a key.\n        # If there are existing values associated to the key it will add\n        # only values that previously did not exist in the list. All values\n        # in the dictionary should be lists\n\n        if dictionary.get(key, None) is None:\n            dictionary[key] = []\n        for value in values:\n            if value not in dictionary[key]:\n                dictionary[key].append(value)\n\n    def query(self, tag, values=None):\n        \"\"\"Groups topics by a specific tag and/or tag value.\n\n        :param tag: The name of the tag to query for.\n        :param values: A list of tag values to only include in query.\n            If no value is provided, all possible tag values will be returned\n\n        :rtype: dictionary\n        :returns: A dictionary whose keys are all possible tag values and the\n            keys' values are all of the topic names that had that tag value\n            in its source file. For example, if ``topic-name-1`` had the tag\n            ``:category: foo, bar`` and ``topic-name-2`` had the tag\n            ``:category: foo`` and we queried based on ``:category:``,\n            the returned dictionary would be:\n\n            {\n             'foo': ['topic-name-1', 'topic-name-2'],\n             'bar': ['topic-name-1']\n            }\n\n        \"\"\"\n        query_dict = {}\n        for topic_name in self._tag_dictionary.keys():\n            # Get the tag values for a specified tag of the topic\n            if self._tag_dictionary[topic_name].get(tag, None) is not None:\n                tag_values = self._tag_dictionary[topic_name][tag]\n                for tag_value in tag_values:\n                    # Add the values to dictionary to be returned if\n                    # no value constraints are provided or if the tag value\n                    # falls in the allowed tag values.\n                    if values is None or tag_value in values:\n                        self._add_key_values(query_dict,\n                                             key=tag_value,\n                                             values=[topic_name])\n        return query_dict\n\n    def get_tag_value(self, topic_name, tag, default_value=None):\n        \"\"\"Get a value of a tag for a topic\n\n        :param topic_name: The name of the topic\n        :param tag: The name of the tag to retrieve\n        :param default_value: The value to return if the topic and/or tag\n            does not exist.\n        \"\"\"\n        if topic_name in self._tag_dictionary:\n            return self._tag_dictionary[topic_name].get(tag, default_value)\n        return default_value\n\n    def get_tag_single_value(self, topic_name, tag):\n        \"\"\"Get the value of a tag for a topic (i.e. not wrapped in a list)\n\n        :param topic_name: The name of the topic\n        :param tag: The name of the tag to retrieve\n        :raises ValueError: Raised if there is not exactly one value\n            in the list value.\n        \"\"\"\n        value = self.get_tag_value(topic_name, tag)\n        if value is not None:\n            if len(value) != 1:\n                raise ValueError(\n                    'Tag %s for topic %s has value %s. Expected a single '\n                    'element in list.' % (tag, topic_name, value)\n                )\n            value = value[0]\n        return value\n", "awscli/schema.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom collections import defaultdict\n\n\nclass ParameterRequiredError(ValueError):\n    pass\n\n\nclass SchemaTransformer(object):\n    \"\"\"\n    Transforms a custom argument parameter schema into an internal\n    model representation so that it can be treated like a normal\n    service model. This includes shorthand JSON parsing and\n    automatic documentation generation. The format of the schema\n    follows JSON Schema, which can be found here:\n\n    http://json-schema.org/\n\n    Only a relevant subset of features is supported here:\n\n    * Types: `object`, `array`, `string`, `integer`, `boolean`\n    * Properties: `type`, `description`, `required`, `enum`\n\n    For example::\n\n    {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"arg1\": {\n                    \"type\": \"string\",\n                    \"required\": True,\n                    \"enum\": [\n                        \"Value1\",\n                        \"Value2\",\n                        \"Value3\"\n                    ]\n                },\n                \"arg2\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The number of calls\"\n                }\n            }\n        }\n    }\n\n    Assuming the schema is applied to a service named `foo`, with an\n    operation named `bar` and that the parameter is called `baz`, you\n    could call it with the shorthand JSON like so::\n\n        $ aws foo bar --baz arg1=Value1,arg2=5 arg1=Value2\n\n    \"\"\"\n    JSON_SCHEMA_TO_AWS_TYPES = {\n        'object': 'structure',\n        'array': 'list',\n    }\n\n    def __init__(self):\n        self._shape_namer = ShapeNameGenerator()\n\n    def transform(self, schema):\n        \"\"\"Convert JSON schema to the format used internally by the AWS CLI.\n\n        :type schema: dict\n        :param schema: The JSON schema describing the argument model.\n\n        :rtype: dict\n        :return: The transformed model in a form that can be consumed\n            internally by the AWS CLI.  The dictionary returned will\n            have a list of shapes, where the shape representing the\n            transformed schema is always named ``InputShape`` in the\n            returned dictionary.\n\n        \"\"\"\n        shapes = {}\n        self._transform(schema, shapes, 'InputShape')\n        return shapes\n\n    def _transform(self, schema, shapes, shape_name):\n        if 'type' not in schema:\n            raise ParameterRequiredError(\"Missing required key: 'type'\")\n        if schema['type'] == 'object':\n            shapes[shape_name] = self._transform_structure(schema, shapes)\n        elif schema['type'] == 'array':\n            shapes[shape_name] = self._transform_list(schema, shapes)\n        elif schema['type'] == 'map':\n            shapes[shape_name] = self._transform_map(schema, shapes)\n        else:\n            shapes[shape_name] = self._transform_scalar(schema)\n        return shapes\n\n    def _transform_scalar(self, schema):\n        return self._populate_initial_shape(schema)\n\n    def _transform_structure(self, schema, shapes):\n        # Transforming a structure involves:\n        # 1. Generating the shape definition for the structure\n        # 2. Generating the shape definitions for its members\n        structure_shape = self._populate_initial_shape(schema)\n        members = {}\n        required_members = []\n\n        for key, value in schema['properties'].items():\n            current_type_name = self._json_schema_to_aws_type(value)\n            current_shape_name = self._shape_namer.new_shape_name(\n                current_type_name)\n            members[key] = {'shape': current_shape_name}\n            if value.get('required', False):\n                required_members.append(key)\n            self._transform(value, shapes, current_shape_name)\n        structure_shape['members'] = members\n        if required_members:\n            structure_shape['required'] = required_members\n        return structure_shape\n\n    def _transform_map(self, schema, shapes):\n        structure_shape = self._populate_initial_shape(schema)\n        for attribute in ['key', 'value']:\n            type_name = self._json_schema_to_aws_type(schema[attribute])\n            shape_name = self._shape_namer.new_shape_name(type_name)\n            structure_shape[attribute] = {'shape': shape_name}\n            self._transform(schema[attribute], shapes, shape_name)\n        return structure_shape\n\n    def _transform_list(self, schema, shapes):\n        # Transforming a structure involves:\n        # 1. Generating the shape definition for the structure\n        # 2. Generating the shape definitions for its 'items' member\n        list_shape = self._populate_initial_shape(schema)\n        member_type = self._json_schema_to_aws_type(schema['items'])\n        member_shape_name = self._shape_namer.new_shape_name(member_type)\n        list_shape['member'] = {'shape': member_shape_name}\n        self._transform(schema['items'], shapes, member_shape_name)\n        return list_shape\n\n    def _populate_initial_shape(self, schema):\n        shape = {'type': self._json_schema_to_aws_type(schema)}\n        if 'description' in schema:\n            shape['documentation'] = schema['description']\n        if 'enum' in schema:\n            shape['enum'] = schema['enum']\n        return shape\n\n    def _json_schema_to_aws_type(self, schema):\n        if 'type' not in schema:\n            raise ParameterRequiredError(\"Missing required key: 'type'\")\n        type_name = schema['type']\n        return self.JSON_SCHEMA_TO_AWS_TYPES.get(type_name, type_name)\n\n\nclass ShapeNameGenerator(object):\n    def __init__(self):\n        self._name_cache = defaultdict(int)\n\n    def new_shape_name(self, type_name):\n        self._name_cache[type_name] += 1\n        current_index = self._name_cache[type_name]\n        return '%sType%s' % (type_name.capitalize(), current_index)\n", "awscli/utils.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport csv\nimport signal\nimport datetime\nimport contextlib\nimport os\nimport sys\nimport subprocess\n\nfrom awscli.compat import (\n    BytesIO, StringIO, get_binary_stdout, get_popen_kwargs_for_pager_cmd\n)\n\n\ndef split_on_commas(value):\n    if not any(char in value for char in ['\"', '\\\\', \"'\", ']', '[']):\n        # No quotes or escaping, just use a simple split.\n        return value.split(',')\n    elif not any(char in value for char in ['\"', \"'\", '[', ']']):\n        # Simple escaping, let the csv module handle it.\n        return list(csv.reader(StringIO(value), escapechar='\\\\'))[0]\n    else:\n        # If there's quotes for the values, we have to handle this\n        # ourselves.\n        return _split_with_quotes(value)\n\n\ndef _split_with_quotes(value):\n    try:\n        parts = list(csv.reader(StringIO(value), escapechar='\\\\'))[0]\n    except csv.Error:\n        raise ValueError(\"Bad csv value: %s\" % value)\n    iter_parts = iter(parts)\n    new_parts = []\n    for part in iter_parts:\n        # Find the first quote\n        quote_char = _find_quote_char_in_part(part)\n\n        # Find an opening list bracket\n        list_start = part.find('=[')\n\n        if list_start >= 0 and value.find(']') != -1 and \\\n           (quote_char is None or part.find(quote_char) > list_start):\n            # This is a list, eat all the items until the end\n            if ']' in part:\n                # Short circuit for only one item\n                new_chunk = part\n            else:\n                new_chunk = _eat_items(value, iter_parts, part, ']')\n            list_items = _split_with_quotes(new_chunk[list_start + 2:-1])\n            new_chunk = new_chunk[:list_start + 1] + ','.join(list_items)\n            new_parts.append(new_chunk)\n            continue\n        elif quote_char is None:\n            new_parts.append(part)\n            continue\n        elif part.count(quote_char) == 2:\n            # Starting and ending quote are in this part.\n            # While it's not needed right now, this will\n            # break down if we ever need to escape quotes while\n            # quoting a value.\n            new_parts.append(part.replace(quote_char, ''))\n            continue\n        # Now that we've found a starting quote char, we\n        # need to combine the parts until we encounter an end quote.\n        new_chunk = _eat_items(value, iter_parts, part, quote_char, quote_char)\n        new_parts.append(new_chunk)\n    return new_parts\n\n\ndef _eat_items(value, iter_parts, part, end_char, replace_char=''):\n    \"\"\"\n    Eat items from an iterator, optionally replacing characters with\n    a blank and stopping when the end_char has been reached.\n    \"\"\"\n    current = part\n    chunks = [current.replace(replace_char, '')]\n    while True:\n        try:\n            current = next(iter_parts)\n        except StopIteration:\n            raise ValueError(value)\n        chunks.append(current.replace(replace_char, ''))\n        if current.endswith(end_char):\n            break\n    return ','.join(chunks)\n\n\ndef _find_quote_char_in_part(part):\n    \"\"\"\n    Returns a single or double quote character, whichever appears first in the\n    given string. None is returned if the given string doesn't have a single or\n    double quote character.\n    \"\"\"\n    quote_char = None\n    for ch in part:\n        if ch in ('\"', \"'\"):\n            quote_char = ch\n            break\n    return quote_char\n\n\ndef find_service_and_method_in_event_name(event_name):\n    \"\"\"\n    Grabs the service id and the operation name from an event name.\n    This is making the assumption that the event name is in the form\n    event.service.operation.\n    \"\"\"\n    split_event = event_name.split('.')[1:]\n    service_name = None\n    if len(split_event) > 0:\n        service_name = split_event[0]\n\n    operation_name = None\n    if len(split_event) > 1:\n        operation_name = split_event[1]\n    return service_name, operation_name\n\n\ndef is_document_type(shape):\n    \"\"\"Check if shape is a document type\"\"\"\n    return getattr(shape, 'is_document_type', False)\n\n\ndef is_document_type_container(shape):\n    \"\"\"Check if the shape is a document type or wraps document types\n\n    This is helpful to determine if a shape purely deals with document types\n    whether the shape is a document type or it is lists or maps whose base\n    values are document types.\n    \"\"\"\n    if not shape:\n        return False\n    recording_visitor = ShapeRecordingVisitor()\n    ShapeWalker().walk(shape, recording_visitor)\n    end_shape = recording_visitor.visited.pop()\n    if not is_document_type(end_shape):\n        return False\n    for shape in recording_visitor.visited:\n        if shape.type_name not in ['list', 'map']:\n            return False\n    return True\n\n\ndef is_streaming_blob_type(shape):\n    \"\"\"Check if the shape is a streaming blob type.\"\"\"\n    return (shape and shape.type_name == 'blob' and\n            shape.serialization.get('streaming', False))\n\n\ndef is_tagged_union_type(shape):\n    \"\"\"Check if the shape is a tagged union structure.\"\"\"\n    return getattr(shape, 'is_tagged_union', False)\n\n\ndef operation_uses_document_types(operation_model):\n    \"\"\"Check if document types are ever used in the operation\"\"\"\n    recording_visitor = ShapeRecordingVisitor()\n    walker = ShapeWalker()\n    walker.walk(operation_model.input_shape, recording_visitor)\n    walker.walk(operation_model.output_shape, recording_visitor)\n    for visited_shape in recording_visitor.visited:\n        if is_document_type(visited_shape):\n            return True\n    return False\n\n\ndef json_encoder(obj):\n    \"\"\"JSON encoder that formats datetimes as ISO8601 format.\"\"\"\n    if isinstance(obj, datetime.datetime):\n        return obj.isoformat()\n    else:\n        return obj\n\n\n@contextlib.contextmanager\ndef ignore_ctrl_c():\n    original = signal.signal(signal.SIGINT, signal.SIG_IGN)\n    try:\n        yield\n    finally:\n        signal.signal(signal.SIGINT, original)\n\n\ndef emit_top_level_args_parsed_event(session, args):\n    session.emit(\n        'top-level-args-parsed', parsed_args=args, session=session)\n\n\ndef is_a_tty():\n    try:\n        return os.isatty(sys.stdout.fileno())\n    except Exception as e:\n        return False\n\n\nclass OutputStreamFactory(object):\n    def __init__(self, popen=None):\n        self._popen = popen\n        if popen is None:\n            self._popen = subprocess.Popen\n\n    @contextlib.contextmanager\n    def get_pager_stream(self, preferred_pager=None):\n        popen_kwargs = self._get_process_pager_kwargs(preferred_pager)\n        try:\n            process = self._popen(**popen_kwargs)\n            yield process.stdin\n        except IOError:\n            # Ignore IOError since this can commonly be raised when a pager\n            # is closed abruptly and causes a broken pipe.\n            pass\n        finally:\n            process.communicate()\n\n    @contextlib.contextmanager\n    def get_stdout_stream(self):\n        yield get_binary_stdout()\n\n    def _get_process_pager_kwargs(self, pager_cmd):\n        kwargs = get_popen_kwargs_for_pager_cmd(pager_cmd)\n        kwargs['stdin'] = subprocess.PIPE\n        return kwargs\n\n\ndef write_exception(ex, outfile):\n    outfile.write(\"\\n\")\n    outfile.write(str(ex))\n    outfile.write(\"\\n\")\n\n\nclass ShapeWalker(object):\n    def walk(self, shape, visitor):\n        \"\"\"Walk through and visit shapes for introspection\n\n        :type shape: botocore.model.Shape\n        :param shape: Shape to walk\n\n        :type visitor: BaseShapeVisitor\n        :param visitor: The visitor to call when walking a shape\n        \"\"\"\n\n        if shape is None:\n            return\n        stack = []\n        return self._walk(shape, visitor, stack)\n\n    def _walk(self, shape, visitor, stack):\n        if shape.name in stack:\n            return\n        stack.append(shape.name)\n        getattr(self, '_walk_%s' % shape.type_name, self._default_scalar_walk)(\n            shape, visitor, stack\n        )\n        stack.pop()\n\n    def _walk_structure(self, shape, visitor, stack):\n        self._do_shape_visit(shape, visitor)\n        for _, member_shape in shape.members.items():\n            self._walk(member_shape, visitor, stack)\n\n    def _walk_list(self, shape, visitor, stack):\n        self._do_shape_visit(shape, visitor)\n        self._walk(shape.member, visitor, stack)\n\n    def _walk_map(self, shape, visitor, stack):\n        self._do_shape_visit(shape, visitor)\n        self._walk(shape.value, visitor, stack)\n\n    def _default_scalar_walk(self, shape, visitor, stack):\n        self._do_shape_visit(shape, visitor)\n\n    def _do_shape_visit(self, shape, visitor):\n        visitor.visit_shape(shape)\n\n\nclass BaseShapeVisitor(object):\n    \"\"\"Visit shape encountered by ShapeWalker\"\"\"\n    def visit_shape(self, shape):\n        pass\n\n\nclass ShapeRecordingVisitor(BaseShapeVisitor):\n    \"\"\"Record shapes visited by ShapeWalker\"\"\"\n    def __init__(self):\n        self.visited = []\n\n    def visit_shape(self, shape):\n        self.visited.append(shape)\n", "awscli/paramfile.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\nimport copy\n\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.httpsession import URLLib3Session\nfrom botocore.exceptions import ProfileNotFound\n\nfrom awscli.compat import compat_open\nfrom awscli.argprocess import ParamError\n\n\nlogger = logging.getLogger(__name__)\n\n# These are special cased arguments that do _not_ get the\n# special param file processing.  This is typically because it\n# refers to an actual URI of some sort and we don't want to actually\n# download the content (i.e TemplateURL in cloudformation).\nPARAMFILE_DISABLED = set([\n    'api-gateway.put-integration.uri',\n    'api-gateway.create-integration.integration-uri',\n    'api-gateway.update-integration.integration-uri',\n    'api-gateway.create-api.target',\n    'api-gateway.update-api.target',\n    'appstream.create-stack.redirect-url',\n    'appstream.create-stack.feedback-url',\n    'appstream.update-stack.redirect-url',\n    'appstream.update-stack.feedback-url',\n    'cloudformation.create-stack.template-url',\n    'cloudformation.update-stack.template-url',\n    'cloudformation.create-stack-set.template-url',\n    'cloudformation.update-stack-set.template-url',\n    'cloudformation.create-change-set.template-url',\n    'cloudformation.validate-template.template-url',\n    'cloudformation.estimate-template-cost.template-url',\n    'cloudformation.get-template-summary.template-url',\n\n    'cloudformation.create-stack.stack-policy-url',\n    'cloudformation.update-stack.stack-policy-url',\n    'cloudformation.set-stack-policy.stack-policy-url',\n    # aws cloudformation package --template-file\n    'custom.package.template-file',\n    # aws cloudformation deploy --template-file\n    'custom.deploy.template-file',\n\n    'cloudformation.update-stack.stack-policy-during-update-url',\n    'cloudformation.register-type.schema-handler-package',\n    # We will want to change the event name to ``s3`` as opposed to\n    # custom in the near future along with ``s3`` to ``s3api``.\n    'custom.cp.website-redirect',\n    'custom.mv.website-redirect',\n    'custom.sync.website-redirect',\n\n    'guardduty.create-ip-set.location',\n    'guardduty.update-ip-set.location',\n    'guardduty.create-threat-intel-set.location',\n    'guardduty.update-threat-intel-set.location',\n    'comprehend.detect-dominant-language.text',\n    'comprehend.batch-detect-dominant-language.text-list',\n    'comprehend.detect-entities.text',\n    'comprehend.batch-detect-entities.text-list',\n    'comprehend.detect-key-phrases.text',\n    'comprehend.batch-detect-key-phrases.text-list',\n    'comprehend.detect-sentiment.text',\n    'comprehend.batch-detect-sentiment.text-list',\n\n    'emr.create-studio.idp-auth-url',\n\n    'iam.create-open-id-connect-provider.url',\n\n    'machine-learning.predict.predict-endpoint',\n\n    'mediatailor.put-playback-configuration.ad-decision-server-url',\n    'mediatailor.put-playback-configuration.slate-ad-url',\n    'mediatailor.put-playback-configuration.video-content-source-url',\n\n    'rds.copy-db-cluster-snapshot.pre-signed-url',\n    'rds.create-db-cluster.pre-signed-url',\n    'rds.copy-db-snapshot.pre-signed-url',\n    'rds.create-db-instance-read-replica.pre-signed-url',\n\n    'sagemaker.create-notebook-instance.default-code-repository',\n    'sagemaker.create-notebook-instance.additional-code-repositories',\n    'sagemaker.update-notebook-instance.default-code-repository',\n    'sagemaker.update-notebook-instance.additional-code-repositories',\n\n    'serverlessapplicationrepository.create-application.home-page-url',\n    'serverlessapplicationrepository.create-application.license-url',\n    'serverlessapplicationrepository.create-application.readme-url',\n    'serverlessapplicationrepository.create-application.source-code-url',\n    'serverlessapplicationrepository.create-application.template-url',\n    'serverlessapplicationrepository.create-application-version.source-code-url',\n    'serverlessapplicationrepository.create-application-version.template-url',\n    'serverlessapplicationrepository.update-application.home-page-url',\n    'serverlessapplicationrepository.update-application.readme-url',\n\n    'service-catalog.create-product.support-url',\n    'service-catalog.update-product.support-url',\n\n    'ses.create-custom-verification-email-template.failure-redirection-url',\n    'ses.create-custom-verification-email-template.success-redirection-url',\n    'ses.put-account-details.website-url',\n    'ses.update-custom-verification-email-template.failure-redirection-url',\n    'ses.update-custom-verification-email-template.success-redirection-url',\n\n    'sqs.add-permission.queue-url',\n    'sqs.change-message-visibility.queue-url',\n    'sqs.change-message-visibility-batch.queue-url',\n    'sqs.delete-message.queue-url',\n    'sqs.delete-message-batch.queue-url',\n    'sqs.delete-queue.queue-url',\n    'sqs.get-queue-attributes.queue-url',\n    'sqs.list-dead-letter-source-queues.queue-url',\n    'sqs.receive-message.queue-url',\n    'sqs.remove-permission.queue-url',\n    'sqs.send-message.queue-url',\n    'sqs.send-message-batch.queue-url',\n    'sqs.set-queue-attributes.queue-url',\n    'sqs.purge-queue.queue-url',\n    'sqs.list-queue-tags.queue-url',\n    'sqs.tag-queue.queue-url',\n    'sqs.untag-queue.queue-url',\n\n    's3.copy-object.website-redirect-location',\n    's3.create-multipart-upload.website-redirect-location',\n    's3.put-object.website-redirect-location',\n\n    # Double check that this has been renamed!\n    'sns.subscribe.notification-endpoint',\n\n    'iot.create-job.document-source',\n    'translate.translate-text.text',\n\n    'workdocs.create-notification-subscription.notification-endpoint'\n])\n\n\nclass ResourceLoadingError(Exception):\n    pass\n\n\ndef register_uri_param_handler(session, **kwargs):\n    prefix_map = copy.deepcopy(LOCAL_PREFIX_MAP)\n    try:\n        fetch_url = session.get_scoped_config().get(\n            'cli_follow_urlparam', 'true') == 'true'\n    except ProfileNotFound:\n        # If a --profile is provided that does not exist, loading\n        # a value from get_scoped_config will crash the CLI.\n        # This function can be called as the first handler for\n        # the session-initialized event, which happens before a\n        # profile can be created, even if the command would have\n        # successfully created a profile. Instead of crashing here\n        # on a ProfileNotFound the CLI should just use 'none'.\n        fetch_url = True\n\n    if fetch_url:\n        prefix_map.update(REMOTE_PREFIX_MAP)\n\n    handler = URIArgumentHandler(prefix_map)\n    session.register('load-cli-arg', handler)\n\n\nclass URIArgumentHandler(object):\n    def __init__(self, prefixes=None):\n        if prefixes is None:\n            prefixes = copy.deepcopy(LOCAL_PREFIX_MAP)\n            prefixes.update(REMOTE_PREFIX_MAP)\n        self._prefixes = prefixes\n\n    def __call__(self, event_name, param, value, **kwargs):\n        \"\"\"Handler that supports param values from URIs.\"\"\"\n        cli_argument = param\n        qualified_param_name = '.'.join(event_name.split('.')[1:])\n        if qualified_param_name in PARAMFILE_DISABLED or \\\n                getattr(cli_argument, 'no_paramfile', None):\n            return\n        else:\n            return self._check_for_uri_param(cli_argument, value)\n\n    def _check_for_uri_param(self, param, value):\n        if isinstance(value, list) and len(value) == 1:\n            value = value[0]\n        try:\n            return get_paramfile(value, self._prefixes)\n        except ResourceLoadingError as e:\n            raise ParamError(param.cli_name, str(e))\n\n\ndef get_paramfile(path, cases):\n    \"\"\"Load parameter based on a resource URI.\n\n    It is possible to pass parameters to operations by referring\n    to files or URI's.  If such a reference is detected, this\n    function attempts to retrieve the data from the file or URI\n    and returns it.  If there are any errors or if the ``path``\n    does not appear to refer to a file or URI, a ``None`` is\n    returned.\n\n    :type path: str\n    :param path: The resource URI, e.g. file://foo.txt.  This value\n        may also be a non resource URI, in which case ``None`` is returned.\n\n    :type cases: dict\n    :param cases: A dictionary of URI prefixes to function mappings\n        that a parameter is checked against.\n\n    :return: The loaded value associated with the resource URI.\n        If the provided ``path`` is not a resource URI, then a\n        value of ``None`` is returned.\n\n    \"\"\"\n    data = None\n    if isinstance(path, str):\n        for prefix, function_spec in cases.items():\n            if path.startswith(prefix):\n                function, kwargs = function_spec\n                data = function(prefix, path, **kwargs)\n    return data\n\n\ndef get_file(prefix, path, mode):\n    file_path = os.path.expandvars(os.path.expanduser(path[len(prefix):]))\n    try:\n        with compat_open(file_path, mode) as f:\n            return f.read()\n    except UnicodeDecodeError:\n        raise ResourceLoadingError(\n            'Unable to load paramfile (%s), text contents could '\n            'not be decoded.  If this is a binary file, please use the '\n            'fileb:// prefix instead of the file:// prefix.' % file_path)\n    except (OSError, IOError) as e:\n        raise ResourceLoadingError('Unable to load paramfile %s: %s' % (\n            path, e))\n\n\ndef get_uri(prefix, uri):\n    try:\n        session = URLLib3Session()\n        r = session.send(AWSRequest('GET', uri).prepare())\n        if r.status_code == 200:\n            return r.text\n        else:\n            raise ResourceLoadingError(\n                \"received non 200 status code of %s\" % (\n                    r.status_code))\n    except Exception as e:\n        raise ResourceLoadingError('Unable to retrieve %s: %s' % (uri, e))\n\n\nLOCAL_PREFIX_MAP = {\n    'file://': (get_file, {'mode': 'r'}),\n    'fileb://': (get_file, {'mode': 'rb'}),\n}\n\n\nREMOTE_PREFIX_MAP = {\n    'http://': (get_uri, {}),\n    'https://': (get_uri, {}),\n}\n", "awscli/clidriver.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport sys\nimport signal\nimport logging\n\nimport botocore.session\nfrom botocore import __version__ as botocore_version\nfrom botocore.hooks import HierarchicalEmitter\nfrom botocore import xform_name\nfrom botocore.compat import copy_kwargs, OrderedDict\nfrom botocore.exceptions import NoCredentialsError\nfrom botocore.exceptions import NoRegionError\nfrom botocore.exceptions import ProfileNotFound\nfrom botocore.history import get_global_history_recorder\n\nfrom awscli import EnvironmentVariables, __version__\nfrom awscli.compat import get_stderr_text_writer\nfrom awscli.formatter import get_formatter\nfrom awscli.plugin import load_plugins\nfrom awscli.commands import CLICommand\nfrom awscli.argparser import MainArgParser\nfrom awscli.argparser import ServiceArgParser\nfrom awscli.argparser import ArgTableArgParser\nfrom awscli.argparser import USAGE\nfrom awscli.help import ProviderHelpCommand\nfrom awscli.help import ServiceHelpCommand\nfrom awscli.help import OperationHelpCommand\nfrom awscli.arguments import CustomArgument\nfrom awscli.arguments import ListArgument\nfrom awscli.arguments import BooleanArgument\nfrom awscli.arguments import CLIArgument\nfrom awscli.arguments import UnknownArgumentError\nfrom awscli.argprocess import unpack_argument\nfrom awscli.alias import AliasLoader\nfrom awscli.alias import AliasCommandInjector\nfrom awscli.utils import emit_top_level_args_parsed_event\nfrom awscli.utils import write_exception\n\n\nLOG = logging.getLogger('awscli.clidriver')\nLOG_FORMAT = (\n    '%(asctime)s - %(threadName)s - %(name)s - %(levelname)s - %(message)s')\nHISTORY_RECORDER = get_global_history_recorder()\n# Don't remove this line.  The idna encoding\n# is used by getaddrinfo when dealing with unicode hostnames,\n# and in some cases, there appears to be a race condition\n# where threads will get a LookupError on getaddrinfo() saying\n# that the encoding doesn't exist.  Using the idna encoding before\n# running any CLI code (and any threads it may create) ensures that\n# the encodings.idna is imported and registered in the codecs registry,\n# which will stop the LookupErrors from happening.\n# See: https://bugs.python.org/issue29288\nu''.encode('idna')\n\n\ndef main():\n    driver = create_clidriver()\n    rc = driver.main()\n    HISTORY_RECORDER.record('CLI_RC', rc, 'CLI')\n    return rc\n\n\ndef create_clidriver():\n    session = botocore.session.Session(EnvironmentVariables)\n    _set_user_agent_for_session(session)\n    load_plugins(session.full_config.get('plugins', {}),\n                 event_hooks=session.get_component('event_emitter'))\n    driver = CLIDriver(session=session)\n    return driver\n\n\ndef _set_user_agent_for_session(session):\n    session.user_agent_name = 'aws-cli'\n    session.user_agent_version = __version__\n    session.user_agent_extra = 'botocore/%s' % botocore_version\n\n\nclass CLIDriver(object):\n\n    def __init__(self, session=None):\n        if session is None:\n            self.session = botocore.session.get_session(EnvironmentVariables)\n            _set_user_agent_for_session(self.session)\n        else:\n            self.session = session\n        self._cli_data = None\n        self._command_table = None\n        self._argument_table = None\n        self.alias_loader = AliasLoader()\n\n    def _get_cli_data(self):\n        # Not crazy about this but the data in here is needed in\n        # several places (e.g. MainArgParser, ProviderHelp) so\n        # we load it here once.\n        if self._cli_data is None:\n            self._cli_data = self.session.get_data('cli')\n        return self._cli_data\n\n    def _get_command_table(self):\n        if self._command_table is None:\n            self._command_table = self._build_command_table()\n        return self._command_table\n\n    def _get_argument_table(self):\n        if self._argument_table is None:\n            self._argument_table = self._build_argument_table()\n        return self._argument_table\n\n    def _build_command_table(self):\n        \"\"\"\n        Create the main parser to handle the global arguments.\n\n        :rtype: ``argparser.ArgumentParser``\n        :return: The parser object\n\n        \"\"\"\n        command_table = self._build_builtin_commands(self.session)\n        self.session.emit('building-command-table.main',\n                          command_table=command_table,\n                          session=self.session,\n                          command_object=self)\n        return command_table\n\n    def _build_builtin_commands(self, session):\n        commands = OrderedDict()\n        services = session.get_available_services()\n        for service_name in services:\n            commands[service_name] = ServiceCommand(cli_name=service_name,\n                                                    session=self.session,\n                                                    service_name=service_name)\n        return commands\n\n    def _add_aliases(self, command_table, parser):\n        injector = AliasCommandInjector(\n            self.session, self.alias_loader)\n        injector.inject_aliases(command_table, parser)\n\n    def _build_argument_table(self):\n        argument_table = OrderedDict()\n        cli_data = self._get_cli_data()\n        cli_arguments = cli_data.get('options', None)\n        for option in cli_arguments:\n            option_params = copy_kwargs(cli_arguments[option])\n            cli_argument = self._create_cli_argument(option, option_params)\n            cli_argument.add_to_arg_table(argument_table)\n        # Then the final step is to send out an event so handlers\n        # can add extra arguments or modify existing arguments.\n        self.session.emit('building-top-level-params',\n                          argument_table=argument_table)\n        return argument_table\n\n    def _create_cli_argument(self, option_name, option_params):\n        return CustomArgument(\n            option_name, help_text=option_params.get('help', ''),\n            dest=option_params.get('dest'),\n            default=option_params.get('default'),\n            action=option_params.get('action'),\n            required=option_params.get('required'),\n            choices=option_params.get('choices'),\n            cli_type_name=option_params.get('type'))\n\n    def create_help_command(self):\n        cli_data = self._get_cli_data()\n        return ProviderHelpCommand(self.session, self._get_command_table(),\n                                   self._get_argument_table(),\n                                   cli_data.get('description', None),\n                                   cli_data.get('synopsis', None),\n                                   cli_data.get('help_usage', None))\n\n    def _create_parser(self, command_table):\n        # Also add a 'help' command.\n        command_table['help'] = self.create_help_command()\n        cli_data = self._get_cli_data()\n        parser = MainArgParser(\n            command_table, self.session.user_agent(),\n            cli_data.get('description', None),\n            self._get_argument_table(),\n            prog=\"aws\")\n        return parser\n\n    def main(self, args=None):\n        \"\"\"\n\n        :param args: List of arguments, with the 'aws' removed.  For example,\n            the command \"aws s3 list-objects --bucket foo\" will have an\n            args list of ``['s3', 'list-objects', '--bucket', 'foo']``.\n\n        \"\"\"\n        if args is None:\n            args = sys.argv[1:]\n        command_table = self._get_command_table()\n        parser = self._create_parser(command_table)\n        self._add_aliases(command_table, parser)\n        parsed_args, remaining = parser.parse_known_args(args)\n        try:\n            # Because _handle_top_level_args emits events, it's possible\n            # that exceptions can be raised, which should have the same\n            # general exception handling logic as calling into the\n            # command table.  This is why it's in the try/except clause.\n            self._handle_top_level_args(parsed_args)\n            self._emit_session_event(parsed_args)\n            HISTORY_RECORDER.record(\n                'CLI_VERSION', self.session.user_agent(), 'CLI')\n            HISTORY_RECORDER.record('CLI_ARGUMENTS', args, 'CLI')\n            return command_table[parsed_args.command](remaining, parsed_args)\n        except UnknownArgumentError as e:\n            sys.stderr.write(\"usage: %s\\n\" % USAGE)\n            sys.stderr.write(str(e))\n            sys.stderr.write(\"\\n\")\n            return 255\n        except NoRegionError as e:\n            msg = ('%s You can also configure your region by running '\n                   '\"aws configure\".' % e)\n            self._show_error(msg)\n            return 255\n        except NoCredentialsError as e:\n            msg = ('%s. You can configure credentials by running '\n                   '\"aws configure\".' % e)\n            self._show_error(msg)\n            return 255\n        except KeyboardInterrupt:\n            # Shell standard for signals that terminate\n            # the process is to return 128 + signum, in this case\n            # SIGINT=2, so we'll have an RC of 130.\n            sys.stdout.write(\"\\n\")\n            return 128 + signal.SIGINT\n        except Exception as e:\n            LOG.debug(\"Exception caught in main()\", exc_info=True)\n            LOG.debug(\"Exiting with rc 255\")\n            write_exception(e, outfile=get_stderr_text_writer())\n            return 255\n\n    def _emit_session_event(self, parsed_args):\n        # This event is guaranteed to run after the session has been\n        # initialized and a profile has been set.  This was previously\n        # problematic because if something in CLIDriver caused the\n        # session components to be reset (such as session.profile = foo)\n        # then all the prior registered components would be removed.\n        self.session.emit(\n            'session-initialized', session=self.session,\n            parsed_args=parsed_args)\n\n    def _show_error(self, msg):\n        LOG.debug(msg, exc_info=True)\n        sys.stderr.write(msg)\n        sys.stderr.write('\\n')\n\n    def _handle_top_level_args(self, args):\n        emit_top_level_args_parsed_event(self.session, args)\n        if args.profile:\n            self.session.set_config_variable('profile', args.profile)\n        if args.region:\n            self.session.set_config_variable('region', args.region)\n        if args.debug:\n            # TODO:\n            # Unfortunately, by setting debug mode here, we miss out\n            # on all of the debug events prior to this such as the\n            # loading of plugins, etc.\n            self.session.set_stream_logger('botocore', logging.DEBUG,\n                                           format_string=LOG_FORMAT)\n            self.session.set_stream_logger('awscli', logging.DEBUG,\n                                           format_string=LOG_FORMAT)\n            self.session.set_stream_logger('s3transfer', logging.DEBUG,\n                                           format_string=LOG_FORMAT)\n            self.session.set_stream_logger('urllib3', logging.DEBUG,\n                                           format_string=LOG_FORMAT)\n            LOG.debug(\"CLI version: %s\", self.session.user_agent())\n            LOG.debug(\"Arguments entered to CLI: %s\", sys.argv[1:])\n\n        else:\n            self.session.set_stream_logger(logger_name='awscli',\n                                           log_level=logging.ERROR)\n\n\nclass ServiceCommand(CLICommand):\n\n    \"\"\"A service command for the CLI.\n\n    For example, ``aws ec2 ...`` we'd create a ServiceCommand\n    object that represents the ec2 service.\n\n    \"\"\"\n\n    def __init__(self, cli_name, session, service_name=None):\n        # The cli_name is the name the user types, the name we show\n        # in doc, etc.\n        # The service_name is the name we used internally with botocore.\n        # For example, we have the 's3api' as the cli_name for the service\n        # but this is actually bound to the 's3' service name in botocore,\n        # i.e. we load s3.json from the botocore data dir.  Most of\n        # the time these are the same thing but in the case of renames,\n        # we want users/external things to be able to rename the cli name\n        # but *not* the service name, as this has to be exactly what\n        # botocore expects.\n        self._name = cli_name\n        self.session = session\n        self._command_table = None\n        if service_name is None:\n            # Then default to using the cli name.\n            self._service_name = cli_name\n        else:\n            self._service_name = service_name\n        self._lineage = [self]\n        self._service_model = None\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value\n\n    @property\n    def service_model(self):\n        return self._get_service_model()\n\n    @property\n    def lineage(self):\n        return self._lineage\n\n    @lineage.setter\n    def lineage(self, value):\n        self._lineage = value\n\n    def _get_command_table(self):\n        if self._command_table is None:\n            self._command_table = self._create_command_table()\n        return self._command_table\n\n    def _get_service_model(self):\n        if self._service_model is None:\n            try:\n                api_version = self.session.get_config_variable(\n                    'api_versions').get(self._service_name, None)\n            except ProfileNotFound:\n                api_version = None\n            self._service_model = self.session.get_service_model(\n                self._service_name, api_version=api_version)\n        return self._service_model\n\n    def __call__(self, args, parsed_globals):\n        # Once we know we're trying to call a service for this operation\n        # we can go ahead and create the parser for it.  We\n        # can also grab the Service object from botocore.\n        service_parser = self._create_parser()\n        parsed_args, remaining = service_parser.parse_known_args(args)\n        command_table = self._get_command_table()\n        return command_table[parsed_args.operation](remaining, parsed_globals)\n\n    def _create_command_table(self):\n        command_table = OrderedDict()\n        service_model = self._get_service_model()\n        for operation_name in service_model.operation_names:\n            cli_name = xform_name(operation_name, '-')\n            operation_model = service_model.operation_model(operation_name)\n            command_table[cli_name] = ServiceOperation(\n                name=cli_name,\n                parent_name=self._name,\n                session=self.session,\n                operation_model=operation_model,\n                operation_caller=CLIOperationCaller(self.session),\n            )\n        self.session.emit('building-command-table.%s' % self._name,\n                          command_table=command_table,\n                          session=self.session,\n                          command_object=self)\n        self._add_lineage(command_table)\n        return command_table\n\n    def _add_lineage(self, command_table):\n        for command in command_table:\n            command_obj = command_table[command]\n            command_obj.lineage = self.lineage + [command_obj]\n\n    def create_help_command(self):\n        command_table = self._get_command_table()\n        return ServiceHelpCommand(session=self.session,\n                                  obj=self._get_service_model(),\n                                  command_table=command_table,\n                                  arg_table=None,\n                                  event_class='.'.join(self.lineage_names),\n                                  name=self._name)\n\n    def _create_parser(self):\n        command_table = self._get_command_table()\n        # Also add a 'help' command.\n        command_table['help'] = self.create_help_command()\n        return ServiceArgParser(\n            operations_table=command_table, service_name=self._name)\n\n\nclass ServiceOperation(object):\n\n    \"\"\"A single operation of a service.\n\n    This class represents a single operation for a service, for\n    example ``ec2.DescribeInstances``.\n\n    \"\"\"\n\n    ARG_TYPES = {\n        'list': ListArgument,\n        'boolean': BooleanArgument,\n    }\n    DEFAULT_ARG_CLASS = CLIArgument\n\n    def __init__(self, name, parent_name, operation_caller,\n                 operation_model, session):\n        \"\"\"\n\n        :type name: str\n        :param name: The name of the operation/subcommand.\n\n        :type parent_name: str\n        :param parent_name: The name of the parent command.\n\n        :type operation_model: ``botocore.model.OperationModel``\n        :param operation_object: The operation model\n            associated with this subcommand.\n\n        :type operation_caller: ``CLIOperationCaller``\n        :param operation_caller: An object that can properly call the\n            operation.\n\n        :type session: ``botocore.session.Session``\n        :param session: The session object.\n\n        \"\"\"\n        self._arg_table = None\n        self._name = name\n        # These is used so we can figure out what the proper event\n        # name should be <parent name>.<name>.\n        self._parent_name = parent_name\n        self._operation_caller = operation_caller\n        self._lineage = [self]\n        self._operation_model = operation_model\n        self._session = session\n        if operation_model.deprecated:\n            self._UNDOCUMENTED = True\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value\n\n    @property\n    def lineage(self):\n        return self._lineage\n\n    @lineage.setter\n    def lineage(self, value):\n        self._lineage = value\n\n    @property\n    def lineage_names(self):\n        # Represents the lineage of a command in terms of command ``name``\n        return [cmd.name for cmd in self.lineage]\n\n    @property\n    def arg_table(self):\n        if self._arg_table is None:\n            self._arg_table = self._create_argument_table()\n        return self._arg_table\n\n    def __call__(self, args, parsed_globals):\n        # Once we know we're trying to call a particular operation\n        # of a service we can go ahead and load the parameters.\n        event = 'before-building-argument-table-parser.%s.%s' % \\\n            (self._parent_name, self._name)\n        self._emit(event, argument_table=self.arg_table, args=args,\n                   session=self._session, parsed_globals=parsed_globals)\n        operation_parser = self._create_operation_parser(self.arg_table)\n        self._add_help(operation_parser)\n        parsed_args, remaining = operation_parser.parse_known_args(args)\n        if parsed_args.help == 'help':\n            op_help = self.create_help_command()\n            return op_help(remaining, parsed_globals)\n        elif parsed_args.help:\n            remaining.append(parsed_args.help)\n        if remaining:\n            raise UnknownArgumentError(\n                \"Unknown options: %s\" % ', '.join(remaining))\n        event = 'operation-args-parsed.%s.%s' % (self._parent_name,\n                                                 self._name)\n        self._emit(event, parsed_args=parsed_args,\n                   parsed_globals=parsed_globals)\n        call_parameters = self._build_call_parameters(\n            parsed_args, self.arg_table)\n\n        event = 'calling-command.%s.%s' % (self._parent_name,\n                                           self._name)\n        override = self._emit_first_non_none_response(\n            event,\n            call_parameters=call_parameters,\n            parsed_args=parsed_args,\n            parsed_globals=parsed_globals\n        )\n        # There are two possible values for override. It can be some type\n        # of exception that will be raised if detected or it can represent\n        # the desired return code. Note that a return code of 0 represents\n        # a success.\n        if override is not None:\n            if isinstance(override, Exception):\n                # If the override value provided back is an exception then\n                # raise the exception\n                raise override\n            else:\n                # This is the value usually returned by the ``invoke()``\n                # method of the operation caller. It represents the return\n                # code of the operation.\n                return override\n        else:\n            # No override value was supplied.\n            return self._operation_caller.invoke(\n                self._operation_model.service_model.service_name,\n                self._operation_model.name,\n                call_parameters, parsed_globals)\n\n    def create_help_command(self):\n        return OperationHelpCommand(\n            self._session,\n            operation_model=self._operation_model,\n            arg_table=self.arg_table,\n            name=self._name, event_class='.'.join(self.lineage_names))\n\n    def _add_help(self, parser):\n        # The 'help' output is processed a little differently from\n        # the operation help because the arg_table has\n        # CLIArguments for values.\n        parser.add_argument('help', nargs='?')\n\n    def _build_call_parameters(self, args, arg_table):\n        # We need to convert the args specified on the command\n        # line as valid **kwargs we can hand to botocore.\n        service_params = {}\n        # args is an argparse.Namespace object so we're using vars()\n        # so we can iterate over the parsed key/values.\n        parsed_args = vars(args)\n        for arg_object in arg_table.values():\n            py_name = arg_object.py_name\n            if py_name in parsed_args:\n                value = parsed_args[py_name]\n                value = self._unpack_arg(arg_object, value)\n                arg_object.add_to_params(service_params, value)\n        return service_params\n\n    def _unpack_arg(self, cli_argument, value):\n        # Unpacks a commandline argument into a Python value by firing the\n        # load-cli-arg.service-name.operation-name event.\n        session = self._session\n        service_name = self._operation_model.service_model.endpoint_prefix\n        operation_name = xform_name(self._name, '-')\n\n        return unpack_argument(session, service_name, operation_name,\n                               cli_argument, value)\n\n    def _create_argument_table(self):\n        argument_table = OrderedDict()\n        input_shape = self._operation_model.input_shape\n        required_arguments = []\n        arg_dict = {}\n        if input_shape is not None:\n            required_arguments = input_shape.required_members\n            arg_dict = input_shape.members\n        for arg_name, arg_shape in arg_dict.items():\n            cli_arg_name = xform_name(arg_name, '-')\n            arg_class = self.ARG_TYPES.get(arg_shape.type_name,\n                                           self.DEFAULT_ARG_CLASS)\n            is_token = arg_shape.metadata.get('idempotencyToken', False)\n            is_required = arg_name in required_arguments and not is_token\n            event_emitter = self._session.get_component('event_emitter')\n            arg_object = arg_class(\n                name=cli_arg_name,\n                argument_model=arg_shape,\n                is_required=is_required,\n                operation_model=self._operation_model,\n                serialized_name=arg_name,\n                event_emitter=event_emitter)\n            arg_object.add_to_arg_table(argument_table)\n        LOG.debug(argument_table)\n        self._emit('building-argument-table.%s.%s' % (self._parent_name,\n                                                      self._name),\n                   operation_model=self._operation_model,\n                   session=self._session,\n                   command=self,\n                   argument_table=argument_table)\n        return argument_table\n\n    def _emit(self, name, **kwargs):\n        return self._session.emit(name, **kwargs)\n\n    def _emit_first_non_none_response(self, name, **kwargs):\n        return self._session.emit_first_non_none_response(\n            name, **kwargs)\n\n    def _create_operation_parser(self, arg_table):\n        parser = ArgTableArgParser(arg_table)\n        return parser\n\n\nclass CLIOperationCaller(object):\n\n    \"\"\"Call an AWS operation and format the response.\"\"\"\n\n    def __init__(self, session):\n        self._session = session\n\n    def invoke(self, service_name, operation_name, parameters, parsed_globals):\n        \"\"\"Invoke an operation and format the response.\n\n        :type service_name: str\n        :param service_name: The name of the service.  Note this is the service name,\n            not the endpoint prefix (e.g. ``ses`` not ``email``).\n\n        :type operation_name: str\n        :param operation_name: The operation name of the service.  The casing\n            of the operation name should match the exact casing used by the service,\n            e.g. ``DescribeInstances``, not ``describe-instances`` or\n            ``describe_instances``.\n\n        :type parameters: dict\n        :param parameters: The parameters for the operation call.  Again, these values\n            have the same casing used by the service.\n\n        :type parsed_globals: Namespace\n        :param parsed_globals: The parsed globals from the command line.\n\n        :return: None, the result is displayed through a formatter, but no\n            value is returned.\n\n        \"\"\"\n        client = self._session.create_client(\n            service_name, region_name=parsed_globals.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl)\n        response = self._make_client_call(\n            client, operation_name, parameters, parsed_globals)\n        self._display_response(operation_name, response, parsed_globals)\n        return 0\n\n    def _make_client_call(self, client, operation_name, parameters,\n                          parsed_globals):\n        py_operation_name = xform_name(operation_name)\n        if client.can_paginate(py_operation_name) and parsed_globals.paginate:\n            paginator = client.get_paginator(py_operation_name)\n            response = paginator.paginate(**parameters)\n        else:\n            response = getattr(client, xform_name(operation_name))(\n                **parameters)\n        return response\n\n    def _display_response(self, command_name, response,\n                          parsed_globals):\n        output = parsed_globals.output\n        if output is None:\n            output = self._session.get_config_variable('output')\n        formatter = get_formatter(output, parsed_globals)\n        formatter(command_name, response)\n", "awscli/alias.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\nimport shlex\nimport subprocess\n\nfrom botocore.configloader import raw_config_parse\n\nfrom awscli.compat import compat_shell_quote\nfrom awscli.commands import CLICommand\nfrom awscli.utils import emit_top_level_args_parsed_event\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass InvalidAliasException(Exception):\n    pass\n\n\nclass AliasLoader(object):\n    def __init__(self,\n                 alias_filename=os.path.expanduser(\n                     os.path.join('~', '.aws', 'cli', 'alias'))):\n        \"\"\"Interface for loading and interacting with alias file\n\n        :param alias_filename: The name of the file to load aliases from.\n            This file must be an INI file.\n        \"\"\"\n        self._filename = alias_filename\n        self._aliases = None\n\n    def _build_aliases(self):\n        self._aliases = self._load_aliases()\n        self._cleanup_alias_values(self._aliases.get('toplevel', {}))\n\n    def _load_aliases(self):\n        if os.path.exists(self._filename):\n            return raw_config_parse(\n                self._filename, parse_subsections=False)\n        return {'toplevel': {}}\n\n    def _cleanup_alias_values(self, aliases):\n        for alias in aliases:\n            # Beginning and end line separators should not be included\n            # in the internal representation of the alias value.\n            aliases[alias] = aliases[alias].strip()\n\n    def get_aliases(self):\n        if self._aliases is None:\n            self._build_aliases()\n        return self._aliases.get('toplevel', {})\n\n\nclass AliasCommandInjector(object):\n    def __init__(self, session, alias_loader):\n        \"\"\"Injects alias commands for a command table\n\n        :type session: botocore.session.Session\n        :param session: The botocore session\n\n        :type alias_loader: awscli.alias.AliasLoader\n        :param alias_loader: The alias loader to use\n        \"\"\"\n        self._session = session\n        self._alias_loader = alias_loader\n\n    def inject_aliases(self, command_table, parser):\n        for alias_name, alias_value in \\\n                self._alias_loader.get_aliases().items():\n            if alias_value.startswith('!'):\n                alias_cmd = ExternalAliasCommand(alias_name, alias_value)\n            else:\n                service_alias_cmd_args = [\n                    alias_name, alias_value, self._session, command_table,\n                    parser\n                ]\n                # If the alias name matches something already in the\n                # command table provide the command it is about\n                # to clobber as a possible reference that it will\n                # need to proxy to.\n                if alias_name in command_table:\n                    service_alias_cmd_args.append(\n                        command_table[alias_name])\n                alias_cmd = ServiceAliasCommand(*service_alias_cmd_args)\n            command_table[alias_name] = alias_cmd\n\n\nclass BaseAliasCommand(CLICommand):\n    _UNDOCUMENTED = True\n\n    def __init__(self, alias_name, alias_value):\n        \"\"\"Base class for alias command\n\n        :type alias_name: string\n        :param alias_name: The name of the alias\n\n        :type alias_value: string\n        :param alias_value: The parsed value of the alias. This can be\n            retrieved from `AliasLoader.get_aliases()[alias_name]`\n        \"\"\"\n        self._alias_name = alias_name\n        self._alias_value = alias_value\n\n    def __call__(self, args, parsed_args):\n        raise NotImplementedError('__call__')\n\n    @property\n    def name(self):\n        return self._alias_name\n\n    @name.setter\n    def name(self, value):\n        self._alias_name = value\n\n\nclass ServiceAliasCommand(BaseAliasCommand):\n    UNSUPPORTED_GLOBAL_PARAMETERS = [\n        'debug',\n        'profile'\n    ]\n\n    def __init__(self, alias_name, alias_value, session, command_table,\n                 parser, shadow_proxy_command=None):\n        \"\"\"Command for a `toplevel` subcommand alias\n\n        :type alias_name: string\n        :param alias_name: The name of the alias\n\n        :type alias_value: string\n        :param alias_value: The parsed value of the alias. This can be\n            retrieved from `AliasLoader.get_aliases()[alias_name]`\n\n        :type session: botocore.session.Session\n        :param session: The botocore session\n\n        :type command_table: dict\n        :param command_table: The command table containing all of the\n            possible service command objects that a particular alias could\n            redirect to.\n\n        :type parser: awscli.argparser.MainArgParser\n        :param parser: The parser to parse commands provided at the top level\n            of a CLI command which includes service commands and global\n            parameters. This is used to parse the service command and any\n            global parameters from the alias's value.\n\n        :type shadow_proxy_command: CLICommand\n        :param shadow_proxy_command: A built-in command that\n            potentially shadows the alias in name. If the alias\n            references this command in its value, the alias should proxy\n            to this command as opposed to proxy to itself in the command\n            table\n        \"\"\"\n        super(ServiceAliasCommand, self).__init__(alias_name, alias_value)\n        self._session = session\n        self._command_table = command_table\n        self._parser = parser\n        self._shadow_proxy_command = shadow_proxy_command\n\n    def __call__(self, args, parsed_globals):\n        alias_args = self._get_alias_args()\n        parsed_alias_args, remaining = self._parser.parse_known_args(\n            alias_args)\n        self._update_parsed_globals(parsed_alias_args, parsed_globals)\n        # Take any of the remaining arguments that were not parsed out and\n        # prepend them to the remaining args provided to the alias.\n        remaining.extend(args)\n        LOG.debug(\n            'Alias %r passing on arguments: %r to %r command',\n            self._alias_name, remaining, parsed_alias_args.command)\n        # Pass the update remaining args and global args to the service command\n        # the alias proxied to.\n        command = self._command_table[parsed_alias_args.command]\n        if self._shadow_proxy_command:\n            shadow_name = self._shadow_proxy_command.name\n            # Use the shadow command only if the aliases value\n            # uses that command indicating it needs to proxy over to\n            # a built-in command.\n            if shadow_name == parsed_alias_args.command:\n                LOG.debug(\n                    'Using shadowed command object: %s '\n                    'for alias: %s', self._shadow_proxy_command,\n                    self._alias_name\n                )\n                command = self._shadow_proxy_command\n        return command(remaining, parsed_globals)\n\n    def _get_alias_args(self):\n        try:\n            alias_args = shlex.split(self._alias_value)\n        except ValueError as e:\n            raise InvalidAliasException(\n                'Value of alias \"%s\" could not be parsed. '\n                'Received error: %s when parsing:\\n%s' % (\n                    self._alias_name, e, self._alias_value)\n            )\n\n        alias_args = [arg.strip(os.linesep) for arg in alias_args]\n        LOG.debug(\n            'Expanded subcommand alias %r with value: %r to: %r',\n            self._alias_name, self._alias_value, alias_args\n        )\n        return alias_args\n\n    def _update_parsed_globals(self, parsed_alias_args, parsed_globals):\n        global_params_to_update = self._get_global_parameters_to_update(\n            parsed_alias_args)\n        # Emit the top level args parsed event to ensure all possible\n        # customizations that typically get applied are applied to the\n        # global parameters provided in the alias before updating\n        # the original provided global parameter values\n        # and passing those onto subsequent commands.\n        emit_top_level_args_parsed_event(self._session, parsed_alias_args)\n        for param_name in global_params_to_update:\n            updated_param_value = getattr(parsed_alias_args, param_name)\n            setattr(parsed_globals, param_name, updated_param_value)\n\n    def _get_global_parameters_to_update(self, parsed_alias_args):\n        # Retrieve a list of global parameters that the newly parsed args\n        # from the alias will have to clobber from the originally provided\n        # parsed globals.\n        global_params_to_update = []\n        for parsed_param, value in vars(parsed_alias_args).items():\n            # To determine which parameters in the alias were global values\n            # compare the parsed alias parameters to the default as\n            # specified by the parser. If the parsed values from the alias\n            # differs from the default value in the parser,\n            # that global parameter must have been provided in the alias.\n            if self._parser.get_default(parsed_param) != value:\n                if parsed_param in self.UNSUPPORTED_GLOBAL_PARAMETERS:\n                    raise InvalidAliasException(\n                        'Global parameter \"--%s\" detected in alias \"%s\" '\n                        'which is not support in subcommand aliases.' % (\n                            parsed_param, self._alias_name))\n                else:\n                    global_params_to_update.append(parsed_param)\n        return global_params_to_update\n\n\nclass ExternalAliasCommand(BaseAliasCommand):\n    def __init__(self, alias_name, alias_value, invoker=subprocess.call):\n        \"\"\"Command for external aliases\n\n        Executes command external of CLI as opposed to being a proxy\n        to another command.\n\n        :type alias_name: string\n        :param alias_name: The name of the alias\n\n        :type alias_value: string\n        :param alias_value: The parsed value of the alias. This can be\n            retrieved from `AliasLoader.get_aliases()[alias_name]`\n\n        :type invoker: callable\n        :param invoker: Callable to run arguments of external alias. The\n            signature should match that of ``subprocess.call``\n        \"\"\"\n        self._alias_name = alias_name\n        self._alias_value = alias_value\n        self._invoker = invoker\n\n    def __call__(self, args, parsed_globals):\n        command_components = [\n            self._alias_value[1:]\n        ]\n        command_components.extend(compat_shell_quote(a) for a in args)\n        command = ' '.join(command_components)\n        LOG.debug(\n            'Using external alias %r with value: %r to run: %r',\n            self._alias_name, self._alias_value, command)\n        return self._invoker(command, shell=True)\n", "awscli/completer.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n\n#     http://aws.amazon.com/apache2.0/\n\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport awscli.clidriver\nimport sys\nimport logging\nimport copy\n\nLOG = logging.getLogger(__name__)\n\n\nclass Completer(object):\n\n    def __init__(self, driver=None):\n        if driver is not None:\n            self.driver = driver\n        else:\n            self.driver = awscli.clidriver.create_clidriver()\n        self.main_help = self.driver.create_help_command()\n        self.main_options = self._get_documented_completions(\n            self.main_help.arg_table)\n\n    def complete(self, cmdline, point=None):\n        if point is None:\n            point = len(cmdline)\n\n        args = cmdline[0:point].split()\n        current_arg = args[-1]\n        cmd_args = [w for w in args if not w.startswith('-')]\n        opts = [w for w in args if w.startswith('-')]\n\n        cmd_name, cmd = self._get_command(self.main_help, cmd_args)\n        subcmd_name, subcmd = self._get_command(cmd, cmd_args)\n\n        if cmd_name is None:\n            # If we didn't find any command names in the cmdline\n            # lets try to complete provider options\n            return self._complete_provider(current_arg, opts)\n        elif subcmd_name is None:\n            return self._complete_command(cmd_name, cmd, current_arg, opts)\n        return self._complete_subcommand(subcmd_name, subcmd, current_arg, opts)\n\n    def _complete_command(self, command_name, command_help, current_arg, opts):\n        if current_arg == command_name:\n            if command_help:\n                return self._get_documented_completions(\n                    command_help.command_table)\n        elif current_arg.startswith('-'):\n            return self._find_possible_options(current_arg, opts)\n        elif command_help is not None:\n            # See if they have entered a partial command name\n            return self._get_documented_completions(\n                command_help.command_table, current_arg)\n        return []\n\n    def _complete_subcommand(self, subcmd_name, subcmd_help, current_arg, opts):\n        if current_arg != subcmd_name and current_arg.startswith('-'):\n            return self._find_possible_options(current_arg, opts, subcmd_help)\n        return []\n\n    def _complete_option(self, option_name):\n        if option_name == '--endpoint-url':\n            return []\n        if option_name == '--output':\n            cli_data = self.driver.session.get_data('cli')\n            return cli_data['options']['output']['choices']\n        if option_name == '--profile':\n            return self.driver.session.available_profiles\n        return []\n\n    def _complete_provider(self, current_arg, opts):\n        if current_arg.startswith('-'):\n            return self._find_possible_options(current_arg, opts)\n        elif current_arg == 'aws':\n            return self._get_documented_completions(\n                self.main_help.command_table)\n        else:\n            # Otherwise, see if they have entered a partial command name\n            return self._get_documented_completions(\n                self.main_help.command_table, current_arg)\n\n    def _get_command(self, command_help, command_args):\n        if command_help is not None and command_help.command_table is not None:\n            for command_name in command_args:\n                if command_name in command_help.command_table:\n                    cmd_obj = command_help.command_table[command_name]\n                    return command_name, cmd_obj.create_help_command()\n        return None, None\n\n    def _get_documented_completions(self, table, startswith=None):\n        names = []\n        for key, command in table.items():\n            if getattr(command, '_UNDOCUMENTED', False):\n                # Don't tab complete undocumented commands/params\n                continue\n            if startswith is not None and not key.startswith(startswith):\n                continue\n            if getattr(command, 'positional_arg', False):\n                continue\n            names.append(key)\n        return names\n\n    def _find_possible_options(self, current_arg, opts, subcmd_help=None):\n        all_options = copy.copy(self.main_options)\n        if subcmd_help is not None:\n            all_options += self._get_documented_completions(\n                subcmd_help.arg_table)\n\n        for option in opts:\n            # Look through list of options on cmdline. If there are\n            # options that have already been specified and they are\n            # not the current word, remove them from list of possibles.\n            if option != current_arg:\n                stripped_opt = option.lstrip('-')\n                if stripped_opt in all_options:\n                    all_options.remove(stripped_opt)\n        cw = current_arg.lstrip('-')\n        possibilities = ['--' + n for n in all_options if n.startswith(cw)]\n        if len(possibilities) == 1 and possibilities[0] == current_arg:\n            return self._complete_option(possibilities[0])\n        return possibilities\n\n\ndef complete(cmdline, point):\n    choices = Completer().complete(cmdline, point)\n    print(' \\n'.join(choices))\n\n\nif __name__ == '__main__':\n    if len(sys.argv) == 3:\n        cmdline = sys.argv[1]\n        point = int(sys.argv[2])\n    elif len(sys.argv) == 2:\n        cmdline = sys.argv[1]\n    else:\n        print('usage: %s <cmdline> <point>' % sys.argv[0])\n        sys.exit(1)\n    print(complete(cmdline, point))\n", "awscli/handlers.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Builtin CLI extensions.\n\nThis is a collection of built in CLI extensions that can be automatically\nregistered with the event system.\n\n\"\"\"\nfrom awscli.argprocess import ParamShorthandParser\nfrom awscli.paramfile import register_uri_param_handler\nfrom awscli.customizations import datapipeline\nfrom awscli.customizations.addexamples import add_examples\nfrom awscli.customizations.argrename import register_arg_renames\nfrom awscli.customizations.assumerole import register_assume_role_provider\nfrom awscli.customizations.awslambda import register_lambda_create_function\nfrom awscli.customizations.cliinputjson import register_cli_input_json\nfrom awscli.customizations.cloudformation import initialize as cloudformation_init\nfrom awscli.customizations.cloudfront import register as register_cloudfront\nfrom awscli.customizations.cloudsearch import initialize as cloudsearch_init\nfrom awscli.customizations.cloudsearchdomain import register_cloudsearchdomain\nfrom awscli.customizations.cloudtrail import initialize as cloudtrail_init\nfrom awscli.customizations.codeartifact import register_codeartifact_commands\nfrom awscli.customizations.codecommit import initialize as codecommit_init\nfrom awscli.customizations.codedeploy.codedeploy import initialize as \\\n    codedeploy_init\nfrom awscli.customizations.configservice.getstatus import register_get_status\nfrom awscli.customizations.configservice.putconfigurationrecorder import \\\n    register_modify_put_configuration_recorder\nfrom awscli.customizations.configservice.rename_cmd import \\\n    register_rename_config\nfrom awscli.customizations.configservice.subscribe import register_subscribe\nfrom awscli.customizations.configure.configure import register_configure_cmd\nfrom awscli.customizations.history import register_history_mode\nfrom awscli.customizations.history import register_history_commands\nfrom awscli.customizations.ec2.addcount import register_count_events\nfrom awscli.customizations.ec2.bundleinstance import register_bundleinstance\nfrom awscli.customizations.ec2.decryptpassword import ec2_add_priv_launch_key\nfrom awscli.customizations.ec2.protocolarg import register_protocol_args\nfrom awscli.customizations.ec2.runinstances import register_runinstances\nfrom awscli.customizations.ec2.secgroupsimplify import register_secgroup\nfrom awscli.customizations.ec2.paginate import register_ec2_page_size_injector\nfrom awscli.customizations.ecr import register_ecr_commands\nfrom awscli.customizations.ecr_public import register_ecr_public_commands\nfrom awscli.customizations.emr.emr import emr_initialize\nfrom awscli.customizations.emrcontainers import \\\n    initialize as emrcontainers_initialize\nfrom awscli.customizations.eks import initialize as eks_initialize\nfrom awscli.customizations.ecs import initialize as ecs_initialize\nfrom awscli.customizations.gamelift import register_gamelift_commands\nfrom awscli.customizations.generatecliskeleton import \\\n    register_generate_cli_skeleton\nfrom awscli.customizations.globalargs import register_parse_global_args\nfrom awscli.customizations.iamvirtmfa import IAMVMFAWrapper\nfrom awscli.customizations.iot import register_create_keys_and_cert_arguments\nfrom awscli.customizations.iot import register_create_keys_from_csr_arguments\nfrom awscli.customizations.iot_data import register_custom_endpoint_note\nfrom awscli.customizations.kms import register_fix_kms_create_grant_docs\nfrom awscli.customizations.dlm.dlm import dlm_initialize\nfrom awscli.customizations.opsworks import initialize as opsworks_init\nfrom awscli.customizations.paginate import register_pagination\nfrom awscli.customizations.preview import register_preview_commands\nfrom awscli.customizations.putmetricdata import register_put_metric_data\nfrom awscli.customizations.rds import register_rds_modify_split\nfrom awscli.customizations.rds import register_add_generate_db_auth_token\nfrom awscli.customizations.rekognition import register_rekognition_detect_labels\nfrom awscli.customizations.removals import register_removals\nfrom awscli.customizations.route53 import register_create_hosted_zone_doc_fix\nfrom awscli.customizations.s3.s3 import s3_plugin_initialize\nfrom awscli.customizations.s3errormsg import register_s3_error_msg\nfrom awscli.customizations.scalarparse import register_scalar_parser\nfrom awscli.customizations.sessendemail import register_ses_send_email\nfrom awscli.customizations.streamingoutputarg import add_streaming_output_arg\nfrom awscli.customizations.translate import register_translate_import_terminology\nfrom awscli.customizations.toplevelbool import register_bool_params\nfrom awscli.customizations.waiters import register_add_waiters\nfrom awscli.customizations.opsworkscm import register_alias_opsworks_cm\nfrom awscli.customizations.mturk import register_alias_mturk_command\nfrom awscli.customizations.sagemaker import register_alias_sagemaker_runtime_command\nfrom awscli.customizations.servicecatalog import register_servicecatalog_commands\nfrom awscli.customizations.s3events import register_event_stream_arg\nfrom awscli.customizations.sessionmanager import register_ssm_session\nfrom awscli.customizations.sms_voice import register_sms_voice_hide\nfrom awscli.customizations.dynamodb import register_dynamodb_paginator_fix\nfrom awscli.customizations.overridesslcommonname import register_override_ssl_common_name\nfrom awscli.customizations.kinesis import \\\n    register_kinesis_list_streams_pagination_backcompat\nfrom awscli.customizations.quicksight import \\\n    register_quicksight_asset_bundle_customizations\nfrom awscli.customizations.logs import register_logs_commands\n\n\ndef awscli_initialize(event_handlers):\n    event_handlers.register('session-initialized', register_uri_param_handler)\n    param_shorthand = ParamShorthandParser()\n    event_handlers.register('process-cli-arg', param_shorthand)\n    # The s3 error message needs to registered before the\n    # generic error handler.\n    register_s3_error_msg(event_handlers)\n#    # The following will get fired for every option we are\n#    # documenting.  It will attempt to add an example_fn on to\n#    # the parameter object if the parameter supports shorthand\n#    # syntax.  The documentation event handlers will then use\n#    # the examplefn to generate the sample shorthand syntax\n#    # in the docs.  Registering here should ensure that this\n#    # handler gets called first but it still feels a bit brittle.\n#    event_handlers.register('doc-option-example.*.*.*',\n#                            param_shorthand.add_example_fn)\n    event_handlers.register('doc-examples.*.*',\n                            add_examples)\n    register_cli_input_json(event_handlers)\n    event_handlers.register('building-argument-table.*',\n                            add_streaming_output_arg)\n    register_count_events(event_handlers)\n    event_handlers.register('building-argument-table.ec2.get-password-data',\n                            ec2_add_priv_launch_key)\n    register_parse_global_args(event_handlers)\n    register_pagination(event_handlers)\n    register_secgroup(event_handlers)\n    register_bundleinstance(event_handlers)\n    s3_plugin_initialize(event_handlers)\n    register_runinstances(event_handlers)\n    register_removals(event_handlers)\n    register_preview_commands(event_handlers)\n    register_rds_modify_split(event_handlers)\n    register_rekognition_detect_labels(event_handlers)\n    register_add_generate_db_auth_token(event_handlers)\n    register_put_metric_data(event_handlers)\n    register_ses_send_email(event_handlers)\n    IAMVMFAWrapper(event_handlers)\n    register_arg_renames(event_handlers)\n    register_configure_cmd(event_handlers)\n    cloudtrail_init(event_handlers)\n    register_ecr_commands(event_handlers)\n    register_ecr_public_commands(event_handlers)\n    register_bool_params(event_handlers)\n    register_protocol_args(event_handlers)\n    datapipeline.register_customizations(event_handlers)\n    cloudsearch_init(event_handlers)\n    emr_initialize(event_handlers)\n    emrcontainers_initialize(event_handlers)\n    eks_initialize(event_handlers)\n    ecs_initialize(event_handlers)\n    register_cloudsearchdomain(event_handlers)\n    register_generate_cli_skeleton(event_handlers)\n    register_assume_role_provider(event_handlers)\n    register_add_waiters(event_handlers)\n    codedeploy_init(event_handlers)\n    register_subscribe(event_handlers)\n    register_get_status(event_handlers)\n    register_rename_config(event_handlers)\n    register_scalar_parser(event_handlers)\n    opsworks_init(event_handlers)\n    register_lambda_create_function(event_handlers)\n    register_fix_kms_create_grant_docs(event_handlers)\n    register_create_hosted_zone_doc_fix(event_handlers)\n    register_modify_put_configuration_recorder(event_handlers)\n    register_codeartifact_commands(event_handlers)\n    codecommit_init(event_handlers)\n    register_custom_endpoint_note(event_handlers)\n    event_handlers.register(\n        'building-argument-table.iot.create-keys-and-certificate',\n        register_create_keys_and_cert_arguments)\n    event_handlers.register(\n        'building-argument-table.iot.create-certificate-from-csr',\n        register_create_keys_from_csr_arguments)\n    register_cloudfront(event_handlers)\n    register_gamelift_commands(event_handlers)\n    register_ec2_page_size_injector(event_handlers)\n    cloudformation_init(event_handlers)\n    register_alias_opsworks_cm(event_handlers)\n    register_alias_mturk_command(event_handlers)\n    register_alias_sagemaker_runtime_command(event_handlers)\n    register_servicecatalog_commands(event_handlers)\n    register_translate_import_terminology(event_handlers)\n    register_history_mode(event_handlers)\n    register_history_commands(event_handlers)\n    register_event_stream_arg(event_handlers)\n    dlm_initialize(event_handlers)\n    register_ssm_session(event_handlers)\n    register_sms_voice_hide(event_handlers)\n    register_dynamodb_paginator_fix(event_handlers)\n    register_override_ssl_common_name(event_handlers)\n    register_kinesis_list_streams_pagination_backcompat(event_handlers)\n    register_quicksight_asset_bundle_customizations(event_handlers)\n    register_logs_commands(event_handlers)\n", "awscli/help.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\nimport sys\nimport platform\nimport shlex\nfrom subprocess import Popen, PIPE\n\nfrom docutils.core import publish_string\nfrom docutils.writers import manpage\n\nfrom awscli.clidocs import ProviderDocumentEventHandler\nfrom awscli.clidocs import ServiceDocumentEventHandler\nfrom awscli.clidocs import OperationDocumentEventHandler\nfrom awscli.clidocs import TopicListerDocumentEventHandler\nfrom awscli.clidocs import TopicDocumentEventHandler\nfrom awscli.bcdoc import docevents\nfrom awscli.bcdoc.restdoc import ReSTDocument\nfrom awscli.bcdoc.textwriter import TextWriter\nfrom awscli.argprocess import ParamShorthandParser\nfrom awscli.argparser import ArgTableArgParser\nfrom awscli.topictags import TopicTagDB\nfrom awscli.utils import ignore_ctrl_c\n\n\nLOG = logging.getLogger('awscli.help')\n\n\nclass ExecutableNotFoundError(Exception):\n    def __init__(self, executable_name):\n        super(ExecutableNotFoundError, self).__init__(\n            'Could not find executable named \"%s\"' % executable_name)\n\n\ndef get_renderer():\n    \"\"\"\n    Return the appropriate HelpRenderer implementation for the\n    current platform.\n    \"\"\"\n    if platform.system() == 'Windows':\n        return WindowsHelpRenderer()\n    else:\n        return PosixHelpRenderer()\n\n\nclass PagingHelpRenderer(object):\n    \"\"\"\n    Interface for a help renderer.\n\n    The renderer is responsible for displaying the help content on\n    a particular platform.\n\n    \"\"\"\n    def __init__(self, output_stream=sys.stdout):\n        self.output_stream = output_stream\n\n    PAGER = None\n    _DEFAULT_DOCUTILS_SETTINGS_OVERRIDES = {\n        # The default for line length limit in docutils is 10,000. However,\n        # currently in the documentation, it inlines all possible enums in\n        # the JSON syntax which exceeds this limit for some EC2 commands\n        # and prevents the manpages from being generated.\n        # This is a temporary fix to allow the manpages for these commands\n        # to be rendered. Long term, we should avoid enumerating over all\n        # enums inline for the JSON syntax snippets.\n        'line_length_limit': 50_000\n    }\n\n    def get_pager_cmdline(self):\n        pager = self.PAGER\n        if 'MANPAGER' in os.environ:\n            pager = os.environ['MANPAGER']\n        elif 'PAGER' in os.environ:\n            pager = os.environ['PAGER']\n        return shlex.split(pager)\n\n    def render(self, contents):\n        \"\"\"\n        Each implementation of HelpRenderer must implement this\n        render method.\n        \"\"\"\n        converted_content = self._convert_doc_content(contents)\n        self._send_output_to_pager(converted_content)\n\n    def _send_output_to_pager(self, output):\n        cmdline = self.get_pager_cmdline()\n        LOG.debug(\"Running command: %s\", cmdline)\n        p = self._popen(cmdline, stdin=PIPE)\n        p.communicate(input=output)\n\n    def _popen(self, *args, **kwargs):\n        return Popen(*args, **kwargs)\n\n    def _convert_doc_content(self, contents):\n        return contents\n\n\nclass PosixHelpRenderer(PagingHelpRenderer):\n    \"\"\"\n    Render help content on a Posix-like system.  This includes\n    Linux and MacOS X.\n    \"\"\"\n\n    PAGER = 'less -R'\n\n    def _convert_doc_content(self, contents):\n        man_contents = publish_string(\n            contents, writer=manpage.Writer(),\n            settings_overrides=self._DEFAULT_DOCUTILS_SETTINGS_OVERRIDES,\n        )\n        if self._exists_on_path('groff'):\n            cmdline = ['groff', '-m', 'man', '-T', 'ascii']\n        elif self._exists_on_path('mandoc'):\n            cmdline = ['mandoc', '-T', 'ascii']\n        else:\n            raise ExecutableNotFoundError('groff or mandoc')\n        LOG.debug(\"Running command: %s\", cmdline)\n        p3 = self._popen(cmdline, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n        output = p3.communicate(input=man_contents)[0]\n        return output\n\n    def _send_output_to_pager(self, output):\n        cmdline = self.get_pager_cmdline()\n        if not self._exists_on_path(cmdline[0]):\n            LOG.debug(\"Pager '%s' not found in PATH, printing raw help.\" %\n                      cmdline[0])\n            self.output_stream.write(output.decode('utf-8') + \"\\n\")\n            self.output_stream.flush()\n            return\n        LOG.debug(\"Running command: %s\", cmdline)\n        with ignore_ctrl_c():\n            # We can't rely on the KeyboardInterrupt from\n            # the CLIDriver being caught because when we\n            # send the output to a pager it will use various\n            # control characters that need to be cleaned\n            # up gracefully.  Otherwise if we simply catch\n            # the Ctrl-C and exit, it will likely leave the\n            # users terminals in a bad state and they'll need\n            # to manually run ``reset`` to fix this issue.\n            # Ignoring Ctrl-C solves this issue.  It's also\n            # the default behavior of less (you can't ctrl-c\n            # out of a manpage).\n            p = self._popen(cmdline, stdin=PIPE)\n            p.communicate(input=output)\n\n    def _exists_on_path(self, name):\n        # Since we're only dealing with POSIX systems, we can\n        # ignore things like PATHEXT.\n        return any([os.path.exists(os.path.join(p, name))\n                    for p in os.environ.get('PATH', '').split(os.pathsep)])\n\n\nclass WindowsHelpRenderer(PagingHelpRenderer):\n    \"\"\"Render help content on a Windows platform.\"\"\"\n\n    PAGER = 'more'\n\n    def _convert_doc_content(self, contents):\n        text_output = publish_string(\n            contents, writer=TextWriter(),\n            settings_overrides=self._DEFAULT_DOCUTILS_SETTINGS_OVERRIDES,\n        )\n        return text_output\n\n    def _popen(self, *args, **kwargs):\n        # Also set the shell value to True.  To get any of the\n        # piping to a pager to work, we need to use shell=True.\n        kwargs['shell'] = True\n        return Popen(*args, **kwargs)\n\n\nclass HelpCommand(object):\n    \"\"\"\n    HelpCommand Interface\n    ---------------------\n    A HelpCommand object acts as the interface between objects in the\n    CLI (e.g. Providers, Services, Operations, etc.) and the documentation\n    system (bcdoc).\n\n    A HelpCommand object wraps the object from the CLI space and provides\n    a consistent interface to critical information needed by the\n    documentation pipeline such as the object's name, description, etc.\n\n    The HelpCommand object is passed to the component of the\n    documentation pipeline that fires documentation events.  It is\n    then passed on to each document event handler that has registered\n    for the events.\n\n    All HelpCommand objects contain the following attributes:\n\n        + ``session`` - A ``botocore`` ``Session`` object.\n        + ``obj`` - The object that is being documented.\n        + ``command_table`` - A dict mapping command names to\n              callable objects.\n        + ``arg_table`` - A dict mapping argument names to callable objects.\n        + ``doc`` - A ``Document`` object that is used to collect the\n              generated documentation.\n\n    In addition, please note the `properties` defined below which are\n    required to allow the object to be used in the document pipeline.\n\n    Implementations of HelpCommand are provided here for Provider,\n    Service and Operation objects.  Other implementations for other\n    types of objects might be needed for customization in plugins.\n    As long as the implementations conform to this basic interface\n    it should be possible to pass them to the documentation system\n    and generate interactive and static help files.\n    \"\"\"\n\n    EventHandlerClass = None\n    \"\"\"\n    Each subclass should define this class variable to point to the\n    EventHandler class used by this HelpCommand.\n    \"\"\"\n\n    def __init__(self, session, obj, command_table, arg_table):\n        self.session = session\n        self.obj = obj\n        if command_table is None:\n            command_table = {}\n        self.command_table = command_table\n        if arg_table is None:\n            arg_table = {}\n        self.arg_table = arg_table\n        self._subcommand_table = {}\n        self._related_items = []\n        self.renderer = get_renderer()\n        self.doc = ReSTDocument(target='man')\n\n    @property\n    def event_class(self):\n        \"\"\"\n        Return the ``event_class`` for this object.\n\n        The ``event_class`` is used by the documentation pipeline\n        when generating documentation events.  For the event below::\n\n            doc-title.<event_class>.<name>\n\n        The document pipeline would use this property to determine\n        the ``event_class`` value.\n        \"\"\"\n        pass\n\n    @property\n    def name(self):\n        \"\"\"\n        Return the name of the wrapped object.\n\n        This would be called by the document pipeline to determine\n        the ``name`` to be inserted into the event, as shown above.\n        \"\"\"\n        pass\n\n    @property\n    def subcommand_table(self):\n        \"\"\"These are the commands that may follow after the help command\"\"\"\n        return self._subcommand_table\n\n    @property\n    def related_items(self):\n        \"\"\"This is list of items that are related to the help command\"\"\"\n        return self._related_items\n\n    def __call__(self, args, parsed_globals):\n        if args:\n            subcommand_parser = ArgTableArgParser({}, self.subcommand_table)\n            parsed, remaining = subcommand_parser.parse_known_args(args)\n            if getattr(parsed, 'subcommand', None) is not None:\n                return self.subcommand_table[parsed.subcommand](remaining,\n                                                                parsed_globals)\n\n        # Create an event handler for a Provider Document\n        instance = self.EventHandlerClass(self)\n        # Now generate all of the events for a Provider document.\n        # We pass ourselves along so that we can, in turn, get passed\n        # to all event handlers.\n        docevents.generate_events(self.session, self)\n        self.renderer.render(self.doc.getvalue())\n        instance.unregister()\n\n\nclass ProviderHelpCommand(HelpCommand):\n    \"\"\"Implements top level help command.\n\n    This is what is called when ``aws help`` is run.\n\n    \"\"\"\n    EventHandlerClass = ProviderDocumentEventHandler\n\n    def __init__(self, session, command_table, arg_table,\n                 description, synopsis, usage):\n        HelpCommand.__init__(self, session, None,\n                             command_table, arg_table)\n        self.description = description\n        self.synopsis = synopsis\n        self.help_usage = usage\n        self._subcommand_table = None\n        self._topic_tag_db = None\n        self._related_items = ['aws help topics']\n\n    @property\n    def event_class(self):\n        return 'aws'\n\n    @property\n    def name(self):\n        return 'aws'\n\n    @property\n    def subcommand_table(self):\n        if self._subcommand_table is None:\n            if self._topic_tag_db is None:\n                self._topic_tag_db = TopicTagDB()\n            self._topic_tag_db.load_json_index()\n            self._subcommand_table = self._create_subcommand_table()\n        return self._subcommand_table\n\n    def _create_subcommand_table(self):\n        subcommand_table = {}\n        # Add the ``aws help topics`` command to the ``topic_table``\n        topic_lister_command = TopicListerCommand(self.session)\n        subcommand_table['topics'] = topic_lister_command\n        topic_names = self._topic_tag_db.get_all_topic_names()\n\n        # Add all of the possible topics to the ``topic_table``\n        for topic_name in topic_names:\n            topic_help_command = TopicHelpCommand(self.session, topic_name)\n            subcommand_table[topic_name] = topic_help_command\n        return subcommand_table\n\n\nclass ServiceHelpCommand(HelpCommand):\n    \"\"\"Implements service level help.\n\n    This is the object invoked whenever a service command\n    help is implemented, e.g. ``aws ec2 help``.\n\n    \"\"\"\n\n    EventHandlerClass = ServiceDocumentEventHandler\n\n    def __init__(self, session, obj, command_table, arg_table, name,\n                 event_class):\n        super(ServiceHelpCommand, self).__init__(session, obj, command_table,\n                                                 arg_table)\n        self._name = name\n        self._event_class = event_class\n\n    @property\n    def event_class(self):\n        return self._event_class\n\n    @property\n    def name(self):\n        return self._name\n\n\nclass OperationHelpCommand(HelpCommand):\n    \"\"\"Implements operation level help.\n\n    This is the object invoked whenever help for a service is requested,\n    e.g. ``aws ec2 describe-instances help``.\n\n    \"\"\"\n    EventHandlerClass = OperationDocumentEventHandler\n\n    def __init__(self, session, operation_model, arg_table, name,\n                 event_class):\n        HelpCommand.__init__(self, session, operation_model, None, arg_table)\n        self.param_shorthand = ParamShorthandParser()\n        self._name = name\n        self._event_class = event_class\n\n    @property\n    def event_class(self):\n        return self._event_class\n\n    @property\n    def name(self):\n        return self._name\n\n\nclass TopicListerCommand(HelpCommand):\n    EventHandlerClass = TopicListerDocumentEventHandler\n\n    def __init__(self, session):\n        super(TopicListerCommand, self).__init__(session, None, {}, {})\n\n    @property\n    def event_class(self):\n        return 'topics'\n\n    @property\n    def name(self):\n        return 'topics'\n\n\nclass TopicHelpCommand(HelpCommand):\n    EventHandlerClass = TopicDocumentEventHandler\n\n    def __init__(self, session, topic_name):\n        super(TopicHelpCommand, self).__init__(session, None, {}, {})\n        self._topic_name = topic_name\n\n    @property\n    def event_class(self):\n        return 'topics.' + self.name\n\n    @property\n    def name(self):\n        return self._topic_name\n", "awscli/arguments.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Abstractions for CLI arguments.\n\nThis module contains abstractions for representing CLI arguments.\nThis includes how the CLI argument parser is created, how arguments\nare serialized, and how arguments are bound (if at all) to operation\narguments.\n\nThe BaseCLIArgument is the interface for all arguments.  This is the interface\nexpected by objects that work with arguments.  If you want to implement your\nown argument subclass, make sure it implements everything in BaseCLIArgument.\n\nArguments generally fall into one of several categories:\n\n* global argument.  These arguments may influence what the CLI does,\n  but aren't part of the input parameters needed to make an API call.  For\n  example, the ``--region`` argument specifies which region to send the request\n  to.  The ``--output`` argument specifies how to display the response to the\n  user.  The ``--query`` argument specifies how to select specific elements\n  from a response.\n* operation argument.  These are arguments that influence the parameters we\n  send to a service when making an API call.  Some of these arguments are\n  automatically created directly from introspecting the JSON service model.\n  Sometimes customizations may provide a pseudo-argument that takes the\n  user input and maps the input value to several API parameters.\n\n\"\"\"\nimport logging\n\nfrom botocore import xform_name\nfrom botocore.hooks import first_non_none_response\n\nfrom awscli.argprocess import unpack_cli_arg\nfrom awscli.schema import SchemaTransformer\nfrom botocore import model\n\n\nLOG = logging.getLogger('awscli.arguments')\n\n\nclass UnknownArgumentError(Exception):\n    pass\n\n\ndef create_argument_model_from_schema(schema):\n    # Given a JSON schema (described in schema.py), convert it\n    # to a shape object from `botocore.model.Shape` that can be\n    # used as the argument_model for the Argument classes below.\n    transformer = SchemaTransformer()\n    shapes_map = transformer.transform(schema)\n    shape_resolver = model.ShapeResolver(shapes_map)\n    # The SchemaTransformer guarantees that the top level shape\n    # will always be named 'InputShape'.\n    arg_shape = shape_resolver.get_shape_by_name('InputShape')\n    return arg_shape\n\n\nclass BaseCLIArgument(object):\n    \"\"\"Interface for CLI argument.\n\n    This class represents the interface used for representing CLI\n    arguments.\n\n    \"\"\"\n\n    def __init__(self, name):\n        self._name = name\n\n    def add_to_arg_table(self, argument_table):\n        \"\"\"Add this object to the argument_table.\n\n        The ``argument_table`` represents the argument for the operation.\n        This is called by the ``ServiceOperation`` object to create the\n        arguments associated with the operation.\n\n        :type argument_table: dict\n        :param argument_table: The argument table.  The key is the argument\n            name, and the value is an object implementing this interface.\n        \"\"\"\n        argument_table[self.name] = self\n\n    def add_to_parser(self, parser):\n        \"\"\"Add this object to the parser instance.\n\n        This method is called by the associated ``ArgumentParser``\n        instance.  This method should make the relevant calls\n        to ``add_argument`` to add itself to the argparser.\n\n        :type parser: ``argparse.ArgumentParser``.\n        :param parser: The argument parser associated with the operation.\n\n        \"\"\"\n        pass\n\n    def add_to_params(self, parameters, value):\n        \"\"\"Add this object to the parameters dict.\n\n        This method is responsible for taking the value specified\n        on the command line, and deciding how that corresponds to\n        parameters used by the service/operation.\n\n        :type parameters: dict\n        :param parameters: The parameters dictionary that will be\n            given to ``botocore``.  This should match up to the\n            parameters associated with the particular operation.\n\n        :param value: The value associated with the CLI option.\n\n        \"\"\"\n        pass\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cli_name(self):\n        return '--' + self._name\n\n    @property\n    def cli_type_name(self):\n        raise NotImplementedError(\"cli_type_name\")\n\n    @property\n    def required(self):\n        raise NotImplementedError(\"required\")\n\n    @property\n    def documentation(self):\n        raise NotImplementedError(\"documentation\")\n\n    @property\n    def cli_type(self):\n        raise NotImplementedError(\"cli_type\")\n\n    @property\n    def py_name(self):\n        return self._name.replace('-', '_')\n\n    @property\n    def choices(self):\n        \"\"\"List valid choices for argument value.\n\n        If this value is not None then this should return a list of valid\n        values for the argument.\n\n        \"\"\"\n        return None\n\n    @property\n    def synopsis(self):\n        return ''\n\n    @property\n    def positional_arg(self):\n        return False\n\n    @property\n    def nargs(self):\n        return None\n\n    @name.setter\n    def name(self, value):\n        self._name = value\n\n    @property\n    def group_name(self):\n        \"\"\"Get the group name associated with the argument.\n\n        An argument can be part of a group.  This property will\n        return the name of that group.\n\n        This base class has no default behavior for groups, code\n        that consumes argument objects can use them for whatever\n        purposes they like (documentation, mutually exclusive group\n        validation, etc.).\n\n        \"\"\"\n        return None\n\n\nclass CustomArgument(BaseCLIArgument):\n    \"\"\"\n    Represents a CLI argument that is configured from a dictionary.\n\n    For example, the \"top level\" arguments used for the CLI\n    (--region, --output) can use a CustomArgument argument,\n    as these are described in the cli.json file as dictionaries.\n\n    This class is also useful for plugins/customizations that want to\n    add additional args.\n\n    \"\"\"\n\n    def __init__(self, name, help_text='', dest=None, default=None,\n                 action=None, required=None, choices=None, nargs=None,\n                 cli_type_name=None, group_name=None, positional_arg=False,\n                 no_paramfile=False, argument_model=None, synopsis='',\n                 const=None):\n        self._name = name\n        self._help = help_text\n        self._dest = dest\n        self._default = default\n        self._action = action\n        self._required = required\n        self._nargs = nargs\n        self._const = const\n        self._cli_type_name = cli_type_name\n        self._group_name = group_name\n        self._positional_arg = positional_arg\n        if choices is None:\n            choices = []\n        self._choices = choices\n        self._synopsis = synopsis\n\n        # These are public attributes that are ok to access from external\n        # objects.\n        self.no_paramfile = no_paramfile\n        self.argument_model = None\n\n        if argument_model is None:\n            argument_model = self._create_scalar_argument_model()\n        self.argument_model = argument_model\n\n        # If the top level element is a list then set nargs to\n        # accept multiple values separated by a space.\n        if self.argument_model is not None and \\\n                self.argument_model.type_name == 'list':\n            self._nargs = '+'\n\n    def _create_scalar_argument_model(self):\n        if self._nargs is not None:\n            # If nargs is not None then argparse will parse the value\n            # as a list, so we don't create an argument_object so we don't\n            # go through param validation.\n            return None\n        # If no argument model is provided, we create a basic\n        # shape argument.\n        type_name = self.cli_type_name\n        return create_argument_model_from_schema({'type': type_name})\n\n    @property\n    def cli_name(self):\n        if self._positional_arg:\n            return self._name\n        else:\n            return '--' + self._name\n\n    def add_to_parser(self, parser):\n        \"\"\"\n\n        See the ``BaseCLIArgument.add_to_parser`` docs for more information.\n\n        \"\"\"\n        cli_name = self.cli_name\n        kwargs = {}\n        if self._dest is not None:\n            kwargs['dest'] = self._dest\n        if self._action is not None:\n            kwargs['action'] = self._action\n        if self._default is not None:\n            kwargs['default'] = self._default\n        if self._choices:\n            kwargs['choices'] = self._choices\n        if self._required is not None:\n            kwargs['required'] = self._required\n        if self._nargs is not None:\n            kwargs['nargs'] = self._nargs\n        if self._const is not None:\n            kwargs['const'] = self._const\n        parser.add_argument(cli_name, **kwargs)\n\n    @property\n    def required(self):\n        if self._required is None:\n            return False\n        return self._required\n\n    @required.setter\n    def required(self, value):\n        self._required = value\n\n    @property\n    def documentation(self):\n        return self._help\n\n    @property\n    def cli_type_name(self):\n        if self._cli_type_name is not None:\n            return self._cli_type_name\n        elif self._action in ['store_true', 'store_false']:\n            return 'boolean'\n        elif self.argument_model is not None:\n            return self.argument_model.type_name\n        else:\n            # Default to 'string' type if we don't have any\n            # other info.\n            return 'string'\n\n    @property\n    def cli_type(self):\n        cli_type = str\n        if self._action in ['store_true', 'store_false']:\n            cli_type = bool\n        return cli_type\n\n    @property\n    def choices(self):\n        return self._choices\n\n    @property\n    def group_name(self):\n        return self._group_name\n\n    @property\n    def synopsis(self):\n        return self._synopsis\n\n    @property\n    def positional_arg(self):\n        return self._positional_arg\n\n    @property\n    def nargs(self):\n        return self._nargs\n\n\nclass CLIArgument(BaseCLIArgument):\n    \"\"\"Represents a CLI argument that maps to a service parameter.\n\n    \"\"\"\n\n    TYPE_MAP = {\n        'structure': str,\n        'map': str,\n        'timestamp': str,\n        'list': str,\n        'string': str,\n        'float': float,\n        'integer': str,\n        'long': int,\n        'boolean': bool,\n        'double': float,\n        'blob': str\n    }\n\n    def __init__(self, name, argument_model, operation_model,\n                 event_emitter, is_required=False,\n                 serialized_name=None):\n        \"\"\"\n\n        :type name: str\n        :param name: The name of the argument in \"cli\" form\n            (e.g.  ``min-instances``).\n\n        :type argument_model: ``botocore.model.Shape``\n        :param argument_model: The shape object that models the argument.\n\n        :type argument_model: ``botocore.model.OperationModel``\n        :param argument_model: The object that models the associated operation.\n\n        :type event_emitter: ``botocore.hooks.BaseEventHooks``\n        :param event_emitter: The event emitter to use when emitting events.\n            This class will emit events during parts of the argument\n            parsing process.  This event emitter is what is used to emit\n            such events.\n\n        :type is_required: boolean\n        :param is_required: Indicates if this parameter is required or not.\n\n        \"\"\"\n        self._name = name\n        # This is the name we need to use when constructing the parameters\n        # dict we send to botocore.  While we can change the .name attribute\n        # which is the name exposed in the CLI, the serialized name we use\n        # for botocore is invariant and should not be changed.\n        if serialized_name is None:\n            serialized_name = name\n        self._serialized_name = serialized_name\n        self.argument_model = argument_model\n        self._required = is_required\n        self._operation_model = operation_model\n        self._event_emitter = event_emitter\n        self._documentation = argument_model.documentation\n\n    @property\n    def py_name(self):\n        return self._name.replace('-', '_')\n\n    @property\n    def required(self):\n        return self._required\n\n    @required.setter\n    def required(self, value):\n        self._required = value\n\n    @property\n    def documentation(self):\n        return self._documentation\n\n    @documentation.setter\n    def documentation(self, value):\n        self._documentation = value\n\n    @property\n    def cli_type_name(self):\n        return self.argument_model.type_name\n\n    @property\n    def cli_type(self):\n        return self.TYPE_MAP.get(self.argument_model.type_name, str)\n\n    def add_to_parser(self, parser):\n        \"\"\"\n\n        See the ``BaseCLIArgument.add_to_parser`` docs for more information.\n\n        \"\"\"\n        cli_name = self.cli_name\n        parser.add_argument(\n            cli_name,\n            help=self.documentation,\n            type=self.cli_type,\n            required=self.required)\n\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        else:\n            # This is a two step process.  First is the process of converting\n            # the command line value into a python value.  Normally this is\n            # handled by argparse directly, but there are cases where extra\n            # processing is needed.  For example, \"--foo name=value\" the value\n            # can be converted from \"name=value\" to {\"name\": \"value\"}.  This is\n            # referred to as the \"unpacking\" process.  Once we've unpacked the\n            # argument value, we have to decide how this is converted into\n            # something that can be consumed by botocore.  Many times this is\n            # just associating the key and value in the params dict as down\n            # below.  Sometimes this can be more complicated, and subclasses\n            # can customize as they need.\n            unpacked = self._unpack_argument(value)\n            LOG.debug('Unpacked value of %r for parameter \"%s\": %r', value,\n                      self.py_name, unpacked)\n            parameters[self._serialized_name] = unpacked\n\n    def _unpack_argument(self, value):\n        service_name = self._operation_model.service_model.service_name\n        operation_name = xform_name(self._operation_model.name, '-')\n        override = self._emit_first_response('process-cli-arg.%s.%s' % (\n            service_name, operation_name), param=self.argument_model,\n            cli_argument=self, value=value)\n        if override is not None:\n            # A plugin supplied an alternate conversion,\n            # use it instead.\n            return override\n        else:\n            # Fall back to the default arg processing.\n            return unpack_cli_arg(self, value)\n\n    def _emit(self, name, **kwargs):\n        return self._event_emitter.emit(name, **kwargs)\n\n    def _emit_first_response(self, name, **kwargs):\n        responses = self._emit(name, **kwargs)\n        return first_non_none_response(responses)\n\n\nclass ListArgument(CLIArgument):\n\n    def add_to_parser(self, parser):\n        cli_name = self.cli_name\n        parser.add_argument(cli_name,\n                            nargs='*',\n                            type=self.cli_type,\n                            required=self.required)\n\n\nclass BooleanArgument(CLIArgument):\n    \"\"\"Represent a boolean CLI argument.\n\n    A boolean parameter is specified without a value::\n\n        aws foo bar --enabled\n\n    For cases where the boolean parameter is required we need to add\n    two parameters::\n\n        aws foo bar --enabled\n        aws foo bar --no-enabled\n\n    We use the capabilities of the CLIArgument to help achieve this.\n\n    \"\"\"\n\n    def __init__(self, name, argument_model, operation_model,\n                 event_emitter,\n                 is_required=False, action='store_true', dest=None,\n                 group_name=None, default=None,\n                 serialized_name=None):\n        super(BooleanArgument, self).__init__(name,\n                                              argument_model,\n                                              operation_model,\n                                              event_emitter,\n                                              is_required,\n                                              serialized_name=serialized_name)\n        self._mutex_group = None\n        self._action = action\n        if dest is None:\n            self._destination = self.py_name\n        else:\n            self._destination = dest\n        if group_name is None:\n            self._group_name = self.name\n        else:\n            self._group_name = group_name\n        self._default = default\n\n    def add_to_params(self, parameters, value):\n        # If a value was explicitly specified (so value is True/False\n        # but *not* None) then we add it to the params dict.\n        # If the value was not explicitly set (value is None)\n        # we don't add it to the params dict.\n        if value is not None:\n            parameters[self._serialized_name] = value\n\n    def add_to_arg_table(self, argument_table):\n        # Boolean parameters are a bit tricky.  For a single boolean parameter\n        # we actually want two CLI params, a --foo, and a --no-foo.  To do this\n        # we need to add two entries to the argument table.  So we can add\n        # ourself as the positive option (--no), and then create a clone of\n        # ourselves for the negative service.  We then insert both into the\n        # arg table.\n        argument_table[self.name] = self\n        negative_name = 'no-%s' % self.name\n        negative_version = self.__class__(\n            negative_name, self.argument_model,\n            self._operation_model, self._event_emitter,\n            action='store_false', dest=self._destination,\n            group_name=self.group_name, serialized_name=self._serialized_name)\n        argument_table[negative_name] = negative_version\n\n    def add_to_parser(self, parser):\n        parser.add_argument(self.cli_name,\n                            help=self.documentation,\n                            action=self._action,\n                            default=self._default,\n                            dest=self._destination)\n\n    @property\n    def group_name(self):\n        return self._group_name\n", "awscli/commands.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nclass CLICommand(object):\n\n    \"\"\"Interface for a CLI command.\n\n    This class represents a top level CLI command\n    (``aws ec2``, ``aws s3``, ``aws config``).\n\n    \"\"\"\n\n    @property\n    def name(self):\n        # Subclasses must implement a name.\n        raise NotImplementedError(\"name\")\n\n    @name.setter\n    def name(self, value):\n        # Subclasses must implement setting/changing the cmd name.\n        raise NotImplementedError(\"name\")\n\n    @property\n    def lineage(self):\n        # Represents how to get to a specific command using the CLI.\n        # It includes all commands that came before it and itself in\n        # a list.\n        return [self]\n\n    @property\n    def lineage_names(self):\n        # Represents the lineage of a command in terms of command ``name``\n        return [cmd.name for cmd in self.lineage]\n\n    def __call__(self, args, parsed_globals):\n        \"\"\"Invoke CLI operation.\n\n        :type args: str\n        :param args: The remaining command line args.\n\n        :type parsed_globals: ``argparse.Namespace``\n        :param parsed_globals: The parsed arguments so far.\n\n        :rtype: int\n        :return: The return code of the operation.  This will be used\n            as the RC code for the ``aws`` process.\n\n        \"\"\"\n        # Subclasses are expected to implement this method.\n        pass\n\n    def create_help_command(self):\n        # Subclasses are expected to implement this method if they want\n        # help docs.\n        return None\n\n    @property\n    def arg_table(self):\n        return {}\n", "awscli/__main__.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nimport sys\n\nfrom awscli.clidriver import main\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", "awscli/__init__.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nAWSCLI\n----\nA Universal Command Line Environment for Amazon Web Services.\n\"\"\"\nimport os\n\n__version__ = '1.33.14'\n\n#\n# Get our data path to be added to botocore's search path\n#\n_awscli_data_path = []\nif 'AWS_DATA_PATH' in os.environ:\n    for path in os.environ['AWS_DATA_PATH'].split(os.pathsep):\n        path = os.path.expandvars(path)\n        path = os.path.expanduser(path)\n        _awscli_data_path.append(path)\n_awscli_data_path.append(\n    os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')\n)\nos.environ['AWS_DATA_PATH'] = os.pathsep.join(_awscli_data_path)\n\n\nEnvironmentVariables = {\n    'ca_bundle': ('ca_bundle', 'AWS_CA_BUNDLE', None, None),\n    'output': ('output', 'AWS_DEFAULT_OUTPUT', 'json', None),\n}\n\n\nSCALAR_TYPES = set([\n    'string', 'float', 'integer', 'long', 'boolean', 'double',\n    'blob', 'timestamp'\n])\nCOMPLEX_TYPES = set(['structure', 'map', 'list'])\n", "awscli/text.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n\n#     http://aws.amazon.com/apache2.0/\n\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\ndef format_text(data, stream):\n    _format_text(data, stream)\n\n\ndef _format_text(item, stream, identifier=None, scalar_keys=None):\n    if isinstance(item, dict):\n        _format_dict(scalar_keys, item, identifier, stream)\n    elif isinstance(item, list):\n        _format_list(item, identifier, stream)\n    else:\n        # If it's not a list or a dict, we just write the scalar\n        # value out directly.\n        stream.write(str(item))\n        stream.write('\\n')\n\n\ndef _format_list(item, identifier, stream):\n    if not item:\n        return\n    if any(isinstance(el, dict) for el in item):\n        all_keys = _all_scalar_keys(item)\n        for element in item:\n            _format_text(element, stream=stream, identifier=identifier,\n                         scalar_keys=all_keys)\n    elif any(isinstance(el, list) for el in item):\n        scalar_elements, non_scalars = _partition_list(item)\n        if scalar_elements:\n            _format_scalar_list(scalar_elements, identifier, stream)\n        for non_scalar in non_scalars:\n            _format_text(non_scalar, stream=stream,\n                         identifier=identifier)\n    else:\n        _format_scalar_list(item, identifier, stream)\n\n\ndef _partition_list(item):\n    scalars = []\n    non_scalars = []\n    for element in item:\n        if isinstance(element, (list, dict)):\n            non_scalars.append(element)\n        else:\n            scalars.append(element)\n    return scalars, non_scalars\n\n\ndef _format_scalar_list(elements, identifier, stream):\n    if identifier is not None:\n        for item in elements:\n            stream.write('%s\\t%s\\n' % (identifier.upper(),\n                                       item))\n    else:\n        # For a bare list, just print the contents.\n        stream.write('\\t'.join([str(item) for item in elements]))\n        stream.write('\\n')\n\n\ndef _format_dict(scalar_keys, item, identifier, stream):\n    scalars, non_scalars = _partition_dict(item, scalar_keys=scalar_keys)\n    if scalars:\n        if identifier is not None:\n            scalars.insert(0, identifier.upper())\n        stream.write('\\t'.join(scalars))\n        stream.write('\\n')\n    for new_identifier, non_scalar in non_scalars:\n        _format_text(item=non_scalar, stream=stream,\n                     identifier=new_identifier)\n\n\ndef _all_scalar_keys(list_of_dicts):\n    keys_seen = set()\n    for item_dict in list_of_dicts:\n        for key, value in item_dict.items():\n            if not isinstance(value, (dict, list)):\n                keys_seen.add(key)\n    return list(sorted(keys_seen))\n\n\ndef _partition_dict(item_dict, scalar_keys):\n    # Given a dictionary, partition it into two list based on the\n    # values associated with the keys.\n    # {'foo': 'scalar', 'bar': 'scalar', 'baz': ['not, 'scalar']}\n    # scalar = [('foo', 'scalar'), ('bar', 'scalar')]\n    # non_scalar = [('baz', ['not', 'scalar'])]\n    scalar = []\n    non_scalar = []\n    if scalar_keys is None:\n        # scalar_keys can have more than just the keys in the item_dict,\n        # but if user does not provide scalar_keys, we'll grab the keys\n        # from the current item_dict\n        for key, value in sorted(item_dict.items()):\n            if isinstance(value, (dict, list)):\n                non_scalar.append((key, value))\n            else:\n                scalar.append(str(value))\n    else:\n        for key in scalar_keys:\n            scalar.append(str(item_dict.get(key, '')))\n        remaining_keys = sorted(set(item_dict.keys()) - set(scalar_keys))\n        for remaining_key in remaining_keys:\n            non_scalar.append((remaining_key, item_dict[remaining_key]))\n    return scalar, non_scalar\n", "awscli/compat.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n\n#     http://aws.amazon.com/apache2.0/\n\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport sys\nimport re\nimport shlex\nimport os\nimport os.path\nimport platform\nimport zipfile\nimport signal\nimport contextlib\nimport queue\nimport io\nimport collections.abc as collections_abc\nimport locale\nimport urllib.parse as urlparse\nfrom urllib.error import URLError\nfrom urllib.request import urlopen\nfrom configparser import RawConfigParser\nfrom functools import partial\n\nfrom urllib.error import URLError\n\nfrom botocore.compat import six\nfrom botocore.compat import OrderedDict\n\n# Backwards compatible definitions from six\nPY3 = sys.version_info[0] == 3\nadvance_iterator = next\nshlex_quote = shlex.quote\nStringIO = io.StringIO\nBytesIO = io.BytesIO\nbinary_type = bytes\nraw_input = input\n\n\n# Most, but not all, python installations will have zlib. This is required to\n# compress any files we send via a push. If we can't compress, we can still\n# package the files in a zip container.\ntry:\n    import zlib\n    ZIP_COMPRESSION_MODE = zipfile.ZIP_DEFLATED\nexcept ImportError:\n    ZIP_COMPRESSION_MODE = zipfile.ZIP_STORED\n\n\ntry:\n    import sqlite3\nexcept ImportError:\n    sqlite3 = None\n\n\nis_windows = sys.platform == 'win32'\n\nis_macos = sys.platform == 'darwin'\n\n\nif is_windows:\n    default_pager = 'more'\nelse:\n    default_pager = 'less -R'\n\n\nclass StdinMissingError(Exception):\n    def __init__(self):\n        message = (\n            'stdin is required for this operation, but is not available.'\n        )\n        super(StdinMissingError, self).__init__(message)\n\n\nclass NonTranslatedStdout(object):\n    \"\"\" This context manager sets the line-end translation mode for stdout.\n\n    It is deliberately set to binary mode so that `\\r` does not get added to\n    the line ending. This can be useful when printing commands where a\n    windows style line ending would cause errors.\n    \"\"\"\n\n    def __enter__(self):\n        if sys.platform == \"win32\":\n            import msvcrt\n            self.previous_mode = msvcrt.setmode(sys.stdout.fileno(),\n                                                os.O_BINARY)\n        return sys.stdout\n\n    def __exit__(self, type, value, traceback):\n        if sys.platform == \"win32\":\n            import msvcrt\n            msvcrt.setmode(sys.stdout.fileno(), self.previous_mode)\n\n\ndef ensure_text_type(s):\n    if isinstance(s, str):\n        return s\n    if isinstance(s, bytes):\n        return s.decode('utf-8')\n    raise ValueError(\"Expected str, unicode or bytes, received %s.\" % type(s))\n\n\ndef get_binary_stdin():\n    if sys.stdin is None:\n        raise StdinMissingError()\n    return sys.stdin.buffer\n\n\ndef get_binary_stdout():\n    return sys.stdout.buffer\n\n\ndef _get_text_writer(stream, errors):\n    return stream\n\n\ndef bytes_print(statement, stdout=None):\n    \"\"\"\n    This function is used to write raw bytes to stdout.\n    \"\"\"\n    if stdout is None:\n        stdout = sys.stdout\n\n    if getattr(stdout, 'buffer', None):\n        stdout.buffer.write(statement)\n    else:\n        # If it is not possible to write to the standard out buffer.\n        # The next best option is to decode and write to standard out.\n        stdout.write(statement.decode('utf-8'))\n\n\ndef compat_open(filename, mode='r', encoding=None, access_permissions=None):\n    \"\"\"Back-port open() that accepts an encoding argument.\n\n    In python3 this uses the built in open() and in python2 this\n    uses the io.open() function.\n\n    If the file is not being opened in binary mode, then we'll\n    use locale.getpreferredencoding() to find the preferred\n    encoding.\n\n    \"\"\"\n    opener = os.open\n    if access_permissions is not None:\n        opener = partial(os.open, mode=access_permissions)\n    if 'b' not in mode:\n        encoding = locale.getpreferredencoding()\n    return open(filename, mode, encoding=encoding, opener=opener)\n\n\ndef get_stdout_text_writer():\n    return _get_text_writer(sys.stdout, errors=\"strict\")\n\n\ndef get_stderr_text_writer():\n    return _get_text_writer(sys.stderr, errors=\"replace\")\n\n\ndef get_stderr_encoding():\n    encoding = getattr(sys.__stderr__, 'encoding', None)\n    if encoding is None:\n        encoding = 'utf-8'\n    return encoding\n\n\ndef compat_input(prompt):\n    \"\"\"\n    Cygwin's pty's are based on pipes. Therefore, when it interacts with a Win32\n    program (such as Win32 python), what that program sees is a pipe instead of\n    a console. This is important because python buffers pipes, and so on a\n    pty-based terminal, text will not necessarily appear immediately. In most\n    cases, this isn't a big deal. But when we're doing an interactive prompt,\n    the result is that the prompts won't display until we fill the buffer. Since\n    raw_input does not flush the prompt, we need to manually write and flush it.\n\n    See https://github.com/mintty/mintty/issues/56 for more details.\n    \"\"\"\n    sys.stdout.write(prompt)\n    sys.stdout.flush()\n    return raw_input()\n\n\ndef compat_shell_quote(s, platform=None):\n    \"\"\"Return a shell-escaped version of the string *s*\n\n    Unfortunately `shlex.quote` doesn't support Windows, so this method\n    provides that functionality.\n    \"\"\"\n    if platform is None:\n        platform = sys.platform\n\n    if platform == \"win32\":\n        return _windows_shell_quote(s)\n    else:\n        return shlex.quote(s)\n\n\ndef _windows_shell_quote(s):\n    \"\"\"Return a Windows shell-escaped version of the string *s*\n\n    Windows has potentially bizarre rules depending on where you look. When\n    spawning a process via the Windows C runtime the rules are as follows:\n\n    https://docs.microsoft.com/en-us/cpp/cpp/parsing-cpp-command-line-arguments\n\n    To summarize the relevant bits:\n\n    * Only space and tab are valid delimiters\n    * Double quotes are the only valid quotes\n    * Backslash is interpreted literally unless it is part of a chain that\n      leads up to a double quote. Then the backslashes escape the backslashes,\n      and if there is an odd number the final backslash escapes the quote.\n\n    :param s: A string to escape\n    :return: An escaped string\n    \"\"\"\n    if not s:\n        return '\"\"'\n\n    buff = []\n    num_backspaces = 0\n    for character in s:\n        if character == '\\\\':\n            # We can't simply append backslashes because we don't know if\n            # they will need to be escaped. Instead we separately keep track\n            # of how many we've seen.\n            num_backspaces += 1\n        elif character == '\"':\n            if num_backspaces > 0:\n                # The backslashes are part of a chain that lead up to a\n                # double quote, so they need to be escaped.\n                buff.append('\\\\' * (num_backspaces * 2))\n                num_backspaces = 0\n\n            # The double quote also needs to be escaped. The fact that we're\n            # seeing it at all means that it must have been escaped in the\n            # original source.\n            buff.append('\\\\\"')\n        else:\n            if num_backspaces > 0:\n                # The backslashes aren't part of a chain leading up to a\n                # double quote, so they can be inserted directly without\n                # being escaped.\n                buff.append('\\\\' * num_backspaces)\n                num_backspaces = 0\n            buff.append(character)\n\n    # There may be some leftover backspaces if they were on the trailing\n    # end, so they're added back in here.\n    if num_backspaces > 0:\n        buff.append('\\\\' * num_backspaces)\n\n    new_s = ''.join(buff)\n    if ' ' in new_s or '\\t' in new_s:\n        # If there are any spaces or tabs then the string needs to be double\n        # quoted.\n        return '\"%s\"' % new_s\n    return new_s\n\n\ndef get_popen_kwargs_for_pager_cmd(pager_cmd=None):\n    \"\"\"Returns the default pager to use dependent on platform\n\n    :rtype: str\n    :returns: A string represent the paging command to run based on the\n        platform being used.\n    \"\"\"\n    popen_kwargs = {}\n    if pager_cmd is None:\n        pager_cmd = default_pager\n    # Similar to what we do with the help command, we need to specify\n    # shell as True to make it work in the pager for Windows\n    if is_windows:\n        popen_kwargs = {'shell': True}\n    else:\n        pager_cmd = shlex.split(pager_cmd)\n    popen_kwargs['args'] = pager_cmd\n    return popen_kwargs\n\n\n@contextlib.contextmanager\ndef ignore_user_entered_signals():\n    \"\"\"\n    Ignores user entered signals to avoid process getting killed.\n    \"\"\"\n    if is_windows:\n        signal_list = [signal.SIGINT]\n    else:\n        signal_list = [signal.SIGINT, signal.SIGQUIT, signal.SIGTSTP]\n    actual_signals = []\n    for user_signal in signal_list:\n        actual_signals.append(signal.signal(user_signal, signal.SIG_IGN))\n    try:\n        yield\n    finally:\n        for sig, user_signal in enumerate(signal_list):\n            signal.signal(user_signal, actual_signals[sig])\n\n\n# linux_distribution is used by the CodeDeploy customization. Python 3.8\n# removed it from the stdlib, so it is vendored here in the case where the\n# import fails.\ntry:\n    from platform import linux_distribution\nexcept ImportError:\n    _UNIXCONFDIR = '/etc'\n    def _dist_try_harder(distname, version, id):\n\n        \"\"\" Tries some special tricks to get the distribution\n            information in case the default method fails.\n            Currently supports older SuSE Linux, Caldera OpenLinux and\n            Slackware Linux distributions.\n        \"\"\"\n        if os.path.exists('/var/adm/inst-log/info'):\n            # SuSE Linux stores distribution information in that file\n            distname = 'SuSE'\n            with open('/var/adm/inst-log/info') as f:\n                for line in f:\n                    tv = line.split()\n                    if len(tv) == 2:\n                        tag, value = tv\n                    else:\n                        continue\n                    if tag == 'MIN_DIST_VERSION':\n                        version = value.strip()\n                    elif tag == 'DIST_IDENT':\n                        values = value.split('-')\n                        id = values[2]\n            return distname, version, id\n\n        if os.path.exists('/etc/.installed'):\n            # Caldera OpenLinux has some infos in that file (thanks to Colin Kong)\n            with open('/etc/.installed') as f:\n                for line in f:\n                    pkg = line.split('-')\n                    if len(pkg) >= 2 and pkg[0] == 'OpenLinux':\n                        # XXX does Caldera support non Intel platforms ? If yes,\n                        #     where can we find the needed id ?\n                        return 'OpenLinux', pkg[1], id\n\n        if os.path.isdir('/usr/lib/setup'):\n            # Check for slackware version tag file (thanks to Greg Andruk)\n            verfiles = os.listdir('/usr/lib/setup')\n            for n in range(len(verfiles)-1, -1, -1):\n                if verfiles[n][:14] != 'slack-version-':\n                    del verfiles[n]\n            if verfiles:\n                verfiles.sort()\n                distname = 'slackware'\n                version = verfiles[-1][14:]\n                return distname, version, id\n\n        return distname, version, id\n\n    _release_filename = re.compile(r'(\\w+)[-_](release|version)', re.ASCII)\n    _lsb_release_version = re.compile(r'(.+)'\n                                      r' release '\n                                      r'([\\d.]+)'\n                                      r'[^(]*(?:\\((.+)\\))?', re.ASCII)\n    _release_version = re.compile(r'([^0-9]+)'\n                                  r'(?: release )?'\n                                  r'([\\d.]+)'\n                                  r'[^(]*(?:\\((.+)\\))?', re.ASCII)\n\n    # See also http://www.novell.com/coolsolutions/feature/11251.html\n    # and http://linuxmafia.com/faq/Admin/release-files.html\n    # and http://data.linux-ntfs.org/rpm/whichrpm\n    # and http://www.die.net/doc/linux/man/man1/lsb_release.1.html\n\n    _supported_dists = (\n        'SuSE', 'debian', 'fedora', 'redhat', 'centos',\n        'mandrake', 'mandriva', 'rocks', 'slackware', 'yellowdog', 'gentoo',\n        'UnitedLinux', 'turbolinux', 'arch', 'mageia')\n\n    def _parse_release_file(firstline):\n\n        # Default to empty 'version' and 'id' strings.  Both defaults are used\n        # when 'firstline' is empty.  'id' defaults to empty when an id can not\n        # be deduced.\n        version = ''\n        id = ''\n\n        # Parse the first line\n        m = _lsb_release_version.match(firstline)\n        if m is not None:\n            # LSB format: \"distro release x.x (codename)\"\n            return tuple(m.groups())\n\n        # Pre-LSB format: \"distro x.x (codename)\"\n        m = _release_version.match(firstline)\n        if m is not None:\n            return tuple(m.groups())\n\n        # Unknown format... take the first two words\n        l = firstline.strip().split()\n        if l:\n            version = l[0]\n            if len(l) > 1:\n                id = l[1]\n        return '', version, id\n\n    _distributor_id_file_re = re.compile(\"(?:DISTRIB_ID\\s*=)\\s*(.*)\", re.I)\n    _release_file_re = re.compile(\"(?:DISTRIB_RELEASE\\s*=)\\s*(.*)\", re.I)\n    _codename_file_re = re.compile(\"(?:DISTRIB_CODENAME\\s*=)\\s*(.*)\", re.I)\n\n    def linux_distribution(distname='', version='', id='',\n                           supported_dists=_supported_dists,\n                           full_distribution_name=1):\n        return _linux_distribution(distname, version, id, supported_dists,\n                                   full_distribution_name)\n\n    def _linux_distribution(distname, version, id, supported_dists,\n                            full_distribution_name):\n\n        \"\"\" Tries to determine the name of the Linux OS distribution name.\n            The function first looks for a distribution release file in\n            /etc and then reverts to _dist_try_harder() in case no\n            suitable files are found.\n            supported_dists may be given to define the set of Linux\n            distributions to look for. It defaults to a list of currently\n            supported Linux distributions identified by their release file\n            name.\n            If full_distribution_name is true (default), the full\n            distribution read from the OS is returned. Otherwise the short\n            name taken from supported_dists is used.\n            Returns a tuple (distname, version, id) which default to the\n            args given as parameters.\n        \"\"\"\n        # check for the Debian/Ubuntu /etc/lsb-release file first, needed so\n        # that the distribution doesn't get identified as Debian.\n        # https://bugs.python.org/issue9514\n        try:\n            with open(\"/etc/lsb-release\", \"r\") as etclsbrel:\n                for line in etclsbrel:\n                    m = _distributor_id_file_re.search(line)\n                    if m:\n                        _u_distname = m.group(1).strip()\n                    m = _release_file_re.search(line)\n                    if m:\n                        _u_version = m.group(1).strip()\n                    m = _codename_file_re.search(line)\n                    if m:\n                        _u_id = m.group(1).strip()\n                if _u_distname and _u_version:\n                    return (_u_distname, _u_version, _u_id)\n        except (EnvironmentError, UnboundLocalError):\n                pass\n\n        try:\n            etc = os.listdir(_UNIXCONFDIR)\n        except OSError:\n            # Probably not a Unix system\n            return distname, version, id\n        etc.sort()\n        for file in etc:\n            m = _release_filename.match(file)\n            if m is not None:\n                _distname, dummy = m.groups()\n                if _distname in supported_dists:\n                    distname = _distname\n                    break\n        else:\n            return _dist_try_harder(distname, version, id)\n\n        # Read the first line\n        with open(os.path.join(_UNIXCONFDIR, file), 'r',\n                  encoding='utf-8', errors='surrogateescape') as f:\n            firstline = f.readline()\n        _distname, _version, _id = _parse_release_file(firstline)\n\n        if _distname and full_distribution_name:\n            distname = _distname\n        if _version:\n            version = _version\n        if _id:\n            id = _id\n        return distname, version, id\n", "awscli/shorthand.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Module for parsing shorthand syntax.\n\nThis module parses any CLI options that use a \"shorthand\"\nsyntax::\n\n    --foo A=b,C=d\n         |------|\n            |\n            Shorthand syntax\n\n\nThis module provides two main classes to do this.\nFirst, there's a ``ShorthandParser`` class.  This class works\non a purely syntactic level.  It looks only at the string value\nprovided to it in order to figure out how the string should be parsed.\n\nHowever, because there was a pre-existing shorthand parser, we need\nto remain backwards compatible with the previous parser.  One of the\nthings the previous parser did was use the associated JSON model to\ncontrol how the expression was parsed.\n\nIn order to accommodate this a post processing class is provided that\ntakes the parsed values from the ``ShorthandParser`` as well as the\ncorresponding JSON model for the CLI argument and makes any adjustments\nnecessary to maintain backwards compatibility.  This is done in the\n``BackCompatVisitor`` class.\n\n\"\"\"\nimport re\nimport string\n\nfrom awscli.utils import is_document_type\n\n\n_EOF = object()\n\n\nclass _NamedRegex(object):\n    def __init__(self, name, regex_str):\n        self.name = name\n        self.regex = re.compile(regex_str, re.UNICODE)\n\n    def match(self, value):\n        return self.regex.match(value)\n\n\nclass ShorthandParseError(Exception):\n\n    def _error_location(self):\n        consumed, remaining, num_spaces = self.value, '', self.index\n        if '\\n' in self.value[:self.index]:\n            # If there's newlines in the consumed expression, we want\n            # to make sure we're only counting the spaces\n            # from the last newline:\n            # foo=bar,\\n\n            # bar==baz\n            #     ^\n            last_newline = self.value[:self.index].rindex('\\n')\n            num_spaces = self.index - last_newline - 1\n        if '\\n' in self.value[self.index:]:\n            # If there's newline in the remaining, divide value\n            # into consumed and remaining\n            # foo==bar,\\n\n            #     ^\n            # bar=baz\n            next_newline = self.index + self.value[self.index:].index('\\n')\n            consumed = self.value[:next_newline]\n            remaining = self.value[next_newline:]\n        return '%s\\n%s%s' % (consumed, (' ' * num_spaces) + '^', remaining)\n\n\nclass ShorthandParseSyntaxError(ShorthandParseError):\n    def __init__(self, value, expected, actual, index):\n        self.value = value\n        self.expected = expected\n        self.actual = actual\n        self.index = index\n        msg = self._construct_msg()\n        super(ShorthandParseSyntaxError, self).__init__(msg)\n\n    def _construct_msg(self):\n        msg = (\n            \"Expected: '%s', received: '%s' for input:\\n\"\n            \"%s\"\n        ) % (self.expected, self.actual, self._error_location())\n        return msg\n\n\nclass DuplicateKeyInObjectError(ShorthandParseError):\n    def __init__(self, key, value, index):\n        self.key = key\n        self.value = value\n        self.index = index\n        msg = self._construct_msg()\n        super(DuplicateKeyInObjectError, self).__init__(msg)\n\n    def _construct_msg(self):\n        msg = (\n            \"Second instance of key \\\"%s\\\" encountered for input:\\n%s\\n\"\n            \"This is often because there is a preceding \\\",\\\" instead of a \"\n            \"space.\"\n        ) % (self.key, self._error_location())\n        return msg\n\n\nclass DocumentTypesNotSupportedError(Exception):\n    pass\n\n\nclass ShorthandParser(object):\n    \"\"\"Parses shorthand syntax in the CLI.\n\n    Note that this parser does not rely on any JSON models to control\n    how to parse the shorthand syntax.\n\n    \"\"\"\n\n    _SINGLE_QUOTED = _NamedRegex('singled quoted', r'\\'(?:\\\\\\\\|\\\\\\'|[^\\'])*\\'')\n    _DOUBLE_QUOTED = _NamedRegex('double quoted', r'\"(?:\\\\\\\\|\\\\\"|[^\"])*\"')\n    _START_WORD = u'\\!\\#-&\\(-\\+\\--\\<\\>-Z\\\\\\\\-z\\u007c-\\uffff'\n    _FIRST_FOLLOW_CHARS = u'\\s\\!\\#-&\\(-\\+\\--\\\\\\\\\\^-\\|~-\\uffff'\n    _SECOND_FOLLOW_CHARS = u'\\s\\!\\#-&\\(-\\+\\--\\<\\>-\\uffff'\n    _ESCAPED_COMMA = '(\\\\\\\\,)'\n    _FIRST_VALUE = _NamedRegex(\n        'first',\n        u'({escaped_comma}|[{start_word}])'\n        u'({escaped_comma}|[{follow_chars}])*'.format(\n            escaped_comma=_ESCAPED_COMMA,\n            start_word=_START_WORD,\n            follow_chars=_FIRST_FOLLOW_CHARS,\n        ))\n    _SECOND_VALUE = _NamedRegex(\n        'second',\n        u'({escaped_comma}|[{start_word}])'\n        u'({escaped_comma}|[{follow_chars}])*'.format(\n            escaped_comma=_ESCAPED_COMMA,\n            start_word=_START_WORD,\n            follow_chars=_SECOND_FOLLOW_CHARS,\n        ))\n\n    def __init__(self):\n        self._tokens = []\n\n    def parse(self, value):\n        \"\"\"Parse shorthand syntax.\n\n        For example::\n\n            parser = ShorthandParser()\n            parser.parse('a=b')  # {'a': 'b'}\n            parser.parse('a=b,c')  # {'a': ['b', 'c']}\n\n        :type value: str\n        :param value: Any value that needs to be parsed.\n\n        :return: Parsed value, which will be a dictionary.\n        \"\"\"\n        self._input_value = value\n        self._index = 0\n        return self._parameter()\n\n    def _parameter(self):\n        # parameter = keyval *(\",\" keyval)\n        params = {}\n        key, val = self._keyval()\n        params[key] = val\n        last_index = self._index\n        while self._index < len(self._input_value):\n            self._expect(',', consume_whitespace=True)\n            key, val = self._keyval()\n            # If a key is already defined, it is likely an incorrectly written\n            # shorthand argument. Raise an error to inform the user.\n            if key in params:\n                raise DuplicateKeyInObjectError(\n                    key, self._input_value, last_index + 1\n                )\n            params[key] = val\n            last_index = self._index\n        return params\n\n    def _keyval(self):\n        # keyval = key \"=\" [values]\n        key = self._key()\n        self._expect('=', consume_whitespace=True)\n        values = self._values()\n        return key, values\n\n    def _key(self):\n        # key = 1*(alpha / %x30-39 / %x5f / %x2e / %x23)  ; [a-zA-Z0-9\\-_.#/]\n        valid_chars = string.ascii_letters + string.digits + '-_.#/:'\n        start = self._index\n        while not self._at_eof():\n            if self._current() not in valid_chars:\n                break\n            self._index += 1\n        return self._input_value[start:self._index]\n\n    def _values(self):\n        # values = csv-list / explicit-list / hash-literal\n        if self._at_eof():\n            return ''\n        elif self._current() == '[':\n            return self._explicit_list()\n        elif self._current() == '{':\n            return self._hash_literal()\n        else:\n            return self._csv_value()\n\n    def _csv_value(self):\n        # Supports either:\n        # foo=bar     -> 'bar'\n        #     ^\n        # foo=bar,baz -> ['bar', 'baz']\n        #     ^\n        first_value = self._first_value()\n        self._consume_whitespace()\n        if self._at_eof() or self._input_value[self._index] != ',':\n            return first_value\n        self._expect(',', consume_whitespace=True)\n        csv_list = [first_value]\n        # Try to parse remaining list values.\n        # It's possible we don't parse anything:\n        # a=b,c=d\n        #     ^-here\n        # In the case above, we'll hit the ShorthandParser,\n        # backtrack to the comma, and return a single scalar\n        # value 'b'.\n        while True:\n            try:\n                current = self._second_value()\n                self._consume_whitespace()\n                if self._at_eof():\n                    csv_list.append(current)\n                    break\n                self._expect(',', consume_whitespace=True)\n                csv_list.append(current)\n            except ShorthandParseSyntaxError:\n                # Backtrack to the previous comma.\n                # This can happen when we reach this case:\n                # foo=a,b,c=d,e=f\n                #     ^-start\n                # foo=a,b,c=d,e=f\n                #          ^-error, \"expected ',' received '='\n                # foo=a,b,c=d,e=f\n                #        ^-backtrack to here.\n                if self._at_eof():\n                    raise\n                self._backtrack_to(',')\n                break\n        if len(csv_list) == 1:\n            # Then this was a foo=bar case, so we expect\n            # this to parse to a scalar value 'bar', i.e\n            # {\"foo\": \"bar\"} instead of {\"bar\": [\"bar\"]}\n            return first_value\n        return csv_list\n\n    def _value(self):\n        result = self._FIRST_VALUE.match(self._input_value[self._index:])\n        if result is not None:\n            consumed = self._consume_matched_regex(result)\n            return consumed.replace('\\\\,', ',').rstrip()\n        return ''\n\n    def _explicit_list(self):\n        # explicit-list = \"[\" [value *(\",' value)] \"]\"\n        self._expect('[', consume_whitespace=True)\n        values = []\n        while self._current() != ']':\n            val = self._explicit_values()\n            values.append(val)\n            self._consume_whitespace()\n            if self._current() != ']':\n                self._expect(',')\n                self._consume_whitespace()\n        self._expect(']')\n        return values\n\n    def _explicit_values(self):\n        # values = csv-list / explicit-list / hash-literal\n        if self._current() == '[':\n            return self._explicit_list()\n        elif self._current() == '{':\n            return self._hash_literal()\n        else:\n            return self._first_value()\n\n    def _hash_literal(self):\n        self._expect('{', consume_whitespace=True)\n        keyvals = {}\n        while self._current() != '}':\n            key = self._key()\n            self._expect('=', consume_whitespace=True)\n            v = self._explicit_values()\n            self._consume_whitespace()\n            if self._current() != '}':\n                self._expect(',')\n                self._consume_whitespace()\n            keyvals[key] = v\n        self._expect('}')\n        return keyvals\n\n    def _first_value(self):\n        # first-value = value / single-quoted-val / double-quoted-val\n        if self._current() == \"'\":\n            return self._single_quoted_value()\n        elif self._current() == '\"':\n            return self._double_quoted_value()\n        return self._value()\n\n    def _single_quoted_value(self):\n        # single-quoted-value = %x27 *(val-escaped-single) %x27\n        # val-escaped-single  = %x20-26 / %x28-7F / escaped-escape /\n        #                       (escape single-quote)\n        return self._consume_quoted(self._SINGLE_QUOTED, escaped_char=\"'\")\n\n    def _consume_quoted(self, regex, escaped_char=None):\n        value = self._must_consume_regex(regex)[1:-1]\n        if escaped_char is not None:\n            value = value.replace(\"\\\\%s\" % escaped_char, escaped_char)\n            value = value.replace(\"\\\\\\\\\", \"\\\\\")\n        return value\n\n    def _double_quoted_value(self):\n        return self._consume_quoted(self._DOUBLE_QUOTED, escaped_char='\"')\n\n    def _second_value(self):\n        if self._current() == \"'\":\n            return self._single_quoted_value()\n        elif self._current() == '\"':\n            return self._double_quoted_value()\n        else:\n            consumed = self._must_consume_regex(self._SECOND_VALUE)\n            return consumed.replace('\\\\,', ',').rstrip()\n\n    def _expect(self, char, consume_whitespace=False):\n        if consume_whitespace:\n            self._consume_whitespace()\n        if self._index >= len(self._input_value):\n            raise ShorthandParseSyntaxError(self._input_value, char,\n                                            'EOF', self._index)\n        actual = self._input_value[self._index]\n        if actual != char:\n            raise ShorthandParseSyntaxError(self._input_value, char,\n                                            actual, self._index)\n        self._index += 1\n        if consume_whitespace:\n            self._consume_whitespace()\n\n    def _must_consume_regex(self, regex):\n        result = regex.match(self._input_value[self._index:])\n        if result is not None:\n            return self._consume_matched_regex(result)\n        raise ShorthandParseSyntaxError(self._input_value, '<%s>' % regex.name,\n                                        '<none>', self._index)\n\n    def _consume_matched_regex(self, result):\n        start, end = result.span()\n        v = self._input_value[self._index+start:self._index+end]\n        self._index += (end - start)\n        return v\n\n    def _current(self):\n        # If the index is at the end of the input value,\n        # then _EOF will be returned.\n        if self._index < len(self._input_value):\n            return self._input_value[self._index]\n        return _EOF\n\n    def _at_eof(self):\n        return self._index >= len(self._input_value)\n\n    def _backtrack_to(self, char):\n        while self._index >= 0 and self._input_value[self._index] != char:\n            self._index -= 1\n\n    def _consume_whitespace(self):\n        while self._current() != _EOF and self._current() in string.whitespace:\n            self._index += 1\n\n\nclass ModelVisitor(object):\n    def visit(self, params, model):\n        self._visit({}, model, '', params)\n\n    def _visit(self, parent, shape, name, value):\n        method = getattr(self, '_visit_%s' % shape.type_name,\n                         self._visit_scalar)\n        method(parent, shape, name, value)\n\n    def _visit_structure(self, parent, shape, name, value):\n        if not isinstance(value, dict):\n            return\n        for member_name, member_shape in shape.members.items():\n            self._visit(value, member_shape, member_name,\n                        value.get(member_name))\n\n    def _visit_list(self, parent, shape, name, value):\n        if not isinstance(value, list):\n            return\n        for i, element in enumerate(value):\n            self._visit(value, shape.member, i, element)\n\n    def _visit_map(self, parent, shape, name, value):\n        if not isinstance(value, dict):\n            return\n        value_shape = shape.value\n        for k, v in value.items():\n            self._visit(value, value_shape, k, v)\n\n    def _visit_scalar(self, parent, shape, name, value):\n        pass\n\n\nclass BackCompatVisitor(ModelVisitor):\n    def _visit_structure(self, parent, shape, name, value):\n        self._raise_if_document_type_found(value, shape)\n        if not isinstance(value, dict):\n            return\n        for member_name, member_shape in shape.members.items():\n            try:\n                self._visit(value, member_shape, member_name,\n                            value.get(member_name))\n            except DocumentTypesNotSupportedError:\n                # Catch and propagate the document type error to a better\n                # error message as when the original error is thrown there is\n                # no reference to the original member that used the document\n                # type.\n                raise ShorthandParseError(\n                    'Shorthand syntax does not support document types. Use '\n                    'JSON input for top-level argument to specify nested '\n                    'parameter: %s' % member_name\n                )\n\n    def _visit_list(self, parent, shape, name, value):\n        if not isinstance(value, list):\n            # Convert a -> [a] because they specified\n            # \"foo=bar\", but \"bar\" should really be [\"bar\"].\n            if value is not None:\n                parent[name] = [value]\n        else:\n            return super(BackCompatVisitor, self)._visit_list(\n                parent, shape, name, value)\n\n    def _visit_scalar(self, parent, shape, name, value):\n        if value is None:\n            return\n        type_name = shape.type_name\n        if type_name in ['integer', 'long']:\n            parent[name] = int(value)\n        elif type_name in ['double', 'float']:\n            parent[name] = float(value)\n        elif type_name == 'boolean':\n            # We want to make sure we only set a value\n            # only if \"true\"/\"false\" is specified.\n            if value.lower() == 'true':\n                parent[name] = True\n            elif value.lower() == 'false':\n                parent[name] = False\n\n    def _raise_if_document_type_found(self, value, member_shape):\n        # Shorthand syntax does not have support for explicit typing and\n        # instead relies on the model to do type coercion. However, document\n        # types are unmodeled. So using short hand syntax on a document type\n        # would result in all values being typed as strings (e.g. 1 -> \"1\",\n        # null -> \"null\") which is probably not desired. So blocking the use\n        # of document types allows us to add proper support for them in the\n        # future in a backwards compatible way.\n        if value is not None and is_document_type(member_shape):\n            raise DocumentTypesNotSupportedError()\n", "awscli/argprocess.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Module for processing CLI args.\"\"\"\nimport os\nimport logging\n\nfrom botocore.compat import OrderedDict, json\n\nfrom awscli import SCALAR_TYPES, COMPLEX_TYPES\nfrom awscli import shorthand\nfrom awscli.utils import (\n    find_service_and_method_in_event_name, is_document_type,\n    is_document_type_container\n)\nfrom botocore.utils import is_json_value_header\n\nLOG = logging.getLogger('awscli.argprocess')\n\n\nclass ParamError(Exception):\n    def __init__(self, cli_name, message):\n        \"\"\"\n\n        :type cli_name: string\n        :param cli_name: The complete cli argument name,\n            e.g. \"--foo-bar\".  It should include the leading\n            hyphens if that's how a user would specify the name.\n\n        :type message: string\n        :param message: The error message to display to the user.\n\n        \"\"\"\n        full_message = (\"Error parsing parameter '%s': %s\" %\n                        (cli_name, message))\n        super(ParamError, self).__init__(full_message)\n        self.cli_name = cli_name\n        self.message = message\n\n\nclass ParamSyntaxError(Exception):\n    pass\n\n\nclass ParamUnknownKeyError(Exception):\n    def __init__(self, key, valid_keys):\n        valid_keys = ', '.join(valid_keys)\n        full_message = (\n            \"Unknown key '%s', valid choices \"\n            \"are: %s\" % (key, valid_keys))\n        super(ParamUnknownKeyError, self).__init__(full_message)\n\n\nclass TooComplexError(Exception):\n    pass\n\n\ndef unpack_argument(session, service_name, operation_name, cli_argument, value):\n    \"\"\"\n    Unpack an argument's value from the commandline. This is part one of a two\n    step process in handling commandline arguments. Emits the load-cli-arg\n    event with service, operation, and parameter names. Example::\n\n        load-cli-arg.ec2.describe-instances.foo\n\n    \"\"\"\n    param_name = getattr(cli_argument, 'name', 'anonymous')\n\n    value_override = session.emit_first_non_none_response(\n        'load-cli-arg.%s.%s.%s' % (service_name,\n                                   operation_name,\n                                   param_name),\n        param=cli_argument, value=value, service_name=service_name,\n        operation_name=operation_name)\n\n    if value_override is not None:\n        value = value_override\n\n    return value\n\n\ndef detect_shape_structure(param):\n    stack = []\n    return _detect_shape_structure(param, stack)\n\n\ndef _detect_shape_structure(param, stack):\n    if param.name in stack:\n        return 'recursive'\n    else:\n        stack.append(param.name)\n    try:\n        if param.type_name in SCALAR_TYPES:\n            return 'scalar'\n        elif param.type_name == 'structure':\n            sub_types = [_detect_shape_structure(p, stack)\n                        for p in param.members.values()]\n            # We're distinguishing between structure(scalar)\n            # and structure(scalars), because for the case of\n            # a single scalar in a structure we can simplify\n            # more than a structure(scalars).\n            if len(sub_types) == 1 and all(p == 'scalar' for p in sub_types):\n                return 'structure(scalar)'\n            elif len(sub_types) > 1 and all(p == 'scalar' for p in sub_types):\n                return 'structure(scalars)'\n            else:\n                return 'structure(%s)' % ', '.join(sorted(set(sub_types)))\n        elif param.type_name == 'list':\n            return 'list-%s' % _detect_shape_structure(param.member, stack)\n        elif param.type_name == 'map':\n            if param.value.type_name in SCALAR_TYPES:\n                return 'map-scalar'\n            else:\n                return 'map-%s' % _detect_shape_structure(param.value, stack)\n    finally:\n        stack.pop()\n\n\ndef unpack_cli_arg(cli_argument, value):\n    \"\"\"\n    Parses and unpacks the encoded string command line parameter\n    and returns native Python data structures that can be passed\n    to the Operation.\n\n    :type cli_argument: :class:`awscli.arguments.BaseCLIArgument`\n    :param cli_argument: The CLI argument object.\n\n    :param value: The value of the parameter.  This can be a number of\n        different python types (str, list, etc).  This is the value as\n        it's specified on the command line.\n\n    :return: The \"unpacked\" argument than can be sent to the `Operation`\n        object in python.\n    \"\"\"\n    return _unpack_cli_arg(cli_argument.argument_model, value,\n                           cli_argument.cli_name)\n\n\ndef _special_type(model):\n    # check if model is jsonvalue header and that value is serializable\n    if model.serialization.get('jsonvalue') and \\\n       model.serialization.get('location') == 'header' and \\\n       model.type_name == 'string':\n        return True\n    return False\n\n\ndef _unpack_cli_arg(argument_model, value, cli_name):\n    if is_json_value_header(argument_model) or \\\n            is_document_type(argument_model):\n        return _unpack_json_cli_arg(argument_model, value, cli_name)\n    elif argument_model.type_name in SCALAR_TYPES:\n        return unpack_scalar_cli_arg(\n            argument_model, value, cli_name)\n    elif argument_model.type_name in COMPLEX_TYPES:\n        return _unpack_complex_cli_arg(\n            argument_model, value, cli_name)\n    else:\n        return str(value)\n\n\ndef _unpack_json_cli_arg(argument_model, value, cli_name):\n    try:\n        return json.loads(value, object_pairs_hook=OrderedDict)\n    except ValueError as e:\n        raise ParamError(\n            cli_name, \"Invalid JSON: %s\\nJSON received: %s\"\n            % (e, value))\n\n\ndef _unpack_complex_cli_arg(argument_model, value, cli_name):\n    type_name = argument_model.type_name\n    if type_name == 'structure' or type_name == 'map':\n        if value.lstrip()[0] == '{':\n            return _unpack_json_cli_arg(argument_model, value, cli_name)\n        raise ParamError(cli_name, \"Invalid JSON:\\n%s\" % value)\n    elif type_name == 'list':\n        if isinstance(value, str):\n            if value.lstrip()[0] == '[':\n                return _unpack_json_cli_arg(argument_model, value, cli_name)\n        elif isinstance(value, list) and len(value) == 1:\n            single_value = value[0].strip()\n            if single_value and single_value[0] == '[':\n                return _unpack_json_cli_arg(argument_model, value[0], cli_name)\n        try:\n            # There's a couple of cases remaining here.\n            # 1. It's possible that this is just a list of strings, i.e\n            # --security-group-ids sg-1 sg-2 sg-3 => ['sg-1', 'sg-2', 'sg-3']\n            # 2. It's possible this is a list of json objects:\n            # --filters '{\"Name\": ..}' '{\"Name\": ...}'\n            member_shape_model = argument_model.member\n            return [_unpack_cli_arg(member_shape_model, v, cli_name)\n                    for v in value]\n        except (ValueError, TypeError) as e:\n            # The list params don't have a name/cli_name attached to them\n            # so they will have bad error messages.  We're going to\n            # attach the parent parameter to this error message to provide\n            # a more helpful error message.\n            raise ParamError(cli_name, value[0])\n\n\ndef unpack_scalar_cli_arg(argument_model, value, cli_name=''):\n    # Note the cli_name is used strictly for error reporting.  It's\n    # not required to use unpack_scalar_cli_arg\n    if argument_model.type_name == 'integer' or argument_model.type_name == 'long':\n        return int(value)\n    elif argument_model.type_name == 'float' or argument_model.type_name == 'double':\n        # TODO: losing precision on double types\n        return float(value)\n    elif argument_model.type_name == 'blob' and \\\n            argument_model.serialization.get('streaming'):\n        file_path = os.path.expandvars(value)\n        file_path = os.path.expanduser(file_path)\n        if not os.path.isfile(file_path):\n            msg = 'Blob values must be a path to a file.'\n            raise ParamError(cli_name, msg)\n        return open(file_path, 'rb')\n    elif argument_model.type_name == 'boolean':\n        if isinstance(value, str) and value.lower() == 'false':\n            return False\n        return bool(value)\n    else:\n        return value\n\n\ndef _supports_shorthand_syntax(model):\n    # Shorthand syntax is only supported if:\n    #\n    # 1. The argument is not a document type nor is a wrapper around a document\n    # type (e.g. is a list of document types or a map of document types). These\n    # should all be expressed as JSON input.\n    #\n    # 2. The argument is sufficiently complex, that is, it's base type is\n    # a complex type *and* if it's a list, then it can't be a list of\n    # scalar types.\n    if is_document_type_container(model):\n        return False\n    return _is_complex_shape(model)\n\n\ndef _is_complex_shape(model):\n    if model.type_name not in ['structure', 'list', 'map']:\n        return False\n    elif model.type_name == 'list':\n        if model.member.type_name not in ['structure', 'list', 'map']:\n            return False\n    return True\n\n\nclass ParamShorthand(object):\n\n    def _uses_old_list_case(self, service_id, operation_name, argument_name):\n        \"\"\"\n        Determines whether a given operation for a service needs to use the\n        deprecated shorthand parsing case for lists of structures that only have\n        a single member.\n        \"\"\"\n        cases = {\n            'firehose': {\n                'put-record-batch': ['records']\n            },\n            'workspaces': {\n                'reboot-workspaces': ['reboot-workspace-requests'],\n                'rebuild-workspaces': ['rebuild-workspace-requests'],\n                'terminate-workspaces': ['terminate-workspace-requests']\n            },\n            'elastic-load-balancing': {\n                'remove-tags': ['tags'],\n                'describe-instance-health': ['instances'],\n                'deregister-instances-from-load-balancer': ['instances'],\n                'register-instances-with-load-balancer': ['instances']\n            }\n        }\n        cases = cases.get(service_id, {}).get(operation_name, [])\n        return argument_name in cases\n\n\nclass ParamShorthandParser(ParamShorthand):\n\n    def __init__(self):\n        self._parser = shorthand.ShorthandParser()\n        self._visitor = shorthand.BackCompatVisitor()\n\n    def __call__(self, cli_argument, value, event_name, **kwargs):\n        \"\"\"Attempt to parse shorthand syntax for values.\n\n        This is intended to be hooked up as an event handler (hence the\n        **kwargs).  Given ``param`` object and its string ``value``,\n        figure out if we can parse it.  If we can parse it, we return\n        the parsed value (typically some sort of python dict).\n\n        :type cli_argument: :class:`awscli.arguments.BaseCLIArgument`\n        :param cli_argument: The CLI argument object.\n\n        :type param: :class:`botocore.parameters.Parameter`\n        :param param: The parameter object (includes various metadata\n            about the parameter).\n\n        :type value: str\n        :param value: The value for the parameter type on the command\n            line, e.g ``--foo this_value``, value would be ``\"this_value\"``.\n\n        :returns: If we can parse the value we return the parsed value.\n            If it looks like JSON, we return None (which tells the event\n            emitter to use the default ``unpack_cli_arg`` provided that\n            no other event handlers can parsed the value).  If we\n            run into an error parsing the value, a ``ParamError`` will\n            be raised.\n\n        \"\"\"\n\n        if not self._should_parse_as_shorthand(cli_argument, value):\n            return\n        else:\n            service_id, operation_name = \\\n                find_service_and_method_in_event_name(event_name)\n            return self._parse_as_shorthand(\n                cli_argument, value, service_id, operation_name)\n\n    def _parse_as_shorthand(self, cli_argument, value, service_id,\n                            operation_name):\n        try:\n            LOG.debug(\"Parsing param %s as shorthand\",\n                        cli_argument.cli_name)\n            handled_value = self._handle_special_cases(\n                cli_argument, value, service_id, operation_name)\n            if handled_value is not None:\n                return handled_value\n            if isinstance(value, list):\n                # Because of how we're using argparse, list shapes\n                # are configured with nargs='+' which means the ``value``\n                # is given to us \"conveniently\" as a list.  When\n                # this happens we need to parse each list element\n                # individually.\n                parsed = [self._parser.parse(v) for v in value]\n                self._visitor.visit(parsed, cli_argument.argument_model)\n            else:\n                # Otherwise value is just a string.\n                parsed = self._parser.parse(value)\n                self._visitor.visit(parsed, cli_argument.argument_model)\n        except shorthand.ShorthandParseError as e:\n            raise ParamError(cli_argument.cli_name, str(e))\n        except (ParamError, ParamUnknownKeyError) as e:\n            # The shorthand parse methods don't have the cli_name,\n            # so any ParamError won't have this value.  To accommodate\n            # this, ParamErrors are caught and reraised with the cli_name\n            # injected.\n            raise ParamError(cli_argument.cli_name, str(e))\n        return parsed\n\n    def _handle_special_cases(self, cli_argument, value, service_id,\n                              operation_name):\n        # We need to handle a few special cases that the previous\n        # parser handled in order to stay backwards compatible.\n        model = cli_argument.argument_model\n        if model.type_name == 'list' and \\\n           model.member.type_name == 'structure' and \\\n           len(model.member.members) == 1 and \\\n           self._uses_old_list_case(service_id, operation_name, cli_argument.name):\n            # First special case is handling a list of structures\n            # of a single element such as:\n            #\n            # --instance-ids id-1 id-2 id-3\n            #\n            # gets parsed as:\n            #\n            # [{\"InstanceId\": \"id-1\"}, {\"InstanceId\": \"id-2\"},\n            #  {\"InstanceId\": \"id-3\"}]\n            key_name = list(model.member.members.keys())[0]\n            new_values = [{key_name: v} for v in value]\n            return new_values\n        elif model.type_name == 'structure' and \\\n                len(model.members) == 1 and \\\n                'Value' in model.members and \\\n                model.members['Value'].type_name == 'string' and \\\n                '=' not in value:\n            # Second special case is where a structure of a single\n            # value whose member name is \"Value\" can be specified\n            # as:\n            # --instance-terminate-behavior shutdown\n            #\n            # gets parsed as:\n            # {\"Value\": \"shutdown\"}\n            return {'Value': value}\n\n    def _should_parse_as_shorthand(self, cli_argument, value):\n        # We first need to make sure this is a parameter that qualifies\n        # for simplification.  The first short-circuit case is if it looks\n        # like json we immediately return.\n        if value and isinstance(value, list):\n            check_val = value[0]\n        else:\n            check_val = value\n        if isinstance(check_val, str) and check_val.strip().startswith(\n                ('[', '{')):\n            LOG.debug(\"Param %s looks like JSON, not considered for \"\n                      \"param shorthand.\", cli_argument.py_name)\n            return False\n        model = cli_argument.argument_model\n        return _supports_shorthand_syntax(model)\n\n\nclass ParamShorthandDocGen(ParamShorthand):\n    \"\"\"Documentation generator for param shorthand syntax.\"\"\"\n\n    _DONT_DOC = object()\n    _MAX_STACK = 3\n\n    def supports_shorthand(self, argument_model):\n        \"\"\"Checks if a CLI argument supports shorthand syntax.\"\"\"\n        if argument_model is not None:\n            return _supports_shorthand_syntax(argument_model)\n        return False\n\n    def generate_shorthand_example(self, cli_argument, service_id,\n                                   operation_name):\n        \"\"\"Generate documentation for a CLI argument.\n\n        :type cli_argument: awscli.arguments.BaseCLIArgument\n        :param cli_argument: The CLI argument which to generate\n            documentation for.\n\n        :return: Returns either a string or ``None``.  If a string\n            is returned, it is the generated shorthand example.\n            If a value of ``None`` is returned then this indicates\n            that no shorthand syntax is available for the provided\n            ``argument_model``.\n\n        \"\"\"\n        docstring = self._handle_special_cases(\n            cli_argument, service_id, operation_name)\n        if docstring is self._DONT_DOC:\n            return None\n        elif docstring:\n            return docstring\n\n        # Otherwise we fall back to the normal docgen for shorthand\n        # syntax.\n        stack = []\n        try:\n            if cli_argument.argument_model.type_name == 'list':\n                argument_model = cli_argument.argument_model.member\n                return self._shorthand_docs(argument_model, stack) + ' ...'\n            else:\n                return self._shorthand_docs(cli_argument.argument_model, stack)\n        except TooComplexError:\n            return ''\n\n    def _handle_special_cases(self, cli_argument, service_id, operation_name):\n        model = cli_argument.argument_model\n        if model.type_name == 'list' and \\\n                model.member.type_name == 'structure' and \\\n                len(model.member.members) == 1 and \\\n                self._uses_old_list_case(\n                    service_id, operation_name, cli_argument.name):\n            member_name = list(model.member.members)[0]\n            # Handle special case where the min/max is exactly one.\n            metadata = model.metadata\n            if metadata.get('min') == 1 and metadata.get('max') == 1:\n                return '%s %s1' % (cli_argument.cli_name, member_name)\n            return '%s %s1 %s2 %s3' % (cli_argument.cli_name, member_name,\n                                       member_name, member_name)\n        elif model.type_name == 'structure' and \\\n                len(model.members) == 1 and \\\n                'Value' in model.members and \\\n                model.members['Value'].type_name == 'string':\n            return self._DONT_DOC\n        return ''\n\n    def _shorthand_docs(self, argument_model, stack):\n        if len(stack) > self._MAX_STACK:\n            raise TooComplexError()\n        if argument_model.type_name == 'structure':\n            return self._structure_docs(argument_model, stack)\n        elif argument_model.type_name == 'list':\n            return self._list_docs(argument_model, stack)\n        elif argument_model.type_name == 'map':\n            return self._map_docs(argument_model, stack)\n        else:\n            return argument_model.type_name\n\n    def _list_docs(self, argument_model, stack):\n        list_member = argument_model.member\n        stack.append(list_member.name)\n        try:\n            element_docs = self._shorthand_docs(argument_model.member, stack)\n        finally:\n            stack.pop()\n        if list_member.type_name in COMPLEX_TYPES or len(stack) > 1:\n            return '[%s,%s]' % (element_docs, element_docs)\n        else:\n            return '%s,%s' % (element_docs, element_docs)\n\n    def _map_docs(self, argument_model, stack):\n        k = argument_model.key\n        value_docs = self._shorthand_docs(argument_model.value, stack)\n        start = 'KeyName1=%s,KeyName2=%s' % (value_docs, value_docs)\n        if k.enum and not stack:\n            start += '\\n\\nWhere valid key names are:\\n'\n            for enum in k.enum:\n                start += '  %s\\n' % enum\n        elif stack:\n            start = '{%s}' % start\n        return start\n\n    def _structure_docs(self, argument_model, stack):\n        parts = []\n        for name, member_shape in argument_model.members.items():\n            if is_document_type_container(member_shape):\n                continue\n            parts.append(self._member_docs(name, member_shape, stack))\n        inner_part = ','.join(parts)\n        if not stack:\n            return inner_part\n        return '{%s}' % inner_part\n\n    def _member_docs(self, name, shape, stack):\n        if stack.count(shape.name) > 0:\n            return '( ... recursive ... )'\n        stack.append(shape.name)\n        try:\n            value_doc = self._shorthand_docs(shape, stack)\n        finally:\n            stack.pop()\n        return '%s=%s' % (name, value_doc)\n", "awscli/argparser.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport argparse\nimport sys\nfrom difflib import get_close_matches\n\n\nAWS_CLI_V2_MESSAGE = (\n    'Note: AWS CLI version 2, the latest major version '\n    'of the AWS CLI, is now stable and recommended for general '\n    'use. For more information, see the AWS CLI version 2 '\n    'installation instructions at: https://docs.aws.amazon.com/cli/'\n    'latest/userguide/install-cliv2.html'\n)\n\nHELP_BLURB = (\n    \"To see help text, you can run:\\n\"\n    \"\\n\"\n    \"  aws help\\n\"\n    \"  aws <command> help\\n\"\n    \"  aws <command> <subcommand> help\\n\"\n)\nUSAGE = (\n    \"\\r%s\\n\\n\"\n    \"usage: aws [options] <command> <subcommand> \"\n    \"[<subcommand> ...] [parameters]\\n\"\n    \"%s\" % (AWS_CLI_V2_MESSAGE, HELP_BLURB)\n)\n\n\nclass CommandAction(argparse.Action):\n    \"\"\"Custom action for CLI command arguments\n\n    Allows the choices for the argument to be mutable. The choices\n    are dynamically retrieved from the keys of the referenced command\n    table\n    \"\"\"\n    def __init__(self, option_strings, dest, command_table, **kwargs):\n        self.command_table = command_table\n        super(CommandAction, self).__init__(\n            option_strings, dest, choices=self.choices, **kwargs\n        )\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        setattr(namespace, self.dest, values)\n\n    @property\n    def choices(self):\n        return list(self.command_table.keys())\n\n    @choices.setter\n    def choices(self, val):\n        # argparse.Action will always try to set this value upon\n        # instantiation, but this value should be dynamically\n        # generated from the command table keys. So make this a\n        # NOOP if argparse.Action tries to set this value.\n        pass\n\n\nclass CLIArgParser(argparse.ArgumentParser):\n    Formatter = argparse.RawTextHelpFormatter\n\n    # When displaying invalid choice error messages,\n    # this controls how many options to show per line.\n    ChoicesPerLine = 2\n\n    def _check_value(self, action, value):\n        \"\"\"\n        It's probably not a great idea to override a \"hidden\" method\n        but the default behavior is pretty ugly and there doesn't\n        seem to be any other way to change it.\n        \"\"\"\n        # converted value must be one of the choices (if specified)\n        if action.choices is not None and value not in action.choices:\n            msg = ['Invalid choice, valid choices are:\\n']\n            for i in range(len(action.choices))[::self.ChoicesPerLine]:\n                current = []\n                for choice in action.choices[i:i+self.ChoicesPerLine]:\n                    current.append('%-40s' % choice)\n                msg.append(' | '.join(current))\n            possible = get_close_matches(value, action.choices, cutoff=0.8)\n            if possible:\n                extra = ['\\n\\nInvalid choice: %r, maybe you meant:\\n' % value]\n                for word in possible:\n                    extra.append('  * %s' % word)\n                msg.extend(extra)\n            raise argparse.ArgumentError(action, '\\n'.join(msg))\n\n    def parse_known_args(self, args, namespace=None):\n        parsed, remaining = super(CLIArgParser, self).parse_known_args(args, namespace)\n        terminal_encoding = getattr(sys.stdin, 'encoding', 'utf-8')\n        if terminal_encoding is None:\n            # In some cases, sys.stdin won't have an encoding set,\n            # (e.g if it's set to a StringIO).  In this case we just\n            # default to utf-8.\n            terminal_encoding = 'utf-8'\n        for arg, value in vars(parsed).items():\n            if isinstance(value, bytes):\n                setattr(parsed, arg, value.decode(terminal_encoding))\n            elif isinstance(value, list):\n                encoded = []\n                for v in value:\n                    if isinstance(v, bytes):\n                        encoded.append(v.decode(terminal_encoding))\n                    else:\n                        encoded.append(v)\n                setattr(parsed, arg, encoded)\n        return parsed, remaining\n\n\nclass MainArgParser(CLIArgParser):\n    Formatter = argparse.RawTextHelpFormatter\n\n    def __init__(self, command_table, version_string,\n                 description, argument_table, prog=None):\n        super(MainArgParser, self).__init__(\n            formatter_class=self.Formatter,\n            add_help=False,\n            conflict_handler='resolve',\n            description=description,\n            usage=USAGE,\n            prog=prog)\n        self._build(command_table, version_string, argument_table)\n\n    def _create_choice_help(self, choices):\n        help_str = ''\n        for choice in sorted(choices):\n            help_str += '* %s\\n' % choice\n        return help_str\n\n    def _build(self, command_table, version_string, argument_table):\n        for argument_name in argument_table:\n            argument = argument_table[argument_name]\n            argument.add_to_parser(self)\n        self.add_argument('--version', action=\"version\",\n                          version=version_string,\n                          help='Display the version of this tool')\n        self.add_argument('command', action=CommandAction,\n                          command_table=command_table)\n\n\nclass ServiceArgParser(CLIArgParser):\n\n    def __init__(self, operations_table, service_name):\n        super(ServiceArgParser, self).__init__(\n            formatter_class=argparse.RawTextHelpFormatter,\n            add_help=False,\n            conflict_handler='resolve',\n            usage=USAGE)\n        self._build(operations_table)\n        self._service_name = service_name\n\n    def _build(self, operations_table):\n        self.add_argument('operation', action=CommandAction,\n                          command_table=operations_table)\n\n\nclass ArgTableArgParser(CLIArgParser):\n    \"\"\"CLI arg parser based on an argument table.\"\"\"\n\n    def __init__(self, argument_table, command_table=None):\n        # command_table is an optional subcommand_table.  If it's passed\n        # in, then we'll update the argparse to parse a 'subcommand' argument\n        # and populate the choices field with the command table keys.\n        super(ArgTableArgParser, self).__init__(\n            formatter_class=self.Formatter,\n            add_help=False,\n            usage=USAGE,\n            conflict_handler='resolve')\n        if command_table is None:\n            command_table = {}\n        self._build(argument_table, command_table)\n\n    def _build(self, argument_table, command_table):\n        for arg_name in argument_table:\n            argument = argument_table[arg_name]\n            argument.add_to_parser(self)\n        if command_table:\n            self.add_argument('subcommand', action=CommandAction,\n                              command_table=command_table, nargs='?')\n\n    def parse_known_args(self, args, namespace=None):\n        if len(args) == 1 and args[0] == 'help':\n            namespace = argparse.Namespace()\n            namespace.help = 'help'\n            return namespace, []\n        else:\n            return super(ArgTableArgParser, self).parse_known_args(\n                args, namespace)\n", "awscli/errorhandler.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nLOG = logging.getLogger(__name__)\n\n\nclass BaseOperationError(Exception):\n    MSG_TEMPLATE = (\"A {error_type} error ({error_code}) occurred \"\n                    \"when calling the {operation_name} operation: \"\n                    \"{error_message}\")\n\n    def __init__(self, error_code, error_message, error_type, operation_name,\n                 http_status_code):\n        msg = self.MSG_TEMPLATE.format(\n            error_code=error_code, error_message=error_message,\n            error_type=error_type, operation_name=operation_name)\n        super(BaseOperationError, self).__init__(msg)\n        self.error_code = error_code\n        self.error_message = error_message\n        self.error_type = error_type\n        self.operation_name = operation_name\n        self.http_status_code = http_status_code\n\n\nclass ClientError(BaseOperationError):\n    pass\n\n\nclass ServerError(BaseOperationError):\n    pass\n\n\nclass ErrorHandler(object):\n    \"\"\"\n    This class is responsible for handling any HTTP errors that occur\n    when a service operation is called.  It is registered for the\n    ``after-call`` event and will have the opportunity to inspect\n    all operation calls.  If the HTTP response contains an error\n    ``status_code`` an appropriate error message will be printed and\n    the handler will short-circuit all further processing by exiting\n    with an appropriate error code.\n    \"\"\"\n\n    def __call__(self, http_response, parsed, model, **kwargs):\n        LOG.debug('HTTP Response Code: %d', http_response.status_code)\n        error_type = None\n        error_class = None\n        if http_response.status_code >= 500:\n            error_type = 'server'\n            error_class = ServerError\n        elif http_response.status_code >= 400 or http_response.status_code == 301:\n            error_type = 'client'\n            error_class = ClientError\n        if error_class is not None:\n            code, message = self._get_error_code_and_message(parsed)\n            raise error_class(\n                error_code=code, error_message=message,\n                error_type=error_type, operation_name=model.name,\n                http_status_code=http_response.status_code)\n\n    def _get_error_code_and_message(self, response):\n        code = 'Unknown'\n        message = 'Unknown'\n        if 'Error' in response:\n            error = response['Error']\n            return error.get('Code', code), error.get('Message', message)\n        return (code, message)\n", "awscli/customizations/codecommit.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\nimport re\nimport sys\nimport logging\nimport fileinput\nimport datetime\n\nfrom botocore.auth import SigV4Auth\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.compat import urlsplit\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.compat import NonTranslatedStdout\n\nlogger = logging.getLogger('botocore.credentials')\n\n\ndef initialize(cli):\n    \"\"\"\n    The entry point for the credential helper\n    \"\"\"\n    cli.register('building-command-table.codecommit', inject_commands)\n\n\ndef inject_commands(command_table, session, **kwargs):\n    \"\"\"\n    Injects new commands into the codecommit subcommand.\n    \"\"\"\n    command_table['credential-helper'] = CodeCommitCommand(session)\n\n\nclass CodeCommitNoOpStoreCommand(BasicCommand):\n    NAME = 'store'\n    DESCRIPTION = ('This operation does nothing, credentials'\n                   ' are calculated each time')\n    SYNOPSIS = ('aws codecommit credential-helper store')\n    EXAMPLES = ''\n    _UNDOCUMENTED = True\n\n    def _run_main(self, args, parsed_globals):\n        return 0\n\n\nclass CodeCommitNoOpEraseCommand(BasicCommand):\n    NAME = 'erase'\n    DESCRIPTION = ('This operation does nothing, no credentials'\n                   ' are ever stored')\n    SYNOPSIS = ('aws codecommit credential-helper erase')\n    EXAMPLES = ''\n    _UNDOCUMENTED = True\n\n    def _run_main(self, args, parsed_globals):\n        return 0\n\n\nclass CodeCommitGetCommand(BasicCommand):\n    NAME = 'get'\n    DESCRIPTION = ('get a username SigV4 credential pair'\n                   ' based on protocol, host and path provided'\n                   ' from standard in. This is primarily'\n                   ' called by git to generate credentials to'\n                   ' authenticate against AWS CodeCommit')\n    SYNOPSIS = ('aws codecommit credential-helper get')\n    EXAMPLES = (r'echo -e \"protocol=https\\\\n'\n                r'path=/v1/repos/myrepo\\\\n'\n                'host=git-codecommit.us-east-1.amazonaws.com\"'\n                ' | aws codecommit credential-helper get')\n    ARG_TABLE = [\n        {\n            'name': 'ignore-host-check',\n            'action': 'store_true',\n            'default': False,\n            'group_name': 'ignore-host-check',\n            'help_text': (\n                'Optional. Generate credentials regardless of whether'\n                ' the domain is an Amazon domain.'\n                )\n            }\n        ]\n\n    def __init__(self, session):\n        super(CodeCommitGetCommand, self).__init__(session)\n\n    def _run_main(self, args, parsed_globals):\n        git_parameters = self.read_git_parameters()\n        if ('amazon.com' in git_parameters['host'] or\n                'amazonaws.com' in git_parameters['host'] or\n                args.ignore_host_check):\n            theUrl = self.extract_url(git_parameters)\n            region = self.extract_region(git_parameters, parsed_globals)\n            signature = self.sign_request(region, theUrl)\n            self.write_git_parameters(signature)\n        return 0\n\n    def write_git_parameters(self, signature):\n        username = self._session.get_credentials().access_key\n        if self._session.get_credentials().token is not None:\n            username += \"%\" + self._session.get_credentials().token\n        # Python will add a \\r to the line ending for a text stdout in Windows.\n        # Git does not like the \\r, so switch to binary\n        with NonTranslatedStdout() as binary_stdout:\n            binary_stdout.write('username={0}\\n'.format(username))\n            logger.debug('username\\n%s', username)\n            binary_stdout.write('password={0}\\n'.format(signature))\n            # need to explicitly flush the buffer here,\n            # before we turn the stream back to text for windows\n            binary_stdout.flush()\n            logger.debug('signature\\n%s', signature)\n\n    def read_git_parameters(self):\n        parsed = {}\n        for line in sys.stdin:\n            line = line.strip()\n            if line:\n                key, value = line.split('=', 1)\n                parsed[key] = value\n        return parsed\n\n    def extract_url(self, parameters):\n        url = '{0}://{1}/{2}'.format(parameters['protocol'],\n                                     parameters['host'],\n                                     parameters['path'])\n        return url\n\n    def extract_region(self, parameters, parsed_globals):\n        match = re.match(r'(vpce-.+\\.)?git-codecommit(-fips)?\\.([^.]+)\\.(vpce\\.)?amazonaws\\.com',\n                         parameters['host'])\n        if match is not None:\n            return match.group(3)\n        elif parsed_globals.region is not None:\n            return parsed_globals.region\n        else:\n            return self._session.get_config_variable('region')\n\n    def sign_request(self, region, url_to_sign):\n        credentials = self._session.get_credentials()\n        signer = SigV4Auth(credentials, 'codecommit', region)\n        request = AWSRequest()\n        request.url = url_to_sign\n        request.method = 'GIT'\n        now = datetime.datetime.utcnow()\n        request.context['timestamp'] = now.strftime('%Y%m%dT%H%M%S')\n        split = urlsplit(request.url)\n        # we don't want to include the port number in the signature\n        hostname = split.netloc.split(':')[0]\n        canonical_request = '{0}\\n{1}\\n\\nhost:{2}\\n\\nhost\\n'.format(\n            request.method,\n            split.path,\n            hostname)\n        logger.debug(\"Calculating signature using v4 auth.\")\n        logger.debug('CanonicalRequest:\\n%s', canonical_request)\n        string_to_sign = signer.string_to_sign(request, canonical_request)\n        logger.debug('StringToSign:\\n%s', string_to_sign)\n        signature = signer.signature(string_to_sign, request)\n        logger.debug('Signature:\\n%s', signature)\n        return '{0}Z{1}'.format(request.context['timestamp'], signature)\n\n\nclass CodeCommitCommand(BasicCommand):\n    NAME = 'credential-helper'\n    SYNOPSIS = ('aws codecommit credential-helper')\n    EXAMPLES = ''\n\n    SUBCOMMANDS = [\n        {'name': 'get', 'command_class': CodeCommitGetCommand},\n        {'name': 'store', 'command_class': CodeCommitNoOpStoreCommand},\n        {'name': 'erase', 'command_class': CodeCommitNoOpEraseCommand},\n    ]\n    DESCRIPTION = ('Provide a SigV4 compatible user name and'\n                   ' password for git smart HTTP '\n                   ' These commands are consumed by git and'\n                   ' should not used directly. Erase and Store'\n                   ' are no-ops. Get is operation to generate'\n                   ' credentials to authenticate AWS CodeCommit.'\n                   ' Run \\\"aws codecommit credential-helper help\\\"'\n                   ' for details')\n\n    def _run_main(self, args, parsed_globals):\n        raise ValueError('usage: aws [options] codecommit'\n                         ' credential-helper <subcommand> '\n                         '[parameters]\\naws: error: too few arguments')\n", "awscli/customizations/addexamples.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nAdd authored examples to MAN and HTML documentation\n---------------------------------------------------\n\nThis customization allows authored examples in ReST format to be\ninserted into the generated help for an Operation.  To get this to\nwork you need to:\n\n* Register the ``add_examples`` function below with the\n  ``doc-examples.*.*`` event.\n* Create a file containing ReST format fragment with the examples.\n  The file needs to be created in the ``examples/<service_name>``\n  directory and needs to be named ``<service_name>-<op_name>.rst``.\n  For example, ``examples/ec2/ec2-create-key-pair.rst``.\n\n\"\"\"\nimport os\nimport logging\n\n\nLOG = logging.getLogger(__name__)\n\n\ndef add_examples(help_command, **kwargs):\n    doc_path = os.path.join(\n        os.path.dirname(\n            os.path.dirname(\n                os.path.abspath(__file__))), 'examples')\n    doc_path = os.path.join(doc_path,\n                            help_command.event_class.replace('.', os.path.sep))\n    doc_path = doc_path + '.rst'\n    LOG.debug(\"Looking for example file at: %s\", doc_path)\n    if os.path.isfile(doc_path):\n        help_command.doc.style.h2('Examples')\n        help_command.doc.style.start_note()\n        msg = (\"<p>To use the following examples, you must have the AWS \"\n               \"CLI installed and configured. See the \"\n               \"<a href='https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-quickstart.html'>\"\n               \"Getting started guide</a> in the <i>AWS CLI User Guide</i> \"\n               \"for more information.</p>\"\n               \"<p>Unless otherwise stated, all examples have unix-like \"\n               \"quotation rules. These examples will need to be adapted \"\n               \"to your terminal's quoting rules. See \"\n               \"<a href='https://docs.aws.amazon.com/cli/v1/userguide/cli-usage-parameters-quoting-strings.html'>\"\n               \"Using quotation marks with strings</a> \"\n               \"in the <i>AWS CLI User Guide</i>.</p>\")\n        help_command.doc.include_doc_string(msg)\n        help_command.doc.style.end_note()\n        fp = open(doc_path)\n        for line in fp.readlines():\n            help_command.doc.write(line)\n", "awscli/customizations/dynamodb.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport binascii\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef register_dynamodb_paginator_fix(event_emitter):\n    DynamoDBPaginatorFix(event_emitter).register_events()\n\n\ndef parse_last_evaluated_key_binary(parsed, **kwargs):\n    # Because we disable parsing blobs into a binary type and leave them as\n    # a base64 string if a binary field is present in the continuation token\n    # as is the case with dynamodb the binary will be double encoded. This\n    # ensures that the continuation token is properly converted to binary to\n    # avoid double encoding the contination token.\n    last_evaluated_key = parsed.get('LastEvaluatedKey', None)\n    if last_evaluated_key is None:\n        return\n    for key, val in last_evaluated_key.items():\n        if 'B' in val:\n            val['B'] = base64.b64decode(val['B'])\n\n\nclass DynamoDBPaginatorFix(object):\n    def __init__(self, event_emitter):\n        self._event_emitter = event_emitter\n\n    def register_events(self):\n        self._event_emitter.register(\n            'calling-command.dynamodb.*', self._maybe_register_pagination_fix\n        )\n\n    def _maybe_register_pagination_fix(self, parsed_globals, **kwargs):\n        if parsed_globals.paginate:\n            self._event_emitter.register(\n                'after-call.dynamodb.*', parse_last_evaluated_key_binary\n            )\n", "awscli/customizations/awslambda.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport zipfile\nimport copy\nfrom contextlib import closing\n\nfrom awscli.arguments import CustomArgument, CLIArgument\nfrom awscli.compat import BytesIO\n\n\nERROR_MSG = (\n    \"--zip-file must be a zip file with the fileb:// prefix.\\n\"\n    \"Example usage:  --zip-file fileb://path/to/file.zip\")\n\nZIP_DOCSTRING = (\n    '<p>The path to the zip file of the {param_type} you are uploading. '\n    'Specify --zip-file or --{param_type}, but not both. '\n    'Example: fileb://{param_type}.zip</p>'\n)\n\n\ndef register_lambda_create_function(cli):\n    cli.register('building-argument-table.lambda.create-function',\n                 ZipFileArgumentHoister('Code').hoist)\n    cli.register('building-argument-table.lambda.publish-layer-version',\n                 ZipFileArgumentHoister('Content').hoist)\n    cli.register('building-argument-table.lambda.update-function-code',\n                 _modify_zipfile_docstring)\n    cli.register('process-cli-arg.lambda.update-function-code',\n                 validate_is_zip_file)\n\n\ndef validate_is_zip_file(cli_argument, value, **kwargs):\n    if cli_argument.name == 'zip-file':\n        _should_contain_zip_content(value)\n\n\nclass ZipFileArgumentHoister(object):\n    \"\"\"Hoists a ZipFile argument up to the top level.\n\n    Injects a top-level ZipFileArgument into the argument table which maps\n    a --zip-file parameter to the underlying ``serialized_name`` ZipFile\n    shape. Replaces the old ZipFile argument with an instance of\n    ReplacedZipFileArgument to prevent its usage and recommend the new\n    top-level injected parameter.\n    \"\"\"\n    def __init__(self, serialized_name):\n        self._serialized_name = serialized_name\n        self._name = serialized_name.lower()\n\n    def hoist(self, session, argument_table, **kwargs):\n        help_text = ZIP_DOCSTRING.format(param_type=self._name)\n        argument_table['zip-file'] = ZipFileArgument(\n            'zip-file', help_text=help_text, cli_type_name='blob',\n            serialized_name=self._serialized_name\n        )\n        argument = argument_table[self._name]\n        model = copy.deepcopy(argument.argument_model)\n        del model.members['ZipFile']\n        argument_table[self._name] = ReplacedZipFileArgument(\n            name=self._name,\n            argument_model=model,\n            operation_model=argument._operation_model,\n            is_required=False,\n            event_emitter=session.get_component('event_emitter'),\n            serialized_name=self._serialized_name,\n        )\n\n\ndef _modify_zipfile_docstring(session, argument_table, **kwargs):\n    if 'zip-file' in argument_table:\n        argument_table['zip-file'].documentation = ZIP_DOCSTRING\n\n\ndef _should_contain_zip_content(value):\n    if not isinstance(value, bytes):\n        # If it's not bytes it's basically impossible for\n        # this to be valid zip content, but we'll at least\n        # still try to load the contents as a zip file\n        # to be absolutely sure.\n        value = value.encode('utf-8')\n    fileobj = BytesIO(value)\n    try:\n        with closing(zipfile.ZipFile(fileobj)) as f:\n            f.infolist()\n    except zipfile.BadZipFile:\n        raise ValueError(ERROR_MSG)\n\n\nclass ZipFileArgument(CustomArgument):\n    \"\"\"A new ZipFile argument to be injected at the top level.\n\n    This class injects a ZipFile argument under the specified serialized_name\n    parameter. This can be used to take a top level parameter like --zip-file\n    and inject it into a nested different parameter like Code so\n    --zip-file foo.zip winds up being serialized as\n    { 'Code': { 'ZipFile': <contents of foo.zip> } }.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self._param_to_replace = kwargs.pop('serialized_name')\n        super(ZipFileArgument, self).__init__(*args, **kwargs)\n\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        _should_contain_zip_content(value)\n        zip_file_param = {'ZipFile': value}\n        if parameters.get(self._param_to_replace):\n            parameters[self._param_to_replace].update(zip_file_param)\n        else:\n            parameters[self._param_to_replace] = zip_file_param\n\n\nclass ReplacedZipFileArgument(CLIArgument):\n    \"\"\"A replacement arugment for nested ZipFile argument.\n\n    This prevents the use of a non-working nested argument that expects binary.\n    Instead an instance of ZipFileArgument should be injected at the top level\n    and used instead. That way fileb:// can be used to load the binary\n    contents. And the argument class can inject those bytes into the correct\n    serialization name.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(ReplacedZipFileArgument, self).__init__(*args, **kwargs)\n        self._cli_name = '--%s' % kwargs['name']\n        self._param_to_replace = kwargs['serialized_name']\n\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        unpacked = self._unpack_argument(value)\n        if 'ZipFile' in unpacked:\n            raise ValueError(\n                \"ZipFile cannot be provided \"\n                \"as part of the %s argument.  \"\n                \"Please use the '--zip-file' \"\n                \"option instead to specify a zip file.\" % self._cli_name)\n        if parameters.get(self._param_to_replace):\n            parameters[self._param_to_replace].update(unpacked)\n        else:\n            parameters[self._param_to_replace] = unpacked\n", "awscli/customizations/route53.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\ndef register_create_hosted_zone_doc_fix(cli):\n    # We can remove this customization once we begin documenting\n    # members of complex parameters because the member's docstring\n    # has the necessary documentation.\n    cli.register(\n        'doc-option.route53.create-hosted-zone.hosted-zone-config',\n        add_private_zone_note)\n\n\ndef add_private_zone_note(help_command, **kwargs):\n    note = (\n        '<p>Note do <b>not</b> include <code>PrivateZone</code> in this '\n        'input structure. Its value is returned in the output to the command.'\n        '</p>'\n    )\n    help_command.doc.include_doc_string(note)\n", "awscli/customizations/sessendemail.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nThis customization provides a simpler interface for the ``ses send-email``\ncommand.  This simplified form is based on the legacy CLI.  The simple format\nwill be::\n\naws ses send-email --subject SUBJECT --from FROM_EMAIL\n    --to-addresses addr ... --cc-addresses addr ...\n    --bcc-addresses addr ... --reply-to-addresses addr ...\n    --return-path addr --text TEXTBODY --html HTMLBODY\n\n\"\"\"\n\nfrom awscli.customizations import utils\nfrom awscli.arguments import CustomArgument\nfrom awscli.customizations.utils import validate_mutually_exclusive_handler\n\n\nTO_HELP = ('The email addresses of the primary recipients.  '\n           'You can specify multiple recipients as space-separated values')\nCC_HELP = ('The email addresses of copy recipients (Cc).  '\n           'You can specify multiple recipients as space-separated values')\nBCC_HELP = ('The email addresses of blind-carbon-copy recipients (Bcc).  '\n            'You can specify multiple recipients as space-separated values')\nSUBJECT_HELP = 'The subject of the message'\nTEXT_HELP = 'The raw text body of the message'\nHTML_HELP = 'The HTML body of the message'\n\n\ndef register_ses_send_email(event_handler):\n    event_handler.register('building-argument-table.ses.send-email',\n                           _promote_args)\n    event_handler.register(\n        'operation-args-parsed.ses.send-email',\n        validate_mutually_exclusive_handler(\n            ['destination'], ['to', 'cc', 'bcc']))\n    event_handler.register(\n        'operation-args-parsed.ses.send-email',\n        validate_mutually_exclusive_handler(\n            ['message'], ['text', 'html']))\n\n\ndef _promote_args(argument_table, **kwargs):\n    argument_table['message'].required = False\n    argument_table['destination'].required = False\n    utils.rename_argument(argument_table, 'source',\n                          new_name='from')\n    argument_table['to'] = AddressesArgument(\n        'to', 'ToAddresses', help_text=TO_HELP)\n    argument_table['cc'] = AddressesArgument(\n        'cc', 'CcAddresses', help_text=CC_HELP)\n    argument_table['bcc'] = AddressesArgument(\n        'bcc', 'BccAddresses', help_text=BCC_HELP)\n    argument_table['subject'] = BodyArgument(\n        'subject', 'Subject', help_text=SUBJECT_HELP)\n    argument_table['text'] = BodyArgument(\n        'text', 'Text', help_text=TEXT_HELP)\n    argument_table['html'] = BodyArgument(\n        'html', 'Html', help_text=HTML_HELP)\n\n\ndef _build_destination(params, key, value):\n    # Build up the Destination data structure\n    if 'Destination' not in params:\n        params['Destination'] = {}\n    params['Destination'][key] = value\n\n\ndef _build_message(params, key, value):\n    # Build up the Message data structure\n    if 'Message' not in params:\n        params['Message'] = {'Subject': {}, 'Body': {}}\n    if key in ('Text', 'Html'):\n        params['Message']['Body'][key] = {'Data': value}\n    elif key == 'Subject':\n        params['Message']['Subject'] = {'Data': value}\n\n\nclass AddressesArgument(CustomArgument):\n\n    def __init__(self, name, json_key, help_text='', dest=None, default=None,\n                 action=None, required=None, choices=None, cli_type_name=None):\n        super(AddressesArgument, self).__init__(name=name, help_text=help_text,\n                                                required=required, nargs='+')\n        self._json_key = json_key\n\n    def add_to_params(self, parameters, value):\n        if value:\n            _build_destination(parameters, self._json_key, value)\n\n\nclass BodyArgument(CustomArgument):\n\n    def __init__(self, name, json_key, help_text='', required=None):\n        super(BodyArgument, self).__init__(name=name, help_text=help_text,\n                                           required=required)\n        self._json_key = json_key\n\n    def add_to_params(self, parameters, value):\n        if value:\n            _build_message(parameters, self._json_key, value)\n\n", "awscli/customizations/binaryhoist.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\n\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom awscli.arguments import CustomArgument, CLIArgument\n\n\n@dataclass\nclass ArgumentParameters:\n    name: str\n    member: Optional[str] = None\n    help_text: Optional[str] = None\n    required: Optional[bool] = False\n\n\nclass InjectingArgument(CustomArgument):\n    def __init__(self, serialized_name, original_member_name, **kwargs):\n        self._serialized_name = serialized_name\n        self._original_member_name = original_member_name\n        super().__init__(**kwargs)\n\n    def add_to_params(self, parameters, value):\n        if value is None:\n            pass\n        wrapped_value = {self._original_member_name: value}\n        if parameters.get(self._serialized_name):\n            parameters[self._serialized_name].update(wrapped_value)\n        else:\n            parameters[self._serialized_name] = wrapped_value\n\n\nclass OriginalArgument(CLIArgument):\n    def __init__(self, original_member_name, error_message, **kwargs):\n        self._serialized_name = kwargs.get(\"serialized_name\")\n        self._original_member_name = original_member_name\n        self._error_message = error_message\n        super().__init__(**kwargs)\n\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n\n        unpacked = self._unpack_argument(value)\n        if self._original_member_name in unpacked and self._error_message:\n            raise ValueError(self._error_message)\n\n        if parameters.get(self._serialized_name):\n            parameters[self._serialized_name].update(unpacked)\n        else:\n            parameters[self._serialized_name] = unpacked\n\n\nclass BinaryBlobArgumentHoister:\n    def __init__(\n        self,\n        new_argument: ArgumentParameters,\n        original_argument: ArgumentParameters,\n        error_if_original_used: Optional[str] = None,\n    ):\n        self._new_argument = new_argument\n        self._original_argument = original_argument\n        self._error_message = error_if_original_used\n\n    def __call__(self, session, argument_table, **kwargs):\n        argument = argument_table[self._original_argument.name]\n        model = copy.deepcopy(argument.argument_model)\n        del model.members[self._original_argument.member]\n\n        argument_table[self._new_argument.name] = InjectingArgument(\n            argument._serialized_name,\n            self._original_argument.member,\n            name=self._new_argument.name,\n            help_text=self._new_argument.help_text,\n            cli_type_name=\"blob\",\n            required=self._new_argument.required,\n        )\n        argument_table[self._original_argument.name] = OriginalArgument(\n            self._original_argument.member,\n            self._error_message,\n            name=self._original_argument.name,\n            argument_model=model,\n            operation_model=argument._operation_model,\n            is_required=self._original_argument.required,\n            event_emitter=session.get_component(\"event_emitter\"),\n            serialized_name=argument._serialized_name,\n        )\n", "awscli/customizations/cliinputjson.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\n\nfrom awscli.paramfile import get_paramfile, LOCAL_PREFIX_MAP\nfrom awscli.argprocess import ParamError\nfrom awscli.customizations.arguments import OverrideRequiredArgsArgument\n\n\ndef register_cli_input_json(cli):\n    cli.register('building-argument-table', add_cli_input_json)\n\n\ndef add_cli_input_json(session, argument_table, **kwargs):\n    # This argument cannot support operations with streaming output which\n    # is designated by the argument name `outfile`.\n    if 'outfile' not in argument_table:\n        cli_input_json_argument = CliInputJSONArgument(session)\n        cli_input_json_argument.add_to_arg_table(argument_table)\n\n\nclass CliInputJSONArgument(OverrideRequiredArgsArgument):\n    \"\"\"This argument inputs a JSON string as the entire input for a command.\n\n    Ideally, the value to this argument should be a filled out JSON file\n    generated by ``--generate-cli-skeleton``. The items in the JSON string\n    will not clobber other arguments entered into the command line.\n    \"\"\"\n    ARG_DATA = {\n        'name': 'cli-input-json',\n        'help_text': 'Performs service operation based on the JSON string '\n                     'provided. The JSON string follows the format provided '\n                     'by ``--generate-cli-skeleton``. If other arguments are '\n                     'provided on the command line, the CLI values will override '\n                     'the JSON-provided values. It is not possible to pass '\n                     'arbitrary binary values using a JSON-provided value as '\n                     'the string will be taken literally.'\n    }\n\n    def __init__(self, session):\n        super(CliInputJSONArgument, self).__init__(session)\n\n    def _register_argument_action(self):\n        self._session.register(\n            'calling-command.*', self.add_to_call_parameters)\n        super(CliInputJSONArgument, self)._register_argument_action()\n\n    def add_to_call_parameters(self, call_parameters, parsed_args,\n                               parsed_globals, **kwargs):\n\n        # Check if ``--cli-input-json`` was specified in the command line.\n        input_json = getattr(parsed_args, 'cli_input_json', None)\n        if input_json is not None:\n            # Retrieve the JSON from the file if needed.\n            retrieved_json = get_paramfile(input_json, LOCAL_PREFIX_MAP)\n            # Nothing was retrieved from the file. So assume the argument\n            # is already a JSON string.\n            if retrieved_json is None:\n                retrieved_json = input_json\n            try:\n                # Try to load the JSON string into a python dictionary\n                input_data = json.loads(retrieved_json)\n            except ValueError as e:\n                raise ParamError(\n                    self.name, \"Invalid JSON: %s\\nJSON received: %s\"\n                    % (e, retrieved_json))\n            # Add the members from the input JSON to the call parameters.\n            self._update_call_parameters(call_parameters, input_data)\n\n    def _update_call_parameters(self, call_parameters, input_data):\n        for input_key in input_data.keys():\n            # Only add the values to ``call_parameters`` if not already\n            # present.\n            if input_key not in call_parameters:\n                call_parameters[input_key] = input_data[input_key]\n", "awscli/customizations/kms.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\ndef register_fix_kms_create_grant_docs(cli):\n    # Docs may actually refer to actual api name (not the CLI command).\n    # In that case we want to remove the translation map.\n    cli.register('doc-title.kms.create-grant', remove_translation_map)\n\n\ndef remove_translation_map(help_command, **kwargs):\n    help_command.doc.translation_map = {}\n", "awscli/customizations/translate.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\n\nfrom awscli.arguments import CustomArgument, CLIArgument\nfrom awscli.customizations.binaryhoist import (\n    BinaryBlobArgumentHoister,\n    ArgumentParameters,\n)\n\nFILE_DOCSTRING = (\n    \"<p>The path to the file of the code you are uploading. \"\n    \"Example: fileb://data.csv</p>\"\n)\nFILE_ERRORSTRING = (\n    \"File cannot be provided as part of the \"\n    \"'--terminology-data' argument. Please use the \"\n    \"'--data-file' option instead to specify a \"\n    \"file.\"\n)\n\nDOCUMENT_DOCSTRING = (\n    \"<p>The path to a file of the content you are uploading \"\n    \"Example: fileb://data.txt</p>\"\n)\nDOCUMENT_ERRORSTRING = (\n    \"Content cannot be provided as a part of the \"\n    \"'--document' argument. Please use the '--document-content' option instead \"\n    \"to specify a file.\"\n)\n\n\ndef register_translate_import_terminology(cli):\n    cli.register(\n        \"building-argument-table.translate.import-terminology\",\n        BinaryBlobArgumentHoister(\n            new_argument=ArgumentParameters(\n                name=\"data-file\",\n                help_text=FILE_DOCSTRING,\n                required=True,\n            ),\n            original_argument=ArgumentParameters(\n                name=\"terminology-data\",\n                member=\"File\",\n                required=False,\n            ),\n            error_if_original_used=FILE_ERRORSTRING,\n        ),\n    ),\n\n    cli.register(\n        \"building-argument-table.translate.translate-document\",\n        BinaryBlobArgumentHoister(\n            new_argument=ArgumentParameters(\n                name=\"document-content\",\n                help_text=DOCUMENT_DOCSTRING,\n                required=True,\n            ),\n            original_argument=ArgumentParameters(\n                name=\"document\",\n                member=\"Content\",\n                required=True,\n            ),\n            error_if_original_used=DOCUMENT_ERRORSTRING,\n        ),\n    )\n", "awscli/customizations/mturk.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations.utils import make_hidden_command_alias\n\n\ndef register_alias_mturk_command(event_emitter):\n    event_emitter.register(\n        'building-command-table.mturk',\n        alias_mturk_command\n    )\n\n\ndef alias_mturk_command(command_table, **kwargs):\n    make_hidden_command_alias(\n        command_table,\n        existing_name='list-hits-for-qualification-type',\n        alias_name='list-hi-ts-for-qualification-type',\n    )\n", "awscli/customizations/quicksight.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.arguments import NestedBlobArgumentHoister\n\n_ASSET_BUNDLE_FILE_DOCSTRING = (\n    '<p>The content of the asset bundle to be uploaded. '\n    'To specify the content of a local file use the '\n    'fileb:// prefix. Example: fileb://asset-bundle.zip</p>')\n\n_ASSET_BUNDLE_DOCSTRING_ADDENDUM = (\n    '<p>To specify a local file use '\n    '<code>--asset-bundle-import-source-bytes</code> instead.</p>')\n\n\ndef register_quicksight_asset_bundle_customizations(cli):\n    cli.register(\n        'building-argument-table.quicksight.start-asset-bundle-import-job',\n        NestedBlobArgumentHoister(\n            source_arg='asset-bundle-import-source',\n            source_arg_blob_member='Body',\n            new_arg='asset-bundle-import-source-bytes',\n            new_arg_doc_string=_ASSET_BUNDLE_FILE_DOCSTRING,\n            doc_string_addendum=_ASSET_BUNDLE_DOCSTRING_ADDENDUM))\n", "awscli/customizations/assumerole.py": "import os\nimport logging\n\nfrom botocore.exceptions import ProfileNotFound\nfrom botocore.credentials import JSONFileCache\n\nLOG = logging.getLogger(__name__)\nCACHE_DIR = os.path.expanduser(os.path.join('~', '.aws', 'cli', 'cache'))\n\n\ndef register_assume_role_provider(event_handlers):\n    event_handlers.register('session-initialized',\n                            inject_assume_role_provider_cache,\n                            unique_id='inject_assume_role_cred_provider_cache')\n\n\ndef inject_assume_role_provider_cache(session, **kwargs):\n    try:\n        cred_chain = session.get_component('credential_provider')\n    except ProfileNotFound:\n        # If a user has provided a profile that does not exist,\n        # trying to retrieve components/config on the session\n        # will raise ProfileNotFound.  Sometimes this is invalid:\n        #\n        # \"ec2 describe-instances --profile unknown\"\n        #\n        # and sometimes this is perfectly valid:\n        #\n        # \"configure set region us-west-2 --profile brand-new-profile\"\n        #\n        # Because we can't know (and don't want to know) whether\n        # the customer is trying to do something valid, we just\n        # immediately return.  If it's invalid something else\n        # up the stack will raise ProfileNotFound, otherwise\n        # the configure (and other) commands will work as expected.\n        LOG.debug(\"ProfileNotFound caught when trying to inject \"\n                  \"assume-role cred provider cache.  Not configuring \"\n                  \"JSONFileCache for assume-role.\")\n        return\n    assume_role_provider = cred_chain.get_provider('assume-role')\n    assume_role_provider.cache = JSONFileCache(CACHE_DIR)\n    web_identity_provider = cred_chain.get_provider(\n        'assume-role-with-web-identity'\n    )\n    web_identity_provider.cache = JSONFileCache(CACHE_DIR)\n", "awscli/customizations/overridesslcommonname.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nSSL_COMMON_NAMES = {\n    \"sqs\": {\n        \"af-south-1\": \"af-south-1.queue.amazonaws.com\",\n        \"ap-east-1\": \"ap-east-1.queue.amazonaws.com\",\n        \"ap-northeast-1\": \"ap-northeast-1.queue.amazonaws.com\",\n        \"ap-northeast-2\": \"ap-northeast-2.queue.amazonaws.com\",\n        \"ap-northeast-3\": \"ap-northeast-3.queue.amazonaws.com\",\n        \"ap-south-1\": \"ap-south-1.queue.amazonaws.com\",\n        \"ap-southeast-1\": \"ap-southeast-1.queue.amazonaws.com\",\n        \"ap-southeast-2\": \"ap-southeast-2.queue.amazonaws.com\",\n        \"ap-southeast-3\": \"ap-southeast-3.queue.amazonaws.com\",\n        \"ca-central-1\": \"ca-central-1.queue.amazonaws.com\",\n        \"eu-central-1\": \"eu-central-1.queue.amazonaws.com\",\n        \"eu-north-1\": \"eu-north-1.queue.amazonaws.com\",\n        \"eu-south-1\": \"eu-south-1.queue.amazonaws.com\",\n        \"eu-west-1\": \"eu-west-1.queue.amazonaws.com\",\n        \"eu-west-2\": \"eu-west-2.queue.amazonaws.com\",\n        \"eu-west-3\": \"eu-west-3.queue.amazonaws.com\",\n        \"me-south-1\": \"me-south-1.queue.amazonaws.com\",\n        \"sa-east-1\": \"sa-east-1.queue.amazonaws.com\",\n        \"us-east-1\": \"queue.amazonaws.com\",\n        \"us-east-2\": \"us-east-2.queue.amazonaws.com\",\n        \"us-west-1\": \"us-west-1.queue.amazonaws.com\",\n        \"us-west-2\": \"us-west-2.queue.amazonaws.com\",\n        \"cn-north-1\": \"cn-north-1.queue.amazonaws.com.cn\",\n        \"cn-northwest-1\": \"cn-northwest-1.queue.amazonaws.com.cn\",\n        \"us-gov-west-1\": \"us-gov-west-1.queue.amazonaws.com\",\n        \"us-isob-east-1\": \"us-isob-east-1.queue.sc2s.sgov.gov\",\n    },\n    \"emr\": {\n        \"af-south-1\": \"af-south-1.elasticmapreduce.amazonaws.com\",\n        \"ap-east-1\": \"ap-east-1.elasticmapreduce.amazonaws.com\",\n        \"ap-northeast-1\": \"ap-northeast-1.elasticmapreduce.amazonaws.com\",\n        \"ap-northeast-2\": \"ap-northeast-2.elasticmapreduce.amazonaws.com\",\n        \"ap-northeast-3\": \"ap-northeast-3.elasticmapreduce.amazonaws.com\",\n        \"ap-south-1\": \"ap-south-1.elasticmapreduce.amazonaws.com\",\n        \"ap-southeast-1\": \"ap-southeast-1.elasticmapreduce.amazonaws.com\",\n        \"ap-southeast-2\": \"ap-southeast-2.elasticmapreduce.amazonaws.com\",\n        \"ap-southeast-3\": \"ap-southeast-3.elasticmapreduce.amazonaws.com\",\n        \"ca-central-1\": \"ca-central-1.elasticmapreduce.amazonaws.com\",\n        \"eu-north-1\": \"eu-north-1.elasticmapreduce.amazonaws.com\",\n        \"eu-south-1\": \"eu-south-1.elasticmapreduce.amazonaws.com\",\n        \"eu-west-1\": \"eu-west-1.elasticmapreduce.amazonaws.com\",\n        \"eu-west-2\": \"eu-west-2.elasticmapreduce.amazonaws.com\",\n        \"eu-west-3\": \"eu-west-3.elasticmapreduce.amazonaws.com\",\n        \"me-south-1\": \"me-south-1.elasticmapreduce.amazonaws.com\",\n        \"sa-east-1\": \"sa-east-1.elasticmapreduce.amazonaws.com\",\n        \"us-east-2\": \"us-east-2.elasticmapreduce.amazonaws.com\",\n        \"us-west-1\": \"us-west-1.elasticmapreduce.amazonaws.com\",\n        \"us-west-2\": \"us-west-2.elasticmapreduce.amazonaws.com\",\n    },\n    \"rds\": {\n        \"us-east-1\": \"rds.amazonaws.com\",\n    },\n    \"docdb\": {\n        \"us-east-1\": \"rds.amazonaws.com\",\n    },\n    \"neptune\": {\n        \"us-east-1\": \"rds.amazonaws.com\",\n    },\n    \"health\": {\n        \"aws-global\": \"health.us-east-1.amazonaws.com\",\n        \"af-south-1\": \"health.us-east-1.amazonaws.com\",\n        \"ap-east-1\": \"health.us-east-1.amazonaws.com\",\n        \"ap-northeast-1\": \"health.us-east-1.amazonaws.com\",\n        \"ap-northeast-2\": \"health.us-east-1.amazonaws.com\",\n        \"ap-northeast-3\": \"health.us-east-1.amazonaws.com\",\n        \"ap-south-1\": \"health.us-east-1.amazonaws.com\",\n        \"ap-southeast-1\": \"health.us-east-1.amazonaws.com\",\n        \"ap-southeast-2\": \"health.us-east-1.amazonaws.com\",\n        \"ap-southeast-3\": \"health.us-east-1.amazonaws.com\",\n        \"ca-central-1\": \"health.us-east-1.amazonaws.com\",\n        \"eu-central-1\": \"health.us-east-1.amazonaws.com\",\n        \"eu-north-1\": \"health.us-east-1.amazonaws.com\",\n        \"eu-south-1\": \"health.us-east-1.amazonaws.com\",\n        \"eu-west-1\": \"health.us-east-1.amazonaws.com\",\n        \"eu-west-2\": \"health.us-east-1.amazonaws.com\",\n        \"eu-west-3\": \"health.us-east-1.amazonaws.com\",\n        \"me-south-1\": \"health.us-east-1.amazonaws.com\",\n        \"sa-east-1\": \"health.us-east-1.amazonaws.com\",\n        \"us-east-1\": \"health.us-east-1.amazonaws.com\",\n        \"us-east-2\": \"health.us-east-1.amazonaws.com\",\n        \"us-west-1\": \"health.us-east-1.amazonaws.com\",\n        \"us-west-2\": \"health.us-east-1.amazonaws.com\",\n        \"cn-north-1\": \"health.cn-northwest-1.amazonaws.com.cn\",\n        \"cn-northwest-1\": \"health.cn-northwest-1.amazonaws.com.cn\",\n        \"aws-cn-global\": \"health.cn-northwest-1.amazonaws.com.cn\",\n    },\n}\n\nREGION_TO_PARTITION_OVERRIDE = {\n    \"aws-global\": \"aws\",\n    \"aws-cn-global\": \"aws-cn\",\n}\n\n\ndef register_override_ssl_common_name(cli):\n    cli.register_last(\n        \"before-building-argument-table-parser\", update_endpoint_url\n    )\n\n\ndef update_endpoint_url(session, parsed_globals, **kwargs):\n    service = parsed_globals.command\n    endpoints = SSL_COMMON_NAMES.get(service)\n    # only change url if user has not overridden already themselves\n    if endpoints is not None and parsed_globals.endpoint_url is None:\n        region = session.get_config_variable(\"region\")\n        endpoint_url = endpoints.get(region)\n        if endpoint_url is not None:\n            parsed_globals.endpoint_url = f\"https://{endpoint_url}\"\n            if service == \"health\":\n                _override_health_region(region, session, parsed_globals)\n\n\ndef _override_health_region(region, session, parsed_globals):\n    if region in REGION_TO_PARTITION_OVERRIDE:\n        partition = REGION_TO_PARTITION_OVERRIDE[region]\n    else:\n        partition = session.get_partition_for_region(region)\n    if partition == \"aws-cn\":\n        parsed_globals.region = \"cn-northwest-1\"\n    else:\n        parsed_globals.region = \"us-east-1\"\n", "awscli/customizations/argrename.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\n\"\"\"\n\nfrom awscli.customizations import utils\n\n\nARGUMENT_RENAMES = {\n    # Mapping of original arg to renamed arg.\n    # The key is <service>.<operation>.argname\n    # The first part of the key is used for event registration\n    # so if you wanted to rename something for an entire service you\n    # could say 'ec2.*.dry-run': 'renamed-arg-name', or if you wanted\n    # to rename across all services you could say '*.*.dry-run': 'new-name'.\n    'ec2.create-image.no-no-reboot': 'reboot',\n    'ec2.*.no-egress': 'ingress',\n    'ec2.*.no-disable-api-termination': 'enable-api-termination',\n    'opsworks.*.region': 'stack-region',\n    'elastictranscoder.*.output': 'job-output',\n    'swf.register-activity-type.version': 'activity-version',\n    'swf.register-workflow-type.version': 'workflow-version',\n    'datapipeline.*.query': 'objects-query',\n    'datapipeline.get-pipeline-definition.version': 'pipeline-version',\n    'emr.*.job-flow-ids': 'cluster-ids',\n    'emr.*.job-flow-id': 'cluster-id',\n    'cloudsearchdomain.search.query': 'search-query',\n    'cloudsearchdomain.suggest.query': 'suggest-query',\n    'sns.subscribe.endpoint': 'notification-endpoint',\n    'deploy.*.s-3-location': 's3-location',\n    'deploy.*.ec-2-tag-filters': 'ec2-tag-filters',\n    'codepipeline.get-pipeline.version': 'pipeline-version',\n    'codepipeline.create-custom-action-type.version': 'action-version',\n    'codepipeline.delete-custom-action-type.version': 'action-version',\n    'kinesisanalytics.add-application-output.output': 'application-output',\n    'kinesisanalyticsv2.add-application-output.output': 'application-output',\n    'route53.delete-traffic-policy.version': 'traffic-policy-version',\n    'route53.get-traffic-policy.version': 'traffic-policy-version',\n    'route53.update-traffic-policy-comment.version': 'traffic-policy-version',\n    'gamelift.create-build.version': 'build-version',\n    'gamelift.update-build.version': 'build-version',\n    'gamelift.create-script.version': 'script-version',\n    'gamelift.update-script.version': 'script-version',\n    'route53domains.view-billing.start': 'start-time',\n    'route53domains.view-billing.end': 'end-time',\n    'apigateway.create-rest-api.version': 'api-version',\n    'apigatewayv2.create-api.version': 'api-version',\n    'apigatewayv2.update-api.version': 'api-version',\n    'pinpoint.get-campaign-version.version': 'campaign-version',\n    'pinpoint.get-segment-version.version': 'segment-version',\n    'pinpoint.delete-email-template.version': 'template-version',\n    'pinpoint.delete-in-app-template.version': 'template-version',\n    'pinpoint.delete-push-template.version': 'template-version',\n    'pinpoint.delete-sms-template.version': 'template-version',\n    'pinpoint.delete-voice-template.version': 'template-version',\n    'pinpoint.get-email-template.version': 'template-version',\n    'pinpoint.get-in-app-template.version': 'template-version',\n    'pinpoint.get-push-template.version': 'template-version',\n    'pinpoint.get-sms-template.version': 'template-version',\n    'pinpoint.get-voice-template.version': 'template-version',\n    'pinpoint.update-email-template.version': 'template-version',\n    'pinpoint.update-in-app-template.version': 'template-version',\n    'pinpoint.update-push-template.version': 'template-version',\n    'pinpoint.update-sms-template.version': 'template-version',\n    'pinpoint.update-voice-template.version': 'template-version',\n    'stepfunctions.send-task-success.output': 'task-output',\n    'clouddirectory.publish-schema.version': 'schema-version',\n    'mturk.list-qualification-types.query': 'types-query',\n    'workdocs.create-notification-subscription.endpoint':\n        'notification-endpoint',\n    'workdocs.describe-users.query': 'user-query',\n    'lex-models.delete-bot.version': 'bot-version',\n    'lex-models.delete-intent.version': 'intent-version',\n    'lex-models.delete-slot-type.version': 'slot-type-version',\n    'lex-models.get-intent.version': 'intent-version',\n    'lex-models.get-slot-type.version': 'slot-type-version',\n    'lex-models.delete-bot-version.version': 'bot-version',\n    'lex-models.delete-intent-version.version': 'intent-version',\n    'lex-models.delete-slot-type-version.version': 'slot-type-version',\n    'lex-models.get-export.version': 'resource-version',\n    'license-manager.get-grant.version': 'grant-version',\n    'license-manager.delete-grant.version': 'grant-version',\n    'license-manager.get-license.version': 'license-version',\n    'mobile.create-project.region': 'project-region',\n    'rekognition.create-stream-processor.output': 'stream-processor-output',\n    'eks.create-cluster.version': 'kubernetes-version',\n    'eks.update-cluster-version.version': 'kubernetes-version',\n    'eks.create-nodegroup.version': 'kubernetes-version',\n    'eks.update-nodegroup-version.version': 'kubernetes-version',\n    'schemas.*.version': 'schema-version',\n    'sagemaker.delete-image-version.version': 'version-number',\n    'sagemaker.describe-image-version.version': 'version-number',\n    'sagemaker.list-aliases.version': 'version-number',\n    'sagemaker.update-image-version.version': 'version-number',\n    'iotwireless.*.lo-ra-wan': 'lorawan',\n    'codepipeline.get-action-type.version': 'action-version',\n    'ecs.*.no-enable-execute-command': 'disable-execute-command',\n    'ecs.execute-command.no-interactive': 'non-interactive',\n    'controltower.create-landing-zone.version': 'landing-zone-version',\n    'controltower.update-landing-zone.version': 'landing-zone-version',\n    'glue.get-unfiltered-partition-metadata.region': 'resource-region',\n    'glue.get-unfiltered-partitions-metadata.region': 'resource-region',\n    'glue.get-unfiltered-table-metadata.region': 'resource-region',\n}\n\n# Same format as ARGUMENT_RENAMES, but instead of renaming the arguments,\n# an alias is created to the original argument and marked as undocumented.\n# This is useful when you need to change the name of an argument but you\n# still need to support the old argument.\nHIDDEN_ALIASES = {\n    'cognito-identity.create-identity-pool.open-id-connect-provider-arns':\n        'open-id-connect-provider-ar-ns',\n    'storagegateway.describe-tapes.tape-arns': 'tape-ar-ns',\n    'storagegateway.describe-tape-archives.tape-arns': 'tape-ar-ns',\n    'storagegateway.describe-vtl-devices.vtl-device-arns': 'vtl-device-ar-ns',\n    'storagegateway.describe-cached-iscsi-volumes.volume-arns': 'volume-ar-ns',\n    'storagegateway.describe-stored-iscsi-volumes.volume-arns': 'volume-ar-ns',\n    'route53domains.view-billing.start-time': 'start',\n    # These come from the xform_name() changes that no longer separates words\n    # by numbers.\n    'deploy.create-deployment-group.ec2-tag-set': 'ec-2-tag-set',\n    'deploy.list-application-revisions.s3-bucket': 's-3-bucket',\n    'deploy.list-application-revisions.s3-key-prefix': 's-3-key-prefix',\n    'deploy.update-deployment-group.ec2-tag-set': 'ec-2-tag-set',\n    'iam.enable-mfa-device.authentication-code1': 'authentication-code-1',\n    'iam.enable-mfa-device.authentication-code2': 'authentication-code-2',\n    'iam.resync-mfa-device.authentication-code1': 'authentication-code-1',\n    'iam.resync-mfa-device.authentication-code2': 'authentication-code-2',\n    'importexport.get-shipping-label.street1': 'street-1',\n    'importexport.get-shipping-label.street2': 'street-2',\n    'importexport.get-shipping-label.street3': 'street-3',\n    'lambda.publish-version.code-sha256': 'code-sha-256',\n    'lightsail.import-key-pair.public-key-base64': 'public-key-base-64',\n    'opsworks.register-volume.ec2-volume-id': 'ec-2-volume-id',\n    'mgn.*.replication-servers-security-groups-ids':\n        'replication-servers-security-groups-i-ds',\n    'mgn.*.source-server-ids': 'source-server-i-ds',\n    'mgn.*.replication-configuration-template-ids':\n        'replication-configuration-template-i-ds',\n    'elasticache.create-replication-group.preferred-cache-cluster-azs':\n        'preferred-cache-cluster-a-zs'\n}\n\n\ndef register_arg_renames(cli):\n    for original, new_name in ARGUMENT_RENAMES.items():\n        event_portion, original_arg_name = original.rsplit('.', 1)\n        cli.register('building-argument-table.%s' % event_portion,\n                     rename_arg(original_arg_name, new_name))\n    for original, new_name in HIDDEN_ALIASES.items():\n        event_portion, original_arg_name = original.rsplit('.', 1)\n        cli.register('building-argument-table.%s' % event_portion,\n                     hidden_alias(original_arg_name, new_name))\n\n\ndef rename_arg(original_arg_name, new_name):\n    def _rename_arg(argument_table, **kwargs):\n        if original_arg_name in argument_table:\n            utils.rename_argument(argument_table, original_arg_name, new_name)\n    return _rename_arg\n\n\ndef hidden_alias(original_arg_name, alias_name):\n    def _alias_arg(argument_table, **kwargs):\n        if original_arg_name in argument_table:\n            utils.make_hidden_alias(argument_table, original_arg_name, alias_name)\n    return _alias_arg\n", "awscli/customizations/cloudsearchdomain.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Customizations for the cloudsearchdomain command.\n\nThis module customizes the cloudsearchdomain command:\n\n    * Add validation that --endpoint-url is required.\n\n\"\"\"\n\ndef register_cloudsearchdomain(cli):\n    cli.register_last('calling-command.cloudsearchdomain',\n                      validate_endpoint_url)\n\n\ndef validate_endpoint_url(parsed_globals, **kwargs):\n    if parsed_globals.endpoint_url is None:\n        return ValueError(\n            \"--endpoint-url is required for cloudsearchdomain commands\")\n", "awscli/customizations/cloudfront.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport sys\nimport time\nimport random\n\nimport rsa\nfrom botocore.utils import parse_to_aware_datetime\nfrom botocore.signers import CloudFrontSigner\n\nfrom awscli.arguments import CustomArgument\nfrom awscli.customizations.utils import validate_mutually_exclusive_handler\nfrom awscli.customizations.commands import BasicCommand\n\n\ndef register(event_handler):\n    event_handler.register('building-command-table.cloudfront', _add_sign)\n\n    # Provides a simpler --paths for ``aws cloudfront create-invalidation``\n    event_handler.register(\n        'building-argument-table.cloudfront.create-invalidation', _add_paths)\n    event_handler.register(\n        'operation-args-parsed.cloudfront.create-invalidation',\n        validate_mutually_exclusive_handler(['invalidation_batch'], ['paths']))\n\n    event_handler.register(\n        'operation-args-parsed.cloudfront.create-distribution',\n        validate_mutually_exclusive_handler(\n            ['default_root_object', 'origin_domain_name'],\n            ['distribution_config']))\n    event_handler.register(\n        'building-argument-table.cloudfront.create-distribution',\n        lambda argument_table, **kwargs: argument_table.__setitem__(\n            'origin-domain-name', OriginDomainName(argument_table)))\n    event_handler.register(\n        'building-argument-table.cloudfront.create-distribution',\n        lambda argument_table, **kwargs: argument_table.__setitem__(\n            'default-root-object', CreateDefaultRootObject(argument_table)))\n\n    context = {}\n    event_handler.register(\n        'top-level-args-parsed', context.update, unique_id='cloudfront')\n    event_handler.register(\n        'operation-args-parsed.cloudfront.update-distribution',\n        validate_mutually_exclusive_handler(\n            ['default_root_object'], ['distribution_config']))\n    event_handler.register(\n        'building-argument-table.cloudfront.update-distribution',\n        lambda argument_table, **kwargs: argument_table.__setitem__(\n            'default-root-object', UpdateDefaultRootObject(\n                context=context, argument_table=argument_table)))\n\n\ndef unique_string(prefix='cli'):\n    return '%s-%s-%s' % (prefix, int(time.time()), random.randint(1, 1000000))\n\n\ndef _add_paths(argument_table, **kwargs):\n    argument_table['invalidation-batch'].required = False\n    argument_table['paths'] = PathsArgument()\n\n\nclass PathsArgument(CustomArgument):\n\n    def __init__(self):\n        doc = (\n            'The space-separated paths to be invalidated.'\n            ' Note: --invalidation-batch and --paths are mutually exclusive.'\n        )\n        super(PathsArgument, self).__init__('paths', nargs='+', help_text=doc)\n\n    def add_to_params(self, parameters, value):\n        if value is not None:\n            parameters['InvalidationBatch'] = {\n                \"CallerReference\": unique_string(),\n                \"Paths\": {\"Quantity\": len(value), \"Items\": value},\n                }\n\n\nclass ExclusiveArgument(CustomArgument):\n    DOC = '%s This argument and --%s are mutually exclusive.'\n\n    def __init__(self, name, argument_table,\n                 exclusive_to='distribution-config', help_text=''):\n        argument_table[exclusive_to].required = False\n        super(ExclusiveArgument, self).__init__(\n            name, help_text=self.DOC % (help_text, exclusive_to))\n\n    def distribution_config_template(self):\n        return {\n            \"CallerReference\": unique_string(),\n            \"Origins\": {\"Quantity\": 0, \"Items\": []},\n            \"DefaultCacheBehavior\": {\n                \"TargetOriginId\": \"placeholder\",\n                \"ForwardedValues\": {\n                    \"QueryString\": False,\n                    \"Cookies\": {\"Forward\": \"none\"},\n                },\n                \"TrustedSigners\": {\n                    \"Enabled\": False,\n                    \"Quantity\": 0\n                },\n                \"ViewerProtocolPolicy\": \"allow-all\",\n                \"MinTTL\": 0\n            },\n            \"Enabled\": True,\n            \"Comment\": \"\",\n        }\n\n\nclass OriginDomainName(ExclusiveArgument):\n    def __init__(self, argument_table):\n        super(OriginDomainName, self).__init__(\n            'origin-domain-name', argument_table,\n            help_text='The domain name for your origin.')\n\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        parameters.setdefault(\n            'DistributionConfig', self.distribution_config_template())\n        origin_id = unique_string(prefix=value)\n        item = {\"Id\": origin_id, \"DomainName\": value, \"OriginPath\": ''}\n        if item['DomainName'].endswith('.s3.amazonaws.com'):\n            # We do not need to detect '.s3[\\w-].amazonaws.com' as S3 buckets,\n            # because CloudFront treats GovCloud S3 buckets as custom domain.\n            # http://docs.aws.amazon.com/govcloud-us/latest/UserGuide/setting-up-cloudfront.html\n            item[\"S3OriginConfig\"] = {\"OriginAccessIdentity\": \"\"}\n        else:\n            item[\"CustomOriginConfig\"] = {\n                'HTTPPort': 80, 'HTTPSPort': 443,\n                'OriginProtocolPolicy': 'http-only'}\n        parameters['DistributionConfig']['Origins'] = {\n            \"Quantity\": 1, \"Items\": [item]}\n        parameters['DistributionConfig']['DefaultCacheBehavior'][\n            'TargetOriginId'] = origin_id\n\n\nclass CreateDefaultRootObject(ExclusiveArgument):\n    def __init__(self, argument_table, help_text=''):\n        super(CreateDefaultRootObject, self).__init__(\n            'default-root-object', argument_table, help_text=help_text or (\n                'The object that you want CloudFront to return (for example, '\n                'index.html) when a viewer request points to your root URL.'))\n\n    def add_to_params(self, parameters, value):\n        if value is not None:\n            parameters.setdefault(\n                'DistributionConfig', self.distribution_config_template())\n            parameters['DistributionConfig']['DefaultRootObject'] = value\n\n\nclass UpdateDefaultRootObject(CreateDefaultRootObject):\n    def __init__(self, context, argument_table):\n        super(UpdateDefaultRootObject, self).__init__(\n            argument_table, help_text=(\n                'The object that you want CloudFront to return (for example, '\n                'index.html) when a viewer request points to your root URL. '\n                'CLI will automatically make a get-distribution-config call '\n                'to load and preserve your other settings.'))\n        self.context = context\n\n    def add_to_params(self, parameters, value):\n        if value is not None:\n            client = self.context['session'].create_client(\n                'cloudfront',\n                region_name=self.context['parsed_args'].region,\n                endpoint_url=self.context['parsed_args'].endpoint_url,\n                verify=self.context['parsed_args'].verify_ssl)\n            response = client.get_distribution_config(Id=parameters['Id'])\n            parameters['IfMatch'] = response['ETag']\n            parameters['DistributionConfig'] = response['DistributionConfig']\n            parameters['DistributionConfig']['DefaultRootObject'] = value\n\n\ndef _add_sign(command_table, session, **kwargs):\n    command_table['sign'] = SignCommand(session)\n\n\nclass SignCommand(BasicCommand):\n    NAME = 'sign'\n    DESCRIPTION = 'Sign a given url.'\n    DATE_FORMAT = \"\"\"Supported formats include:\n        YYYY-MM-DD (which means 0AM UTC of that day),\n        YYYY-MM-DDThh:mm:ss (with default timezone as UTC),\n        YYYY-MM-DDThh:mm:ss+hh:mm or YYYY-MM-DDThh:mm:ss-hh:mm (with offset),\n        or EpochTime (which always means UTC).\n        Do NOT use YYYYMMDD, because it will be treated as EpochTime.\"\"\"\n    ARG_TABLE = [\n        {\n            'name': 'url',\n            'no_paramfile': True,  # To disable the default paramfile behavior\n            'required': True,\n            'help_text': 'The URL to be signed',\n        },\n        {\n            'name': 'key-pair-id',\n            'required': True,\n            'help_text': (\n                \"The active CloudFront key pair Id for the key pair \"\n                \"that you're using to generate the signature.\"),\n        },\n        {\n            'name': 'private-key',\n            'required': True,\n            'help_text': 'file://path/to/your/private-key.pem',\n        },\n        {\n            'name': 'date-less-than', 'required': True,\n            'help_text':\n                'The expiration date and time for the URL. ' + DATE_FORMAT,\n        },\n        {\n            'name': 'date-greater-than',\n            'help_text':\n                'An optional start date and time for the URL. ' + DATE_FORMAT,\n        },\n        {\n            'name': 'ip-address',\n            'help_text': (\n                'An optional IP address or IP address range to allow client '\n                'making the GET request from. Format: x.x.x.x/x or x.x.x.x'),\n        },\n    ]\n\n    def _run_main(self, args, parsed_globals):\n        signer = CloudFrontSigner(\n            args.key_pair_id, RSASigner(args.private_key).sign)\n        date_less_than = parse_to_aware_datetime(args.date_less_than)\n        date_greater_than = args.date_greater_than\n        if date_greater_than is not None:\n            date_greater_than = parse_to_aware_datetime(date_greater_than)\n        if date_greater_than is not None or args.ip_address is not None:\n            policy = signer.build_policy(\n                args.url, date_less_than, date_greater_than=date_greater_than,\n                ip_address=args.ip_address)\n            sys.stdout.write(signer.generate_presigned_url(\n                args.url, policy=policy))\n        else:\n            sys.stdout.write(signer.generate_presigned_url(\n                args.url, date_less_than=date_less_than))\n        return 0\n\n\nclass RSASigner(object):\n    def __init__(self, private_key):\n        self.priv_key = rsa.PrivateKey.load_pkcs1(private_key.encode('utf8'))\n\n    def sign(self, message):\n        return rsa.sign(message, self.priv_key, 'SHA-1')\n", "awscli/customizations/streamingoutputarg.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.model import Shape\n\nfrom awscli.arguments import BaseCLIArgument\n\n\ndef add_streaming_output_arg(argument_table, operation_model,\n                             session, **kwargs):\n    # Implementation detail:  hooked up to 'building-argument-table'\n    # event.\n    if _has_streaming_output(operation_model):\n        streaming_argument_name = _get_streaming_argument_name(operation_model)\n        argument_table['outfile'] = StreamingOutputArgument(\n            response_key=streaming_argument_name,\n            operation_model=operation_model,\n            session=session, name='outfile')\n\n\ndef _has_streaming_output(model):\n    return model.has_streaming_output\n\n\ndef _get_streaming_argument_name(model):\n    return model.output_shape.serialization['payload']\n\n\nclass StreamingOutputArgument(BaseCLIArgument):\n\n    BUFFER_SIZE = 32768\n    HELP = 'Filename where the content will be saved'\n\n    def __init__(self, response_key, operation_model, name,\n                 session, buffer_size=None):\n        self._name = name\n        self.argument_model = Shape('StreamingOutputArgument',\n                                    {'type': 'string'})\n        if buffer_size is None:\n            buffer_size = self.BUFFER_SIZE\n        self._buffer_size = buffer_size\n        # This is the key in the response body where we can find the\n        # streamed contents.\n        self._response_key = response_key\n        self._output_file = None\n        self._name = name\n        self._required = True\n        self._operation_model = operation_model\n        self._session = session\n\n    @property\n    def cli_name(self):\n        # Because this is a parameter, not an option, it shouldn't have the\n        # '--' prefix. We want to use the self.py_name to indicate that it's an\n        # argument.\n        return self._name\n\n    @property\n    def cli_type_name(self):\n        return 'string'\n\n    @property\n    def required(self):\n        return self._required\n\n    @required.setter\n    def required(self, value):\n        self._required = value\n\n    @property\n    def documentation(self):\n        return self.HELP\n\n    def add_to_parser(self, parser):\n        parser.add_argument(self._name, metavar=self.py_name,\n                            help=self.HELP)\n\n    def add_to_params(self, parameters, value):\n        self._output_file = value\n        service_id = self._operation_model.service_model.service_id.hyphenize()\n        operation_name = self._operation_model.name\n        self._session.register('after-call.%s.%s' % (\n            service_id, operation_name), self.save_file)\n\n    def save_file(self, parsed, **kwargs):\n        if self._response_key not in parsed:\n            # If the response key is not in parsed, then\n            # we've received an error message and we'll let the AWS CLI\n            # error handler print out an error message.  We have no\n            # file to save in this situation.\n            return\n        body = parsed[self._response_key]\n        buffer_size = self._buffer_size\n        with open(self._output_file, 'wb') as fp:\n            data = body.read(buffer_size)\n            while data:\n                fp.write(data)\n                data = body.read(buffer_size)\n        # We don't want to include the streaming param in\n        # the returned response.\n        del parsed[self._response_key]\n", "awscli/customizations/s3uploader.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport hashlib\nimport logging\nimport threading\nimport os\nimport sys\n\nimport botocore\nimport botocore.exceptions\nfrom s3transfer.manager import TransferManager\nfrom s3transfer.subscribers import BaseSubscriber\n\nfrom awscli.compat import collections_abc\n\nLOG = logging.getLogger(__name__)\n\n\nclass NoSuchBucketError(Exception):\n    def __init__(self, **kwargs):\n        msg = self.fmt.format(**kwargs)\n        Exception.__init__(self, msg)\n        self.kwargs = kwargs\n\n\n    fmt = (\"S3 Bucket does not exist. \"\n           \"Execute the command to create a new bucket\"\n           \"\\n\"\n           \"aws s3 mb s3://{bucket_name}\")\n\n\nclass S3Uploader(object):\n    \"\"\"\n    Class to upload objects to S3 bucket that use versioning. If bucket\n    does not already use versioning, this class will turn on versioning.\n    \"\"\"\n\n    @property\n    def artifact_metadata(self):\n        \"\"\"\n        Metadata to attach to the object(s) uploaded by the uploader.\n        \"\"\"\n        return self._artifact_metadata\n\n    @artifact_metadata.setter\n    def artifact_metadata(self, val):\n        if val is not None and not isinstance(val, collections_abc.Mapping):\n            raise TypeError(\"Artifact metadata should be in dict type\")\n        self._artifact_metadata = val\n\n    def __init__(self, s3_client,\n                 bucket_name,\n                 prefix=None,\n                 kms_key_id=None,\n                 force_upload=False,\n                 transfer_manager=None):\n        self.bucket_name = bucket_name\n        self.prefix = prefix\n        self.kms_key_id = kms_key_id or None\n        self.force_upload = force_upload\n        self.s3 = s3_client\n\n        self.transfer_manager = transfer_manager\n        if not transfer_manager:\n            self.transfer_manager = TransferManager(self.s3)\n\n        self._artifact_metadata = None\n\n    def upload(self, file_name, remote_path):\n        \"\"\"\n        Uploads given file to S3\n        :param file_name: Path to the file that will be uploaded\n        :param remote_path:  be uploaded\n        :return: VersionId of the latest upload\n        \"\"\"\n\n        if self.prefix and len(self.prefix) > 0:\n            remote_path = \"{0}/{1}\".format(self.prefix, remote_path)\n\n        # Check if a file with same data exists\n        if not self.force_upload and self.file_exists(remote_path):\n            LOG.debug(\"File with same data already exists at {0}. \"\n                      \"Skipping upload\".format(remote_path))\n            return self.make_url(remote_path)\n\n        try:\n\n            # Default to regular server-side encryption unless customer has\n            # specified their own KMS keys\n            additional_args = {\n                \"ServerSideEncryption\": \"AES256\"\n            }\n\n            if self.kms_key_id:\n                additional_args[\"ServerSideEncryption\"] = \"aws:kms\"\n                additional_args[\"SSEKMSKeyId\"] = self.kms_key_id\n\n            if self.artifact_metadata:\n                additional_args[\"Metadata\"] = self.artifact_metadata\n\n            print_progress_callback = \\\n                ProgressPercentage(file_name, remote_path)\n            future = self.transfer_manager.upload(file_name,\n                                                  self.bucket_name,\n                                                  remote_path,\n                                                  additional_args,\n                                                  [print_progress_callback])\n            future.result()\n\n            return self.make_url(remote_path)\n\n        except botocore.exceptions.ClientError as ex:\n            error_code = ex.response[\"Error\"][\"Code\"]\n            if error_code == \"NoSuchBucket\":\n                raise NoSuchBucketError(bucket_name=self.bucket_name)\n            raise ex\n\n    def upload_with_dedup(self, file_name, extension=None):\n        \"\"\"\n        Makes and returns name of the S3 object based on the file's MD5 sum\n\n        :param file_name: file to upload\n        :param extension: String of file extension to append to the object\n        :return: S3 URL of the uploaded object\n        \"\"\"\n\n        # This construction of remote_path is critical to preventing duplicate\n        # uploads of same object. Uploader will check if the file exists in S3\n        # and re-upload only if necessary. So the template points to same file\n        # in multiple places, this will upload only once\n\n        filemd5 = self.file_checksum(file_name)\n        remote_path = filemd5\n        if extension:\n            remote_path = remote_path + \".\" + extension\n\n        return self.upload(file_name, remote_path)\n\n    def file_exists(self, remote_path):\n        \"\"\"\n        Check if the file we are trying to upload already exists in S3\n\n        :param remote_path:\n        :return: True, if file exists. False, otherwise\n        \"\"\"\n\n        try:\n            # Find the object that matches this ETag\n            self.s3.head_object(\n                Bucket=self.bucket_name, Key=remote_path)\n            return True\n        except botocore.exceptions.ClientError:\n            # Either File does not exist or we are unable to get\n            # this information.\n            return False\n\n    def make_url(self, obj_path):\n        return \"s3://{0}/{1}\".format(\n            self.bucket_name, obj_path)\n\n    def file_checksum(self, file_name):\n\n        with open(file_name, \"rb\") as file_handle:\n            md5 = hashlib.md5()\n            # Read file in chunks of 4096 bytes\n            block_size = 4096\n\n            # Save current cursor position and reset cursor to start of file\n            curpos = file_handle.tell()\n            file_handle.seek(0)\n\n            buf = file_handle.read(block_size)\n            while len(buf) > 0:\n                md5.update(buf)\n                buf = file_handle.read(block_size)\n\n            # Restore file cursor's position\n            file_handle.seek(curpos)\n\n            return md5.hexdigest()\n\n    def to_path_style_s3_url(self, key, version=None):\n        \"\"\"\n            This link describes the format of Path Style URLs\n            http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro\n        \"\"\"\n        base = self.s3.meta.endpoint_url\n        result = \"{0}/{1}/{2}\".format(base, self.bucket_name, key)\n        if version:\n            result = \"{0}?versionId={1}\".format(result, version)\n\n        return result\n\n\nclass ProgressPercentage(BaseSubscriber):\n    # This class was copied directly from S3Transfer docs\n\n    def __init__(self, filename, remote_path):\n        self._filename = filename\n        self._remote_path = remote_path\n        self._size = float(os.path.getsize(filename))\n        self._seen_so_far = 0\n        self._lock = threading.Lock()\n\n    def on_progress(self, future, bytes_transferred, **kwargs):\n\n        # To simplify we'll assume this is hooked up\n        # to a single filename.\n        with self._lock:\n            self._seen_so_far += bytes_transferred\n            percentage = (self._seen_so_far / self._size) * 100\n            sys.stderr.write(\n                    \"\\rUploading to %s  %s / %s  (%.2f%%)\" %\n                    (self._remote_path, self._seen_so_far,\n                     self._size, percentage))\n            sys.stderr.flush()\n", "awscli/customizations/globalargs.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport sys\nimport os\n\nfrom botocore.client import Config\nfrom botocore.endpoint import DEFAULT_TIMEOUT\nfrom botocore.handlers import disable_signing\nimport jmespath\n\nfrom awscli.compat import urlparse\n\n\ndef register_parse_global_args(cli):\n    cli.register('top-level-args-parsed', resolve_types,\n                 unique_id='resolve-types')\n    cli.register('top-level-args-parsed', no_sign_request,\n                 unique_id='no-sign')\n    cli.register('top-level-args-parsed', resolve_verify_ssl,\n                 unique_id='resolve-verify-ssl')\n    cli.register('top-level-args-parsed', resolve_cli_read_timeout,\n                 unique_id='resolve-cli-read-timeout')\n    cli.register('top-level-args-parsed', resolve_cli_connect_timeout,\n                 unique_id='resolve-cli-connect-timeout')\n\n\ndef resolve_types(parsed_args, **kwargs):\n    # This emulates the \"type\" arg from argparse, but does so in a way\n    # that plugins can also hook into this process.\n    _resolve_arg(parsed_args, 'query')\n    _resolve_arg(parsed_args, 'endpoint_url')\n\n\ndef _resolve_arg(parsed_args, name):\n    value = getattr(parsed_args, name, None)\n    if value is not None:\n        new_value = getattr(sys.modules[__name__], '_resolve_%s' % name)(value)\n        setattr(parsed_args, name, new_value)\n\n\ndef _resolve_query(value):\n    try:\n        return jmespath.compile(value)\n    except Exception as e:\n        raise ValueError(\"Bad value for --query %s: %s\" % (value, str(e)))\n\n\ndef _resolve_endpoint_url(value):\n    parsed = urlparse.urlparse(value)\n    # Our http library requires you specify an endpoint url\n    # that contains a scheme, so we'll verify that up front.\n    if not parsed.scheme:\n        raise ValueError('Bad value for --endpoint-url \"%s\": scheme is '\n                         'missing.  Must be of the form '\n                         'http://<hostname>/ or https://<hostname>/' % value)\n    return value\n\n\ndef resolve_verify_ssl(parsed_args, session, **kwargs):\n    arg_name = 'verify_ssl'\n    arg_value = getattr(parsed_args, arg_name, None)\n    if arg_value is not None:\n        verify = None\n        # Only consider setting a custom ca_bundle if they\n        # haven't provided --no-verify-ssl.\n        if not arg_value:\n            verify = False\n        else:\n            # in case if `ca_bundle` not in args it'll be retrieved\n            # from config on session.client creation step\n            verify = getattr(parsed_args, 'ca_bundle', None)\n        setattr(parsed_args, arg_name, verify)\n\n\ndef no_sign_request(parsed_args, session, **kwargs):\n    if not parsed_args.sign_request:\n        # In order to make signing disabled for all requests\n        # we need to use botocore's ``disable_signing()`` handler.\n        # Register this first to override other handlers.\n        emitter = session.get_component('event_emitter')\n        emitter.register_first(\n            'choose-signer', disable_signing, unique_id='disable-signing',\n        )\n\n\ndef resolve_cli_connect_timeout(parsed_args, session, **kwargs):\n    arg_name = 'connect_timeout'\n    _resolve_timeout(session, parsed_args, arg_name)\n\n\ndef resolve_cli_read_timeout(parsed_args, session, **kwargs):\n    arg_name = 'read_timeout'\n    _resolve_timeout(session, parsed_args, arg_name)\n\n\ndef _resolve_timeout(session, parsed_args, arg_name):\n    arg_value = getattr(parsed_args, arg_name, None)\n    if arg_value is None:\n        arg_value = DEFAULT_TIMEOUT\n    arg_value = int(arg_value)\n    if arg_value == 0:\n        arg_value = None\n    setattr(parsed_args, arg_name, arg_value)\n    # Update in the default client config so that the timeout will be used\n    # by all clients created from then on.\n    _update_default_client_config(session, arg_name, arg_value)\n\n\ndef _update_default_client_config(session, arg_name, arg_value):\n    current_default_config = session.get_default_client_config()\n    new_default_config = Config(**{arg_name: arg_value})\n    if current_default_config is not None:\n        new_default_config = current_default_config.merge(new_default_config)\n    session.set_default_client_config(new_default_config)\n", "awscli/customizations/utils.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nUtility functions to make it easier to work with customizations.\n\n\"\"\"\nimport copy\nimport sys\n\nfrom botocore.exceptions import ClientError\n\n\ndef rename_argument(argument_table, existing_name, new_name):\n    current = argument_table[existing_name]\n    argument_table[new_name] = current\n    current.name = new_name\n    del argument_table[existing_name]\n\n\ndef _copy_argument(argument_table, current_name, copy_name):\n    current = argument_table[current_name]\n    copy_arg = copy.copy(current)\n    copy_arg.name = copy_name\n    argument_table[copy_name] = copy_arg\n    return copy_arg\n\n\ndef make_hidden_alias(argument_table, existing_name, alias_name):\n    \"\"\"Create a hidden alias for an existing argument.\n\n    This will copy an existing argument object in an arg table,\n    and add a new entry to the arg table with a different name.\n    The new argument will also be undocumented.\n\n    This is needed if you want to check an existing argument,\n    but you still need the other one to work for backwards\n    compatibility reasons.\n\n    \"\"\"\n    current = argument_table[existing_name]\n    copy_arg = _copy_argument(argument_table, existing_name, alias_name)\n    copy_arg._UNDOCUMENTED = True\n    if current.required:\n        # If the current argument is required, then\n        # we'll mark both as not required, but\n        # flag _DOCUMENT_AS_REQUIRED so our doc gen\n        # knows to still document this argument as required.\n        copy_arg.required = False\n        current.required = False\n        current._DOCUMENT_AS_REQUIRED = True\n\n\ndef rename_command(command_table, existing_name, new_name):\n    current = command_table[existing_name]\n    command_table[new_name] = current\n    current.name = new_name\n    del command_table[existing_name]\n\n\ndef alias_command(command_table, existing_name, new_name):\n    \"\"\"Moves an argument to a new name, keeping the old as a hidden alias.\n\n    :type command_table: dict\n    :param command_table: The full command table for the CLI or a service.\n\n    :type existing_name: str\n    :param existing_name: The current name of the command.\n\n    :type new_name: str\n    :param new_name: The new name for the command.\n    \"\"\"\n    current = command_table[existing_name]\n    _copy_argument(command_table, existing_name, new_name)\n    current._UNDOCUMENTED = True\n\n\ndef make_hidden_command_alias(command_table, existing_name, alias_name):\n    \"\"\"Create a hidden alias for an exiting command.\n\n    This will copy an existing command object in a command table and add a new\n    entry to the command table with a different name. The new command will\n    be undocumented.\n\n    This is needed if you want to change an existing command, but you still\n    need the old name to work for backwards compatibility reasons.\n\n    :type command_table: dict\n    :param command_table: The full command table for the CLI or a service.\n\n    :type existing_name: str\n    :param existing_name: The current name of the command.\n\n    :type alias_name: str\n    :param alias_name: The new name for the command.\n    \"\"\"\n    new = _copy_argument(command_table, existing_name, alias_name)\n    new._UNDOCUMENTED = True\n\n\ndef validate_mutually_exclusive_handler(*groups):\n    def _handler(parsed_args, **kwargs):\n        return validate_mutually_exclusive(parsed_args, *groups)\n    return _handler\n\n\ndef validate_mutually_exclusive(parsed_args, *groups):\n    \"\"\"Validate mutually exclusive groups in the parsed args.\"\"\"\n    args_dict = vars(parsed_args)\n    all_args = set(arg for group in groups for arg in group)\n    if not any(k in all_args for k in args_dict if args_dict[k] is not None):\n        # If none of the specified args are in a mutually exclusive group\n        # there is nothing left to validate.\n        return\n    current_group = None\n    for key in [k for k in args_dict if args_dict[k] is not None]:\n        key_group = _get_group_for_key(key, groups)\n        if key_group is None:\n            # If they key is not part of a mutex group, we can move on.\n            continue\n        if current_group is None:\n            current_group = key_group\n        elif not key_group == current_group:\n            raise ValueError('The key \"%s\" cannot be specified when one '\n                             'of the following keys are also specified: '\n                             '%s' % (key, ', '.join(current_group)))\n\n\ndef _get_group_for_key(key, groups):\n    for group in groups:\n        if key in group:\n            return group\n\n\ndef s3_bucket_exists(s3_client, bucket_name):\n    bucket_exists = True\n    try:\n        # See if the bucket exists by running a head bucket\n        s3_client.head_bucket(Bucket=bucket_name)\n    except ClientError as e:\n        # If a client error is thrown. Check that it was a 404 error.\n        # If it was a 404 error, than the bucket does not exist.\n        error_code = int(e.response['Error']['Code'])\n        if error_code == 404:\n            bucket_exists = False\n    return bucket_exists\n\n\ndef create_client_from_parsed_globals(session, service_name, parsed_globals,\n                                      overrides=None):\n    \"\"\"Creates a service client, taking parsed_globals into account\n\n    Any values specified in overrides will override the returned dict. Note\n    that this override occurs after 'region' from parsed_globals has been\n    translated into 'region_name' in the resulting dict.\n    \"\"\"\n    client_args = {}\n    if 'region' in parsed_globals:\n        client_args['region_name'] = parsed_globals.region\n    if 'endpoint_url' in parsed_globals:\n        client_args['endpoint_url'] = parsed_globals.endpoint_url\n    if 'verify_ssl' in parsed_globals:\n        client_args['verify'] = parsed_globals.verify_ssl\n    if overrides:\n        client_args.update(overrides)\n    return session.create_client(service_name, **client_args)\n\n\ndef uni_print(statement, out_file=None):\n    \"\"\"\n    This function is used to properly write unicode to a file, usually\n    stdout or stdderr.  It ensures that the proper encoding is used if the\n    statement is not a string type.\n    \"\"\"\n    if out_file is None:\n        out_file = sys.stdout\n    try:\n        # Otherwise we assume that out_file is a\n        # text writer type that accepts str/unicode instead\n        # of bytes.\n        out_file.write(statement)\n    except UnicodeEncodeError:\n        # Some file like objects like cStringIO will\n        # try to decode as ascii on python2.\n        #\n        # This can also fail if our encoding associated\n        # with the text writer cannot encode the unicode\n        # ``statement`` we've been given.  This commonly\n        # happens on windows where we have some S3 key\n        # previously encoded with utf-8 that can't be\n        # encoded using whatever codepage the user has\n        # configured in their console.\n        #\n        # At this point we've already failed to do what's\n        # been requested.  We now try to make a best effort\n        # attempt at printing the statement to the outfile.\n        # We're using 'ascii' as the default because if the\n        # stream doesn't give us any encoding information\n        # we want to pick an encoding that has the highest\n        # chance of printing successfully.\n        new_encoding = getattr(out_file, 'encoding', 'ascii')\n        # When the output of the aws command is being piped,\n        # ``sys.stdout.encoding`` is ``None``.\n        if new_encoding is None:\n            new_encoding = 'ascii'\n        new_statement = statement.encode(\n            new_encoding, 'replace').decode(new_encoding)\n        out_file.write(new_statement)\n    out_file.flush()\n\n\ndef get_policy_arn_suffix(region):\n    \"\"\"Method to return region value as expected by policy arn\"\"\"\n    region_string = region.lower()\n    if region_string.startswith(\"cn-\"):\n        return \"aws-cn\"\n    elif region_string.startswith(\"us-gov\"):\n        return \"aws-us-gov\"\n    else:\n        return \"aws\"\n", "awscli/customizations/generatecliskeleton.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport sys\n\nfrom botocore import xform_name\nfrom botocore.stub import Stubber\nfrom botocore.utils import ArgumentGenerator\n\nfrom awscli.clidriver import CLIOperationCaller\nfrom awscli.customizations.arguments import OverrideRequiredArgsArgument\nfrom awscli.utils import json_encoder\n\n\ndef register_generate_cli_skeleton(cli):\n    cli.register('building-argument-table', add_generate_skeleton)\n\n\ndef add_generate_skeleton(session, operation_model, argument_table, **kwargs):\n    # This argument cannot support operations with streaming output which\n    # is designated by the argument name `outfile`.\n    if 'outfile' not in argument_table:\n        generate_cli_skeleton_argument = GenerateCliSkeletonArgument(\n            session, operation_model)\n        generate_cli_skeleton_argument.add_to_arg_table(argument_table)\n\n\nclass GenerateCliSkeletonArgument(OverrideRequiredArgsArgument):\n    \"\"\"This argument writes a generated JSON skeleton to stdout\n\n    The argument, if present in the command line, will prevent the intended\n    command from taking place. Instead, it will generate a JSON skeleton and\n    print it to standard output.\n    \"\"\"\n    ARG_DATA = {\n        'name': 'generate-cli-skeleton',\n        'help_text': (\n            'Prints a JSON skeleton to standard output without sending '\n            'an API request. If provided with no value or the value '\n            '``input``, prints a sample input JSON that can be used as an '\n            'argument for ``--cli-input-json``. If provided with the value '\n            '``output``, it validates the command inputs and returns a '\n            'sample output JSON for that command.'\n        ),\n        'nargs': '?',\n        'const': 'input',\n        'choices': ['input', 'output'],\n    }\n\n    def __init__(self, session, operation_model):\n        super(GenerateCliSkeletonArgument, self).__init__(session)\n        self._operation_model = operation_model\n\n    def _register_argument_action(self):\n        self._session.register(\n            'calling-command.*', self.generate_json_skeleton)\n        super(GenerateCliSkeletonArgument, self)._register_argument_action()\n\n    def override_required_args(self, argument_table, args, **kwargs):\n        arg_name = '--' + self.name\n        if arg_name in args:\n            arg_location = args.index(arg_name)\n            try:\n                # If the value of --generate-cli-skeleton is ``output``,\n                # do not force required arguments to be optional as\n                # ``--generate-cli-skeleton output`` validates commands\n                # as well as print out the sample output.\n                if args[arg_location + 1] == 'output':\n                    return\n            except IndexError:\n                pass\n            super(GenerateCliSkeletonArgument, self).override_required_args(\n                argument_table, args, **kwargs)\n\n    def generate_json_skeleton(self, call_parameters, parsed_args,\n                               parsed_globals, **kwargs):\n        if getattr(parsed_args, 'generate_cli_skeleton', None):\n            for_output = parsed_args.generate_cli_skeleton == 'output'\n            operation_model = self._operation_model\n\n            if for_output:\n                service_name = operation_model.service_model.service_name\n                operation_name = operation_model.name\n                # TODO: It would be better to abstract this logic into\n                # classes for both the input and output option such that\n                # a similar set of inputs are taken in and output\n                # similar functionality.\n                return StubbedCLIOperationCaller(self._session).invoke(\n                    service_name, operation_name, call_parameters,\n                    parsed_globals)\n            else:\n                argument_generator = ArgumentGenerator()\n                operation_input_shape = operation_model.input_shape\n                if operation_input_shape is None:\n                    skeleton = {}\n                else:\n                    skeleton = argument_generator.generate_skeleton(\n                        operation_input_shape)\n\n                sys.stdout.write(\n                    json.dumps(skeleton, indent=4, default=json_encoder)\n                )\n                sys.stdout.write('\\n')\n                return 0\n\n\nclass StubbedCLIOperationCaller(CLIOperationCaller):\n    \"\"\"A stubbed CLIOperationCaller\n\n    It generates a fake response and uses the response and provided parameters\n    to make a stubbed client call for an operation command.\n    \"\"\"\n    def _make_client_call(self, client, operation_name, parameters,\n                          parsed_globals):\n        method_name = xform_name(operation_name)\n        operation_model = client.meta.service_model.operation_model(\n            operation_name)\n        fake_response = {}\n        if operation_model.output_shape:\n            argument_generator = ArgumentGenerator(use_member_names=True)\n            fake_response = argument_generator.generate_skeleton(\n                operation_model.output_shape)\n        with Stubber(client) as stubber:\n            stubber.add_response(method_name, fake_response)\n            return getattr(client, method_name)(**parameters)\n", "awscli/customizations/scalarparse.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Change the scalar response parsing behavior for the AWS CLI.\n\nThe underlying library used by botocore has some response parsing\nbehavior that we'd like to modify in the AWS CLI.  There are two:\n\n    * Parsing binary content.\n    * Parsing timestamps (dates)\n\nFor the first option we can't print binary content to the terminal,\nso this customization leaves the binary content base64 encoded.  If the\nuser wants the binary content, they can then base64 decode the appropriate\nfields as needed.\n\nThere's nothing currently done for timestamps, but this will change\nin the future.\n\n\"\"\"\nfrom botocore.utils import parse_timestamp\nfrom botocore.exceptions import ProfileNotFound\n\n\ndef register_scalar_parser(event_handlers):\n    event_handlers.register_first(\n        'session-initialized', add_scalar_parsers)\n\n\ndef identity(x):\n    return x\n\n\ndef iso_format(value):\n    return parse_timestamp(value).isoformat()\n\n\ndef add_timestamp_parser(session):\n    factory = session.get_component('response_parser_factory')\n    try:\n        timestamp_format = session.get_scoped_config().get(\n            'cli_timestamp_format',\n            'none')\n    except ProfileNotFound:\n        # If a --profile is provided that does not exist, loading\n        # a value from get_scoped_config will crash the CLI.\n        # This function can be called as the first handler for\n        # the session-initialized event, which happens before a\n        # profile can be created, even if the command would have\n        # successfully created a profile. Instead of crashing here\n        # on a ProfileNotFound the CLI should just use 'none'.\n        timestamp_format = 'none'\n    if timestamp_format == 'none':\n        # For backwards compatibility reasons, we replace botocore's timestamp\n        # parser (which parses to a datetime.datetime object) with the\n        # identity function which prints the date exactly the same as it comes\n        # across the wire.\n        timestamp_parser = identity\n    elif timestamp_format == 'iso8601':\n        timestamp_parser = iso_format\n    else:\n        raise ValueError('Unknown cli_timestamp_format value: %s, valid values'\n                         ' are \"none\" or \"iso8601\"' % timestamp_format)\n    factory.set_parser_defaults(timestamp_parser=timestamp_parser)\n\n\ndef add_scalar_parsers(session, **kwargs):\n    factory = session.get_component('response_parser_factory')\n    factory.set_parser_defaults(blob_parser=identity)\n    add_timestamp_parser(session)\n", "awscli/customizations/s3errormsg.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Give better S3 error messages.\n\"\"\"\n\n\nREGION_ERROR_MSG = (\n    'You can fix this issue by explicitly providing the correct region '\n    'location using the --region argument, the AWS_DEFAULT_REGION '\n    'environment variable, or the region variable in the AWS CLI '\n    \"configuration file.  You can get the bucket's location by \"\n    'running \"aws s3api get-bucket-location --bucket BUCKET\".'\n)\n\nENABLE_SIGV4_MSG = (\n    ' You can enable AWS Signature Version 4 by running the command: \\n'\n    'aws configure set s3.signature_version s3v4'\n)\n\n\ndef register_s3_error_msg(event_handlers):\n    event_handlers.register('after-call.s3', enhance_error_msg)\n\n\ndef enhance_error_msg(parsed, **kwargs):\n    if parsed is None or 'Error' not in parsed:\n        # There's no error message to enhance so we can continue.\n        return\n    if _is_sigv4_error_message(parsed):\n        message = (\n            'You are attempting to operate on a bucket in a region '\n            'that requires Signature Version 4.  '\n        )\n        message += REGION_ERROR_MSG\n        parsed['Error']['Message'] = message\n    elif _is_permanent_redirect_message(parsed):\n        endpoint = parsed['Error']['Endpoint']\n        message = parsed['Error']['Message']\n        new_message = message[:-1] + ': %s\\n' % endpoint\n        new_message += REGION_ERROR_MSG\n        parsed['Error']['Message'] = new_message\n    elif _is_kms_sigv4_error_message(parsed):\n        parsed['Error']['Message'] += ENABLE_SIGV4_MSG\n\n\ndef _is_sigv4_error_message(parsed):\n    return ('Please use AWS4-HMAC-SHA256' in\n            parsed.get('Error', {}).get('Message', ''))\n\n\ndef _is_permanent_redirect_message(parsed):\n    return parsed.get('Error', {}).get('Code', '') == 'PermanentRedirect'\n\n\ndef _is_kms_sigv4_error_message(parsed):\n    return ('AWS KMS managed keys require AWS Signature Version 4' in\n            parsed.get('Error', {}).get('Message', ''))\n", "awscli/customizations/s3events.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Add S3 specific event streaming output arg.\"\"\"\nfrom awscli.arguments import CustomArgument\n\n\nSTREAM_HELP_TEXT = 'Filename where the records will be saved'\n\n\nclass DocSectionNotFoundError(Exception):\n    pass\n\n\ndef register_event_stream_arg(event_handlers):\n    event_handlers.register(\n        'building-argument-table.s3api.select-object-content',\n        add_event_stream_output_arg)\n    event_handlers.register_last(\n        'doc-output.s3api.select-object-content',\n        replace_event_stream_docs\n    )\n\n\ndef add_event_stream_output_arg(argument_table, operation_model,\n                                session, **kwargs):\n    argument_table['outfile'] = S3SelectStreamOutputArgument(\n        name='outfile', help_text=STREAM_HELP_TEXT,\n        cli_type_name='string', positional_arg=True,\n        stream_key=operation_model.output_shape.serialization['payload'],\n        session=session)\n\n\ndef replace_event_stream_docs(help_command, **kwargs):\n    doc = help_command.doc\n    current = ''\n    while current != '======\\nOutput\\n======':\n        try:\n            current = doc.pop_write()\n        except IndexError:\n            # This should never happen, but in the rare case that it does\n            # we should be raising something with a helpful error message.\n            raise DocSectionNotFoundError(\n                'Could not find the \"output\" section for the command: %s'\n                % help_command)\n    doc.write('======\\nOutput\\n======\\n')\n    doc.write(\"This command generates no output.  The selected \"\n              \"object content is written to the specified outfile.\\n\")\n\n\nclass S3SelectStreamOutputArgument(CustomArgument):\n    _DOCUMENT_AS_REQUIRED = True\n\n    def __init__(self, stream_key, session, **kwargs):\n        super(S3SelectStreamOutputArgument, self).__init__(**kwargs)\n        # This is the key in the response body where we can find the\n        # streamed contents.\n        self._stream_key = stream_key\n        self._output_file = None\n        self._session = session\n\n    def add_to_params(self, parameters, value):\n        self._output_file = value\n        self._session.register('after-call.s3.SelectObjectContent',\n                               self.save_file)\n\n    def save_file(self, parsed, **kwargs):\n        # This method is hooked into after-call which fires\n        # before the error checking happens in the client.\n        # Therefore if the stream_key is not in the parsed\n        # response we immediately return and let the default\n        # error handling happen.\n        if self._stream_key not in parsed:\n            return\n        event_stream = parsed[self._stream_key]\n        with open(self._output_file, 'wb') as fp:\n            for event in event_stream:\n                if 'Records' in event:\n                    fp.write(event['Records']['Payload'])\n        # We don't want to include the streaming param in\n        # the returned response, it's not JSON serializable.\n        del parsed[self._stream_key]\n", "awscli/customizations/iot.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nThis customization makes it easier to save various pieces of data\nreturned from iot commands that would typically need to be saved to a\nfile. This customization adds the following options:\n\n- aws iot create-certificate-from-csr\n  - ``--certificate-pem-outfile``: certificatePem\n- aws iot create-keys-and-certificate\n  - ``--certificate-pem-outfile``: certificatePem\n  - ``--public-key-outfile``: keyPair.PublicKey\n  - ``--private-key-outfile``: keyPair.PrivateKey\n\"\"\"\nfrom awscli.customizations.arguments import QueryOutFileArgument\n\n\ndef register_create_keys_and_cert_arguments(session, argument_table, **kwargs):\n    \"\"\"Add outfile save arguments to create-keys-and-certificate\n\n    - ``--certificate-pem-outfile``\n    - ``--public-key-outfile``\n    - ``--private-key-outfile``\n    \"\"\"\n    after_event = 'after-call.iot.CreateKeysAndCertificate'\n    argument_table['certificate-pem-outfile'] = QueryOutFileArgument(\n        session=session, name='certificate-pem-outfile',\n        query='certificatePem', after_call_event=after_event, perm=0o600)\n    argument_table['public-key-outfile'] = QueryOutFileArgument(\n        session=session, name='public-key-outfile', query='keyPair.PublicKey',\n        after_call_event=after_event, perm=0o600)\n    argument_table['private-key-outfile'] = QueryOutFileArgument(\n        session=session, name='private-key-outfile',\n        query='keyPair.PrivateKey', after_call_event=after_event, perm=0o600)\n\n\ndef register_create_keys_from_csr_arguments(session, argument_table, **kwargs):\n    \"\"\"Add certificate-pem-outfile to create-certificate-from-csr\"\"\"\n    argument_table['certificate-pem-outfile'] = QueryOutFileArgument(\n        session=session, name='certificate-pem-outfile',\n        query='certificatePem',\n        after_call_event='after-call.iot.CreateCertificateFromCsr', perm=0o600)\n", "awscli/customizations/preview.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"This module enables the preview-mode customization.\n\nIf a service is marked as being in preview mode, then any attempts\nto call operations on that service will print a message pointing\nthe user to alternate solutions.  A user can still access this\nservice by enabling the service in their config file via:\n\n    [preview]\n    servicename=true\n\nor by running:\n\n    aws configure set preview.servicename true\n\nAlso any service that is marked as being in preview will *not*\nbe listed in the help docs, unless the service has been enabled\nin the config file as shown above.\n\n\"\"\"\nimport logging\nimport sys\nimport textwrap\n\n\nlogger = logging.getLogger(__name__)\n\n\nPREVIEW_SERVICES = [\n    'sdb',\n]\n\n\ndef register_preview_commands(events):\n    events.register('building-command-table.main', mark_as_preview)\n\n\ndef mark_as_preview(command_table, session, **kwargs):\n    # These are services that are marked as preview but are\n    # explicitly enabled in the config file.\n    allowed_services = _get_allowed_services(session)\n    for preview_service in PREVIEW_SERVICES:\n        is_enabled = False\n        if preview_service in allowed_services:\n            # Then we don't need to swap it as a preview\n            # service, the user has specifically asked to\n            # enable this service.\n            logger.debug(\"Preview service enabled through config file: %s\",\n                         preview_service)\n            is_enabled = True\n        original_command = command_table[preview_service]\n        preview_cls = type(\n            'PreviewCommand',\n            (PreviewModeCommandMixin, original_command.__class__), {})\n        command_table[preview_service] = preview_cls(\n            cli_name=original_command.name,\n            session=session,\n            service_name=original_command.service_model.service_name,\n            is_enabled=is_enabled)\n        # We also want to register a handler that will update the\n        # description in the docs to say that this is a preview service.\n        session.get_component('event_emitter').register_last(\n            'doc-description.%s' % preview_service,\n            update_description_with_preview)\n\n\ndef update_description_with_preview(help_command, **kwargs):\n    style = help_command.doc.style\n    style.start_note()\n    style.bold(PreviewModeCommandMixin.HELP_SNIPPET.strip())\n    # bcdoc does not currently allow for what I'd like to do\n    # which is have a code block like:\n    #\n    # ::\n    #    [preview]\n    #    service=true\n    #\n    #    aws configure set preview.service true\n    #\n    # So for now we're just going to add the configure command\n    # to enable this.\n    style.doc.write(\"You can enable this service by running: \")\n    # The service name will always be the first element in the\n    # event class for the help object\n    service_name = help_command.event_class.split('.')[0]\n    style.code(\"aws configure set preview.%s true\" % service_name)\n    style.end_note()\n\n\ndef _get_allowed_services(session):\n    # For a service to be marked as preview, it must be in the\n    # [preview] section and it must have a value of 'true'\n    # (case insensitive).\n    allowed = []\n    preview_services = session.full_config.get('preview', {})\n    for preview, value in preview_services.items():\n        if value == 'true':\n            allowed.append(preview)\n    return allowed\n\n\nclass PreviewModeCommandMixin(object):\n    ENABLE_DOCS = textwrap.dedent(\"\"\"\\\n    However, if you'd like to use the \"aws {service}\" commands with the\n    AWS CLI, you can enable this service by adding the following to your CLI\n    config file:\n\n        [preview]\n        {service}=true\n\n    or by running:\n\n        aws configure set preview.{service} true\n\n    \"\"\")\n    HELP_SNIPPET = (\"AWS CLI support for this service is only \"\n                    \"available in a preview stage.\\n\")\n\n    def __init__(self, *args, **kwargs):\n        self._is_enabled = kwargs.pop('is_enabled')\n        super(PreviewModeCommandMixin, self).__init__(*args, **kwargs)\n\n    def __call__(self, args, parsed_globals):\n        if self._is_enabled or self._is_help_command(args):\n            return super(PreviewModeCommandMixin, self).__call__(\n                args, parsed_globals)\n        else:\n            return self._display_opt_in_message()\n\n    def _is_help_command(self, args):\n        return args and args[-1] == 'help'\n\n    def _display_opt_in_message(self):\n        sys.stderr.write(self.HELP_SNIPPET)\n        sys.stderr.write(\"\\n\")\n        # Then let them know how to enable this service.\n        sys.stderr.write(self.ENABLE_DOCS.format(service=self._service_name))\n        return 1\n", "awscli/customizations/ecr_public.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.utils import create_client_from_parsed_globals\n\nfrom base64 import b64decode\nimport sys\n\n\ndef register_ecr_public_commands(cli):\n    cli.register('building-command-table.ecr-public', _inject_commands)\n\n\ndef _inject_commands(command_table, session, **kwargs):\n    command_table['get-login-password'] = ECRPublicGetLoginPassword(session)\n\n\nclass ECRPublicGetLoginPassword(BasicCommand):\n    \"\"\"Get a password to be used with container clients such as Docker\"\"\"\n    NAME = 'get-login-password'\n\n    DESCRIPTION = BasicCommand.FROM_FILE(\n            'ecr-public/get-login-password_description.rst')\n\n    def _run_main(self, parsed_args, parsed_globals):\n        ecr_public_client = create_client_from_parsed_globals(\n                self._session,\n                'ecr-public',\n                parsed_globals)\n        result = ecr_public_client.get_authorization_token()\n        auth = result['authorizationData']\n        auth_token = b64decode(auth['authorizationToken']).decode()\n        _, password = auth_token.split(':')\n        sys.stdout.write(password)\n        sys.stdout.write('\\n')\n        return 0\n", "awscli/customizations/ecr.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.utils import create_client_from_parsed_globals\n\nfrom base64 import b64decode\nimport sys\n\n\ndef register_ecr_commands(cli):\n    cli.register('building-command-table.ecr', _inject_commands)\n\n\ndef _inject_commands(command_table, session, **kwargs):\n    command_table['get-login'] = ECRLogin(session)\n    command_table['get-login-password'] = ECRGetLoginPassword(session)\n\n\nclass ECRLogin(BasicCommand):\n    \"\"\"Log in with 'docker login'\"\"\"\n    NAME = 'get-login'\n\n    DESCRIPTION = BasicCommand.FROM_FILE('ecr/get-login_description.rst')\n\n    ARG_TABLE = [\n        {\n            'name': 'registry-ids',\n            'help_text': 'A list of AWS account IDs that correspond to the '\n                         'Amazon ECR registries that you want to log in to.',\n            'required': False,\n            'nargs': '+'\n        },\n        {\n            'name': 'include-email',\n            'action': 'store_true',\n            'group_name': 'include-email',\n            'dest': 'include_email',\n            'default': True,\n            'required': False,\n            'help_text': (\n                \"Specify if the '-e' flag should be included in the \"\n                \"'docker login' command.  The '-e' option has been deprecated \"\n                \"and is removed in Docker version 17.06 and later.  You must \"\n                \"specify --no-include-email if you're using Docker version \"\n                \"17.06 or later.  The default behavior is to include the \"\n                \"'-e' flag in the 'docker login' output.\"),\n        },\n        {\n            'name': 'no-include-email',\n            'help_text': 'Include email arg',\n            'action': 'store_false',\n            'default': True,\n            'group_name': 'include-email',\n            'dest': 'include_email',\n            'required': False,\n        },\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        ecr_client = create_client_from_parsed_globals(\n            self._session, 'ecr', parsed_globals)\n        if not parsed_args.registry_ids:\n            result = ecr_client.get_authorization_token()\n        else:\n            result = ecr_client.get_authorization_token(\n                registryIds=parsed_args.registry_ids)\n        for auth in result['authorizationData']:\n            auth_token = b64decode(auth['authorizationToken']).decode()\n            username, password = auth_token.split(':')\n            command = ['docker', 'login', '-u', username, '-p', password]\n            if parsed_args.include_email:\n                command.extend(['-e', 'none'])\n            command.append(auth['proxyEndpoint'])\n            sys.stdout.write(' '.join(command))\n            sys.stdout.write('\\n')\n        return 0\n\n\nclass ECRGetLoginPassword(BasicCommand):\n    \"\"\"Get a password to be used with container clients such as Docker\"\"\"\n    NAME = 'get-login-password'\n\n    DESCRIPTION = BasicCommand.FROM_FILE(\n            'ecr/get-login-password_description.rst')\n\n    def _run_main(self, parsed_args, parsed_globals):\n        ecr_client = create_client_from_parsed_globals(\n                self._session,\n                'ecr',\n                parsed_globals)\n        result = ecr_client.get_authorization_token()\n        auth = result['authorizationData'][0]\n        auth_token = b64decode(auth['authorizationToken']).decode()\n        _, password = auth_token.split(':')\n        sys.stdout.write(password)\n        sys.stdout.write('\\n')\n        return 0\n", "awscli/customizations/kinesis.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\ndef register_kinesis_list_streams_pagination_backcompat(event_emitter):\n    # The ListStreams previously used the ExclusiveStartStreamName parameter\n    # for input tokens to pagination. This operation was then updated to\n    # also allow for the typical NextToken input and output parameters. The\n    # pagination model was also updated to use the NextToken field instead of\n    # the ExclusiveStartStreamName field for input tokens. However, the\n    # ExclusiveStartStreamName is still a valid parameter to control pagination\n    # of this operation and is incompatible with the NextToken parameter. So,\n    # the CLI needs to continue to treat the ExclusiveStartStreamName as if it\n    # is a raw input token parameter to the API by disabling auto-pagination if\n    # provided. Otherwise, if it was treated as a normal API parameter, errors\n    # would be thrown when paginating across multiple pages since the parameter\n    # is incompatible with the NextToken parameter.\n    event_emitter.register(\n        'building-argument-table.kinesis.list-streams',\n        undocument_exclusive_start_stream_name,\n    )\n    event_emitter.register(\n        'operation-args-parsed.kinesis.list-streams',\n        disable_pagination_when_exclusive_start_stream_name_provided,\n    )\n\n\ndef undocument_exclusive_start_stream_name(argument_table, **kwargs):\n    argument_table['exclusive-start-stream-name']._UNDOCUMENTED = True\n\n\ndef disable_pagination_when_exclusive_start_stream_name_provided(\n    parsed_args, parsed_globals, **kwargs\n):\n    if parsed_args.exclusive_start_stream_name is not None:\n        parsed_globals.paginate = False\n", "awscli/customizations/waiters.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore import xform_name\nfrom botocore.exceptions import DataNotFoundError\n\nfrom awscli.clidriver import ServiceOperation\nfrom awscli.customizations.commands import BasicCommand, BasicHelp, \\\n    BasicDocHandler\n\n\ndef register_add_waiters(cli):\n    cli.register('building-command-table', add_waiters)\n\n\ndef add_waiters(command_table, session, command_object, **kwargs):\n    # Check if the command object passed in has a ``service_object``. We\n    # only want to add wait commands to top level model-driven services.\n    # These require service objects.\n    service_model = getattr(command_object, 'service_model', None)\n    if service_model is not None:\n        # Get a client out of the service object.\n        waiter_model = get_waiter_model_from_service_model(session,\n                                                           service_model)\n        if waiter_model is None:\n            return\n        waiter_names = waiter_model.waiter_names\n        # If there are waiters make a wait command.\n        if waiter_names:\n            command_table['wait'] = WaitCommand(\n                session, waiter_model, service_model)\n\n\ndef get_waiter_model_from_service_model(session, service_model):\n    try:\n        model = session.get_waiter_model(service_model.service_name,\n                                         service_model.api_version)\n    except DataNotFoundError:\n        return None\n    return model\n\n\nclass WaitCommand(BasicCommand):\n    NAME = 'wait'\n    DESCRIPTION = ('Wait until a particular condition is satisfied. Each '\n                  'subcommand polls an API until the listed requirement '\n                  'is met.')\n\n    def __init__(self, session, waiter_model, service_model):\n        self._model = waiter_model\n        self._service_model = service_model\n        self.waiter_cmd_builder = WaiterStateCommandBuilder(\n            session=session,\n            model=self._model,\n            service_model=self._service_model\n        )\n        super(WaitCommand, self).__init__(session)\n\n    def _run_main(self, parsed_args, parsed_globals):\n        if parsed_args.subcommand is None:\n            raise ValueError(\"usage: aws [options] <command> <subcommand> \"\n                             \"[parameters]\\naws: error: too few arguments\")\n\n    def _build_subcommand_table(self):\n        subcommand_table = super(WaitCommand, self)._build_subcommand_table()\n        self.waiter_cmd_builder.build_all_waiter_state_cmds(subcommand_table)\n        self._add_lineage(subcommand_table)\n        return subcommand_table\n\n    def create_help_command(self):\n        return BasicHelp(self._session, self,\n                         command_table=self.subcommand_table,\n                         arg_table=self.arg_table,\n                         event_handler_class=WaiterCommandDocHandler)\n\n\nclass WaiterStateCommandBuilder(object):\n    def __init__(self, session, model, service_model):\n        self._session = session\n        self._model = model\n        self._service_model = service_model\n\n    def build_all_waiter_state_cmds(self, subcommand_table):\n        \"\"\"This adds waiter state commands to the subcommand table passed in.\n\n        This is the method that adds waiter state commands like\n        ``instance-running`` to ``ec2 wait``.\n        \"\"\"\n        waiter_names = self._model.waiter_names\n        for waiter_name in waiter_names:\n            waiter_cli_name = xform_name(waiter_name, '-')\n            subcommand_table[waiter_cli_name] = \\\n                self._build_waiter_state_cmd(waiter_name)\n\n    def _build_waiter_state_cmd(self, waiter_name):\n        # Get the waiter\n        waiter_config = self._model.get_waiter(waiter_name)\n\n        # Create the cli name for the waiter operation\n        waiter_cli_name = xform_name(waiter_name, '-')\n\n        # Obtain the name of the service operation that is used to implement\n        # the specified waiter.\n        operation_name = waiter_config.operation\n\n        # Create an operation object to make a command for the waiter. The\n        # operation object is used to generate the arguments for the waiter\n        # state command.\n        operation_model = self._service_model.operation_model(operation_name)\n\n        waiter_state_command = WaiterStateCommand(\n            name=waiter_cli_name, parent_name='wait',\n            operation_caller=WaiterCaller(self._session, waiter_name),\n            session=self._session,\n            operation_model=operation_model,\n        )\n        # Build the top level description for the waiter state command.\n        # Most waiters do not have a description so they need to be generated\n        # using the waiter configuration.\n        waiter_state_doc_builder = WaiterStateDocBuilder(waiter_config)\n        description = waiter_state_doc_builder.build_waiter_state_description()\n        waiter_state_command.DESCRIPTION = description\n        return waiter_state_command\n\n\nclass WaiterStateDocBuilder(object):\n    SUCCESS_DESCRIPTIONS = {\n        'error': u'%s is thrown ',\n        'path': u'%s ',\n        'pathAll': u'%s for all elements ',\n        'pathAny': u'%s for any element ',\n        'status': u'%s response is received '\n    }\n\n    def __init__(self, waiter_config):\n        self._waiter_config = waiter_config\n\n    def build_waiter_state_description(self):\n        description = self._waiter_config.description\n        # Use the description provided in the waiter config file. If no\n        # description is provided, use a heuristic to generate a description\n        # for the waiter.\n        if not description:\n            description = u'Wait until '\n            # Look at all of the acceptors and find the success state\n            # acceptor.\n            for acceptor in self._waiter_config.acceptors:\n                # Build the description off of the success acceptor.\n                if acceptor.state == 'success':\n                    description += self._build_success_description(acceptor)\n                    break\n            # Include what operation is being used.\n            description += self._build_operation_description(\n                self._waiter_config.operation)\n        description += self._build_polling_description(\n            self._waiter_config.delay, self._waiter_config.max_attempts)\n        return description\n\n    def _build_success_description(self, acceptor):\n        matcher = acceptor.matcher\n        # Pick the description template to use based on what the matcher is.\n        success_description = self.SUCCESS_DESCRIPTIONS[matcher]\n        resource_description = None\n        # If success is based off of the state of a resource include the\n        # description about what resource is looked at.\n        if matcher in ['path', 'pathAny', 'pathAll']:\n            resource_description = u'JMESPath query %s returns ' % \\\n                acceptor.argument\n            # Prepend the resource description to the template description\n            success_description = resource_description + success_description\n        # Complete the description by filling in the expected success state.\n        full_success_description = success_description % acceptor.expected\n        return full_success_description\n\n    def _build_operation_description(self, operation):\n        operation_name = xform_name(operation).replace('_', '-')\n        return u'when polling with ``%s``.' % operation_name\n\n    def _build_polling_description(self, delay, max_attempts):\n        description = (\n            ' It will poll every %s seconds until a successful state '\n            'has been reached. This will exit with a return code of 255 '\n            'after %s failed checks.'\n            % (delay, max_attempts))\n        return description\n\n\nclass WaiterCaller(object):\n    def __init__(self, session, waiter_name):\n        self._session = session\n        self._waiter_name = waiter_name\n\n    def invoke(self, service_name, operation_name, parameters, parsed_globals):\n        client = self._session.create_client(\n            service_name, region_name=parsed_globals.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl)\n        waiter = client.get_waiter(xform_name(self._waiter_name))\n        waiter.wait(**parameters)\n        return 0\n\n\nclass WaiterStateCommand(ServiceOperation):\n    DESCRIPTION = ''\n\n    def create_help_command(self):\n        help_command = super(WaiterStateCommand, self).create_help_command()\n        # Change the operation object's description by changing it to the\n        # description for a waiter state command.\n        self._operation_model.documentation = self.DESCRIPTION\n        # Change the output shape because waiters provide no output.\n        self._operation_model.output_shape = None\n        return help_command\n\n\nclass WaiterCommandDocHandler(BasicDocHandler):\n    def doc_synopsis_start(self, help_command, **kwargs):\n        pass\n\n    def doc_synopsis_option(self, arg_name, help_command, **kwargs):\n        pass\n\n    def doc_synopsis_end(self, help_command, **kwargs):\n        pass\n\n    def doc_options_start(self, help_command, **kwargs):\n        pass\n\n    def doc_options_end(self, help_command, **kwargs):\n        pass\n\n    def doc_option(self, arg_name, help_command, **kwargs):\n        pass\n", "awscli/customizations/opsworks.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\nimport json\nimport logging\nimport os\nimport platform\nimport re\nimport shlex\nimport socket\nimport subprocess\nimport tempfile\nimport textwrap\n\nfrom botocore.exceptions import ClientError\n\nfrom awscli.compat import urlopen, ensure_text_type\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.utils import create_client_from_parsed_globals\n\n\nLOG = logging.getLogger(__name__)\n\nIAM_USER_POLICY_NAME = \"OpsWorks-Instance\"\nIAM_USER_POLICY_TIMEOUT = datetime.timedelta(minutes=15)\nIAM_PATH = '/AWS/OpsWorks/'\nIAM_POLICY_ARN = 'arn:aws:iam::aws:policy/AWSOpsWorksInstanceRegistration'\n\nHOSTNAME_RE = re.compile(r\"^(?!-)[a-z0-9-]{1,63}(?<!-)$\", re.I)\nINSTANCE_ID_RE = re.compile(r\"^i-[0-9a-f]+$\")\nIP_ADDRESS_RE = re.compile(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+$\")\n\nIDENTITY_URL = \\\n    \"http://169.254.169.254/latest/dynamic/instance-identity/document\"\n\nREMOTE_SCRIPT = \"\"\"\nset -e\numask 007\nAGENT_TMP_DIR=$(mktemp -d /tmp/opsworks-agent-installer.XXXXXXXXXXXXXXXX)\ncurl --retry 5 -L %(agent_installer_url)s | tar xz -C $AGENT_TMP_DIR\ncat >$AGENT_TMP_DIR/opsworks-agent-installer/preconfig <<EOF\n%(preconfig)s\nEOF\nexec sudo /bin/sh -c \"\\\nOPSWORKS_ASSETS_DOWNLOAD_BUCKET=%(assets_download_bucket)s \\\n$AGENT_TMP_DIR/opsworks-agent-installer/boot-registration; \\\nrm -rf $AGENT_TMP_DIR\"\n\"\"\".lstrip()\n\n\ndef initialize(cli):\n    cli.register('building-command-table.opsworks', inject_commands)\n\n\ndef inject_commands(command_table, session, **kwargs):\n    command_table['register'] = OpsWorksRegister(session)\n\n\nclass OpsWorksRegister(BasicCommand):\n    NAME = \"register\"\n    DESCRIPTION = textwrap.dedent(\"\"\"\n        Registers an EC2 instance or machine with AWS OpsWorks.\n\n        Registering a machine using this command will install the AWS OpsWorks\n        agent on the target machine and register it with an existing OpsWorks\n        stack.\n    \"\"\").strip()\n\n    ARG_TABLE = [\n        {'name': 'stack-id', 'required': True,\n         'help_text': \"\"\"A stack ID. The instance will be registered with the\n                         given stack.\"\"\"},\n        {'name': 'infrastructure-class', 'required': True,\n         'choices': ['ec2', 'on-premises'],\n         'help_text': \"\"\"Specifies whether to register an EC2 instance (`ec2`)\n                         or an on-premises instance (`on-premises`).\"\"\"},\n        {'name': 'override-hostname', 'dest': 'hostname',\n         'help_text': \"\"\"The instance hostname. If not provided, the current\n                         hostname of the machine will be used.\"\"\"},\n        {'name': 'override-private-ip', 'dest': 'private_ip',\n         'help_text': \"\"\"An IP address. If you set this parameter, the given IP\n                         address will be used as the private IP address within\n                         OpsWorks.  Otherwise the private IP address will be\n                         determined automatically. Not to be used with EC2\n                         instances.\"\"\"},\n        {'name': 'override-public-ip', 'dest': 'public_ip',\n         'help_text': \"\"\"An IP address. If you set this parameter, the given IP\n                         address will be used as the public IP address within\n                         OpsWorks.  Otherwise the public IP address will be\n                         determined automatically. Not to be used with EC2\n                         instances.\"\"\"},\n        {'name': 'override-ssh', 'dest': 'ssh',\n         'help_text': \"\"\"If you set this parameter, the given command will be\n                         used to connect to the machine.\"\"\"},\n        {'name': 'ssh-username', 'dest': 'username',\n         'help_text': \"\"\"If provided, this username will be used to connect to\n                         the host.\"\"\"},\n        {'name': 'ssh-private-key', 'dest': 'private_key',\n         'help_text': \"\"\"If provided, the given private key file will be used\n                         to connect to the machine.\"\"\"},\n        {'name': 'local', 'action': 'store_true',\n         'help_text': \"\"\"If given, instead of a remote machine, the local\n                         machine will be imported. Cannot be used together\n                         with `target`.\"\"\"},\n        {'name': 'use-instance-profile', 'action': 'store_true',\n         'help_text': \"\"\"Use the instance profile instead of creating an IAM\n                         user.\"\"\"},\n        {'name': 'target', 'positional_arg': True, 'nargs': '?',\n         'synopsis': '[<target>]',\n         'help_text': \"\"\"Either the EC2 instance ID or the hostname of the\n                         instance or machine to be registered with OpsWorks.\n                         Cannot be used together with `--local`.\"\"\"},\n    ]\n\n    def __init__(self, session):\n        super(OpsWorksRegister, self).__init__(session)\n        self._stack = None\n        self._ec2_instance = None\n        self._prov_params = None\n        self._use_address = None\n        self._use_hostname = None\n        self._name_for_iam = None\n        self.access_key = None\n\n    def _create_clients(self, args, parsed_globals):\n        self.iam = self._session.create_client('iam')\n        self.opsworks = create_client_from_parsed_globals(\n            self._session, 'opsworks', parsed_globals)\n\n    def _run_main(self, args, parsed_globals):\n        self._create_clients(args, parsed_globals)\n\n        self.prevalidate_arguments(args)\n        self.retrieve_stack(args)\n        self.validate_arguments(args)\n        self.determine_details(args)\n        self.create_iam_entities(args)\n        self.setup_target_machine(args)\n\n    def prevalidate_arguments(self, args):\n        \"\"\"\n        Validates command line arguments before doing anything else.\n        \"\"\"\n        if not args.target and not args.local:\n            raise ValueError(\"One of target or --local is required.\")\n        elif args.target and args.local:\n            raise ValueError(\n                \"Arguments target and --local are mutually exclusive.\")\n\n        if args.local and platform.system() != 'Linux':\n            raise ValueError(\n                \"Non-Linux instances are not supported by AWS OpsWorks.\")\n\n        if args.ssh and (args.username or args.private_key):\n            raise ValueError(\n                \"Argument --override-ssh cannot be used together with \"\n                \"--ssh-username or --ssh-private-key.\")\n\n        if args.infrastructure_class == 'ec2':\n            if args.private_ip:\n                raise ValueError(\n                    \"--override-private-ip is not supported for EC2.\")\n            if args.public_ip:\n                raise ValueError(\n                    \"--override-public-ip is not supported for EC2.\")\n\n        if args.infrastructure_class == 'on-premises' and \\\n                args.use_instance_profile:\n            raise ValueError(\n                \"--use-instance-profile is only supported for EC2.\")\n\n        if args.hostname:\n            if not HOSTNAME_RE.match(args.hostname):\n                raise ValueError(\n                    \"Invalid hostname: '%s'. Hostnames must consist of \"\n                    \"letters, digits and dashes only and must not start or \"\n                    \"end with a dash.\" % args.hostname)\n\n    def retrieve_stack(self, args):\n        \"\"\"\n        Retrieves the stack from the API, thereby ensures that it exists.\n\n        Provides `self._stack`, `self._prov_params`, `self._use_address`, and\n        `self._ec2_instance`.\n        \"\"\"\n\n        LOG.debug(\"Retrieving stack and provisioning parameters\")\n        self._stack = self.opsworks.describe_stacks(\n            StackIds=[args.stack_id]\n        )['Stacks'][0]\n        self._prov_params = \\\n            self.opsworks.describe_stack_provisioning_parameters(\n                StackId=self._stack['StackId']\n            )\n\n        if args.infrastructure_class == 'ec2' and not args.local:\n            LOG.debug(\"Retrieving EC2 instance information\")\n            ec2 = self._session.create_client(\n                'ec2', region_name=self._stack['Region'])\n\n            # `desc_args` are arguments for the describe_instances call,\n            # whereas `conditions` is a list of lambdas for further filtering\n            # on the results of the call.\n            desc_args = {'Filters': []}\n            conditions = []\n\n            # make sure that the platforms (EC2/VPC) and VPC IDs of the stack\n            # and the instance match\n            if 'VpcId' in self._stack:\n                desc_args['Filters'].append(\n                    {'Name': 'vpc-id', 'Values': [self._stack['VpcId']]}\n                )\n            else:\n                # Cannot search for non-VPC instances directly, thus filter\n                # afterwards\n                conditions.append(lambda instance: 'VpcId' not in instance)\n\n            # target may be an instance ID, an IP address, or a name\n            if INSTANCE_ID_RE.match(args.target):\n                desc_args['InstanceIds'] = [args.target]\n            elif IP_ADDRESS_RE.match(args.target):\n                # Cannot search for either private or public IP at the same\n                # time, thus filter afterwards\n                conditions.append(\n                    lambda instance:\n                        instance.get('PrivateIpAddress') == args.target or\n                        instance.get('PublicIpAddress') == args.target)\n                # also use the given address to connect\n                self._use_address = args.target\n            else:\n                # names are tags\n                desc_args['Filters'].append(\n                    {'Name': 'tag:Name', 'Values': [args.target]}\n                )\n\n            # find all matching instances\n            instances = [\n                i\n                for r in ec2.describe_instances(**desc_args)['Reservations']\n                for i in r['Instances']\n                if all(c(i) for c in conditions)\n            ]\n\n            if not instances:\n                raise ValueError(\n                    \"Did not find any instance matching %s.\" % args.target)\n            elif len(instances) > 1:\n                raise ValueError(\n                    \"Found multiple instances matching %s: %s.\" % (\n                        args.target,\n                        \", \".join(i['InstanceId'] for i in instances)))\n\n            self._ec2_instance = instances[0]\n\n    def validate_arguments(self, args):\n        \"\"\"\n        Validates command line arguments using the retrieved information.\n        \"\"\"\n\n        if args.hostname:\n            instances = self.opsworks.describe_instances(\n                StackId=self._stack['StackId']\n            )['Instances']\n            if any(args.hostname.lower() == instance['Hostname']\n                   for instance in instances):\n                raise ValueError(\n                    \"Invalid hostname: '%s'. Hostnames must be unique within \"\n                    \"a stack.\" % args.hostname)\n\n        if args.infrastructure_class == 'ec2' and args.local:\n            # make sure the regions match\n            region = json.loads(\n                ensure_text_type(urlopen(IDENTITY_URL).read()))['region']\n            if region != self._stack['Region']:\n                raise ValueError(\n                    \"The stack's and the instance's region must match.\")\n\n    def determine_details(self, args):\n        \"\"\"\n        Determine details (like the address to connect to and the hostname to\n        use) from the given arguments and the retrieved data.\n\n        Provides `self._use_address` (if not provided already),\n        `self._use_hostname` and `self._name_for_iam`.\n        \"\"\"\n\n        # determine the address to connect to\n        if not self._use_address:\n            if args.local:\n                pass\n            elif args.infrastructure_class == 'ec2':\n                if 'PublicIpAddress' in self._ec2_instance:\n                    self._use_address = self._ec2_instance['PublicIpAddress']\n                elif 'PrivateIpAddress' in self._ec2_instance:\n                    LOG.warning(\n                        \"Instance does not have a public IP address. Trying \"\n                        \"to use the private address to connect.\")\n                    self._use_address = self._ec2_instance['PrivateIpAddress']\n                else:\n                    # Should never happen\n                    raise ValueError(\n                        \"The instance does not seem to have an IP address.\")\n            elif args.infrastructure_class == 'on-premises':\n                self._use_address = args.target\n\n        # determine the names to use\n        if args.hostname:\n            self._use_hostname = args.hostname\n            self._name_for_iam = args.hostname\n        elif args.local:\n            self._use_hostname = None\n            self._name_for_iam = socket.gethostname()\n        else:\n            self._use_hostname = None\n            self._name_for_iam = args.target\n\n    def create_iam_entities(self, args):\n        \"\"\"\n        Creates an IAM group, user and corresponding credentials.\n\n        Provides `self.access_key`.\n        \"\"\"\n\n        if args.use_instance_profile:\n            LOG.debug(\"Skipping IAM entity creation\")\n            self.access_key = None\n            return\n\n        LOG.debug(\"Creating the IAM group if necessary\")\n        group_name = \"OpsWorks-%s\" % clean_for_iam(self._stack['StackId'])\n        try:\n            self.iam.create_group(GroupName=group_name, Path=IAM_PATH)\n            LOG.debug(\"Created IAM group %s\", group_name)\n        except ClientError as e:\n            if e.response.get('Error', {}).get('Code') == 'EntityAlreadyExists':\n                LOG.debug(\"IAM group %s exists, continuing\", group_name)\n                # group already exists, good\n                pass\n            else:\n                raise\n\n        # create the IAM user, trying alternatives if it already exists\n        LOG.debug(\"Creating an IAM user\")\n        base_username = \"OpsWorks-%s-%s\" % (\n            shorten_name(clean_for_iam(self._stack['Name']), 25),\n            shorten_name(clean_for_iam(self._name_for_iam), 25)\n        )\n        for try_ in range(20):\n            username = base_username + (\"+%s\" % try_ if try_ else \"\")\n            try:\n                self.iam.create_user(UserName=username, Path=IAM_PATH)\n            except ClientError as e:\n                if e.response.get('Error', {}).get('Code') == 'EntityAlreadyExists':\n                    LOG.debug(\n                        \"IAM user %s already exists, trying another name\",\n                        username\n                    )\n                    # user already exists, try the next one\n                    pass\n                else:\n                    raise\n            else:\n                LOG.debug(\"Created IAM user %s\", username)\n                break\n        else:\n            raise ValueError(\"Couldn't find an unused IAM user name.\")\n\n        LOG.debug(\"Adding the user to the group and attaching a policy\")\n        self.iam.add_user_to_group(GroupName=group_name, UserName=username)\n\n        try:\n            self.iam.attach_user_policy(\n                PolicyArn=IAM_POLICY_ARN,\n                UserName=username\n            )\n        except ClientError as e:\n            if e.response.get('Error', {}).get('Code') == 'AccessDenied':\n                LOG.debug(\n                    \"Unauthorized to attach policy %s to user %s. Trying \"\n                    \"to put user policy\",\n                    IAM_POLICY_ARN,\n                    username\n                )\n                self.iam.put_user_policy(\n                    PolicyName=IAM_USER_POLICY_NAME,\n                    PolicyDocument=self._iam_policy_document(\n                        self._stack['Arn'], IAM_USER_POLICY_TIMEOUT),\n                    UserName=username\n                )\n                LOG.debug(\n                    \"Put policy %s to user %s\",\n                    IAM_USER_POLICY_NAME,\n                    username\n                )\n            else:\n                raise\n        else:\n            LOG.debug(\n                \"Attached policy %s to user %s\",\n                IAM_POLICY_ARN,\n                username\n            )\n\n        LOG.debug(\"Creating an access key\")\n        self.access_key = self.iam.create_access_key(\n            UserName=username\n        )['AccessKey']\n\n    def setup_target_machine(self, args):\n        \"\"\"\n        Setups the target machine by copying over the credentials and starting\n        the installation process.\n        \"\"\"\n\n        remote_script = REMOTE_SCRIPT % {\n            'agent_installer_url':\n                self._prov_params['AgentInstallerUrl'],\n            'preconfig':\n                self._to_ruby_yaml(self._pre_config_document(args)),\n            'assets_download_bucket':\n                self._prov_params['Parameters']['assets_download_bucket']\n        }\n\n        if args.local:\n            LOG.debug(\"Running the installer locally\")\n            subprocess.check_call([\"/bin/sh\", \"-c\", remote_script])\n        else:\n            LOG.debug(\"Connecting to the target machine to run the installer.\")\n            self.ssh(args, remote_script)\n\n    def ssh(self, args, remote_script):\n        \"\"\"\n        Runs a (sh) script on a remote machine via SSH.\n        \"\"\"\n\n        if platform.system() == 'Windows':\n            try:\n                script_file = tempfile.NamedTemporaryFile(\"wt\", delete=False)\n                script_file.write(remote_script)\n                script_file.close()\n                if args.ssh:\n                    call = args.ssh\n                else:\n                    call = 'plink'\n                    if args.username:\n                        call += ' -l \"%s\"' % args.username\n                    if args.private_key:\n                        call += ' -i \"%s\"' % args.private_key\n                    call += ' \"%s\"' % self._use_address\n                    call += ' -m'\n                call += ' \"%s\"' % script_file.name\n\n                subprocess.check_call(call, shell=True)\n            finally:\n                os.remove(script_file.name)\n        else:\n            if args.ssh:\n                call = shlex.split(str(args.ssh))\n            else:\n                call = ['ssh', '-tt']\n                if args.username:\n                    call.extend(['-l', args.username])\n                if args.private_key:\n                    call.extend(['-i', args.private_key])\n                call.append(self._use_address)\n\n            remote_call = [\"/bin/sh\", \"-c\", remote_script]\n            call.append(\" \".join(shlex.quote(word) for word in remote_call))\n            subprocess.check_call(call)\n\n    def _pre_config_document(self, args):\n        parameters = dict(\n            stack_id=self._stack['StackId'],\n            **self._prov_params[\"Parameters\"]\n        )\n        if self.access_key:\n            parameters['access_key_id'] = self.access_key['AccessKeyId']\n            parameters['secret_access_key'] = \\\n                self.access_key['SecretAccessKey']\n        if self._use_hostname:\n            parameters['hostname'] = self._use_hostname\n        if args.private_ip:\n            parameters['private_ip'] = args.private_ip\n        if args.public_ip:\n            parameters['public_ip'] = args.public_ip\n        parameters['import'] = args.infrastructure_class == 'ec2'\n        LOG.debug(\"Using pre-config: %r\", parameters)\n        return parameters\n\n    @staticmethod\n    def _iam_policy_document(arn, timeout=None):\n        statement = {\n            \"Action\": \"opsworks:RegisterInstance\",\n            \"Effect\": \"Allow\",\n            \"Resource\": arn,\n        }\n        if timeout is not None:\n            valid_until = datetime.datetime.utcnow() + timeout\n            statement[\"Condition\"] = {\n                \"DateLessThan\": {\n                    \"aws:CurrentTime\":\n                        valid_until.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n                }\n            }\n        policy_document = {\n            \"Statement\": [statement],\n            \"Version\": \"2012-10-17\"\n        }\n        return json.dumps(policy_document)\n\n    @staticmethod\n    def _to_ruby_yaml(parameters):\n        return \"\\n\".join(\":%s: %s\" % (k, json.dumps(v))\n                         for k, v in sorted(parameters.items()))\n\n\ndef clean_for_iam(name):\n    \"\"\"\n    Cleans a name to fit IAM's naming requirements.\n    \"\"\"\n\n    return re.sub(r'[^A-Za-z0-9+=,.@_-]+', '-', name)\n\n\ndef shorten_name(name, max_length):\n    \"\"\"\n    Shortens a name to the given number of characters.\n    \"\"\"\n\n    if len(name) <= max_length:\n        return name\n    q, r = divmod(max_length - 3, 2)\n    return name[:q + r] + \"...\" + name[-q:]\n", "awscli/customizations/cloudsearch.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\n\nfrom awscli.customizations.flatten import FlattenArguments, SEP\nfrom botocore.compat import OrderedDict\n\nLOG = logging.getLogger(__name__)\n\nDEFAULT_VALUE_TYPE_MAP = {\n    'Int': int,\n    'Double': float,\n    'IntArray': int,\n    'DoubleArray': float\n}\n\n\ndef index_hydrate(params, container, cli_type, key, value):\n    \"\"\"\n    Hydrate an index-field option value to construct something like::\n\n        {\n            'index_field': {\n                'DoubleOptions': {\n                    'DefaultValue': 0.0\n                }\n            }\n        }\n    \"\"\"\n    if 'IndexField' not in params:\n        params['IndexField'] = {}\n\n    if 'IndexFieldType' not in params['IndexField']:\n        raise RuntimeError('You must pass the --type option.')\n\n    # Find the type and transform it for the type options field name\n    # E.g: int-array => IntArray\n    _type = params['IndexField']['IndexFieldType']\n    _type = ''.join([i.capitalize() for i in _type.split('-')])\n\n    # ``index_field`` of type ``latlon`` is mapped to ``Latlon``.\n    # However, it is defined as ``LatLon`` in the model so it needs to\n    # be changed.\n    if _type == 'Latlon':\n        _type = 'LatLon'\n\n    # Transform string value to the correct type?\n    if key.split(SEP)[-1] == 'DefaultValue':\n        value = DEFAULT_VALUE_TYPE_MAP.get(_type, lambda x: x)(value)\n\n    # Set the proper options field\n    if _type + 'Options' not in params['IndexField']:\n        params['IndexField'][_type + 'Options'] = {}\n\n    params['IndexField'][_type + 'Options'][key.split(SEP)[-1]] = value\n\n\nFLATTEN_CONFIG = {\n    \"define-expression\": {\n        \"expression\": {\n            \"keep\": False,\n            \"flatten\": OrderedDict([\n                # Order is crucial here!  We're\n                # flattening ExpressionValue to be \"expression\",\n                # but this is the name (\"expression\") of the our parent\n                # key, the top level nested param.\n                (\"ExpressionName\", {\"name\": \"name\"}),\n                (\"ExpressionValue\", {\"name\": \"expression\"}),]),\n        }\n    },\n    \"define-index-field\": {\n        \"index-field\": {\n            \"keep\": False,\n            # We use an ordered dict because `type` needs to be parsed before\n            # any of the <X>Options values.\n            \"flatten\": OrderedDict([\n                (\"IndexFieldName\", {\"name\": \"name\"}),\n                (\"IndexFieldType\", {\"name\": \"type\"}),\n                (\"IntOptions.DefaultValue\", {\"name\": \"default-value\",\n                                             \"type\": \"string\",\n                                             \"hydrate\": index_hydrate}),\n                (\"IntOptions.FacetEnabled\", {\"name\": \"facet-enabled\",\n                                             \"hydrate\": index_hydrate }),\n                (\"IntOptions.SearchEnabled\", {\"name\": \"search-enabled\",\n                                              \"hydrate\": index_hydrate}),\n                (\"IntOptions.ReturnEnabled\", {\"name\": \"return-enabled\",\n                                              \"hydrate\": index_hydrate}),\n                (\"IntOptions.SortEnabled\", {\"name\": \"sort-enabled\",\n                                            \"hydrate\": index_hydrate}),\n                (\"IntOptions.SourceField\", {\"name\": \"source-field\",\n                                            \"type\": \"string\",\n                                            \"hydrate\": index_hydrate }),\n                (\"TextOptions.HighlightEnabled\", {\"name\": \"highlight-enabled\",\n                                                  \"hydrate\": index_hydrate}),\n                (\"TextOptions.AnalysisScheme\", {\"name\": \"analysis-scheme\",\n                                                \"hydrate\": index_hydrate})\n            ])\n        }\n    }\n}\n\n\ndef initialize(cli):\n    \"\"\"\n    The entry point for CloudSearch customizations.\n    \"\"\"\n    flattened = FlattenArguments('cloudsearch', FLATTEN_CONFIG)\n    flattened.register(cli)\n", "awscli/customizations/arguments.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport re\n\nfrom awscli.arguments import CustomArgument\nfrom awscli.compat import compat_open\nimport jmespath\n\n\ndef resolve_given_outfile_path(path):\n    \"\"\"Asserts that a path is writable and returns the expanded path\"\"\"\n    if path is None:\n        return\n    outfile = os.path.expanduser(os.path.expandvars(path))\n    if not os.access(os.path.dirname(os.path.abspath(outfile)), os.W_OK):\n        raise ValueError('Unable to write to file: %s' % outfile)\n    return outfile\n\n\ndef is_parsed_result_successful(parsed_result):\n    \"\"\"Returns True if a parsed result is successful\"\"\"\n    return parsed_result['ResponseMetadata']['HTTPStatusCode'] < 300\n\n\nclass OverrideRequiredArgsArgument(CustomArgument):\n    \"\"\"An argument that if specified makes all other arguments not required\n\n    By not required, it refers to not having an error thrown when the\n    parser does not find an argument that is required on the command line.\n    To obtain this argument's property of ignoring required arguments,\n    subclass from this class and fill out the ``ARG_DATA`` parameter as\n    described below. Note this class is really only useful for subclassing.\n    \"\"\"\n\n    # ``ARG_DATA`` follows the same format as a member of ``ARG_TABLE`` in\n    # ``BasicCommand`` class as specified in\n    # ``awscli/customizations/commands.py``.\n    #\n    # For example, an ``ARG_DATA`` variable would be filled out as:\n    #\n    # ARG_DATA =\n    # {'name': 'my-argument',\n    #  'help_text': 'This is argument ensures the argument is specified'\n    #               'no other arguments are required'}\n    ARG_DATA = {'name': 'no-required-args'}\n\n    def __init__(self, session):\n        self._session = session\n        self._register_argument_action()\n        super(OverrideRequiredArgsArgument, self).__init__(**self.ARG_DATA)\n\n    def _register_argument_action(self):\n        self._session.register('before-building-argument-table-parser',\n                               self.override_required_args)\n\n    def override_required_args(self, argument_table, args, **kwargs):\n        name_in_cmdline = '--' + self.name\n        # Set all ``Argument`` objects in ``argument_table`` to not required\n        # if this argument's name is present in the command line.\n        if name_in_cmdline in args:\n            for arg_name in argument_table.keys():\n                argument_table[arg_name].required = False\n\n\nclass StatefulArgument(CustomArgument):\n    \"\"\"An argument that maintains a stateful value\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(StatefulArgument, self).__init__(*args, **kwargs)\n        self._value = None\n\n    def add_to_params(self, parameters, value):\n        super(StatefulArgument, self).add_to_params(parameters, value)\n        self._value = value\n\n    @property\n    def value(self):\n        return self._value\n\n\nclass QueryOutFileArgument(StatefulArgument):\n    \"\"\"An argument that write a JMESPath query result to a file\"\"\"\n\n    def __init__(self, session, name, query, after_call_event, perm,\n                 *args, **kwargs):\n        self._session = session\n        self._query = query\n        self._after_call_event = after_call_event\n        self._perm = perm\n        # Generate default help_text if text was not provided.\n        if 'help_text' not in kwargs:\n            kwargs['help_text'] = ('Saves the command output contents of %s '\n                                   'to the given filename' % self.query)\n        super(QueryOutFileArgument, self).__init__(name, *args, **kwargs)\n\n    @property\n    def query(self):\n        return self._query\n\n    @property\n    def perm(self):\n        return self._perm\n\n    def add_to_params(self, parameters, value):\n        value = resolve_given_outfile_path(value)\n        super(QueryOutFileArgument, self).add_to_params(parameters, value)\n        if self.value is not None:\n            # Only register the event to save the argument if it is set\n            self._session.register(self._after_call_event, self.save_query)\n\n    def save_query(self, parsed, **kwargs):\n        \"\"\"Saves the result of a JMESPath expression to a file.\n\n        This method only saves the query data if the response code of\n        the parsed result is < 300.\n        \"\"\"\n        if is_parsed_result_successful(parsed):\n            contents = jmespath.search(self.query, parsed)\n            with compat_open(\n                    self.value, 'w', access_permissions=self.perm) as fp:\n                # Don't write 'None' to a file -- write ''.\n                if contents is None:\n                    fp.write('')\n                else:\n                    fp.write(contents)\n                # Even though the file is opened using the requested mode\n                # (e.g. 0o600), the mode is only applied if a new file is\n                # created. This means if the file already exists, its\n                # permissions will not be changed. So, the os.chmod call is\n                # retained here to preserve behavior of this argument always\n                # clobbering a preexisting file's permissions to the desired\n                # mode.\n                os.chmod(self.value, self.perm)\n\n\nclass NestedBlobArgumentHoister(object):\n    \"\"\"Can be registered to update a single argument / model value combination\n    mapping that to a new top-level argument.\n    Currently limited to blob argument types as these are the only ones\n    requiring the hoist.\n    \"\"\"\n\n    def __init__(self, source_arg, source_arg_blob_member,\n                 new_arg, new_arg_doc_string, doc_string_addendum):\n        self._source_arg = source_arg\n        self._source_arg_blob_member = source_arg_blob_member\n        self._new_arg = new_arg\n        self._new_arg_doc_string = new_arg_doc_string\n        self._doc_string_addendum = doc_string_addendum\n\n    def __call__(self, session, argument_table, **kwargs):\n        if not self._valid_target(argument_table):\n            return\n        self._update_arg(\n            argument_table, self._source_arg, self._new_arg)\n\n    def _valid_target(self, argument_table):\n        # Find the source argument and check that it has a member of\n        # the same name and type.\n        if self._source_arg in argument_table:\n            arg = argument_table[self._source_arg]\n            input_model = arg.argument_model\n            member = input_model.members.get(self._source_arg_blob_member)\n            if (member is not None and\n                    member.type_name == 'blob'):\n                return True\n        return False\n\n    def _update_arg(self, argument_table, source_arg, new_arg):\n        argument_table[new_arg] = _NestedBlobArgumentParamOverwrite(\n            new_arg, source_arg, self._source_arg_blob_member,\n            help_text=self._new_arg_doc_string,\n            cli_type_name='blob')\n        argument_table[source_arg].required = False\n        argument_table[source_arg].documentation += self._doc_string_addendum\n\n\nclass _NestedBlobArgumentParamOverwrite(CustomArgument):\n    def __init__(self, new_arg, source_arg, source_arg_blob_member, **kwargs):\n        super(_NestedBlobArgumentParamOverwrite, self).__init__(\n            new_arg, **kwargs)\n        self._param_to_overwrite = _reverse_xform_name(source_arg)\n        self._source_arg_blob_member = source_arg_blob_member\n\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        param_value = {self._source_arg_blob_member: value}\n        if parameters.get(self._param_to_overwrite):\n            parameters[self._param_to_overwrite].update(param_value)\n        else:\n            parameters[self._param_to_overwrite] = param_value\n\n\ndef _upper(match):\n    return match.group(1).lstrip('-').upper()\n\n\ndef _reverse_xform_name(name):\n    return re.sub(r'(^.|-.)', _upper, name)\n", "awscli/customizations/commands.py": "import logging\nimport os\n\nfrom botocore import model\nfrom botocore.compat import OrderedDict\nfrom botocore.validate import validate_parameters\n\nimport awscli\nfrom awscli.argparser import ArgTableArgParser\nfrom awscli.argprocess import unpack_argument, unpack_cli_arg\nfrom awscli.arguments import CustomArgument, create_argument_model_from_schema\nfrom awscli.clidocs import OperationDocumentEventHandler\nfrom awscli.clidriver import CLICommand\nfrom awscli.bcdoc import docevents\nfrom awscli.help import HelpCommand\nfrom awscli.schema import SchemaTransformer\n\nLOG = logging.getLogger(__name__)\n_open = open\n\n\nclass _FromFile(object):\n\n    def __init__(self, *paths, **kwargs):\n        \"\"\"\n        ``**kwargs`` can contain a ``root_module`` argument\n        that contains the root module where the file contents\n        should be searched.  This is an optional argument, and if\n        no value is provided, will default to ``awscli``.  This means\n        that by default we look for examples in the ``awscli`` module.\n\n        \"\"\"\n        self.filename = None\n        if paths:\n            self.filename = os.path.join(*paths)\n        if 'root_module' in kwargs:\n            self.root_module = kwargs['root_module']\n        else:\n            self.root_module = awscli\n\n\nclass BasicCommand(CLICommand):\n\n    \"\"\"Basic top level command with no subcommands.\n\n    If you want to create a new command, subclass this and\n    provide the values documented below.\n\n    \"\"\"\n\n    # This is the name of your command, so if you want to\n    # create an 'aws mycommand ...' command, the NAME would be\n    # 'mycommand'\n    NAME = 'commandname'\n    # This is the description that will be used for the 'help'\n    # command.\n    DESCRIPTION = 'describe the command'\n    # This is optional, if you are fine with the default synopsis\n    # (the way all the built in operations are documented) then you\n    # can leave this empty.\n    SYNOPSIS = ''\n    # If you want to provide some hand written examples, you can do\n    # so here.  This is written in RST format.  This is optional,\n    # you don't have to provide any examples, though highly encouraged!\n    EXAMPLES = ''\n    # If your command has arguments, you can specify them here.  This is\n    # somewhat of an implementation detail, but this is a list of dicts\n    # where the dicts match the kwargs of the CustomArgument's __init__.\n    # For example, if I want to add a '--argument-one' and an\n    # '--argument-two' command, I'd say:\n    #\n    # ARG_TABLE = [\n    #     {'name': 'argument-one', 'help_text': 'This argument does foo bar.',\n    #      'action': 'store', 'required': False, 'cli_type_name': 'string',},\n    #     {'name': 'argument-two', 'help_text': 'This argument does some other thing.',\n    #      'action': 'store', 'choices': ['a', 'b', 'c']},\n    # ]\n    #\n    # A `schema` parameter option is available to accept a custom JSON\n    # structure as input. See the file `awscli/schema.py` for more info.\n    ARG_TABLE = []\n    # If you want the command to have subcommands, you can provide a list of\n    # dicts.  We use a list here because we want to allow a user to provide\n    # the order they want to use for subcommands.\n    # SUBCOMMANDS = [\n    #     {'name': 'subcommand1', 'command_class': SubcommandClass},\n    #     {'name': 'subcommand2', 'command_class': SubcommandClass2},\n    # ]\n    # The command_class must subclass from ``BasicCommand``.\n    SUBCOMMANDS = []\n\n    FROM_FILE = _FromFile\n    # You can set the DESCRIPTION, SYNOPSIS, and EXAMPLES to FROM_FILE\n    # and we'll automatically read in that data from the file.\n    # This is useful if you have a lot of content and would prefer to keep\n    # the docs out of the class definition.  For example:\n    #\n    # DESCRIPTION = FROM_FILE\n    #\n    # will set the DESCRIPTION value to the contents of\n    # awscli/examples/<command name>/_description.rst\n    # The naming conventions for these attributes are:\n    #\n    # DESCRIPTION = awscli/examples/<command name>/_description.rst\n    # SYNOPSIS = awscli/examples/<command name>/_synopsis.rst\n    # EXAMPLES = awscli/examples/<command name>/_examples.rst\n    #\n    # You can also provide a relative path and we'll load the file\n    # from the specified location:\n    #\n    # DESCRIPTION = awscli/examples/<filename>\n    #\n    # For example:\n    #\n    # DESCRIPTION = FROM_FILE('command, 'subcommand, '_description.rst')\n    # DESCRIPTION = 'awscli/examples/command/subcommand/_description.rst'\n    #\n\n    # At this point, the only other thing you have to implement is a _run_main\n    # method (see the method for more information).\n\n    def __init__(self, session):\n        self._session = session\n        self._arg_table = None\n        self._subcommand_table = None\n        self._lineage = [self]\n\n    def __call__(self, args, parsed_globals):\n        # args is the remaining unparsed args.\n        # We might be able to parse these args so we need to create\n        # an arg parser and parse them.\n        self._subcommand_table = self._build_subcommand_table()\n        self._arg_table = self._build_arg_table()\n        event = 'before-building-argument-table-parser.%s' % \\\n            \".\".join(self.lineage_names)\n        self._session.emit(event, argument_table=self._arg_table, args=args,\n                           session=self._session, parsed_globals=parsed_globals)\n        parser = ArgTableArgParser(self.arg_table, self.subcommand_table)\n        parsed_args, remaining = parser.parse_known_args(args)\n\n        # Unpack arguments\n        for key, value in vars(parsed_args).items():\n            cli_argument = None\n\n            # Convert the name to use dashes instead of underscore\n            # as these are how the parameters are stored in the\n            # `arg_table`.\n            xformed = key.replace('_', '-')\n            if xformed in self.arg_table:\n                cli_argument = self.arg_table[xformed]\n\n            value = unpack_argument(\n                self._session,\n                'custom',\n                self.name,\n                cli_argument,\n                value\n            )\n\n            # If this parameter has a schema defined, then allow plugins\n            # a chance to process and override its value.\n            if self._should_allow_plugins_override(cli_argument, value):\n                override = self._session\\\n                    .emit_first_non_none_response(\n                        'process-cli-arg.%s.%s' % ('custom', self.name),\n                        cli_argument=cli_argument, value=value, operation=None)\n\n                if override is not None:\n                    # A plugin supplied a conversion\n                    value = override\n                else:\n                    # Unpack the argument, which is a string, into the\n                    # correct Python type (dict, list, etc)\n                    value = unpack_cli_arg(cli_argument, value)\n                self._validate_value_against_schema(\n                    cli_argument.argument_model, value)\n\n            setattr(parsed_args, key, value)\n\n        if hasattr(parsed_args, 'help'):\n            self._display_help(parsed_args, parsed_globals)\n        elif getattr(parsed_args, 'subcommand', None) is None:\n            # No subcommand was specified so call the main\n            # function for this top level command.\n            if remaining:\n                raise ValueError(\"Unknown options: %s\" % ','.join(remaining))\n            return self._run_main(parsed_args, parsed_globals)\n        else:\n            return self.subcommand_table[parsed_args.subcommand](remaining,\n                                                                 parsed_globals)\n\n    def _validate_value_against_schema(self, model, value):\n        validate_parameters(value, model)\n\n    def _should_allow_plugins_override(self, param, value):\n        if (param and param.argument_model is not None and\n                value is not None):\n            return True\n        return False\n\n    def _run_main(self, parsed_args, parsed_globals):\n        # Subclasses should implement this method.\n        # parsed_globals are the parsed global args (things like region,\n        # profile, output, etc.)\n        # parsed_args are any arguments you've defined in your ARG_TABLE\n        # that are parsed.  These will come through as whatever you've\n        # provided as the 'dest' key.  Otherwise they default to the\n        # 'name' key.  For example: ARG_TABLE[0] = {\"name\": \"foo-arg\", ...}\n        # can be accessed by ``parsed_args.foo_arg``.\n        raise NotImplementedError(\"_run_main\")\n\n    def _build_subcommand_table(self):\n        subcommand_table = OrderedDict()\n        for subcommand in self.SUBCOMMANDS:\n            subcommand_name = subcommand['name']\n            subcommand_class = subcommand['command_class']\n            subcommand_table[subcommand_name] = subcommand_class(self._session)\n        self._session.emit('building-command-table.%s' % self.NAME,\n                           command_table=subcommand_table,\n                           session=self._session,\n                           command_object=self)\n        self._add_lineage(subcommand_table)\n        return subcommand_table\n\n    def _display_help(self, parsed_args, parsed_globals):\n        help_command = self.create_help_command()\n        help_command(parsed_args, parsed_globals)\n\n    def create_help_command(self):\n        command_help_table = {}\n        if self.SUBCOMMANDS:\n            command_help_table = self.create_help_command_table()\n        return BasicHelp(self._session, self, command_table=command_help_table,\n                         arg_table=self.arg_table)\n\n    def create_help_command_table(self):\n        \"\"\"\n        Create the command table into a form that can be handled by the\n        BasicDocHandler.\n        \"\"\"\n        commands = {}\n        for command in self.SUBCOMMANDS:\n            commands[command['name']] = command['command_class'](self._session)\n        self._add_lineage(commands)\n        return commands\n\n    def _build_arg_table(self):\n        arg_table = OrderedDict()\n        self._session.emit('building-arg-table.%s' % self.NAME,\n                           arg_table=self.ARG_TABLE)\n        for arg_data in self.ARG_TABLE:\n\n            # If a custom schema was passed in, create the argument_model\n            # so that it can be validated and docs can be generated.\n            if 'schema' in arg_data:\n                argument_model = create_argument_model_from_schema(\n                    arg_data.pop('schema'))\n                arg_data['argument_model'] = argument_model\n            custom_argument = CustomArgument(**arg_data)\n\n            arg_table[arg_data['name']] = custom_argument\n        return arg_table\n\n    def _add_lineage(self, command_table):\n        for command in command_table:\n            command_obj = command_table[command]\n            command_obj.lineage = self.lineage + [command_obj]\n\n    @property\n    def arg_table(self):\n        if self._arg_table is None:\n            self._arg_table = self._build_arg_table()\n        return self._arg_table\n\n    @property\n    def subcommand_table(self):\n        if self._subcommand_table is None:\n            self._subcommand_table = self._build_subcommand_table()\n        return self._subcommand_table\n\n    @classmethod\n    def add_command(cls, command_table, session, **kwargs):\n        command_table[cls.NAME] = cls(session)\n\n    @property\n    def name(self):\n        return self.NAME\n\n    @property\n    def lineage(self):\n        return self._lineage\n\n    @lineage.setter\n    def lineage(self, value):\n        self._lineage = value\n\n\nclass BasicHelp(HelpCommand):\n\n    def __init__(self, session, command_object, command_table, arg_table,\n                 event_handler_class=None):\n        super(BasicHelp, self).__init__(session, command_object,\n                                        command_table, arg_table)\n        # This is defined in HelpCommand so we're matching the\n        # casing here.\n        if event_handler_class is None:\n            event_handler_class = BasicDocHandler\n        self.EventHandlerClass = event_handler_class\n\n        # These are public attributes that are mapped from the command\n        # object.  These are used by the BasicDocHandler below.\n        self._description = command_object.DESCRIPTION\n        self._synopsis = command_object.SYNOPSIS\n        self._examples = command_object.EXAMPLES\n\n    @property\n    def name(self):\n        return self.obj.NAME\n\n    @property\n    def description(self):\n        return self._get_doc_contents('_description')\n\n    @property\n    def synopsis(self):\n        return self._get_doc_contents('_synopsis')\n\n    @property\n    def examples(self):\n        return self._get_doc_contents('_examples')\n\n    @property\n    def event_class(self):\n        return '.'.join(self.obj.lineage_names)\n\n    def _get_doc_contents(self, attr_name):\n        value = getattr(self, attr_name)\n        if isinstance(value, BasicCommand.FROM_FILE):\n            if value.filename is not None:\n                trailing_path = value.filename\n            else:\n                trailing_path = os.path.join(self.name, attr_name + '.rst')\n            root_module = value.root_module\n            doc_path = os.path.join(\n                os.path.abspath(os.path.dirname(root_module.__file__)),\n                'examples', trailing_path)\n            with _open(doc_path) as f:\n                return f.read()\n        else:\n            return value\n\n    def __call__(self, args, parsed_globals):\n        # Create an event handler for a Provider Document\n        instance = self.EventHandlerClass(self)\n        # Now generate all of the events for a Provider document.\n        # We pass ourselves along so that we can, in turn, get passed\n        # to all event handlers.\n        docevents.generate_events(self.session, self)\n        self.renderer.render(self.doc.getvalue())\n        instance.unregister()\n\n\nclass BasicDocHandler(OperationDocumentEventHandler):\n\n    def __init__(self, help_command):\n        super(BasicDocHandler, self).__init__(help_command)\n        self.doc = help_command.doc\n\n    def doc_description(self, help_command, **kwargs):\n        self.doc.style.h2('Description')\n        self.doc.write(help_command.description)\n        self.doc.style.new_paragraph()\n\n    def doc_synopsis_start(self, help_command, **kwargs):\n        if not help_command.synopsis:\n            super(BasicDocHandler, self).doc_synopsis_start(\n                help_command=help_command, **kwargs)\n        else:\n            self.doc.style.h2('Synopsis')\n            self.doc.style.start_codeblock()\n            self.doc.writeln(help_command.synopsis)\n\n    def doc_synopsis_option(self, arg_name, help_command, **kwargs):\n        if not help_command.synopsis:\n            doc = help_command.doc\n            argument = help_command.arg_table[arg_name]\n            if argument.synopsis:\n                option_str = argument.synopsis\n            elif argument.group_name in self._arg_groups:\n                if argument.group_name in self._documented_arg_groups:\n                    # This arg is already documented so we can move on.\n                    return\n                option_str = ' | '.join(\n                    [a.cli_name for a in\n                     self._arg_groups[argument.group_name]])\n                self._documented_arg_groups.append(argument.group_name)\n            elif argument.cli_type_name == 'boolean':\n                option_str = '%s' % argument.cli_name\n            elif argument.nargs == '+':\n                option_str = \"%s <value> [<value>...]\" % argument.cli_name\n            else:\n                option_str = '%s <value>' % argument.cli_name\n            if not (argument.required or argument.positional_arg):\n                option_str = '[%s]' % option_str\n            doc.writeln('%s' % option_str)\n\n        else:\n            # A synopsis has been provided so we don't need to write\n            # anything here.\n            pass\n\n    def doc_synopsis_end(self, help_command, **kwargs):\n        if not help_command.synopsis and not help_command.command_table:\n            super(BasicDocHandler, self).doc_synopsis_end(\n                help_command=help_command, **kwargs)\n        else:\n            self.doc.style.end_codeblock()\n\n    def doc_global_option(self, help_command, **kwargs):\n        if not help_command.command_table:\n            super().doc_global_option(help_command, **kwargs)\n\n    def doc_examples(self, help_command, **kwargs):\n        if help_command.examples:\n            self.doc.style.h2('Examples')\n            self.doc.write(help_command.examples)\n\n    def doc_subitems_start(self, help_command, **kwargs):\n        if help_command.command_table:\n            doc = help_command.doc\n            doc.style.h2('Available Commands')\n            doc.style.toctree()\n\n    def doc_subitem(self, command_name, help_command, **kwargs):\n        if help_command.command_table:\n            doc = help_command.doc\n            doc.style.tocitem(command_name)\n\n    def doc_subitems_end(self, help_command, **kwargs):\n        pass\n\n    def doc_output(self, help_command, event_name, **kwargs):\n        pass\n", "awscli/customizations/sessionmanager.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport json\nimport errno\nimport os\nimport re\n\nfrom subprocess import check_call, check_output\nfrom awscli.compat import ignore_user_entered_signals\nfrom awscli.clidriver import ServiceOperation, CLIOperationCaller\n\nlogger = logging.getLogger(__name__)\n\nERROR_MESSAGE = (\n    'SessionManagerPlugin is not found. ',\n    'Please refer to SessionManager Documentation here: ',\n    'http://docs.aws.amazon.com/console/systems-manager/',\n    'session-manager-plugin-not-found'\n)\n\n\ndef register_ssm_session(event_handlers):\n    event_handlers.register('building-command-table.ssm',\n                            add_custom_start_session)\n\n\ndef add_custom_start_session(session, command_table, **kwargs):\n    command_table['start-session'] = StartSessionCommand(\n        name='start-session',\n        parent_name='ssm',\n        session=session,\n        operation_model=session.get_service_model(\n            'ssm').operation_model('StartSession'),\n        operation_caller=StartSessionCaller(session),\n    )\n\n\nclass VersionRequirement:\n    WHITESPACE_REGEX = re.compile(r\"\\s+\")\n    SSM_SESSION_PLUGIN_VERSION_REGEX = re.compile(r\"^\\d+(\\.\\d+){0,3}$\")\n\n    def __init__(self, min_version):\n        self.min_version = min_version\n\n    def meets_requirement(self, version):\n        ssm_plugin_version = self._sanitize_plugin_version(version)\n        if self._is_valid_version(ssm_plugin_version):\n            norm_version, norm_min_version = self._normalize(\n                ssm_plugin_version, self.min_version\n            )\n            return norm_version > norm_min_version\n        else:\n            return False\n\n    def _sanitize_plugin_version(self, plugin_version):\n        return re.sub(self.WHITESPACE_REGEX, \"\", plugin_version)\n\n    def _is_valid_version(self, plugin_version):\n        return bool(\n            self.SSM_SESSION_PLUGIN_VERSION_REGEX.match(plugin_version)\n        )\n\n    def _normalize(self, v1, v2):\n        v1_parts = [int(v) for v in v1.split(\".\")]\n        v2_parts = [int(v) for v in v2.split(\".\")]\n        while len(v1_parts) != len(v2_parts):\n            if len(v1_parts) - len(v2_parts) > 0:\n                v2_parts.append(0)\n            else:\n                v1_parts.append(0)\n        return v1_parts, v2_parts\n\n\nclass StartSessionCommand(ServiceOperation):\n    def create_help_command(self):\n        help_command = super(\n            StartSessionCommand, self).create_help_command()\n        # Change the output shape because the command provides no output.\n        self._operation_model.output_shape = None\n        return help_command\n\n\nclass StartSessionCaller(CLIOperationCaller):\n    LAST_PLUGIN_VERSION_WITHOUT_ENV_VAR = \"1.2.497.0\"\n    DEFAULT_SSM_ENV_NAME = \"AWS_SSM_START_SESSION_RESPONSE\"\n\n    def invoke(self, service_name, operation_name, parameters,\n               parsed_globals):\n        client = self._session.create_client(\n            service_name, region_name=parsed_globals.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl)\n        response = client.start_session(**parameters)\n        session_id = response['SessionId']\n        region_name = client.meta.region_name\n        # Profile_name is used to passed on to session manager plugin\n        # to fetch same profile credentials to make an api call in the plugin.\n        # If --profile flag is configured, pass it to Session Manager plugin.\n        # If not, set empty string.\n        profile_name = parsed_globals.profile \\\n            if parsed_globals.profile is not None else ''\n        endpoint_url = client.meta.endpoint_url\n        ssm_env_name = self.DEFAULT_SSM_ENV_NAME\n\n        try:\n            session_parameters = {\n                \"SessionId\": response[\"SessionId\"],\n                \"TokenValue\": response[\"TokenValue\"],\n                \"StreamUrl\": response[\"StreamUrl\"],\n            }\n            start_session_response = json.dumps(session_parameters)\n\n            plugin_version = check_output(\n                [\"session-manager-plugin\", \"--version\"], text=True\n            )\n            env = os.environ.copy()\n\n            # Check if this plugin supports passing the start session response\n            # as an environment variable name. If it does, it will set the\n            # value to the response from the start_session operation to the env\n            # variable defined in DEFAULT_SSM_ENV_NAME. If the session plugin\n            # version is invalid or older than the version defined in\n            # LAST_PLUGIN_VERSION_WITHOUT_ENV_VAR, it will fall back to\n            # passing the start_session response directly.\n            version_requirement = VersionRequirement(\n                min_version=self.LAST_PLUGIN_VERSION_WITHOUT_ENV_VAR\n            )\n            if version_requirement.meets_requirement(plugin_version):\n                env[ssm_env_name] = start_session_response\n                start_session_response = ssm_env_name\n            # ignore_user_entered_signals ignores these signals\n            # because if signals which kills the process are not\n            # captured would kill the foreground process but not the\n            # background one. Capturing these would prevents process\n            # from getting killed and these signals are input to plugin\n            # and handling in there\n            with ignore_user_entered_signals():\n                # call executable with necessary input\n                check_call([\"session-manager-plugin\",\n                            start_session_response,\n                            region_name,\n                            \"StartSession\",\n                            profile_name,\n                            json.dumps(parameters),\n                            endpoint_url], env=env)\n\n            return 0\n        except OSError as ex:\n            if ex.errno == errno.ENOENT:\n                logger.debug('SessionManagerPlugin is not present',\n                             exc_info=True)\n                # start-session api call returns response and starts the\n                # session on ssm-agent and response is forwarded to\n                # session-manager-plugin. If plugin is not present, terminate\n                # is called so that service and ssm-agent terminates the\n                # session to avoid zombie session active on ssm-agent for\n                # default self terminate time\n                client.terminate_session(SessionId=session_id)\n                raise ValueError(''.join(ERROR_MESSAGE))\n", "awscli/customizations/__init__.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nCustomizations\n==============\n\nAs we start to accumulate more and more of these *built-in* customizations\nwe probably need to come up with some way to organize them and to make\nit easy to add them and register them.\n\nOne idea I had was to place them all with a package like this.  That\nat least keeps them all in one place.  Each module in this package\nshould contain a single customization (I think).\n\nTo take it a step further, we could have each module define a couple\nof well-defined attributes:\n\n* ``EVENT`` would be a string containing the event that this customization\n  needs to be registered with.  Or, perhaps this should be a list of\n  events?\n* ``handler`` is a callable that will be registered as the handler\n  for the event.\n\nUsing a convention like this, we could perhaps automatically discover\nall customizations and register them without having to manually edit\n``handlers.py`` each time.\n\"\"\"\n", "awscli/customizations/rds.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nThis customization splits the modify-option-group into two separate commands:\n\n* ``add-option-group``\n* ``remove-option-group``\n\nIn both commands the ``--options-to-remove`` and ``--options-to-add`` args will\nbe renamed to just ``--options``.\n\nAll the remaining args will be available in both commands (which proxy\nmodify-option-group).\n\n\"\"\"\n\nfrom awscli.clidriver import ServiceOperation\nfrom awscli.clidriver import CLIOperationCaller\nfrom awscli.customizations import utils\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.utils import uni_print\n\n\ndef register_rds_modify_split(cli):\n    cli.register('building-command-table.rds', _building_command_table)\n    cli.register('building-argument-table.rds.add-option-to-option-group',\n                 _rename_add_option)\n    cli.register('building-argument-table.rds.remove-option-from-option-group',\n                 _rename_remove_option)\n\n\ndef register_add_generate_db_auth_token(cli):\n    cli.register('building-command-table.rds', _add_generate_db_auth_token)\n\n\ndef _add_generate_db_auth_token(command_table, session, **kwargs):\n    command = GenerateDBAuthTokenCommand(session)\n    command_table['generate-db-auth-token'] = command\n\n\ndef _rename_add_option(argument_table, **kwargs):\n    utils.rename_argument(argument_table, 'options-to-include',\n                          new_name='options')\n    del argument_table['options-to-remove']\n\n\ndef _rename_remove_option(argument_table, **kwargs):\n    utils.rename_argument(argument_table, 'options-to-remove',\n                          new_name='options')\n    del argument_table['options-to-include']\n\n\ndef _building_command_table(command_table, session, **kwargs):\n    # Hooked up to building-command-table.rds\n    # We don't need the modify-option-group operation.\n    del command_table['modify-option-group']\n    # We're going to replace modify-option-group with two commands:\n    # add-option-group and remove-option-group\n    rds_model = session.get_service_model('rds')\n    modify_operation_model = rds_model.operation_model('ModifyOptionGroup')\n    command_table['add-option-to-option-group'] = ServiceOperation(\n        parent_name='rds', name='add-option-to-option-group',\n        operation_caller=CLIOperationCaller(session),\n        session=session,\n        operation_model=modify_operation_model)\n    command_table['remove-option-from-option-group'] = ServiceOperation(\n        parent_name='rds', name='remove-option-from-option-group',\n        session=session,\n        operation_model=modify_operation_model,\n        operation_caller=CLIOperationCaller(session))\n\n\nclass GenerateDBAuthTokenCommand(BasicCommand):\n    NAME = 'generate-db-auth-token'\n    DESCRIPTION = (\n        'Generates an auth token used to connect to a db with IAM credentials.'\n    )\n    ARG_TABLE = [\n        {'name': 'hostname', 'required': True,\n         'help_text': 'The hostname of the database to connect to.'},\n        {'name': 'port', 'cli_type_name': 'integer', 'required': True,\n         'help_text': 'The port number the database is listening on.'},\n        {'name': 'username', 'required': True,\n         'help_text': 'The username to log in as.'}\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        rds = self._session.create_client(\n            'rds',\n            region_name=parsed_globals.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl\n        )\n        token = rds.generate_db_auth_token(\n            DBHostname=parsed_args.hostname,\n            Port=parsed_args.port,\n            DBUsername=parsed_args.username\n        )\n        uni_print(token)\n        uni_print('\\n')\n        return 0\n", "awscli/customizations/removals.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nRemove deprecated commands\n--------------------------\n\nThis customization removes commands that are either deprecated or not\nyet fully supported.\n\n\"\"\"\nimport logging\nfrom functools import partial\n\nLOG = logging.getLogger(__name__)\n\n\ndef register_removals(event_handler):\n    cmd_remover = CommandRemover(event_handler)\n    cmd_remover.remove(on_event='building-command-table.ses',\n                       remove_commands=['delete-verified-email-address',\n                                        'list-verified-email-addresses',\n                                        'verify-email-address'])\n    cmd_remover.remove(on_event='building-command-table.ec2',\n                       remove_commands=['import-instance', 'import-volume'])\n    cmd_remover.remove(on_event='building-command-table.emr',\n                       remove_commands=['run-job-flow', 'describe-job-flows',\n                                        'add-job-flow-steps',\n                                        'terminate-job-flows',\n                                        'list-bootstrap-actions',\n                                        'list-instance-groups',\n                                        'set-termination-protection',\n                                        'set-keep-job-flow-alive-when-no-steps',\n                                        'set-visible-to-all-users',\n                                        'set-unhealthy-node-replacement'])\n    cmd_remover.remove(on_event='building-command-table.kinesis',\n                       remove_commands=['subscribe-to-shard'])\n    cmd_remover.remove(on_event='building-command-table.lexv2-runtime',\n                         remove_commands=['start-conversation'])\n    cmd_remover.remove(on_event='building-command-table.lambda',\n                         remove_commands=['invoke-with-response-stream'])\n    cmd_remover.remove(on_event='building-command-table.sagemaker-runtime',\n                         remove_commands=['invoke-endpoint-with-response-stream'])\n    cmd_remover.remove(on_event='building-command-table.bedrock-runtime',\n                         remove_commands=['invoke-model-with-response-stream',\n                                          'converse-stream'])\n    cmd_remover.remove(on_event='building-command-table.bedrock-agent-runtime',\n                         remove_commands=['invoke-agent'])\n    cmd_remover.remove(on_event='building-command-table.qbusiness',\n                        remove_commands=['chat'])\n\n\nclass CommandRemover(object):\n    def __init__(self, events):\n        self._events = events\n\n    def remove(self, on_event, remove_commands):\n        self._events.register(on_event,\n                              self._create_remover(remove_commands))\n\n    def _create_remover(self, commands_to_remove):\n        return partial(_remove_commands, commands_to_remove=commands_to_remove)\n\n\ndef _remove_commands(command_table, commands_to_remove, **kwargs):\n    # Hooked up to building-command-table.<service>\n    for command in commands_to_remove:\n        try:\n            LOG.debug(\"Removing operation: %s\", command)\n            del command_table[command]\n        except KeyError:\n            LOG.warning(\"Attempting to delete command that does not exist: %s\",\n                        command)\n", "awscli/customizations/sagemaker.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations.utils import make_hidden_command_alias\n\n\ndef register_alias_sagemaker_runtime_command(event_emitter):\n    event_emitter.register(\n        'building-command-table.main',\n        alias_sagemaker_runtime_command\n    )\n\n\ndef alias_sagemaker_runtime_command(command_table, **kwargs):\n    make_hidden_command_alias(\n        command_table,\n        existing_name='sagemaker-runtime',\n        alias_name='runtime.sagemaker',\n    )\n", "awscli/customizations/iamvirtmfa.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nThis customization makes it easier to deal with the bootstrapping\ndata returned by the ``iam create-virtual-mfa-device`` command.\nYou can choose to bootstrap via a QRCode or via a Base32String.\nYou specify your choice via the ``--bootstrap-method`` option\nwhich should be either \"QRCodePNG\" or \"Base32StringSeed\".  You\nthen specify the path to where you would like your bootstrapping\ndata saved using the ``--outfile`` option.  The command will\npull the appropriate data field out of the response and write it\nto the specified file.  It will also remove the two bootstrap data\nfields from the response.\n\"\"\"\nimport base64\n\nfrom awscli.customizations.arguments import StatefulArgument\nfrom awscli.customizations.arguments import resolve_given_outfile_path\nfrom awscli.customizations.arguments import is_parsed_result_successful\n\n\nCHOICES = ('QRCodePNG', 'Base32StringSeed')\nOUTPUT_HELP = ('The output path and file name where the bootstrap '\n               'information will be stored.')\nBOOTSTRAP_HELP = ('Method to use to seed the virtual MFA.  '\n                  'Valid values are: %s | %s' % CHOICES)\n\n\nclass FileArgument(StatefulArgument):\n\n    def add_to_params(self, parameters, value):\n        # Validate the file here so we can raise an error prior\n        # calling the service.\n        value = resolve_given_outfile_path(value)\n        super(FileArgument, self).add_to_params(parameters, value)\n\n\nclass IAMVMFAWrapper(object):\n\n    def __init__(self, event_handler):\n        self._event_handler = event_handler\n        self._outfile = FileArgument(\n            'outfile', help_text=OUTPUT_HELP, required=True)\n        self._method = StatefulArgument(\n            'bootstrap-method', help_text=BOOTSTRAP_HELP,\n            choices=CHOICES, required=True)\n        self._event_handler.register(\n            'building-argument-table.iam.create-virtual-mfa-device',\n            self._add_options)\n        self._event_handler.register(\n            'after-call.iam.CreateVirtualMFADevice', self._save_file)\n\n    def _add_options(self, argument_table, **kwargs):\n        argument_table['outfile'] = self._outfile\n        argument_table['bootstrap-method'] = self._method\n\n    def _save_file(self, parsed, **kwargs):\n        if not is_parsed_result_successful(parsed):\n            return\n        method = self._method.value\n        outfile = self._outfile.value\n        if method in parsed['VirtualMFADevice']:\n            body = parsed['VirtualMFADevice'][method]\n            with open(outfile, 'wb') as fp:\n                fp.write(base64.b64decode(body))\n            for choice in CHOICES:\n                if choice in parsed['VirtualMFADevice']:\n                    del parsed['VirtualMFADevice'][choice]\n", "awscli/customizations/rekognition.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.arguments import NestedBlobArgumentHoister\n\nIMAGE_FILE_DOCSTRING = ('<p>The content of the image to be uploaded. '\n                        'To specify the content of a local file use the '\n                        'fileb:// prefix. '\n                        'Example: fileb://image.png</p>')\nIMAGE_DOCSTRING_ADDENDUM = ('<p>To specify a local file use <code>--%s</code> '\n                            'instead.</p>')\n\n\nFILE_PARAMETER_UPDATES = {\n    'compare-faces.source-image': 'source-image-bytes',\n    'compare-faces.target-image': 'target-image-bytes',\n    '*.image': 'image-bytes',\n}\n\n\ndef register_rekognition_detect_labels(cli):\n    for target, new_param in FILE_PARAMETER_UPDATES.items():\n        operation, old_param = target.rsplit('.', 1)\n        doc_string_addendum = IMAGE_DOCSTRING_ADDENDUM % new_param\n        cli.register('building-argument-table.rekognition.%s' % operation,\n                     NestedBlobArgumentHoister(\n                         source_arg=old_param,\n                         source_arg_blob_member='Bytes',\n                         new_arg=new_param,\n                         new_arg_doc_string=IMAGE_FILE_DOCSTRING,\n                         doc_string_addendum=doc_string_addendum))\n", "awscli/customizations/flatten.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\n\nfrom awscli.arguments import CustomArgument\n\nLOG = logging.getLogger(__name__)\n\n# Nested argument member separator\nSEP = '.'\n\n\nclass FlattenedArgument(CustomArgument):\n    \"\"\"\n    A custom argument which has been flattened from an existing structure. When\n    added to the call params it is hydrated back into the structure.\n\n    Supports both an object and a list of objects, in which case the flattened\n    parameters will hydrate a list with a single object in it.\n    \"\"\"\n    def __init__(self, name, container, prop, help_text='', required=None,\n                 type=None, hydrate=None, hydrate_value=None):\n        self.type = type\n        self._container = container\n        self._property = prop\n        self._hydrate = hydrate\n        self._hydrate_value = hydrate_value\n        super(FlattenedArgument, self).__init__(name=name, help_text=help_text,\n                                                required=required)\n\n    @property\n    def cli_type_name(self):\n        return self.type\n\n    def add_to_params(self, parameters, value):\n        \"\"\"\n        Hydrate the original structure with the value of this flattened\n        argument.\n\n        TODO: This does not hydrate nested structures (``XmlName1.XmlName2``)!\n              To do this for now you must provide your own ``hydrate`` method.\n        \"\"\"\n        container = self._container.argument_model.name\n        cli_type = self._container.cli_type_name\n        key = self._property\n\n        LOG.debug('Hydrating {0}[{1}]'.format(container, key))\n\n        if value is not None:\n            # Convert type if possible\n            if self.type == 'boolean':\n                value = not value.lower() == 'false'\n            elif self.type in ['integer', 'long']:\n                value = int(value)\n            elif self.type in ['float', 'double']:\n                value = float(value)\n\n            if self._hydrate:\n                self._hydrate(parameters, container, cli_type, key, value)\n            else:\n                if container not in parameters:\n                    if cli_type == 'list':\n                        parameters[container] = [{}]\n                    else:\n                        parameters[container] = {}\n\n                if self._hydrate_value:\n                    value = self._hydrate_value(value)\n\n                if cli_type == 'list':\n                    parameters[container][0][key] = value\n                else:\n                    parameters[container][key] = value\n\n\nclass FlattenArguments(object):\n    \"\"\"\n    Flatten arguments for one or more commands for a particular service from\n    a given configuration which maps service call parameters to flattened\n    names. Takes in a configuration dict of the form::\n\n        {\n            \"command-cli-name\": {\n                \"argument-cli-name\": {\n                    \"keep\": False,\n                    \"flatten\": {\n                        \"XmlName\": {\n                            \"name\": \"flattened-cli-name\",\n                            \"type\": \"Optional custom type\",\n                            \"required\": \"Optional custom required\",\n                            \"help_text\": \"Optional custom docs\",\n                            \"hydrate_value\": Optional function to hydrate value,\n                            \"hydrate\": Optional function to hydrate\n                        },\n                        ...\n                    }\n                },\n                ...\n            },\n            ...\n        }\n\n    The ``type``, ``required`` and ``help_text`` arguments are entirely\n    optional and by default are pulled from the model. You should only set them\n    if you wish to override the default values in the model.\n\n    The ``keep`` argument determines whether the original command is still\n    accessible vs. whether it is removed. It defaults to ``False`` if not\n    present, which removes the original argument.\n\n    The keys inside of ``flatten`` (e.g. ``XmlName`` above) can include nested\n    references to structures via a colon. For example, ``XmlName1:XmlName2``\n    for the following structure::\n\n        {\n            \"XmlName1\": {\n                \"XmlName2\": ...\n            }\n        }\n\n    The ``hydrate_value`` function takes in a value and should return a value.\n    It is only called when the value is not ``None``. Example::\n\n        \"hydrate_value\": lambda (value): value.upper()\n\n    The ``hydrate`` function takes in a list of existing parameters, the name\n    of the container, its type, the name of the container key and its set\n    value. For the example above, the container would be\n    ``'argument-cli-name'``, the key would be ``'XmlName'`` and the value\n    whatever the user passed in. Example::\n\n        def my_hydrate(params, container, cli_type, key, value):\n            if container not in params:\n                params[container] = {'default': 'values'}\n\n            params[container][key] = value\n\n    It's possible for ``cli_type`` to be ``list``, in which case you should\n    ensure that a list of one or more objects is hydrated rather than a\n    single object.\n    \"\"\"\n    def __init__(self, service_name, configs):\n        self.configs = configs\n        self.service_name = service_name\n\n    def register(self, cli):\n        \"\"\"\n        Register with a CLI instance, listening for events that build the\n        argument table for operations in the configuration dict.\n        \"\"\"\n        # Flatten each configured operation when they are built\n        service = self.service_name\n        for operation in self.configs:\n            cli.register('building-argument-table.{0}.{1}'.format(service,\n                                                                  operation),\n                         self.flatten_args)\n\n    def flatten_args(self, command, argument_table, **kwargs):\n        # For each argument with a bag of parameters\n        for name, argument in self.configs[command.name].items():\n            argument_from_table = argument_table[name]\n            overwritten = False\n\n            LOG.debug('Flattening {0} argument {1} into {2}'.format(\n                command.name, name,\n                ', '.join([v['name'] for k, v in argument['flatten'].items()])\n            ))\n\n            # For each parameter to flatten out\n            for sub_argument, new_config in argument['flatten'].items():\n                config = new_config.copy()\n                config['container'] = argument_from_table\n                config['prop'] = sub_argument\n\n                # Handle nested arguments\n                _arg = self._find_nested_arg(\n                    argument_from_table.argument_model, sub_argument\n                )\n\n                # Pull out docs and required attribute\n                self._merge_member_config(_arg, sub_argument, config)\n\n                # Create and set the new flattened argument\n                new_arg = FlattenedArgument(**config)\n                argument_table[new_config['name']] = new_arg\n\n                if name == new_config['name']:\n                    overwritten = True\n\n            # Delete the original argument?\n            if not overwritten and ('keep' not in argument or\n                                    not argument['keep']):\n                del argument_table[name]\n\n    def _find_nested_arg(self, argument, name):\n        \"\"\"\n        Find and return a nested argument, if it exists. If no nested argument\n        is requested then the original argument is returned. If the nested\n        argument cannot be found, then a ValueError is raised.\n        \"\"\"\n        if SEP in name:\n            # Find the actual nested argument to pull out\n            LOG.debug('Finding nested argument in {0}'.format(name))\n            for piece in name.split(SEP)[:-1]:\n                for member_name, member in argument.members.items():\n                    if member_name == piece:\n                        argument = member\n                        break\n                else:\n                    raise ValueError('Invalid piece {0}'.format(piece))\n\n        return argument\n\n    def _merge_member_config(self, argument, name, config):\n        \"\"\"\n        Merges an existing config taken from the configuration dict with an\n        existing member of an existing argument object. This pulls in\n        attributes like ``required`` and ``help_text`` if they have not been\n        overridden in the configuration dict. Modifies the config in-place.\n        \"\"\"\n        # Pull out docs and required attribute\n        for member_name, member in argument.members.items():\n            if member_name == name.split(SEP)[-1]:\n                if 'help_text' not in config:\n                    config['help_text'] = member.documentation\n\n                if 'required' not in config:\n                    config['required'] = member_name in argument.required_members\n\n                if 'type' not in config:\n                    config['type'] = member.type_name\n\n                break\n", "awscli/customizations/toplevelbool.py": "# language governing permissions and limitations under the License.\n\"\"\"\nTop Level Boolean Parameters\n----------------------------\n\nThis customization will take a parameter that has\na structure of a single boolean element and allow the argument\nto be specified without a value.\n\nInstead of having to say::\n\n    --ebs-optimized '{\"Value\": true}'\n    --ebs-optimized '{\"Value\": false}'\n\nYou can instead say `--ebs-optimized/--no-ebs-optimized`.\n\n\n\"\"\"\nimport logging\nfrom functools import partial\n\n\nfrom awscli.argprocess import detect_shape_structure\nfrom awscli import arguments\nfrom awscli.customizations.utils import validate_mutually_exclusive_handler\n\n\nLOG = logging.getLogger(__name__)\n# This sentinel object is used to distinguish when\n# a parameter is not specified vs. specified with no value\n# (a value of None).\n_NOT_SPECIFIED = object()\n\n\ndef register_bool_params(event_handler):\n    event_handler.register('building-argument-table.ec2.*',\n                           partial(pull_up_bool,\n                                   event_handler=event_handler))\n\n\ndef _qualifies_for_simplification(arg_model):\n    if detect_shape_structure(arg_model) == 'structure(scalar)':\n        members = arg_model.members\n        if (len(members) == 1 and\n            list(members.keys())[0] == 'Value' and\n            list(members.values())[0].type_name == 'boolean'):\n            return True\n    return False\n\n\ndef pull_up_bool(argument_table, event_handler, **kwargs):\n    # List of tuples of (positive_bool, negative_bool)\n    # This is used to validate that we don't specify\n    # an --option and a --no-option.\n    boolean_pairs = []\n    event_handler.register(\n        'operation-args-parsed.ec2.*',\n        partial(validate_boolean_mutex_groups,\n                boolean_pairs=boolean_pairs))\n    for value in list(argument_table.values()):\n        if hasattr(value, 'argument_model'):\n            arg_model = value.argument_model\n            if _qualifies_for_simplification(arg_model):\n                # Swap out the existing CLIArgument for two args:\n                # one that supports --option and --option <some value>\n                # and another arg of --no-option.\n                new_arg = PositiveBooleanArgument(\n                    value.name, arg_model, value._operation_model,\n                    value._event_emitter,\n                    group_name=value.name,\n                    serialized_name=value._serialized_name)\n                argument_table[value.name] = new_arg\n                negative_name = 'no-%s' % value.name\n                negative_arg = NegativeBooleanParameter(\n                    negative_name, arg_model, value._operation_model,\n                    value._event_emitter,\n                    action='store_true', dest='no_%s' % new_arg.py_name,\n                    group_name=value.name,\n                    serialized_name=value._serialized_name)\n                argument_table[negative_name] = negative_arg\n                # If we've pulled up a structure(scalar) arg\n                # into a pair of top level boolean args, we need\n                # to validate that a user only provides the argument\n                # once.  They can't say --option/--no-option, nor\n                # can they say --option --option Value=false.\n                boolean_pairs.append((new_arg, negative_arg))\n\n\ndef validate_boolean_mutex_groups(boolean_pairs, parsed_args, **kwargs):\n    # Validate we didn't pass in an --option and a --no-option.\n    for positive, negative in boolean_pairs:\n        if getattr(parsed_args, positive.py_name) is not _NOT_SPECIFIED and \\\n                getattr(parsed_args, negative.py_name) is not _NOT_SPECIFIED:\n            raise ValueError(\n                'Cannot specify both the \"%s\" option and '\n                'the \"%s\" option.' % (positive.cli_name, negative.cli_name))\n\n\nclass PositiveBooleanArgument(arguments.CLIArgument):\n    def __init__(self, name, argument_model, operation_model,\n                 event_emitter, serialized_name, group_name):\n        super(PositiveBooleanArgument, self).__init__(\n            name, argument_model, operation_model, event_emitter,\n            serialized_name=serialized_name)\n        self._group_name = group_name\n\n    @property\n    def group_name(self):\n        return self._group_name\n\n    def add_to_parser(self, parser):\n        # We need to support three forms:\n        # --option-name\n        # --option-name Value=(true|false)\n        parser.add_argument(self.cli_name,\n                            help=self.documentation,\n                            action='store',\n                            default=_NOT_SPECIFIED,\n                            nargs='?')\n\n    def add_to_params(self, parameters, value):\n        if value is _NOT_SPECIFIED:\n            return\n        elif value is None:\n            # Then this means that the user explicitly\n            # specified this arg with no value,\n            # e.g. --boolean-parameter\n            # which means we should add a true value\n            # to the parameters dict.\n            parameters[self._serialized_name] = {'Value': True}\n        else:\n            # Otherwise the arg was specified with a value.\n            parameters[self._serialized_name] = self._unpack_argument(\n                value)\n\n\nclass NegativeBooleanParameter(arguments.BooleanArgument):\n    def __init__(self, name, argument_model, operation_model,\n                 event_emitter, serialized_name, action='store_true',\n                 dest=None, group_name=None):\n        super(NegativeBooleanParameter, self).__init__(\n            name, argument_model, operation_model, event_emitter,\n            default=_NOT_SPECIFIED, serialized_name=serialized_name)\n        self._group_name = group_name\n\n    def add_to_params(self, parameters, value):\n        if value is not _NOT_SPECIFIED and value:\n            parameters[self._serialized_name] = {'Value': False}\n", "awscli/customizations/sms_voice.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\ndef register_sms_voice_hide(event_emitter):\n    event_emitter.register('building-command-table.main',\n                           hide_sms_voice)\n\n\ndef hide_sms_voice(command_table, session, **kwargs):\n    command_table['sms-voice']._UNDOCUMENTED = True\n", "awscli/customizations/opsworkscm.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations.utils import alias_command\n\n\ndef register_alias_opsworks_cm(event_emitter):\n    event_emitter.register('building-command-table.main', alias_opsworks_cm)\n\n\ndef alias_opsworks_cm(command_table, **kwargs):\n    alias_command(command_table, 'opsworkscm', 'opsworks-cm')\n", "awscli/customizations/paginate.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"This module has customizations to unify paging parameters.\n\nFor any operation that can be paginated, we will:\n\n    * Hide the service specific pagination params.  This can vary across\n    services and we're going to replace them with a consistent set of\n    arguments.  The arguments will still work, but they are not\n    documented.  This allows us to add a pagination config after\n    the fact and still remain backwards compatible with users that\n    were manually doing pagination.\n    * Add a ``--starting-token`` and a ``--max-items`` argument.\n\n\"\"\"\nimport logging\nfrom functools import partial\n\nfrom botocore import xform_name\nfrom botocore.exceptions import DataNotFoundError, PaginationError\nfrom botocore import model\n\nfrom awscli.arguments import BaseCLIArgument\n\n\nlogger = logging.getLogger(__name__)\n\n\nSTARTING_TOKEN_HELP = \"\"\"\n<p>A token to specify where to start paginating.  This is the\n<code>NextToken</code> from a previously truncated response.</p>\n<p>For usage examples, see <a\nhref=\"https://docs.aws.amazon.com/cli/latest/userguide/pagination.html\"\n>Pagination</a> in the <i>AWS Command Line Interface User\nGuide</i>.</p>\n\"\"\"\n\nMAX_ITEMS_HELP = \"\"\"\n<p>The total number of items to return in the command's output.\nIf the total number of items available is more than the value\nspecified, a <code>NextToken</code> is provided in the command's\noutput.  To resume pagination, provide the\n<code>NextToken</code> value in the <code>starting-token</code>\nargument of a subsequent command.  <b>Do not</b> use the\n<code>NextToken</code> response element directly outside of the\nAWS CLI.</p>\n<p>For usage examples, see <a\nhref=\"https://docs.aws.amazon.com/cli/latest/userguide/pagination.html\"\n>Pagination</a> in the <i>AWS Command Line Interface User\nGuide</i>.</p>\n\"\"\"\n\nPAGE_SIZE_HELP = \"\"\"\n<p>The size of each page to get in the AWS service call.  This\ndoes not affect the number of items returned in the command's\noutput.  Setting a smaller page size results in more calls to\nthe AWS service, retrieving fewer items in each call.  This can\nhelp prevent the AWS service calls from timing out.</p>\n<p>For usage examples, see <a\nhref=\"https://docs.aws.amazon.com/cli/latest/userguide/pagination.html\"\n>Pagination</a> in the <i>AWS Command Line Interface User\nGuide</i>.</p>\n\"\"\"\n\n\ndef register_pagination(event_handlers):\n    event_handlers.register('building-argument-table', unify_paging_params)\n    event_handlers.register_last('doc-description', add_paging_description)\n\n\ndef get_paginator_config(session, service_name, operation_name):\n    try:\n        paginator_model = session.get_paginator_model(service_name)\n    except DataNotFoundError:\n        return None\n    try:\n        operation_paginator_config = paginator_model.get_paginator(\n            operation_name)\n    except ValueError:\n        return None\n    return operation_paginator_config\n\n\ndef add_paging_description(help_command, **kwargs):\n    # This customization is only applied to the description of\n    # Operations, so we must filter out all other events.\n    if not isinstance(help_command.obj, model.OperationModel):\n        return\n    service_name = help_command.obj.service_model.service_name\n    paginator_config = get_paginator_config(\n        help_command.session, service_name, help_command.obj.name)\n    if not paginator_config:\n        return\n    help_command.doc.style.new_paragraph()\n    help_command.doc.writeln(\n        ('``%s`` is a paginated operation. Multiple API calls may be issued '\n         'in order to retrieve the entire data set of results. You can '\n         'disable pagination by providing the ``--no-paginate`` argument.')\n        % help_command.name)\n    # Only include result key information if it is present.\n    if paginator_config.get('result_key'):\n        queries = paginator_config['result_key']\n        if type(queries) is not list:\n            queries = [queries]\n        queries = \", \".join([('``%s``' % s) for s in queries])\n        help_command.doc.writeln(\n            ('When using ``--output text`` and the ``--query`` argument on a '\n             'paginated response, the ``--query`` argument must extract data '\n             'from the results of the following query expressions: %s')\n            % queries)\n\n\ndef unify_paging_params(argument_table, operation_model, event_name,\n                        session, **kwargs):\n    paginator_config = get_paginator_config(\n        session, operation_model.service_model.service_name,\n        operation_model.name)\n    if paginator_config is None:\n        # We only apply these customizations to paginated responses.\n        return\n    logger.debug(\"Modifying paging parameters for operation: %s\",\n                 operation_model.name)\n    _remove_existing_paging_arguments(argument_table, paginator_config)\n    parsed_args_event = event_name.replace('building-argument-table.',\n                                           'operation-args-parsed.')\n    shadowed_args = {}\n    add_paging_argument(argument_table, 'starting-token',\n                        PageArgument('starting-token', STARTING_TOKEN_HELP,\n                                     parse_type='string',\n                                     serialized_name='StartingToken'),\n                        shadowed_args)\n    input_members = operation_model.input_shape.members\n    type_name = 'integer'\n    if 'limit_key' in paginator_config:\n        limit_key_shape = input_members[paginator_config['limit_key']]\n        type_name = limit_key_shape.type_name\n        if type_name not in PageArgument.type_map:\n            raise TypeError(\n                ('Unsupported pagination type {0} for operation {1}'\n                 ' and parameter {2}').format(\n                    type_name, operation_model.name,\n                    paginator_config['limit_key']))\n        add_paging_argument(argument_table, 'page-size',\n                            PageArgument('page-size', PAGE_SIZE_HELP,\n                                         parse_type=type_name,\n                                         serialized_name='PageSize'),\n                            shadowed_args)\n\n    add_paging_argument(argument_table, 'max-items',\n                        PageArgument('max-items', MAX_ITEMS_HELP,\n                                     parse_type=type_name,\n                                     serialized_name='MaxItems'),\n                        shadowed_args)\n    session.register(\n        parsed_args_event,\n        partial(check_should_enable_pagination,\n                list(_get_all_cli_input_tokens(paginator_config)),\n                shadowed_args, argument_table))\n\n\ndef add_paging_argument(argument_table, arg_name, argument, shadowed_args):\n    if arg_name in argument_table:\n        # If there's already an entry in the arg table for this argument,\n        # this means we're shadowing an argument for this operation.  We\n        # need to store this later in case pagination is turned off because\n        # we put these arguments back.\n        # See the comment in check_should_enable_pagination() for more info.\n        shadowed_args[arg_name] = argument_table[arg_name]\n    argument_table[arg_name] = argument\n\n\ndef check_should_enable_pagination(input_tokens, shadowed_args, argument_table,\n                                   parsed_args, parsed_globals, **kwargs):\n    normalized_paging_args = ['start_token', 'max_items']\n    for token in input_tokens:\n        py_name = token.replace('-', '_')\n        if getattr(parsed_args, py_name) is not None and \\\n                py_name not in normalized_paging_args:\n            # The user has specified a manual (undocumented) pagination arg.\n            # We need to automatically turn pagination off.\n            logger.debug(\"User has specified a manual pagination arg. \"\n                         \"Automatically setting --no-paginate.\")\n            parsed_globals.paginate = False\n\n    if not parsed_globals.paginate:\n        ensure_paging_params_not_set(parsed_args, shadowed_args)\n        # Because pagination is now disabled, there's a chance that\n        # we were shadowing arguments.  For example, we inject a\n        # --max-items argument in unify_paging_params().  If the\n        # the operation also provides its own MaxItems (which we\n        # expose as --max-items) then our custom pagination arg\n        # was shadowing the customers arg.  When we turn pagination\n        # off we need to put back the original argument which is\n        # what we're doing here.\n        for key, value in shadowed_args.items():\n            argument_table[key] = value\n\n\ndef ensure_paging_params_not_set(parsed_args, shadowed_args):\n    paging_params = ['starting_token', 'page_size', 'max_items']\n    shadowed_params = [p.replace('-', '_') for p in shadowed_args.keys()]\n    params_used = [p for p in paging_params if\n                   p not in shadowed_params and getattr(parsed_args, p, None)]\n\n    if len(params_used) > 0:\n        converted_params = ', '.join(\n            [\"--\" + p.replace('_', '-') for p in params_used])\n        raise PaginationError(\n            message=\"Cannot specify --no-paginate along with pagination \"\n                    \"arguments: %s\" % converted_params)\n\n\ndef _remove_existing_paging_arguments(argument_table, pagination_config):\n    for cli_name in _get_all_cli_input_tokens(pagination_config):\n        argument_table[cli_name]._UNDOCUMENTED = True\n\n\ndef _get_all_cli_input_tokens(pagination_config):\n    # Get all input tokens including the limit_key\n    # if it exists.\n    tokens = _get_input_tokens(pagination_config)\n    for token_name in tokens:\n        cli_name = xform_name(token_name, '-')\n        yield cli_name\n    if 'limit_key' in pagination_config:\n        key_name = pagination_config['limit_key']\n        cli_name = xform_name(key_name, '-')\n        yield cli_name\n\n\ndef _get_input_tokens(pagination_config):\n    tokens = pagination_config['input_token']\n    if not isinstance(tokens, list):\n        return [tokens]\n    return tokens\n\n\ndef _get_cli_name(param_objects, token_name):\n    for param in param_objects:\n        if param.name == token_name:\n            return param.cli_name.lstrip('-')\n\n\nclass PageArgument(BaseCLIArgument):\n    type_map = {\n        'string': str,\n        'integer': int,\n        'long': int,\n    }\n\n    def __init__(self, name, documentation, parse_type, serialized_name):\n        self.argument_model = model.Shape('PageArgument', {'type': 'string'})\n        self._name = name\n        self._serialized_name = serialized_name\n        self._documentation = documentation\n        self._parse_type = parse_type\n        self._required = False\n\n    @property\n    def cli_name(self):\n        return '--' + self._name\n\n    @property\n    def cli_type_name(self):\n        return self._parse_type\n\n    @property\n    def required(self):\n        return self._required\n\n    @required.setter\n    def required(self, value):\n        self._required = value\n\n    @property\n    def documentation(self):\n        return self._documentation\n\n    def add_to_parser(self, parser):\n        parser.add_argument(self.cli_name, dest=self.py_name,\n                            type=self.type_map[self._parse_type])\n\n    def add_to_params(self, parameters, value):\n        if value is not None:\n            pagination_config = parameters.get('PaginationConfig', {})\n            pagination_config[self._serialized_name] = value\n            parameters['PaginationConfig'] = pagination_config\n", "awscli/customizations/ec2/runinstances.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nThis customization adds two new parameters to the ``ec2 run-instance``\ncommand.  The first, ``--secondary-private-ip-addresses`` allows a list\nof IP addresses within the specified subnet to be associated with the\nnew instance.  The second, ``--secondary-ip-address-count`` allows you\nto specify how many additional IP addresses you want but the actual\naddress will be assigned for you.\n\nThis functionality (and much more) is also available using the\n``--network-interfaces`` complex argument.  This just makes two of\nthe most commonly used features available more easily.\n\"\"\"\nfrom awscli.arguments import CustomArgument\n\n# --secondary-private-ip-address\nSECONDARY_PRIVATE_IP_ADDRESSES_DOCS = (\n    '[EC2-VPC] A secondary private IP address for the network interface '\n    'or instance. You can specify this multiple times to assign multiple '\n    'secondary IP addresses.  If you want additional private IP addresses '\n    'but do not need a specific address, use the '\n    '--secondary-private-ip-address-count option.')\n\n# --secondary-private-ip-address-count\nSECONDARY_PRIVATE_IP_ADDRESS_COUNT_DOCS = (\n    '[EC2-VPC] The number of secondary IP addresses to assign to '\n    'the network interface or instance.')\n\n# --associate-public-ip-address\nASSOCIATE_PUBLIC_IP_ADDRESS_DOCS = (\n    '[EC2-VPC] If specified a public IP address will be assigned '\n    'to the new instance in a VPC.')\n\n\ndef _add_params(argument_table, **kwargs):\n    arg = SecondaryPrivateIpAddressesArgument(\n        name='secondary-private-ip-addresses',\n        help_text=SECONDARY_PRIVATE_IP_ADDRESSES_DOCS)\n    argument_table['secondary-private-ip-addresses'] = arg\n    arg = SecondaryPrivateIpAddressCountArgument(\n        name='secondary-private-ip-address-count',\n        help_text=SECONDARY_PRIVATE_IP_ADDRESS_COUNT_DOCS)\n    argument_table['secondary-private-ip-address-count'] = arg\n    arg = AssociatePublicIpAddressArgument(\n        name='associate-public-ip-address',\n        help_text=ASSOCIATE_PUBLIC_IP_ADDRESS_DOCS,\n        action='store_true', group_name='associate_public_ip')\n    argument_table['associate-public-ip-address'] = arg\n    arg = NoAssociatePublicIpAddressArgument(\n        name='no-associate-public-ip-address',\n        help_text=ASSOCIATE_PUBLIC_IP_ADDRESS_DOCS,\n        action='store_false', group_name='associate_public_ip')\n    argument_table['no-associate-public-ip-address'] = arg\n\n\ndef _check_args(parsed_args, **kwargs):\n    # This function checks the parsed args.  If the user specified\n    # the --network-interfaces option with any of the scalar options we\n    # raise an error.\n    arg_dict = vars(parsed_args)\n    if arg_dict['network_interfaces']:\n        for key in ('secondary_private_ip_addresses',\n                    'secondary_private_ip_address_count',\n                    'associate_public_ip_address'):\n            if arg_dict[key]:\n                msg = ('Mixing the --network-interfaces option '\n                       'with the simple, scalar options is '\n                       'not supported.')\n                raise ValueError(msg)\n\n\ndef _fix_args(params, **kwargs):\n    # The RunInstances request provides some parameters\n    # such as --subnet-id and --security-group-id that can be specified\n    # as separate options only if the request DOES NOT include a\n    # NetworkInterfaces structure.  In those cases, the values for\n    # these parameters must be specified inside the NetworkInterfaces\n    # structure.  This function checks for those parameters\n    # and fixes them if necessary.\n    # NOTE: If the user is a default VPC customer, RunInstances\n    # allows them to specify the security group by name or by id.\n    # However, in this scenario we can only support id because\n    # we can't place a group name in the NetworkInterfaces structure.\n    network_interface_params = [\n        'PrivateIpAddresses',\n        'SecondaryPrivateIpAddressCount',\n        'AssociatePublicIpAddress'\n    ]\n    if 'NetworkInterfaces' in params:\n        interface = params['NetworkInterfaces'][0]\n        if any(param in interface for param in network_interface_params):\n            if 'SubnetId' in params:\n                interface['SubnetId'] = params['SubnetId']\n                del params['SubnetId']\n            if 'SecurityGroupIds' in params:\n                interface['Groups'] = params['SecurityGroupIds']\n                del params['SecurityGroupIds']\n            if 'PrivateIpAddress' in params:\n                ip_addr = {'PrivateIpAddress': params['PrivateIpAddress'],\n                           'Primary': True}\n                interface['PrivateIpAddresses'] = [ip_addr]\n                del params['PrivateIpAddress']\n            if 'Ipv6AddressCount' in params:\n                interface['Ipv6AddressCount'] = params['Ipv6AddressCount']\n                del params['Ipv6AddressCount']\n            if 'Ipv6Addresses' in params:\n                interface['Ipv6Addresses'] = params['Ipv6Addresses']\n                del params['Ipv6Addresses']\n            if 'EnablePrimaryIpv6' in params:\n                interface['PrimaryIpv6'] = params['EnablePrimaryIpv6']\n                del params['EnablePrimaryIpv6']\n\n\nEVENTS = [\n    ('building-argument-table.ec2.run-instances', _add_params),\n    ('operation-args-parsed.ec2.run-instances', _check_args),\n    ('before-parameter-build.ec2.RunInstances', _fix_args),\n]\n\n\ndef register_runinstances(event_handler):\n    # Register all of the events for customizing BundleInstance\n    for event, handler in EVENTS:\n        event_handler.register(event, handler)\n\n\ndef _build_network_interfaces(params, key, value):\n    # Build up the NetworkInterfaces data structure\n    if 'NetworkInterfaces' not in params:\n        params['NetworkInterfaces'] = [{'DeviceIndex': 0}]\n\n    if key == 'PrivateIpAddresses':\n        if 'PrivateIpAddresses' not in params['NetworkInterfaces'][0]:\n            params['NetworkInterfaces'][0]['PrivateIpAddresses'] = value\n    else:\n        params['NetworkInterfaces'][0][key] = value\n\n\nclass SecondaryPrivateIpAddressesArgument(CustomArgument):\n\n    def add_to_parser(self, parser, cli_name=None):\n        parser.add_argument(self.cli_name, dest=self.py_name,\n                            default=self._default, nargs='*')\n\n    def add_to_params(self, parameters, value):\n        if value:\n            value = [{'PrivateIpAddress': v, 'Primary': False} for v in value]\n            _build_network_interfaces(\n                parameters, 'PrivateIpAddresses', value)\n\n\nclass SecondaryPrivateIpAddressCountArgument(CustomArgument):\n\n    def add_to_parser(self, parser, cli_name=None):\n        parser.add_argument(self.cli_name, dest=self.py_name,\n                            default=self._default, type=int)\n\n    def add_to_params(self, parameters, value):\n        if value:\n            _build_network_interfaces(\n                parameters, 'SecondaryPrivateIpAddressCount', value)\n\n\nclass AssociatePublicIpAddressArgument(CustomArgument):\n\n    def add_to_params(self, parameters, value):\n        if value is True:\n            _build_network_interfaces(\n                parameters, 'AssociatePublicIpAddress', value)\n\n\nclass NoAssociatePublicIpAddressArgument(CustomArgument):\n\n    def add_to_params(self, parameters, value):\n        if value is False:\n            _build_network_interfaces(\n                parameters, 'AssociatePublicIpAddress', value)\n", "awscli/customizations/ec2/secgroupsimplify.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nThis customization adds the following scalar parameters to the\nauthorize operations:\n\n* --protocol: tcp | udp | icmp or any protocol number\n* --port:  A single integer or a range (min-max). You can specify ``all``\n  to mean all ports (for example, port range 0-65535)\n* --source-group: Either the source security group ID or name.\n* --cidr -  The IPv4 address range, in CIDR format. Cannot be used when specifying a source or\n  destination security group.\n\"\"\"\n\nfrom awscli.arguments import CustomArgument\n\n\ndef _add_params(argument_table, **kwargs):\n    arg = ProtocolArgument('protocol',\n                           help_text=PROTOCOL_DOCS)\n    argument_table['protocol'] = arg\n    argument_table['ip-protocol']._UNDOCUMENTED = True\n\n    arg = PortArgument('port', help_text=PORT_DOCS)\n    argument_table['port'] = arg\n    # Port handles both the from-port and to-port,\n    # we need to not document both args.\n    argument_table['from-port']._UNDOCUMENTED = True\n    argument_table['to-port']._UNDOCUMENTED = True\n\n    arg = CidrArgument('cidr', help_text=CIDR_DOCS)\n    argument_table['cidr'] = arg\n    argument_table['cidr-ip']._UNDOCUMENTED = True\n\n    arg = SourceGroupArgument('source-group',\n                              help_text=SOURCEGROUP_DOCS)\n    argument_table['source-group'] = arg\n    argument_table['source-security-group-name']._UNDOCUMENTED = True\n\n    arg = GroupOwnerArgument('group-owner',\n                             help_text=GROUPOWNER_DOCS)\n    argument_table['group-owner'] = arg\n    argument_table['source-security-group-owner-id']._UNDOCUMENTED = True\n\n\ndef _check_args(parsed_args, **kwargs):\n    # This function checks the parsed args.  If the user specified\n    # the --ip-permissions option with any of the scalar options we\n    # raise an error.\n    arg_dict = vars(parsed_args)\n    if arg_dict['ip_permissions']:\n        for key in ('protocol', 'port', 'cidr',\n                    'source_group', 'group_owner'):\n            if arg_dict[key]:\n                msg = ('The --%s option is not compatible '\n                       'with the --ip-permissions option ') % key\n                raise ValueError(msg)\n\n\ndef _add_docs(help_command, **kwargs):\n    doc = help_command.doc\n    doc.style.new_paragraph()\n    doc.style.start_note()\n    msg = ('To specify multiple rules in a single command '\n           'use the <code>--ip-permissions</code> option')\n    doc.include_doc_string(msg)\n    doc.style.end_note()\n\n\nEVENTS = [\n    ('building-argument-table.ec2.authorize-security-group-ingress',\n     _add_params),\n    ('building-argument-table.ec2.authorize-security-group-egress',\n     _add_params),\n    ('building-argument-table.ec2.revoke-security-group-ingress', _add_params),\n    ('building-argument-table.ec2.revoke-security-group-egress', _add_params),\n    ('operation-args-parsed.ec2.authorize-security-group-ingress',\n     _check_args),\n    ('operation-args-parsed.ec2.authorize-security-group-egress', _check_args),\n    ('operation-args-parsed.ec2.revoke-security-group-ingress', _check_args),\n    ('operation-args-parsed.ec2.revoke-security-group-egress', _check_args),\n    ('doc-description.ec2.authorize-security-group-ingress', _add_docs),\n    ('doc-description.ec2.authorize-security-group-egress', _add_docs),\n    ('doc-description.ec2.revoke-security-group-ingress', _add_docs),\n    ('doc-description.ec2.revoke-security-groupdoc-ingress', _add_docs),\n]\nPROTOCOL_DOCS = ('<p>The IP protocol: <code>tcp</code> | '\n                 '<code>udp</code> | <code>icmp</code></p> '\n                 '<p>(VPC only) Use <code>all</code> to specify all protocols.</p>'\n                 '<p>If this argument is provided without also providing the '\n                 '<code>port</code> argument, then it will be applied to all '\n                 'ports for the specified protocol.</p>')\nPORT_DOCS = ('<p>For TCP or UDP: The range of ports to allow.'\n             '  A single integer or a range (<code>min-max</code>).</p>'\n             '<p>For ICMP: A single integer or a range (<code>type-code</code>)'\n             ' representing the ICMP type'\n             ' number and the ICMP code number respectively.'\n             ' A value of -1 indicates all ICMP codes for'\n             ' all ICMP types. A value of -1 just for <code>type</code>'\n             ' indicates all ICMP codes for the specified ICMP type.</p>')\nCIDR_DOCS = '<p>The IPv4 address range, in CIDR format.</p>'\nSOURCEGROUP_DOCS = ('<p>The name or ID of the source security group.</p>')\nGROUPOWNER_DOCS = ('<p>The AWS account ID that owns the source security '\n                   'group. Cannot be used when specifying a CIDR IP '\n                   'address.</p>')\n\n\ndef register_secgroup(event_handler):\n    for event, handler in EVENTS:\n        event_handler.register(event, handler)\n\n\ndef _build_ip_permissions(params, key, value):\n    if 'IpPermissions' not in params:\n        params['IpPermissions'] = [{}]\n    if key == 'CidrIp':\n        if 'IpRanges' not in params['ip_permissions'][0]:\n            params['IpPermissions'][0]['IpRanges'] = []\n        params['IpPermissions'][0]['IpRanges'].append(value)\n    elif key in ('GroupId', 'GroupName', 'UserId'):\n        if 'UserIdGroupPairs' not in params['IpPermissions'][0]:\n            params['IpPermissions'][0]['UserIdGroupPairs'] = [{}]\n        params['IpPermissions'][0]['UserIdGroupPairs'][0][key] = value\n    else:\n        params['IpPermissions'][0][key] = value\n\n\nclass ProtocolArgument(CustomArgument):\n\n    def add_to_params(self, parameters, value):\n        if value:\n            try:\n                int_value = int(value)\n                if (int_value < 0 or int_value > 255) and int_value != -1:\n                    msg = ('protocol numbers must be in the range 0-255 '\n                           'or -1 to specify all protocols')\n                    raise ValueError(msg)\n            except ValueError:\n                if value not in ('tcp', 'udp', 'icmp', 'all'):\n                    msg = ('protocol parameter should be one of: '\n                           'tcp|udp|icmp|all or any valid protocol number.')\n                    raise ValueError(msg)\n                if value == 'all':\n                    value = '-1'\n            _build_ip_permissions(parameters, 'IpProtocol', value)\n\n\nclass PortArgument(CustomArgument):\n\n    def add_to_params(self, parameters, value):\n        if value:\n            try:\n                if value == '-1' or value == 'all':\n                    fromstr = '-1'\n                    tostr = '-1'\n                elif '-' in value:\n                    # We can get away with simple logic here because\n                    # argparse will not allow values such as\n                    # \"-1-8\", and these aren't actually valid\n                    # values any from from/to ports.\n                    fromstr, tostr = value.split('-', 1)\n                else:\n                    fromstr, tostr = (value, value)\n                _build_ip_permissions(parameters, 'FromPort', int(fromstr))\n                _build_ip_permissions(parameters, 'ToPort', int(tostr))\n            except ValueError:\n                msg = ('port parameter should be of the '\n                       'form <from[-to]> (e.g. 22 or 22-25)')\n                raise ValueError(msg)\n\n\nclass CidrArgument(CustomArgument):\n\n    def add_to_params(self, parameters, value):\n        if value:\n            value = [{'CidrIp': value}]\n            _build_ip_permissions(parameters, 'IpRanges', value)\n\n\nclass SourceGroupArgument(CustomArgument):\n\n    def add_to_params(self, parameters, value):\n        if value:\n            if value.startswith('sg-'):\n                _build_ip_permissions(parameters, 'GroupId', value)\n            else:\n                _build_ip_permissions(parameters, 'GroupName', value)\n\n\nclass GroupOwnerArgument(CustomArgument):\n\n    def add_to_params(self, parameters, value):\n        if value:\n            _build_ip_permissions(parameters, 'UserId', value)\n", "awscli/customizations/ec2/decryptpassword.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\nimport base64\nimport rsa\n\nfrom botocore import model\n\nfrom awscli.arguments import BaseCLIArgument\n\n\nlogger = logging.getLogger(__name__)\n\n\nHELP = \"\"\"<p>The file that contains the private key used to launch\nthe instance (e.g. windows-keypair.pem).  If this is supplied, the\npassword data sent from EC2 will be decrypted before display.</p>\"\"\"\n\n\ndef ec2_add_priv_launch_key(argument_table, operation_model, session,\n                            **kwargs):\n    \"\"\"\n    This handler gets called after the argument table for the\n    operation has been created.  It's job is to add the\n    ``priv-launch-key`` parameter.\n    \"\"\"\n    argument_table['priv-launch-key'] = LaunchKeyArgument(\n        session, operation_model, 'priv-launch-key')\n\n\nclass LaunchKeyArgument(BaseCLIArgument):\n\n    def __init__(self, session, operation_model, name):\n        self._session = session\n        self.argument_model = model.Shape('LaunchKeyArgument', {'type': 'string'})\n        self._operation_model = operation_model\n        self._name = name\n        self._key_path = None\n        self._required = False\n\n    @property\n    def cli_type_name(self):\n        return 'string'\n\n    @property\n    def required(self):\n        return self._required\n\n    @required.setter\n    def required(self, value):\n        self._required = value\n\n    @property\n    def documentation(self):\n        return HELP\n\n    def add_to_parser(self, parser):\n        parser.add_argument(self.cli_name, dest=self.py_name,\n                            help='SSH Private Key file')\n\n    def add_to_params(self, parameters, value):\n        \"\"\"\n        This gets called with the value of our ``--priv-launch-key``\n        if it is specified.  It needs to determine if the path\n        provided is valid and, if it is, it stores it in the instance\n        variable ``_key_path`` for use by the decrypt routine.\n        \"\"\"\n        if value:\n            path = os.path.expandvars(value)\n            path = os.path.expanduser(path)\n            if os.path.isfile(path):\n                self._key_path = path\n                endpoint_prefix = \\\n                    self._operation_model.service_model.endpoint_prefix\n                event = 'after-call.%s.%s' % (endpoint_prefix,\n                                              self._operation_model.name)\n                self._session.register(event, self._decrypt_password_data)\n            else:\n                msg = ('priv-launch-key should be a path to the '\n                       'local SSH private key file used to launch '\n                       'the instance.')\n                raise ValueError(msg)\n\n    def _decrypt_password_data(self, parsed, **kwargs):\n        \"\"\"\n        This handler gets called after the GetPasswordData command has been\n        executed.  It is called with the and the ``parsed`` data.  It checks to\n        see if a private launch key was specified on the command.  If it was,\n        it tries to use that private key to decrypt the password data and\n        replace it in the returned data dictionary.\n        \"\"\"\n        if self._key_path is not None:\n            logger.debug(\"Decrypting password data using: %s\", self._key_path)\n            value = parsed.get('PasswordData')\n            if not value:\n                return\n            try:\n                with open(self._key_path) as pk_file:\n                    pk_contents = pk_file.read()\n                    private_key = rsa.PrivateKey.load_pkcs1(pk_contents.encode(\"latin-1\"))\n                    value = base64.b64decode(value)\n                    value = rsa.decrypt(value, private_key)\n                    logger.debug(parsed)\n                    parsed['PasswordData'] = value.decode('utf-8')\n                    logger.debug(parsed)\n            except Exception:\n                logger.debug('Unable to decrypt PasswordData', exc_info=True)\n                msg = ('Unable to decrypt password data using '\n                       'provided private key file.')\n                raise ValueError(msg)\n", "awscli/customizations/ec2/addcount.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nfrom botocore import model\n\nfrom awscli.arguments import BaseCLIArgument\n\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT = 1\nHELP = \"\"\"\n<p>Number of instances to launch. If a single number is provided, it\nis assumed to be the minimum to launch (defaults to %d).  If a range is\nprovided in the form <code>min:max</code> then the first number is\ninterpreted as the minimum number of instances to launch and the second\nis interpreted as the maximum number of instances to launch.</p>\"\"\" % DEFAULT\n\n\ndef register_count_events(event_handler):\n    event_handler.register(\n        'building-argument-table.ec2.run-instances', ec2_add_count)\n    event_handler.register(\n        'before-parameter-build.ec2.RunInstances', set_default_count)\n\n\ndef ec2_add_count(argument_table, **kwargs):\n    argument_table['count'] = CountArgument('count')\n    del argument_table['min-count']\n    del argument_table['max-count']\n\n\ndef set_default_count(params, **kwargs):\n    params.setdefault('MaxCount', DEFAULT)\n    params.setdefault('MinCount', DEFAULT)\n\n\nclass CountArgument(BaseCLIArgument):\n\n    def __init__(self, name):\n        self.argument_model = model.Shape('CountArgument', {'type': 'string'})\n        self._name = name\n        self._required = False\n\n    @property\n    def cli_name(self):\n        return '--' + self._name\n\n    @property\n    def cli_type_name(self):\n        return 'string'\n\n    @property\n    def required(self):\n        return self._required\n\n    @required.setter\n    def required(self, value):\n        self._required = value\n\n    @property\n    def documentation(self):\n        return HELP\n\n    def add_to_parser(self, parser):\n        # We do NOT set default value here. It will be set later by event hook.\n        parser.add_argument(self.cli_name, metavar=self.py_name,\n                            help='Number of instances to launch')\n\n    def add_to_params(self, parameters, value):\n        if value is None:\n            # NO-OP if value is not explicitly set by user\n            return\n        try:\n            if ':' in value:\n                minstr, maxstr = value.split(':')\n            else:\n                minstr, maxstr = (value, value)\n            parameters['MinCount'] = int(minstr)\n            parameters['MaxCount'] = int(maxstr)\n        except:\n            msg = ('count parameter should be of '\n                   'form min[:max] (e.g. 1 or 1:10)')\n            raise ValueError(msg)\n", "awscli/customizations/ec2/bundleinstance.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\nfrom hashlib import sha1\nimport hmac\nimport base64\nimport datetime\n\nfrom awscli.arguments import CustomArgument\n\nlogger = logging.getLogger('ec2bundleinstance')\n\n# This customization adds the following scalar parameters to the\n# bundle-instance operation:\n\n# --bucket:\nBUCKET_DOCS = ('The bucket in which to store the AMI.  '\n               'You can specify a bucket that you already own or '\n               'a new bucket that Amazon EC2 creates on your behalf.  '\n               'If you specify a bucket that belongs to someone else, '\n               'Amazon EC2 returns an error.')\n\n# --prefix:\nPREFIX_DOCS = ('The prefix for the image component names being stored '\n               'in Amazon S3.')\n\n# --owner-akid\nOWNER_AKID_DOCS = 'The access key ID of the owner of the Amazon S3 bucket.'\n\n# --policy\nPOLICY_DOCS = (\n    \"An Amazon S3 upload policy that gives \"\n    \"Amazon EC2 permission to upload items into Amazon S3 \"\n    \"on the user's behalf. If you provide this parameter, \"\n    \"you must also provide \"\n    \"your secret access key, so we can create a policy \"\n    \"signature for you (the secret access key is not passed \"\n    \"to Amazon EC2). If you do not provide this parameter, \"\n    \"we generate an upload policy for you automatically. \"\n    \"For more information about upload policies see the \"\n    \"sections about policy construction and signatures in the \"\n    '<a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev'\n    '/HTTPPOSTForms.html\">'\n    'Amazon Simple Storage Service Developer Guide</a>.')\n\n# --owner-sak\nOWNER_SAK_DOCS = ('The AWS secret access key for the owner of the '\n                  'Amazon S3 bucket specified in the --bucket '\n                  'parameter. This parameter is required so that a '\n                  'signature can be computed for the policy.')\n\n\ndef _add_params(argument_table, **kwargs):\n    # Add the scalar parameters and also change the complex storage\n    # param to not be required so the user doesn't get an error from\n    # argparse if they only supply scalar params.\n    storage_arg = argument_table['storage']\n    storage_arg.required = False\n    arg = BundleArgument(storage_param='Bucket',\n                         name='bucket',\n                         help_text=BUCKET_DOCS)\n    argument_table['bucket'] = arg\n    arg = BundleArgument(storage_param='Prefix',\n                         name='prefix',\n                         help_text=PREFIX_DOCS)\n    argument_table['prefix'] = arg\n    arg = BundleArgument(storage_param='AWSAccessKeyId',\n                         name='owner-akid',\n                         help_text=OWNER_AKID_DOCS)\n    argument_table['owner-akid'] = arg\n    arg = BundleArgument(storage_param='_SAK',\n                         name='owner-sak',\n                         help_text=OWNER_SAK_DOCS)\n    argument_table['owner-sak'] = arg\n    arg = BundleArgument(storage_param='UploadPolicy',\n                         name='policy',\n                         help_text=POLICY_DOCS)\n    argument_table['policy'] = arg\n\n\ndef _check_args(parsed_args, **kwargs):\n    # This function checks the parsed args.  If the user specified\n    # the --ip-permissions option with any of the scalar options we\n    # raise an error.\n    logger.debug(parsed_args)\n    arg_dict = vars(parsed_args)\n    if arg_dict['storage']:\n        for key in ('bucket', 'prefix', 'owner_akid',\n                    'owner_sak', 'policy'):\n            if arg_dict[key]:\n                msg = ('Mixing the --storage option '\n                       'with the simple, scalar options is '\n                       'not recommended.')\n                raise ValueError(msg)\n\nPOLICY = ('{{\"expiration\": \"{expires}\",'\n          '\"conditions\": ['\n          '{{\"bucket\": \"{bucket}\"}},'\n          '{{\"acl\": \"ec2-bundle-read\"}},'\n          '[\"starts-with\", \"$key\", \"{prefix}\"]'\n          ']}}'\n          )\n\n\ndef _generate_policy(params):\n    # Called if there is no policy supplied by the user.\n    # Creates a policy that provides access for 24 hours.\n    delta = datetime.timedelta(hours=24)\n    expires = datetime.datetime.utcnow() + delta\n    expires_iso = expires.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n    policy = POLICY.format(expires=expires_iso,\n                           bucket=params['Bucket'],\n                           prefix=params['Prefix'])\n    params['UploadPolicy'] = policy\n\n\ndef _generate_signature(params):\n    # If we have a policy and a sak, create the signature.\n    policy = params.get('UploadPolicy')\n    sak = params.get('_SAK')\n    if policy and sak:\n        policy = base64.b64encode(policy.encode('latin-1')).decode('utf-8')\n        new_hmac = hmac.new(sak.encode('utf-8'), digestmod=sha1)\n        new_hmac.update(policy.encode('latin-1'))\n        ps = base64.encodebytes(new_hmac.digest()).strip().decode('utf-8')\n        params['UploadPolicySignature'] = ps\n        del params['_SAK']\n\n\ndef _check_params(params, **kwargs):\n    # Called just before call but prior to building the params.\n    # Adds information not supplied by the user.\n    storage = params['Storage']['S3']\n    if 'UploadPolicy' not in storage:\n        _generate_policy(storage)\n    if 'UploadPolicySignature' not in storage:\n        _generate_signature(storage)\n\n\nEVENTS = [\n    ('building-argument-table.ec2.bundle-instance', _add_params),\n    ('operation-args-parsed.ec2.bundle-instance', _check_args),\n    ('before-parameter-build.ec2.BundleInstance', _check_params),\n]\n\n\ndef register_bundleinstance(event_handler):\n    # Register all of the events for customizing BundleInstance\n    for event, handler in EVENTS:\n        event_handler.register(event, handler)\n\n\nclass BundleArgument(CustomArgument):\n\n    def __init__(self, storage_param, *args, **kwargs):\n        super(BundleArgument, self).__init__(*args, **kwargs)\n        self._storage_param = storage_param\n\n    def _build_storage(self, params, value):\n        # Build up the Storage data structure\n        if 'Storage' not in params:\n            params['Storage'] = {'S3': {}}\n        params['Storage']['S3'][self._storage_param] = value\n\n    def add_to_params(self, parameters, value):\n        if value:\n            self._build_storage(parameters, value)\n", "awscli/customizations/ec2/protocolarg.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nThis customization allows the user to specify the values \"tcp\", \"udp\",\nor \"icmp\" as values for the --protocol parameter.  The actual Protocol\nparameter of the operation accepts only integer protocol numbers.\n\"\"\"\n\n\ndef _fix_args(params, **kwargs):\n    key_name = 'Protocol'\n    if key_name in params:\n        if params[key_name] == 'tcp':\n            params[key_name] = '6'\n        elif params[key_name] == 'udp':\n            params[key_name] = '17'\n        elif params[key_name] == 'icmp':\n            params[key_name] = '1'\n        elif params[key_name] == 'all':\n            params[key_name] = '-1'\n\n\ndef register_protocol_args(cli):\n    cli.register('before-parameter-build.ec2.CreateNetworkAclEntry',\n                 _fix_args)\n    cli.register('before-parameter-build.ec2.ReplaceNetworkAclEntry',\n                 _fix_args)\n", "awscli/customizations/ec2/__init__.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "awscli/customizations/ec2/paginate.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\ndef register_ec2_page_size_injector(event_emitter):\n    EC2PageSizeInjector().register(event_emitter)\n\n\nclass EC2PageSizeInjector(object):\n\n    # Operations to auto-paginate and their specific whitelists.\n    # Format:\n    #    Key:   Operation\n    #    Value: List of parameters to add to whitelist for that operation.\n    TARGET_OPERATIONS = {\n        \"describe-volumes\": [],\n        \"describe-snapshots\": ['OwnerIds', 'RestorableByUserIds']\n    }\n\n    # Parameters which should be whitelisted for every operation.\n    UNIVERSAL_WHITELIST = ['NextToken', 'DryRun', 'PaginationConfig']\n\n    DEFAULT_PAGE_SIZE = 1000\n\n    def register(self, event_emitter):\n        \"\"\"Register `inject` for each target operation.\"\"\"\n        event_template = \"calling-command.ec2.%s\"\n        for operation in self.TARGET_OPERATIONS:\n            event = event_template % operation\n            event_emitter.register_last(event, self.inject)\n\n    def inject(self, event_name, parsed_globals, call_parameters, **kwargs):\n        \"\"\"Conditionally inject PageSize.\"\"\"\n        if not parsed_globals.paginate:\n            return\n\n        pagination_config = call_parameters.get('PaginationConfig', {})\n        if 'PageSize' in pagination_config:\n            return\n\n        operation_name = event_name.split('.')[-1]\n\n        whitelisted_params = self.TARGET_OPERATIONS.get(operation_name)\n        if whitelisted_params is None:\n            return\n\n        whitelisted_params = whitelisted_params + self.UNIVERSAL_WHITELIST\n\n        for param in call_parameters:\n            if param not in whitelisted_params:\n                return\n\n        pagination_config['PageSize'] = self.DEFAULT_PAGE_SIZE\n        call_parameters['PaginationConfig'] = pagination_config\n", "awscli/customizations/emr/addtags.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nfrom awscli.arguments import CustomArgument\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr import emrutils\n\n\ndef modify_tags_argument(argument_table, **kwargs):\n    argument_table['tags'] = TagsArgument('tags', required=True,\n                                          help_text=helptext.TAGS, nargs='+')\n\n\nclass TagsArgument(CustomArgument):\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        parameters['Tags'] = emrutils.parse_tags(value)\n", "awscli/customizations/emr/instancefleetsutils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import exceptions\n\n\ndef validate_and_build_instance_fleets(parsed_instance_fleets):\n    \"\"\"\n    Helper method that converts --instance-fleets option value in\n    create-cluster to Amazon Elastic MapReduce InstanceFleetConfig\n    data type.\n    \"\"\"\n    instance_fleets = []\n    for instance_fleet in parsed_instance_fleets:\n        instance_fleet_config = {}\n\n        keys = instance_fleet.keys()\n\n        if 'Name' in keys:\n            instance_fleet_config['Name'] = instance_fleet['Name']\n        else:\n            instance_fleet_config['Name'] = instance_fleet['InstanceFleetType']\n        instance_fleet_config['InstanceFleetType'] = instance_fleet['InstanceFleetType']\n\n        if 'TargetOnDemandCapacity' in keys:\n            instance_fleet_config['TargetOnDemandCapacity'] = instance_fleet['TargetOnDemandCapacity']\n\n        if 'TargetSpotCapacity' in keys:\n            instance_fleet_config['TargetSpotCapacity'] = instance_fleet['TargetSpotCapacity']\n\n        if 'InstanceTypeConfigs' in keys:\n            instance_fleet_config['InstanceTypeConfigs'] = instance_fleet['InstanceTypeConfigs']\n\n        if 'LaunchSpecifications' in keys:\n            instanceFleetProvisioningSpecifications = instance_fleet['LaunchSpecifications']\n            instance_fleet_config['LaunchSpecifications'] = {}\n\n            if 'SpotSpecification' in instanceFleetProvisioningSpecifications:\n                instance_fleet_config['LaunchSpecifications']['SpotSpecification'] = \\\n                    instanceFleetProvisioningSpecifications['SpotSpecification']\n\n            if 'OnDemandSpecification' in instanceFleetProvisioningSpecifications:\n                instance_fleet_config['LaunchSpecifications']['OnDemandSpecification'] = \\\n                    instanceFleetProvisioningSpecifications['OnDemandSpecification']\n\n        if 'ResizeSpecifications' in keys:\n            instanceFleetResizeSpecifications = instance_fleet['ResizeSpecifications']\n            instance_fleet_config['ResizeSpecifications'] = {}\n\n            if 'SpotResizeSpecification' in instanceFleetResizeSpecifications:\n                instance_fleet_config['ResizeSpecifications']['SpotResizeSpecification'] = \\\n                    instanceFleetResizeSpecifications['SpotResizeSpecification']\n\n            if 'OnDemandResizeSpecification' in instanceFleetResizeSpecifications:\n                instance_fleet_config['ResizeSpecifications']['OnDemandResizeSpecification'] = \\\n                    instanceFleetResizeSpecifications['OnDemandResizeSpecification']\n\n        instance_fleets.append(instance_fleet_config)\n    return instance_fleets\n", "awscli/customizations/emr/steputils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import exceptions\n\n\ndef build_step_config_list(parsed_step_list, region, release_label):\n    step_config_list = []\n    for step in parsed_step_list:\n        step_type = step.get('Type')\n        if step_type is None:\n            step_type = constants.CUSTOM_JAR\n\n        step_type = step_type.lower()\n        step_config = {}\n        if step_type == constants.CUSTOM_JAR:\n            step_config = build_custom_jar_step(parsed_step=step)\n        elif step_type == constants.STREAMING:\n            step_config = build_streaming_step(\n                parsed_step=step, release_label=release_label)\n        elif step_type == constants.HIVE:\n            step_config = build_hive_step(\n                parsed_step=step, region=region,\n                release_label=release_label)\n        elif step_type == constants.PIG:\n            step_config = build_pig_step(\n                parsed_step=step, region=region,\n                release_label=release_label)\n        elif step_type == constants.IMPALA:\n            step_config = build_impala_step(\n                parsed_step=step, region=region,\n                release_label=release_label)\n        elif step_type == constants.SPARK:\n            step_config = build_spark_step(\n                parsed_step=step, region=region,\n                release_label=release_label)\n        else:\n            raise exceptions.UnknownStepTypeError(step_type=step_type)\n\n        step_config_list.append(step_config)\n\n    return step_config_list\n\n\ndef build_custom_jar_step(parsed_step):\n    name = _apply_default_value(\n        arg=parsed_step.get('Name'),\n        value=constants.DEFAULT_CUSTOM_JAR_STEP_NAME)\n    action_on_failure = _apply_default_value(\n        arg=parsed_step.get('ActionOnFailure'),\n        value=constants.DEFAULT_FAILURE_ACTION)\n    emrutils.check_required_field(\n        structure=constants.CUSTOM_JAR_STEP_CONFIG,\n        name='Jar',\n        value=parsed_step.get('Jar'))\n    return emrutils.build_step(\n        jar=parsed_step.get('Jar'),\n        args=parsed_step.get('Args'),\n        name=name,\n        action_on_failure=action_on_failure,\n        main_class=parsed_step.get('MainClass'),\n        properties=emrutils.parse_key_value_string(\n            parsed_step.get('Properties')))\n\n\ndef build_streaming_step(parsed_step, release_label):\n    name = _apply_default_value(\n        arg=parsed_step.get('Name'),\n        value=constants.DEFAULT_STREAMING_STEP_NAME)\n    action_on_failure = _apply_default_value(\n        arg=parsed_step.get('ActionOnFailure'),\n        value=constants.DEFAULT_FAILURE_ACTION)\n\n    args = parsed_step.get('Args')\n    emrutils.check_required_field(\n        structure=constants.STREAMING_STEP_CONFIG,\n        name='Args',\n        value=args)\n    emrutils.check_empty_string_list(name='Args', value=args)\n    args_list = []\n\n    if release_label:\n        jar = constants.COMMAND_RUNNER\n        args_list.append(constants.HADOOP_STREAMING_COMMAND)\n    else:\n        jar = constants.HADOOP_STREAMING_PATH\n\n    args_list += args\n\n    return emrutils.build_step(\n        jar=jar,\n        args=args_list,\n        name=name,\n        action_on_failure=action_on_failure)\n\n\ndef build_hive_step(parsed_step, release_label, region=None):\n    args = parsed_step.get('Args')\n    emrutils.check_required_field(\n        structure=constants.HIVE_STEP_CONFIG, name='Args', value=args)\n    emrutils.check_empty_string_list(name='Args', value=args)\n    name = _apply_default_value(\n        arg=parsed_step.get('Name'),\n        value=constants.DEFAULT_HIVE_STEP_NAME)\n    action_on_failure = \\\n        _apply_default_value(\n            arg=parsed_step.get('ActionOnFailure'),\n            value=constants.DEFAULT_FAILURE_ACTION)\n\n    return emrutils.build_step(\n        jar=_get_runner_jar(release_label, region),\n        args=_build_hive_args(args, release_label, region),\n        name=name,\n        action_on_failure=action_on_failure)\n\n\ndef _build_hive_args(args, release_label, region):\n    args_list = []\n    if release_label:\n        args_list.append(constants.HIVE_SCRIPT_COMMAND)\n    else:\n        args_list.append(emrutils.build_s3_link(\n            relative_path=constants.HIVE_SCRIPT_PATH, region=region))\n\n    args_list.append(constants.RUN_HIVE_SCRIPT)\n\n    if not release_label:\n        args_list.append(constants.HIVE_VERSIONS)\n        args_list.append(constants.LATEST)\n\n    args_list.append(constants.ARGS)\n    args_list += args\n\n    return args_list\n\n\ndef build_pig_step(parsed_step, release_label, region=None):\n    args = parsed_step.get('Args')\n    emrutils.check_required_field(\n        structure=constants.PIG_STEP_CONFIG, name='Args', value=args)\n    emrutils.check_empty_string_list(name='Args', value=args)\n    name = _apply_default_value(\n        arg=parsed_step.get('Name'),\n        value=constants.DEFAULT_PIG_STEP_NAME)\n    action_on_failure = _apply_default_value(\n        arg=parsed_step.get('ActionOnFailure'),\n        value=constants.DEFAULT_FAILURE_ACTION)\n\n    return emrutils.build_step(\n        jar=_get_runner_jar(release_label, region),\n        args=_build_pig_args(args, release_label, region),\n        name=name,\n        action_on_failure=action_on_failure)\n\n\ndef _build_pig_args(args, release_label, region):\n    args_list = []\n    if release_label:\n        args_list.append(constants.PIG_SCRIPT_COMMAND)\n    else:\n        args_list.append(emrutils.build_s3_link(\n            relative_path=constants.PIG_SCRIPT_PATH, region=region))\n\n    args_list.append(constants.RUN_PIG_SCRIPT)\n\n    if not release_label:\n        args_list.append(constants.PIG_VERSIONS)\n        args_list.append(constants.LATEST)\n\n    args_list.append(constants.ARGS)\n    args_list += args\n\n    return args_list\n\n\ndef build_impala_step(parsed_step, release_label, region=None):\n    if release_label:\n        raise exceptions.UnknownStepTypeError(step_type=constants.IMPALA)\n    name = _apply_default_value(\n        arg=parsed_step.get('Name'),\n        value=constants.DEFAULT_IMPALA_STEP_NAME)\n    action_on_failure = _apply_default_value(\n        arg=parsed_step.get('ActionOnFailure'),\n        value=constants.DEFAULT_FAILURE_ACTION)\n    args_list = [\n        emrutils.build_s3_link(\n            relative_path=constants.IMPALA_INSTALL_PATH, region=region),\n        constants.RUN_IMPALA_SCRIPT]\n    args = parsed_step.get('Args')\n    emrutils.check_required_field(\n        structure=constants.IMPALA_STEP_CONFIG, name='Args', value=args)\n    args_list += args\n\n    return emrutils.build_step(\n        jar=emrutils.get_script_runner(region),\n        args=args_list,\n        name=name,\n        action_on_failure=action_on_failure)\n\n\ndef build_spark_step(parsed_step, release_label, region=None):\n    name = _apply_default_value(\n        arg=parsed_step.get('Name'),\n        value=constants.DEFAULT_SPARK_STEP_NAME)\n    action_on_failure = _apply_default_value(\n        arg=parsed_step.get('ActionOnFailure'),\n        value=constants.DEFAULT_FAILURE_ACTION)\n    args = parsed_step.get('Args')\n    emrutils.check_required_field(\n        structure=constants.SPARK_STEP_CONFIG, name='Args', value=args)\n\n    return emrutils.build_step(\n        jar=_get_runner_jar(release_label, region),\n        args=_build_spark_args(args, release_label, region),\n        name=name,\n        action_on_failure=action_on_failure)\n\n\ndef _build_spark_args(args, release_label, region):\n    args_list = []\n    if release_label:\n        args_list.append(constants.SPARK_SUBMIT_COMMAND)\n    else:\n        args_list.append(constants.SPARK_SUBMIT_PATH)\n    args_list += args\n\n    return args_list\n\n\ndef _apply_default_value(arg, value):\n    if arg is None:\n        arg = value\n\n    return arg\n\n\ndef _get_runner_jar(release_label, region):\n    return constants.COMMAND_RUNNER if release_label \\\n        else emrutils.get_script_runner(region)\n", "awscli/customizations/emr/addsteps.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import argumentschema\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr import steputils\nfrom awscli.customizations.emr.command import Command\n\n\nclass AddSteps(Command):\n    NAME = 'add-steps'\n    DESCRIPTION = ('Add a list of steps to a cluster.')\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': helptext.CLUSTER_ID\n         },\n        {'name': 'steps',\n         'required': True,\n         'nargs': '+',\n         'schema': argumentschema.STEPS_SCHEMA,\n         'help_text': helptext.STEPS\n         },\n        {'name': 'execution-role-arn',\n         'required': False,\n         'help_text': helptext.EXECUTION_ROLE_ARN\n         }\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        parsed_steps = parsed_args.steps\n\n        release_label = emrutils.get_release_label(\n            parsed_args.cluster_id, self._session, self.region,\n            parsed_globals.endpoint_url, parsed_globals.verify_ssl)\n\n        step_list = steputils.build_step_config_list(\n            parsed_step_list=parsed_steps, region=self.region,\n            release_label=release_label)\n        parameters = {\n            'JobFlowId': parsed_args.cluster_id,\n            'Steps': step_list\n        }\n\n        if parsed_args.execution_role_arn is not None:\n            parameters['ExecutionRoleArn'] = parsed_args.execution_role_arn\n\n        emrutils.call_and_display_response(self._session, 'AddJobFlowSteps',\n                                           parameters, parsed_globals)\n        return 0\n", "awscli/customizations/emr/command.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.emr import config\nfrom awscli.customizations.emr import configutils\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import exceptions\n\nLOG = logging.getLogger(__name__)\n\n\nclass Command(BasicCommand):\n    region = None\n\n    UNSUPPORTED_COMMANDS_FOR_RELEASE_BASED_CLUSTERS = set([\n        'install-applications',\n        'restore-from-hbase-backup',\n        'schedule-hbase-backup',\n        'create-hbase-backup',\n        'disable-hbase-backups',\n    ])\n\n    def supports_arg(self, name):\n        return any((x['name'] == name for x in self.ARG_TABLE))\n\n    def _run_main(self, parsed_args, parsed_globals):\n\n        self._apply_configs(parsed_args,\n                            configutils.get_configs(self._session))\n        self.region = emrutils.get_region(self._session, parsed_globals)\n        self._validate_unsupported_commands_for_release_based_clusters(\n            parsed_args, parsed_globals)\n        return self._run_main_command(parsed_args, parsed_globals)\n\n    def _apply_configs(self, parsed_args, parsed_configs):\n        applicable_configurations = \\\n            self._get_applicable_configurations(parsed_args, parsed_configs)\n\n        configs_added = {}\n        for configuration in applicable_configurations:\n            configuration.add(self, parsed_args,\n                              parsed_configs[configuration.name])\n            configs_added[configuration.name] = \\\n                parsed_configs[configuration.name]\n\n        if configs_added:\n            LOG.debug(\"Updated arguments with configs: %s\" % configs_added)\n        else:\n            LOG.debug(\"No configs applied\")\n        LOG.debug(\"Running command with args: %s\" % parsed_args)\n\n    def _get_applicable_configurations(self, parsed_args, parsed_configs):\n        # We need to find the applicable configurations by applying\n        # following filters:\n        # 1. Configurations that are applicable to this command\n        # 3. Configurations that are present in parsed_configs\n        # 2. Configurations that are not present in parsed_args\n\n        configurations = \\\n            config.get_applicable_configurations(self)\n\n        configurations = [x for x in configurations\n                          if x.name in parsed_configs and\n                          not x.is_present(parsed_args)]\n\n        configurations = self._filter_configurations_in_special_cases(\n            configurations, parsed_args, parsed_configs)\n\n        return configurations\n\n    def _filter_configurations_in_special_cases(self, configurations,\n                                                parsed_args, parsed_configs):\n        # Subclasses can override this method to filter the applicable\n        # configurations further based upon some custom logic\n        # Default behavior is to return the configurations list as is\n        return configurations\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        # Subclasses should implement this method.\n        # parsed_globals are the parsed global args (things like region,\n        # profile, output, etc.)\n        # parsed_args are any arguments you've defined in your ARG_TABLE\n        # that are parsed.\n        # parsed_args are updated to include any emr specific configuration\n        # from the config file if the corresponding argument is not\n        # explicitly specified on the CLI\n        raise NotImplementedError(\"_run_main_command\")\n\n    def _validate_unsupported_commands_for_release_based_clusters(\n            self, parsed_args, parsed_globals):\n        command = self.NAME\n\n        if (command in self.UNSUPPORTED_COMMANDS_FOR_RELEASE_BASED_CLUSTERS and\n                hasattr(parsed_args, 'cluster_id')):\n            release_label = emrutils.get_release_label(\n                parsed_args.cluster_id, self._session, self.region,\n                parsed_globals.endpoint_url, parsed_globals.verify_ssl)\n            if release_label:\n                raise exceptions.UnsupportedCommandWithReleaseError(\n                    command=command,\n                    release_label=release_label)\n\n\ndef override_args_required_option(argument_table, args, session, **kwargs):\n    # This function overrides the 'required' property of an argument\n    # if a value corresponding to that argument is present in the config\n    # file\n    # We don't want to override when user is viewing the help so that we\n    # can show the required options correctly in the help\n    need_to_override = False if len(args) == 1 and args[0] == 'help' \\\n        else True\n\n    if need_to_override:\n        parsed_configs = configutils.get_configs(session)\n        for arg_name in argument_table.keys():\n            if arg_name.replace('-', '_') in parsed_configs:\n                argument_table[arg_name].required = False\n", "awscli/customizations/emr/configutils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\n\nfrom awscli.customizations.configure.writer import ConfigFileWriter\nfrom awscli.customizations.emr.constants import EC2_ROLE_NAME\nfrom awscli.customizations.emr.constants import EMR_ROLE_NAME\n\nLOG = logging.getLogger(__name__)\n\n\ndef get_configs(session):\n    return session.get_scoped_config().get('emr', {})\n\n\ndef get_current_profile_name(session):\n    profile_name = session.get_config_variable('profile')\n    return 'default' if profile_name is None else profile_name\n\n\ndef get_current_profile_var_name(session):\n    return _get_profile_str(session, '.')\n\n\ndef _get_profile_str(session, separator):\n    profile_name = session.get_config_variable('profile')\n    return 'default' if profile_name is None \\\n        else 'profile%c%s' % (separator, profile_name)\n\n\ndef is_any_role_configured(session):\n    parsed_configs = get_configs(session)\n    return True if ('instance_profile' in parsed_configs or\n                    'service_role' in parsed_configs) \\\n        else False\n\n\ndef update_roles(session):\n    if is_any_role_configured(session):\n        LOG.debug(\"At least one of the roles is already associated with \"\n                  \"your current profile \")\n    else:\n        config_writer = ConfigWriter(session)\n        config_writer.update_config('service_role', EMR_ROLE_NAME)\n        config_writer.update_config('instance_profile', EC2_ROLE_NAME)\n        LOG.debug(\"Associated default roles with your current profile\")\n\n\nclass ConfigWriter(object):\n\n    def __init__(self, session):\n        self.session = session\n        self.section = _get_profile_str(session, ' ')\n        self.config_file_writer = ConfigFileWriter()\n\n    def update_config(self, key, value):\n        config_filename = \\\n            os.path.expanduser(self.session.get_config_variable('config_file'))\n        updated_config = {'__section__': self.section,\n                          'emr': {key: value}}\n        self.config_file_writer.update_config(updated_config, config_filename)\n", "awscli/customizations/emr/config.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\nfrom awscli.customizations.emr import configutils\nfrom awscli.customizations.emr import exceptions\n\nLOG = logging.getLogger(__name__)\n\nSUPPORTED_CONFIG_LIST = [\n    {'name': 'service_role'},\n    {'name': 'log_uri'},\n    {'name': 'instance_profile', 'arg_name': 'ec2_attributes',\n     'arg_value_key': 'InstanceProfile'},\n    {'name': 'key_name', 'arg_name': 'ec2_attributes',\n     'arg_value_key': 'KeyName'},\n    {'name': 'enable_debugging', 'type': 'boolean'},\n    {'name': 'key_pair_file'}\n]\n\nTYPES = ['string', 'boolean']\n\n\ndef get_applicable_configurations(command):\n    supported_configurations = _create_supported_configurations()\n    return [x for x in supported_configurations if x.is_applicable(command)]\n\n\ndef _create_supported_configuration(config):\n    config_type = config['type'] if 'type' in config else 'string'\n\n    if (config_type == 'string'):\n        config_arg_name = config['arg_name'] \\\n            if 'arg_name' in config else config['name']\n        config_arg_value_key = config['arg_value_key'] \\\n            if 'arg_value_key' in config else None\n        configuration = StringConfiguration(config['name'],\n                                            config_arg_name,\n                                            config_arg_value_key)\n    elif (config_type == 'boolean'):\n        configuration = BooleanConfiguration(config['name'])\n\n    return configuration\n\n\ndef _create_supported_configurations():\n    return [_create_supported_configuration(config)\n            for config in SUPPORTED_CONFIG_LIST]\n\n\nclass Configuration(object):\n\n    def __init__(self, name, arg_name):\n        self.name = name\n        self.arg_name = arg_name\n\n    def is_applicable(self, command):\n        raise NotImplementedError(\"is_applicable\")\n\n    def is_present(self, parsed_args):\n        raise NotImplementedError(\"is_present\")\n\n    def add(self, command, parsed_args, value):\n        raise NotImplementedError(\"add\")\n\n    def _check_arg(self, parsed_args, arg_name):\n        return getattr(parsed_args, arg_name, None)\n\n\nclass StringConfiguration(Configuration):\n\n    def __init__(self, name, arg_name, arg_value_key=None):\n        super(StringConfiguration, self).__init__(name, arg_name)\n        self.arg_value_key = arg_value_key\n\n    def is_applicable(self, command):\n        return command.supports_arg(self.arg_name.replace('_', '-'))\n\n    def is_present(self, parsed_args):\n        if (not self.arg_value_key):\n            return self._check_arg(parsed_args, self.arg_name)\n        else:\n            return self._check_arg(parsed_args, self.arg_name) \\\n                and self.arg_value_key in getattr(parsed_args, self.arg_name)\n\n    def add(self, command, parsed_args, value):\n        if (not self.arg_value_key):\n            setattr(parsed_args, self.arg_name, value)\n        else:\n            if (not self._check_arg(parsed_args, self.arg_name)):\n                setattr(parsed_args, self.arg_name, {})\n            getattr(parsed_args, self.arg_name)[self.arg_value_key] = value\n\n\nclass BooleanConfiguration(Configuration):\n\n    def __init__(self, name):\n        super(BooleanConfiguration, self).__init__(name, name)\n        self.no_version_arg_name = \"no_\" + name\n\n    def is_applicable(self, command):\n        return command.supports_arg(self.arg_name.replace('_', '-')) and \\\n            command.supports_arg(self.no_version_arg_name.replace('_', '-'))\n\n    def is_present(self, parsed_args):\n        return self._check_arg(parsed_args, self.arg_name) \\\n            or self._check_arg(parsed_args, self.no_version_arg_name)\n\n    def add(self, command, parsed_args, value):\n        if (value.lower() == 'true'):\n            setattr(parsed_args, self.arg_name, True)\n            setattr(parsed_args, self.no_version_arg_name, False)\n        elif (value.lower() == 'false'):\n            setattr(parsed_args, self.arg_name, False)\n            setattr(parsed_args, self.no_version_arg_name, True)\n        else:\n            raise exceptions.InvalidBooleanConfigError(\n                config_value=value,\n                config_key=self.arg_name,\n                profile_var_name=configutils.get_current_profile_var_name(\n                    command._session))\n", "awscli/customizations/emr/modifyclusterattributes.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import exceptions\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr.command import Command\n\n\nclass ModifyClusterAttr(Command):\n    NAME = 'modify-cluster-attributes'\n    DESCRIPTION = (\"Modifies the cluster attributes 'visible-to-all-users', \"\n                   \" 'termination-protected' and 'unhealthy-node-replacement'.\")\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n            'help_text': helptext.CLUSTER_ID},\n        {'name': 'visible-to-all-users', 'required': False, 'action':\n            'store_true', 'group_name': 'visible',\n            'help_text': helptext.VISIBILITY},\n        {'name': 'no-visible-to-all-users', 'required': False, 'action':\n            'store_true', 'group_name': 'visible',\n            'help_text': helptext.VISIBILITY},\n        {'name': 'termination-protected', 'required': False, 'action':\n            'store_true', 'group_name': 'terminate',\n            'help_text': 'Set termination protection on or off'},\n        {'name': 'no-termination-protected', 'required': False, 'action':\n            'store_true', 'group_name': 'terminate',\n            'help_text': 'Set termination protection on or off'},\n        {'name': 'auto-terminate', 'required': False, 'action':\n            'store_true', 'group_name': 'auto_terminate',\n            'help_text': 'Set cluster auto terminate after completing all the steps on or off'},\n        {'name': 'no-auto-terminate', 'required': False, 'action':\n            'store_true', 'group_name': 'auto_terminate',\n            'help_text': 'Set cluster auto terminate after completing all the steps on or off'},\n        {'name': 'unhealthy-node-replacement', 'required': False, 'action':\n            'store_true', 'group_name': 'UnhealthyReplacement',\n            'help_text': 'Set Unhealthy Node Replacement on or off'},\n        {'name': 'no-unhealthy-node-replacement', 'required': False, 'action':\n            'store_true', 'group_name': 'UnhealthyReplacement',\n            'help_text': 'Set Unhealthy Node Replacement on or off'},\n    ]\n\n    def _run_main_command(self, args, parsed_globals):\n\n        if (args.visible_to_all_users and args.no_visible_to_all_users):\n            raise exceptions.MutualExclusiveOptionError(\n                option1='--visible-to-all-users',\n                option2='--no-visible-to-all-users')\n        if (args.termination_protected and args.no_termination_protected):\n            raise exceptions.MutualExclusiveOptionError(\n                option1='--termination-protected',\n                option2='--no-termination-protected')\n        if (args.auto_terminate and args.no_auto_terminate):\n            raise exceptions.MutualExclusiveOptionError(\n                option1='--auto-terminate',\n                option2='--no-auto-terminate')\n        if (args.unhealthy_node_replacement and args.no_unhealthy_node_replacement):\n            raise exceptions.MutualExclusiveOptionError(\n                option1='--unhealthy-node-replacement',\n                option2='--no-unhealthy-node-replacement')\n        if not(args.termination_protected or args.no_termination_protected or\n               args.visible_to_all_users or args.no_visible_to_all_users or\n               args.auto_terminate or args.no_auto_terminate or\n               args.unhealthy_node_replacement or args.no_unhealthy_node_replacement):\n            raise exceptions.MissingClusterAttributesError()\n\n        if (args.visible_to_all_users or args.no_visible_to_all_users):\n            visible = (args.visible_to_all_users and\n                       not args.no_visible_to_all_users)\n            parameters = {'JobFlowIds': [args.cluster_id],\n                          'VisibleToAllUsers': visible}\n            emrutils.call_and_display_response(self._session,\n                                               'SetVisibleToAllUsers',\n                                               parameters, parsed_globals)\n\n        if (args.termination_protected or args.no_termination_protected):\n            protected = (args.termination_protected and\n                         not args.no_termination_protected)\n            parameters = {'JobFlowIds': [args.cluster_id],\n                          'TerminationProtected': protected}\n            emrutils.call_and_display_response(self._session,\n                                               'SetTerminationProtection',\n                                               parameters, parsed_globals)\n\n        if (args.auto_terminate or args.no_auto_terminate):\n            auto_terminate = (args.auto_terminate and\n                         not args.no_auto_terminate)\n            parameters = {'JobFlowIds': [args.cluster_id],\n                          'KeepJobFlowAliveWhenNoSteps': not auto_terminate}\n            emrutils.call_and_display_response(self._session,\n                                               'SetKeepJobFlowAliveWhenNoSteps',\n                                               parameters, parsed_globals)\n            \n        if (args.unhealthy_node_replacement or args.no_unhealthy_node_replacement):\n            protected = (args.unhealthy_node_replacement and\n                         not args.no_unhealthy_node_replacement)\n            parameters = {'JobFlowIds': [args.cluster_id],\n                          'UnhealthyNodeReplacement': protected}\n            emrutils.call_and_display_response(self._session,\n                                               'SetUnhealthyNodeReplacement',\n                                               parameters, parsed_globals)\n\n        return 0\n", "awscli/customizations/emr/createcluster.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport re\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.emr import applicationutils\nfrom awscli.customizations.emr import argumentschema\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import emrfsutils\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import exceptions\nfrom awscli.customizations.emr import hbaseutils\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr import instancegroupsutils\nfrom awscli.customizations.emr import instancefleetsutils\nfrom awscli.customizations.emr import steputils\nfrom awscli.customizations.emr.command import Command\nfrom awscli.customizations.emr.constants import EC2_ROLE_NAME\nfrom awscli.customizations.emr.constants import EMR_ROLE_NAME\nfrom botocore.compat import json\n\n\nclass CreateCluster(Command):\n    NAME = 'create-cluster'\n    DESCRIPTION = helptext.CREATE_CLUSTER_DESCRIPTION\n    ARG_TABLE = [\n        {'name': 'release-label',\n         'help_text': helptext.RELEASE_LABEL},\n        {'name': 'os-release-label',\n         'help_text': helptext.OS_RELEASE_LABEL},\n        {'name': 'ami-version',\n         'help_text': helptext.AMI_VERSION},\n        {'name': 'instance-groups',\n         'schema': argumentschema.INSTANCE_GROUPS_SCHEMA,\n         'help_text': helptext.INSTANCE_GROUPS},\n        {'name': 'instance-type',\n         'help_text': helptext.INSTANCE_TYPE},\n        {'name': 'instance-count',\n         'help_text': helptext.INSTANCE_COUNT},\n        {'name': 'auto-terminate', 'action': 'store_true',\n         'group_name': 'auto_terminate',\n         'help_text': helptext.AUTO_TERMINATE},\n        {'name': 'no-auto-terminate', 'action': 'store_true',\n         'group_name': 'auto_terminate'},\n        {'name': 'instance-fleets',\n         'schema': argumentschema.INSTANCE_FLEETS_SCHEMA,\n         'help_text': helptext.INSTANCE_FLEETS},\n        {'name': 'name',\n         'default': 'Development Cluster',\n         'help_text': helptext.CLUSTER_NAME},\n        {'name': 'log-uri',\n         'help_text': helptext.LOG_URI},\n        {'name': 'log-encryption-kms-key-id',\n         'help_text': helptext.LOG_ENCRYPTION_KMS_KEY_ID},\n        {'name': 'service-role',\n         'help_text': helptext.SERVICE_ROLE},\n        {'name': 'auto-scaling-role',\n         'help_text': helptext.AUTOSCALING_ROLE},\n        {'name': 'use-default-roles', 'action': 'store_true',\n         'help_text': helptext.USE_DEFAULT_ROLES},\n        {'name': 'configurations',\n         'help_text': helptext.CONFIGURATIONS},\n        {'name': 'ec2-attributes',\n         'help_text': helptext.EC2_ATTRIBUTES,\n         'schema': argumentschema.EC2_ATTRIBUTES_SCHEMA},\n        {'name': 'termination-protected', 'action': 'store_true',\n         'group_name': 'termination_protected',\n         'help_text': helptext.TERMINATION_PROTECTED},\n        {'name': 'no-termination-protected', 'action': 'store_true',\n         'group_name': 'termination_protected'},\n        {'name': 'unhealthy-node-replacement', 'action': 'store_true',\n        'group_name': 'unhealthy_node_replacement',\n        'help_text': helptext.UNHEALTHY_NODE_REPLACEMENT},\n        {'name': 'no-unhealthy-node-replacement', 'action': 'store_true',\n        'group_name': 'unhealthy_node_replacement'},\n        {'name': 'scale-down-behavior',\n         'help_text': helptext.SCALE_DOWN_BEHAVIOR},\n        {'name': 'visible-to-all-users', 'action': 'store_true',\n         'group_name': 'visibility',\n         'help_text': helptext.VISIBILITY},\n        {'name': 'no-visible-to-all-users', 'action': 'store_true',\n         'group_name': 'visibility'},\n        {'name': 'enable-debugging', 'action': 'store_true',\n         'group_name': 'debug',\n         'help_text': helptext.DEBUGGING},\n        {'name': 'no-enable-debugging', 'action': 'store_true',\n         'group_name': 'debug'},\n        {'name': 'tags', 'nargs': '+',\n         'help_text': helptext.TAGS,\n         'schema': argumentschema.TAGS_SCHEMA},\n        {'name': 'bootstrap-actions',\n         'help_text': helptext.BOOTSTRAP_ACTIONS,\n         'schema': argumentschema.BOOTSTRAP_ACTIONS_SCHEMA},\n        {'name': 'applications',\n         'help_text': helptext.APPLICATIONS,\n         'schema': argumentschema.APPLICATIONS_SCHEMA},\n        {'name': 'emrfs',\n         'help_text': helptext.EMR_FS,\n         'schema': argumentschema.EMR_FS_SCHEMA},\n        {'name': 'steps',\n         'schema': argumentschema.STEPS_SCHEMA,\n         'help_text': helptext.STEPS},\n        {'name': 'additional-info',\n         'help_text': helptext.ADDITIONAL_INFO},\n        {'name': 'restore-from-hbase-backup',\n         'schema': argumentschema.HBASE_RESTORE_FROM_BACKUP_SCHEMA,\n         'help_text': helptext.RESTORE_FROM_HBASE},\n        {'name': 'security-configuration',\n         'help_text': helptext.SECURITY_CONFIG},\n        {'name': 'custom-ami-id',\n         'help_text' : helptext.CUSTOM_AMI_ID},\n        {'name': 'ebs-root-volume-size',\n         'help_text' : helptext.EBS_ROOT_VOLUME_SIZE},\n        {'name': 'ebs-root-volume-iops',\n         'help_text' : helptext.EBS_ROOT_VOLUME_IOPS},\n        {'name': 'ebs-root-volume-throughput',\n         'help_text' : helptext.EBS_ROOT_VOLUME_THROUGHPUT},\n        {'name': 'repo-upgrade-on-boot',\n         'help_text' : helptext.REPO_UPGRADE_ON_BOOT},\n        {'name': 'kerberos-attributes',\n         'schema': argumentschema.KERBEROS_ATTRIBUTES_SCHEMA,\n         'help_text': helptext.KERBEROS_ATTRIBUTES},\n        {'name': 'step-concurrency-level',\n         'cli_type_name': 'integer',\n         'help_text': helptext.STEP_CONCURRENCY_LEVEL},\n        {'name': 'managed-scaling-policy',\n         'schema': argumentschema.MANAGED_SCALING_POLICY_SCHEMA,\n         'help_text': helptext.MANAGED_SCALING_POLICY},\n        {'name': 'placement-group-configs',\n         'schema': argumentschema.PLACEMENT_GROUP_CONFIGS_SCHEMA,\n         'help_text': helptext.PLACEMENT_GROUP_CONFIGS},\n        {'name': 'auto-termination-policy',\n         'schema': argumentschema.AUTO_TERMINATION_POLICY_SCHEMA,\n         'help_text': helptext.AUTO_TERMINATION_POLICY}\n    ]\n    SYNOPSIS = BasicCommand.FROM_FILE('emr', 'create-cluster-synopsis.txt')\n    EXAMPLES = BasicCommand.FROM_FILE('emr', 'create-cluster-examples.rst')\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        params = {}\n        params['Name'] = parsed_args.name\n\n        self._validate_release_label_ami_version(parsed_args)\n\n        service_role_validation_message = (\n            \" Either choose --use-default-roles or use both --service-role \"\n            \"<roleName> and --ec2-attributes InstanceProfile=<profileName>.\")\n\n        if parsed_args.use_default_roles is True and \\\n                parsed_args.service_role is not None:\n            raise exceptions.MutualExclusiveOptionError(\n                option1=\"--use-default-roles\",\n                option2=\"--service-role\",\n                message=service_role_validation_message)\n\n        if parsed_args.use_default_roles is True and \\\n                parsed_args.ec2_attributes is not None and \\\n                'InstanceProfile' in parsed_args.ec2_attributes:\n            raise exceptions.MutualExclusiveOptionError(\n                option1=\"--use-default-roles\",\n                option2=\"--ec2-attributes InstanceProfile\",\n                message=service_role_validation_message)\n\n        if parsed_args.instance_groups is not None and \\\n                parsed_args.instance_fleets is not None:\n            raise exceptions.MutualExclusiveOptionError(\n                option1=\"--instance-groups\",\n                option2=\"--instance-fleets\")\n\n        instances_config = {}\n        if parsed_args.instance_fleets is not None:\n            instances_config['InstanceFleets'] = \\\n                instancefleetsutils.validate_and_build_instance_fleets(\n                    parsed_args.instance_fleets)\n        else:\n            instances_config['InstanceGroups'] = \\\n                instancegroupsutils.validate_and_build_instance_groups(\n                    instance_groups=parsed_args.instance_groups,\n                    instance_type=parsed_args.instance_type,\n                    instance_count=parsed_args.instance_count)\n\n        if parsed_args.release_label is not None:\n            params[\"ReleaseLabel\"] = parsed_args.release_label\n            if parsed_args.configurations is not None:\n                try:\n                    params[\"Configurations\"] = json.loads(\n                        parsed_args.configurations)\n                except ValueError:\n                    raise ValueError('aws: error: invalid json argument for '\n                                     'option --configurations')\n\n        if (parsed_args.release_label is None and\n                parsed_args.ami_version is not None):\n            is_valid_ami_version = re.match('\\d?\\..*', parsed_args.ami_version)\n            if is_valid_ami_version is None:\n                raise exceptions.InvalidAmiVersionError(\n                    ami_version=parsed_args.ami_version)\n            params['AmiVersion'] = parsed_args.ami_version\n        emrutils.apply_dict(\n            params, 'AdditionalInfo', parsed_args.additional_info)\n        emrutils.apply_dict(params, 'LogUri', parsed_args.log_uri)\n\n        if parsed_args.os_release_label is not None:\n            emrutils.apply_dict(params, 'OSReleaseLabel',\n                parsed_args.os_release_label)\n\n        if parsed_args.log_encryption_kms_key_id is not None:\n            emrutils.apply_dict(params, 'LogEncryptionKmsKeyId',\n                parsed_args.log_encryption_kms_key_id)\n\n        if parsed_args.use_default_roles is True:\n            parsed_args.service_role = EMR_ROLE_NAME\n            if parsed_args.ec2_attributes is None:\n                parsed_args.ec2_attributes = {}\n            parsed_args.ec2_attributes['InstanceProfile'] = EC2_ROLE_NAME\n\n        emrutils.apply_dict(params, 'ServiceRole', parsed_args.service_role)\n\n        if parsed_args.instance_groups is not None:\n            for instance_group in instances_config['InstanceGroups']:\n                if 'AutoScalingPolicy' in instance_group.keys():\n                    if parsed_args.auto_scaling_role is None:\n                        raise exceptions.MissingAutoScalingRoleError()\n\n        emrutils.apply_dict(params, 'AutoScalingRole', parsed_args.auto_scaling_role)\n\n        if parsed_args.scale_down_behavior is not None:\n            emrutils.apply_dict(params, 'ScaleDownBehavior', parsed_args.scale_down_behavior)\n\n        if (\n                parsed_args.no_auto_terminate is False and\n                parsed_args.auto_terminate is False):\n            parsed_args.no_auto_terminate = True\n\n        instances_config['KeepJobFlowAliveWhenNoSteps'] = \\\n            emrutils.apply_boolean_options(\n                parsed_args.no_auto_terminate,\n                '--no-auto-terminate',\n                parsed_args.auto_terminate,\n                '--auto-terminate')\n\n        instances_config['TerminationProtected'] = \\\n            emrutils.apply_boolean_options(\n                parsed_args.termination_protected,\n                '--termination-protected',\n                parsed_args.no_termination_protected,\n                '--no-termination-protected')\n        \n        if (parsed_args.unhealthy_node_replacement or parsed_args.no_unhealthy_node_replacement):\n            instances_config['UnhealthyNodeReplacement'] = \\\n            emrutils.apply_boolean_options(\n                parsed_args.unhealthy_node_replacement,\n                '--unhealthy-node-replacement',\n                parsed_args.no_unhealthy_node_replacement,\n                '--no-unhealthy-node-replacement')\n\n        if (parsed_args.visible_to_all_users is False and\n                parsed_args.no_visible_to_all_users is False):\n            parsed_args.visible_to_all_users = True\n\n        params['VisibleToAllUsers'] = \\\n            emrutils.apply_boolean_options(\n                parsed_args.visible_to_all_users,\n                '--visible-to-all-users',\n                parsed_args.no_visible_to_all_users,\n                '--no-visible-to-all-users')\n\n        params['Tags'] = emrutils.parse_tags(parsed_args.tags)\n        params['Instances'] = instances_config\n\n        if parsed_args.ec2_attributes is not None:\n            self._build_ec2_attributes(\n                cluster=params, parsed_attrs=parsed_args.ec2_attributes)\n\n        debugging_enabled = emrutils.apply_boolean_options(\n            parsed_args.enable_debugging,\n            '--enable-debugging',\n            parsed_args.no_enable_debugging,\n            '--no-enable-debugging')\n\n        if parsed_args.log_uri is None and debugging_enabled is True:\n            raise exceptions.LogUriError\n\n        if debugging_enabled is True:\n            self._update_cluster_dict(\n                cluster=params,\n                key='Steps',\n                value=[\n                    self._build_enable_debugging(parsed_args, parsed_globals)])\n\n        if parsed_args.applications is not None:\n            if parsed_args.release_label is None:\n                app_list, ba_list, step_list = \\\n                    applicationutils.build_applications(\n                        region=self.region,\n                        parsed_applications=parsed_args.applications,\n                        ami_version=params['AmiVersion'])\n                self._update_cluster_dict(\n                    params, 'NewSupportedProducts', app_list)\n                self._update_cluster_dict(\n                    params, 'BootstrapActions', ba_list)\n                self._update_cluster_dict(\n                    params, 'Steps', step_list)\n            else:\n                params[\"Applications\"] = []\n                for application in parsed_args.applications:\n                    params[\"Applications\"].append(application)\n\n        hbase_restore_config = parsed_args.restore_from_hbase_backup\n        if hbase_restore_config is not None:\n            args = hbaseutils.build_hbase_restore_from_backup_args(\n                dir=hbase_restore_config.get('Dir'),\n                backup_version=hbase_restore_config.get('BackupVersion'))\n            step_config = emrutils.build_step(\n                jar=constants.HBASE_JAR_PATH,\n                name=constants.HBASE_RESTORE_STEP_NAME,\n                action_on_failure=constants.CANCEL_AND_WAIT,\n                args=args)\n            self._update_cluster_dict(\n                params, 'Steps', [step_config])\n\n        if parsed_args.bootstrap_actions is not None:\n            self._build_bootstrap_actions(\n                cluster=params,\n                parsed_boostrap_actions=parsed_args.bootstrap_actions)\n\n        if parsed_args.emrfs is not None:\n            self._handle_emrfs_parameters(\n                cluster=params,\n                emrfs_args=parsed_args.emrfs,\n                release_label=parsed_args.release_label)\n\n        if parsed_args.steps is not None:\n            steps_list = steputils.build_step_config_list(\n                parsed_step_list=parsed_args.steps,\n                region=self.region,\n                release_label=parsed_args.release_label)\n            self._update_cluster_dict(\n                cluster=params, key='Steps', value=steps_list)\n\n        if parsed_args.security_configuration is not None:\n            emrutils.apply_dict(\n                params, 'SecurityConfiguration', parsed_args.security_configuration)\n\n        if parsed_args.custom_ami_id is not None:\n            emrutils.apply_dict(\n                params, 'CustomAmiId', parsed_args.custom_ami_id\n            )\n        if parsed_args.ebs_root_volume_size is not None:\n            emrutils.apply_dict(\n                params, 'EbsRootVolumeSize', int(parsed_args.ebs_root_volume_size)\n            )\n        if parsed_args.ebs_root_volume_iops is not None:\n            emrutils.apply_dict(\n                params, 'EbsRootVolumeIops', int(parsed_args.ebs_root_volume_iops)\n            )\n        if parsed_args.ebs_root_volume_throughput is not None:\n            emrutils.apply_dict(\n                params, 'EbsRootVolumeThroughput', int(parsed_args.ebs_root_volume_throughput)\n            )\n\n        if parsed_args.repo_upgrade_on_boot is not None:\n            emrutils.apply_dict(\n                params, 'RepoUpgradeOnBoot', parsed_args.repo_upgrade_on_boot\n            )\n\n        if parsed_args.kerberos_attributes is not None:\n            emrutils.apply_dict(\n                params, 'KerberosAttributes', parsed_args.kerberos_attributes)\n\n        if parsed_args.step_concurrency_level is not None:\n            params['StepConcurrencyLevel'] = parsed_args.step_concurrency_level\n\n        if parsed_args.managed_scaling_policy is not None:\n            emrutils.apply_dict(\n                params, 'ManagedScalingPolicy', parsed_args.managed_scaling_policy)\n\n        if parsed_args.placement_group_configs is not None:\n            emrutils.apply_dict(\n                params, 'PlacementGroupConfigs',\n                parsed_args.placement_group_configs)\n\n        if parsed_args.auto_termination_policy is not None:\n            emrutils.apply_dict(\n                params, 'AutoTerminationPolicy',\n                parsed_args.auto_termination_policy)\n\n        self._validate_required_applications(parsed_args)\n\n        run_job_flow_response = emrutils.call(\n            self._session, 'run_job_flow', params, self.region,\n            parsed_globals.endpoint_url, parsed_globals.verify_ssl)\n\n        constructed_result = self._construct_result(run_job_flow_response)\n        emrutils.display_response(self._session, 'run_job_flow',\n                                  constructed_result, parsed_globals)\n\n        return 0\n\n    def _construct_result(self, run_job_flow_result):\n        jobFlowId = None\n        clusterArn = None\n        if run_job_flow_result is not None:\n            jobFlowId = run_job_flow_result.get('JobFlowId')\n            clusterArn = run_job_flow_result.get('ClusterArn')\n\n        if jobFlowId is not None:\n            return {'ClusterId': jobFlowId,\n                    'ClusterArn': clusterArn }\n        else:\n            return {}\n\n    def _build_ec2_attributes(self, cluster, parsed_attrs):\n        keys = parsed_attrs.keys()\n        instances = cluster['Instances']\n\n        if ('SubnetId' in keys and 'SubnetIds' in keys):\n            raise exceptions.MutualExclusiveOptionError(\n                option1=\"SubnetId\",\n                option2=\"SubnetIds\")\n\n        if ('AvailabilityZone' in keys and 'AvailabilityZones' in keys):\n            raise exceptions.MutualExclusiveOptionError(\n                option1=\"AvailabilityZone\",\n                option2=\"AvailabilityZones\")\n\n        if ('SubnetId' in keys or 'SubnetIds' in keys) \\\n                and ('AvailabilityZone' in keys or 'AvailabilityZones' in keys):\n            raise exceptions.SubnetAndAzValidationError\n\n        emrutils.apply_params(\n            src_params=parsed_attrs, src_key='KeyName',\n            dest_params=instances, dest_key='Ec2KeyName')\n        emrutils.apply_params(\n            src_params=parsed_attrs, src_key='SubnetId',\n            dest_params=instances, dest_key='Ec2SubnetId')\n        emrutils.apply_params(\n            src_params=parsed_attrs, src_key='SubnetIds',\n            dest_params=instances, dest_key='Ec2SubnetIds')\n\n        if 'AvailabilityZone' in keys:\n            instances['Placement'] = dict()\n            emrutils.apply_params(\n                src_params=parsed_attrs, src_key='AvailabilityZone',\n                dest_params=instances['Placement'],\n                dest_key='AvailabilityZone')\n\n        if 'AvailabilityZones' in keys:\n            instances['Placement'] = dict()\n            emrutils.apply_params(\n                src_params=parsed_attrs, src_key='AvailabilityZones',\n                dest_params=instances['Placement'],\n                dest_key='AvailabilityZones')\n\n        emrutils.apply_params(\n            src_params=parsed_attrs, src_key='InstanceProfile',\n            dest_params=cluster, dest_key='JobFlowRole')\n\n        emrutils.apply_params(\n            src_params=parsed_attrs, src_key='EmrManagedMasterSecurityGroup',\n            dest_params=instances, dest_key='EmrManagedMasterSecurityGroup')\n\n        emrutils.apply_params(\n            src_params=parsed_attrs, src_key='EmrManagedSlaveSecurityGroup',\n            dest_params=instances, dest_key='EmrManagedSlaveSecurityGroup')\n\n        emrutils.apply_params(\n            src_params=parsed_attrs, src_key='ServiceAccessSecurityGroup',\n            dest_params=instances, dest_key='ServiceAccessSecurityGroup')\n\n        emrutils.apply_params(\n            src_params=parsed_attrs, src_key='AdditionalMasterSecurityGroups',\n            dest_params=instances, dest_key='AdditionalMasterSecurityGroups')\n\n        emrutils.apply_params(\n            src_params=parsed_attrs, src_key='AdditionalSlaveSecurityGroups',\n            dest_params=instances, dest_key='AdditionalSlaveSecurityGroups')\n\n        emrutils.apply(params=cluster, key='Instances', value=instances)\n\n        return cluster\n\n    def _build_bootstrap_actions(\n            self, cluster, parsed_boostrap_actions):\n        cluster_ba_list = cluster.get('BootstrapActions')\n        if cluster_ba_list is None:\n            cluster_ba_list = []\n\n        bootstrap_actions = []\n        if len(cluster_ba_list) + len(parsed_boostrap_actions) \\\n                > constants.MAX_BOOTSTRAP_ACTION_NUMBER:\n            raise ValueError('aws: error: maximum number of '\n                             'bootstrap actions for a cluster exceeded.')\n\n        for ba in parsed_boostrap_actions:\n            ba_config = {}\n            if ba.get('Name') is not None:\n                ba_config['Name'] = ba.get('Name')\n            else:\n                ba_config['Name'] = constants.BOOTSTRAP_ACTION_NAME\n            script_arg_config = {}\n            emrutils.apply_params(\n                src_params=ba, src_key='Path',\n                dest_params=script_arg_config, dest_key='Path')\n            emrutils.apply_params(\n                src_params=ba, src_key='Args',\n                dest_params=script_arg_config, dest_key='Args')\n            emrutils.apply(\n                params=ba_config,\n                key='ScriptBootstrapAction',\n                value=script_arg_config)\n            bootstrap_actions.append(ba_config)\n\n        result = cluster_ba_list + bootstrap_actions\n        if result:\n            cluster['BootstrapActions'] = result\n\n        return cluster\n\n    def _build_enable_debugging(self, parsed_args, parsed_globals):\n        if parsed_args.release_label:\n            jar = constants.COMMAND_RUNNER\n            args = [constants.DEBUGGING_COMMAND]\n        else:\n            jar = emrutils.get_script_runner(self.region)\n            args = [emrutils.build_s3_link(\n                relative_path=constants.DEBUGGING_PATH,\n                region=self.region)]\n\n        return emrutils.build_step(\n            name=constants.DEBUGGING_NAME,\n            action_on_failure=constants.TERMINATE_CLUSTER,\n            jar=jar,\n            args=args)\n\n    def _update_cluster_dict(self, cluster, key, value):\n        if key in cluster:\n            cluster[key] += value\n        elif value:\n            cluster[key] = value\n        return cluster\n\n    def _validate_release_label_ami_version(self, parsed_args):\n        if parsed_args.ami_version is not None and \\\n                parsed_args.release_label is not None:\n            raise exceptions.MutualExclusiveOptionError(\n                option1=\"--ami-version\",\n                option2=\"--release-label\")\n\n        if parsed_args.ami_version is None and \\\n                parsed_args.release_label is None:\n            raise exceptions.RequiredOptionsError(\n                option1=\"--ami-version\",\n                option2=\"--release-label\")\n\n    # Checks if the applications required by steps are specified\n    # using the --applications option.\n    def _validate_required_applications(self, parsed_args):\n\n        specified_apps = set([])\n        if parsed_args.applications is not None:\n            specified_apps = \\\n                set([app['Name'].lower() for app in parsed_args.applications])\n\n        missing_apps = self._get_missing_applications_for_steps(specified_apps,\n                                                                parsed_args)\n        # Check for HBase.\n        if parsed_args.restore_from_hbase_backup is not None:\n            if constants.HBASE not in specified_apps:\n                missing_apps.add(constants.HBASE.title())\n\n        if missing_apps:\n            raise exceptions.MissingApplicationsError(\n                applications=missing_apps)\n\n    def _get_missing_applications_for_steps(self, specified_apps, parsed_args):\n        allowed_app_steps = set([constants.HIVE, constants.PIG,\n                                 constants.IMPALA])\n        missing_apps = set()\n        if parsed_args.steps is not None:\n            for step in parsed_args.steps:\n                if len(missing_apps) == len(allowed_app_steps):\n                    break\n                step_type = step.get('Type')\n\n                if step_type is not None:\n                    step_type = step_type.lower()\n                    if step_type in allowed_app_steps and \\\n                            step_type not in specified_apps:\n                        missing_apps.add(step['Type'].title())\n        return missing_apps\n\n    def _filter_configurations_in_special_cases(self, configurations,\n                                                parsed_args, parsed_configs):\n        if parsed_args.use_default_roles:\n            configurations = [x for x in configurations\n                              if x.name != 'service_role' and\n                              x.name != 'instance_profile']\n        return configurations\n\n    def _handle_emrfs_parameters(self, cluster, emrfs_args, release_label):\n        if release_label:\n            self.validate_no_emrfs_configuration(cluster)\n            emrfs_configuration = emrfsutils.build_emrfs_confiuration(\n                emrfs_args)\n\n            self._update_cluster_dict(\n                cluster=cluster, key='Configurations',\n                value=[emrfs_configuration])\n        else:\n            emrfs_ba_config_list = emrfsutils.build_bootstrap_action_configs(\n                self.region, emrfs_args)\n            self._update_cluster_dict(\n                cluster=cluster, key='BootstrapActions',\n                value=emrfs_ba_config_list)\n\n    def validate_no_emrfs_configuration(self, cluster):\n        if 'Configurations' in cluster:\n            for config in cluster['Configurations']:\n                if config is not None and \\\n                        config.get('Classification') == constants.EMRFS_SITE:\n                    raise exceptions.DuplicateEmrFsConfigurationError\n", "awscli/customizations/emr/sshutils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\n\nfrom awscli.customizations.emr import exceptions\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import constants\nfrom botocore.exceptions import WaiterError\n\nLOG = logging.getLogger(__name__)\n\n\ndef validate_and_find_master_dns(session, parsed_globals, cluster_id):\n    \"\"\"\n    Utility method for ssh, socks, put and get command.\n    Check if the cluster to be connected to is\n     terminated or being terminated.\n    Check if the cluster is running.\n    Find master instance public dns of a given cluster.\n    Return the latest created master instance public dns name.\n    Throw MasterDNSNotAvailableError or ClusterTerminatedError.\n    \"\"\"\n    cluster_state = emrutils.get_cluster_state(\n        session, parsed_globals, cluster_id)\n\n    if cluster_state in constants.TERMINATED_STATES:\n        raise exceptions.ClusterTerminatedError\n\n    emr = emrutils.get_client(session, parsed_globals)\n\n    try:\n        cluster_running_waiter = emr.get_waiter('cluster_running')\n        if cluster_state in constants.STARTING_STATES:\n            print(\"Waiting for the cluster to start.\")\n        cluster_running_waiter.wait(ClusterId=cluster_id)\n    except WaiterError:\n        raise exceptions.MasterDNSNotAvailableError\n\n    return emrutils.find_master_dns(\n        session=session, cluster_id=cluster_id,\n        parsed_globals=parsed_globals)\n\n\ndef validate_ssh_with_key_file(key_file):\n    if (emrutils.which('putty.exe') or emrutils.which('ssh') or\n            emrutils.which('ssh.exe')) is None:\n        raise exceptions.SSHNotFoundError\n    else:\n        check_ssh_key_format(key_file)\n\n\ndef validate_scp_with_key_file(key_file):\n    if (emrutils.which('pscp.exe') or emrutils.which('scp') or\n            emrutils.which('scp.exe')) is None:\n        raise exceptions.SCPNotFoundError\n    else:\n        check_scp_key_format(key_file)\n\n\ndef check_scp_key_format(key_file):\n    # If only pscp is present and the file format is incorrect\n    if (emrutils.which('pscp.exe') is not None and\n            (emrutils.which('scp.exe') or emrutils.which('scp')) is None):\n        if check_command_key_format(key_file, ['ppk']) is False:\n            raise exceptions.WrongPuttyKeyError\n    else:\n        pass\n\n\ndef check_ssh_key_format(key_file):\n    # If only putty is present and the file format is incorrect\n    if (emrutils.which('putty.exe') is not None and\n            (emrutils.which('ssh.exe') or emrutils.which('ssh')) is None):\n        if check_command_key_format(key_file, ['ppk']) is False:\n            raise exceptions.WrongPuttyKeyError\n    else:\n        pass\n\n\ndef check_command_key_format(key_file, accepted_file_format=[]):\n    if any(key_file.endswith(i) for i in accepted_file_format):\n        return True\n    else:\n        return False\n", "awscli/customizations/emr/exceptions.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nclass EmrError(Exception):\n\n    \"\"\"\n    The base exception class for Emr exceptions.\n\n    :ivar msg: The descriptive message associated with the error.\n    \"\"\"\n    fmt = 'An unspecified error occurred'\n\n    def __init__(self, **kwargs):\n        msg = self.fmt.format(**kwargs)\n        Exception.__init__(self, msg)\n        self.kwargs = kwargs\n\n\nclass MissingParametersError(EmrError):\n\n    \"\"\"\n    One or more required parameters were not supplied.\n\n    :ivar object_name: The object that has missing parameters.\n        This can be an operation or a parameter (in the\n        case of inner params).  The str() of this object\n        will be used so it doesn't need to implement anything\n        other than str().\n    :ivar missing: The names of the missing parameters.\n    \"\"\"\n    fmt = ('aws: error: The following required parameters are missing for '\n           '{object_name}: {missing}.')\n\n\nclass EmptyListError(EmrError):\n\n    \"\"\"\n    The provided list is empty.\n\n    :ivar param: The provided list parameter\n    \"\"\"\n    fmt = ('aws: error: The prameter {param} cannot be an empty list.')\n\n\nclass MissingRequiredInstanceGroupsError(EmrError):\n\n    \"\"\"\n    In create-cluster command, none of --instance-group,\n    --instance-count nor --instance-type were not supplied.\n    \"\"\"\n    fmt = ('aws: error: Must specify either --instance-groups or '\n           '--instance-type with --instance-count(optional) to '\n           'configure instance groups.')\n\n\nclass InstanceGroupsValidationError(EmrError):\n\n    \"\"\"\n    --instance-type and --instance-count are shortcut option\n    for --instance-groups and they cannot be specified\n    together with --instance-groups\n    \"\"\"\n    fmt = ('aws: error: You may not specify --instance-type '\n           'or --instance-count with --instance-groups, '\n           'because --instance-type and --instance-count are '\n           'shortcut options for --instance-groups.')\n\n\nclass InvalidAmiVersionError(EmrError):\n\n    \"\"\"\n    The supplied ami-version is invalid.\n    :ivar ami_version: The provided ami_version.\n    \"\"\"\n    fmt = ('aws: error: The supplied AMI version \"{ami_version}\" is invalid.'\n           ' Please see AMI Versions Supported in Amazon EMR in '\n           'Amazon Elastic MapReduce Developer Guide: '\n           'http://docs.aws.amazon.com/ElasticMapReduce/'\n           'latest/DeveloperGuide/ami-versions-supported.html')\n\n\nclass MissingBooleanOptionsError(EmrError):\n\n    \"\"\"\n    Required boolean options are not supplied.\n\n    :ivar true_option\n    :ivar false_option\n    \"\"\"\n    fmt = ('aws: error: Must specify one of the following boolean options: '\n           '{true_option}|{false_option}.')\n\n\nclass UnknownStepTypeError(EmrError):\n\n    \"\"\"\n    The provided step type is not supported.\n\n    :ivar step_type: the step_type provided.\n    \"\"\"\n    fmt = ('aws: error: The step type {step_type} is not supported.')\n\n\nclass UnknownIamEndpointError(EmrError):\n\n    \"\"\"\n    The IAM endpoint is not known for the specified region.\n\n    :ivar region: The region specified.\n    \"\"\"\n    fmt = 'IAM endpoint not known for region: {region}.' +\\\n          ' Specify the iam-endpoint using the --iam-endpoint option.'\n\n\nclass ResolveServicePrincipalError(EmrError):\n\n    \"\"\"\n    The service principal could not be resolved from the region or the\n    endpoint.\n    \"\"\"\n    fmt = 'Could not resolve the service principal from' +\\\n          ' the region or the endpoint.'\n\n\nclass LogUriError(EmrError):\n\n    \"\"\"\n    The LogUri is not specified and debugging is enabled for the cluster.\n    \"\"\"\n    fmt = ('aws: error: LogUri not specified. You must specify a logUri '\n           'if you enable debugging when creating a cluster.')\n\n\nclass MasterDNSNotAvailableError(EmrError):\n\n    \"\"\"\n    Cannot get dns of master node on the cluster.\n    \"\"\"\n    fmt = 'Cannot get DNS of master node on the cluster. '\\\n          ' Please try again after some time.'\n\n\nclass WrongPuttyKeyError(EmrError):\n\n    \"\"\"\n    A wrong key has been used with a compatible program.\n    \"\"\"\n    fmt = 'Key file file format is incorrect. Putty expects a ppk file. '\\\n          'Please refer to documentation at http://docs.aws.amazon.com/'\\\n          'ElasticMapReduce/latest/DeveloperGuide/EMR_SetUp_SSH.html. '\n\n\nclass SSHNotFoundError(EmrError):\n\n    \"\"\"\n    SSH or Putty not available.\n    \"\"\"\n    fmt = 'SSH or Putty not available. Please refer to the documentation '\\\n          'at http://docs.aws.amazon.com/ElasticMapReduce/latest/'\\\n          'DeveloperGuide/EMR_SetUp_SSH.html.'\n\n\nclass SCPNotFoundError(EmrError):\n\n    \"\"\"\n    SCP or Pscp not available.\n    \"\"\"\n    fmt = 'SCP or Pscp not available. Please refer to the documentation '\\\n          'at http://docs.aws.amazon.com/ElasticMapReduce/latest/'\\\n          'DeveloperGuide/EMR_SetUp_SSH.html. '\n\n\nclass SubnetAndAzValidationError(EmrError):\n\n    \"\"\"\n    SubnetId and AvailabilityZone are mutual exclusive in --ec2-attributes.\n    \"\"\"\n    fmt = ('aws: error: You may not specify both a SubnetId and an Availabili'\n           'tyZone (placement) because ec2SubnetId implies a placement.')\n\n\nclass RequiredOptionsError(EmrError):\n\n    \"\"\"\n    Either of option1 or option2 is required.\n    \"\"\"\n\n    fmt = ('aws: error: Either {option1} or {option2} is required.')\n\n\nclass MutualExclusiveOptionError(EmrError):\n\n    \"\"\"\n    The provided option1 and option2 are mutually exclusive.\n\n    :ivar option1\n    :ivar option2\n    :ivar message (optional)\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        msg = ('aws: error: You cannot specify both ' +\n               kwargs.get('option1', '') + ' and ' +\n               kwargs.get('option2', '') + ' options together.' +\n               kwargs.get('message', ''))\n        Exception.__init__(self, msg)\n\n\nclass MissingApplicationsError(EmrError):\n\n    \"\"\"\n    The application required for a step is not installed when creating a\n    cluster.\n\n    :ivar applications\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        msg = ('aws: error: Some of the steps require the following'\n               ' applications to be installed: ' +\n               ', '.join(kwargs['applications']) + '. Please install the'\n               ' applications using --applications.')\n        Exception.__init__(self, msg)\n\n\nclass ClusterTerminatedError(EmrError):\n\n    \"\"\"\n    The cluster is terminating or has already terminated.\n    \"\"\"\n    fmt = 'aws: error: Cluster terminating or already terminated.'\n\n\nclass ClusterStatesFilterValidationError(EmrError):\n\n    \"\"\"\n    In the list-clusters command, customers can specify only one\n    of the following states filters:\n    --cluster-states, --active, --terminated, --failed\n\n    \"\"\"\n    fmt = ('aws: error: You can specify only one of the cluster state '\n           'filters: --cluster-states, --active, --terminated, --failed.')\n\n\nclass MissingClusterAttributesError(EmrError):\n\n    \"\"\"\n    In the modify-cluster-attributes command, customers need to provide\n    at least one of the following cluster attributes: --visible-to-all-users,\n    --no-visible-to-all-users, --termination-protected, --no-termination-protected,\n    --auto-terminate and --no-auto-terminate\n    \"\"\"\n    fmt = ('aws: error: Must specify one of the following boolean options: '\n           '--visible-to-all-users|--no-visible-to-all-users, '\n           '--termination-protected|--no-termination-protected, '\n           '--auto-terminate|--no-auto-terminate, '\n           '--unhealthy-node-replacement|--no-unhealthy-node-replacement.')\n\n\nclass InvalidEmrFsArgumentsError(EmrError):\n\n    \"\"\"\n    The provided EMRFS parameters are invalid as parent feature e.g.,\n    Consistent View, CSE, SSE is not configured\n\n    :ivar invalid: Invalid parameters\n    :ivar parent_object_name: Parent feature name\n    \"\"\"\n\n    fmt = ('aws: error: {parent_object_name} is not specified. Thus, '\n           ' following parameters are invalid: {invalid}')\n\n\nclass DuplicateEmrFsConfigurationError(EmrError):\n\n    fmt = ('aws: error: EMRFS should be configured either using '\n           '--configuration or --emrfs but not both')\n\n\nclass UnknownCseProviderTypeError(EmrError):\n\n    \"\"\"\n    The provided EMRFS client-side encryption provider type is not supported.\n\n    :ivar provider_type: the provider_type provided.\n    \"\"\"\n    fmt = ('aws: error: The client side encryption type \"{provider_type}\" is '\n           'not supported. You must specify either KMS or Custom')\n\n\nclass UnknownEncryptionTypeError(EmrError):\n\n    \"\"\"\n    The provided encryption type is not supported.\n\n    :ivar provider_type: the provider_type provided.\n    \"\"\"\n    fmt = ('aws: error: The encryption type \"{encryption}\" is invalid. '\n           'You must specify either ServerSide or ClientSide')\n\n\nclass BothSseAndEncryptionConfiguredError(EmrError):\n\n    \"\"\"\n    Only one of SSE or Encryption can be configured.\n\n    :ivar sse: Value for SSE\n    :ivar encryption: Value for encryption\n    \"\"\"\n\n    fmt = ('aws: error: Both SSE={sse} and Encryption={encryption} are '\n           'configured for --emrfs. You must specify only one of the two.')\n\n\nclass InvalidBooleanConfigError(EmrError):\n\n    fmt = (\"aws: error: {config_value} for {config_key} in the config file is \"\n           \"invalid. The value should be either 'True' or 'False'. Use \"\n           \"'aws configure set {profile_var_name}.emr.{config_key} <value>' \"\n           \"command to set a valid value.\")\n\n\nclass UnsupportedCommandWithReleaseError(EmrError):\n\n    fmt = (\"aws: error: {command} is not supported with \"\n           \"'{release_label}' release.\")\n\nclass MissingAutoScalingRoleError(EmrError):\n\n    fmt = (\"aws: error: Must specify --auto-scaling-role when configuring an \"\n           \"AutoScaling policy for an instance group.\")\n\n", "awscli/customizations/emr/describecluster.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr.command import Command\nfrom botocore.exceptions import NoCredentialsError\n\n\nclass DescribeCluster(Command):\n    NAME = 'describe-cluster'\n    DESCRIPTION = helptext.DESCRIBE_CLUSTER_DESCRIPTION\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': helptext.CLUSTER_ID}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        parameters = {'ClusterId': parsed_args.cluster_id}\n        list_instance_fleets_result = None\n        list_instance_groups_result = None\n        is_fleet_based_cluster = False\n\n        describe_cluster_result = self._call(\n            self._session, 'describe_cluster', parameters, parsed_globals)\n\n\n        if 'Cluster' in describe_cluster_result:\n            describe_cluster = describe_cluster_result['Cluster']\n            if describe_cluster.get('InstanceCollectionType') == constants.INSTANCE_FLEET_TYPE:\n                is_fleet_based_cluster = True\n\n        if is_fleet_based_cluster:\n            list_instance_fleets_result = self._call(\n                self._session, 'list_instance_fleets', parameters,\n                parsed_globals)\n        else:\n            list_instance_groups_result = self._call(\n                self._session, 'list_instance_groups', parameters,\n                parsed_globals)\n\n        list_bootstrap_actions_result = self._call(\n            self._session, 'list_bootstrap_actions',\n            parameters, parsed_globals)\n\n        constructed_result = self._construct_result(\n            describe_cluster_result,\n            list_instance_fleets_result,\n            list_instance_groups_result,\n            list_bootstrap_actions_result)\n\n        emrutils.display_response(self._session, 'describe_cluster',\n                                  constructed_result, parsed_globals)\n\n        return 0\n\n    def _call(self, session, operation_name, parameters, parsed_globals):\n        return emrutils.call(\n            session, operation_name, parameters,\n            region_name=self.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl)\n\n    def _get_key_of_result(self, keys):\n        # Return the first key that is not \"Marker\"\n        for key in keys:\n            if key != \"Marker\":\n                return key\n\n    def _construct_result(\n            self, describe_cluster_result, list_instance_fleets_result,\n            list_instance_groups_result, list_bootstrap_actions_result):\n        result = describe_cluster_result\n        result['Cluster']['BootstrapActions'] = []\n\n        if (list_instance_fleets_result is not None and\n                list_instance_fleets_result.get('InstanceFleets') is not None):\n            result['Cluster']['InstanceFleets'] = \\\n                list_instance_fleets_result.get('InstanceFleets')\n        if (list_instance_groups_result is not None and\n                list_instance_groups_result.get('InstanceGroups') is not None):\n            result['Cluster']['InstanceGroups'] = \\\n                list_instance_groups_result.get('InstanceGroups')\n        if (list_bootstrap_actions_result is not None and\n                list_bootstrap_actions_result.get('BootstrapActions')\n                is not None):\n            result['Cluster']['BootstrapActions'] = \\\n                list_bootstrap_actions_result['BootstrapActions']\n\n        return result\n", "awscli/customizations/emr/hbaseutils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import constants\n\n\ndef build_hbase_restore_from_backup_args(dir, backup_version=None):\n    args = [constants.HBASE_MAIN,\n            constants.HBASE_RESTORE,\n            constants.HBASE_BACKUP_DIR, dir]\n\n    if backup_version is not None:\n        args.append(constants.HBASE_BACKUP_VERSION_FOR_RESTORE)\n        args.append(backup_version)\n\n    return args\n", "awscli/customizations/emr/listclusters.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nfrom awscli.arguments import CustomArgument\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr import exceptions\nfrom awscli.customizations.emr import constants\n\n\ndef modify_list_clusters_argument(argument_table, **kwargs):\n    argument_table['cluster-states'] = \\\n        ClusterStatesArgument(\n            name='cluster-states',\n            help_text=helptext.LIST_CLUSTERS_CLUSTER_STATES,\n            nargs='+')\n    argument_table['active'] = \\\n        ActiveStateArgument(\n            name='active', help_text=helptext.LIST_CLUSTERS_STATE_FILTERS,\n            action='store_true', group_name='states_filter')\n    argument_table['terminated'] = \\\n        TerminatedStateArgument(\n            name='terminated',\n            action='store_true', group_name='states_filter')\n    argument_table['failed'] = \\\n        FailedStateArgument(\n            name='failed', action='store_true', group_name='states_filter')\n    argument_table['created-before'] = CreatedBefore(\n        name='created-before', help_text=helptext.LIST_CLUSTERS_CREATED_BEFORE,\n        cli_type_name='timestamp')\n    argument_table['created-after'] = CreatedAfter(\n        name='created-after', help_text=helptext.LIST_CLUSTERS_CREATED_AFTER,\n        cli_type_name='timestamp')\n\n\nclass ClusterStatesArgument(CustomArgument):\n    def add_to_params(self, parameters, value):\n        if value is not None:\n            if (parameters.get('ClusterStates') is not None and\n                    len(parameters.get('ClusterStates')) > 0):\n                raise exceptions.ClusterStatesFilterValidationError()\n            parameters['ClusterStates'] = value\n\n\nclass ActiveStateArgument(CustomArgument):\n    def add_to_params(self, parameters, value):\n        if value is True:\n            if (parameters.get('ClusterStates') is not None and\n                    len(parameters.get('ClusterStates')) > 0):\n                raise exceptions.ClusterStatesFilterValidationError()\n            parameters['ClusterStates'] = constants.LIST_CLUSTERS_ACTIVE_STATES\n\n\nclass TerminatedStateArgument(CustomArgument):\n    def add_to_params(self, parameters, value):\n        if value is True:\n            if (parameters.get('ClusterStates') is not None and\n                    len(parameters.get('ClusterStates')) > 0):\n                raise exceptions.ClusterStatesFilterValidationError()\n            parameters['ClusterStates'] = \\\n                constants.LIST_CLUSTERS_TERMINATED_STATES\n\n\nclass FailedStateArgument(CustomArgument):\n    def add_to_params(self, parameters, value):\n        if value is True:\n            if (parameters.get('ClusterStates') is not None and\n                    len(parameters.get('ClusterStates')) > 0):\n                raise exceptions.ClusterStatesFilterValidationError()\n            parameters['ClusterStates'] = constants.LIST_CLUSTERS_FAILED_STATES\n\n\nclass CreatedBefore(CustomArgument):\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        parameters['CreatedBefore'] = value\n\n\nclass CreatedAfter(CustomArgument):\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        parameters['CreatedAfter'] = value\n", "awscli/customizations/emr/emr.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import hbase\nfrom awscli.customizations.emr import ssh\nfrom awscli.customizations.emr.addsteps import AddSteps\nfrom awscli.customizations.emr.createcluster import CreateCluster\nfrom awscli.customizations.emr.addinstancegroups import AddInstanceGroups\nfrom awscli.customizations.emr.createdefaultroles import CreateDefaultRoles\nfrom awscli.customizations.emr.modifyclusterattributes import ModifyClusterAttr\nfrom awscli.customizations.emr.installapplications import InstallApplications\nfrom awscli.customizations.emr.describecluster import DescribeCluster\nfrom awscli.customizations.emr.terminateclusters import TerminateClusters\nfrom awscli.customizations.emr.addtags import modify_tags_argument\nfrom awscli.customizations.emr.listclusters \\\n    import modify_list_clusters_argument\nfrom awscli.customizations.emr.command import override_args_required_option\n\n\ndef emr_initialize(cli):\n    \"\"\"\n    The entry point for EMR high level commands.\n    \"\"\"\n    cli.register('building-command-table.emr', register_commands)\n    cli.register('building-argument-table.emr.add-tags', modify_tags_argument)\n    cli.register(\n        'building-argument-table.emr.list-clusters',\n        modify_list_clusters_argument)\n    cli.register('before-building-argument-table-parser.emr.*',\n                 override_args_required_option)\n\n\ndef register_commands(command_table, session, **kwargs):\n    \"\"\"\n    Called when the EMR command table is being built. Used to inject new\n    high level commands into the command list. These high level commands\n    must not collide with existing low-level API call names.\n    \"\"\"\n    command_table['terminate-clusters'] = TerminateClusters(session)\n    command_table['describe-cluster'] = DescribeCluster(session)\n    command_table['modify-cluster-attributes'] = ModifyClusterAttr(session)\n    command_table['install-applications'] = InstallApplications(session)\n    command_table['create-cluster'] = CreateCluster(session)\n    command_table['add-steps'] = AddSteps(session)\n    command_table['restore-from-hbase-backup'] = \\\n        hbase.RestoreFromHBaseBackup(session)\n    command_table['create-hbase-backup'] = hbase.CreateHBaseBackup(session)\n    command_table['schedule-hbase-backup'] = hbase.ScheduleHBaseBackup(session)\n    command_table['disable-hbase-backups'] = \\\n        hbase.DisableHBaseBackups(session)\n    command_table['create-default-roles'] = CreateDefaultRoles(session)\n    command_table['add-instance-groups'] = AddInstanceGroups(session)\n    command_table['ssh'] = ssh.SSH(session)\n    command_table['socks'] = ssh.Socks(session)\n    command_table['get'] = ssh.Get(session)\n    command_table['put'] = ssh.Put(session)\n", "awscli/customizations/emr/emrfsutils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import exceptions\nfrom botocore.compat import OrderedDict\n\n\nCONSISTENT_OPTIONAL_KEYS = ['RetryCount', 'RetryPeriod']\nCSE_KMS_REQUIRED_KEYS = ['KMSKeyId']\nCSE_CUSTOM_REQUIRED_KEYS = ['CustomProviderLocation', 'CustomProviderClass']\nCSE_PROVIDER_TYPES = [constants.EMRFS_KMS, constants.EMRFS_CUSTOM]\nENCRYPTION_TYPES = [constants.EMRFS_CLIENT_SIDE, constants.EMRFS_SERVER_SIDE]\n\nCONSISTENT_OPTION_NAME = \"--emrfs Consistent=true/false\"\nCSE_OPTION_NAME = '--emrfs Encryption=ClientSide'\nCSE_KMS_OPTION_NAME = '--emrfs Encryption=ClientSide,ProviderType=KMS'\nCSE_CUSTOM_OPTION_NAME = '--emrfs Encryption=ClientSide,ProviderType=Custom'\n\n\ndef build_bootstrap_action_configs(region, emrfs_args):\n    bootstrap_actions = []\n\n    _verify_emrfs_args(emrfs_args)\n\n    if _need_to_configure_cse(emrfs_args, 'CUSTOM'):\n        # Download custom encryption provider from Amazon S3 to EMR Cluster\n        bootstrap_actions.append(\n            emrutils.build_bootstrap_action(\n                path=constants.EMRFS_CSE_CUSTOM_S3_GET_BA_PATH,\n                name=constants.S3_GET_BA_NAME,\n                args=[constants.S3_GET_BA_SRC,\n                      emrfs_args.get('CustomProviderLocation'),\n                      constants.S3_GET_BA_DEST,\n                      constants.EMRFS_CUSTOM_DEST_PATH,\n                      constants.S3_GET_BA_FORCE]))\n\n    emrfs_setup_ba_args = _build_ba_args_to_setup_emrfs(emrfs_args)\n    bootstrap_actions.append(\n        emrutils.build_bootstrap_action(\n            path=emrutils.build_s3_link(\n                relative_path=constants.CONFIG_HADOOP_PATH,\n                region=region),\n            name=constants.EMRFS_BA_NAME,\n            args=emrfs_setup_ba_args))\n\n    return bootstrap_actions\n\n\ndef build_emrfs_confiuration(emrfs_args):\n    _verify_emrfs_args(emrfs_args)\n    emrfs_properties = _build_emrfs_properties(emrfs_args)\n\n    if _need_to_configure_cse(emrfs_args, 'CUSTOM'):\n        emrfs_properties[constants.EMRFS_CSE_CUSTOM_PROVIDER_URI_KEY] = \\\n            emrfs_args.get('CustomProviderLocation')\n\n    emrfs_configuration = {\n        'Classification': constants.EMRFS_SITE,\n        'Properties': emrfs_properties}\n\n    return emrfs_configuration\n\n\ndef _verify_emrfs_args(emrfs_args):\n    # Encryption should have a valid value\n    if 'Encryption' in emrfs_args \\\n            and emrfs_args['Encryption'].upper() not in ENCRYPTION_TYPES:\n        raise exceptions.UnknownEncryptionTypeError(\n            encryption=emrfs_args['Encryption'])\n\n    # Only one of SSE and Encryption should be configured\n    if 'SSE' in emrfs_args and 'Encryption' in emrfs_args:\n        raise exceptions.BothSseAndEncryptionConfiguredError(\n            sse=emrfs_args['SSE'], encryption=emrfs_args['Encryption'])\n\n    # CSE should be configured correctly\n    # ProviderType should be present and should have valid value\n    # Given the type, the required parameters should be present\n    if ('Encryption' in emrfs_args and\n            emrfs_args['Encryption'].upper() == constants.EMRFS_CLIENT_SIDE):\n        if 'ProviderType' not in emrfs_args:\n            raise exceptions.MissingParametersError(\n                object_name=CSE_OPTION_NAME, missing='ProviderType')\n        elif emrfs_args['ProviderType'].upper() not in CSE_PROVIDER_TYPES:\n            raise exceptions.UnknownCseProviderTypeError(\n                provider_type=emrfs_args['ProviderType'])\n        elif emrfs_args['ProviderType'].upper() == 'KMS':\n            _verify_required_args(emrfs_args.keys(), CSE_KMS_REQUIRED_KEYS,\n                                  CSE_KMS_OPTION_NAME)\n        elif emrfs_args['ProviderType'].upper() == 'CUSTOM':\n            _verify_required_args(emrfs_args.keys(), CSE_CUSTOM_REQUIRED_KEYS,\n                                  CSE_CUSTOM_OPTION_NAME)\n\n    # No child attributes should be present if the parent feature is not\n    # configured\n    if 'Consistent' not in emrfs_args:\n        _verify_child_args(emrfs_args.keys(), CONSISTENT_OPTIONAL_KEYS,\n                           CONSISTENT_OPTION_NAME)\n    if not _need_to_configure_cse(emrfs_args, 'KMS'):\n        _verify_child_args(emrfs_args.keys(), CSE_KMS_REQUIRED_KEYS,\n                           CSE_KMS_OPTION_NAME)\n    if not _need_to_configure_cse(emrfs_args, 'CUSTOM'):\n        _verify_child_args(emrfs_args.keys(), CSE_CUSTOM_REQUIRED_KEYS,\n                           CSE_CUSTOM_OPTION_NAME)\n\n\ndef _verify_required_args(actual_keys, required_keys, object_name):\n    if any(x not in actual_keys for x in required_keys):\n        missing_keys = list(\n            sorted(set(required_keys).difference(set(actual_keys))))\n        raise exceptions.MissingParametersError(\n            object_name=object_name, missing=emrutils.join(missing_keys))\n\n\ndef _verify_child_args(actual_keys, child_keys, parent_object_name):\n    if any(x in actual_keys for x in child_keys):\n        invalid_keys = list(\n            sorted(set(child_keys).intersection(set(actual_keys))))\n        raise exceptions.InvalidEmrFsArgumentsError(\n            invalid=emrutils.join(invalid_keys),\n            parent_object_name=parent_object_name)\n\n\ndef _build_ba_args_to_setup_emrfs(emrfs_args):\n    emrfs_properties = _build_emrfs_properties(emrfs_args)\n\n    return _create_ba_args(emrfs_properties)\n\n\ndef _build_emrfs_properties(emrfs_args):\n    \"\"\"\n    Assumption: emrfs_args is valid i.e. all required attributes are present\n    \"\"\"\n    emrfs_properties = OrderedDict()\n\n    if _need_to_configure_consistent_view(emrfs_args):\n        _update_properties_for_consistent_view(emrfs_properties, emrfs_args)\n\n    if _need_to_configure_sse(emrfs_args):\n        _update_properties_for_sse(emrfs_properties, emrfs_args)\n\n    if _need_to_configure_cse(emrfs_args, 'KMS'):\n        _update_properties_for_cse(emrfs_properties, emrfs_args, 'KMS')\n\n    if _need_to_configure_cse(emrfs_args, 'CUSTOM'):\n        _update_properties_for_cse(emrfs_properties, emrfs_args, 'CUSTOM')\n\n    if 'Args' in emrfs_args:\n        for arg_value in emrfs_args.get('Args'):\n            key, value = emrutils.split_to_key_value(arg_value)\n            emrfs_properties[key] = value\n\n    return emrfs_properties\n\n\ndef _need_to_configure_consistent_view(emrfs_args):\n    return 'Consistent' in emrfs_args\n\n\ndef _need_to_configure_sse(emrfs_args):\n    return 'SSE' in emrfs_args \\\n        or ('Encryption' in emrfs_args and\n            emrfs_args['Encryption'].upper() == constants.EMRFS_SERVER_SIDE)\n\n\ndef _need_to_configure_cse(emrfs_args, cse_type):\n    return ('Encryption' in emrfs_args and\n            emrfs_args['Encryption'].upper() == constants.EMRFS_CLIENT_SIDE and\n            'ProviderType' in emrfs_args and\n            emrfs_args['ProviderType'].upper() == cse_type)\n\n\ndef _update_properties_for_consistent_view(emrfs_properties, emrfs_args):\n    emrfs_properties[constants.EMRFS_CONSISTENT_KEY] = \\\n        str(emrfs_args['Consistent']).lower()\n\n    if 'RetryCount' in emrfs_args:\n        emrfs_properties[constants.EMRFS_RETRY_COUNT_KEY] = \\\n            str(emrfs_args['RetryCount'])\n\n    if 'RetryPeriod' in emrfs_args:\n        emrfs_properties[constants.EMRFS_RETRY_PERIOD_KEY] = \\\n            str(emrfs_args['RetryPeriod'])\n\n\ndef _update_properties_for_sse(emrfs_properties, emrfs_args):\n    sse_value = emrfs_args['SSE'] if 'SSE' in emrfs_args else True\n    # if 'SSE' is not in emrfs_args then 'Encryption' must be 'ServerSide'\n\n    emrfs_properties[constants.EMRFS_SSE_KEY] = str(sse_value).lower()\n\n\ndef _update_properties_for_cse(emrfs_properties, emrfs_args, cse_type):\n    emrfs_properties[constants.EMRFS_CSE_KEY] = 'true'\n    if cse_type == 'KMS':\n        emrfs_properties[\n            constants.EMRFS_CSE_ENCRYPTION_MATERIALS_PROVIDER_KEY] = \\\n            constants.EMRFS_CSE_KMS_PROVIDER_FULL_CLASS_NAME\n\n        emrfs_properties[constants.EMRFS_CSE_KMS_KEY_ID_KEY] =\\\n            emrfs_args['KMSKeyId']\n\n    elif cse_type == 'CUSTOM':\n        emrfs_properties[\n            constants.EMRFS_CSE_ENCRYPTION_MATERIALS_PROVIDER_KEY] = \\\n            emrfs_args['CustomProviderClass']\n\n\ndef _update_emrfs_ba_args(ba_args, key_value):\n    ba_args.append(constants.EMRFS_BA_ARG_KEY)\n    ba_args.append(key_value)\n\n\ndef _create_ba_args(emrfs_properties):\n    ba_args = []\n    for key, value in emrfs_properties.items():\n        key_value = key\n        if value:\n            key_value = key_value + \"=\" + value\n        _update_emrfs_ba_args(ba_args, key_value)\n\n    return ba_args\n", "awscli/customizations/emr/constants.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n# Declare all the constants used by EMR in this file.\n\nEC2_ROLE_NAME = \"EMR_EC2_DefaultRole\"\nEMR_ROLE_NAME = \"EMR_DefaultRole\"\nEMR_AUTOSCALING_ROLE_NAME = \"EMR_AutoScaling_DefaultRole\"\nROLE_ARN_PATTERN = \"arn:{{region_suffix}}:iam::aws:policy/service-role/{{policy_name}}\"\nEC2_ROLE_POLICY_NAME = \"AmazonElasticMapReduceforEC2Role\"\nEMR_ROLE_POLICY_NAME = \"AmazonElasticMapReduceRole\"\nEMR_AUTOSCALING_ROLE_POLICY_NAME = \"AmazonElasticMapReduceforAutoScalingRole\"\nEMR_AUTOSCALING_SERVICE_NAME = \"application-autoscaling\"\nEMR_AUTOSCALING_SERVICE_PRINCIPAL = \"application-autoscaling.amazonaws.com\"\nEC2_SERVICE_PRINCIPAL = \"ec2.amazonaws.com\"\n\n# Action on failure\nCONTINUE = 'CONTINUE'\nCANCEL_AND_WAIT = 'CANCEL_AND_WAIT'\nTERMINATE_CLUSTER = 'TERMINATE_CLUSTER'\nDEFAULT_FAILURE_ACTION = CONTINUE\n\n# Market type\nSPOT = 'SPOT'\nON_DEMAND = 'ON_DEMAND'\n\nSCRIPT_RUNNER_PATH = '/libs/script-runner/script-runner.jar'\nCOMMAND_RUNNER = 'command-runner.jar'\nDEBUGGING_PATH = '/libs/state-pusher/0.1/fetch'\nDEBUGGING_COMMAND = 'state-pusher-script'\nDEBUGGING_NAME = 'Setup Hadoop Debugging'\n\nCONFIG_HADOOP_PATH = '/bootstrap-actions/configure-hadoop'\n\n# S3 copy bootstrap action\nS3_GET_BA_NAME = 'S3 get'\nS3_GET_BA_SRC = '-s'\nS3_GET_BA_DEST = '-d'\nS3_GET_BA_FORCE = '-f'\n\n# EMRFS\nEMRFS_BA_NAME = 'Setup EMRFS'\nEMRFS_BA_ARG_KEY = '-e'\nEMRFS_CONSISTENT_KEY = 'fs.s3.consistent'\nEMRFS_SSE_KEY = 'fs.s3.enableServerSideEncryption'\nEMRFS_RETRY_COUNT_KEY = 'fs.s3.consistent.retryCount'\nEMRFS_RETRY_PERIOD_KEY = 'fs.s3.consistent.retryPeriodSeconds'\nEMRFS_CSE_KEY = 'fs.s3.cse.enabled'\nEMRFS_CSE_KMS_KEY_ID_KEY = 'fs.s3.cse.kms.keyId'\nEMRFS_CSE_ENCRYPTION_MATERIALS_PROVIDER_KEY = \\\n    'fs.s3.cse.encryptionMaterialsProvider'\nEMRFS_CSE_CUSTOM_PROVIDER_URI_KEY = 'fs.s3.cse.encryptionMaterialsProvider.uri'\n\nEMRFS_CSE_KMS_PROVIDER_FULL_CLASS_NAME = ('com.amazon.ws.emr.hadoop.fs.cse.'\n                                          'KMSEncryptionMaterialsProvider')\nEMRFS_CSE_CUSTOM_S3_GET_BA_PATH = 'file:/usr/share/aws/emr/scripts/s3get'\nEMRFS_CUSTOM_DEST_PATH = '/usr/share/aws/emr/auxlib'\n\nEMRFS_SERVER_SIDE = 'SERVERSIDE'\nEMRFS_CLIENT_SIDE = 'CLIENTSIDE'\nEMRFS_KMS = 'KMS'\nEMRFS_CUSTOM = 'CUSTOM'\n\nEMRFS_SITE = 'emrfs-site'\n\nMAX_BOOTSTRAP_ACTION_NUMBER = 16\nBOOTSTRAP_ACTION_NAME = 'Bootstrap action'\n\nHIVE_BASE_PATH = '/libs/hive'\nHIVE_SCRIPT_PATH = '/libs/hive/hive-script'\nHIVE_SCRIPT_COMMAND = 'hive-script'\n\nPIG_BASE_PATH = '/libs/pig'\nPIG_SCRIPT_PATH = '/libs/pig/pig-script'\nPIG_SCRIPT_COMMAND = 'pig-script'\n\nGANGLIA_INSTALL_BA_PATH = '/bootstrap-actions/install-ganglia'\n\n# HBase\nHBASE_INSTALL_BA_PATH = '/bootstrap-actions/setup-hbase'\nHBASE_PATH_HADOOP1_INSTALL_JAR = '/home/hadoop/lib/hbase-0.92.0.jar'\nHBASE_PATH_HADOOP2_INSTALL_JAR = '/home/hadoop/lib/hbase.jar'\nHBASE_INSTALL_ARG = ['emr.hbase.backup.Main', '--start-master']\nHBASE_JAR_PATH = '/home/hadoop/lib/hbase.jar'\nHBASE_MAIN = 'emr.hbase.backup.Main'\n\n# HBase commands\nHBASE_RESTORE = '--restore'\nHBASE_BACKUP_DIR_FOR_RESTORE = '--backup-dir-to-restore'\nHBASE_BACKUP_VERSION_FOR_RESTORE = '--backup-version'\nHBASE_BACKUP = '--backup'\nHBASE_SCHEDULED_BACKUP = '--set-scheduled-backup'\nHBASE_BACKUP_DIR = '--backup-dir'\nHBASE_INCREMENTAL_BACKUP_INTERVAL = '--incremental-backup-time-interval'\nHBASE_INCREMENTAL_BACKUP_INTERVAL_UNIT = '--incremental-backup-time-unit'\nHBASE_FULL_BACKUP_INTERVAL = '--full-backup-time-interval'\nHBASE_FULL_BACKUP_INTERVAL_UNIT = '--full-backup-time-unit'\nHBASE_DISABLE_FULL_BACKUP = '--disable-full-backups'\nHBASE_DISABLE_INCREMENTAL_BACKUP = '--disable-incremental-backups'\nHBASE_BACKUP_STARTTIME = '--start-time'\nHBASE_BACKUP_CONSISTENT = '--consistent'\nHBASE_BACKUP_STEP_NAME = 'Backup HBase'\nHBASE_RESTORE_STEP_NAME = 'Restore HBase'\nHBASE_SCHEDULE_BACKUP_STEP_NAME = 'Modify Backup Schedule'\n\nIMPALA_INSTALL_PATH = '/libs/impala/setup-impala'\n\n# Step\nHADOOP_STREAMING_PATH = '/home/hadoop/contrib/streaming/hadoop-streaming.jar'\nHADOOP_STREAMING_COMMAND = 'hadoop-streaming'\n\nCUSTOM_JAR = 'custom_jar'\nHIVE = 'hive'\nPIG = 'pig'\nIMPALA = 'impala'\nSTREAMING = 'streaming'\nGANGLIA = 'ganglia'\nHBASE = 'hbase'\nSPARK = 'spark'\n\nDEFAULT_CUSTOM_JAR_STEP_NAME = 'Custom JAR'\nDEFAULT_STREAMING_STEP_NAME = 'Streaming program'\nDEFAULT_HIVE_STEP_NAME = 'Hive program'\nDEFAULT_PIG_STEP_NAME = 'Pig program'\nDEFAULT_IMPALA_STEP_NAME = 'Impala program'\nDEFAULT_SPARK_STEP_NAME = 'Spark application'\n\nARGS = '--args'\nRUN_HIVE_SCRIPT = '--run-hive-script'\nHIVE_VERSIONS = '--hive-versions'\nHIVE_STEP_CONFIG = 'HiveStepConfig'\nRUN_PIG_SCRIPT = '--run-pig-script'\nPIG_VERSIONS = '--pig-versions'\nPIG_STEP_CONFIG = 'PigStepConfig'\nRUN_IMPALA_SCRIPT = '--run-impala-script'\nSPARK_SUBMIT_PATH = '/home/hadoop/spark/bin/spark-submit'\nSPARK_SUBMIT_COMMAND = 'spark-submit'\nIMPALA_STEP_CONFIG = 'ImpalaStepConfig'\nSPARK_STEP_CONFIG = 'SparkStepConfig'\nSTREAMING_STEP_CONFIG = 'StreamingStepConfig'\nCUSTOM_JAR_STEP_CONFIG = 'CustomJARStepConfig'\n\nINSTALL_PIG_ARG = '--install-pig'\nINSTALL_PIG_NAME = 'Install Pig'\nINSTALL_HIVE_ARG = '--install-hive'\nINSTALL_HIVE_NAME = 'Install Hive'\nHIVE_SITE_KEY = '--hive-site'\nINSTALL_HIVE_SITE_ARG = '--install-hive-site'\nINSTALL_HIVE_SITE_NAME = 'Install Hive Site Configuration'\nBASE_PATH_ARG = '--base-path'\nINSTALL_GANGLIA_NAME = 'Install Ganglia'\nINSTALL_HBASE_NAME = 'Install HBase'\nSTART_HBASE_NAME = 'Start HBase'\nINSTALL_IMPALA_NAME = 'Install Impala'\nIMPALA_VERSION = '--impala-version'\nIMPALA_CONF = '--impala-conf'\n\nFULL = 'full'\nINCREMENTAL = 'incremental'\n\nMINUTES = 'minutes'\nHOURS = 'hours'\nDAYS = 'days'\nNOW = 'now'\n\nTRUE = 'true'\nFALSE = 'false'\n\nEC2 = 'ec2'\nEMR = 'elasticmapreduce'\nAPPLICATION_AUTOSCALING = 'application-autoscaling'\nLATEST = 'latest'\n\nAPPLICATIONS = [\"HIVE\", \"PIG\", \"HBASE\", \"GANGLIA\", \"IMPALA\", \"SPARK\", \"MAPR\",\n                \"MAPR_M3\", \"MAPR_M5\", \"MAPR_M7\"]\n\nSSH_USER = 'hadoop'\nSTARTING_STATES = ['STARTING', 'BOOTSTRAPPING']\nTERMINATED_STATES = ['TERMINATED', 'TERMINATING', 'TERMINATED_WITH_ERRORS']\n\n# list-clusters\nLIST_CLUSTERS_ACTIVE_STATES = ['STARTING', 'BOOTSTRAPPING', 'RUNNING',\n                               'WAITING', 'TERMINATING']\nLIST_CLUSTERS_TERMINATED_STATES = ['TERMINATED']\nLIST_CLUSTERS_FAILED_STATES = ['TERMINATED_WITH_ERRORS']\n\nINSTANCE_FLEET_TYPE = 'INSTANCE_FLEET'\n", "awscli/customizations/emr/installapplications.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nfrom awscli.customizations.emr import applicationutils\nfrom awscli.customizations.emr import argumentschema\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr.command import Command\n\n\nclass InstallApplications(Command):\n    NAME = 'install-applications'\n    DESCRIPTION = ('Installs applications on a running cluster. Currently only'\n                   ' Hive and Pig can be installed using this command, and'\n                   ' this command is only supported by AMI versions'\n                   ' (3.x and 2.x).')\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': helptext.CLUSTER_ID},\n        {'name': 'applications', 'required': True,\n         'help_text': helptext.INSTALL_APPLICATIONS,\n         'schema': argumentschema.APPLICATIONS_SCHEMA},\n    ]\n    # Applications supported by the install-applications command.\n    supported_apps = ['HIVE', 'PIG']\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n\n        parameters = {'JobFlowId': parsed_args.cluster_id}\n\n        self._check_for_supported_apps(parsed_args.applications)\n        parameters['Steps'] = applicationutils.build_applications(\n            self.region, parsed_args.applications)[2]\n\n        emrutils.call_and_display_response(self._session, 'AddJobFlowSteps',\n                                           parameters, parsed_globals)\n        return 0\n\n    def _check_for_supported_apps(self, parsed_applications):\n        for app_config in parsed_applications:\n            app_name = app_config['Name'].upper()\n\n            if app_name in constants.APPLICATIONS:\n                if app_name not in self.supported_apps:\n                    raise ValueError(\n                        \"aws: error: \" + app_config['Name'] + \" cannot be\"\n                        \" installed on a running cluster. 'Name' should be one\"\n                        \" of the following: \" +\n                        ', '.join(self.supported_apps))\n            else:\n                raise ValueError(\n                    \"aws: error: Unknown application: \" + app_config['Name'] +\n                    \". 'Name' should be one of the following: \" +\n                    ', '.join(constants.APPLICATIONS))\n", "awscli/customizations/emr/instancegroupsutils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import exceptions\n\n\ndef build_instance_groups(parsed_instance_groups):\n    \"\"\"\n    Helper method that converts --instance-groups option value in\n    create-cluster and add-instance-groups to\n    Amazon Elastic MapReduce InstanceGroupConfig data type.\n    \"\"\"\n    instance_groups = []\n    for instance_group in parsed_instance_groups:\n        ig_config = {}\n\n        keys = instance_group.keys()\n        if 'Name' in keys:\n            ig_config['Name'] = instance_group['Name']\n        else:\n            ig_config['Name'] = instance_group['InstanceGroupType']\n        ig_config['InstanceType'] = instance_group['InstanceType']\n        ig_config['InstanceCount'] = instance_group['InstanceCount']\n        ig_config['InstanceRole'] = instance_group['InstanceGroupType'].upper()\n\n        if 'BidPrice' in keys:\n            if instance_group['BidPrice'] != 'OnDemandPrice':\n                ig_config['BidPrice'] = instance_group['BidPrice']\n            ig_config['Market'] = constants.SPOT\n        else:\n            ig_config['Market'] = constants.ON_DEMAND\n        if 'EbsConfiguration' in keys:\n            ig_config['EbsConfiguration'] = instance_group['EbsConfiguration']\n\n        if 'AutoScalingPolicy' in keys:\n            ig_config['AutoScalingPolicy'] = instance_group['AutoScalingPolicy']\n\n        if 'Configurations' in keys:\n            ig_config['Configurations'] = instance_group['Configurations']\n\n        if 'CustomAmiId' in keys:\n            ig_config['CustomAmiId'] = instance_group['CustomAmiId']\n\n        instance_groups.append(ig_config)\n    return instance_groups\n\n\ndef _build_instance_group(\n        instance_type, instance_count, instance_group_type):\n    ig_config = {}\n    ig_config['InstanceType'] = instance_type\n    ig_config['InstanceCount'] = instance_count\n    ig_config['InstanceRole'] = instance_group_type.upper()\n    ig_config['Name'] = ig_config['InstanceRole']\n    ig_config['Market'] = constants.ON_DEMAND\n    return ig_config\n\n\ndef validate_and_build_instance_groups(\n        instance_groups, instance_type, instance_count):\n    if (instance_groups is None and instance_type is None):\n        raise exceptions.MissingRequiredInstanceGroupsError\n\n    if (instance_groups is not None and\n        (instance_type is not None or\n            instance_count is not None)):\n        raise exceptions.InstanceGroupsValidationError\n\n    if instance_groups is not None:\n        return build_instance_groups(instance_groups)\n    else:\n        instance_groups = []\n        master_ig = _build_instance_group(\n            instance_type=instance_type,\n            instance_count=1,\n            instance_group_type=\"MASTER\")\n        instance_groups.append(master_ig)\n        if instance_count is not None and int(instance_count) > 1:\n            core_ig = _build_instance_group(\n                instance_type=instance_type,\n                instance_count=int(instance_count) - 1,\n                instance_group_type=\"CORE\")\n            instance_groups.append(core_ig)\n\n        return instance_groups\n", "awscli/customizations/emr/terminateclusters.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr.command import Command\n\n\nclass TerminateClusters(Command):\n    NAME = 'terminate-clusters'\n    DESCRIPTION = helptext.TERMINATE_CLUSTERS\n    ARG_TABLE = [{\n        'name': 'cluster-ids', 'nargs': '+', 'required': True,\n        'help_text': '<p>A list of clusters to terminate.</p>',\n        'schema': {'type': 'array', 'items': {'type': 'string'}},\n    }]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        parameters = {'JobFlowIds': parsed_args.cluster_ids}\n        emrutils.call_and_display_response(self._session,\n                                           'TerminateJobFlows', parameters,\n                                           parsed_globals)\n        return 0\n", "awscli/customizations/emr/helptext.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr.createdefaultroles import EMR_ROLE_NAME\nfrom awscli.customizations.emr.createdefaultroles import EC2_ROLE_NAME\n\nTERMINATE_CLUSTERS = (\n    'Shuts down one or more clusters, each specified by cluster ID. '\n    'Use this command only on clusters that do not have termination '\n    'protection enabled. Clusters with termination protection enabled '\n    'are not terminated. When a cluster is shut '\n    'down, any step not yet completed is canceled and the '\n    'Amazon EC2 instances in the cluster are terminated. '\n    'Any log files not already saved are uploaded to '\n    'Amazon S3 if a --log-uri was specified when the cluster was created. '\n    'The maximum number of clusters allowed in the list is 10. '\n    'The command is asynchronous. Depending on the '\n    'configuration of the cluster, it may take from 5 to 20 minutes for the '\n    'cluster to terminate completely and release allocated resources such as '\n    'Amazon EC2 instances.')\n\nCLUSTER_ID = (\n    '<p>A unique string that identifies a cluster. The '\n    '<code>create-cluster</code> command returns this identifier. You can '\n    'use the <code>list-clusters</code> command to get cluster IDs.</p>')\n\nHBASE_BACKUP_DIR = (\n    '<p>The Amazon S3 location of the Hbase backup. Example: '\n    '<code>s3://mybucket/mybackup</code>, where <code>mybucket</code> is the '\n    'specified Amazon S3 bucket and mybackup is the specified backup '\n    'location. The path argument must begin with s3://, which '\n    'refers to an Amazon S3 bucket.</p>')\n\nHBASE_BACKUP_VERSION = (\n    '<p>The backup version to restore from. If not specified, the latest backup '\n    'in the specified location is used.</p>')\n\n# create-cluster options help text\n\nCREATE_CLUSTER_DESCRIPTION = (\n    'Creates an Amazon EMR cluster with the specified configurations.')\n\nDESCRIBE_CLUSTER_DESCRIPTION = (\n    'Provides  cluster-level details including status, hardware '\n    'and software configuration, VPC settings, bootstrap '\n    'actions, instance groups and so on. '\n    'Permissions needed for describe-cluster include '\n    'elasticmapreduce:ListBootstrapActions, '\n    'elasticmapreduce:ListInstanceFleets, '\n    'elasticmapreduce:DescribeCluster, '\n    'and elasticmapreduce:ListInstanceGroups.')\n\nCLUSTER_NAME = (\n    '<p>The name of the cluster. If not provided, the default is \"Development Cluster\".</p>')\n\nLOG_URI = (\n    '<p>Specifies the location in Amazon S3 to which log files '\n    'are periodically written. If a value is not provided, '\n    'logs files are not written to Amazon S3 from the master node '\n    'and are lost if the master node terminates.</p>')\n\nLOG_ENCRYPTION_KMS_KEY_ID = (\n    '<p> Specifies the KMS Id utilized for log encryption. If a value is '\n    'not provided, log files will be encrypted by default encryption method '\n    'AES-256. This attribute is only available with EMR version 5.30.0 and later, '\n    'excluding EMR 6.0.0.</p>')\n\nSERVICE_ROLE = (\n    '<p>Specifies an IAM service role, which Amazon EMR requires to call other AWS services '\n    'on your behalf during cluster operation. This parameter '\n    'is usually specified when a customized service role is used. '\n    'To specify the default service role, as well as the default instance '\n    'profile, use the <code>--use-default-roles</code> parameter. '\n    'If the role and instance profile do not already exist, use the '\n    '<code>aws emr create-default-roles</code> command to create them.</p>')\n\nAUTOSCALING_ROLE = (\n    '<p>Specify <code>--auto-scaling-role EMR_AutoScaling_DefaultRole</code>'\n    ' if an automatic scaling policy is specified for an instance group'\n    ' using the <code>--instance-groups</code> parameter. This default'\n    ' IAM role allows the automatic scaling feature'\n    ' to launch and terminate Amazon EC2 instances during scaling operations.</p>')\n\nUSE_DEFAULT_ROLES = (\n    '<p>Specifies that the cluster should use the default'\n    ' service role (EMR_DefaultRole) and instance profile (EMR_EC2_DefaultRole)'\n    ' for permissions to access other AWS services.</p>'\n    '<p>Make sure that the role and instance profile exist first. To create them,'\n    ' use the <code>create-default-roles</code> command.</p>')\n\nAMI_VERSION = (\n    '<p>Applies only to Amazon EMR release versions earlier than 4.0. Use'\n    ' <code>--release-label</code> for 4.0 and later. Specifies'\n    ' the version of Amazon Linux Amazon Machine Image (AMI)'\n    ' to use when launching Amazon EC2 instances in the cluster.'\n    ' For example, <code>--ami-version 3.1.0</code>.')\n\nRELEASE_LABEL = (\n    '<p>Specifies the Amazon EMR release version, which determines'\n    ' the versions of application software that are installed on the cluster.'\n    ' For example, <code>--release-label emr-5.15.0</code> installs'\n    ' the application versions and features available in that version.'\n    ' For details about application versions and features available'\n    ' in each release, see the Amazon EMR Release Guide:</p>'\n    '<p>https://docs.aws.amazon.com/emr/latest/ReleaseGuide</p>'\n    '<p>Use <code>--release-label</code> only for Amazon EMR release version 4.0'\n    ' and later. Use <code>--ami-version</code> for earlier versions.'\n    ' You cannot specify both a release label and AMI version.</p>')\n\nOS_RELEASE_LABEL = (\n    '<p>Specifies a particular Amazon Linux release for all nodes in a cluster'\n    ' launch request. If a release is not specified, EMR uses the latest validated' \n    ' Amazon Linux release for cluster launch.</p>')\n\nCONFIGURATIONS = (\n    '<p>Specifies a JSON file that contains configuration classifications,'\n    ' which you can use to customize applications that Amazon EMR installs'\n    ' when cluster instances launch. Applies only to Amazon EMR 4.0 and later.'\n    ' The file referenced can either be stored locally (for example,'\n    ' <code>--configurations file://configurations.json</code>)'\n    ' or stored in Amazon S3 (for example, <code>--configurations'\n    ' https://s3.amazonaws.com/myBucket/configurations.json</code>).'\n    ' Each classification usually corresponds to the xml configuration'\n    ' file for an application, such as <code>yarn-site</code> for YARN. For a list of'\n    ' available configuration classifications and example JSON, see'\n    ' the following topic in the Amazon EMR Release Guide:</p>'\n    '<p>https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html</p>')\n\nINSTANCE_GROUPS = (\n    '<p>Specifies the number and type of Amazon EC2 instances'\n    ' to create for each node type in a cluster, using uniform instance groups.'\n    ' You can specify either <code>--instance-groups</code> or'\n    ' <code>--instance-fleets</code> but not both.'\n    ' For more information, see the following topic in the EMR Management Guide:</p>'\n    '<p>https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-group-configuration.html</p>'\n    '<p>You can specify arguments individually using multiple'\n    ' <code>InstanceGroupType</code> argument blocks, one for the <code>MASTER</code>'\n    ' instance group, one for a <code>CORE</code> instance group,'\n    ' and optional, multiple <code>TASK</code> instance groups.</p>'\n    '<p>If you specify inline JSON structures, enclose the entire'\n    ' <code>InstanceGroupType</code> argument block in single quotation marks.'\n    '<p>Each <code>InstanceGroupType</code> block takes the following inline arguments.'\n    ' Optional arguments are shown in [square brackets].</p>'\n    '<li><code>[Name]</code> - An optional friendly name for the instance group.</li>'\n    '<li><code>InstanceGroupType</code> - <code>MASTER</code>, <code>CORE</code>, or <code>TASK</code>.</li>'\n    '<li><code>InstanceType</code> - The type of EC2 instance, for'\n    ' example <code>m4.large</code>,'\n    ' to use for all nodes in the instance group.</li>'\n    '<li><code>InstanceCount</code> - The number of EC2 instances to provision in the instance group.</li>'\n    '<li><code>[BidPrice]</code> - If specified, indicates that the instance group uses Spot Instances.'\n    ' This is the maximum price you are willing to pay for Spot Instances. Specify OnDemandPrice'\n    ' to set the amount equal to the On-Demand price, or specify an amount in USD.</li>'\n    '<li><code>[EbsConfiguration]</code> - Specifies additional Amazon EBS storage volumes attached'\n    ' to EC2 instances using an inline JSON structure.</li>'\n    '<li><code>[AutoScalingPolicy]</code> - Specifies an automatic scaling policy for the'\n    ' instance group using an inline JSON structure.</li>')\n\nINSTANCE_FLEETS = (\n    '<p>Applies only to Amazon EMR release version 5.0 and later. Specifies'\n    ' the number and type of Amazon EC2 instances to create'\n    ' for each node type in a cluster, using instance fleets.'\n    ' You can specify either <code>--instance-fleets</code> or'\n    ' <code>--instance-groups</code> but not both.'\n    ' For more information and examples, see the following topic in the Amazon EMR Management Guide:</p>'\n    '<p>https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html</p>'\n    '<p>You can specify arguments individually using multiple'\n    ' <code>InstanceFleetType</code> argument blocks, one for the <code>MASTER</code>'\n    ' instance fleet, one for a <code>CORE</code> instance fleet,'\n    ' and an optional <code>TASK</code> instance fleet.</p>'\n    '<p>The following arguments can be specified for each instance fleet. Optional arguments are shown in [square brackets].</p>'\n    '<li><code>[Name]</code> - An optional friendly name for the instance fleet.</li>'\n    '<li><code>InstanceFleetType</code> - <code>MASTER</code>, <code>CORE</code>, or <code>TASK</code>.</li>'\n    '<li><code>TargetOnDemandCapacity</code> - The target capacity of On-Demand units'\n    ' for the instance fleet, which determines how many On-Demand Instances to provision.'\n    ' The <code>WeightedCapacity</code> specified for an instance type within'\n    ' <code>InstanceTypeConfigs</code> counts toward this total when an instance type'\n    ' with the On-Demand purchasing option launches.</li>'\n    '<li><code>TargetSpotCapacity</code> - The target capacity of Spot units'\n    ' for the instance fleet, which determines how many Spot Instances to provision.'\n    ' The <code>WeightedCapacity</code> specified for an instance type within'\n    ' <code>InstanceTypeConfigs</code> counts toward this total when an instance'\n    ' type with the Spot purchasing option launches.</li>'\n    '<li><code>[LaunchSpecifications]</code> - When <code>TargetSpotCapacity</code> is specified,'\n    ' specifies the block duration and timeout action for Spot Instances.'\n    '<li><code>InstanceTypeConfigs</code> - Specify up to five EC2 instance types to'\n    ' use in the instance fleet, including details such as Spot price and Amazon EBS configuration.'\n    ' When you use an On-Demand or Spot Instance allocation strategy,'\n    ' you can specify up to 30 instance types per instance fleet.</li>')\n\nINSTANCE_TYPE = (\n    '<p>Shortcut parameter as an alternative to <code>--instance-groups</code>.'\n    ' Specifies the type of Amazon EC2 instance to use in a cluster.'\n    ' If used without the <code>--instance-count</code> parameter,'\n    ' the cluster consists of a single master node running on the EC2 instance type'\n    ' specified. When used together with <code>--instance-count</code>,'\n    ' one instance is used for the master node, and the remainder'\n    ' are used for the core node type.</p>')\n\nINSTANCE_COUNT = (\n    '<p>Shortcut parameter as an alternative to <code>--instance-groups</code>'\n    ' when used together with <code>--instance-type</code>. Specifies the'\n    ' number of Amazon EC2 instances to create for a cluster.'\n    ' One instance is used for the master node, and the remainder'\n    ' are used for the core node type.</p>')\n\nADDITIONAL_INFO = (\n    '<p>Specifies additional information during cluster creation. To set development mode when starting your EMR cluster,'\n    ' set this parameter to <code>{\"clusterType\":\"development\"}</code>.</p>')\n\nEC2_ATTRIBUTES = (\n    '<p>Configures cluster and Amazon EC2 instance configurations. Accepts'\n    ' the following arguments:</p>'\n    '<li><code>KeyName</code> - Specifies the name of the AWS EC2 key pair that will be used for'\n    ' SSH connections to the master node and other instances on the cluster.</li>'\n    '<li><code>AvailabilityZone</code> - Applies to clusters that use the uniform instance group configuration.'\n    ' Specifies the availability zone in which to launch the cluster.'\n    ' For example, <code>us-west-1b</code>. <code>AvailabilityZone</code> is used for uniform instance groups,'\n    ' while <code>AvailabilityZones</code> (plural) is used for instance fleets.</li>'\n    '<li><code>AvailabilityZones</code> - Applies to clusters that use the instance fleet configuration.'\n    ' When multiple Availability Zones are specified, Amazon EMR evaluates them and launches instances' \n    ' in the optimal Availability Zone. <code>AvailabilityZone</code> is used for uniform instance groups,'\n    ' while <code>AvailabilityZones</code> (plural) is used for instance fleets.</li>'\n    '<li><code>SubnetId</code> - Applies to clusters that use the uniform instance group configuration.' \n    ' Specify the VPC subnet in which to create the cluster. <code>SubnetId</code> is used for uniform instance groups,'\n    ' while <code>SubnetIds</code> (plural) is used for instance fleets.</li>'\n    '<li><code>SubnetIds</code> - Applies to clusters that use the instance fleet configuration.'\n    ' When multiple EC2 subnet IDs are specified, Amazon EMR evaluates them and launches instances in the optimal subnet.'\n    ' <code>SubnetId</code> is used for uniform instance groups,'\n    ' while <code>SubnetIds</code> (plural) is used for instance fleets.</li>'\n    '<li><code>InstanceProfile</code> - An IAM role that allows EC2 instances to'\n    ' access other AWS services, such as Amazon S3, that'\n    ' are required for operations.</li>'\n    '<li><code>EmrManagedMasterSecurityGroup</code> - The security group ID of the Amazon EC2'\n    ' security group for the master node.</li>'\n    '<li><code>EmrManagedSlaveSecurityGroup</code> - The security group ID of the Amazon EC2'\n    ' security group for the slave nodes.</li>'\n    '<li><code>ServiceAccessSecurityGroup</code> - The security group ID of the Amazon EC2 '\n    'security group for Amazon EMR access to clusters in VPC private subnets.</li>'\n    '<li><code>AdditionalMasterSecurityGroups</code> - A list of additional Amazon EC2'\n    ' security group IDs for the master node.</li>'\n    '<li><code>AdditionalSlaveSecurityGroups</code> - A list of additional Amazon EC2'\n    ' security group IDs for the slave nodes.</li>')\n\nAUTO_TERMINATE = (\n    '<p>Specifies whether the cluster should terminate after'\n    ' completing all the steps. Auto termination is off by default.</p>')\n\nTERMINATION_PROTECTED = (\n    '<p>Specifies whether to lock the cluster to prevent the'\n    ' Amazon EC2 instances from being terminated by API call,'\n    ' user intervention, or an error.</p>')\n\nSCALE_DOWN_BEHAVIOR = (\n    '<p>Specifies the way that individual Amazon EC2 instances terminate'\n    ' when an automatic scale-in activity occurs or an instance group is resized.</p>'\n    '<p>Accepted values:</p>'\n    '<li><code>TERMINATE_AT_TASK_COMPLETION</code> - Specifies that Amazon EMR'\n    ' blacklists and drains tasks from nodes before terminating the instance.</li>'\n    '<li><code>TERMINATE_AT_INSTANCE_HOUR</code> - Specifies that Amazon EMR'\n    ' terminate EC2 instances at the instance-hour boundary, regardless of when'\n    ' the request to terminate was submitted.</li>'\n)\n\nVISIBILITY = (\n    '<p>Specifies whether the cluster is visible to all IAM users'\n    ' of the AWS account associated with the cluster. If a user'\n    ' has the proper policy permissions set, they can also manage the cluster.</p>'\n    '<p>Visibility is on by default. The <code>--no-visible-to-all-users</code> option'\n    ' is no longer supported. To restrict cluster visibility, use an IAM policy.</p>')\n\nDEBUGGING = (\n    '<p>Specifies that the debugging tool is enabled for the cluster,'\n    ' which allows you to browse log files using the Amazon EMR console.'\n    ' Turning debugging on requires that you specify <code>--log-uri</code>'\n    ' because log files must be stored in Amazon S3 so that'\n    ' Amazon EMR can index them for viewing in the console.'\n    ' Effective January 23, 2023, Amazon EMR will discontinue the debugging tool for all versions.</p>')\n\nTAGS = (\n    '<p>A list of tags to associate with a cluster, which apply to'\n    ' each Amazon EC2 instance in the cluster. Tags are key-value pairs that'\n    ' consist of a required key string'\n    ' with a maximum of 128 characters, and an optional value string'\n    ' with a maximum of 256 characters.</p>'\n    '<p>You can specify tags in <code>key=value</code> format or you can add a'\n    ' tag without a value using only the key name, for example <code>key</code>.'\n    ' Use a space to separate multiple tags.</p>')\n\nBOOTSTRAP_ACTIONS = (\n    '<p>Specifies a list of bootstrap actions to run on each EC2 instance when'\n    ' a cluster is created. Bootstrap actions run on each instance'\n    ' immediately after Amazon EMR provisions the EC2 instance and'\n    ' before Amazon EMR installs specified applications.</p>'\n    '<p>You can specify a bootstrap action as an inline JSON structure'\n    ' enclosed in single quotation marks, or you can use a shorthand'\n    ' syntax, specifying multiple bootstrap actions, each separated'\n    ' by a space. When using the shorthand syntax, each bootstrap'\n    ' action takes the following parameters, separated by'\n    ' commas with no trailing space. Optional parameters'\n    ' are shown in [square brackets].</p>'\n    '<li><code>Path</code> - The path and file name of the script'\n    ' to run, which must be accessible to each instance in the cluster.'\n    ' For example, <code>Path=s3://mybucket/myscript.sh</code>.</li>'\n    '<li><code>[Name]</code> - A friendly name to help you identify'\n    ' the bootstrap action. For example, <code>Name=BootstrapAction1</code></li>'\n    '<li><code>[Args]</code> - A comma-separated list of arguments'\n    ' to pass to the bootstrap action script. Arguments can be'\n    ' either a list of values (<code>Args=arg1,arg2,arg3</code>)'\n    ' or a list of key-value pairs, as well as optional values,'\n    ' enclosed in square brackets (<code>Args=[arg1,arg2=arg2value,arg3])</li>.')\n\nAPPLICATIONS = (\n    '<p>Specifies the applications to install on the cluster.'\n    ' Available applications and their respective versions vary'\n    ' by Amazon EMR release. For more information, see the'\n    ' Amazon EMR Release Guide:</p>'\n    '<p>https://docs.aws.amazon.com/emr/latest/ReleaseGuide/</p>'\n    '<p>When using versions of Amazon EMR earlier than 4.0,'\n    ' some applications take optional arguments for configuration.'\n    ' Arguments should either be a comma-separated list of values'\n    ' (<code>Args=arg1,arg2,arg3</code>) or a bracket-enclosed list of values'\n    ' and key-value pairs (<code>Args=[arg1,arg2=arg3,arg4]</code>).</p>')\n\nEMR_FS = (\n    '<p>Specifies EMRFS configuration options, such as consistent view'\n    ' and Amazon S3 encryption parameters.</p>'\n    '<p>When you use Amazon EMR release version 4.8.0 or later, we recommend'\n    ' that you use the <code>--configurations</code> option together'\n    ' with the <code>emrfs-site</code> configuration classification'\n    ' to configure EMRFS, and use security configurations'\n    ' to configure encryption for EMRFS data in Amazon S3 instead.'\n    ' For more information, see the following topic in the Amazon EMR Management Guide:</p>'\n    '<p>https://docs.aws.amazon.com/emr/latest/ManagementGuide/emrfs-configure-consistent-view.html</p>')\n\nRESTORE_FROM_HBASE = (\n    '<p>Applies only when using Amazon EMR release versions earlier than 4.0.'\n    ' Launches a new HBase cluster and populates it with'\n    ' data from a previous backup of an HBase cluster. HBase'\n    ' must be installed using the <code>--applications</code> option.</p>')\n\nSTEPS = (\n    '<p>Specifies a list of steps to be executed by the cluster. Steps run'\n    ' only on the master node after applications are installed'\n    ' and are used to submit work to a cluster. A step can be'\n    ' specified using the shorthand syntax, by referencing a JSON file'\n    ' or by specifying an inline JSON structure. <code>Args</code> supplied with steps'\n    ' should be a comma-separated list of values (<code>Args=arg1,arg2,arg3</code>) or'\n    ' a bracket-enclosed list of values and key-value'\n    ' pairs (<code>Args=[arg1,arg2=value,arg4</code>).</p>')\n\nINSTALL_APPLICATIONS = (\n    '<p>The applications to be installed.'\n    ' Takes the following parameters: '\n    '<code>Name</code> and <code>Args</code>.</p>')\n\nEBS_ROOT_VOLUME_SIZE = (\n    '<p>This option is available only with Amazon EMR version 4.x and later. Specifies the size,'\n    ' in GiB, of the EBS root device volume of the Amazon Linux AMI'\n    ' that is used for each EC2 instance in the cluster. </p>')\n\nEBS_ROOT_VOLUME_IOPS = (\n    '<p>This option is available only with Amazon EMR version 6.15.0 and later. Specifies the IOPS,'\n    ' of the EBS root device volume of the Amazon Linux AMI'\n    ' that is used for each EC2 instance in the cluster. </p>')\n\nEBS_ROOT_VOLUME_THROUGHPUT = (\n    '<p>This option is available only with Amazon EMR version 6.15.0 and later. Specifies the throughput,'\n    ' in MiB/s, of the EBS root device volume of the Amazon Linux AMI'\n    ' that is used for each EC2 instance in the cluster. </p>')\n\n\nSECURITY_CONFIG = (\n    '<p>Specifies the name of a security configuration to use for the cluster.'\n    ' A security configuration defines data encryption settings and'\n    ' other security options. For more information, see'\n    ' the following topic in the Amazon EMR Management Guide:</p>'\n    '<p>https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-encryption-enable-security-configuration.html</p>'\n    '<p>Use <code>list-security-configurations</code> to get a list of available'\n    ' security configurations in the active account.</p>')\n\nCUSTOM_AMI_ID = (\n    '<p>Applies only to Amazon EMR release version 5.7.0 and later.'\n    ' Specifies the AMI ID of a custom AMI to use'\n    ' when Amazon EMR provisions EC2 instances. A custom'\n    ' AMI can be used to encrypt the Amazon EBS root volume. It'\n    ' can also be used instead of bootstrap actions to customize'\n    ' cluster node configurations. For more information, see'\n    ' the following topic in the Amazon EMR Management Guide:</p>'\n    '<p>https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html</p>')\n\nREPO_UPGRADE_ON_BOOT = (\n    '<p>Applies only when a <code>--custom-ami-id</code> is'\n    ' specified. On first boot, by default, Amazon Linux AMIs'\n    ' connect to package repositories to install security updates'\n    ' before other services start. You can set this parameter'\n    ' using <code>--rep-upgrade-on-boot NONE</code> to'\n    ' disable these updates. CAUTION: This creates additional'\n    ' security risks.</p>')\n\nKERBEROS_ATTRIBUTES = (\n     '<p>Specifies required cluster attributes for Kerberos when Kerberos authentication'\n     ' is enabled in the specified <code>--security-configuration</code>.'\n     ' Takes the following arguments:</p>'\n     ' <li><code>Realm</code> - Specifies the name of the Kerberos'\n     ' realm to which all nodes in a cluster belong. For example,'\n     ' <code>Realm=EC2.INTERNAL</code>.</li>'\n     ' <li><code>KdcAdminPassword</code> - Specifies the password used within the cluster'\n     ' for the kadmin service, which maintains Kerberos principals, password'\n     ' policies, and keytabs for the cluster.</li>'\n     ' <li><code>CrossRealmTrustPrincipalPassword</code> - Required when establishing a cross-realm trust'\n     ' with a KDC in a different realm. This is the cross-realm principal password,'\n     ' which must be identical across realms.</li>'\n     ' <li><code>ADDomainJoinUser</code> - Required when establishing trust with an Active Directory'\n     ' domain. This is the User logon name of an AD account with sufficient privileges to join resources to the domain.</li>'\n     ' <li><code>ADDomainJoinPassword</code> - The AD password for <code>ADDomainJoinUser</code>.</li>')\n\n# end create-cluster options help descriptions\n\nLIST_CLUSTERS_CLUSTER_STATES = (\n    '<p>Specifies that only clusters in the states specified are'\n    ' listed. Alternatively, you can use the shorthand'\n    ' form for single states or a group of states.</p>'\n    '<p>Takes the following state values:</p>'\n    '<li><code>STARTING</code></li>'\n    '<li><code>BOOTSTRAPPING</code></li>'\n    '<li><code>RUNNING</code></li>'\n    '<li><code>WAITING</code></li>'\n    '<li><code>TERMINATING</code></li>'\n    '<li><code>TERMINATED</code></li>'\n    '<li><code>TERMINATED_WITH_ERRORS</code></li>')\n\nLIST_CLUSTERS_STATE_FILTERS = (\n    '<p>Shortcut options for --cluster-states. The'\n    ' following shortcut options can be specified:</p>'\n    '<li><code>--active</code> - list only clusters that'\n    ' are <code>STARTING</code>,<code>BOOTSTRAPPING</code>,'\n    ' <code>RUNNING</code>, <code>WAITING</code>, or <code>TERMINATING</code>. </li>'\n    '<li><code>--terminated</code> - list only clusters that are <code>TERMINATED</code>. </li>'\n    '<li><code>--failed</code> - list only clusters that are <code>TERMINATED_WITH_ERRORS</code>.</li>')\n\nLIST_CLUSTERS_CREATED_AFTER = (\n    '<p>List only those clusters created after the date and time'\n    ' specified in the format yyyy-mm-ddThh:mm:ss. For example,'\n    ' <code>--created-after 2017-07-04T00:01:30.</p>')\n\nLIST_CLUSTERS_CREATED_BEFORE = (\n    '<p>List only those clusters created before the date and time'\n    ' specified in the format yyyy-mm-ddThh:mm:ss. For example,'\n    ' <code>--created-before 2017-07-04T00:01:30.</p>')\n\nEMR_MANAGED_MASTER_SECURITY_GROUP = (\n    '<p>The identifier of the Amazon EC2 security group '\n    'for the master node.</p>')\n\nEMR_MANAGED_SLAVE_SECURITY_GROUP = (\n    '<p>The identifier of the Amazon EC2 security group '\n    'for the slave nodes.</p>')\n\nSERVICE_ACCESS_SECURITY_GROUP = (\n    '<p>The identifier of the Amazon EC2 security group '\n    'for Amazon EMR to access clusters in VPC private subnets.</p>')\n\nADDITIONAL_MASTER_SECURITY_GROUPS = (\n    '<p> A list of additional Amazon EC2 security group IDs for '\n    'the master node</p>')\n\nADDITIONAL_SLAVE_SECURITY_GROUPS = (\n    '<p>A list of additional Amazon EC2 security group IDs for '\n    'the slave nodes.</p>')\n\nAVAILABLE_ONLY_FOR_AMI_VERSIONS = (\n    'This command is only available when using Amazon EMR versions'\n    'earlier than 4.0.')\n\nSTEP_CONCURRENCY_LEVEL = (\n    'This command specifies the step concurrency level of the cluster.'\n    'Default is 1 which is non-concurrent.'\n)\n\nMANAGED_SCALING_POLICY = (\n    '<p>Managed scaling policy for an Amazon EMR cluster. The policy '\n    'specifies the limits for resources that can be added or terminated '\n    'from a cluster. You can specify the ComputeLimits which include '\n    'the MaximumCapacityUnits, MaximumCoreCapacityUnits, MinimumCapacityUnits, '\n    'MaximumOnDemandCapacityUnits and UnitType. For an '\n    'InstanceFleet cluster, the UnitType must be InstanceFleetUnits. For '\n    'InstanceGroup clusters, the UnitType can be either VCPU or Instances.</p>'\n)\n\nPLACEMENT_GROUP_CONFIGS = (\n    '<p>Placement group configuration for an Amazon EMR ' \n    'cluster. The configuration specifies the EC2 placement group ' \n    'strategy associated with each EMR Instance Role.</p> ' \n    '<p>Currently, we support placement group only for <code>MASTER</code> ' \n    'role with <code>SPREAD</code> strategy by default. You can opt-in by '\n    'passing <code>--placement-group-configs InstanceRole=MASTER</code> '\n    'during cluster creation.</p>'\n)\n\nAUTO_TERMINATION_POLICY = (\n    '<p>Auto termination policy for an Amazon EMR cluster. '\n    'The configuration specifies the termination idle timeout'\n    'threshold for an cluster.</p> '\n)\n\nEXECUTION_ROLE_ARN = (\n    '<p>You must grant the execution role the permissions needed '\n    'to access the same IAM resources that the step can access. '\n    'The execution role can be a cross-account IAM Role.</p> '\n)\n\nUNHEALTHY_NODE_REPLACEMENT = (\n    '<p>Unhealthy node replacement for an Amazon EMR cluster.</p> '\n)\n", "awscli/customizations/emr/ssh.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\nimport subprocess\nimport tempfile\n\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import sshutils\nfrom awscli.customizations.emr.command import Command\n\nKEY_PAIR_FILE_HELP_TEXT = '\\nA value for the variable Key Pair File ' \\\n    'can be set in the AWS CLI config file using the ' \\\n    '\"aws configure set emr.key_pair_file <value>\" command.\\n'\n\n\nclass Socks(Command):\n    NAME = 'socks'\n    DESCRIPTION = ('Create a socks tunnel on port 8157 from your machine '\n                   'to the master.\\n%s' % KEY_PAIR_FILE_HELP_TEXT)\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': 'Cluster Id of cluster you want to ssh into'},\n        {'name': 'key-pair-file', 'required': True,\n         'help_text': 'Private key file to use for login'},\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        try:\n            master_dns = sshutils.validate_and_find_master_dns(\n                session=self._session,\n                parsed_globals=parsed_globals,\n                cluster_id=parsed_args.cluster_id)\n\n            key_file = parsed_args.key_pair_file\n            sshutils.validate_ssh_with_key_file(key_file)\n            f = tempfile.NamedTemporaryFile(delete=False)\n            if (emrutils.which('ssh') or emrutils.which('ssh.exe')):\n                command = ['ssh', '-o', 'StrictHostKeyChecking=no', '-o',\n                           'ServerAliveInterval=10', '-ND', '8157', '-i',\n                           parsed_args.key_pair_file, constants.SSH_USER +\n                           '@' + master_dns]\n            else:\n                command = ['putty', '-ssh', '-i', parsed_args.key_pair_file,\n                           constants.SSH_USER + '@' + master_dns, '-N', '-D',\n                           '8157']\n\n            print(' '.join(command))\n            rc = subprocess.call(command)\n            return rc\n        except KeyboardInterrupt:\n            print('Disabling Socks Tunnel.')\n            return 0\n\n\nclass SSH(Command):\n    NAME = 'ssh'\n    DESCRIPTION = ('SSH into master node of the cluster.\\n%s' %\n                   KEY_PAIR_FILE_HELP_TEXT)\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': 'Cluster Id of cluster you want to ssh into'},\n        {'name': 'key-pair-file', 'required': True,\n         'help_text': 'Private key file to use for login'},\n        {'name': 'command', 'help_text': 'Command to execute on Master Node'}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        master_dns = sshutils.validate_and_find_master_dns(\n            session=self._session,\n            parsed_globals=parsed_globals,\n            cluster_id=parsed_args.cluster_id)\n\n        key_file = parsed_args.key_pair_file\n        sshutils.validate_ssh_with_key_file(key_file)\n        f = tempfile.NamedTemporaryFile(delete=False)\n        if (emrutils.which('ssh') or emrutils.which('ssh.exe')):\n            command = ['ssh', '-o', 'StrictHostKeyChecking=no', '-o',\n                       'ServerAliveInterval=10', '-i',\n                       parsed_args.key_pair_file, constants.SSH_USER +\n                       '@' + master_dns, '-t']\n            if parsed_args.command:\n                command.append(parsed_args.command)\n        else:\n            command = ['putty', '-ssh', '-i', parsed_args.key_pair_file,\n                       constants.SSH_USER + '@' + master_dns, '-t']\n            if parsed_args.command:\n                f.write(parsed_args.command)\n                f.write('\\nread -n1 -r -p \"Command completed. Press any key.\"')\n                command.append('-m')\n                command.append(f.name)\n\n        f.close()\n        print(' '.join(command))\n        rc = subprocess.call(command)\n        os.remove(f.name)\n        return rc\n\n\nclass Put(Command):\n    NAME = 'put'\n    DESCRIPTION = ('Put file onto the master node.\\n%s' %\n                   KEY_PAIR_FILE_HELP_TEXT)\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': 'Cluster Id of cluster you want to put file onto'},\n        {'name': 'key-pair-file', 'required': True,\n         'help_text': 'Private key file to use for login'},\n        {'name': 'src', 'required': True,\n         'help_text': 'Source file path on local machine'},\n        {'name': 'dest', 'help_text': 'Destination file path on remote host'}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        master_dns = sshutils.validate_and_find_master_dns(\n            session=self._session,\n            parsed_globals=parsed_globals,\n            cluster_id=parsed_args.cluster_id)\n\n        key_file = parsed_args.key_pair_file\n        sshutils.validate_scp_with_key_file(key_file)\n        if (emrutils.which('scp') or emrutils.which('scp.exe')):\n            command = ['scp', '-r', '-o StrictHostKeyChecking=no',\n                       '-i', parsed_args.key_pair_file, parsed_args.src,\n                       constants.SSH_USER + '@' + master_dns]\n        else:\n            command = ['pscp', '-scp', '-r', '-i', parsed_args.key_pair_file,\n                       parsed_args.src, constants.SSH_USER + '@' + master_dns]\n\n        # if the instance is not terminated\n        if parsed_args.dest:\n            command[-1] = command[-1] + \":\" + parsed_args.dest\n        else:\n            command[-1] = command[-1] + \":\" + parsed_args.src.split('/')[-1]\n        print(' '.join(command))\n        rc = subprocess.call(command)\n        return rc\n\n\nclass Get(Command):\n    NAME = 'get'\n    DESCRIPTION = ('Get file from master node.\\n%s' % KEY_PAIR_FILE_HELP_TEXT)\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': 'Cluster Id of cluster you want to get file from'},\n        {'name': 'key-pair-file', 'required': True,\n         'help_text': 'Private key file to use for login'},\n        {'name': 'src', 'required': True,\n         'help_text': 'Source file path on remote host'},\n        {'name': 'dest', 'help_text': 'Destination file path on your machine'}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        master_dns = sshutils.validate_and_find_master_dns(\n            session=self._session,\n            parsed_globals=parsed_globals,\n            cluster_id=parsed_args.cluster_id)\n\n        key_file = parsed_args.key_pair_file\n        sshutils.validate_scp_with_key_file(key_file)\n        if (emrutils.which('scp') or emrutils.which('scp.exe')):\n            command = ['scp', '-r', '-o StrictHostKeyChecking=no', '-i',\n                       parsed_args.key_pair_file, constants.SSH_USER + '@' +\n                       master_dns + ':' + parsed_args.src]\n        else:\n            command = ['pscp', '-scp', '-r', '-i', parsed_args.key_pair_file,\n                       constants.SSH_USER + '@' + master_dns + ':' +\n                       parsed_args.src]\n\n        if parsed_args.dest:\n            command.append(parsed_args.dest)\n        else:\n            command.append(parsed_args.src.split('/')[-1])\n        print(' '.join(command))\n        rc = subprocess.call(command)\n        return rc\n", "awscli/customizations/emr/__init__.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "awscli/customizations/emr/addinstancegroups.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nfrom awscli.customizations.emr import argumentschema\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr import instancegroupsutils\nfrom awscli.customizations.emr.command import Command\n\n\nclass AddInstanceGroups(Command):\n    NAME = 'add-instance-groups'\n    DESCRIPTION = 'Adds an instance group to a running cluster.'\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': helptext.CLUSTER_ID},\n        {'name': 'instance-groups', 'required': True,\n         'help_text': helptext.INSTANCE_GROUPS,\n         'schema': argumentschema.INSTANCE_GROUPS_SCHEMA}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        parameters = {'JobFlowId': parsed_args.cluster_id}\n        parameters['InstanceGroups'] = \\\n            instancegroupsutils.build_instance_groups(\n            parsed_args.instance_groups)\n\n        add_instance_groups_response = emrutils.call(\n            self._session, 'add_instance_groups', parameters,\n            self.region, parsed_globals.endpoint_url,\n            parsed_globals.verify_ssl)\n\n        constructed_result = self._construct_result(\n            add_instance_groups_response)\n\n        emrutils.display_response(self._session, 'add_instance_groups',\n                                  constructed_result, parsed_globals)\n        return 0\n\n    def _construct_result(self, add_instance_groups_result):\n        jobFlowId = None\n        instanceGroupIds = None\n        clusterArn = None\n        if add_instance_groups_result is not None:\n            jobFlowId = add_instance_groups_result.get('JobFlowId')\n            instanceGroupIds = add_instance_groups_result.get(\n                'InstanceGroupIds')\n            clusterArn = add_instance_groups_result.get('ClusterArn')\n\n        if jobFlowId is not None and instanceGroupIds is not None:\n            return {'ClusterId': jobFlowId,\n                    'InstanceGroupIds': instanceGroupIds,\n                    'ClusterArn': clusterArn}\n        else:\n            return {}\n", "awscli/customizations/emr/emrutils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport json\nimport logging\nimport os\n\n\nfrom awscli.clidriver import CLIOperationCaller\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import exceptions\nfrom botocore.exceptions import WaiterError, NoCredentialsError\nfrom botocore import xform_name\n\nLOG = logging.getLogger(__name__)\n\n\ndef parse_tags(raw_tags_list):\n    tags_dict_list = []\n    if raw_tags_list:\n        for tag in raw_tags_list:\n            if tag.find('=') == -1:\n                key, value = tag, ''\n            else:\n                key, value = tag.split('=', 1)\n            tags_dict_list.append({'Key': key, 'Value': value})\n\n    return tags_dict_list\n\n\ndef parse_key_value_string(key_value_string):\n    # raw_key_value_string is a list of key value pairs separated by comma.\n    # Examples: \"k1=v1,k2='v  2',k3,k4\"\n    key_value_list = []\n    if key_value_string is not None:\n        raw_key_value_list = key_value_string.split(',')\n        for kv in raw_key_value_list:\n            if kv.find('=') == -1:\n                key, value = kv, ''\n            else:\n                key, value = kv.split('=', 1)\n            key_value_list.append({'Key': key, 'Value': value})\n        return key_value_list\n    else:\n        return None\n\n\ndef apply_boolean_options(\n        true_option, true_option_name, false_option, false_option_name):\n    if true_option and false_option:\n        error_message = \\\n            'aws: error: cannot use both ' + true_option_name + \\\n            ' and ' + false_option_name + ' options together.'\n        raise ValueError(error_message)\n    elif true_option:\n        return True\n    else:\n        return False\n\n\n# Deprecate. Rename to apply_dict\ndef apply(params, key, value):\n    if value:\n        params[key] = value\n\n    return params\n\n\ndef apply_dict(params, key, value):\n    if value:\n        params[key] = value\n\n    return params\n\n\ndef apply_params(src_params, src_key, dest_params, dest_key):\n    if src_key in src_params.keys() and src_params[src_key]:\n        dest_params[dest_key] = src_params[src_key]\n\n    return dest_params\n\n\ndef build_step(\n        jar, name='Step',\n        action_on_failure=constants.DEFAULT_FAILURE_ACTION,\n        args=None,\n        main_class=None,\n        properties=None):\n    check_required_field(\n        structure='HadoopJarStep', name='Jar', value=jar)\n\n    step = {}\n    apply_dict(step, 'Name', name)\n    apply_dict(step, 'ActionOnFailure', action_on_failure)\n    jar_config = {}\n    jar_config['Jar'] = jar\n    apply_dict(jar_config, 'Args', args)\n    apply_dict(jar_config, 'MainClass', main_class)\n    apply_dict(jar_config, 'Properties', properties)\n    step['HadoopJarStep'] = jar_config\n\n    return step\n\n\ndef build_bootstrap_action(\n        path,\n        name='Bootstrap Action',\n        args=None):\n    if path is None:\n        raise exceptions.MissingParametersError(\n            object_name='ScriptBootstrapActionConfig', missing='Path')\n    ba_config = {}\n    apply_dict(ba_config, 'Name', name)\n    script_config = {}\n    apply_dict(script_config, 'Args', args)\n    script_config['Path'] = path\n    apply_dict(ba_config, 'ScriptBootstrapAction', script_config)\n\n    return ba_config\n\n\ndef build_s3_link(relative_path='', region='us-east-1'):\n    if region is None:\n        region = 'us-east-1'\n    return 's3://{0}.elasticmapreduce{1}'.format(region, relative_path)\n\n\ndef get_script_runner(region='us-east-1'):\n    if region is None:\n        region = 'us-east-1'\n    return build_s3_link(\n        relative_path=constants.SCRIPT_RUNNER_PATH, region=region)\n\n\ndef check_required_field(structure, name, value):\n    if not value:\n        raise exceptions.MissingParametersError(\n            object_name=structure, missing=name)\n\n\ndef check_empty_string_list(name, value):\n    if not value or (len(value) == 1 and value[0].strip() == \"\"):\n        raise exceptions.EmptyListError(param=name)\n\n\ndef call(session, operation_name, parameters, region_name=None,\n         endpoint_url=None, verify=None):\n    # We could get an error from get_endpoint() about not having\n    # a region configured.  Before this happens we want to check\n    # for credentials so we can give a good error message.\n    if session.get_credentials() is None:\n        raise NoCredentialsError()\n\n    client = session.create_client(\n        'emr', region_name=region_name, endpoint_url=endpoint_url,\n        verify=verify)\n    LOG.debug('Calling ' + str(operation_name))\n    return getattr(client, operation_name)(**parameters)\n\n\ndef get_example_file(command):\n    return open('awscli/examples/emr/' + command + '.rst')\n\n\ndef dict_to_string(dict, indent=2):\n    return json.dumps(dict, indent=indent)\n\n\ndef get_client(session, parsed_globals):\n    return session.create_client(\n        'emr',\n        region_name=get_region(session, parsed_globals),\n        endpoint_url=parsed_globals.endpoint_url,\n        verify=parsed_globals.verify_ssl)\n\n\ndef get_cluster_state(session, parsed_globals, cluster_id):\n    client = get_client(session, parsed_globals)\n    data = client.describe_cluster(ClusterId=cluster_id)\n    return data['Cluster']['Status']['State']\n\n\ndef find_master_dns(session, parsed_globals, cluster_id):\n    \"\"\"\n    Returns the master_instance's 'PublicDnsName'.\n    \"\"\"\n    client = get_client(session, parsed_globals)\n    data = client.describe_cluster(ClusterId=cluster_id)\n    return data['Cluster']['MasterPublicDnsName']\n\n\ndef which(program):\n    for path in os.environ[\"PATH\"].split(os.pathsep):\n        path = path.strip('\"')\n        exe_file = os.path.join(path, program)\n        if os.path.isfile(exe_file) and os.access(exe_file, os.X_OK):\n            return exe_file\n\n    return None\n\n\ndef call_and_display_response(session, operation_name, parameters,\n                              parsed_globals):\n    cli_operation_caller = CLIOperationCaller(session)\n    cli_operation_caller.invoke(\n        'emr', operation_name,\n        parameters, parsed_globals)\n\n\ndef display_response(session, operation_name, result, parsed_globals):\n    cli_operation_caller = CLIOperationCaller(session)\n    # Calling a private method. Should be changed after the functionality\n    # is moved outside CliOperationCaller.\n    cli_operation_caller._display_response(\n        operation_name, result, parsed_globals)\n\n\ndef get_region(session, parsed_globals):\n    region = parsed_globals.region\n    if region is None:\n        region = session.get_config_variable('region')\n    return region\n\n\ndef join(values, separator=',', lastSeparator='and'):\n    \"\"\"\n    Helper method to print a list of values\n    [1,2,3] -> '1, 2 and 3'\n    \"\"\"\n    values = [str(x) for x in values]\n    if len(values) < 1:\n        return \"\"\n    elif len(values) == 1:\n        return values[0]\n    else:\n        separator = '%s ' % separator\n        return ' '.join([separator.join(values[:-1]),\n                         lastSeparator, values[-1]])\n\n\ndef split_to_key_value(string):\n    if string.find('=') == -1:\n        return string, ''\n    else:\n        return string.split('=', 1)\n\n\ndef get_cluster(cluster_id, session, region,\n                endpoint_url, verify_ssl):\n        describe_cluster_params = {'ClusterId': cluster_id}\n        describe_cluster_response = call(\n            session, 'describe_cluster', describe_cluster_params,\n            region, endpoint_url,\n            verify_ssl)\n\n        if describe_cluster_response is not None:\n            return describe_cluster_response.get('Cluster')\n\n\ndef get_release_label(cluster_id, session, region,\n                      endpoint_url, verify_ssl):\n        cluster = get_cluster(cluster_id, session, region,\n                              endpoint_url, verify_ssl)\n        if cluster is not None:\n            return cluster.get('ReleaseLabel')\n", "awscli/customizations/emr/argumentschema.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr.createdefaultroles import EC2_ROLE_NAME\n\nCONFIGURATIONS_PROPERTIES_SCHEMA = {\n    \"type\": \"map\",\n    \"key\": {\n        \"type\": \"string\",\n        \"description\": \"Configuration key\"\n    },\n    \"value\": {\n        \"type\": \"string\",\n        \"description\": \"Configuration value\"\n    },\n    \"description\": \"Application configuration properties\"\n}\n\nCONFIGURATIONS_CLASSIFICATION_SCHEMA = {\n    \"type\": \"string\",\n    \"description\": \"Application configuration classification name\",\n}\n\nINNER_CONFIGURATIONS_SCHEMA = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"Classification\": CONFIGURATIONS_CLASSIFICATION_SCHEMA,\n            \"Properties\": CONFIGURATIONS_PROPERTIES_SCHEMA\n        }\n    },\n    \"description\": \"Instance group application configurations.\"\n}\n\nOUTER_CONFIGURATIONS_SCHEMA = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"Classification\": CONFIGURATIONS_CLASSIFICATION_SCHEMA,\n            \"Properties\": CONFIGURATIONS_PROPERTIES_SCHEMA,\n            \"Configurations\": INNER_CONFIGURATIONS_SCHEMA\n        }\n    },\n    \"description\": \"Instance group application configurations.\"\n}\n\nINSTANCE_GROUPS_SCHEMA = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"Name\": {\n                \"type\": \"string\",\n                \"description\":\n                    \"Friendly name given to the instance group.\"\n            },\n            \"InstanceGroupType\": {\n                \"type\": \"string\",\n                \"description\":\n                    \"The type of the instance group in the cluster.\",\n                \"enum\": [\"MASTER\", \"CORE\", \"TASK\"],\n                \"required\": True\n            },\n            \"BidPrice\": {\n                \"type\": \"string\",\n                \"description\":\n                    \"Bid price for each Amazon EC2 instance in the \"\n                    \"instance group when launching nodes as Spot Instances, \"\n                    \"expressed in USD.\"\n            },\n            \"InstanceType\": {\n                \"type\": \"string\",\n                \"description\":\n                    \"The Amazon EC2 instance type for all instances \"\n                    \"in the instance group.\",\n                \"required\": True\n            },\n            \"InstanceCount\": {\n                \"type\": \"integer\",\n                \"description\": \"Target number of Amazon EC2 instances \"\n                \"for the instance group\",\n                \"required\": True\n            },\n            \"CustomAmiId\": {\n                \"type\": \"string\",\n                \"description\": \"The AMI ID of a custom AMI to use when Amazon EMR provisions EC2 instances.\"\n            },\n            \"EbsConfiguration\": {\n                \"type\": \"object\",\n                \"description\": \"EBS configuration that will be associated with the instance group.\",\n                \"properties\": {\n                    \"EbsOptimized\": {\n                        \"type\": \"boolean\",\n                        \"description\": \"Boolean flag used to tag EBS-optimized instances.\",\n                    },\n                    \"EbsBlockDeviceConfigs\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"VolumeSpecification\" : {\n                                    \"type\": \"object\",\n                                    \"description\": \"The EBS volume specification that will be created and attached to every instance in this instance group.\",\n                                    \"properties\": {\n                                        \"VolumeType\": {\n                                            \"type\": \"string\",\n                                            \"description\": \"The EBS volume type that is attached to all the instances in the instance group. Valid types are: gp2, io1, and standard.\",\n                                            \"required\": True\n                                        },\n                                        \"SizeInGB\": {\n                                            \"type\": \"integer\",\n                                            \"description\": \"The EBS volume size, in GB, that is attached to all the instances in the instance group.\",\n                                            \"required\": True\n                                        },\n                                        \"Iops\": {\n                                            \"type\": \"integer\",\n                                            \"description\": \"The IOPS of the EBS volume that is attached to all the instances in the instance group.\",\n                                        },\n                                        \"Throughput\": {\n                                            \"type\": \"integer\",\n                                            \"description\": \"The throughput of the EBS volume that is attached to all the instances in the instance group.\",\n                                        }\n                                    }\n                                },\n                                \"VolumesPerInstance\": {\n                                    \"type\": \"integer\",\n                                    \"description\": \"The number of EBS volumes that will be created and attached to each instance in the instance group.\",\n                                }\n                            }\n                        }\n                    }\n                }\n            },\n            \"AutoScalingPolicy\": {\n                \"type\": \"object\",\n                \"description\": \"Auto Scaling policy that will be associated with the instance group.\",\n                \"properties\": {\n                    \"Constraints\": {\n                        \"type\": \"object\",\n                        \"description\": \"The Constraints that will be associated to an Auto Scaling policy.\",\n                        \"properties\": {\n                            \"MinCapacity\": {\n                                \"type\": \"integer\",\n                                \"description\": \"The minimum value for the instances to scale in\"\n                                               \" to in response to scaling activities.\"\n                            },\n                            \"MaxCapacity\": {\n                                \"type\": \"integer\",\n                                \"description\": \"The maximum value for the instances to scale out to in response\"\n                                               \" to scaling activities\"\n                            }\n                        }\n                    },\n                    \"Rules\": {\n                        \"type\": \"array\",\n                        \"description\": \"The Rules associated to an Auto Scaling policy.\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"Name\": {\n                                    \"type\": \"string\",\n                                    \"description\": \"Name of the Auto Scaling rule.\"\n                                },\n                                \"Description\": {\n                                    \"type\": \"string\",\n                                    \"description\": \"Description of the Auto Scaling rule.\"\n                                },\n                                \"Action\": {\n                                    \"type\": \"object\",\n                                    \"description\": \"The Action associated to an Auto Scaling rule.\",\n                                    \"properties\": {\n                                        \"Market\": {  # Required for Instance Fleets\n                                            \"type\": \"string\",\n                                            \"description\": \"Market type of the Amazon EC2 instances used to create a \"\n                                                           \"cluster node by Auto Scaling action.\",\n                                            \"enum\": [\"ON_DEMAND\", \"SPOT\"]\n                                        },\n                                        \"SimpleScalingPolicyConfiguration\": {\n                                            \"type\": \"object\",\n                                            \"description\": \"The Simple scaling configuration that will be associated\"\n                                                           \"to Auto Scaling action.\",\n                                            \"properties\": {\n                                                \"AdjustmentType\": {\n                                                    \"type\": \"string\",\n                                                    \"description\": \"Specifies how the ScalingAdjustment parameter is \"\n                                                                   \"interpreted.\",\n                                                    \"enum\": [\"CHANGE_IN_CAPACITY\", \"PERCENT_CHANGE_IN_CAPACITY\",\n                                                             \"EXACT_CAPACITY\"]\n                                                },\n                                                \"ScalingAdjustment\": {\n                                                    \"type\": \"integer\",\n                                                    \"description\": \"The amount by which to scale, based on the \"\n                                                                   \"specified adjustment type.\"\n                                                },\n                                                \"CoolDown\": {\n                                                    \"type\": \"integer\",\n                                                    \"description\": \"The amount of time, in seconds, after a scaling \"\n                                                                   \"activity completes and before the next scaling \"\n                                                                   \"activity can start.\"\n                                                }\n                                            }\n                                        }\n                                    }\n                                },\n                                \"Trigger\": {\n                                    \"type\": \"object\",\n                                    \"description\": \"The Trigger associated to an Auto Scaling rule.\",\n                                    \"properties\": {\n                                        \"CloudWatchAlarmDefinition\": {\n                                            \"type\": \"object\",\n                                            \"description\": \"The Alarm to be registered with CloudWatch, to trigger\"\n                                                           \" scaling activities.\",\n                                            \"properties\": {\n                                                \"ComparisonOperator\": {\n                                                    \"type\": \"string\",\n                                                    \"description\": \"The arithmetic operation to use when comparing the\"\n                                                                   \" specified Statistic and Threshold.\"\n                                                },\n                                                \"EvaluationPeriods\": {\n                                                    \"type\": \"integer\",\n                                                    \"description\": \"The number of periods over which data is compared\"\n                                                                   \" to the specified threshold.\"\n                                                },\n                                                \"MetricName\": {\n                                                    \"type\": \"string\",\n                                                    \"description\": \"The name for the alarm's associated metric.\"\n                                                },\n                                                \"Namespace\": {\n                                                    \"type\": \"string\",\n                                                    \"description\": \"The namespace for the alarm's associated metric.\"\n                                                },\n                                                \"Period\": {\n                                                    \"type\": \"integer\",\n                                                    \"description\": \"The period in seconds over which the specified \"\n                                                                   \"statistic is applied.\"\n                                                },\n                                                \"Statistic\": {\n                                                    \"type\": \"string\",\n                                                    \"description\": \"The statistic to apply to the alarm's associated \"\n                                                                   \"metric.\"\n                                                },\n                                                \"Threshold\": {\n                                                    \"type\": \"double\",\n                                                    \"description\": \"The value against which the specified statistic is \"\n                                                                   \"compared.\"\n                                                },\n                                                \"Unit\": {\n                                                    \"type\": \"string\",\n                                                    \"description\": \"The statistic's unit of measure.\"\n                                                },\n                                                \"Dimensions\": {\n                                                    \"type\": \"array\",\n                                                    \"description\": \"The dimensions for the alarm's associated metric.\",\n                                                    \"items\": {\n                                                        \"type\": \"object\",\n                                                        \"properties\": {\n                                                            \"Key\": {\n                                                                \"type\": \"string\",\n                                                                \"description\": \"Dimension Key.\"\n                                                            },\n                                                            \"Value\": {\n                                                                \"type\": \"string\",\n                                                                \"description\": \"Dimension Value.\"\n                                                            }\n                                                        }\n                                                    }\n                                                }\n                                            }\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n            },\n            \"Configurations\": OUTER_CONFIGURATIONS_SCHEMA\n        }\n    }\n}\n\nINSTANCE_FLEETS_SCHEMA = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"Name\": {\n                \"type\": \"string\",\n                \"description\": \"Friendly name given to the instance fleet.\"\n            },\n            \"InstanceFleetType\": {\n                \"type\": \"string\",\n                \"description\": \"The type of the instance fleet in the cluster.\",\n                \"enum\": [\"MASTER\", \"CORE\", \"TASK\"],\n                \"required\": True\n            },\n            \"TargetOnDemandCapacity\": {\n                \"type\": \"integer\",\n                \"description\": \"Target on-demand capacity for the instance fleet.\"\n            },\n            \"TargetSpotCapacity\": {\n                \"type\": \"integer\",\n                \"description\": \"Target spot capacity for the instance fleet.\"\n            },\n            \"InstanceTypeConfigs\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"InstanceType\": {\n                            \"type\": \"string\",\n                            \"description\": \"The Amazon EC2 instance type for the instance fleet.\",\n                            \"required\": True\n                        },\n                        \"WeightedCapacity\": {\n                            \"type\": \"integer\",\n                            \"description\": \"The weight assigned to an instance type, which will impact the overall fulfillment of the capacity.\"\n                        },\n                        \"BidPrice\": {\n                            \"type\": \"string\",\n                            \"description\": \"Bid price for each Amazon EC2 instance in the \"\n                                \"instance fleet when launching nodes as Spot Instances, \"\n                                \"expressed in USD.\"\n                        },\n                        \"BidPriceAsPercentageOfOnDemandPrice\": {\n                            \"type\": \"double\",\n                            \"description\": \"Bid price as percentage of on-demand price.\"\n                        },\n                        \"CustomAmiId\": {\n                            \"type\": \"string\",\n                            \"description\": \"The AMI ID of a custom AMI to use when Amazon EMR provisions EC2 instances.\"\n                        },\n                        \"EbsConfiguration\": {\n                            \"type\": \"object\",\n                            \"description\": \"EBS configuration that is associated with the instance group.\",\n                            \"properties\": {\n                                \"EbsOptimized\": {\n                                    \"type\": \"boolean\",\n                                    \"description\": \"Boolean flag used to tag EBS-optimized instances.\",\n                                },\n                                \"EbsBlockDeviceConfigs\": {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                        \"type\": \"object\",\n                                        \"properties\": {\n                                            \"VolumeSpecification\" : {\n                                                \"type\": \"object\",\n                                                \"description\": \"The EBS volume specification that is created \"\n                                                    \"and attached to each instance in the instance group.\",\n                                                \"properties\": {\n                                                    \"VolumeType\": {\n                                                        \"type\": \"string\",\n                                                        \"description\": \"The EBS volume type that is attached to all \"\n                                                            \"the instances in the instance group. Valid types are: \"\n                                                            \"gp2, io1, and standard.\",\n                                                            \"required\": True\n                                                    },\n                                                    \"SizeInGB\": {\n                                                        \"type\": \"integer\",\n                                                        \"description\": \"The EBS volume size, in GB, that is attached \"\n                                                            \"to all the instances in the instance group.\",\n                                                        \"required\": True\n                                                    },\n                                                    \"Iops\": {\n                                                        \"type\": \"integer\",\n                                                        \"description\": \"The IOPS of the EBS volume that is attached to \"\n                                                            \"all the instances in the instance group.\",\n                                                    },\n                                                    \"Throughput\": {\n                                                         \"type\": \"integer\",\n                                                         \"description\": \"The throughput of the EBS volume that is attached to \"\n                                                             \"all the instances in the instance group.\",\n                                                    }\n                                                }\n                                            },\n                                            \"VolumesPerInstance\": {\n                                                \"type\": \"integer\",\n                                                \"description\": \"The number of EBS volumes that will be created and \"\n                                                    \"attached to each instance in the instance group.\",\n                                            }\n                                        }\n                                    }\n                                }\n                            }\n                        },\n                        \"Configurations\": OUTER_CONFIGURATIONS_SCHEMA\n                    }\n                }\n            },\n            \"LaunchSpecifications\": {\n                \"type\": \"object\",\n                \"properties\" : {\n                    \"OnDemandSpecification\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"AllocationStrategy\": {\n                                \"type\": \"string\",\n                                \"description\": \"The strategy to use in launching On-Demand instance fleets.\",\n                                \"enum\": [\"lowest-price\"]\n                            },\n                            \"CapacityReservationOptions\": {\n                                \"type\": \"object\",\n                                \"properties\" : {\n                                    \"UsageStrategy\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"The strategy of whether to use unused Capacity Reservations for fulfilling On-Demand capacity.\",\n                                        \"enum\": [\"use-capacity-reservations-first\"]\n                                    },\n                                    \"CapacityReservationPreference\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"The preference of the instance's Capacity Reservation.\",\n                                        \"enum\": [\n                                            \"open\",\n                                            \"none\"\n                                        ]\n                                    },\n                                    \"CapacityReservationResourceGroupArn\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"The ARN of the Capacity Reservation resource group in which to run the instance.\"\n                                    }\n                                }\n                            }\n                        }\n                    },\n                    \"SpotSpecification\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"TimeoutDurationMinutes\": {\n                                \"type\": \"integer\",\n                                \"description\": \"The time, in minutes, after which the action specified in TimeoutAction field will be performed if requested resources are unavailable.\"\n                            },\n                            \"TimeoutAction\": {\n                                \"type\": \"string\",\n                                \"description\": \"The action that is performed after TimeoutDurationMinutes.\",\n                                \"enum\": [\n                                    \"TERMINATE_CLUSTER\",\n                                    \"SWITCH_TO_ONDEMAND\"\n                                ]\n                            },\n                            \"BlockDurationMinutes\": {\n                                \"type\": \"integer\",\n                                \"description\": \"Block duration in minutes.\"\n                            },\n                            \"AllocationStrategy\": {\n                                \"type\": \"string\",\n                                \"description\": \"The strategy to use in launching Spot instance fleets.\",\n                                \"enum\": [\"capacity-optimized\", \"price-capacity-optimized\", \"lowest-price\", \"diversified\"]\n                            }\n                        }\n                    }\n                }\n            },\n            \"ResizeSpecifications\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"SpotResizeSpecification\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"TimeoutDurationMinutes\": {\n                                \"type\" : \"integer\",\n                                \"description\": \"The time, in minutes, after which the resize will be stopped if requested resources are unavailable.\"\n                            }\n                        }\n                    },\n                    \"OnDemandResizeSpecification\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"TimeoutDurationMinutes\": {\n                                \"type\" : \"integer\",\n                                \"description\": \"The time, in minutes, after which the resize will be stopped if requested resources are unavailable.\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nEC2_ATTRIBUTES_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"KeyName\": {\n            \"type\": \"string\",\n            \"description\":\n                \"The name of the Amazon EC2 key pair that can \"\n                \"be used to ssh to the master node as the user 'hadoop'.\"\n        },\n        \"SubnetId\": {\n            \"type\": \"string\",\n            \"description\":\n                \"To launch the cluster in Amazon \"\n                \"Virtual Private Cloud (Amazon VPC), set this parameter to \"\n                \"the identifier of the Amazon VPC subnet where you want \"\n                \"the cluster to launch. If you do not specify this value, \"\n                \"the cluster is launched in the normal Amazon Web Services \"\n                \"cloud, outside of an Amazon VPC. \"\n        },\n        \"SubnetIds\": {\n            \"type\": \"array\",\n            \"description\":\n                \"List of SubnetIds.\",\n            \"items\": {\n                \"type\": \"string\"\n            }\n        },\n        \"AvailabilityZone\": {\n            \"type\": \"string\",\n            \"description\": \"The Availability Zone the cluster will run in.\"\n        },\n        \"AvailabilityZones\": {\n            \"type\": \"array\",\n            \"description\": \"List of AvailabilityZones.\",\n            \"items\": {\n                \"type\": \"string\"\n            }\n        },\n        \"InstanceProfile\": {\n            \"type\": \"string\",\n            \"description\":\n                \"An IAM role for the cluster. The EC2 instances of the cluster\"\n                \" assume this role. The default role is \" +\n                EC2_ROLE_NAME + \". In order to use the default\"\n                \" role, you must have already created it using the \"\n                \"<code>create-default-roles</code> command. \"\n        },\n        \"EmrManagedMasterSecurityGroup\": {\n            \"type\": \"string\",\n            \"description\": helptext.EMR_MANAGED_MASTER_SECURITY_GROUP\n        },\n        \"EmrManagedSlaveSecurityGroup\": {\n            \"type\": \"string\",\n            \"description\": helptext.EMR_MANAGED_SLAVE_SECURITY_GROUP\n        },\n        \"ServiceAccessSecurityGroup\": {\n            \"type\": \"string\",\n            \"description\": helptext.SERVICE_ACCESS_SECURITY_GROUP\n        },\n        \"AdditionalMasterSecurityGroups\": {\n            \"type\": \"array\",\n            \"description\": helptext.ADDITIONAL_MASTER_SECURITY_GROUPS,\n            \"items\": {\n                \"type\": \"string\"\n            }\n        },\n        \"AdditionalSlaveSecurityGroups\": {\n            \"type\": \"array\",\n            \"description\": helptext.ADDITIONAL_SLAVE_SECURITY_GROUPS,\n            \"items\": {\n                \"type\": \"string\"\n            }\n        }\n    }\n}\n\n\nAPPLICATIONS_SCHEMA = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"Name\": {\n                \"type\": \"string\",\n                \"description\": \"Application name.\",\n                \"enum\": [\"MapR\", \"HUE\", \"HIVE\", \"PIG\", \"HBASE\",\n                         \"IMPALA\", \"GANGLIA\", \"HADOOP\", \"SPARK\"],\n                \"required\": True\n            },\n            \"Args\": {\n                \"type\": \"array\",\n                \"description\":\n                    \"A list of arguments to pass to the application.\",\n                \"items\": {\n                    \"type\": \"string\"\n                }\n            }\n        }\n    }\n}\n\nBOOTSTRAP_ACTIONS_SCHEMA = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"Name\": {\n                \"type\": \"string\",\n                \"default\": \"Bootstrap Action\"\n            },\n            \"Path\": {\n                \"type\": \"string\",\n                \"description\":\n                    \"Location of the script to run during a bootstrap action. \"\n                    \"Can be either a location in Amazon S3 or \"\n                    \"on a local file system.\",\n                \"required\": True\n            },\n            \"Args\": {\n                \"type\": \"array\",\n                \"description\":\n                    \"A list of command line arguments to pass to \"\n                    \"the bootstrap action script\",\n                \"items\": {\n                    \"type\": \"string\"\n                }\n            }\n        }\n    }\n}\n\n\nSTEPS_SCHEMA = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"Type\": {\n                \"type\": \"string\",\n                \"description\":\n                    \"The type of a step to be added to the cluster.\",\n                \"default\": \"custom_jar\",\n                \"enum\": [\"CUSTOM_JAR\", \"STREAMING\", \"HIVE\", \"PIG\", \"IMPALA\"],\n            },\n            \"Name\": {\n                \"type\": \"string\",\n                \"description\": \"The name of the step. \",\n            },\n            \"ActionOnFailure\": {\n                \"type\": \"string\",\n                \"description\": \"The action to take if the cluster step fails.\",\n                \"enum\": [\"TERMINATE_CLUSTER\", \"CANCEL_AND_WAIT\", \"CONTINUE\"],\n                \"default\": \"CONTINUE\"\n            },\n            \"Jar\": {\n                \"type\": \"string\",\n                \"description\": \"A path to a JAR file run during the step.\",\n            },\n            \"Args\": {\n                \"type\": \"array\",\n                \"description\":\n                    \"A list of command line arguments to pass to the step.\",\n                \"items\": {\n                        \"type\": \"string\"\n                }\n            },\n            \"MainClass\": {\n                \"type\": \"string\",\n                \"description\":\n                    \"The name of the main class in the specified \"\n                    \"Java file. If not specified, the JAR file should \"\n                    \"specify a Main-Class in its manifest file.\"\n            },\n            \"Properties\": {\n                \"type\": \"string\",\n                \"description\":\n                    \"A list of Java properties that are set when the step \"\n                    \"runs. You can use these properties to pass key value \"\n                    \"pairs to your main function.\"\n            }\n        }\n    }\n}\n\nHBASE_RESTORE_FROM_BACKUP_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"Dir\": {\n            \"type\": \"string\",\n            \"description\": helptext.HBASE_BACKUP_DIR\n        },\n        \"BackupVersion\": {\n            \"type\": \"string\",\n            \"description\": helptext.HBASE_BACKUP_VERSION\n        }\n    }\n}\n\nEMR_FS_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"Consistent\": {\n            \"type\": \"boolean\",\n            \"description\": \"Enable EMRFS consistent view.\"\n        },\n        \"SSE\": {\n            \"type\": \"boolean\",\n            \"description\": \"Enable Amazon S3 server-side encryption on files \"\n                           \"written to S3 by EMRFS.\"\n        },\n        \"RetryCount\": {\n            \"type\": \"integer\",\n            \"description\":\n                \"The maximum number of times to retry upon S3 inconsistency.\"\n        },\n        \"RetryPeriod\": {\n            \"type\": \"integer\",\n            \"description\": \"The amount of time (in seconds) until the first \"\n                           \"retry. Subsequent retries use an exponential \"\n                           \"back-off.\"\n        },\n        \"Args\": {\n            \"type\": \"array\",\n            \"description\": \"A list of arguments to pass for additional \"\n                           \"EMRFS configuration.\",\n            \"items\": {\n                \"type\": \"string\"\n            }\n        },\n        \"Encryption\": {\n            \"type\": \"string\",\n            \"description\": \"EMRFS encryption type.\",\n            \"enum\": [\"SERVERSIDE\", \"CLIENTSIDE\"]\n        },\n        \"ProviderType\": {\n            \"type\": \"string\",\n            \"description\": \"EMRFS client-side encryption provider type.\",\n            \"enum\": [\"KMS\", \"CUSTOM\"]\n        },\n        \"KMSKeyId\": {\n            \"type\": \"string\",\n            \"description\": \"AWS KMS's customer master key identifier\",\n        },\n        \"CustomProviderLocation\": {\n            \"type\": \"string\",\n            \"description\": \"Custom encryption provider JAR location.\"\n        },\n        \"CustomProviderClass\": {\n            \"type\": \"string\",\n            \"description\": \"Custom encryption provider full class name.\"\n        }\n    }\n}\n\nTAGS_SCHEMA = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"string\"\n    }\n}\n\nKERBEROS_ATTRIBUTES_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"Realm\": {\n            \"type\": \"string\",\n            \"description\": \"The name of Kerberos realm.\"\n        },\n        \"KdcAdminPassword\": {\n            \"type\": \"string\",\n            \"description\": \"The password of Kerberos administrator.\"\n        },\n        \"CrossRealmTrustPrincipalPassword\": {\n            \"type\": \"string\",\n            \"description\": \"The password to establish cross-realm trusts.\"\n        },\n        \"ADDomainJoinUser\": {\n            \"type\": \"string\",\n            \"description\": \"The name of the user with privileges to join instances to Active Directory.\"\n        },\n        \"ADDomainJoinPassword\": {\n            \"type\": \"string\",\n            \"description\": \"The password of the user with privileges to join instances to Active Directory.\"\n        }\n    }\n}\n\nMANAGED_SCALING_POLICY_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"ComputeLimits\": {\n            \"type\": \"object\",\n            \"description\": \n                \"The EC2 unit limits for a managed scaling policy. \"\n                \"The managed scaling activity of a cluster is not allowed to go above \"\n                \"or below these limits. The limits apply to CORE and TASK groups \"\n                \"and exclude the capacity of the MASTER group.\",\n            \"properties\": {\n               \"MinimumCapacityUnits\": {\n                  \"type\": \"integer\",\n                  \"description\": \n                      \"The lower boundary of EC2 units. It is measured through \"\n                      \"VCPU cores or instances for instance groups and measured \"\n                      \"through units for instance fleets. Managed scaling \"\n                      \"activities are not allowed beyond this boundary.\",\n                  \"required\": True\n               },\n               \"MaximumCapacityUnits\": {\n                  \"type\": \"integer\",\n                  \"description\": \n                      \"The upper boundary of EC2 units. It is measured through \"\n                      \"VCPU cores or instances for instance groups and measured \"\n                      \"through units for instance fleets. Managed scaling \"\n                      \"activities are not allowed beyond this boundary.\",\n                  \"required\": True\n               },\n               \"MaximumOnDemandCapacityUnits\": {\n                  \"type\": \"integer\",\n                  \"description\": \n                      \"The upper boundary of on-demand EC2 units. It is measured through \"\n                      \"VCPU cores or instances for instance groups and measured \"\n                      \"through units for instance fleets. The on-demand units are not \"\n                      \"allowed to scale beyond this boundary. \"\n                      \"This value must be lower than MaximumCapacityUnits.\"\n               },\n               \"UnitType\": {\n                  \"type\": \"string\",\n                  \"description\": \"The unit type used for specifying a managed scaling policy.\",\n                  \"enum\": [\"VCPU\", \"Instances\", \"InstanceFleetUnits\"],\n                  \"required\": True\n               },\n               \"MaximumCoreCapacityUnits\": {\n                  \"type\": \"integer\",\n                  \"description\":\n                      \"The upper boundary of EC2 units for core node type in a cluster. \"\n                      \"It is measured through VCPU cores or instances for instance groups \"\n                      \"and measured through units for instance fleets. \"\n                      \"The core units are not allowed to scale beyond this boundary. \"\n                      \"The parameter is used to split capacity allocation between core and task nodes.\"\n               }\n            } \n        }\n    }\n}\n\nPLACEMENT_GROUP_CONFIGS_SCHEMA = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"InstanceRole\": {\n                \"type\": \"string\",\n                \"description\": \"Role of the instance in the cluster.\",\n                \"enum\": [\"MASTER\", \"CORE\", \"TASK\"],\n                \"required\": True\n            },\n            \"PlacementStrategy\": {\n                \"type\": \"string\",\n                \"description\": \"EC2 Placement Group strategy associated \"\n                               \"with instance role.\",\n                \"enum\": [\"SPREAD\", \"PARTITION\", \"CLUSTER\", \"NONE\"]\n            }\n        }\n    }\n}\n\nAUTO_TERMINATION_POLICY_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\":  {\n        \"IdleTimeout\": {\n            \"type\": \"long\",\n            \"description\":\n                \"Specifies the amount of idle time in seconds after which the cluster automatically terminates. \"\n                \"You can specify a minimum of 60 seconds and a maximum of 604800 seconds (seven days).\",\n        }\n    }\n}\n", "awscli/customizations/emr/createdefaultroles.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\nimport re\nimport botocore.exceptions\nimport botocore.session\nfrom botocore import xform_name\n\nfrom awscli.customizations.utils import get_policy_arn_suffix\nfrom awscli.customizations.emr import configutils\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import exceptions\nfrom awscli.customizations.emr.command import Command\nfrom awscli.customizations.emr.constants import EC2\nfrom awscli.customizations.emr.constants import EC2_ROLE_NAME\nfrom awscli.customizations.emr.constants import EC2_SERVICE_PRINCIPAL\nfrom awscli.customizations.emr.constants import ROLE_ARN_PATTERN\nfrom awscli.customizations.emr.constants import EMR\nfrom awscli.customizations.emr.constants import EMR_ROLE_NAME\nfrom awscli.customizations.emr.constants import EMR_AUTOSCALING_ROLE_NAME\nfrom awscli.customizations.emr.constants import APPLICATION_AUTOSCALING\nfrom awscli.customizations.emr.constants import EC2_ROLE_POLICY_NAME\nfrom awscli.customizations.emr.constants import EMR_ROLE_POLICY_NAME\nfrom awscli.customizations.emr.constants \\\n    import EMR_AUTOSCALING_ROLE_POLICY_NAME\nfrom awscli.customizations.emr.constants import EMR_AUTOSCALING_SERVICE_NAME\nfrom awscli.customizations.emr.constants \\\n    import EMR_AUTOSCALING_SERVICE_PRINCIPAL\nfrom awscli.customizations.emr.exceptions import ResolveServicePrincipalError\n\n\nLOG = logging.getLogger(__name__)\n\n\ndef assume_role_policy(serviceprincipal):\n    return {\n        \"Version\": \"2008-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"\",\n                \"Effect\": \"Allow\",\n                \"Principal\": {\"Service\": serviceprincipal},\n                \"Action\": \"sts:AssumeRole\"\n            }\n        ]\n    }\n\n\ndef get_role_policy_arn(region, policy_name):\n    region_suffix = get_policy_arn_suffix(region)\n    role_arn = ROLE_ARN_PATTERN.replace(\"{{region_suffix}}\", region_suffix)\n    role_arn = role_arn.replace(\"{{policy_name}}\", policy_name)\n    return role_arn\n\n\ndef get_service_principal(service, endpoint_host, session=None):\n    if service == EC2:\n        return EC2_SERVICE_PRINCIPAL\n\n    suffix, region = _get_suffix_and_region_from_endpoint_host(endpoint_host)\n    if session is None:\n        session = botocore.session.Session()\n\n    if service == EMR_AUTOSCALING_SERVICE_NAME:\n        if region not in session.get_available_regions('emr', 'aws-cn'):\n            return EMR_AUTOSCALING_SERVICE_PRINCIPAL\n\n    return service + '.' + suffix\n\n\ndef _get_suffix_and_region_from_endpoint_host(endpoint_host):\n    suffix_match = _get_regex_match_from_endpoint_host(endpoint_host)\n\n    if suffix_match is not None and suffix_match.lastindex >= 3:\n        suffix = suffix_match.group(3)\n        region = suffix_match.group(2)\n    else:\n        raise ResolveServicePrincipalError\n\n    return suffix, region\n\n\ndef _get_regex_match_from_endpoint_host(endpoint_host):\n    if endpoint_host is None:\n        return None\n    regex_match = re.match(\"(https?://)([^.]+).elasticmapreduce.([^/]*)\",\n                           endpoint_host)\n\n    # Supports 'elasticmapreduce.{region}.' and '{region}.elasticmapreduce.'\n    if regex_match is None:\n        regex_match = re.match(\"(https?://elasticmapreduce).([^.]+).([^/]*)\",\n                               endpoint_host)\n    return regex_match\n\n\nclass CreateDefaultRoles(Command):\n    NAME = \"create-default-roles\"\n    DESCRIPTION = ('Creates the default IAM role ' +\n                   EC2_ROLE_NAME + ' and ' +\n                   EMR_ROLE_NAME + ' which can be used when creating the'\n                   ' cluster using the create-cluster command. The default'\n                   ' roles for EMR use managed policies, which are updated'\n                   ' automatically to support future EMR functionality.\\n'\n                   '\\nIf you do not have a Service Role and Instance Profile '\n                   'variable set for your create-cluster command in the AWS '\n                   'CLI config file, create-default-roles will automatically '\n                   'set the values for these variables with these default '\n                   'roles. If you have already set a value for Service Role '\n                   'or Instance Profile, create-default-roles will not '\n                   'automatically set the defaults for these variables in the '\n                   'AWS CLI config file. You can view settings for variables '\n                   'in the config file using the \"aws configure get\" command.'\n                   '\\n')\n    ARG_TABLE = [\n        {'name': 'iam-endpoint',\n         'no_paramfile': True,\n         'help_text': '<p>The IAM endpoint to call for creating the roles.'\n                      ' This is optional and should only be specified when a'\n                      ' custom endpoint should be called for IAM operations'\n                      '.</p>'}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n\n        self.iam_endpoint_url = parsed_args.iam_endpoint\n\n        self._check_for_iam_endpoint(self.region, self.iam_endpoint_url)\n        self.emr_endpoint_url = \\\n            self._session.create_client(\n                'emr',\n                region_name=self.region,\n                endpoint_url=parsed_globals.endpoint_url,\n                verify=parsed_globals.verify_ssl).meta.endpoint_url\n\n        LOG.debug('elasticmapreduce endpoint used for resolving'\n                  ' service principal: ' + self.emr_endpoint_url)\n\n        # Create default EC2 Role for EMR if it does not exist.\n        ec2_result, ec2_policy = self._create_role_if_not_exists(parsed_globals, EC2_ROLE_NAME,\n                                                                 EC2_ROLE_POLICY_NAME, [EC2])\n\n        # Create default EC2 Instance Profile for EMR if it does not exist.\n        instance_profile_name = EC2_ROLE_NAME\n        if self.check_if_instance_profile_exists(instance_profile_name,\n                                                 parsed_globals):\n            LOG.debug('Instance Profile ' + instance_profile_name + ' exists.')\n        else:\n            LOG.debug('Instance Profile ' + instance_profile_name +\n                      'does not exist. Creating default Instance Profile ' +\n                      instance_profile_name)\n            self._create_instance_profile_with_role(instance_profile_name,\n                                                    instance_profile_name,\n                                                    parsed_globals)\n\n        # Create default EMR Role if it does not exist.\n        emr_result, emr_policy = self._create_role_if_not_exists(parsed_globals, EMR_ROLE_NAME,\n                                                                 EMR_ROLE_POLICY_NAME, [EMR])\n\n        # Create default EMR AutoScaling Role if it does not exist.\n        emr_autoscaling_result, emr_autoscaling_policy = \\\n            self._create_role_if_not_exists(parsed_globals, EMR_AUTOSCALING_ROLE_NAME,\n                                            EMR_AUTOSCALING_ROLE_POLICY_NAME, [EMR, APPLICATION_AUTOSCALING])\n\n        configutils.update_roles(self._session)\n        emrutils.display_response(\n            self._session,\n            'create_role',\n            self._construct_result(ec2_result, ec2_policy,\n                                   emr_result, emr_policy,\n                                   emr_autoscaling_result, emr_autoscaling_policy),\n            parsed_globals)\n\n        return 0\n\n    def _create_role_if_not_exists(self, parsed_globals, role_name, policy_name, service_names):\n        result = None\n        policy = None\n\n        if self.check_if_role_exists(role_name, parsed_globals):\n            LOG.debug('Role ' + role_name + ' exists.')\n        else:\n            LOG.debug('Role ' + role_name + ' does not exist.'\n                      ' Creating default role: ' + role_name)\n            role_arn = get_role_policy_arn(self.region, policy_name)\n            result = self._create_role_with_role_policy(\n                    role_name, service_names, role_arn, parsed_globals)\n            policy = self._get_role_policy(role_arn, parsed_globals)\n        return result, policy\n\n    def _check_for_iam_endpoint(self, region, iam_endpoint):\n        try:\n            self._session.create_client('emr', region)\n        except botocore.exceptions.UnknownEndpointError:\n            if iam_endpoint is None:\n                raise exceptions.UnknownIamEndpointError(region=region)\n\n    def _construct_result(self, ec2_response, ec2_policy,\n                          emr_response, emr_policy,\n                          emr_autoscaling_response, emr_autoscaling_policy):\n        result = []\n        self._construct_role_and_role_policy_structure(\n            result, ec2_response, ec2_policy)\n        self._construct_role_and_role_policy_structure(\n            result, emr_response, emr_policy)\n        self._construct_role_and_role_policy_structure(\n            result, emr_autoscaling_response, emr_autoscaling_policy)\n        return result\n\n    def _construct_role_and_role_policy_structure(\n            self, list, response, policy):\n        if response is not None and response['Role'] is not None:\n            list.append({'Role': response['Role'], 'RolePolicy': policy})\n            return list\n\n    def check_if_role_exists(self, role_name, parsed_globals):\n        parameters = {'RoleName': role_name}\n\n        try:\n            self._call_iam_operation('GetRole', parameters, parsed_globals)\n        except botocore.exceptions.ClientError as e:\n            role_not_found_code = \"NoSuchEntity\"\n            error_code = e.response.get('Error', {}).get('Code', '')\n            if role_not_found_code == error_code:\n                # No role error.\n                return False\n            else:\n                # Some other error. raise.\n                raise e\n\n        return True\n\n    def check_if_instance_profile_exists(self, instance_profile_name,\n                                         parsed_globals):\n        parameters = {'InstanceProfileName': instance_profile_name}\n        try:\n            self._call_iam_operation('GetInstanceProfile', parameters,\n                                     parsed_globals)\n        except botocore.exceptions.ClientError as e:\n            profile_not_found_code = 'NoSuchEntity'\n            error_code = e.response.get('Error', {}).get('Code')\n            if profile_not_found_code == error_code:\n                # No instance profile error.\n                return False\n            else:\n                # Some other error. raise.\n                raise e\n\n        return True\n\n    def _get_role_policy(self, arn, parsed_globals):\n        parameters = {}\n        parameters['PolicyArn'] = arn\n        policy_details = self._call_iam_operation('GetPolicy', parameters,\n                                                  parsed_globals)\n        parameters[\"VersionId\"] = policy_details[\"Policy\"][\"DefaultVersionId\"]\n        policy_version_details = self._call_iam_operation('GetPolicyVersion',\n                                                          parameters,\n                                                          parsed_globals)\n        return policy_version_details[\"PolicyVersion\"][\"Document\"]\n\n    def _create_role_with_role_policy(\n            self, role_name, service_names, role_arn, parsed_globals):\n\n        if len(service_names) == 1:\n            service_principal = get_service_principal(\n                service_names[0], self.emr_endpoint_url, self._session)\n        else:\n            service_principal = []\n            for service in service_names:\n                service_principal.append(get_service_principal(\n                    service, self.emr_endpoint_url, self._session))\n\n        LOG.debug(f'Adding service principal(s) to trust policy: {service_principal}')\n\n        parameters = {'RoleName': role_name}\n        _assume_role_policy = \\\n            emrutils.dict_to_string(assume_role_policy(service_principal))\n        parameters['AssumeRolePolicyDocument'] = _assume_role_policy\n        create_role_response = self._call_iam_operation('CreateRole',\n                                                        parameters,\n                                                        parsed_globals)\n\n        parameters = {}\n        parameters['PolicyArn'] = role_arn\n        parameters['RoleName'] = role_name\n        self._call_iam_operation('AttachRolePolicy',\n                                 parameters, parsed_globals)\n\n        return create_role_response\n\n    def _create_instance_profile_with_role(self, instance_profile_name,\n                                           role_name, parsed_globals):\n        # Creating an Instance Profile\n        parameters = {'InstanceProfileName': instance_profile_name}\n        self._call_iam_operation('CreateInstanceProfile', parameters,\n                                 parsed_globals)\n        # Adding the role to the Instance Profile\n        parameters = {}\n        parameters['InstanceProfileName'] = instance_profile_name\n        parameters['RoleName'] = role_name\n        self._call_iam_operation('AddRoleToInstanceProfile', parameters,\n                                 parsed_globals)\n\n    def _call_iam_operation(self, operation_name, parameters, parsed_globals):\n        client = self._session.create_client(\n            'iam', region_name=self.region, endpoint_url=self.iam_endpoint_url,\n            verify=parsed_globals.verify_ssl)\n        return getattr(client, xform_name(operation_name))(**parameters)\n", "awscli/customizations/emr/applicationutils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import exceptions\n\n\ndef build_applications(region,\n                       parsed_applications, ami_version=None):\n    app_list = []\n    step_list = []\n    ba_list = []\n\n    for app_config in parsed_applications:\n        app_name = app_config['Name'].lower()\n\n        if app_name == constants.HIVE:\n            hive_version = constants.LATEST\n            step_list.append(\n                _build_install_hive_step(region=region))\n            args = app_config.get('Args')\n            if args is not None:\n                hive_site_path = _find_matching_arg(\n                    key=constants.HIVE_SITE_KEY, args_list=args)\n                if hive_site_path is not None:\n                    step_list.append(\n                        _build_install_hive_site_step(\n                            region=region,\n                            hive_site_path=hive_site_path))\n        elif app_name == constants.PIG:\n            pig_version = constants.LATEST\n            step_list.append(\n                _build_pig_install_step(\n                    region=region))\n        elif app_name == constants.GANGLIA:\n            ba_list.append(\n                _build_ganglia_install_bootstrap_action(\n                    region=region))\n        elif app_name == constants.HBASE:\n            ba_list.append(\n                _build_hbase_install_bootstrap_action(\n                    region=region))\n            if ami_version >= '3.0':\n                step_list.append(\n                    _build_hbase_install_step(\n                        constants.HBASE_PATH_HADOOP2_INSTALL_JAR))\n            elif ami_version >= '2.1':\n                step_list.append(\n                    _build_hbase_install_step(\n                        constants.HBASE_PATH_HADOOP1_INSTALL_JAR))\n            else:\n                raise ValueError('aws: error: AMI version ' + ami_version +\n                                 'is not compatible with HBase.')\n        elif app_name == constants.IMPALA:\n            ba_list.append(\n                _build_impala_install_bootstrap_action(\n                    region=region,\n                    args=app_config.get('Args')))\n        else:\n            app_list.append(\n                _build_supported_product(\n                    app_config['Name'], app_config.get('Args')))\n\n    return app_list, ba_list, step_list\n\n\ndef _build_supported_product(name, args):\n    if args is None:\n        args = []\n    config = {'Name': name.lower(), 'Args': args}\n    return config\n\n\ndef _build_ganglia_install_bootstrap_action(region):\n    return emrutils.build_bootstrap_action(\n        name=constants.INSTALL_GANGLIA_NAME,\n        path=emrutils.build_s3_link(\n            relative_path=constants.GANGLIA_INSTALL_BA_PATH,\n            region=region))\n\n\ndef _build_hbase_install_bootstrap_action(region):\n    return emrutils.build_bootstrap_action(\n        name=constants.INSTALL_HBASE_NAME,\n        path=emrutils.build_s3_link(\n            relative_path=constants.HBASE_INSTALL_BA_PATH,\n            region=region))\n\n\ndef _build_hbase_install_step(jar):\n    return emrutils.build_step(\n        jar=jar,\n        name=constants.START_HBASE_NAME,\n        action_on_failure=constants.TERMINATE_CLUSTER,\n        args=constants.HBASE_INSTALL_ARG)\n\n\ndef _build_impala_install_bootstrap_action(region, args=None):\n    args_list = [\n        constants.BASE_PATH_ARG,\n        emrutils.build_s3_link(region=region),\n        constants.IMPALA_VERSION,\n        constants.LATEST]\n    if args is not None:\n        args_list.append(constants.IMPALA_CONF)\n        args_list.append(','.join(args))\n    return emrutils.build_bootstrap_action(\n        name=constants.INSTALL_IMPALA_NAME,\n        path=emrutils.build_s3_link(\n            relative_path=constants.IMPALA_INSTALL_PATH,\n            region=region),\n        args=args_list)\n\n\ndef _build_install_hive_step(region,\n                             action_on_failure=constants.TERMINATE_CLUSTER):\n    step_args = [\n        emrutils.build_s3_link(constants.HIVE_SCRIPT_PATH, region),\n        constants.INSTALL_HIVE_ARG,\n        constants.BASE_PATH_ARG,\n        emrutils.build_s3_link(constants.HIVE_BASE_PATH, region),\n        constants.HIVE_VERSIONS,\n        constants.LATEST]\n    step = emrutils.build_step(\n        name=constants.INSTALL_HIVE_NAME,\n        action_on_failure=action_on_failure,\n        jar=emrutils.build_s3_link(constants.SCRIPT_RUNNER_PATH, region),\n        args=step_args)\n    return step\n\n\ndef _build_install_hive_site_step(region, hive_site_path,\n                                  action_on_failure=constants.CANCEL_AND_WAIT):\n    step_args = [\n        emrutils.build_s3_link(constants.HIVE_SCRIPT_PATH, region),\n        constants.BASE_PATH_ARG,\n        emrutils.build_s3_link(constants.HIVE_BASE_PATH),\n        constants.INSTALL_HIVE_SITE_ARG,\n        hive_site_path,\n        constants.HIVE_VERSIONS,\n        constants.LATEST]\n    step = emrutils.build_step(\n        name=constants.INSTALL_HIVE_SITE_NAME,\n        action_on_failure=action_on_failure,\n        jar=emrutils.build_s3_link(constants.SCRIPT_RUNNER_PATH, region),\n        args=step_args)\n    return step\n\n\ndef _build_pig_install_step(region,\n                            action_on_failure=constants.TERMINATE_CLUSTER):\n    step_args = [\n        emrutils.build_s3_link(constants.PIG_SCRIPT_PATH, region),\n        constants.INSTALL_PIG_ARG,\n        constants.BASE_PATH_ARG,\n        emrutils.build_s3_link(constants.PIG_BASE_PATH, region),\n        constants.PIG_VERSIONS,\n        constants.LATEST]\n    step = emrutils.build_step(\n        name=constants.INSTALL_PIG_NAME,\n        action_on_failure=action_on_failure,\n        jar=emrutils.build_s3_link(constants.SCRIPT_RUNNER_PATH, region),\n        args=step_args)\n    return step\n\n\ndef _find_matching_arg(key, args_list):\n    for arg in args_list:\n        if key in arg:\n            return arg\n\n    return None\n", "awscli/customizations/emr/hbase.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emr import constants\nfrom awscli.customizations.emr import emrutils\nfrom awscli.customizations.emr import hbaseutils\nfrom awscli.customizations.emr import helptext\nfrom awscli.customizations.emr.command import Command\n\n\nclass RestoreFromHBaseBackup(Command):\n    NAME = 'restore-from-hbase-backup'\n    DESCRIPTION = ('Restores HBase from S3. ' +\n                   helptext.AVAILABLE_ONLY_FOR_AMI_VERSIONS)\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': helptext.CLUSTER_ID},\n        {'name': 'dir', 'required': True,\n         'help_text': helptext.HBASE_BACKUP_DIR},\n        {'name': 'backup-version',\n         'help_text': helptext.HBASE_BACKUP_VERSION}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        steps = []\n        args = hbaseutils.build_hbase_restore_from_backup_args(\n            parsed_args.dir, parsed_args.backup_version)\n\n        step_config = emrutils.build_step(\n            jar=constants.HBASE_JAR_PATH,\n            name=constants.HBASE_RESTORE_STEP_NAME,\n            action_on_failure=constants.CANCEL_AND_WAIT,\n            args=args)\n\n        steps.append(step_config)\n        parameters = {'JobFlowId': parsed_args.cluster_id,\n                      'Steps': steps}\n        emrutils.call_and_display_response(self._session, 'AddJobFlowSteps',\n                                           parameters, parsed_globals)\n        return 0\n\n\nclass ScheduleHBaseBackup(Command):\n    NAME = 'schedule-hbase-backup'\n    DESCRIPTION = ('Adds a step to schedule automated HBase backup. ' +\n                   helptext.AVAILABLE_ONLY_FOR_AMI_VERSIONS)\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': helptext.CLUSTER_ID},\n        {'name': 'type', 'required': True,\n         'help_text': \"<p>Backup type. You can specify 'incremental' or \"\n                      \"'full'.</p>\"},\n        {'name': 'dir', 'required': True,\n         'help_text': helptext.HBASE_BACKUP_DIR},\n        {'name': 'interval', 'required': True,\n         'help_text': '<p>The time between backups.</p>'},\n        {'name': 'unit', 'required': True,\n         'help_text': \"<p>The time unit for backup's time-interval. \"\n                      \"You can specify one of the following values:\"\n                      \" 'minutes', 'hours', or 'days'.</p>\"},\n        {'name': 'start-time',\n         'help_text': '<p>The time of the first backup in ISO format.</p>'\n         ' e.g. 2014-04-21T05:26:10Z. Default is now.'},\n        {'name': 'consistent', 'action': 'store_true',\n         'help_text': '<p>Performs a consistent backup.'\n                      ' Pauses all write operations to the HBase cluster'\n                      ' during the backup process.</p>'}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        steps = []\n        self._check_type(parsed_args.type)\n        self._check_unit(parsed_args.unit)\n        args = self._build_hbase_schedule_backup_args(parsed_args)\n\n        step_config = emrutils.build_step(\n            jar=constants.HBASE_JAR_PATH,\n            name=constants.HBASE_SCHEDULE_BACKUP_STEP_NAME,\n            action_on_failure=constants.CANCEL_AND_WAIT,\n            args=args)\n\n        steps.append(step_config)\n        parameters = {'JobFlowId': parsed_args.cluster_id,\n                      'Steps': steps}\n        emrutils.call_and_display_response(self._session, 'AddJobFlowSteps',\n                                           parameters, parsed_globals)\n        return 0\n\n    def _check_type(self, type):\n        type = type.lower()\n        if type != constants.FULL and type != constants.INCREMENTAL:\n            raise ValueError('aws: error: invalid type. '\n                             'type should be either ' +\n                             constants.FULL + ' or ' + constants.INCREMENTAL +\n                             '.')\n\n    def _check_unit(self, unit):\n        unit = unit.lower()\n        if (unit != constants.MINUTES and\n                unit != constants.HOURS and\n                unit != constants.DAYS):\n            raise ValueError('aws: error: invalid unit. unit should be one of'\n                             ' the following values: ' + constants.MINUTES +\n                             ', ' + constants.HOURS + ' or ' + constants.DAYS +\n                             '.')\n\n    def _build_hbase_schedule_backup_args(self, parsed_args):\n        args = [constants.HBASE_MAIN, constants.HBASE_SCHEDULED_BACKUP,\n                constants.TRUE, constants.HBASE_BACKUP_DIR, parsed_args.dir]\n\n        type = parsed_args.type.lower()\n        unit = parsed_args.unit.lower()\n\n        if parsed_args.consistent is True:\n            args.append(constants.HBASE_BACKUP_CONSISTENT)\n\n        if type == constants.FULL:\n            args.append(constants.HBASE_FULL_BACKUP_INTERVAL)\n        else:\n            args.append(constants.HBASE_INCREMENTAL_BACKUP_INTERVAL)\n\n        args.append(parsed_args.interval)\n\n        if type == constants.FULL:\n            args.append(constants.HBASE_FULL_BACKUP_INTERVAL_UNIT)\n        else:\n            args.append(constants.HBASE_INCREMENTAL_BACKUP_INTERVAL_UNIT)\n\n        args.append(unit)\n        args.append(constants.HBASE_BACKUP_STARTTIME)\n\n        if parsed_args.start_time is not None:\n            args.append(parsed_args.start_time)\n        else:\n            args.append(constants.NOW)\n\n        return args\n\n\nclass CreateHBaseBackup(Command):\n    NAME = 'create-hbase-backup'\n    DESCRIPTION = ('Creates a HBase backup in S3. ' +\n                   helptext.AVAILABLE_ONLY_FOR_AMI_VERSIONS)\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': helptext.CLUSTER_ID},\n        {'name': 'dir', 'required': True,\n         'help_text': helptext.HBASE_BACKUP_DIR},\n        {'name': 'consistent', 'action': 'store_true',\n         'help_text': '<p>Performs a consistent backup. Pauses all write'\n                      ' operations to the HBase cluster during the backup'\n                      ' process.</p>'}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        steps = []\n        args = self._build_hbase_backup_args(parsed_args)\n\n        step_config = emrutils.build_step(\n            jar=constants.HBASE_JAR_PATH,\n            name=constants.HBASE_BACKUP_STEP_NAME,\n            action_on_failure=constants.CANCEL_AND_WAIT,\n            args=args)\n\n        steps.append(step_config)\n        parameters = {'JobFlowId': parsed_args.cluster_id,\n                      'Steps': steps}\n        emrutils.call_and_display_response(self._session, 'AddJobFlowSteps',\n                                           parameters, parsed_globals)\n        return 0\n\n    def _build_hbase_backup_args(self, parsed_args):\n        args = [constants.HBASE_MAIN,\n                constants.HBASE_BACKUP,\n                constants.HBASE_BACKUP_DIR, parsed_args.dir]\n\n        if parsed_args.consistent is True:\n            args.append(constants.HBASE_BACKUP_CONSISTENT)\n\n        return args\n\n\nclass DisableHBaseBackups(Command):\n    NAME = 'disable-hbase-backups'\n    DESCRIPTION = ('Add a step to disable automated HBase backups. ' +\n                   helptext.AVAILABLE_ONLY_FOR_AMI_VERSIONS)\n    ARG_TABLE = [\n        {'name': 'cluster-id', 'required': True,\n         'help_text': helptext.CLUSTER_ID},\n        {'name': 'full', 'action': 'store_true',\n         'help_text': 'Disables full backup.'},\n        {'name': 'incremental', 'action': 'store_true',\n         'help_text': 'Disables incremental backup.'}\n    ]\n\n    def _run_main_command(self, parsed_args, parsed_globals):\n        steps = []\n\n        args = self._build_hbase_disable_backups_args(parsed_args)\n\n        step_config = emrutils.build_step(\n            constants.HBASE_JAR_PATH,\n            constants.HBASE_SCHEDULE_BACKUP_STEP_NAME,\n            constants.CANCEL_AND_WAIT,\n            args)\n\n        steps.append(step_config)\n        parameters = {'JobFlowId': parsed_args.cluster_id,\n                      'Steps': steps}\n        emrutils.call_and_display_response(self._session, 'AddJobFlowSteps',\n                                           parameters, parsed_globals)\n        return 0\n\n    def _build_hbase_disable_backups_args(self, parsed_args):\n        args = [constants.HBASE_MAIN, constants.HBASE_SCHEDULED_BACKUP,\n                constants.FALSE]\n        if parsed_args.full is False and parsed_args.incremental is False:\n            error_message = 'Should specify at least one of --' +\\\n                            constants.FULL + ' and --' +\\\n                            constants.INCREMENTAL + '.'\n            raise ValueError(error_message)\n        if parsed_args.full is True:\n            args.append(constants.HBASE_DISABLE_FULL_BACKUP)\n        if parsed_args.incremental is True:\n            args.append(constants.HBASE_DISABLE_INCREMENTAL_BACKUP)\n\n        return args\n", "awscli/customizations/dlm/iam.py": "import json\n\n\nclass IAM(object):\n\n    def __init__(self, iam_client):\n        self.iam_client = iam_client\n\n    def check_if_role_exists(self, role_name):\n        \"\"\"Method to verify if a particular role exists\"\"\"\n        try:\n            self.iam_client.get_role(RoleName=role_name)\n        except self.iam_client.exceptions.NoSuchEntityException:\n            return False\n        return True\n\n    def check_if_policy_exists(self, policy_arn):\n        \"\"\"Method to verify if a particular policy exists\"\"\"\n        try:\n            self.iam_client.get_policy(PolicyArn=policy_arn)\n        except self.iam_client.exceptions.NoSuchEntityException:\n            return False\n        return True\n\n    def attach_policy_to_role(self, policy_arn, role_name):\n        \"\"\"Method to attach LifecyclePolicy to role specified by role_name\"\"\"\n        return self.iam_client.attach_role_policy(\n            PolicyArn=policy_arn,\n            RoleName=role_name\n        )\n\n    def create_role_with_trust_policy(self, role_name, assume_role_policy):\n        \"\"\"Method to create role with a given role name\n            and assume_role_policy\n        \"\"\"\n        return self.iam_client.create_role(\n            RoleName=role_name,\n            AssumeRolePolicyDocument=json.dumps(assume_role_policy))\n\n    def get_policy(self, arn):\n        \"\"\"Method to get the Policy for a particular ARN\n        This is used to display the policy contents to the user\n        \"\"\"\n        pol_det = self.iam_client.get_policy(PolicyArn=arn)\n        policy_version_details = self.iam_client.get_policy_version(\n            PolicyArn=arn,\n            VersionId=pol_det.get(\"Policy\", {}).get(\"DefaultVersionId\", \"\")\n        )\n        return policy_version_details\\\n            .get(\"PolicyVersion\", {})\\\n            .get(\"Document\", {})\n", "awscli/customizations/dlm/constants.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n# Declare all the constants used by Lifecycle in this file\n\n# Lifecycle role names\nLIFECYCLE_DEFAULT_ROLE_NAME = \"AWSDataLifecycleManagerDefaultRole\"\nLIFECYCLE_DEFAULT_ROLE_NAME_AMI = \\\n    \"AWSDataLifecycleManagerDefaultRoleForAMIManagement\"\n\n# Lifecycle role arn names\nLIFECYCLE_DEFAULT_MANAGED_POLICY_NAME = \"AWSDataLifecycleManagerServiceRole\"\nLIFECYCLE_DEFAULT_MANAGED_POLICY_NAME_AMI = \\\n    \"AWSDataLifecycleManagerServiceRoleForAMIManagement\"\n\nPOLICY_ARN_PATTERN = \"arn:{0}:iam::aws:policy/service-role/{1}\"\n\n# Assume Role Policy definitions for roles\nLIFECYCLE_DEFAULT_ROLE_ASSUME_POLICY = {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"Service\": \"dlm.amazonaws.com\"},\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n\nRESOURCE_TYPE_SNAPSHOT = \"snapshot\"\nRESOURCE_TYPE_IMAGE = \"image\"\n\nRESOURCES = {\n    RESOURCE_TYPE_SNAPSHOT: {\n        'default_role_name': LIFECYCLE_DEFAULT_ROLE_NAME,\n        'default_policy_name': LIFECYCLE_DEFAULT_MANAGED_POLICY_NAME\n    },\n    RESOURCE_TYPE_IMAGE: {\n        'default_role_name': LIFECYCLE_DEFAULT_ROLE_NAME_AMI,\n        'default_policy_name': LIFECYCLE_DEFAULT_MANAGED_POLICY_NAME_AMI\n    }\n}\n", "awscli/customizations/dlm/__init__.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "awscli/customizations/dlm/dlm.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.dlm.createdefaultrole import CreateDefaultRole\n\n\ndef dlm_initialize(cli):\n    \"\"\"\n    The entry point for Lifecycle high level commands.\n    \"\"\"\n    cli.register('building-command-table.dlm', register_commands)\n\n\ndef register_commands(command_table, session, **kwargs):\n    \"\"\"\n    Called when the Lifecycle command table is being built. Used to inject new\n    high level commands into the command list. These high level commands\n    must not collide with existing low-level API call names.\n    \"\"\"\n    command_table['create-default-role'] = CreateDefaultRole(session)\n", "awscli/customizations/dlm/createdefaultrole.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n# Class to create default roles for lifecycle\nimport logging\nfrom awscli.clidriver import CLIOperationCaller\nfrom awscli.customizations.utils import get_policy_arn_suffix\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.dlm.iam import IAM\nfrom awscli.customizations.dlm.constants \\\n    import RESOURCES, \\\n    LIFECYCLE_DEFAULT_ROLE_ASSUME_POLICY, \\\n    POLICY_ARN_PATTERN, \\\n    RESOURCE_TYPE_SNAPSHOT, \\\n    RESOURCE_TYPE_IMAGE\n\nLOG = logging.getLogger(__name__)\n\n\ndef _construct_result(create_role_response, get_policy_response):\n    get_policy_response.pop('ResponseMetadata', None)\n    create_role_response.pop('ResponseMetadata', None)\n    result = {'RolePolicy': get_policy_response}\n    result.update(create_role_response)\n    return result\n\n\n# Display the result as formatted json\ndef display_response(session, operation_name, result, parsed_globals):\n    if result is not None:\n        cli_operation_caller = CLIOperationCaller(session)\n        # Calling a private method. Should be changed after the functionality\n        # is moved outside CliOperationCaller.\n        cli_operation_caller._display_response(\n            operation_name, result, parsed_globals)\n\n\n# Get policy arn from region and policy name\ndef get_policy_arn(region, policy_name):\n    region_suffix = get_policy_arn_suffix(region)\n    role_arn = POLICY_ARN_PATTERN.format(region_suffix, policy_name)\n    return role_arn\n\n\n# Method to parse the arguments to get the region value\ndef get_region(session, parsed_globals):\n    region = parsed_globals.region\n    if region is None:\n        region = session.get_config_variable('region')\n    return region\n\n\nclass CreateDefaultRole(BasicCommand):\n    NAME = \"create-default-role\"\n    DESCRIPTION = ('Creates the default IAM role '\n                   ' which will be used by Lifecycle service.\\n'\n                   'If the role does not exist, create-default-role '\n                   'will automatically create it and set its policy.'\n                   ' If the role has been already '\n                   'created, create-default-role'\n                   ' will not update its policy.'\n                   '\\n')\n    ARG_TABLE = [\n        {'name': 'iam-endpoint',\n         'no_paramfile': True,\n         'help_text': '<p>The IAM endpoint to call for creating the roles.'\n                      ' This is optional and should only be specified when a'\n                      ' custom endpoint should be called for IAM operations'\n                      '.</p>'},\n        {'name': 'resource-type',\n         'default': RESOURCE_TYPE_SNAPSHOT,\n         'choices': [RESOURCE_TYPE_SNAPSHOT, RESOURCE_TYPE_IMAGE],\n         'help_text': (\n                 \"<p>The resource type for which the role needs to be created.\"\n                 \" The available options are '%s' and '%s'.\"\n                 \" This parameter defaults to '%s'.</p>\"\n                 % (RESOURCE_TYPE_SNAPSHOT, RESOURCE_TYPE_IMAGE,\n                    RESOURCE_TYPE_SNAPSHOT))}\n\n    ]\n\n    def __init__(self, session):\n        super(CreateDefaultRole, self).__init__(session)\n\n    def _run_main(self, parsed_args, parsed_globals):\n        \"\"\"Call to run the commands\"\"\"\n\n        self._region = get_region(self._session, parsed_globals)\n        self._endpoint_url = parsed_args.iam_endpoint\n        self._resource_type = parsed_args.resource_type\n        self._iam_client = IAM(self._session.create_client(\n            'iam',\n            region_name=self._region,\n            endpoint_url=self._endpoint_url,\n            verify=parsed_globals.verify_ssl\n        ))\n\n        result = self._create_default_role_if_not_exists(parsed_globals)\n\n        display_response(\n            self._session,\n            'create_role',\n            result,\n            parsed_globals\n        )\n\n        return 0\n\n    def _create_default_role_if_not_exists(self, parsed_globals):\n        \"\"\"Method to create default lifecycle role\n            if it doesn't exist already\n        \"\"\"\n\n        role_name = RESOURCES[self._resource_type]['default_role_name']\n        assume_role_policy = LIFECYCLE_DEFAULT_ROLE_ASSUME_POLICY\n\n        if self._iam_client.check_if_role_exists(role_name):\n            LOG.debug('Role %s exists', role_name)\n            return None\n\n        LOG.debug('Role %s does not exist. '\n                  'Creating default role for Lifecycle', role_name)\n\n        # Get Region\n        region = get_region(self._session, parsed_globals)\n\n        if region is None:\n            raise ValueError('You must specify a region. '\n                             'You can also configure your region '\n                             'by running \"aws configure\".')\n\n        managed_policy_arn = get_policy_arn(\n            region,\n            RESOURCES[self._resource_type]['default_policy_name']\n        )\n\n        # Don't proceed if managed policy does not exist\n        if not self._iam_client.check_if_policy_exists(managed_policy_arn):\n            LOG.debug('Managed Policy %s does not exist.', managed_policy_arn)\n            return None\n\n        LOG.debug('Managed Policy %s exists.', managed_policy_arn)\n        # Create default role\n        create_role_response = \\\n            self._iam_client.create_role_with_trust_policy(\n                role_name,\n                assume_role_policy\n            )\n        # Attach policy to role\n        self._iam_client.attach_policy_to_role(\n            managed_policy_arn,\n            role_name\n        )\n\n        # Construct result\n        get_policy_response = self._iam_client.get_policy(managed_policy_arn)\n        return _construct_result(create_role_response, get_policy_response)\n", "awscli/customizations/codedeploy/deregister.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport sys\n\nfrom botocore.exceptions import ClientError\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.codedeploy.utils import \\\n    validate_region, validate_instance_name, INSTANCE_NAME_ARG\n\n\nclass Deregister(BasicCommand):\n    NAME = 'deregister'\n\n    DESCRIPTION = (\n        'Removes any tags from the on-premises instance; deregisters the '\n        'on-premises instance from AWS CodeDeploy; and, unless requested '\n        'otherwise, deletes the IAM user for the on-premises instance.'\n    )\n\n    ARG_TABLE = [\n        INSTANCE_NAME_ARG,\n        {\n            'name': 'no-delete-iam-user',\n            'action': 'store_true',\n            'default': False,\n            'help_text': (\n                'Optional. Do not delete the IAM user for the registered '\n                'on-premises instance.'\n            )\n        }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        params = parsed_args\n        params.session = self._session\n        validate_region(params, parsed_globals)\n        validate_instance_name(params)\n\n        self.codedeploy = self._session.create_client(\n            'codedeploy',\n            region_name=params.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl\n        )\n        self.iam = self._session.create_client(\n            'iam',\n            region_name=params.region\n        )\n\n        try:\n            self._get_instance_info(params)\n            if params.tags:\n                self._remove_tags(params)\n            self._deregister_instance(params)\n            if not params.no_delete_iam_user:\n                self._delete_user_policy(params)\n                self._delete_access_key(params)\n                self._delete_iam_user(params)\n            sys.stdout.write(\n                'Run the following command on the on-premises instance to '\n                'uninstall the codedeploy-agent:\\n'\n                'aws deploy uninstall\\n'\n            )\n        except Exception as e:\n            sys.stdout.flush()\n            sys.stderr.write(\n                'ERROR\\n'\n                '{0}\\n'\n                'Deregister the on-premises instance by following the '\n                'instructions in \"Configure Existing On-Premises Instances by '\n                'Using AWS CodeDeploy\" in the AWS CodeDeploy User '\n                'Guide.\\n'.format(e)\n            )\n\n    def _get_instance_info(self, params):\n        sys.stdout.write('Retrieving on-premises instance information... ')\n        response = self.codedeploy.get_on_premises_instance(\n            instanceName=params.instance_name\n        )\n        params.iam_user_arn = response['instanceInfo']['iamUserArn']\n        start = params.iam_user_arn.rfind('/') + 1\n        params.user_name = params.iam_user_arn[start:]\n        params.tags = response['instanceInfo']['tags']\n        sys.stdout.write(\n            'DONE\\n'\n            'IamUserArn: {0}\\n'.format(\n                params.iam_user_arn\n            )\n        )\n        if params.tags:\n            sys.stdout.write('Tags:')\n            for tag in params.tags:\n                sys.stdout.write(\n                    ' Key={0},Value={1}'.format(tag['Key'], tag['Value'])\n                )\n            sys.stdout.write('\\n')\n\n    def _remove_tags(self, params):\n        sys.stdout.write('Removing tags from the on-premises instance... ')\n        self.codedeploy.remove_tags_from_on_premises_instances(\n            tags=params.tags,\n            instanceNames=[params.instance_name]\n        )\n        sys.stdout.write('DONE\\n')\n\n    def _deregister_instance(self, params):\n        sys.stdout.write('Deregistering the on-premises instance... ')\n        self.codedeploy.deregister_on_premises_instance(\n            instanceName=params.instance_name\n        )\n        sys.stdout.write('DONE\\n')\n\n    def _delete_user_policy(self, params):\n        sys.stdout.write('Deleting the IAM user policies... ')\n        list_user_policies = self.iam.get_paginator('list_user_policies')\n        try:\n            for response in list_user_policies.paginate(\n                    UserName=params.user_name):\n                for policy_name in response['PolicyNames']:\n                    self.iam.delete_user_policy(\n                        UserName=params.user_name,\n                        PolicyName=policy_name\n                    )\n        except ClientError as e:\n            if e.response.get('Error', {}).get('Code') != 'NoSuchEntity':\n                raise e\n        sys.stdout.write('DONE\\n')\n\n    def _delete_access_key(self, params):\n        sys.stdout.write('Deleting the IAM user access keys... ')\n        list_access_keys = self.iam.get_paginator('list_access_keys')\n        try:\n            for response in list_access_keys.paginate(\n                    UserName=params.user_name):\n                for access_key in response['AccessKeyMetadata']:\n                    self.iam.delete_access_key(\n                        UserName=params.user_name,\n                        AccessKeyId=access_key['AccessKeyId']\n                    )\n        except ClientError as e:\n            if e.response.get('Error', {}).get('Code') != 'NoSuchEntity':\n                raise e\n        sys.stdout.write('DONE\\n')\n\n    def _delete_iam_user(self, params):\n        sys.stdout.write('Deleting the IAM user ({0})... '.format(\n            params.user_name\n        ))\n        try:\n            self.iam.delete_user(UserName=params.user_name)\n        except ClientError as e:\n            if e.response.get('Error', {}).get('Code') != 'NoSuchEntity':\n                raise e\n        sys.stdout.write('DONE\\n')\n", "awscli/customizations/codedeploy/register.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport sys\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.codedeploy.systems import DEFAULT_CONFIG_FILE\nfrom awscli.customizations.codedeploy.utils import \\\n    validate_region, validate_instance_name, validate_tags, \\\n    validate_iam_user_arn, INSTANCE_NAME_ARG, IAM_USER_ARN_ARG\n\n\nclass Register(BasicCommand):\n    NAME = 'register'\n\n    DESCRIPTION = (\n        \"Creates an IAM user for the on-premises instance, if not provided, \"\n        \"and saves the user's credentials to an on-premises instance \"\n        \"configuration file; registers the on-premises instance with AWS \"\n        \"CodeDeploy; and optionally adds tags to the on-premises instance.\"\n    )\n\n    TAGS_SCHEMA = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"Key\": {\n                    \"description\": \"The tag key.\",\n                    \"type\": \"string\",\n                    \"required\": True\n                },\n                \"Value\": {\n                    \"description\": \"The tag value.\",\n                    \"type\": \"string\",\n                    \"required\": True\n                }\n            }\n        }\n    }\n\n    ARG_TABLE = [\n        INSTANCE_NAME_ARG,\n        {\n            'name': 'tags',\n            'synopsis': '--tags <value>',\n            'required': False,\n            'nargs': '+',\n            'schema': TAGS_SCHEMA,\n            'help_text': (\n                'Optional. The list of key/value pairs to tag the on-premises '\n                'instance.'\n            )\n        },\n        IAM_USER_ARN_ARG\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        params = parsed_args\n        params.session = self._session\n        validate_region(params, parsed_globals)\n        validate_instance_name(params)\n        validate_tags(params)\n        validate_iam_user_arn(params)\n\n        self.codedeploy = self._session.create_client(\n            'codedeploy',\n            region_name=params.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl\n        )\n        self.iam = self._session.create_client(\n            'iam',\n            region_name=params.region\n        )\n\n        try:\n            if not params.iam_user_arn:\n                self._create_iam_user(params)\n                self._create_access_key(params)\n                self._create_user_policy(params)\n                self._create_config(params)\n            self._register_instance(params)\n            if params.tags:\n                self._add_tags(params)\n            sys.stdout.write(\n                'Copy the on-premises configuration file named {0} to the '\n                'on-premises instance, and run the following command on the '\n                'on-premises instance to install and configure the AWS '\n                'CodeDeploy Agent:\\n'\n                'aws deploy install --config-file {0}\\n'.format(\n                    DEFAULT_CONFIG_FILE\n                )\n            )\n        except Exception as e:\n            sys.stdout.flush()\n            sys.stderr.write(\n                'ERROR\\n'\n                '{0}\\n'\n                'Register the on-premises instance by following the '\n                'instructions in \"Configure Existing On-Premises Instances by '\n                'Using AWS CodeDeploy\" in the AWS CodeDeploy User '\n                'Guide.\\n'.format(e)\n            )\n\n    def _create_iam_user(self, params):\n        sys.stdout.write('Creating the IAM user... ')\n        params.user_name = params.instance_name\n        response = self.iam.create_user(\n            Path='/AWS/CodeDeploy/',\n            UserName=params.user_name\n        )\n        params.iam_user_arn = response['User']['Arn']\n        sys.stdout.write(\n            'DONE\\n'\n            'IamUserArn: {0}\\n'.format(\n                params.iam_user_arn\n            )\n        )\n\n    def _create_access_key(self, params):\n        sys.stdout.write('Creating the IAM user access key... ')\n        response = self.iam.create_access_key(\n            UserName=params.user_name\n        )\n        params.access_key_id = response['AccessKey']['AccessKeyId']\n        params.secret_access_key = response['AccessKey']['SecretAccessKey']\n        sys.stdout.write(\n            'DONE\\n'\n            'AccessKeyId: {0}\\n'\n            'SecretAccessKey: {1}\\n'.format(\n                params.access_key_id,\n                params.secret_access_key\n            )\n        )\n\n    def _create_user_policy(self, params):\n        sys.stdout.write('Creating the IAM user policy... ')\n        params.policy_name = 'codedeploy-agent'\n        params.policy_document = (\n            '{\\n'\n            '    \"Version\": \"2012-10-17\",\\n'\n            '    \"Statement\": [ {\\n'\n            '        \"Action\": [ \"s3:Get*\", \"s3:List*\" ],\\n'\n            '        \"Effect\": \"Allow\",\\n'\n            '        \"Resource\": \"*\"\\n'\n            '    } ]\\n'\n            '}'\n        )\n        self.iam.put_user_policy(\n            UserName=params.user_name,\n            PolicyName=params.policy_name,\n            PolicyDocument=params.policy_document\n        )\n        sys.stdout.write(\n            'DONE\\n'\n            'PolicyName: {0}\\n'\n            'PolicyDocument: {1}\\n'.format(\n                params.policy_name,\n                params.policy_document\n            )\n        )\n\n    def _create_config(self, params):\n        sys.stdout.write(\n            'Creating the on-premises instance configuration file named {0}'\n            '...'.format(DEFAULT_CONFIG_FILE)\n        )\n        with open(DEFAULT_CONFIG_FILE, 'w') as f:\n            f.write(\n                '---\\n'\n                'region: {0}\\n'\n                'iam_user_arn: {1}\\n'\n                'aws_access_key_id: {2}\\n'\n                'aws_secret_access_key: {3}\\n'.format(\n                    params.region,\n                    params.iam_user_arn,\n                    params.access_key_id,\n                    params.secret_access_key\n                )\n            )\n        sys.stdout.write('DONE\\n')\n\n    def _register_instance(self, params):\n        sys.stdout.write('Registering the on-premises instance... ')\n        self.codedeploy.register_on_premises_instance(\n            instanceName=params.instance_name,\n            iamUserArn=params.iam_user_arn\n        )\n        sys.stdout.write('DONE\\n')\n\n    def _add_tags(self, params):\n        sys.stdout.write('Adding tags to the on-premises instance... ')\n        self.codedeploy.add_tags_to_on_premises_instances(\n            tags=params.tags,\n            instanceNames=[params.instance_name]\n        )\n        sys.stdout.write('DONE\\n')\n", "awscli/customizations/codedeploy/systems.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport ctypes\nimport os\nimport subprocess\n\nDEFAULT_CONFIG_FILE = 'codedeploy.onpremises.yml'\n\n\nclass System:\n    UNSUPPORTED_SYSTEM_MSG = (\n        'Only Ubuntu Server, Red Hat Enterprise Linux Server and '\n        'Windows Server operating systems are supported.'\n    )\n\n    def __init__(self, params):\n        self.session = params.session\n        self.s3 = self.session.create_client(\n            's3',\n            region_name=params.region\n        )\n\n    def validate_administrator(self):\n        raise NotImplementedError('validate_administrator')\n\n    def install(self, params):\n        raise NotImplementedError('install')\n\n    def uninstall(self, params):\n        raise NotImplementedError('uninstall')\n\n\nclass Windows(System):\n    CONFIG_DIR = r'C:\\ProgramData\\Amazon\\CodeDeploy'\n    CONFIG_FILE = 'conf.onpremises.yml'\n    CONFIG_PATH = r'{0}\\{1}'.format(CONFIG_DIR, CONFIG_FILE)\n    INSTALLER = 'codedeploy-agent.msi'\n\n    def validate_administrator(self):\n        if not ctypes.windll.shell32.IsUserAnAdmin():\n            raise RuntimeError(\n                'You must run this command as an Administrator.'\n            )\n\n    def install(self, params):\n        if 'installer' in params:\n            self.INSTALLER = params.installer\n\n        process = subprocess.Popen(\n            [\n                'powershell.exe',\n                '-Command', 'Stop-Service',\n                '-Name', 'codedeployagent'\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        (output, error) = process.communicate()\n        not_found = (\n            \"Cannot find any service with service name 'codedeployagent'\"\n        )\n        if process.returncode != 0 and not_found not in error:\n            raise RuntimeError(\n                'Failed to stop the AWS CodeDeploy Agent:\\n{0}'.format(error)\n            )\n\n        response = self.s3.get_object(Bucket=params.bucket, Key=params.key)\n        with open(self.INSTALLER, 'wb') as f:\n            f.write(response['Body'].read())\n\n        subprocess.check_call(\n            [\n                r'.\\{0}'.format(self.INSTALLER),\n                '/quiet',\n                '/l', r'.\\codedeploy-agent-install-log.txt'\n            ],\n            shell=True\n        )\n        subprocess.check_call([\n            'powershell.exe',\n            '-Command', 'Restart-Service',\n            '-Name', 'codedeployagent'\n        ])\n\n        process = subprocess.Popen(\n            [\n                'powershell.exe',\n                '-Command', 'Get-Service',\n                '-Name', 'codedeployagent'\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        (output, error) = process.communicate()\n        if \"Running\" not in output:\n            raise RuntimeError(\n                'The AWS CodeDeploy Agent did not start after installation.'\n            )\n\n    def uninstall(self, params):\n        process = subprocess.Popen(\n            [\n                'powershell.exe',\n                '-Command', 'Stop-Service',\n                '-Name', 'codedeployagent'\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        (output, error) = process.communicate()\n        not_found = (\n            \"Cannot find any service with service name 'codedeployagent'\"\n        )\n        if process.returncode == 0:\n            self._remove_agent()\n        elif not_found not in error:\n            raise RuntimeError(\n                'Failed to stop the AWS CodeDeploy Agent:\\n{0}'.format(error)\n            )\n\n    def _remove_agent(self):\n        process = subprocess.Popen(\n            [\n                'wmic',\n                'product', 'where', 'name=\"CodeDeploy Host Agent\"',\n                'call', 'uninstall', '/nointeractive'\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        (output, error) = process.communicate()\n        if process.returncode != 0:\n            raise RuntimeError(\n                'Failed to uninstall the AWS CodeDeploy Agent:\\n{0}'.format(\n                    error\n                )\n            )\n\n\nclass Linux(System):\n    CONFIG_DIR = '/etc/codedeploy-agent/conf'\n    CONFIG_FILE = DEFAULT_CONFIG_FILE\n    CONFIG_PATH = '{0}/{1}'.format(CONFIG_DIR, CONFIG_FILE)\n    INSTALLER = 'install'\n\n    def validate_administrator(self):\n        if os.geteuid() != 0:\n            raise RuntimeError('You must run this command as sudo.')\n\n    def install(self, params):\n        if 'installer' in params:\n            self.INSTALLER = params.installer\n\n        self._update_system(params)\n        self._stop_agent(params)\n\n        response = self.s3.get_object(Bucket=params.bucket, Key=params.key)\n        with open(self.INSTALLER, 'wb') as f:\n            f.write(response['Body'].read())\n\n        subprocess.check_call(\n            ['chmod', '+x', './{0}'.format(self.INSTALLER)]\n        )\n\n        credentials = self.session.get_credentials()\n        environment = os.environ.copy()\n        environment['AWS_REGION'] = params.region\n        environment['AWS_ACCESS_KEY_ID'] = credentials.access_key\n        environment['AWS_SECRET_ACCESS_KEY'] = credentials.secret_key\n        if credentials.token is not None:\n            environment['AWS_SESSION_TOKEN'] = credentials.token\n        subprocess.check_call(\n            ['./{0}'.format(self.INSTALLER), 'auto'],\n            env=environment\n        )\n\n    def uninstall(self, params):\n        process = self._stop_agent(params)\n        if process.returncode == 0:\n            self._remove_agent(params)\n\n    def _update_system(self, params):\n        raise NotImplementedError('preinstall')\n\n    def _remove_agent(self, params):\n        raise NotImplementedError('remove_agent')\n\n    def _stop_agent(self, params):\n        process = subprocess.Popen(\n            ['service', 'codedeploy-agent', 'stop'],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        (output, error) = process.communicate()\n        if process.returncode != 0 and params.not_found_msg not in error:\n            raise RuntimeError(\n                'Failed to stop the AWS CodeDeploy Agent:\\n{0}'.format(error)\n            )\n        return process\n\n\nclass Ubuntu(Linux):\n    def _update_system(self, params):\n        subprocess.check_call(['apt-get', '-y', 'update'])\n        subprocess.check_call(['apt-get', '-y', 'install', 'ruby2.0'])\n\n    def _remove_agent(self, params):\n        subprocess.check_call(['dpkg', '-r', 'codedeploy-agent'])\n\n    def _stop_agent(self, params):\n        params.not_found_msg = 'codedeploy-agent: unrecognized service'\n        return Linux._stop_agent(self, params)\n\n\nclass RHEL(Linux):\n    def _update_system(self, params):\n        subprocess.check_call(['yum', '-y', 'install', 'ruby'])\n\n    def _remove_agent(self, params):\n        subprocess.check_call(['yum', '-y', 'erase', 'codedeploy-agent'])\n\n    def _stop_agent(self, params):\n        params.not_found_msg = 'Redirecting to /bin/systemctl stop  codedeploy-agent.service'\n        return Linux._stop_agent(self, params)\n", "awscli/customizations/codedeploy/uninstall.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\nimport sys\nimport errno\n\nfrom awscli.customizations.codedeploy.utils import validate_instance, \\\n    validate_region\nfrom awscli.customizations.commands import BasicCommand\n\n\nclass Uninstall(BasicCommand):\n    NAME = 'uninstall'\n\n    DESCRIPTION = (\n        'Uninstalls the AWS CodeDeploy Agent from the on-premises instance.'\n    )\n\n    def _run_main(self, parsed_args, parsed_globals):\n        params = parsed_args\n        params.session = self._session\n        validate_region(params, parsed_globals)\n        validate_instance(params)\n        params.system.validate_administrator()\n\n        try:\n            self._uninstall_agent(params)\n            self._delete_config_file(params)\n        except Exception as e:\n            sys.stdout.flush()\n            sys.stderr.write(\n                'ERROR\\n'\n                '{0}\\n'\n                'Uninstall the AWS CodeDeploy Agent on the on-premises '\n                'instance by following the instructions in \"Configure '\n                'Existing On-Premises Instances by Using AWS CodeDeploy\" in '\n                'the AWS CodeDeploy User Guide.\\n'.format(e)\n            )\n\n    def _uninstall_agent(self, params):\n        sys.stdout.write('Uninstalling the AWS CodeDeploy Agent... ')\n        params.system.uninstall(params)\n        sys.stdout.write('DONE\\n')\n\n    def _delete_config_file(self, params):\n        sys.stdout.write('Deleting the on-premises instance configuration... ')\n        try:\n            os.remove(params.system.CONFIG_PATH)\n        except OSError as e:\n            if e.errno != errno.ENOENT:\n                raise e\n        sys.stdout.write('DONE\\n')\n", "awscli/customizations/codedeploy/install.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport errno\nimport os\nimport shutil\nimport sys\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.codedeploy.utils import \\\n    validate_region, validate_s3_location, validate_instance\n\n\nclass Install(BasicCommand):\n    NAME = 'install'\n\n    DESCRIPTION = (\n        'Configures and installs the AWS CodeDeploy Agent on the on-premises '\n        'instance.'\n    )\n\n    ARG_TABLE = [\n        {\n            'name': 'config-file',\n            'synopsis': '--config-file <path>',\n            'required': True,\n            'help_text': (\n                'Required. The path to the on-premises instance configuration '\n                'file.'\n            )\n        },\n        {\n            'name': 'override-config',\n            'action': 'store_true',\n            'default': False,\n            'help_text': (\n                'Optional. Overrides the on-premises instance configuration '\n                'file.'\n            )\n        },\n        {\n            'name': 'agent-installer',\n            'synopsis': '--agent-installer <s3-location>',\n            'required': False,\n            'help_text': (\n                'Optional. The AWS CodeDeploy Agent installer file.'\n            )\n        }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        params = parsed_args\n        params.session = self._session\n        validate_region(params, parsed_globals)\n        validate_instance(params)\n        params.system.validate_administrator()\n        self._validate_override_config(params)\n        self._validate_agent_installer(params)\n\n        try:\n            self._create_config(params)\n            self._install_agent(params)\n        except Exception as e:\n            sys.stdout.flush()\n            sys.stderr.write(\n                'ERROR\\n'\n                '{0}\\n'\n                'Install the AWS CodeDeploy Agent on the on-premises instance '\n                'by following the instructions in \"Configure Existing '\n                'On-Premises Instances by Using AWS CodeDeploy\" in the AWS '\n                'CodeDeploy User Guide.\\n'.format(e)\n            )\n\n    def _validate_override_config(self, params):\n        if os.path.isfile(params.system.CONFIG_PATH) and \\\n                not params.override_config:\n            raise RuntimeError(\n                'The on-premises instance configuration file already exists. '\n                'Specify --override-config to update the existing on-premises '\n                'instance configuration file.'\n            )\n\n    def _validate_agent_installer(self, params):\n        validate_s3_location(params, 'agent_installer')\n        if 'bucket' not in params:\n            params.bucket = 'aws-codedeploy-{0}'.format(params.region)\n        if 'key' not in params:\n            params.key = 'latest/{0}'.format(params.system.INSTALLER)\n            params.installer = params.system.INSTALLER\n        else:\n            start = params.key.rfind('/') + 1\n            params.installer = params.key[start:]\n\n    def _create_config(self, params):\n        sys.stdout.write(\n            'Creating the on-premises instance configuration file... '\n        )\n        try:\n            os.makedirs(params.system.CONFIG_DIR)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise e\n        if params.config_file != params.system.CONFIG_PATH:\n            shutil.copyfile(params.config_file, params.system.CONFIG_PATH)\n        sys.stdout.write('DONE\\n')\n\n    def _install_agent(self, params):\n        sys.stdout.write('Installing the AWS CodeDeploy Agent... ')\n        params.system.install(params)\n        sys.stdout.write('DONE\\n')\n", "awscli/customizations/codedeploy/utils.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport platform\nimport re\n\nimport awscli.compat\nfrom awscli.compat import urlopen, URLError\nfrom awscli.customizations.codedeploy.systems import System, Ubuntu, Windows, RHEL\nfrom socket import timeout\n\n\nMAX_INSTANCE_NAME_LENGTH = 100\nMAX_TAGS_PER_INSTANCE = 10\nMAX_TAG_KEY_LENGTH = 128\nMAX_TAG_VALUE_LENGTH = 256\n\nINSTANCE_NAME_PATTERN = r'^[A-Za-z0-9+=,.@_-]+$'\nIAM_USER_ARN_PATTERN = r'^arn:aws:iam::[0-9]{12}:user/[A-Za-z0-9/+=,.@_-]+$'\n\nINSTANCE_NAME_ARG = {\n    'name': 'instance-name',\n    'synopsis': '--instance-name <instance-name>',\n    'required': True,\n    'help_text': (\n        'Required. The name of the on-premises instance.'\n    )\n}\n\nIAM_USER_ARN_ARG = {\n    'name': 'iam-user-arn',\n    'synopsis': '--iam-user-arn <iam-user-arn>',\n    'required': False,\n    'help_text': (\n        'Optional. The IAM user associated with the on-premises instance.'\n    )\n}\n\n\ndef validate_region(params, parsed_globals):\n    if parsed_globals.region:\n        params.region = parsed_globals.region\n    else:\n        params.region = params.session.get_config_variable('region')\n    if not params.region:\n        raise RuntimeError('Region not specified.')\n\n\ndef validate_instance_name(params):\n    if params.instance_name:\n        if not re.match(INSTANCE_NAME_PATTERN, params.instance_name):\n            raise ValueError('Instance name contains invalid characters.')\n        if params.instance_name.startswith('i-'):\n            raise ValueError('Instance name cannot start with \\'i-\\'.')\n        if len(params.instance_name) > MAX_INSTANCE_NAME_LENGTH:\n            raise ValueError(\n                'Instance name cannot be longer than {0} characters.'.format(\n                    MAX_INSTANCE_NAME_LENGTH\n                )\n            )\n\n\ndef validate_tags(params):\n    if params.tags:\n        if len(params.tags) > MAX_TAGS_PER_INSTANCE:\n            raise ValueError(\n                'Instances can only have a maximum of {0} tags.'.format(\n                    MAX_TAGS_PER_INSTANCE\n                )\n            )\n        for tag in params.tags:\n            if len(tag['Key']) > MAX_TAG_KEY_LENGTH:\n                raise ValueError(\n                    'Tag Key cannot be longer than {0} characters.'.format(\n                        MAX_TAG_KEY_LENGTH\n                    )\n                )\n            if len(tag['Value']) > MAX_TAG_VALUE_LENGTH:\n                raise ValueError(\n                    'Tag Value cannot be longer than {0} characters.'.format(\n                        MAX_TAG_VALUE_LENGTH\n                    )\n                )\n\n\ndef validate_iam_user_arn(params):\n    if params.iam_user_arn and \\\n            not re.match(IAM_USER_ARN_PATTERN, params.iam_user_arn):\n        raise ValueError('Invalid IAM user ARN.')\n\n\ndef validate_instance(params):\n    if platform.system() == 'Linux':\n        distribution = awscli.compat.linux_distribution()[0]\n        if 'Ubuntu' in distribution:\n            params.system = Ubuntu(params)\n        if 'Red Hat Enterprise Linux Server' in distribution:\n            params.system = RHEL(params)\n    elif platform.system() == 'Windows':\n        params.system = Windows(params)\n    if 'system' not in params:\n        raise RuntimeError(\n            System.UNSUPPORTED_SYSTEM_MSG\n        )\n    try:\n        urlopen('http://169.254.169.254/latest/meta-data/', timeout=1)\n        raise RuntimeError('Amazon EC2 instances are not supported.')\n    except (URLError, timeout):\n        pass\n\n\ndef validate_s3_location(params, arg_name):\n    arg_name = arg_name.replace('-', '_')\n    if arg_name in params:\n        s3_location = getattr(params, arg_name)\n        if s3_location:\n            matcher = re.match('s3://(.+?)/(.+)', str(s3_location))\n            if matcher:\n                params.bucket = matcher.group(1)\n                params.key = matcher.group(2)\n            else:\n                raise ValueError(\n                    '--{0} must specify the Amazon S3 URL format as '\n                    's3://<bucket>/<key>.'.format(\n                        arg_name.replace('_', '-')\n                    )\n                )\n", "awscli/customizations/codedeploy/locationargs.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.argprocess import unpack_cli_arg\nfrom awscli.arguments import CustomArgument\nfrom awscli.arguments import create_argument_model_from_schema\n\nS3_LOCATION_ARG_DESCRIPTION = {\n    'name': 's3-location',\n    'required': False,\n    'help_text': (\n        'Information about the location of the application revision in Amazon '\n        'S3. You must specify the bucket, the key, and bundleType. '\n        'Optionally, you can also specify an eTag and version.'\n    )\n}\n\nS3_LOCATION_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"bucket\": {\n            \"type\": \"string\",\n            \"description\": \"The Amazon S3 bucket name.\",\n            \"required\": True\n        },\n        \"key\": {\n            \"type\": \"string\",\n            \"description\": \"The Amazon S3 object key name.\",\n            \"required\": True\n        },\n        \"bundleType\": {\n            \"type\": \"string\",\n            \"description\": \"The format of the bundle stored in Amazon S3.\",\n            \"enum\": [\"tar\", \"tgz\", \"zip\"],\n            \"required\": True\n        },\n        \"eTag\": {\n            \"type\": \"string\",\n            \"description\": \"The Amazon S3 object eTag.\",\n            \"required\": False\n        },\n        \"version\": {\n            \"type\": \"string\",\n            \"description\": \"The Amazon S3 object version.\",\n            \"required\": False\n        }\n    }\n}\n\nGITHUB_LOCATION_ARG_DESCRIPTION = {\n    'name': 'github-location',\n    'required': False,\n    'help_text': (\n        'Information about the location of the application revision in '\n        'GitHub. You must specify the repository and commit ID that '\n        'references the application revision. For the repository, use the '\n        'format GitHub-account/repository-name or GitHub-org/repository-name. '\n        'For the commit ID, use the SHA1 Git commit reference.'\n    )\n}\n\nGITHUB_LOCATION_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"repository\": {\n            \"type\": \"string\",\n            \"description\": (\n                \"The GitHub account or organization and repository. Specify \"\n                \"as GitHub-account/repository or GitHub-org/repository.\"\n            ),\n            \"required\": True\n        },\n        \"commitId\": {\n            \"type\": \"string\",\n            \"description\": \"The SHA1 Git commit reference.\",\n            \"required\": True\n        }\n    }\n}\n\n\ndef modify_revision_arguments(argument_table, session, **kwargs):\n    s3_model = create_argument_model_from_schema(S3_LOCATION_SCHEMA)\n    argument_table[S3_LOCATION_ARG_DESCRIPTION['name']] = (\n        S3LocationArgument(\n            argument_model=s3_model,\n            session=session,\n            **S3_LOCATION_ARG_DESCRIPTION\n        )\n    )\n    github_model = create_argument_model_from_schema(GITHUB_LOCATION_SCHEMA)\n    argument_table[GITHUB_LOCATION_ARG_DESCRIPTION['name']] = (\n        GitHubLocationArgument(\n            argument_model=github_model,\n            session=session,\n            **GITHUB_LOCATION_ARG_DESCRIPTION\n        )\n    )\n    argument_table['revision'].required = False\n\n\nclass LocationArgument(CustomArgument):\n    def __init__(self, session, *args, **kwargs):\n        super(LocationArgument, self).__init__(*args, **kwargs)\n        self._session = session\n\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        parsed = self._session.emit_first_non_none_response(\n            'process-cli-arg.codedeploy.%s' % self.name,\n            param=self.argument_model,\n            cli_argument=self,\n            value=value,\n            operation=None\n        )\n        if parsed is None:\n            parsed = unpack_cli_arg(self, value)\n        parameters['revision'] = self.build_revision_location(parsed)\n\n    def build_revision_location(self, value_dict):\n        \"\"\"\n        Repack the input structure into a revisionLocation.\n        \"\"\"\n        raise NotImplementedError(\"build_revision_location\")\n\n\nclass S3LocationArgument(LocationArgument):\n    def build_revision_location(self, value_dict):\n        required = ['bucket', 'key', 'bundleType']\n        valid = lambda k: value_dict.get(k, False)\n        if not all(map(valid, required)):\n            raise RuntimeError(\n                '--s3-location must specify bucket, key and bundleType.'\n            )\n        revision = {\n            \"revisionType\": \"S3\",\n            \"s3Location\": {\n                \"bucket\": value_dict['bucket'],\n                \"key\": value_dict['key'],\n                \"bundleType\": value_dict['bundleType']\n            }\n        }\n        if 'eTag' in value_dict:\n            revision['s3Location']['eTag'] = value_dict['eTag']\n        if 'version' in value_dict:\n            revision['s3Location']['version'] = value_dict['version']\n        return revision\n\n\nclass GitHubLocationArgument(LocationArgument):\n    def build_revision_location(self, value_dict):\n        required = ['repository', 'commitId']\n        valid = lambda k: value_dict.get(k, False)\n        if not all(map(valid, required)):\n            raise RuntimeError(\n                '--github-location must specify repository and commitId.'\n            )\n        return {\n            \"revisionType\": \"GitHub\",\n            \"gitHubLocation\": {\n                \"repository\": value_dict['repository'],\n                \"commitId\": value_dict['commitId']\n            }\n        }\n", "awscli/customizations/codedeploy/codedeploy.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations import utils\nfrom awscli.customizations.codedeploy.locationargs import \\\n    modify_revision_arguments\nfrom awscli.customizations.codedeploy.push import Push\nfrom awscli.customizations.codedeploy.register import Register\nfrom awscli.customizations.codedeploy.deregister import Deregister\nfrom awscli.customizations.codedeploy.install import Install\nfrom awscli.customizations.codedeploy.uninstall import Uninstall\n\n\ndef initialize(cli):\n    \"\"\"\n    The entry point for CodeDeploy high level commands.\n    \"\"\"\n    cli.register(\n        'building-command-table.main',\n        change_name\n    )\n    cli.register(\n        'building-command-table.deploy',\n        inject_commands\n    )\n    cli.register(\n        'building-argument-table.deploy.get-application-revision',\n        modify_revision_arguments\n    )\n    cli.register(\n        'building-argument-table.deploy.register-application-revision',\n        modify_revision_arguments\n    )\n    cli.register(\n        'building-argument-table.deploy.create-deployment',\n        modify_revision_arguments\n    )\n\n\ndef change_name(command_table, session, **kwargs):\n    \"\"\"\n    Change all existing 'aws codedeploy' commands to 'aws deploy' commands.\n    \"\"\"\n    utils.rename_command(command_table, 'codedeploy', 'deploy')\n\n\ndef inject_commands(command_table, session, **kwargs):\n    \"\"\"\n    Inject custom 'aws deploy' commands.\n    \"\"\"\n    command_table['push'] = Push(session)\n    command_table['register'] = Register(session)\n    command_table['deregister'] = Deregister(session)\n    command_table['install'] = Install(session)\n    command_table['uninstall'] = Uninstall(session)\n", "awscli/customizations/codedeploy/__init__.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "awscli/customizations/codedeploy/push.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\nimport sys\nimport zipfile\nimport tempfile\nimport contextlib\nfrom datetime import datetime\n\nfrom botocore.exceptions import ClientError\n\nfrom awscli.customizations.codedeploy.utils import validate_s3_location\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.compat import BytesIO, ZIP_COMPRESSION_MODE\n\n\nONE_MB = 1 << 20\nMULTIPART_LIMIT = 6 * ONE_MB\n\n\nclass Push(BasicCommand):\n    NAME = 'push'\n\n    DESCRIPTION = (\n        'Bundles and uploads to Amazon Simple Storage Service (Amazon S3) an '\n        'application revision, which is a zip archive file that contains '\n        'deployable content and an accompanying Application Specification '\n        'file (AppSpec file). If the upload is successful, a message is '\n        'returned that describes how to call the create-deployment command to '\n        'deploy the application revision from Amazon S3 to target Amazon '\n        'Elastic Compute Cloud (Amazon EC2) instances.'\n    )\n\n    ARG_TABLE = [\n        {\n            'name': 'application-name',\n            'synopsis': '--application-name <app-name>',\n            'required': True,\n            'help_text': (\n                'Required. The name of the AWS CodeDeploy application to be '\n                'associated with the application revision.'\n            )\n        },\n        {\n            'name': 's3-location',\n            'synopsis': '--s3-location s3://<bucket>/<key>',\n            'required': True,\n            'help_text': (\n                'Required. Information about the location of the application '\n                'revision to be uploaded to Amazon S3. You must specify both '\n                'a bucket and a key that represent the Amazon S3 bucket name '\n                'and the object key name. Content will be zipped before '\n                'uploading. Use the format s3://\\<bucket\\>/\\<key\\>'\n            )\n        },\n        {\n            'name': 'ignore-hidden-files',\n            'action': 'store_true',\n            'default': False,\n            'group_name': 'ignore-hidden-files',\n            'help_text': (\n                'Optional. Set the --ignore-hidden-files flag to not bundle '\n                'and upload hidden files to Amazon S3; otherwise, set the '\n                '--no-ignore-hidden-files flag (the default) to bundle and '\n                'upload hidden files to Amazon S3.'\n            )\n        },\n        {\n            'name': 'no-ignore-hidden-files',\n            'action': 'store_true',\n            'default': False,\n            'group_name': 'ignore-hidden-files'\n        },\n        {\n            'name': 'source',\n            'synopsis': '--source <path>',\n            'default': '.',\n            'help_text': (\n                'Optional. The location of the deployable content and the '\n                'accompanying AppSpec file on the development machine to be '\n                'zipped and uploaded to Amazon S3. If not specified, the '\n                'current directory is used.'\n            )\n        },\n        {\n            'name': 'description',\n            'synopsis': '--description <description>',\n            'help_text': (\n                'Optional. A comment that summarizes the application '\n                'revision. If not specified, the default string \"Uploaded by '\n                'AWS CLI \\'time\\' UTC\" is used, where \\'time\\' is the current '\n                'system time in Coordinated Universal Time (UTC).'\n            )\n        }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        self._validate_args(parsed_args)\n        self.codedeploy = self._session.create_client(\n            'codedeploy',\n            region_name=parsed_globals.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl\n        )\n        self.s3 = self._session.create_client(\n            's3',\n            region_name=parsed_globals.region\n        )\n        self._push(parsed_args)\n\n    def _validate_args(self, parsed_args):\n        validate_s3_location(parsed_args, 's3_location')\n        if parsed_args.ignore_hidden_files \\\n                and parsed_args.no_ignore_hidden_files:\n            raise RuntimeError(\n                'You cannot specify both --ignore-hidden-files and '\n                '--no-ignore-hidden-files.'\n            )\n        if not parsed_args.description:\n            parsed_args.description = (\n                'Uploaded by AWS CLI {0} UTC'.format(\n                    datetime.utcnow().isoformat()\n                )\n            )\n\n    def _push(self, params):\n        with self._compress(\n                params.source,\n                params.ignore_hidden_files\n        ) as bundle:\n            try:\n                upload_response = self._upload_to_s3(params, bundle)\n                params.eTag = upload_response['ETag'].replace('\"', \"\")\n                if 'VersionId' in upload_response:\n                    params.version = upload_response['VersionId']\n            except Exception as e:\n                raise RuntimeError(\n                    'Failed to upload \\'%s\\' to \\'%s\\': %s' %\n                    (params.source,\n                     params.s3_location,\n                     str(e))\n                )\n        self._register_revision(params)\n\n        if 'version' in params:\n            version_string = ',version={0}'.format(params.version)\n        else:\n            version_string = ''\n        s3location_string = (\n            '--s3-location bucket={0},key={1},'\n            'bundleType=zip,eTag={2}{3}'.format(\n                params.bucket,\n                params.key,\n                params.eTag,\n                version_string\n            )\n        )\n        sys.stdout.write(\n            'To deploy with this revision, run:\\n'\n            'aws deploy create-deployment '\n            '--application-name {0} {1} '\n            '--deployment-group-name <deployment-group-name> '\n            '--deployment-config-name <deployment-config-name> '\n            '--description <description>\\n'.format(\n                params.application_name,\n                s3location_string\n            )\n        )\n\n    @contextlib.contextmanager\n    def _compress(self, source, ignore_hidden_files=False):\n        source_path = os.path.abspath(source)\n        appspec_path = os.path.sep.join([source_path, 'appspec.yml'])\n        with tempfile.TemporaryFile('w+b') as tf:\n            zf = zipfile.ZipFile(tf, 'w', allowZip64=True)\n            # Using 'try'/'finally' instead of 'with' statement since ZipFile\n            # does not have support context manager in Python 2.6.\n            try:\n                contains_appspec = False\n                for root, dirs, files in os.walk(source, topdown=True):\n                    if ignore_hidden_files:\n                        files = [fn for fn in files if not fn.startswith('.')]\n                        dirs[:] = [dn for dn in dirs if not dn.startswith('.')]\n                    for fn in files:\n                        filename = os.path.join(root, fn)\n                        filename = os.path.abspath(filename)\n                        arcname = filename[len(source_path) + 1:]\n                        if filename == appspec_path:\n                            contains_appspec = True\n                        zf.write(filename, arcname, ZIP_COMPRESSION_MODE)\n                if not contains_appspec:\n                    raise RuntimeError(\n                        '{0} was not found'.format(appspec_path)\n                    )\n            finally:\n                zf.close()\n            yield tf\n\n    def _upload_to_s3(self, params, bundle):\n        size_remaining = self._bundle_size(bundle)\n        if size_remaining < MULTIPART_LIMIT:\n            return self.s3.put_object(\n                Bucket=params.bucket,\n                Key=params.key,\n                Body=bundle\n            )\n        else:\n            return self._multipart_upload_to_s3(\n                params,\n                bundle,\n                size_remaining\n            )\n\n    def _bundle_size(self, bundle):\n        bundle.seek(0, 2)\n        size = bundle.tell()\n        bundle.seek(0)\n        return size\n\n    def _multipart_upload_to_s3(self, params, bundle, size_remaining):\n        create_response = self.s3.create_multipart_upload(\n            Bucket=params.bucket,\n            Key=params.key\n        )\n        upload_id = create_response['UploadId']\n        try:\n            part_num = 1\n            multipart_list = []\n            bundle.seek(0)\n            while size_remaining > 0:\n                data = bundle.read(MULTIPART_LIMIT)\n                upload_response = self.s3.upload_part(\n                    Bucket=params.bucket,\n                    Key=params.key,\n                    UploadId=upload_id,\n                    PartNumber=part_num,\n                    Body=BytesIO(data)\n                )\n                multipart_list.append({\n                    'PartNumber': part_num,\n                    'ETag': upload_response['ETag']\n                })\n                part_num += 1\n                size_remaining -= len(data)\n            return self.s3.complete_multipart_upload(\n                Bucket=params.bucket,\n                Key=params.key,\n                UploadId=upload_id,\n                MultipartUpload={'Parts': multipart_list}\n            )\n        except ClientError as e:\n            self.s3.abort_multipart_upload(\n                Bucket=params.bucket,\n                Key=params.key,\n                UploadId=upload_id\n            )\n            raise e\n\n    def _register_revision(self, params):\n        revision = {\n            'revisionType': 'S3',\n            's3Location': {\n                'bucket': params.bucket,\n                'key': params.key,\n                'bundleType': 'zip',\n                'eTag': params.eTag\n            }\n        }\n        if 'version' in params:\n            revision['s3Location']['version'] = params.version\n        self.codedeploy.register_application_revision(\n            applicationName=params.application_name,\n            revision=revision,\n            description=params.description\n        )\n", "awscli/customizations/history/filters.py": "import re\n\n\nclass RegexFilter(object):\n    def __init__(self, pattern, replacement):\n        self._pattern = pattern\n        self._replacement = replacement\n        self._regex = None\n\n    def filter_text(self, text):\n        regex = self._get_regex()\n        filtered_text = regex.subn(self._replacement, text)\n        return filtered_text[0]\n\n    def _get_regex(self):\n        if self._regex is None:\n            self._regex = re.compile(self._pattern)\n        return self._regex\n", "awscli/customizations/history/db.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport uuid\nimport time\nimport json\nimport datetime\nimport threading\nimport logging\nfrom awscli.compat import collections_abc\n\nfrom botocore.history import BaseHistoryHandler\n\nfrom awscli.compat import sqlite3\nfrom awscli.compat import binary_type\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass DatabaseConnection(object):\n    _CREATE_TABLE = \"\"\"\n        CREATE TABLE IF NOT EXISTS records (\n          id TEXT,\n          request_id TEXT,\n          source TEXT,\n          event_type TEXT,\n          timestamp INTEGER,\n          payload TEXT\n        )\"\"\"\n    _ENABLE_WAL = 'PRAGMA journal_mode=WAL'\n\n    def __init__(self, db_filename):\n        self._connection = sqlite3.connect(\n            db_filename, check_same_thread=False, isolation_level=None)\n        self._ensure_database_setup()\n\n    def close(self):\n        self._connection.close()\n\n    def execute(self, query, *parameters):\n        return self._connection.execute(query, *parameters)\n\n    def _ensure_database_setup(self):\n        self._create_record_table()\n        self._try_to_enable_wal()\n\n    def _create_record_table(self):\n        self.execute(self._CREATE_TABLE)\n\n    def _try_to_enable_wal(self):\n        try:\n            self.execute(self._ENABLE_WAL)\n        except sqlite3.Error:\n            # This is just a performance enhancement so it is optional. Not all\n            # systems will have a sqlite compiled with the WAL enabled.\n            LOG.debug('Failed to enable sqlite WAL.')\n\n    @property\n    def row_factory(self):\n        return self._connection.row_factory\n\n    @row_factory.setter\n    def row_factory(self, row_factory):\n        self._connection.row_factory = row_factory\n\n\nclass PayloadSerializer(json.JSONEncoder):\n    def _encode_mutable_mapping(self, obj):\n        return dict(obj)\n\n    def _encode_datetime(self, obj):\n        return obj.isoformat()\n\n    def _try_decode_bytes(self, obj):\n        try:\n            obj = obj.decode('utf-8')\n        except UnicodeDecodeError:\n            obj = '<Byte sequence>'\n        return obj\n\n    def _remove_non_unicode_stings(self, obj):\n        if isinstance(obj, str):\n            obj = self._try_decode_bytes(obj)\n        elif isinstance(obj, dict):\n            obj = dict((k, self._remove_non_unicode_stings(v)) for k, v\n                       in obj.items())\n        elif isinstance(obj, (list, tuple)):\n            obj = [self._remove_non_unicode_stings(o) for o in obj]\n        return obj\n\n    def encode(self, obj):\n        try:\n            return super(PayloadSerializer, self).encode(obj)\n        except UnicodeDecodeError:\n            # This happens in PY2 in the case where a record payload has some\n            # binary data in it that is not utf-8 encodable. PY2 will not call\n            # the default method on the individual field with bytes in it since\n            # it thinks it can handle it with the normal string serialization\n            # method. Since it cannot tell the difference between a utf-8 str\n            # and a str with raw bytes in it we will get a UnicodeDecodeError\n            # here at the top level. There are no hooks into the serialization\n            # process in PY2 that allow us to fix this behavior, so instead\n            # when we encounter the unicode error we climb the structure\n            # ourselves and replace all strings that are not utf-8 decodable\n            # and try to encode again.\n            scrubbed_obj = self._remove_non_unicode_stings(obj)\n            return super(PayloadSerializer, self).encode(scrubbed_obj)\n\n    def default(self, obj):\n        if isinstance(obj, datetime.datetime):\n            return self._encode_datetime(obj)\n        elif isinstance(obj, collections_abc.MutableMapping):\n            return self._encode_mutable_mapping(obj)\n        elif isinstance(obj, binary_type):\n            # In PY3 the bytes type differs from the str type so the default\n            # method will be called when a bytes object is encountered.\n            # We call the same _try_decode_bytes method that either decodes it\n            # to a utf-8 string and continues serialization, or removes the\n            # value if it is not valid utf-8 string.\n            return self._try_decode_bytes(obj)\n        else:\n            return repr(obj)\n\n\nclass DatabaseRecordWriter(object):\n    _WRITE_RECORD = \"\"\"\n        INSERT INTO records(\n            id, request_id, source, event_type, timestamp, payload)\n        VALUES (?,?,?,?,?,?) \"\"\"\n\n    def __init__(self, connection):\n        self._connection = connection\n        self._lock = threading.Lock()\n\n    def close(self):\n        self._connection.close()\n\n    def write_record(self, record):\n        db_record = self._create_db_record(record)\n        with self._lock:\n            self._connection.execute(self._WRITE_RECORD, db_record)\n\n    def _create_db_record(self, record):\n        event_type = record['event_type']\n        json_serialized_payload = json.dumps(record['payload'],\n                                             cls=PayloadSerializer)\n        db_record = (\n            record['command_id'],\n            record.get('request_id'),\n            record['source'],\n            event_type,\n            record['timestamp'],\n            json_serialized_payload\n        )\n        return db_record\n\n\nclass DatabaseRecordReader(object):\n    _ORDERING = 'ORDER BY timestamp'\n    _GET_LAST_ID_RECORDS = \"\"\"\n        SELECT * FROM records\n        WHERE id =\n        (SELECT id FROM records WHERE timestamp =\n        (SELECT max(timestamp) FROM records)) %s;\"\"\" % _ORDERING\n    _GET_RECORDS_BY_ID = 'SELECT * from records where id = ? %s' % _ORDERING\n    _GET_ALL_RECORDS = (\n        'SELECT a.id AS id_a, '\n        '    b.id AS id_b, '\n        '    a.timestamp as timestamp, '\n        '    a.payload AS args, '\n        '    b.payload AS rc '\n        'FROM records a, records b '\n        'where a.event_type == \"CLI_ARGUMENTS\" AND '\n        '    b.event_type = \"CLI_RC\" AND '\n        '    id_a == id_b '\n        '%s DESC' % _ORDERING\n    )\n\n    def __init__(self, connection):\n        self._connection = connection\n        self._connection.row_factory = self._row_factory\n\n    def close(self):\n        self._connection.close()\n\n    def _row_factory(self, cursor, row):\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            val = row[idx]\n            if col[0] == 'payload':\n                val = json.loads(val)\n            d[col[0]] = val\n        return d\n\n    def iter_latest_records(self):\n        cursor = self._connection.execute(self._GET_LAST_ID_RECORDS)\n        for row in cursor:\n            yield row\n\n    def iter_records(self, record_id):\n        cursor = self._connection.execute(self._GET_RECORDS_BY_ID, [record_id])\n        for row in cursor:\n            yield row\n\n    def iter_all_records(self):\n        cursor = self._connection.execute(self._GET_ALL_RECORDS)\n        for row in cursor:\n            yield row\n\n\nclass RecordBuilder(object):\n    _REQUEST_LIFECYCLE_EVENTS = set(\n        ['API_CALL', 'HTTP_REQUEST', 'HTTP_RESPONSE', 'PARSED_RESPONSE'])\n    _START_OF_REQUEST_LIFECYCLE_EVENT = 'API_CALL'\n\n    def __init__(self):\n        self._identifier = None\n        self._locals = threading.local()\n\n    def _get_current_thread_request_id(self):\n        request_id = getattr(self._locals, 'request_id', None)\n        return request_id\n\n    def _start_http_lifecycle(self):\n        setattr(self._locals, 'request_id', str(uuid.uuid4()))\n\n    def _get_request_id(self, event_type):\n        if event_type == self._START_OF_REQUEST_LIFECYCLE_EVENT:\n            self._start_http_lifecycle()\n        if event_type in self._REQUEST_LIFECYCLE_EVENTS:\n            request_id = self._get_current_thread_request_id()\n            return request_id\n        return None\n\n    def _get_identifier(self):\n        if self._identifier is None:\n            self._identifier = str(uuid.uuid4())\n        return self._identifier\n\n    def build_record(self, event_type, payload, source):\n        uid = self._get_identifier()\n        record = {\n            'command_id': uid,\n            'event_type': event_type,\n            'payload': payload,\n            'source': source,\n            'timestamp': int(time.time() * 1000)\n        }\n        request_id = self._get_request_id(event_type)\n        if request_id:\n            record['request_id'] = request_id\n        return record\n\n\nclass DatabaseHistoryHandler(BaseHistoryHandler):\n    def __init__(self, writer, record_builder):\n        self._writer = writer\n        self._record_builder = record_builder\n\n    def emit(self, event_type, payload, source):\n        record = self._record_builder.build_record(event_type, payload, source)\n        self._writer.write_record(record)\n", "awscli/customizations/history/show.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\nimport json\nimport sys\nimport xml.parsers.expat\nimport xml.dom.minidom\n\nimport colorama\n\nfrom awscli.table import COLORAMA_KWARGS\nfrom awscli.customizations.history.commands import HistorySubcommand\nfrom awscli.customizations.history.filters import RegexFilter\n\n\nclass Formatter(object):\n    def __init__(self, output=None, include=None, exclude=None):\n        \"\"\"Formats and outputs CLI history events\n\n        :type output: File-like obj\n        :param output: The stream to write the formatted event to. By default\n            sys.stdout is used.\n\n        :type include: list\n        :param include: A filter specifying which event to only be displayed.\n            This parameter is mutually exclusive with exclude.\n\n        :type exclude: list\n        :param exclude: A filter specifying which events to exclude from being\n            displayed. This parameter is mutually exclusive with include.\n\n        \"\"\"\n        self._output = output\n        if self._output is None:\n            self._output = sys.stdout\n        if include and exclude:\n            raise ValueError(\n                'Either input or exclude can be provided but not both')\n        self._include = include\n        self._exclude = exclude\n\n    def display(self, event_record):\n        \"\"\"Displays a formatted version of the event record\n\n        :type event_record: dict\n        :param event_record: The event record to format and display.\n        \"\"\"\n        if self._should_display(event_record):\n            self._display(event_record)\n\n    def _display(self, event_record):\n        raise NotImplementedError('_display()')\n\n    def _should_display(self, event_record):\n        if self._include:\n            return event_record['event_type'] in self._include\n        elif self._exclude:\n            return event_record['event_type'] not in self._exclude\n        else:\n            return True\n\n\nclass DetailedFormatter(Formatter):\n    _SIG_FILTER = RegexFilter(\n        'Signature=([a-z0-9]{4})[a-z0-9]{60}',\n        r'Signature=\\1...',\n    )\n\n    _SECTIONS = {\n        'CLI_VERSION': {\n            'title': 'AWS CLI command entered',\n            'values': [\n                {'description': 'with AWS CLI version'}\n            ]\n        },\n        'CLI_ARGUMENTS': {\n            'values': [\n                {'description': 'with arguments'}\n            ]\n        },\n        'API_CALL': {\n            'title': 'API call made',\n            'values': [\n                {\n                    'description': 'to service',\n                    'payload_key': 'service'\n                },\n                {\n                    'description': 'using operation',\n                    'payload_key': 'operation'\n                },\n                {\n                    'description': 'with parameters',\n                    'payload_key': 'params',\n                    'value_format': 'dictionary'\n                }\n            ]\n        },\n        'HTTP_REQUEST': {\n            'title': 'HTTP request sent',\n            'values': [\n                {\n                    'description': 'to URL',\n                    'payload_key': 'url'\n                },\n                {\n                    'description': 'with method',\n                    'payload_key': 'method'\n                },\n                {\n                    'description': 'with headers',\n                    'payload_key': 'headers',\n                    'value_format': 'dictionary',\n                    'filters': [_SIG_FILTER]\n                },\n                {\n                    'description': 'with body',\n                    'payload_key': 'body',\n                    'value_format': 'http_body'\n                }\n\n            ]\n        },\n        'HTTP_RESPONSE': {\n            'title': 'HTTP response received',\n            'values': [\n                {\n                    'description': 'with status code',\n                    'payload_key': 'status_code'\n                },\n                {\n                    'description': 'with headers',\n                    'payload_key': 'headers',\n                    'value_format': 'dictionary'\n                },\n                {\n                    'description': 'with body',\n                    'payload_key': 'body',\n                    'value_format': 'http_body'\n                }\n            ]\n        },\n        'PARSED_RESPONSE': {\n            'title': 'HTTP response parsed',\n            'values': [\n                {\n                    'description': 'parsed to',\n                    'value_format': 'dictionary'\n                }\n            ]\n        },\n        'CLI_RC': {\n            'title': 'AWS CLI command exited',\n            'values': [\n                {'description': 'with return code'}\n            ]\n        },\n    }\n\n    _COMPONENT_COLORS = {\n        'title': colorama.Style.BRIGHT,\n        'description': colorama.Fore.CYAN\n    }\n\n    def __init__(self, output=None, include=None, exclude=None, colorize=True):\n        super(DetailedFormatter, self).__init__(output, include, exclude)\n        self._request_id_to_api_num = {}\n        self._num_api_calls = 0\n        self._colorize = colorize\n        self._value_pformatter = SectionValuePrettyFormatter()\n        if self._colorize:\n            colorama.init(**COLORAMA_KWARGS)\n\n    def _display(self, event_record):\n        section_definition = self._SECTIONS.get(event_record['event_type'])\n        if section_definition is not None:\n            self._display_section(event_record, section_definition)\n\n    def _display_section(self, event_record, section_definition):\n        if 'title' in section_definition:\n            self._display_title(section_definition['title'], event_record)\n        for value_definition in section_definition['values']:\n            self._display_value(value_definition, event_record)\n\n    def _display_title(self, title, event_record):\n        formatted_title = self._format_section_title(title, event_record)\n        self._write_output(formatted_title)\n\n    def _display_value(self, value_definition, event_record):\n        value_description = value_definition['description']\n        event_record_payload = event_record['payload']\n        value = event_record_payload\n        if 'payload_key' in value_definition:\n            value = event_record_payload[value_definition['payload_key']]\n        formatted_value = self._format_description(value_description)\n        formatted_value += self._format_value(\n            value, event_record, value_definition.get('value_format')\n        )\n        if 'filters' in value_definition:\n            for text_filter in value_definition['filters']:\n                formatted_value = text_filter.filter_text(formatted_value)\n        self._write_output(formatted_value)\n\n    def _write_output(self, content):\n        if isinstance(content, str):\n            content = content.encode('utf-8')\n        self._output.write(content)\n\n    def _format_section_title(self, title, event_record):\n        formatted_title = title\n        api_num = self._get_api_num(event_record)\n        if api_num is not None:\n            formatted_title = ('[%s] ' % api_num) + formatted_title\n        formatted_title = self._color_if_configured(formatted_title, 'title')\n        formatted_title += '\\n'\n\n        formatted_timestamp = self._format_description('at time')\n        formatted_timestamp += self._format_value(\n            event_record['timestamp'], event_record, value_format='timestamp')\n\n        return '\\n' + formatted_title + formatted_timestamp\n\n    def _get_api_num(self, event_record):\n        request_id = event_record['request_id']\n        if request_id:\n            if request_id not in self._request_id_to_api_num:\n                self._request_id_to_api_num[\n                    request_id] = self._num_api_calls\n                self._num_api_calls += 1\n            return self._request_id_to_api_num[request_id]\n\n    def _format_description(self, value_description):\n        return self._color_if_configured(\n            value_description + ': ', 'description')\n\n    def _format_value(self, value, event_record, value_format=None):\n        if value_format:\n            formatted_value = self._value_pformatter.pformat(\n                value, value_format, event_record)\n        else:\n            formatted_value = str(value)\n        return formatted_value + '\\n'\n\n    def _color_if_configured(self, text, component):\n        if self._colorize:\n            color = self._COMPONENT_COLORS[component]\n            return color + text + colorama.Style.RESET_ALL\n        return text\n\n\nclass SectionValuePrettyFormatter(object):\n    def pformat(self, value, value_format, event_record):\n        return getattr(self, '_pformat_' + value_format)(value, event_record)\n\n    def _pformat_timestamp(self, event_timestamp, event_record=None):\n        return datetime.datetime.fromtimestamp(\n            event_timestamp/1000.0).strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n\n    def _pformat_dictionary(self, obj, event_record=None):\n        return json.dumps(obj=obj, sort_keys=True, indent=4)\n\n    def _pformat_http_body(self, body, event_record):\n        if not body:\n            return 'There is no associated body'\n        elif event_record['payload'].get('streaming', False):\n            return 'The body is a stream and will not be displayed'\n        elif self._is_xml(body):\n            # TODO: Figure out a way to minimize the number of times we have\n            # to parse the XML. Currently at worst, it will take three times.\n            # One to determine if it is XML, another to strip whitespace, and\n            # a third to convert to make it pretty. This is an issue as it\n            # can cause issues when there are large XML payloads such as\n            # an s3 ListObjects call.\n            return self._get_pretty_xml(body)\n        elif self._is_json_structure(body):\n            return self._get_pretty_json(body)\n        else:\n            return body\n\n    def _get_pretty_xml(self, body):\n        # The body is parsed and whitespace is stripped because some services\n        # like ec2 already return pretty XML and if toprettyxml() was applied\n        # to it, it will add even more newlines and spaces on top of it.\n        # So this just removes all whitespace from the start to prevent the\n        # chance of adding to much newlines and spaces when toprettyxml()\n        # is called.\n        stripped_body = self._strip_whitespace(body)\n        xml_dom = xml.dom.minidom.parseString(stripped_body)\n        return xml_dom.toprettyxml(indent=' '*4, newl='\\n')\n\n    def _get_pretty_json(self, body):\n        # The json body is loaded so it can be dumped in a format that\n        # is desired.\n        obj = json.loads(body)\n        return self._pformat_dictionary(obj)\n\n    def _is_xml(self, body):\n        try:\n            xml.dom.minidom.parseString(body)\n        except xml.parsers.expat.ExpatError:\n            return False\n        return True\n\n    def _strip_whitespace(self, xml_string):\n        xml_dom = xml.dom.minidom.parseString(xml_string)\n        return ''.join(\n            [line.strip() for line in xml_dom.toxml().splitlines()]\n        )\n\n    def _is_json_structure(self, body):\n        if body.startswith('{'):\n            try:\n                json.loads(body)\n                return True\n            except json.decoder.JSONDecodeError:\n                return False\n        return False\n\n\nclass ShowCommand(HistorySubcommand):\n    NAME = 'show'\n    DESCRIPTION = (\n        'Shows the various events related to running a specific CLI command. '\n        'If this command is ran without any positional arguments, it will '\n        'display the events for the last CLI command ran.'\n    )\n    FORMATTERS = {\n        'detailed': DetailedFormatter\n    }\n    ARG_TABLE = [\n        {'name': 'command_id', 'nargs': '?', 'default': 'latest',\n         'positional_arg': True,\n         'help_text': (\n             'The ID of the CLI command to show. If this positional argument '\n             'is omitted, it will show the last the CLI command ran.')},\n        {'name': 'include', 'nargs': '+',\n         'help_text': (\n             'Specifies which events to **only** include when showing the '\n             'CLI command. This argument is mutually exclusive with '\n             '``--exclude``.')},\n        {'name': 'exclude', 'nargs': '+',\n         'help_text': (\n             'Specifies which events to exclude when showing the '\n             'CLI command. This argument is mutually exclusive with '\n             '``--include``.')},\n        {'name': 'format', 'choices': FORMATTERS.keys(),\n         'default': 'detailed', 'help_text': (\n            'Specifies which format to use in showing the events for '\n            'the specified CLI command. The following formats are '\n            'supported:\\n\\n'\n            '<ul>'\n            '<li> detailed - This the default format. It prints out a '\n            'detailed overview of the CLI command ran. It displays all '\n            'of the key events in the command lifecycle where each '\n            'important event has a title and its important values '\n            'underneath. The events are ordered by timestamp and events of '\n            'the same API call are associated together with the '\n            '[``api_id``] notation where events that share the same '\n            '``api_id`` belong to the lifecycle of the same API call.'\n            '</li>'\n            '</ul>'\n            )\n         }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        self._connect_to_history_db()\n        try:\n            self._validate_args(parsed_args)\n            with self._get_output_stream() as output_stream:\n                formatter = self._get_formatter(\n                    parsed_args, parsed_globals, output_stream)\n                for record in self._get_record_iterator(parsed_args):\n                    formatter.display(record)\n        finally:\n            self._close_history_db()\n        return 0\n\n    def _validate_args(self, parsed_args):\n        if parsed_args.exclude and parsed_args.include:\n            raise ValueError(\n                'Either --exclude or --include can be provided but not both')\n\n    def _get_formatter(self, parsed_args, parsed_globals, output_stream):\n        format_type = parsed_args.format\n        formatter_kwargs = {\n            'include': parsed_args.include,\n            'exclude': parsed_args.exclude,\n            'output': output_stream\n        }\n        if format_type == 'detailed':\n            formatter_kwargs['colorize'] = self._should_use_color(\n                parsed_globals)\n        return self.FORMATTERS[format_type](**formatter_kwargs)\n\n    def _get_record_iterator(self, parsed_args):\n        if parsed_args.command_id == 'latest':\n            return self._db_reader.iter_latest_records()\n        else:\n            return self._db_reader.iter_records(parsed_args.command_id)\n", "awscli/customizations/history/constants.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\n\nHISTORY_FILENAME_ENV_VAR = 'AWS_CLI_HISTORY_FILE'\nDEFAULT_HISTORY_FILENAME = os.path.expanduser(\n    os.path.join('~', '.aws', 'cli', 'history', 'history.db'))\n", "awscli/customizations/history/commands.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nfrom awscli.compat import is_windows\nfrom awscli.utils import is_a_tty\nfrom awscli.utils import OutputStreamFactory\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.history.db import DatabaseConnection\nfrom awscli.customizations.history.constants import HISTORY_FILENAME_ENV_VAR\nfrom awscli.customizations.history.constants import DEFAULT_HISTORY_FILENAME\nfrom awscli.customizations.history.db import DatabaseRecordReader\n\n\nclass HistorySubcommand(BasicCommand):\n    def __init__(self, session, db_reader=None, output_stream_factory=None):\n        super(HistorySubcommand, self).__init__(session)\n        self._db_reader = db_reader\n        self._output_stream_factory = output_stream_factory\n        if output_stream_factory is None:\n            self._output_stream_factory = OutputStreamFactory()\n\n    def _connect_to_history_db(self):\n        if self._db_reader is None:\n            connection = DatabaseConnection(self._get_history_db_filename())\n            self._db_reader = DatabaseRecordReader(connection)\n\n    def _close_history_db(self):\n        self._db_reader.close()\n\n    def _get_history_db_filename(self):\n        filename = os.environ.get(\n            HISTORY_FILENAME_ENV_VAR, DEFAULT_HISTORY_FILENAME)\n        if not os.path.exists(filename):\n            raise RuntimeError(\n                'Could not locate history. Make sure cli_history is set to '\n                'enabled in the ~/.aws/config file'\n            )\n        return filename\n\n    def _should_use_color(self, parsed_globals):\n        if parsed_globals.color == 'on':\n            return True\n        elif parsed_globals.color == 'off':\n            return False\n        return is_a_tty() and not is_windows\n\n    def _get_output_stream(self, preferred_pager=None):\n        if is_a_tty():\n            return self._output_stream_factory.get_pager_stream(\n                preferred_pager)\n        return self._output_stream_factory.get_stdout_stream()\n", "awscli/customizations/history/__init__.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport sys\nimport logging\n\nfrom botocore.history import get_global_history_recorder\nfrom botocore.exceptions import ProfileNotFound\n\nfrom awscli.compat import sqlite3\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.history.constants import HISTORY_FILENAME_ENV_VAR\nfrom awscli.customizations.history.constants import DEFAULT_HISTORY_FILENAME\nfrom awscli.customizations.history.db import DatabaseConnection\nfrom awscli.customizations.history.db import DatabaseRecordWriter\nfrom awscli.customizations.history.db import RecordBuilder\nfrom awscli.customizations.history.db import DatabaseHistoryHandler\nfrom awscli.customizations.history.show import ShowCommand\nfrom awscli.customizations.history.list import ListCommand\n\n\nLOG = logging.getLogger(__name__)\nHISTORY_RECORDER = get_global_history_recorder()\n\n\ndef register_history_mode(event_handlers):\n    event_handlers.register(\n        'session-initialized', attach_history_handler)\n\n\ndef register_history_commands(event_handlers):\n    event_handlers.register(\n        \"building-command-table.main\", add_history_commands)\n\n\ndef attach_history_handler(session, parsed_args, **kwargs):\n    if _should_enable_cli_history(session, parsed_args):\n        LOG.debug('Enabling CLI history')\n\n        history_filename = os.environ.get(\n            HISTORY_FILENAME_ENV_VAR, DEFAULT_HISTORY_FILENAME)\n        if not os.path.isdir(os.path.dirname(history_filename)):\n            os.makedirs(os.path.dirname(history_filename))\n\n        connection = DatabaseConnection(history_filename)\n        writer = DatabaseRecordWriter(connection)\n        record_builder = RecordBuilder()\n        db_handler = DatabaseHistoryHandler(writer, record_builder)\n\n        HISTORY_RECORDER.add_handler(db_handler)\n        HISTORY_RECORDER.enable()\n\n\ndef _should_enable_cli_history(session, parsed_args):\n    if parsed_args.command == 'history':\n        return False\n    try:\n        scoped_config = session.get_scoped_config()\n    except ProfileNotFound:\n        # If the profile does not exist, cli history is definitely not\n        # enabled, but don't let the error get propagated as commands down\n        # the road may handle this such as the configure set command with\n        # a --profile flag set.\n        return False\n    has_history_enabled = scoped_config.get('cli_history') == 'enabled'\n    if has_history_enabled and sqlite3 is None:\n        if has_history_enabled:\n            sys.stderr.write(\n                'cli_history is enabled but sqlite3 is unavailable. '\n                'Unable to collect CLI history.\\n'\n            )\n        return False\n    return has_history_enabled\n\n\ndef add_history_commands(command_table, session, **kwargs):\n    command_table['history'] = HistoryCommand(session)\n\n\nclass HistoryCommand(BasicCommand):\n    NAME = 'history'\n    DESCRIPTION = (\n        'Commands to interact with the history of AWS CLI commands ran '\n        'over time. To record the history of AWS CLI commands set '\n        '``cli_history`` to ``enabled`` in the ``~/.aws/config`` file. '\n        'This can be done by running:\\n\\n'\n        '``$ aws configure set cli_history enabled``'\n    )\n    SUBCOMMANDS = [\n        {'name': 'show', 'command_class': ShowCommand},\n        {'name': 'list', 'command_class': ListCommand}\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        if parsed_args.subcommand is None:\n            raise ValueError(\"usage: aws [options] <command> <subcommand> \"\n                             \"[parameters]\\naws: error: too few arguments\")\n", "awscli/customizations/history/list.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport datetime\n\nfrom awscli.compat import default_pager\nfrom awscli.customizations.history.commands import HistorySubcommand\n\n\nclass ListCommand(HistorySubcommand):\n    NAME = 'list'\n    DESCRIPTION = (\n        'Shows a list of previously run commands and their command_ids. '\n        'Each row shows only a bare minimum of details including the '\n        'command_id, date, arguments and return code. You can use the '\n        '``history show`` with the command_id to see more details about '\n        'a particular entry.'\n    )\n    _COL_WIDTHS = {\n        'id_a': 38,\n        'timestamp': 24,\n        'args': 50,\n        'rc': 0\n    }\n\n    def _run_main(self, parsed_args, parsed_globals):\n        self._connect_to_history_db()\n        try:\n            raw_records = self._db_reader.iter_all_records()\n            records = RecordAdapter(raw_records)\n            if not records.has_next():\n                raise RuntimeError(\n                    'No commands were found in your history. Make sure you have '\n                    'enabled history mode by adding \"cli_history = enabled\" '\n                    'to the config file.')\n\n            preferred_pager = self._get_preferred_pager()\n            with self._get_output_stream(preferred_pager) as output_stream:\n                formatter = TextFormatter(self._COL_WIDTHS, output_stream)\n                formatter(records)\n        finally:\n            self._close_history_db()\n        return 0\n\n    def _get_preferred_pager(self):\n        preferred_pager = default_pager\n        if preferred_pager.startswith('less'):\n            preferred_pager = 'less -SR'\n        return preferred_pager\n\n\nclass RecordAdapter(object):\n    \"\"\"This class is just to read one ahead to make sure there are records\n\n    If there are no records we can just exit early.\n    \"\"\"\n    def __init__(self, records):\n        self._records = records\n        self._next = None\n        self._advance()\n\n    def has_next(self):\n        return self._next is not None\n\n    def _advance(self):\n        try:\n            self._next = next(self._records)\n        except StopIteration:\n            self._next = None\n\n    def __iter__(self):\n        while self.has_next():\n            yield self._next\n            self._advance()\n\n\nclass TextFormatter(object):\n    def __init__(self, col_widths, output_stream):\n        self._col_widths = col_widths\n        self._output_stream = output_stream\n\n    def _format_time(self, timestamp):\n        command_time = datetime.datetime.fromtimestamp(timestamp / 1000)\n        formatted = datetime.datetime.strftime(\n            command_time, '%Y-%m-%d %I:%M:%S %p')\n        return formatted\n\n    def _format_args(self, args, arg_width):\n        json_value = json.loads(args)\n        formatted = ' '.join(json_value[:2])\n        if len(formatted) >= arg_width:\n            formatted = '%s...' % formatted[:arg_width-4]\n        return formatted\n\n    def _format_record(self, record):\n        fmt_string = \"{0:<%s}{1:<%s}{2:<%s}{3}\\n\" % (\n            self._col_widths['id_a'],\n            self._col_widths['timestamp'],\n            self._col_widths['args']\n        )\n        record_line = fmt_string.format(\n            record['id_a'],\n            self._format_time(record['timestamp']),\n            self._format_args(record['args'], self._col_widths['args']),\n            record['rc']\n        )\n        return record_line\n\n    def __call__(self, record_adapter):\n        for record in record_adapter:\n            formatted_record = self._format_record(record)\n            self._output_stream.write(formatted_record.encode('utf-8'))\n", "awscli/customizations/codeartifact/login.py": "import errno\nimport os\nimport platform\nimport sys\nimport subprocess\nimport re\n\nfrom datetime import datetime\nfrom dateutil.tz import tzutc\nfrom dateutil.relativedelta import relativedelta\nfrom botocore.utils import parse_timestamp\n\nfrom awscli.compat import (\n    is_windows, urlparse, RawConfigParser, StringIO,\n    get_stderr_encoding, is_macos\n)\nfrom awscli.customizations import utils as cli_utils\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.utils import uni_print\n\n\ndef get_relative_expiration_time(remaining):\n    values = []\n    prev_non_zero_attr = False\n    for attr in [\"years\", \"months\", \"days\", \"hours\", \"minutes\"]:\n        value = getattr(remaining, attr)\n        if value > 0:\n            if prev_non_zero_attr:\n                values.append(\"and\")\n            values.append(str(value))\n            values.append(attr[:-1] if value == 1 else attr)\n        if prev_non_zero_attr:\n            break\n        prev_non_zero_attr = value > 0\n\n    message = \" \".join(values)\n    return message\n\n\nclass CommandFailedError(Exception):\n    def __init__(self, called_process_error):\n        msg = str(called_process_error)\n        if called_process_error.stderr is not None:\n            msg +=(\n                f' Stderr from command:\\n'\n                f'{called_process_error.stderr.decode(get_stderr_encoding())}'\n            )\n        Exception.__init__(self, msg)\n\n\nclass BaseLogin(object):\n    _TOOL_NOT_FOUND_MESSAGE = '%s was not found. Please verify installation.'\n\n    def __init__(self, auth_token, expiration, repository_endpoint,\n                 domain, repository, subprocess_utils, namespace=None):\n        self.auth_token = auth_token\n        self.expiration = expiration\n        self.repository_endpoint = repository_endpoint\n        self.domain = domain\n        self.repository = repository\n        self.subprocess_utils = subprocess_utils\n        self.namespace = namespace\n\n    def login(self, dry_run=False):\n        raise NotImplementedError('login()')\n\n    def _dry_run_commands(self, tool, commands):\n        for command in commands:\n            sys.stdout.write(' '.join(command))\n            sys.stdout.write(os.linesep)\n            sys.stdout.write(os.linesep)\n\n    def _write_success_message(self, tool):\n        # add extra 30 seconds make expiration more reasonable\n        # for some corner case\n        # e.g. 11 hours 59 minutes 31 seconds should output --> 12 hours.\n        remaining = relativedelta(\n            self.expiration, datetime.now(tzutc())) + relativedelta(seconds=30)\n        expiration_message = get_relative_expiration_time(remaining)\n\n        sys.stdout.write('Successfully configured {} to use '\n                         'AWS CodeArtifact repository {} '\n                         .format(tool, self.repository_endpoint))\n        sys.stdout.write(os.linesep)\n        sys.stdout.write('Login expires in {} at {}'.format(\n            expiration_message, self.expiration))\n        sys.stdout.write(os.linesep)\n\n    def _run_commands(self, tool, commands, dry_run=False):\n        if dry_run:\n            self._dry_run_commands(tool, commands)\n            return\n\n        for command in commands:\n            self._run_command(tool, command)\n\n        self._write_success_message(tool)\n\n    def _run_command(self, tool, command, *, ignore_errors=False):\n        try:\n            self.subprocess_utils.run(\n                command,\n                capture_output=True,\n                check=True\n            )\n        except subprocess.CalledProcessError as ex:\n            if not ignore_errors:\n                raise CommandFailedError(ex)\n        except OSError as ex:\n            if ex.errno == errno.ENOENT:\n                raise ValueError(\n                    self._TOOL_NOT_FOUND_MESSAGE % tool\n                )\n            raise ex\n\n    @classmethod\n    def get_commands(cls, endpoint, auth_token, **kwargs):\n        raise NotImplementedError('get_commands()')\n\n\nclass SwiftLogin(BaseLogin):\n\n    DEFAULT_NETRC_FMT = \\\n        u'machine {hostname} login token password {auth_token}'\n\n    NETRC_REGEX_FMT = \\\n        r'(?P<entry_start>\\bmachine\\s+{escaped_hostname}\\s+login\\s+\\S+\\s+password\\s+)' \\\n        r'(?P<token>\\S+)'\n\n    def login(self, dry_run=False):\n        scope = self.get_scope(\n            self.namespace\n        )\n        commands = self.get_commands(\n            self.repository_endpoint, self.auth_token, scope=scope\n        )\n\n        if not is_macos:\n            hostname = urlparse.urlparse(self.repository_endpoint).hostname\n            new_entry = self.DEFAULT_NETRC_FMT.format(\n                hostname=hostname,\n                auth_token=self.auth_token\n            )\n            if dry_run:\n                self._display_new_netrc_entry(new_entry, self.get_netrc_path())\n            else:\n                self._update_netrc_entry(hostname, new_entry, self.get_netrc_path())\n\n        self._run_commands('swift', commands, dry_run)\n\n    def _display_new_netrc_entry(self, new_entry, netrc_path):\n        sys.stdout.write('Dryrun mode is enabled, not writing to netrc.')\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(\n            f'The following line would have been written to {netrc_path}:'\n        )\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(new_entry)\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(os.linesep)\n        sys.stdout.write('And would have run the following commands:')\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(os.linesep)\n\n    def _update_netrc_entry(self, hostname, new_entry, netrc_path):\n        pattern = re.compile(\n            self.NETRC_REGEX_FMT.format(escaped_hostname=re.escape(hostname)),\n            re.M\n        )\n        if not os.path.isfile(netrc_path):\n            self._create_netrc_file(netrc_path, new_entry)\n        else:\n            with open(netrc_path, 'r') as f:\n                contents = f.read()\n            escaped_auth_token = self.auth_token.replace('\\\\', r'\\\\')\n            new_contents = re.sub(\n                pattern,\n                rf\"\\g<entry_start>{escaped_auth_token}\",\n                contents\n            )\n\n            if new_contents == contents:\n                new_contents = self._append_netrc_entry(new_contents, new_entry)\n\n            with open(netrc_path, 'w') as f:\n                f.write(new_contents)\n\n    def _create_netrc_file(self, netrc_path, new_entry):\n        dirname = os.path.split(netrc_path)[0]\n        if not os.path.isdir(dirname):\n            os.makedirs(dirname)\n        with os.fdopen(os.open(netrc_path,\n                               os.O_WRONLY | os.O_CREAT, 0o600), 'w') as f:\n            f.write(new_entry + '\\n')\n\n    def _append_netrc_entry(self, contents, new_entry):\n        if contents.endswith('\\n'):\n            return contents + new_entry + '\\n'\n        else:\n            return contents + '\\n' + new_entry + '\\n'\n\n    @classmethod\n    def get_netrc_path(cls):\n        return os.path.join(os.path.expanduser(\"~\"), \".netrc\")\n\n    @classmethod\n    def get_scope(cls, namespace):\n        # Regex for valid scope name\n        valid_scope_name = re.compile(\n            r'\\A[a-zA-Z0-9](?:[a-zA-Z0-9]|-(?=[a-zA-Z0-9])){0,38}\\Z'\n        )\n\n        if namespace is None:\n            return namespace\n\n        if not valid_scope_name.match(namespace):\n            raise ValueError(\n                'Invalid scope name, scope must contain URL-safe '\n                'characters, no leading dots or underscores and no '\n                'more than 39 characters'\n            )\n\n        return namespace\n\n    @classmethod\n    def get_commands(cls, endpoint, auth_token, **kwargs):\n        commands = []\n        scope = kwargs.get('scope')\n\n        # Set up the codeartifact repository as the swift registry.\n        set_registry_command = [\n            'swift', 'package-registry', 'set', endpoint\n        ]\n        if scope is not None:\n            set_registry_command.extend(['--scope', scope])\n        commands.append(set_registry_command)\n\n        # Authenticate against the repository.\n        # We will write token to .netrc for Linux and Windows\n        # MacOS will store the token from command line option to Keychain\n        login_registry_command = [\n            'swift', 'package-registry', 'login', f'{endpoint}login'\n        ]\n        if is_macos:\n            login_registry_command.extend(['--token', auth_token])\n        commands.append(login_registry_command)\n\n        return commands\n\n\nclass NuGetBaseLogin(BaseLogin):\n    _NUGET_INDEX_URL_FMT = '{endpoint}v3/index.json'\n\n    # When adding new sources we can specify that we added the source to the\n    # user level NuGet.Config file. However, when updating an existing source\n    # we cannot be specific about which level NuGet.Config file was updated\n    # because it is possible that the existing source was not in the user\n    # level NuGet.Config. The source listing command returns all configured\n    # sources from all NuGet.Config levels. The update command updates the\n    # source in whichever NuGet.Config file the source was found.\n    _SOURCE_ADDED_MESSAGE = 'Added source %s to the user level NuGet.Config\\n'\n    _SOURCE_UPDATED_MESSAGE = 'Updated source %s in the NuGet.Config\\n'\n    # Example line the below regex should match:\n    # 1.  nuget.org [Enabled]\n    _SOURCE_REGEX = re.compile(r'^\\d+\\.\\s(?P<source_name>.+)\\s\\[.*\\]')\n\n    def login(self, dry_run=False):\n        try:\n            source_to_url_dict = self._get_source_to_url_dict()\n        except OSError as ex:\n            if ex.errno == errno.ENOENT:\n                raise ValueError(\n                    self._TOOL_NOT_FOUND_MESSAGE % self._get_tool_name()\n                )\n            raise ex\n\n        nuget_index_url = self._NUGET_INDEX_URL_FMT.format(\n            endpoint=self.repository_endpoint\n        )\n        source_name, already_exists = self._get_source_name(\n            nuget_index_url, source_to_url_dict\n        )\n\n        if already_exists:\n            command = self._get_configure_command(\n                'update', nuget_index_url, source_name\n            )\n            source_configured_message = self._SOURCE_UPDATED_MESSAGE\n        else:\n            command = self._get_configure_command('add', nuget_index_url, source_name)\n            source_configured_message = self._SOURCE_ADDED_MESSAGE\n\n        if dry_run:\n            dry_run_command = ' '.join([str(cd) for cd in command])\n            uni_print(dry_run_command)\n            uni_print('\\n')\n            return\n\n        try:\n            self.subprocess_utils.run(\n                command,\n                capture_output=True,\n                check=True\n            )\n        except subprocess.CalledProcessError as e:\n            uni_print('Failed to update the NuGet.Config\\n')\n            raise CommandFailedError(e)\n\n        uni_print(source_configured_message % source_name)\n        self._write_success_message('nuget')\n\n    def _get_source_to_url_dict(self):\n        \"\"\"\n        Parses the output of the nuget sources list command.\n\n        A dict is created where the keys are the source names\n        and the values the corresponding URL.\n\n        The output of the command can contain header and footer information\n        around the 'Registered Sources' section, which is ignored.\n\n        Example output that is parsed:\n\n        Registered Sources:\n\n        1. Source Name 1 [Enabled]\n           https://source1.com/index.json\n        2. Source Name 2 [Disabled]\n           https://source2.com/index.json\n        100. Source Name 100 [Activ\u00e9]\n             https://source100.com/index.json\n        \"\"\"\n        response = self.subprocess_utils.check_output(\n            self._get_list_command(),\n            stderr=self.subprocess_utils.PIPE\n        )\n\n        lines = response.decode(os.device_encoding(1) or \"utf-8\").splitlines()\n        lines = [line for line in lines if line.strip() != '']\n\n        source_to_url_dict = {}\n        for i in range(len(lines)):\n            result = self._SOURCE_REGEX.match(lines[i].strip())\n            if result:\n                source_to_url_dict[result[\"source_name\"].strip()] = \\\n                    lines[i + 1].strip()\n\n        return source_to_url_dict\n\n    def _get_source_name(self, codeartifact_url, source_dict):\n        default_name = '{}/{}'.format(self.domain, self.repository)\n\n        # Check if the CodeArtifact URL is already present in the\n        # NuGet.Config file. If the URL already exists, use the source name\n        # already assigned to the CodeArtifact URL.\n        for source_name, source_url in source_dict.items():\n            if source_url == codeartifact_url:\n                return source_name, True\n\n        # If the CodeArtifact URL is not present in the NuGet.Config file,\n        # check if the default source name already exists so we can know\n        # whether we need to add a new entry or update the existing entry.\n        for source_name in source_dict.keys():\n            if source_name == default_name:\n                return source_name, True\n\n        # If neither the source url nor the source name already exist in the\n        # NuGet.Config file, use the default source name.\n        return default_name, False\n\n    def _get_tool_name(self):\n        raise NotImplementedError('_get_tool_name()')\n\n    def _get_list_command(self):\n        raise NotImplementedError('_get_list_command()')\n\n    def _get_configure_command(self, operation, nuget_index_url, source_name):\n        raise NotImplementedError('_get_configure_command()')\n\n\nclass NuGetLogin(NuGetBaseLogin):\n\n    def _get_tool_name(self):\n        return 'nuget'\n\n    def _get_list_command(self):\n        return ['nuget', 'sources', 'list', '-format', 'detailed']\n\n    def _get_configure_command(self, operation, nuget_index_url, source_name):\n        return [\n            'nuget', 'sources', operation,\n            '-name', source_name,\n            '-source', nuget_index_url,\n            '-username', 'aws',\n            '-password', self.auth_token\n        ]\n\n\nclass DotNetLogin(NuGetBaseLogin):\n\n    def _get_tool_name(self):\n        return 'dotnet'\n\n    def _get_list_command(self):\n        return ['dotnet', 'nuget', 'list', 'source', '--format', 'detailed']\n\n    def _get_configure_command(self, operation, nuget_index_url, source_name):\n        command = ['dotnet', 'nuget', operation, 'source']\n\n        if operation == 'add':\n            command.append(nuget_index_url)\n            command += ['--name', source_name]\n        else:\n            command.append(source_name)\n            command += ['--source', nuget_index_url]\n\n        command += [\n            '--username', 'aws',\n            '--password', self.auth_token\n        ]\n\n        # Encryption is not supported on non-Windows platforms.\n        if not is_windows:\n            command.append('--store-password-in-clear-text')\n\n        return command\n\n\nclass NpmLogin(BaseLogin):\n\n    # On Windows we need to be explicit about the .cmd file to execute\n    # (unless we execute through the shell, i.e. with shell=True).\n    NPM_CMD = 'npm.cmd' if platform.system().lower() == 'windows' else 'npm'\n\n    def login(self, dry_run=False):\n        scope = self.get_scope(\n            self.namespace\n        )\n        commands = self.get_commands(\n            self.repository_endpoint, self.auth_token, scope=scope\n        )\n        self._run_commands('npm', commands, dry_run)\n\n    def _run_command(self, tool, command):\n        ignore_errors = any('always-auth' in arg for arg in command)\n        super()._run_command(tool, command, ignore_errors=ignore_errors)\n\n    @classmethod\n    def get_scope(cls, namespace):\n        # Regex for valid scope name\n        valid_scope_name = re.compile('^(@[a-z0-9-~][a-z0-9-._~]*)')\n\n        if namespace is None:\n            return namespace\n\n        # Add @ prefix to scope if it doesn't exist\n        if namespace.startswith('@'):\n            scope = namespace\n        else:\n            scope = '@{}'.format(namespace)\n\n        if not valid_scope_name.match(scope):\n            raise ValueError(\n                'Invalid scope name, scope must contain URL-safe '\n                'characters, no leading dots or underscores'\n            )\n\n        return scope\n\n    @classmethod\n    def get_commands(cls, endpoint, auth_token, **kwargs):\n        commands = []\n        scope = kwargs.get('scope')\n\n        # prepend scope if it exists\n        registry = '{}:registry'.format(scope) if scope else 'registry'\n\n        # set up the codeartifact repository as the npm registry.\n        commands.append(\n            [cls.NPM_CMD, 'config', 'set', registry, endpoint]\n        )\n\n        repo_uri = urlparse.urlsplit(endpoint)\n\n        # configure npm to always require auth for the repository.\n        always_auth_config = '//{}{}:always-auth'.format(\n            repo_uri.netloc, repo_uri.path\n        )\n        commands.append(\n            [cls.NPM_CMD, 'config', 'set', always_auth_config, 'true']\n        )\n\n        # set auth info for the repository.\n        auth_token_config = '//{}{}:_authToken'.format(\n            repo_uri.netloc, repo_uri.path\n        )\n        commands.append(\n            [cls.NPM_CMD, 'config', 'set', auth_token_config, auth_token]\n        )\n\n        return commands\n\n\nclass PipLogin(BaseLogin):\n\n    PIP_INDEX_URL_FMT = '{scheme}://aws:{auth_token}@{netloc}{path}simple/'\n\n    def login(self, dry_run=False):\n        commands = self.get_commands(\n            self.repository_endpoint, self.auth_token\n        )\n        self._run_commands('pip', commands, dry_run)\n\n    @classmethod\n    def get_commands(cls, endpoint, auth_token, **kwargs):\n        repo_uri = urlparse.urlsplit(endpoint)\n        pip_index_url = cls.PIP_INDEX_URL_FMT.format(\n            scheme=repo_uri.scheme,\n            auth_token=auth_token,\n            netloc=repo_uri.netloc,\n            path=repo_uri.path\n        )\n\n        return [['pip', 'config', 'set', 'global.index-url', pip_index_url]]\n\n\nclass TwineLogin(BaseLogin):\n\n    DEFAULT_PYPI_RC_FMT = u'''\\\n[distutils]\nindex-servers=\n    pypi\n    codeartifact\n\n[codeartifact]\nrepository: {repository_endpoint}\nusername: aws\npassword: {auth_token}'''\n\n    def __init__(\n        self,\n        auth_token,\n        expiration,\n        repository_endpoint,\n        domain,\n        repository,\n        subprocess_utils,\n        pypi_rc_path=None\n    ):\n        if pypi_rc_path is None:\n            pypi_rc_path = self.get_pypi_rc_path()\n        self.pypi_rc_path = pypi_rc_path\n        super(TwineLogin, self).__init__(\n            auth_token, expiration, repository_endpoint,\n            domain, repository, subprocess_utils)\n\n    @classmethod\n    def get_commands(cls, endpoint, auth_token, **kwargs):\n        # TODO(ujjwalpa@): We don't really have a command to execute for Twine\n        # as we directly write to the pypirc file (or to stdout for dryrun)\n        # with python itself instead. Nevertheless, we're using this method for\n        # testing so we'll keep the interface for now but return a string with\n        # the expected pypirc content instead of a list of commands to\n        # execute. This definitely reeks of code smell and there is probably\n        # room for rethinking and refactoring the interfaces of these adapter\n        # helper classes in the future.\n\n        assert 'pypi_rc_path' in kwargs, 'pypi_rc_path must be provided.'\n        pypi_rc_path = kwargs['pypi_rc_path']\n\n        default_pypi_rc = cls.DEFAULT_PYPI_RC_FMT.format(\n            repository_endpoint=endpoint,\n            auth_token=auth_token\n        )\n\n        pypi_rc = RawConfigParser()\n        if os.path.exists(pypi_rc_path):\n            try:\n                pypi_rc.read(pypi_rc_path)\n                index_servers = pypi_rc.get('distutils', 'index-servers')\n                servers = [\n                    server.strip()\n                    for server in index_servers.split('\\n')\n                    if server.strip() != ''\n                ]\n\n                if 'codeartifact' not in servers:\n                    servers.append('codeartifact')\n                    pypi_rc.set(\n                        'distutils', 'index-servers', '\\n' + '\\n'.join(servers)\n                    )\n\n                if 'codeartifact' not in pypi_rc.sections():\n                    pypi_rc.add_section('codeartifact')\n\n                pypi_rc.set('codeartifact', 'repository', endpoint)\n                pypi_rc.set('codeartifact', 'username', 'aws')\n                pypi_rc.set('codeartifact', 'password', auth_token)\n            except Exception as e:  # invalid .pypirc file\n                sys.stdout.write('%s is in an invalid state.' % pypi_rc_path)\n                sys.stdout.write(os.linesep)\n                raise e\n        else:\n            pypi_rc.read_string(default_pypi_rc)\n\n        pypi_rc_stream = StringIO()\n        pypi_rc.write(pypi_rc_stream)\n        pypi_rc_str = pypi_rc_stream.getvalue()\n        pypi_rc_stream.close()\n\n        return pypi_rc_str\n\n    def login(self, dry_run=False):\n        # No command to execute for Twine, we get the expected pypirc content\n        # instead.\n        pypi_rc_str = self.get_commands(\n            self.repository_endpoint,\n            self.auth_token,\n            pypi_rc_path=self.pypi_rc_path\n        )\n\n        if dry_run:\n            sys.stdout.write('Dryrun mode is enabled, not writing to pypirc.')\n            sys.stdout.write(os.linesep)\n            sys.stdout.write(\n                '%s would have been set to the following:' % self.pypi_rc_path\n            )\n            sys.stdout.write(os.linesep)\n            sys.stdout.write(os.linesep)\n            sys.stdout.write(pypi_rc_str)\n            sys.stdout.write(os.linesep)\n        else:\n            with open(self.pypi_rc_path, 'w+') as fp:\n                fp.write(pypi_rc_str)\n\n            self._write_success_message('twine')\n\n    @classmethod\n    def get_pypi_rc_path(cls):\n        return os.path.join(os.path.expanduser(\"~\"), \".pypirc\")\n\n\nclass CodeArtifactLogin(BasicCommand):\n    '''Log in to the idiomatic tool for the requested package format.'''\n\n    TOOL_MAP = {\n        'swift': {\n            'package_format': 'swift',\n            'login_cls': SwiftLogin,\n            'namespace_support': True,\n        },\n        'nuget': {\n            'package_format': 'nuget',\n            'login_cls': NuGetLogin,\n            'namespace_support': False,\n        },\n        'dotnet': {\n            'package_format': 'nuget',\n            'login_cls': DotNetLogin,\n            'namespace_support': False,\n        },\n        'npm': {\n            'package_format': 'npm',\n            'login_cls': NpmLogin,\n            'namespace_support': True,\n        },\n        'pip': {\n            'package_format': 'pypi',\n            'login_cls': PipLogin,\n            'namespace_support': False,\n        },\n        'twine': {\n            'package_format': 'pypi',\n            'login_cls': TwineLogin,\n            'namespace_support': False,\n        }\n    }\n\n    NAME = 'login'\n\n    DESCRIPTION = (\n        'Sets up the idiomatic tool for your package format to use your '\n        'CodeArtifact repository. Your login information is valid for up '\n        'to 12 hours after which you must login again.'\n    )\n\n    ARG_TABLE = [\n        {\n            'name': 'tool',\n            'help_text': 'The tool you want to connect with your repository',\n            'choices': list(TOOL_MAP.keys()),\n            'required': True,\n        },\n        {\n            'name': 'domain',\n            'help_text': 'Your CodeArtifact domain name',\n            'required': True,\n        },\n        {\n            'name': 'domain-owner',\n            'help_text': 'The AWS account ID that owns your CodeArtifact '\n                         'domain',\n            'required': False,\n        },\n        {\n            'name': 'namespace',\n            'help_text': 'Associates a namespace with your repository tool',\n            'required': False,\n        },\n        {\n            'name': 'duration-seconds',\n            'cli_type_name': 'integer',\n            'help_text': 'The time, in seconds, that the login information '\n                         'is valid',\n            'required': False,\n        },\n        {\n            'name': 'repository',\n            'help_text': 'Your CodeArtifact repository name',\n            'required': True,\n        },\n        {\n            'name': 'dry-run',\n            'action': 'store_true',\n            'help_text': 'Only print the commands that would be executed '\n                         'to connect your tool with your repository without '\n                         'making any changes to your configuration',\n            'required': False,\n            'default': False\n        },\n    ]\n\n    def _get_namespace(self, tool, parsed_args):\n        namespace_compatible = self.TOOL_MAP[tool]['namespace_support']\n\n        if not namespace_compatible and parsed_args.namespace:\n            raise ValueError(\n                'Argument --namespace is not supported for {}'.format(tool)\n            )\n        else:\n            return parsed_args.namespace\n\n    def _get_repository_endpoint(\n        self, codeartifact_client, parsed_args, package_format\n    ):\n        kwargs = {\n            'domain': parsed_args.domain,\n            'repository': parsed_args.repository,\n            'format': package_format\n        }\n        if parsed_args.domain_owner:\n            kwargs['domainOwner'] = parsed_args.domain_owner\n\n        get_repository_endpoint_response = \\\n            codeartifact_client.get_repository_endpoint(**kwargs)\n\n        return get_repository_endpoint_response['repositoryEndpoint']\n\n    def _get_authorization_token(self, codeartifact_client, parsed_args):\n        kwargs = {\n            'domain': parsed_args.domain\n        }\n        if parsed_args.domain_owner:\n            kwargs['domainOwner'] = parsed_args.domain_owner\n\n        if parsed_args.duration_seconds:\n            kwargs['durationSeconds'] = parsed_args.duration_seconds\n\n        get_authorization_token_response = \\\n            codeartifact_client.get_authorization_token(**kwargs)\n\n        return get_authorization_token_response\n\n    def _run_main(self, parsed_args, parsed_globals):\n        tool = parsed_args.tool.lower()\n\n        package_format = self.TOOL_MAP[tool]['package_format']\n\n        codeartifact_client = cli_utils.create_client_from_parsed_globals(\n            self._session, 'codeartifact', parsed_globals\n        )\n\n        auth_token_res = self._get_authorization_token(\n            codeartifact_client, parsed_args\n        )\n\n        repository_endpoint = self._get_repository_endpoint(\n            codeartifact_client, parsed_args, package_format\n        )\n\n        domain = parsed_args.domain\n        repository = parsed_args.repository\n        namespace = self._get_namespace(tool, parsed_args)\n\n        auth_token = auth_token_res['authorizationToken']\n        expiration = parse_timestamp(auth_token_res['expiration'])\n        login = self.TOOL_MAP[tool]['login_cls'](\n            auth_token, expiration, repository_endpoint,\n            domain, repository, subprocess, namespace\n        )\n\n        login.login(parsed_args.dry_run)\n\n        return 0\n", "awscli/customizations/codeartifact/__init__.py": "from awscli.customizations.codeartifact.login import CodeArtifactLogin\n\n\ndef register_codeartifact_commands(event_emitter):\n    event_emitter.register(\n        'building-command-table.codeartifact', inject_commands\n    )\n\n\ndef inject_commands(command_table, session, **kwargs):\n    command_table['login'] = CodeArtifactLogin(session)\n", "awscli/customizations/servicecatalog/generateproduct.py": "# Copyright 2012-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport sys\n\nfrom awscli.customizations.servicecatalog import helptext\nfrom awscli.customizations.servicecatalog.generatebase \\\n    import GenerateBaseCommand\nfrom botocore.compat import json\n\n\nclass GenerateProductCommand(GenerateBaseCommand):\n    NAME = \"product\"\n    DESCRIPTION = helptext.PRODUCT_COMMAND_DESCRIPTION\n    ARG_TABLE = [\n        {\n            'name': 'product-name',\n            'required': True,\n            'help_text': helptext.PRODUCT_NAME\n        },\n        {\n            'name': 'product-owner',\n            'required': True,\n            'help_text': helptext.OWNER\n        },\n        {\n            'name': 'product-type',\n            'required': True,\n            'help_text': helptext.PRODUCT_TYPE,\n            'choices': ['CLOUD_FORMATION_TEMPLATE', 'MARKETPLACE']\n        },\n        {\n            'name': 'product-description',\n            'required': False,\n            'help_text': helptext.PRODUCT_DESCRIPTION\n        },\n        {\n            'name': 'product-distributor',\n            'required': False,\n            'help_text': helptext.DISTRIBUTOR\n        },\n        {\n            'name': 'tags',\n            'required': False,\n            'schema': {\n                'type': 'array',\n                'items': {\n                    'type': 'string'\n                }\n            },\n            'default': [],\n            'synopsis': '--tags Key=key1,Value=value1 Key=key2,Value=value2',\n            'help_text': helptext.TAGS\n        },\n        {\n            'name': 'file-path',\n            'required': True,\n            'help_text': helptext.FILE_PATH\n        },\n        {\n            'name': 'bucket-name',\n            'required': True,\n            'help_text': helptext.BUCKET_NAME\n        },\n        {\n            'name': 'support-description',\n            'required': False,\n            'help_text': helptext.SUPPORT_DESCRIPTION\n        },\n        {\n            'name': 'support-email',\n            'required': False,\n            'help_text': helptext.SUPPORT_EMAIL\n        },\n        {\n            'name': 'provisioning-artifact-name',\n            'required': True,\n            'help_text': helptext.PA_NAME\n        },\n        {\n            'name': 'provisioning-artifact-description',\n            'required': True,\n            'help_text': helptext.PA_DESCRIPTION\n        },\n        {\n            'name': 'provisioning-artifact-type',\n            'required': True,\n            'help_text': helptext.PA_TYPE,\n            'choices': [\n                'CLOUD_FORMATION_TEMPLATE',\n                'MARKETPLACE_AMI',\n                'MARKETPLACE_CAR'\n            ]\n        }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        super(GenerateProductCommand, self)._run_main(parsed_args,\n                                                      parsed_globals)\n        self.region = self.get_and_validate_region(parsed_globals)\n\n        self.s3_url = self.create_s3_url(parsed_args.bucket_name,\n                                         parsed_args.file_path)\n        self.scs_client = self._session.create_client(\n            'servicecatalog', region_name=self.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl\n        )\n\n        response = self.create_product(self.build_args(parsed_args,\n                                                       self.s3_url),\n                                       parsed_globals)\n        sys.stdout.write(json.dumps(response, indent=2, ensure_ascii=False))\n\n        return 0\n\n    def create_product(self, args, parsed_globals):\n        response = self.scs_client.create_product(**args)\n        if 'ResponseMetadata' in response:\n            del response['ResponseMetadata']\n        return response\n\n    def _extract_tags(self, args_tags):\n        tags = []\n        for tag in args_tags:\n            tags.append(dict(t.split('=') for t in tag.split(',')))\n        return tags\n\n    def build_args(self, parsed_args, s3_url):\n        args = {\n            \"Name\": parsed_args.product_name,\n            \"Owner\": parsed_args.product_owner,\n            \"ProductType\": parsed_args.product_type,\n            \"Tags\": self._extract_tags(parsed_args.tags),\n            \"ProvisioningArtifactParameters\": {\n                'Name': parsed_args.provisioning_artifact_name,\n                'Description': parsed_args.provisioning_artifact_description,\n                'Info': {\n                    'LoadTemplateFromURL': s3_url\n                },\n                'Type': parsed_args.provisioning_artifact_type\n            }\n        }\n\n        # Non-required args\n        if parsed_args.support_description:\n            args[\"SupportDescription\"] = parsed_args.support_description\n        if parsed_args.product_description:\n            args[\"Description\"] = parsed_args.product_description\n        if parsed_args.support_email:\n            args[\"SupportEmail\"] = parsed_args.support_email\n        if parsed_args.product_distributor:\n            args[\"Distributor\"] = parsed_args.product_distributor\n\n        return args\n", "awscli/customizations/servicecatalog/exceptions.py": "# Copyright 2012-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nclass ServiceCatalogCommandError(Exception):\n    fmt = 'An unspecified error occurred'\n\n    def __init__(self, **kwargs):\n        msg = self.fmt.format(**kwargs)\n        Exception.__init__(self, msg)\n        self.kwargs = kwargs\n\n\nclass InvalidParametersException(ServiceCatalogCommandError):\n    fmt = \"An error occurred (InvalidParametersException) : {message}\"\n", "awscli/customizations/servicecatalog/generatebase.py": "# Copyright 2012-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.servicecatalog.utils \\\n    import make_url, get_s3_path\nfrom awscli.customizations.s3uploader import S3Uploader\nfrom awscli.customizations.servicecatalog import exceptions\n\n\nclass GenerateBaseCommand(BasicCommand):\n\n    def _run_main(self, parsed_args, parsed_globals):\n        self.region = self.get_and_validate_region(parsed_globals)\n        self.s3_client = self._session.create_client(\n            's3',\n            region_name=self.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl\n        )\n        self.s3_uploader = S3Uploader(self.s3_client,\n                                      parsed_args.bucket_name,\n                                      force_upload=True)\n        try:\n            self.s3_uploader.upload(parsed_args.file_path,\n                                get_s3_path(parsed_args.file_path))\n        except OSError as ex:\n            raise RuntimeError(\"%s cannot be found\" % parsed_args.file_path)\n\n    def get_and_validate_region(self, parsed_globals):\n        region = parsed_globals.region\n        if region is None:\n            region = self._session.get_config_variable('region')\n        if region not in self._session.get_available_regions('servicecatalog'):\n            raise exceptions.InvalidParametersException(\n                message=\"Region {0} is not supported\".format(\n                    parsed_globals.region))\n        return region\n\n    def create_s3_url(self, bucket_name, file_path):\n        return make_url(self.region,\n                        bucket_name,\n                        get_s3_path(file_path))\n", "awscli/customizations/servicecatalog/utils.py": "# Copyright 2012-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\n\n\ndef make_url(region, bucket_name, obj_path, version=None):\n    \"\"\"\n        This link describes the format of Path Style URLs\n        http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro\n    \"\"\"\n    base = \"https://s3.amazonaws.com\"\n    if region and region != \"us-east-1\":\n        base = \"https://s3-{0}.amazonaws.com\".format(region)\n\n    result = \"{0}/{1}/{2}\".format(base, bucket_name, obj_path)\n    if version:\n        result = \"{0}?versionId={1}\".format(result, version)\n\n    return result\n\n\ndef get_s3_path(file_path):\n    return os.path.basename(file_path)\n", "awscli/customizations/servicecatalog/helptext.py": "# Copyright 2012-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nTAGS = \"Tags to associate with the new product.\"\n\nBUCKET_NAME = (\"Name of the S3 bucket name where the CloudFormation \"\n               \"template will be uploaded to\")\n\nSUPPORT_DESCRIPTION = \"Support information about the product\"\n\nSUPPORT_EMAIL = \"Contact email for product support\"\n\nPA_NAME = \"The name assigned to the provisioning artifact\"\n\nPA_DESCRIPTION = \"The text description of the provisioning artifact\"\n\nPA_TYPE = \"The type of the provisioning artifact\"\n\nDISTRIBUTOR = \"The distributor of the product\"\n\nPRODUCT_ID = \"The product identifier\"\n\nPRODUCT_NAME = \"The name of the product\"\n\nOWNER = \"The owner of the product\"\n\nPRODUCT_TYPE = \"The type of the product to create\"\n\nPRODUCT_DESCRIPTION = \"The text description of the product\"\n\nPRODUCT_COMMAND_DESCRIPTION = (\"Create a new product using a CloudFormation \"\n                               \"template specified as a local file path\")\n\nPA_COMMAND_DESCRIPTION = (\"Create a new provisioning artifact for the \"\n                          \"specified product using a CloudFormation template \"\n                          \"specified as a local file path\")\n\nGENERATE_COMMAND = (\"Generate a Service Catalog product or provisioning \"\n                    \"artifact using a CloudFormation template specified \"\n                    \"as a local file path\")\n\nFILE_PATH = \"A local file path that references the CloudFormation template\"\n", "awscli/customizations/servicecatalog/generate.py": "# Copyright 2012-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.servicecatalog import helptext\nfrom awscli.customizations.servicecatalog.generateproduct \\\n    import GenerateProductCommand\nfrom awscli.customizations.servicecatalog.generateprovisioningartifact \\\n    import GenerateProvisioningArtifactCommand\n\n\nclass GenerateCommand(BasicCommand):\n    NAME = \"generate\"\n    DESCRIPTION = helptext.GENERATE_COMMAND\n    SUBCOMMANDS = [\n        {'name': 'product',\n         'command_class': GenerateProductCommand},\n        {'name': 'provisioning-artifact',\n         'command_class': GenerateProvisioningArtifactCommand}\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        if parsed_args.subcommand is None:\n            raise ValueError(\"usage: aws [options] <command> <subcommand> \"\n                             \"[parameters]\\naws: error: too few arguments\")\n", "awscli/customizations/servicecatalog/generateprovisioningartifact.py": "# Copyright 2012-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport sys\n\nfrom awscli.customizations.servicecatalog import helptext\nfrom awscli.customizations.servicecatalog.generatebase \\\n    import GenerateBaseCommand\nfrom botocore.compat import json\n\n\nclass GenerateProvisioningArtifactCommand(GenerateBaseCommand):\n    NAME = 'provisioning-artifact'\n    DESCRIPTION = helptext.PA_COMMAND_DESCRIPTION\n    ARG_TABLE = [\n        {\n            'name': 'file-path',\n            'required': True,\n            'help_text': helptext.FILE_PATH\n        },\n        {\n            'name': 'bucket-name',\n            'required': True,\n            'help_text': helptext.BUCKET_NAME\n        },\n        {\n            'name': 'provisioning-artifact-name',\n            'required': True,\n            'help_text': helptext.PA_NAME\n        },\n        {\n            'name': 'provisioning-artifact-description',\n            'required': True,\n            'help_text': helptext.PA_DESCRIPTION\n        },\n        {\n            'name': 'provisioning-artifact-type',\n            'required': True,\n            'help_text': helptext.PA_TYPE,\n            'choices': [\n                'CLOUD_FORMATION_TEMPLATE',\n                'MARKETPLACE_AMI',\n                'MARKETPLACE_CAR'\n            ]\n        },\n        {\n            'name': 'product-id',\n            'required': True,\n            'help_text': helptext.PRODUCT_ID\n        }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        super(GenerateProvisioningArtifactCommand, self)._run_main(\n            parsed_args, parsed_globals)\n        self.region = self.get_and_validate_region(parsed_globals)\n\n        self.s3_url = self.create_s3_url(parsed_args.bucket_name,\n                                         parsed_args.file_path)\n        self.scs_client = self._session.create_client(\n            'servicecatalog', region_name=self.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl\n        )\n\n        response = self.create_provisioning_artifact(parsed_args,\n                                                     self.s3_url)\n\n        sys.stdout.write(json.dumps(response, indent=2, ensure_ascii=False))\n\n        return 0\n\n    def create_provisioning_artifact(self, parsed_args, s3_url):\n        response = self.scs_client.create_provisioning_artifact(\n            ProductId=parsed_args.product_id,\n            Parameters={\n                'Name': parsed_args.provisioning_artifact_name,\n                'Description': parsed_args.provisioning_artifact_description,\n                'Info': {\n                    'LoadTemplateFromURL': s3_url\n                },\n                'Type': parsed_args.provisioning_artifact_type\n            }\n        )\n\n        if 'ResponseMetadata' in response:\n            del response['ResponseMetadata']\n        return response\n", "awscli/customizations/servicecatalog/__init__.py": "# Copyright 2012-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.servicecatalog.generate \\\n    import GenerateCommand\n\n\ndef register_servicecatalog_commands(event_emitter):\n    event_emitter.register('building-command-table.servicecatalog',\n                           inject_commands)\n\n\ndef inject_commands(command_table, session, **kwargs):\n    command_table['generate'] = GenerateCommand(session)\n", "awscli/customizations/eks/ordered_yaml.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport yaml\nfrom botocore.compat import OrderedDict\n\n\nclass SafeOrderedLoader(yaml.SafeLoader):\n    \"\"\" Safely load a yaml file into an OrderedDict.\"\"\"\n\n\nclass SafeOrderedDumper(yaml.SafeDumper):\n    \"\"\" Safely dump an OrderedDict as yaml.\"\"\"\n\n\ndef _ordered_constructor(loader, node):\n        loader.flatten_mapping(node)\n        return OrderedDict(loader.construct_pairs(node))\n\n\nSafeOrderedLoader.add_constructor(\n                    yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,\n                    _ordered_constructor)\n\n\ndef _ordered_representer(dumper, data):\n        return dumper.represent_mapping(\n            yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,\n            data.items())\n\n\nSafeOrderedDumper.add_representer(OrderedDict, _ordered_representer)\n\n\ndef ordered_yaml_load(stream):\n    \"\"\" Load an OrderedDict object from a yaml stream.\"\"\"\n    return yaml.load(stream, SafeOrderedLoader)\n\n\ndef ordered_yaml_dump(to_dump, stream=None):\n    \"\"\"\n    Dump an OrderedDict object to yaml.\n\n    :param to_dump: The OrderedDict to dump\n    :type to_dump: OrderedDict\n\n    :param stream: The file to dump to\n    If not given or if None, only return the value\n    :type stream: file\n    \"\"\"\n    return yaml.dump(to_dump, stream,\n                     SafeOrderedDumper, default_flow_style=False)\n", "awscli/customizations/eks/update_kubeconfig.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\nimport logging\n\nfrom botocore.compat import OrderedDict\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.utils import uni_print\nfrom awscli.customizations.eks.exceptions import EKSClusterError\nfrom awscli.customizations.eks.kubeconfig import (Kubeconfig,\n                                                  KubeconfigError,\n                                                  KubeconfigLoader,\n                                                  KubeconfigWriter,\n                                                  KubeconfigValidator,\n                                                  KubeconfigAppender)\nfrom awscli.customizations.eks.ordered_yaml import ordered_yaml_dump\n\nLOG = logging.getLogger(__name__)\n\nDEFAULT_PATH = os.path.expanduser(\"~/.kube/config\")\n\n# At the time EKS no longer supports Kubernetes v1.21 (probably ~Dec 2023),\n# this can be safely changed to default to writing \"v1\"\nAPI_VERSION = \"client.authentication.k8s.io/v1beta1\"\n\nclass UpdateKubeconfigCommand(BasicCommand):\n    NAME = 'update-kubeconfig'\n\n    DESCRIPTION = BasicCommand.FROM_FILE(\n        'eks',\n        'update-kubeconfig',\n        '_description.rst'\n    )\n\n    ARG_TABLE = [\n        {\n            'name': 'name',\n            'dest': 'cluster_name',\n            'help_text': (\"The name of the cluster for which \"\n                          \"to create a kubeconfig entry. \"\n                          \"This cluster must exist in your account and in the \"\n                          \"specified or configured default Region \"\n                          \"for your AWS CLI installation.\"),\n            'required': True\n        },\n        {\n            'name': 'kubeconfig',\n            'help_text': (\"Optionally specify a kubeconfig file to append \"\n                          \"with your configuration. \"\n                          \"By default, the configuration is written to the \"\n                          \"first file path in the KUBECONFIG \"\n                          \"environment variable (if it is set) \"\n                          \"or the default kubeconfig path (.kube/config) \"\n                          \"in your home directory.\"),\n            'required': False\n        },\n        {\n            'name': 'role-arn',\n            'help_text': (\"To assume a role for cluster authentication, \"\n                          \"specify an IAM role ARN with this option. \"\n                          \"For example, if you created a cluster \"\n                          \"while assuming an IAM role, \"\n                          \"then you must also assume that role to \"\n                          \"connect to the cluster the first time.\"),\n            'required': False\n        },\n        {\n            'name': 'dry-run',\n            'action': 'store_true',\n            'default': False,\n            'help_text': (\"Print the merged kubeconfig to stdout instead of \"\n                          \"writing it to the specified file.\"),\n            'required': False\n        },\n        {\n            'name': 'verbose',\n            'action': 'store_true',\n            'default': False,\n            'help_text': (\"Print more detailed output \"\n                          \"when writing to the kubeconfig file, \"\n                          \"including the appended entries.\")\n        },\n        {\n            'name': 'alias',\n            'help_text': (\"Alias for the cluster context name. \"\n                          \"Defaults to match cluster ARN.\"),\n            'required': False\n        },\n        {\n            'name': 'user-alias',\n            'help_text': (\"Alias for the generated user name. \"\n                          \"Defaults to match cluster ARN.\"),\n            'required': False\n        }\n    ]\n\n    def _display_entries(self, entries):\n        \"\"\"\n        Display entries in yaml format\n\n        :param entries: a list of OrderedDicts to be printed\n        :type entries: list\n        \"\"\"\n        uni_print(\"Entries:\\n\\n\")\n        for entry in entries:\n            uni_print(ordered_yaml_dump(entry))\n            uni_print(\"\\n\")\n\n    def _run_main(self, parsed_args, parsed_globals):\n        client = EKSClient(self._session,\n                           parsed_args=parsed_args,\n                           parsed_globals=parsed_globals)\n        new_cluster_dict = client.get_cluster_entry()\n        new_user_dict = client.get_user_entry(user_alias=parsed_args.user_alias)\n\n        config_selector = KubeconfigSelector(\n            os.environ.get(\"KUBECONFIG\", \"\"),\n            parsed_args.kubeconfig\n        )\n        config = config_selector.choose_kubeconfig(\n            new_cluster_dict[\"name\"]\n        )\n        updating_existing = config.has_cluster(new_cluster_dict[\"name\"])\n        appender = KubeconfigAppender()\n        new_context_dict = appender.insert_cluster_user_pair(config,\n                                                             new_cluster_dict,\n                                                             new_user_dict,\n                                                             parsed_args.alias)\n\n        if parsed_args.dry_run:\n            uni_print(config.dump_content())\n        else:\n            writer = KubeconfigWriter()\n            writer.write_kubeconfig(config)\n\n            if updating_existing:\n                uni_print(\"Updated context {0} in {1}\\n\".format(\n                    new_context_dict[\"name\"], config.path\n                ))\n            else:\n                uni_print(\"Added new context {0} to {1}\\n\".format(\n                    new_context_dict[\"name\"], config.path\n                ))\n\n            if parsed_args.verbose:\n                self._display_entries([\n                    new_context_dict,\n                    new_user_dict,\n                    new_cluster_dict\n                ])\n\n\n\nclass KubeconfigSelector(object):\n\n    def __init__(self, env_variable, path_in, validator=None,\n                                              loader=None):\n        \"\"\"\n        Parse KUBECONFIG into a list of absolute paths.\n        Also replace the empty list with DEFAULT_PATH\n\n        :param env_variable: KUBECONFIG as a long string\n        :type env_variable: string\n\n        :param path_in: The path passed in through the CLI\n        :type path_in: string or None\n        \"\"\"\n        if validator is None:\n            validator = KubeconfigValidator()\n        self._validator = validator\n\n        if loader is None:\n            loader = KubeconfigLoader(validator)\n        self._loader = loader\n\n        if path_in is not None:\n            # Override environment variable\n            self._paths = [self._expand_path(path_in)]\n        else:\n            # Get the list of paths from the environment variable\n            if env_variable == \"\":\n                env_variable = DEFAULT_PATH\n            self._paths = [self._expand_path(element)\n                           for element in env_variable.split(os.pathsep)\n                           if len(element.strip()) > 0]\n            if len(self._paths) == 0:\n                self._paths = [DEFAULT_PATH]\n\n    def choose_kubeconfig(self, cluster_name):\n        \"\"\"\n        Choose which kubeconfig file to read from.\n        If name is already an entry in one of the $KUBECONFIG files,\n        choose that one.\n        Otherwise choose the first file.\n\n        :param cluster_name: The name of the cluster which is going to be added\n        :type cluster_name: String\n\n        :return: a chosen Kubeconfig based on above rules\n        :rtype: Kubeconfig\n        \"\"\"\n        # Search for an existing entry to update\n        for candidate_path in self._paths:\n            try:\n                loaded_config = self._loader.load_kubeconfig(candidate_path)\n\n                if loaded_config.has_cluster(cluster_name):\n                    LOG.debug(\"Found entry to update at {0}\".format(\n                        candidate_path\n                    ))\n                    return loaded_config\n            except KubeconfigError as e:\n                LOG.warning(\"Passing {0}:{1}\".format(candidate_path, e))\n\n        # No entry was found, use the first file in KUBECONFIG\n        #\n        # Note: This could raise KubeconfigErrors if paths[0] is corrupted\n        return self._loader.load_kubeconfig(self._paths[0])\n\n    def _expand_path(self, path):\n        \"\"\" A helper to expand a path to a full absolute path. \"\"\"\n        return os.path.abspath(os.path.expanduser(path))\n\n\nclass EKSClient(object):\n    def __init__(self, session, parsed_args, parsed_globals=None):\n        self._session = session\n        self._cluster_name = parsed_args.cluster_name\n        self._cluster_description = None\n        self._parsed_globals = parsed_globals\n        self._parsed_args = parsed_args\n\n    @property\n    def cluster_description(self):\n        \"\"\"\n        Use an eks describe-cluster call to get the cluster description\n        Cache the response in self._cluster_description.\n        describe-cluster will only be called once.\n        \"\"\"\n        if self._cluster_description is None:\n            if self._parsed_globals is None:\n                client = self._session.create_client(\"eks\")\n            else:\n                client = self._session.create_client(\n                    \"eks\",\n                    region_name=self._parsed_globals.region,\n                    endpoint_url=self._parsed_globals.endpoint_url,\n                    verify=self._parsed_globals.verify_ssl\n                )\n            full_description = client.describe_cluster(name=self._cluster_name)\n            self._cluster_description = full_description[\"cluster\"]\n\n            if \"status\" not in self._cluster_description:\n                raise EKSClusterError(\"Cluster not found\")\n            if self._cluster_description[\"status\"] not in [\"ACTIVE\", \"UPDATING\"]:\n                raise EKSClusterError(\"Cluster status is {0}\".format(\n                    self._cluster_description[\"status\"]\n                ))\n\n        return self._cluster_description\n\n    def get_cluster_entry(self):\n        \"\"\"\n        Return a cluster entry generated using\n        the previously obtained description.\n        \"\"\"\n\n        cert_data = self.cluster_description.get(\"certificateAuthority\", {}).get(\"data\", \"\")\n        endpoint = self.cluster_description.get(\"endpoint\")\n        arn = self.cluster_description.get(\"arn\")\n\n        return OrderedDict([\n            (\"cluster\", OrderedDict([\n                (\"certificate-authority-data\", cert_data),\n                (\"server\", endpoint)\n            ])),\n            (\"name\", arn)\n        ])\n\n    def get_user_entry(self, user_alias=None):\n        \"\"\"\n        Return a user entry generated using\n        the previously obtained description.\n        \"\"\"\n        region = self.cluster_description.get(\"arn\").split(\":\")[3]\n        outpost_config = self.cluster_description.get(\"outpostConfig\")\n\n        if outpost_config is None:\n            cluster_identification_parameter = \"--cluster-name\"\n            cluster_identification_value = self._cluster_name\n        else:\n            # If cluster contains outpostConfig, use id for identification\n            cluster_identification_parameter = \"--cluster-id\"\n            cluster_identification_value = self.cluster_description.get(\"id\")\n\n        generated_user = OrderedDict([\n            (\"name\", user_alias or self.cluster_description.get(\"arn\", \"\")),\n            (\"user\", OrderedDict([\n                (\"exec\", OrderedDict([\n                    (\"apiVersion\", API_VERSION),\n                    (\"args\",\n                        [\n                            \"--region\",\n                            region,\n                            \"eks\",\n                            \"get-token\",\n                            cluster_identification_parameter,\n                            cluster_identification_value,\n                            \"--output\",\n                            \"json\",\n                        ]),\n                    (\"command\", \"aws\"),\n                ]))\n            ]))\n        ])\n\n        if self._parsed_args.role_arn is not None:\n            generated_user[\"user\"][\"exec\"][\"args\"].extend([\n                \"--role\",\n                self._parsed_args.role_arn\n            ])\n\n        if self._session.profile:\n            generated_user[\"user\"][\"exec\"][\"env\"] = [OrderedDict([\n                (\"name\", \"AWS_PROFILE\"),\n                (\"value\", self._session.profile)\n            ])]\n\n        return generated_user\n", "awscli/customizations/eks/exceptions.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nclass EKSError(Exception):\n    \"\"\" Base class for all EKSErrors.\"\"\"\n\n\nclass EKSClusterError(EKSError):\n    \"\"\" Raised when a cluster is not in the correct state.\"\"\"\n", "awscli/customizations/eks/kubeconfig.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\nimport yaml\nimport logging\nimport errno\nfrom botocore.compat import OrderedDict\n\nfrom awscli.customizations.eks.exceptions import EKSError\nfrom awscli.customizations.eks.ordered_yaml import (ordered_yaml_load,\n                                                    ordered_yaml_dump)\n\n\nclass KubeconfigError(EKSError):\n    \"\"\" Base class for all kubeconfig errors.\"\"\"\n\n\nclass KubeconfigCorruptedError(KubeconfigError):\n    \"\"\" Raised when a kubeconfig cannot be parsed.\"\"\"\n\n\nclass KubeconfigInaccessableError(KubeconfigError):\n    \"\"\" Raised when a kubeconfig cannot be opened for read/writing.\"\"\"\n\n\ndef _get_new_kubeconfig_content():\n    return OrderedDict([\n        (\"apiVersion\", \"v1\"),\n        (\"clusters\", []),\n        (\"contexts\", []),\n        (\"current-context\", \"\"),\n        (\"kind\", \"Config\"),\n        (\"preferences\", OrderedDict()),\n        (\"users\", [])\n    ])\n\n\nclass Kubeconfig(object):\n    def __init__(self, path, content=None):\n        self.path = path\n        if content is None:\n            content = _get_new_kubeconfig_content()\n        self.content = content\n\n    def dump_content(self):\n        \"\"\" Return the stored content in yaml format. \"\"\"\n        return ordered_yaml_dump(self.content)\n\n    def has_cluster(self, name):\n        \"\"\"\n        Return true if this kubeconfig contains an entry\n        For the passed cluster name.\n        \"\"\"\n        if self.content.get('clusters') is None:\n            return False\n        return name in [cluster['name']\n                        for cluster in self.content['clusters'] if 'name' in cluster]\n\n    def __eq__(self, other):\n        return (\n            isinstance(other, Kubeconfig)\n            and self.path == other.path\n            and self.content == other.content\n        )\n\n\nclass KubeconfigValidator(object):\n    def __init__(self):\n        # Validation_content is an empty Kubeconfig\n        # It is used as a way to know what types different entries should be\n        self._validation_content = Kubeconfig(None, None).content\n\n    def validate_config(self, config):\n        \"\"\"\n        Raises KubeconfigCorruptedError if the passed content is invalid\n\n        :param config: The config to validate\n        :type config: Kubeconfig\n        \"\"\"\n        if not isinstance(config, Kubeconfig):\n            raise KubeconfigCorruptedError(\"Internal error: \"\n                                           f\"Not a {Kubeconfig}.\")\n        self._validate_config_types(config)\n        self._validate_list_entry_types(config)\n\n    def _validate_config_types(self, config):\n        \"\"\"\n        Raises KubeconfigCorruptedError if any of the entries in config\n        are the wrong type\n\n        :param config: The config to validate\n        :type config: Kubeconfig\n        \"\"\"\n        if not isinstance(config.content, dict):\n            raise KubeconfigCorruptedError(f\"Content not a {dict}.\")\n        for key, value in self._validation_content.items():\n            if (key in config.content and\n                    config.content[key] is not None and\n                    not isinstance(config.content[key], type(value))):\n                raise KubeconfigCorruptedError(\n                    f\"{key} is wrong type: {type(config.content[key])} \"\n                    f\"(Should be {type(value)})\"\n                )\n\n    def _validate_list_entry_types(self, config):\n        \"\"\"\n        Raises KubeconfigCorruptedError if any lists in config contain objects\n        which are not dictionaries\n\n        :param config: The config to validate\n        :type config: Kubeconfig\n        \"\"\"\n        for key, value in self._validation_content.items():\n            if (key in config.content and\n                    type(config.content[key]) == list):\n                for element in config.content[key]:\n                    if not isinstance(element, OrderedDict):\n                        raise KubeconfigCorruptedError(\n                            f\"Entry in {key} not a {dict}. \")\n\n\nclass KubeconfigLoader(object):\n    def __init__(self, validator = None):\n        if validator is None:\n            validator=KubeconfigValidator()\n        self._validator=validator\n\n    def load_kubeconfig(self, path):\n        \"\"\"\n        Loads the kubeconfig found at the given path.\n        If no file is found at the given path,\n        Generate a new kubeconfig to write back.\n        If the kubeconfig is valid, loads the content from it.\n        If the kubeconfig is invalid, throw the relevant exception.\n\n        :param path: The path to load a kubeconfig from\n        :type path: string\n\n        :raises KubeconfigInaccessableError: if the kubeconfig can't be opened\n        :raises KubeconfigCorruptedError: if the kubeconfig is invalid\n\n        :return: The loaded kubeconfig\n        :rtype: Kubeconfig\n        \"\"\"\n        try:\n            with open(path, \"r\") as stream:\n                loaded_content=ordered_yaml_load(stream)\n        except IOError as e:\n            if e.errno == errno.ENOENT:\n                loaded_content=None\n            else:\n                raise KubeconfigInaccessableError(\n                    f\"Can't open kubeconfig for reading: {e}\")\n        except yaml.YAMLError as e:\n            raise KubeconfigCorruptedError(\n                f\"YamlError while loading kubeconfig: {e}\")\n\n        loaded_config=Kubeconfig(path, loaded_content)\n        self._validator.validate_config(loaded_config)\n\n        return loaded_config\n\n\nclass KubeconfigWriter(object):\n    def write_kubeconfig(self, config):\n        \"\"\"\n        Write config to disk.\n        OK if the file doesn't exist.\n\n        :param config: The kubeconfig to write\n        :type config: Kubeconfig\n\n        :raises KubeconfigInaccessableError: if the kubeconfig\n        can't be opened for writing\n        \"\"\"\n        directory=os.path.dirname(config.path)\n\n        try:\n            os.makedirs(directory)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise KubeconfigInaccessableError(\n                    f\"Can't create directory for writing: {e}\")\n        try:\n            with os.fdopen(\n                    os.open(\n                        config.path,\n                        os.O_CREAT | os.O_RDWR | os.O_TRUNC,\n                        0o600),\n                    \"w+\") as stream:\n                ordered_yaml_dump(config.content, stream)\n        except (IOError, OSError) as e:\n            raise KubeconfigInaccessableError(\n                f\"Can't open kubeconfig for writing: {e}\")\n\n\nclass KubeconfigAppender(object):\n    def insert_entry(self, config, key, new_entry):\n        \"\"\"\n        Insert entry into the entries list at content[key]\n        Overwrite an existing entry if they share the same name\n\n        :param config: The kubeconfig to insert an entry into\n        :type config: Kubeconfig\n        \"\"\"\n        entries=self._setdefault_existing_entries(config, key)\n        same_name_index=self._index_same_name(entries, new_entry)\n        if same_name_index is None:\n            entries.append(new_entry)\n        else:\n            entries[same_name_index]=new_entry\n        return config\n\n    def _setdefault_existing_entries(self, config, key):\n        config.content[key]=config.content.get(key) or []\n        entries=config.content[key]\n        if not isinstance(entries, list):\n            raise KubeconfigError(f\"Tried to insert into {key}, \"\n                                  f\"which is a {type(entries)} \"\n                                  f\"not a {list}\")\n        return entries\n\n    def _index_same_name(self, entries, new_entry):\n        if \"name\" in new_entry:\n            name_to_search=new_entry[\"name\"]\n            for i, entry in enumerate(entries):\n                if \"name\" in entry and entry[\"name\"] == name_to_search:\n                    return i\n        return None\n\n    def _make_context(self, cluster, user, alias = None):\n        \"\"\" Generate a context to associate cluster and user with a given alias.\"\"\"\n        return OrderedDict([\n            (\"context\", OrderedDict([\n                (\"cluster\", cluster[\"name\"]),\n                (\"user\", user[\"name\"])\n            ])),\n            (\"name\", alias or user[\"name\"])\n        ])\n\n    def insert_cluster_user_pair(self, config, cluster, user, alias = None):\n        \"\"\"\n        Insert the passed cluster entry and user entry,\n        then make a context to associate them\n        and set current-context to be the new context.\n        Returns the new context\n\n        :param config: the Kubeconfig to insert the pair into\n        :type config: Kubeconfig\n\n        :param cluster: the cluster entry\n        :type cluster: OrderedDict\n\n        :param user: the user entry\n        :type user: OrderedDict\n\n        :param alias: the alias for the context; defaults top user entry name\n        :type context: str\n\n        :return: The generated context\n        :rtype: OrderedDict\n        \"\"\"\n        context=self._make_context(cluster, user, alias = alias)\n        self.insert_entry(config, \"clusters\", cluster)\n        self.insert_entry(config, \"users\", user)\n        self.insert_entry(config, \"contexts\", context)\n\n        config.content[\"current-context\"]=context[\"name\"]\n\n        return context\n", "awscli/customizations/eks/get_token.py": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport botocore\nimport json\nimport os\nimport sys\n\nfrom datetime import datetime, timedelta\nfrom botocore.signers import RequestSigner\nfrom botocore.model import ServiceId\n\nfrom awscli.formatter import get_formatter\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.utils import uni_print\nfrom awscli.customizations.utils import validate_mutually_exclusive\n\nAUTH_SERVICE = \"sts\"\nAUTH_COMMAND = \"GetCallerIdentity\"\nAUTH_API_VERSION = \"2011-06-15\"\nAUTH_SIGNING_VERSION = \"v4\"\n\nALPHA_API = \"client.authentication.k8s.io/v1alpha1\"\nBETA_API = \"client.authentication.k8s.io/v1beta1\"\nV1_API = \"client.authentication.k8s.io/v1\"\n\nFULLY_SUPPORTED_API_VERSIONS = [\n    V1_API,\n    BETA_API,\n]\nDEPRECATED_API_VERSIONS = [\n    ALPHA_API,\n]\n\nERROR_MSG_TPL = (\n    \"{0} KUBERNETES_EXEC_INFO, defaulting to {1}. This is likely a \"\n    \"bug in your Kubernetes client. Please update your Kubernetes \"\n    \"client.\"\n)\nUNRECOGNIZED_MSG_TPL = (\n    \"Unrecognized API version in KUBERNETES_EXEC_INFO, defaulting to \"\n    \"{0}. This is likely due to an outdated AWS \"\n    \"CLI. Please update your AWS CLI.\"\n)\nDEPRECATION_MSG_TPL = (\n    \"Kubeconfig user entry is using deprecated API version {0}. Run \"\n    \"'aws eks update-kubeconfig' to update.\"\n)\n\n# Presigned url timeout in seconds\nURL_TIMEOUT = 60\n\nTOKEN_EXPIRATION_MINS = 14\n\nTOKEN_PREFIX = 'k8s-aws-v1.'\n\nK8S_AWS_ID_HEADER = 'x-k8s-aws-id'\n\n\nclass GetTokenCommand(BasicCommand):\n    NAME = 'get-token'\n\n    DESCRIPTION = (\n        \"Get a token for authentication with an Amazon EKS cluster. \"\n        \"This can be used as an alternative to the \"\n        \"aws-iam-authenticator.\"\n    )\n\n    ARG_TABLE = [\n        {\n            'name': 'cluster-name',\n            'help_text': (\n                \"Specify the name of the Amazon EKS cluster to create a token for. (Note: for local clusters on AWS Outposts, please use --cluster-id parameter)\"\n            ),\n            'required': False,\n        },\n        {\n            'name': 'role-arn',\n            'help_text': (\n                \"Assume this role for credentials when signing the token. \"\n                \"Use this optional parameter when the credentials for signing \"\n                \"the token differ from that of the current role session. \"\n                \"Using this parameter results in new role session credentials \"\n                \"that are used to sign the token.\"\n            ),\n            'required': False,\n        },\n        {\n            'name': 'cluster-id',\n            # When EKS in-region cluster supports cluster-id, we will need to update this help text\n            'help_text': (\n                \"Specify the id of the Amazon EKS cluster to create a token for. (Note: for local clusters on AWS Outposts only)\"\n            ),\n            'required': False,\n        },\n    ]\n\n    def get_expiration_time(self):\n        token_expiration = datetime.utcnow() + timedelta(\n            minutes=TOKEN_EXPIRATION_MINS\n        )\n        return token_expiration.strftime('%Y-%m-%dT%H:%M:%SZ')\n\n    def _run_main(self, parsed_args, parsed_globals):\n        client_factory = STSClientFactory(self._session)\n        sts_client = client_factory.get_sts_client(\n            region_name=parsed_globals.region, role_arn=parsed_args.role_arn\n        )\n        \n        validate_mutually_exclusive(parsed_args, ['cluster_name'], ['cluster_id'])\n\n        if parsed_args.cluster_id:\n            identifier = parsed_args.cluster_id\n        elif parsed_args.cluster_name:\n            identifier = parsed_args.cluster_name\n        else:\n            return ValueError(\"Either parameter --cluster-name or --cluster-id must be specified.\")\n\n        token = TokenGenerator(sts_client).get_token(identifier)\n\n        # By default STS signs the url for 15 minutes so we are creating a\n        # rfc3339 timestamp with expiration in 14 minutes as part of the token, which\n        # is used by some clients (client-go) who will refresh the token after 14 mins\n        token_expiration = self.get_expiration_time()\n\n        full_object = {\n            \"kind\": \"ExecCredential\",\n            \"apiVersion\": self.discover_api_version(),\n            \"spec\": {},\n            \"status\": {\n                \"expirationTimestamp\": token_expiration,\n                \"token\": token,\n            },\n        }\n\n        output = parsed_globals.output\n        if output is None:\n            output = self._session.get_config_variable('output')\n        formatter = get_formatter(output, parsed_globals)\n        formatter.query = parsed_globals.query\n\n        formatter(self.NAME, full_object)\n        uni_print('\\n')\n        return 0\n\n    def discover_api_version(self):\n        \"\"\"\n        Parses the KUBERNETES_EXEC_INFO environment variable and returns the\n        API version. If the environment variable is malformed or invalid,\n        return the v1beta1 response and print a message to stderr.\n\n        If the v1alpha1 API is specified explicitly, a message is printed to\n        stderr with instructions to update.\n\n        :return: The client authentication API version\n        :rtype: string\n        \"\"\"\n        # At the time Kubernetes v1.29 is released upstream (approx Dec 2023),\n        # \"v1beta1\" will be removed. At or around that time, EKS will likely\n        # support v1.22 through v1.28, in which client API version \"v1beta1\"\n        # will be supported by all EKS versions.\n        fallback_api_version = BETA_API\n\n        error_prefixes = {\n            \"error\": \"Error parsing\",\n            \"empty\": \"Empty\",\n        }\n\n        exec_info_raw = os.environ.get(\"KUBERNETES_EXEC_INFO\", \"\")\n        if not exec_info_raw:\n            # All kube clients should be setting this, but client-go clients\n            # (kubectl, kubelet, etc) < 1.20 were not setting this if the API\n            # version defined in the kubeconfig was not v1alpha1.\n            #\n            # This was changed in kubernetes/kubernetes#95489 so that\n            # KUBERNETES_EXEC_INFO is always provided\n            return fallback_api_version\n        try:\n            exec_info = json.loads(exec_info_raw)\n        except json.JSONDecodeError:\n            # The environment variable was malformed\n            uni_print(\n                ERROR_MSG_TPL.format(\n                    error_prefixes[\"error\"],\n                    fallback_api_version,\n                ),\n                sys.stderr,\n            )\n            uni_print(\"\\n\", sys.stderr)\n            return fallback_api_version\n\n        api_version_raw = exec_info.get(\"apiVersion\")\n        if api_version_raw in FULLY_SUPPORTED_API_VERSIONS:\n            return api_version_raw\n        elif api_version_raw in DEPRECATED_API_VERSIONS:\n            uni_print(DEPRECATION_MSG_TPL.format(api_version_raw), sys.stderr)\n            uni_print(\"\\n\", sys.stderr)\n            return api_version_raw\n        else:\n            uni_print(\n                UNRECOGNIZED_MSG_TPL.format(fallback_api_version),\n                sys.stderr,\n            )\n            uni_print(\"\\n\", sys.stderr)\n            return fallback_api_version\n\n\nclass TokenGenerator(object):\n    def __init__(self, sts_client):\n        self._sts_client = sts_client\n\n    def get_token(self, k8s_aws_id):\n        \"\"\"Generate a presigned url token to pass to kubectl.\"\"\"\n        url = self._get_presigned_url(k8s_aws_id)\n        token = TOKEN_PREFIX + base64.urlsafe_b64encode(\n            url.encode('utf-8')\n        ).decode('utf-8').rstrip('=')\n        return token\n\n    def _get_presigned_url(self, k8s_aws_id):\n        return self._sts_client.generate_presigned_url(\n            'get_caller_identity',\n            Params={K8S_AWS_ID_HEADER: k8s_aws_id},\n            ExpiresIn=URL_TIMEOUT,\n            HttpMethod='GET',\n        )\n\n\nclass STSClientFactory(object):\n    def __init__(self, session):\n        self._session = session\n\n    def get_sts_client(self, region_name=None, role_arn=None):\n        client_kwargs = {'region_name': region_name}\n        if role_arn is not None:\n            creds = self._get_role_credentials(region_name, role_arn)\n            client_kwargs['aws_access_key_id'] = creds['AccessKeyId']\n            client_kwargs['aws_secret_access_key'] = creds['SecretAccessKey']\n            client_kwargs['aws_session_token'] = creds['SessionToken']\n        sts = self._session.create_client('sts', **client_kwargs)\n        self._register_k8s_aws_id_handlers(sts)\n        return sts\n\n    def _get_role_credentials(self, region_name, role_arn):\n        sts = self._session.create_client('sts', region_name)\n        return sts.assume_role(\n            RoleArn=role_arn, RoleSessionName='EKSGetTokenAuth'\n        )['Credentials']\n\n    def _register_k8s_aws_id_handlers(self, sts_client):\n        sts_client.meta.events.register(\n            'provide-client-params.sts.GetCallerIdentity',\n            self._retrieve_k8s_aws_id,\n        )\n        sts_client.meta.events.register(\n            'before-sign.sts.GetCallerIdentity',\n            self._inject_k8s_aws_id_header,\n        )\n\n    def _retrieve_k8s_aws_id(self, params, context, **kwargs):\n        if K8S_AWS_ID_HEADER in params:\n            context[K8S_AWS_ID_HEADER] = params.pop(K8S_AWS_ID_HEADER)\n\n    def _inject_k8s_aws_id_header(self, request, **kwargs):\n        if K8S_AWS_ID_HEADER in request.context:\n            request.headers[K8S_AWS_ID_HEADER] = request.context[K8S_AWS_ID_HEADER]\n", "awscli/customizations/eks/__init__.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.eks.update_kubeconfig import UpdateKubeconfigCommand\nfrom awscli.customizations.eks.get_token import GetTokenCommand\n\n\ndef initialize(cli):\n    \"\"\"\n    The entry point for EKS high level commands.\n    \"\"\"\n    cli.register('building-command-table.eks', inject_commands)\n\n\ndef inject_commands(command_table, session, **kwargs):\n    \"\"\"\n    Called when the EKS command table is being built.\n    Used to inject new high level commands into the command list.\n    \"\"\"\n    command_table['update-kubeconfig'] = UpdateKubeconfigCommand(session)\n    command_table['get-token'] = GetTokenCommand(session)\n", "awscli/customizations/configservice/putconfigurationrecorder.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\n\nfrom awscli.arguments import CLIArgument\n\n\ndef register_modify_put_configuration_recorder(cli):\n    cli.register(\n        'building-argument-table.configservice.put-configuration-recorder',\n        extract_recording_group)\n\n\ndef extract_recording_group(session, argument_table, **kwargs):\n    # The purpose of this customization is to extract the recordingGroup\n    # member from ConfigurationRecorder into its own argument.\n    # This customization is needed because the recordingGroup member\n    # breaks the shorthand syntax as it is a structure and not a scalar value.\n    configuration_recorder_argument = argument_table['configuration-recorder']\n\n    configuration_recorder_model = copy.deepcopy(\n        configuration_recorder_argument.argument_model)\n    recording_group_model = copy.deepcopy(\n        configuration_recorder_argument.argument_model.\n        members['recordingGroup'])\n\n    del configuration_recorder_model.members['recordingGroup']\n    argument_table['configuration-recorder'] = ConfigurationRecorderArgument(\n        name='configuration-recorder',\n        argument_model=configuration_recorder_model,\n        operation_model=configuration_recorder_argument._operation_model,\n        is_required=True,\n        event_emitter=session.get_component('event_emitter'),\n        serialized_name='ConfigurationRecorder'\n    )\n\n    argument_table['recording-group'] = RecordingGroupArgument(\n        name='recording-group',\n        argument_model=recording_group_model,\n        operation_model=configuration_recorder_argument._operation_model,\n        is_required=False,\n        event_emitter=session.get_component('event_emitter'),\n        serialized_name='recordingGroup'\n    )\n\n\nclass ConfigurationRecorderArgument(CLIArgument):\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        unpacked = self._unpack_argument(value)\n        if 'ConfigurationRecorder' in parameters:\n            current_value = parameters['ConfigurationRecorder']\n            current_value.update(unpacked)\n        else:\n            parameters['ConfigurationRecorder'] = unpacked\n\n\nclass RecordingGroupArgument(CLIArgument):\n    def add_to_params(self, parameters, value):\n        if value is None:\n            return\n        unpacked = self._unpack_argument(value)\n        if 'ConfigurationRecorder' in parameters:\n            parameters['ConfigurationRecorder']['recordingGroup'] = unpacked\n        else:\n            parameters['ConfigurationRecorder'] = {}\n            parameters['ConfigurationRecorder']['recordingGroup'] = unpacked\n", "awscli/customizations/configservice/subscribe.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport sys\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.utils import s3_bucket_exists\nfrom awscli.customizations.s3.utils import find_bucket_key\n\n\nS3_BUCKET = {'name': 's3-bucket', 'required': True,\n             'help_text': ('The S3 bucket that the AWS Config delivery channel'\n                           ' will use. If the bucket does not exist, it will '\n                           'be automatically created. The value for this '\n                           'argument should follow the form '\n                           'bucket/prefix. Note that the prefix is optional.')}\n\nSNS_TOPIC = {'name': 'sns-topic', 'required': True,\n             'help_text': ('The SNS topic that the AWS Config delivery channel'\n                           ' will use. If the SNS topic does not exist, it '\n                           'will be automatically created. Value for this '\n                           'should be a valid SNS topic name or the ARN of an '\n                           'existing SNS topic.')}\n\nIAM_ROLE = {'name': 'iam-role', 'required': True,\n            'help_text': ('The IAM role that the AWS Config configuration '\n                          'recorder will use to record current resource '\n                          'configurations. Value for this should be the '\n                          'ARN of the desired IAM role.')}\n\n\ndef register_subscribe(cli):\n    cli.register('building-command-table.configservice', add_subscribe)\n\n\ndef add_subscribe(command_table, session, **kwargs):\n    command_table['subscribe'] = SubscribeCommand(session)\n\n\nclass SubscribeCommand(BasicCommand):\n    NAME = 'subscribe'\n    DESCRIPTION = ('Subscribes user to AWS Config by creating an AWS Config '\n                   'delivery channel and configuration recorder to track '\n                   'AWS resource configurations. The names of the default '\n                   'channel and configuration recorder will be default.')\n    ARG_TABLE = [S3_BUCKET, SNS_TOPIC, IAM_ROLE]\n\n    def __init__(self, session):\n        self._s3_client = None\n        self._sns_client = None\n        self._config_client = None\n        super(SubscribeCommand, self).__init__(session)\n\n    def _run_main(self, parsed_args, parsed_globals):\n        # Setup the necessary all of the necessary clients.\n        self._setup_clients(parsed_globals)\n\n        # Prepare a s3 bucket for use.\n        s3_bucket_helper = S3BucketHelper(self._s3_client)\n        bucket, prefix = s3_bucket_helper.prepare_bucket(parsed_args.s3_bucket)\n\n        # Prepare a sns topic for use.\n        sns_topic_helper = SNSTopicHelper(self._sns_client)\n        sns_topic_arn = sns_topic_helper.prepare_topic(parsed_args.sns_topic)\n\n        name = 'default'\n\n        # Create a configuration recorder.\n        self._config_client.put_configuration_recorder(\n            ConfigurationRecorder={\n                'name': name,\n                'roleARN': parsed_args.iam_role\n            }\n        )\n\n        # Create a delivery channel.\n        delivery_channel = {\n            'name': name,\n            's3BucketName': bucket,\n            'snsTopicARN': sns_topic_arn\n        }\n\n        if prefix:\n            delivery_channel['s3KeyPrefix'] = prefix\n\n        self._config_client.put_delivery_channel(\n            DeliveryChannel=delivery_channel)\n\n        # Start the configuration recorder.\n        self._config_client.start_configuration_recorder(\n            ConfigurationRecorderName=name\n        )\n\n        # Describe the configuration recorders\n        sys.stdout.write('Subscribe succeeded:\\n\\n')\n        sys.stdout.write('Configuration Recorders: ')\n        response = self._config_client.describe_configuration_recorders()\n        sys.stdout.write(\n            json.dumps(response['ConfigurationRecorders'], indent=4))\n        sys.stdout.write('\\n\\n')\n\n        # Describe the delivery channels\n        sys.stdout.write('Delivery Channels: ')\n        response = self._config_client.describe_delivery_channels()\n        sys.stdout.write(json.dumps(response['DeliveryChannels'], indent=4))\n        sys.stdout.write('\\n')\n\n        return 0\n\n    def _setup_clients(self, parsed_globals):\n        client_args = {\n            'verify': parsed_globals.verify_ssl,\n            'region_name': parsed_globals.region\n        }\n        self._s3_client = self._session.create_client('s3', **client_args)\n        self._sns_client = self._session.create_client('sns', **client_args)\n        # Use the specified endpoint only for config related commands.\n        client_args['endpoint_url'] = parsed_globals.endpoint_url\n        self._config_client = self._session.create_client('config',\n                                                          **client_args)\n\n\nclass S3BucketHelper(object):\n    def __init__(self, s3_client):\n        self._s3_client = s3_client\n\n    def prepare_bucket(self, s3_path):\n        bucket, key = find_bucket_key(s3_path)\n        bucket_exists = self._check_bucket_exists(bucket)\n        if not bucket_exists:\n            self._create_bucket(bucket)\n            sys.stdout.write('Using new S3 bucket: %s\\n' % bucket)\n        else:\n            sys.stdout.write('Using existing S3 bucket: %s\\n' % bucket)\n        return bucket, key\n\n    def _check_bucket_exists(self, bucket):\n        return s3_bucket_exists(self._s3_client, bucket)\n\n    def _create_bucket(self, bucket):\n        region_name = self._s3_client.meta.region_name\n        params = {\n            'Bucket': bucket\n        }\n        bucket_config = {'LocationConstraint': region_name}\n        if region_name != 'us-east-1':\n            params['CreateBucketConfiguration'] = bucket_config\n        self._s3_client.create_bucket(**params)\n\n\nclass SNSTopicHelper(object):\n    def __init__(self, sns_client):\n        self._sns_client = sns_client\n\n    def prepare_topic(self, sns_topic):\n        sns_topic_arn = sns_topic\n        # Create the topic if a name is given.\n        if not self._check_is_arn(sns_topic):\n            response = self._sns_client.create_topic(Name=sns_topic)\n            sns_topic_arn = response['TopicArn']\n            sys.stdout.write('Using new SNS topic: %s\\n' % sns_topic_arn)\n        else:\n            sys.stdout.write('Using existing SNS topic: %s\\n' % sns_topic_arn)\n        return sns_topic_arn\n\n    def _check_is_arn(self, sns_topic):\n        # The name of topic cannot contain a colon only arns have colons.\n        return ':' in sns_topic\n", "awscli/customizations/configservice/rename_cmd.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations import utils\n\n\ndef register_rename_config(cli):\n    cli.register('building-command-table.main', change_name)\n\n\ndef change_name(command_table, session, **kwargs):\n    \"\"\"\n    Change all existing ``aws config`` commands to ``aws configservice``\n    commands.\n    \"\"\"\n    utils.rename_command(command_table, 'config', 'configservice')\n", "awscli/customizations/configservice/getstatus.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport sys\n\nfrom awscli.customizations.commands import BasicCommand\n\n\ndef register_get_status(cli):\n    cli.register('building-command-table.configservice', add_get_status)\n\n\ndef add_get_status(command_table, session, **kwargs):\n    command_table['get-status'] = GetStatusCommand(session)\n\n\nclass GetStatusCommand(BasicCommand):\n    NAME = 'get-status'\n    DESCRIPTION = ('Reports the status of all of configuration '\n                   'recorders and delivery channels.')\n\n    def __init__(self, session):\n        self._config_client = None\n        super(GetStatusCommand, self).__init__(session)\n\n    def _run_main(self, parsed_args, parsed_globals):\n        self._setup_client(parsed_globals)\n        self._check_configuration_recorders()\n        self._check_delivery_channels()\n        return 0\n\n    def _setup_client(self, parsed_globals):\n        client_args = {\n            'verify': parsed_globals.verify_ssl,\n            'region_name': parsed_globals.region,\n            'endpoint_url': parsed_globals.endpoint_url\n        }\n        self._config_client = self._session.create_client('config',\n                                                          **client_args)\n\n    def _check_configuration_recorders(self):\n        status = self._config_client.describe_configuration_recorder_status()\n        sys.stdout.write('Configuration Recorders:\\n\\n')\n        for configuration_recorder in status['ConfigurationRecordersStatus']:\n            self._check_configure_recorder_status(configuration_recorder)\n            sys.stdout.write('\\n')\n\n    def _check_configure_recorder_status(self, configuration_recorder):\n        # Get the name of the recorder and print it out.\n        name = configuration_recorder['name']\n        sys.stdout.write('name: %s\\n' % name)\n\n        # Get the recording status and print it out.\n        recording = configuration_recorder['recording']\n        recording_map = {False: 'OFF', True: 'ON'}\n        sys.stdout.write('recorder: %s\\n' % recording_map[recording])\n\n        # If the recorder is on, get the last status and print it out.\n        if recording:\n            self._check_last_status(configuration_recorder)\n\n    def _check_delivery_channels(self):\n        status = self._config_client.describe_delivery_channel_status()\n        sys.stdout.write('Delivery Channels:\\n\\n')\n        for delivery_channel in status['DeliveryChannelsStatus']:\n            self._check_delivery_channel_status(delivery_channel)\n            sys.stdout.write('\\n')\n\n    def _check_delivery_channel_status(self, delivery_channel):\n        # Get the name of the delivery channel and print it out.\n        name = delivery_channel['name']\n        sys.stdout.write('name: %s\\n' % name)\n\n        # Obtain the various delivery statuses.\n        stream_delivery = delivery_channel['configStreamDeliveryInfo']\n        history_delivery = delivery_channel['configHistoryDeliveryInfo']\n        snapshot_delivery = delivery_channel['configSnapshotDeliveryInfo']\n\n        # Print the statuses out if they exist.\n        if stream_delivery:\n            self._check_last_status(stream_delivery, 'stream delivery ')\n        if history_delivery:\n            self._check_last_status(history_delivery, 'history delivery ')\n        if snapshot_delivery:\n            self._check_last_status(snapshot_delivery, 'snapshot delivery ')\n\n    def _check_last_status(self, status, status_name=''):\n        last_status = status['lastStatus']\n        sys.stdout.write('last %sstatus: %s\\n' % (status_name, last_status))\n        if last_status == \"FAILURE\":\n            sys.stdout.write('error code: %s\\n' % status['lastErrorCode'])\n            sys.stdout.write('message: %s\\n' % status['lastErrorMessage'])\n", "awscli/customizations/configservice/__init__.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "awscli/customizations/configure/addmodel.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport os\n\nfrom botocore.model import ServiceModel\n\nfrom awscli.customizations.commands import BasicCommand\n\n\ndef _get_endpoint_prefix_to_name_mappings(session):\n    # Get the mappings of endpoint prefixes to service names from the\n    # available service models.\n    prefixes_to_services = {}\n    for service_name in session.get_available_services():\n        service_model = session.get_service_model(service_name)\n        prefixes_to_services[service_model.endpoint_prefix] = service_name\n    return prefixes_to_services\n\n\ndef _get_service_name(session, endpoint_prefix):\n    if endpoint_prefix in session.get_available_services():\n        # Check if the endpoint prefix is a pre-existing service.\n        # If it is, use that endpoint prefix as the service name.\n        return endpoint_prefix\n    else:\n        # The service may have a different endpoint prefix than its name\n        # So we need to determine what the correct mapping may be.\n\n        # Figure out the mappings of endpoint prefix to service names.\n        name_mappings = _get_endpoint_prefix_to_name_mappings(session)\n        # Determine the service name from the mapping.\n        # If it does not exist in the mapping, return the original endpoint\n        # prefix.\n        return name_mappings.get(endpoint_prefix, endpoint_prefix)\n\n\ndef get_model_location(session, service_definition, service_name=None):\n    \"\"\"Gets the path of where a service-2.json file should go in ~/.aws/models\n\n    :type session: botocore.session.Session\n    :param session: A session object\n\n    :type service_definition: dict\n    :param service_definition: The json loaded service definition\n\n    :type service_name: str\n    :param service_name: The service name to use. If this not provided,\n        this will be determined from a combination of available services\n        and the service definition.\n\n    :returns: The path to where are model should be placed based on\n        the service definition and the current services in botocore.\n    \"\"\"\n    # Add the ServiceModel abstraction over the service json definition to\n    # make it easier to work with.\n    service_model = ServiceModel(service_definition)\n\n    # Determine the service_name if not provided\n    if service_name is None:\n        endpoint_prefix = service_model.endpoint_prefix\n        service_name = _get_service_name(session, endpoint_prefix)\n    api_version = service_model.api_version\n\n    # For the model location we only want the custom data path (~/.aws/models\n    # not the one set by AWS_DATA_PATH)\n    data_path = session.get_component('data_loader').CUSTOMER_DATA_PATH\n    # Use the version of the model to determine the file's naming convention.\n    service_model_name = (\n        'service-%d.json' % int(\n            float(service_definition.get('version', '2.0'))))\n    return os.path.join(data_path, service_name, api_version,\n        service_model_name)\n\n\nclass AddModelCommand(BasicCommand):\n    NAME = 'add-model'\n    DESCRIPTION = (\n        'Adds a service JSON model to the appropriate location in '\n        '~/.aws/models. Once the model gets added, CLI commands and Boto3 '\n        'clients will be immediately available for the service JSON model '\n        'provided.'\n    )\n    ARG_TABLE = [\n        {'name': 'service-model', 'required': True, 'help_text': (\n            'The contents of the service JSON model.')},\n        {'name': 'service-name', 'help_text': (\n            'Overrides the default name used by the service JSON '\n            'model to generate CLI service commands and Boto3 clients.')}\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        service_definition = json.loads(parsed_args.service_model)\n\n        # Get the path to where the model should be written\n        model_location = get_model_location(\n            self._session, service_definition, parsed_args.service_name\n        )\n\n        # If the service_name/api_version directories do not exist,\n        # then create them.\n        model_directory = os.path.dirname(model_location)\n        if not os.path.exists(model_directory):\n            os.makedirs(model_directory)\n\n        # Write the model to the specified location\n        with open(model_location, 'wb') as f:\n            f.write(parsed_args.service_model.encode('utf-8'))\n\n        return 0\n", "awscli/customizations/configure/set.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.configure.writer import ConfigFileWriter\n\nfrom . import PREDEFINED_SECTION_NAMES, profile_to_section\n\n\nclass ConfigureSetCommand(BasicCommand):\n    NAME = 'set'\n    DESCRIPTION = BasicCommand.FROM_FILE('configure', 'set',\n                                         '_description.rst')\n    SYNOPSIS = 'aws configure set varname value [--profile profile-name]'\n    EXAMPLES = BasicCommand.FROM_FILE('configure', 'set', '_examples.rst')\n    ARG_TABLE = [\n        {'name': 'varname',\n         'help_text': 'The name of the config value to set.',\n         'action': 'store',\n         'cli_type_name': 'string', 'positional_arg': True},\n        {'name': 'value',\n         'help_text': 'The value to set.',\n         'action': 'store',\n         'no_paramfile': True,  # To disable the default paramfile behavior\n         'cli_type_name': 'string', 'positional_arg': True},\n    ]\n    # Any variables specified in this list will be written to\n    # the ~/.aws/credentials file instead of ~/.aws/config.\n    _WRITE_TO_CREDS_FILE = ['aws_access_key_id', 'aws_secret_access_key',\n                            'aws_session_token']\n\n    def __init__(self, session, config_writer=None):\n        super(ConfigureSetCommand, self).__init__(session)\n        if config_writer is None:\n            config_writer = ConfigFileWriter()\n        self._config_writer = config_writer\n\n    def _get_config_file(self, path):\n        config_path = self._session.get_config_variable(path)\n        return os.path.expanduser(config_path)\n\n    def _run_main(self, args, parsed_globals):\n        varname = args.varname\n        value = args.value\n        profile = 'default'\n        # Before handing things off to the config writer,\n        # we need to find out three things:\n        # 1. What section we're writing to (profile).\n        # 2. The name of the config key (varname)\n        # 3. The actual value (value).\n        if '.' not in varname:\n            # unqualified name, scope it to the current\n            # profile (or leave it as the 'default' section if\n            # no profile is set).\n            if self._session.profile is not None:\n                profile = self._session.profile\n        else:\n            # First figure out if it's been scoped to a profile.\n            parts = varname.split('.')\n            if parts[0] in ('default', 'profile'):\n                # Then we know we're scoped to a profile.\n                if parts[0] == 'default':\n                    profile = 'default'\n                    remaining = parts[1:]\n                else:\n                    # [profile, profile_name, ...]\n                    profile = parts[1]\n                    remaining = parts[2:]\n                varname = remaining[0]\n                if len(remaining) == 2:\n                    value = {remaining[1]: value}\n            elif parts[0] not in PREDEFINED_SECTION_NAMES:\n                if self._session.profile is not None:\n                    profile = self._session.profile\n                else:\n                    profile_name = self._session.get_config_variable('profile')\n                    if profile_name is not None:\n                        profile = profile_name\n                varname = parts[0]\n                if len(parts) == 2:\n                    value = {parts[1]: value}\n            elif len(parts) == 2:\n                # Otherwise it's something like \"set preview.service true\"\n                # of something in the [plugin] section.\n                profile, varname = parts\n        config_filename = self._get_config_file('config_file')\n        if varname in self._WRITE_TO_CREDS_FILE:\n            # When writing to the creds file, the section is just the profile\n            section = profile\n            config_filename = self._get_config_file('credentials_file')\n        elif profile in PREDEFINED_SECTION_NAMES or profile == 'default':\n            section = profile\n        else:\n            section = profile_to_section(profile)\n        updated_config = {'__section__': section, varname: value}\n        self._config_writer.update_config(updated_config, config_filename)\n        return 0\n", "awscli/customizations/configure/get.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport sys\nimport logging\n\nfrom awscli.customizations.commands import BasicCommand\n\nfrom . import PREDEFINED_SECTION_NAMES\n\nLOG = logging.getLogger(__name__)\n\n\nclass ConfigureGetCommand(BasicCommand):\n    NAME = 'get'\n    DESCRIPTION = BasicCommand.FROM_FILE('configure', 'get',\n                                         '_description.rst')\n    SYNOPSIS = 'aws configure get varname [--profile profile-name]'\n    EXAMPLES = BasicCommand.FROM_FILE('configure', 'get', '_examples.rst')\n    ARG_TABLE = [\n        {'name': 'varname',\n         'help_text': 'The name of the config value to retrieve.',\n         'action': 'store',\n         'cli_type_name': 'string', 'positional_arg': True},\n    ]\n\n    def __init__(self, session, stream=None, error_stream=None):\n        super(ConfigureGetCommand, self).__init__(session)\n        if stream is None:\n            stream = sys.stdout\n        if error_stream is None:\n            error_stream = sys.stderr\n        self._stream = stream\n        self._error_stream = error_stream\n\n    def _run_main(self, args, parsed_globals):\n        varname = args.varname\n\n        if '.' not in varname:\n            # get_scoped_config() returns the config variables in the config\n            # file (not the logical_var names), which is what we want.\n            config = self._session.get_scoped_config()\n            value = config.get(varname)\n        else:\n            value = self._get_dotted_config_value(varname)\n\n        LOG.debug(u'Config value retrieved: %s' % value)\n\n        if isinstance(value, str):\n            self._stream.write(value)\n            self._stream.write('\\n')\n            return 0\n        elif isinstance(value, dict):\n            # TODO: add support for this. We would need to print it off in\n            # the same format as the config file.\n            self._error_stream.write(\n                'varname (%s) must reference a value, not a section or '\n                'sub-section.' % varname\n            )\n            return 1\n        else:\n            return 1\n\n    def _get_dotted_config_value(self, varname):\n        parts = varname.split('.')\n        num_dots = varname.count('.')\n\n        # Logic to deal with predefined sections like [preview], [plugin] and\n        # etc.\n        if num_dots == 1 and parts[0] in PREDEFINED_SECTION_NAMES:\n            full_config = self._session.full_config\n            section, config_name = varname.split('.')\n            value = full_config.get(section, {}).get(config_name)\n            if value is None:\n                # Try to retrieve it from the profile config.\n                value = full_config['profiles'].get(\n                    section, {}).get(config_name)\n            return value\n\n        if parts[0] == 'profile':\n            profile_name = parts[1]\n            config_name = parts[2]\n            remaining = parts[3:]\n        # Check if varname starts with 'default' profile (e.g.\n        # default.emr-dev.emr.instance_profile) If not, go further to check\n        # if varname starts with a known profile name\n        elif parts[0] == 'default' or (\n                parts[0] in self._session.full_config['profiles']):\n            profile_name = parts[0]\n            config_name = parts[1]\n            remaining = parts[2:]\n        else:\n            profile_name = self._session.get_config_variable('profile')\n            if profile_name is None:\n                profile_name = 'default'\n            config_name = parts[0]\n            remaining = parts[1:]\n\n        value = self._session.full_config['profiles'].get(\n            profile_name, {}).get(config_name)\n        if len(remaining) == 1:\n            try:\n                value = value.get(remaining[-1])\n            except AttributeError:\n                value = None\n        return value\n", "awscli/customizations/configure/writer.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport re\n\nfrom . import SectionNotFoundError\n\n\nclass ConfigFileWriter(object):\n    SECTION_REGEX = re.compile(r'^\\s*\\[(?P<header>[^]]+)\\]')\n    OPTION_REGEX = re.compile(\n        r'(?P<option>[^:=][^:=]*)'\n        r'\\s*(?P<vi>[:=])\\s*'\n        r'(?P<value>.*)$'\n    )\n\n    def update_config(self, new_values, config_filename):\n        \"\"\"Update config file with new values.\n\n        This method will update a section in a config file with\n        new key value pairs.\n\n        This method provides a few conveniences:\n\n        * If the ``config_filename`` does not exist, it will\n          be created.  Any parent directories will also be created\n          if necessary.\n        * If the section to update does not exist, it will be created.\n        * Any existing lines that are specified by ``new_values``\n          **will not be touched**.  This ensures that commented out\n          values are left unaltered.\n\n        :type new_values: dict\n        :param new_values: The values to update.  There is a special\n            key ``__section__``, that specifies what section in the INI\n            file to update.  If this key is not present, then the\n            ``default`` section will be updated with the new values.\n\n        :type config_filename: str\n        :param config_filename: The config filename where values will be\n            written.\n\n        \"\"\"\n        section_name = new_values.pop('__section__', 'default')\n        if not os.path.isfile(config_filename):\n            self._create_file(config_filename)\n            self._write_new_section(section_name, new_values, config_filename)\n            return\n        with open(config_filename, 'r') as f:\n            contents = f.readlines()\n        # We can only update a single section at a time so we first need\n        # to find the section in question\n        try:\n            self._update_section_contents(contents, section_name, new_values)\n            with open(config_filename, 'w') as f:\n                f.write(''.join(contents))\n        except SectionNotFoundError:\n            self._write_new_section(section_name, new_values, config_filename)\n\n    def _create_file(self, config_filename):\n        # Create the file as well as the parent dir if needed.\n        dirname = os.path.split(config_filename)[0]\n        if not os.path.isdir(dirname):\n            os.makedirs(dirname)\n        with os.fdopen(os.open(config_filename,\n                               os.O_WRONLY | os.O_CREAT, 0o600), 'w'):\n            pass\n\n    def _check_file_needs_newline(self, filename):\n        # check if the last byte is a newline\n        with open(filename, 'rb') as f:\n            # check if the file is empty\n            f.seek(0, os.SEEK_END)\n            if not f.tell():\n                return False\n            f.seek(-1, os.SEEK_END)\n            last = f.read()\n            return last != b'\\n'\n\n    def _write_new_section(self, section_name, new_values, config_filename):\n        needs_newline = self._check_file_needs_newline(config_filename)\n        with open(config_filename, 'a') as f:\n            if needs_newline:\n                f.write('\\n')\n            f.write('[%s]\\n' % section_name)\n            contents = []\n            self._insert_new_values(line_number=0,\n                                    contents=contents,\n                                    new_values=new_values)\n            f.write(''.join(contents))\n\n    def _find_section_start(self, contents, section_name):\n        for i in range(len(contents)):\n            line = contents[i]\n            if line.strip().startswith(('#', ';')):\n                # This is a comment, so we can safely ignore this line.\n                continue\n            match = self.SECTION_REGEX.search(line)\n            if match is not None and self._matches_section(match,\n                                                           section_name):\n                return i\n        raise SectionNotFoundError(section_name)\n\n    def _update_section_contents(self, contents, section_name, new_values):\n        # First, find the line where the section_name is defined.\n        # This will be the value of i.\n        new_values = new_values.copy()\n        # ``contents`` is a list of file line contents.\n        section_start_line_num = self._find_section_start(contents,\n                                                          section_name)\n        # If we get here, then we've found the section.  We now need\n        # to figure out if we're updating a value or adding a new value.\n        # There's 2 cases.  Either we're setting a normal scalar value\n        # of, we're setting a nested value.\n        last_matching_line = section_start_line_num\n        j = last_matching_line + 1\n        while j < len(contents):\n            line = contents[j]\n            if self.SECTION_REGEX.search(line) is not None:\n                # We've hit a new section which means the config key is\n                # not in the section.  We need to add it here.\n                self._insert_new_values(line_number=last_matching_line,\n                                        contents=contents,\n                                        new_values=new_values)\n                return\n            match = self.OPTION_REGEX.search(line)\n            if match is not None:\n                last_matching_line = j\n                key_name = match.group(1).strip()\n                if key_name in new_values:\n                    # We've found the line that defines the option name.\n                    # if the value is not a dict, then we can write the line\n                    # out now.\n                    if not isinstance(new_values[key_name], dict):\n                        option_value = new_values[key_name]\n                        new_line = '%s = %s\\n' % (key_name, option_value)\n                        contents[j] = new_line\n                        del new_values[key_name]\n                    else:\n                        j = self._update_subattributes(\n                            j, contents, new_values[key_name],\n                            len(match.group(1)) - len(match.group(1).lstrip()))\n                        return\n            j += 1\n\n        if new_values:\n            if not contents[-1].endswith('\\n'):\n                contents.append('\\n')\n            self._insert_new_values(line_number=last_matching_line + 1,\n                                    contents=contents,\n                                    new_values=new_values)\n\n    def _update_subattributes(self, index, contents, values, starting_indent):\n        index += 1\n        for i in range(index, len(contents)):\n            line = contents[i]\n            match = self.OPTION_REGEX.search(line)\n            if match is not None:\n                current_indent = len(\n                    match.group(1)) - len(match.group(1).lstrip())\n                key_name = match.group(1).strip()\n                if key_name in values:\n                    option_value = values[key_name]\n                    new_line = '%s%s = %s\\n' % (' ' * current_indent,\n                                                key_name, option_value)\n                    contents[i] = new_line\n                    del values[key_name]\n            if starting_indent == current_indent or \\\n                    self.SECTION_REGEX.search(line) is not None:\n                # We've arrived at the starting indent level so we can just\n                # write out all the values now.\n                self._insert_new_values(i - 1, contents, values, '    ')\n                break\n        else:\n            if starting_indent != current_indent:\n                # The option is the last option in the file\n                self._insert_new_values(i, contents, values, '    ')\n        return i\n\n    def _insert_new_values(self, line_number, contents, new_values, indent=''):\n        new_contents = []\n        for key, value in list(new_values.items()):\n            if isinstance(value, dict):\n                subindent = indent + '    '\n                new_contents.append('%s%s =\\n' % (indent, key))\n                for subkey, subval in list(value.items()):\n                    new_contents.append('%s%s = %s\\n' % (subindent, subkey,\n                                                         subval))\n            else:\n                new_contents.append('%s%s = %s\\n' % (indent, key, value))\n            del new_values[key]\n        contents.insert(line_number + 1, ''.join(new_contents))\n\n    def _matches_section(self, match, section_name):\n        parts = section_name.split(' ')\n        unquoted_match = match.group(0) == '[%s]' % section_name\n        if len(parts) > 1:\n            quoted_match = match.group(0) == '[%s \"%s\"]' % (\n                parts[0], ' '.join(parts[1:]))\n            return unquoted_match or quoted_match\n        return unquoted_match\n", "awscli/customizations/configure/__init__.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport string\nfrom awscli.compat import shlex\n\nNOT_SET = '<not set>'\nPREDEFINED_SECTION_NAMES = ('preview', 'plugins')\n_WHITESPACE = ' \\t'\n\n\nclass ConfigValue(object):\n\n    def __init__(self, value, config_type, config_variable):\n        self.value = value\n        self.config_type = config_type\n        self.config_variable = config_variable\n\n    def mask_value(self):\n        if self.value is NOT_SET:\n            return\n        self.value = mask_value(self.value)\n\n\nclass SectionNotFoundError(Exception):\n    pass\n\n\ndef mask_value(current_value):\n    if current_value is None:\n        return 'None'\n    else:\n        return ('*' * 16) + current_value[-4:]\n\n\ndef profile_to_section(profile_name):\n    \"\"\"Converts a profile name to a section header to be used in the config.\"\"\"\n    if any(c in _WHITESPACE for c in profile_name):\n        profile_name = shlex.quote(profile_name)\n    return 'profile %s' % profile_name\n", "awscli/customizations/configure/list.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport sys\n\nfrom awscli.customizations.commands import BasicCommand\n\nfrom . import ConfigValue, NOT_SET\n\n\nclass ConfigureListCommand(BasicCommand):\n    NAME = 'list'\n    DESCRIPTION = (\n        'Lists the profile, access key, secret key, and region configuration '\n        'information used for the specified profile. For each configuration '\n        'item, it shows the value, where the configuration value '\n        'was retrieved, and the configuration variable name.\\n'\n        '\\n'\n        'For example, '\n        'if you provide the AWS region in an environment variable, this '\n        'command shows you the name of the region you\\'ve configured, '\n        'that this value came from an environment '\n        'variable, and the name of the environment '\n        'variable.\\n'\n        '\\n'\n        'For temporary credential methods such as roles and IAM Identity '\n        'Center, this command displays the temporarily cached access key and '\n        'secret access key is displayed.\\n'\n    )\n    SYNOPSIS = 'aws configure list [--profile profile-name]'\n    EXAMPLES = (\n        'To show your current configuration values::\\n'\n        '\\n'\n        '  $ aws configure list\\n'\n        '        Name                    Value             Type    Location\\n'\n        '        ----                    -----             ----    --------\\n'\n        '     profile                <not set>             None    None\\n'\n        '  access_key     ****************ABCD      config_file    ~/.aws/config\\n'\n        '  secret_key     ****************ABCD      config_file    ~/.aws/config\\n'\n        '      region                us-west-2              env    AWS_DEFAULT_REGION\\n'\n        '\\n'\n    )\n\n    def __init__(self, session, stream=None):\n        super(ConfigureListCommand, self).__init__(session)\n        if stream is None:\n            stream = sys.stdout\n        self._stream = stream\n\n    def _run_main(self, args, parsed_globals):\n        self._display_config_value(ConfigValue('Value', 'Type', 'Location'),\n                                   'Name')\n        self._display_config_value(ConfigValue('-----', '----', '--------'),\n                                   '----')\n\n        if parsed_globals and parsed_globals.profile is not None:\n            profile = ConfigValue(self._session.profile, 'manual', '--profile')\n        else:\n            profile = self._lookup_config('profile')\n        self._display_config_value(profile, 'profile')\n\n        access_key, secret_key = self._lookup_credentials()\n        self._display_config_value(access_key, 'access_key')\n        self._display_config_value(secret_key, 'secret_key')\n\n        region = self._lookup_config('region')\n        self._display_config_value(region, 'region')\n        return 0\n\n    def _display_config_value(self, config_value, config_name):\n        self._stream.write('%10s %24s %16s    %s\\n' % (\n            config_name, config_value.value, config_value.config_type,\n            config_value.config_variable))\n\n    def _lookup_credentials(self):\n        # First try it with _lookup_config.  It's possible\n        # that we don't find credentials this way (for example,\n        # if we're using an IAM role).\n        access_key = self._lookup_config('access_key')\n        if access_key.value is not NOT_SET:\n            secret_key = self._lookup_config('secret_key')\n            access_key.mask_value()\n            secret_key.mask_value()\n            return access_key, secret_key\n        else:\n            # Otherwise we can try to use get_credentials().\n            # This includes a few more lookup locations\n            # (IAM roles, some of the legacy configs, etc.)\n            credentials = self._session.get_credentials()\n            if credentials is None:\n                no_config = ConfigValue(NOT_SET, None, None)\n                return no_config, no_config\n            else:\n                # For the ConfigValue, we don't track down the\n                # config_variable because that info is not\n                # visible from botocore.credentials.  I think\n                # the credentials.method is sufficient to show\n                # where the credentials are coming from.\n                access_key = ConfigValue(credentials.access_key,\n                                         credentials.method, '')\n                secret_key = ConfigValue(credentials.secret_key,\n                                         credentials.method, '')\n                access_key.mask_value()\n                secret_key.mask_value()\n                return access_key, secret_key\n\n    def _lookup_config(self, name):\n        # First try to look up the variable in the env.\n        value = self._session.get_config_variable(name, methods=('env',))\n        if value is not None:\n            return ConfigValue(value, 'env', self._session.session_var_map[name][1])\n        # Then try to look up the variable in the config file.\n        value = self._session.get_config_variable(name, methods=('config',))\n        if value is not None:\n            return ConfigValue(value, 'config-file',\n                               self._session.get_config_variable('config_file'))\n        else:\n            return ConfigValue(NOT_SET, None, None)\n", "awscli/customizations/configure/configure.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport logging\n\nfrom botocore.exceptions import ProfileNotFound\n\nfrom awscli.compat import compat_input\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.configure.addmodel import AddModelCommand\nfrom awscli.customizations.configure.set import ConfigureSetCommand\nfrom awscli.customizations.configure.get import ConfigureGetCommand\nfrom awscli.customizations.configure.list import ConfigureListCommand\nfrom awscli.customizations.configure.writer import ConfigFileWriter\n\nfrom . import mask_value, profile_to_section\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef register_configure_cmd(cli):\n    cli.register('building-command-table.main',\n                 ConfigureCommand.add_command)\n\n\nclass InteractivePrompter(object):\n\n    def get_value(self, current_value, config_name, prompt_text=''):\n        if config_name in ('aws_access_key_id', 'aws_secret_access_key'):\n            current_value = mask_value(current_value)\n        response = compat_input(\"%s [%s]: \" % (prompt_text, current_value))\n        if not response:\n            # If the user hits enter, we return a value of None\n            # instead of an empty string.  That way we can determine\n            # whether or not a value has changed.\n            response = None\n        return response\n\n\nclass ConfigureCommand(BasicCommand):\n    NAME = 'configure'\n    DESCRIPTION = BasicCommand.FROM_FILE()\n    SYNOPSIS = ('aws configure [--profile profile-name]')\n    EXAMPLES = (\n        'To create a new configuration::\\n'\n        '\\n'\n        '    $ aws configure\\n'\n        '    AWS Access Key ID [None]: accesskey\\n'\n        '    AWS Secret Access Key [None]: secretkey\\n'\n        '    Default region name [None]: us-west-2\\n'\n        '    Default output format [None]:\\n'\n        '\\n'\n        'To update just the region name::\\n'\n        '\\n'\n        '    $ aws configure\\n'\n        '    AWS Access Key ID [****]:\\n'\n        '    AWS Secret Access Key [****]:\\n'\n        '    Default region name [us-west-1]: us-west-2\\n'\n        '    Default output format [None]:\\n'\n    )\n    SUBCOMMANDS = [\n        {'name': 'list', 'command_class': ConfigureListCommand},\n        {'name': 'get', 'command_class': ConfigureGetCommand},\n        {'name': 'set', 'command_class': ConfigureSetCommand},\n        {'name': 'add-model', 'command_class': AddModelCommand}\n    ]\n\n    # If you want to add new values to prompt, update this list here.\n    VALUES_TO_PROMPT = [\n        # (logical_name, config_name, prompt_text)\n        ('aws_access_key_id', \"AWS Access Key ID\"),\n        ('aws_secret_access_key', \"AWS Secret Access Key\"),\n        ('region', \"Default region name\"),\n        ('output', \"Default output format\"),\n    ]\n\n    def __init__(self, session, prompter=None, config_writer=None):\n        super(ConfigureCommand, self).__init__(session)\n        if prompter is None:\n            prompter = InteractivePrompter()\n        self._prompter = prompter\n        if config_writer is None:\n            config_writer = ConfigFileWriter()\n        self._config_writer = config_writer\n\n    def _run_main(self, parsed_args, parsed_globals):\n        # Called when invoked with no args \"aws configure\"\n        new_values = {}\n        # This is the config from the config file scoped to a specific\n        # profile.\n        try:\n            config = self._session.get_scoped_config()\n        except ProfileNotFound:\n            config = {}\n        for config_name, prompt_text in self.VALUES_TO_PROMPT:\n            current_value = config.get(config_name)\n            new_value = self._prompter.get_value(current_value, config_name,\n                                                 prompt_text)\n            if new_value is not None and new_value != current_value:\n                new_values[config_name] = new_value\n        config_filename = os.path.expanduser(\n            self._session.get_config_variable('config_file'))\n        if new_values:\n            profile = self._session.profile\n            self._write_out_creds_file_values(new_values, profile)\n            if profile is not None:\n                section = profile_to_section(profile)\n                new_values['__section__'] = section\n            self._config_writer.update_config(new_values, config_filename)\n\n    def _write_out_creds_file_values(self, new_values, profile_name):\n        # The access_key/secret_key are now *always* written to the shared\n        # credentials file (~/.aws/credentials), see aws/aws-cli#847.\n        # post-conditions: ~/.aws/credentials will have the updated credential\n        # file values and new_values will have the cred vars removed.\n        credential_file_values = {}\n        if 'aws_access_key_id' in new_values:\n            credential_file_values['aws_access_key_id'] = new_values.pop(\n                'aws_access_key_id')\n        if 'aws_secret_access_key' in new_values:\n            credential_file_values['aws_secret_access_key'] = new_values.pop(\n                'aws_secret_access_key')\n        if credential_file_values:\n            if profile_name is not None:\n                credential_file_values['__section__'] = profile_name\n            shared_credentials_filename = os.path.expanduser(\n                self._session.get_config_variable('credentials_file'))\n            self._config_writer.update_config(\n                credential_file_values,\n                shared_credentials_filename)\n", "awscli/customizations/ecs/exceptions.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nclass ECSError(Exception):\n    \"\"\" Base class for all ECSErrors.\"\"\"\n    fmt = 'An unspecified error occurred'\n\n    def __init__(self, **kwargs):\n        msg = self.fmt.format(**kwargs)\n        super(ECSError, self).__init__(msg)\n        self.kwargs = kwargs\n\n\nclass MissingPropertyError(ECSError):\n    fmt = \\\n        \"Error: Resource '{resource}' must include property '{prop_name}'\"\n\n\nclass FileLoadError(ECSError):\n    fmt = \"Error: Unable to load file at {file_path}: {error}\"\n\n\nclass InvalidPlatformError(ECSError):\n    fmt = \"Error: {resource} '{name}' must support 'ECS' compute platform\"\n\n\nclass InvalidProperyError(ECSError):\n    fmt = (\"Error: deployment group '{dg_name}' does not target \"\n           \"ECS {resource} '{resource_name}'\")\n\n\nclass InvalidServiceError(ECSError):\n    fmt = \"Error: Service '{service}' not found in cluster '{cluster}'\"\n\n\nclass ServiceClientError(ECSError):\n    fmt = \"Failed to {action}:\\n{error}\"", "awscli/customizations/ecs/deploy.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport hashlib\nimport json\nimport os\nimport sys\n\nfrom botocore import compat, config\nfrom botocore.exceptions import ClientError\nfrom awscli.compat import compat_open\nfrom awscli.customizations.ecs import exceptions, filehelpers\nfrom awscli.customizations.commands import BasicCommand\n\nTIMEOUT_BUFFER_MIN = 10\nDEFAULT_DELAY_SEC = 15\nMAX_WAIT_MIN = 360  # 6 hours\n\n\nclass ECSDeploy(BasicCommand):\n    NAME = 'deploy'\n\n    DESCRIPTION = (\n        \"Deploys a new task definition to the specified ECS service. \"\n        \"Only services that use CodeDeploy for deployments are supported. \"\n        \"This command will register a new task definition, update the \"\n        \"CodeDeploy appspec with the new task definition revision, create a \"\n        \"CodeDeploy deployment, and wait for the deployment to successfully \"\n        \"complete. This command will exit with a return code of 255 if the \"\n        \"deployment does not succeed within 30 minutes by default or \"\n        \"up to 10 minutes more than your deployment group's configured wait \"\n        \"time (max of 6 hours).\"\n    )\n\n    ARG_TABLE = [\n        {\n            'name': 'service',\n            'help_text': (\"The short name or full Amazon Resource Name \"\n                          \"(ARN) of the service to update\"),\n            'required': True\n        },\n        {\n            'name': 'task-definition',\n            'help_text': (\"The file path where your task definition file is \"\n                          \"located. The format of the file must be the same \"\n                          \"as the JSON output of: <codeblock>aws ecs \"\n                          \"register-task-definition \"\n                          \"--generate-cli-skeleton</codeblock>\"),\n            'required': True\n        },\n        {\n            'name': 'codedeploy-appspec',\n            'help_text': (\"The file path where your AWS CodeDeploy appspec \"\n                          \"file is located. The appspec file may be in JSON \"\n                          \"or YAML format. The <code>TaskDefinition</code> \"\n                          \"property will be updated within the appspec with \"\n                          \"the newly registered task definition ARN, \"\n                          \"overwriting any placeholder values in the file.\"),\n            'required': True\n        },\n        {\n            'name': 'cluster',\n            'help_text': (\"The short name or full Amazon Resource Name \"\n                          \"(ARN) of the cluster that your service is \"\n                          \"running within. If you do not specify a \"\n                          \"cluster, the \\\"default\\\" cluster is assumed.\"),\n            'required': False\n        },\n        {\n            'name': 'codedeploy-application',\n            'help_text': (\"The name of the AWS CodeDeploy application \"\n                          \"to use for the deployment. The specified \"\n                          \"application must use the 'ECS' compute \"\n                          \"platform. If you do not specify an \"\n                          \"application, the application name \"\n                          \"<code>AppECS-[CLUSTER_NAME]-[SERVICE_NAME]</code> \"\n                          \"is assumed.\"),\n            'required': False\n        },\n        {\n            'name': 'codedeploy-deployment-group',\n            'help_text': (\"The name of the AWS CodeDeploy deployment \"\n                          \"group to use for the deployment. The \"\n                          \"specified deployment group must be associated \"\n                          \"with the specified ECS service and cluster. \"\n                          \"If you do not specify a deployment group, \"\n                          \"the deployment group name \"\n                          \"<code>DgpECS-[CLUSTER_NAME]-[SERVICE_NAME]</code> \"\n                          \"is assumed.\"),\n            'required': False\n        }\n    ]\n\n    MSG_TASK_DEF_REGISTERED = \\\n        \"Successfully registered new ECS task definition {arn}\\n\"\n\n    MSG_CREATED_DEPLOYMENT = \"Successfully created deployment {id}\\n\"\n\n    MSG_SUCCESS = (\"Successfully deployed {task_def} to \"\n                   \"service '{service}'\\n\")\n\n    USER_AGENT_EXTRA = 'customization/ecs-deploy'\n\n    def _run_main(self, parsed_args, parsed_globals):\n\n        register_task_def_kwargs, appspec_obj = \\\n            self._load_file_args(parsed_args.task_definition,\n                                 parsed_args.codedeploy_appspec)\n\n        ecs_client_wrapper = ECSClient(\n            self._session, parsed_args, parsed_globals, self.USER_AGENT_EXTRA)\n\n        self.resources = self._get_resource_names(\n            parsed_args, ecs_client_wrapper)\n\n        codedeploy_client = self._session.create_client(\n            'codedeploy',\n            region_name=parsed_globals.region,\n            verify=parsed_globals.verify_ssl,\n            config=config.Config(user_agent_extra=self.USER_AGENT_EXTRA))\n\n        self._validate_code_deploy_resources(codedeploy_client)\n\n        self.wait_time = self._cd_validator.get_deployment_wait_time()\n\n        self.task_def_arn = self._register_task_def(\n            register_task_def_kwargs, ecs_client_wrapper)\n\n        self._create_and_wait_for_deployment(codedeploy_client, appspec_obj)\n\n    def _create_and_wait_for_deployment(self, client, appspec):\n        deployer = CodeDeployer(client, appspec)\n        deployer.update_task_def_arn(self.task_def_arn)\n        deployment_id = deployer.create_deployment(\n            self.resources['app_name'],\n            self.resources['deployment_group_name'])\n\n        sys.stdout.write(self.MSG_CREATED_DEPLOYMENT.format(\n            id=deployment_id))\n\n        deployer.wait_for_deploy_success(deployment_id, self.wait_time)\n        service_name = self.resources['service']\n\n        sys.stdout.write(\n            self.MSG_SUCCESS.format(\n                task_def=self.task_def_arn, service=service_name))\n        sys.stdout.flush()\n\n    def _get_file_contents(self, file_path):\n        full_path = os.path.expandvars(os.path.expanduser(file_path))\n        try:\n            with compat_open(full_path) as f:\n                return f.read()\n        except (OSError, IOError, UnicodeDecodeError) as e:\n            raise exceptions.FileLoadError(\n                file_path=file_path, error=e)\n\n    def _get_resource_names(self, args, ecs_client):\n        service_details = ecs_client.get_service_details()\n        service_name = service_details['service_name']\n        cluster_name = service_details['cluster_name']\n\n        application_name = filehelpers.get_app_name(\n            service_name, cluster_name, args.codedeploy_application)\n        deployment_group_name = filehelpers.get_deploy_group_name(\n            service_name, cluster_name, args.codedeploy_deployment_group)\n\n        return {\n            'service': service_name,\n            'service_arn': service_details['service_arn'],\n            'cluster': cluster_name,\n            'cluster_arn': service_details['cluster_arn'],\n            'app_name': application_name,\n            'deployment_group_name': deployment_group_name\n        }\n\n    def _load_file_args(self, task_def_arg, appspec_arg):\n        task_def_string = self._get_file_contents(task_def_arg)\n        register_task_def_kwargs = json.loads(task_def_string)\n\n        appspec_string = self._get_file_contents(appspec_arg)\n        appspec_obj = filehelpers.parse_appspec(appspec_string)\n\n        return register_task_def_kwargs, appspec_obj\n\n    def _register_task_def(self, task_def_kwargs, ecs_client):\n        response = ecs_client.register_task_definition(task_def_kwargs)\n\n        task_def_arn = response['taskDefinition']['taskDefinitionArn']\n\n        sys.stdout.write(self.MSG_TASK_DEF_REGISTERED.format(\n            arn=task_def_arn))\n        sys.stdout.flush()\n\n        return task_def_arn\n\n    def _validate_code_deploy_resources(self, client):\n        validator = CodeDeployValidator(client, self.resources)\n        validator.describe_cd_resources()\n        validator.validate_all()\n        self._cd_validator = validator\n\n\nclass CodeDeployer():\n\n    MSG_WAITING = (\"Waiting for {deployment_id} to succeed \"\n                   \"(will wait up to {wait} minutes)...\\n\")\n\n    def __init__(self, cd_client, appspec_dict):\n        self._client = cd_client\n        self._appspec_dict = appspec_dict\n\n    def create_deployment(self, app_name, deploy_grp_name):\n        request_obj = self._get_create_deploy_request(\n            app_name, deploy_grp_name)\n\n        try:\n            response = self._client.create_deployment(**request_obj)\n        except ClientError as e:\n            raise exceptions.ServiceClientError(\n                action='create deployment', error=e)\n\n        return response['deploymentId']\n\n    def _get_appspec_hash(self):\n        appspec_str = json.dumps(self._appspec_dict)\n        appspec_encoded = compat.ensure_bytes(appspec_str)\n        return hashlib.sha256(appspec_encoded).hexdigest()\n\n    def _get_create_deploy_request(self, app_name, deploy_grp_name):\n        return {\n            \"applicationName\": app_name,\n            \"deploymentGroupName\": deploy_grp_name,\n            \"revision\": {\n                \"revisionType\": \"AppSpecContent\",\n                \"appSpecContent\": {\n                    \"content\": json.dumps(self._appspec_dict),\n                    \"sha256\": self._get_appspec_hash()\n                }\n            }\n        }\n\n    def update_task_def_arn(self, new_arn):\n        \"\"\"\n        Inserts the ARN of the previously created ECS task definition\n        into the provided appspec.\n\n        Expected format of ECS appspec (YAML) is:\n            version: 0.0\n            resources:\n              - <service-name>:\n                  type: AWS::ECS::Service\n                  properties:\n                    taskDefinition: <value>  # replace this\n                    loadBalancerInfo:\n                      containerName: <value>\n                      containerPort: <value>\n        \"\"\"\n        appspec_obj = self._appspec_dict\n\n        resources_key = filehelpers.find_required_key(\n            'codedeploy-appspec', appspec_obj, 'resources')\n        updated_resources = []\n\n        # 'resources' is a list of string:obj dictionaries\n        for resource in appspec_obj[resources_key]:\n            for name in resource:\n                # get content of resource\n                resource_content = resource[name]\n                # get resource properties\n                properties_key = filehelpers.find_required_key(\n                    name, resource_content, 'properties')\n                properties_content = resource_content[properties_key]\n                # find task definition property\n                task_def_key = filehelpers.find_required_key(\n                    properties_key, properties_content, 'taskDefinition')\n\n                # insert new task def ARN into resource\n                properties_content[task_def_key] = new_arn\n\n            updated_resources.append(resource)\n\n        appspec_obj[resources_key] = updated_resources\n        self._appspec_dict = appspec_obj\n\n    def wait_for_deploy_success(self, id, wait_min):\n        waiter = self._client.get_waiter(\"deployment_successful\")\n\n        if wait_min is not None and wait_min > MAX_WAIT_MIN:\n            wait_min = MAX_WAIT_MIN\n\n        elif wait_min is None or wait_min < 30:\n            wait_min = 30\n\n        delay_sec = DEFAULT_DELAY_SEC\n        max_attempts = (wait_min * 60) / delay_sec\n        config = {\n            'Delay': delay_sec,\n            'MaxAttempts': max_attempts\n        }\n\n        self._show_deploy_wait_msg(id, wait_min)\n        waiter.wait(deploymentId=id, WaiterConfig=config)\n\n    def _show_deploy_wait_msg(self, id, wait_min):\n        sys.stdout.write(\n            self.MSG_WAITING.format(deployment_id=id,\n                                    wait=wait_min))\n        sys.stdout.flush()\n\n\nclass CodeDeployValidator():\n    def __init__(self, cd_client, resources):\n        self._client = cd_client\n        self._resource_names = resources\n\n    def describe_cd_resources(self):\n        try:\n            self.app_details = self._client.get_application(\n                applicationName=self._resource_names['app_name'])\n        except ClientError as e:\n            raise exceptions.ServiceClientError(\n                action='describe Code Deploy application', error=e)\n\n        try:\n            dgp = self._resource_names['deployment_group_name']\n            app = self._resource_names['app_name']\n            self.deployment_group_details = self._client.get_deployment_group(\n                applicationName=app, deploymentGroupName=dgp)\n        except ClientError as e:\n            raise exceptions.ServiceClientError(\n                action='describe Code Deploy deployment group', error=e)\n\n    def get_deployment_wait_time(self):\n\n        if (not hasattr(self, 'deployment_group_details') or\n                self.deployment_group_details is None):\n            return None\n        else:\n            dgp_info = self.deployment_group_details['deploymentGroupInfo']\n            blue_green_info = dgp_info['blueGreenDeploymentConfiguration']\n\n            deploy_ready_wait_min = \\\n                blue_green_info['deploymentReadyOption']['waitTimeInMinutes']\n\n            terminate_key = 'terminateBlueInstancesOnDeploymentSuccess'\n            termination_wait_min = \\\n                blue_green_info[terminate_key]['terminationWaitTimeInMinutes']\n\n            configured_wait = deploy_ready_wait_min + termination_wait_min\n\n            return configured_wait + TIMEOUT_BUFFER_MIN\n\n    def validate_all(self):\n        self.validate_application()\n        self.validate_deployment_group()\n\n    def validate_application(self):\n        app_name = self._resource_names['app_name']\n        if self.app_details['application']['computePlatform'] != 'ECS':\n            raise exceptions.InvalidPlatformError(\n                resource='Application', name=app_name)\n\n    def validate_deployment_group(self):\n        dgp = self._resource_names['deployment_group_name']\n        service = self._resource_names['service']\n        service_arn = self._resource_names['service_arn']\n        cluster = self._resource_names['cluster']\n        cluster_arn = self._resource_names['cluster_arn']\n\n        grp_info = self.deployment_group_details['deploymentGroupInfo']\n        compute_platform = grp_info['computePlatform']\n\n        if compute_platform != 'ECS':\n            raise exceptions.InvalidPlatformError(\n                resource='Deployment Group', name=dgp)\n\n        target_services = \\\n            self.deployment_group_details['deploymentGroupInfo']['ecsServices']\n\n        # either ECS resource names or ARNs can be stored, so check both\n        for target in target_services:\n            target_serv = target['serviceName']\n            if target_serv != service and target_serv != service_arn:\n                raise exceptions.InvalidProperyError(\n                    dg_name=dgp, resource='service', resource_name=service)\n\n            target_cluster = target['clusterName']\n            if target_cluster != cluster and target_cluster != cluster_arn:\n                raise exceptions.InvalidProperyError(\n                    dg_name=dgp, resource='cluster', resource_name=cluster)\n\n\nclass ECSClient():\n\n    def __init__(self, session, parsed_args, parsed_globals, user_agent_extra):\n        self._args = parsed_args\n        self._custom_config = config.Config(user_agent_extra=user_agent_extra)\n        self._client = session.create_client(\n            'ecs',\n            region_name=parsed_globals.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl,\n            config=self._custom_config)\n\n    def get_service_details(self):\n        cluster = self._args.cluster\n\n        if cluster is None or '':\n            cluster = 'default'\n\n        try:\n            service_response = self._client.describe_services(\n                cluster=cluster, services=[self._args.service])\n        except ClientError as e:\n            raise exceptions.ServiceClientError(\n                action='describe ECS service', error=e)\n\n        if len(service_response['services']) == 0:\n            raise exceptions.InvalidServiceError(\n                service=self._args.service, cluster=cluster)\n\n        service_details = service_response['services'][0]\n        cluster_name = \\\n            filehelpers.get_cluster_name_from_arn(\n                service_details['clusterArn'])\n\n        return {\n            'service_arn': service_details['serviceArn'],\n            'service_name': service_details['serviceName'],\n            'cluster_arn': service_details['clusterArn'],\n            'cluster_name': cluster_name\n        }\n\n    def register_task_definition(self, kwargs):\n        try:\n            response = \\\n                self._client.register_task_definition(**kwargs)\n        except ClientError as e:\n            raise exceptions.ServiceClientError(\n                action='register ECS task definition', error=e)\n\n        return response\n", "awscli/customizations/ecs/filehelpers.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport json\nimport yaml\n\nfrom awscli.customizations.ecs import exceptions\n\nMAX_CHAR_LENGTH = 46\nAPP_PREFIX = 'AppECS-'\nDGP_PREFIX = 'DgpECS-'\n\n\ndef find_required_key(resource_name, obj, key):\n\n    if obj is None:\n        raise exceptions.MissingPropertyError(\n            resource=resource_name, prop_name=key)\n\n    result = _get_case_insensitive_key(obj, key)\n\n    if result is None:\n        raise exceptions.MissingPropertyError(\n            resource=resource_name, prop_name=key)\n    else:\n        return result\n\n\ndef _get_case_insensitive_key(target_obj, target_key):\n    key_to_match = target_key.lower()\n    key_list = target_obj.keys()\n\n    for key in key_list:\n        if key.lower() == key_to_match:\n            return key\n\n\ndef get_app_name(service, cluster, app_value):\n    if app_value is not None:\n        return app_value\n    else:\n        suffix = _get_ecs_suffix(service, cluster)\n        return APP_PREFIX + suffix\n\n\ndef get_cluster_name_from_arn(arn):\n    return arn.split('/')[1]\n\n\ndef get_deploy_group_name(service, cluster, dg_value):\n    if dg_value is not None:\n        return dg_value\n    else:\n        suffix = _get_ecs_suffix(service, cluster)\n        return DGP_PREFIX + suffix\n\n\ndef _get_ecs_suffix(service, cluster):\n    if cluster is None:\n        cluster_name = 'default'\n    else:\n        cluster_name = cluster[:MAX_CHAR_LENGTH]\n\n    return cluster_name + '-' + service[:MAX_CHAR_LENGTH]\n\n\ndef parse_appspec(appspec_str):\n    try:\n        return json.loads(appspec_str)\n    except ValueError:\n        return yaml.safe_load(appspec_str)\n", "awscli/customizations/ecs/executecommand.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport json\nimport errno\n\nfrom subprocess import check_call\nfrom awscli.compat import ignore_user_entered_signals\nfrom awscli.clidriver import ServiceOperation, CLIOperationCaller\n\nlogger = logging.getLogger(__name__)\n\nERROR_MESSAGE = (\n    'SessionManagerPlugin is not found. ',\n    'Please refer to SessionManager Documentation here: ',\n    'http://docs.aws.amazon.com/console/systems-manager/',\n    'session-manager-plugin-not-found'\n)\n\nTASK_NOT_FOUND = (\n    'The task provided in the request was '\n    'not found.'\n)\n\n\nclass ECSExecuteCommand(ServiceOperation):\n\n    def create_help_command(self):\n        help_command = super(ECSExecuteCommand, self).create_help_command()\n        # change the output shape because the command provides no output.\n        self._operation_model.output_shape = None\n        return help_command\n\n\ndef get_container_runtime_id(client, container_name, task_id, cluster_name):\n    describe_tasks_params = {\n        \"cluster\": cluster_name,\n        \"tasks\": [task_id]\n    }\n    describe_tasks_response = client.describe_tasks(**describe_tasks_params)\n    # need to fail here if task has failed in the intermediate time\n    tasks = describe_tasks_response['tasks']\n    if not tasks:\n        raise ValueError(TASK_NOT_FOUND)\n    response = describe_tasks_response['tasks'][0]['containers']\n    for container in response:\n        if container_name == container['name']:\n            return container['runtimeId']\n\n\ndef build_ssm_request_paramaters(response, client):\n    cluster_name = response['clusterArn'].split('/')[-1]\n    task_id = response['taskArn'].split('/')[-1]\n    container_name = response['containerName']\n    # in order to get container run-time id\n    # we need to make a call to describe-tasks\n    container_runtime_id = \\\n        get_container_runtime_id(client, container_name,\n                                 task_id, cluster_name)\n    target = \"ecs:{}_{}_{}\".format(cluster_name, task_id,\n                                   container_runtime_id)\n    ssm_request_params = {\"Target\": target}\n    return ssm_request_params\n\n\nclass ExecuteCommandCaller(CLIOperationCaller):\n    def invoke(self, service_name, operation_name, parameters, parsed_globals):\n        try:\n            # making an execute-command call to connect to an\n            # active session on a container would require\n            # session-manager-plugin to be installed on the client machine.\n            # Hence, making this empty session-manager-plugin call\n            # before calling execute-command to ensure that\n            # session-manager-plugin is installed\n            # before execute-command-command is made\n            check_call([\"session-manager-plugin\"])\n            client = self._session.create_client(\n                service_name, region_name=parsed_globals.region,\n                endpoint_url=parsed_globals.endpoint_url,\n                verify=parsed_globals.verify_ssl)\n            response = client.execute_command(**parameters)\n            region_name = client.meta.region_name\n            profile_name = self._session.profile \\\n                if self._session.profile is not None else ''\n            endpoint_url = client.meta.endpoint_url\n            ssm_request_params = build_ssm_request_paramaters(response, client)\n            # ignore_user_entered_signals ignores these signals\n            # because if signals which kills the process are not\n            # captured would kill the foreground process but not the\n            # background one. Capturing these would prevents process\n            # from getting killed and these signals are input to plugin\n            # and handling in there\n            with ignore_user_entered_signals():\n                # call executable with necessary input\n                check_call([\"session-manager-plugin\",\n                            json.dumps(response['session']),\n                            region_name,\n                            \"StartSession\",\n                            profile_name,\n                            json.dumps(ssm_request_params),\n                            endpoint_url])\n            return 0\n        except OSError as ex:\n            if ex.errno == errno.ENOENT:\n                logger.debug('SessionManagerPlugin is not present',\n                             exc_info=True)\n                raise ValueError(''.join(ERROR_MESSAGE))\n", "awscli/customizations/ecs/__init__.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.ecs.deploy import ECSDeploy\nfrom awscli.customizations.ecs.executecommand import ECSExecuteCommand\nfrom awscli.customizations.ecs.executecommand import ExecuteCommandCaller\n\n\ndef initialize(cli):\n    \"\"\"\n    The entry point for ECS high level commands.\n    \"\"\"\n    cli.register('building-command-table.ecs', inject_commands)\n\n\ndef inject_commands(command_table, session, **kwargs):\n    \"\"\"\n    Called when the ECS command table is being built. Used to inject new\n    high level commands into the command list.\n    \"\"\"\n    command_table['deploy'] = ECSDeploy(session)\n    command_table['execute-command'] = ECSExecuteCommand(\n        name='execute-command',\n        parent_name='ecs',\n        session=session,\n        operation_model=session.get_service_model('ecs')\n                    .operation_model('ExecuteCommand'),\n        operation_caller=ExecuteCommandCaller(session),\n    )\n", "awscli/customizations/cloudtrail/subscribe.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport logging\nimport sys\n\nfrom .utils import get_account_id\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.utils import s3_bucket_exists\nfrom botocore.exceptions import ClientError\n\n\nLOG = logging.getLogger(__name__)\nS3_POLICY_TEMPLATE = 'policy/S3/AWSCloudTrail-S3BucketPolicy-2014-12-17.json'\nSNS_POLICY_TEMPLATE = 'policy/SNS/AWSCloudTrail-SnsTopicPolicy-2014-12-17.json'\n\n\nclass CloudTrailError(Exception):\n    pass\n\n\nclass CloudTrailSubscribe(BasicCommand):\n    \"\"\"\n    Subscribe/update a user account to CloudTrail, creating the required S3 bucket,\n    the optional SNS topic, and starting the CloudTrail monitoring and logging.\n    \"\"\"\n    NAME = 'create-subscription'\n    DESCRIPTION = ('Creates and configures the AWS resources necessary to use'\n                   ' CloudTrail, creates a trail using those resources, and '\n                   'turns on logging.')\n    SYNOPSIS = ('aws cloudtrail create-subscription'\n                ' (--s3-use-bucket|--s3-new-bucket) bucket-name'\n                ' [--sns-new-topic topic-name]\\n')\n\n    ARG_TABLE = [\n        {'name': 'name', 'required': True, 'help_text': 'Cloudtrail name'},\n        {'name': 's3-new-bucket',\n         'help_text': 'Create a new S3 bucket with this name'},\n        {'name': 's3-use-bucket',\n         'help_text': 'Use an existing S3 bucket with this name'},\n        {'name': 's3-prefix', 'help_text': 'S3 object prefix'},\n        {'name': 'sns-new-topic',\n         'help_text': 'Create a new SNS topic with this name'},\n        {'name': 'include-global-service-events',\n         'help_text': 'Whether to include global service events'},\n        {'name': 's3-custom-policy',\n         'help_text': 'Custom S3 policy template or URL'},\n        {'name': 'sns-custom-policy',\n         'help_text': 'Custom SNS policy template or URL'}\n    ]\n    UPDATE = False\n    _UNDOCUMENTED = True\n\n    def _run_main(self, args, parsed_globals):\n        self.setup_services(args, parsed_globals)\n        # Run the command and report success\n        self._call(args, parsed_globals)\n\n        return 0\n\n    def setup_services(self, args, parsed_globals):\n        client_args = {\n            'region_name': None,\n            'verify': None\n        }\n        if parsed_globals.region is not None:\n            client_args['region_name'] = parsed_globals.region\n        if parsed_globals.verify_ssl is not None:\n            client_args['verify'] = parsed_globals.verify_ssl\n\n        # Initialize services\n        LOG.debug('Initializing S3, SNS and CloudTrail...')\n        self.sts = self._session.create_client('sts', **client_args)\n        self.s3 = self._session.create_client('s3', **client_args)\n        self.sns = self._session.create_client('sns', **client_args)\n        self.region_name = self.s3.meta.region_name\n\n        # If the endpoint is specified, it is designated for the cloudtrail\n        # service. Not all of the other services will use it.\n        if parsed_globals.endpoint_url is not None:\n            client_args['endpoint_url'] = parsed_globals.endpoint_url\n        self.cloudtrail = self._session.create_client('cloudtrail', **client_args)\n\n    def _call(self, options, parsed_globals):\n        \"\"\"\n        Run the command. Calls various services based on input options and\n        outputs the final CloudTrail configuration.\n        \"\"\"\n        gse = options.include_global_service_events\n        if gse:\n            if gse.lower() == 'true':\n                gse = True\n            elif gse.lower() == 'false':\n                gse = False\n            else:\n                raise ValueError('You must pass either true or false to'\n                                 ' --include-global-service-events.')\n\n        bucket = options.s3_use_bucket\n\n        if options.s3_new_bucket:\n            bucket = options.s3_new_bucket\n\n            if self.UPDATE and options.s3_prefix is None:\n                # Prefix was not passed and this is updating the S3 bucket,\n                # so let's find the existing prefix and use that if possible\n                res = self.cloudtrail.describe_trails(\n                    trailNameList=[options.name])\n                trail_info = res['trailList'][0]\n\n                if 'S3KeyPrefix' in trail_info:\n                    LOG.debug('Setting S3 prefix to {0}'.format(\n                        trail_info['S3KeyPrefix']))\n                    options.s3_prefix = trail_info['S3KeyPrefix']\n\n            self.setup_new_bucket(bucket, options.s3_prefix,\n                                  options.s3_custom_policy)\n        elif not bucket and not self.UPDATE:\n            # No bucket was passed for creation.\n            raise ValueError('You must pass either --s3-use-bucket or'\n                             ' --s3-new-bucket to create.')\n\n        if options.sns_new_topic:\n            try:\n                topic_result = self.setup_new_topic(options.sns_new_topic,\n                                                    options.sns_custom_policy)\n            except Exception:\n                # Roll back any S3 bucket creation\n                if options.s3_new_bucket:\n                    self.s3.delete_bucket(Bucket=options.s3_new_bucket)\n                raise\n\n        try:\n            cloudtrail_config = self.upsert_cloudtrail_config(\n                options.name,\n                bucket,\n                options.s3_prefix,\n                options.sns_new_topic,\n                gse\n            )\n        except Exception:\n            # Roll back any S3 bucket / SNS topic creations\n            if options.s3_new_bucket:\n                self.s3.delete_bucket(Bucket=options.s3_new_bucket)\n            if options.sns_new_topic:\n                self.sns.delete_topic(TopicArn=topic_result['TopicArn'])\n            raise\n\n        sys.stdout.write('CloudTrail configuration:\\n{config}\\n'.format(\n            config=json.dumps(cloudtrail_config, indent=2)))\n\n        if not self.UPDATE:\n            # If the configure call command above completes then this should\n            # have a really high chance of also completing\n            self.start_cloudtrail(options.name)\n\n            sys.stdout.write(\n                'Logs will be delivered to {bucket}:{prefix}\\n'.format(\n                    bucket=bucket, prefix=options.s3_prefix or ''))\n\n    def _get_policy(self, key_name):\n        try:\n            data = self.s3.get_object(\n                Bucket='awscloudtrail-policy-' + self.region_name,\n                Key=key_name)\n            return data['Body'].read().decode('utf-8')\n        except Exception as e:\n            raise CloudTrailError(\n                'Unable to get regional policy template for'\n                ' region %s: %s. Error: %s', self.region_name, key_name, e)\n\n    def setup_new_bucket(self, bucket, prefix, custom_policy=None):\n        \"\"\"\n        Creates a new S3 bucket with an appropriate policy to let CloudTrail\n        write to the prefix path.\n        \"\"\"\n        sys.stdout.write(\n            'Setting up new S3 bucket {bucket}...\\n'.format(bucket=bucket))\n\n        account_id = get_account_id(self.sts)\n\n        # Clean up the prefix - it requires a trailing slash if set\n        if prefix and not prefix.endswith('/'):\n            prefix += '/'\n\n        # Fetch policy data from S3 or a custom URL\n        if custom_policy is not None:\n            policy = custom_policy\n        else:\n            policy = self._get_policy(S3_POLICY_TEMPLATE)\n\n        policy = policy.replace('<BucketName>', bucket)\\\n                       .replace('<CustomerAccountID>', account_id)\n\n        if '<Prefix>/' in policy:\n            policy = policy.replace('<Prefix>/', prefix or '')\n        else:\n            policy = policy.replace('<Prefix>', prefix or '')\n\n        LOG.debug('Bucket policy:\\n{0}'.format(policy))\n        bucket_exists = s3_bucket_exists(self.s3, bucket)\n        if bucket_exists:\n            raise Exception('Bucket {bucket} already exists.'.format(\n                bucket=bucket))\n\n        # If we are not using the us-east-1 region, then we must set\n        # a location constraint on the new bucket.\n        params = {'Bucket': bucket}\n        if self.region_name != 'us-east-1':\n            bucket_config = {'LocationConstraint': self.region_name}\n            params['CreateBucketConfiguration'] = bucket_config\n\n        data = self.s3.create_bucket(**params)\n\n        try:\n            self.s3.put_bucket_policy(Bucket=bucket, Policy=policy)\n        except ClientError:\n            # Roll back bucket creation.\n            self.s3.delete_bucket(Bucket=bucket)\n            raise\n\n        return data\n\n    def setup_new_topic(self, topic, custom_policy=None):\n        \"\"\"\n        Creates a new SNS topic with an appropriate policy to let CloudTrail\n        post messages to the topic.\n        \"\"\"\n        sys.stdout.write(\n            'Setting up new SNS topic {topic}...\\n'.format(topic=topic))\n\n        account_id = get_account_id(self.sts)\n\n        # Make sure topic doesn't already exist\n        # Warn but do not fail if ListTopics permissions\n        # are missing from the IAM role?\n        try:\n            topics = self.sns.list_topics()['Topics']\n        except Exception:\n            topics = []\n            LOG.warn('Unable to list topics, continuing...')\n\n        if [t for t in topics if t['TopicArn'].split(':')[-1] == topic]:\n            raise Exception('Topic {topic} already exists.'.format(\n                topic=topic))\n\n        region = self.sns.meta.region_name\n\n        # Get the SNS topic policy information to allow CloudTrail\n        # write-access.\n        if custom_policy is not None:\n            policy = custom_policy\n        else:\n            policy = self._get_policy(SNS_POLICY_TEMPLATE)\n\n        policy = policy.replace('<Region>', region)\\\n                       .replace('<SNSTopicOwnerAccountId>', account_id)\\\n                       .replace('<SNSTopicName>', topic)\n\n        topic_result = self.sns.create_topic(Name=topic)\n\n        try:\n            # Merge any existing topic policy with our new policy statements\n            topic_attr = self.sns.get_topic_attributes(\n                TopicArn=topic_result['TopicArn'])\n\n            policy = self.merge_sns_policy(topic_attr['Attributes']['Policy'],\n                                           policy)\n\n            LOG.debug('Topic policy:\\n{0}'.format(policy))\n\n            # Set the topic policy\n            self.sns.set_topic_attributes(TopicArn=topic_result['TopicArn'],\n                                          AttributeName='Policy',\n                                          AttributeValue=policy)\n        except Exception:\n            # Roll back topic creation\n            self.sns.delete_topic(TopicArn=topic_result['TopicArn'])\n            raise\n\n        return topic_result\n\n    def merge_sns_policy(self, left, right):\n        \"\"\"\n        Merge two SNS topic policy documents. The id information from\n        ``left`` is used in the final document, and the statements\n        from ``right`` are merged into ``left``.\n\n        http://docs.aws.amazon.com/sns/latest/dg/BasicStructure.html\n\n        :type left: string\n        :param left: First policy JSON document\n        :type right: string\n        :param right: Second policy JSON document\n        :rtype: string\n        :return: Merged policy JSON\n        \"\"\"\n        left_parsed = json.loads(left)\n        right_parsed = json.loads(right)\n        left_parsed['Statement'] += right_parsed['Statement']\n        return json.dumps(left_parsed)\n\n    def upsert_cloudtrail_config(self, name, bucket, prefix, topic, gse):\n        \"\"\"\n        Either create or update the CloudTrail configuration depending on\n        whether this command is a create or update command.\n        \"\"\"\n        sys.stdout.write('Creating/updating CloudTrail configuration...\\n')\n        config = {\n            'Name': name\n        }\n        if bucket is not None:\n            config['S3BucketName'] = bucket\n        if prefix is not None:\n            config['S3KeyPrefix'] = prefix\n        if topic is not None:\n            config['SnsTopicName'] = topic\n        if gse is not None:\n            config['IncludeGlobalServiceEvents'] = gse\n        if not self.UPDATE:\n            self.cloudtrail.create_trail(**config)\n        else:\n            self.cloudtrail.update_trail(**config)\n        return self.cloudtrail.describe_trails()\n\n    def start_cloudtrail(self, name):\n        \"\"\"\n        Start the CloudTrail service, which begins logging.\n        \"\"\"\n        sys.stdout.write('Starting CloudTrail service...\\n')\n        return self.cloudtrail.start_logging(Name=name)\n\n\nclass CloudTrailUpdate(CloudTrailSubscribe):\n    \"\"\"\n    Like subscribe above, but the update version of the command.\n    \"\"\"\n    NAME = 'update-subscription'\n    UPDATE = True\n\n    DESCRIPTION = ('Updates any of the trail configuration settings, and'\n                   ' creates and configures any new AWS resources specified.')\n\n    SYNOPSIS = ('aws cloudtrail update-subscription'\n                ' [(--s3-use-bucket|--s3-new-bucket) bucket-name]'\n                ' [--sns-new-topic topic-name]\\n')\n", "awscli/customizations/cloudtrail/utils.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\ndef get_account_id_from_arn(trail_arn):\n    \"\"\"Gets the account ID portion of an ARN\"\"\"\n    return trail_arn.split(':')[4]\n\n\ndef get_account_id(sts_client):\n    \"\"\"Retrieve the AWS account ID for the authenticated user or role\"\"\"\n    response = sts_client.get_caller_identity()\n    return response['Account']\n\n\ndef get_trail_by_arn(cloudtrail_client, trail_arn):\n    \"\"\"Gets trail information based on the trail's ARN\"\"\"\n    trails = cloudtrail_client.describe_trails()['trailList']\n    for trail in trails:\n        if trail.get('TrailARN', None) == trail_arn:\n            return trail\n    raise ValueError('A trail could not be found for %s' % trail_arn)\n", "awscli/customizations/cloudtrail/__init__.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom .subscribe import CloudTrailSubscribe, CloudTrailUpdate\nfrom .validation import CloudTrailValidateLogs\n\n\ndef initialize(cli):\n    \"\"\"\n    The entry point for CloudTrail high level commands.\n    \"\"\"\n    cli.register('building-command-table.cloudtrail', inject_commands)\n\n\ndef inject_commands(command_table, session, **kwargs):\n    \"\"\"\n    Called when the CloudTrail command table is being built. Used to inject new\n    high level commands into the command list. These high level commands\n    must not collide with existing low-level API call names.\n    \"\"\"\n    command_table['create-subscription'] = CloudTrailSubscribe(session)\n    command_table['update-subscription'] = CloudTrailUpdate(session)\n    command_table['validate-logs'] = CloudTrailValidateLogs(session)\n", "awscli/customizations/cloudtrail/validation.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport binascii\nimport json\nimport hashlib\nimport logging\nimport re\nimport sys\nimport zlib\nfrom zlib import error as ZLibError\nfrom datetime import datetime, timedelta\nfrom dateutil import tz, parser\n\nfrom pyasn1.error import PyAsn1Error\nimport rsa\n\nfrom awscli.customizations.cloudtrail.utils import get_trail_by_arn, \\\n    get_account_id_from_arn\nfrom awscli.customizations.commands import BasicCommand\nfrom botocore.exceptions import ClientError\nfrom awscli.schema import ParameterRequiredError\n\n\nLOG = logging.getLogger(__name__)\nDATE_FORMAT = '%Y%m%dT%H%M%SZ'\nDISPLAY_DATE_FORMAT = '%Y-%m-%dT%H:%M:%SZ'\n\n\ndef format_date(date):\n    \"\"\"Returns a formatted date string in a CloudTrail date format\"\"\"\n    return date.strftime(DATE_FORMAT)\n\n\ndef format_display_date(date):\n    \"\"\"Returns a formatted date string meant for CLI output\"\"\"\n    return date.strftime(DISPLAY_DATE_FORMAT)\n\n\ndef normalize_date(date):\n    \"\"\"Returns a normalized date using a UTC timezone\"\"\"\n    return date.replace(tzinfo=tz.tzutc())\n\n\ndef extract_digest_key_date(digest_s3_key):\n    \"\"\"Extract the timestamp portion of a manifest file.\n\n    Manifest file names take the following form:\n    AWSLogs/{account}/CloudTrail-Digest/{region}/{ymd}/{account}_CloudTrail \\\n    -Digest_{region}_{name}_region_{date}.json.gz\n    \"\"\"\n    return digest_s3_key[-24:-8]\n\n\ndef parse_date(date_string):\n    try:\n        return parser.parse(date_string)\n    except ValueError:\n        raise ValueError('Unable to parse date value: %s' % date_string)\n\n\ndef assert_cloudtrail_arn_is_valid(trail_arn):\n    \"\"\"Ensures that the arn looks correct.\n\n    ARNs look like: arn:aws:cloudtrail:us-east-1:123456789012:trail/foo\"\"\"\n    pattern = re.compile('arn:.+:cloudtrail:.+:\\d{12}:trail/.+')\n    if not pattern.match(trail_arn):\n        raise ValueError('Invalid trail ARN provided: %s' % trail_arn)\n\n\ndef create_digest_traverser(cloudtrail_client, organization_client,\n                            s3_client_provider, trail_arn,\n                            trail_source_region=None, on_invalid=None,\n                            on_gap=None, on_missing=None, bucket=None,\n                            prefix=None, account_id=None):\n    \"\"\"Creates a CloudTrail DigestTraverser and its object graph.\n\n    :type cloudtrail_client: botocore.client.CloudTrail\n    :param cloudtrail_client: Client used to connect to CloudTrail\n    :type organization_client: botocore.client.organizations\n    :param organization_client: Client used to connect to Organizations\n    :type s3_client_provider: S3ClientProvider\n    :param s3_client_provider: Used to create Amazon S3 client per/region.\n    :param trail_arn: CloudTrail trail ARN\n    :param trail_source_region: The scanned region of a trail.\n    :param on_invalid: Callback that is invoked when validating a digest fails.\n    :param on_gap: Callback that is invoked when a digest has no link to the\n        previous digest, but there are more digests to validate. This can\n        happen when a trail is disabled for a period of time.\n    :param on_missing: Callback that is invoked when a digest file has been\n        deleted from Amazon S3 but is supposed to be present.\n    :param bucket: Amazon S3 bucket of the trail if it is different than the\n        bucket that is currently associated with the trail.\n    :param prefix: bucket: Key prefix prepended to each digest and log placed\n        in the Amazon S3 bucket if it is different than the prefix that is\n        currently associated with the trail.\n    :param account_id: The account id for which the digest files are\n        validated. For normal trails this is the caller account, for\n        organization trails it is the member accout.\n\n    ``on_gap``, ``on_invalid``, and ``on_missing`` callbacks are invoked with\n    the following named arguments:\n\n    - ``bucket`: The next S3 bucket.\n    - ``next_key``: (optional) Next digest key that was found in the bucket.\n    - ``next_end_date``: (optional) End date of the next found digest.\n    - ``last_key``: The last digest key that was found.\n    - ``last_start_date``: (optional) Start date of last found digest.\n    - ``message``: (optional) Message string about the notification.\n    \"\"\"\n    assert_cloudtrail_arn_is_valid(trail_arn)\n    organization_id = None\n    if bucket is None:\n        # Determine the bucket and prefix based on the trail arn.\n        trail_info = get_trail_by_arn(cloudtrail_client, trail_arn)\n        LOG.debug('Loaded trail info: %s', trail_info)\n        bucket = trail_info['S3BucketName']\n        prefix = trail_info.get('S3KeyPrefix', None)\n        is_org_trail = trail_info.get('IsOrganizationTrail')\n        if is_org_trail:\n            if not account_id:\n                raise ParameterRequiredError(\n                    \"Missing required parameter for organization \"\n                    \"trail: '--account-id'\")\n            organization_id = organization_client.describe_organization()[\n                'Organization']['Id']\n\n    # Determine the region from the ARN (e.g., arn:aws:cloudtrail:REGION:...)\n    trail_region = trail_arn.split(':')[3]\n    # Determine the name from the ARN (the last part after \"/\")\n    trail_name = trail_arn.split('/')[-1]\n    # If account id is not specified parse it from trail ARN\n    if not account_id:\n        account_id = get_account_id_from_arn(trail_arn)\n\n    digest_provider = DigestProvider(\n        account_id=account_id, trail_name=trail_name,\n        s3_client_provider=s3_client_provider,\n        trail_source_region=trail_source_region,\n        trail_home_region=trail_region,\n        organization_id=organization_id)\n    return DigestTraverser(\n        digest_provider=digest_provider, starting_bucket=bucket,\n        starting_prefix=prefix, on_invalid=on_invalid, on_gap=on_gap,\n        on_missing=on_missing,\n        public_key_provider=PublicKeyProvider(cloudtrail_client))\n\n\nclass S3ClientProvider(object):\n    \"\"\"Creates Amazon S3 clients and determines the region name of a client.\n\n    This class will cache the location constraints of previously requested\n    buckets and cache previously created clients for the same region.\n    \"\"\"\n    def __init__(self, session, get_bucket_location_region='us-east-1'):\n        self._session = session\n        self._get_bucket_location_region = get_bucket_location_region\n        self._client_cache = {}\n        self._region_cache = {}\n\n    def get_client(self, bucket_name):\n        \"\"\"Creates an S3 client that can work with the given bucket name\"\"\"\n        region_name = self._get_bucket_region(bucket_name)\n        return self._create_client(region_name)\n\n    def _get_bucket_region(self, bucket_name):\n        \"\"\"Returns the region of a bucket\"\"\"\n        if bucket_name not in self._region_cache:\n            client = self._create_client(self._get_bucket_location_region)\n            result = client.get_bucket_location(Bucket=bucket_name)\n            region = result['LocationConstraint'] or 'us-east-1'\n            self._region_cache[bucket_name] = region\n        return self._region_cache[bucket_name]\n\n    def _create_client(self, region_name):\n        \"\"\"Creates an Amazon S3 client for the given region name\"\"\"\n        if region_name not in self._client_cache:\n            client = self._session.create_client('s3', region_name)\n            # Remove the CLI error event that prevents exceptions.\n            self._client_cache[region_name] = client\n        return self._client_cache[region_name]\n\n\nclass DigestError(ValueError):\n    \"\"\"Exception raised when a digest fails to validate\"\"\"\n    pass\n\n\nclass DigestSignatureError(DigestError):\n    \"\"\"Exception raised when a digest signature is invalid\"\"\"\n    def __init__(self, bucket, key):\n        message = ('Digest file\\ts3://%s/%s\\tINVALID: signature verification '\n                   'failed') % (bucket, key)\n        super(DigestSignatureError, self).__init__(message)\n\n\nclass InvalidDigestFormat(DigestError):\n    \"\"\"Exception raised when a digest has an invalid format\"\"\"\n    def __init__(self, bucket, key):\n        message = 'Digest file\\ts3://%s/%s\\tINVALID: invalid format' % (bucket,\n                                                                        key)\n        super(InvalidDigestFormat, self).__init__(message)\n\n\nclass PublicKeyProvider(object):\n    \"\"\"Retrieves public keys from CloudTrail within a date range.\"\"\"\n    def __init__(self, cloudtrail_client):\n        self._cloudtrail_client = cloudtrail_client\n\n    def get_public_keys(self, start_date, end_date):\n        \"\"\"Loads public keys in a date range into a returned dict.\n\n        :type start_date: datetime\n        :param start_date: Start date of a date range.\n        :type end_date: datetime\n        :param end_date: End date of a date range.\n        :rtype: dict\n        :return: Returns a dict where each key is the fingerprint of the\n            public key, and each value is a dict of public key data.\n        \"\"\"\n        public_keys = self._cloudtrail_client.list_public_keys(\n            StartTime=start_date, EndTime=end_date)\n        public_keys_in_range = public_keys['PublicKeyList']\n        LOG.debug('Loaded public keys in range: %s', public_keys_in_range)\n        return dict((key['Fingerprint'], key) for key in public_keys_in_range)\n\n\nclass DigestProvider(object):\n    \"\"\"\n    Retrieves digest keys and digests from Amazon S3.\n\n    This class is responsible for determining the full list of digest files\n    in a bucket and loading digests from the bucket into a JSON decoded\n    dict. This class is not responsible for validation or iterating from\n    one digest to the next.\n    \"\"\"\n    def __init__(self, s3_client_provider, account_id, trail_name,\n                 trail_home_region, trail_source_region=None,\n                 organization_id=None):\n        self._client_provider = s3_client_provider\n        self.trail_name = trail_name\n        self.account_id = account_id\n        self.trail_home_region = trail_home_region\n        self.trail_source_region = trail_source_region or trail_home_region\n        self.organization_id = organization_id\n\n    def load_digest_keys_in_range(self, bucket, prefix, start_date, end_date):\n        \"\"\"Returns a list of digest keys in the date range.\n\n        This method uses a list_objects API call and provides a Marker\n        parameter that is calculated based on the start_date provided.\n        Amazon S3 then returns all keys in the bucket that start after\n        the given key (non-inclusive). We then iterate over the keys\n        until the date extracted from the yielded keys is greater than\n        the given end_date.\n        \"\"\"\n        digests = []\n        marker = self._create_digest_key(start_date, prefix)\n        client = self._client_provider.get_client(bucket)\n        paginator = client.get_paginator('list_objects')\n        page_iterator = paginator.paginate(Bucket=bucket, Marker=marker)\n        key_filter = page_iterator.search('Contents[*].Key')\n        # Create a target start end end date\n        target_start_date = format_date(normalize_date(start_date))\n        # Add one hour to the end_date to get logs that spilled over to next.\n        target_end_date = format_date(\n            normalize_date(end_date + timedelta(hours=1)))\n        # Ensure digests are from the same trail.\n        digest_key_regex = re.compile(self._create_digest_key_regex(prefix))\n        for key in key_filter:\n            if digest_key_regex.match(key):\n                # Use a lexicographic comparison to know when to stop.\n                extracted_date = extract_digest_key_date(key)\n                if extracted_date > target_end_date:\n                    break\n                # Only append digests after the start date.\n                if extracted_date >= target_start_date:\n                    digests.append(key)\n        return digests\n\n    def fetch_digest(self, bucket, key):\n        \"\"\"Loads a digest by key from S3.\n\n        Returns the JSON decode data and GZIP inflated raw content.\n        \"\"\"\n        client = self._client_provider.get_client(bucket)\n        result = client.get_object(Bucket=bucket, Key=key)\n        try:\n            digest = zlib.decompress(result['Body'].read(),\n                                     zlib.MAX_WBITS | 16)\n            digest_data = json.loads(digest.decode())\n        except (ValueError, ZLibError):\n            # Cannot gzip decode or JSON parse.\n            raise InvalidDigestFormat(bucket, key)\n        # Add the expected digest signature and algorithm to the dict.\n        if 'signature' not in result['Metadata'] \\\n                or 'signature-algorithm' not in result['Metadata']:\n            raise DigestSignatureError(bucket, key)\n        digest_data['_signature'] = result['Metadata']['signature']\n        digest_data['_signature_algorithm'] = \\\n            result['Metadata']['signature-algorithm']\n        return digest_data, digest\n\n    def _create_digest_key(self, start_date, key_prefix):\n        \"\"\"Computes an Amazon S3 key based on the provided data.\n\n        The computed is what would have been placed in the S3 bucket if\n        a log digest were created at a specific time. This computed key\n        does not have to actually exist as it will only be used to as\n        a Marker parameter in a list_objects call.\n\n        :return: Returns a computed key as a string.\n        \"\"\"\n        # Subtract one minute to ensure the dates are inclusive.\n        date = start_date - timedelta(minutes=1)\n        template = 'AWSLogs/'\n        template_params = {\n            'account_id': self.account_id,\n            'date': format_date(date),\n            'ymd': date.strftime('%Y/%m/%d'),\n            'source_region': self.trail_source_region,\n            'home_region': self.trail_home_region,\n            'name': self.trail_name\n        }\n        if self.organization_id:\n            template += '{organization_id}/'\n            template_params['organization_id'] = self.organization_id\n        template += (\n            '{account_id}/CloudTrail-Digest/{source_region}/'\n            '{ymd}/{account_id}_CloudTrail-Digest_{source_region}_{name}_'\n            '{home_region}_{date}.json.gz'\n        )\n        key = template.format(**template_params)\n        if key_prefix:\n            key = key_prefix + '/' + key\n        return key\n\n    def _create_digest_key_regex(self, key_prefix):\n        \"\"\"Creates a regular expression used to match against S3 keys\"\"\"\n        template = 'AWSLogs/'\n        template_params = {\n            'account_id': re.escape(self.account_id),\n            'source_region': re.escape(self.trail_source_region),\n            'home_region': re.escape(self.trail_home_region),\n            'name': re.escape(self.trail_name)\n        }\n        if self.organization_id:\n            template += '{organization_id}/'\n            template_params['organization_id'] = self.organization_id\n        template += (\n            '{account_id}/CloudTrail\\\\-Digest/{source_region}/'\n            '\\\\d+/\\\\d+/\\\\d+/{account_id}_CloudTrail\\\\-Digest_'\n            '{source_region}_{name}_{home_region}_.+\\\\.json\\\\.gz'\n        )\n        key = template.format(**template_params)\n        if key_prefix:\n            key = re.escape(key_prefix) + '/' + key\n        return '^' + key + '$'\n\n\nclass DigestTraverser(object):\n    \"\"\"Retrieves and validates digests within a date range.\"\"\"\n    # These keys are required to be present before validating the contents\n    # of a digest.\n    required_digest_keys = ['digestPublicKeyFingerprint', 'digestS3Bucket',\n                            'digestS3Object', 'previousDigestSignature',\n                            'digestEndTime', 'digestStartTime']\n\n    def __init__(self, digest_provider, starting_bucket, starting_prefix,\n                 public_key_provider, digest_validator=None,\n                 on_invalid=None, on_gap=None, on_missing=None):\n        \"\"\"\n        :type digest_provider: DigestProvider\n        :param digest_provider: DigestProvider object\n        :param starting_bucket: S3 bucket where the digests are stored.\n        :param starting_prefix: An optional prefix applied to each S3 key.\n        :param public_key_provider: Provides public keys for a range.\n        :param digest_validator: Validates digest using a validate method.\n        :param on_invalid: Callback invoked when a digest is invalid.\n        :param on_gap: Callback invoked when a digest has no parent, but\n            there are still more digests to validate.\n        :param on_missing: Callback invoked when a digest file is missing.\n        \"\"\"\n        self.starting_bucket = starting_bucket\n        self.starting_prefix = starting_prefix\n        self.digest_provider = digest_provider\n        self._public_key_provider = public_key_provider\n        self._on_gap = on_gap\n        self._on_invalid = on_invalid\n        self._on_missing = on_missing\n        if digest_validator is None:\n            digest_validator = Sha256RSADigestValidator()\n        self._digest_validator = digest_validator\n\n    def traverse(self, start_date, end_date=None):\n        \"\"\"Creates and returns a generator that yields validated digest data.\n\n        Each yielded digest dictionary contains information about the digest\n        and the log file associated with the digest. Digest files are validated\n        before they are yielded. Whether or not the digest is successfully\n        validated is stated in the \"isValid\" key value pair of the yielded\n        dictionary.\n\n        :type start_date: datetime\n        :param start_date: Date to start validating from (inclusive).\n        :type start_date: datetime\n        :param end_date: Date to stop validating at (inclusive).\n        \"\"\"\n        if end_date is None:\n            end_date = datetime.utcnow()\n        end_date = normalize_date(end_date)\n        start_date = normalize_date(start_date)\n        bucket = self.starting_bucket\n        prefix = self.starting_prefix\n        digests = self._load_digests(bucket, prefix, start_date, end_date)\n        public_keys = self._load_public_keys(start_date, end_date)\n        key, end_date = self._get_last_digest(digests)\n        last_start_date = end_date\n        while key and start_date <= last_start_date:\n            try:\n                digest, end_date = self._load_and_validate_digest(\n                    public_keys, bucket, key)\n                last_start_date = normalize_date(\n                    parse_date(digest['digestStartTime']))\n                previous_bucket = digest.get('previousDigestS3Bucket', None)\n                yield digest\n                if previous_bucket is None:\n                    # The chain is broken, so find next in digest store.\n                    key, end_date = self._find_next_digest(\n                        digests=digests, bucket=bucket, last_key=key,\n                        last_start_date=last_start_date, cb=self._on_gap,\n                        is_cb_conditional=True)\n                else:\n                    key = digest['previousDigestS3Object']\n                    if previous_bucket != bucket:\n                        bucket = previous_bucket\n                        # The bucket changed so reload the digest list.\n                        digests = self._load_digests(\n                            bucket, prefix, start_date, end_date)\n            except ClientError as e:\n                if e.response['Error']['Code'] != 'NoSuchKey':\n                    raise e\n                key, end_date = self._find_next_digest(\n                    digests=digests, bucket=bucket, last_key=key,\n                    last_start_date=last_start_date, cb=self._on_missing,\n                    message=str(e))\n            except DigestError as e:\n                key, end_date = self._find_next_digest(\n                    digests=digests, bucket=bucket, last_key=key,\n                    last_start_date=last_start_date, cb=self._on_invalid,\n                    message=str(e))\n            except Exception as e:\n                # Any other unexpected errors.\n                key, end_date = self._find_next_digest(\n                    digests=digests, bucket=bucket, last_key=key,\n                    last_start_date=last_start_date, cb=self._on_invalid,\n                    message='Digest file\\ts3://%s/%s\\tINVALID: %s'\n                            % (bucket, key, str(e)))\n\n    def _load_digests(self, bucket, prefix, start_date, end_date):\n        return self.digest_provider.load_digest_keys_in_range(\n            bucket=bucket, prefix=prefix,\n            start_date=start_date, end_date=end_date)\n\n    def _find_next_digest(self, digests, bucket, last_key, last_start_date,\n                          cb=None, is_cb_conditional=False, message=None):\n        \"\"\"Finds the next digest in the bucket and invokes any callback.\"\"\"\n        next_key, next_end_date = self._get_last_digest(digests, last_key)\n        if cb and (not is_cb_conditional or next_key):\n            cb(bucket=bucket, next_key=next_key, last_key=last_key,\n               next_end_date=next_end_date, last_start_date=last_start_date,\n               message=message)\n        return next_key, next_end_date\n\n    def _get_last_digest(self, digests, before_key=None):\n        \"\"\"Finds the previous digest key (either the last or before before_key)\n\n        If no key is provided, the last digest is used. If a digest is found,\n        the end date of the provider is adjusted to match the found key's end\n        date.\n        \"\"\"\n        if not digests:\n            return None, None\n        elif before_key is None:\n            next_key = digests.pop()\n            next_key_date = normalize_date(\n                parse_date(extract_digest_key_date(next_key)))\n            return next_key, next_key_date\n        # find a key before the given key.\n        before_key_date = parse_date(extract_digest_key_date(before_key))\n        while digests:\n            next_key = digests.pop()\n            next_key_date = normalize_date(\n                parse_date(extract_digest_key_date(next_key)))\n            if next_key_date < before_key_date:\n                LOG.debug(\"Next found key: %s\", next_key)\n                return next_key, next_key_date\n        return None, None\n\n    def _load_and_validate_digest(self, public_keys, bucket, key):\n        \"\"\"Loads and validates a digest from S3.\n\n        :param public_keys: Public key dictionary of fingerprint to dict.\n        :return: Returns a tuple of the digest data as a dict and end_date\n        :rtype: tuple\n        \"\"\"\n        digest_data, digest = self.digest_provider.fetch_digest(bucket, key)\n        for required_key in self.required_digest_keys:\n            if required_key not in digest_data:\n                raise InvalidDigestFormat(bucket, key)\n        # Ensure the bucket and key are the same as what's expected.\n        if digest_data['digestS3Bucket'] != bucket \\\n                or digest_data['digestS3Object'] != key:\n            raise DigestError(\n                ('Digest file\\ts3://%s/%s\\tINVALID: has been moved from its '\n                 'original location') % (bucket, key))\n        # Get the public keys in the given time range.\n        fingerprint = digest_data['digestPublicKeyFingerprint']\n        if fingerprint not in public_keys:\n            raise DigestError(\n                ('Digest file\\ts3://%s/%s\\tINVALID: public key not found in '\n                 'region %s for fingerprint %s') %\n                (bucket, key, self.digest_provider.trail_home_region,\n                 fingerprint))\n        public_key_hex = public_keys[fingerprint]['Value']\n        self._digest_validator.validate(\n            bucket, key, public_key_hex, digest_data, digest)\n        end_date = normalize_date(parse_date(digest_data['digestEndTime']))\n        return digest_data, end_date\n\n    def _load_public_keys(self, start_date, end_date):\n        public_keys = self._public_key_provider.get_public_keys(\n            start_date, end_date)\n        if not public_keys:\n            raise RuntimeError(\n                'No public keys found between %s and %s' %\n                (format_display_date(start_date),\n                 format_display_date(end_date)))\n        return public_keys\n\n\nclass Sha256RSADigestValidator(object):\n    \"\"\"\n    Validates SHA256withRSA signed digests.\n\n    The result of validating the digest is inserted into the digest_data\n    dictionary using the isValid key value pair.\n    \"\"\"\n\n    def validate(self, bucket, key, public_key, digest_data, inflated_digest):\n        \"\"\"Validates a digest file.\n\n        Throws a DigestError when the digest is invalid.\n\n        :param bucket: Bucket of the digest file\n        :param key: Key of the digest file\n        :param public_key: Public key bytes.\n        :param digest_data: Dict of digest data returned when JSON\n            decoding a manifest.\n        :param inflated_digest: Inflated digest file contents as bytes.\n        \"\"\"\n        try:\n            decoded_key = base64.b64decode(public_key)\n            public_key = rsa.PublicKey.load_pkcs1(decoded_key, format='DER')\n            to_sign = self._create_string_to_sign(digest_data, inflated_digest)\n            signature_bytes = binascii.unhexlify(digest_data['_signature'])\n            rsa.verify(to_sign, signature_bytes, public_key)\n        except PyAsn1Error:\n            raise DigestError(\n                ('Digest file\\ts3://%s/%s\\tINVALID: Unable to load PKCS #1 key'\n                 ' with fingerprint %s')\n                % (bucket, key, digest_data['digestPublicKeyFingerprint']))\n        except rsa.pkcs1.VerificationError:\n            # Note from the Python-RSA docs: Never display the stack trace of\n            # a rsa.pkcs1.VerificationError exception. It shows where in the\n            # code the exception occurred, and thus leaks information about\n            # the key.\n            raise DigestSignatureError(bucket, key)\n\n    def _create_string_to_sign(self, digest_data, inflated_digest):\n        previous_signature = digest_data['previousDigestSignature']\n        if previous_signature is None:\n            # The value must be 'null' to match the Java implementation.\n            previous_signature = 'null'\n        string_to_sign = \"%s\\n%s/%s\\n%s\\n%s\" % (\n            digest_data['digestEndTime'],\n            digest_data['digestS3Bucket'],\n            digest_data['digestS3Object'],\n            hashlib.sha256(inflated_digest).hexdigest(),\n            previous_signature)\n        LOG.debug('Digest string to sign: %s', string_to_sign)\n        return string_to_sign.encode()\n\n\nclass CloudTrailValidateLogs(BasicCommand):\n    \"\"\"\n    Validates log digests and log files, optionally saving them to disk.\n    \"\"\"\n    NAME = 'validate-logs'\n    DESCRIPTION = \"\"\"\n    Validates CloudTrail logs for a given period of time.\n\n    This command uses the digest files delivered to your S3 bucket to perform\n    the validation.\n\n    The AWS CLI allows you to detect the following types of changes:\n\n    - Modification or deletion of CloudTrail log files.\n    - Modification or deletion of CloudTrail digest files.\n\n    To validate log files with the AWS CLI, the following preconditions must\n    be met:\n\n    - You must have online connectivity to AWS.\n    - You must have read access to the S3 bucket that contains the digest and\n      log files.\n    - The digest and log files must not have been moved from the original S3\n      location where CloudTrail delivered them.\n    - For organization trails you must have access to describe-organization to\n      validate digest files\n\n    When you disable Log File Validation, the chain of digest files is broken\n    after one hour. CloudTrail will not digest log files that were delivered\n    during a period in which the Log File Validation feature was disabled.\n    For example, if you enable Log File Validation on January 1, disable it\n    on January 2, and re-enable it on January 10, digest files will not be\n    created for the log files delivered from January 3 to January 9. The same\n    applies whenever you stop CloudTrail logging or delete a trail.\n\n    .. note::\n\n        Log files that have been downloaded to local disk cannot be validated\n        with the AWS CLI. The CLI will download all log files each time this\n        command is executed.\n\n    .. note::\n\n        This command requires that the role executing the command has\n        permission to call ListObjects, GetObject, and GetBucketLocation for\n        each bucket referenced by the trail.\n\n    \"\"\"\n\n    ARG_TABLE = [\n        {'name': 'trail-arn', 'required': True, 'cli_type_name': 'string',\n         'help_text': 'Specifies the ARN of the trail to be validated'},\n        {'name': 'start-time', 'required': True, 'cli_type_name': 'string',\n         'help_text': ('Specifies that log files delivered on or after the '\n                       'specified UTC timestamp value will be validated. '\n                       'Example: \"2015-01-08T05:21:42Z\".')},\n        {'name': 'end-time', 'cli_type_name': 'string',\n         'help_text': ('Optionally specifies that log files delivered on or '\n                       'before the specified UTC timestamp value will be '\n                       'validated. The default value is the current time. '\n                       'Example: \"2015-01-08T12:31:41Z\".')},\n        {'name': 's3-bucket', 'cli_type_name': 'string',\n         'help_text': ('Optionally specifies the S3 bucket where the digest '\n                       'files are stored. If a bucket name is not specified, '\n                       'the CLI will retrieve it by calling describe_trails')},\n        {'name': 's3-prefix', 'cli_type_name': 'string',\n         'help_text': ('Optionally specifies the optional S3 prefix where the '\n                       'digest files are stored. If not specified, the CLI '\n                       'will determine the prefix automatically by calling '\n                       'describe_trails.')},\n        {'name': 'account-id', 'cli_type_name': 'string',\n         'help_text': ('Optionally specifies the account for validating logs. '\n                       'This parameter is needed for organization trails '\n                       'for validating logs for specific account inside an '\n                       'organization')},\n        {'name': 'verbose', 'cli_type_name': 'boolean',\n         'action': 'store_true',\n         'help_text': 'Display verbose log validation information'}\n    ]\n\n    def __init__(self, session):\n        super(CloudTrailValidateLogs, self).__init__(session)\n        self.trail_arn = None\n        self.is_verbose = False\n        self.start_time = None\n        self.end_time = None\n        self.s3_bucket = None\n        self.s3_prefix = None\n        self.s3_client_provider = None\n        self.cloudtrail_client = None\n        self.account_id = None\n        self._source_region = None\n        self._valid_digests = 0\n        self._invalid_digests = 0\n        self._valid_logs = 0\n        self._invalid_logs = 0\n        self._is_last_status_double_space = True\n        self._found_start_time = None\n        self._found_end_time = None\n\n    def _run_main(self, args, parsed_globals):\n        self.handle_args(args)\n        self.setup_services(parsed_globals)\n        self._call()\n        if self._invalid_digests > 0 or self._invalid_logs > 0:\n            return 1\n        return 0\n\n    def handle_args(self, args):\n        self.trail_arn = args.trail_arn\n        self.is_verbose = args.verbose\n        self.s3_bucket = args.s3_bucket\n        self.s3_prefix = args.s3_prefix\n        self.account_id = args.account_id\n        self.start_time = normalize_date(parse_date(args.start_time))\n        if args.end_time:\n            self.end_time = normalize_date(parse_date(args.end_time))\n        else:\n            self.end_time = normalize_date(datetime.utcnow())\n        if self.start_time > self.end_time:\n            raise ValueError(('Invalid time range specified: start-time must '\n                              'occur before end-time'))\n        # Found start time always defaults to the given start time. This value\n        # may change if the earliest found digest is after the given start\n        # time. Note that the summary output report of what date ranges were\n        # actually found is only shown if a valid digest is encountered,\n        # thereby setting self._found_end_time to a value.\n        self._found_start_time = self.start_time\n\n    def setup_services(self, parsed_globals):\n        self._source_region = parsed_globals.region\n        # Use the the same region as the region of the CLI to get locations.\n        self.s3_client_provider = S3ClientProvider(\n            self._session, self._source_region)\n        client_args = {'region_name': parsed_globals.region,\n                       'verify': parsed_globals.verify_ssl}\n        self.organization_client = self._session.create_client(\n            'organizations', **client_args)\n\n        if parsed_globals.endpoint_url is not None:\n            client_args['endpoint_url'] = parsed_globals.endpoint_url\n        self.cloudtrail_client = self._session.create_client(\n            'cloudtrail', **client_args)\n\n    def _call(self):\n        traverser = create_digest_traverser(\n            trail_arn=self.trail_arn, cloudtrail_client=self.cloudtrail_client,\n            organization_client=self.organization_client,\n            trail_source_region=self._source_region,\n            s3_client_provider=self.s3_client_provider, bucket=self.s3_bucket,\n            prefix=self.s3_prefix, on_missing=self._on_missing_digest,\n            on_invalid=self._on_invalid_digest, on_gap=self._on_digest_gap,\n            account_id=self.account_id)\n        self._write_startup_text()\n        digests = traverser.traverse(self.start_time, self.end_time)\n        for digest in digests:\n            # Only valid digests are yielded and only valid digests can adjust\n            # the found times that are reported in the CLI output summary.\n            self._track_found_times(digest)\n            self._valid_digests += 1\n            self._write_status(\n                'Digest file\\ts3://%s/%s\\tvalid'\n                % (digest['digestS3Bucket'], digest['digestS3Object']))\n            if not digest['logFiles']:\n                continue\n            for log in digest['logFiles']:\n                self._download_log(log)\n        self._write_summary_text()\n\n    def _track_found_times(self, digest):\n        # Track the earliest found start time, but do not use a date before\n        # the user supplied start date.\n        digest_start_time = parse_date(digest['digestStartTime'])\n        if digest_start_time > self.start_time:\n            self._found_start_time = digest_start_time\n        # Only use the last found end time if it is less than the\n        # user supplied end time (or the current date).\n        if not self._found_end_time:\n            digest_end_time = parse_date(digest['digestEndTime'])\n            self._found_end_time = min(digest_end_time, self.end_time)\n\n    def _download_log(self, log):\n        \"\"\" Download a log, decompress, and compare SHA256 checksums\"\"\"\n        try:\n            # Create a client that can work with this bucket.\n            client = self.s3_client_provider.get_client(log['s3Bucket'])\n            response = client.get_object(\n                Bucket=log['s3Bucket'], Key=log['s3Object'])\n            gzip_inflater = zlib.decompressobj(zlib.MAX_WBITS | 16)\n            rolling_hash = hashlib.sha256()\n            for chunk in iter(lambda: response['Body'].read(2048), b\"\"):\n                data = gzip_inflater.decompress(chunk)\n                rolling_hash.update(data)\n            remaining_data = gzip_inflater.flush()\n            if remaining_data:\n                rolling_hash.update(remaining_data)\n            computed_hash = rolling_hash.hexdigest()\n            if computed_hash != log['hashValue']:\n                self._on_log_invalid(log)\n            else:\n                self._valid_logs += 1\n                self._write_status(('Log file\\ts3://%s/%s\\tvalid'\n                                    % (log['s3Bucket'], log['s3Object'])))\n        except ClientError as e:\n            if e.response['Error']['Code'] != 'NoSuchKey':\n                raise\n            self._on_missing_log(log)\n        except Exception:\n            self._on_invalid_log_format(log)\n\n    def _write_status(self, message, is_error=False):\n        if is_error:\n            if self._is_last_status_double_space:\n                sys.stderr.write(\"%s\\n\\n\" % message)\n            else:\n                sys.stderr.write(\"\\n%s\\n\\n\" % message)\n            self._is_last_status_double_space = True\n        elif self.is_verbose:\n            self._is_last_status_double_space = False\n            sys.stdout.write(\"%s\\n\" % message)\n\n    def _write_startup_text(self):\n        sys.stdout.write(\n            'Validating log files for trail %s between %s and %s\\n\\n'\n            % (self.trail_arn, format_display_date(self.start_time),\n               format_display_date(self.end_time)))\n\n    def _write_summary_text(self):\n        if not self._is_last_status_double_space:\n            sys.stdout.write('\\n')\n        sys.stdout.write('Results requested for %s to %s\\n'\n                         % (format_display_date(self.start_time),\n                            format_display_date(self.end_time)))\n        if not self._valid_digests and not self._invalid_digests:\n            sys.stdout.write('No digests found\\n')\n            return\n        if not self._found_start_time or not self._found_end_time:\n            sys.stdout.write('No valid digests found in range\\n')\n        else:\n            sys.stdout.write('Results found for %s to %s:\\n'\n                             % (format_display_date(self._found_start_time),\n                                format_display_date(self._found_end_time)))\n        self._write_ratio(self._valid_digests, self._invalid_digests, 'digest')\n        self._write_ratio(self._valid_logs, self._invalid_logs, 'log')\n        sys.stdout.write('\\n')\n\n    def _write_ratio(self, valid, invalid, name):\n        total = valid + invalid\n        if total > 0:\n            sys.stdout.write('\\n%d/%d %s files valid' % (valid, total, name))\n            if invalid > 0:\n                sys.stdout.write(', %d/%d %s files INVALID' % (invalid, total,\n                                                               name))\n\n    def _on_missing_digest(self, bucket, last_key, **kwargs):\n        self._invalid_digests += 1\n        self._write_status('Digest file\\ts3://%s/%s\\tINVALID: not found'\n                           % (bucket, last_key), True)\n\n    def _on_digest_gap(self, **kwargs):\n        self._write_status(\n            'No log files were delivered by CloudTrail between %s and %s'\n            % (format_display_date(kwargs['next_end_date']),\n               format_display_date(kwargs['last_start_date'])), True)\n\n    def _on_invalid_digest(self, message, **kwargs):\n        self._invalid_digests += 1\n        self._write_status(message, True)\n\n    def _on_invalid_log_format(self, log_data):\n        self._invalid_logs += 1\n        self._write_status(\n            ('Log file\\ts3://%s/%s\\tINVALID: invalid format'\n             % (log_data['s3Bucket'], log_data['s3Object'])), True)\n\n    def _on_log_invalid(self, log_data):\n        self._invalid_logs += 1\n        self._write_status(\n            \"Log file\\ts3://%s/%s\\tINVALID: hash value doesn't match\"\n            % (log_data['s3Bucket'], log_data['s3Object']), True)\n\n    def _on_missing_log(self, log_data):\n        self._invalid_logs += 1\n        self._write_status(\n            'Log file\\ts3://%s/%s\\tINVALID: not found'\n            % (log_data['s3Bucket'], log_data['s3Object']), True)\n", "awscli/customizations/gamelift/getlog.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport sys\nfrom functools import partial\n\nfrom awscli.compat import urlopen\nfrom awscli.customizations.commands import BasicCommand\n\n\nclass GetGameSessionLogCommand(BasicCommand):\n    NAME = 'get-game-session-log'\n    DESCRIPTION = 'Download a compressed log file for a game session.'\n    ARG_TABLE = [\n        {'name': 'game-session-id', 'required': True,\n         'help_text': 'The game session ID'},\n        {'name': 'save-as', 'required': True,\n         'help_text': 'The filename to which the file should be saved (.zip)'}\n    ]\n\n    def _run_main(self, args, parsed_globals):\n        client = self._session.create_client(\n            'gamelift', region_name=parsed_globals.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl\n        )\n\n        # Retrieve a signed url.\n        response = client.get_game_session_log_url(\n            GameSessionId=args.game_session_id)\n        url = response['PreSignedUrl']\n\n        # Retrieve the content from the presigned url and save it locally.\n        contents = urlopen(url)\n\n        sys.stdout.write(\n            'Downloading log archive for game session %s...\\r' %\n            args.game_session_id\n        )\n\n        with open(args.save_as, 'wb') as f:\n            for chunk in iter(partial(contents.read, 1024), b''):\n                f.write(chunk)\n\n        sys.stdout.write(\n            'Successfully downloaded log archive for game '\n            'session %s to %s\\n' % (args.game_session_id, args.save_as))\n\n        return 0\n", "awscli/customizations/gamelift/uploadbuild.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport threading\nimport contextlib\nimport os\nimport tempfile\nimport sys\nimport zipfile\n\nfrom s3transfer import S3Transfer\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.s3.utils import human_readable_size\n\n\nclass UploadBuildCommand(BasicCommand):\n    NAME = 'upload-build'\n    DESCRIPTION = 'Upload a new build to AWS GameLift.'\n    ARG_TABLE = [\n        {'name': 'name', 'required': True,\n         'help_text': 'The name of the build'},\n        {'name': 'build-version', 'required': True,\n         'help_text': 'The version of the build'},\n        {'name': 'build-root', 'required': True,\n         'help_text':\n         'The path to the directory containing the build to upload'},\n        {'name': 'server-sdk-version', 'required': False,\n         'help_text':\n             'The version of the GameLift server SDK used to '\n             'create the game server'},\n        {'name': 'operating-system', 'required': False,\n         'help_text': 'The operating system the build runs on'}\n    ]\n\n    def _run_main(self, args, parsed_globals):\n        gamelift_client = self._session.create_client(\n            'gamelift', region_name=parsed_globals.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl\n        )\n        # Validate a build directory\n        if not validate_directory(args.build_root):\n            sys.stderr.write(\n                'Fail to upload %s. '\n                'The build root directory is empty or does not exist.\\n'\n                % (args.build_root)\n            )\n\n            return 255\n        # Create a build based on the operating system given.\n        create_build_kwargs = {\n            'Name': args.name,\n            'Version': args.build_version\n        }\n        if args.operating_system:\n            create_build_kwargs['OperatingSystem'] = args.operating_system\n        if args.server_sdk_version:\n            create_build_kwargs['ServerSdkVersion'] = args.server_sdk_version\n        response = gamelift_client.create_build(**create_build_kwargs)\n        build_id = response['Build']['BuildId']\n\n        # Retrieve a set of credentials and the s3 bucket and key.\n        response = gamelift_client.request_upload_credentials(\n            BuildId=build_id)\n        upload_credentials = response['UploadCredentials']\n        bucket = response['StorageLocation']['Bucket']\n        key = response['StorageLocation']['Key']\n\n        # Create the S3 Client for uploading the build based on the\n        # credentials returned from creating the build.\n        access_key = upload_credentials['AccessKeyId']\n        secret_key = upload_credentials['SecretAccessKey']\n        session_token = upload_credentials['SessionToken']\n        s3_client = self._session.create_client(\n            's3', aws_access_key_id=access_key,\n            aws_secret_access_key=secret_key,\n            aws_session_token=session_token,\n            region_name=parsed_globals.region,\n            verify=parsed_globals.verify_ssl\n        )\n\n        s3_transfer_mgr = S3Transfer(s3_client)\n\n        try:\n            fd, temporary_zipfile = tempfile.mkstemp('%s.zip' % build_id)\n            zip_directory(temporary_zipfile, args.build_root)\n            s3_transfer_mgr.upload_file(\n                temporary_zipfile, bucket, key,\n                callback=ProgressPercentage(\n                    temporary_zipfile,\n                    label='Uploading ' + args.build_root + ':'\n                )\n            )\n        finally:\n            os.close(fd)\n            os.remove(temporary_zipfile)\n\n        sys.stdout.write(\n            'Successfully uploaded %s to AWS GameLift\\n'\n            'Build ID: %s\\n' % (args.build_root, build_id))\n\n        return 0\n\n\ndef zip_directory(zipfile_name, source_root):\n    source_root = os.path.abspath(source_root)\n    with open(zipfile_name, 'wb') as f:\n        zip_file = zipfile.ZipFile(f, 'w', zipfile.ZIP_DEFLATED, True)\n        with contextlib.closing(zip_file) as zf:\n            for root, dirs, files in os.walk(source_root):\n                for filename in files:\n                    full_path = os.path.join(root, filename)\n                    relative_path = os.path.relpath(\n                        full_path, source_root)\n                    zf.write(full_path, relative_path)\n\n\ndef validate_directory(source_root):\n    # For Python26 on Windows, passing an empty string equates to the\n    # current directory, which is not intended behavior.\n    if not source_root:\n        return False\n    # We walk the root because we want to validate there's at least one file\n    # that exists recursively from the root directory\n    for path, dirs, files in os.walk(source_root):\n        if files:\n            return True\n    return False\n\n\n# TODO: Remove this class once available to CLI from s3transfer\n# docstring.\nclass ProgressPercentage(object):\n    def __init__(self, filename, label=None):\n        self._filename = filename\n        self._label = label\n        if self._label is None:\n            self._label = self._filename\n        self._size = float(os.path.getsize(filename))\n        self._seen_so_far = 0\n        self._lock = threading.Lock()\n\n    def __call__(self, bytes_amount):\n        with self._lock:\n            self._seen_so_far += bytes_amount\n            if self._size > 0:\n                percentage = (self._seen_so_far / self._size) * 100\n                sys.stdout.write(\n                    \"\\r%s  %s / %s  (%.2f%%)\" % (\n                        self._label, human_readable_size(self._seen_so_far),\n                        human_readable_size(self._size), percentage\n                    )\n                )\n                sys.stdout.flush()\n", "awscli/customizations/gamelift/__init__.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations.gamelift.uploadbuild import UploadBuildCommand\nfrom awscli.customizations.gamelift.getlog import GetGameSessionLogCommand\n\n\ndef register_gamelift_commands(event_emitter):\n    event_emitter.register('building-command-table.gamelift', inject_commands)\n\n\ndef inject_commands(command_table, session, **kwargs):\n    command_table['upload-build'] = UploadBuildCommand(session)\n    command_table['get-game-session-log'] = GetGameSessionLogCommand(session)\n", "awscli/customizations/s3/fileinfobuilder.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations.s3.fileinfo import FileInfo\n\n\nclass FileInfoBuilder(object):\n    \"\"\"\n    This class takes a ``FileBase`` object's attributes and generates\n    a ``FileInfo`` object so that the operation can be performed.\n    \"\"\"\n    def __init__(self, client, source_client=None,\n                 parameters = None, is_stream=False):\n        self._client = client\n        self._source_client = client\n        if source_client is not None:\n            self._source_client = source_client\n        self._parameters = parameters\n        self._is_stream = is_stream\n\n    def call(self, files):\n        for file_base in files:\n            file_info = self._inject_info(file_base)\n            yield file_info\n\n    def _inject_info(self, file_base):\n        file_info_attr = {}\n        file_info_attr['src'] = file_base.src\n        file_info_attr['dest'] = file_base.dest\n        file_info_attr['compare_key'] = file_base.compare_key\n        file_info_attr['size'] = file_base.size\n        file_info_attr['last_update'] = file_base.last_update\n        file_info_attr['src_type'] = file_base.src_type\n        file_info_attr['dest_type'] = file_base.dest_type\n        file_info_attr['operation_name'] = file_base.operation_name\n        file_info_attr['parameters'] = self._parameters\n        file_info_attr['is_stream'] = self._is_stream\n        file_info_attr['associated_response_data'] = file_base.response_data\n\n        # This is a bit quirky. The below conditional hinges on the --delete\n        # flag being set, which only occurs during a sync command. The source\n        # client in a sync delete refers to the source of the sync rather than\n        # the source of the delete. What this means is that the client that\n        # gets called during the delete process would point to the wrong region.\n        # Normally this doesn't matter because DNS will re-route the request\n        # to the correct region. In the case of s3v4 signing, however, this\n        # would result in a failed delete. The conditional below fixes this\n        # issue by swapping clients only in the case of a sync delete since\n        # swapping which client is used in the delete function would then break\n        # moving under s3v4.\n        if (file_base.operation_name == 'delete' and\n                self._parameters.get('delete')):\n            file_info_attr['client'] = self._source_client\n            file_info_attr['source_client'] = self._client\n        else:\n            file_info_attr['client'] = self._client\n            file_info_attr['source_client'] = self._source_client\n\n        return FileInfo(**file_info_attr)\n", "awscli/customizations/s3/filters.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport fnmatch\nimport os\n\nfrom awscli.customizations.s3.utils import split_s3_bucket_key\n\n\nLOG = logging.getLogger(__name__)\n\n\ndef create_filter(parameters):\n    \"\"\"Given the CLI parameters dict, create a Filter object.\"\"\"\n    # We need to evaluate all the filters based on the source\n    # directory.\n    if parameters['filters']:\n        cli_filters = parameters['filters']\n        real_filters = []\n        for filter_type, filter_pattern in cli_filters:\n            real_filters.append((filter_type.lstrip('-'),\n                                 filter_pattern))\n        source_location = parameters['src']\n        if source_location.startswith('s3://'):\n            # This gives us (bucket, keyname) and we want\n            # the bucket to be the root dir.\n            src_rootdir = _get_s3_root(source_location,\n                                       parameters['dir_op'])\n        else:\n            src_rootdir = _get_local_root(parameters['src'], parameters['dir_op'])\n\n        destination_location = parameters['dest']\n        if destination_location.startswith('s3://'):\n            dst_rootdir = _get_s3_root(parameters['dest'],\n                                       parameters['dir_op'])\n        else:\n            dst_rootdir = _get_local_root(parameters['dest'],\n                                          parameters['dir_op'])\n\n        return Filter(real_filters, src_rootdir, dst_rootdir)\n    else:\n        return Filter({}, None, None)\n\n\ndef _get_s3_root(source_location, dir_op):\n    # Obtain the bucket and the key.\n    bucket, key = split_s3_bucket_key(source_location)\n    if not dir_op and not key.endswith('/'):\n        # If we are not performing an operation on a directory and the key\n        # is of the form: ``prefix/key``. We only want ``prefix`` included in\n        # the the s3 root and not ``key``.\n        key = '/'.join(key.split('/')[:-1])\n    # Rejoin the bucket and key back together.\n    s3_path = '/'.join([bucket, key])\n    return s3_path\n\n\ndef _get_local_root(source_location, dir_op):\n    if dir_op:\n        rootdir = os.path.abspath(source_location)\n    else:\n        rootdir = os.path.abspath(os.path.dirname(source_location))\n    return rootdir\n\n\nclass Filter(object):\n    \"\"\"\n    This is a universal exclude/include filter.\n    \"\"\"\n    def __init__(self, patterns, rootdir, dst_rootdir):\n        \"\"\"\n        :var patterns: A list of patterns. A pattern consists of a list\n            whose first member is a string 'exclude' or 'include'.\n            The second member is the actual rule.\n        :var rootdir: The root directory where the patterns are evaluated.\n            This will generally be the directory of the source location.\n        :var dst_rootdir: The destination root directory where the patterns are\n            evaluated.  This is only useful when the --delete option is\n            also specified.\n\n        \"\"\"\n        self._original_patterns = patterns\n        self.patterns = self._full_path_patterns(patterns, rootdir)\n        self.dst_patterns = self._full_path_patterns(patterns, dst_rootdir)\n\n    def _full_path_patterns(self, original_patterns, rootdir):\n        # We need to transform the patterns into patterns that have\n        # the root dir prefixed, so things like ``--exclude \"*\"``\n        # will actually be ['exclude', '/path/to/root/*']\n        full_patterns = []\n        for pattern in original_patterns:\n            full_patterns.append(\n                (pattern[0], os.path.join(rootdir, pattern[1])))\n        return full_patterns\n\n    def call(self, file_infos):\n        \"\"\"\n        This function iterates over through the yielded file_info objects.  It\n        determines the type of the file and applies pattern matching to\n        determine if the rule applies.  While iterating though the patterns the\n        file is assigned a boolean flag to determine if a file should be\n        yielded on past the filer.  Anything identified by the exclude filter\n        has its flag set to false.  Anything identified by the include filter\n        has its flag set to True.  All files begin with the flag set to true.\n        Rules listed at the end will overwrite flags thrown by rules listed\n        before it.\n        \"\"\"\n        for file_info in file_infos:\n            file_path = file_info.src\n            file_status = (file_info, True)\n            for pattern, dst_pattern in zip(self.patterns, self.dst_patterns):\n                current_file_status = self._match_pattern(pattern, file_info)\n                if current_file_status is not None:\n                    file_status = current_file_status\n                dst_current_file_status = self._match_pattern(dst_pattern, file_info)\n                if dst_current_file_status is not None:\n                    file_status = dst_current_file_status\n            LOG.debug(\"=%s final filtered status, should_include: %s\",\n                      file_path, file_status[1])\n            if file_status[1]:\n                yield file_info\n\n    def _match_pattern(self, pattern, file_info):\n        file_status = None\n        file_path = file_info.src\n        pattern_type = pattern[0]\n        if file_info.src_type == 'local':\n            path_pattern = pattern[1].replace('/', os.sep)\n        else:\n            path_pattern = pattern[1].replace(os.sep, '/')\n        is_match = fnmatch.fnmatch(file_path, path_pattern)\n        if is_match and pattern_type == 'include':\n            file_status = (file_info, True)\n            LOG.debug(\"%s matched include filter: %s\",\n                        file_path, path_pattern)\n        elif is_match and pattern_type == 'exclude':\n            file_status = (file_info, False)\n            LOG.debug(\"%s matched exclude filter: %s\",\n                        file_path, path_pattern)\n        else:\n            LOG.debug(\"%s did not match %s filter: %s\",\n                        file_path, pattern_type, path_pattern)\n        return file_status\n", "awscli/customizations/s3/utils.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport argparse\nimport logging\nfrom datetime import datetime\nimport mimetypes\nimport errno\nimport os\nimport re\nimport time\nfrom collections import namedtuple, deque\n\nfrom dateutil.parser import parse\nfrom dateutil.tz import tzlocal, tzutc\nfrom s3transfer.subscribers import BaseSubscriber\n\nfrom awscli.compat import bytes_print\nfrom awscli.compat import queue\n\nLOGGER = logging.getLogger(__name__)\nHUMANIZE_SUFFIXES = ('KiB', 'MiB', 'GiB', 'TiB', 'PiB', 'EiB')\nEPOCH_TIME = datetime(1970, 1, 1, tzinfo=tzutc())\n# Maximum object size allowed in S3.\n# See: http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html\nMAX_UPLOAD_SIZE = 5 * (1024 ** 4)\nSIZE_SUFFIX = {\n    'kb': 1024,\n    'mb': 1024 ** 2,\n    'gb': 1024 ** 3,\n    'tb': 1024 ** 4,\n    'kib': 1024,\n    'mib': 1024 ** 2,\n    'gib': 1024 ** 3,\n    'tib': 1024 ** 4,\n}\n_S3_ACCESSPOINT_TO_BUCKET_KEY_REGEX = re.compile(\n    r'^(?P<bucket>arn:(aws).*:s3:[a-z\\-0-9]*:[0-9]{12}:accesspoint[:/][^/]+)/?'\n    r'(?P<key>.*)$'\n)\n_S3_OUTPOST_TO_BUCKET_KEY_REGEX = re.compile(\n    r'^(?P<bucket>arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:]'\n    r'[a-zA-Z0-9\\-]{1,63}[/:]accesspoint[/:][a-zA-Z0-9\\-]{1,63})[/:]?(?P<key>.*)$'\n)\n\n_S3_OUTPOST_BUCKET_ARN_TO_BUCKET_KEY_REGEX = re.compile(\n    r'^(?P<bucket>arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:]'\n    r'[a-zA-Z0-9\\-]{1,63}[/:]bucket[/:]'\n    r'[a-zA-Z0-9\\-]{1,63})[/:]?(?P<key>.*)$'\n)\n\n_S3_OBJECT_LAMBDA_TO_BUCKET_KEY_REGEX = re.compile(\n    r'^(?P<bucket>arn:(aws).*:s3-object-lambda:[a-z\\-0-9]+:[0-9]{12}:'\n    r'accesspoint[/:][a-zA-Z0-9\\-]{1,63})[/:]?(?P<key>.*)$'\n)\n\n\ndef human_readable_size(value):\n    \"\"\"Convert a size in bytes into a human readable format.\n\n    For example::\n\n        >>> human_readable_size(1)\n        '1 Byte'\n        >>> human_readable_size(10)\n        '10 Bytes'\n        >>> human_readable_size(1024)\n        '1.0 KiB'\n        >>> human_readable_size(1024 * 1024)\n        '1.0 MiB'\n\n    :param value: The size in bytes.\n    :return: The size in a human readable format based on base-2 units.\n\n    \"\"\"\n    base = 1024\n    bytes_int = float(value)\n\n    if bytes_int == 1:\n        return '1 Byte'\n    elif bytes_int < base:\n        return '%d Bytes' % bytes_int\n\n    for i, suffix in enumerate(HUMANIZE_SUFFIXES):\n        unit = base ** (i+2)\n        if round((bytes_int / unit) * base) < base:\n            return '%.1f %s' % ((base * bytes_int / unit), suffix)\n\n\ndef human_readable_to_bytes(value):\n    \"\"\"Converts a human readable size to bytes.\n\n    :param value: A string such as \"10MB\".  If a suffix is not included,\n        then the value is assumed to be an integer representing the size\n        in bytes.\n    :returns: The converted value in bytes as an integer\n\n    \"\"\"\n    value = value.lower()\n    if value[-2:] == 'ib':\n        # Assume IEC suffix.\n        suffix = value[-3:].lower()\n    else:\n        suffix = value[-2:].lower()\n    has_size_identifier = (\n        len(value) >= 2 and suffix in SIZE_SUFFIX)\n    if not has_size_identifier:\n        try:\n            return int(value)\n        except ValueError:\n            raise ValueError(\"Invalid size value: %s\" % value)\n    else:\n        multiplier = SIZE_SUFFIX[suffix]\n        return int(value[:-len(suffix)]) * multiplier\n\n\nclass AppendFilter(argparse.Action):\n    \"\"\"\n    This class is used as an action when parsing the parameters.\n    Specifically it is used for actions corresponding to exclude\n    and include filters.  What it does is that it appends a list\n    consisting of the name of the parameter and its value onto\n    a list containing these [parameter, value] lists.  In this\n    case, the name of the parameter will either be --include or\n    --exclude and the value will be the rule to apply.  This will\n    format all of the rules inputted into the command line\n    in a way compatible with the Filter class.  Note that rules that\n    appear later in the command line take preference over rulers that\n    appear earlier.\n    \"\"\"\n    def __call__(self, parser, namespace, values, option_string=None):\n        filter_list = getattr(namespace, self.dest)\n        if filter_list:\n            filter_list.append([option_string, values[0]])\n        else:\n            filter_list = [[option_string, values[0]]]\n        setattr(namespace, self.dest, filter_list)\n\n\nclass CreateDirectoryError(Exception):\n    pass\n\n\nclass StablePriorityQueue(queue.Queue):\n    \"\"\"Priority queue that maintains FIFO order for same priority items.\n\n    This class was written to handle the tasks created in\n    awscli.customizations.s3.tasks, but it's possible to use this\n    class outside of that context.  In order for this to be the case,\n    the following conditions should be met:\n\n        * Objects that are queued should have a PRIORITY attribute.\n          This should be an integer value not to exceed the max_priority\n          value passed into the ``__init__``.  Objects with lower\n          priority numbers are retrieved before objects with higher\n          priority numbers.\n        * A relatively small max_priority should be chosen.  ``get()``\n          calls are O(max_priority).\n\n    Any object that does not have a ``PRIORITY`` attribute or whose\n    priority exceeds ``max_priority`` will be queued at the highest\n    (least important) priority available.\n\n    \"\"\"\n    def __init__(self, maxsize=0, max_priority=20):\n        queue.Queue.__init__(self, maxsize=maxsize)\n        self.priorities = [deque([]) for i in range(max_priority + 1)]\n        self.default_priority = max_priority\n\n    def _qsize(self):\n        size = 0\n        for bucket in self.priorities:\n            size += len(bucket)\n        return size\n\n    def _put(self, item):\n        priority = min(getattr(item, 'PRIORITY', self.default_priority),\n                        self.default_priority)\n        self.priorities[priority].append(item)\n\n    def _get(self):\n        for bucket in self.priorities:\n            if not bucket:\n                continue\n            return bucket.popleft()\n\n\ndef block_unsupported_resources(s3_path):\n    # AWS CLI s3 commands don't support object lambdas only direct API calls\n    # are available for such resources\n    if _S3_OBJECT_LAMBDA_TO_BUCKET_KEY_REGEX.match(s3_path):\n        # In AWS CLI v2 we should use\n        # awscli.customizations.exceptions.ParamValidationError\n        # instead of ValueError\n        raise ValueError(\n            's3 commands do not support S3 Object Lambda resources. '\n            'Use s3api commands instead.'\n        )\n    # AWS S3 API and AWS CLI s3 commands don't support Outpost bucket ARNs\n    # only s3control API supports them so far\n    if _S3_OUTPOST_BUCKET_ARN_TO_BUCKET_KEY_REGEX.match(s3_path):\n        raise ValueError(\n            's3 commands do not support Outpost Bucket ARNs. '\n            'Use s3control commands instead.'\n        )\n\n\ndef find_bucket_key(s3_path):\n    \"\"\"\n    This is a helper function that given an s3 path such that the path is of\n    the form: bucket/key\n    It will return the bucket and the key represented by the s3 path\n    \"\"\"\n    block_unsupported_resources(s3_path)\n    match = _S3_ACCESSPOINT_TO_BUCKET_KEY_REGEX.match(s3_path)\n    if match:\n        return match.group('bucket'), match.group('key')\n    match = _S3_OUTPOST_TO_BUCKET_KEY_REGEX.match(s3_path)\n    if match:\n        return match.group('bucket'), match.group('key')\n    s3_components = s3_path.split('/', 1)\n    bucket = s3_components[0]\n    s3_key = ''\n    if len(s3_components) > 1:\n        s3_key = s3_components[1]\n    return bucket, s3_key\n\n\ndef split_s3_bucket_key(s3_path):\n    \"\"\"Split s3 path into bucket and key prefix.\n\n    This will also handle the s3:// prefix.\n\n    :return: Tuple of ('bucketname', 'keyname')\n\n    \"\"\"\n    if s3_path.startswith('s3://'):\n        s3_path = s3_path[5:]\n    return find_bucket_key(s3_path)\n\n\ndef get_file_stat(path):\n    \"\"\"\n    This is a helper function that given a local path return the size of\n    the file in bytes and time of last modification.\n    \"\"\"\n    try:\n        stats = os.stat(path)\n    except IOError as e:\n        raise ValueError('Could not retrieve file stat of \"%s\": %s' % (\n            path, e))\n\n    try:\n        update_time = datetime.fromtimestamp(stats.st_mtime, tzlocal())\n    except (ValueError, OSError, OverflowError):\n        # Python's fromtimestamp raises value errors when the timestamp is out\n        # of range of the platform's C localtime() function. This can cause\n        # issues when syncing from systems with a wide range of valid\n        # timestamps to systems with a lower range. Some systems support\n        # 64-bit timestamps, for instance, while others only support 32-bit.\n        # We don't want to fail in these cases, so instead we pass along none.\n        update_time = None\n\n    return stats.st_size, update_time\n\n\ndef find_dest_path_comp_key(files, src_path=None):\n    \"\"\"\n    This is a helper function that determines the destination path and compare\n    key given parameters received from the ``FileFormat`` class.\n    \"\"\"\n    src = files['src']\n    dest = files['dest']\n    src_type = src['type']\n    dest_type = dest['type']\n    if src_path is None:\n        src_path = src['path']\n\n    sep_table = {'s3': '/', 'local': os.sep}\n\n    if files['dir_op']:\n        rel_path = src_path[len(src['path']):]\n    else:\n        rel_path = src_path.split(sep_table[src_type])[-1]\n    compare_key = rel_path.replace(sep_table[src_type], '/')\n    if files['use_src_name']:\n        dest_path = dest['path']\n        dest_path += rel_path.replace(sep_table[src_type],\n                                      sep_table[dest_type])\n    else:\n        dest_path = dest['path']\n    return dest_path, compare_key\n\n\ndef create_warning(path, error_message, skip_file=True):\n    \"\"\"\n    This creates a ``PrintTask`` for whenever a warning is to be thrown.\n    \"\"\"\n    print_string = \"warning: \"\n    if skip_file:\n        print_string = print_string + \"Skipping file \" + path + \". \"\n    print_string = print_string + error_message\n    warning_message = WarningResult(message=print_string, error=False,\n                                    warning=True)\n    return warning_message\n\n\nclass StdoutBytesWriter(object):\n    \"\"\"\n    This class acts as a file-like object that performs the bytes_print\n    function on write.\n    \"\"\"\n    def __init__(self, stdout=None):\n        self._stdout = stdout\n\n    def write(self, b):\n        \"\"\"\n        Writes data to stdout as bytes.\n\n        :param b: data to write\n        \"\"\"\n        bytes_print(b, self._stdout)\n\n\ndef guess_content_type(filename):\n    \"\"\"Given a filename, guess it's content type.\n\n    If the type cannot be guessed, a value of None is returned.\n    \"\"\"\n    try:\n        return mimetypes.guess_type(filename)[0]\n    # This catches a bug in the mimetype library where some MIME types\n    # specifically on windows machines cause a UnicodeDecodeError\n    # because the MIME type in the Windows registry has an encoding\n    # that cannot be properly encoded using the default system encoding.\n    # https://bugs.python.org/issue9291\n    #\n    # So instead of hard failing, just log the issue and fall back to the\n    # default guessed content type of None.\n    except UnicodeDecodeError:\n        LOGGER.debug(\n            'Unable to guess content type for %s due to '\n            'UnicodeDecodeError: ', filename, exc_info=True\n        )\n\n\ndef relative_path(filename, start=os.path.curdir):\n    \"\"\"Cross platform relative path of a filename.\n\n    If no relative path can be calculated (i.e different\n    drives on Windows), then instead of raising a ValueError,\n    the absolute path is returned.\n\n    \"\"\"\n    try:\n        dirname, basename = os.path.split(filename)\n        relative_dir = os.path.relpath(dirname, start)\n        return os.path.join(relative_dir, basename)\n    except ValueError:\n        return os.path.abspath(filename)\n\n\ndef set_file_utime(filename, desired_time):\n    \"\"\"\n    Set the utime of a file, and if it fails, raise a more explicit error.\n\n    :param filename: the file to modify\n    :param desired_time: the epoch timestamp to set for atime and mtime.\n    :raises: SetFileUtimeError: if you do not have permission (errno 1)\n    :raises: OSError: for all errors other than errno 1\n    \"\"\"\n    try:\n        os.utime(filename, (desired_time, desired_time))\n    except OSError as e:\n        # Only raise a more explicit exception when it is a permission issue.\n        if e.errno != errno.EPERM:\n            raise e\n        raise SetFileUtimeError(\n            (\"The file was downloaded, but attempting to modify the \"\n             \"utime of the file failed. Is the file owned by another user?\"))\n\n\nclass SetFileUtimeError(Exception):\n    pass\n\n\ndef _date_parser(date_string):\n    return parse(date_string).astimezone(tzlocal())\n\n\nclass BucketLister(object):\n    \"\"\"List keys in a bucket.\"\"\"\n    def __init__(self, client, date_parser=_date_parser):\n        self._client = client\n        self._date_parser = date_parser\n\n    def list_objects(self, bucket, prefix=None, page_size=None,\n                     extra_args=None):\n        kwargs = {'Bucket': bucket, 'PaginationConfig': {'PageSize': page_size}}\n        if prefix is not None:\n            kwargs['Prefix'] = prefix\n        if extra_args is not None:\n            kwargs.update(extra_args)\n\n        paginator = self._client.get_paginator('list_objects_v2')\n        pages = paginator.paginate(**kwargs)\n        for page in pages:\n            contents = page.get('Contents', [])\n            for content in contents:\n                source_path = bucket + '/' + content['Key']\n                content['LastModified'] = self._date_parser(\n                    content['LastModified'])\n                yield source_path, content\n\n\nclass PrintTask(namedtuple('PrintTask',\n                          ['message', 'error', 'total_parts', 'warning'])):\n    def __new__(cls, message, error=False, total_parts=None, warning=None):\n        \"\"\"\n        :param message: An arbitrary string associated with the entry.   This\n            can be used to communicate the result of the task.\n        :param error: Boolean indicating a failure.\n        :param total_parts: The total number of parts for multipart transfers.\n        :param warning: Boolean indicating a warning\n        \"\"\"\n        return super(PrintTask, cls).__new__(cls, message, error, total_parts,\n                                             warning)\n\nWarningResult = PrintTask\n\n\nclass RequestParamsMapper(object):\n    \"\"\"A utility class that maps CLI params to request params\n\n    Each method in the class maps to a particular operation and will set\n    the request parameters depending on the operation and CLI parameters\n    provided. For each of the class's methods the parameters are as follows:\n\n    :type request_params: dict\n    :param request_params: A dictionary to be filled out with the appropriate\n        parameters for the specified client operation using the current CLI\n        parameters\n\n    :type cli_params: dict\n    :param cli_params: A dictionary of the current CLI params that will be\n        used to generate the request parameters for the specified operation\n\n    For example, take the mapping of request parameters for PutObject::\n\n        >>> cli_request_params = {'sse': 'AES256', 'storage_class': 'GLACIER'}\n        >>> request_params = {}\n        >>> RequestParamsMapper.map_put_object_params(\n                request_params, cli_request_params)\n        >>> print(request_params)\n        {'StorageClass': 'GLACIER', 'ServerSideEncryption': 'AES256'}\n\n    Note that existing parameters in ``request_params`` will be overridden if\n    a parameter in ``cli_params`` maps to the existing parameter.\n    \"\"\"\n    @classmethod\n    def map_put_object_params(cls, request_params, cli_params):\n        \"\"\"Map CLI params to PutObject request params\"\"\"\n        cls._set_general_object_params(request_params, cli_params)\n        cls._set_metadata_params(request_params, cli_params)\n        cls._set_sse_request_params(request_params, cli_params)\n        cls._set_sse_c_request_params(request_params, cli_params)\n        cls._set_request_payer_param(request_params, cli_params)\n\n    @classmethod\n    def map_get_object_params(cls, request_params, cli_params):\n        \"\"\"Map CLI params to GetObject request params\"\"\"\n        cls._set_sse_c_request_params(request_params, cli_params)\n        cls._set_request_payer_param(request_params, cli_params)\n\n    @classmethod\n    def map_copy_object_params(cls, request_params, cli_params):\n        \"\"\"Map CLI params to CopyObject request params\"\"\"\n        cls._set_general_object_params(request_params, cli_params)\n        cls._set_metadata_directive_param(request_params, cli_params)\n        cls._set_metadata_params(request_params, cli_params)\n        cls._auto_populate_metadata_directive(request_params)\n        cls._set_sse_request_params(request_params, cli_params)\n        cls._set_sse_c_and_copy_source_request_params(\n            request_params, cli_params)\n        cls._set_request_payer_param(request_params, cli_params)\n\n    @classmethod\n    def map_head_object_params(cls, request_params, cli_params):\n        \"\"\"Map CLI params to HeadObject request params\"\"\"\n        cls._set_sse_c_request_params(request_params, cli_params)\n        cls._set_request_payer_param(request_params, cli_params)\n\n    @classmethod\n    def map_create_multipart_upload_params(cls, request_params, cli_params):\n        \"\"\"Map CLI params to CreateMultipartUpload request params\"\"\"\n        cls._set_general_object_params(request_params, cli_params)\n        cls._set_sse_request_params(request_params, cli_params)\n        cls._set_sse_c_request_params(request_params, cli_params)\n        cls._set_metadata_params(request_params, cli_params)\n        cls._set_request_payer_param(request_params, cli_params)\n\n    @classmethod\n    def map_upload_part_params(cls, request_params, cli_params):\n        \"\"\"Map CLI params to UploadPart request params\"\"\"\n        cls._set_sse_c_request_params(request_params, cli_params)\n        cls._set_request_payer_param(request_params, cli_params)\n\n    @classmethod\n    def map_upload_part_copy_params(cls, request_params, cli_params):\n        \"\"\"Map CLI params to UploadPartCopy request params\"\"\"\n        cls._set_sse_c_and_copy_source_request_params(\n            request_params, cli_params)\n        cls._set_request_payer_param(request_params, cli_params)\n\n    @classmethod\n    def map_delete_object_params(cls, request_params, cli_params):\n        cls._set_request_payer_param(request_params, cli_params)\n\n    @classmethod\n    def map_list_objects_v2_params(cls, request_params, cli_params):\n        cls._set_request_payer_param(request_params, cli_params)\n\n    @classmethod\n    def _set_request_payer_param(cls, request_params, cli_params):\n        if cli_params.get('request_payer'):\n            request_params['RequestPayer'] = cli_params['request_payer']\n\n    @classmethod\n    def _set_general_object_params(cls, request_params, cli_params):\n        # Parameters set in this method should be applicable to the following\n        # operations involving objects: PutObject, CopyObject, and\n        # CreateMultipartUpload.\n        general_param_translation = {\n            'acl': 'ACL',\n            'storage_class': 'StorageClass',\n            'website_redirect': 'WebsiteRedirectLocation',\n            'content_type': 'ContentType',\n            'cache_control': 'CacheControl',\n            'content_disposition': 'ContentDisposition',\n            'content_encoding': 'ContentEncoding',\n            'content_language': 'ContentLanguage',\n            'expires': 'Expires'\n        }\n        for cli_param_name in general_param_translation:\n            if cli_params.get(cli_param_name):\n                request_param_name = general_param_translation[cli_param_name]\n                request_params[request_param_name] = cli_params[cli_param_name]\n        cls._set_grant_params(request_params, cli_params)\n\n    @classmethod\n    def _set_grant_params(cls, request_params, cli_params):\n        if cli_params.get('grants'):\n            for grant in cli_params['grants']:\n                try:\n                    permission, grantee = grant.split('=', 1)\n                except ValueError:\n                    raise ValueError('grants should be of the form '\n                                     'permission=principal')\n                request_params[cls._permission_to_param(permission)] = grantee\n\n    @classmethod\n    def _permission_to_param(cls, permission):\n        if permission == 'read':\n            return 'GrantRead'\n        if permission == 'full':\n            return 'GrantFullControl'\n        if permission == 'readacl':\n            return 'GrantReadACP'\n        if permission == 'writeacl':\n            return 'GrantWriteACP'\n        raise ValueError('permission must be one of: '\n                         'read|readacl|writeacl|full')\n\n    @classmethod\n    def _set_metadata_params(cls, request_params, cli_params):\n        if cli_params.get('metadata'):\n            request_params['Metadata'] = cli_params['metadata']\n\n    @classmethod\n    def _auto_populate_metadata_directive(cls, request_params):\n        if request_params.get('Metadata') and \\\n                not request_params.get('MetadataDirective'):\n            request_params['MetadataDirective'] = 'REPLACE'\n\n    @classmethod\n    def _set_metadata_directive_param(cls, request_params, cli_params):\n        if cli_params.get('metadata_directive'):\n            request_params['MetadataDirective'] = cli_params[\n                'metadata_directive']\n\n    @classmethod\n    def _set_sse_request_params(cls, request_params, cli_params):\n        if cli_params.get('sse'):\n            request_params['ServerSideEncryption'] = cli_params['sse']\n        if  cli_params.get('sse_kms_key_id'):\n            request_params['SSEKMSKeyId'] = cli_params['sse_kms_key_id']\n\n    @classmethod\n    def _set_sse_c_request_params(cls, request_params, cli_params):\n        if cli_params.get('sse_c'):\n            request_params['SSECustomerAlgorithm'] = cli_params['sse_c']\n            request_params['SSECustomerKey'] = cli_params['sse_c_key']\n\n    @classmethod\n    def _set_sse_c_copy_source_request_params(cls, request_params, cli_params):\n        if cli_params.get('sse_c_copy_source'):\n            request_params['CopySourceSSECustomerAlgorithm'] = cli_params[\n                'sse_c_copy_source']\n            request_params['CopySourceSSECustomerKey'] = cli_params[\n                'sse_c_copy_source_key']\n\n    @classmethod\n    def _set_sse_c_and_copy_source_request_params(cls, request_params,\n                                                  cli_params):\n        cls._set_sse_c_request_params(request_params, cli_params)\n        cls._set_sse_c_copy_source_request_params(request_params, cli_params)\n\n\nclass ProvideSizeSubscriber(BaseSubscriber):\n    \"\"\"\n    A subscriber which provides the transfer size before it's queued.\n    \"\"\"\n    def __init__(self, size):\n        self.size = size\n\n    def on_queued(self, future, **kwargs):\n        future.meta.provide_transfer_size(self.size)\n\n\n# TODO: Eventually port this down to the BaseSubscriber or a new subscriber\n# class in s3transfer. The functionality is very convenient but may need\n# some further design decisions to make it a feature in s3transfer.\nclass OnDoneFilteredSubscriber(BaseSubscriber):\n    \"\"\"Subscriber that differentiates between successes and failures\n\n    It is really a convenience class so developers do not have to have\n    to constantly remember to have a general try/except around future.result()\n    \"\"\"\n    def on_done(self, future, **kwargs):\n        future_exception = None\n        try:\n\n            future.result()\n        except Exception as e:\n            future_exception = e\n        # If the result propagates an error, call the on_failure\n        # method instead.\n        if future_exception:\n            self._on_failure(future, future_exception)\n        else:\n            self._on_success(future)\n\n    def _on_success(self, future):\n        pass\n\n    def _on_failure(self, future, e):\n        pass\n\n\nclass DeleteSourceSubscriber(OnDoneFilteredSubscriber):\n    \"\"\"A subscriber which deletes the source of the transfer.\"\"\"\n    def _on_success(self, future):\n        try:\n            self._delete_source(future)\n        except Exception as e:\n            future.set_exception(e)\n\n    def _delete_source(self, future):\n        raise NotImplementedError('_delete_source()')\n\n\nclass DeleteSourceObjectSubscriber(DeleteSourceSubscriber):\n    \"\"\"A subscriber which deletes an object.\"\"\"\n    def __init__(self, client):\n        self._client = client\n\n    def _get_bucket(self, call_args):\n        return call_args.bucket\n\n    def _get_key(self, call_args):\n        return call_args.key\n\n    def _delete_source(self, future):\n        call_args = future.meta.call_args\n        delete_object_kwargs = {\n            'Bucket': self._get_bucket(call_args),\n            'Key': self._get_key(call_args)\n        }\n        if call_args.extra_args.get('RequestPayer'):\n            delete_object_kwargs['RequestPayer'] = call_args.extra_args[\n                'RequestPayer']\n        self._client.delete_object(**delete_object_kwargs)\n\n\nclass DeleteCopySourceObjectSubscriber(DeleteSourceObjectSubscriber):\n    \"\"\"A subscriber which deletes the copy source.\"\"\"\n    def _get_bucket(self, call_args):\n        return call_args.copy_source['Bucket']\n\n    def _get_key(self, call_args):\n        return call_args.copy_source['Key']\n\n\nclass DeleteSourceFileSubscriber(DeleteSourceSubscriber):\n    \"\"\"A subscriber which deletes a file.\"\"\"\n    def _delete_source(self, future):\n        os.remove(future.meta.call_args.fileobj)\n\n\nclass BaseProvideContentTypeSubscriber(BaseSubscriber):\n    \"\"\"A subscriber that provides content type when creating s3 objects\"\"\"\n\n    def on_queued(self, future, **kwargs):\n        guessed_type = guess_content_type(self._get_filename(future))\n        if guessed_type is not None:\n            future.meta.call_args.extra_args['ContentType'] = guessed_type\n\n    def _get_filename(self, future):\n        raise NotImplementedError('_get_filename()')\n\n\nclass ProvideUploadContentTypeSubscriber(BaseProvideContentTypeSubscriber):\n    def _get_filename(self, future):\n        return future.meta.call_args.fileobj\n\n\nclass ProvideCopyContentTypeSubscriber(BaseProvideContentTypeSubscriber):\n    def _get_filename(self, future):\n        return future.meta.call_args.copy_source['Key']\n\n\nclass ProvideLastModifiedTimeSubscriber(OnDoneFilteredSubscriber):\n    \"\"\"Sets utime for a downloaded file\"\"\"\n    def __init__(self, last_modified_time, result_queue):\n        self._last_modified_time = last_modified_time\n        self._result_queue = result_queue\n\n    def _on_success(self, future, **kwargs):\n        filename = future.meta.call_args.fileobj\n        try:\n            last_update_tuple = self._last_modified_time.timetuple()\n            mod_timestamp = time.mktime(last_update_tuple)\n            set_file_utime(filename, int(mod_timestamp))\n        except Exception as e:\n            warning_message = (\n                'Successfully Downloaded %s but was unable to update the '\n                'last modified time. %s' % (filename, e))\n            self._result_queue.put(create_warning(filename, warning_message))\n\n\nclass DirectoryCreatorSubscriber(BaseSubscriber):\n    \"\"\"Creates a directory to download if it does not exist\"\"\"\n    def on_queued(self, future, **kwargs):\n        d = os.path.dirname(future.meta.call_args.fileobj)\n        try:\n            if not os.path.exists(d):\n                os.makedirs(d)\n        except OSError as e:\n            if not e.errno == errno.EEXIST:\n                raise CreateDirectoryError(\n                    \"Could not create directory %s: %s\" % (d, e))\n\n\nclass NonSeekableStream(object):\n    \"\"\"Wrap a file like object as a non seekable stream.\n\n    This class is used to wrap an existing file like object\n    such that it only has a ``.read()`` method.\n\n    There are some file like objects that aren't truly seekable\n    but appear to be.  For example, on windows, sys.stdin has\n    a ``seek()`` method, and calling ``seek(0)`` even appears\n    to work.  However, subsequent ``.read()`` calls will just\n    return an empty string.\n\n    Consumers of these file like object have no way of knowing\n    if these files are truly seekable or not, so this class\n    can be used to force non-seekable behavior when you know\n    for certain that a fileobj is non seekable.\n\n    \"\"\"\n    def __init__(self, fileobj):\n        self._fileobj = fileobj\n\n    def read(self, amt=None):\n        if amt is None:\n            return self._fileobj.read()\n        else:\n            return self._fileobj.read(amt)\n\n\nclass S3PathResolver:\n    _S3_ACCESSPOINT_ARN_TO_ACCOUNT_NAME_REGEX = re.compile(\n        r'^arn:aws.*:s3:[a-z0-9\\-]+:(?P<account>[0-9]{12}):accesspoint[:/]'\n        r'(?P<name>[a-z0-9\\-]{3,50})$'\n    )\n    _S3_OUTPOST_ACCESSPOINT_ARN_TO_ACCOUNT_REGEX = re.compile(\n        r'^arn:aws.*:s3-outposts:[a-z0-9\\-]+:(?P<account>[0-9]{12}):outpost/'\n        r'op-[a-zA-Z0-9]+/accesspoint[:/][a-z0-9\\-]{3,50}$'\n    )\n    _S3_MRAP_ARN_TO_ACCOUNT_ALIAS_REGEX = re.compile(\n        r'^arn:aws:s3::(?P<account>[0-9]{12}):accesspoint[:/]'\n        r'(?P<alias>[a-zA-Z0-9]+\\.mrap)$'\n    )\n\n    def __init__(self, s3control_client, sts_client):\n        self._s3control_client = s3control_client\n        self._sts_client = sts_client\n\n    @classmethod\n    def has_underlying_s3_path(self, path):\n        bucket, _ = split_s3_bucket_key(path)\n        return bool(\n            self._S3_ACCESSPOINT_ARN_TO_ACCOUNT_NAME_REGEX.match(bucket) or\n            self._S3_OUTPOST_ACCESSPOINT_ARN_TO_ACCOUNT_REGEX.match(bucket) or\n            self._S3_MRAP_ARN_TO_ACCOUNT_ALIAS_REGEX.match(bucket) or\n            bucket.endswith('-s3alias') or bucket.endswith('--op-s3'))\n\n    @classmethod\n    def from_session(cls, session, region, verify_ssl):\n        s3control_client = session.create_client(\n            's3control',\n            region_name=region,\n            verify=verify_ssl,\n        )\n        sts_client = session.create_client(\n            'sts',\n            verify=verify_ssl,\n        )\n        return cls(s3control_client, sts_client)\n\n    def resolve_underlying_s3_paths(self, path):\n        bucket, key = split_s3_bucket_key(path)\n        match = self._S3_ACCESSPOINT_ARN_TO_ACCOUNT_NAME_REGEX.match(bucket)\n        if match:\n            return self._resolve_accesspoint_arn(\n                match.group('account'), match.group('name'), key\n            )\n        match = self._S3_OUTPOST_ACCESSPOINT_ARN_TO_ACCOUNT_REGEX.match(bucket)\n        if match:\n            return self._resolve_accesspoint_arn(\n                match.group('account'), bucket, key\n            )\n        match = self._S3_MRAP_ARN_TO_ACCOUNT_ALIAS_REGEX.match(bucket)\n        if match:\n            return self._resolve_mrap_alias(\n                match.group('account'), match.group('alias'), key\n            )\n        if bucket.endswith('-s3alias'):\n            return self._resolve_accesspoint_alias(bucket, key)\n        if bucket.endswith('--op-s3'):\n            raise ValueError(\n                \"Can't resolve underlying bucket name of s3 outposts \"\n                \"access point alias. Use arn instead to resolve the \"\n                \"bucket name and validate the mv command.\"\n            )\n        return [path]\n\n    def _resolve_accesspoint_arn(self, account, name, key):\n        bucket = self._get_access_point_bucket(account, name)\n        return [f\"s3://{bucket}/{key}\"]\n\n    def _resolve_accesspoint_alias(self, alias, key):\n        account = self._get_account_id()\n        bucket = self._get_access_point_bucket(account, alias)\n        return [f\"s3://{bucket}/{key}\"]\n\n    def _resolve_mrap_alias(self, account, alias, key):\n        buckets = self._get_mrap_buckets(account, alias)\n        return [f\"s3://{bucket}/{key}\" for bucket in buckets]\n\n    def _get_access_point_bucket(self, account, name):\n        return self._s3control_client.get_access_point(\n            AccountId=account,\n            Name=name\n        )['Bucket']\n\n    def _get_account_id(self):\n        return self._sts_client.get_caller_identity()['Account']\n\n    def _get_mrap_buckets(self, account, alias):\n        next_token = None\n        while True:\n            args = {\"AccountId\": account}\n            if next_token:\n                args['NextToken'] = next_token\n            response = self._s3control_client.list_multi_region_access_points(\n                **args\n            )\n            for access_point in response['AccessPoints']:\n                if access_point['Alias'] == alias:\n                    return [\n                        region[\"Bucket\"] for region in access_point[\"Regions\"]\n                    ]\n            next_token = response.get('NextToken')\n            if not next_token:\n                raise ValueError(\n                    \"Couldn't find multi-region access point \"\n                    f\"with alias {alias} in account {account}\"\n                )\n", "awscli/customizations/s3/subcommands.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport logging\nimport sys\n\nfrom botocore.client import Config\nfrom botocore.utils import is_s3express_bucket, ensure_boolean\nfrom dateutil.parser import parse\nfrom dateutil.tz import tzlocal\n\nfrom awscli.compat import queue\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.s3.comparator import Comparator\nfrom awscli.customizations.s3.fileinfobuilder import FileInfoBuilder\nfrom awscli.customizations.s3.fileformat import FileFormat\nfrom awscli.customizations.s3.filegenerator import FileGenerator\nfrom awscli.customizations.s3.fileinfo import FileInfo\nfrom awscli.customizations.s3.filters import create_filter\nfrom awscli.customizations.s3.s3handler import S3TransferHandlerFactory\nfrom awscli.customizations.s3.utils import find_bucket_key, AppendFilter, \\\n    find_dest_path_comp_key, human_readable_size, \\\n    RequestParamsMapper, split_s3_bucket_key, block_unsupported_resources, \\\n    S3PathResolver\nfrom awscli.customizations.utils import uni_print\nfrom awscli.customizations.s3.syncstrategy.base import MissingFileSync, \\\n    SizeAndLastModifiedSync, NeverSync\nfrom awscli.customizations.s3 import transferconfig\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nRECURSIVE = {'name': 'recursive', 'action': 'store_true', 'dest': 'dir_op',\n             'help_text': (\n                 \"Command is performed on all files or objects \"\n                 \"under the specified directory or prefix.\")}\n\n\nHUMAN_READABLE = {'name': 'human-readable', 'action': 'store_true',\n                  'help_text': \"Displays file sizes in human readable format.\"}\n\n\nSUMMARIZE = {'name': 'summarize', 'action': 'store_true',\n             'help_text': (\n                 \"Displays summary information \"\n                 \"(number of objects, total size).\")}\n\n\nDRYRUN = {'name': 'dryrun', 'action': 'store_true',\n          'help_text': (\n              \"Displays the operations that would be performed using the \"\n              \"specified command without actually running them.\")}\n\n\nQUIET = {'name': 'quiet', 'action': 'store_true',\n         'help_text': (\n             \"Does not display the operations performed from the specified \"\n             \"command.\")}\n\n\nFORCE = {'name': 'force', 'action': 'store_true',\n         'help_text': (\n             \"Deletes all objects in the bucket including the bucket itself. \"\n             \"Note that versioned objects will not be deleted in this \"\n             \"process which would cause the bucket deletion to fail because \"\n             \"the bucket would not be empty. To delete versioned \"\n             \"objects use the ``s3api delete-object`` command with \"\n             \"the ``--version-id`` parameter.\")}\n\n\nFOLLOW_SYMLINKS = {'name': 'follow-symlinks', 'action': 'store_true',\n                   'default': True, 'group_name': 'follow_symlinks',\n                   'help_text': (\n                       \"Symbolic links are followed \"\n                       \"only when uploading to S3 from the local filesystem. \"\n                       \"Note that S3 does not support symbolic links, so the \"\n                       \"contents of the link target are uploaded under the \"\n                       \"name of the link. When neither ``--follow-symlinks`` \"\n                       \"nor ``--no-follow-symlinks`` is specified, the default \"\n                       \"is to follow symlinks.\")}\n\n\nNO_FOLLOW_SYMLINKS = {'name': 'no-follow-symlinks', 'action': 'store_false',\n                      'dest': 'follow_symlinks', 'default': True,\n                      'group_name': 'follow_symlinks'}\n\n\nNO_GUESS_MIME_TYPE = {'name': 'no-guess-mime-type', 'action': 'store_false',\n                      'dest': 'guess_mime_type', 'default': True,\n                      'help_text': (\n                          \"Do not try to guess the mime type for \"\n                          \"uploaded files.  By default the mime type of a \"\n                          \"file is guessed when it is uploaded.\")}\n\n\nCONTENT_TYPE = {'name': 'content-type',\n                'help_text': (\n                    \"Specify an explicit content type for this operation.  \"\n                    \"This value overrides any guessed mime types.\")}\n\n\nEXCLUDE = {'name': 'exclude', 'action': AppendFilter, 'nargs': 1,\n           'dest': 'filters',\n           'help_text': (\n               \"Exclude all files or objects from the command that matches \"\n               \"the specified pattern.\")}\n\n\nINCLUDE = {'name': 'include', 'action': AppendFilter, 'nargs': 1,\n           'dest': 'filters',\n           'help_text': (\n               \"Don't exclude files or objects \"\n               \"in the command that match the specified pattern. \"\n               'See <a href=\"http://docs.aws.amazon.com/cli/latest/reference'\n               '/s3/index.html#use-of-exclude-and-include-filters\">Use of '\n               'Exclude and Include Filters</a> for details.')}\n\n\nACL = {'name': 'acl',\n       'choices': ['private', 'public-read', 'public-read-write',\n                   'authenticated-read', 'aws-exec-read', 'bucket-owner-read',\n                   'bucket-owner-full-control', 'log-delivery-write'],\n       'help_text': (\n           \"Sets the ACL for the object when the command is \"\n           \"performed.  If you use this parameter you must have the \"\n           '\"s3:PutObjectAcl\" permission included in the list of actions '\n           \"for your IAM policy. \"\n           \"Only accepts values of ``private``, ``public-read``, \"\n           \"``public-read-write``, ``authenticated-read``, ``aws-exec-read``, \"\n           \"``bucket-owner-read``, ``bucket-owner-full-control`` and \"\n           \"``log-delivery-write``. \"\n           'See <a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/'\n           'acl-overview.html#canned-acl\">Canned ACL</a> for details')}\n\n\nGRANTS = {\n    'name': 'grants', 'nargs': '+',\n    'help_text': (\n        '<p>Grant specific permissions to individual users or groups. You '\n        'can supply a list of grants of the form</p><codeblock>--grants '\n        'Permission=Grantee_Type=Grantee_ID [Permission=Grantee_Type='\n        'Grantee_ID ...]</codeblock>To specify the same permission type '\n        'for multiple '\n        'grantees, specify the permission as such as <codeblock>--grants '\n        'Permission=Grantee_Type=Grantee_ID,Grantee_Type=Grantee_ID,...'\n        '</codeblock>Each value contains the following elements:'\n        '<ul><li><code>Permission</code> - Specifies '\n        'the granted permissions, and can be set to read, readacl, '\n        'writeacl, or full.</li><li><code>Grantee_Type</code> - '\n        'Specifies how the grantee is to be identified, and can be set '\n        'to uri or id.</li><li><code>Grantee_ID</code> - '\n        'Specifies the grantee based on Grantee_Type. The '\n        '<code>Grantee_ID</code> value can be one of:<ul><li><b>uri</b> '\n        '- The group\\'s URI. For more information, see '\n        '<a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/'\n        'ACLOverview.html#SpecifyingGrantee\">'\n        'Who Is a Grantee?</a></li>'\n        '<li><b>id</b> - The account\\'s canonical ID</li></ul>'\n        '</li></ul>'\n        'For more information on Amazon S3 access control, see '\n        '<a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/'\n        'UsingAuthAccess.html\">Access Control</a>')}\n\n\nSSE = {\n    'name': 'sse', 'nargs': '?', 'const': 'AES256',\n    'choices': ['AES256', 'aws:kms'],\n    'help_text': (\n        'Specifies server-side encryption of the object in S3. '\n        'Valid values are ``AES256`` and ``aws:kms``. If the parameter is '\n        'specified but no value is provided, ``AES256`` is used.'\n    )\n}\n\n\nSSE_C = {\n    'name': 'sse-c', 'nargs': '?', 'const': 'AES256', 'choices': ['AES256'],\n    'help_text': (\n        'Specifies server-side encryption using customer provided keys '\n        'of the the object in S3. ``AES256`` is the only valid value. '\n        'If the parameter is specified but no value is provided, '\n        '``AES256`` is used. If you provide this value, ``--sse-c-key`` '\n        'must be specified as well.'\n    )\n}\n\n\nSSE_C_KEY = {\n    'name': 'sse-c-key', 'cli_type_name': 'blob',\n    'help_text': (\n        'The customer-provided encryption key to use to server-side '\n        'encrypt the object in S3. If you provide this value, '\n        '``--sse-c`` must be specified as well. The key provided should '\n        '**not** be base64 encoded.'\n    )\n}\n\n\nSSE_KMS_KEY_ID = {\n    'name': 'sse-kms-key-id',\n    'help_text': (\n        'The customer-managed AWS Key Management Service (KMS) key ID that '\n        'should be used to server-side encrypt the object in S3. You should '\n        'only provide this parameter if you are using a customer managed '\n        'customer master key (CMK) and not the AWS managed KMS CMK.'\n    )\n}\n\n\nSSE_C_COPY_SOURCE = {\n    'name': 'sse-c-copy-source', 'nargs': '?',\n    'const': 'AES256', 'choices': ['AES256'],\n    'help_text': (\n        'This parameter should only be specified when copying an S3 object '\n        'that was encrypted server-side with a customer-provided '\n        'key. It specifies the algorithm to use when decrypting the source '\n        'object. ``AES256`` is the only valid '\n        'value. If the parameter is specified but no value is provided, '\n        '``AES256`` is used. If you provide this value, '\n        '``--sse-c-copy-source-key`` must be specified as well. '\n    )\n}\n\n\nSSE_C_COPY_SOURCE_KEY = {\n    'name': 'sse-c-copy-source-key', 'cli_type_name': 'blob',\n    'help_text': (\n        'This parameter should only be specified when copying an S3 object '\n        'that was encrypted server-side with a customer-provided '\n        'key. Specifies the customer-provided encryption key for Amazon S3 '\n        'to use to decrypt the source object. The encryption key provided '\n        'must be one that was used when the source object was created. '\n        'If you provide this value, ``--sse-c-copy-source`` be specified as '\n        'well. The key provided should **not** be base64 encoded.'\n    )\n}\n\n\nSTORAGE_CLASS = {'name': 'storage-class',\n                 'choices': ['STANDARD', 'REDUCED_REDUNDANCY', 'STANDARD_IA',\n                             'ONEZONE_IA', 'INTELLIGENT_TIERING', 'GLACIER',\n                             'DEEP_ARCHIVE', 'GLACIER_IR'],\n                 'help_text': (\n                     \"The type of storage to use for the object. \"\n                     \"Valid choices are: STANDARD | REDUCED_REDUNDANCY \"\n                     \"| STANDARD_IA | ONEZONE_IA | INTELLIGENT_TIERING \"\n                     \"| GLACIER | DEEP_ARCHIVE | GLACIER_IR. \"\n                     \"Defaults to 'STANDARD'\")}\n\n\nWEBSITE_REDIRECT = {'name': 'website-redirect',\n                    'help_text': (\n                        \"If the bucket is configured as a website, \"\n                        \"redirects requests for this object to another object \"\n                        \"in the same bucket or to an external URL. Amazon S3 \"\n                        \"stores the value of this header in the object \"\n                        \"metadata.\")}\n\n\nCACHE_CONTROL = {'name': 'cache-control',\n                 'help_text': (\n                     \"Specifies caching behavior along the \"\n                     \"request/reply chain.\")}\n\n\nCONTENT_DISPOSITION = {'name': 'content-disposition',\n                       'help_text': (\n                           \"Specifies presentational information \"\n                           \"for the object.\")}\n\n\nCONTENT_ENCODING = {'name': 'content-encoding',\n                    'help_text': (\n                        \"Specifies what content encodings have been \"\n                        \"applied to the object and thus what decoding \"\n                        \"mechanisms must be applied to obtain the media-type \"\n                        \"referenced by the Content-Type header field.\")}\n\n\nCONTENT_LANGUAGE = {'name': 'content-language',\n                    'help_text': (\"The language the content is in.\")}\n\n\nSOURCE_REGION = {'name': 'source-region',\n                 'help_text': (\n                     \"When transferring objects from an s3 bucket to an s3 \"\n                     \"bucket, this specifies the region of the source bucket.\"\n                     \" Note the region specified by ``--region`` or through \"\n                     \"configuration of the CLI refers to the region of the \"\n                     \"destination bucket.  If ``--source-region`` is not \"\n                     \"specified the region of the source will be the same \"\n                     \"as the region of the destination bucket.\")}\n\n\nEXPIRES = {\n    'name': 'expires',\n    'help_text': (\n        \"The date and time at which the object is no longer cacheable.\")\n}\n\n\nMETADATA = {\n    'name': 'metadata', 'cli_type_name': 'map',\n    'schema': {\n        'type': 'map',\n        'key': {'type': 'string'},\n        'value': {'type': 'string'}\n    },\n    'help_text': (\n        \"A map of metadata to store with the objects in S3. This will be \"\n        \"applied to every object which is part of this request. In a sync, this \"\n        \"means that files which haven't changed won't receive the new metadata. \"\n        \"When copying between two s3 locations, the metadata-directive \"\n        \"argument will default to 'REPLACE' unless otherwise specified.\"\n    )\n}\n\n\nMETADATA_DIRECTIVE = {\n    'name': 'metadata-directive', 'choices': ['COPY', 'REPLACE'],\n    'help_text': (\n        'Specifies whether the metadata is copied from the source object '\n        'or replaced with metadata provided when copying S3 objects. '\n        'Note that if the object is copied over in parts, the source '\n        'object\\'s metadata will not be copied over, no matter the value for '\n        '``--metadata-directive``, and instead the desired metadata values '\n        'must be specified as parameters on the command line. '\n        'Valid values are ``COPY`` and ``REPLACE``. If this parameter is not '\n        'specified, ``COPY`` will be used by default. If ``REPLACE`` is used, '\n        'the copied object will only have the metadata values that were'\n        ' specified by the CLI command. Note that if you are '\n        'using any of the following parameters: ``--content-type``, '\n        '``content-language``, ``--content-encoding``, '\n        '``--content-disposition``, ``--cache-control``, or ``--expires``, you '\n        'will need to specify ``--metadata-directive REPLACE`` for '\n        'non-multipart copies if you want the copied objects to have the '\n        'specified metadata values.')\n}\n\n\nINDEX_DOCUMENT = {'name': 'index-document',\n                  'help_text': (\n                      'A suffix that is appended to a request that is for '\n                      'a directory on the website endpoint (e.g. if the '\n                      'suffix is index.html and you make a request to '\n                      'samplebucket/images/ the data that is returned '\n                      'will be for the object with the key name '\n                      'images/index.html) The suffix must not be empty and '\n                      'must not include a slash character.')}\n\n\nERROR_DOCUMENT = {'name': 'error-document',\n                  'help_text': (\n                      'The object key name to use when '\n                      'a 4XX class error occurs.')}\n\n\nONLY_SHOW_ERRORS = {'name': 'only-show-errors', 'action': 'store_true',\n                    'help_text': (\n                        'Only errors and warnings are displayed. All other '\n                        'output is suppressed.')}\n\n\nNO_PROGRESS = {'name': 'no-progress',\n               'action': 'store_false',\n               'dest': 'progress',\n               'help_text': (\n                   'File transfer progress is not displayed. This flag '\n                   'is only applied when the quiet and only-show-errors '\n                   'flags are not provided.')}\n\n\nEXPECTED_SIZE = {'name': 'expected-size',\n                 'help_text': (\n                     'This argument specifies the expected size of a stream '\n                     'in terms of bytes. Note that this argument is needed '\n                     'only when a stream is being uploaded to s3 and the size '\n                     'is larger than 50GB.  Failure to include this argument '\n                     'under these conditions may result in a failed upload '\n                     'due to too many parts in upload.')}\n\n\nPAGE_SIZE = {'name': 'page-size', 'cli_type_name': 'integer',\n             'help_text': (\n                 'The number of results to return in each response to a list '\n                 'operation. The default value is 1000 (the maximum allowed). '\n                 'Using a lower value may help if an operation times out.')}\n\n\nIGNORE_GLACIER_WARNINGS = {\n    'name': 'ignore-glacier-warnings', 'action': 'store_true',\n    'help_text': (\n        'Turns off glacier warnings. Warnings about an operation that cannot '\n        'be performed because it involves copying, downloading, or moving '\n        'a glacier object will no longer be printed to standard error and '\n        'will no longer cause the return code of the command to be ``2``.'\n    )\n}\n\n\nFORCE_GLACIER_TRANSFER = {\n    'name': 'force-glacier-transfer', 'action': 'store_true',\n    'help_text': (\n        'Forces a transfer request on all Glacier objects in a sync or '\n        'recursive copy.'\n    )\n}\n\nREQUEST_PAYER = {\n    'name': 'request-payer', 'choices': ['requester'],\n    'nargs': '?', 'const': 'requester',\n    'help_text': (\n        'Confirms that the requester knows that they will be charged '\n        'for the request. Bucket owners need not specify this parameter in '\n        'their requests. Documentation on downloading objects from requester '\n        'pays buckets can be found at '\n        'http://docs.aws.amazon.com/AmazonS3/latest/dev/'\n        'ObjectsinRequesterPaysBuckets.html'\n    )\n}\n\nVALIDATE_SAME_S3_PATHS = {\n    'name': 'validate-same-s3-paths', 'action': 'store_true',\n    'help_text': (\n        'Resolves the source and destination S3 URIs to their '\n        'underlying buckets and verifies that the file or object '\n        'is not being moved onto itself. If you are using any type '\n        'of access point ARNs or access point aliases in your S3 URIs, '\n        'we strongly recommended using this parameter to help prevent '\n        'accidental deletions of the source file or object. This '\n        'parameter resolves the underlying buckets of S3 access point '\n        'ARNs and aliases, S3 on Outposts access point ARNs, and '\n        'Multi-Region Access Point ARNs. S3 on Outposts access point '\n        'aliases are not supported. Instead of using this parameter, '\n        'you can set the environment variable '\n        '``AWS_CLI_S3_MV_VALIDATE_SAME_S3_PATHS`` to ``true``. '\n        'NOTE: Path validation requires making additional API calls. '\n        'Future updates to this path-validation mechanism might change '\n        'which API calls are made.'\n    )\n}\n\nTRANSFER_ARGS = [DRYRUN, QUIET, INCLUDE, EXCLUDE, ACL,\n                 FOLLOW_SYMLINKS, NO_FOLLOW_SYMLINKS, NO_GUESS_MIME_TYPE,\n                 SSE, SSE_C, SSE_C_KEY, SSE_KMS_KEY_ID, SSE_C_COPY_SOURCE,\n                 SSE_C_COPY_SOURCE_KEY, STORAGE_CLASS, GRANTS,\n                 WEBSITE_REDIRECT, CONTENT_TYPE, CACHE_CONTROL,\n                 CONTENT_DISPOSITION, CONTENT_ENCODING, CONTENT_LANGUAGE,\n                 EXPIRES, SOURCE_REGION, ONLY_SHOW_ERRORS, NO_PROGRESS,\n                 PAGE_SIZE, IGNORE_GLACIER_WARNINGS, FORCE_GLACIER_TRANSFER,\n                 REQUEST_PAYER]\n\n\ndef get_client(session, region, endpoint_url, verify, config=None):\n    return session.create_client('s3', region_name=region,\n                                 endpoint_url=endpoint_url, verify=verify,\n                                 config=config)\n\n\nclass S3Command(BasicCommand):\n    def _run_main(self, parsed_args, parsed_globals):\n        self.client = get_client(self._session, parsed_globals.region,\n                                 parsed_globals.endpoint_url,\n                                 parsed_globals.verify_ssl)\n\n\nclass ListCommand(S3Command):\n    NAME = 'ls'\n    DESCRIPTION = (\"List S3 objects and common prefixes under a prefix or \"\n                   \"all S3 buckets. Note that the --output and --no-paginate \"\n                   \"arguments are ignored for this command.\")\n    USAGE = \"<S3Uri> or NONE\"\n    ARG_TABLE = [{'name': 'paths', 'nargs': '?', 'default': 's3://',\n                  'positional_arg': True, 'synopsis': USAGE}, RECURSIVE,\n                 PAGE_SIZE, HUMAN_READABLE, SUMMARIZE, REQUEST_PAYER]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        super(ListCommand, self)._run_main(parsed_args, parsed_globals)\n        self._empty_result = False\n        self._at_first_page = True\n        self._size_accumulator = 0\n        self._total_objects = 0\n        self._human_readable = parsed_args.human_readable\n        path = parsed_args.paths\n        if path.startswith('s3://'):\n            path = path[5:]\n        bucket, key = find_bucket_key(path)\n        if not bucket:\n            self._list_all_buckets()\n        elif parsed_args.dir_op:\n            # Then --recursive was specified.\n            self._list_all_objects_recursive(\n                bucket, key, parsed_args.page_size, parsed_args.request_payer)\n        else:\n            self._list_all_objects(\n                bucket, key, parsed_args.page_size, parsed_args.request_payer)\n        if parsed_args.summarize:\n            self._print_summary()\n        if key:\n            # User specified a key to look for. We should return an rc of one\n            # if there are no matching keys and/or prefixes or return an rc\n            # of zero if there are matching keys or prefixes.\n            return self._check_no_objects()\n        else:\n            # This covers the case when user is trying to list all of of\n            # the buckets or is trying to list the objects of a bucket\n            # (without specifying a key). For both situations, a rc of 0\n            # should be returned because applicable errors are supplied by\n            # the server (i.e. bucket not existing). These errors will be\n            # thrown before reaching the automatic return of rc of zero.\n            return 0\n\n    def _list_all_objects(self, bucket, key, page_size=None,\n                          request_payer=None):\n        paginator = self.client.get_paginator('list_objects_v2')\n        paging_args = {\n            'Bucket': bucket, 'Prefix': key, 'Delimiter': '/',\n            'PaginationConfig': {'PageSize': page_size}\n        }\n        if request_payer is not None:\n            paging_args['RequestPayer'] = request_payer\n        iterator = paginator.paginate(**paging_args)\n        for response_data in iterator:\n            self._display_page(response_data)\n\n    def _display_page(self, response_data, use_basename=True):\n        common_prefixes = response_data.get('CommonPrefixes', [])\n        contents = response_data.get('Contents', [])\n        if not contents and not common_prefixes:\n            self._empty_result = True\n            return\n        for common_prefix in common_prefixes:\n            prefix_components = common_prefix['Prefix'].split('/')\n            prefix = prefix_components[-2]\n            pre_string = \"PRE\".rjust(30, \" \")\n            print_str = pre_string + ' ' + prefix + '/\\n'\n            uni_print(print_str)\n        for content in contents:\n            last_mod_str = self._make_last_mod_str(content['LastModified'])\n            self._size_accumulator += int(content['Size'])\n            self._total_objects += 1\n            size_str = self._make_size_str(content['Size'])\n            if use_basename:\n                filename_components = content['Key'].split('/')\n                filename = filename_components[-1]\n            else:\n                filename = content['Key']\n            print_str = last_mod_str + ' ' + size_str + ' ' + \\\n                filename + '\\n'\n            uni_print(print_str)\n        self._at_first_page = False\n\n    def _list_all_buckets(self):\n        response_data = self.client.list_buckets()\n        buckets = response_data['Buckets']\n        for bucket in buckets:\n            last_mod_str = self._make_last_mod_str(bucket['CreationDate'])\n            print_str = last_mod_str + ' ' + bucket['Name'] + '\\n'\n            uni_print(print_str)\n\n    def _list_all_objects_recursive(self, bucket, key, page_size=None,\n                                    request_payer=None):\n        paginator = self.client.get_paginator('list_objects_v2')\n        paging_args = {\n            'Bucket': bucket, 'Prefix': key,\n            'PaginationConfig': {'PageSize': page_size}\n        }\n        if request_payer is not None:\n            paging_args['RequestPayer'] = request_payer\n        iterator = paginator.paginate(**paging_args)\n        for response_data in iterator:\n            self._display_page(response_data, use_basename=False)\n\n    def _check_no_objects(self):\n        if self._empty_result and self._at_first_page:\n            # Nothing was returned in the first page of results when listing\n            # the objects.\n            return 1\n        return 0\n\n    def _make_last_mod_str(self, last_mod):\n        \"\"\"\n        This function creates the last modified time string whenever objects\n        or buckets are being listed\n        \"\"\"\n        last_mod = parse(last_mod)\n        last_mod = last_mod.astimezone(tzlocal())\n        last_mod_tup = (str(last_mod.year), str(last_mod.month).zfill(2),\n                        str(last_mod.day).zfill(2),\n                        str(last_mod.hour).zfill(2),\n                        str(last_mod.minute).zfill(2),\n                        str(last_mod.second).zfill(2))\n        last_mod_str = \"%s-%s-%s %s:%s:%s\" % last_mod_tup\n        return last_mod_str.ljust(19, ' ')\n\n    def _make_size_str(self, size):\n        \"\"\"\n        This function creates the size string when objects are being listed.\n        \"\"\"\n        if self._human_readable:\n            size_str = human_readable_size(size)\n        else:\n            size_str = str(size)\n        return size_str.rjust(10, ' ')\n\n    def _print_summary(self):\n        \"\"\"\n        This function prints a summary of total objects and total bytes\n        \"\"\"\n        print_str = str(self._total_objects)\n        uni_print(\"\\nTotal Objects: \".rjust(15, ' ') + print_str + \"\\n\")\n        if self._human_readable:\n            print_str = human_readable_size(self._size_accumulator)\n        else:\n            print_str = str(self._size_accumulator)\n        uni_print(\"Total Size: \".rjust(15, ' ') + print_str + \"\\n\")\n\n\nclass WebsiteCommand(S3Command):\n    NAME = 'website'\n    DESCRIPTION = 'Set the website configuration for a bucket.'\n    USAGE = '<S3Uri>'\n    ARG_TABLE = [{'name': 'paths', 'nargs': 1, 'positional_arg': True,\n                  'synopsis': USAGE}, INDEX_DOCUMENT, ERROR_DOCUMENT]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        super(WebsiteCommand, self)._run_main(parsed_args, parsed_globals)\n        bucket = self._get_bucket_name(parsed_args.paths[0])\n        website_configuration = self._build_website_configuration(parsed_args)\n        self.client.put_bucket_website(\n            Bucket=bucket, WebsiteConfiguration=website_configuration)\n        return 0\n\n    def _build_website_configuration(self, parsed_args):\n        website_config = {}\n        if parsed_args.index_document is not None:\n            website_config['IndexDocument'] = \\\n                {'Suffix': parsed_args.index_document}\n        if parsed_args.error_document is not None:\n            website_config['ErrorDocument'] = \\\n                {'Key': parsed_args.error_document}\n        return website_config\n\n    def _get_bucket_name(self, path):\n        # We support either:\n        # s3://bucketname\n        # bucketname\n        #\n        # We also strip off the trailing slash if a user\n        # accidentally appends a slash.\n        if path.startswith('s3://'):\n            path = path[5:]\n        if path.endswith('/'):\n            path = path[:-1]\n        block_unsupported_resources(path)\n        return path\n\n\nclass PresignCommand(S3Command):\n    NAME = 'presign'\n    DESCRIPTION = (\n        \"Generate a pre-signed URL for an Amazon S3 object. This allows \"\n        \"anyone who receives the pre-signed URL to retrieve the S3 object \"\n        \"with an HTTP GET request. For sigv4 requests the region needs to be \"\n        \"configured explicitly.\"\n    )\n    USAGE = \"<S3Uri>\"\n    ARG_TABLE = [{'name': 'path',\n                  'positional_arg': True, 'synopsis': USAGE},\n                 {'name': 'expires-in', 'default': 3600,\n                  'cli_type_name': 'integer',\n                  'help_text': (\n                      'Number of seconds until the pre-signed '\n                      'URL expires.  Default is 3600 seconds.')}]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        super(PresignCommand, self)._run_main(parsed_args, parsed_globals)\n        path = parsed_args.path\n        if path.startswith('s3://'):\n            path = path[5:]\n        bucket, key = find_bucket_key(path)\n        url = self.client.generate_presigned_url(\n            'get_object',\n            {'Bucket': bucket, 'Key': key},\n            ExpiresIn=parsed_args.expires_in\n        )\n        uni_print(url)\n        uni_print('\\n')\n        return 0\n\n\nclass S3TransferCommand(S3Command):\n    def _run_main(self, parsed_args, parsed_globals):\n        super(S3TransferCommand, self)._run_main(parsed_args, parsed_globals)\n        self._convert_path_args(parsed_args)\n        params = self._build_call_parameters(parsed_args, {})\n        cmd_params = CommandParameters(self.NAME, params,\n                                       self.USAGE,\n                                       self._session,\n                                       parsed_globals)\n        cmd_params.add_region(parsed_globals)\n        cmd_params.add_endpoint_url(parsed_globals)\n        cmd_params.add_verify_ssl(parsed_globals)\n        cmd_params.add_page_size(parsed_args)\n        cmd_params.add_paths(parsed_args.paths)\n\n        runtime_config = transferconfig.RuntimeConfig().build_config(\n            **self._session.get_scoped_config().get('s3', {}))\n        cmd = CommandArchitecture(self._session, self.NAME,\n                                  cmd_params.parameters,\n                                  runtime_config)\n        cmd.set_clients()\n        cmd.create_instructions()\n        return cmd.run()\n\n    def _build_call_parameters(self, args, command_params):\n        \"\"\"\n        This takes all of the commands in the name space and puts them\n        into a dictionary\n        \"\"\"\n        for name, value in vars(args).items():\n            command_params[name] = value\n        return command_params\n\n    def _convert_path_args(self, parsed_args):\n        if not isinstance(parsed_args.paths, list):\n            parsed_args.paths = [parsed_args.paths]\n        for i in range(len(parsed_args.paths)):\n            path = parsed_args.paths[i]\n            if isinstance(path, bytes):\n                dec_path = path.decode(sys.getfilesystemencoding())\n                enc_path = dec_path.encode('utf-8')\n                new_path = enc_path.decode('utf-8')\n                parsed_args.paths[i] = new_path\n\n\nclass CpCommand(S3TransferCommand):\n    NAME = 'cp'\n    DESCRIPTION = \"Copies a local file or S3 object to another location \" \\\n                  \"locally or in S3.\"\n    USAGE = \"<LocalPath> <S3Uri> or <S3Uri> <LocalPath> \" \\\n            \"or <S3Uri> <S3Uri>\"\n    ARG_TABLE = [{'name': 'paths', 'nargs': 2, 'positional_arg': True,\n                  'synopsis': USAGE}] + TRANSFER_ARGS + \\\n                [METADATA, METADATA_DIRECTIVE, EXPECTED_SIZE, RECURSIVE]\n\n\nclass MvCommand(S3TransferCommand):\n    NAME = 'mv'\n    DESCRIPTION = BasicCommand.FROM_FILE('s3', 'mv', '_description.rst')\n    USAGE = \"<LocalPath> <S3Uri> or <S3Uri> <LocalPath> \" \\\n            \"or <S3Uri> <S3Uri>\"\n    ARG_TABLE = [{'name': 'paths', 'nargs': 2, 'positional_arg': True,\n                  'synopsis': USAGE}] + TRANSFER_ARGS +\\\n                [METADATA, METADATA_DIRECTIVE, RECURSIVE, VALIDATE_SAME_S3_PATHS]\n\n\nclass RmCommand(S3TransferCommand):\n    NAME = 'rm'\n    DESCRIPTION = \"Deletes an S3 object.\"\n    USAGE = \"<S3Uri>\"\n    ARG_TABLE = [{'name': 'paths', 'nargs': 1, 'positional_arg': True,\n                  'synopsis': USAGE}, DRYRUN, QUIET, RECURSIVE, REQUEST_PAYER,\n                 INCLUDE, EXCLUDE, ONLY_SHOW_ERRORS, PAGE_SIZE]\n\n\nclass SyncCommand(S3TransferCommand):\n    NAME = 'sync'\n    DESCRIPTION = \"Syncs directories and S3 prefixes. Recursively copies \" \\\n                  \"new and updated files from the source directory to \" \\\n                  \"the destination. Only creates folders in the destination \" \\\n                  \"if they contain one or more files.\"\n    USAGE = \"<LocalPath> <S3Uri> or <S3Uri> \" \\\n            \"<LocalPath> or <S3Uri> <S3Uri>\"\n    ARG_TABLE = [{'name': 'paths', 'nargs': 2, 'positional_arg': True,\n                  'synopsis': USAGE}] + TRANSFER_ARGS + \\\n                [METADATA, METADATA_DIRECTIVE]\n\n\nclass MbCommand(S3Command):\n    NAME = 'mb'\n    DESCRIPTION = \"Creates an S3 bucket.\"\n    USAGE = \"<S3Uri>\"\n    ARG_TABLE = [{'name': 'path', 'positional_arg': True, 'synopsis': USAGE}]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        super(MbCommand, self)._run_main(parsed_args, parsed_globals)\n\n        if not parsed_args.path.startswith('s3://'):\n            raise TypeError(\"%s\\nError: Invalid argument type\" % self.USAGE)\n        bucket, _ = split_s3_bucket_key(parsed_args.path)\n\n        bucket_config = {'LocationConstraint': self.client.meta.region_name}\n        params = {'Bucket': bucket}\n        if self.client.meta.region_name != 'us-east-1':\n            params['CreateBucketConfiguration'] = bucket_config\n\n        # TODO: Consolidate how we handle return codes and errors\n        try:\n            self.client.create_bucket(**params)\n            uni_print(\"make_bucket: %s\\n\" % bucket)\n            return 0\n        except Exception as e:\n            uni_print(\n                \"make_bucket failed: %s %s\\n\" % (parsed_args.path, e),\n                sys.stderr\n            )\n            return 1\n\n\nclass RbCommand(S3Command):\n    NAME = 'rb'\n    DESCRIPTION = (\n        \"Deletes an empty S3 bucket. A bucket must be completely empty \"\n        \"of objects and versioned objects before it can be deleted. \"\n        \"However, the ``--force`` parameter can be used to delete \"\n        \"the non-versioned objects in the bucket before the bucket is \"\n        \"deleted.\"\n    )\n    USAGE = \"<S3Uri>\"\n    ARG_TABLE = [{'name': 'path', 'positional_arg': True,\n                  'synopsis': USAGE}, FORCE]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        super(RbCommand, self)._run_main(parsed_args, parsed_globals)\n\n        if not parsed_args.path.startswith('s3://'):\n            raise TypeError(\"%s\\nError: Invalid argument type\" % self.USAGE)\n        bucket, key = split_s3_bucket_key(parsed_args.path)\n\n        if key:\n            raise ValueError('Please specify a valid bucket name only.'\n                             ' E.g. s3://%s' % bucket)\n\n        if parsed_args.force:\n            self._force(parsed_args.path, parsed_globals)\n\n        try:\n            self.client.delete_bucket(Bucket=bucket)\n            uni_print(\"remove_bucket: %s\\n\" % bucket)\n            return 0\n        except Exception as e:\n            uni_print(\n                \"remove_bucket failed: %s %s\\n\" % (parsed_args.path, e),\n                sys.stderr\n            )\n            return 1\n\n    def _force(self, path, parsed_globals):\n        \"\"\"Calls rm --recursive on the given path.\"\"\"\n        rm = RmCommand(self._session)\n        rc = rm([path, '--recursive'], parsed_globals)\n        if rc != 0:\n            raise RuntimeError(\n                \"remove_bucket failed: Unable to delete all objects in the \"\n                \"bucket, bucket will not be deleted.\")\n\n\nclass CommandArchitecture(object):\n    \"\"\"\n    This class drives the actual command.  A command is performed in two\n    steps.  First a list of instructions is generated.  This list of\n    instructions identifies which type of components are required based on the\n    name of the command and the parameters passed to the command line.  After\n    the instructions are generated the second step involves using the\n    list of instructions to wire together an assortment of generators to\n    perform the command.\n    \"\"\"\n    def __init__(self, session, cmd, parameters, runtime_config=None):\n        self.session = session\n        self.cmd = cmd\n        self.parameters = parameters\n        self.instructions = []\n        self._runtime_config = runtime_config\n        self._endpoint = None\n        self._source_endpoint = None\n        self._client = None\n        self._source_client = None\n\n    def set_clients(self):\n        client_config = None\n        if self.parameters.get('sse') == 'aws:kms':\n            client_config = Config(signature_version='s3v4')\n        self._client = get_client(\n            self.session,\n            region=self.parameters['region'],\n            endpoint_url=self.parameters['endpoint_url'],\n            verify=self.parameters['verify_ssl'],\n            config=client_config\n        )\n        self._source_client = get_client(\n            self.session,\n            region=self.parameters['region'],\n            endpoint_url=self.parameters['endpoint_url'],\n            verify=self.parameters['verify_ssl'],\n            config=client_config\n        )\n        if self.parameters['source_region']:\n            if self.parameters['paths_type'] == 's3s3':\n                self._source_client = get_client(\n                    self.session,\n                    region=self.parameters['source_region'],\n                    endpoint_url=None,\n                    verify=self.parameters['verify_ssl'],\n                    config=client_config\n                )\n\n    def create_instructions(self):\n        \"\"\"\n        This function creates the instructions based on the command name and\n        extra parameters.  Note that all commands must have an s3_handler\n        instruction in the instructions and must be at the end of the\n        instruction list because it sends the request to S3 and does not\n        yield anything.\n        \"\"\"\n        if self.needs_filegenerator():\n            self.instructions.append('file_generator')\n            if self.parameters.get('filters'):\n                self.instructions.append('filters')\n            if self.cmd == 'sync':\n                self.instructions.append('comparator')\n            self.instructions.append('file_info_builder')\n        self.instructions.append('s3_handler')\n\n    def needs_filegenerator(self):\n        return not self.parameters['is_stream']\n\n    def choose_sync_strategies(self):\n        \"\"\"Determines the sync strategy for the command.\n\n        It defaults to the default sync strategies but a customizable sync\n        strategy can override the default strategy if it returns the instance\n        of its self when the event is emitted.\n        \"\"\"\n        sync_strategies = {}\n        # Set the default strategies.\n        sync_strategies['file_at_src_and_dest_sync_strategy'] = \\\n            SizeAndLastModifiedSync()\n        sync_strategies['file_not_at_dest_sync_strategy'] = MissingFileSync()\n        sync_strategies['file_not_at_src_sync_strategy'] = NeverSync()\n\n        # Determine what strategies to override if any.\n        responses = self.session.emit(\n            'choosing-s3-sync-strategy', params=self.parameters)\n        if responses is not None:\n            for response in responses:\n                override_sync_strategy = response[1]\n                if override_sync_strategy is not None:\n                    sync_type = override_sync_strategy.sync_type\n                    sync_type += '_sync_strategy'\n                    sync_strategies[sync_type] = override_sync_strategy\n\n        return sync_strategies\n\n    def run(self):\n        \"\"\"\n        This function wires together all of the generators and completes\n        the command.  First a dictionary is created that is indexed first by\n        the command name.  Then using the instruction, another dictionary\n        can be indexed to obtain the objects corresponding to the\n        particular instruction for that command.  To begin the wiring,\n        either a ``FileFormat`` or ``TaskInfo`` object, depending on the\n        command, is put into a list.  Then the function enters a while loop\n        that pops off an instruction.  It then determines the object needed\n        and calls the call function of the object using the list as the input.\n        Depending on the number of objects in the input list and the number\n        of components in the list corresponding to the instruction, the call\n        method of the component can be called two different ways.  If the\n        number of inputs is equal to the number of components a 1:1 mapping of\n        inputs to components is used when calling the call function.  If the\n        there are more inputs than components, then a 2:1 mapping of inputs to\n        components is used where the component call method takes two inputs\n        instead of one.  Whatever files are yielded from the call function\n        is appended to a list and used as the input for the next repetition\n        of the while loop until there are no more instructions.\n        \"\"\"\n        src = self.parameters['src']\n        dest = self.parameters['dest']\n        paths_type = self.parameters['paths_type']\n        files = FileFormat().format(src, dest, self.parameters)\n        rev_files = FileFormat().format(dest, src, self.parameters)\n\n        cmd_translation = {\n            'locals3': 'upload',\n            's3s3': 'copy',\n            's3local': 'download',\n            's3': 'delete'\n        }\n        result_queue = queue.Queue()\n        operation_name = cmd_translation[paths_type]\n\n        fgen_kwargs = {\n            'client': self._source_client, 'operation_name': operation_name,\n            'follow_symlinks': self.parameters['follow_symlinks'],\n            'page_size': self.parameters['page_size'],\n            'result_queue': result_queue,\n        }\n        rgen_kwargs = {\n            'client': self._client, 'operation_name': '',\n            'follow_symlinks': self.parameters['follow_symlinks'],\n            'page_size': self.parameters['page_size'],\n            'result_queue': result_queue,\n        }\n\n        fgen_request_parameters = \\\n            self._get_file_generator_request_parameters_skeleton()\n        self._map_request_payer_params(fgen_request_parameters)\n        self._map_sse_c_params(fgen_request_parameters, paths_type)\n        fgen_kwargs['request_parameters'] = fgen_request_parameters\n\n        rgen_request_parameters =  \\\n            self._get_file_generator_request_parameters_skeleton()\n        self._map_request_payer_params(rgen_request_parameters)\n        rgen_kwargs['request_parameters'] = rgen_request_parameters\n\n        file_generator = FileGenerator(**fgen_kwargs)\n        rev_generator = FileGenerator(**rgen_kwargs)\n        stream_dest_path, stream_compare_key = find_dest_path_comp_key(files)\n        stream_file_info = [FileInfo(src=files['src']['path'],\n                                     dest=stream_dest_path,\n                                     compare_key=stream_compare_key,\n                                     src_type=files['src']['type'],\n                                     dest_type=files['dest']['type'],\n                                     operation_name=operation_name,\n                                     client=self._client,\n                                     is_stream=True)]\n        file_info_builder = FileInfoBuilder(\n            self._client, self._source_client, self.parameters)\n\n        s3_transfer_handler = S3TransferHandlerFactory(\n            self.parameters, self._runtime_config)(\n                self._client, result_queue)\n\n        sync_strategies = self.choose_sync_strategies()\n\n        command_dict = {}\n        if self.cmd == 'sync':\n            command_dict = {'setup': [files, rev_files],\n                            'file_generator': [file_generator,\n                                               rev_generator],\n                            'filters': [create_filter(self.parameters),\n                                        create_filter(self.parameters)],\n                            'comparator': [Comparator(**sync_strategies)],\n                            'file_info_builder': [file_info_builder],\n                            's3_handler': [s3_transfer_handler]}\n        elif self.cmd == 'cp' and self.parameters['is_stream']:\n            command_dict = {'setup': [stream_file_info],\n                            's3_handler': [s3_transfer_handler]}\n        elif self.cmd == 'cp':\n            command_dict = {'setup': [files],\n                            'file_generator': [file_generator],\n                            'filters': [create_filter(self.parameters)],\n                            'file_info_builder': [file_info_builder],\n                            's3_handler': [s3_transfer_handler]}\n        elif self.cmd == 'rm':\n            command_dict = {'setup': [files],\n                            'file_generator': [file_generator],\n                            'filters': [create_filter(self.parameters)],\n                            'file_info_builder': [file_info_builder],\n                            's3_handler': [s3_transfer_handler]}\n        elif self.cmd == 'mv':\n            command_dict = {'setup': [files],\n                            'file_generator': [file_generator],\n                            'filters': [create_filter(self.parameters)],\n                            'file_info_builder': [file_info_builder],\n                            's3_handler': [s3_transfer_handler]}\n\n        files = command_dict['setup']\n        while self.instructions:\n            instruction = self.instructions.pop(0)\n            file_list = []\n            components = command_dict[instruction]\n            for i in range(len(components)):\n                if len(files) > len(components):\n                    file_list.append(components[i].call(*files))\n                else:\n                    file_list.append(components[i].call(files[i]))\n            files = file_list\n        # This is kinda quirky, but each call through the instructions\n        # will replaces the files attr with the return value of the\n        # file_list.  The very last call is a single list of\n        # [s3_handler], and the s3_handler returns the number of\n        # tasks failed and the number of tasks warned.\n        # This means that files[0] now contains a namedtuple with\n        # the number of failed tasks and the number of warned tasks.\n        # In terms of the RC, we're keeping it simple and saying\n        # that > 0 failed tasks will give a 1 RC and > 0 warned\n        # tasks will give a 2 RC.  Otherwise a RC of zero is returned.\n        rc = 0\n        if files[0].num_tasks_failed > 0:\n            rc = 1\n        elif files[0].num_tasks_warned > 0:\n            rc = 2\n        return rc\n\n    def _get_file_generator_request_parameters_skeleton(self):\n        return {\n            'HeadObject': {},\n            'ListObjects': {},\n            'ListObjectsV2': {}\n        }\n\n    def _map_request_payer_params(self, request_parameters):\n        RequestParamsMapper.map_head_object_params(\n            request_parameters['HeadObject'], {\n                'request_payer': self.parameters.get('request_payer')\n            }\n        )\n        RequestParamsMapper.map_list_objects_v2_params(\n            request_parameters['ListObjectsV2'], {\n                'request_payer': self.parameters.get('request_payer')\n            }\n        )\n\n    def _map_sse_c_params(self, request_parameters, paths_type):\n        # SSE-C may be needed for HeadObject for copies/downloads/deletes\n        # If the operation is s3 to s3, the FileGenerator should use the\n        # copy source key and algorithm. Otherwise, use the regular\n        # SSE-C key and algorithm. Note the reverse FileGenerator does\n        # not need any of these because it is used only for sync operations\n        # which only use ListObjects which does not require HeadObject.\n        RequestParamsMapper.map_head_object_params(\n            request_parameters['HeadObject'], self.parameters)\n        if paths_type == 's3s3':\n            RequestParamsMapper.map_head_object_params(\n                request_parameters['HeadObject'], {\n                    'sse_c': self.parameters.get('sse_c_copy_source'),\n                    'sse_c_key': self.parameters.get('sse_c_copy_source_key')\n                }\n            )\n\n\nclass CommandParameters(object):\n    \"\"\"\n    This class is used to do some initial error based on the\n    parameters and arguments passed to the command line.\n    \"\"\"\n    def __init__(self, cmd, parameters, usage,\n                 session=None, parsed_globals=None):\n        \"\"\"\n        Stores command name and parameters.  Ensures that the ``dir_op`` flag\n        is true if a certain command is being used.\n\n        :param cmd: The name of the command, e.g. \"rm\".\n        :param parameters: A dictionary of parameters.\n        :param usage: A usage string\n\n        \"\"\"\n        self.cmd = cmd\n        self.parameters = parameters\n        self.usage = usage\n        self._session = session\n        self._parsed_globals = parsed_globals\n        if 'dir_op' not in parameters:\n            self.parameters['dir_op'] = False\n        if 'follow_symlinks' not in parameters:\n            self.parameters['follow_symlinks'] = True\n        if 'source_region' not in parameters:\n            self.parameters['source_region'] = None\n        if self.cmd in ['sync', 'mb', 'rb']:\n            self.parameters['dir_op'] = True\n        if self.cmd == 'mv':\n            self.parameters['is_move'] = True\n        else:\n            self.parameters['is_move'] = False\n\n    def add_paths(self, paths):\n        \"\"\"\n        Reformats the parameters dictionary by including a key and\n        value for the source and the destination.  If a destination is\n        not used the destination is the same as the source to ensure\n        the destination always have some value.\n        \"\"\"\n        self.check_path_type(paths)\n        self._normalize_s3_trailing_slash(paths)\n        src_path = paths[0]\n        self.parameters['src'] = src_path\n        if len(paths) == 2:\n            self.parameters['dest'] = paths[1]\n        elif len(paths) == 1:\n            self.parameters['dest'] = paths[0]\n        self._validate_streaming_paths()\n        self._validate_path_args()\n        self._validate_sse_c_args()\n        self._validate_not_s3_express_bucket_for_sync()\n\n    def _validate_not_s3_express_bucket_for_sync(self):\n        if self.cmd == 'sync' and \\\n            (self._is_s3express_path(self.parameters['src']) or\n             self._is_s3express_path(self.parameters['dest'])):\n            raise ValueError(\n                \"Cannot use sync command with a directory bucket.\"\n            )\n\n    def _is_s3express_path(self, path):\n        if path.startswith(\"s3://\"):\n            bucket = split_s3_bucket_key(path)[0]\n            return is_s3express_bucket(bucket)\n        return False\n\n    def _validate_streaming_paths(self):\n        self.parameters['is_stream'] = False\n        if self.parameters['src'] == '-' or self.parameters['dest'] == '-':\n            if self.cmd != 'cp' or self.parameters.get('dir_op'):\n                raise ValueError(\n                    \"Streaming currently is only compatible with \"\n                    \"non-recursive cp commands\"\n                )\n            self.parameters['is_stream'] = True\n            self.parameters['dir_op'] = False\n            self.parameters['only_show_errors'] = True\n\n    def _validate_path_args(self):\n        # If we're using a mv command, you can't copy the object onto itself.\n        params = self.parameters\n        if self.cmd == 'mv' and params['paths_type']=='s3s3':\n            self._raise_if_mv_same_paths(params['src'], params['dest'])\n            if self._should_validate_same_underlying_s3_paths():\n                self._validate_same_underlying_s3_paths()\n            if self._should_emit_validate_s3_paths_warning():\n                self._emit_validate_s3_paths_warning()\n\n        # If the user provided local path does not exist, hard fail because\n        # we know that we will not be able to upload the file.\n        if 'locals3' == params['paths_type'] and not params['is_stream']:\n            if not os.path.exists(params['src']):\n                raise RuntimeError(\n                    'The user-provided path %s does not exist.' %\n                    params['src'])\n        # If the operation is downloading to a directory that does not exist,\n        # create the directories so no warnings are thrown during the syncing\n        # process.\n        elif 's3local' == params['paths_type'] and params['dir_op']:\n            if not os.path.exists(params['dest']):\n                os.makedirs(params['dest'])\n\n    def _same_path(self, src, dest):\n        if not self.parameters['paths_type'] == 's3s3':\n            return False\n        elif src == dest:\n            return True\n        elif dest.endswith('/'):\n            src_base = os.path.basename(src)\n            return src == os.path.join(dest, src_base)\n\n    def _same_key(self, src, dest):\n        _, src_key = split_s3_bucket_key(src)\n        _, dest_key = split_s3_bucket_key(dest)\n        return self._same_path(f'/{src_key}', f'/{dest_key}')\n\n    def _validate_same_s3_paths_enabled(self):\n        validate_env_var = ensure_boolean(\n            os.environ.get('AWS_CLI_S3_MV_VALIDATE_SAME_S3_PATHS'))\n        return (self.parameters.get('validate_same_s3_paths') or\n                validate_env_var)\n\n    def _should_emit_validate_s3_paths_warning(self):\n        is_same_key = self._same_key(\n            self.parameters['src'], self.parameters['dest'])\n        src_has_underlying_path = S3PathResolver.has_underlying_s3_path(\n            self.parameters['src'])\n        dest_has_underlying_path = S3PathResolver.has_underlying_s3_path(\n            self.parameters['dest'])\n        return (is_same_key and not self._validate_same_s3_paths_enabled() and\n                (src_has_underlying_path or dest_has_underlying_path))\n\n    def _emit_validate_s3_paths_warning(self):\n        msg = (\n            \"warning: Provided s3 paths may resolve to same underlying \"\n            \"s3 object(s) and result in deletion instead of being moved. \"\n            \"To resolve and validate underlying s3 paths are not the same, \"\n            \"specify the --validate-same-s3-paths flag or set the \"\n            \"AWS_CLI_S3_MV_VALIDATE_SAME_S3_PATHS environment variable to true. \"\n            \"To resolve s3 outposts access point path, the arn must be \"\n            \"used instead of the alias.\\n\"\n        )\n        uni_print(msg, sys.stderr)\n\n    def _should_validate_same_underlying_s3_paths(self):\n        is_same_key = self._same_key(\n            self.parameters['src'], self.parameters['dest'])\n        return is_same_key and self._validate_same_s3_paths_enabled()\n\n    def _validate_same_underlying_s3_paths(self):\n        src_paths = S3PathResolver.from_session(\n            self._session,\n            self.parameters.get('source_region', self._parsed_globals.region),\n            self._parsed_globals.verify_ssl\n        ).resolve_underlying_s3_paths(self.parameters['src'])\n        dest_paths = S3PathResolver.from_session(\n            self._session,\n            self._parsed_globals.region,\n            self._parsed_globals.verify_ssl\n        ).resolve_underlying_s3_paths(self.parameters['dest'])\n        for src_path in src_paths:\n            for dest_path in dest_paths:\n                self._raise_if_mv_same_paths(src_path, dest_path)\n\n    def _raise_if_mv_same_paths(self, src, dest):\n        if self._same_path(src, dest):\n            raise ValueError(\n                \"Cannot mv a file onto itself: \"\n                f\"{self.parameters['src']} - {self.parameters['dest']}\"\n            )\n\n    def _normalize_s3_trailing_slash(self, paths):\n        for i, path in enumerate(paths):\n            if path.startswith('s3://'):\n                bucket, key = find_bucket_key(path[5:])\n                if not key and not path.endswith('/'):\n                    # If only a bucket was specified, we need\n                    # to normalize the path and ensure it ends\n                    # with a '/', s3://bucket -> s3://bucket/\n                    path += '/'\n                    paths[i] = path\n\n    def check_path_type(self, paths):\n        \"\"\"\n        This initial check ensures that the path types for the specified\n        command is correct.\n        \"\"\"\n        template_type = {'s3s3': ['cp', 'sync', 'mv'],\n                         's3local': ['cp', 'sync', 'mv'],\n                         'locals3': ['cp', 'sync', 'mv'],\n                         's3': ['mb', 'rb', 'rm'],\n                         'local': [], 'locallocal': []}\n        paths_type = ''\n        usage = \"usage: aws s3 %s %s\" % (self.cmd,\n                                         self.usage)\n        for i in range(len(paths)):\n            if paths[i].startswith('s3://'):\n                paths_type = paths_type + 's3'\n            else:\n                paths_type = paths_type + 'local'\n        if self.cmd in template_type[paths_type]:\n            self.parameters['paths_type'] = paths_type\n        else:\n            raise TypeError(\"%s\\nError: Invalid argument type\" % usage)\n\n    def add_region(self, parsed_globals):\n        self.parameters['region'] = parsed_globals.region\n\n    def add_endpoint_url(self, parsed_globals):\n        \"\"\"\n        Adds endpoint_url to the parameters.\n        \"\"\"\n        if 'endpoint_url' in parsed_globals:\n            self.parameters['endpoint_url'] = getattr(parsed_globals,\n                                                      'endpoint_url')\n        else:\n            self.parameters['endpoint_url'] = None\n\n    def add_verify_ssl(self, parsed_globals):\n        self.parameters['verify_ssl'] = parsed_globals.verify_ssl\n\n    def add_page_size(self, parsed_args):\n        self.parameters['page_size'] = getattr(parsed_args, 'page_size', None)\n\n    def _validate_sse_c_args(self):\n        self._validate_sse_c_arg()\n        self._validate_sse_c_arg('sse_c_copy_source')\n        self._validate_sse_c_copy_source_for_paths()\n\n    def _validate_sse_c_arg(self, sse_c_type='sse_c'):\n        sse_c_key_type = sse_c_type + '_key'\n        sse_c_type_param = '--' + sse_c_type.replace('_', '-')\n        sse_c_key_type_param = '--' + sse_c_key_type.replace('_', '-')\n        if self.parameters.get(sse_c_type):\n            if not self.parameters.get(sse_c_key_type):\n                raise ValueError(\n                    'It %s is specified, %s must be specified '\n                    'as well.' % (sse_c_type_param, sse_c_key_type_param)\n                )\n        if self.parameters.get(sse_c_key_type):\n            if not self.parameters.get(sse_c_type):\n                raise ValueError(\n                    'It %s is specified, %s must be specified '\n                    'as well.' % (sse_c_key_type_param, sse_c_type_param)\n                )\n\n    def _validate_sse_c_copy_source_for_paths(self):\n        if self.parameters.get('sse_c_copy_source'):\n            if self.parameters['paths_type'] != 's3s3':\n                raise ValueError(\n                    '--sse-c-copy-source is only supported for '\n                    'copy operations.'\n                )\n", "awscli/customizations/s3/filegenerator.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport sys\nimport stat\n\nfrom dateutil.parser import parse\nfrom dateutil.tz import tzlocal\nfrom botocore.exceptions import ClientError\n\nfrom awscli.customizations.s3.utils import find_bucket_key, get_file_stat\nfrom awscli.customizations.s3.utils import BucketLister, create_warning, \\\n    find_dest_path_comp_key, EPOCH_TIME\nfrom awscli.compat import queue\n\n_open = open\n\n\ndef is_special_file(path):\n    \"\"\"\n    This function checks to see if a special file.  It checks if the\n    file is a character special device, block special device, FIFO, or\n    socket.\n    \"\"\"\n    mode = os.stat(path).st_mode\n    # Character special device.\n    if stat.S_ISCHR(mode):\n        return True\n    # Block special device\n    if stat.S_ISBLK(mode):\n        return True\n    # FIFO.\n    if stat.S_ISFIFO(mode):\n        return True\n    # Socket.\n    if stat.S_ISSOCK(mode):\n        return True\n    return False\n\n\ndef is_readable(path):\n    \"\"\"\n    This function checks to see if a file or a directory can be read.\n    This is tested by performing an operation that requires read access\n    on the file or the directory.\n    \"\"\"\n    if os.path.isdir(path):\n        try:\n            os.listdir(path)\n        except (OSError, IOError):\n            return False\n    else:\n        try:\n            with _open(path, 'r') as fd:\n                pass\n        except (OSError, IOError):\n            return False\n    return True\n\n\n# This class is provided primarily to provide a detailed error message.\n\nclass FileDecodingError(Exception):\n    \"\"\"Raised when there was an issue decoding the file.\"\"\"\n\n    ADVICE = (\n        \"Please check your locale settings.  The filename was decoded as: %s\\n\"\n        \"On posix platforms, check the LC_CTYPE environment variable.\"\n        % (sys.getfilesystemencoding())\n    )\n\n    def __init__(self, directory, filename):\n        self.directory = directory\n        self.file_name = filename\n        self.error_message = (\n            'There was an error trying to decode the the file %s in '\n            'directory \"%s\". \\n%s' % (repr(self.file_name),\n                                      self.directory,\n                                      self.ADVICE)\n        )\n        super(FileDecodingError, self).__init__(self.error_message)\n\n\nclass FileStat(object):\n    def __init__(self, src, dest=None, compare_key=None, size=None,\n                 last_update=None, src_type=None, dest_type=None,\n                 operation_name=None, response_data=None):\n        self.src = src\n        self.dest = dest\n        self.compare_key = compare_key\n        self.size = size\n        self.last_update = last_update\n        self.src_type = src_type\n        self.dest_type = dest_type\n        self.operation_name = operation_name\n        self.response_data = response_data\n\n\nclass FileGenerator(object):\n    \"\"\"\n    This is a class the creates a generator to yield files based on information\n    returned from the ``FileFormat`` class.  It is universal in the sense that\n    it will handle s3 files, local files, local directories, and s3 objects\n    under the same common prefix.  The generator yields corresponding\n    ``FileInfo`` objects to send to a ``Comparator`` or ``S3Handler``.\n    \"\"\"\n    def __init__(self, client, operation_name, follow_symlinks=True,\n                 page_size=None, result_queue=None, request_parameters=None):\n        self._client = client\n        self.operation_name = operation_name\n        self.follow_symlinks = follow_symlinks\n        self.page_size = page_size\n        self.result_queue = result_queue\n        if not result_queue:\n            self.result_queue = queue.Queue()\n        self.request_parameters = {}\n        if request_parameters is not None:\n            self.request_parameters = request_parameters\n\n    def call(self, files):\n        \"\"\"\n        This is the generalized function to yield the ``FileInfo`` objects.\n        ``dir_op`` and ``use_src_name`` flags affect which files are used and\n        ensure the proper destination paths and compare keys are formed.\n        \"\"\"\n        function_table = {'s3': self.list_objects, 'local': self.list_files}\n        source = files['src']['path']\n        src_type = files['src']['type']\n        dest_type = files['dest']['type']\n        file_iterator = function_table[src_type](source, files['dir_op'])\n        for src_path, extra_information in file_iterator:\n            dest_path, compare_key = find_dest_path_comp_key(files, src_path)\n            file_stat_kwargs = {\n                'src': src_path, 'dest': dest_path, 'compare_key': compare_key,\n                'src_type': src_type, 'dest_type': dest_type,\n                'operation_name': self.operation_name\n            }\n            self._inject_extra_information(file_stat_kwargs, extra_information)\n            yield FileStat(**file_stat_kwargs)\n\n    def _inject_extra_information(self, file_stat_kwargs, extra_information):\n        src_type = file_stat_kwargs['src_type']\n        file_stat_kwargs['size'] = extra_information['Size']\n        file_stat_kwargs['last_update'] = extra_information['LastModified']\n\n        # S3 objects require the response data retrieved from HeadObject\n        # and ListObject\n        if src_type == 's3':\n            file_stat_kwargs['response_data'] = extra_information\n\n    def list_files(self, path, dir_op):\n        \"\"\"\n        This function yields the appropriate local file or local files\n        under a directory depending on if the operation is on a directory.\n        For directories a depth first search is implemented in order to\n        follow the same sorted pattern as a s3 list objects operation\n        outputs.  It yields the file's source path, size, and last\n        update\n        \"\"\"\n        join, isdir, isfile = os.path.join, os.path.isdir, os.path.isfile\n        error, listdir = os.error, os.listdir\n        if not self.should_ignore_file(path):\n            if not dir_op:\n                stats = self._safely_get_file_stats(path)\n                if stats:\n                    yield stats\n            else:\n                # We need to list files in byte order based on the full\n                # expanded path of the key: 'test/1/2/3.txt'  However,\n                # listdir() will only give us contents a single directory\n                # at a time, so we'll get 'test'.  At the same time we don't\n                # want to load the entire list of files into memory.  This\n                # is handled by first going through the current directory\n                # contents and adding the directory separator to any\n                # directories.  We can then sort the contents,\n                # and ensure byte order.\n                listdir_names = listdir(path)\n                names = []\n                for name in listdir_names:\n                    if not self.should_ignore_file_with_decoding_warnings(\n                            path, name):\n                        file_path = join(path, name)\n                        if isdir(file_path):\n                            name = name + os.path.sep\n                        names.append(name)\n                self.normalize_sort(names, os.sep, '/')\n                for name in names:\n                    file_path = join(path, name)\n                    if isdir(file_path):\n                        # Anything in a directory will have a prefix of\n                        # this current directory and will come before the\n                        # remaining contents in this directory.  This\n                        # means we need to recurse into this sub directory\n                        # before yielding the rest of this directory's\n                        # contents.\n                        for x in self.list_files(file_path, dir_op):\n                            yield x\n                    else:\n                        stats = self._safely_get_file_stats(file_path)\n                        if stats:\n                            yield stats\n\n    def _safely_get_file_stats(self, file_path):\n        try:\n            size, last_update = get_file_stat(file_path)\n        except (OSError, ValueError):\n            self.triggers_warning(file_path)\n        else:\n            last_update = self._validate_update_time(last_update, file_path)\n            return file_path, {'Size': size, 'LastModified': last_update}\n\n    def _validate_update_time(self, update_time, path):\n        # If the update time is None we know we ran into an invalid timestamp.\n        if update_time is None:\n            warning = create_warning(\n                path=path,\n                error_message=\"File has an invalid timestamp. Passing epoch \"\n                              \"time as timestamp.\",\n                skip_file=False)\n            self.result_queue.put(warning)\n            return EPOCH_TIME\n        return update_time\n\n    def normalize_sort(self, names, os_sep, character):\n        \"\"\"\n        The purpose of this function is to ensure that the same path separator\n        is used when sorting.  In windows, the path operator is a backslash as\n        opposed to a forward slash which can lead to differences in sorting\n        between s3 and a windows machine.\n        \"\"\"\n        names.sort(key=lambda item: item.replace(os_sep, character))\n\n    def should_ignore_file_with_decoding_warnings(self, dirname, filename):\n        \"\"\"\n        We can get a UnicodeDecodeError if we try to listdir(<unicode>) and\n        can't decode the contents with sys.getfilesystemencoding().  In this\n        case listdir() returns the bytestring, which means that\n        join(<unicode>, <str>) could raise a UnicodeDecodeError.  When this\n        happens we warn using a FileDecodingError that provides more\n        information into what's going on.\n        \"\"\"\n        if not isinstance(filename, str):\n            decoding_error = FileDecodingError(dirname, filename)\n            warning = create_warning(repr(filename),\n                                     decoding_error.error_message)\n            self.result_queue.put(warning)\n            return True\n        path = os.path.join(dirname, filename)\n        return self.should_ignore_file(path)\n\n    def should_ignore_file(self, path):\n        \"\"\"\n        This function checks whether a file should be ignored in the\n        file generation process.  This includes symlinks that are not to be\n        followed and files that generate warnings.\n        \"\"\"\n        if not self.follow_symlinks:\n            if os.path.isdir(path) and path.endswith(os.sep):\n                # Trailing slash must be removed to check if it is a symlink.\n                path = path[:-1]\n            if os.path.islink(path):\n                return True\n        warning_triggered = self.triggers_warning(path)\n        if warning_triggered:\n            return True\n        return False\n\n    def triggers_warning(self, path):\n        \"\"\"\n        This function checks the specific types and properties of a file.\n        If the file would cause trouble, the function adds a\n        warning to the result queue to be printed out and returns a boolean\n        value notify whether the file caused a warning to be generated.\n        Files that generate warnings are skipped.  Currently, this function\n        checks for files that do not exist and files that the user does\n        not have read access.\n        \"\"\"\n        if not os.path.exists(path):\n            warning = create_warning(path, \"File does not exist.\")\n            self.result_queue.put(warning)\n            return True\n        if is_special_file(path):\n            warning = create_warning(path,\n                                     (\"File is character special device, \"\n                                      \"block special device, FIFO, or \"\n                                      \"socket.\"))\n            self.result_queue.put(warning)\n            return True\n        if not is_readable(path):\n            warning = create_warning(path, \"File/Directory is not readable.\")\n            self.result_queue.put(warning)\n            return True\n        return False\n\n    def list_objects(self, s3_path, dir_op):\n        \"\"\"\n        This function yields the appropriate object or objects under a\n        common prefix depending if the operation is on objects under a\n        common prefix.  It yields the file's source path, size, and last\n        update.\n        \"\"\"\n        # Short circuit path: if we are not recursing into the s3\n        # bucket and a specific path was given, we can just yield\n        # that path and not have to call any operation in s3.\n        bucket, prefix = find_bucket_key(s3_path)\n        if not dir_op and prefix:\n            yield self._list_single_object(s3_path)\n        else:\n            lister = BucketLister(self._client)\n            extra_args = self.request_parameters.get('ListObjectsV2', {})\n            for key in lister.list_objects(bucket=bucket, prefix=prefix,\n                                           page_size=self.page_size,\n                                           extra_args=extra_args):\n                source_path, response_data = key\n                if response_data['Size'] == 0 and source_path.endswith('/'):\n                    if self.operation_name == 'delete':\n                        # This is to filter out manually created folders\n                        # in S3.  They have a size zero and would be\n                        # undesirably downloaded.  Local directories\n                        # are automatically created when they do not\n                        # exist locally.  But user should be able to\n                        # delete them.\n                        yield source_path, response_data\n                elif not dir_op and s3_path != source_path:\n                    pass\n                else:\n                    yield source_path, response_data\n\n    def _list_single_object(self, s3_path):\n        # When we know we're dealing with a single object, we can avoid\n        # a ListObjects operation (which causes concern for anyone setting\n        # IAM policies with the smallest set of permissions needed) and\n        # instead use a HeadObject request.\n        if self.operation_name == 'delete':\n            # If the operation is just a single remote delete, there is\n            # no need to run HeadObject on the S3 object as none of the\n            # information gained from HeadObject is required to delete the\n            # object.\n            return s3_path, {'Size': None, 'LastModified': None}\n        bucket, key = find_bucket_key(s3_path)\n        try:\n            params = {'Bucket': bucket, 'Key': key}\n            params.update(self.request_parameters.get('HeadObject', {}))\n            response = self._client.head_object(**params)\n        except ClientError as e:\n            # We want to try to give a more helpful error message.\n            # This is what the customer is going to see so we want to\n            # give as much detail as we have.\n            if not e.response['Error']['Code'] == '404':\n                raise\n            # The key does not exist so we'll raise a more specific\n            # error message here.\n            response = e.response.copy()\n            response['Error']['Message'] = 'Key \"%s\" does not exist' % key\n            raise ClientError(response, 'HeadObject')\n        response['Size'] = int(response.pop('ContentLength'))\n        last_update = parse(response['LastModified'])\n        response['LastModified'] = last_update.astimezone(tzlocal())\n        return s3_path, response\n", "awscli/customizations/s3/results.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import division\nimport logging\nimport sys\nimport threading\nimport time\nfrom collections import namedtuple\nfrom collections import defaultdict\n\nfrom s3transfer.exceptions import CancelledError\nfrom s3transfer.exceptions import FatalError\nfrom s3transfer.subscribers import BaseSubscriber\n\nfrom awscli.compat import queue, ensure_text_type\nfrom awscli.customizations.s3.utils import relative_path\nfrom awscli.customizations.s3.utils import human_readable_size\nfrom awscli.customizations.utils import uni_print\nfrom awscli.customizations.s3.utils import WarningResult\nfrom awscli.customizations.s3.utils import OnDoneFilteredSubscriber\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nBaseResult = namedtuple('BaseResult', ['transfer_type', 'src', 'dest'])\n\n\ndef _create_new_result_cls(name, extra_fields=None, base_cls=BaseResult):\n    # Creates a new namedtuple class that subclasses from BaseResult for the\n    # benefit of filtering by type and ensuring particular base attrs.\n\n    # NOTE: _fields is a public attribute that has an underscore to avoid\n    # naming collisions for namedtuples:\n    # https://docs.python.org/2/library/collections.html#collections.somenamedtuple._fields\n    fields = list(base_cls._fields)\n    if extra_fields:\n        fields += extra_fields\n    return type(name, (namedtuple(name, fields), base_cls), {})\n\n\nQueuedResult = _create_new_result_cls('QueuedResult', ['total_transfer_size'])\n\nProgressResult = _create_new_result_cls(\n    'ProgressResult', ['bytes_transferred', 'total_transfer_size',\n                       'timestamp'])\n\nSuccessResult = _create_new_result_cls('SuccessResult')\n\nFailureResult = _create_new_result_cls('FailureResult', ['exception'])\n\nDryRunResult = _create_new_result_cls('DryRunResult')\n\nErrorResult = namedtuple('ErrorResult', ['exception'])\n\nCtrlCResult = _create_new_result_cls('CtrlCResult', base_cls=ErrorResult)\n\nCommandResult = namedtuple(\n    'CommandResult', ['num_tasks_failed', 'num_tasks_warned'])\n\nFinalTotalSubmissionsResult = namedtuple(\n    'FinalTotalSubmissionsResult', ['total_submissions'])\n\n\nclass ShutdownThreadRequest(object):\n    pass\n\n\nclass BaseResultSubscriber(OnDoneFilteredSubscriber):\n    TRANSFER_TYPE = None\n\n    def __init__(self, result_queue, transfer_type=None):\n        \"\"\"Subscriber to send result notifications during transfer process\n\n        :param result_queue: The queue to place results to be processed later\n            on.\n        \"\"\"\n        self._result_queue = result_queue\n        self._result_kwargs_cache = {}\n        self._transfer_type = transfer_type\n        if transfer_type is None:\n            self._transfer_type = self.TRANSFER_TYPE\n\n    def on_queued(self, future, **kwargs):\n        self._add_to_result_kwargs_cache(future)\n        result_kwargs = self._result_kwargs_cache[future.meta.transfer_id]\n        queued_result = QueuedResult(**result_kwargs)\n        self._result_queue.put(queued_result)\n\n    def on_progress(self, future, bytes_transferred, **kwargs):\n        result_kwargs = self._result_kwargs_cache[future.meta.transfer_id]\n        progress_result = ProgressResult(\n            bytes_transferred=bytes_transferred, timestamp=time.time(),\n            **result_kwargs)\n        self._result_queue.put(progress_result)\n\n    def _on_success(self, future):\n        result_kwargs = self._on_done_pop_from_result_kwargs_cache(future)\n        self._result_queue.put(SuccessResult(**result_kwargs))\n\n    def _on_failure(self, future, e):\n        result_kwargs = self._on_done_pop_from_result_kwargs_cache(future)\n        if isinstance(e, CancelledError):\n            error_result_cls = CtrlCResult\n            if isinstance(e, FatalError):\n                error_result_cls = ErrorResult\n            self._result_queue.put(error_result_cls(exception=e))\n        else:\n            self._result_queue.put(FailureResult(exception=e, **result_kwargs))\n\n    def _add_to_result_kwargs_cache(self, future):\n        src, dest = self._get_src_dest(future)\n        result_kwargs = {\n            'transfer_type': self._transfer_type,\n            'src': src,\n            'dest': dest,\n            'total_transfer_size': future.meta.size\n        }\n        self._result_kwargs_cache[future.meta.transfer_id] = result_kwargs\n\n    def _on_done_pop_from_result_kwargs_cache(self, future):\n        result_kwargs = self._result_kwargs_cache.pop(future.meta.transfer_id)\n        result_kwargs.pop('total_transfer_size')\n        return result_kwargs\n\n    def _get_src_dest(self, future):\n        raise NotImplementedError('_get_src_dest()')\n\n\nclass UploadResultSubscriber(BaseResultSubscriber):\n    TRANSFER_TYPE = 'upload'\n\n    def _get_src_dest(self, future):\n        call_args = future.meta.call_args\n        src = self._get_src(call_args.fileobj)\n        dest = 's3://' + call_args.bucket + '/' + call_args.key\n        return src, dest\n\n    def _get_src(self, fileobj):\n        return relative_path(fileobj)\n\n\nclass UploadStreamResultSubscriber(UploadResultSubscriber):\n    def _get_src(self, fileobj):\n        return '-'\n\n\nclass DownloadResultSubscriber(BaseResultSubscriber):\n    TRANSFER_TYPE = 'download'\n\n    def _get_src_dest(self, future):\n        call_args = future.meta.call_args\n        src = 's3://' + call_args.bucket + '/' + call_args.key\n        dest = self._get_dest(call_args.fileobj)\n        return src, dest\n\n    def _get_dest(self, fileobj):\n        return relative_path(fileobj)\n\n\nclass DownloadStreamResultSubscriber(DownloadResultSubscriber):\n    def _get_dest(self, fileobj):\n        return '-'\n\n\nclass CopyResultSubscriber(BaseResultSubscriber):\n    TRANSFER_TYPE = 'copy'\n\n    def _get_src_dest(self, future):\n        call_args = future.meta.call_args\n        copy_source = call_args.copy_source\n        src = 's3://' + copy_source['Bucket'] + '/' + copy_source['Key']\n        dest = 's3://' + call_args.bucket + '/' + call_args.key\n        return src, dest\n\n\nclass DeleteResultSubscriber(BaseResultSubscriber):\n    TRANSFER_TYPE = 'delete'\n\n    def _get_src_dest(self, future):\n        call_args = future.meta.call_args\n        src = 's3://' + call_args.bucket + '/' + call_args.key\n        return src, None\n\n\nclass BaseResultHandler(object):\n    \"\"\"Base handler class to be called in the ResultProcessor\"\"\"\n    def __call__(self, result):\n        raise NotImplementedError('__call__()')\n\n\nclass ResultRecorder(BaseResultHandler):\n    \"\"\"Records and track transfer statistics based on results received\"\"\"\n    def __init__(self):\n        self.bytes_transferred = 0\n        self.bytes_failed_to_transfer = 0\n        self.files_transferred = 0\n        self.files_failed = 0\n        self.files_warned = 0\n        self.errors = 0\n        self.expected_bytes_transferred = 0\n        self.expected_files_transferred = 0\n        self.final_expected_files_transferred = None\n\n        self.start_time = None\n        self.bytes_transfer_speed = 0\n\n        self._ongoing_progress = defaultdict(int)\n        self._ongoing_total_sizes = {}\n\n        self._result_handler_map = {\n            QueuedResult: self._record_queued_result,\n            ProgressResult: self._record_progress_result,\n            SuccessResult: self._record_success_result,\n            FailureResult: self._record_failure_result,\n            WarningResult: self._record_warning_result,\n            ErrorResult: self._record_error_result,\n            CtrlCResult: self._record_error_result,\n            FinalTotalSubmissionsResult: self._record_final_expected_files,\n        }\n\n    def expected_totals_are_final(self):\n        return (\n            self.final_expected_files_transferred ==\n            self.expected_files_transferred\n        )\n\n    def __call__(self, result):\n        \"\"\"Record the result of an individual Result object\"\"\"\n        self._result_handler_map.get(type(result), self._record_noop)(\n            result=result)\n\n    def _get_ongoing_dict_key(self, result):\n        if not isinstance(result, BaseResult):\n            raise ValueError(\n                'Any result using _get_ongoing_dict_key must subclass from '\n                'BaseResult. Provided result is of type: %s' % type(result)\n            )\n        key_parts = []\n        for result_property in [result.transfer_type, result.src, result.dest]:\n            if result_property is not None:\n                key_parts.append(ensure_text_type(result_property))\n        return u':'.join(key_parts)\n\n    def _pop_result_from_ongoing_dicts(self, result):\n        ongoing_key = self._get_ongoing_dict_key(result)\n        total_progress = self._ongoing_progress.pop(ongoing_key, 0)\n        total_file_size = self._ongoing_total_sizes.pop(ongoing_key, None)\n        return total_progress, total_file_size\n\n    def _record_noop(self, **kwargs):\n        # If the result does not have a handler, then do nothing with it.\n        pass\n\n    def _record_queued_result(self, result, **kwargs):\n        if self.start_time is None:\n            self.start_time = time.time()\n        total_transfer_size = result.total_transfer_size\n        self._ongoing_total_sizes[\n            self._get_ongoing_dict_key(result)] = total_transfer_size\n        # The total transfer size can be None if we do not know the size\n        # immediately so do not add to the total right away.\n        if total_transfer_size:\n            self.expected_bytes_transferred += total_transfer_size\n        self.expected_files_transferred += 1\n\n    def _record_progress_result(self, result, **kwargs):\n        bytes_transferred = result.bytes_transferred\n        self._update_ongoing_transfer_size_if_unknown(result)\n        self._ongoing_progress[\n            self._get_ongoing_dict_key(result)] += bytes_transferred\n        self.bytes_transferred += bytes_transferred\n        # Since the start time is captured in the result recorder and\n        # capture timestamps in the subscriber, there is a chance that if\n        # a progress result gets created right after the queued result\n        # gets created that the timestamp on the progress result is less\n        # than the timestamp of when the result processor actually\n        # processes that initial queued result. So this will avoid\n        # negative progress being displayed or zero division occurring.\n        if result.timestamp > self.start_time:\n            self.bytes_transfer_speed = self.bytes_transferred / (\n                result.timestamp - self.start_time)\n\n    def _update_ongoing_transfer_size_if_unknown(self, result):\n        # This is a special case when the transfer size was previous not\n        # known but was provided in a progress result.\n        ongoing_key = self._get_ongoing_dict_key(result)\n\n        # First, check if the total size is None, meaning its size is\n        # currently unknown.\n        if self._ongoing_total_sizes[ongoing_key] is None:\n            total_transfer_size = result.total_transfer_size\n            # If the total size is no longer None that means we just learned\n            # of the size so let's update the appropriate places with this\n            # knowledge\n            if result.total_transfer_size is not None:\n                self._ongoing_total_sizes[ongoing_key] = total_transfer_size\n                # Figure out how many bytes have been unaccounted for as\n                # the recorder has been keeping track of how many bytes\n                # it has seen so far and add it to the total expected amount.\n                ongoing_progress = self._ongoing_progress[ongoing_key]\n                unaccounted_bytes = total_transfer_size - ongoing_progress\n                self.expected_bytes_transferred += unaccounted_bytes\n            # If we still do not know what the total transfer size is\n            # just update the expected bytes with the know bytes transferred\n            # as we know at the very least, those bytes are expected.\n            else:\n                self.expected_bytes_transferred += result.bytes_transferred\n\n    def _record_success_result(self, result, **kwargs):\n        self._pop_result_from_ongoing_dicts(result)\n        self.files_transferred += 1\n\n    def _record_failure_result(self, result, **kwargs):\n        # If there was a failure, we want to account for the failure in\n        # the count for bytes transferred by just adding on the remaining bytes\n        # that did not get transferred.\n        total_progress, total_file_size = self._pop_result_from_ongoing_dicts(\n            result)\n        if total_file_size is not None:\n            progress_left = total_file_size - total_progress\n            self.bytes_failed_to_transfer += progress_left\n\n        self.files_failed += 1\n        self.files_transferred += 1\n\n    def _record_warning_result(self, **kwargs):\n        self.files_warned += 1\n\n    def _record_error_result(self, **kwargs):\n        self.errors += 1\n\n    def _record_final_expected_files(self, result, **kwargs):\n        self.final_expected_files_transferred = result.total_submissions\n\n\nclass ResultPrinter(BaseResultHandler):\n    _FILES_REMAINING = \"{remaining_files} file(s) remaining\"\n    _ESTIMATED_EXPECTED_TOTAL = \"~{expected_total}\"\n    _STILL_CALCULATING_TOTALS = \" (calculating...)\"\n    BYTE_PROGRESS_FORMAT = (\n        'Completed {bytes_completed}/{expected_bytes_completed} '\n        '({transfer_speed}) with ' + _FILES_REMAINING\n    )\n    FILE_PROGRESS_FORMAT = (\n        'Completed {files_completed} file(s) with ' + _FILES_REMAINING\n    )\n    SUCCESS_FORMAT = (\n        u'{transfer_type}: {transfer_location}'\n    )\n    DRY_RUN_FORMAT = u'(dryrun) ' + SUCCESS_FORMAT\n    FAILURE_FORMAT = (\n        u'{transfer_type} failed: {transfer_location} {exception}'\n    )\n    # TODO: Add \"warning: \" prefix once all commands are converted to using\n    # result printer and remove \"warning: \" prefix from ``create_warning``.\n    WARNING_FORMAT = (\n        u'{message}'\n    )\n    ERROR_FORMAT = (\n        u'fatal error: {exception}'\n    )\n    CTRL_C_MSG = 'cancelled: ctrl-c received'\n\n    SRC_DEST_TRANSFER_LOCATION_FORMAT = u'{src} to {dest}'\n    SRC_TRANSFER_LOCATION_FORMAT = u'{src}'\n\n    def __init__(self, result_recorder, out_file=None, error_file=None):\n        \"\"\"Prints status of ongoing transfer\n\n        :type result_recorder: ResultRecorder\n        :param result_recorder: The associated result recorder\n\n        :type out_file: file-like obj\n        :param out_file: Location to write progress and success statements.\n            By default, the location is sys.stdout.\n\n        :type error_file: file-like obj\n        :param error_file: Location to write warnings and errors.\n            By default, the location is sys.stderr.\n        \"\"\"\n        self._result_recorder = result_recorder\n        self._out_file = out_file\n        if self._out_file is None:\n            self._out_file = sys.stdout\n        self._error_file = error_file\n        if self._error_file is None:\n            self._error_file = sys.stderr\n        self._progress_length = 0\n        self._result_handler_map = {\n            ProgressResult: self._print_progress,\n            SuccessResult: self._print_success,\n            FailureResult: self._print_failure,\n            WarningResult: self._print_warning,\n            ErrorResult: self._print_error,\n            CtrlCResult: self._print_ctrl_c,\n            DryRunResult: self._print_dry_run,\n            FinalTotalSubmissionsResult:\n                self._clear_progress_if_no_more_expected_transfers,\n        }\n\n    def __call__(self, result):\n        \"\"\"Print the progress of the ongoing transfer based on a result\"\"\"\n        self._result_handler_map.get(type(result), self._print_noop)(\n            result=result)\n\n    def _print_noop(self, **kwargs):\n        # If the result does not have a handler, then do nothing with it.\n        pass\n\n    def _print_dry_run(self, result, **kwargs):\n        statement = self.DRY_RUN_FORMAT.format(\n            transfer_type=result.transfer_type,\n            transfer_location=self._get_transfer_location(result)\n        )\n        statement = self._adjust_statement_padding(statement)\n        self._print_to_out_file(statement)\n\n    def _print_success(self, result, **kwargs):\n        success_statement = self.SUCCESS_FORMAT.format(\n            transfer_type=result.transfer_type,\n            transfer_location=self._get_transfer_location(result)\n        )\n        success_statement = self._adjust_statement_padding(success_statement)\n        self._print_to_out_file(success_statement)\n        self._redisplay_progress()\n\n    def _print_failure(self, result, **kwargs):\n        failure_statement = self.FAILURE_FORMAT.format(\n            transfer_type=result.transfer_type,\n            transfer_location=self._get_transfer_location(result),\n            exception=result.exception\n        )\n        failure_statement = self._adjust_statement_padding(failure_statement)\n        self._print_to_error_file(failure_statement)\n        self._redisplay_progress()\n\n    def _print_warning(self, result, **kwargs):\n        warning_statement = self.WARNING_FORMAT.format(message=result.message)\n        warning_statement = self._adjust_statement_padding(warning_statement)\n        self._print_to_error_file(warning_statement)\n        self._redisplay_progress()\n\n    def _print_error(self, result, **kwargs):\n        self._flush_error_statement(\n            self.ERROR_FORMAT.format(exception=result.exception))\n\n    def _print_ctrl_c(self, result, **kwargs):\n        self._flush_error_statement(self.CTRL_C_MSG)\n\n    def _flush_error_statement(self, error_statement):\n        error_statement = self._adjust_statement_padding(error_statement)\n        self._print_to_error_file(error_statement)\n\n    def _get_transfer_location(self, result):\n        if result.dest is None:\n            return self.SRC_TRANSFER_LOCATION_FORMAT.format(src=result.src)\n        return self.SRC_DEST_TRANSFER_LOCATION_FORMAT.format(\n            src=result.src, dest=result.dest)\n\n    def _redisplay_progress(self):\n        # Reset to zero because done statements are printed with new lines\n        # meaning there are no carriage returns to take into account when\n        # printing the next line.\n        self._progress_length = 0\n        self._add_progress_if_needed()\n\n    def _add_progress_if_needed(self):\n        if self._has_remaining_progress():\n            self._print_progress()\n\n    def _print_progress(self, **kwargs):\n        # Get all of the statistics in the correct form.\n        remaining_files = self._get_expected_total(\n            str(self._result_recorder.expected_files_transferred -\n                self._result_recorder.files_transferred)\n        )\n\n        # Create the display statement.\n        if self._result_recorder.expected_bytes_transferred > 0:\n            bytes_completed = human_readable_size(\n                self._result_recorder.bytes_transferred +\n                self._result_recorder.bytes_failed_to_transfer\n            )\n            expected_bytes_completed = self._get_expected_total(\n                human_readable_size(\n                    self._result_recorder.expected_bytes_transferred))\n\n            transfer_speed = human_readable_size(\n                self._result_recorder.bytes_transfer_speed) + '/s'\n            progress_statement = self.BYTE_PROGRESS_FORMAT.format(\n                bytes_completed=bytes_completed,\n                expected_bytes_completed=expected_bytes_completed,\n                transfer_speed=transfer_speed,\n                remaining_files=remaining_files\n            )\n        else:\n            # We're not expecting any bytes to be transferred, so we should\n            # only print of information about number of files transferred.\n            progress_statement = self.FILE_PROGRESS_FORMAT.format(\n                files_completed=self._result_recorder.files_transferred,\n                remaining_files=remaining_files\n            )\n\n        if not self._result_recorder.expected_totals_are_final():\n            progress_statement += self._STILL_CALCULATING_TOTALS\n\n        # Make sure that it overrides any previous progress bar.\n        progress_statement = self._adjust_statement_padding(\n                progress_statement, ending_char='\\r')\n        # We do not want to include the carriage return in this calculation\n        # as progress length is used for determining whitespace padding.\n        # So we subtract one off of the length.\n        self._progress_length = len(progress_statement) - 1\n\n        # Print the progress out.\n        self._print_to_out_file(progress_statement)\n\n    def _get_expected_total(self, expected_total):\n        if not self._result_recorder.expected_totals_are_final():\n            return self._ESTIMATED_EXPECTED_TOTAL.format(\n                expected_total=expected_total)\n        return expected_total\n\n    def _adjust_statement_padding(self, print_statement, ending_char='\\n'):\n        print_statement = print_statement.ljust(self._progress_length, ' ')\n        return print_statement + ending_char\n\n    def _has_remaining_progress(self):\n        if not self._result_recorder.expected_totals_are_final():\n            return True\n        actual = self._result_recorder.files_transferred\n        expected = self._result_recorder.expected_files_transferred\n        return actual != expected\n\n    def _print_to_out_file(self, statement):\n        uni_print(statement, self._out_file)\n\n    def _print_to_error_file(self, statement):\n        uni_print(statement, self._error_file)\n\n    def _clear_progress_if_no_more_expected_transfers(self, **kwargs):\n        if self._progress_length and not self._has_remaining_progress():\n            uni_print(self._adjust_statement_padding(''), self._out_file)\n\n\nclass NoProgressResultPrinter(ResultPrinter):\n    \"\"\"A result printer that doesn't print progress\"\"\"\n    def _print_progress(self, **kwargs):\n        pass\n\n\nclass OnlyShowErrorsResultPrinter(ResultPrinter):\n    \"\"\"A result printer that only prints out errors\"\"\"\n    def _print_progress(self, **kwargs):\n        pass\n\n    def _print_success(self, result, **kwargs):\n        pass\n\n\nclass ResultProcessor(threading.Thread):\n    def __init__(self, result_queue, result_handlers=None):\n        \"\"\"Thread to process results from result queue\n\n        This includes recording statistics and printing transfer status\n\n        :param result_queue: The result queue to process results from\n        :param result_handlers: A list of callables that take a result in as\n            a parameter to process the result for that handler.\n        \"\"\"\n        threading.Thread.__init__(self)\n        self._result_queue = result_queue\n        self._result_handlers = result_handlers\n        if self._result_handlers is None:\n            self._result_handlers = []\n        self._result_handlers_enabled = True\n\n    def run(self):\n        while True:\n            try:\n                result = self._result_queue.get(True)\n                if isinstance(result, ShutdownThreadRequest):\n                    LOGGER.debug(\n                        'Shutdown request received in result processing '\n                        'thread, shutting down result thread.')\n                    break\n                if self._result_handlers_enabled:\n                    self._process_result(result)\n                # ErrorResults are fatal to the command. If a fatal error\n                # is seen, we know that the command is trying to shutdown\n                # so disable all of the handlers and quickly consume all\n                # of the results in the result queue in order to get to\n                # the shutdown request to clean up the process.\n                if isinstance(result, ErrorResult):\n                    self._result_handlers_enabled = False\n            except queue.Empty:\n                pass\n\n    def _process_result(self, result):\n        for result_handler in self._result_handlers:\n            try:\n                result_handler(result)\n            except Exception as e:\n                LOGGER.debug(\n                    'Error processing result %s with handler %s: %s',\n                    result, result_handler, e, exc_info=True)\n\n\nclass CommandResultRecorder(object):\n    def __init__(self, result_queue, result_recorder, result_processor):\n        \"\"\"Records the result for an entire command\n\n        It will fully process all results in a result queue and determine\n        a CommandResult representing the entire command.\n\n        :type result_queue: queue.Queue\n        :param result_queue: The result queue in which results are placed on\n            and processed from\n\n        :type result_recorder: ResultRecorder\n        :param result_recorder: The result recorder to track the various\n            results sent through the result queue\n\n        :type result_processor: ResultProcessor\n        :param result_processor: The result processor to process results\n            placed on the queue\n        \"\"\"\n        self.result_queue = result_queue\n        self._result_recorder = result_recorder\n        self._result_processor = result_processor\n\n    def start(self):\n        self._result_processor.start()\n\n    def shutdown(self):\n        self.result_queue.put(ShutdownThreadRequest())\n        self._result_processor.join()\n\n    def get_command_result(self):\n        \"\"\"Get the CommandResult representing the result of a command\n\n        :rtype: CommandResult\n        :returns: The CommandResult representing the total result from running\n            a particular command\n        \"\"\"\n        return CommandResult(\n            self._result_recorder.files_failed + self._result_recorder.errors,\n            self._result_recorder.files_warned\n        )\n\n    def notify_total_submissions(self, total):\n        self.result_queue.put(FinalTotalSubmissionsResult(total))\n\n    def __enter__(self):\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_value, *args):\n        if exc_type:\n            LOGGER.debug('Exception caught during command execution: %s',\n                         exc_value, exc_info=True)\n            self.result_queue.put(ErrorResult(exception=exc_value))\n            self.shutdown()\n            return True\n        self.shutdown()\n", "awscli/customizations/s3/transferconfig.py": "# Copyright 2013-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom s3transfer.manager import TransferConfig\n\nfrom awscli.customizations.s3.utils import human_readable_to_bytes\n# If the user does not specify any overrides,\n# these are the default values we use for the s3 transfer\n# commands.\nDEFAULTS = {\n    'multipart_threshold': 8 * (1024 ** 2),\n    'multipart_chunksize': 8 * (1024 ** 2),\n    'max_concurrent_requests': 10,\n    'max_queue_size': 1000,\n    'max_bandwidth': None\n}\n\n\nclass InvalidConfigError(Exception):\n    pass\n\n\nclass RuntimeConfig(object):\n\n    POSITIVE_INTEGERS = ['multipart_chunksize', 'multipart_threshold',\n                         'max_concurrent_requests', 'max_queue_size',\n                         'max_bandwidth']\n    HUMAN_READABLE_SIZES = ['multipart_chunksize', 'multipart_threshold']\n    HUMAN_READABLE_RATES = ['max_bandwidth']\n\n    @staticmethod\n    def defaults():\n        return DEFAULTS.copy()\n\n    def build_config(self, **kwargs):\n        \"\"\"Create and convert a runtime config dictionary.\n\n        This method will merge and convert S3 runtime configuration\n        data into a single dictionary that can then be passed to classes\n        that use this runtime config.\n\n        :param kwargs:  Any key in the ``DEFAULTS`` dict.\n        :return: A dictionary of the merged and converted values.\n\n        \"\"\"\n        runtime_config = DEFAULTS.copy()\n        if kwargs:\n            runtime_config.update(kwargs)\n        self._convert_human_readable_sizes(runtime_config)\n        self._convert_human_readable_rates(runtime_config)\n        self._validate_config(runtime_config)\n        return runtime_config\n\n    def _convert_human_readable_sizes(self, runtime_config):\n        for attr in self.HUMAN_READABLE_SIZES:\n            value = runtime_config.get(attr)\n            if value is not None and not isinstance(value, int):\n                runtime_config[attr] = human_readable_to_bytes(value)\n\n    def _convert_human_readable_rates(self, runtime_config):\n        for attr in self.HUMAN_READABLE_RATES:\n            value = runtime_config.get(attr)\n            if value is not None and not isinstance(value, int):\n                if not value.endswith('B/s'):\n                    raise InvalidConfigError(\n                        'Invalid rate: %s. The value must be expressed '\n                        'as a rate in terms of bytes per seconds '\n                        '(e.g. 10MB/s or 800KB/s)' % value)\n                runtime_config[attr] = human_readable_to_bytes(value[:-2])\n\n    def _validate_config(self, runtime_config):\n        for attr in self.POSITIVE_INTEGERS:\n            value = runtime_config.get(attr)\n            if value is not None:\n                try:\n                    runtime_config[attr] = int(value)\n                    if not runtime_config[attr] > 0:\n                        self._error_positive_value(attr, value)\n                except ValueError:\n                    self._error_positive_value(attr, value)\n\n    def _error_positive_value(self, name, value):\n        raise InvalidConfigError(\n            \"Value for %s must be a positive integer: %s\" % (name, value))\n\n\ndef create_transfer_config_from_runtime_config(runtime_config):\n    \"\"\"\n    Creates an equivalent s3transfer TransferConfig\n\n    :type runtime_config: dict\n    :argument runtime_config: A valid RuntimeConfig-generated dict.\n\n    :returns: A TransferConfig with the same configuration as the runtime\n        config.\n    \"\"\"\n    translation_map = {\n        'max_concurrent_requests': 'max_request_concurrency',\n        'max_queue_size': 'max_request_queue_size',\n        'multipart_threshold': 'multipart_threshold',\n        'multipart_chunksize': 'multipart_chunksize',\n        'max_bandwidth': 'max_bandwidth',\n    }\n    kwargs = {}\n    for key, value in runtime_config.items():\n        if key not in translation_map:\n            continue\n        kwargs[translation_map[key]] = value\n    return TransferConfig(**kwargs)\n", "awscli/customizations/s3/comparator.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nfrom awscli.compat import advance_iterator\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass Comparator(object):\n    \"\"\"\n    This class performs all of the comparisons behind the sync operation\n    \"\"\"\n    def __init__(self, file_at_src_and_dest_sync_strategy,\n                 file_not_at_dest_sync_strategy,\n                 file_not_at_src_sync_strategy):\n\n        self._sync_strategy = file_at_src_and_dest_sync_strategy\n        self._not_at_dest_sync_strategy = file_not_at_dest_sync_strategy\n        self._not_at_src_sync_strategy = file_not_at_src_sync_strategy\n\n    def call(self, src_files, dest_files):\n        \"\"\"\n        This function preforms the actual comparisons.  The parameters it takes\n        are the generated files for both the source and the destination.  The\n        key concept in this function is that no matter the type of where the\n        files are coming from, they are listed in the same order, least to\n        greatest in collation order.  This allows for easy comparisons to\n        determine if file needs to be added or deleted.  Comparison keys are\n        used to determine if two files are the same and each file has a\n        unique comparison key.  If they are the same compare the size and\n        last modified times to see if a file needs to be updated.   Ultimately,\n        it will yield a sequence of file info objectsthat will be sent to\n        the ``S3Handler``.\n\n        :param src_files: The generated FileInfo objects from the source.\n        :param dest_files: The generated FileInfo objects from the dest.\n\n        :returns: Yields the FilInfo objects of the files that need to be\n            operated on\n\n        Algorithm:\n            Try to take next from both files. If it is empty signal\n            corresponding done flag.  If both generated lists are not done\n            compare compare_keys.  If equal, compare size and time to see if\n            it needs to be updated.  If source compare_key is less than dest\n            compare_key, the file needs to be added to the destination.  Take\n            the next source file but not not destination file.  If the source\n            compare_key is greater than dest compare_key, that destination file\n            needs to be deleted from the destination.  Take the next dest file\n            but not the source file.  If the source list is empty delete the\n            rest of the files in the dest list from the destination.  If the\n            dest list is empty add the rest of the file in source list to\n            the destination.\n        \"\"\"\n        # :var src_done: True if there are no more files from the source left.\n        src_done = False\n        # :var dest_done: True if there are no more files form the dest left.\n        dest_done = False\n        # :var src_take: Take the next source file from the generated files if\n        #     true\n        src_take = True\n        # :var dest_take: Take the next dest file from the generated files if\n        #     true\n        dest_take = True\n        while True:\n            try:\n                if (not src_done) and src_take:\n                    src_file = advance_iterator(src_files)\n            except StopIteration:\n                src_file = None\n                src_done = True\n            try:\n                if (not dest_done) and dest_take:\n                    dest_file = advance_iterator(dest_files)\n            except StopIteration:\n                dest_file = None\n                dest_done = True\n\n            if (not src_done) and (not dest_done):\n                src_take = True\n                dest_take = True\n\n                compare_keys = self.compare_comp_key(src_file, dest_file)\n\n                if compare_keys == 'equal':\n                    should_sync = self._sync_strategy.determine_should_sync(\n                        src_file, dest_file\n                    )\n                    if should_sync:\n                        yield src_file\n                elif compare_keys == 'less_than':\n                    src_take = True\n                    dest_take = False\n                    should_sync = self._not_at_dest_sync_strategy.determine_should_sync(src_file, None)\n                    if should_sync:\n                        yield src_file\n\n                elif compare_keys == 'greater_than':\n                    src_take = False\n                    dest_take = True\n                    should_sync = self._not_at_src_sync_strategy.determine_should_sync(None, dest_file)\n                    if should_sync:\n                        yield dest_file\n\n            elif (not src_done) and dest_done:\n                src_take = True\n                should_sync = self._not_at_dest_sync_strategy.determine_should_sync(src_file, None)\n                if should_sync:\n                    yield src_file\n\n            elif src_done and (not dest_done):\n                dest_take = True\n                should_sync = self._not_at_src_sync_strategy.determine_should_sync(None, dest_file)\n                if should_sync:\n                    yield dest_file\n            else:\n                break\n\n    def compare_comp_key(self, src_file, dest_file):\n        \"\"\"\n        Determines if the source compare_key is less than, equal to,\n        or greater than the destination compare_key\n        \"\"\"\n\n        src_comp_key = src_file.compare_key\n        dest_comp_key = dest_file.compare_key\n        if (src_comp_key == dest_comp_key):\n            return 'equal'\n\n        elif (src_comp_key < dest_comp_key):\n            return 'less_than'\n\n        else:\n            return 'greater_than'\n", "awscli/customizations/s3/s3.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations import utils\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.s3.subcommands import ListCommand, WebsiteCommand, \\\n    CpCommand, MvCommand, RmCommand, SyncCommand, MbCommand, RbCommand, \\\n    PresignCommand\nfrom awscli.customizations.s3.syncstrategy.register import \\\n    register_sync_strategies\n\n\ndef awscli_initialize(cli):\n    \"\"\"\n    This function is require to use the plugin.  It calls the functions\n    required to add all necessary commands and parameters to the CLI.\n    This function is necessary to install the plugin using a configuration\n    file\n    \"\"\"\n    cli.register(\"building-command-table.main\", add_s3)\n    cli.register('building-command-table.sync', register_sync_strategies)\n\n\ndef s3_plugin_initialize(event_handlers):\n    \"\"\"\n    This is a wrapper to make the plugin built-in to the cli as opposed\n    to specifying it in the configuration file.\n    \"\"\"\n    awscli_initialize(event_handlers)\n\n\ndef add_s3(command_table, session, **kwargs):\n    \"\"\"\n    This creates a new service object for the s3 plugin.  It sends the\n    old s3 commands to the namespace ``s3api``.\n    \"\"\"\n    utils.rename_command(command_table, 's3', 's3api')\n    command_table['s3'] = S3(session)\n\n\nclass S3(BasicCommand):\n    NAME = 's3'\n    DESCRIPTION = BasicCommand.FROM_FILE('s3/_concepts.rst')\n    SYNOPSIS = \"aws s3 <Command> [<Arg> ...]\"\n    SUBCOMMANDS = [\n        {'name': 'ls', 'command_class': ListCommand},\n        {'name': 'website', 'command_class': WebsiteCommand},\n        {'name': 'cp', 'command_class': CpCommand},\n        {'name': 'mv', 'command_class': MvCommand},\n        {'name': 'rm', 'command_class': RmCommand},\n        {'name': 'sync', 'command_class': SyncCommand},\n        {'name': 'mb', 'command_class': MbCommand},\n        {'name': 'rb', 'command_class': RbCommand},\n        {'name': 'presign', 'command_class': PresignCommand},\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        if parsed_args.subcommand is None:\n            raise ValueError(\"usage: aws [options] <command> <subcommand> \"\n                             \"[parameters]\\naws: error: too few arguments\")\n", "awscli/customizations/s3/fileformat.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\n\nclass FileFormat(object):\n    def format(self, src, dest, parameters):\n        \"\"\"\n        This function formats the source and destination\n        path to the proper form for a file generator.\n\n        Note that a file is designated as an s3 file if it begins with s3://\n\n        :param src: The path of the source\n        :type src: string\n        :param dest: The path of the dest\n        :type dest: string\n        :param parameters: A dictionary that will be formed when the arguments\n            of the command line have been parsed.  For this\n            function the dictionary should have the key 'dir_op'\n            which is a boolean value that is true when\n            the operation is being performed on a local directory/\n            all objects under a common prefix in s3 or false when\n            it is on a single file/object.\n\n        :returns: A dictionary that will be passed to a file generator.\n            The dictionary contains the keys src, dest, dir_op, and\n            use_src_name. src is a dictionary containing the source path\n            and whether its located locally or in s3. dest is a dictionary\n            containing the destination path and whether its located\n            locally or in s3.\n        \"\"\"\n        src_type, src_path = self.identify_type(src)\n        dest_type, dest_path = self.identify_type(dest)\n        format_table = {'s3': self.s3_format, 'local': self.local_format}\n        # :var dir_op: True when the operation being performed is on a\n        #     directory/objects under a common prefix or false when it\n        #     is a single file\n        dir_op = parameters['dir_op']\n        src_path = format_table[src_type](src_path, dir_op)[0]\n        # :var use_src_name: True when the destination file/object will take on\n        #     the name of the source file/object.  False when it\n        #     will take on the name the user specified in the\n        #     command line.\n        dest_path, use_src_name = format_table[dest_type](dest_path, dir_op)\n        files = {'src': {'path': src_path, 'type': src_type},\n                 'dest': {'path': dest_path, 'type': dest_type},\n                 'dir_op': dir_op, 'use_src_name': use_src_name}\n        return files\n\n    def local_format(self, path, dir_op):\n        \"\"\"\n        This function formats the path of local files and returns whether the\n        destination will keep its own name or take the source's name along with\n        the edited path.\n        Formatting Rules:\n            1) If a destination file is taking on a source name, it must end\n               with the appropriate operating system separator\n\n        General Options:\n            1) If the operation is on a directory, the destination file will\n               always use the name of the corresponding source file.\n            2) If the path of the destination exists and is a directory it\n               will always use the name of the source file.\n            3) If the destination path ends with the appropriate operating\n               system seperator but is not an existing directory, the\n               appropriate directories will be made and the file will use the\n               source's name.\n            4) If the destination path does not end with the appropriate\n               operating system seperator and is not an existing directory, the\n               appropriate directories will be created and the file name will\n               be of the one provided.\n        \"\"\"\n        full_path = os.path.abspath(path)\n        if (os.path.exists(full_path) and os.path.isdir(full_path)) or dir_op:\n            full_path += os.sep\n            return full_path, True\n        else:\n            if path.endswith(os.sep):\n                full_path += os.sep\n                return full_path, True\n            else:\n                return full_path, False\n\n    def s3_format(self, path, dir_op):\n        \"\"\"\n        This function formats the path of source files and returns whether the\n        destination will keep its own name or take the source's name along\n        with the edited path.\n        Formatting Rules:\n            1) If a destination file is taking on a source name, it must end\n               with a forward slash.\n        General Options:\n            1) If the operation is on objects under a common prefix,\n               the destination file will always use the name of the\n               corresponding source file.\n            2) If the path ends with a forward slash, the appropriate prefixes\n               will be formed and will use the name of the source.\n            3) If the path does not end with a forward slash, the appropriate\n               prefix will be formed but use the the name provided as opposed\n               to the source name.\n        \"\"\"\n        if dir_op:\n            if not path.endswith('/'):\n                path += '/'\n            return path, True\n        else:\n            if not path.endswith('/'):\n                return path, False\n            else:\n                return path, True\n\n    def identify_type(self, path):\n        \"\"\"\n        It identifies whether the path is from local or s3.  Returns the\n        adjusted pathname and a string stating whether the file is from local\n        or s3.  If from s3 it strips off the s3:// from the beginning of the\n        path\n        \"\"\"\n        if path.startswith('s3://'):\n            return 's3', path[5:]\n        else:\n            return 'local', path\n", "awscli/customizations/s3/__init__.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "awscli/customizations/s3/s3handler.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\n\nfrom s3transfer.manager import TransferManager\n\nfrom awscli.customizations.s3.utils import (\n    human_readable_size, MAX_UPLOAD_SIZE, find_bucket_key, relative_path,\n    create_warning, NonSeekableStream)\nfrom awscli.customizations.s3.transferconfig import \\\n    create_transfer_config_from_runtime_config\nfrom awscli.customizations.s3.results import UploadResultSubscriber\nfrom awscli.customizations.s3.results import DownloadResultSubscriber\nfrom awscli.customizations.s3.results import CopyResultSubscriber\nfrom awscli.customizations.s3.results import UploadStreamResultSubscriber\nfrom awscli.customizations.s3.results import DownloadStreamResultSubscriber\nfrom awscli.customizations.s3.results import DeleteResultSubscriber\nfrom awscli.customizations.s3.results import QueuedResult\nfrom awscli.customizations.s3.results import SuccessResult\nfrom awscli.customizations.s3.results import FailureResult\nfrom awscli.customizations.s3.results import DryRunResult\nfrom awscli.customizations.s3.results import ResultRecorder\nfrom awscli.customizations.s3.results import ResultPrinter\nfrom awscli.customizations.s3.results import OnlyShowErrorsResultPrinter\nfrom awscli.customizations.s3.results import NoProgressResultPrinter\nfrom awscli.customizations.s3.results import ResultProcessor\nfrom awscli.customizations.s3.results import CommandResultRecorder\nfrom awscli.customizations.s3.utils import RequestParamsMapper\nfrom awscli.customizations.s3.utils import StdoutBytesWriter\nfrom awscli.customizations.s3.utils import ProvideSizeSubscriber\nfrom awscli.customizations.s3.utils import ProvideUploadContentTypeSubscriber\nfrom awscli.customizations.s3.utils import ProvideCopyContentTypeSubscriber\nfrom awscli.customizations.s3.utils import ProvideLastModifiedTimeSubscriber\nfrom awscli.customizations.s3.utils import DirectoryCreatorSubscriber\nfrom awscli.customizations.s3.utils import DeleteSourceFileSubscriber\nfrom awscli.customizations.s3.utils import DeleteSourceObjectSubscriber\nfrom awscli.customizations.s3.utils import DeleteCopySourceObjectSubscriber\nfrom awscli.compat import get_binary_stdin\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass S3TransferHandlerFactory(object):\n    MAX_IN_MEMORY_CHUNKS = 6\n\n    def __init__(self, cli_params, runtime_config):\n        \"\"\"Factory for S3TransferHandlers\n\n        :type cli_params: dict\n        :param cli_params: The parameters provide to the CLI command\n\n        :type runtime_config: RuntimeConfig\n        :param runtime_config: The runtime config for the CLI command\n            being run\n        \"\"\"\n        self._cli_params = cli_params\n        self._runtime_config = runtime_config\n\n    def __call__(self, client, result_queue):\n        \"\"\"Creates a S3TransferHandler instance\n\n        :type client: botocore.client.Client\n        :param client: The client to power the S3TransferHandler\n\n        :type result_queue: queue.Queue\n        :param result_queue: The result queue to be used to process results\n            for the S3TransferHandler\n\n        :returns: A S3TransferHandler instance\n        \"\"\"\n        transfer_config = create_transfer_config_from_runtime_config(\n            self._runtime_config)\n        transfer_config.max_in_memory_upload_chunks = self.MAX_IN_MEMORY_CHUNKS\n        transfer_config.max_in_memory_download_chunks = \\\n            self.MAX_IN_MEMORY_CHUNKS\n\n        transfer_manager = TransferManager(client, transfer_config)\n\n        LOGGER.debug(\n            \"Using a multipart threshold of %s and a part size of %s\",\n            transfer_config.multipart_threshold,\n            transfer_config.multipart_chunksize\n        )\n        result_recorder = ResultRecorder()\n        result_processor_handlers = [result_recorder]\n        self._add_result_printer(result_recorder, result_processor_handlers)\n        result_processor = ResultProcessor(\n            result_queue, result_processor_handlers)\n        command_result_recorder = CommandResultRecorder(\n            result_queue, result_recorder, result_processor)\n\n        return S3TransferHandler(\n            transfer_manager, self._cli_params, command_result_recorder)\n\n    def _add_result_printer(self, result_recorder, result_processor_handlers):\n        if self._cli_params.get('quiet'):\n            return\n        elif self._cli_params.get('only_show_errors'):\n            result_printer = OnlyShowErrorsResultPrinter(result_recorder)\n        elif self._cli_params.get('is_stream'):\n            result_printer = OnlyShowErrorsResultPrinter(result_recorder)\n        elif not self._cli_params.get('progress'):\n            result_printer = NoProgressResultPrinter(result_recorder)\n        else:\n            result_printer = ResultPrinter(result_recorder)\n        result_processor_handlers.append(result_printer)\n\n\nclass S3TransferHandler(object):\n    def __init__(self, transfer_manager, cli_params, result_command_recorder):\n        \"\"\"Backend for performing S3 transfers\n\n        :type transfer_manager: s3transfer.manager.TransferManager\n        :param transfer_manager: Transfer manager to use for transfers\n\n        :type cli_params: dict\n        :param cli_params: The parameters passed to the CLI command in the\n            form of a dictionary\n\n        :type result_command_recorder: ResultCommandRecorder\n        :param result_command_recorder: The result command recorder to be\n            used to get the final result of the transfer\n        \"\"\"\n        self._transfer_manager = transfer_manager\n        # TODO: Ideally the s3 transfer handler should not need to know\n        # about the result command recorder. It really only needs an interface\n        # for adding results to the queue. When all of the commands have\n        # converted to use this transfer handler, an effort should be made\n        # to replace the passing of a result command recorder with an\n        # abstraction to enqueue results.\n        self._result_command_recorder = result_command_recorder\n\n        submitter_args = (\n            self._transfer_manager, self._result_command_recorder.result_queue,\n            cli_params\n        )\n        self._submitters = [\n            UploadStreamRequestSubmitter(*submitter_args),\n            DownloadStreamRequestSubmitter(*submitter_args),\n            UploadRequestSubmitter(*submitter_args),\n            DownloadRequestSubmitter(*submitter_args),\n            CopyRequestSubmitter(*submitter_args),\n            DeleteRequestSubmitter(*submitter_args),\n            LocalDeleteRequestSubmitter(*submitter_args)\n        ]\n\n    def call(self, fileinfos):\n        \"\"\"Process iterable of FileInfos for transfer\n\n        :type fileinfos: iterable of FileInfos\n        param fileinfos: Set of FileInfos to submit to underlying transfer\n            request submitters to make transfer API calls to S3\n\n        :rtype: CommandResult\n        :returns: The result of the command that specifies the number of\n            failures and warnings encountered.\n        \"\"\"\n        with self._result_command_recorder:\n            with self._transfer_manager:\n                total_submissions = 0\n                for fileinfo in fileinfos:\n                    for submitter in self._submitters:\n                        if submitter.can_submit(fileinfo):\n                            if submitter.submit(fileinfo):\n                                total_submissions += 1\n                            break\n                self._result_command_recorder.notify_total_submissions(\n                    total_submissions)\n        return self._result_command_recorder.get_command_result()\n\n\nclass BaseTransferRequestSubmitter(object):\n    REQUEST_MAPPER_METHOD = None\n    RESULT_SUBSCRIBER_CLASS = None\n\n    def __init__(self, transfer_manager, result_queue, cli_params):\n        \"\"\"Submits transfer requests to the TransferManager\n\n        Given a FileInfo object and provided CLI parameters, it will add the\n        necessary extra arguments and subscribers in making a call to the\n        TransferManager.\n\n        :type transfer_manager: s3transfer.manager.TransferManager\n        :param transfer_manager: The underlying transfer manager\n\n        :type result_queue: queue.Queue\n        :param result_queue: The result queue to use\n\n        :type cli_params: dict\n        :param cli_params: The associated CLI parameters passed in to the\n            command as a dictionary.\n        \"\"\"\n        self._transfer_manager = transfer_manager\n        self._result_queue = result_queue\n        self._cli_params = cli_params\n\n    def submit(self, fileinfo):\n        \"\"\"Submits a transfer request based on the FileInfo provided\n\n        There is no guarantee that the transfer request will be made on\n        behalf of the fileinfo as a fileinfo may be skipped based on\n        circumstances in which the transfer is not possible.\n\n        :type fileinfo: awscli.customizations.s3.fileinfo.FileInfo\n        :param fileinfo: The FileInfo to be used to submit a transfer\n            request to the underlying transfer manager.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: A TransferFuture representing the transfer if it the\n            transfer was submitted. If it was not submitted nothing\n            is returned.\n        \"\"\"\n        should_skip = self._warn_and_signal_if_skip(fileinfo)\n        if not should_skip:\n            return self._do_submit(fileinfo)\n\n    def can_submit(self, fileinfo):\n        \"\"\"Checks whether it can submit a particular FileInfo\n\n        :type fileinfo: awscli.customizations.s3.fileinfo.FileInfo\n        :param fileinfo: The FileInfo to check if the transfer request\n            submitter can handle.\n\n        :returns: True if it can use the provided FileInfo to make a transfer\n            request to the underlying transfer manager. False, otherwise.\n        \"\"\"\n        raise NotImplementedError('can_submit()')\n\n    def _do_submit(self, fileinfo):\n        extra_args = {}\n        if self.REQUEST_MAPPER_METHOD:\n            self.REQUEST_MAPPER_METHOD(extra_args, self._cli_params)\n        subscribers = []\n        self._add_additional_subscribers(subscribers, fileinfo)\n        # The result subscriber class should always be the last registered\n        # subscriber to ensure it is not missing any information that\n        # may have been added in a different subscriber such as size.\n        if self.RESULT_SUBSCRIBER_CLASS:\n            result_kwargs = {'result_queue': self._result_queue}\n            if self._cli_params.get('is_move', False):\n                result_kwargs['transfer_type'] = 'move'\n            subscribers.append(self.RESULT_SUBSCRIBER_CLASS(**result_kwargs))\n\n        if not self._cli_params.get('dryrun'):\n            return self._submit_transfer_request(\n                fileinfo, extra_args, subscribers)\n        else:\n            self._submit_dryrun(fileinfo)\n\n    def _submit_dryrun(self, fileinfo):\n        transfer_type = fileinfo.operation_name\n        if self._cli_params.get('is_move', False):\n            transfer_type = 'move'\n        src, dest = self._format_src_dest(fileinfo)\n        self._result_queue.put(DryRunResult(\n            transfer_type=transfer_type, src=src, dest=dest))\n\n    def _add_additional_subscribers(self, subscribers, fileinfo):\n        pass\n\n    def _submit_transfer_request(self, fileinfo, extra_args, subscribers):\n        raise NotImplementedError('_submit_transfer_request()')\n\n    def _warn_and_signal_if_skip(self, fileinfo):\n        for warning_handler in self._get_warning_handlers():\n            if warning_handler(fileinfo):\n                # On the first warning handler that returns a signal to skip\n                # immediately propagate this signal and no longer check\n                # the other warning handlers as no matter what the file will\n                # be skipped.\n                return True\n\n    def _get_warning_handlers(self):\n        # Returns a list of warning handlers, which are callables that\n        # take in a single parameter representing a FileInfo. It will then\n        # add a warning to result_queue if needed and return True if\n        # that FileInfo should be skipped.\n        return []\n\n    def _should_inject_content_type(self):\n        return (\n            self._cli_params.get('guess_mime_type') and\n            not self._cli_params.get('content_type')\n        )\n\n    def _warn_glacier(self, fileinfo):\n        if not self._cli_params.get('force_glacier_transfer'):\n            if not fileinfo.is_glacier_compatible():\n                LOGGER.debug(\n                    'Encountered glacier object s3://%s. Not performing '\n                    '%s on object.' % (fileinfo.src, fileinfo.operation_name))\n                if not self._cli_params.get('ignore_glacier_warnings'):\n                    warning = create_warning(\n                        's3://'+fileinfo.src,\n                        'Object is of storage class GLACIER. Unable to '\n                        'perform %s operations on GLACIER objects. You must '\n                        'restore the object to be able to perform the '\n                        'operation. See aws s3 %s help for additional '\n                        'parameter options to ignore or force these '\n                        'transfers.' %\n                        (fileinfo.operation_name, fileinfo.operation_name)\n                    )\n                    self._result_queue.put(warning)\n                return True\n        return False\n\n    def _warn_parent_reference(self, fileinfo):\n        # normpath() will use the OS path separator so we\n        # need to take that into account when checking for a parent prefix.\n        parent_prefix = '..' + os.path.sep\n        escapes_cwd = os.path.normpath(fileinfo.compare_key).startswith(\n            parent_prefix)\n        if escapes_cwd:\n            warning = create_warning(\n                fileinfo.compare_key, \"File references a parent directory.\")\n            self._result_queue.put(warning)\n            return True\n        return False\n\n    def _format_src_dest(self, fileinfo):\n        \"\"\"Returns formatted versions of a fileinfos source and destination.\"\"\"\n        raise NotImplementedError('_format_src_dest')\n\n    def _format_local_path(self, path):\n        return relative_path(path)\n\n    def _format_s3_path(self, path):\n        if path.startswith('s3://'):\n            return path\n        return 's3://' + path\n\n\nclass UploadRequestSubmitter(BaseTransferRequestSubmitter):\n    REQUEST_MAPPER_METHOD = RequestParamsMapper.map_put_object_params\n    RESULT_SUBSCRIBER_CLASS = UploadResultSubscriber\n\n    def can_submit(self, fileinfo):\n        return fileinfo.operation_name == 'upload'\n\n    def _add_additional_subscribers(self, subscribers, fileinfo):\n        subscribers.append(ProvideSizeSubscriber(fileinfo.size))\n        if self._should_inject_content_type():\n            subscribers.append(ProvideUploadContentTypeSubscriber())\n        if self._cli_params.get('is_move', False):\n            subscribers.append(DeleteSourceFileSubscriber())\n\n    def _submit_transfer_request(self, fileinfo, extra_args, subscribers):\n        bucket, key = find_bucket_key(fileinfo.dest)\n        filein = self._get_filein(fileinfo)\n        return self._transfer_manager.upload(\n            fileobj=filein, bucket=bucket, key=key,\n            extra_args=extra_args, subscribers=subscribers\n        )\n\n    def _get_filein(self, fileinfo):\n        return fileinfo.src\n\n    def _get_warning_handlers(self):\n        return [self._warn_if_too_large]\n\n    def _warn_if_too_large(self, fileinfo):\n        if getattr(fileinfo, 'size') and fileinfo.size > MAX_UPLOAD_SIZE:\n            file_path = relative_path(fileinfo.src)\n            warning_message = (\n                \"File %s exceeds s3 upload limit of %s.\" % (\n                    file_path, human_readable_size(MAX_UPLOAD_SIZE)))\n            warning = create_warning(\n                file_path, warning_message, skip_file=False)\n            self._result_queue.put(warning)\n\n    def _format_src_dest(self, fileinfo):\n        src = self._format_local_path(fileinfo.src)\n        dest = self._format_s3_path(fileinfo.dest)\n        return src, dest\n\n\nclass DownloadRequestSubmitter(BaseTransferRequestSubmitter):\n    REQUEST_MAPPER_METHOD = RequestParamsMapper.map_get_object_params\n    RESULT_SUBSCRIBER_CLASS = DownloadResultSubscriber\n\n    def can_submit(self, fileinfo):\n        return fileinfo.operation_name == 'download'\n\n    def _add_additional_subscribers(self, subscribers, fileinfo):\n        subscribers.append(ProvideSizeSubscriber(fileinfo.size))\n        subscribers.append(DirectoryCreatorSubscriber())\n        subscribers.append(ProvideLastModifiedTimeSubscriber(\n            fileinfo.last_update, self._result_queue))\n        if self._cli_params.get('is_move', False):\n            subscribers.append(DeleteSourceObjectSubscriber(\n                fileinfo.source_client))\n\n    def _submit_transfer_request(self, fileinfo, extra_args, subscribers):\n        bucket, key = find_bucket_key(fileinfo.src)\n        fileout = self._get_fileout(fileinfo)\n        return self._transfer_manager.download(\n            fileobj=fileout, bucket=bucket, key=key,\n            extra_args=extra_args, subscribers=subscribers\n        )\n\n    def _get_fileout(self, fileinfo):\n        return fileinfo.dest\n\n    def _get_warning_handlers(self):\n        return [self._warn_glacier, self._warn_parent_reference]\n\n    def _format_src_dest(self, fileinfo):\n        src = self._format_s3_path(fileinfo.src)\n        dest = self._format_local_path(fileinfo.dest)\n        return src, dest\n\n\nclass CopyRequestSubmitter(BaseTransferRequestSubmitter):\n    REQUEST_MAPPER_METHOD = RequestParamsMapper.map_copy_object_params\n    RESULT_SUBSCRIBER_CLASS = CopyResultSubscriber\n\n    def can_submit(self, fileinfo):\n        return fileinfo.operation_name == 'copy'\n\n    def _add_additional_subscribers(self, subscribers, fileinfo):\n        subscribers.append(ProvideSizeSubscriber(fileinfo.size))\n        if self._should_inject_content_type():\n            subscribers.append(ProvideCopyContentTypeSubscriber())\n        if self._cli_params.get('is_move', False):\n            subscribers.append(DeleteCopySourceObjectSubscriber(\n                fileinfo.source_client))\n\n    def _submit_transfer_request(self, fileinfo, extra_args, subscribers):\n        bucket, key = find_bucket_key(fileinfo.dest)\n        source_bucket, source_key = find_bucket_key(fileinfo.src)\n        copy_source = {'Bucket': source_bucket, 'Key': source_key}\n        return self._transfer_manager.copy(\n            bucket=bucket, key=key, copy_source=copy_source,\n            extra_args=extra_args, subscribers=subscribers,\n            source_client=fileinfo.source_client\n        )\n\n    def _get_warning_handlers(self):\n        return [self._warn_glacier]\n\n    def _format_src_dest(self, fileinfo):\n        src = self._format_s3_path(fileinfo.src)\n        dest = self._format_s3_path(fileinfo.dest)\n        return src, dest\n\n\nclass UploadStreamRequestSubmitter(UploadRequestSubmitter):\n    RESULT_SUBSCRIBER_CLASS = UploadStreamResultSubscriber\n\n    def can_submit(self, fileinfo):\n        return (\n            fileinfo.operation_name == 'upload' and\n            self._cli_params.get('is_stream')\n        )\n\n    def _add_additional_subscribers(self, subscribers, fileinfo):\n        expected_size = self._cli_params.get('expected_size', None)\n        if expected_size is not None:\n            subscribers.append(ProvideSizeSubscriber(int(expected_size)))\n\n    def _get_filein(self, fileinfo):\n        binary_stdin = get_binary_stdin()\n        return NonSeekableStream(binary_stdin)\n\n    def _format_local_path(self, path):\n        return '-'\n\n\nclass DownloadStreamRequestSubmitter(DownloadRequestSubmitter):\n    RESULT_SUBSCRIBER_CLASS = DownloadStreamResultSubscriber\n\n    def can_submit(self, fileinfo):\n        return (\n            fileinfo.operation_name == 'download' and\n            self._cli_params.get('is_stream')\n        )\n\n    def _add_additional_subscribers(self, subscribers, fileinfo):\n        pass\n\n    def _get_fileout(self, fileinfo):\n        return StdoutBytesWriter()\n\n    def _format_local_path(self, path):\n        return '-'\n\n\nclass DeleteRequestSubmitter(BaseTransferRequestSubmitter):\n    REQUEST_MAPPER_METHOD = RequestParamsMapper.map_delete_object_params\n    RESULT_SUBSCRIBER_CLASS = DeleteResultSubscriber\n\n    def can_submit(self, fileinfo):\n        return fileinfo.operation_name == 'delete' and \\\n            fileinfo.src_type == 's3'\n\n    def _submit_transfer_request(self, fileinfo, extra_args, subscribers):\n        bucket, key = find_bucket_key(fileinfo.src)\n        return self._transfer_manager.delete(\n            bucket=bucket, key=key, extra_args=extra_args,\n            subscribers=subscribers)\n\n    def _format_src_dest(self, fileinfo):\n        return self._format_s3_path(fileinfo.src), None\n\n\nclass LocalDeleteRequestSubmitter(BaseTransferRequestSubmitter):\n    REQUEST_MAPPER_METHOD = None\n    RESULT_SUBSCRIBER_CLASS = None\n\n    def can_submit(self, fileinfo):\n        return fileinfo.operation_name == 'delete' and \\\n            fileinfo.src_type == 'local'\n\n    def _submit_transfer_request(self, fileinfo, extra_args, subscribers):\n        # This is quirky but essentially instead of relying on a built-in\n        # method of s3 transfer, the logic lives directly in the submitter.\n        # The reason a explicit delete local file does not\n        # live in s3transfer is because it is outside the scope of s3transfer;\n        # it should only have interfaces for interacting with S3. Therefore,\n        # the burden of this functionality should live in the CLI.\n\n        # The main downsides in doing this is that delete and the result\n        # creation happens in the main thread as opposed to a separate thread\n        # in s3transfer. However, this is not too big of a downside because\n        # deleting a local file only happens for sync --delete downloads and\n        # is very fast compared to all of the other types of transfers.\n        src, dest = self._format_src_dest(fileinfo)\n        result_kwargs = {\n            'transfer_type': 'delete',\n            'src': src,\n            'dest': dest\n        }\n        try:\n            self._result_queue.put(QueuedResult(\n                total_transfer_size=0, **result_kwargs))\n            os.remove(fileinfo.src)\n            self._result_queue.put(SuccessResult(**result_kwargs))\n        except Exception as e:\n            self._result_queue.put(\n                FailureResult(exception=e, **result_kwargs))\n        finally:\n            # Return True to indicate that the transfer was submitted\n            return True\n\n    def _format_src_dest(self, fileinfo):\n        return self._format_local_path(fileinfo.src), None\n", "awscli/customizations/s3/fileinfo.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nclass FileInfo(object):\n    \"\"\"This class contains important details related to performing a task.\n\n    It can perform operations such as ``upload``, ``download``, ``copy``,\n    ``delete``, ``move``.  Similarly to ``TaskInfo`` objects attributes\n    like ``session`` need to be set in order to perform operations.\n\n    :param dest: the destination path\n    :type dest: string\n    :param compare_key: the name of the file relative to the specified\n        directory/prefix.  This variable is used when performing syncing\n        or if the destination file is adopting the source file's name.\n    :type compare_key: string\n    :param size: The size of the file in bytes.\n    :type size: integer\n    :param last_update: the local time of last modification.\n    :type last_update: datetime object\n    :param dest_type: if the destination is s3 or local.\n    :param dest_type: string\n    :param parameters: a dictionary of important values this is assigned in\n        the ``BasicTask`` object.\n    :param associated_response_data: The response data used by\n        the ``FileGenerator`` to create this task. It is either an dictionary\n        from the list of a ListObjects or the response from a HeadObject. It\n        will only be filled if the task was generated from an S3 bucket.\n    \"\"\"\n    def __init__(self, src, dest=None, compare_key=None, size=None,\n                 last_update=None, src_type=None, dest_type=None,\n                 operation_name=None, client=None, parameters=None,\n                 source_client=None, is_stream=False,\n                 associated_response_data=None):\n        self.src = src\n        self.src_type = src_type\n        self.operation_name = operation_name\n        self.client = client\n        self.dest = dest\n        self.dest_type = dest_type\n        self.compare_key = compare_key\n        self.size = size\n        self.last_update = last_update\n        # Usually inject ``parameters`` from ``BasicTask`` class.\n        self.parameters = {}\n        if parameters is not None:\n            self.parameters = parameters\n        self.source_client = source_client\n        self.is_stream = is_stream\n        self.associated_response_data = associated_response_data\n\n    def is_glacier_compatible(self):\n        \"\"\"Determines if a file info object is glacier compatible\n\n        Operations will fail if the S3 object has a storage class of GLACIER\n        and it involves copying from S3 to S3, downloading from S3, or moving\n        where S3 is the source (the delete will actually succeed, but we do\n        not want fail to transfer the file and then successfully delete it).\n\n        :returns: True if the FileInfo's operation will not fail because the\n            operation is on a glacier object. False if it will fail.\n        \"\"\"\n        if self._is_glacier_object(self.associated_response_data):\n            if self.operation_name in ['copy', 'download']:\n                return False\n            elif self.operation_name == 'move':\n                if self.src_type == 's3':\n                    return False\n        return True\n\n    def _is_glacier_object(self, response_data):\n        glacier_storage_classes = ['GLACIER', 'DEEP_ARCHIVE']\n        if response_data:\n            if response_data.get('StorageClass') in glacier_storage_classes \\\n                    and not self._is_restored(response_data):\n                return True\n        return False\n\n    def _is_restored(self, response_data):\n        # Returns True is this is a glacier object that has been\n        # restored back to S3.\n        # 'Restore' looks like: 'ongoing-request=\"false\", expiry-date=\"...\"'\n        return 'ongoing-request=\"false\"' in response_data.get('Restore', '')\n", "awscli/customizations/s3/syncstrategy/register.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations.s3.syncstrategy.sizeonly import SizeOnlySync\nfrom awscli.customizations.s3.syncstrategy.exacttimestamps import \\\n    ExactTimestampsSync\nfrom awscli.customizations.s3.syncstrategy.delete import DeleteSync\n\n\ndef register_sync_strategy(session, strategy_cls,\n                           sync_type='file_at_src_and_dest'):\n    \"\"\"Registers a single sync strategy\n\n    :param session: The session that the sync strategy is being registered to.\n    :param strategy_cls: The class of the sync strategy to be registered.\n    :param sync_type: A string representing when to perform the sync strategy.\n        See ``__init__`` method of ``BaseSyncStrategy`` for possible options.\n    \"\"\"\n    strategy = strategy_cls(sync_type)\n    strategy.register_strategy(session)\n\n\ndef register_sync_strategies(command_table, session, **kwargs):\n    \"\"\"Registers the different sync strategies.\n\n    To register a sync strategy add\n    ``register_sync_strategy(session, YourSyncStrategyClass, sync_type)``\n    to the list of registered strategies in this function.\n    \"\"\"\n\n    # Register the size only sync strategy.\n    register_sync_strategy(session, SizeOnlySync)\n\n    # Register the exact timestamps sync strategy.\n    register_sync_strategy(session, ExactTimestampsSync)\n\n    # Register the delete sync strategy.\n    register_sync_strategy(session, DeleteSync, 'file_not_at_src')\n\n    # Register additional sync strategies here...\n", "awscli/customizations/s3/syncstrategy/exacttimestamps.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nfrom awscli.customizations.s3.syncstrategy.base import SizeAndLastModifiedSync\n\n\nLOG = logging.getLogger(__name__)\n\n\nEXACT_TIMESTAMPS = {'name': 'exact-timestamps', 'action': 'store_true',\n                    'help_text': (\n                        'When syncing from S3 to local, same-sized '\n                        'items will be ignored only when the timestamps '\n                        'match exactly. The default behavior is to ignore '\n                        'same-sized items unless the local version is newer '\n                        'than the S3 version.')}\n\n\nclass ExactTimestampsSync(SizeAndLastModifiedSync):\n\n    ARGUMENT = EXACT_TIMESTAMPS\n\n    def compare_time(self, src_file, dest_file):\n        src_time = src_file.last_update\n        dest_time = dest_file.last_update\n        delta = dest_time - src_time\n        cmd = src_file.operation_name\n        if cmd == 'download':\n            return self.total_seconds(delta) == 0\n        else:\n            return super(ExactTimestampsSync, self).compare_time(src_file,\n                                                                 dest_file)\n", "awscli/customizations/s3/syncstrategy/delete.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nfrom awscli.customizations.s3.syncstrategy.base import BaseSync\n\n\nLOG = logging.getLogger(__name__)\n\n\nDELETE = {'name': 'delete', 'action': 'store_true',\n          'help_text': (\n              \"Files that exist in the destination but not in the source are \"\n              \"deleted during sync. Note that files excluded by filters are \"\n              \"excluded from deletion.\")}\n\n\nclass DeleteSync(BaseSync):\n\n    ARGUMENT = DELETE\n\n    def determine_should_sync(self, src_file, dest_file):\n        dest_file.operation_name = 'delete'\n        LOG.debug(\"syncing: (None) -> %s (remove), file does not \"\n                  \"exist at source (%s) and delete mode enabled\",\n                  dest_file.src, dest_file.dest)\n        return True\n", "awscli/customizations/s3/syncstrategy/base.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\n\nLOG = logging.getLogger(__name__)\n\nVALID_SYNC_TYPES = ['file_at_src_and_dest', 'file_not_at_dest',\n                    'file_not_at_src']\n\n\nclass BaseSync(object):\n    \"\"\"Base sync strategy\n\n    To create a new sync strategy, subclass from this class.\n    \"\"\"\n\n    # This is the argument that will be added to the ``SyncCommand`` arg table.\n    # This argument will represent the sync strategy when the arguments for\n    # the sync command are parsed.  ``ARGUMENT`` follows the same format as\n    # a member of ``ARG_TABLE`` in ``BasicCommand`` class as specified in\n    # ``awscli/customizations/commands.py``.\n    #\n    # For example, if I wanted to perform the sync strategy whenever I type\n    # ``--my-sync-strategy``, I would say:\n    #\n    # ARGUMENT =\n    #     {'name': 'my-sync-strategy', 'action': 'store-true',\n    #      'help_text': 'Performs my sync strategy'}\n    #\n    # Typically, the argument's ``action`` should ``store_true`` to\n    # minimize amount of extra code in making a custom sync strategy.\n    ARGUMENT = None\n\n    # At this point all that need to be done is implement\n    # ``determine_should_sync`` method (see method for more information).\n\n    def __init__(self, sync_type='file_at_src_and_dest'):\n        \"\"\"\n        :type sync_type: string\n        :param sync_type: This determines where the sync strategy will be\n            used. There are three strings to choose from:\n\n            'file_at_src_and_dest': apply sync strategy on a file that\n            exists both at the source and the destination.\n\n            'file_not_at_dest': apply sync strategy on a file that\n            exists at the source but not the destination.\n\n            'file_not_at_src': apply sync strategy on a file that\n            exists at the destination but not the source.\n        \"\"\"\n        self._check_sync_type(sync_type)\n        self._sync_type = sync_type\n\n    def _check_sync_type(self, sync_type):\n        if sync_type not in VALID_SYNC_TYPES:\n            raise ValueError(\"Unknown sync_type: %s.\\n\"\n                             \"Valid options are %s.\" %\n                             (sync_type, VALID_SYNC_TYPES))\n\n    @property\n    def sync_type(self):\n        return self._sync_type\n\n    def register_strategy(self, session):\n        \"\"\"Registers the sync strategy class to the given session.\"\"\"\n\n        session.register('building-arg-table.sync',\n                         self.add_sync_argument)\n        session.register('choosing-s3-sync-strategy', self.use_sync_strategy)\n\n    def determine_should_sync(self, src_file, dest_file):\n        \"\"\"Subclasses should implement this method.\n\n        This function takes two ``FileStat`` objects (one from the source and\n        one from the destination).  Then makes a decision on whether a given\n        operation (e.g. a upload, copy, download) should be allowed\n        to take place.\n\n        The function currently raises a ``NotImplementedError``.  So this\n        method must be overwritten when this class is subclassed.  Note\n        that this method must return a Boolean as documented below.\n\n        :type src_file: ``FileStat`` object\n        :param src_file: A representation of the operation that is to be\n            performed on a specific file existing in the source.  Note if\n            the file does not exist at the source, ``src_file`` is None.\n\n        :type dest_file: ``FileStat`` object\n        :param dest_file: A representation of the operation that is to be\n            performed on a specific file existing in the destination. Note if\n            the file does not exist at the destination, ``dest_file`` is None.\n\n        :rtype: Boolean\n        :return: True if an operation based on the ``FileStat`` should be\n            allowed to occur.\n            False if if an operation based on the ``FileStat`` should not be\n            allowed to occur. Note the operation being referred to depends on\n            the ``sync_type`` of the sync strategy:\n\n            'file_at_src_and_dest': refers to ``src_file``\n\n            'file_not_at_dest': refers to ``src_file``\n\n            'file_not_at_src': refers to ``dest_file``\n         \"\"\"\n\n        raise NotImplementedError(\"determine_should_sync\")\n\n    @property\n    def arg_name(self):\n        # Retrieves the ``name`` of the sync strategy's ``ARGUMENT``.\n        name = None\n        if self.ARGUMENT is not None:\n            name = self.ARGUMENT.get('name', None)\n        return name\n\n    @property\n    def arg_dest(self):\n        # Retrieves the ``dest`` of the sync strategy's ``ARGUMENT``.\n        dest = None\n        if self.ARGUMENT is not None:\n            dest = self.ARGUMENT.get('dest', None)\n        return dest\n\n    def add_sync_argument(self, arg_table, **kwargs):\n        # This function adds sync strategy's argument to the ``SyncCommand``\n        # argument table.\n        if self.ARGUMENT is not None:\n            arg_table.append(self.ARGUMENT)\n\n    def use_sync_strategy(self, params, **kwargs):\n        # This function determines which sync strategy the ``SyncCommand`` will\n        # use. The sync strategy object must be returned by this method\n        # if it is to be chosen as the sync strategy to use.\n        #\n        # ``params`` is a dictionary that specifies all of the arguments\n        # the sync command is able to process as well as their values.\n        #\n        # Since ``ARGUMENT`` was added to the ``SyncCommand`` arg table,\n        # the argument will be present in ``params``.\n        #\n        # If the argument was included in the actual ``aws s3 sync`` command\n        # its value will show up as ``True`` in ``params`` otherwise its value\n        # will be ``False`` in ``params`` assuming the argument's ``action``\n        # is ``store_true``.\n        #\n        # Note: If the ``action`` of ``ARGUMENT`` was not set to\n        # ``store_true``, this method will need to be overwritten.\n        #\n        name_in_params = None\n        # Check if a ``dest`` was specified in ``ARGUMENT`` as if it is\n        # specified, the boolean value will be located at the argument's\n        # ``dest`` value in the ``params`` dictionary.\n        if self.arg_dest is not None:\n            name_in_params = self.arg_dest\n        # Then check ``name`` of ``ARGUMENT``, the boolean value will be\n        # located at the argument's ``name`` value in the ``params``\n        # dictionary.\n        elif self.arg_name is not None:\n            # ``name`` has all ``-`` replaced with ``_`` in ``params``.\n            name_in_params = self.arg_name.replace('-', '_')\n        if name_in_params is not None:\n            if params.get(name_in_params):\n                # Return the sync strategy object to be used for syncing.\n                return self\n        return None\n\n    def total_seconds(self, td):\n        \"\"\"\n        timedelta's time_seconds() function for python 2.6 users\n\n        :param td: The difference between two datetime objects.\n        \"\"\"\n        return (td.microseconds + (td.seconds + td.days * 24 *\n                                   3600) * 10**6) / 10**6\n\n    def compare_size(self, src_file, dest_file):\n        \"\"\"\n        :returns: True if the sizes are the same.\n            False otherwise.\n        \"\"\"\n        return src_file.size == dest_file.size\n\n    def compare_time(self, src_file, dest_file):\n        \"\"\"\n        :returns: True if the file does not need updating based on time of\n            last modification and type of operation.\n            False if the file does need updating based on the time of\n            last modification and type of operation.\n        \"\"\"\n        src_time = src_file.last_update\n        dest_time = dest_file.last_update\n        delta = dest_time - src_time\n        cmd = src_file.operation_name\n        if cmd == \"upload\" or cmd == \"copy\":\n            if self.total_seconds(delta) >= 0:\n                # Destination is newer than source.\n                return True\n            else:\n                # Destination is older than source, so\n                # we have a more recently updated file\n                # at the source location.\n                return False\n        elif cmd == \"download\":\n\n            if self.total_seconds(delta) <= 0:\n                return True\n            else:\n                # delta is positive, so the destination\n                # is newer than the source.\n                return False\n\n\nclass SizeAndLastModifiedSync(BaseSync):\n\n    def determine_should_sync(self, src_file, dest_file):\n        same_size = self.compare_size(src_file, dest_file)\n        same_last_modified_time = self.compare_time(src_file, dest_file)\n        should_sync = (not same_size) or (not same_last_modified_time)\n        if should_sync:\n            LOG.debug(\n                \"syncing: %s -> %s, size: %s -> %s, modified time: %s -> %s\",\n                src_file.src, src_file.dest,\n                src_file.size, dest_file.size,\n                src_file.last_update, dest_file.last_update)\n        return should_sync\n\n\nclass NeverSync(BaseSync):\n    def __init__(self, sync_type='file_not_at_src'):\n        super(NeverSync, self).__init__(sync_type)\n\n    def determine_should_sync(self, src_file, dest_file):\n        return False\n\n\nclass MissingFileSync(BaseSync):\n    def __init__(self, sync_type='file_not_at_dest'):\n        super(MissingFileSync, self).__init__(sync_type)\n\n    def determine_should_sync(self, src_file, dest_file):\n        LOG.debug(\"syncing: %s -> %s, file does not exist at destination\",\n                  src_file.src, src_file.dest)\n        return True\n", "awscli/customizations/s3/syncstrategy/sizeonly.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nfrom awscli.customizations.s3.syncstrategy.base import BaseSync\n\n\nLOG = logging.getLogger(__name__)\n\n\nSIZE_ONLY = {'name': 'size-only', 'action': 'store_true',\n             'help_text': (\n                 'Makes the size of each key the only criteria used to '\n                 'decide whether to sync from source to destination.')}\n\n\nclass SizeOnlySync(BaseSync):\n\n    ARGUMENT = SIZE_ONLY\n\n    def determine_should_sync(self, src_file, dest_file):\n        same_size = self.compare_size(src_file, dest_file)\n        should_sync = not same_size\n        if should_sync:\n            LOG.debug(\"syncing: %s -> %s, size_changed: %s\",\n                      src_file.src, src_file.dest, not same_size)\n        return should_sync\n", "awscli/customizations/s3/syncstrategy/__init__.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "awscli/customizations/logs/startlivetail.py": "# Copyright 2024 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom functools import partial\nfrom threading import Thread\nimport contextlib\nimport signal\nimport sys\nimport time\n\nfrom awscli.compat import get_stdout_text_writer\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.utils import is_a_tty\n\n\nDESCRIPTION = (\n    \"Starts a Live Tail streaming session for one or more log groups. \"\n    \"A Live Tail session provides a near real-time streaming of \"\n    \"log events as they are ingested into selected log groups. \"\n    \"A session can go on for a maximum of 3 hours.\\n\\n\"\n    \"You must have logs:StartLiveTail permission to perform this operation. \"\n    \"If the log events matching the filters are more than 500 events per second, \"\n    \"we sample the events to provide the real-time tailing experience.\\n\\n\"\n    \"If you are using CloudWatch cross-account observability, \"\n    \"you can use this operation in a monitoring account and start tailing on \"\n    \"Log Group(s) present in the linked source accounts. \"\n    \"For more information, see \"\n    \"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html.\\n\\n\"\n    \"Live Tail sessions incur charges by session usage time, per minute. \"\n    \"For pricing details, please refer to \"\n    \"https://aws.amazon.com/cloudwatch/pricing/.\"\n)\n\nLIST_SCHEMA = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n\nLOG_GROUP_IDENTIFIERS = {\n    \"name\": \"log-group-identifiers\",\n    \"required\": True,\n    \"positional_arg\": False,\n    \"nargs\": \"+\",\n    \"schema\": LIST_SCHEMA,\n    \"help_text\": (\n        \"The Log Group Identifiers are the ARNs for the CloudWatch Logs groups to tail. \"\n        \"You can provide up to 10 Log Group Identifiers.\\n\\n\"\n        \"Logs can be filtered by Log Stream(s) by providing  \"\n        \"--log-stream-names or --log-stream-name-prefixes. \"\n        \"If more than one Log Group is provided \"\n        \"--log-stream-names and --log-stream-name-prefixes  is disabled. \"\n        \"--log-stream-names and --log-stream-name-prefixes can't be provided simultaneously.\\n\\n\"\n        \"Note -  The Log Group ARN must be in the following format. \"\n        \"Replace REGION and ACCOUNT_ID with your Region and account ID. \"\n        \"``arn:aws:logs:REGION :ACCOUNT_ID :log-group:LOG_GROUP_NAME``. \"\n        \"A ``:*`` after the ARN is prohibited.\"\n        \"For more information about ARN format, \"\n        'see <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/iam-access-control-overview-cwl.html\">CloudWatch Logs resources and operations</a>.'\n    ),\n}\n\nLOG_STREAM_NAMES = {\n    \"name\": \"log-stream-names\",\n    \"positional_arg\": False,\n    \"nargs\": \"+\",\n    \"schema\": LIST_SCHEMA,\n    \"help_text\": (\n        \"The list of stream names to filter logs by.\\n\\n This parameter cannot be \"\n        \"specified when --log-stream-name-prefixes are also specified. \"\n        \"This parameter cannot be specified when multiple log-group-identifiers are specified\"\n    ),\n}\n\nLOG_STREAM_NAME_PREFIXES = {\n    \"name\": \"log-stream-name-prefixes\",\n    \"positional_arg\": False,\n    \"nargs\": \"+\",\n    \"schema\": LIST_SCHEMA,\n    \"help_text\": (\n        \"The prefix to filter logs by. Only events from log streams with names beginning \"\n        \"with this prefix will be returned. \\n\\nThis parameter cannot be specified when \"\n        \"--log-stream-names is also specified. This parameter cannot be specified when \"\n        \"multiple log-group-identifiers are specified\"\n    ),\n}\n\nLOG_EVENT_FILTER_PATTERN = {\n    \"name\": \"log-event-filter-pattern\",\n    \"positional_arg\": False,\n    \"cli_type_name\": \"string\",\n    \"help_text\": (\n        \"The filter pattern to use. \"\n        'See <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">Filter and Pattern Syntax</a> '\n        \"for details. If not provided, all the events are matched. \"\n        \"This option can be used to include or exclude log events patterns.  \"\n        \"Additionally, when multiple filter patterns are provided, they must be encapsulated by quotes.\"\n    ),\n}\n\n\ndef signal_handler(printer, signum, frame):\n    printer.interrupt_session = True\n\n\n@contextlib.contextmanager\ndef handle_signal(printer):\n    signal_list = [signal.SIGINT, signal.SIGTERM]\n    if sys.platform != \"win32\":\n        signal_list.append(signal.SIGPIPE)\n    actual_signals = []\n    for user_signal in signal_list:\n        actual_signals.append(\n            signal.signal(user_signal, partial(signal_handler, printer))\n        )\n    try:\n        yield\n    finally:\n        for sig, user_signal in enumerate(signal_list):\n            signal.signal(user_signal, actual_signals[sig])\n\n\nclass LiveTailSessionMetadata:\n    def __init__(self) -> None:\n        self._session_start_time = time.time()\n        self._is_sampled = False\n\n    @property\n    def session_start_time(self):\n        return self._session_start_time\n\n    @property\n    def is_sampled(self):\n        return self._is_sampled\n\n    def update_metadata(self, session_metadata):\n        self._is_sampled = session_metadata[\"sampled\"]\n\n\nclass PrintOnlyPrinter:\n    def __init__(self, output, log_events) -> None:\n        self._output = output\n        self._log_events = log_events\n        self.interrupt_session = False\n\n    def _print_log_events(self):\n        for log_event in self._log_events:\n            self._output.write(log_event + \"\\n\")\n            self._output.flush()\n\n        self._log_events.clear()\n\n    def run(self):\n        try:\n            while True:\n                self._print_log_events()\n\n                if self.interrupt_session:\n                    break\n\n                time.sleep(1)\n        except (BrokenPipeError, KeyboardInterrupt):\n            pass\n\n\nclass PrintOnlyUI:\n    def __init__(self, output, log_events) -> None:\n        self._log_events = log_events\n        self._printer = PrintOnlyPrinter(output, self._log_events)\n\n    def exit(self):\n        self._printer.interrupt_session = True\n\n    def run(self):\n        with handle_signal(self._printer):\n            self._printer.run()\n\n\nclass LiveTailLogEventsCollector(Thread):\n    def __init__(\n        self,\n        output,\n        ui,\n        response_stream,\n        log_events: list,\n        session_metadata: LiveTailSessionMetadata,\n    ) -> None:\n        super().__init__()\n        self._output = output\n        self._ui = ui\n        self._response_stream = response_stream\n        self._log_events = log_events\n        self._session_metadata = session_metadata\n        self._exception = None\n\n    def _collect_log_events(self):\n        try:\n            for event in self._response_stream:\n                if not \"sessionUpdate\" in event:\n                    continue\n\n                session_update = event[\"sessionUpdate\"]\n                self._session_metadata.update_metadata(\n                    session_update[\"sessionMetadata\"]\n                )\n                logEvents = session_update[\"sessionResults\"]\n                for logEvent in logEvents:\n                    self._log_events.append(logEvent[\"message\"])\n        except Exception as e:\n            self._exception = e\n\n        self._ui.exit()\n\n    def stop(self):\n        if self._exception is not None:\n            self._output.write(str(self._exception) + \"\\n\")\n            self._output.flush()\n\n    def run(self):\n        self._collect_log_events()\n\n\nclass StartLiveTailCommand(BasicCommand):\n    NAME = \"start-live-tail\"\n    DESCRIPTION = DESCRIPTION\n    ARG_TABLE = [\n        LOG_GROUP_IDENTIFIERS,\n        LOG_STREAM_NAMES,\n        LOG_STREAM_NAME_PREFIXES,\n        LOG_EVENT_FILTER_PATTERN,\n    ]\n\n    def __init__(self, session):\n        super(StartLiveTailCommand, self).__init__(session)\n        self._output = get_stdout_text_writer()\n\n    def _get_client(self, parsed_globals):\n        return self._session.create_client(\n            \"logs\",\n            region_name=parsed_globals.region,\n            endpoint_url=parsed_globals.endpoint_url,\n            verify=parsed_globals.verify_ssl,\n        )\n\n    def _get_start_live_tail_kwargs(self, parsed_args):\n        kwargs = {\"logGroupIdentifiers\": parsed_args.log_group_identifiers}\n\n        if parsed_args.log_stream_names is not None:\n            kwargs[\"logStreamNames\"] = parsed_args.log_stream_names\n        if parsed_args.log_stream_name_prefixes is not None:\n            kwargs[\"logStreamNamePrefixes\"] = parsed_args.log_stream_name_prefixes\n        if parsed_args.log_event_filter_pattern is not None:\n            kwargs[\"logEventFilterPattern\"] = parsed_args.log_event_filter_pattern\n\n        return kwargs\n\n    def _is_color_allowed(self, color):\n        if color == \"on\":\n            return True\n        elif color == \"off\":\n            return False\n        return is_a_tty()\n\n    def _run_main(self, parsed_args, parsed_globals):\n        self._client = self._get_client(parsed_globals)\n\n        start_live_tail_kwargs = self._get_start_live_tail_kwargs(parsed_args)\n        response = self._client.start_live_tail(**start_live_tail_kwargs)\n\n        log_events = []\n        session_metadata = LiveTailSessionMetadata()\n\n        ui = PrintOnlyUI(self._output, log_events)\n\n        log_events_collector = LiveTailLogEventsCollector(\n            self._output, ui, response[\"responseStream\"], log_events, session_metadata\n        )\n        log_events_collector.daemon = True\n\n        log_events_collector.start()\n        ui.run()\n\n        log_events_collector.stop()\n        sys.exit(0)\n", "awscli/customizations/logs/__init__.py": "from awscli.customizations.logs.startlivetail import StartLiveTailCommand\n\n\ndef register_logs_commands(cli):\n    cli.register('building-command-table.logs', inject_start_live_tail_command)\n\n\ndef inject_start_live_tail_command(command_table, session, **kwargs):\n    command_table['start-live-tail'] = StartLiveTailCommand(session)", "awscli/customizations/cloudformation/yamlhelper.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.compat import json\nfrom botocore.compat import OrderedDict\n\nimport yaml\nfrom yaml.resolver import ScalarNode, SequenceNode\n\n\ndef intrinsics_multi_constructor(loader, tag_prefix, node):\n    \"\"\"\n    YAML constructor to parse CloudFormation intrinsics.\n    This will return a dictionary with key being the intrinsic name\n    \"\"\"\n\n    # Get the actual tag name excluding the first exclamation\n    tag = node.tag[1:]\n\n    # Some intrinsic functions doesn't support prefix \"Fn::\"\n    prefix = \"Fn::\"\n    if tag in [\"Ref\", \"Condition\"]:\n        prefix = \"\"\n\n    cfntag = prefix + tag\n\n    if tag == \"GetAtt\" and isinstance(node.value, str):\n        # ShortHand notation for !GetAtt accepts Resource.Attribute format\n        # while the standard notation is to use an array\n        # [Resource, Attribute]. Convert shorthand to standard format\n        value = node.value.split(\".\", 1)\n\n    elif isinstance(node, ScalarNode):\n        # Value of this node is scalar\n        value = loader.construct_scalar(node)\n\n    elif isinstance(node, SequenceNode):\n        # Value of this node is an array (Ex: [1,2])\n        value = loader.construct_sequence(node)\n\n    else:\n        # Value of this node is an mapping (ex: {foo: bar})\n        value = loader.construct_mapping(node)\n\n    return {cfntag: value}\n\n\ndef _dict_representer(dumper, data):\n    return dumper.represent_dict(data.items())\n\n\ndef yaml_dump(dict_to_dump):\n    \"\"\"\n    Dumps the dictionary as a YAML document\n    :param dict_to_dump:\n    :return:\n    \"\"\"\n    FlattenAliasDumper.add_representer(OrderedDict, _dict_representer)\n    return yaml.dump(\n        dict_to_dump,\n        default_flow_style=False,\n        Dumper=FlattenAliasDumper,\n    )\n\n\ndef _dict_constructor(loader, node):\n    # Necessary in order to make yaml merge tags work\n    loader.flatten_mapping(node)\n    return OrderedDict(loader.construct_pairs(node))\n\n\nclass SafeLoaderWrapper(yaml.SafeLoader):\n    \"\"\"Isolated safe loader to allow for customizations without global changes.\n    \"\"\"\n\n    pass\n\ndef yaml_parse(yamlstr):\n    \"\"\"Parse a yaml string\"\"\"\n    try:\n        # PyYAML doesn't support json as well as it should, so if the input\n        # is actually just json it is better to parse it with the standard\n        # json parser.\n        return json.loads(yamlstr, object_pairs_hook=OrderedDict)\n    except ValueError:\n        loader = SafeLoaderWrapper\n        loader.add_constructor(yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG, \n                               _dict_constructor)\n        loader.add_multi_constructor(\"!\", intrinsics_multi_constructor)\n        return yaml.load(yamlstr, loader)\n\n\nclass FlattenAliasDumper(yaml.SafeDumper):\n    def ignore_aliases(self, data):\n        return True\n", "awscli/customizations/cloudformation/exceptions.py": "\nclass CloudFormationCommandError(Exception):\n    fmt = 'An unspecified error occurred'\n\n    def __init__(self, **kwargs):\n        msg = self.fmt.format(**kwargs)\n        Exception.__init__(self, msg)\n        self.kwargs = kwargs\n\n\nclass InvalidTemplatePathError(CloudFormationCommandError):\n    fmt = \"Invalid template path {template_path}\"\n\n\nclass ChangeEmptyError(CloudFormationCommandError):\n    fmt = \"No changes to deploy. Stack {stack_name} is up to date\"\n\n\nclass InvalidLocalPathError(CloudFormationCommandError):\n    fmt = (\"Parameter {property_name} of resource {resource_id} refers \"\n           \"to a file or folder that does not exist {local_path}\")\n\n\nclass InvalidTemplateUrlParameterError(CloudFormationCommandError):\n    fmt = (\"{property_name} parameter of {resource_id} resource is invalid. \"\n           \"It must be a S3 URL or path to CloudFormation \"\n           \"template file. Actual: {template_path}\")\n\n\nclass ExportFailedError(CloudFormationCommandError):\n    fmt = (\"Unable to upload artifact {property_value} referenced \"\n           \"by {property_name} parameter of {resource_id} resource.\"\n           \"\\n\"\n           \"{ex}\")\n\n\nclass InvalidKeyValuePairArgumentError(CloudFormationCommandError):\n    fmt = (\"{value} value passed to --{argname} must be of format \"\n           \"Key=Value\")\n\n\nclass DeployFailedError(CloudFormationCommandError):\n    fmt = \\\n        (\"Failed to create/update the stack. Run the following command\"\n         \"\\n\"\n         \"to fetch the list of events leading up to the failure\"\n         \"\\n\"\n         \"aws cloudformation describe-stack-events --stack-name {stack_name}\")\n\nclass DeployBucketRequiredError(CloudFormationCommandError):\n    fmt = \\\n        (\"Templates with a size greater than 51,200 bytes must be deployed \"\n         \"via an S3 Bucket. Please add the --s3-bucket parameter to your \"\n         \"command. The local template will be copied to that S3 bucket and \"\n         \"then deployed.\")\n\n\nclass InvalidForEachIntrinsicFunctionError(CloudFormationCommandError):\n    fmt = 'The value of {resource_id} has an invalid \"Fn::ForEach::\" format: Must be a list of three entries'\n", "awscli/customizations/cloudformation/artifact_exporter.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\nimport os\nimport tempfile\nimport zipfile\nimport contextlib\nimport uuid\nimport shutil\nfrom botocore.utils import set_value_from_jmespath\n\nfrom awscli.compat import urlparse\nfrom contextlib import contextmanager\nfrom awscli.customizations.cloudformation import exceptions\nfrom awscli.customizations.cloudformation.yamlhelper import yaml_dump, \\\n    yaml_parse\nimport jmespath\n\n\nLOG = logging.getLogger(__name__)\n\n\ndef is_path_value_valid(path):\n    return isinstance(path, str)\n\n\ndef make_abs_path(directory, path):\n    if is_path_value_valid(path) and not os.path.isabs(path):\n        return os.path.normpath(os.path.join(directory, path))\n    else:\n        return path\n\n\ndef is_s3_url(url):\n    try:\n        parse_s3_url(url)\n        return True\n    except ValueError:\n        return False\n\n\ndef is_local_folder(path):\n    return is_path_value_valid(path) and os.path.isdir(path)\n\n\ndef is_local_file(path):\n    return is_path_value_valid(path) and os.path.isfile(path)\n\n\ndef is_zip_file(path):\n    return (\n        is_path_value_valid(path) and\n        zipfile.is_zipfile(path))\n\n\ndef parse_s3_url(url,\n                 bucket_name_property=\"Bucket\",\n                 object_key_property=\"Key\",\n                 version_property=None):\n\n    if isinstance(url, str) \\\n            and url.startswith(\"s3://\"):\n\n        # Python < 2.7.10 don't parse query parameters from URI with custom\n        # scheme such as s3://blah/blah. As a workaround, remove scheme\n        # altogether to trigger the parser \"s3://foo/bar?v=1\" =>\"//foo/bar?v=1\"\n        parsed = urlparse.urlparse(url[3:])\n        query = urlparse.parse_qs(parsed.query)\n\n        if parsed.netloc and parsed.path:\n            result = dict()\n            result[bucket_name_property] = parsed.netloc\n            result[object_key_property] = parsed.path.lstrip('/')\n\n            # If there is a query string that has a single versionId field,\n            # set the object version and return\n            if version_property is not None \\\n                    and 'versionId' in query \\\n                    and len(query['versionId']) == 1:\n                result[version_property] = query['versionId'][0]\n\n            return result\n\n    raise ValueError(\"URL given to the parse method is not a valid S3 url \"\n                     \"{0}\".format(url))\n\n\ndef upload_local_artifacts(resource_id, resource_dict, property_name,\n                           parent_dir, uploader):\n    \"\"\"\n    Upload local artifacts referenced by the property at given resource and\n    return S3 URL of the uploaded object. It is the responsibility of callers\n    to ensure property value is a valid string\n\n    If path refers to a file, this method will upload the file. If path refers\n    to a folder, this method will zip the folder and upload the zip to S3.\n    If path is omitted, this method will zip the current working folder and\n    upload.\n\n    If path is already a path to S3 object, this method does nothing.\n\n    :param resource_id:     Id of the CloudFormation resource\n    :param resource_dict:   Dictionary containing resource definition\n    :param property_name:   Property name of CloudFormation resource where this\n                            local path is present\n    :param parent_dir:      Resolve all relative paths with respect to this\n                            directory\n    :param uploader:        Method to upload files to S3\n\n    :return:                S3 URL of the uploaded object\n    :raise:                 ValueError if path is not a S3 URL or a local path\n    \"\"\"\n\n    local_path = jmespath.search(property_name, resource_dict)\n\n    if local_path is None:\n        # Build the root directory and upload to S3\n        local_path = parent_dir\n\n    if is_s3_url(local_path):\n        # A valid CloudFormation template will specify artifacts as S3 URLs.\n        # This check is supporting the case where your resource does not\n        # refer to local artifacts\n        # Nothing to do if property value is an S3 URL\n        LOG.debug(\"Property {0} of {1} is already a S3 URL\"\n                  .format(property_name, resource_id))\n        return local_path\n\n    local_path = make_abs_path(parent_dir, local_path)\n\n    # Or, pointing to a folder. Zip the folder and upload\n    if is_local_folder(local_path):\n        return zip_and_upload(local_path, uploader)\n\n    # Path could be pointing to a file. Upload the file\n    elif is_local_file(local_path):\n        return uploader.upload_with_dedup(local_path)\n\n    raise exceptions.InvalidLocalPathError(\n            resource_id=resource_id,\n            property_name=property_name,\n            local_path=local_path)\n\n\ndef zip_and_upload(local_path, uploader):\n    with zip_folder(local_path) as zipfile:\n            return uploader.upload_with_dedup(zipfile)\n\n\n@contextmanager\ndef zip_folder(folder_path):\n    \"\"\"\n    Zip the entire folder and return a file to the zip. Use this inside\n    a \"with\" statement to cleanup the zipfile after it is used.\n\n    :param folder_path:\n    :return: Name of the zipfile\n    \"\"\"\n\n    filename = os.path.join(\n        tempfile.gettempdir(), \"data-\" + uuid.uuid4().hex)\n\n    zipfile_name = make_zip(filename, folder_path)\n    try:\n        yield zipfile_name\n    finally:\n        if os.path.exists(zipfile_name):\n            os.remove(zipfile_name)\n\n\ndef make_zip(filename, source_root):\n    zipfile_name = \"{0}.zip\".format(filename)\n    source_root = os.path.abspath(source_root)\n    with open(zipfile_name, 'wb') as f:\n        zip_file = zipfile.ZipFile(f, 'w', zipfile.ZIP_DEFLATED)\n        with contextlib.closing(zip_file) as zf:\n            for root, dirs, files in os.walk(source_root, followlinks=True):\n                for filename in files:\n                    full_path = os.path.join(root, filename)\n                    relative_path = os.path.relpath(\n                        full_path, source_root)\n                    zf.write(full_path, relative_path)\n\n    return zipfile_name\n\n\n@contextmanager\ndef mktempfile():\n    directory = tempfile.gettempdir()\n    filename = os.path.join(directory, uuid.uuid4().hex)\n\n    try:\n        with open(filename, \"w+\") as handle:\n            yield handle\n    finally:\n        if os.path.exists(filename):\n            os.remove(filename)\n\n\ndef copy_to_temp_dir(filepath):\n    tmp_dir = tempfile.mkdtemp()\n    dst = os.path.join(tmp_dir, os.path.basename(filepath))\n    shutil.copy(filepath, dst)\n    return tmp_dir\n\n\nclass Resource(object):\n    \"\"\"\n    Base class representing a CloudFormation resource that can be exported\n    \"\"\"\n\n    RESOURCE_TYPE = None\n    PROPERTY_NAME = None\n    PACKAGE_NULL_PROPERTY = True\n    # Set this property to True in base class if you want the exporter to zip\n    # up the file before uploading This is useful for Lambda functions.\n    FORCE_ZIP = False\n\n    def __init__(self, uploader):\n        self.uploader = uploader\n\n    def export(self, resource_id, resource_dict, parent_dir):\n        if resource_dict is None:\n            return\n\n        property_value = jmespath.search(self.PROPERTY_NAME, resource_dict)\n\n        if not property_value and not self.PACKAGE_NULL_PROPERTY:\n            return\n\n        if isinstance(property_value, dict):\n            LOG.debug(\"Property {0} of {1} resource is not a URL\"\n                      .format(self.PROPERTY_NAME, resource_id))\n            return\n\n        # If property is a file but not a zip file, place file in temp\n        # folder and send the temp folder to be zipped\n        temp_dir = None\n        if is_local_file(property_value) and not \\\n                is_zip_file(property_value) and self.FORCE_ZIP:\n            temp_dir = copy_to_temp_dir(property_value)\n            set_value_from_jmespath(resource_dict, self.PROPERTY_NAME, temp_dir)\n\n        try:\n            self.do_export(resource_id, resource_dict, parent_dir)\n\n        except Exception as ex:\n            LOG.debug(\"Unable to export\", exc_info=ex)\n            raise exceptions.ExportFailedError(\n                    resource_id=resource_id,\n                    property_name=self.PROPERTY_NAME,\n                    property_value=property_value,\n                    ex=ex)\n        finally:\n            if temp_dir:\n                shutil.rmtree(temp_dir)\n\n    def do_export(self, resource_id, resource_dict, parent_dir):\n        \"\"\"\n        Default export action is to upload artifacts and set the property to\n        S3 URL of the uploaded object\n        \"\"\"\n        uploaded_url = upload_local_artifacts(resource_id, resource_dict,\n                                   self.PROPERTY_NAME,\n                                   parent_dir, self.uploader)\n        set_value_from_jmespath(resource_dict, self.PROPERTY_NAME, uploaded_url)\n\n\nclass ResourceWithS3UrlDict(Resource):\n    \"\"\"\n    Represents CloudFormation resources that need the S3 URL to be specified as\n    an dict like {Bucket: \"\", Key: \"\", Version: \"\"}\n    \"\"\"\n\n    BUCKET_NAME_PROPERTY = None\n    OBJECT_KEY_PROPERTY = None\n    VERSION_PROPERTY = None\n\n    def __init__(self, uploader):\n        super(ResourceWithS3UrlDict, self).__init__(uploader)\n\n    def do_export(self, resource_id, resource_dict, parent_dir):\n        \"\"\"\n        Upload to S3 and set property to an dict representing the S3 url\n        of the uploaded object\n        \"\"\"\n\n        artifact_s3_url = \\\n            upload_local_artifacts(resource_id, resource_dict,\n                                   self.PROPERTY_NAME,\n                                   parent_dir, self.uploader)\n\n        parsed_url = parse_s3_url(\n                artifact_s3_url,\n                bucket_name_property=self.BUCKET_NAME_PROPERTY,\n                object_key_property=self.OBJECT_KEY_PROPERTY,\n                version_property=self.VERSION_PROPERTY)\n        set_value_from_jmespath(resource_dict, self.PROPERTY_NAME, parsed_url)\n\n\nclass ServerlessFunctionResource(Resource):\n    RESOURCE_TYPE = \"AWS::Serverless::Function\"\n    PROPERTY_NAME = \"CodeUri\"\n    FORCE_ZIP = True\n\n\nclass ServerlessApiResource(Resource):\n    RESOURCE_TYPE = \"AWS::Serverless::Api\"\n    PROPERTY_NAME = \"DefinitionUri\"\n    # Don't package the directory if DefinitionUri is omitted.\n    # Necessary to support DefinitionBody\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass GraphQLSchemaResource(Resource):\n    RESOURCE_TYPE = \"AWS::AppSync::GraphQLSchema\"\n    PROPERTY_NAME = \"DefinitionS3Location\"\n    # Don't package the directory if DefinitionS3Location is omitted.\n    # Necessary to support Definition\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass AppSyncResolverRequestTemplateResource(Resource):\n    RESOURCE_TYPE = \"AWS::AppSync::Resolver\"\n    PROPERTY_NAME = \"RequestMappingTemplateS3Location\"\n    # Don't package the directory if RequestMappingTemplateS3Location is omitted.\n    # Necessary to support RequestMappingTemplate\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass AppSyncResolverResponseTemplateResource(Resource):\n    RESOURCE_TYPE = \"AWS::AppSync::Resolver\"\n    PROPERTY_NAME = \"ResponseMappingTemplateS3Location\"\n    # Don't package the directory if ResponseMappingTemplateS3Location is omitted.\n    # Necessary to support ResponseMappingTemplate\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass AppSyncFunctionConfigurationRequestTemplateResource(Resource):\n    RESOURCE_TYPE = \"AWS::AppSync::FunctionConfiguration\"\n    PROPERTY_NAME = \"RequestMappingTemplateS3Location\"\n    # Don't package the directory if RequestMappingTemplateS3Location is omitted.\n    # Necessary to support RequestMappingTemplate\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass AppSyncFunctionConfigurationResponseTemplateResource(Resource):\n    RESOURCE_TYPE = \"AWS::AppSync::FunctionConfiguration\"\n    PROPERTY_NAME = \"ResponseMappingTemplateS3Location\"\n    # Don't package the directory if ResponseMappingTemplateS3Location is omitted.\n    # Necessary to support ResponseMappingTemplate\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass LambdaFunctionResource(ResourceWithS3UrlDict):\n    RESOURCE_TYPE = \"AWS::Lambda::Function\"\n    PROPERTY_NAME = \"Code\"\n    BUCKET_NAME_PROPERTY = \"S3Bucket\"\n    OBJECT_KEY_PROPERTY = \"S3Key\"\n    VERSION_PROPERTY = \"S3ObjectVersion\"\n    FORCE_ZIP = True\n\n\nclass ApiGatewayRestApiResource(ResourceWithS3UrlDict):\n    RESOURCE_TYPE = \"AWS::ApiGateway::RestApi\"\n    PROPERTY_NAME = \"BodyS3Location\"\n    PACKAGE_NULL_PROPERTY = False\n    BUCKET_NAME_PROPERTY = \"Bucket\"\n    OBJECT_KEY_PROPERTY = \"Key\"\n    VERSION_PROPERTY = \"Version\"\n\n\nclass ElasticBeanstalkApplicationVersion(ResourceWithS3UrlDict):\n    RESOURCE_TYPE = \"AWS::ElasticBeanstalk::ApplicationVersion\"\n    PROPERTY_NAME = \"SourceBundle\"\n    BUCKET_NAME_PROPERTY = \"S3Bucket\"\n    OBJECT_KEY_PROPERTY = \"S3Key\"\n    VERSION_PROPERTY = None\n\n\nclass LambdaLayerVersionResource(ResourceWithS3UrlDict):\n    RESOURCE_TYPE = \"AWS::Lambda::LayerVersion\"\n    PROPERTY_NAME = \"Content\"\n    BUCKET_NAME_PROPERTY = \"S3Bucket\"\n    OBJECT_KEY_PROPERTY = \"S3Key\"\n    VERSION_PROPERTY = \"S3ObjectVersion\"\n    FORCE_ZIP = True\n\n\nclass ServerlessLayerVersionResource(Resource):\n    RESOURCE_TYPE = \"AWS::Serverless::LayerVersion\"\n    PROPERTY_NAME = \"ContentUri\"\n    FORCE_ZIP = True\n\n\nclass ServerlessRepoApplicationReadme(Resource):\n    RESOURCE_TYPE = \"AWS::ServerlessRepo::Application\"\n    PROPERTY_NAME = \"ReadmeUrl\"\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass ServerlessRepoApplicationLicense(Resource):\n    RESOURCE_TYPE = \"AWS::ServerlessRepo::Application\"\n    PROPERTY_NAME = \"LicenseUrl\"\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass StepFunctionsStateMachineDefinitionResource(ResourceWithS3UrlDict):\n    RESOURCE_TYPE = \"AWS::StepFunctions::StateMachine\"\n    PROPERTY_NAME = \"DefinitionS3Location\"\n    BUCKET_NAME_PROPERTY = \"Bucket\"\n    OBJECT_KEY_PROPERTY = \"Key\"\n    VERSION_PROPERTY = \"Version\"\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass ServerlessStateMachineDefinitionResource(ResourceWithS3UrlDict):\n    RESOURCE_TYPE = \"AWS::Serverless::StateMachine\"\n    PROPERTY_NAME = \"DefinitionUri\"\n    BUCKET_NAME_PROPERTY = \"Bucket\"\n    OBJECT_KEY_PROPERTY = \"Key\"\n    VERSION_PROPERTY = \"Version\"\n    PACKAGE_NULL_PROPERTY = False\n\n\nclass CloudFormationStackResource(Resource):\n    \"\"\"\n    Represents CloudFormation::Stack resource that can refer to a nested\n    stack template via TemplateURL property.\n    \"\"\"\n    RESOURCE_TYPE = \"AWS::CloudFormation::Stack\"\n    PROPERTY_NAME = \"TemplateURL\"\n\n    def __init__(self, uploader):\n        super(CloudFormationStackResource, self).__init__(uploader)\n\n    def do_export(self, resource_id, resource_dict, parent_dir):\n        \"\"\"\n        If the nested stack template is valid, this method will\n        export on the nested template, upload the exported template to S3\n        and set property to URL of the uploaded S3 template\n        \"\"\"\n\n        template_path = resource_dict.get(self.PROPERTY_NAME, None)\n\n        if template_path is None or is_s3_url(template_path) or \\\n                template_path.startswith(\"http://\") or \\\n                template_path.startswith(\"https://\"):\n            # Nothing to do\n            return\n\n        abs_template_path = make_abs_path(parent_dir, template_path)\n        if not is_local_file(abs_template_path):\n            raise exceptions.InvalidTemplateUrlParameterError(\n                    property_name=self.PROPERTY_NAME,\n                    resource_id=resource_id,\n                    template_path=abs_template_path)\n\n        exported_template_dict = \\\n            Template(template_path, parent_dir, self.uploader).export()\n\n        exported_template_str = yaml_dump(exported_template_dict)\n\n        with mktempfile() as temporary_file:\n            temporary_file.write(exported_template_str)\n            temporary_file.flush()\n\n            url = self.uploader.upload_with_dedup(\n                    temporary_file.name, \"template\")\n\n            # TemplateUrl property requires S3 URL to be in path-style format\n            parts = parse_s3_url(url, version_property=\"Version\")\n            s3_path_url = self.uploader.to_path_style_s3_url(\n                    parts[\"Key\"], parts.get(\"Version\", None))\n            set_value_from_jmespath(resource_dict, self.PROPERTY_NAME, s3_path_url)\n\n\nclass ServerlessApplicationResource(CloudFormationStackResource):\n    \"\"\"\n    Represents Serverless::Application resource that can refer to a nested\n    app template via Location property.\n    \"\"\"\n    RESOURCE_TYPE = \"AWS::Serverless::Application\"\n    PROPERTY_NAME = \"Location\"\n\n\n\nclass GlueJobCommandScriptLocationResource(Resource):\n    \"\"\"\n    Represents Glue::Job resource.\n    \"\"\"\n    RESOURCE_TYPE = \"AWS::Glue::Job\"\n    # Note the PROPERTY_NAME includes a '.' implying it's nested.\n    PROPERTY_NAME = \"Command.ScriptLocation\"\n\n\nclass CodeCommitRepositoryS3Resource(ResourceWithS3UrlDict):\n    \"\"\"\n    Represents CodeCommit::Repository resource.\n    \"\"\"\n    RESOURCE_TYPE = \"AWS::CodeCommit::Repository\"\n    PROPERTY_NAME = \"Code.S3\"\n    BUCKET_NAME_PROPERTY = \"Bucket\"\n    OBJECT_KEY_PROPERTY = \"Key\"\n    VERSION_PROPERTY = \"ObjectVersion\"\n    # Don't package the directory if S3 is omitted.\n    PACKAGE_NULL_PROPERTY = False\n    FORCE_ZIP = True\n\n\nRESOURCES_EXPORT_LIST = [\n    ServerlessFunctionResource,\n    ServerlessApiResource,\n    GraphQLSchemaResource,\n    AppSyncResolverRequestTemplateResource,\n    AppSyncResolverResponseTemplateResource,\n    AppSyncFunctionConfigurationRequestTemplateResource,\n    AppSyncFunctionConfigurationResponseTemplateResource,\n    ApiGatewayRestApiResource,\n    LambdaFunctionResource,\n    ElasticBeanstalkApplicationVersion,\n    CloudFormationStackResource,\n    ServerlessApplicationResource,\n    ServerlessLayerVersionResource,\n    LambdaLayerVersionResource,\n    GlueJobCommandScriptLocationResource,\n    StepFunctionsStateMachineDefinitionResource,\n    ServerlessStateMachineDefinitionResource,\n    CodeCommitRepositoryS3Resource\n]\n\nMETADATA_EXPORT_LIST = [\n    ServerlessRepoApplicationReadme,\n    ServerlessRepoApplicationLicense\n]\n\n\ndef include_transform_export_handler(template_dict, uploader, parent_dir):\n    if template_dict.get(\"Name\", None) != \"AWS::Include\":\n        return template_dict\n\n    include_location = template_dict.get(\"Parameters\", {}).get(\"Location\", None)\n    if not include_location \\\n            or not is_path_value_valid(include_location) \\\n            or is_s3_url(include_location):\n        # `include_location` is either empty, or not a string, or an S3 URI\n        return template_dict\n\n    # We are confident at this point that `include_location` is a string containing the local path\n    abs_include_location = os.path.join(parent_dir, include_location)\n    if is_local_file(abs_include_location):\n        template_dict[\"Parameters\"][\"Location\"] = uploader.upload_with_dedup(abs_include_location)\n    else:\n        raise exceptions.InvalidLocalPathError(\n            resource_id=\"AWS::Include\",\n            property_name=\"Location\",\n            local_path=abs_include_location)\n\n    return template_dict\n\n\nGLOBAL_EXPORT_DICT = {\n    \"Fn::Transform\": include_transform_export_handler\n}\n\n\nclass Template(object):\n    \"\"\"\n    Class to export a CloudFormation template\n    \"\"\"\n\n    def __init__(self, template_path, parent_dir, uploader,\n                 resources_to_export=RESOURCES_EXPORT_LIST,\n                 metadata_to_export=METADATA_EXPORT_LIST):\n        \"\"\"\n        Reads the template and makes it ready for export\n        \"\"\"\n\n        if not (is_local_folder(parent_dir) and os.path.isabs(parent_dir)):\n            raise ValueError(\"parent_dir parameter must be \"\n                             \"an absolute path to a folder {0}\"\n                             .format(parent_dir))\n\n        abs_template_path = make_abs_path(parent_dir, template_path)\n        template_dir = os.path.dirname(abs_template_path)\n\n        with open(abs_template_path, \"r\") as handle:\n            template_str = handle.read()\n\n        self.template_dict = yaml_parse(template_str)\n        self.template_dir = template_dir\n        self.resources_to_export = resources_to_export\n        self.metadata_to_export = metadata_to_export\n        self.uploader = uploader\n\n    def export_global_artifacts(self, template_dict):\n        \"\"\"\n        Template params such as AWS::Include transforms are not specific to\n        any resource type but contain artifacts that should be exported,\n        here we iterate through the template dict and export params with a\n        handler defined in GLOBAL_EXPORT_DICT\n        \"\"\"\n        for key, val in template_dict.items():\n            if key in GLOBAL_EXPORT_DICT:\n                template_dict[key] = GLOBAL_EXPORT_DICT[key](val, self.uploader, self.template_dir)\n            elif isinstance(val, dict):\n                self.export_global_artifacts(val)\n            elif isinstance(val, list):\n                for item in val:\n                    if isinstance(item, dict):\n                        self.export_global_artifacts(item)\n        return template_dict\n\n    def export_metadata(self, template_dict):\n        \"\"\"\n        Exports the local artifacts referenced by the metadata section in\n        the given template to an s3 bucket.\n\n        :return: The template with references to artifacts that have been\n        exported to s3.\n        \"\"\"\n        if \"Metadata\" not in template_dict:\n            return template_dict\n\n        for metadata_type, metadata_dict in template_dict[\"Metadata\"].items():\n            for exporter_class in self.metadata_to_export:\n                if exporter_class.RESOURCE_TYPE != metadata_type:\n                    continue\n\n                exporter = exporter_class(self.uploader)\n                exporter.export(metadata_type, metadata_dict, self.template_dir)\n\n        return template_dict\n\n    def export(self):\n        \"\"\"\n        Exports the local artifacts referenced by the given template to an\n        s3 bucket.\n\n        :return: The template with references to artifacts that have been\n        exported to s3.\n        \"\"\"\n        self.template_dict = self.export_metadata(self.template_dict)\n\n        if \"Resources\" not in self.template_dict:\n            return self.template_dict\n\n        self.template_dict = self.export_global_artifacts(self.template_dict)\n\n        self.export_resources(self.template_dict[\"Resources\"])\n\n        return self.template_dict\n\n    def export_resources(self, resource_dict):\n        for resource_id, resource in resource_dict.items():\n\n            if resource_id.startswith(\"Fn::ForEach::\"):\n                if not isinstance(resource, list) or len(resource) != 3:\n                    raise exceptions.InvalidForEachIntrinsicFunctionError(resource_id=resource_id)\n                self.export_resources(resource[2])\n                continue\n\n            resource_type = resource.get(\"Type\", None)\n            resource_dict = resource.get(\"Properties\", None)\n\n            for exporter_class in self.resources_to_export:\n                if exporter_class.RESOURCE_TYPE != resource_type:\n                    continue\n\n                # Export code resources\n                exporter = exporter_class(self.uploader)\n                exporter.export(resource_id, resource_dict, self.template_dir)\n", "awscli/customizations/cloudformation/deploy.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\nimport sys\nimport logging\n\nfrom botocore.client import Config\n\nfrom awscli.customizations.cloudformation import exceptions\nfrom awscli.customizations.cloudformation.deployer import Deployer\nfrom awscli.customizations.s3uploader import S3Uploader\nfrom awscli.customizations.cloudformation.yamlhelper import yaml_parse\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.compat import get_stdout_text_writer\nfrom awscli.utils import write_exception\n\nLOG = logging.getLogger(__name__)\n\n\nclass DeployCommand(BasicCommand):\n\n    MSG_NO_EXECUTE_CHANGESET = \\\n        (\"Changeset created successfully. Run the following command to \"\n         \"review changes:\"\n         \"\\n\"\n         \"aws cloudformation describe-change-set --change-set-name \"\n         \"{changeset_id}\"\n         \"\\n\")\n\n    MSG_EXECUTE_SUCCESS = \"Successfully created/updated stack - {stack_name}\\n\"\n\n    PARAMETER_OVERRIDE_CMD = \"parameter-overrides\"\n    TAGS_CMD = \"tags\"\n\n    NAME = 'deploy'\n    DESCRIPTION = BasicCommand.FROM_FILE(\"cloudformation\",\n                                         \"_deploy_description.rst\")\n\n    ARG_TABLE = [\n        {\n            'name': 'template-file',\n            'required': True,\n            'help_text': (\n                'The path where your AWS CloudFormation'\n                ' template is located.'\n            )\n        },\n        {\n            'name': 'stack-name',\n            'action': 'store',\n            'required': True,\n            'help_text': (\n                'The name of the AWS CloudFormation stack you\\'re deploying to.'\n                ' If you specify an existing stack, the command updates the'\n                ' stack. If you specify a new stack, the command creates it.'\n            )\n        },\n        {\n            'name': 's3-bucket',\n            'required': False,\n            'help_text': (\n                'The name of the S3 bucket where this command uploads your '\n                'CloudFormation template. This is required the deployments of '\n                'templates sized greater than 51,200 bytes'\n            )\n        },\n        {\n            \"name\": \"force-upload\",\n            \"action\": \"store_true\",\n            \"help_text\": (\n                'Indicates whether to override existing files in the S3 bucket.'\n                ' Specify this flag to upload artifacts even if they '\n                ' match existing artifacts in the S3 bucket.'\n            )\n        },\n        {\n            'name': 's3-prefix',\n            'help_text': (\n                'A prefix name that the command adds to the'\n                ' artifacts\\' name when it uploads them to the S3 bucket.'\n                ' The prefix name is a path name (folder name) for'\n                ' the S3 bucket.'\n            )\n        },\n\n        {\n            'name': 'kms-key-id',\n            'help_text': (\n                'The ID of an AWS KMS key that the command uses'\n                ' to encrypt artifacts that are at rest in the S3 bucket.'\n            )\n        },\n        {\n            'name': PARAMETER_OVERRIDE_CMD,\n            'action': 'store',\n            'required': False,\n            'schema': {\n                'type': 'array',\n                'items': {\n                    'type': 'string'\n                }\n            },\n            'default': [],\n            'help_text': (\n                'A list of parameter structures that specify input parameters'\n                ' for your stack template. If you\\'re updating a stack and you'\n                ' don\\'t specify a parameter, the command uses the stack\\'s'\n                ' existing value. For new stacks, you must specify'\n                ' parameters that don\\'t have a default value.'\n                ' Syntax: ParameterKey1=ParameterValue1'\n                ' ParameterKey2=ParameterValue2 ...'\n            )\n        },\n        {\n            'name': 'capabilities',\n            'action': 'store',\n            'required': False,\n            'schema': {\n                'type': 'array',\n                'items': {\n                    'type': 'string',\n                    'enum': [\n                        'CAPABILITY_IAM',\n                        'CAPABILITY_NAMED_IAM'\n                    ]\n                }\n            },\n            'default': [],\n            'help_text': (\n                'A list of capabilities that you must specify before AWS'\n                ' Cloudformation can create certain stacks. Some stack'\n                ' templates might include resources that can affect'\n                ' permissions in your AWS account, for example, by creating'\n                ' new AWS Identity and Access Management (IAM) users. For'\n                ' those stacks, you must explicitly acknowledge their'\n                ' capabilities by specifying this parameter. '\n                ' The only valid values are CAPABILITY_IAM and'\n                ' CAPABILITY_NAMED_IAM. If you have IAM resources, you can'\n                ' specify either capability. If you have IAM resources with'\n                ' custom names, you must specify CAPABILITY_NAMED_IAM. If you'\n                ' don\\'t specify this parameter, this action returns an'\n                ' InsufficientCapabilities error.'\n            )\n\n        },\n        {\n            'name': 'no-execute-changeset',\n            'action': 'store_false',\n            'dest': 'execute_changeset',\n            'required': False,\n            'help_text': (\n                'Indicates whether to execute the change set. Specify this'\n                ' flag if you want to view your stack changes before'\n                ' executing the change set. The command creates an'\n                ' AWS CloudFormation change set and then exits without'\n                ' executing the change set. After you view the change set,'\n                ' execute it to implement your changes.'\n            )\n        },\n        {\n            'name': 'disable-rollback',\n            'required': False,\n            'action': 'store_true',\n            'group_name': 'disable-rollback',\n            'dest': 'disable_rollback',\n            'default': False,\n            'help_text': (\n                'Preserve the state of previously provisioned resources when '\n                'the execute-change-set operation fails.'\n            )\n        },\n        {\n            'name': 'no-disable-rollback',\n            'required': False,\n            'action': 'store_false',\n            'group_name': 'disable-rollback',\n            'dest': 'disable_rollback',\n            'default': True,\n            'help_text': (\n                'Roll back all resource changes when the execute-change-set '\n                'operation fails.'\n            )\n        },\n        {\n            'name': 'role-arn',\n            'required': False,\n            'help_text': (\n                'The Amazon Resource Name (ARN) of an AWS Identity and Access '\n                'Management (IAM) role that AWS CloudFormation assumes when '\n                'executing the change set.'\n            )\n        },\n        {\n            'name': 'notification-arns',\n            'required': False,\n            'schema': {\n                'type': 'array',\n                'items': {\n                    'type': 'string'\n                }\n            },\n            'help_text': (\n                'Amazon Simple Notification Service topic Amazon Resource Names'\n                ' (ARNs) that AWS CloudFormation associates with the stack.'\n            )\n        },\n        {\n            'name': 'fail-on-empty-changeset',\n            'required': False,\n            'action': 'store_true',\n            'group_name': 'fail-on-empty-changeset',\n            'dest': 'fail_on_empty_changeset',\n            'default': True,\n            'help_text': (\n                'Specify if the CLI should return a non-zero exit code if '\n                'there are no changes to be made to the stack. The default '\n                'behavior is to return a non-zero exit code.'\n            )\n        },\n        {\n            'name': 'no-fail-on-empty-changeset',\n            'required': False,\n            'action': 'store_false',\n            'group_name': 'fail-on-empty-changeset',\n            'dest': 'fail_on_empty_changeset',\n            'default': True,\n            'help_text': (\n                'Causes the CLI to return an exit code of 0 if there are no '\n                'changes to be made to the stack.'\n            )\n        },\n        {\n            'name': TAGS_CMD,\n            'action': 'store',\n            'required': False,\n            'schema': {\n                'type': 'array',\n                'items': {\n                    'type': 'string'\n                }\n            },\n            'default': [],\n            'help_text': (\n                'A list of tags to associate with the stack that is created'\n                ' or updated. AWS CloudFormation also propagates these tags'\n                ' to resources in the stack if the resource supports it.'\n                ' Syntax: TagKey1=TagValue1 TagKey2=TagValue2 ...'\n            )\n        }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        cloudformation_client = \\\n            self._session.create_client(\n                    'cloudformation', region_name=parsed_globals.region,\n                    endpoint_url=parsed_globals.endpoint_url,\n                    verify=parsed_globals.verify_ssl)\n\n        template_path = parsed_args.template_file\n        if not os.path.isfile(template_path):\n            raise exceptions.InvalidTemplatePathError(\n                    template_path=template_path)\n\n        # Parse parameters\n        with open(template_path, \"r\") as handle:\n            template_str = handle.read()\n\n        stack_name = parsed_args.stack_name\n        parameter_overrides = self.parse_key_value_arg(\n                parsed_args.parameter_overrides,\n                self.PARAMETER_OVERRIDE_CMD)\n\n        tags_dict = self.parse_key_value_arg(parsed_args.tags, self.TAGS_CMD)\n        tags = [{\"Key\": key, \"Value\": value}\n                for key, value in tags_dict.items()]\n\n        template_dict = yaml_parse(template_str)\n\n        parameters = self.merge_parameters(template_dict, parameter_overrides)\n\n        template_size = os.path.getsize(parsed_args.template_file)\n        if template_size > 51200 and not parsed_args.s3_bucket:\n            raise exceptions.DeployBucketRequiredError()\n\n        bucket = parsed_args.s3_bucket\n        if bucket:\n            s3_client = self._session.create_client(\n                \"s3\",\n                config=Config(signature_version='s3v4'),\n                region_name=parsed_globals.region,\n                verify=parsed_globals.verify_ssl)\n\n            s3_uploader = S3Uploader(s3_client,\n                                      bucket,\n                                      parsed_args.s3_prefix,\n                                      parsed_args.kms_key_id,\n                                      parsed_args.force_upload)\n        else:\n            s3_uploader = None\n\n        deployer = Deployer(cloudformation_client)\n        return self.deploy(deployer, stack_name, template_str,\n                           parameters, parsed_args.capabilities,\n                           parsed_args.execute_changeset, parsed_args.role_arn,\n                           parsed_args.notification_arns, s3_uploader,\n                           tags, parsed_args.fail_on_empty_changeset,\n                           parsed_args.disable_rollback)\n\n    def deploy(self, deployer, stack_name, template_str,\n               parameters, capabilities, execute_changeset, role_arn,\n               notification_arns, s3_uploader, tags,\n               fail_on_empty_changeset=True, disable_rollback=False):\n        try:\n            result = deployer.create_and_wait_for_changeset(\n                stack_name=stack_name,\n                cfn_template=template_str,\n                parameter_values=parameters,\n                capabilities=capabilities,\n                role_arn=role_arn,\n                notification_arns=notification_arns,\n                s3_uploader=s3_uploader,\n                tags=tags\n            )\n        except exceptions.ChangeEmptyError as ex:\n            if fail_on_empty_changeset:\n                raise\n            write_exception(ex, outfile=get_stdout_text_writer())\n            return 0\n\n        if execute_changeset:\n            deployer.execute_changeset(result.changeset_id, stack_name,\n                                       disable_rollback)\n            deployer.wait_for_execute(stack_name, result.changeset_type)\n            sys.stdout.write(self.MSG_EXECUTE_SUCCESS.format(\n                    stack_name=stack_name))\n        else:\n            sys.stdout.write(self.MSG_NO_EXECUTE_CHANGESET.format(\n                    changeset_id=result.changeset_id))\n\n        sys.stdout.flush()\n        return 0\n\n    def merge_parameters(self, template_dict, parameter_overrides):\n        \"\"\"\n        CloudFormation CreateChangeset requires a value for every parameter\n        from the template, either specifying a new value or use previous value.\n        For convenience, this method will accept new parameter values and\n        generates a dict of all parameters in a format that ChangeSet API\n        will accept\n\n        :param parameter_overrides:\n        :return:\n        \"\"\"\n        parameter_values = []\n\n        if not isinstance(template_dict.get(\"Parameters\", None), dict):\n            return parameter_values\n\n        for key, value in template_dict[\"Parameters\"].items():\n\n            obj = {\n                \"ParameterKey\": key\n            }\n\n            if key in parameter_overrides:\n                obj[\"ParameterValue\"] = parameter_overrides[key]\n            else:\n                obj[\"UsePreviousValue\"] = True\n\n            parameter_values.append(obj)\n\n        return parameter_values\n\n    def parse_key_value_arg(self, arg_value, argname):\n        \"\"\"\n        Converts arguments that are passed as list of \"Key=Value\" strings\n        into a real dictionary.\n\n        :param arg_value list: Array of strings, where each string is of\n            form Key=Value\n        :param argname string: Name of the argument that contains the value\n        :return dict: Dictionary representing the key/value pairs\n        \"\"\"\n        result = {}\n        for data in arg_value:\n\n            # Split at first '=' from left\n            key_value_pair = data.split(\"=\", 1)\n\n            if len(key_value_pair) != 2:\n                raise exceptions.InvalidKeyValuePairArgumentError(\n                        argname=argname,\n                        value=key_value_pair)\n\n            result[key_value_pair[0]] = key_value_pair[1]\n\n        return result\n\n\n\n", "awscli/customizations/cloudformation/package.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\nimport logging\nimport sys\n\nimport json\n\nfrom botocore.client import Config\n\nfrom awscli.customizations.cloudformation.artifact_exporter import Template\nfrom awscli.customizations.cloudformation.yamlhelper import yaml_dump\nfrom awscli.customizations.cloudformation import exceptions\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.s3uploader import S3Uploader\n\nLOG = logging.getLogger(__name__)\n\n\nclass PackageCommand(BasicCommand):\n\n    MSG_PACKAGED_TEMPLATE_WRITTEN = (\n        \"Successfully packaged artifacts and wrote output template \"\n        \"to file {output_file_name}.\"\n        \"\\n\"\n        \"Execute the following command to deploy the packaged template\"\n        \"\\n\"\n        \"aws cloudformation deploy --template-file {output_file_path} \"\n        \"--stack-name <YOUR STACK NAME>\"\n        \"\\n\")\n\n    NAME = \"package\"\n\n    DESCRIPTION = BasicCommand.FROM_FILE(\"cloudformation\",\n                                         \"_package_description.rst\")\n\n    ARG_TABLE = [\n        {\n            'name': 'template-file',\n            'required': True,\n            'help_text': (\n                'The path where your AWS CloudFormation'\n                ' template is located.'\n            )\n        },\n\n        {\n            'name': 's3-bucket',\n            'required': True,\n            'help_text': (\n                'The name of the S3 bucket where this command uploads'\n                ' the artifacts that are referenced in your template.'\n            )\n        },\n\n        {\n            'name': 's3-prefix',\n            'help_text': (\n                'A prefix name that the command adds to the'\n                ' artifacts\\' name when it uploads them to the S3 bucket.'\n                ' The prefix name is a path name (folder name) for'\n                ' the S3 bucket.'\n            )\n        },\n\n        {\n            'name': 'kms-key-id',\n            'help_text': (\n                'The ID of an AWS KMS key that the command uses'\n                ' to encrypt artifacts that are at rest in the S3 bucket.'\n            )\n        },\n\n        {\n            \"name\": \"output-template-file\",\n            \"help_text\": (\n                \"The path to the file where the command writes the\"\n                \" output AWS CloudFormation template. If you don't specify\"\n                \" a path, the command writes the template to the standard\"\n                \" output.\"\n            )\n        },\n\n        {\n            \"name\": \"use-json\",\n            \"action\": \"store_true\",\n            \"help_text\": (\n                \"Indicates whether to use JSON as the format for the output AWS\"\n                \" CloudFormation template. YAML is used by default.\"\n            )\n        },\n\n        {\n            \"name\": \"force-upload\",\n            \"action\": \"store_true\",\n            \"help_text\": (\n                'Indicates whether to override existing files in the S3 bucket.'\n                ' Specify this flag to upload artifacts even if they '\n                ' match existing artifacts in the S3 bucket.'\n            )\n        },\n        {\n            \"name\": \"metadata\",\n            \"cli_type_name\": \"map\",\n            \"schema\": {\n                \"type\": \"map\",\n                \"key\": {\"type\": \"string\"},\n                \"value\": {\"type\": \"string\"}\n            },\n            \"help_text\": \"A map of metadata to attach to *ALL* the artifacts that\"\n            \" are referenced in your template.\"\n        }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        s3_client = self._session.create_client(\n            \"s3\",\n            config=Config(signature_version='s3v4'),\n            region_name=parsed_globals.region,\n            verify=parsed_globals.verify_ssl)\n\n        template_path = parsed_args.template_file\n        if not os.path.isfile(template_path):\n            raise exceptions.InvalidTemplatePathError(\n                    template_path=template_path)\n\n        bucket = parsed_args.s3_bucket\n\n        self.s3_uploader = S3Uploader(s3_client,\n                                      bucket,\n                                      parsed_args.s3_prefix,\n                                      parsed_args.kms_key_id,\n                                      parsed_args.force_upload)\n        # attach the given metadata to the artifacts to be uploaded\n        self.s3_uploader.artifact_metadata = parsed_args.metadata\n\n        output_file = parsed_args.output_template_file\n        use_json = parsed_args.use_json\n        exported_str = self._export(template_path, use_json)\n\n        sys.stdout.write(\"\\n\")\n        self.write_output(output_file, exported_str)\n\n        if output_file:\n            msg = self.MSG_PACKAGED_TEMPLATE_WRITTEN.format(\n                    output_file_name=output_file,\n                    output_file_path=os.path.abspath(output_file))\n            sys.stdout.write(msg)\n\n        sys.stdout.flush()\n        return 0\n\n    def _export(self, template_path, use_json):\n        template = Template(template_path, os.getcwd(), self.s3_uploader)\n        exported_template = template.export()\n\n        if use_json:\n            exported_str = json.dumps(exported_template, indent=4, ensure_ascii=False)\n        else:\n            exported_str = yaml_dump(exported_template)\n\n        return exported_str\n\n    def write_output(self, output_file_name, data):\n        if output_file_name is None:\n            sys.stdout.write(data)\n            return\n\n        with open(output_file_name, \"w\") as fp:\n            fp.write(data)\n", "awscli/customizations/cloudformation/deployer.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport sys\nimport time\nimport logging\nimport botocore\nimport collections\n\nfrom awscli.customizations.cloudformation import exceptions\nfrom awscli.customizations.cloudformation.artifact_exporter import mktempfile, parse_s3_url\n\nfrom datetime import datetime\n\nLOG = logging.getLogger(__name__)\n\nChangeSetResult = collections.namedtuple(\n                \"ChangeSetResult\", [\"changeset_id\", \"changeset_type\"])\n\n\nclass Deployer(object):\n\n    def __init__(self, cloudformation_client,\n                 changeset_prefix=\"awscli-cloudformation-package-deploy-\"):\n        self._client = cloudformation_client\n        self.changeset_prefix = changeset_prefix\n\n    def has_stack(self, stack_name):\n        \"\"\"\n        Checks if a CloudFormation stack with given name exists\n\n        :param stack_name: Name or ID of the stack\n        :return: True if stack exists. False otherwise\n        \"\"\"\n        try:\n            resp = self._client.describe_stacks(StackName=stack_name)\n            if len(resp[\"Stacks\"]) != 1:\n                return False\n\n            # When you run CreateChangeSet on a a stack that does not exist,\n            # CloudFormation will create a stack and set it's status\n            # REVIEW_IN_PROGRESS. However this stack is cannot be manipulated\n            # by \"update\" commands. Under this circumstances, we treat like\n            # this stack does not exist and call CreateChangeSet will\n            # ChangeSetType set to CREATE and not UPDATE.\n            stack = resp[\"Stacks\"][0]\n            return stack[\"StackStatus\"] != \"REVIEW_IN_PROGRESS\"\n\n        except botocore.exceptions.ClientError as e:\n            # If a stack does not exist, describe_stacks will throw an\n            # exception. Unfortunately we don't have a better way than parsing\n            # the exception msg to understand the nature of this exception.\n            msg = str(e)\n\n            if \"Stack with id {0} does not exist\".format(stack_name) in msg:\n                LOG.debug(\"Stack with id {0} does not exist\".format(\n                    stack_name))\n                return False\n            else:\n                # We don't know anything about this exception. Don't handle\n                LOG.debug(\"Unable to get stack details.\", exc_info=e)\n                raise e\n\n    def create_changeset(self, stack_name, cfn_template,\n                         parameter_values, capabilities, role_arn,\n                         notification_arns, s3_uploader, tags):\n        \"\"\"\n        Call Cloudformation to create a changeset and wait for it to complete\n\n        :param stack_name: Name or ID of stack\n        :param cfn_template: CloudFormation template string\n        :param parameter_values: Template parameters object\n        :param capabilities: Array of capabilities passed to CloudFormation\n        :param tags: Array of tags passed to CloudFormation\n        :return:\n        \"\"\"\n\n        now = datetime.utcnow().isoformat()\n        description = \"Created by AWS CLI at {0} UTC\".format(now)\n\n        # Each changeset will get a unique name based on time\n        changeset_name = self.changeset_prefix + str(int(time.time()))\n\n        if not self.has_stack(stack_name):\n            changeset_type = \"CREATE\"\n            # When creating a new stack, UsePreviousValue=True is invalid.\n            # For such parameters, users should either override with new value,\n            # or set a Default value in template to successfully create a stack.\n            parameter_values = [x for x in parameter_values\n                                if not x.get(\"UsePreviousValue\", False)]\n        else:\n            changeset_type = \"UPDATE\"\n            # UsePreviousValue not valid if parameter is new\n            summary = self._client.get_template_summary(StackName=stack_name)\n            existing_parameters = [parameter['ParameterKey'] for parameter in \\\n                                   summary['Parameters']]\n            parameter_values = [x for x in parameter_values\n                                if not (x.get(\"UsePreviousValue\", False) and \\\n                                x[\"ParameterKey\"] not in existing_parameters)]\n\n        kwargs = {\n            'ChangeSetName': changeset_name,\n            'StackName': stack_name,\n            'TemplateBody': cfn_template,\n            'ChangeSetType': changeset_type,\n            'Parameters': parameter_values,\n            'Capabilities': capabilities,\n            'Description': description,\n            'Tags': tags,\n        }\n\n        # If an S3 uploader is available, use TemplateURL to deploy rather than\n        # TemplateBody. This is required for large templates.\n        if s3_uploader:\n            with mktempfile() as temporary_file:\n                temporary_file.write(kwargs.pop('TemplateBody'))\n                temporary_file.flush()\n                url = s3_uploader.upload_with_dedup(\n                        temporary_file.name, \"template\")\n                # TemplateUrl property requires S3 URL to be in path-style format\n                parts = parse_s3_url(url, version_property=\"Version\")\n                kwargs['TemplateURL'] = s3_uploader.to_path_style_s3_url(parts[\"Key\"], parts.get(\"Version\", None))\n\n        # don't set these arguments if not specified to use existing values\n        if role_arn is not None:\n            kwargs['RoleARN'] = role_arn\n        if notification_arns is not None:\n            kwargs['NotificationARNs'] = notification_arns\n        try:\n            resp = self._client.create_change_set(**kwargs)\n            return ChangeSetResult(resp[\"Id\"], changeset_type)\n        except Exception as ex:\n            LOG.debug(\"Unable to create changeset\", exc_info=ex)\n            raise ex\n\n    def wait_for_changeset(self, changeset_id, stack_name):\n        \"\"\"\n        Waits until the changeset creation completes\n\n        :param changeset_id: ID or name of the changeset\n        :param stack_name:   Stack name\n        :return: Latest status of the create-change-set operation\n        \"\"\"\n        sys.stdout.write(\"\\nWaiting for changeset to be created..\\n\")\n        sys.stdout.flush()\n\n        # Wait for changeset to be created\n        waiter = self._client.get_waiter(\"change_set_create_complete\")\n        # Poll every 5 seconds. Changeset creation should be fast\n        waiter_config = {'Delay': 5}\n        try:\n            waiter.wait(ChangeSetName=changeset_id, StackName=stack_name,\n                        WaiterConfig=waiter_config)\n        except botocore.exceptions.WaiterError as ex:\n            LOG.debug(\"Create changeset waiter exception\", exc_info=ex)\n\n            resp = ex.last_response\n            status = resp[\"Status\"]\n            reason = resp[\"StatusReason\"]\n\n            if status == \"FAILED\" and \\\n               \"The submitted information didn't contain changes.\" in reason or \\\n                            \"No updates are to be performed\" in reason:\n                    raise exceptions.ChangeEmptyError(stack_name=stack_name)\n\n            raise RuntimeError(\"Failed to create the changeset: {0} \"\n                               \"Status: {1}. Reason: {2}\"\n                               .format(ex, status, reason))\n\n    def execute_changeset(self, changeset_id, stack_name,\n                          disable_rollback=False):\n        \"\"\"\n        Calls CloudFormation to execute changeset\n\n        :param changeset_id: ID of the changeset\n        :param stack_name: Name or ID of the stack\n        :param disable_rollback: Disable rollback of all resource changes\n        :return: Response from execute-change-set call\n        \"\"\"\n        return self._client.execute_change_set(\n                ChangeSetName=changeset_id,\n                StackName=stack_name,\n                DisableRollback=disable_rollback)\n\n    def wait_for_execute(self, stack_name, changeset_type):\n\n        sys.stdout.write(\"Waiting for stack create/update to complete\\n\")\n        sys.stdout.flush()\n\n        # Pick the right waiter\n        if changeset_type == \"CREATE\":\n            waiter = self._client.get_waiter(\"stack_create_complete\")\n        elif changeset_type == \"UPDATE\":\n            waiter = self._client.get_waiter(\"stack_update_complete\")\n        else:\n            raise RuntimeError(\"Invalid changeset type {0}\"\n                               .format(changeset_type))\n\n        # Poll every 30 seconds. Polling too frequently risks hitting rate limits\n        # on CloudFormation's DescribeStacks API\n        waiter_config = {\n            'Delay': 30,\n            'MaxAttempts': 120,\n        }\n\n        try:\n            waiter.wait(StackName=stack_name, WaiterConfig=waiter_config)\n        except botocore.exceptions.WaiterError as ex:\n            LOG.debug(\"Execute changeset waiter exception\", exc_info=ex)\n\n            raise exceptions.DeployFailedError(stack_name=stack_name)\n\n    def create_and_wait_for_changeset(self, stack_name, cfn_template,\n                                      parameter_values, capabilities, role_arn,\n                                      notification_arns, s3_uploader, tags):\n\n        result = self.create_changeset(\n                stack_name, cfn_template, parameter_values, capabilities,\n                role_arn, notification_arns, s3_uploader, tags)\n        self.wait_for_changeset(result.changeset_id, stack_name)\n\n        return result\n", "awscli/customizations/cloudformation/__init__.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.customizations.cloudformation.package import PackageCommand\nfrom awscli.customizations.cloudformation.deploy import DeployCommand\n\n\ndef initialize(cli):\n    \"\"\"\n    The entry point for CloudFormation high level commands.\n    \"\"\"\n    cli.register('building-command-table.cloudformation', inject_commands)\n\n\ndef inject_commands(command_table, session, **kwargs):\n    \"\"\"\n    Called when the CloudFormation command table is being built. Used to\n    inject new high level commands into the command list. These high level\n    commands must not collide with existing low-level API call names.\n    \"\"\"\n    command_table['package'] = PackageCommand(session)\n    command_table['deploy'] = DeployCommand(session)\n", "awscli/customizations/emrcontainers/eks.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nclass EKS(object):\n    def __init__(self, eks_client):\n        self.eks_client = eks_client\n        self.cluster_info = {}\n\n    def get_oidc_issuer_id(self, cluster_name):\n        \"\"\"Method to get OIDC issuer id for the given EKS cluster\"\"\"\n        if cluster_name not in self.cluster_info:\n            self.cluster_info[cluster_name] = self.eks_client.describe_cluster(\n                name=cluster_name\n            )\n\n        oidc_issuer = self.cluster_info[cluster_name].get(\"cluster\", {}).get(\n            \"identity\", {}).get(\"oidc\", {}).get(\"issuer\", \"\")\n\n        return oidc_issuer.split('https://')[1]\n\n    def get_account_id(self, cluster_name):\n        \"\"\"Method to get account id for the given EKS cluster\"\"\"\n        if cluster_name not in self.cluster_info:\n            self.cluster_info[cluster_name] = self.eks_client.describe_cluster(\n                name=cluster_name\n            )\n\n        cluster_arn = self.cluster_info[cluster_name].get(\"cluster\", {}).get(\n            \"arn\", \"\")\n\n        return cluster_arn.split(':')[4]\n", "awscli/customizations/emrcontainers/iam.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport json\n\n\nclass IAM(object):\n    def __init__(self, iam_client):\n        self.iam_client = iam_client\n\n    def get_assume_role_policy(self, role_name):\n        \"\"\"Method to retrieve trust policy of given role name\"\"\"\n        role = self.iam_client.get_role(RoleName=role_name)\n        return role.get(\"Role\").get(\"AssumeRolePolicyDocument\")\n\n    def update_assume_role_policy(self, role_name, assume_role_policy):\n        \"\"\"Method to update trust policy of given role name\"\"\"\n        return self.iam_client.update_assume_role_policy(\n            RoleName=role_name,\n            PolicyDocument=json.dumps(assume_role_policy)\n        )\n", "awscli/customizations/emrcontainers/update_role_trust_policy.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport json\nimport logging\n\nfrom awscli.customizations.commands import BasicCommand\nfrom awscli.customizations.emrcontainers.constants \\\n    import TRUST_POLICY_STATEMENT_FORMAT, \\\n    TRUST_POLICY_STATEMENT_ALREADY_EXISTS, \\\n    TRUST_POLICY_UPDATE_SUCCESSFUL\nfrom awscli.customizations.emrcontainers.base36 import Base36\nfrom awscli.customizations.emrcontainers.eks import EKS\nfrom awscli.customizations.emrcontainers.iam import IAM\nfrom awscli.customizations.utils import uni_print, get_policy_arn_suffix\n\nLOG = logging.getLogger(__name__)\n\n\n# Method to parse the arguments to get the region value\ndef get_region(session, parsed_globals):\n    region = parsed_globals.region\n\n    if region is None:\n        region = session.get_config_variable('region')\n\n    return region\n\n\ndef check_if_statement_exists(expected_statement, actual_assume_role_document):\n    if actual_assume_role_document is None:\n        return False\n\n    existing_statements = actual_assume_role_document.get(\"Statement\", [])\n    for existing_statement in existing_statements:\n        matches = check_if_dict_matches(expected_statement, existing_statement)\n        if matches:\n            return True\n\n    return False\n\n\ndef check_if_dict_matches(expected_dict, actual_dict):\n    if len(expected_dict) != len(actual_dict):\n        return False\n\n    for key in expected_dict:\n        key_str = str(key)\n        val = expected_dict[key_str]\n        if isinstance(val, dict):\n            if not check_if_dict_matches(val, actual_dict.get(key_str, {})):\n                return False\n        else:\n            if key_str not in actual_dict or actual_dict[key_str] != str(val):\n                return False\n\n    return True\n\n\nclass UpdateRoleTrustPolicyCommand(BasicCommand):\n    NAME = 'update-role-trust-policy'\n\n    DESCRIPTION = BasicCommand.FROM_FILE(\n        'emr-containers',\n        'update-role-trust-policy',\n        '_description.rst'\n    )\n\n    ARG_TABLE = [\n        {\n            'name': 'cluster-name',\n            'help_text': (\"Specify the name of the Amazon EKS cluster with \"\n                          \"which the IAM Role would be used.\"),\n            'required': True\n        },\n        {\n            'name': 'namespace',\n            'help_text': (\"Specify the namespace from the Amazon EKS cluster \"\n                          \"with which the IAM Role would be used.\"),\n            'required': True\n        },\n        {\n            'name': 'role-name',\n            'help_text': (\"Specify the IAM Role name that you want to use\"\n                          \"with Amazon EMR on EKS.\"),\n            'required': True\n        },\n        {\n            'name': 'iam-endpoint',\n            'no_paramfile': True,\n            'help_text': (\"The  IAM  endpoint  to call for updating the role \"\n                          \"trust policy. This is optional and should only be\"\n                          \"specified when a custom endpoint should be called\"\n                          \"for IAM operations.\"),\n            'required': False\n        },\n        {\n            'name': 'dry-run',\n            'action': 'store_true',\n            'default': False,\n            'help_text': (\"Print the merged trust policy document to\"\n                          \"stdout instead of updating the role trust\"\n                          \"policy directly.\"),\n            'required': False\n        }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        \"\"\"Call to run the commands\"\"\"\n\n        self._cluster_name = parsed_args.cluster_name\n        self._namespace = parsed_args.namespace\n        self._role_name = parsed_args.role_name\n        self._region = get_region(self._session, parsed_globals)\n        self._endpoint_url = parsed_args.iam_endpoint\n        self._dry_run = parsed_args.dry_run\n\n        result = self._update_role_trust_policy(parsed_globals)\n        uni_print(result)\n        uni_print(\"\\n\")\n\n        return 0\n\n    def _update_role_trust_policy(self, parsed_globals):\n        \"\"\"Method to update  trust policy if not done already\"\"\"\n\n        base36 = Base36()\n\n        eks_client = EKS(self._session.create_client(\n            'eks',\n            region_name=self._region,\n            verify=parsed_globals.verify_ssl\n        ))\n\n        account_id = eks_client.get_account_id(self._cluster_name)\n        oidc_provider = eks_client.get_oidc_issuer_id(self._cluster_name)\n\n        base36_encoded_role_name = base36.encode(self._role_name)\n        LOG.debug('Base36 encoded role name: %s', base36_encoded_role_name)\n        trust_policy_statement = json.loads(TRUST_POLICY_STATEMENT_FORMAT % {\n            \"AWS_ACCOUNT_ID\": account_id,\n            \"OIDC_PROVIDER\": oidc_provider,\n            \"NAMESPACE\": self._namespace,\n            \"BASE36_ENCODED_ROLE_NAME\": base36_encoded_role_name,\n            \"AWS_PARTITION\": get_policy_arn_suffix(self._region)\n        })\n\n        LOG.debug('Computed Trust Policy Statement:\\n%s', json.dumps(\n            trust_policy_statement, indent=2))\n        iam_client = IAM(self._session.create_client(\n            'iam',\n            region_name=self._region,\n            endpoint_url=self._endpoint_url,\n            verify=parsed_globals.verify_ssl\n        ))\n\n        assume_role_document = iam_client.get_assume_role_policy(\n            self._role_name)\n        matches = check_if_statement_exists(trust_policy_statement,\n                                            assume_role_document)\n\n        if not matches:\n            LOG.debug('Role %s does not have the required trust policy ',\n                      self._role_name)\n\n            existing_statements = assume_role_document.get(\"Statement\")\n            if existing_statements is None:\n                assume_role_document[\"Statement\"] = [trust_policy_statement]\n            else:\n                existing_statements.append(trust_policy_statement)\n\n            if self._dry_run:\n                return json.dumps(assume_role_document, indent=2)\n            else:\n                LOG.debug('Updating trust policy of role %s', self._role_name)\n                iam_client.update_assume_role_policy(self._role_name,\n                                                     assume_role_document)\n                return TRUST_POLICY_UPDATE_SUCCESSFUL % self._role_name\n        else:\n            return TRUST_POLICY_STATEMENT_ALREADY_EXISTS % self._role_name\n", "awscli/customizations/emrcontainers/constants.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n# Declare all the constants used by Lifecycle in this file\n\n# Lifecycle role names\nTRUST_POLICY_STATEMENT_FORMAT = '{ \\\n    \"Effect\": \"Allow\", \\\n    \"Principal\": { \\\n        \"Federated\": \"arn:%(AWS_PARTITION)s:iam::%(AWS_ACCOUNT_ID)s:oidc-provider/' \\\n                                '%(OIDC_PROVIDER)s\" \\\n    }, \\\n    \"Action\": \"sts:AssumeRoleWithWebIdentity\", \\\n    \"Condition\": { \\\n        \"StringLike\": { \\\n            \"%(OIDC_PROVIDER)s:sub\": \"system:serviceaccount:%(NAMESPACE)s' \\\n                                ':emr-containers-sa-*-*-%(AWS_ACCOUNT_ID)s-' \\\n                                '%(BASE36_ENCODED_ROLE_NAME)s\" \\\n        } \\\n    } \\\n}'\n\nTRUST_POLICY_STATEMENT_ALREADY_EXISTS = \"Trust policy statement already \" \\\n                                        \"exists for role %s. No changes \" \\\n                                        \"were made!\"\n\nTRUST_POLICY_UPDATE_SUCCESSFUL = \"Successfully updated trust policy of role %s\"\n", "awscli/customizations/emrcontainers/__init__.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom awscli.customizations.emrcontainers.update_role_trust_policy \\\n    import UpdateRoleTrustPolicyCommand\n\n\ndef initialize(cli):\n    \"\"\"\n    The entry point for EMR Containers high level commands.\n    \"\"\"\n    cli.register('building-command-table.emr-containers', inject_commands)\n\n\ndef inject_commands(command_table, session, **kwargs):\n    \"\"\"\n    Called when the EMR Containers command table is being built.\n    Used to inject new high level commands into the command list.\n    \"\"\"\n    command_table['update-role-trust-policy'] = UpdateRoleTrustPolicyCommand(\n        session)\n", "awscli/customizations/emrcontainers/base36.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\nclass Base36(object):\n    def str_to_int(self, request):\n        \"\"\"Method to convert given string into decimal representation\"\"\"\n        result = 0\n        for char in request:\n            result = result * 256 + ord(char)\n\n        return result\n\n    def encode(self, request):\n        \"\"\"Method to return base36 encoded form of the input string\"\"\"\n        decimal_number = self.str_to_int(str(request))\n        alphabet, base36 = ['0123456789abcdefghijklmnopqrstuvwxyz', '']\n\n        while decimal_number:\n            decimal_number, i = divmod(decimal_number, 36)\n            base36 = alphabet[i] + base36\n\n        return base36 or alphabet[0]\n", "scripts/performance/benchmark_utils.py": "import s3transfer\nimport os\nimport subprocess\nimport uuid\nimport shutil\nimport argparse\nimport tempfile\n\n\ndef summarize(script, result_dir, summary_dir):\n    \"\"\"Run the given summary script on every file in the given directory.\n\n    :param script: A summarization script that takes a list of csv files.\n    :param result_dir: A directory containing csv performance result files.\n    :param summary_dir: The directory to put the summary file in.\n    \"\"\"\n    summarize_args = [script]\n    for f in os.listdir(result_dir):\n        path = os.path.join(result_dir, f)\n        if os.path.isfile(path):\n            summarize_args.append(path)\n\n    with open(os.path.join(summary_dir, 'summary.txt'), 'wb') as f:\n        subprocess.check_call(summarize_args, stdout=f)\n    with open(os.path.join(summary_dir, 'summary.json'), 'wb') as f:\n        summarize_args.extend(['--output-format', 'json'])\n        subprocess.check_call(summarize_args, stdout=f)\n\n\ndef _get_s3transfer_performance_script(script_name):\n    \"\"\"Retrieves an s3transfer performance script if available.\"\"\"\n    s3transfer_directory = os.path.dirname(s3transfer.__file__)\n    s3transfer_directory = os.path.dirname(s3transfer_directory)\n    scripts_directory = 'scripts/performance'\n    scripts_directory = os.path.join(s3transfer_directory, scripts_directory)\n    script = os.path.join(scripts_directory, script_name)\n\n    if os.path.isfile(script):\n        return script\n    else:\n        return None\n\n\ndef get_benchmark_script():\n    return _get_s3transfer_performance_script('benchmark')\n\n\ndef get_summarize_script():\n    return _get_s3transfer_performance_script('summarize')\n\n\ndef backup(source, recursive):\n    \"\"\"Backup a given source to a temporary location.\n\n    :type source: str\n    :param source: A local path or s3 path to backup.\n\n    :type recursive: bool\n    :param recursive: if True, the source will be treated as a directory.\n    \"\"\"\n    if source[:5] == 's3://':\n        parts = source.split('/')\n        parts.insert(3, str(uuid.uuid4()))\n        backup_path = '/'.join(parts)\n    else:\n        name = os.path.split(source)[-1]\n        temp_dir = tempfile.mkdtemp()\n        backup_path = os.path.join(temp_dir, name)\n\n    copy(source, backup_path, recursive)\n    return backup_path\n\n\ndef copy(source, destination, recursive):\n    \"\"\"Copy files from one location to another.\n\n    The source and destination must both be s3 paths or both be local paths.\n\n    :type source: str\n    :param source: A local path or s3 path to backup.\n\n    :type destination: str\n    :param destination: A local path or s3 path to backup the source to.\n\n    :type recursive: bool\n    :param recursive: if True, the source will be treated as a directory.\n    \"\"\"\n    if 's3://' in [source[:5], destination[:5]]:\n        cp_args = ['aws', 's3', 'cp', source, destination, '--quiet']\n        if recursive:\n            cp_args.append('--recursive')\n        subprocess.check_call(cp_args)\n        return\n\n    if recursive:\n        shutil.copytree(source, destination)\n    else:\n        shutil.copy(source, destination)\n\n\ndef clean(destination, recursive):\n    \"\"\"Delete a file or directory either locally or on S3.\"\"\"\n    if destination[:5] == 's3://':\n        rm_args = ['aws', 's3', 'rm', '--quiet', destination]\n        if recursive:\n            rm_args.append('--recursive')\n        subprocess.check_call(rm_args)\n    else:\n        if recursive:\n            shutil.rmtree(destination)\n        else:\n            os.remove(destination)\n\n\ndef create_random_subfolder(destination):\n    \"\"\"Create a random subdirectory in a given directory.\"\"\"\n    folder_name = str(uuid.uuid4())\n    if destination.startswith('s3://'):\n        parts = destination.split('/')\n        parts.append(folder_name)\n        return '/'.join(parts)\n    else:\n        parts = list(os.path.split(destination))\n        parts.append(folder_name)\n        path = os.path.join(*parts)\n        os.makedirs(path)\n        return path\n\n\ndef get_transfer_command(command, recursive, quiet):\n    \"\"\"Get a full cli transfer command.\n\n    Performs common transformations, e.g. adding --quiet\n    \"\"\"\n    cli_command = 'aws s3 ' + command\n\n    if recursive:\n        cli_command += ' --recursive'\n\n    if quiet:\n        cli_command += ' --quiet'\n    else:\n        print(cli_command)\n\n    return cli_command\n\n\ndef benchmark_command(command, benchmark_script, summarize_script,\n                      output_dir, num_iterations, dry_run, upkeep=None,\n                      cleanup=None):\n    \"\"\"Benchmark several runs of a long-running command.\n\n    :type command: str\n    :param command: The full aws cli command to benchmark\n\n    :type benchmark_script: str\n    :param benchmark_script: A benchmark script that takes a command to run\n        and outputs performance data to a file. This should be from s3transfer.\n\n    :type summarize_script: str\n    :param summarize_script:  A summarization script that the output of the\n        benchmark script. This should be from s3transfer.\n\n    :type output_dir: str\n    :param output_dir: The directory to output performance results to.\n\n    :type num_iterations: int\n    :param num_iterations: The number of times to run the benchmark on the\n        command.\n\n    :type dry_run: bool\n    :param dry_run: Whether or not to actually run the benchmarks.\n\n    :type upkeep: function that takes no arguments\n    :param upkeep: A function that is run after every iteration of the\n        benchmark process. This should be used for upkeep, such as restoring\n        files that were deleted as part of the command executing.\n\n    :type cleanup: function that takes no arguments\n    :param cleanup: A function that is run at the end of the benchmark\n        process or if there are any problems during the benchmark process.\n        It should be uses for the final cleanup, such as deleting files that\n        were created at some destination.\n    \"\"\"\n    performance_dir = os.path.join(output_dir, 'performance')\n    if os.path.exists(performance_dir):\n        shutil.rmtree(performance_dir)\n    os.makedirs(performance_dir)\n\n    try:\n        for i in range(num_iterations):\n            out_file = 'performance%s.csv' % i\n            out_file = os.path.join(performance_dir, out_file)\n            benchmark_args = [\n                benchmark_script, command, '--output-file', out_file\n            ]\n            if not dry_run:\n                subprocess.check_call(benchmark_args)\n                if upkeep is not None:\n                    upkeep()\n\n        if not dry_run:\n            summarize(summarize_script, performance_dir, output_dir)\n    finally:\n        if not dry_run and cleanup is not None:\n            cleanup()\n\n\ndef get_default_argparser():\n    \"\"\"Get an ArgumentParser with all the base benchmark arguments added in.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--no-cleanup', action='store_true', default=False,\n        help='Do not remove the destination after the tests complete.'\n    )\n    parser.add_argument(\n        '--recursive', action='store_true', default=False,\n        help='Indicates that this is a recursive transfer.'\n    )\n    benchmark_script = get_benchmark_script()\n    parser.add_argument(\n        '--benchmark-script', default=benchmark_script,\n        required=benchmark_script is None,\n        help=('The benchmark script to run the commands with. This should be '\n              'from s3transfer.')\n    )\n    summarize_script = get_summarize_script()\n    parser.add_argument(\n        '--summarize-script', default=summarize_script,\n        required=summarize_script is None,\n        help=('The summarize script to run the commands with. This should be '\n              'from s3transfer.')\n    )\n    parser.add_argument(\n        '-o', '--result-dir', default='results',\n        help='The directory to output performance results to. Existing '\n             'results will be deleted.'\n    )\n    parser.add_argument(\n        '--dry-run', default=False, action='store_true',\n        help='If set, commands will only be printed out, not executed.'\n    )\n    parser.add_argument(\n        '--quiet', default=False, action='store_true',\n        help='If set, output is suppressed.'\n    )\n    parser.add_argument(\n        '-n', '--num-iterations', default=1, type=int,\n        help='The number of times to run the test.'\n    )\n    return parser\n"}