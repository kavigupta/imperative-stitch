{"run.py": "import time\nimport os\nimport argparse\nimport torch\nimport glob\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom models.ppg_extractor import load_model\nimport librosa\nimport soundfile as sf\nfrom utils.hparams import HpsYaml\n\nfrom models.encoder.audio import preprocess_wav\nfrom models.encoder import inference as speacker_encoder\nfrom models.vocoder.hifigan import inference as vocoder\nfrom models.ppg2mel import MelDecoderMOLv2\nfrom utils.f0_utils import compute_f0, f02lf0, compute_mean_std, get_converted_lf0uv\n\n\ndef _build_ppg2mel_model(model_config, model_file, device):\n    ppg2mel_model = MelDecoderMOLv2(\n        **model_config[\"model\"]\n    ).to(device)\n    ckpt = torch.load(model_file, map_location=device)\n    ppg2mel_model.load_state_dict(ckpt[\"model\"])\n    ppg2mel_model.eval()\n    return ppg2mel_model\n\n\n@torch.no_grad()\ndef convert(args):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n\n    step = os.path.basename(args.ppg2mel_model_file)[:-4].split(\"_\")[-1]\n\n    # Build models\n    print(\"Load PPG-model, PPG2Mel-model, Vocoder-model...\")\n    ppg_model = load_model(\n        Path('./ppg_extractor/saved_models/24epoch.pt'),\n        device,\n    )\n    ppg2mel_model = _build_ppg2mel_model(HpsYaml(args.ppg2mel_model_train_config), args.ppg2mel_model_file, device) \n    # vocoder.load_model('./vocoder/saved_models/pretrained/g_hifigan.pt', \"./vocoder/hifigan/config_16k_.json\")\n    vocoder.load_model('./vocoder/saved_models/24k/g_02830000.pt')\n    # Data related\n    ref_wav_path = args.ref_wav_path\n    ref_wav = preprocess_wav(ref_wav_path)\n    ref_fid = os.path.basename(ref_wav_path)[:-4]\n    \n    # TODO: specify encoder\n    speacker_encoder.load_model(Path(\"encoder/saved_models/pretrained_bak_5805000.pt\"))\n    ref_spk_dvec = speacker_encoder.embed_utterance(ref_wav)\n    ref_spk_dvec = torch.from_numpy(ref_spk_dvec).unsqueeze(0).to(device)\n    ref_lf0_mean, ref_lf0_std = compute_mean_std(f02lf0(compute_f0(ref_wav)))\n    \n    source_file_list = sorted(glob.glob(f\"{args.wav_dir}/*.wav\"))\n    print(f\"Number of source utterances: {len(source_file_list)}.\")\n    \n    total_rtf = 0.0\n    cnt = 0\n    for src_wav_path in tqdm(source_file_list):\n        # Load the audio to a numpy array:\n        src_wav, _ = librosa.load(src_wav_path, sr=16000)\n        src_wav_tensor = torch.from_numpy(src_wav).unsqueeze(0).float().to(device)\n        src_wav_lengths = torch.LongTensor([len(src_wav)]).to(device)\n        ppg = ppg_model(src_wav_tensor, src_wav_lengths)\n\n        lf0_uv = get_converted_lf0uv(src_wav, ref_lf0_mean, ref_lf0_std, convert=True)\n        min_len = min(ppg.shape[1], len(lf0_uv))\n\n        ppg = ppg[:, :min_len]\n        lf0_uv = lf0_uv[:min_len]\n        \n        start = time.time()\n        _, mel_pred, att_ws = ppg2mel_model.inference(\n            ppg,\n            logf0_uv=torch.from_numpy(lf0_uv).unsqueeze(0).float().to(device),\n            spembs=ref_spk_dvec,\n        )\n        src_fid = os.path.basename(src_wav_path)[:-4]\n        wav_fname = f\"{output_dir}/vc_{src_fid}_ref_{ref_fid}_step{step}.wav\"\n        mel_len = mel_pred.shape[0]\n        rtf = (time.time() - start) / (0.01 * mel_len)\n        total_rtf += rtf\n        cnt += 1\n        # continue\n        mel_pred= mel_pred.transpose(0, 1)\n        y, output_sample_rate = vocoder.infer_waveform(mel_pred.cpu())\n        sf.write(wav_fname, y.squeeze(), output_sample_rate, \"PCM_16\")\n    \n    print(\"RTF:\")\n    print(total_rtf / cnt)\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=\"Conversion from wave input\")\n    parser.add_argument(\n        \"--wav_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Source wave directory.\",\n    )\n    parser.add_argument(\n        \"--ref_wav_path\",\n        type=str,\n        required=True,\n        help=\"Reference wave file path.\",\n    )\n    parser.add_argument(\n        \"--ppg2mel_model_train_config\", \"-c\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Training config file (yaml file)\",\n    )\n    parser.add_argument(\n        \"--ppg2mel_model_file\", \"-m\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"ppg2mel model checkpoint file path\"\n    )\n    parser.add_argument(\n        \"--output_dir\", \"-o\",\n        type=str,\n        default=\"vc_gens_vctk_oneshot\",\n        help=\"Output folder to save the converted wave.\"\n    )\n    \n    return parser\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n    convert(args)\n\nif __name__ == \"__main__\":\n    main()\n", "train.py": "import argparse\n\ndef main():\n    # Arguments\n    preparser = argparse.ArgumentParser(description=\n            'Training model.')\n    preparser.add_argument('--type', type=str, \n                        help='type of training ')\n\n    ###\n    paras, _ = preparser.parse_known_args()\n    if paras.type == \"synth\":\n        from control.cli.synthesizer_train import new_train\n        new_train()\n    if paras.type == \"vits\":\n        from models.synthesizer.train_vits import new_train\n        new_train()\n\nif __name__ == \"__main__\":\n    main()   \n", "web.py": "import os\nimport sys\nimport typer\n\ncli = typer.Typer()\n\n@cli.command()\ndef launch(port: int = typer.Option(8080, \"--port\", \"-p\")) -> None:\n    \"\"\"Start a graphical UI server for the opyrator.\n\n    The UI is auto-generated from the input- and output-schema of the given function.\n    \"\"\"\n    # Add the current working directory to the sys path\n    # This is required to resolve the opyrator path\n    sys.path.append(os.getcwd())\n\n    from control.mkgui.base.ui.streamlit_ui import launch_ui\n    launch_ui(port)\n\nif __name__ == \"__main__\":\n    cli()", "demo_toolbox.py": "from pathlib import Path\nfrom control.toolbox import Toolbox\nfrom utils.argutils import print_args\nfrom utils.modelutils import check_model_paths\nimport argparse\nimport os\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=\"Runs the toolbox\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    \n    parser.add_argument(\"-d\", \"--datasets_root\", type=Path, help= \\\n        \"Path to the directory containing your datasets. See toolbox/__init__.py for a list of \"\n        \"supported datasets.\", default=None)\n    parser.add_argument(\"-vc\", \"--vc_mode\", action=\"store_true\", \n                        help=\"Voice Conversion Mode(PPG based)\")\n    parser.add_argument(\"-e\", \"--enc_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}encoder\", \n                        help=\"Directory containing saved encoder models\")\n    parser.add_argument(\"-s\", \"--syn_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}synthesizer\", \n                        help=\"Directory containing saved synthesizer models\")\n    parser.add_argument(\"-v\", \"--voc_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}vocoder\", \n                        help=\"Directory containing saved vocoder models\")\n    parser.add_argument(\"-ex\", \"--extractor_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}ppg_extractor\", \n                        help=\"Directory containing saved extrator models\")\n    parser.add_argument(\"-cv\", \"--convertor_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}ppg2mel\", \n                        help=\"Directory containing saved convert models\")\n    parser.add_argument(\"--cpu\", action=\"store_true\", help=\\\n        \"If True, processing is done on CPU, even when a GPU is available.\")\n    parser.add_argument(\"--seed\", type=int, default=None, help=\\\n        \"Optional random number seed value to make toolbox deterministic.\")\n    parser.add_argument(\"--no_mp3_support\", action=\"store_true\", help=\\\n        \"If True, no mp3 files are allowed.\")\n    args = parser.parse_args()\n    print_args(args, parser)\n\n    if args.cpu:\n        # Hide GPUs from Pytorch to force CPU processing\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n    del args.cpu\n\n    ## Remind the user to download pretrained models if needed\n    check_model_paths(encoder_path=args.enc_models_dir, synthesizer_path=args.syn_models_dir,\n                      vocoder_path=args.voc_models_dir)\n\n    # Launch the toolbox\n    Toolbox(**vars(args))    \n", "gen_voice.py": "from models.synthesizer.inference import Synthesizer\nfrom models.encoder import inference as encoder\nfrom models.vocoder.hifigan import inference as gan_vocoder\nfrom pathlib import Path\nimport numpy as np\nimport soundfile as sf\nimport torch\nimport sys\nimport os\nimport re\nimport cn2an\n\nvocoder = gan_vocoder\n\ndef gen_one_wav(synthesizer, in_fpath, embed, texts, file_name, seq):\n    embeds = [embed] * len(texts)\n    # If you know what the attention layer alignments are, you can retrieve them here by\n    # passing return_alignments=True\n    specs = synthesizer.synthesize_spectrograms(texts, embeds, style_idx=-1, min_stop_token=4, steps=400)\n    #spec = specs[0]\n    breaks = [spec.shape[1] for spec in specs]\n    spec = np.concatenate(specs, axis=1)\n\n    # If seed is specified, reset torch seed and reload vocoder\n    # Synthesizing the waveform is fairly straightforward. Remember that the longer the\n    # spectrogram, the more time-efficient the vocoder.\n    generated_wav, output_sample_rate = vocoder.infer_waveform(spec)\n    \n    # Add breaks\n    b_ends = np.cumsum(np.array(breaks) * synthesizer.hparams.hop_size)\n    b_starts = np.concatenate(([0], b_ends[:-1]))\n    wavs = [generated_wav[start:end] for start, end, in zip(b_starts, b_ends)]\n    breaks = [np.zeros(int(0.15 * synthesizer.sample_rate))] * len(breaks)\n    generated_wav = np.concatenate([i for w, b in zip(wavs, breaks) for i in (w, b)])\n    \n    ## Post-generation\n    # There's a bug with sounddevice that makes the audio cut one second earlier, so we\n    # pad it.\n\n    # Trim excess silences to compensate for gaps in spectrograms (issue #53)\n    generated_wav = encoder.preprocess_wav(generated_wav)\n    generated_wav = generated_wav / np.abs(generated_wav).max() * 0.97\n        \n    # Save it on the disk\n    model=os.path.basename(in_fpath)\n    filename = \"%s_%d_%s.wav\" %(file_name, seq, model)\n    sf.write(filename, generated_wav, synthesizer.sample_rate)\n\n    print(\"\\nSaved output as %s\\n\\n\" % filename)\n    \n    \ndef generate_wav(enc_model_fpath, syn_model_fpath, voc_model_fpath, in_fpath, input_txt, file_name): \n    if torch.cuda.is_available():\n        device_id = torch.cuda.current_device()\n        gpu_properties = torch.cuda.get_device_properties(device_id)\n        ## Print some environment information (for debugging purposes)\n        print(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n            \"%.1fGb total memory.\\n\" % \n            (torch.cuda.device_count(),\n            device_id,\n            gpu_properties.name,\n            gpu_properties.major,\n            gpu_properties.minor,\n            gpu_properties.total_memory / 1e9))\n    else:\n        print(\"Using CPU for inference.\\n\")\n\n    print(\"Preparing the encoder, the synthesizer and the vocoder...\")\n    encoder.load_model(enc_model_fpath)\n    synthesizer = Synthesizer(syn_model_fpath)\n    vocoder.load_model(voc_model_fpath)\n\n    encoder_wav = synthesizer.load_preprocess_wav(in_fpath)\n    embed, partial_embeds, _ = encoder.embed_utterance(encoder_wav, return_partials=True)\n\n    texts = input_txt.split(\"\\n\")\n    seq=0\n    each_num=1500\n    \n    punctuation = '\uff01\uff0c\u3002\u3001,' # punctuate and split/clean text\n    processed_texts = []\n    cur_num = 0\n    for text in texts:\n      for processed_text in re.sub(r'[{}]+'.format(punctuation), '\\n', text).split('\\n'):\n        if processed_text:\n            processed_texts.append(processed_text.strip())\n            cur_num += len(processed_text.strip())\n      if cur_num > each_num:\n        seq = seq +1\n        gen_one_wav(synthesizer, in_fpath, embed, processed_texts, file_name, seq)\n        processed_texts = []\n        cur_num = 0\n    \n    if len(processed_texts)>0:\n      seq = seq +1\n      gen_one_wav(synthesizer, in_fpath, embed, processed_texts, file_name, seq)\n\nif (len(sys.argv)>=3):\n    my_txt = \"\"\n    print(\"reading from :\", sys.argv[1])\n    with open(sys.argv[1], \"r\") as f:\n        for line in f.readlines():\n            #line = line.strip('\\n')\n            my_txt += line\n    txt_file_name = sys.argv[1]\n    wav_file_name = sys.argv[2]\n\n    output = cn2an.transform(my_txt, \"an2cn\")\n    print(output)\n    generate_wav(\n    Path(\"encoder/saved_models/pretrained.pt\"),\n    Path(\"synthesizer/saved_models/mandarin.pt\"),\n    Path(\"vocoder/saved_models/pretrained/g_hifigan.pt\"), wav_file_name, output, txt_file_name\n    )\n\nelse:\n    print(\"please input the file name\")\n    exit(1)\n\n\n", "pre.py": "from models.synthesizer.preprocess import create_embeddings, preprocess_dataset, create_emo\nfrom models.synthesizer.hparams import hparams\nfrom pathlib import Path\nimport argparse\n\nrecognized_datasets = [\n    \"aidatatang_200zh\",\n    \"aidatatang_200zh_s\",\n    \"magicdata\",\n    \"aishell3\",\n    \"data_aishell\"\n]\n\n#TODO: add for emotional data \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Preprocesses audio files from datasets, encodes them as mel spectrograms \"\n                    \"and writes them to  the disk. Audio files are also saved, to be used by the \"\n                    \"vocoder for training.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\"datasets_root\", type=Path, help=\\\n        \"Path to the directory containing your datasets.\")\n    parser.add_argument(\"-o\", \"--out_dir\", type=Path, default=argparse.SUPPRESS, help=\\\n        \"Path to the output directory that will contain the mel spectrograms, the audios and the \"\n        \"embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/\")\n    parser.add_argument(\"-n\", \"--n_processes\", type=int, default=1, help=\\\n        \"Number of processes in parallel.\")\n    parser.add_argument(\"-s\", \"--skip_existing\", action=\"store_true\", help=\\\n        \"Whether to overwrite existing files with the same name. Useful if the preprocessing was \"\n        \"interrupted. \")\n    parser.add_argument(\"--hparams\", type=str, default=\"\", help=\\\n        \"Hyperparameter overrides as a comma-separated list of name-value pairs\")\n    parser.add_argument(\"--no_trim\", action=\"store_true\", help=\\\n        \"Preprocess audio without trimming silences (not recommended).\")\n    parser.add_argument(\"--no_alignments\", action=\"store_true\", help=\\\n        \"Use this option when dataset does not include alignments\\\n        (these are used to split long audio files into sub-utterances.)\")\n    parser.add_argument(\"-d\", \"--dataset\", type=str, default=\"aidatatang_200zh\", help=\\\n        \"Name of the dataset to process, allowing values: magicdata, aidatatang_200zh, aishell3, data_aishell.\")\n    parser.add_argument(\"-e\", \"--encoder_model_fpath\", type=Path, default=\"data/ckpt/encoder/pretrained.pt\", help=\\\n        \"Path your trained encoder model.\")\n    parser.add_argument(\"-ne\", \"--n_processes_embed\", type=int, default=1, help=\\\n        \"Number of processes in parallel.An encoder is created for each, so you may need to lower \"\n        \"this value on GPUs with low memory. Set it to 1 if CUDA is unhappy\")\n    parser.add_argument(\"-ee\",\"--emotion_extract\", action=\"store_true\", help=\\\n        \"Preprocess audio to extract emotional numpy (for emotional vits).\")\n    args = parser.parse_args()\n\n    # Process the arguments\n    if not hasattr(args, \"out_dir\"):\n        args.out_dir = args.datasets_root.joinpath(\"SV2TTS\", \"synthesizer\")\n    assert args.dataset in recognized_datasets, 'is not supported, please vote for it in https://github.com/babysor/MockingBird/issues/10'\n    # Create directories\n    assert args.datasets_root.exists()\n    args.out_dir.mkdir(exist_ok=True, parents=True)\n\n    # Verify webrtcvad is available\n    if not args.no_trim:\n        try:\n            import webrtcvad\n        except:\n            raise ModuleNotFoundError(\"Package 'webrtcvad' not found. This package enables \"\n                \"noise removal and is recommended. Please install and try again. If installation fails, \"\n                \"use --no_trim to disable this error message.\")\n    encoder_model_fpath = args.encoder_model_fpath\n    del args.no_trim\n   \n    args.hparams = hparams.parse(args.hparams)\n    n_processes_embed = args.n_processes_embed\n    del args.n_processes_embed\n    preprocess_dataset(**vars(args))\n    \n    create_embeddings(synthesizer_root=args.out_dir, n_processes=n_processes_embed, encoder_model_fpath=encoder_model_fpath, skip_existing=args.skip_existing)\n    \n    if args.emotion_extract:\n        create_emo(synthesizer_root=args.out_dir, n_processes=n_processes_embed, skip_existing=args.skip_existing, hparams=args.hparams)\n", "models/synthesizer/preprocess.py": "from multiprocessing.pool import Pool \n\nfrom functools import partial\nfrom itertools import chain\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom models.encoder import inference as encoder\nfrom models.synthesizer.preprocess_audio import preprocess_general, extract_emo\nfrom models.synthesizer.preprocess_transcript import preprocess_transcript_aishell3, preprocess_transcript_magicdata\n\ndata_info = {\n    \"aidatatang_200zh\": {\n        \"subfolders\": [\"corpus/train\"],\n        \"trans_filepath\": \"transcript/aidatatang_200_zh_transcript.txt\",\n        \"speak_func\": preprocess_general\n    },\n    \"aidatatang_200zh_s\": {\n        \"subfolders\": [\"corpus/train\"],\n        \"trans_filepath\": \"transcript/aidatatang_200_zh_transcript.txt\",\n        \"speak_func\": preprocess_general\n    },\n    \"magicdata\": {\n        \"subfolders\": [\"train\"],\n        \"trans_filepath\": \"train/TRANS.txt\",\n        \"speak_func\": preprocess_general,\n        \"transcript_func\": preprocess_transcript_magicdata,\n    },\n    \"aishell3\":{\n        \"subfolders\": [\"train/wav\"],\n        \"trans_filepath\": \"train/content.txt\",\n        \"speak_func\": preprocess_general,\n        \"transcript_func\": preprocess_transcript_aishell3,\n    },\n    \"data_aishell\":{\n        \"subfolders\": [\"wav/train\"],\n        \"trans_filepath\": \"transcript/aishell_transcript_v0.8.txt\",\n        \"speak_func\": preprocess_general\n    }\n}\n\ndef should_skip(fpath: Path, skip_existing: bool) -> bool:\n    return skip_existing and fpath.exists()\n\ndef preprocess_dataset(datasets_root: Path, out_dir: Path, n_processes: int,\n                           skip_existing: bool, hparams, no_alignments: bool, \n                           dataset: str, emotion_extract = False, encoder_model_fpath=None):\n    dataset_info = data_info[dataset]\n    # Gather the input directories\n    dataset_root = datasets_root.joinpath(dataset)\n    input_dirs = [dataset_root.joinpath(subfolder.strip()) for subfolder in dataset_info[\"subfolders\"]]\n    print(\"\\n    \".join(map(str, [\"Using data from:\"] + input_dirs)))\n    assert all(input_dir.exists() for input_dir in input_dirs)\n    \n    # Create the output directories for each output file type\n    out_dir.joinpath(\"mels\").mkdir(exist_ok=True)\n    out_dir.joinpath(\"audio\").mkdir(exist_ok=True)\n    if emotion_extract:\n        out_dir.joinpath(\"emo\").mkdir(exist_ok=True)\n    \n    # Create a metadata file\n    metadata_fpath = out_dir.joinpath(\"train.txt\")\n    metadata_file = metadata_fpath.open(\"a\" if skip_existing else \"w\", encoding=\"utf-8\")\n\n    # Preprocess the dataset\n    dict_info = {}\n    transcript_dirs = dataset_root.joinpath(dataset_info[\"trans_filepath\"])\n    assert transcript_dirs.exists(), str(transcript_dirs)+\" not exist.\"\n    with open(transcript_dirs, \"r\", encoding=\"utf-8\") as dict_transcript:\n        # process with specific function for your dataset \n        if \"transcript_func\" in dataset_info:\n            dataset_info[\"transcript_func\"](dict_info, dict_transcript)\n        else:\n            for v in dict_transcript:\n                if not v:\n                    continue\n                v = v.strip().replace(\"\\n\",\"\").replace(\"\\t\",\" \").split(\" \")\n                dict_info[v[0]] = \" \".join(v[1:])\n\n    speaker_dirs = list(chain.from_iterable(input_dir.glob(\"*\") for input_dir in input_dirs))\n    \n    func = partial(dataset_info[\"speak_func\"], out_dir=out_dir, skip_existing=skip_existing, \n                   hparams=hparams, dict_info=dict_info, no_alignments=no_alignments, encoder_model_fpath=encoder_model_fpath)\n    job = Pool(n_processes).imap_unordered(func, speaker_dirs)\n    \n    for speaker_metadata in tqdm(job, dataset, len(speaker_dirs), unit=\"speakers\"):\n        if speaker_metadata is not None:\n            for metadatum in speaker_metadata:\n                metadata_file.write(\"|\".join(map(str,metadatum)) + \"\\n\")\n    metadata_file.close()\n\n    # Verify the contents of the metadata file\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n    mel_frames = sum([int(m[4]) for m in metadata])\n    timesteps = sum([int(m[3]) for m in metadata])\n    sample_rate = hparams.sample_rate\n    hours = (timesteps / sample_rate) / 3600\n    print(\"The dataset consists of %d utterances, %d mel frames, %d audio timesteps (%.2f hours).\" %\n          (len(metadata), mel_frames, timesteps, hours))\n    print(\"Max input length (text chars): %d\" % max(len(m[5]) for m in metadata))\n    print(\"Max mel frames length: %d\" % max(int(m[4]) for m in metadata))\n    print(\"Max audio timesteps length: %d\" % max(int(m[3]) for m in metadata))\n\ndef _embed_utterance(fpaths: str, encoder_model_fpath: str):\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n    \ndef _emo_extract_from_utterance(fpaths, hparams):\n    wav_fpath, emo_fpath = fpaths\n    wav = np.load(wav_fpath)\n    emo = extract_emo(np.expand_dims(wav, 0), hparams.sample_rate, True)\n    np.save(emo_fpath, emo.squeeze(0), allow_pickle=False)\n \ndef create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int, skip_existing: bool):\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert wav_dir.exists() and metadata_fpath.exists()\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n    \n    # Gather the input wave filepath and the target output embed filepath\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        fpaths = [(wav_dir.joinpath(m[0]), embed_dir.joinpath(m[2])) for m in metadata if not should_skip(embed_dir.joinpath(m[2]), skip_existing)]\n\n    # TODO: improve on the multiprocessing, it's terrible. Disk I/O is the bottleneck here.\n    # Embed the utterances in separate threads\n    func = partial(_embed_utterance, encoder_model_fpath=encoder_model_fpath)\n    job = Pool(n_processes).imap(func, fpaths)\n    tuple(tqdm(job, \"Embedding\", len(fpaths), unit=\"utterances\"))\n\ndef create_emo(synthesizer_root: Path, n_processes: int, skip_existing: bool, hparams):\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert wav_dir.exists() and metadata_fpath.exists()\n    emo_dir = synthesizer_root.joinpath(\"emo\")\n    emo_dir.mkdir(exist_ok=True)\n\n    # Gather the input wave filepath and the target output embed filepath\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        fpaths = [(wav_dir.joinpath(m[0]), emo_dir.joinpath(m[0].replace(\"audio-\", \"emo-\"))) for m in metadata if not should_skip(emo_dir.joinpath(m[0].replace(\"audio-\", \"emo-\")), skip_existing)]\n        \n    # TODO: improve on the multiprocessing, it's terrible. Disk I/O is the bottleneck here.\n    # Embed the utterances in separate threads\n    func = partial(_emo_extract_from_utterance, hparams=hparams)\n    job = Pool(n_processes).imap(func, fpaths)\n    tuple(tqdm(job, \"Emo\", len(fpaths), unit=\"utterances\"))\n", "models/synthesizer/train.py": "import torch\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom models.synthesizer import audio\nfrom models.synthesizer.models.tacotron import Tacotron\nfrom models.synthesizer.synthesizer_dataset import SynthesizerDataset, collate_synthesizer\nfrom models.synthesizer.utils import ValueWindow, data_parallel_workaround\nfrom models.synthesizer.utils.plot import plot_spectrogram, plot_spectrogram_and_trace\nfrom models.synthesizer.utils.symbols import symbols\nfrom models.synthesizer.utils.text import sequence_to_text\nfrom models.vocoder.display import *\nfrom datetime import datetime\nimport json\nimport numpy as np\nfrom pathlib import Path\nimport time\nimport os\n\ndef np_now(x: torch.Tensor): return x.detach().cpu().numpy()\n\ndef time_string():\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\ndef train(run_id: str, syn_dir: str, models_dir: str, save_every: int,\n         backup_every: int, log_every:int, force_restart:bool, hparams):\n\n    syn_dir = Path(syn_dir)\n    models_dir = Path(models_dir)\n    models_dir.mkdir(exist_ok=True)\n\n    model_dir = models_dir.joinpath(run_id)\n    plot_dir = model_dir.joinpath(\"plots\")\n    wav_dir = model_dir.joinpath(\"wavs\")\n    mel_output_dir = model_dir.joinpath(\"mel-spectrograms\")\n    meta_folder = model_dir.joinpath(\"metas\")\n    model_dir.mkdir(exist_ok=True)\n    plot_dir.mkdir(exist_ok=True)\n    wav_dir.mkdir(exist_ok=True)\n    mel_output_dir.mkdir(exist_ok=True)\n    meta_folder.mkdir(exist_ok=True)\n    \n    weights_fpath = model_dir.joinpath(run_id).with_suffix(\".pt\")\n    metadata_fpath = syn_dir.joinpath(\"train.txt\")\n    \n    print(\"Checkpoint path: {}\".format(weights_fpath))\n    print(\"Loading training data from: {}\".format(metadata_fpath))\n    print(\"Using model: Tacotron\")\n    \n    # Book keeping\n    step = 0\n    time_window = ValueWindow(100)\n    loss_window = ValueWindow(100)\n    \n    \n    # From WaveRNN/train_tacotron.py\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n\n        for session in hparams.tts_schedule:\n            _, _, _, batch_size = session\n            if batch_size % torch.cuda.device_count() != 0:\n                raise ValueError(\"`batch_size` must be evenly divisible by n_gpus!\")\n    else:\n        device = torch.device(\"cpu\")\n    print(\"Using device:\", device)\n\n    # Instantiate Tacotron Model\n    print(\"\\nInitialising Tacotron Model...\\n\")\n    num_chars = len(symbols)\n    if weights_fpath.exists():\n        # for compatibility purpose, change symbols accordingly:\n        loaded_shape = torch.load(str(weights_fpath), map_location=device)[\"model_state\"][\"encoder.embedding.weight\"].shape\n        if num_chars != loaded_shape[0]:\n            print(\"WARNING: you are using compatible mode due to wrong sympols length, please modify varible _characters in `utils\\symbols.py`\")\n            num_chars != loaded_shape[0]\n                # Try to scan config file\n        model_config_fpaths = list(weights_fpath.parent.rglob(\"*.json\"))\n        if len(model_config_fpaths)>0 and model_config_fpaths[0].exists():\n            hparams.loadJson(model_config_fpaths[0])\n        else:  # save a config\n            hparams.dumpJson(weights_fpath.parent.joinpath(run_id).with_suffix(\".json\"))\n\n\n    model = Tacotron(embed_dims=hparams.tts_embed_dims,\n                     num_chars=num_chars,\n                     encoder_dims=hparams.tts_encoder_dims,\n                     decoder_dims=hparams.tts_decoder_dims,\n                     n_mels=hparams.num_mels,\n                     fft_bins=hparams.num_mels,\n                     postnet_dims=hparams.tts_postnet_dims,\n                     encoder_K=hparams.tts_encoder_K,\n                     lstm_dims=hparams.tts_lstm_dims,\n                     postnet_K=hparams.tts_postnet_K,\n                     num_highways=hparams.tts_num_highways,\n                     dropout=hparams.tts_dropout,\n                     stop_threshold=hparams.tts_stop_threshold,\n                     speaker_embedding_size=hparams.speaker_embedding_size).to(device)\n\n    # Initialize the optimizer\n    optimizer = optim.Adam(model.parameters(), amsgrad=True)\n\n    # Load the weights\n    if force_restart or not weights_fpath.exists():\n        print(\"\\nStarting the training of Tacotron from scratch\\n\")\n        model.save(weights_fpath)\n\n        # Embeddings metadata\n        char_embedding_fpath = meta_folder.joinpath(\"CharacterEmbeddings.tsv\")\n        with open(char_embedding_fpath, \"w\", encoding=\"utf-8\") as f:\n            for symbol in symbols:\n                if symbol == \" \":\n                    symbol = \"\\\\s\"  # For visual purposes, swap space with \\s\n\n                f.write(\"{}\\n\".format(symbol))\n\n    else:\n        print(\"\\nLoading weights at %s\" % weights_fpath)\n        model.load(weights_fpath, device, optimizer)\n        print(\"Tacotron weights loaded from step %d\" % model.step)\n    \n    # Initialize the dataset\n    metadata_fpath = syn_dir.joinpath(\"train.txt\")\n    mel_dir = syn_dir.joinpath(\"mels\")\n    embed_dir = syn_dir.joinpath(\"embeds\")\n    dataset = SynthesizerDataset(metadata_fpath, mel_dir, embed_dir, hparams)\n    test_loader = DataLoader(dataset,\n                             batch_size=1,\n                             shuffle=True,\n                             pin_memory=True)\n\n    # tracing training step\n    sw = SummaryWriter(log_dir=model_dir.joinpath(\"logs\"))\n\n    for i, session in enumerate(hparams.tts_schedule):\n        current_step = model.get_step()\n\n        r, lr, max_step, batch_size = session\n\n        training_steps = max_step - current_step\n\n        # Do we need to change to the next session?\n        if current_step >= max_step:\n            # Are there no further sessions than the current one?\n            if i == len(hparams.tts_schedule) - 1:\n                # We have completed training. Save the model and exit\n                model.save(weights_fpath, optimizer)\n                break\n            else:\n                # There is a following session, go to it\n                continue\n\n        model.r = r\n        # Begin the training\n        simple_table([(f\"Steps with r={r}\", str(training_steps // 1000) + \"k Steps\"),\n                      (\"Batch Size\", batch_size),\n                      (\"Learning Rate\", lr),\n                      (\"Outputs/Step (r)\", model.r)])\n\n        for p in optimizer.param_groups:\n            p[\"lr\"] = lr\n        if hparams.tts_finetune_layers is not None and len(hparams.tts_finetune_layers) > 0:\n            model.finetune_partial(hparams.tts_finetune_layers)\n\n        data_loader = DataLoader(dataset,\n                                 collate_fn=collate_synthesizer,\n                                 batch_size=batch_size, #change if you got graphic card OOM\n                                 num_workers=2,\n                                 shuffle=True,\n                                 pin_memory=True)\n\n        total_iters = len(dataset) \n        steps_per_epoch = np.ceil(total_iters / batch_size).astype(np.int32)\n        epochs = np.ceil(training_steps / steps_per_epoch).astype(np.int32)\n\n        for epoch in range(1, epochs+1):\n            for i, (texts, mels, embeds, idx) in enumerate(data_loader, 1):\n                start_time = time.time()\n\n                # Generate stop tokens for training\n                stop = torch.ones(mels.shape[0], mels.shape[2])\n                for j, k in enumerate(idx):\n                    stop[j, :int(dataset.metadata[k][4])-1] = 0\n\n                texts = texts.to(device)\n                mels = mels.to(device)\n                embeds = embeds.to(device)\n                stop = stop.to(device)\n\n                # Forward pass\n                # Parallelize model onto GPUS using workaround due to python bug\n                if device.type == \"cuda\" and torch.cuda.device_count() > 1:\n                    m1_hat, m2_hat, attention, stop_pred = data_parallel_workaround(model, texts,\n                                                                                    mels, embeds)\n                else:\n                    m1_hat, m2_hat, attention, stop_pred = model(texts, mels, embeds)\n\n                # Backward pass\n                m1_loss = F.mse_loss(m1_hat, mels) + F.l1_loss(m1_hat, mels)\n                m2_loss = F.mse_loss(m2_hat, mels)\n                stop_loss = F.binary_cross_entropy(stop_pred, stop)\n\n                loss = m1_loss + m2_loss + stop_loss\n\n                optimizer.zero_grad()\n                loss.backward()\n\n                if hparams.tts_clip_grad_norm is not None:\n                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), hparams.tts_clip_grad_norm)\n                    if np.isnan(grad_norm.cpu()):\n                        print(\"grad_norm was NaN!\")\n\n                optimizer.step()\n\n                time_window.append(time.time() - start_time)\n                loss_window.append(loss.item())\n\n                step = model.get_step()\n                k = step // 1000\n\n                \n                msg = f\"| Epoch: {epoch}/{epochs} ({i}/{steps_per_epoch}) | Loss: {loss_window.average:#.4} | {1./time_window.average:#.2} steps/s | Step: {k}k | \"\n                stream(msg)\n\n                if log_every != 0 and step % log_every == 0 :\n                    sw.add_scalar(\"training/loss\", loss_window.average, step)\n\n                # Backup or save model as appropriate\n                if backup_every != 0 and step % backup_every == 0 : \n                    backup_fpath = Path(\"{}/{}_{}.pt\".format(str(weights_fpath.parent), run_id, step))\n                    model.save(backup_fpath, optimizer)\n\n                if save_every != 0 and step % save_every == 0 : \n                    # Must save latest optimizer state to ensure that resuming training\n                    # doesn't produce artifacts\n                    model.save(weights_fpath, optimizer)\n                    \n\n                # Evaluate model to generate samples\n                epoch_eval = hparams.tts_eval_interval == -1 and i == steps_per_epoch  # If epoch is done\n                step_eval = hparams.tts_eval_interval > 0 and step % hparams.tts_eval_interval == 0  # Every N steps\n                if epoch_eval or step_eval:\n                    for sample_idx in range(hparams.tts_eval_num_samples):\n                        # At most, generate samples equal to number in the batch\n                        if sample_idx + 1 <= len(texts):\n                            # Remove padding from mels using frame length in metadata\n                            mel_length = int(dataset.metadata[idx[sample_idx]][4])\n                            mel_prediction = np_now(m2_hat[sample_idx]).T[:mel_length]\n                            target_spectrogram = np_now(mels[sample_idx]).T[:mel_length]\n                            attention_len = mel_length // model.r\n                            # eval_loss = F.mse_loss(mel_prediction, target_spectrogram)\n                            # sw.add_scalar(\"validing/loss\", eval_loss.item(), step)\n                            eval_model(attention=np_now(attention[sample_idx][:, :attention_len]),\n                                       mel_prediction=mel_prediction,\n                                       target_spectrogram=target_spectrogram,\n                                       input_seq=np_now(texts[sample_idx]),\n                                       step=step,\n                                       plot_dir=plot_dir,\n                                       mel_output_dir=mel_output_dir,\n                                       wav_dir=wav_dir,\n                                       sample_num=sample_idx + 1,\n                                       loss=loss,\n                                       hparams=hparams,\n                                       sw=sw)\n                    MAX_SAVED_COUNT = 20\n                    if (step / hparams.tts_eval_interval) % MAX_SAVED_COUNT == 0:\n                        # clean up and save last MAX_SAVED_COUNT;\n                        plots = next(os.walk(plot_dir), (None, None, []))[2]\n                        for plot in plots[-MAX_SAVED_COUNT:]:\n                            os.remove(plot_dir.joinpath(plot))\n                        mel_files = next(os.walk(mel_output_dir), (None, None, []))[2]\n                        for mel_file in mel_files[-MAX_SAVED_COUNT:]:\n                            os.remove(mel_output_dir.joinpath(mel_file))\n                        wavs = next(os.walk(wav_dir), (None, None, []))[2]\n                        for w in wavs[-MAX_SAVED_COUNT:]:\n                            os.remove(wav_dir.joinpath(w))\n                        \n                # Break out of loop to update training schedule\n                if step >= max_step:\n                    break\n\n            # Add line break after every epoch\n            print(\"\")\n\ndef eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams, sw):\n    # Save some results for evaluation\n    attention_path = str(plot_dir.joinpath(\"attention_step_{}_sample_{}\".format(step, sample_num)))\n    # save_attention(attention, attention_path)\n    save_and_trace_attention(attention, attention_path, sw, step)\n\n    # save predicted mel spectrogram to disk (debug)\n    mel_output_fpath = mel_output_dir.joinpath(\"mel-prediction-step-{}_sample_{}.npy\".format(step, sample_num))\n    np.save(str(mel_output_fpath), mel_prediction, allow_pickle=False)\n\n    # save griffin lim inverted wav for debug (mel -> wav)\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_fpath = wav_dir.joinpath(\"step-{}-wave-from-mel_sample_{}.wav\".format(step, sample_num))\n    audio.save_wav(wav, str(wav_fpath), sr=hparams.sample_rate)\n\n    # save real and predicted mel-spectrogram plot to disk (control purposes)\n    spec_fpath = plot_dir.joinpath(\"step-{}-mel-spectrogram_sample_{}.png\".format(step, sample_num))\n    title_str = \"{}, {}, step={}, loss={:.5f}\".format(\"Tacotron\", time_string(), step, loss)\n    # plot_spectrogram(mel_prediction, str(spec_fpath), title=title_str,\n    #                  target_spectrogram=target_spectrogram,\n    #                  max_len=target_spectrogram.size // hparams.num_mels)\n    plot_spectrogram_and_trace(\n        mel_prediction, \n        str(spec_fpath), \n        title=title_str,\n        target_spectrogram=target_spectrogram,\n        max_len=target_spectrogram.size // hparams.num_mels,\n        sw=sw,\n        step=step)\n    print(\"Input at step {}: {}\".format(step, sequence_to_text(input_seq)))\n", "models/synthesizer/preprocess_transcript.py": "def preprocess_transcript_aishell3(dict_info, dict_transcript):\n    for v in dict_transcript:\n        if not v:\n            continue\n        v = v.strip().replace(\"\\n\",\"\").replace(\"\\t\",\" \").split(\" \")\n        transList = []\n        for i in range(2, len(v), 2):\n            transList.append(v[i])\n        dict_info[v[0]] = \" \".join(transList)\n\n\ndef preprocess_transcript_magicdata(dict_info, dict_transcript):\n    for v in dict_transcript:\n        if not v:\n            continue\n        v = v.strip().replace(\"\\n\",\"\").replace(\"\\t\",\" \").split(\" \")\n        dict_info[v[0]] = \" \".join(v[2:])\n       ", "models/synthesizer/synthesizer_dataset.py": "import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom pathlib import Path\nfrom models.synthesizer.utils.text import text_to_sequence\n\n\nclass SynthesizerDataset(Dataset):\n    def __init__(self, metadata_fpath: Path, mel_dir: Path, embed_dir: Path, hparams):\n        print(\"Using inputs from:\\n\\t%s\\n\\t%s\\n\\t%s\" % (metadata_fpath, mel_dir, embed_dir))\n        \n        with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n            metadata = [line.split(\"|\") for line in metadata_file]\n        \n        mel_fnames = [x[1] for x in metadata if int(x[4])]\n        mel_fpaths = [mel_dir.joinpath(fname) for fname in mel_fnames]\n        embed_fnames = [x[2] for x in metadata if int(x[4])]\n        embed_fpaths = [embed_dir.joinpath(fname) for fname in embed_fnames]\n        self.samples_fpaths = list(zip(mel_fpaths, embed_fpaths))\n        self.samples_texts = [x[5].strip() for x in metadata if int(x[4])]\n        self.metadata = metadata\n        self.hparams = hparams\n        \n        print(\"Found %d samples\" % len(self.samples_fpaths))\n    \n    def __getitem__(self, index):  \n        # Sometimes index may be a list of 2 (not sure why this happens)\n        # If that is the case, return a single item corresponding to first element in index\n        if index is list:\n            index = index[0]\n\n        mel_path, embed_path = self.samples_fpaths[index]\n        mel = np.load(mel_path).T.astype(np.float32)\n        \n        # Load the embed\n        embed = np.load(embed_path)\n\n        # Get the text and clean it\n        text = text_to_sequence(self.samples_texts[index], self.hparams.tts_cleaner_names)\n        \n        # Convert the list returned by text_to_sequence to a numpy array\n        text = np.asarray(text).astype(np.int32)\n\n        return text, mel.astype(np.float32), embed.astype(np.float32), index\n\n    def __len__(self):\n        return len(self.samples_fpaths)\n\n\ndef collate_synthesizer(batch):\n    # Text\n    x_lens = [len(x[0]) for x in batch]\n    max_x_len = max(x_lens)\n\n    chars = [pad1d(x[0], max_x_len) for x in batch]\n    chars = np.stack(chars)\n\n    # Mel spectrogram\n    spec_lens = [x[1].shape[-1] for x in batch]\n    max_spec_len = max(spec_lens) + 1 \n    if max_spec_len % 2 != 0:  # FIXIT: Hardcoded due to incompatibility with Windows (no lambda)\n        max_spec_len += 2 - max_spec_len % 2\n\n    # WaveRNN mel spectrograms are normalized to [0, 1] so zero padding adds silence\n    # By default, SV2TTS uses symmetric mels, where -1*max_abs_value is silence.\n    # if hparams.symmetric_mels:\n    #     mel_pad_value = -1 * hparams.max_abs_value\n    # else:\n    #     mel_pad_value = 0\n    mel_pad_value = -4 # FIXIT: Hardcoded due to incompatibility with Windows (no lambda)\n    mel = [pad2d(x[1], max_spec_len, pad_value=mel_pad_value) for x in batch]\n    mel = np.stack(mel)\n\n    # Speaker embedding (SV2TTS)\n    embeds = [x[2] for x in batch]\n    embeds = np.stack(embeds)\n\n    # Index (for vocoder preprocessing)\n    indices = [x[3] for x in batch]\n\n\n    # Convert all to tensor\n    chars = torch.tensor(chars).long()\n    mel = torch.tensor(mel)\n    embeds = torch.tensor(embeds)\n\n    return chars, mel, embeds, indices\n\ndef pad1d(x, max_len, pad_value=0):\n    return np.pad(x, (0, max_len - len(x)), mode=\"constant\", constant_values=pad_value)\n\ndef pad2d(x, max_len, pad_value=0):\n    return np.pad(x, ((0, 0), (0, max_len - x.shape[-1])), mode=\"constant\", constant_values=pad_value)\n", "models/synthesizer/preprocess_audio.py": "import librosa\nimport numpy as np\n\nfrom models.encoder import inference as encoder\nfrom utils import logmmse\nfrom models.synthesizer import audio\nfrom pathlib import Path\nfrom pypinyin import Style\nfrom pypinyin.contrib.neutral_tone import NeutralToneWith5Mixin\nfrom pypinyin.converter import DefaultConverter\nfrom pypinyin.core import Pinyin\nimport torch\nfrom transformers import Wav2Vec2Processor\nfrom .models.wav2emo import EmotionExtractorModel\n\nclass PinyinConverter(NeutralToneWith5Mixin, DefaultConverter):\n    pass\n\npinyin = Pinyin(PinyinConverter()).pinyin\n\n\n# load model from hub \ndevice = 'cuda' if torch.cuda.is_available() else \"cpu\"\nmodel_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\nprocessor = Wav2Vec2Processor.from_pretrained(model_name)\nmodel = EmotionExtractorModel.from_pretrained(model_name).to(device)\n\ndef extract_emo(\n    x: np.ndarray,\n    sampling_rate: int,\n    embeddings: bool = False,\n) -> np.ndarray:\n    r\"\"\"Predict emotions or extract embeddings from raw audio signal.\"\"\"\n    y = processor(x, sampling_rate=sampling_rate)\n    y = y['input_values'][0]\n    y = torch.from_numpy(y).to(device)\n\n    # run through model\n    with torch.no_grad():\n        y = model(y)[0 if embeddings else 1]\n\n    # convert to numpy\n    y = y.detach().cpu().numpy()\n\n    return y\n\ndef _process_utterance(wav: np.ndarray, text: str, out_dir: Path, basename: str, \n                      mel_fpath: str, wav_fpath: str, hparams, encoder_model_fpath):\n    ## FOR REFERENCE:\n    # For you not to lose your head if you ever wish to change things here or implement your own\n    # synthesizer.\n    # - Both the audios and the mel spectrograms are saved as numpy arrays\n    # - There is no processing done to the audios that will be saved to disk beyond volume  \n    #   normalization (in split_on_silences)\n    # - However, pre-emphasis is applied to the audios before computing the mel spectrogram. This\n    #   is why we re-apply it on the audio on the side of the vocoder.\n    # - Librosa pads the waveform before computing the mel spectrogram. Here, the waveform is saved\n    #   without extra padding. This means that you won't have an exact relation between the length\n    #   of the wav and of the mel spectrogram. See the vocoder data loader.\n        \n    # Trim silence\n    if hparams.trim_silence:\n        if not encoder.is_loaded():\n            encoder.load_model(encoder_model_fpath)\n        wav = encoder.preprocess_wav(wav, normalize=False, trim_silence=True)\n    \n    # Skip utterances that are too short\n    if len(wav) < hparams.utterance_min_duration * hparams.sample_rate:\n        return None\n    \n    # Compute the mel spectrogram\n    mel_spectrogram = audio.melspectrogram(wav, hparams).astype(np.float32)\n    mel_frames = mel_spectrogram.shape[1]\n    \n    # Skip utterances that are too long\n    if mel_frames > hparams.max_mel_frames and hparams.clip_mels_length:\n        return None\n    # Write the spectrogram, embed and audio to disk\n    np.save(mel_fpath, mel_spectrogram.T, allow_pickle=False)\n    np.save(wav_fpath, wav, allow_pickle=False)\n\n    # Return a tuple describing this training example\n    return wav_fpath.name, mel_fpath.name, \"embed-%s.npy\" % basename, wav, mel_frames, text\n \n\ndef _split_on_silences(wav_fpath, words, hparams):\n    # Load the audio waveform\n    wav, _ = librosa.load(wav_fpath, sr= hparams.sample_rate)\n    wav = librosa.effects.trim(wav, top_db= 40, frame_length=2048, hop_length=1024)[0]\n    if hparams.rescale:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n    # denoise, we may not need it here.\n    if len(wav) > hparams.sample_rate*(0.3+0.1):\n        noise_wav = np.concatenate([wav[:int(hparams.sample_rate*0.15)],\n                                    wav[-int(hparams.sample_rate*0.15):]])\n        profile = logmmse.profile_noise(noise_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    resp = pinyin(words, style=Style.TONE3)\n    res = filter(lambda v : not v.isspace(),map(lambda v: v[0],resp)) \n    res = \" \".join(res)\n\n    return wav, res\n\ndef preprocess_general(speaker_dir, out_dir: Path, skip_existing: bool, hparams, dict_info, no_alignments: bool, encoder_model_fpath: Path):\n    metadata = []\n    extensions = (\"*.wav\", \"*.flac\", \"*.mp3\")\n    for extension in extensions:\n        wav_fpath_list = speaker_dir.glob(extension)\n        # Iterate over each wav\n        for wav_fpath in wav_fpath_list:\n            words = dict_info.get(wav_fpath.name.split(\".\")[0])\n            if not words:\n                words = dict_info.get(wav_fpath.name) # try with extension \n                if not words:\n                    print(f\"No word found in dict_info for {wav_fpath.name}, skip it\")\n                    continue\n            sub_basename = \"%s_%02d\" % (wav_fpath.name, 0)\n            mel_fpath = out_dir.joinpath(\"mels\", f\"mel-{sub_basename}.npy\")\n            wav_fpath = out_dir.joinpath(\"audio\", f\"audio-{sub_basename}.npy\")\n            \n            if skip_existing and mel_fpath.exists() and wav_fpath.exists():\n                continue\n            wav, text = _split_on_silences(wav_fpath, words, hparams)\n            result = _process_utterance(wav, text, out_dir, sub_basename, \n                                                False, hparams, encoder_model_fpath) # accelarate\n            if result is None:\n                continue\n            wav_fpath_name, mel_fpath_name, embed_fpath_name, wav, mel_frames, text = result\n            metadata.append ((wav_fpath_name, mel_fpath_name, embed_fpath_name, len(wav), mel_frames, text))\n\n    return metadata\n", "models/synthesizer/vits_dataset.py": "import os\nimport random\nimport numpy as np\nimport torch.nn.functional as F\nimport torch\nimport torch.utils.data\n\nfrom utils.audio_utils import load_wav_to_torch, spectrogram\nfrom utils.util import intersperse\nfrom models.synthesizer.utils.text import text_to_sequence\n\n\n\"\"\"Multi speaker version\"\"\"\nclass VitsDataset(torch.utils.data.Dataset):\n    \"\"\"\n        1) loads audio, speaker_id, text pairs\n        2) normalizes text and converts them to sequences of integers\n        3) computes spectrograms from audio files.\n    \"\"\"\n    def __init__(self, audio_file_path, hparams):\n        with open(audio_file_path, encoding='utf-8') as f:\n            self.audio_metadata = [line.strip().split('|') for line in f]\n        self.text_cleaners = hparams.text_cleaners\n        self.max_wav_value = hparams.max_wav_value\n        self.sampling_rate = hparams.sampling_rate\n        self.filter_length  = hparams.filter_length\n        self.hop_length     = hparams.hop_length\n        self.win_length     = hparams.win_length\n        self.sampling_rate  = hparams.sampling_rate\n\n        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n\n        self.add_blank = hparams.add_blank\n        self.datasets_root = hparams.datasets_root\n\n        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n\n        random.seed(1234)\n        random.shuffle(self.audio_metadata)\n        self._filter()\n\n    def _filter(self):\n        \"\"\"\n        Filter text & store spec lengths\n        \"\"\"\n        # Store spectrogram lengths for Bucketing\n        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n        # spec_length = wav_length // hop_length\n\n        audio_metadata_new = []\n        lengths = []\n        \n        # for audiopath, sid, text in self.audio_metadata:\n        for wav_fpath, mel_fpath, embed_path, wav_length, mel_frames, text, spkid in self.audio_metadata:\n            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n                audio_metadata_new.append([wav_fpath, mel_fpath, embed_path, wav_length, mel_frames, text, spkid])\n                lengths.append(os.path.getsize(f'{self.datasets_root}{os.sep}audio{os.sep}{wav_fpath}') // (2 * self.hop_length))\n        self.audio_metadata = audio_metadata_new\n        self.lengths = lengths\n\n    def get_audio_text_speaker_pair(self, audio_metadata):\n        # separate filename, speaker_id and text\n        wav_fpath, text, sid = audio_metadata[0], audio_metadata[5], audio_metadata[6]\n        text = self.get_text(text)\n\n        spec, wav = self.get_audio(f'{self.datasets_root}{os.sep}audio{os.sep}{wav_fpath}')\n        sid = self.get_sid(sid)\n        emo = torch.FloatTensor(np.load(f'{self.datasets_root}{os.sep}emo{os.sep}{wav_fpath.replace(\"audio\", \"emo\")}'))\n        return (text, spec, wav, sid, emo)\n\n    def get_audio(self, filename):\n        # Load preprocessed wav npy instead of reading from wav file\n        audio = torch.FloatTensor(np.load(filename))\n        audio_norm = audio.unsqueeze(0)\n\n        spec_filename = filename.replace(\".wav\", \".spec\")\n        if os.path.exists(spec_filename):\n            spec = torch.load(spec_filename)\n        else:\n            spec = spectrogram(audio_norm, self.filter_length,self.hop_length, self.win_length,\n                center=False)\n            torch.save(spec, spec_filename)\n        spec = torch.squeeze(spec, 0)\n        return spec, audio_norm\n\n    def get_text(self, text):\n        if self.cleaned_text:\n            text_norm = text_to_sequence(text, self.text_cleaners)\n        if self.add_blank:\n            text_norm = intersperse(text_norm, 0) # \u5728\u6240\u6709\u6587\u672c\u6570\u503c\u5e8f\u5217\u4e2d\u7684\u5143\u7d20\u524d\u540e\u90fd\u8865\u5145\u4e00\u4e2a0 - \u4e0d\u9002\u7528\u4e8e\u4e2d\u6587\n        text_norm = torch.LongTensor(text_norm)\n        return text_norm\n\n    def get_sid(self, sid):\n        sid = torch.LongTensor([int(sid)])\n        return sid\n\n    def __getitem__(self, index):\n        return self.get_audio_text_speaker_pair(self.audio_metadata[index])\n\n    def __len__(self):\n        return len(self.audio_metadata)\n\n\nclass VitsDatasetCollate():\n    \"\"\" Zero-pads model inputs and targets\n    \"\"\"\n    def __init__(self, return_ids=False):\n        self.return_ids = return_ids\n\n    def __call__(self, batch):\n        \"\"\"Collate's training batch from normalized text, audio and speaker identities\n        PARAMS\n        ------\n        batch: [text_normalized, spec_normalized, wav_normalized, sid]\n        \"\"\"\n        # Right zero-pad all one-hot text sequences to max input length\n        _, ids_sorted_decreasing = torch.sort(\n            torch.LongTensor([x[1].size(1) for x in batch]),\n            dim=0, descending=True)\n\n        max_text_len = max([len(x[0]) for x in batch])\n        max_spec_len = max([x[1].size(1) for x in batch])\n        max_wav_len = max([x[2].size(1) for x in batch])\n\n        text_lengths = torch.LongTensor(len(batch))\n        spec_lengths = torch.LongTensor(len(batch))\n        wav_lengths = torch.LongTensor(len(batch))\n        sid = torch.LongTensor(len(batch))\n\n        text_padded = torch.LongTensor(len(batch), max_text_len)\n        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n        emo = torch.FloatTensor(len(batch), 1024)\n\n        text_padded.zero_()\n        spec_padded.zero_()\n        wav_padded.zero_()\n        emo.zero_()\n\n        for i in range(len(ids_sorted_decreasing)):\n            row = batch[ids_sorted_decreasing[i]]\n\n            text = row[0]\n            text_padded[i, :text.size(0)] = text\n            text_lengths[i] = text.size(0)\n\n            spec = row[1]\n            spec_padded[i, :, :spec.size(1)] = spec\n            spec_lengths[i] = spec.size(1)\n\n            wav = row[2]\n            wav_padded[i, :, :wav.size(1)] = wav\n            wav_lengths[i] = wav.size(1)\n\n            sid[i] = row[3]\n\n            emo[i, :] = row[4]\n\n        if self.return_ids:\n            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid, ids_sorted_decreasing, emo\n        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid, emo\n\n\nclass DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):\n    \"\"\"\n    Maintain similar input lengths in a batch.\n    Length groups are specified by boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\n  \n    It removes samples which are not included in the boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\n    \"\"\"\n    def __init__(self, dataset, batch_size, boundaries, num_replicas=None, rank=None, shuffle=True):\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\n        self.lengths = dataset.lengths\n        self.batch_size = batch_size\n        self.boundaries = boundaries\n  \n        self.buckets, self.num_samples_per_bucket = self._create_buckets()\n        self.total_size = sum(self.num_samples_per_bucket)\n        self.num_samples = self.total_size // self.num_replicas\n  \n    def _create_buckets(self):\n        buckets = [[] for _ in range(len(self.boundaries) - 1)]\n        for i in range(len(self.lengths)):\n            length = self.lengths[i]\n            idx_bucket = self._bisect(length)\n            if idx_bucket != -1:\n                buckets[idx_bucket].append(i)\n  \n        for i in range(len(buckets) - 1, 0, -1):\n            if len(buckets[i]) == 0:\n                buckets.pop(i)\n                self.boundaries.pop(i+1)\n  \n        num_samples_per_bucket = []\n        for i in range(len(buckets)):\n            len_bucket = len(buckets[i])\n            total_batch_size = self.num_replicas * self.batch_size\n            rem = (total_batch_size - (len_bucket % total_batch_size)) % total_batch_size\n            num_samples_per_bucket.append(len_bucket + rem)\n        return buckets, num_samples_per_bucket\n  \n    def __iter__(self):\n      # deterministically shuffle based on epoch\n      g = torch.Generator()\n      g.manual_seed(self.epoch)\n  \n      indices = []\n      if self.shuffle:\n          for bucket in self.buckets:\n              indices.append(torch.randperm(len(bucket), generator=g).tolist())\n      else:\n          for bucket in self.buckets:\n              indices.append(list(range(len(bucket))))\n  \n      batches = []\n      for i in range(len(self.buckets)):\n          bucket = self.buckets[i]\n          len_bucket = len(bucket)\n          ids_bucket = indices[i]\n          num_samples_bucket = self.num_samples_per_bucket[i]\n  \n          # add extra samples to make it evenly divisible\n          rem = num_samples_bucket - len_bucket\n          ids_bucket = ids_bucket + ids_bucket * (rem // len_bucket) + ids_bucket[:(rem % len_bucket)]\n  \n          # subsample\n          ids_bucket = ids_bucket[self.rank::self.num_replicas]\n  \n          # batching\n          for j in range(len(ids_bucket) // self.batch_size):\n              batch = [bucket[idx] for idx in ids_bucket[j*self.batch_size:(j+1)*self.batch_size]]\n              batches.append(batch)\n  \n      if self.shuffle:\n          batch_ids = torch.randperm(len(batches), generator=g).tolist()\n          batches = [batches[i] for i in batch_ids]\n      self.batches = batches\n  \n      assert len(self.batches) * self.batch_size == self.num_samples\n      return iter(self.batches)\n  \n    def _bisect(self, x, lo=0, hi=None):\n      if hi is None:\n          hi = len(self.boundaries) - 1\n  \n      if hi > lo:\n          mid = (hi + lo) // 2\n          if self.boundaries[mid] < x and x <= self.boundaries[mid+1]:\n              return mid\n          elif x <= self.boundaries[mid]:\n              return self._bisect(x, lo, mid)\n          else:\n              return self._bisect(x, mid + 1, hi)\n      else:\n          return -1\n\n    def __len__(self):\n        return self.num_samples // self.batch_size\n", "models/synthesizer/audio.py": "import librosa\nimport librosa.filters\nimport numpy as np\nfrom scipy import signal\nfrom scipy.io import wavfile\nimport soundfile as sf\n\n\ndef load_wav(path, sr):\n    return librosa.core.load(path, sr=sr)[0]\n\ndef save_wav(wav, path, sr):\n    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n    #proposed by @dsmiller\n    wavfile.write(path, sr, wav.astype(np.int16))\n\ndef save_wavenet_wav(wav, path, sr):\n    sf.write(path, wav.astype(np.float32), sr)\n\ndef preemphasis(wav, k, preemphasize=True):\n    if preemphasize:\n        return signal.lfilter([1, -k], [1], wav)\n    return wav\n\ndef inv_preemphasis(wav, k, inv_preemphasize=True):\n    if inv_preemphasize:\n        return signal.lfilter([1], [1, -k], wav)\n    return wav\n\n#From https://github.com/r9y9/wavenet_vocoder/blob/master/audio.py\ndef start_and_end_indices(quantized, silence_threshold=2):\n    for start in range(quantized.size):\n        if abs(quantized[start] - 127) > silence_threshold:\n            break\n    for end in range(quantized.size - 1, 1, -1):\n        if abs(quantized[end] - 127) > silence_threshold:\n            break\n    \n    assert abs(quantized[start] - 127) > silence_threshold\n    assert abs(quantized[end] - 127) > silence_threshold\n    \n    return start, end\n\ndef get_hop_size(hparams):\n    hop_size = hparams.hop_size\n    if hop_size is None:\n        assert hparams.frame_shift_ms is not None\n        hop_size = int(hparams.frame_shift_ms / 1000 * hparams.sample_rate)\n    return hop_size\n\ndef linearspectrogram(wav, hparams):\n    D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)\n    S = _amp_to_db(np.abs(D), hparams) - hparams.ref_level_db\n    \n    if hparams.signal_normalization:\n        return _normalize(S, hparams)\n    return S\n\ndef melspectrogram(wav, hparams):\n    D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)\n    S = _amp_to_db(_linear_to_mel(np.abs(D), hparams), hparams) - hparams.ref_level_db\n    \n    if hparams.signal_normalization:\n        return _normalize(S, hparams)\n    return S\n\ndef inv_linear_spectrogram(linear_spectrogram, hparams):\n    \"\"\"Converts linear spectrogram to waveform using librosa\"\"\"\n    if hparams.signal_normalization:\n        D = _denormalize(linear_spectrogram, hparams)\n    else:\n        D = linear_spectrogram\n    \n    S = _db_to_amp(D + hparams.ref_level_db) #Convert back to linear\n    \n    if hparams.use_lws:\n        processor = _lws_processor(hparams)\n        D = processor.run_lws(S.astype(np.float64).T ** hparams.power)\n        y = processor.istft(D).astype(np.float32)\n        return inv_preemphasis(y, hparams.preemphasis, hparams.preemphasize)\n    else:\n        return inv_preemphasis(_griffin_lim(S ** hparams.power, hparams), hparams.preemphasis, hparams.preemphasize)\n\ndef inv_mel_spectrogram(mel_spectrogram, hparams):\n    \"\"\"Converts mel spectrogram to waveform using librosa\"\"\"\n    if hparams.signal_normalization:\n        D = _denormalize(mel_spectrogram, hparams)\n    else:\n        D = mel_spectrogram\n    \n    S = _mel_to_linear(_db_to_amp(D + hparams.ref_level_db), hparams)  # Convert back to linear\n    \n    if hparams.use_lws:\n        processor = _lws_processor(hparams)\n        D = processor.run_lws(S.astype(np.float64).T ** hparams.power)\n        y = processor.istft(D).astype(np.float32)\n        return inv_preemphasis(y, hparams.preemphasis, hparams.preemphasize)\n    else:\n        return inv_preemphasis(_griffin_lim(S ** hparams.power, hparams), hparams.preemphasis, hparams.preemphasize)\n\ndef _lws_processor(hparams):\n    import lws\n    return lws.lws(hparams.n_fft, get_hop_size(hparams), fftsize=hparams.win_size, mode=\"speech\")\n\ndef _griffin_lim(S, hparams):\n    \"\"\"librosa implementation of Griffin-Lim\n    Based on https://github.com/librosa/librosa/issues/434\n    \"\"\"\n    angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n    S_complex = np.abs(S).astype(complex)\n    y = _istft(S_complex * angles, hparams)\n    for i in range(hparams.griffin_lim_iters):\n        angles = np.exp(1j * np.angle(_stft(y, hparams)))\n        y = _istft(S_complex * angles, hparams)\n    return y\n\ndef _stft(y, hparams):\n    if hparams.use_lws:\n        return _lws_processor(hparams).stft(y).T\n    else:\n        return librosa.stft(y=y, n_fft=hparams.n_fft, hop_length=get_hop_size(hparams), win_length=hparams.win_size)\n\ndef _istft(y, hparams):\n    return librosa.istft(y, hop_length=get_hop_size(hparams), win_length=hparams.win_size)\n\n##########################################################\n#Those are only correct when using lws!!! (This was messing with Wavenet quality for a long time!)\ndef num_frames(length, fsize, fshift):\n    \"\"\"Compute number of time frames of spectrogram\n    \"\"\"\n    pad = (fsize - fshift)\n    if length % fshift == 0:\n        M = (length + pad * 2 - fsize) // fshift + 1\n    else:\n        M = (length + pad * 2 - fsize) // fshift + 2\n    return M\n\n\ndef pad_lr(x, fsize, fshift):\n    \"\"\"Compute left and right padding\n    \"\"\"\n    M = num_frames(len(x), fsize, fshift)\n    pad = (fsize - fshift)\n    T = len(x) + 2 * pad\n    r = (M - 1) * fshift + fsize - T\n    return pad, pad + r\n##########################################################\n#Librosa correct padding\ndef librosa_pad_lr(x, fsize, fshift):\n    return 0, (x.shape[0] // fshift + 1) * fshift - x.shape[0]\n\n# Conversions\n_mel_basis = None\n_inv_mel_basis = None\n\ndef _linear_to_mel(spectogram, hparams):\n    global _mel_basis\n    if _mel_basis is None:\n        _mel_basis = _build_mel_basis(hparams)\n    return np.dot(_mel_basis, spectogram)\n\ndef _mel_to_linear(mel_spectrogram, hparams):\n    global _inv_mel_basis\n    if _inv_mel_basis is None:\n        _inv_mel_basis = np.linalg.pinv(_build_mel_basis(hparams))\n    return np.maximum(1e-10, np.dot(_inv_mel_basis, mel_spectrogram))\n\ndef _build_mel_basis(hparams):\n    assert hparams.fmax <= hparams.sample_rate // 2\n    return librosa.filters.mel(sr=hparams.sample_rate, n_fft=hparams.n_fft, n_mels=hparams.num_mels,\n                               fmin=hparams.fmin, fmax=hparams.fmax)\n\ndef _amp_to_db(x, hparams):\n    min_level = np.exp(hparams.min_level_db / 20 * np.log(10))\n    return 20 * np.log10(np.maximum(min_level, x))\n\ndef _db_to_amp(x):\n    return np.power(10.0, (x) * 0.05)\n\ndef _normalize(S, hparams):\n    if hparams.allow_clipping_in_normalization:\n        if hparams.symmetric_mels:\n            return np.clip((2 * hparams.max_abs_value) * ((S - hparams.min_level_db) / (-hparams.min_level_db)) - hparams.max_abs_value,\n                           -hparams.max_abs_value, hparams.max_abs_value)\n        else:\n            return np.clip(hparams.max_abs_value * ((S - hparams.min_level_db) / (-hparams.min_level_db)), 0, hparams.max_abs_value)\n    \n    assert S.max() <= 0 and S.min() - hparams.min_level_db >= 0\n    if hparams.symmetric_mels:\n        return (2 * hparams.max_abs_value) * ((S - hparams.min_level_db) / (-hparams.min_level_db)) - hparams.max_abs_value\n    else:\n        return hparams.max_abs_value * ((S - hparams.min_level_db) / (-hparams.min_level_db))\n\ndef _denormalize(D, hparams):\n    if hparams.allow_clipping_in_normalization:\n        if hparams.symmetric_mels:\n            return (((np.clip(D, -hparams.max_abs_value,\n                              hparams.max_abs_value) + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value))\n                    + hparams.min_level_db)\n        else:\n            return ((np.clip(D, 0, hparams.max_abs_value) * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n    \n    if hparams.symmetric_mels:\n        return (((D + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value)) + hparams.min_level_db)\n    else:\n        return ((D * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n", "models/synthesizer/hparams.py": "from utils.hparams import HParams\n\nhparams = HParams(\n        ### Signal Processing (used in both synthesizer and vocoder)\n        sample_rate = 16000,\n        n_fft = 1024, # filter_length\n        num_mels = 80,\n        hop_size = 256,                             # Tacotron uses 12.5 ms frame shift (set to sample_rate * 0.0125)\n        win_size = 1024,                             # Tacotron uses 50 ms frame length (set to sample_rate * 0.050)\n        fmin = 55,\n        min_level_db = -100,\n        ref_level_db = 20,\n        max_abs_value = 4.,                         # Gradient explodes if too big, premature convergence if too small.\n        preemphasis = 0.97,                         # Filter coefficient to use if preemphasize is True\n        preemphasize = True,\n\n        ### Tacotron Text-to-Speech (TTS)\n        tts_embed_dims = 512,                       # Embedding dimension for the graphemes/phoneme inputs\n        tts_encoder_dims = 256,\n        tts_decoder_dims = 128,\n        tts_postnet_dims = 512,\n        tts_encoder_K = 5,\n        tts_lstm_dims = 1024,\n        tts_postnet_K = 5,\n        tts_num_highways = 4,\n        tts_dropout = 0.5,\n        tts_cleaner_names = [\"basic_cleaners\"],\n        tts_stop_threshold = -3.4,                  # Value below which audio generation ends.\n                                                    # For example, for a range of [-4, 4], this\n                                                    # will terminate the sequence at the first\n                                                    # frame that has all values < -3.4\n\n        ### Tacotron Training\n        tts_schedule = [(2,  1e-3,  10_000,  12),   # Progressive training schedule\n                        (2,  5e-4,  15_000,  12),   # (r, lr, step, batch_size)\n                        (2,  2e-4,  20_000,  12),   # (r, lr, step, batch_size)\n                        (2,  1e-4,  30_000,  12),   #\n                        (2,  5e-5,  40_000,  12),   #\n                        (2,  1e-5,  60_000,  12),   #\n                        (2,  5e-6, 160_000,  12),   # r = reduction factor (# of mel frames\n                        (2,  3e-6, 320_000,  12),   #     synthesized for each decoder iteration)\n                        (2,  1e-6, 640_000,  12)],  # lr = learning rate\n\n        tts_clip_grad_norm = 1.0,                   # clips the gradient norm to prevent explosion - set to None if not needed\n        tts_eval_interval = 500,                    # Number of steps between model evaluation (sample generation)\n                                                    # Set to -1 to generate after completing epoch, or 0 to disable\n        tts_eval_num_samples = 1,                   # Makes this number of samples\n\n        ## For finetune usage, if set, only selected layers will be trained, available: encoder,encoder_proj,gst,decoder,postnet,post_proj\n        tts_finetune_layers = [], \n\n        ### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.\n\n        ### Mel Visualization and Griffin-Lim\n        signal_normalization = True,\n        power = 1.5,\n        griffin_lim_iters = 60,\n\n        ### Audio processing options\n        fmax = 7600,                                # Should not exceed (sample_rate // 2)\n        allow_clipping_in_normalization = True,     # Used when signal_normalization = True\n        clip_mels_length = True,                    # If true, discards samples exceeding max_mel_frames\n        use_lws = False,                            # \"Fast spectrogram phase recovery using local weighted sums\"\n        symmetric_mels = True,                      # Sets mel range to [-max_abs_value, max_abs_value] if True,\n                                                    #               and [0, max_abs_value] if False\n        trim_silence = False,                        # Use with sample_rate of 16000 for best results\n\n        ### SV2TTS\n        speaker_embedding_size = 256,               # Dimension for the speaker embedding\n        silence_min_duration_split = 0.4,           # Duration in seconds of a silence for an utterance to be split\n        utterance_min_duration = 0.5,               # Duration in seconds below which utterances are discarded\n        use_gst = True,                             # Whether to use global style token    \n        use_ser_for_gst = True,                     # Whether to use speaker embedding referenced for global style token  \n        )\n", "models/synthesizer/gst_hyperparameters.py": "class GSTHyperparameters():\n    E = 512\n\n    # reference encoder\n    ref_enc_filters = [32, 32, 64, 64, 128, 128]\n\n    # style token layer\n    token_num = 10\n    # token_emb_size = 256\n    num_heads = 8\n\n    n_mels = 256  # Number of Mel banks to generate\n    \n", "models/synthesizer/__init__.py": "", "models/synthesizer/inference.py": "import torch\nfrom models.synthesizer import audio\nfrom models.synthesizer.hparams import hparams\nfrom models.synthesizer.models.tacotron import Tacotron\nfrom models.synthesizer.utils.symbols import symbols\nfrom models.synthesizer.utils.text import text_to_sequence\nfrom models.vocoder.display import simple_table\nfrom pathlib import Path\nfrom typing import Union, List\nimport numpy as np\nimport librosa\nfrom utils import logmmse\nfrom pypinyin import lazy_pinyin, Style\n\nclass Synthesizer:\n    sample_rate = hparams.sample_rate\n    hparams = hparams\n    \n    def __init__(self, model_fpath: Path, verbose=True):\n        \"\"\"\n        The model isn't instantiated and loaded in memory until needed or until load() is called.\n        \n        :param model_fpath: path to the trained model file\n        :param verbose: if False, prints less information when using the model\n        \"\"\"\n        self.model_fpath = model_fpath\n        self.verbose = verbose\n \n        # Check for GPU\n        if torch.cuda.is_available():\n            self.device = torch.device(\"cuda\")\n        else:\n            self.device = torch.device(\"cpu\")\n        if self.verbose:\n            print(\"Synthesizer using device:\", self.device)\n        \n        # Tacotron model will be instantiated later on first use.\n        self._model = None\n\n    def is_loaded(self):\n        \"\"\"\n        Whether the model is loaded in memory.\n        \"\"\"\n        return self._model is not None\n    \n    def load(self):\n        # Try to scan config file\n        model_config_fpaths = list(self.model_fpath.parent.rglob(\"*.json\"))\n        if len(model_config_fpaths)>0 and model_config_fpaths[0].exists():\n            hparams.loadJson(model_config_fpaths[0])\n        \"\"\"\n        Instantiates and loads the model given the weights file that was passed in the constructor.\n        \"\"\"\n        self._model = Tacotron(embed_dims=hparams.tts_embed_dims,\n                               num_chars=len(symbols),\n                               encoder_dims=hparams.tts_encoder_dims,\n                               decoder_dims=hparams.tts_decoder_dims,\n                               n_mels=hparams.num_mels,\n                               fft_bins=hparams.num_mels,\n                               postnet_dims=hparams.tts_postnet_dims,\n                               encoder_K=hparams.tts_encoder_K,\n                               lstm_dims=hparams.tts_lstm_dims,\n                               postnet_K=hparams.tts_postnet_K,\n                               num_highways=hparams.tts_num_highways,\n                               dropout=hparams.tts_dropout,\n                               stop_threshold=hparams.tts_stop_threshold,\n                               speaker_embedding_size=hparams.speaker_embedding_size).to(self.device)\n\n        self._model.load(self.model_fpath, self.device)\n        self._model.eval()\n\n        if self.verbose:\n            print(\"Loaded synthesizer \\\"%s\\\" trained to step %d\" % (self.model_fpath.name, self._model.state_dict()[\"step\"]))\n\n    def synthesize_spectrograms(self, texts: List[str],\n                                embeddings: Union[np.ndarray, List[np.ndarray]],\n                                return_alignments=False, style_idx=0, min_stop_token=5, steps=2000):\n        \"\"\"\n        Synthesizes mel spectrograms from texts and speaker embeddings.\n\n        :param texts: a list of N text prompts to be synthesized\n        :param embeddings: a numpy array or list of speaker embeddings of shape (N, 256) \n        :param return_alignments: if True, a matrix representing the alignments between the \n        characters\n        and each decoder output step will be returned for each spectrogram\n        :return: a list of N melspectrograms as numpy arrays of shape (80, Mi), where Mi is the \n        sequence length of spectrogram i, and possibly the alignments.\n        \"\"\"\n        # Load the model on the first request.\n        if not self.is_loaded():\n            self.load()\n\n            # Print some info about the model when it is loaded            \n            tts_k = self._model.get_step() // 1000\n\n            simple_table([(\"Tacotron\", str(tts_k) + \"k\"),\n                        (\"r\", self._model.r)])\n        \n        print(\"Read \" + str(texts))\n        texts = [\" \".join(lazy_pinyin(v, style=Style.TONE3, neutral_tone_with_five=True)) for v in texts]\n        print(\"Synthesizing \" + str(texts))\n        # Preprocess text inputs\n        inputs = [text_to_sequence(text, hparams.tts_cleaner_names) for text in texts]\n        if not isinstance(embeddings, list):\n            embeddings = [embeddings]\n\n        # Batch inputs\n        batched_inputs = [inputs[i:i+hparams.synthesis_batch_size]\n                             for i in range(0, len(inputs), hparams.synthesis_batch_size)]\n        batched_embeds = [embeddings[i:i+hparams.synthesis_batch_size]\n                             for i in range(0, len(embeddings), hparams.synthesis_batch_size)]\n\n        specs = []\n        for i, batch in enumerate(batched_inputs, 1):\n            if self.verbose:\n                print(f\"\\n| Generating {i}/{len(batched_inputs)}\")\n\n            # Pad texts so they are all the same length\n            text_lens = [len(text) for text in batch]\n            max_text_len = max(text_lens)\n            chars = [pad1d(text, max_text_len) for text in batch]\n            chars = np.stack(chars)\n\n            # Stack speaker embeddings into 2D array for batch processing\n            speaker_embeds = np.stack(batched_embeds[i-1])\n\n            # Convert to tensor\n            chars = torch.tensor(chars).long().to(self.device)\n            speaker_embeddings = torch.tensor(speaker_embeds).float().to(self.device)\n\n            # Inference\n            _, mels, alignments = self._model.generate(chars, speaker_embeddings, style_idx=style_idx, min_stop_token=min_stop_token, steps=steps)\n            mels = mels.detach().cpu().numpy()\n            for m in mels:\n                # Trim silence from end of each spectrogram\n                while np.max(m[:, -1]) < hparams.tts_stop_threshold:\n                    m = m[:, :-1]\n                specs.append(m)\n\n        if self.verbose:\n            print(\"\\n\\nDone.\\n\")\n        return (specs, alignments) if return_alignments else specs\n\n    @staticmethod\n    def load_preprocess_wav(fpath):\n        \"\"\"\n        Loads and preprocesses an audio file under the same conditions the audio files were used to\n        train the synthesizer. \n        \"\"\"\n        wav = librosa.load(path=str(fpath), sr=hparams.sample_rate)[0]\n        if hparams.rescale:\n            wav = wav / np.abs(wav).max() * hparams.rescaling_max\n        # denoise\n        if len(wav) > hparams.sample_rate*(0.3+0.1):\n            noise_wav = np.concatenate([wav[:int(hparams.sample_rate*0.15)],\n                                        wav[-int(hparams.sample_rate*0.15):]])\n            profile = logmmse.profile_noise(noise_wav, hparams.sample_rate)\n            wav = logmmse.denoise(wav, profile)\n        return wav\n\n    @staticmethod\n    def make_spectrogram(fpath_or_wav: Union[str, Path, np.ndarray]):\n        \"\"\"\n        Creates a mel spectrogram from an audio file in the same manner as the mel spectrograms that \n        were fed to the synthesizer when training.\n        \"\"\"\n        if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):\n            wav = Synthesizer.load_preprocess_wav(fpath_or_wav)\n        else:\n            wav = fpath_or_wav\n        \n        mel_spectrogram = audio.melspectrogram(wav, hparams).astype(np.float32)\n        return mel_spectrogram\n    \n    @staticmethod\n    def griffin_lim(mel):\n        \"\"\"\n        Inverts a mel spectrogram using Griffin-Lim. The mel spectrogram is expected to have been built\n        with the same parameters present in hparams.py.\n        \"\"\"\n        return audio.inv_mel_spectrogram(mel, hparams)\n\n\ndef pad1d(x, max_len, pad_value=0):\n    return np.pad(x, (0, max_len - len(x)), mode=\"constant\", constant_values=pad_value)\n", "models/synthesizer/synthesize.py": "import torch\nfrom torch.utils.data import DataLoader\nfrom models.synthesizer.synthesizer_dataset import SynthesizerDataset, collate_synthesizer\nfrom models.synthesizer.models.tacotron import Tacotron\nfrom models.synthesizer.utils.symbols import symbols\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport sys\n\n\ndef run_synthesis(in_dir, out_dir, model_dir, hparams):\n    # This generates ground truth-aligned mels for vocoder training\n    synth_dir = Path(out_dir).joinpath(\"mels_gta\")\n    synth_dir.mkdir(parents=True, exist_ok=True)\n    print(str(hparams))\n\n    # Check for GPU\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        if hparams.synthesis_batch_size % torch.cuda.device_count() != 0:\n            raise ValueError(\"`hparams.synthesis_batch_size` must be evenly divisible by n_gpus!\")\n    else:\n        device = torch.device(\"cpu\")\n    print(\"Synthesizer using device:\", device)\n\n    # Instantiate Tacotron model\n    model = Tacotron(embed_dims=hparams.tts_embed_dims,\n                     num_chars=len(symbols),\n                     encoder_dims=hparams.tts_encoder_dims,\n                     decoder_dims=hparams.tts_decoder_dims,\n                     n_mels=hparams.num_mels,\n                     fft_bins=hparams.num_mels,\n                     postnet_dims=hparams.tts_postnet_dims,\n                     encoder_K=hparams.tts_encoder_K,\n                     lstm_dims=hparams.tts_lstm_dims,\n                     postnet_K=hparams.tts_postnet_K,\n                     num_highways=hparams.tts_num_highways,\n                     dropout=0., # Use zero dropout for gta mels\n                     stop_threshold=hparams.tts_stop_threshold,\n                     speaker_embedding_size=hparams.speaker_embedding_size).to(device)\n\n    # Load the weights\n    model_dir = Path(model_dir)\n    model_fpath = model_dir.joinpath(model_dir.stem).with_suffix(\".pt\")\n    print(\"\\nLoading weights at %s\" % model_fpath)\n    model.load(model_fpath, device)\n    print(\"Tacotron weights loaded from step %d\" % model.step)\n\n    # Synthesize using same reduction factor as the model is currently trained\n    r = np.int32(model.r)\n\n    # Set model to eval mode (disable gradient and zoneout)\n    model.eval()\n\n    # Initialize the dataset\n    in_dir = Path(in_dir)\n    metadata_fpath = in_dir.joinpath(\"train.txt\")\n    mel_dir = in_dir.joinpath(\"mels\")\n    embed_dir = in_dir.joinpath(\"embeds\")\n    num_workers = 0 if sys.platform.startswith(\"win\") else 2;\n    dataset = SynthesizerDataset(metadata_fpath, mel_dir, embed_dir, hparams)\n    data_loader = DataLoader(dataset,\n                             collate_fn=lambda batch: collate_synthesizer(batch),\n                             batch_size=hparams.synthesis_batch_size,\n                             num_workers=num_workers,\n                             shuffle=False,\n                             pin_memory=True)\n\n    # Generate GTA mels\n    meta_out_fpath = Path(out_dir).joinpath(\"synthesized.txt\")\n    with open(meta_out_fpath, \"w\") as file:\n        for i, (texts, mels, embeds, idx) in tqdm(enumerate(data_loader), total=len(data_loader)):\n            texts = texts.to(device)\n            mels = mels.to(device)\n            embeds = embeds.to(device)\n\n            # Parallelize model onto GPUS using workaround due to python bug\n            if device.type == \"cuda\" and torch.cuda.device_count() > 1:\n                _, mels_out, _ , _ = data_parallel_workaround(model, texts, mels, embeds)\n            else:\n                _, mels_out, _, _  = model(texts, mels, embeds)\n\n            for j, k in enumerate(idx):\n                # Note: outputs mel-spectrogram files and target ones have same names, just different folders\n                mel_filename = Path(synth_dir).joinpath(dataset.metadata[k][1])\n                mel_out = mels_out[j].detach().cpu().numpy().T\n\n                # Use the length of the ground truth mel to remove padding from the generated mels\n                mel_out = mel_out[:int(dataset.metadata[k][4])]\n\n                # Write the spectrogram to disk\n                np.save(mel_filename, mel_out, allow_pickle=False)\n\n                # Write metadata into the synthesized file\n                file.write(\"|\".join(dataset.metadata[k]))\n", "models/synthesizer/train_vits.py": "import os\nfrom loguru import logger\nimport torch\nimport glob\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.cuda.amp import autocast, GradScaler\nfrom utils.audio_utils import mel_spectrogram, spec_to_mel\nfrom utils.loss import feature_loss, generator_loss, discriminator_loss, kl_loss\nfrom utils.util import slice_segments, clip_grad_value_\nfrom models.synthesizer.vits_dataset import (\n    VitsDataset,\n    VitsDatasetCollate,\n    DistributedBucketSampler\n)\nfrom models.synthesizer.models.vits import (\n    Vits,\n    MultiPeriodDiscriminator,\n)\nfrom models.synthesizer.utils.symbols import symbols\nfrom models.synthesizer.utils.plot import plot_spectrogram_to_numpy, plot_alignment_to_numpy\nfrom pathlib import Path\nfrom utils.hparams import HParams\nimport torch.multiprocessing as mp\nimport argparse\n\n# torch.backends.cudnn.benchmark = True\nglobal_step = 0\n\n\ndef new_train():\n    \"\"\"Assume Single Node Multi GPUs Training Only\"\"\"\n    assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--syn_dir\", type=str, default=\"../audiodata/SV2TTS/synthesizer\", help= \\\n        \"Path to the synthesizer directory that contains the ground truth mel spectrograms, \"\n        \"the wavs, the emos and the embeds.\")\n    parser.add_argument(\"-m\", \"--model_dir\", type=str, default=\"data/ckpt/synthesizer/vits2\", help=\\\n        \"Path to the output directory that will contain the saved model weights and the logs.\")\n    parser.add_argument('--ckptG', type=str, required=False,\n                      help='original VITS G checkpoint path')\n    parser.add_argument('--ckptD', type=str, required=False,\n                      help='original VITS D checkpoint path')\n    args, _ = parser.parse_known_args()\n\n    datasets_root = Path(args.syn_dir)\n    hparams= HParams(\n        model_dir = args.model_dir,\n    )\n    hparams.loadJson(Path(hparams.model_dir).joinpath(\"config.json\"))\n    hparams.data[\"training_files\"] = str(datasets_root.joinpath(\"train.txt\"))\n    hparams.data[\"validation_files\"] = str(datasets_root.joinpath(\"train.txt\"))\n    hparams.data[\"datasets_root\"] = str(datasets_root)\n    hparams.ckptG = args.ckptG\n    hparams.ckptD = args.ckptD\n    n_gpus = torch.cuda.device_count()\n    # for spawn\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '8899'\n    # mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hparams))\n    run(0, 1, hparams)\n\n\ndef load_checkpoint(checkpoint_path, model, optimizer=None, is_old=False, epochs=10000):\n  assert os.path.isfile(checkpoint_path)\n  checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n  iteration = checkpoint_dict['iteration']\n  learning_rate = checkpoint_dict['learning_rate']\n  if optimizer is not None:\n    if not is_old:\n      optimizer.load_state_dict(checkpoint_dict['optimizer'])\n    else:\n      new_opt_dict = optimizer.state_dict()\n      new_opt_dict_params = new_opt_dict['param_groups'][0]['params']\n      new_opt_dict['param_groups'] = checkpoint_dict['optimizer']['param_groups']\n      new_opt_dict['param_groups'][0]['params'] = new_opt_dict_params\n      optimizer.load_state_dict(new_opt_dict)\n  saved_state_dict = checkpoint_dict['model']\n  if hasattr(model, 'module'):\n    state_dict = model.module.state_dict()\n  else:\n    state_dict = model.state_dict()\n  new_state_dict= {}\n  for k, v in state_dict.items():\n    try:\n      new_state_dict[k] = saved_state_dict[k]\n    except:\n        if k == 'step':\n            new_state_dict[k] = iteration * epochs\n        else:\n            logger.info(\"%s is not in the checkpoint\" % k)\n            new_state_dict[k] = v\n\n  if hasattr(model, 'module'):\n    model.module.load_state_dict(new_state_dict, strict=False)\n  else:\n    model.load_state_dict(new_state_dict, strict=False)\n  logger.info(\"Loaded checkpoint '{}' (iteration {})\" .format(\n    checkpoint_path, iteration))\n  return model, optimizer, learning_rate, iteration\n\n\ndef save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):\n  logger.info(\"Saving model and optimizer state at iteration {} to {}\".format(\n    iteration, checkpoint_path))\n  if hasattr(model, 'module'):\n    state_dict = model.module.state_dict()\n  else:\n    state_dict = model.state_dict()\n  torch.save({'model': state_dict,\n              'iteration': iteration,\n              'optimizer': optimizer.state_dict(),\n              'learning_rate': learning_rate}, checkpoint_path)\n\ndef latest_checkpoint_path(dir_path, regex=\"G_*.pth\"):\n  f_list = glob.glob(os.path.join(dir_path, regex))\n  f_list.sort(key=lambda f: int(\"\".join(filter(str.isdigit, f))))\n  x = f_list[-1]\n  print(x)\n  return x\n\ndef run(rank, n_gpus, hps):\n    global global_step\n    if rank == 0:\n        logger.info(hps)\n        writer = SummaryWriter(log_dir=hps.model_dir)\n        writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n\n    dist.init_process_group(backend='gloo', init_method='env://', world_size=n_gpus, rank=rank)\n    torch.manual_seed(hps.train.seed)\n    torch.cuda.set_device(rank)\n    train_dataset = VitsDataset(hps.data.training_files, hps.data)\n    train_sampler = DistributedBucketSampler(\n        train_dataset,\n        hps.train.batch_size,\n        [32, 300, 400, 500, 600, 700, 800, 900, 1000],\n        num_replicas=n_gpus,\n        rank=rank,\n        shuffle=True)\n    collate_fn = VitsDatasetCollate()\n    train_loader = DataLoader(train_dataset, num_workers=8, shuffle=False, pin_memory=True,\n                              collate_fn=collate_fn, batch_sampler=train_sampler)\n    if rank == 0:\n        eval_dataset = VitsDataset(hps.data.validation_files, hps.data)\n        eval_loader = DataLoader(eval_dataset, num_workers=8, shuffle=False,\n                                 batch_size=hps.train.batch_size, pin_memory=True,\n                                 drop_last=False, collate_fn=collate_fn)\n\n    net_g = Vits(\n        len(symbols),\n        hps.data.filter_length // 2 + 1,\n        hps.train.segment_size // hps.data.hop_length,\n        n_speakers=hps.data.n_speakers,\n        **hps.model).cuda(rank)\n    net_d = MultiPeriodDiscriminator(hps.model.use_spectral_norm).cuda(rank)\n    optim_g = torch.optim.AdamW(\n        net_g.parameters(),\n        hps.train.learning_rate,\n        betas=hps.train.betas,\n        eps=hps.train.eps)\n    optim_d = torch.optim.AdamW(\n        net_d.parameters(),\n        hps.train.learning_rate,\n        betas=hps.train.betas,\n        eps=hps.train.eps)\n    net_g = DDP(net_g, device_ids=[rank])\n    net_d = DDP(net_d, device_ids=[rank])\n    ckptG = hps.ckptG\n    ckptD = hps.ckptD\n    try:\n        if ckptG is not None:\n            _, _, _, epoch_str = load_checkpoint(ckptG, net_g, optim_g, is_old=True)\n            print(\"\u52a0\u8f7d\u539f\u7248VITS\u6a21\u578bG\u8bb0\u5f55\u70b9\u6210\u529f\")\n        else:\n            _, _, _, epoch_str = load_checkpoint(latest_checkpoint_path(hps.model_dir, \"G_*.pth\"), net_g,\n                                                   optim_g, epochs=hps.train.epochs)\n        if ckptD is not None:\n            _, _, _, epoch_str = load_checkpoint(ckptG, net_g, optim_g, is_old=True)\n            print(\"\u52a0\u8f7d\u539f\u7248VITS\u6a21\u578bD\u8bb0\u5f55\u70b9\u6210\u529f\")\n        else:\n            _, _, _, epoch_str = load_checkpoint(latest_checkpoint_path(hps.model_dir, \"D_*.pth\"), net_d,\n                                                   optim_d, epochs=hps.train.epochs)\n        global_step = (epoch_str - 1) * len(train_loader)\n    except:\n        epoch_str = 1\n        global_step = 0\n    if ckptG is not None or ckptD is not None:\n        epoch_str = 1\n        global_step = 0\n    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=hps.train.lr_decay, last_epoch=epoch_str - 2)\n    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=hps.train.lr_decay, last_epoch=epoch_str - 2)\n\n    scaler = GradScaler(enabled=hps.train.fp16_run)\n\n    for epoch in range(epoch_str, hps.train.epochs + 1):\n        if rank == 0:\n            train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler,\n                               [train_loader, eval_loader], logger, [writer, writer_eval])\n        else:\n            train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler,\n                               [train_loader, None], None, None)\n        scheduler_g.step()\n        scheduler_d.step()\n\n\ndef train_and_evaluate(rank, epoch, hps, nets, optims, schedulers, scaler, loaders, logger, writers):\n    net_g, net_d = nets\n    optim_g, optim_d = optims\n    scheduler_g, scheduler_d = schedulers\n    train_loader, eval_loader = loaders\n    if writers is not None:\n        writer, writer_eval = writers\n    train_loader.batch_sampler.set_epoch(epoch)\n    global global_step\n\n    net_g.train()\n    net_d.train()\n    for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths, speakers, emo) in enumerate(train_loader):\n        # logger.info(f'====> Step: 1 {batch_idx}')\n        x, x_lengths = x.cuda(rank), x_lengths.cuda(rank)\n        spec, spec_lengths = spec.cuda(rank), spec_lengths.cuda(rank)\n        y, y_lengths = y.cuda(rank), y_lengths.cuda(rank)\n        speakers = speakers.cuda(rank)\n        emo = emo.cuda(rank)\n        # logger.info(f'====> Step: 1.0 {batch_idx}')\n        with autocast(enabled=hps.train.fp16_run):\n            y_hat, l_length, attn, ids_slice, x_mask, z_mask, \\\n            (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths, speakers, emo)\n            # logger.info(f'====> Step: 1.1 {batch_idx}')\n            mel = spec_to_mel(\n                spec,\n                hps.data.filter_length,\n                hps.data.n_mel_channels,\n                hps.data.sampling_rate,\n                hps.data.mel_fmin,\n                hps.data.mel_fmax)\n            y_mel = slice_segments(mel, ids_slice, hps.train.segment_size // hps.data.hop_length)\n            y_hat_mel = mel_spectrogram(\n                y_hat.squeeze(1),\n                hps.data.filter_length,\n                hps.data.n_mel_channels,\n                hps.data.sampling_rate,\n                hps.data.hop_length,\n                hps.data.win_length,\n                hps.data.mel_fmin,\n                hps.data.mel_fmax\n            )\n\n            y = slice_segments(y, ids_slice * hps.data.hop_length, hps.train.segment_size)  # slice\n            # logger.info(f'====> Step: 1.3 {batch_idx}')\n            # Discriminator\n            y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n            with autocast(enabled=False):\n                loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g)\n                loss_disc_all = loss_disc\n        optim_d.zero_grad()\n        scaler.scale(loss_disc_all).backward()\n        scaler.unscale_(optim_d)\n        grad_norm_d = clip_grad_value_(net_d.parameters(), None)\n        scaler.step(optim_d)\n\n        with autocast(enabled=hps.train.fp16_run):\n            # Generator\n            y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)\n            with autocast(enabled=False):\n                loss_dur = torch.sum(l_length.float())\n                loss_mel = F.l1_loss(y_mel, y_hat_mel) * hps.train.c_mel\n                loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * hps.train.c_kl\n\n                loss_fm = feature_loss(fmap_r, fmap_g)\n                loss_gen, losses_gen = generator_loss(y_d_hat_g)\n                loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n        optim_g.zero_grad()\n        scaler.scale(loss_gen_all.float()).backward()\n        scaler.unscale_(optim_g)\n        grad_norm_g = clip_grad_value_(net_g.parameters(), None)\n        scaler.step(optim_g)\n        scaler.update()\n        if rank == 0:\n            if global_step % hps.train.log_interval == 0:\n                lr = optim_g.param_groups[0]['lr']\n                losses = [loss_disc, loss_gen, loss_fm, loss_mel, loss_dur, loss_kl]\n                logger.info('Train Epoch: {} [{:.0f}%]'.format(\n                    epoch,\n                    100. * batch_idx / len(train_loader)))\n                logger.info([x.item() for x in losses] + [global_step, lr])\n\n                scalar_dict = {\"loss/g/total\": loss_gen_all, \"loss/d/total\": loss_disc_all, \"learning_rate\": lr,\n                               \"grad_norm_d\": grad_norm_d, \"grad_norm_g\": grad_norm_g}\n                scalar_dict.update(\n                    {\"loss/g/fm\": loss_fm, \"loss/g/mel\": loss_mel, \"loss/g/dur\": loss_dur, \"loss/g/kl\": loss_kl})\n\n                scalar_dict.update({\"loss/g/{}\".format(i): v for i, v in enumerate(losses_gen)})\n                scalar_dict.update({\"loss/d_r/{}\".format(i): v for i, v in enumerate(losses_disc_r)})\n                scalar_dict.update({\"loss/d_g/{}\".format(i): v for i, v in enumerate(losses_disc_g)})\n                image_dict = {\n                    \"slice/mel_org\": plot_spectrogram_to_numpy(y_mel[0].data.cpu().numpy()),\n                    \"slice/mel_gen\": plot_spectrogram_to_numpy(y_hat_mel[0].data.cpu().numpy()),\n                    \"all/mel\": plot_spectrogram_to_numpy(mel[0].data.cpu().numpy()),\n                    \"all/attn\": plot_alignment_to_numpy(attn[0, 0].data.cpu().numpy())\n                }\n                summarize(\n                    writer=writer,\n                    global_step=global_step,\n                    images=image_dict,\n                    scalars=scalar_dict)\n\n            if global_step % hps.train.eval_interval == 0:\n                evaluate(hps, net_g, eval_loader, writer_eval)\n                save_checkpoint(net_g, optim_g, hps.train.learning_rate, epoch,\n                                      os.path.join(hps.model_dir, \"G_{}.pth\".format(global_step)))\n                save_checkpoint(net_d, optim_d, hps.train.learning_rate, epoch,\n                                      os.path.join(hps.model_dir, \"D_{}.pth\".format(global_step)))\n        global_step += 1\n\n    if rank == 0:\n        logger.info('====> Epoch: {}'.format(epoch))\n\n\ndef evaluate(hps, generator, eval_loader, writer_eval):\n    generator.eval()\n    with torch.no_grad():\n        for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths, speakers, emo) in enumerate(eval_loader):\n            x, x_lengths = x.cuda(0), x_lengths.cuda(0)\n            spec, spec_lengths = spec.cuda(0), spec_lengths.cuda(0)\n            y, y_lengths = y.cuda(0), y_lengths.cuda(0)\n            speakers = speakers.cuda(0)\n            emo = emo.cuda(0)\n            # remove else\n            x = x[:1]\n            x_lengths = x_lengths[:1]\n            spec = spec[:1]\n            spec_lengths = spec_lengths[:1]\n            y = y[:1]\n            y_lengths = y_lengths[:1]\n            speakers = speakers[:1]\n            emo = emo[:1]\n            break\n        y_hat, attn, mask, *_ = generator.module.infer(x, x_lengths, speakers, emo, max_len=1000)\n        # y_hat, attn, mask, *_ = generator.infer(x, x_lengths, speakers, emo, max_len=1000) # for non DistributedDataParallel object\n        \n        y_hat_lengths = mask.sum([1, 2]).long() * hps.data.hop_length\n\n        mel = spec_to_mel(\n            spec,\n            hps.data.filter_length,\n            hps.data.n_mel_channels,\n            hps.data.sampling_rate,\n            hps.data.mel_fmin,\n            hps.data.mel_fmax)\n        y_hat_mel = mel_spectrogram(\n            y_hat.squeeze(1).float(),\n            hps.data.filter_length,\n            hps.data.n_mel_channels,\n            hps.data.sampling_rate,\n            hps.data.hop_length,\n            hps.data.win_length,\n            hps.data.mel_fmin,\n            hps.data.mel_fmax\n        )\n    image_dict = {\n        \"gen/mel\": plot_spectrogram_to_numpy(y_hat_mel[0].cpu().numpy())\n    }\n    audio_dict = {\n        \"gen/audio\": y_hat[0, :, :y_hat_lengths[0]]\n    }\n    if global_step == 0:\n        image_dict.update({\"gt/mel\": plot_spectrogram_to_numpy(mel[0].cpu().numpy())})\n        audio_dict.update({\"gt/audio\": y[0, :, :y_lengths[0]]})\n\n    summarize(\n        writer=writer_eval,\n        global_step=global_step,\n        images=image_dict,\n        audios=audio_dict,\n        audio_sampling_rate=hps.data.sampling_rate\n    )\n    generator.train()\n\ndef summarize(writer, global_step, scalars={}, histograms={}, images={}, audios={}, audio_sampling_rate=22050):\n    for k, v in scalars.items():\n        writer.add_scalar(k, v, global_step)\n    for k, v in histograms.items():\n        writer.add_histogram(k, v, global_step)\n    for k, v in images.items():\n        writer.add_image(k, v, global_step, dataformats='HWC')\n    for k, v in audios.items():\n        writer.add_audio(k, v, global_step, audio_sampling_rate)\n\n", "models/synthesizer/models/base.py": "import torch\nimport torch.nn as nn\nimport imp\nimport numpy as np\n\nclass Base(nn.Module):\n    def __init__(self, stop_threshold):\n        super().__init__()\n\n        self.init_model()\n        self.num_params()\n\n        self.register_buffer(\"step\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"stop_threshold\", torch.tensor(stop_threshold, dtype=torch.float32))\n\n    @property\n    def r(self):\n        return self.decoder.r.item()\n\n    @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def finetune_partial(self, whitelist_layers):\n        self.zero_grad()\n        for name, child in self.named_children():\n            if name in whitelist_layers:\n                print(\"Trainable Layer: %s\" % name)\n                print(\"Trainable Parameters: %.3f\" % sum([np.prod(p.size()) for p in child.parameters()]))\n                for param in child.parameters():\n                    param.requires_grad = False\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \"a\") as f:\n            print(msg, file=f)\n\n    def load(self, path, device, optimizer=None):\n        # Use device of model params as location for loaded state\n        checkpoint = torch.load(str(path), map_location=device)\n        if \"model_state\" in checkpoint:\n            state = checkpoint[\"model_state\"]\n        else:\n            state = checkpoint[\"model\"]\n        self.load_state_dict(state, strict=False)\n\n        if \"optimizer_state\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n    def save(self, path, optimizer=None):\n        if optimizer is not None:\n            torch.save({\n                \"model_state\": self.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            }, str(path))\n        else:\n            torch.save({\n                \"model_state\": self.state_dict(),\n            }, str(path))\n\n\n    def num_params(self, print_out=True):\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n        if print_out:\n            print(\"Trainable Parameters: %.3fM\" % parameters)\n        return parameters\n", "models/synthesizer/models/tacotron.py": "import torch\nimport torch.nn as nn\nfrom .sublayer.global_style_token import GlobalStyleToken\nfrom .sublayer.pre_net import PreNet\nfrom .sublayer.cbhg import CBHG\nfrom .sublayer.lsa import LSA\nfrom .base import Base\nfrom models.synthesizer.gst_hyperparameters import GSTHyperparameters as gst_hp\nfrom models.synthesizer.hparams import hparams\n\nclass Encoder(nn.Module):\n    def __init__(self, num_chars, embed_dims=512, encoder_dims=256, K=5, num_highways=4, dropout=0.5):\n        \"\"\" Encoder for SV2TTS\n\n        Args:\n            num_chars (int): length of symbols\n            embed_dims (int, optional): embedding dim for input texts. Defaults to 512.\n            encoder_dims (int, optional): output dim for encoder. Defaults to 256.\n            K (int, optional): _description_. Defaults to 5.\n            num_highways (int, optional): _description_. Defaults to 4.\n            dropout (float, optional): _description_. Defaults to 0.5.\n        \"\"\"             \n        super().__init__()\n        self.embedding = nn.Embedding(num_chars, embed_dims)\n        self.pre_net = PreNet(embed_dims, fc1_dims=encoder_dims, fc2_dims=encoder_dims,\n                              dropout=dropout)\n        self.cbhg = CBHG(K=K, in_channels=encoder_dims, channels=encoder_dims,\n                         proj_channels=[encoder_dims, encoder_dims],\n                         num_highways=num_highways)\n\n    def forward(self, x):\n        \"\"\"forward pass for encoder\n\n        Args:\n            x (2D tensor with size `[batch_size, text_num_chars]`): input texts list\n\n        Returns:\n            3D tensor with size `[batch_size, text_num_chars, encoder_dims]`\n            \n        \"\"\"\n        x = self.embedding(x) # return: [batch_size, text_num_chars, tts_embed_dims]\n        x = self.pre_net(x) # return: [batch_size, text_num_chars, encoder_dims]\n        x.transpose_(1, 2)  # return: [batch_size, encoder_dims, text_num_chars]\n        return self.cbhg(x) # return: [batch_size, text_num_chars, encoder_dims]\n\nclass Decoder(nn.Module):\n    # Class variable because its value doesn't change between classes\n    # yet ought to be scoped by class because its a property of a Decoder\n    max_r = 20\n    def __init__(self, n_mels, input_dims, decoder_dims, lstm_dims,\n                 dropout, speaker_embedding_size):\n        super().__init__()\n        self.register_buffer(\"r\", torch.tensor(1, dtype=torch.int))\n        self.n_mels = n_mels\n        self.prenet = PreNet(n_mels, fc1_dims=decoder_dims * 2, fc2_dims=decoder_dims * 2,\n                             dropout=dropout)\n        self.attn_net = LSA(decoder_dims)\n        if hparams.use_gst:\n            speaker_embedding_size += gst_hp.E\n        self.attn_rnn = nn.GRUCell(input_dims + decoder_dims * 2, decoder_dims)\n        self.rnn_input = nn.Linear(input_dims  + decoder_dims, lstm_dims)\n        self.res_rnn1 = nn.LSTMCell(lstm_dims, lstm_dims)\n        self.res_rnn2 = nn.LSTMCell(lstm_dims, lstm_dims)\n        self.mel_proj = nn.Linear(lstm_dims, n_mels * self.max_r, bias=False)\n        self.stop_proj = nn.Linear(input_dims + lstm_dims, 1)\n\n    def zoneout(self, prev, current, device, p=0.1):\n        mask = torch.zeros(prev.size(),device=device).bernoulli_(p)\n        return prev * mask + current * (1 - mask)\n\n    def forward(self, encoder_seq, encoder_seq_proj, prenet_in,\n                hidden_states, cell_states, context_vec, times, chars):\n        \"\"\"_summary_\n\n        Args:\n            encoder_seq (3D tensor `[batch_size, text_num_chars, project_dim(default to 512)]`): _description_\n            encoder_seq_proj (3D tensor `[batch_size, text_num_chars, decoder_dims(default to 128)]`): _description_\n            prenet_in (2D tensor `[batch_size, n_mels]`): _description_\n            hidden_states (_type_): _description_\n            cell_states (_type_): _description_\n            context_vec (2D tensor `[batch_size, project_dim(default to 512)]`): _description_\n            times (int): the number of times runned\n            chars (2D tensor with size `[batch_size, text_num_chars]`): original texts list input\n\n        \"\"\"\n        # Need this for reshaping mels\n        batch_size = encoder_seq.size(0)\n        device = encoder_seq.device\n        # Unpack the hidden and cell states\n        attn_hidden, rnn1_hidden, rnn2_hidden = hidden_states\n        rnn1_cell, rnn2_cell = cell_states\n\n        # PreNet for the Attention RNN\n        prenet_out = self.prenet(prenet_in) # return: `[batch_size, decoder_dims * 2(256)]`\n\n        # Compute the Attention RNN hidden state\n        attn_rnn_in = torch.cat([context_vec, prenet_out], dim=-1) # `[batch_size, project_dim + decoder_dims * 2 (768)]`\n        attn_hidden = self.attn_rnn(attn_rnn_in.squeeze(1), attn_hidden) #  `[batch_size, decoder_dims (128)]`\n\n        # Compute the attention scores\n        scores = self.attn_net(encoder_seq_proj, attn_hidden, times, chars)\n\n        # Dot product to create the context vector\n        context_vec = scores @ encoder_seq\n        context_vec = context_vec.squeeze(1)\n\n        # Concat Attention RNN output w. Context Vector & project\n        x = torch.cat([context_vec, attn_hidden], dim=1) # `[batch_size, project_dim + decoder_dims (630)]`\n        x = self.rnn_input(x) # `[batch_size, lstm_dims(1024)]`\n\n        # Compute first Residual RNN, training with fixed zoneout rate 0.1\n        rnn1_hidden_next, rnn1_cell = self.res_rnn1(x, (rnn1_hidden, rnn1_cell)) # `[batch_size, lstm_dims(1024)]`\n        if self.training:\n            rnn1_hidden = self.zoneout(rnn1_hidden, rnn1_hidden_next,device=device)\n        else:\n            rnn1_hidden = rnn1_hidden_next\n        x = x + rnn1_hidden\n\n        # Compute second Residual RNN\n        rnn2_hidden_next, rnn2_cell = self.res_rnn2(x, (rnn2_hidden, rnn2_cell)) # `[batch_size, lstm_dims(1024)]`\n        if self.training:\n            rnn2_hidden = self.zoneout(rnn2_hidden, rnn2_hidden_next, device=device)\n        else:\n            rnn2_hidden = rnn2_hidden_next\n        x = x + rnn2_hidden\n\n        # Project Mels\n        mels = self.mel_proj(x) # `[batch_size, 1600]`\n        mels = mels.view(batch_size, self.n_mels, self.max_r)[:, :, :self.r] # `[batch_size, n_mels, r]`\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Stop token prediction\n        s = torch.cat((x, context_vec), dim=1)\n        s = self.stop_proj(s)\n        stop_tokens = torch.sigmoid(s)\n\n        return mels, scores, hidden_states, cell_states, context_vec, stop_tokens\n\nclass Tacotron(Base):\n    def __init__(self, embed_dims, num_chars, encoder_dims, decoder_dims, n_mels, \n                 fft_bins, postnet_dims, encoder_K, lstm_dims, postnet_K, num_highways,\n                 dropout, stop_threshold, speaker_embedding_size):\n        super().__init__(stop_threshold)\n        self.n_mels = n_mels\n        self.lstm_dims = lstm_dims\n        self.encoder_dims = encoder_dims\n        self.decoder_dims = decoder_dims\n        self.speaker_embedding_size = speaker_embedding_size\n        self.encoder = Encoder(num_chars, embed_dims, encoder_dims,\n                               encoder_K, num_highways, dropout)\n        self.project_dims = encoder_dims + speaker_embedding_size\n        if hparams.use_gst: \n            self.project_dims += gst_hp.E\n        self.encoder_proj = nn.Linear(self.project_dims, decoder_dims, bias=False)\n        if hparams.use_gst: \n            self.gst = GlobalStyleToken(speaker_embedding_size)\n        self.decoder = Decoder(n_mels, self.project_dims, decoder_dims, lstm_dims,\n                               dropout, speaker_embedding_size)\n        self.postnet = CBHG(postnet_K, n_mels, postnet_dims,\n                            [postnet_dims, fft_bins], num_highways)\n        self.post_proj = nn.Linear(postnet_dims, fft_bins, bias=False)\n\n    @staticmethod\n    def _concat_speaker_embedding(outputs, speaker_embeddings):\n        speaker_embeddings_ = speaker_embeddings.expand(\n            outputs.size(0), outputs.size(1), -1)\n        outputs = torch.cat([outputs, speaker_embeddings_], dim=-1)\n        return outputs\n\n    @staticmethod\n    def _add_speaker_embedding(x, speaker_embedding):\n        \"\"\"Add speaker embedding\n            This concats the speaker embedding for each char in the encoder output\n        Args:\n            x (3D tensor with size `[batch_size, text_num_chars, encoder_dims]`): the encoder output\n            speaker_embedding (2D tensor `[batch_size, speaker_embedding_size]`): the speaker embedding\n\n        Returns:\n            3D tensor with size `[batch_size, text_num_chars, encoder_dims+speaker_embedding_size]`\n        \"\"\"        \n        # Save the dimensions as human-readable names\n        batch_size = x.size()[0]\n        text_num_chars = x.size()[1]\n\n        # Start by making a copy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, text_num_chars * speaker_embedding_size)\n        speaker_embedding_size = speaker_embedding.size()[1]\n        e = speaker_embedding.repeat_interleave(text_num_chars, dim=1)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, text_num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n    def forward(self, texts, mels, speaker_embedding, steps=2000, style_idx=0, min_stop_token=5):\n        \"\"\"Forward pass for Tacotron\n\n        Args:\n            texts (`[batch_size, text_num_chars]`): input texts list\n            mels (`[batch_size, varied_mel_lengths, steps]`): mels for comparison (training only)\n            speaker_embedding (`[batch_size, speaker_embedding_size(default to 256)]`): referring embedding.\n            steps (int, optional): . Defaults to 2000.\n            style_idx (int, optional): GST style selected. Defaults to 0.\n            min_stop_token (int, optional): decoder min_stop_token. Defaults to 5.\n        \"\"\"\n        device = texts.device  # use same device as parameters\n\n        if self.training:\n            self.step += 1\n            batch_size, _, steps  = mels.size()\n        else:\n            batch_size, _  = texts.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(texts)\n        \n        encoder_seq = self._add_speaker_embedding(encoder_seq, speaker_embedding)\n\n        if hparams.use_gst and self.gst is not None:\n            if self.training:\n                style_embed = self.gst(speaker_embedding, speaker_embedding) # for training, speaker embedding can represent both style inputs and referenced\n                # style_embed = style_embed.expand_as(encoder_seq)\n                # encoder_seq = torch.cat((encoder_seq, style_embed), 2)\n            elif style_idx >= 0 and style_idx < 10:\n                query = torch.zeros(1, 1, self.gst.stl.attention.num_units)\n                if device.type == 'cuda':\n                    query = query.cuda()\n                gst_embed = torch.tanh(self.gst.stl.embed)\n                key = gst_embed[style_idx].unsqueeze(0).expand(1, -1, -1)\n                style_embed = self.gst.stl.attention(query, key)\n            else:\n                speaker_embedding_style = torch.zeros(speaker_embedding.size()[0], 1, self.speaker_embedding_size).to(device)\n                style_embed = self.gst(speaker_embedding_style, speaker_embedding)\n            encoder_seq = self._concat_speaker_embedding(encoder_seq, style_embed) # return: [batch_size, text_num_chars, project_dims]\n        \n        encoder_seq_proj = self.encoder_proj(encoder_seq) # return: [batch_size, text_num_chars, decoder_dims]\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.project_dims, device=device)\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            if self.training:\n                prenet_in = mels[:, :, t -1] if t > 0 else go_frame\n            else:\n                prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, texts)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            if not self.training and (stop_tokens * 10 > min_stop_token).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        if self.training:\n            self.train()\n            \n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding, steps=2000, style_idx=0, min_stop_token=5):\n        self.eval()\n        mel_outputs, linear, attn_scores, _ =  self.forward(x, None, speaker_embedding, steps, style_idx, min_stop_token)\n        return mel_outputs, linear, attn_scores\n", "models/synthesizer/models/wav2emo.py": "import torch\nimport torch.nn as nn\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import (\n    Wav2Vec2Model,\n    Wav2Vec2PreTrainedModel,\n)\n\n\nclass RegressionHead(nn.Module):\n    r\"\"\"Classification head.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.final_dropout)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n\n        return x\n\n\nclass EmotionExtractorModel(Wav2Vec2PreTrainedModel):\n    r\"\"\"Speech emotion classifier.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.config = config\n        self.wav2vec2 = Wav2Vec2Model(config)\n        self.classifier = RegressionHead(config)\n        self.init_weights()\n\n    def forward(\n            self,\n            input_values,\n    ):\n        outputs = self.wav2vec2(input_values)\n        hidden_states = outputs[0]\n        hidden_states = torch.mean(hidden_states, dim=1)\n        logits = self.classifier(hidden_states)\n\n        return hidden_states, logits\n", "models/synthesizer/models/vits.py": "import math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom loguru import logger\n\nfrom .sublayer.vits_modules import *\nimport monotonic_align\n\nfrom torch.nn import Conv1d, ConvTranspose1d, Conv2d\nfrom torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\nfrom utils.util import init_weights, get_padding, sequence_mask, rand_slice_segments, generate_path\n\n\nclass StochasticDurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n    super().__init__()\n    filter_channels = in_channels # it needs to be removed from future version.\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.log_flow = Log()\n    self.flows = nn.ModuleList()\n    self.flows.append(ElementwiseAffine(2))\n    for i in range(n_flows):\n      self.flows.append(ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.flows.append(Flip())\n\n    self.post_pre = nn.Conv1d(1, filter_channels, 1)\n    self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.post_convs = DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    self.post_flows = nn.ModuleList()\n    self.post_flows.append(ElementwiseAffine(2))\n    for i in range(4):\n      self.post_flows.append(ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.post_flows.append(Flip())\n\n    self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n    self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.convs = DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n\n  def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n    x = torch.detach(x)\n    x = self.pre(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.convs(x, x_mask)\n    x = self.proj(x) * x_mask\n\n    if not reverse:\n      flows = self.flows\n      assert w is not None\n\n      logdet_tot_q = 0 \n      h_w = self.post_pre(w)\n      h_w = self.post_convs(h_w, x_mask)\n      h_w = self.post_proj(h_w) * x_mask\n      e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n      z_q = e_q\n      for flow in self.post_flows:\n        z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\n        logdet_tot_q += logdet_q\n      z_u, z1 = torch.split(z_q, [1, 1], 1) \n      u = torch.sigmoid(z_u) * x_mask\n      z0 = (w - u) * x_mask\n      logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\n      logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\n\n      logdet_tot = 0\n      z0, logdet = self.log_flow(z0, x_mask)\n      logdet_tot += logdet\n      z = torch.cat([z0, z1], 1)\n      for flow in flows:\n        z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n        logdet_tot = logdet_tot + logdet\n      nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\n      return nll + logq # [b]\n    else:\n      flows = list(reversed(self.flows))\n      flows = flows[:-2] + [flows[-1]] # remove a useless vflow\n      z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n      for flow in flows:\n        z = flow(z, x_mask, g=x, reverse=reverse)\n      z0, z1 = torch.split(z, [1, 1], 1)\n      logw = z0\n      return logw\n\n\nclass DurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, gin_channels=0):\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.gin_channels = gin_channels\n\n    self.drop = nn.Dropout(p_dropout)\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_1 = LayerNorm(filter_channels)\n    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_2 = LayerNorm(filter_channels)\n    self.proj = nn.Conv1d(filter_channels, 1, 1)\n\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, in_channels, 1)\n\n  def forward(self, x, x_mask, g=None):\n    x = torch.detach(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.conv_1(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_1(x)\n    x = self.drop(x)\n    x = self.conv_2(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_2(x)\n    x = self.drop(x)\n    x = self.proj(x * x_mask)\n    return x * x_mask\n\n\nclass TextEncoder(nn.Module):\n  def __init__(self,\n      n_vocab,\n      out_channels,\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout):\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n\n    self.emb = nn.Embedding(n_vocab, hidden_channels)\n    self.emo_proj = nn.Linear(1024, hidden_channels)\n\n    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n\n    self.encoder = Encoder(\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout)\n    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths, emo):\n    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n    x = x + self.emo_proj(emo.unsqueeze(1))\n    x = torch.transpose(x, 1, -1) # [b, h, t]\n    x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n\n    x = self.encoder(x * x_mask, x_mask)\n    stats = self.proj(x) * x_mask\n\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    return x, m, logs, x_mask\n\n\nclass ResidualCouplingBlock(nn.Module):\n  def __init__(self,\n      channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      n_flows=4,\n      gin_channels=0):\n    super().__init__()\n    self.channels = channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.flows = nn.ModuleList()\n    for i in range(n_flows):\n      self.flows.append(ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\n      self.flows.append(Flip())\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    if not reverse:\n      for flow in self.flows:\n        x, _ = flow(x, x_mask, g=g, reverse=reverse)\n    else:\n      for flow in reversed(self.flows):\n        x = flow(x, x_mask, g=g, reverse=reverse)\n    return x\n\n\nclass PosteriorEncoder(nn.Module):\n  def __init__(self,\n      in_channels,\n      out_channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      gin_channels=0):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.gin_channels = gin_channels\n\n    self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n    self.enc = WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n    self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths, g=None):\n    x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n    x = self.pre(x) * x_mask\n    x = self.enc(x, x_mask, g=g)\n    stats = self.proj(x) * x_mask\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n    return z, m, logs, x_mask\n\n\nclass Generator(torch.nn.Module):\n    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n        resblock = ResBlock1 if resblock == '1' else ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(weight_norm(\n                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n                                k, u, padding=(k-u)//2)))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n          x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()\n\n\nclass DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        self.use_spectral_norm = use_spectral_norm\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(get_padding(kernel_size, 1), 0))),\n        ])\n        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\n    def forward(self, x):\n        fmap = []\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0: # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n            norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n            norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n            norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n            norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n        ])\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        fmap = []\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(MultiPeriodDiscriminator, self).__init__()\n        periods = [2,3,5,7,11]\n\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods]\n        self.discriminators = nn.ModuleList(discs)\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            y_d_gs.append(y_d_g)\n            fmap_rs.append(fmap_r)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n\nclass Vits(nn.Module):\n  \"\"\"\n  Synthesizer of Vits\n  \"\"\"\n\n  def __init__(self, \n    n_vocab,\n    spec_channels,\n    segment_size,\n    inter_channels,\n    hidden_channels,\n    filter_channels,\n    n_heads,\n    n_layers,\n    kernel_size,\n    p_dropout,\n    resblock, \n    resblock_kernel_sizes, \n    resblock_dilation_sizes, \n    upsample_rates, \n    upsample_initial_channel, \n    upsample_kernel_sizes,\n    n_speakers=0,\n    gin_channels=0,\n    use_sdp=True,\n    **kwargs):\n\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.spec_channels = spec_channels\n    self.inter_channels = inter_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.resblock = resblock\n    self.resblock_kernel_sizes = resblock_kernel_sizes\n    self.resblock_dilation_sizes = resblock_dilation_sizes\n    self.upsample_rates = upsample_rates\n    self.upsample_initial_channel = upsample_initial_channel\n    self.upsample_kernel_sizes = upsample_kernel_sizes\n    self.segment_size = segment_size\n    self.n_speakers = n_speakers\n    self.gin_channels = gin_channels\n\n    self.use_sdp = use_sdp\n\n    self.enc_p = TextEncoder(n_vocab,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout)\n    self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n    self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n    self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n\n    if use_sdp:\n      self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n    else:\n      self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n\n    if n_speakers > 1:\n      self.emb_g = nn.Embedding(n_speakers, gin_channels)\n\n  def forward(self, x, x_lengths, y, y_lengths, sid=None, emo=None):\n    # logger.info(f'====> Forward: 1.1.0')\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths, emo)\n    if self.n_speakers > 0:\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n    else:\n      g = None\n\n    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n    z_p = self.flow(z, y_mask, g=g)\n    # logger.info(f'====> Forward: 1.1.1')\n    with torch.no_grad():\n      # negative cross-entropy\n      s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\n      neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\n      neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n      neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n      neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n      neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\n      #logger.info(f'====> Forward: 1.1.1.1')\n      attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n      attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\n\n    # logger.info(f'====> Forward: 1.1.2')\n    w = attn.sum(2)\n    if self.use_sdp:\n      l_length = self.dp(x, x_mask, w, g=g)\n      l_length = l_length / torch.sum(x_mask)\n    else:\n      logw_ = torch.log(w + 1e-6) * x_mask\n      logw = self.dp(x, x_mask, g=g)\n      l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging \n    # expand prior\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\n\n    z_slice, ids_slice = rand_slice_segments(z, y_lengths, self.segment_size)\n    o = self.dec(z_slice, g=g)\n    return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n\n  def infer(self, x, x_lengths, sid=None, emo=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n    # logger.info(f'====> Infer: 1.1.0')\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths,emo)\n    # logger.info(f'====> Infer: 1.1.1')\n    if self.n_speakers > 0:\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n    else:\n      g = None\n\n    if self.use_sdp:\n      logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n    else:\n      logw = self.dp(x, x_mask, g=g)\n    w = torch.exp(logw) * x_mask * length_scale\n    w_ceil = torch.ceil(w)\n    y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n    y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n    attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n    attn = generate_path(w_ceil, attn_mask)\n\n    # logger.info(f'====> Infer: 1.1.2')\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n\n    z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n    z = self.flow(z_p, y_mask, g=g, reverse=True)\n    o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n    \n    # logger.info(f'====> Infer: 1.1.3')\n    return o, attn, y_mask, (z, z_p, m_p, logs_p)\n\n", "models/synthesizer/models/sublayer/global_style_token.py": "import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as tFunctional\nfrom models.synthesizer.gst_hyperparameters import GSTHyperparameters as hp\nfrom models.synthesizer.hparams import hparams\n\n\nclass GlobalStyleToken(nn.Module):\n    \"\"\"\n    inputs: style mel spectrograms [batch_size, num_spec_frames, num_mel]\n    speaker_embedding: speaker mel spectrograms [batch_size, num_spec_frames, num_mel]\n    outputs: [batch_size, embedding_dim]\n    \"\"\"\n    def __init__(self, speaker_embedding_dim=None):\n\n        super().__init__()\n        self.encoder = ReferenceEncoder()\n        self.stl = STL(speaker_embedding_dim)\n\n    def forward(self, inputs, speaker_embedding=None):\n        enc_out = self.encoder(inputs)\n        # concat speaker_embedding according to https://github.com/mozilla/TTS/blob/master/TTS/tts/layers/gst_layers.py\n        if hparams.use_ser_for_gst and speaker_embedding is not None:\n            enc_out = torch.cat([enc_out, speaker_embedding], dim=-1)\n        style_embed = self.stl(enc_out)\n\n        return style_embed\n\n\nclass ReferenceEncoder(nn.Module):\n    '''\n    inputs --- [N, Ty/r, n_mels*r]  mels\n    outputs --- [N, ref_enc_gru_size]\n    '''\n\n    def __init__(self):\n\n        super().__init__()\n        K = len(hp.ref_enc_filters)\n        filters = [1] + hp.ref_enc_filters\n        convs = [nn.Conv2d(in_channels=filters[i],\n                           out_channels=filters[i + 1],\n                           kernel_size=(3, 3),\n                           stride=(2, 2),\n                           padding=(1, 1)) for i in range(K)]\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList([nn.BatchNorm2d(num_features=hp.ref_enc_filters[i]) for i in range(K)])\n\n        out_channels = self.calculate_channels(hp.n_mels, 3, 2, 1, K)\n        self.gru = nn.GRU(input_size=hp.ref_enc_filters[-1] * out_channels,\n                          hidden_size=hp.E // 2,\n                          batch_first=True)\n\n    def forward(self, inputs):\n        N = inputs.size(0)\n        out = inputs.view(N, 1, -1, hp.n_mels)  # [N, 1, Ty, n_mels]\n        for conv, bn in zip(self.convs, self.bns):\n            out = conv(out)\n            out = bn(out)\n            out = tFunctional.relu(out)  # [N, 128, Ty//2^K, n_mels//2^K]\n\n        out = out.transpose(1, 2)  # [N, Ty//2^K, 128, n_mels//2^K]\n        T = out.size(1)\n        N = out.size(0)\n        out = out.contiguous().view(N, T, -1)  # [N, Ty//2^K, 128*n_mels//2^K]\n\n        self.gru.flatten_parameters()\n        memory, out = self.gru(out)  # out --- [1, N, E//2]\n\n        return out.squeeze(0)\n\n    def calculate_channels(self, L, kernel_size, stride, pad, n_convs):\n        for i in range(n_convs):\n            L = (L - kernel_size + 2 * pad) // stride + 1\n        return L\n\n\nclass STL(nn.Module):\n    '''\n    inputs --- [N, E//2]\n    '''\n\n    def __init__(self, speaker_embedding_dim=None):\n\n        super().__init__()\n        self.embed = nn.Parameter(torch.FloatTensor(hp.token_num, hp.E // hp.num_heads))\n        d_q = hp.E // 2\n        d_k = hp.E // hp.num_heads\n        # self.attention = MultiHeadAttention(hp.num_heads, d_model, d_q, d_v)\n        if hparams.use_ser_for_gst and speaker_embedding_dim is not None:\n            d_q += speaker_embedding_dim\n        self.attention = MultiHeadAttention(query_dim=d_q, key_dim=d_k, num_units=hp.E, num_heads=hp.num_heads)\n\n        init.normal_(self.embed, mean=0, std=0.5)\n\n    def forward(self, inputs):\n        N = inputs.size(0)\n        query = inputs.unsqueeze(1)  # [N, 1, E//2]\n        keys = torch.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]\n        style_embed = self.attention(query, keys)\n\n        return style_embed\n\n\nclass MultiHeadAttention(nn.Module):\n    '''\n    input:\n        query --- [N, T_q, query_dim]\n        key --- [N, T_k, key_dim]\n    output:\n        out --- [N, T_q, num_units]\n    '''\n\n    def __init__(self, query_dim, key_dim, num_units, num_heads):\n\n        super().__init__()\n        self.num_units = num_units\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n\n        self.W_query = nn.Linear(in_features=query_dim, out_features=num_units, bias=False)\n        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n        self.W_value = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n\n    def forward(self, query, key):\n        querys = self.W_query(query)  # [N, T_q, num_units]\n        keys = self.W_key(key)  # [N, T_k, num_units]\n        values = self.W_value(key)\n\n        split_size = self.num_units // self.num_heads\n        querys = torch.stack(torch.split(querys, split_size, dim=2), dim=0)  # [h, N, T_q, num_units/h]\n        keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n        values = torch.stack(torch.split(values, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n\n        # score = softmax(QK^T / (d_k ** 0.5))\n        scores = torch.matmul(querys, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n        scores = scores / (self.key_dim ** 0.5)\n        scores = tFunctional.softmax(scores, dim=3)\n\n        # out = score * V\n        out = torch.matmul(scores, values)  # [h, N, T_q, num_units/h]\n        out = torch.cat(torch.split(out, 1, dim=0), dim=3).squeeze(0)  # [N, T_q, num_units]\n\n        return out\n", "models/synthesizer/models/sublayer/pre_net.py": "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass PreNet(nn.Module):\n    def __init__(self, in_dims, fc1_dims=256, fc2_dims=128, dropout=0.5):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dims, fc1_dims)\n        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n        self.p = dropout\n\n    def forward(self, x):\n        \"\"\"forward\n\n        Args:\n            x (3D tensor with size `[batch_size, num_chars, tts_embed_dims]`): input texts list\n\n        Returns:\n            3D tensor with size `[batch_size, num_chars, encoder_dims]`\n            \n        \"\"\"        \n        x = self.fc1(x)\n        x = F.relu(x)\n        x = F.dropout(x, self.p, training=True)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = F.dropout(x, self.p, training=True)\n        return x\n", "models/synthesizer/models/sublayer/cbhg.py": "import torch\nimport torch.nn as nn\nfrom .common.batch_norm_conv import BatchNormConv\nfrom .common.highway_network import HighwayNetwork\n\nclass CBHG(nn.Module):\n    def __init__(self, K, in_channels, channels, proj_channels, num_highways):\n        super().__init__()\n\n        # List of all rnns to call `flatten_parameters()` on\n        self._to_flatten = []\n\n        self.bank_kernels = [i for i in range(1, K + 1)]\n        self.conv1d_bank = nn.ModuleList()\n        for k in self.bank_kernels:\n            conv = BatchNormConv(in_channels, channels, k)\n            self.conv1d_bank.append(conv)\n\n        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n\n        self.conv_project1 = BatchNormConv(len(self.bank_kernels) * channels, proj_channels[0], 3)\n        self.conv_project2 = BatchNormConv(proj_channels[0], proj_channels[1], 3, relu=False)\n\n        # Fix the highway input if necessary\n        if proj_channels[-1] != channels:\n            self.highway_mismatch = True\n            self.pre_highway = nn.Linear(proj_channels[-1], channels, bias=False)\n        else:\n            self.highway_mismatch = False\n\n        self.highways = nn.ModuleList()\n        for i in range(num_highways):\n            hn = HighwayNetwork(channels)\n            self.highways.append(hn)\n\n        self.rnn = nn.GRU(channels, channels // 2, batch_first=True, bidirectional=True)\n        self._to_flatten.append(self.rnn)\n\n        # Avoid fragmentation of RNN parameters and associated warning\n        self._flatten_parameters()\n\n    def forward(self, x):\n        # Although we `_flatten_parameters()` on init, when using DataParallel\n        # the model gets replicated, making it no longer guaranteed that the\n        # weights are contiguous in GPU memory. Hence, we must call it again\n        self.rnn.flatten_parameters()\n\n        # Save these for later\n        residual = x\n        seq_len = x.size(-1)\n        conv_bank = []\n\n        # Convolution Bank\n        for conv in self.conv1d_bank:\n            c = conv(x) # Convolution\n            conv_bank.append(c[:, :, :seq_len])\n\n        # Stack along the channel axis\n        conv_bank = torch.cat(conv_bank, dim=1)\n\n        # dump the last padding to fit residual\n        x = self.maxpool(conv_bank)[:, :, :seq_len]\n\n        # Conv1d projections\n        x = self.conv_project1(x)\n        x = self.conv_project2(x)\n\n        # Residual Connect\n        x = x + residual\n\n        # Through the highways\n        x = x.transpose(1, 2)\n        if self.highway_mismatch is True:\n            x = self.pre_highway(x)\n        for h in self.highways: x = h(x)\n\n        # And then the RNN\n        x, _ = self.rnn(x)\n        return x\n\n    def _flatten_parameters(self):\n        \"\"\"Calls `flatten_parameters` on all the rnns used by the WaveRNN. Used\n        to improve efficiency and avoid PyTorch yelling at us.\"\"\"\n        [m.flatten_parameters() for m in self._to_flatten]\n\n", "models/synthesizer/models/sublayer/lsa.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LSA(nn.Module):\n    def __init__(self, attn_dim, kernel_size=31, filters=32):\n        super().__init__()\n        self.conv = nn.Conv1d(1, filters, padding=(kernel_size - 1) // 2, kernel_size=kernel_size, bias=True)\n        self.L = nn.Linear(filters, attn_dim, bias=False)\n        self.W = nn.Linear(attn_dim, attn_dim, bias=True) # Include the attention bias in this term\n        self.v = nn.Linear(attn_dim, 1, bias=False)\n        self.cumulative = None\n        self.attention = None\n\n    def init_attention(self, encoder_seq_proj):\n        device = encoder_seq_proj.device  # use same device as parameters\n        b, t, c = encoder_seq_proj.size()\n        self.cumulative = torch.zeros(b, t, device=device)\n        self.attention = torch.zeros(b, t, device=device)\n\n    def forward(self, encoder_seq_proj, query, times, chars):\n\n        if times == 0: self.init_attention(encoder_seq_proj)\n\n        processed_query = self.W(query).unsqueeze(1)\n\n        location = self.cumulative.unsqueeze(1)\n        processed_loc = self.L(self.conv(location).transpose(1, 2))\n\n        u = self.v(torch.tanh(processed_query + encoder_seq_proj + processed_loc))\n        u = u.squeeze(-1)\n\n        # Mask zero padding chars\n        u = u * (chars != 0).float()\n\n        # Smooth Attention\n        # scores = torch.sigmoid(u) / torch.sigmoid(u).sum(dim=1, keepdim=True)\n        scores = F.softmax(u, dim=1)\n        self.attention = scores\n        self.cumulative = self.cumulative + self.attention\n\n        return scores.unsqueeze(-1).transpose(1, 2)\n", "models/synthesizer/models/sublayer/__init__.py": "#", "models/synthesizer/models/sublayer/vits_modules.py": "import math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom torch.nn import Conv1d\nfrom torch.nn.utils import weight_norm, remove_weight_norm\nfrom utils.util import init_weights, get_padding, convert_pad_shape, convert_pad_shape, subsequent_mask, fused_add_tanh_sigmoid_multiply\nfrom .common.transforms import piecewise_rational_quadratic_transform\n\nLRELU_SLOPE = 0.1\n\nclass LayerNorm(nn.Module):\n  def __init__(self, channels, eps=1e-5):\n    super().__init__()\n    self.channels = channels\n    self.eps = eps\n\n    self.gamma = nn.Parameter(torch.ones(channels))\n    self.beta = nn.Parameter(torch.zeros(channels))\n\n  def forward(self, x):\n    x = x.transpose(1, -1)\n    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n    return x.transpose(1, -1)\n\n \nclass ConvReluNorm(nn.Module):\n  def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, n_layers, p_dropout):\n    super().__init__()\n    self.in_channels = in_channels\n    self.hidden_channels = hidden_channels\n    self.out_channels = out_channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.p_dropout = p_dropout\n    assert n_layers > 1, \"Number of layers should be larger than 0.\"\n\n    self.conv_layers = nn.ModuleList()\n    self.norm_layers = nn.ModuleList()\n    self.conv_layers.append(nn.Conv1d(in_channels, hidden_channels, kernel_size, padding=kernel_size//2))\n    self.norm_layers.append(LayerNorm(hidden_channels))\n    self.relu_drop = nn.Sequential(\n        nn.ReLU(),\n        nn.Dropout(p_dropout))\n    for _ in range(n_layers-1):\n      self.conv_layers.append(nn.Conv1d(hidden_channels, hidden_channels, kernel_size, padding=kernel_size//2))\n      self.norm_layers.append(LayerNorm(hidden_channels))\n    self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n    self.proj.weight.data.zero_()\n    self.proj.bias.data.zero_()\n\n  def forward(self, x, x_mask):\n    x_org = x\n    for i in range(self.n_layers):\n      x = self.conv_layers[i](x * x_mask)\n      x = self.norm_layers[i](x)\n      x = self.relu_drop(x)\n    x = x_org + self.proj(x)\n    return x * x_mask\n\n\nclass DDSConv(nn.Module):\n  \"\"\"\n  Dilated and Depth-Separable Convolution\n  \"\"\"\n  def __init__(self, channels, kernel_size, n_layers, p_dropout=0.):\n    super().__init__()\n    self.channels = channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.p_dropout = p_dropout\n\n    self.drop = nn.Dropout(p_dropout)\n    self.convs_sep = nn.ModuleList()\n    self.convs_1x1 = nn.ModuleList()\n    self.norms_1 = nn.ModuleList()\n    self.norms_2 = nn.ModuleList()\n    for i in range(n_layers):\n      dilation = kernel_size ** i\n      padding = (kernel_size * dilation - dilation) // 2\n      self.convs_sep.append(nn.Conv1d(channels, channels, kernel_size, \n          groups=channels, dilation=dilation, padding=padding\n      ))\n      self.convs_1x1.append(nn.Conv1d(channels, channels, 1))\n      self.norms_1.append(LayerNorm(channels))\n      self.norms_2.append(LayerNorm(channels))\n\n  def forward(self, x, x_mask, g=None):\n    if g is not None:\n      x = x + g\n    for i in range(self.n_layers):\n      y = self.convs_sep[i](x * x_mask)\n      y = self.norms_1[i](y)\n      y = F.gelu(y)\n      y = self.convs_1x1[i](y)\n      y = self.norms_2[i](y)\n      y = F.gelu(y)\n      y = self.drop(y)\n      x = x + y\n    return x * x_mask\n\n\nclass WN(torch.nn.Module):\n  def __init__(self, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0, p_dropout=0):\n    super(WN, self).__init__()\n    assert(kernel_size % 2 == 1)\n    self.hidden_channels =hidden_channels\n    self.kernel_size = kernel_size,\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.gin_channels = gin_channels\n    self.p_dropout = p_dropout\n\n    self.in_layers = torch.nn.ModuleList()\n    self.res_skip_layers = torch.nn.ModuleList()\n    self.drop = nn.Dropout(p_dropout)\n\n    if gin_channels != 0:\n      cond_layer = torch.nn.Conv1d(gin_channels, 2*hidden_channels*n_layers, 1)\n      self.cond_layer = torch.nn.utils.weight_norm(cond_layer, name='weight')\n\n    for i in range(n_layers):\n      dilation = dilation_rate ** i\n      padding = int((kernel_size * dilation - dilation) / 2)\n      in_layer = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, kernel_size,\n                                 dilation=dilation, padding=padding)\n      in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n      self.in_layers.append(in_layer)\n\n      # last one is not necessary\n      if i < n_layers - 1:\n        res_skip_channels = 2 * hidden_channels\n      else:\n        res_skip_channels = hidden_channels\n\n      res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n      res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n      self.res_skip_layers.append(res_skip_layer)\n\n  def forward(self, x, x_mask, g=None, **kwargs):\n    output = torch.zeros_like(x)\n    n_channels_tensor = torch.IntTensor([self.hidden_channels])\n\n    if g is not None:\n      g = self.cond_layer(g)\n\n    for i in range(self.n_layers):\n      x_in = self.in_layers[i](x)\n      if g is not None:\n        cond_offset = i * 2 * self.hidden_channels\n        g_l = g[:,cond_offset:cond_offset+2*self.hidden_channels,:]\n      else:\n        g_l = torch.zeros_like(x_in)\n\n      acts = fused_add_tanh_sigmoid_multiply(\n          x_in,\n          g_l,\n          n_channels_tensor)\n      acts = self.drop(acts)\n\n      res_skip_acts = self.res_skip_layers[i](acts)\n      if i < self.n_layers - 1:\n        res_acts = res_skip_acts[:,:self.hidden_channels,:]\n        x = (x + res_acts) * x_mask\n        output = output + res_skip_acts[:,self.hidden_channels:,:]\n      else:\n        output = output + res_skip_acts\n    return output * x_mask\n\n  def remove_weight_norm(self):\n    if self.gin_channels != 0:\n      torch.nn.utils.remove_weight_norm(self.cond_layer)\n    for l in self.in_layers:\n      torch.nn.utils.remove_weight_norm(l)\n    for l in self.res_skip_layers:\n     torch.nn.utils.remove_weight_norm(l)\n\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock1, self).__init__()\n        self.convs1 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n                               padding=get_padding(kernel_size, dilation[2])))\n        ])\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1)))\n        ])\n        self.convs2.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c2(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)\n\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.convs = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1])))\n        ])\n        self.convs.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)\n\n\nclass Log(nn.Module):\n  def forward(self, x, x_mask, reverse=False, **kwargs):\n    if not reverse:\n      y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask\n      logdet = torch.sum(-y, [1, 2])\n      return y, logdet\n    else:\n      x = torch.exp(x) * x_mask\n      return x\n    \n\nclass Flip(nn.Module):\n  def forward(self, x, *args, reverse=False, **kwargs):\n    x = torch.flip(x, [1])\n    if not reverse:\n      logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device)\n      return x, logdet\n    else:\n      return x\n\n\nclass ElementwiseAffine(nn.Module):\n  def __init__(self, channels):\n    super().__init__()\n    self.channels = channels\n    self.m = nn.Parameter(torch.zeros(channels,1))\n    self.logs = nn.Parameter(torch.zeros(channels,1))\n\n  def forward(self, x, x_mask, reverse=False, **kwargs):\n    if not reverse:\n      y = self.m + torch.exp(self.logs) * x\n      y = y * x_mask\n      logdet = torch.sum(self.logs * x_mask, [1,2])\n      return y, logdet\n    else:\n      x = (x - self.m) * torch.exp(-self.logs) * x_mask\n      return x\n\n\nclass ResidualCouplingLayer(nn.Module):\n  def __init__(self,\n      channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      p_dropout=0,\n      gin_channels=0,\n      mean_only=False):\n    assert channels % 2 == 0, \"channels should be divisible by 2\"\n    super().__init__()\n    self.channels = channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.half_channels = channels // 2\n    self.mean_only = mean_only\n\n    self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n    self.enc = WN(hidden_channels, kernel_size, dilation_rate, n_layers, p_dropout=p_dropout, gin_channels=gin_channels)\n    self.post = nn.Conv1d(hidden_channels, self.half_channels * (2 - mean_only), 1)\n    self.post.weight.data.zero_()\n    self.post.bias.data.zero_()\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n    h = self.pre(x0) * x_mask\n    h = self.enc(h, x_mask, g=g)\n    stats = self.post(h) * x_mask\n    if not self.mean_only:\n      m, logs = torch.split(stats, [self.half_channels]*2, 1)\n    else:\n      m = stats\n      logs = torch.zeros_like(m)\n\n    if not reverse:\n      x1 = m + x1 * torch.exp(logs) * x_mask\n      x = torch.cat([x0, x1], 1)\n      logdet = torch.sum(logs, [1,2])\n      return x, logdet\n    else:\n      x1 = (x1 - m) * torch.exp(-logs) * x_mask\n      x = torch.cat([x0, x1], 1)\n      return x\n\n\nclass ConvFlow(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, n_layers, num_bins=10, tail_bound=5.0):\n    super().__init__()\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.num_bins = num_bins\n    self.tail_bound = tail_bound\n    self.half_channels = in_channels // 2\n\n    self.pre = nn.Conv1d(self.half_channels, filter_channels, 1)\n    self.convs = DDSConv(filter_channels, kernel_size, n_layers, p_dropout=0.)\n    self.proj = nn.Conv1d(filter_channels, self.half_channels * (num_bins * 3 - 1), 1)\n    self.proj.weight.data.zero_()\n    self.proj.bias.data.zero_()\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n    h = self.pre(x0)\n    h = self.convs(h, x_mask, g=g)\n    h = self.proj(h) * x_mask\n\n    b, c, t = x0.shape\n    h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2) # [b, cx?, t] -> [b, c, t, ?]\n\n    unnormalized_widths = h[..., :self.num_bins] / math.sqrt(self.filter_channels)\n    unnormalized_heights = h[..., self.num_bins:2*self.num_bins] / math.sqrt(self.filter_channels)\n    unnormalized_derivatives = h[..., 2 * self.num_bins:]\n\n    x1, logabsdet = piecewise_rational_quadratic_transform(x1,\n        unnormalized_widths,\n        unnormalized_heights,\n        unnormalized_derivatives,\n        inverse=reverse,\n        tails='linear',\n        tail_bound=self.tail_bound\n    )\n\n    x = torch.cat([x0, x1], 1) * x_mask\n    logdet = torch.sum(logabsdet * x_mask, [1,2])\n    if not reverse:\n        return x, logdet\n    else:\n        return x\n\nclass Encoder(nn.Module):\n  def __init__(self, hidden_channels, filter_channels, n_heads, n_layers, kernel_size=1, p_dropout=0., window_size=4, **kwargs):\n    super().__init__()\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.window_size = window_size\n\n    self.drop = nn.Dropout(p_dropout)\n    self.attn_layers = nn.ModuleList()\n    self.norm_layers_1 = nn.ModuleList()\n    self.ffn_layers = nn.ModuleList()\n    self.norm_layers_2 = nn.ModuleList()\n    for i in range(self.n_layers):\n      self.attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout, window_size=window_size))\n      self.norm_layers_1.append(LayerNorm(hidden_channels))\n      self.ffn_layers.append(FFN(hidden_channels, hidden_channels, filter_channels, kernel_size, p_dropout=p_dropout))\n      self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n  def forward(self, x, x_mask):\n    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n    x = x * x_mask\n    for i in range(self.n_layers):\n      y = self.attn_layers[i](x, x, attn_mask)\n      y = self.drop(y)\n      x = self.norm_layers_1[i](x + y)\n\n      y = self.ffn_layers[i](x, x_mask)\n      y = self.drop(y)\n      x = self.norm_layers_2[i](x + y)\n    x = x * x_mask\n    return x\n\n\nclass Decoder(nn.Module):\n  def __init__(self, hidden_channels, filter_channels, n_heads, n_layers, kernel_size=1, p_dropout=0., proximal_bias=False, proximal_init=True, **kwargs):\n    super().__init__()\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.proximal_bias = proximal_bias\n    self.proximal_init = proximal_init\n\n    self.drop = nn.Dropout(p_dropout)\n    self.self_attn_layers = nn.ModuleList()\n    self.norm_layers_0 = nn.ModuleList()\n    self.encdec_attn_layers = nn.ModuleList()\n    self.norm_layers_1 = nn.ModuleList()\n    self.ffn_layers = nn.ModuleList()\n    self.norm_layers_2 = nn.ModuleList()\n    for i in range(self.n_layers):\n      self.self_attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout, proximal_bias=proximal_bias, proximal_init=proximal_init))\n      self.norm_layers_0.append(LayerNorm(hidden_channels))\n      self.encdec_attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout))\n      self.norm_layers_1.append(LayerNorm(hidden_channels))\n      self.ffn_layers.append(FFN(hidden_channels, hidden_channels, filter_channels, kernel_size, p_dropout=p_dropout, causal=True))\n      self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n  def forward(self, x, x_mask, h, h_mask):\n    \"\"\"\n    x: decoder input\n    h: encoder output\n    \"\"\"\n    self_attn_mask = subsequent_mask(x_mask.size(2)).to(device=x.device, dtype=x.dtype)\n    encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n    x = x * x_mask\n    for i in range(self.n_layers):\n      y = self.self_attn_layers[i](x, x, self_attn_mask)\n      y = self.drop(y)\n      x = self.norm_layers_0[i](x + y)\n\n      y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\n      y = self.drop(y)\n      x = self.norm_layers_1[i](x + y)\n      \n      y = self.ffn_layers[i](x, x_mask)\n      y = self.drop(y)\n      x = self.norm_layers_2[i](x + y)\n    x = x * x_mask\n    return x\n\n\nclass MultiHeadAttention(nn.Module):\n  def __init__(self, channels, out_channels, n_heads, p_dropout=0., window_size=None, heads_share=True, block_length=None, proximal_bias=False, proximal_init=False):\n    super().__init__()\n    assert channels % n_heads == 0\n\n    self.channels = channels\n    self.out_channels = out_channels\n    self.n_heads = n_heads\n    self.p_dropout = p_dropout\n    self.window_size = window_size\n    self.heads_share = heads_share\n    self.block_length = block_length\n    self.proximal_bias = proximal_bias\n    self.proximal_init = proximal_init\n    self.attn = None\n\n    self.k_channels = channels // n_heads\n    self.conv_q = nn.Conv1d(channels, channels, 1)\n    self.conv_k = nn.Conv1d(channels, channels, 1)\n    self.conv_v = nn.Conv1d(channels, channels, 1)\n    self.conv_o = nn.Conv1d(channels, out_channels, 1)\n    self.drop = nn.Dropout(p_dropout)\n\n    if window_size is not None:\n      n_heads_rel = 1 if heads_share else n_heads\n      rel_stddev = self.k_channels**-0.5\n      self.emb_rel_k = nn.Parameter(torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels) * rel_stddev)\n      self.emb_rel_v = nn.Parameter(torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels) * rel_stddev)\n\n    nn.init.xavier_uniform_(self.conv_q.weight)\n    nn.init.xavier_uniform_(self.conv_k.weight)\n    nn.init.xavier_uniform_(self.conv_v.weight)\n    if proximal_init:\n      with torch.no_grad():\n        self.conv_k.weight.copy_(self.conv_q.weight)\n        self.conv_k.bias.copy_(self.conv_q.bias)\n      \n  def forward(self, x, c, attn_mask=None):\n    q = self.conv_q(x)\n    k = self.conv_k(c)\n    v = self.conv_v(c)\n    \n    x, self.attn = self.attention(q, k, v, mask=attn_mask)\n\n    x = self.conv_o(x)\n    return x\n\n  def attention(self, query, key, value, mask=None):\n    # reshape [b, d, t] -> [b, n_h, t, d_k]\n    b, d, t_s, t_t = (*key.size(), query.size(2))\n    query = query.view(b, self.n_heads, self.k_channels, t_t).transpose(2, 3)\n    key = key.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n    value = value.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n\n    scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1))\n    if self.window_size is not None:\n      assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n      key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n      rel_logits = self._matmul_with_relative_keys(query /math.sqrt(self.k_channels), key_relative_embeddings)\n      scores_local = self._relative_position_to_absolute_position(rel_logits)\n      scores = scores + scores_local\n    if self.proximal_bias:\n      assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\n      scores = scores + self._attention_bias_proximal(t_s).to(device=scores.device, dtype=scores.dtype)\n    if mask is not None:\n      scores = scores.masked_fill(mask == 0, -1e4)\n      if self.block_length is not None:\n        assert t_s == t_t, \"Local attention is only available for self-attention.\"\n        block_mask = torch.ones_like(scores).triu(-self.block_length).tril(self.block_length)\n        scores = scores.masked_fill(block_mask == 0, -1e4)\n    p_attn = F.softmax(scores, dim=-1) # [b, n_h, t_t, t_s]\n    p_attn = self.drop(p_attn)\n    output = torch.matmul(p_attn, value)\n    if self.window_size is not None:\n      relative_weights = self._absolute_position_to_relative_position(p_attn)\n      value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\n      output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings)\n    output = output.transpose(2, 3).contiguous().view(b, d, t_t) # [b, n_h, t_t, d_k] -> [b, d, t_t]\n    return output, p_attn\n\n  def _matmul_with_relative_values(self, x, y):\n    \"\"\"\n    x: [b, h, l, m]\n    y: [h or 1, m, d]\n    ret: [b, h, l, d]\n    \"\"\"\n    ret = torch.matmul(x, y.unsqueeze(0))\n    return ret\n\n  def _matmul_with_relative_keys(self, x, y):\n    \"\"\"\n    x: [b, h, l, d]\n    y: [h or 1, m, d]\n    ret: [b, h, l, m]\n    \"\"\"\n    ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n    return ret\n\n  def _get_relative_embeddings(self, relative_embeddings, length):\n    max_relative_position = 2 * self.window_size + 1\n    # Pad first before slice to avoid using cond ops.\n    pad_length = max(length - (self.window_size + 1), 0)\n    slice_start_position = max((self.window_size + 1) - length, 0)\n    slice_end_position = slice_start_position + 2 * length - 1\n    if pad_length > 0:\n      padded_relative_embeddings = F.pad(\n          relative_embeddings,\n          convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]))\n    else:\n      padded_relative_embeddings = relative_embeddings\n    used_relative_embeddings = padded_relative_embeddings[:,slice_start_position:slice_end_position]\n    return used_relative_embeddings\n\n  def _relative_position_to_absolute_position(self, x):\n    \"\"\"\n    x: [b, h, l, 2*l-1]\n    ret: [b, h, l, l]\n    \"\"\"\n    batch, heads, length, _ = x.size()\n    # Concat columns of pad to shift from relative to absolute indexing.\n    x = F.pad(x, convert_pad_shape([[0,0],[0,0],[0,0],[0,1]]))\n\n    # Concat extra elements so to add up to shape (len+1, 2*len-1).\n    x_flat = x.view([batch, heads, length * 2 * length])\n    x_flat = F.pad(x_flat, convert_pad_shape([[0,0],[0,0],[0,length-1]]))\n\n    # Reshape and slice out the padded elements.\n    x_final = x_flat.view([batch, heads, length+1, 2*length-1])[:, :, :length, length-1:]\n    return x_final\n\n  def _absolute_position_to_relative_position(self, x):\n    \"\"\"\n    x: [b, h, l, l]\n    ret: [b, h, l, 2*l-1]\n    \"\"\"\n    batch, heads, length, _ = x.size()\n    # padd along column\n    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length-1]]))\n    x_flat = x.view([batch, heads, length**2 + length*(length -1)])\n    # add 0's in the beginning that will skew the elements after reshape\n    x_flat = F.pad(x_flat, convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n    x_final = x_flat.view([batch, heads, length, 2*length])[:,:,:,1:]\n    return x_final\n\n  def _attention_bias_proximal(self, length):\n    \"\"\"Bias for self-attention to encourage attention to close positions.\n    Args:\n      length: an integer scalar.\n    Returns:\n      a Tensor with shape [1, 1, length, length]\n    \"\"\"\n    r = torch.arange(length, dtype=torch.float32)\n    diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n    return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)\n\n\nclass FFN(nn.Module):\n  def __init__(self, in_channels, out_channels, filter_channels, kernel_size, p_dropout=0., activation=None, causal=False):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.activation = activation\n    self.causal = causal\n\n    if causal:\n      self.padding = self._causal_padding\n    else:\n      self.padding = self._same_padding\n\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size)\n    self.conv_2 = nn.Conv1d(filter_channels, out_channels, kernel_size)\n    self.drop = nn.Dropout(p_dropout)\n\n  def forward(self, x, x_mask):\n    x = self.conv_1(self.padding(x * x_mask))\n    if self.activation == \"gelu\":\n      x = x * torch.sigmoid(1.702 * x)\n    else:\n      x = torch.relu(x)\n    x = self.drop(x)\n    x = self.conv_2(self.padding(x * x_mask))\n    return x * x_mask\n  \n  def _causal_padding(self, x):\n    if self.kernel_size == 1:\n      return x\n    pad_l = self.kernel_size - 1\n    pad_r = 0\n    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n    x = F.pad(x, convert_pad_shape(padding))\n    return x\n\n  def _same_padding(self, x):\n    if self.kernel_size == 1:\n      return x\n    pad_l = (self.kernel_size - 1) // 2\n    pad_r = self.kernel_size // 2\n    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n    x = F.pad(x, convert_pad_shape(padding))\n    return x\n", "models/synthesizer/models/sublayer/common/highway_network.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass HighwayNetwork(nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.W1 = nn.Linear(size, size)\n        self.W2 = nn.Linear(size, size)\n        self.W1.bias.data.fill_(0.)\n\n    def forward(self, x):\n        x1 = self.W1(x)\n        x2 = self.W2(x)\n        g = torch.sigmoid(x2)\n        y = g * F.relu(x1) + (1. - g) * x\n        return y\n", "models/synthesizer/models/sublayer/common/batch_norm_conv.py": "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass BatchNormConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel, relu=True):\n        super().__init__()\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel, stride=1, padding=kernel // 2, bias=False)\n        self.bnorm = nn.BatchNorm1d(out_channels)\n        self.relu = relu\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.relu(x) if self.relu is True else x\n        return self.bnorm(x)", "models/synthesizer/models/sublayer/common/transforms.py": "import torch\nfrom torch.nn import functional as F\n\nimport numpy as np\n\n\nDEFAULT_MIN_BIN_WIDTH = 1e-3\nDEFAULT_MIN_BIN_HEIGHT = 1e-3\nDEFAULT_MIN_DERIVATIVE = 1e-3\n\n\ndef piecewise_rational_quadratic_transform(inputs, \n                                           unnormalized_widths,\n                                           unnormalized_heights,\n                                           unnormalized_derivatives,\n                                           inverse=False,\n                                           tails=None, \n                                           tail_bound=1.,\n                                           min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                                           min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                                           min_derivative=DEFAULT_MIN_DERIVATIVE):\n\n    if tails is None:\n        spline_fn = rational_quadratic_spline\n        spline_kwargs = {}\n    else:\n        spline_fn = unconstrained_rational_quadratic_spline\n        spline_kwargs = {\n            'tails': tails,\n            'tail_bound': tail_bound\n        }\n\n    outputs, logabsdet = spline_fn(\n            inputs=inputs,\n            unnormalized_widths=unnormalized_widths,\n            unnormalized_heights=unnormalized_heights,\n            unnormalized_derivatives=unnormalized_derivatives,\n            inverse=inverse,\n            min_bin_width=min_bin_width,\n            min_bin_height=min_bin_height,\n            min_derivative=min_derivative,\n            **spline_kwargs\n    )\n    return outputs, logabsdet\n\n\ndef searchsorted(bin_locations, inputs, eps=1e-6):\n    bin_locations[..., -1] += eps\n    return torch.sum(\n        inputs[..., None] >= bin_locations,\n        dim=-1\n    ) - 1\n\n\ndef unconstrained_rational_quadratic_spline(inputs,\n                                            unnormalized_widths,\n                                            unnormalized_heights,\n                                            unnormalized_derivatives,\n                                            inverse=False,\n                                            tails='linear',\n                                            tail_bound=1.,\n                                            min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                                            min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                                            min_derivative=DEFAULT_MIN_DERIVATIVE):\n    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n    outside_interval_mask = ~inside_interval_mask\n\n    outputs = torch.zeros_like(inputs)\n    logabsdet = torch.zeros_like(inputs)\n\n    if tails == 'linear':\n        unnormalized_derivatives = F.pad(unnormalized_derivatives, pad=(1, 1))\n        constant = np.log(np.exp(1 - min_derivative) - 1)\n        unnormalized_derivatives[..., 0] = constant\n        unnormalized_derivatives[..., -1] = constant\n\n        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n        logabsdet[outside_interval_mask] = 0\n    else:\n        raise RuntimeError('{} tails are not implemented.'.format(tails))\n\n    outputs[inside_interval_mask], logabsdet[inside_interval_mask] = rational_quadratic_spline(\n        inputs=inputs[inside_interval_mask],\n        unnormalized_widths=unnormalized_widths[inside_interval_mask, :],\n        unnormalized_heights=unnormalized_heights[inside_interval_mask, :],\n        unnormalized_derivatives=unnormalized_derivatives[inside_interval_mask, :],\n        inverse=inverse,\n        left=-tail_bound, right=tail_bound, bottom=-tail_bound, top=tail_bound,\n        min_bin_width=min_bin_width,\n        min_bin_height=min_bin_height,\n        min_derivative=min_derivative\n    )\n\n    return outputs, logabsdet\n\ndef rational_quadratic_spline(inputs,\n                              unnormalized_widths,\n                              unnormalized_heights,\n                              unnormalized_derivatives,\n                              inverse=False,\n                              left=0., right=1., bottom=0., top=1.,\n                              min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                              min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                              min_derivative=DEFAULT_MIN_DERIVATIVE):\n    if torch.min(inputs) < left or torch.max(inputs) > right:\n        raise ValueError('Input to a transform is not within its domain')\n\n    num_bins = unnormalized_widths.shape[-1]\n\n    if min_bin_width * num_bins > 1.0:\n        raise ValueError('Minimal bin width too large for the number of bins')\n    if min_bin_height * num_bins > 1.0:\n        raise ValueError('Minimal bin height too large for the number of bins')\n\n    widths = F.softmax(unnormalized_widths, dim=-1)\n    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\n    cumwidths = torch.cumsum(widths, dim=-1)\n    cumwidths = F.pad(cumwidths, pad=(1, 0), mode='constant', value=0.0)\n    cumwidths = (right - left) * cumwidths + left\n    cumwidths[..., 0] = left\n    cumwidths[..., -1] = right\n    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\n\n    derivatives = min_derivative + F.softplus(unnormalized_derivatives)\n\n    heights = F.softmax(unnormalized_heights, dim=-1)\n    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\n    cumheights = torch.cumsum(heights, dim=-1)\n    cumheights = F.pad(cumheights, pad=(1, 0), mode='constant', value=0.0)\n    cumheights = (top - bottom) * cumheights + bottom\n    cumheights[..., 0] = bottom\n    cumheights[..., -1] = top\n    heights = cumheights[..., 1:] - cumheights[..., :-1]\n\n    if inverse:\n        bin_idx = searchsorted(cumheights, inputs)[..., None]\n    else:\n        bin_idx = searchsorted(cumwidths, inputs)[..., None]\n\n    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\n    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\n\n    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\n    delta = heights / widths\n    input_delta = delta.gather(-1, bin_idx)[..., 0]\n\n    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\n    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)[..., 0]\n\n    input_heights = heights.gather(-1, bin_idx)[..., 0]\n\n    if inverse:\n        a = (((inputs - input_cumheights) * (input_derivatives\n                                             + input_derivatives_plus_one\n                                             - 2 * input_delta)\n              + input_heights * (input_delta - input_derivatives)))\n        b = (input_heights * input_derivatives\n             - (inputs - input_cumheights) * (input_derivatives\n                                              + input_derivatives_plus_one\n                                              - 2 * input_delta))\n        c = - input_delta * (inputs - input_cumheights)\n\n        discriminant = b.pow(2) - 4 * a * c\n        assert (discriminant >= 0).all()\n\n        root = (2 * c) / (-b - torch.sqrt(discriminant))\n        outputs = root * input_bin_widths + input_cumwidths\n\n        theta_one_minus_theta = root * (1 - root)\n        denominator = input_delta + ((input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n                                     * theta_one_minus_theta)\n        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * root.pow(2)\n                                                     + 2 * input_delta * theta_one_minus_theta\n                                                     + input_derivatives * (1 - root).pow(2))\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, -logabsdet\n    else:\n        theta = (inputs - input_cumwidths) / input_bin_widths\n        theta_one_minus_theta = theta * (1 - theta)\n\n        numerator = input_heights * (input_delta * theta.pow(2)\n                                     + input_derivatives * theta_one_minus_theta)\n        denominator = input_delta + ((input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n                                     * theta_one_minus_theta)\n        outputs = input_cumheights + numerator / denominator\n\n        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * theta.pow(2)\n                                                     + 2 * input_delta * theta_one_minus_theta\n                                                     + input_derivatives * (1 - theta).pow(2))\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, logabsdet\n", "models/synthesizer/utils/_cmudict.py": "import re\n\nvalid_symbols = [\n  \"AA\", \"AA0\", \"AA1\", \"AA2\", \"AE\", \"AE0\", \"AE1\", \"AE2\", \"AH\", \"AH0\", \"AH1\", \"AH2\",\n  \"AO\", \"AO0\", \"AO1\", \"AO2\", \"AW\", \"AW0\", \"AW1\", \"AW2\", \"AY\", \"AY0\", \"AY1\", \"AY2\",\n  \"B\", \"CH\", \"D\", \"DH\", \"EH\", \"EH0\", \"EH1\", \"EH2\", \"ER\", \"ER0\", \"ER1\", \"ER2\", \"EY\",\n  \"EY0\", \"EY1\", \"EY2\", \"F\", \"G\", \"HH\", \"IH\", \"IH0\", \"IH1\", \"IH2\", \"IY\", \"IY0\", \"IY1\",\n  \"IY2\", \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\", \"OW\", \"OW0\", \"OW1\", \"OW2\", \"OY\", \"OY0\",\n  \"OY1\", \"OY2\", \"P\", \"R\", \"S\", \"SH\", \"T\", \"TH\", \"UH\", \"UH0\", \"UH1\", \"UH2\", \"UW\",\n  \"UW0\", \"UW1\", \"UW2\", \"V\", \"W\", \"Y\", \"Z\", \"ZH\"\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n  \"\"\"Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\"\"\"\n  def __init__(self, file_or_path, keep_ambiguous=True):\n    if isinstance(file_or_path, str):\n      with open(file_or_path, encoding=\"latin-1\") as f:\n        entries = _parse_cmudict(f)\n    else:\n      entries = _parse_cmudict(file_or_path)\n    if not keep_ambiguous:\n      entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n    self._entries = entries\n\n\n  def __len__(self):\n    return len(self._entries)\n\n\n  def lookup(self, word):\n    \"\"\"Returns list of ARPAbet pronunciations of the given word.\"\"\"\n    return self._entries.get(word.upper())\n\n\n\n_alt_re = re.compile(r\"\\([0-9]+\\)\")\n\n\ndef _parse_cmudict(file):\n  cmudict = {}\n  for line in file:\n    if len(line) and (line[0] >= \"A\" and line[0] <= \"Z\" or line[0] == \"'\"):\n      parts = line.split(\"  \")\n      word = re.sub(_alt_re, \"\", parts[0])\n      pronunciation = _get_pronunciation(parts[1])\n      if pronunciation:\n        if word in cmudict:\n          cmudict[word].append(pronunciation)\n        else:\n          cmudict[word] = [pronunciation]\n  return cmudict\n\n\ndef _get_pronunciation(s):\n  parts = s.strip().split(\" \")\n  for part in parts:\n    if part not in _valid_symbol_set:\n      return None\n  return \" \".join(parts)\n", "models/synthesizer/utils/plot.py": "import matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nMATPLOTLIB_FLAG = False\n\ndef split_title_line(title_text, max_words=5):\n\t\"\"\"\n\tA function that splits any string based on specific character\n\t(returning it with the string), with maximum number of words on it\n\t\"\"\"\n\tseq = title_text.split()\n\treturn \"\\n\".join([\" \".join(seq[i:i + max_words]) for i in range(0, len(seq), max_words)])\n\ndef plot_alignment(alignment, path, title=None, split_title=False, max_len=None):\n\tif max_len is not None:\n\t\talignment = alignment[:, :max_len]\n\n\tfig = plt.figure(figsize=(8, 6))\n\tax = fig.add_subplot(111)\n\n\tim = ax.imshow(\n\t\talignment,\n\t\taspect=\"auto\",\n\t\torigin=\"lower\",\n\t\tinterpolation=\"none\")\n\tfig.colorbar(im, ax=ax)\n\txlabel = \"Decoder timestep\"\n\n\tif split_title:\n\t\ttitle = split_title_line(title)\n\n\tplt.xlabel(xlabel)\n\tplt.title(title)\n\tplt.ylabel(\"Encoder timestep\")\n\tplt.tight_layout()\n\tplt.savefig(path, format=\"png\")\n\tplt.close()\n\n\ndef plot_spectrogram(pred_spectrogram, path, title=None, split_title=False, target_spectrogram=None, max_len=None, auto_aspect=False):\n\tif max_len is not None:\n\t\ttarget_spectrogram = target_spectrogram[:max_len]\n\t\tpred_spectrogram = pred_spectrogram[:max_len]\n\n\tif split_title:\n\t\ttitle = split_title_line(title)\n\n\tfig = plt.figure(figsize=(10, 8))\n\t# Set common labels\n\tfig.text(0.5, 0.18, title, horizontalalignment=\"center\", fontsize=16)\n\n\t#target spectrogram subplot\n\tif target_spectrogram is not None:\n\t\tax1 = fig.add_subplot(311)\n\t\tax2 = fig.add_subplot(312)\n\n\t\tif auto_aspect:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), aspect=\"auto\", interpolation=\"none\")\n\t\telse:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), interpolation=\"none\")\n\t\tax1.set_title(\"Target Mel-Spectrogram\")\n\t\tfig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax1)\n\t\tax2.set_title(\"Predicted Mel-Spectrogram\")\n\telse:\n\t\tax2 = fig.add_subplot(211)\n\n\tif auto_aspect:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), aspect=\"auto\", interpolation=\"none\")\n\telse:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), interpolation=\"none\")\n\tfig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax2)\n\n\tplt.tight_layout()\n\tplt.savefig(path, format=\"png\")\n\tplt.close()\n\n\ndef plot_spectrogram_and_trace(pred_spectrogram, path, title=None, split_title=False, target_spectrogram=None, max_len=None, auto_aspect=False, sw=None, step=0):\n\tif max_len is not None:\n\t\ttarget_spectrogram = target_spectrogram[:max_len]\n\t\tpred_spectrogram = pred_spectrogram[:max_len]\n\n\tif split_title:\n\t\ttitle = split_title_line(title)\n\n\tfig = plt.figure(figsize=(10, 8))\n\t# Set common labels\n\tfig.text(0.5, 0.18, title, horizontalalignment=\"center\", fontsize=16)\n\n\t#target spectrogram subplot\n\tif target_spectrogram is not None:\n\t\tax1 = fig.add_subplot(311)\n\t\tax2 = fig.add_subplot(312)\n\n\t\tif auto_aspect:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), aspect=\"auto\", interpolation=\"none\")\n\t\telse:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), interpolation=\"none\")\n\t\tax1.set_title(\"Target Mel-Spectrogram\")\n\t\tfig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax1)\n\t\tax2.set_title(\"Predicted Mel-Spectrogram\")\n\telse:\n\t\tax2 = fig.add_subplot(211)\n\n\tif auto_aspect:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), aspect=\"auto\", interpolation=\"none\")\n\telse:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), interpolation=\"none\")\n\tfig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax2)\n\n\tplt.tight_layout()\n\tplt.savefig(path, format=\"png\")\n\tsw.add_figure(\"spectrogram\", fig, step)\n\tplt.close()\n\n\ndef plot_spectrogram_to_numpy(spectrogram):\n  global MATPLOTLIB_FLAG\n  if not MATPLOTLIB_FLAG:\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    MATPLOTLIB_FLAG = True\n  import matplotlib.pylab as plt\n  import numpy as np\n  \n  fig, ax = plt.subplots(figsize=(10,2))\n  im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n                  interpolation='none')\n  plt.colorbar(im, ax=ax)\n  plt.xlabel(\"Frames\")\n  plt.ylabel(\"Channels\")\n  plt.tight_layout()\n\n  fig.canvas.draw()\n  data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n  plt.close()\n  return data\n\n\ndef plot_alignment_to_numpy(alignment, info=None):\n  global MATPLOTLIB_FLAG\n  if not MATPLOTLIB_FLAG:\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    MATPLOTLIB_FLAG = True\n  import matplotlib.pylab as plt\n  import numpy as np\n\n  fig, ax = plt.subplots(figsize=(6, 4))\n  im = ax.imshow(alignment.transpose(), aspect='auto', origin='lower',\n                  interpolation='none')\n  fig.colorbar(im, ax=ax)\n  xlabel = 'Decoder timestep'\n  if info is not None:\n      xlabel += '\\n\\n' + info\n  plt.xlabel(xlabel)\n  plt.ylabel('Encoder timestep')\n  plt.tight_layout()\n\n  fig.canvas.draw()\n  data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n  plt.close()\n  return data\n", "models/synthesizer/utils/symbols.py": "\"\"\"\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run\nthrough Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n\"\"\"\n# from . import cmudict\n\n_pad        = \"_\"\n_eos        = \"~\"\n_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890!\\'(),-.:;? '\n\n#_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz12340!\\'(),-.:;? ' # use this old one if you want to train old model \n# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n#_arpabet = [\"@' + s for s in cmudict.valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad, _eos] + list(_characters) #+ _arpabet\n", "models/synthesizer/utils/cleaners.py": "\"\"\"\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\nhyperparameter. Some cleaners are English-specific. You\"ll typically want to use:\n  1. \"english_cleaners\" for English text\n  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\"\"\"\n\nimport re\nfrom unidecode import unidecode\nfrom .numbers import normalize_numbers\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\"\\s+\")\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1]) for x in [\n  (\"mrs\", \"misess\"),\n  (\"mr\", \"mister\"),\n  (\"dr\", \"doctor\"),\n  (\"st\", \"saint\"),\n  (\"co\", \"company\"),\n  (\"jr\", \"junior\"),\n  (\"maj\", \"major\"),\n  (\"gen\", \"general\"),\n  (\"drs\", \"doctors\"),\n  (\"rev\", \"reverend\"),\n  (\"lt\", \"lieutenant\"),\n  (\"hon\", \"honorable\"),\n  (\"sgt\", \"sergeant\"),\n  (\"capt\", \"captain\"),\n  (\"esq\", \"esquire\"),\n  (\"ltd\", \"limited\"),\n  (\"col\", \"colonel\"),\n  (\"ft\", \"fort\"),\n]]\n\n\ndef expand_abbreviations(text):\n  for regex, replacement in _abbreviations:\n    text = re.sub(regex, replacement, text)\n  return text\n\n\ndef expand_numbers(text):\n  return normalize_numbers(text)\n\n\ndef lowercase(text):\n  \"\"\"lowercase input tokens.\"\"\"\n  return text.lower()\n\n\ndef collapse_whitespace(text):\n  return re.sub(_whitespace_re, \" \", text)\n\n\ndef convert_to_ascii(text):\n  return unidecode(text)\n\n\ndef basic_cleaners(text):\n  \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef transliteration_cleaners(text):\n  \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef english_cleaners(text):\n  \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = expand_numbers(text)\n  text = expand_abbreviations(text)\n  text = collapse_whitespace(text)\n  return text\n", "models/synthesizer/utils/__init__.py": "import torch\n\n\n_output_ref = None\n_replicas_ref = None\n\ndef data_parallel_workaround(model, *input):\n    global _output_ref\n    global _replicas_ref\n    device_ids = list(range(torch.cuda.device_count()))\n    output_device = device_ids[0]\n    replicas = torch.nn.parallel.replicate(model, device_ids)\n    # input.shape = (num_args, batch, ...)\n    inputs = torch.nn.parallel.scatter(input, device_ids)\n    # inputs.shape = (num_gpus, num_args, batch/num_gpus, ...)\n    replicas = replicas[:len(inputs)]\n    outputs = torch.nn.parallel.parallel_apply(replicas, inputs)\n    y_hat = torch.nn.parallel.gather(outputs, output_device)\n    _output_ref = outputs\n    _replicas_ref = replicas\n    return y_hat\n\n\nclass ValueWindow():\n  def __init__(self, window_size=100):\n    self._window_size = window_size\n    self._values = []\n\n  def append(self, x):\n    self._values = self._values[-(self._window_size - 1):] + [x]\n\n  @property\n  def sum(self):\n    return sum(self._values)\n\n  @property\n  def count(self):\n    return len(self._values)\n\n  @property\n  def average(self):\n    return self.sum / max(1, self.count)\n\n  def reset(self):\n    self._values = []\n", "models/synthesizer/utils/text.py": "from .symbols import symbols\nfrom . import cleaners\nimport re\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r\"(.*?)\\{(.+?)\\}(.*)\")\n\n\ndef text_to_sequence(text, cleaner_names):\n  \"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n    in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n\n    Returns:\n      List of integers corresponding to the symbols in the text\n  \"\"\"\n  sequence = []\n\n  # Check for curly braces and treat their contents as ARPAbet:\n  while len(text):\n    m = _curly_re.match(text)\n    if not m:\n      sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n      break\n    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n    sequence += _arpabet_to_sequence(m.group(2))\n    text = m.group(3)\n\n  # Append EOS token\n  sequence.append(_symbol_to_id[\"~\"])\n  return sequence\n\n\ndef sequence_to_text(sequence):\n  \"\"\"Converts a sequence of IDs back to a string\"\"\"\n  result = \"\"\n  for symbol_id in sequence:\n    if symbol_id in _id_to_symbol:\n      s = _id_to_symbol[symbol_id]\n      # Enclose ARPAbet back in curly braces:\n      if len(s) > 1 and s[0] == \"@\":\n        s = \"{%s}\" % s[1:]\n      result += s\n  return result.replace(\"}{\", \" \")\n\n\ndef _clean_text(text, cleaner_names):\n  for name in cleaner_names:\n    cleaner = getattr(cleaners, name)\n    if not cleaner:\n      raise Exception(\"Unknown cleaner: %s\" % name)\n    text = cleaner(text)\n  return text\n\n\ndef _symbols_to_sequence(symbols):\n  return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n  return _symbols_to_sequence([\"@\" + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n  return s in _symbol_to_id and s not in (\"_\", \"~\")\n", "models/synthesizer/utils/numbers.py": "import re\nimport inflect\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n_pounds_re = re.compile(r\"\u00a3([0-9\\,]*[0-9]+)\")\n_dollars_re = re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"[0-9]+\")\n\n\ndef _remove_commas(m):\n  return m.group(1).replace(\",\", \"\")\n\n\ndef _expand_decimal_point(m):\n  return m.group(1).replace(\".\", \" point \")\n\n\ndef _expand_dollars(m):\n  match = m.group(1)\n  parts = match.split(\".\")\n  if len(parts) > 2:\n    return match + \" dollars\"  # Unexpected format\n  dollars = int(parts[0]) if parts[0] else 0\n  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n  if dollars and cents:\n    dollar_unit = \"dollar\" if dollars == 1 else \"dollars\"\n    cent_unit = \"cent\" if cents == 1 else \"cents\"\n    return \"%s %s, %s %s\" % (dollars, dollar_unit, cents, cent_unit)\n  elif dollars:\n    dollar_unit = \"dollar\" if dollars == 1 else \"dollars\"\n    return \"%s %s\" % (dollars, dollar_unit)\n  elif cents:\n    cent_unit = \"cent\" if cents == 1 else \"cents\"\n    return \"%s %s\" % (cents, cent_unit)\n  else:\n    return \"zero dollars\"\n\n\ndef _expand_ordinal(m):\n  return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n  num = int(m.group(0))\n  if num > 1000 and num < 3000:\n    if num == 2000:\n      return \"two thousand\"\n    elif num > 2000 and num < 2010:\n      return \"two thousand \" + _inflect.number_to_words(num % 100)\n    elif num % 100 == 0:\n      return _inflect.number_to_words(num // 100) + \" hundred\"\n    else:\n      return _inflect.number_to_words(num, andword=\"\", zero=\"oh\", group=2).replace(\", \", \" \")\n  else:\n    return _inflect.number_to_words(num, andword=\"\")\n\n\ndef normalize_numbers(text):\n  text = re.sub(_comma_number_re, _remove_commas, text)\n  text = re.sub(_pounds_re, r\"\\1 pounds\", text)\n  text = re.sub(_dollars_re, _expand_dollars, text)\n  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n  text = re.sub(_ordinal_re, _expand_ordinal, text)\n  text = re.sub(_number_re, _expand_number, text)\n  return text\n", "models/vocoder/display.py": "import matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport sys\n\n\ndef progbar(i, n, size=16):\n    done = (i * size) // n\n    bar = ''\n    for i in range(size):\n        bar += '\u2588' if i <= done else '\u2591'\n    return bar\n\n\ndef stream(message) :\n    try:\n        sys.stdout.write(\"\\r{%s}\" % message)\n    except:\n        #Remove non-ASCII characters from message\n        message = ''.join(i for i in message if ord(i)<128)\n        sys.stdout.write(\"\\r{%s}\" % message)\n\n\ndef simple_table(item_tuples) :\n\n    border_pattern = '+---------------------------------------'\n    whitespace = '                                            '\n\n    headings, cells, = [], []\n\n    for item in item_tuples :\n\n        heading, cell = str(item[0]), str(item[1])\n\n        pad_head = True if len(heading) < len(cell) else False\n\n        pad = abs(len(heading) - len(cell))\n        pad = whitespace[:pad]\n\n        pad_left = pad[:len(pad)//2]\n        pad_right = pad[len(pad)//2:]\n\n        if pad_head :\n            heading = pad_left + heading + pad_right\n        else :\n            cell = pad_left + cell + pad_right\n\n        headings += [heading]\n        cells += [cell]\n\n    border, head, body = '', '', ''\n\n    for i in range(len(item_tuples)) :\n\n        temp_head = f'| {headings[i]} '\n        temp_body = f'| {cells[i]} '\n\n        border += border_pattern[:len(temp_head)]\n        head += temp_head\n        body += temp_body\n\n        if i == len(item_tuples) - 1 :\n            head += '|'\n            body += '|'\n            border += '+'\n\n    print(border)\n    print(head)\n    print(border)\n    print(body)\n    print(border)\n    print(' ')\n\n\ndef time_since(started) :\n    elapsed = time.time() - started\n    m = int(elapsed // 60)\n    s = int(elapsed % 60)\n    if m >= 60 :\n        h = int(m // 60)\n        m = m % 60\n        return f'{h}h {m}m {s}s'\n    else :\n        return f'{m}m {s}s'\n\n\ndef save_attention(attn, path) :\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    plt.close(fig)\n\n\ndef save_and_trace_attention(attn, path, sw, step):\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    sw.add_figure('attention', fig, step)\n    plt.close(fig)\n\n\ndef save_spectrogram(M, path, length=None) :\n    M = np.flip(M, axis=0)\n    if length : M = M[:, :length]\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(M, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    plt.close(fig)\n\n\ndef plot(array) : \n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n\ndef plot_spec(M) :\n    M = np.flip(M, axis=0)\n    plt.figure(figsize=(18,4))\n    plt.imshow(M, interpolation='nearest', aspect='auto')\n    plt.show()\n\n", "models/vocoder/vocoder_dataset.py": "from torch.utils.data import Dataset\nfrom pathlib import Path\nfrom models.vocoder.wavernn import audio\nimport models.vocoder.wavernn.hparams as hp\nimport numpy as np\nimport torch\n\n\nclass VocoderDataset(Dataset):\n    def __init__(self, metadata_fpath: Path, mel_dir: Path, wav_dir: Path):\n        print(\"Using inputs from:\\n\\t%s\\n\\t%s\\n\\t%s\" % (metadata_fpath, mel_dir, wav_dir))\n        \n        with metadata_fpath.open(\"r\") as metadata_file:\n            metadata = [line.split(\"|\") for line in metadata_file]\n        \n        gta_fnames = [x[1] for x in metadata if int(x[4])]\n        gta_fpaths = [mel_dir.joinpath(fname) for fname in gta_fnames]\n        wav_fnames = [x[0] for x in metadata if int(x[4])]\n        wav_fpaths = [wav_dir.joinpath(fname) for fname in wav_fnames]\n        self.samples_fpaths = list(zip(gta_fpaths, wav_fpaths))\n        \n        print(\"Found %d samples\" % len(self.samples_fpaths))\n    \n    def __getitem__(self, index):  \n        mel_path, wav_path = self.samples_fpaths[index]\n        \n        # Load the mel spectrogram and adjust its range to [-1, 1]\n        mel = np.load(mel_path).T.astype(np.float32) / hp.mel_max_abs_value\n        \n        # Load the wav\n        wav = np.load(wav_path)\n        if hp.apply_preemphasis:\n            wav = audio.pre_emphasis(wav)\n        wav = np.clip(wav, -1, 1)\n        \n        # Fix for missing padding   # TODO: settle on whether this is any useful\n        r_pad =  (len(wav) // hp.hop_length + 1) * hp.hop_length - len(wav)\n        wav = np.pad(wav, (0, r_pad), mode='constant')\n        assert len(wav) >= mel.shape[1] * hp.hop_length\n        wav = wav[:mel.shape[1] * hp.hop_length]\n        assert len(wav) % hp.hop_length == 0\n        \n        # Quantize the wav\n        if hp.voc_mode == 'RAW':\n            if hp.mu_law:\n                quant = audio.encode_mu_law(wav, mu=2 ** hp.bits)\n            else:\n                quant = audio.float_2_label(wav, bits=hp.bits)\n        elif hp.voc_mode == 'MOL':\n            quant = audio.float_2_label(wav, bits=16)\n            \n        return mel.astype(np.float32), quant.astype(np.int64)\n\n    def __len__(self):\n        return len(self.samples_fpaths)\n        \n        \ndef collate_vocoder(batch):\n    mel_win = hp.voc_seq_len // hp.hop_length + 2 * hp.voc_pad\n    max_offsets = [x[0].shape[-1] -2 - (mel_win + 2 * hp.voc_pad) for x in batch]\n    mel_offsets = [np.random.randint(0, offset) for offset in max_offsets]\n    sig_offsets = [(offset + hp.voc_pad) * hp.hop_length for offset in mel_offsets]\n\n    mels = [x[0][:, mel_offsets[i]:mel_offsets[i] + mel_win] for i, x in enumerate(batch)]\n\n    labels = [x[1][sig_offsets[i]:sig_offsets[i] + hp.voc_seq_len + 1] for i, x in enumerate(batch)]\n\n    mels = np.stack(mels).astype(np.float32)\n    labels = np.stack(labels).astype(np.int64)\n\n    mels = torch.tensor(mels)\n    labels = torch.tensor(labels).long()\n\n    x = labels[:, :hp.voc_seq_len]\n    y = labels[:, 1:]\n\n    bits = 16 if hp.voc_mode == 'MOL' else hp.bits\n\n    x = audio.label_2_float(x.float(), bits)\n\n    if hp.voc_mode == 'MOL' :\n        y = audio.label_2_float(y.float(), bits)\n\n    return x, y, mels", "models/vocoder/distribution.py": "import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\ndef log_sum_exp(x):\n    \"\"\" numerically stable log_sum_exp implementation that prevents overflow \"\"\"\n    # TF ordering\n    axis = len(x.size()) - 1\n    m, _ = torch.max(x, dim=axis)\n    m2, _ = torch.max(x, dim=axis, keepdim=True)\n    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n\n\n# It is adapted from https://github.com/r9y9/wavenet_vocoder/blob/master/wavenet_vocoder/mixture.py\ndef discretized_mix_logistic_loss(y_hat, y, num_classes=65536,\n                                  log_scale_min=None, reduce=True):\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    y_hat = y_hat.permute(0,2,1)\n    assert y_hat.dim() == 3\n    assert y_hat.size(1) % 3 == 0\n    nr_mix = y_hat.size(1) // 3\n\n    # (B x T x C)\n    y_hat = y_hat.transpose(1, 2)\n\n    # unpack parameters. (B, T, num_mixtures) x 3\n    logit_probs = y_hat[:, :, :nr_mix]\n    means = y_hat[:, :, nr_mix:2 * nr_mix]\n    log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix], min=log_scale_min)\n\n    # B x T x 1 -> B x T x num_mixtures\n    y = y.expand_as(means)\n\n    centered_y = y - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n    cdf_plus = torch.sigmoid(plus_in)\n    min_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n    cdf_min = torch.sigmoid(min_in)\n\n    # log probability for edge case of 0 (before scaling)\n    # equivalent: torch.log(F.sigmoid(plus_in))\n    log_cdf_plus = plus_in - F.softplus(plus_in)\n\n    # log probability for edge case of 255 (before scaling)\n    # equivalent: (1 - F.sigmoid(min_in)).log()\n    log_one_minus_cdf_min = -F.softplus(min_in)\n\n    # probability for all other cases\n    cdf_delta = cdf_plus - cdf_min\n\n    mid_in = inv_stdv * centered_y\n    # log probability in the center of the bin, to be used in extreme cases\n    # (not actually used in our code)\n    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n\n    # tf equivalent\n    \"\"\"\n    log_probs = tf.where(x < -0.999, log_cdf_plus,\n                         tf.where(x > 0.999, log_one_minus_cdf_min,\n                                  tf.where(cdf_delta > 1e-5,\n                                           tf.log(tf.maximum(cdf_delta, 1e-12)),\n                                           log_pdf_mid - np.log(127.5))))\n    \"\"\"\n    # TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value\n    # for num_classes=65536 case? 1e-7? not sure..\n    inner_inner_cond = (cdf_delta > 1e-5).float()\n\n    inner_inner_out = inner_inner_cond * \\\n        torch.log(torch.clamp(cdf_delta, min=1e-12)) + \\\n        (1. - inner_inner_cond) * (log_pdf_mid - np.log((num_classes - 1) / 2))\n    inner_cond = (y > 0.999).float()\n    inner_out = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n    cond = (y < -0.999).float()\n    log_probs = cond * log_cdf_plus + (1. - cond) * inner_out\n\n    log_probs = log_probs + F.log_softmax(logit_probs, -1)\n\n    if reduce:\n        return -torch.mean(log_sum_exp(log_probs))\n    else:\n        return -log_sum_exp(log_probs).unsqueeze(-1)\n\n\ndef sample_from_discretized_mix_logistic(y, log_scale_min=None):\n    \"\"\"\n    Sample from discretized mixture of logistic distributions\n    Args:\n        y (Tensor): B x C x T\n        log_scale_min (float): Log scale minimum value\n    Returns:\n        Tensor: sample in range of [-1, 1].\n    \"\"\"\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    assert y.size(1) % 3 == 0\n    nr_mix = y.size(1) // 3\n\n    # B x T x C\n    y = y.transpose(1, 2)\n    logit_probs = y[:, :, :nr_mix]\n\n    # sample mixture indicator from softmax\n    temp = logit_probs.data.new(logit_probs.size()).uniform_(1e-5, 1.0 - 1e-5)\n    temp = logit_probs.data - torch.log(- torch.log(temp))\n    _, argmax = temp.max(dim=-1)\n\n    # (B, T) -> (B, T, nr_mix)\n    one_hot = to_one_hot(argmax, nr_mix)\n    # select logistic parameters\n    means = torch.sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, dim=-1)\n    log_scales = torch.clamp(torch.sum(\n        y[:, :, 2 * nr_mix:3 * nr_mix] * one_hot, dim=-1), min=log_scale_min)\n    # sample from logistic & clip to interval\n    # we don't actually round to the nearest 8bit value when sampling\n    u = means.data.new(means.size()).uniform_(1e-5, 1.0 - 1e-5)\n    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n\n    x = torch.clamp(torch.clamp(x, min=-1.), max=1.)\n\n    return x\n\n\ndef to_one_hot(tensor, n, fill_with=1.):\n    # we perform one hot encore with respect to the last axis\n    one_hot = torch.FloatTensor(tensor.size() + (n,)).zero_()\n    if tensor.is_cuda:\n        one_hot = one_hot.cuda()\n    one_hot.scatter_(len(tensor.size()), tensor.unsqueeze(-1), fill_with)\n    return one_hot\n", "models/vocoder/__init__.py": "#", "models/vocoder/fregan/modules.py": "import torch\nimport torch.nn.functional as F\n\nclass KernelPredictor(torch.nn.Module):\n    ''' Kernel predictor for the location-variable convolutions\n    '''\n\n    def __init__(self,\n                 cond_channels,\n                 conv_in_channels,\n                 conv_out_channels,\n                 conv_layers,\n                 conv_kernel_size=3,\n                 kpnet_hidden_channels=64,\n                 kpnet_conv_size=3,\n                 kpnet_dropout=0.0,\n                 kpnet_nonlinear_activation=\"LeakyReLU\",\n                 kpnet_nonlinear_activation_params={\"negative_slope\": 0.1}\n                 ):\n        '''\n        Args:\n            cond_channels (int): number of channel for the conditioning sequence,\n            conv_in_channels (int): number of channel for the input sequence,\n            conv_out_channels (int): number of channel for the output sequence,\n            conv_layers (int):\n            kpnet_\n        '''\n        super().__init__()\n\n        self.conv_in_channels = conv_in_channels\n        self.conv_out_channels = conv_out_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.conv_layers = conv_layers\n\n        l_w = conv_in_channels * conv_out_channels * conv_kernel_size * conv_layers\n        l_b = conv_out_channels * conv_layers\n\n        padding = (kpnet_conv_size - 1) // 2\n        self.input_conv = torch.nn.Sequential(\n            torch.nn.Conv1d(cond_channels, kpnet_hidden_channels, 5, padding=(5 - 1) // 2, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n        )\n\n        self.residual_conv = torch.nn.Sequential(\n            torch.nn.Dropout(kpnet_dropout),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Dropout(kpnet_dropout),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Dropout(kpnet_dropout),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n        )\n\n        self.kernel_conv = torch.nn.Conv1d(kpnet_hidden_channels, l_w, kpnet_conv_size,\n                                           padding=padding, bias=True)\n        self.bias_conv = torch.nn.Conv1d(kpnet_hidden_channels, l_b, kpnet_conv_size, padding=padding,\n                                         bias=True)\n\n    def forward(self, c):\n        '''\n        Args:\n            c (Tensor): the conditioning sequence (batch, cond_channels, cond_length)\n        Returns:\n        '''\n        batch, cond_channels, cond_length = c.shape\n\n        c = self.input_conv(c)\n        c = c + self.residual_conv(c)\n        k = self.kernel_conv(c)\n        b = self.bias_conv(c)\n\n        kernels = k.contiguous().view(batch,\n                                      self.conv_layers,\n                                      self.conv_in_channels,\n                                      self.conv_out_channels,\n                                      self.conv_kernel_size,\n                                      cond_length)\n        bias = b.contiguous().view(batch,\n                                   self.conv_layers,\n                                   self.conv_out_channels,\n                                   cond_length)\n        return kernels, bias\n\n\nclass LVCBlock(torch.nn.Module):\n    ''' the location-variable convolutions\n    '''\n\n    def __init__(self,\n                 in_channels,\n                 cond_channels,\n                 upsample_ratio,\n                 conv_layers=4,\n                 conv_kernel_size=3,\n                 cond_hop_length=256,\n                 kpnet_hidden_channels=64,\n                 kpnet_conv_size=3,\n                 kpnet_dropout=0.0\n                 ):\n        super().__init__()\n\n        self.cond_hop_length = cond_hop_length\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.convs = torch.nn.ModuleList()\n\n        self.upsample = torch.nn.ConvTranspose1d(in_channels, in_channels,\n                                    kernel_size=upsample_ratio*2, stride=upsample_ratio,\n                                    padding=upsample_ratio // 2 + upsample_ratio % 2,\n                                    output_padding=upsample_ratio % 2)\n\n\n        self.kernel_predictor = KernelPredictor(\n            cond_channels=cond_channels,\n            conv_in_channels=in_channels,\n            conv_out_channels=2 * in_channels,\n            conv_layers=conv_layers,\n            conv_kernel_size=conv_kernel_size,\n            kpnet_hidden_channels=kpnet_hidden_channels,\n            kpnet_conv_size=kpnet_conv_size,\n            kpnet_dropout=kpnet_dropout\n        )\n\n\n        for i in range(conv_layers):\n            padding = (3 ** i) * int((conv_kernel_size - 1) / 2)\n            conv = torch.nn.Conv1d(in_channels, in_channels, kernel_size=conv_kernel_size, padding=padding, dilation=3 ** i)\n\n            self.convs.append(conv)\n\n\n    def forward(self, x, c):\n        ''' forward propagation of the location-variable convolutions.\n        Args:\n            x (Tensor): the input sequence (batch, in_channels, in_length)\n            c (Tensor): the conditioning sequence (batch, cond_channels, cond_length)\n\n        Returns:\n            Tensor: the output sequence (batch, in_channels, in_length)\n        '''\n        batch, in_channels, in_length = x.shape\n\n\n        kernels, bias = self.kernel_predictor(c)\n\n        x = F.leaky_relu(x, 0.2)\n        x = self.upsample(x)\n\n        for i in range(self.conv_layers):\n            y = F.leaky_relu(x, 0.2)\n            y = self.convs[i](y)\n            y = F.leaky_relu(y, 0.2)\n\n            k = kernels[:, i, :, :, :, :]\n            b = bias[:, i, :, :]\n            y = self.location_variable_convolution(y, k, b, 1, self.cond_hop_length)\n            x = x + torch.sigmoid(y[:, :in_channels, :]) * torch.tanh(y[:, in_channels:, :])\n        return x\n\n    def location_variable_convolution(self, x, kernel, bias, dilation, hop_size):\n        ''' perform location-variable convolution operation on the input sequence (x) using the local convolution kernl.\n        Time: 414 \u03bcs \u00b1 309 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each), test on NVIDIA V100.\n        Args:\n            x (Tensor): the input sequence (batch, in_channels, in_length).\n            kernel (Tensor): the local convolution kernel (batch, in_channel, out_channels, kernel_size, kernel_length)\n            bias (Tensor): the bias for the local convolution (batch, out_channels, kernel_length)\n            dilation (int): the dilation of convolution.\n            hop_size (int): the hop_size of the conditioning sequence.\n        Returns:\n            (Tensor): the output sequence after performing local convolution. (batch, out_channels, in_length).\n        '''\n        batch, in_channels, in_length = x.shape\n        batch, in_channels, out_channels, kernel_size, kernel_length = kernel.shape\n\n\n        assert in_length == (kernel_length * hop_size), \"length of (x, kernel) is not matched\"\n\n        padding = dilation * int((kernel_size - 1) / 2)\n        x = F.pad(x, (padding, padding), 'constant', 0)  # (batch, in_channels, in_length + 2*padding)\n        x = x.unfold(2, hop_size + 2 * padding, hop_size)  # (batch, in_channels, kernel_length, hop_size + 2*padding)\n\n        if hop_size < dilation:\n            x = F.pad(x, (0, dilation), 'constant', 0)\n        x = x.unfold(3, dilation,\n                     dilation)  # (batch, in_channels, kernel_length, (hop_size + 2*padding)/dilation, dilation)\n        x = x[:, :, :, :, :hop_size]\n        x = x.transpose(3, 4)  # (batch, in_channels, kernel_length, dilation, (hop_size + 2*padding)/dilation)\n        x = x.unfold(4, kernel_size, 1)  # (batch, in_channels, kernel_length, dilation, _, kernel_size)\n\n        o = torch.einsum('bildsk,biokl->bolsd', x, kernel)\n        o = o + bias.unsqueeze(-1).unsqueeze(-1)\n        o = o.contiguous().view(batch, out_channels, -1)\n        return o\n", "models/vocoder/fregan/train.py": "import warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport itertools\nimport os\nimport time\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DistributedSampler, DataLoader\nfrom torch.distributed import init_process_group\nfrom torch.nn.parallel import DistributedDataParallel\nfrom models.vocoder.fregan.meldataset import MelDataset, mel_spectrogram, get_dataset_filelist\nfrom models.vocoder.fregan.generator import FreGAN\nfrom models.vocoder.fregan.discriminator import ResWiseMultiPeriodDiscriminator, ResWiseMultiScaleDiscriminator\nfrom utils.loss import feature_loss, generator_loss, discriminator_loss\nfrom models.vocoder.fregan.utils import plot_spectrogram, scan_checkpoint, load_checkpoint, save_checkpoint\n\n\ntorch.backends.cudnn.benchmark = True\n\n\ndef train(rank, a, h):\n\n    a.checkpoint_path = a.models_dir.joinpath(a.run_id+'_fregan')\n    a.checkpoint_path.mkdir(exist_ok=True)\n    a.training_epochs = 3100\n    a.stdout_interval = 5\n    a.checkpoint_interval = a.backup_every\n    a.summary_interval = 5000\n    a.validation_interval = 1000\n    a.fine_tuning = True\n\n    a.input_wavs_dir = a.syn_dir.joinpath(\"audio\")\n    a.input_mels_dir = a.syn_dir.joinpath(\"mels\")\n\n    if h.num_gpus > 1:\n        init_process_group(backend=h.dist_config['dist_backend'], init_method=h.dist_config['dist_url'],\n                           world_size=h.dist_config['world_size'] * h.num_gpus, rank=rank)\n\n    torch.cuda.manual_seed(h.seed)\n    device = torch.device('cuda:{:d}'.format(rank))\n\n    generator = FreGAN(h).to(device)\n    mpd = ResWiseMultiPeriodDiscriminator().to(device)\n    msd = ResWiseMultiScaleDiscriminator().to(device)\n\n    if rank == 0:\n        print(generator)\n        os.makedirs(a.checkpoint_path, exist_ok=True)\n        print(\"checkpoints directory : \", a.checkpoint_path)\n\n    if os.path.isdir(a.checkpoint_path):\n        cp_g = scan_checkpoint(a.checkpoint_path, 'g_fregan_')\n        cp_do = scan_checkpoint(a.checkpoint_path, 'do_fregan_')\n\n    steps = 0\n    if cp_g is None or cp_do is None:\n        state_dict_do = None\n        last_epoch = -1\n    else:\n        state_dict_g = load_checkpoint(cp_g, device)\n        state_dict_do = load_checkpoint(cp_do, device)\n        generator.load_state_dict(state_dict_g['generator'])\n        mpd.load_state_dict(state_dict_do['mpd'])\n        msd.load_state_dict(state_dict_do['msd'])\n        steps = state_dict_do['steps'] + 1\n        last_epoch = state_dict_do['epoch']\n\n    if h.num_gpus > 1:\n        generator = DistributedDataParallel(generator, device_ids=[rank]).to(device)\n        mpd = DistributedDataParallel(mpd, device_ids=[rank]).to(device)\n        msd = DistributedDataParallel(msd, device_ids=[rank]).to(device)\n\n    optim_g = torch.optim.AdamW(generator.parameters(), h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n    optim_d = torch.optim.AdamW(itertools.chain(msd.parameters(), mpd.parameters()),\n                                h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n\n    if state_dict_do is not None:\n        optim_g.load_state_dict(state_dict_do['optim_g'])\n        optim_d.load_state_dict(state_dict_do['optim_d'])\n\n    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=h.lr_decay, last_epoch=last_epoch)\n    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=h.lr_decay, last_epoch=last_epoch)\n\n    training_filelist, validation_filelist = get_dataset_filelist(a)\n\n    trainset = MelDataset(training_filelist, h.segment_size, h.n_fft, h.num_mels,\n                          h.hop_size, h.win_size, h.sampling_rate, h.fmin, h.fmax, n_cache_reuse=0,\n                          shuffle=False if h.num_gpus > 1 else True, fmax_loss=h.fmax_for_loss, device=device,\n                          fine_tuning=a.fine_tuning, base_mels_path=a.input_mels_dir)\n\n    train_sampler = DistributedSampler(trainset) if h.num_gpus > 1 else None\n\n    train_loader = DataLoader(trainset, num_workers=h.num_workers, shuffle=False,\n                              sampler=train_sampler,\n                              batch_size=h.batch_size,\n                              pin_memory=True,\n                              drop_last=True)\n\n    if rank == 0:\n        validset = MelDataset(validation_filelist, h.segment_size, h.n_fft, h.num_mels,\n                              h.hop_size, h.win_size, h.sampling_rate, h.fmin, h.fmax, False, False, n_cache_reuse=0,\n                              fmax_loss=h.fmax_for_loss, device=device, fine_tuning=a.fine_tuning,\n                              base_mels_path=a.input_mels_dir)\n        validation_loader = DataLoader(validset, num_workers=1, shuffle=False,\n                                       sampler=None,\n                                       batch_size=1,\n                                       pin_memory=True,\n                                       drop_last=True)\n\n        sw = SummaryWriter(os.path.join(a.checkpoint_path, 'logs'))\n\n    generator.train()\n    mpd.train()\n    msd.train()\n    for epoch in range(max(0, last_epoch), a.training_epochs):\n        if rank == 0:\n            start = time.time()\n            print(\"Epoch: {}\".format(epoch + 1))\n\n        if h.num_gpus > 1:\n            train_sampler.set_epoch(epoch)\n\n        for i, batch in enumerate(train_loader):\n            if rank == 0:\n                start_b = time.time()\n            x, y, _, y_mel = batch\n            x = torch.autograd.Variable(x.to(device, non_blocking=True))\n            y = torch.autograd.Variable(y.to(device, non_blocking=True))\n            y_mel = torch.autograd.Variable(y_mel.to(device, non_blocking=True))\n            y = y.unsqueeze(1)\n            y_g_hat = generator(x)\n            y_g_hat_mel = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels, h.sampling_rate, h.hop_size,\n                                          h.win_size,\n                                          h.fmin, h.fmax_for_loss)\n\n            if steps > h.disc_start_step:\n                optim_d.zero_grad()\n\n                # MPD\n                y_df_hat_r, y_df_hat_g, _, _ = mpd(y, y_g_hat.detach())\n                loss_disc_f, losses_disc_f_r, losses_disc_f_g = discriminator_loss(y_df_hat_r, y_df_hat_g)\n\n                # MSD\n                y_ds_hat_r, y_ds_hat_g, _, _ = msd(y, y_g_hat.detach())\n                loss_disc_s, losses_disc_s_r, losses_disc_s_g = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n\n                loss_disc_all = loss_disc_s + loss_disc_f\n\n                loss_disc_all.backward()\n                optim_d.step()\n\n            # Generator\n            optim_g.zero_grad()\n\n\n            # L1 Mel-Spectrogram Loss\n            loss_mel = F.l1_loss(y_mel, y_g_hat_mel) * 45\n\n            # sc_loss, mag_loss = stft_loss(y_g_hat[:, :, :y.size(2)].squeeze(1), y.squeeze(1))\n            # loss_mel = h.lambda_aux * (sc_loss + mag_loss)  # STFT Loss\n\n            if steps > h.disc_start_step:\n                y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g = mpd(y, y_g_hat)\n                y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = msd(y, y_g_hat)\n                loss_fm_f = feature_loss(fmap_f_r, fmap_f_g)\n                loss_fm_s = feature_loss(fmap_s_r, fmap_s_g)\n                loss_gen_f, losses_gen_f = generator_loss(y_df_hat_g)\n                loss_gen_s, losses_gen_s = generator_loss(y_ds_hat_g)\n                loss_gen_all = loss_gen_s + loss_gen_f + (2 * (loss_fm_s + loss_fm_f)) + loss_mel\n            else:\n                loss_gen_all = loss_mel\n\n            loss_gen_all.backward()\n            optim_g.step()\n\n            if rank == 0:\n                # STDOUT logging\n                if steps % a.stdout_interval == 0:\n                    with torch.no_grad():\n                        mel_error = F.l1_loss(y_mel, y_g_hat_mel).item()\n\n                    print('Steps : {:d}, Gen Loss Total : {:4.3f}, Mel-Spec. Error : {:4.3f}, s/b : {:4.3f}'.\n                          format(steps, loss_gen_all, mel_error, time.time() - start_b))\n\n                # checkpointing\n                if steps % a.checkpoint_interval == 0 and steps != 0:\n                    checkpoint_path = \"{}/g_fregan_{:08d}.pt\".format(a.checkpoint_path, steps)\n                    save_checkpoint(checkpoint_path,\n                                    {'generator': (generator.module if h.num_gpus > 1 else generator).state_dict()})\n                    checkpoint_path = \"{}/do_fregan_{:08d}.pt\".format(a.checkpoint_path, steps)\n                    save_checkpoint(checkpoint_path,\n                                    {'mpd': (mpd.module if h.num_gpus > 1\n                                             else mpd).state_dict(),\n                                     'msd': (msd.module if h.num_gpus > 1\n                                             else msd).state_dict(),\n                                     'optim_g': optim_g.state_dict(), 'optim_d': optim_d.state_dict(), 'steps': steps,\n                                     'epoch': epoch})\n\n                # Tensorboard summary logging\n                if steps % a.summary_interval == 0:\n                    sw.add_scalar(\"training/gen_loss_total\", loss_gen_all, steps)\n                    sw.add_scalar(\"training/mel_spec_error\", mel_error, steps)\n\n                # Validation\n                if steps % a.validation_interval == 0:  # and steps != 0:\n                    generator.eval()\n                    torch.cuda.empty_cache()\n                    val_err_tot = 0\n                    with torch.no_grad():\n                        for j, batch in enumerate(validation_loader):\n                            x, y, _, y_mel = batch\n                            y_g_hat = generator(x.to(device))\n                            y_mel = torch.autograd.Variable(y_mel.to(device, non_blocking=True))\n                            y_g_hat_mel = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels, h.sampling_rate,\n                                                          h.hop_size, h.win_size,\n                                                          h.fmin, h.fmax_for_loss)\n                            #val_err_tot += F.l1_loss(y_mel, y_g_hat_mel).item()\n\n                            if j <= 4:\n                                if steps == 0:\n                                    sw.add_audio('gt/y_{}'.format(j), y[0], steps, h.sampling_rate)\n                                    sw.add_figure('gt/y_spec_{}'.format(j), plot_spectrogram(x[0]), steps)\n\n                                sw.add_audio('generated/y_hat_{}'.format(j), y_g_hat[0], steps, h.sampling_rate)\n                                y_hat_spec = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels,\n                                                             h.sampling_rate, h.hop_size, h.win_size,\n                                                             h.fmin, h.fmax)\n                                sw.add_figure('generated/y_hat_spec_{}'.format(j),\n                                              plot_spectrogram(y_hat_spec.squeeze(0).cpu().numpy()), steps)\n\n                        val_err = val_err_tot / (j + 1)\n                        sw.add_scalar(\"validation/mel_spec_error\", val_err, steps)\n\n                    generator.train()\n\n            steps += 1\n\n        scheduler_g.step()\n        scheduler_d.step()\n\n        if rank == 0:\n            print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, int(time.time() - start)))\n\n\n", "models/vocoder/fregan/utils.py": "import glob\nimport os\nimport matplotlib\nimport torch\nfrom torch.nn.utils import weight_norm\nmatplotlib.use(\"Agg\")\nimport matplotlib.pylab as plt\nimport shutil\n\n\ndef build_env(config, config_name, path):\n    t_path = os.path.join(path, config_name)\n    if config != t_path:\n        os.makedirs(path, exist_ok=True)\n        shutil.copyfile(config, os.path.join(path, config_name))\n\n\ndef plot_spectrogram(spectrogram):\n    fig, ax = plt.subplots(figsize=(10, 2))\n    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n                   interpolation='none')\n    plt.colorbar(im, ax=ax)\n\n    fig.canvas.draw()\n    plt.close()\n\n    return fig\n\n\ndef apply_weight_norm(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        weight_norm(m)\n\n\n\ndef load_checkpoint(filepath, device):\n    assert os.path.isfile(filepath)\n    print(\"Loading '{}'\".format(filepath))\n    checkpoint_dict = torch.load(filepath, map_location=device)\n    print(\"Complete.\")\n    return checkpoint_dict\n\n\ndef save_checkpoint(filepath, obj):\n    print(\"Saving checkpoint to {}\".format(filepath))\n    torch.save(obj, filepath)\n    print(\"Complete.\")\n\n\ndef scan_checkpoint(cp_dir, prefix):\n    pattern = os.path.join(cp_dir, prefix + '????????.pt')\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]", "models/vocoder/fregan/dwt.py": "# Copyright (c) 2019, Adobe Inc. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike\n# 4.0 International Public License. To view a copy of this license, visit\n# https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.\n\n# DWT code borrow from https://github.com/LiQiufu/WaveSNet/blob/12cb9d24208c3d26917bf953618c30f0c6b0f03d/DWT_IDWT/DWT_IDWT_layer.py\n\n\nimport pywt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = ['DWT_1D']\nPad_Mode = ['constant', 'reflect', 'replicate', 'circular']\n\n\nclass DWT_1D(nn.Module):\n    def __init__(self, pad_type='reflect', wavename='haar',\n                 stride=2, in_channels=1, out_channels=None, groups=None,\n                 kernel_size=None, trainable=False):\n\n        super(DWT_1D, self).__init__()\n        self.trainable = trainable\n        self.kernel_size = kernel_size\n        if not self.trainable:\n            assert self.kernel_size == None\n        self.in_channels = in_channels\n        self.out_channels = self.in_channels if out_channels == None else out_channels\n        self.groups = self.in_channels if groups == None else groups\n        assert isinstance(self.groups, int) and self.in_channels % self.groups == 0\n        self.stride = stride\n        assert self.stride == 2\n        self.wavename = wavename\n        self.pad_type = pad_type\n        assert self.pad_type in Pad_Mode\n        self.get_filters()\n        self.initialization()\n\n    def get_filters(self):\n        wavelet = pywt.Wavelet(self.wavename)\n        band_low = torch.tensor(wavelet.rec_lo)\n        band_high = torch.tensor(wavelet.rec_hi)\n        length_band = band_low.size()[0]\n        self.kernel_size = length_band if self.kernel_size == None else self.kernel_size\n        assert self.kernel_size >= length_band\n        a = (self.kernel_size - length_band) // 2\n        b = - (self.kernel_size - length_band - a)\n        b = None if b == 0 else b\n        self.filt_low = torch.zeros(self.kernel_size)\n        self.filt_high = torch.zeros(self.kernel_size)\n        self.filt_low[a:b] = band_low\n        self.filt_high[a:b] = band_high\n\n    def initialization(self):\n        self.filter_low = self.filt_low[None, None, :].repeat((self.out_channels, self.in_channels // self.groups, 1))\n        self.filter_high = self.filt_high[None, None, :].repeat((self.out_channels, self.in_channels // self.groups, 1))\n        if torch.cuda.is_available():\n            self.filter_low = self.filter_low.cuda()\n            self.filter_high = self.filter_high.cuda()\n        if self.trainable:\n            self.filter_low = nn.Parameter(self.filter_low)\n            self.filter_high = nn.Parameter(self.filter_high)\n        if self.kernel_size % 2 == 0:\n            self.pad_sizes = [self.kernel_size // 2 - 1, self.kernel_size // 2 - 1]\n        else:\n            self.pad_sizes = [self.kernel_size // 2, self.kernel_size // 2]\n\n    def forward(self, input):\n        assert isinstance(input, torch.Tensor)\n        assert len(input.size()) == 3\n        assert input.size()[1] == self.in_channels\n        input = F.pad(input, pad=self.pad_sizes, mode=self.pad_type)\n        return F.conv1d(input, self.filter_low.to(input.device), stride=self.stride, groups=self.groups), \\\n               F.conv1d(input, self.filter_high.to(input.device), stride=self.stride, groups=self.groups)\n", "models/vocoder/fregan/generator.py": "import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\nfrom torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\nfrom utils.util import init_weights, get_padding\n\nLRELU_SLOPE = 0.1\n\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5, 7)):\n        super(ResBlock1, self).__init__()\n        self.h = h\n        self.convs1 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n                               padding=get_padding(kernel_size, dilation[2]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[3],\n                               padding=get_padding(kernel_size, dilation[3])))\n        ])\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1)))\n        ])\n        self.convs2.apply(init_weights)\n\n    def forward(self, x):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            xt = c2(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)\n\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.h = h\n        self.convs = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1])))\n        ])\n        self.convs.apply(init_weights)\n\n    def forward(self, x):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)\n\n\nclass FreGAN(torch.nn.Module):\n    def __init__(self, h, top_k=4):\n        super(FreGAN, self).__init__()\n        self.h = h\n\n        self.num_kernels = len(h.resblock_kernel_sizes)\n        self.num_upsamples = len(h.upsample_rates)\n        self.upsample_rates = h.upsample_rates\n        self.up_kernels = h.upsample_kernel_sizes\n        self.cond_level = self.num_upsamples - top_k\n        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n\n        self.ups = nn.ModuleList()\n        self.cond_up = nn.ModuleList()\n        self.res_output = nn.ModuleList()\n        upsample_ = 1\n        kr = 80\n\n        for i, (u, k) in enumerate(zip(self.upsample_rates, self.up_kernels)):\n#            self.ups.append(weight_norm(\n #               ConvTranspose1d(h.upsample_initial_channel // (2 ** i), h.upsample_initial_channel // (2 ** (i + 1)),\n #                               k, u, padding=(k - u) // 2)))\n            self.ups.append(weight_norm(ConvTranspose1d(h.upsample_initial_channel//(2**i),\n                            h.upsample_initial_channel//(2**(i+1)),\n                            k, u, padding=(u//2 + u%2), output_padding=u%2)))\n\n            if i > (self.num_upsamples - top_k):\n                self.res_output.append(\n                    nn.Sequential(\n                        nn.Upsample(scale_factor=u, mode='nearest'),\n                        weight_norm(nn.Conv1d(h.upsample_initial_channel // (2 ** i),\n                                              h.upsample_initial_channel // (2 ** (i + 1)), 1))\n                    )\n                )\n            if i >= (self.num_upsamples - top_k):\n                self.cond_up.append(\n                    weight_norm(\n                        ConvTranspose1d(kr, h.upsample_initial_channel // (2 ** i),\n                                        self.up_kernels[i - 1], self.upsample_rates[i - 1],\n                                        padding=(self.upsample_rates[i-1]//2+self.upsample_rates[i-1]%2), output_padding=self.upsample_rates[i-1]%2))\n                )\n                kr = h.upsample_initial_channel // (2 ** i)\n\n            upsample_ *= u\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = h.upsample_initial_channel // (2 ** (i + 1))\n            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n                self.resblocks.append(resblock(h, ch, k, d))\n\n        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n        self.ups.apply(init_weights)\n        self.conv_post.apply(init_weights)\n        self.cond_up.apply(init_weights)\n        self.res_output.apply(init_weights)\n\n    def forward(self, x):\n        mel = x\n        x = self.conv_pre(x)\n        output = None\n        for i in range(self.num_upsamples):\n            if i >= self.cond_level:\n                mel = self.cond_up[i - self.cond_level](mel)\n                x += mel\n            if i > self.cond_level:\n                if output is None:\n                    output = self.res_output[i - self.cond_level - 1](x)\n                else:\n                    output = self.res_output[i - self.cond_level - 1](output)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i * self.num_kernels + j](x)\n                else:\n                    xs += self.resblocks[i * self.num_kernels + j](x)\n            x = xs / self.num_kernels\n            if output is not None:\n                output = output + x\n\n        x = F.leaky_relu(output)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()\n        for l in self.cond_up:\n            remove_weight_norm(l)\n        for l in self.res_output:\n            remove_weight_norm(l[1])\n        remove_weight_norm(self.conv_pre)\n        remove_weight_norm(self.conv_post)\n\n\n'''\n    to run this, fix \n    from . import ResStack\n    into\n    from res_stack import ResStack\n'''\nif __name__ == '__main__':\n    '''\n    torch.Size([3, 80, 10])\n    torch.Size([3, 1, 2000])\n    4527362\n    '''\n    with open('config.json') as f:\n        data = f.read()\n    from utils import AttrDict\n    import json\n    json_config = json.loads(data)\n    h = AttrDict(json_config)\n    model = FreGAN(h)\n\n    c = torch.randn(3, 80, 10)  # (B, channels, T).\n    print(c.shape)\n\n    y = model(c) # (B, 1, T ** prod(upsample_scales)\n    print(y.shape)\n    assert y.shape == torch.Size([3, 1, 2560])  # For normal melgan torch.Size([3, 1, 2560])\n\n    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(pytorch_total_params)", "models/vocoder/fregan/stft_loss.py": "# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n\"\"\"STFT-based Loss modules.\"\"\"\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef stft(x, fft_size, hop_size, win_length, window):\n    \"\"\"Perform STFT and convert to magnitude spectrogram.\n    Args:\n        x (Tensor): Input signal tensor (B, T).\n        fft_size (int): FFT size.\n        hop_size (int): Hop size.\n        win_length (int): Window length.\n        window (str): Window function type.\n    Returns:\n        Tensor: Magnitude spectrogram (B, #frames, fft_size // 2 + 1).\n    \"\"\"\n    x_stft = torch.stft(x, fft_size, hop_size, win_length, window)\n    real = x_stft[..., 0]\n    imag = x_stft[..., 1]\n\n    # NOTE(kan-bayashi): clamp is needed to avoid nan or inf\n    return torch.sqrt(torch.clamp(real ** 2 + imag ** 2, min=1e-7)).transpose(2, 1)\n\n\nclass SpectralConvergengeLoss(torch.nn.Module):\n    \"\"\"Spectral convergence loss module.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initilize spectral convergence loss module.\"\"\"\n        super(SpectralConvergengeLoss, self).__init__()\n\n    def forward(self, x_mag, y_mag):\n        \"\"\"Calculate forward propagation.\n        Args:\n            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n        Returns:\n            Tensor: Spectral convergence loss value.\n        \"\"\"\n        return torch.norm(y_mag - x_mag, p=\"fro\") / torch.norm(y_mag, p=\"fro\")\n\n\nclass LogSTFTMagnitudeLoss(torch.nn.Module):\n    \"\"\"Log STFT magnitude loss module.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initilize los STFT magnitude loss module.\"\"\"\n        super(LogSTFTMagnitudeLoss, self).__init__()\n\n    def forward(self, x_mag, y_mag):\n        \"\"\"Calculate forward propagation.\n        Args:\n            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n        Returns:\n            Tensor: Log STFT magnitude loss value.\n        \"\"\"\n        return F.l1_loss(torch.log(y_mag), torch.log(x_mag))\n\n\nclass STFTLoss(torch.nn.Module):\n    \"\"\"STFT loss module.\"\"\"\n\n    def __init__(self, fft_size=1024, shift_size=120, win_length=600, window=\"hann_window\"):\n        \"\"\"Initialize STFT loss module.\"\"\"\n        super(STFTLoss, self).__init__()\n        self.fft_size = fft_size\n        self.shift_size = shift_size\n        self.win_length = win_length\n        self.window = getattr(torch, window)(win_length)\n        self.spectral_convergenge_loss = SpectralConvergengeLoss()\n        self.log_stft_magnitude_loss = LogSTFTMagnitudeLoss()\n\n    def forward(self, x, y):\n        \"\"\"Calculate forward propagation.\n        Args:\n            x (Tensor): Predicted signal (B, T).\n            y (Tensor): Groundtruth signal (B, T).\n        Returns:\n            Tensor: Spectral convergence loss value.\n            Tensor: Log STFT magnitude loss value.\n        \"\"\"\n        x_mag = stft(x, self.fft_size, self.shift_size, self.win_length, self.window.to(x.get_device()))\n        y_mag = stft(y, self.fft_size, self.shift_size, self.win_length, self.window.to(x.get_device()))\n        sc_loss = self.spectral_convergenge_loss(x_mag, y_mag)\n        mag_loss = self.log_stft_magnitude_loss(x_mag, y_mag)\n\n        return sc_loss, mag_loss\n\n\nclass MultiResolutionSTFTLoss(torch.nn.Module):\n    \"\"\"Multi resolution STFT loss module.\"\"\"\n\n    def __init__(self,\n                 fft_sizes=[1024, 2048, 512],\n                 hop_sizes=[120, 240, 50],\n                 win_lengths=[600, 1200, 240],\n                 window=\"hann_window\"):\n        \"\"\"Initialize Multi resolution STFT loss module.\n        Args:\n            fft_sizes (list): List of FFT sizes.\n            hop_sizes (list): List of hop sizes.\n            win_lengths (list): List of window lengths.\n            window (str): Window function type.\n        \"\"\"\n        super(MultiResolutionSTFTLoss, self).__init__()\n        assert len(fft_sizes) == len(hop_sizes) == len(win_lengths)\n        self.stft_losses = torch.nn.ModuleList()\n        for fs, ss, wl in zip(fft_sizes, hop_sizes, win_lengths):\n            self.stft_losses += [STFTLoss(fs, ss, wl, window)]\n\n    def forward(self, x, y):\n        \"\"\"Calculate forward propagation.\n        Args:\n            x (Tensor): Predicted signal (B, T).\n            y (Tensor): Groundtruth signal (B, T).\n        Returns:\n            Tensor: Multi resolution spectral convergence loss value.\n            Tensor: Multi resolution log STFT magnitude loss value.\n        \"\"\"\n        sc_loss = 0.0\n        mag_loss = 0.0\n        for f in self.stft_losses:\n            sc_l, mag_l = f(x, y)\n            sc_loss += sc_l\n            mag_loss += mag_l\n        sc_loss /= len(self.stft_losses)\n        mag_loss /= len(self.stft_losses)\n\n        return sc_loss, mag_loss", "models/vocoder/fregan/meldataset.py": "import math\nimport os\nimport random\nimport torch\nimport torch.utils.data\nimport numpy as np\nfrom librosa.util import normalize\nfrom scipy.io.wavfile import read\nfrom utils.audio_utils import mel_spectrogram\n\nMAX_WAV_VALUE = 32768.0\n\n\ndef load_wav(full_path):\n    sampling_rate, data = read(full_path)\n    return data, sampling_rate\n\n\ndef get_dataset_filelist(a):\n    #with open(a.input_training_file, 'r', encoding='utf-8') as fi:\n    #    training_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n    #                      for x in fi.read().split('\\n') if len(x) > 0]\n\n    #with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n    #   validation_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n    #                        for x in fi.read().split('\\n') if len(x) > 0]\n    files = os.listdir(a.input_wavs_dir)\n    random.shuffle(files)\n    files = [os.path.join(a.input_wavs_dir, f) for f in files]\n    training_files = files[: -int(len(files) * 0.05)]\n    validation_files = files[-int(len(files) * 0.05):]\n    return training_files, validation_files\n\n\nclass MelDataset(torch.utils.data.Dataset):\n    def __init__(self, training_files, segment_size, n_fft, num_mels,\n                 hop_size, win_size, sampling_rate,  fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n                 device=None, fmax_loss=None, fine_tuning=False, base_mels_path=None):\n        self.audio_files = training_files\n        random.seed(1234)\n        if shuffle:\n            random.shuffle(self.audio_files)\n        self.segment_size = segment_size\n        self.sampling_rate = sampling_rate\n        self.split = split\n        self.n_fft = n_fft\n        self.num_mels = num_mels\n        self.hop_size = hop_size\n        self.win_size = win_size\n        self.fmin = fmin\n        self.fmax = fmax\n        self.fmax_loss = fmax_loss\n        self.cached_wav = None\n        self.n_cache_reuse = n_cache_reuse\n        self._cache_ref_count = 0\n        self.device = device\n        self.fine_tuning = fine_tuning\n        self.base_mels_path = base_mels_path\n\n    def __getitem__(self, index):\n        filename = self.audio_files[index]\n        if self._cache_ref_count == 0:\n            #audio, sampling_rate = load_wav(filename)\n            #audio = audio / MAX_WAV_VALUE\n            audio = np.load(filename)\n            if not self.fine_tuning:\n                audio = normalize(audio) * 0.95\n            self.cached_wav = audio\n            #if sampling_rate != self.sampling_rate:\n            #    raise ValueError(\"{} SR doesn't match target {} SR\".format(\n            #        sampling_rate, self.sampling_rate))\n            self._cache_ref_count = self.n_cache_reuse\n        else:\n            audio = self.cached_wav\n            self._cache_ref_count -= 1\n\n        audio = torch.FloatTensor(audio)\n        audio = audio.unsqueeze(0)\n\n        if not self.fine_tuning:\n            if self.split:\n                if audio.size(1) >= self.segment_size:\n                    max_audio_start = audio.size(1) - self.segment_size\n                    audio_start = random.randint(0, max_audio_start)\n                    audio = audio[:, audio_start:audio_start+self.segment_size]\n                else:\n                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n\n            mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n                                  center=False)\n        else:\n            mel_path = os.path.join(self.base_mels_path, \"mel\" + \"-\" + filename.split(\"/\")[-1].split(\"-\")[-1])\n            mel = np.load(mel_path).T\n            #mel = np.load(\n            #    os.path.join(self.base_mels_path, os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'))\n            mel = torch.from_numpy(mel)\n\n            if len(mel.shape) < 3:\n                mel = mel.unsqueeze(0)\n\n            if self.split:\n                frames_per_seg = math.ceil(self.segment_size / self.hop_size)\n\n                if audio.size(1) >= self.segment_size:\n                    mel_start = random.randint(0, mel.size(2) - frames_per_seg - 1)\n                    mel = mel[:, :, mel_start:mel_start + frames_per_seg]\n                    audio = audio[:, mel_start * self.hop_size:(mel_start + frames_per_seg) * self.hop_size]\n                else:\n                    mel = torch.nn.functional.pad(mel, (0, frames_per_seg - mel.size(2)), 'constant')\n                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n\n        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n                                   center=False)\n\n        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n\n    def __len__(self):\n        return len(self.audio_files)", "models/vocoder/fregan/discriminator.py": "import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn import Conv1d, AvgPool1d, Conv2d\nfrom torch.nn.utils import weight_norm, spectral_norm\nfrom models.vocoder.fregan.utils import get_padding\nfrom models.vocoder.fregan.stft_loss import stft\nfrom models.vocoder.fregan.dwt import DWT_1D\nLRELU_SLOPE = 0.1\n\n\n\nclass SpecDiscriminator(nn.Module):\n    \"\"\"docstring for Discriminator.\"\"\"\n\n    def __init__(self, fft_size=1024, shift_size=120, win_length=600, window=\"hann_window\", use_spectral_norm=False):\n        super(SpecDiscriminator, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.fft_size = fft_size\n        self.shift_size = shift_size\n        self.win_length = win_length\n        self.window = getattr(torch, window)(win_length)\n        self.discriminators = nn.ModuleList([\n            norm_f(nn.Conv2d(1, 32, kernel_size=(3, 9), padding=(1, 4))),\n            norm_f(nn.Conv2d(32, 32, kernel_size=(3, 9), stride=(1,2), padding=(1, 4))),\n            norm_f(nn.Conv2d(32, 32, kernel_size=(3, 9), stride=(1,2), padding=(1, 4))),\n            norm_f(nn.Conv2d(32, 32, kernel_size=(3, 9), stride=(1,2), padding=(1, 4))),\n            norm_f(nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1,1), padding=(1, 1))),\n        ])\n\n        self.out = norm_f(nn.Conv2d(32, 1, 3, 1, 1))\n\n    def forward(self, y):\n\n        fmap = []\n        with torch.no_grad():\n            y = y.squeeze(1)\n            y = stft(y, self.fft_size, self.shift_size, self.win_length, self.window.to(y.get_device()))\n        y = y.unsqueeze(1)\n        for i, d in enumerate(self.discriminators):\n            y = d(y)\n            y = F.leaky_relu(y, LRELU_SLOPE)\n            fmap.append(y)\n\n        y = self.out(y)\n        fmap.append(y)\n\n        return torch.flatten(y, 1, -1), fmap\n\nclass MultiResSpecDiscriminator(torch.nn.Module):\n\n    def __init__(self,\n                 fft_sizes=[1024, 2048, 512],\n                 hop_sizes=[120, 240, 50],\n                 win_lengths=[600, 1200, 240],\n                 window=\"hann_window\"):\n\n        super(MultiResSpecDiscriminator, self).__init__()\n        self.discriminators = nn.ModuleList([\n            SpecDiscriminator(fft_sizes[0], hop_sizes[0], win_lengths[0], window),\n            SpecDiscriminator(fft_sizes[1], hop_sizes[1], win_lengths[1], window),\n            SpecDiscriminator(fft_sizes[2], hop_sizes[2], win_lengths[2], window)\n            ])\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            fmap_rs.append(fmap_r)\n            y_d_gs.append(y_d_g)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n\n\nclass DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.dwt1d = DWT_1D()\n        self.dwt_conv1 = norm_f(Conv1d(2, 1, 1))\n        self.dwt_proj1 = norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0)))\n        self.dwt_conv2 = norm_f(Conv1d(4, 1, 1))\n        self.dwt_proj2 = norm_f(Conv2d(1, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0)))\n        self.dwt_conv3 = norm_f(Conv1d(8, 1, 1))\n        self.dwt_proj3 = norm_f(Conv2d(1, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0)))\n        self.convs = nn.ModuleList([\n            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(2, 0))),\n        ])\n        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\n    def forward(self, x):\n        fmap = []\n\n        # DWT 1\n        x_d1_high1, x_d1_low1 = self.dwt1d(x)\n        x_d1 = self.dwt_conv1(torch.cat([x_d1_high1, x_d1_low1], dim=1))\n        # 1d to 2d\n        b, c, t = x_d1.shape\n        if t % self.period != 0:  # pad first\n            n_pad = self.period - (t % self.period)\n            x_d1 = F.pad(x_d1, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x_d1 = x_d1.view(b, c, t // self.period, self.period)\n\n        x_d1 = self.dwt_proj1(x_d1)\n\n        # DWT 2\n        x_d2_high1, x_d2_low1 = self.dwt1d(x_d1_high1)\n        x_d2_high2, x_d2_low2 = self.dwt1d(x_d1_low1)\n        x_d2 = self.dwt_conv2(torch.cat([x_d2_high1, x_d2_low1, x_d2_high2, x_d2_low2], dim=1))\n        # 1d to 2d\n        b, c, t = x_d2.shape\n        if t % self.period != 0:  # pad first\n            n_pad = self.period - (t % self.period)\n            x_d2 = F.pad(x_d2, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x_d2 = x_d2.view(b, c, t // self.period, self.period)\n\n        x_d2 = self.dwt_proj2(x_d2)\n\n        # DWT 3\n\n        x_d3_high1, x_d3_low1 = self.dwt1d(x_d2_high1)\n        x_d3_high2, x_d3_low2 = self.dwt1d(x_d2_low1)\n        x_d3_high3, x_d3_low3 = self.dwt1d(x_d2_high2)\n        x_d3_high4, x_d3_low4 = self.dwt1d(x_d2_low2)\n        x_d3 = self.dwt_conv3(\n            torch.cat([x_d3_high1, x_d3_low1, x_d3_high2, x_d3_low2, x_d3_high3, x_d3_low3, x_d3_high4, x_d3_low4],\n                      dim=1))\n        # 1d to 2d\n        b, c, t = x_d3.shape\n        if t % self.period != 0:  # pad first\n            n_pad = self.period - (t % self.period)\n            x_d3 = F.pad(x_d3, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x_d3 = x_d3.view(b, c, t // self.period, self.period)\n\n        x_d3 = self.dwt_proj3(x_d3)\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0:  # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n        i = 0\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n\n            fmap.append(x)\n            if i == 0:\n                x = torch.cat([x, x_d1], dim=2)\n            elif i == 1:\n                x = torch.cat([x, x_d2], dim=2)\n            elif i == 2:\n                x = torch.cat([x, x_d3], dim=2)\n            else:\n                x = x\n            i = i + 1\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass ResWiseMultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self):\n        super(ResWiseMultiPeriodDiscriminator, self).__init__()\n        self.discriminators = nn.ModuleList([\n            DiscriminatorP(2),\n            DiscriminatorP(3),\n            DiscriminatorP(5),\n            DiscriminatorP(7),\n            DiscriminatorP(11),\n        ])\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            fmap_rs.append(fmap_r)\n            y_d_gs.append(y_d_g)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n\n\nclass DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.dwt1d = DWT_1D()\n        self.dwt_conv1 = norm_f(Conv1d(2, 128, 15, 1, padding=7))\n        self.dwt_conv2 = norm_f(Conv1d(4, 128, 41, 2, padding=20))\n        self.convs = nn.ModuleList([\n            norm_f(Conv1d(1, 128, 15, 1, padding=7)),\n            norm_f(Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n            norm_f(Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n            norm_f(Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n            norm_f(Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n            norm_f(Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n        ])\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        fmap = []\n\n        # DWT 1\n        x_d1_high1, x_d1_low1 = self.dwt1d(x)\n        x_d1 = self.dwt_conv1(torch.cat([x_d1_high1, x_d1_low1], dim=1))\n\n        # DWT 2\n        x_d2_high1, x_d2_low1 = self.dwt1d(x_d1_high1)\n        x_d2_high2, x_d2_low2 = self.dwt1d(x_d1_low1)\n        x_d2 = self.dwt_conv2(torch.cat([x_d2_high1, x_d2_low1, x_d2_high2, x_d2_low2], dim=1))\n\n        i = 0\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            fmap.append(x)\n            if i == 0:\n                x = torch.cat([x, x_d1], dim=2)\n            if i == 1:\n                x = torch.cat([x, x_d2], dim=2)\n            i = i + 1\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass ResWiseMultiScaleDiscriminator(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(ResWiseMultiScaleDiscriminator, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.dwt1d = DWT_1D()\n        self.dwt_conv1 = norm_f(Conv1d(2, 1, 1))\n        self.dwt_conv2 = norm_f(Conv1d(4, 1, 1))\n        self.discriminators = nn.ModuleList([\n            DiscriminatorS(use_spectral_norm=True),\n            DiscriminatorS(),\n            DiscriminatorS(),\n        ])\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        # DWT 1\n        y_hi, y_lo = self.dwt1d(y)\n        y_1 = self.dwt_conv1(torch.cat([y_hi, y_lo], dim=1))\n        x_d1_high1, x_d1_low1 = self.dwt1d(y_hat)\n        y_hat_1 = self.dwt_conv1(torch.cat([x_d1_high1, x_d1_low1], dim=1))\n\n        # DWT 2\n        x_d2_high1, x_d2_low1 = self.dwt1d(y_hi)\n        x_d2_high2, x_d2_low2 = self.dwt1d(y_lo)\n        y_2 = self.dwt_conv2(torch.cat([x_d2_high1, x_d2_low1, x_d2_high2, x_d2_low2], dim=1))\n\n        x_d2_high1, x_d2_low1 = self.dwt1d(x_d1_high1)\n        x_d2_high2, x_d2_low2 = self.dwt1d(x_d1_low1)\n        y_hat_2 = self.dwt_conv2(torch.cat([x_d2_high1, x_d2_low1, x_d2_high2, x_d2_low2], dim=1))\n\n        for i, d in enumerate(self.discriminators):\n\n            if i == 1:\n                y = y_1\n                y_hat = y_hat_1\n            if i == 2:\n                y = y_2\n                y_hat = y_hat_2\n\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            fmap_rs.append(fmap_r)\n            y_d_gs.append(y_d_g)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs", "models/vocoder/fregan/__init__.py": "#", "models/vocoder/fregan/inference.py": "from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport json\nimport torch\nfrom utils.util import AttrDict\nfrom models.vocoder.fregan.generator import FreGAN\n\ngenerator = None       # type: FreGAN\noutput_sample_rate = None\n_device = None\n\n\ndef load_checkpoint(filepath, device):\n    assert os.path.isfile(filepath)\n    print(\"Loading '{}'\".format(filepath))\n    checkpoint_dict = torch.load(filepath, map_location=device)\n    print(\"Complete.\")\n    return checkpoint_dict\n\n\ndef load_model(weights_fpath, config_fpath=None, verbose=True):\n    global generator, _device, output_sample_rate\n\n    if verbose:\n        print(\"Building fregan\")\n\n    if config_fpath == None:\n        model_config_fpaths = list(weights_fpath.parent.rglob(\"*.json\"))\n        if len(model_config_fpaths) > 0:\n            config_fpath = model_config_fpaths[0]\n        else:\n            config_fpath = \"./vocoder/fregan/config.json\"\n    with open(config_fpath) as f:\n        data = f.read()\n    json_config = json.loads(data)\n    h = AttrDict(json_config)\n    output_sample_rate = h.sampling_rate\n    torch.manual_seed(h.seed)\n\n    if torch.cuda.is_available():\n        # _model = _model.cuda()\n        _device = torch.device('cuda')\n    else:\n        _device = torch.device('cpu')\n\n    generator = FreGAN(h).to(_device)\n    state_dict_g = load_checkpoint(\n        weights_fpath, _device\n    )\n    generator.load_state_dict(state_dict_g['generator'])\n    generator.eval()\n    generator.remove_weight_norm()\n\n\ndef is_loaded():\n    return generator is not None\n\n\ndef infer_waveform(mel, progress_callback=None):\n\n    if generator is None:\n        raise Exception(\"Please load fre-gan in memory before using it\")\n\n    mel = torch.FloatTensor(mel).to(_device)\n    mel = mel.unsqueeze(0)\n\n    with torch.no_grad():\n        y_g_hat = generator(mel)\n        audio = y_g_hat.squeeze()\n    audio = audio.cpu().numpy()\n\n    return audio, output_sample_rate\n\n", "models/vocoder/hifigan/train.py": "import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport itertools\nimport os\nimport time\nimport argparse\nimport json\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DistributedSampler, DataLoader\nimport torch.multiprocessing as mp\nfrom torch.distributed import init_process_group\nfrom torch.nn.parallel import DistributedDataParallel\nfrom models.vocoder.hifigan.meldataset import MelDataset, mel_spectrogram, get_dataset_filelist\nfrom models.vocoder.hifigan.models import Generator, MultiPeriodDiscriminator, MultiScaleDiscriminator\nfrom utils.loss import feature_loss, generator_loss, discriminator_loss\n\nfrom models.vocoder.hifigan.utils import plot_spectrogram, scan_checkpoint, load_checkpoint, save_checkpoint\n\ntorch.backends.cudnn.benchmark = True\n\n\ndef train(rank, a, h):\n\n    a.checkpoint_path = a.models_dir.joinpath(a.run_id+'_hifigan')      \n    a.checkpoint_path.mkdir(exist_ok=True)\n    a.training_epochs = 3100\n    a.stdout_interval = 5\n    a.checkpoint_interval = a.backup_every\n    a.summary_interval = 5000\n    a.validation_interval = 1000\n    a.fine_tuning = True\n\n    a.input_wavs_dir = a.syn_dir.joinpath(\"audio\")\n    a.input_mels_dir = a.syn_dir.joinpath(\"mels\")\n\n    if h.num_gpus > 1:\n        init_process_group(backend=h.dist_config['dist_backend'], init_method=h.dist_config['dist_url'],\n                           world_size=h.dist_config['world_size'] * h.num_gpus, rank=rank)\n\n    torch.cuda.manual_seed(h.seed)\n    device = torch.device('cuda:{:d}'.format(rank))\n\n    generator = Generator(h).to(device)\n    mpd = MultiPeriodDiscriminator().to(device)\n    msd = MultiScaleDiscriminator().to(device)\n\n    if rank == 0:\n        print(generator)\n        os.makedirs(a.checkpoint_path, exist_ok=True)\n        print(\"checkpoints directory : \", a.checkpoint_path)\n\n    if os.path.isdir(a.checkpoint_path):\n        cp_g = scan_checkpoint(a.checkpoint_path, 'g_hifigan_')\n        cp_do = scan_checkpoint(a.checkpoint_path, 'do_hifigan_')\n\n    steps = 0\n    if cp_g is None or cp_do is None:\n        state_dict_do = None\n        last_epoch = -1\n    else:\n        state_dict_g = load_checkpoint(cp_g, device)\n        state_dict_do = load_checkpoint(cp_do, device)\n        generator.load_state_dict(state_dict_g['generator'])\n        mpd.load_state_dict(state_dict_do['mpd'])\n        msd.load_state_dict(state_dict_do['msd'])\n        steps = state_dict_do['steps'] + 1\n        last_epoch = state_dict_do['epoch']\n\n    if h.num_gpus > 1:\n        generator = DistributedDataParallel(generator, device_ids=[rank]).to(device)\n        mpd = DistributedDataParallel(mpd, device_ids=[rank]).to(device)\n        msd = DistributedDataParallel(msd, device_ids=[rank]).to(device)\n\n    optim_g = torch.optim.AdamW(generator.parameters(), h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n    optim_d = torch.optim.AdamW(itertools.chain(msd.parameters(), mpd.parameters()),\n                                h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n\n    if state_dict_do is not None:\n        optim_g.load_state_dict(state_dict_do['optim_g'])\n        optim_d.load_state_dict(state_dict_do['optim_d'])\n\n    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=h.lr_decay, last_epoch=last_epoch)\n    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=h.lr_decay, last_epoch=last_epoch)\n\n    training_filelist, validation_filelist = get_dataset_filelist(a)\n\n    # print(training_filelist)\n    # exit()\n\n    trainset = MelDataset(training_filelist, h.segment_size, h.n_fft, h.num_mels,\n                          h.hop_size, h.win_size, h.sampling_rate, h.fmin, h.fmax, n_cache_reuse=0,\n                          shuffle=False if h.num_gpus > 1 else True, fmax_loss=h.fmax_for_loss, device=device,\n                          fine_tuning=a.fine_tuning, base_mels_path=a.input_mels_dir)\n\n    train_sampler = DistributedSampler(trainset) if h.num_gpus > 1 else None\n\n    train_loader = DataLoader(trainset, num_workers=h.num_workers, shuffle=False,\n                              sampler=train_sampler,\n                              batch_size=h.batch_size,\n                              pin_memory=True,\n                              drop_last=True)\n\n    if rank == 0:\n        validset = MelDataset(validation_filelist, h.segment_size, h.n_fft, h.num_mels,\n                              h.hop_size, h.win_size, h.sampling_rate, h.fmin, h.fmax, False, False, n_cache_reuse=0,\n                              fmax_loss=h.fmax_for_loss, device=device, fine_tuning=a.fine_tuning,\n                              base_mels_path=a.input_mels_dir)\n        validation_loader = DataLoader(validset, num_workers=1, shuffle=False,\n                                       sampler=None,\n                                       batch_size=1,\n                                       pin_memory=True,\n                                       drop_last=True)\n\n        sw = SummaryWriter(os.path.join(a.checkpoint_path, 'logs'))\n\n    generator.train()\n    mpd.train()\n    msd.train()\n    for epoch in range(max(0, last_epoch), a.training_epochs):\n        if rank == 0:\n            start = time.time()\n            print(\"Epoch: {}\".format(epoch+1))\n\n        if h.num_gpus > 1:\n            train_sampler.set_epoch(epoch)\n\n        for i, batch in enumerate(train_loader):\n            if rank == 0:\n                start_b = time.time()\n            x, y, _, y_mel = batch\n            x = torch.autograd.Variable(x.to(device, non_blocking=True))\n            y = torch.autograd.Variable(y.to(device, non_blocking=True))\n            y_mel = torch.autograd.Variable(y_mel.to(device, non_blocking=True))\n            y = y.unsqueeze(1)\n\n            y_g_hat = generator(x)\n            y_g_hat_mel = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels, h.sampling_rate, h.hop_size, h.win_size,\n                                          h.fmin, h.fmax_for_loss)\n            if steps > h.disc_start_step:\n                optim_d.zero_grad()\n\n                # MPD\n                y_df_hat_r, y_df_hat_g, _, _ = mpd(y, y_g_hat.detach())\n                loss_disc_f, losses_disc_f_r, losses_disc_f_g = discriminator_loss(y_df_hat_r, y_df_hat_g)\n\n                # MSD\n                y_ds_hat_r, y_ds_hat_g, _, _ = msd(y, y_g_hat.detach())\n                loss_disc_s, losses_disc_s_r, losses_disc_s_g = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n\n                loss_disc_all = loss_disc_s + loss_disc_f\n\n                loss_disc_all.backward()\n                optim_d.step()\n\n            # Generator\n            optim_g.zero_grad()\n\n            # L1 Mel-Spectrogram Loss\n            loss_mel = F.l1_loss(y_mel, y_g_hat_mel) * 45\n\n            if steps > h.disc_start_step:\n                y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g = mpd(y, y_g_hat)\n                y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = msd(y, y_g_hat)\n                loss_fm_f = feature_loss(fmap_f_r, fmap_f_g)\n                loss_fm_s = feature_loss(fmap_s_r, fmap_s_g)\n                loss_gen_f, losses_gen_f = generator_loss(y_df_hat_g)\n                loss_gen_s, losses_gen_s = generator_loss(y_ds_hat_g)\n                loss_gen_all = loss_gen_s + loss_gen_f + loss_fm_s + loss_fm_f + loss_mel\n            else:\n                loss_gen_all = loss_mel\n\n            loss_gen_all.backward()\n            optim_g.step()\n\n            if rank == 0:\n                # STDOUT logging\n                if steps % a.stdout_interval == 0:\n                    with torch.no_grad():\n                        mel_error = F.l1_loss(y_mel, y_g_hat_mel).item()\n\n                    print('Steps : {:d}, Gen Loss Total : {:4.3f}, Mel-Spec. Error : {:4.3f}, s/b : {:4.3f}'.\n                          format(steps, loss_gen_all, mel_error, time.time() - start_b))\n\n                # checkpointing\n                if steps % a.checkpoint_interval == 0 and steps != 0:\n                    checkpoint_path = \"{}/g_hifigan_{:08d}.pt\".format(a.checkpoint_path, steps)\n                    save_checkpoint(checkpoint_path,\n                                    {'generator': (generator.module if h.num_gpus > 1 else generator).state_dict()})\n                    checkpoint_path = \"{}/do_hifigan_{:08d}.pt\".format(a.checkpoint_path, steps)\n                    save_checkpoint(checkpoint_path,\n                                    {'mpd': (mpd.module if h.num_gpus > 1 else mpd).state_dict(),\n                                     'msd': (msd.module if h.num_gpus > 1 else msd).state_dict(),\n                                     'optim_g': optim_g.state_dict(), 'optim_d': optim_d.state_dict(), 'steps': steps,\n                                     'epoch': epoch})\n\n                # Tensorboard summary logging\n                if steps % a.summary_interval == 0:\n                    sw.add_scalar(\"training/gen_loss_total\", loss_gen_all, steps)\n                    sw.add_scalar(\"training/mel_spec_error\", mel_error, steps)\n                \n\n                # save temperate hifigan model\n                if steps % a.save_every == 0:\n                    checkpoint_path = \"{}/g_hifigan.pt\".format(a.checkpoint_path)\n                    save_checkpoint(checkpoint_path,\n                                    {'generator': (generator.module if h.num_gpus > 1 else generator).state_dict()})\n                    checkpoint_path = \"{}/do_hifigan.pt\".format(a.checkpoint_path)\n                    save_checkpoint(checkpoint_path,\n                                    {'mpd': (mpd.module if h.num_gpus > 1 else mpd).state_dict(),\n                                     'msd': (msd.module if h.num_gpus > 1 else msd).state_dict(),\n                                     'optim_g': optim_g.state_dict(), 'optim_d': optim_d.state_dict(), 'steps': steps,\n                                     'epoch': epoch})\n\n                # Validation\n                if steps % a.validation_interval == 0:  # and steps != 0:\n                    generator.eval()\n                    torch.cuda.empty_cache()\n                    val_err_tot = 0\n                    with torch.no_grad():\n                        for j, batch in enumerate(validation_loader):\n                            x, y, _, y_mel = batch\n                            y_g_hat = generator(x.to(device))\n                            y_mel = torch.autograd.Variable(y_mel.to(device, non_blocking=True))\n                            y_g_hat_mel = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels, h.sampling_rate,\n                                                          h.hop_size, h.win_size,\n                                                          h.fmin, h.fmax_for_loss)\n#                             val_err_tot += F.l1_loss(y_mel, y_g_hat_mel).item()\n\n                            if j <= 4:\n                                if steps == 0:\n                                    sw.add_audio('gt/y_{}'.format(j), y[0], steps, h.sampling_rate)\n                                    sw.add_figure('gt/y_spec_{}'.format(j), plot_spectrogram(x[0]), steps)\n\n                                sw.add_audio('generated/y_hat_{}'.format(j), y_g_hat[0], steps, h.sampling_rate)\n                                y_hat_spec = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels,\n                                                             h.sampling_rate, h.hop_size, h.win_size,\n                                                             h.fmin, h.fmax)\n                                sw.add_figure('generated/y_hat_spec_{}'.format(j),\n                                              plot_spectrogram(y_hat_spec.squeeze(0).cpu().numpy()), steps)\n\n                        val_err = val_err_tot / (j+1)\n                        sw.add_scalar(\"validation/mel_spec_error\", val_err, steps)\n\n                    generator.train()\n\n            steps += 1\n\n        scheduler_g.step()\n        scheduler_d.step()\n        \n        if rank == 0:\n            print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, int(time.time() - start)))\n", "models/vocoder/hifigan/utils.py": "import glob\nimport os\nimport matplotlib\nimport torch\nfrom torch.nn.utils import weight_norm\nmatplotlib.use(\"Agg\")\nimport matplotlib.pylab as plt\n\ndef plot_spectrogram(spectrogram):\n    fig, ax = plt.subplots(figsize=(10, 2))\n    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n                   interpolation='none')\n    plt.colorbar(im, ax=ax)\n\n    fig.canvas.draw()\n    plt.close()\n\n    return fig\n\n\ndef apply_weight_norm(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        weight_norm(m)\n\n\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size*dilation - dilation)/2)\n\n\ndef load_checkpoint(filepath, device):\n    assert os.path.isfile(filepath)\n    print(\"Loading '{}'\".format(filepath))\n    checkpoint_dict = torch.load(filepath, map_location=device)\n    print(\"Complete.\")\n    return checkpoint_dict\n\n\ndef save_checkpoint(filepath, obj):\n    print(\"Saving checkpoint to {}\".format(filepath))\n    torch.save(obj, filepath)\n    print(\"Complete.\")\n\n\ndef scan_checkpoint(cp_dir, prefix):\n    pattern = os.path.join(cp_dir, prefix + '????????.pt')\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n", "models/vocoder/hifigan/models.py": "import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\nfrom torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\nfrom utils.util import init_weights, get_padding\n\nLRELU_SLOPE = 0.1\n\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock1, self).__init__()\n        self.h = h\n        self.convs1 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n                               padding=get_padding(kernel_size, dilation[2])))\n        ])\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1)))\n        ])\n        self.convs2.apply(init_weights)\n\n    def forward(self, x):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            xt = c2(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)\n\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.h = h\n        self.convs = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1])))\n        ])\n        self.convs.apply(init_weights)\n\n    def forward(self, x):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)\n\nclass InterpolationBlock(torch.nn.Module):\n    def __init__(self, scale_factor, mode='nearest', align_corners=None, downsample=False):\n        super(InterpolationBlock, self).__init__()\n        self.downsample = downsample\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n    \n    def forward(self, x):\n        outputs = torch.nn.functional.interpolate(\n            x,\n            size=x.shape[-1] * self.scale_factor \\\n                if not self.downsample else x.shape[-1] // self.scale_factor,\n            mode=self.mode,\n            align_corners=self.align_corners,\n            recompute_scale_factor=False\n        )\n        return outputs\n\nclass Generator(torch.nn.Module):\n    def __init__(self, h):\n        super(Generator, self).__init__()\n        self.h = h\n        self.num_kernels = len(h.resblock_kernel_sizes)\n        self.num_upsamples = len(h.upsample_rates)\n        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n\n        self.ups = nn.ModuleList()\n#         for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n# #             self.ups.append(weight_norm(\n# #                 ConvTranspose1d(h.upsample_initial_channel//(2**i), h.upsample_initial_channel//(2**(i+1)),\n# #                                 k, u, padding=(k-u)//2)))\n        if h.sampling_rate == 24000:\n            for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n                self.ups.append(\n                    torch.nn.Sequential(\n                        InterpolationBlock(u),\n                        weight_norm(torch.nn.Conv1d(\n                            h.upsample_initial_channel//(2**i),\n                            h.upsample_initial_channel//(2**(i+1)),\n                            k, padding=(k-1)//2,\n                        ))\n                    )\n                )\n        else:\n            for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n                self.ups.append(weight_norm(ConvTranspose1d(h.upsample_initial_channel//(2**i), \n                    h.upsample_initial_channel//(2**(i+1)),\n                    k, u, padding=(u//2 + u%2), output_padding=u%2)))\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = h.upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n                self.resblocks.append(resblock(h, ch, k, d))\n\n        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n        self.ups.apply(init_weights)\n        self.conv_post.apply(init_weights)\n\n    def forward(self, x):\n        x = self.conv_pre(x)\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            if self.h.sampling_rate == 24000:\n                remove_weight_norm(l[-1])\n            else:\n                remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()\n        remove_weight_norm(self.conv_pre)\n        remove_weight_norm(self.conv_post)\n\n\nclass DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(2, 0))),\n        ])\n        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\n    def forward(self, x):\n        fmap = []\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0: # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self):\n        super(MultiPeriodDiscriminator, self).__init__()\n        self.discriminators = nn.ModuleList([\n            DiscriminatorP(2),\n            DiscriminatorP(3),\n            DiscriminatorP(5),\n            DiscriminatorP(7),\n            DiscriminatorP(11),\n        ])\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            fmap_rs.append(fmap_r)\n            y_d_gs.append(y_d_g)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n\n\nclass DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv1d(1, 128, 15, 1, padding=7)),\n            norm_f(Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n            norm_f(Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n            norm_f(Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n            norm_f(Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n            norm_f(Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n        ])\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        fmap = []\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass MultiScaleDiscriminator(torch.nn.Module):\n    def __init__(self):\n        super(MultiScaleDiscriminator, self).__init__()\n        self.discriminators = nn.ModuleList([\n            DiscriminatorS(use_spectral_norm=True),\n            DiscriminatorS(),\n            DiscriminatorS(),\n        ])\n        self.meanpools = nn.ModuleList([\n            AvgPool1d(4, 2, padding=2),\n            AvgPool1d(4, 2, padding=2)\n        ])\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            if i != 0:\n                y = self.meanpools[i-1](y)\n                y_hat = self.meanpools[i-1](y_hat)\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            fmap_rs.append(fmap_r)\n            y_d_gs.append(y_d_g)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n", "models/vocoder/hifigan/meldataset.py": "import math\nimport os\nimport random\nimport torch\nimport torch.utils.data\nimport numpy as np\nfrom librosa.util import normalize\nfrom scipy.io.wavfile import read\nfrom utils.audio_utils import mel_spectrogram\n\nMAX_WAV_VALUE = 32768.0\n\n\ndef load_wav(full_path):\n    sampling_rate, data = read(full_path)\n    return data, sampling_rate\n\n\ndef dynamic_range_compression(x, C=1, clip_val=1e-5):\n    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n\n\ndef dynamic_range_decompression(x, C=1):\n    return np.exp(x) / C\n\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef dynamic_range_decompression_torch(x, C=1):\n    return torch.exp(x) / C\n\n\ndef get_dataset_filelist(a):\n    # with open(a.input_training_file, 'r', encoding='utf-8') as fi:\n    #     training_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n    #                       for x in fi.read().split('\\n') if len(x) > 0]\n\n    # with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n    #     validation_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n    #                         for x in fi.read().split('\\n') if len(x) > 0]\n\n    files = os.listdir(a.input_wavs_dir)\n    random.shuffle(files)\n    files = [os.path.join(a.input_wavs_dir, f) for f in files]\n    training_files = files[: -int(len(files)*0.05)]\n    validation_files = files[-int(len(files)*0.05): ]\n\n    return training_files, validation_files\n\n\nclass MelDataset(torch.utils.data.Dataset):\n    def __init__(self, training_files, segment_size, n_fft, num_mels,\n                 hop_size, win_size, sampling_rate,  fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n                 device=None, fmax_loss=None, fine_tuning=False, base_mels_path=None):\n        self.audio_files = training_files\n        random.seed(1234)\n        if shuffle:\n            random.shuffle(self.audio_files)\n        self.segment_size = segment_size\n        self.sampling_rate = sampling_rate\n        self.split = split\n        self.n_fft = n_fft\n        self.num_mels = num_mels\n        self.hop_size = hop_size\n        self.win_size = win_size\n        self.fmin = fmin\n        self.fmax = fmax\n        self.fmax_loss = fmax_loss\n        self.cached_wav = None\n        self.n_cache_reuse = n_cache_reuse\n        self._cache_ref_count = 0\n        self.device = device\n        self.fine_tuning = fine_tuning\n        self.base_mels_path = base_mels_path\n\n    def __getitem__(self, index):\n        filename = self.audio_files[index]\n        if self._cache_ref_count == 0:\n            # audio, sampling_rate = load_wav(filename)\n            # audio = audio / MAX_WAV_VALUE\n            audio = np.load(filename)\n            if not self.fine_tuning:\n                audio = normalize(audio) * 0.95\n            self.cached_wav = audio\n            # if sampling_rate != self.sampling_rate:\n            #     raise ValueError(\"{} SR doesn't match target {} SR\".format(\n            #         sampling_rate, self.sampling_rate))\n            self._cache_ref_count = self.n_cache_reuse\n        else:\n            audio = self.cached_wav\n            self._cache_ref_count -= 1\n\n        audio = torch.FloatTensor(audio)\n        audio = audio.unsqueeze(0)\n\n        if not self.fine_tuning:\n            if self.split:\n                if audio.size(1) >= self.segment_size:\n                    max_audio_start = audio.size(1) - self.segment_size\n                    audio_start = random.randint(0, max_audio_start)\n                    audio = audio[:, audio_start:audio_start+self.segment_size]\n                else:\n                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n\n            mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n                                  center=False)\n        else:\n            mel_path = os.path.join(self.base_mels_path, \"mel\" + \"-\" + filename.split(\"/\")[-1].split(\"-\")[-1])\n            mel = np.load(mel_path).T\n            # mel = np.load(\n            #     os.path.join(self.base_mels_path, os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'))\n            mel = torch.from_numpy(mel)\n\n            if len(mel.shape) < 3:\n                mel = mel.unsqueeze(0)\n\n            if self.split:\n                frames_per_seg = math.ceil(self.segment_size / self.hop_size)\n\n                if audio.size(1) >= self.segment_size:\n                    mel_start = random.randint(0, mel.size(2) - frames_per_seg - 1)\n                    mel = mel[:, :, mel_start:mel_start + frames_per_seg]\n                    audio = audio[:, mel_start * self.hop_size:(mel_start + frames_per_seg) * self.hop_size]\n                else:\n                    mel = torch.nn.functional.pad(mel, (0, frames_per_seg - mel.size(2)), 'constant')\n                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n\n        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n                                   center=False)\n\n        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n\n    def __len__(self):\n        return len(self.audio_files)\n", "models/vocoder/hifigan/env.py": "import os\nimport shutil\n\ndef build_env(config, config_name, path):\n    t_path = os.path.join(path, config_name)\n    if config != t_path:\n        os.makedirs(path, exist_ok=True)\n        shutil.copyfile(config, os.path.join(path, config_name))\n", "models/vocoder/hifigan/__init__.py": "#", "models/vocoder/hifigan/inference.py": "from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport json\nimport torch\nfrom utils.util import AttrDict\nfrom models.vocoder.hifigan.models import Generator\n\ngenerator = None       # type: Generator\noutput_sample_rate = None     \n_device = None\n\n\ndef load_checkpoint(filepath, device):\n    assert os.path.isfile(filepath)\n    print(\"Loading '{}'\".format(filepath))\n    checkpoint_dict = torch.load(filepath, map_location=device)\n    print(\"Complete.\")\n    return checkpoint_dict\n\n\ndef load_model(weights_fpath, config_fpath=None, verbose=True):\n    global generator, _device, output_sample_rate\n\n    if verbose:\n        print(\"Building hifigan\")\n\n    if config_fpath == None:\n        model_config_fpaths = list(weights_fpath.parent.rglob(\"*.json\"))\n        if len(model_config_fpaths) > 0:\n            config_fpath = model_config_fpaths[0]\n        else:\n            config_fpath = \"./vocoder/hifigan/config_16k_.json\"\n    with open(config_fpath) as f:\n        data = f.read()\n    json_config = json.loads(data)\n    h = AttrDict(json_config)\n    output_sample_rate = h.sampling_rate\n    torch.manual_seed(h.seed)\n\n    if torch.cuda.is_available():\n        # _model = _model.cuda()\n        _device = torch.device('cuda')\n    else:\n        _device = torch.device('cpu')\n\n    generator = Generator(h).to(_device)\n    state_dict_g = load_checkpoint(\n        weights_fpath, _device\n    )\n    generator.load_state_dict(state_dict_g['generator'])\n    generator.eval()\n    generator.remove_weight_norm()\n\n\ndef is_loaded():\n    return generator is not None\n\n\ndef infer_waveform(mel, progress_callback=None):\n\n    if generator is None:\n        raise Exception(\"Please load hifi-gan in memory before using it\")\n\n    mel = torch.FloatTensor(mel).to(_device)\n    mel = mel.unsqueeze(0)\n\n    with torch.no_grad():\n        y_g_hat = generator(mel)\n        audio = y_g_hat.squeeze()\n    audio = audio.cpu().numpy()\n\n    return audio, output_sample_rate\n\n", "models/vocoder/wavernn/train.py": "from models.vocoder.wavernn.models.fatchord_version import WaveRNN\nfrom models.vocoder.vocoder_dataset import VocoderDataset, collate_vocoder\nfrom models.vocoder.distribution import discretized_mix_logistic_loss\nfrom models.vocoder.display import stream, simple_table\nfrom models.vocoder.wavernn.gen_wavernn import gen_testset\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom torch import optim\nimport torch.nn.functional as F\nimport models.vocoder.wavernn.hparams as hp\nimport numpy as np\nimport time\nimport torch\n\n\ndef train(run_id: str, syn_dir: Path, voc_dir: Path, models_dir: Path, ground_truth: bool,\n          save_every: int, backup_every: int, force_restart: bool):\n    # Check to make sure the hop length is correctly factorised\n    assert np.cumprod(hp.voc_upsample_factors)[-1] == hp.hop_length\n    \n    # Instantiate the model\n    print(\"Initializing the model...\")\n    model = WaveRNN(\n        rnn_dims=hp.voc_rnn_dims,\n        fc_dims=hp.voc_fc_dims,\n        bits=hp.bits,\n        pad=hp.voc_pad,\n        upsample_factors=hp.voc_upsample_factors,\n        feat_dims=hp.num_mels,\n        compute_dims=hp.voc_compute_dims,\n        res_out_dims=hp.voc_res_out_dims,\n        res_blocks=hp.voc_res_blocks,\n        hop_length=hp.hop_length,\n        sample_rate=hp.sample_rate,\n        mode=hp.voc_mode\n    )\n\n    if torch.cuda.is_available():\n        model = model.cuda()\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')   \n\n    # Initialize the optimizer\n    optimizer = optim.Adam(model.parameters())\n    for p in optimizer.param_groups: \n        p[\"lr\"] = hp.voc_lr\n    loss_func = F.cross_entropy if model.mode == \"RAW\" else discretized_mix_logistic_loss\n\n    # Load the weights\n    model_dir = models_dir.joinpath(run_id)\n    model_dir.mkdir(exist_ok=True)\n    weights_fpath = model_dir.joinpath(run_id + \".pt\")\n    if force_restart or not weights_fpath.exists():\n        print(\"\\nStarting the training of WaveRNN from scratch\\n\")\n        model.save(weights_fpath, optimizer)\n    else:\n        print(\"\\nLoading weights at %s\" % weights_fpath)\n        model.load(weights_fpath, optimizer)\n        print(\"WaveRNN weights loaded from step %d\" % model.step)\n    \n    # Initialize the dataset\n    metadata_fpath = syn_dir.joinpath(\"train.txt\") if ground_truth else \\\n        voc_dir.joinpath(\"synthesized.txt\")\n    mel_dir = syn_dir.joinpath(\"mels\") if ground_truth else voc_dir.joinpath(\"mels_gta\")\n    wav_dir = syn_dir.joinpath(\"audio\")\n    dataset = VocoderDataset(metadata_fpath, mel_dir, wav_dir)\n    test_loader = DataLoader(dataset,\n                             batch_size=1,\n                             shuffle=True,\n                             pin_memory=True)\n\n    # Begin the training\n    simple_table([('Batch size', hp.voc_batch_size),\n                  ('LR', hp.voc_lr),\n                  ('Sequence Len', hp.voc_seq_len)])\n    \n    for epoch in range(1, 350):\n        data_loader = DataLoader(dataset,\n                                 collate_fn=collate_vocoder,\n                                 batch_size=hp.voc_batch_size,\n                                 num_workers=2,\n                                 shuffle=True,\n                                 pin_memory=True)\n        start = time.time()\n        running_loss = 0.\n\n        for i, (x, y, m) in enumerate(data_loader, 1):\n            if torch.cuda.is_available():\n                x, m, y = x.cuda(), m.cuda(), y.cuda()\n            \n            # Forward pass\n            y_hat = model(x, m)\n            if model.mode == 'RAW':\n                y_hat = y_hat.transpose(1, 2).unsqueeze(-1)\n            elif model.mode == 'MOL':\n                y = y.float()\n            y = y.unsqueeze(-1)\n            \n            # Backward pass\n            loss = loss_func(y_hat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            speed = i / (time.time() - start)\n            avg_loss = running_loss / i\n\n            step = model.get_step()\n            k = step // 1000\n\n            if backup_every != 0 and step % backup_every == 0 :\n                model.checkpoint(model_dir, optimizer)\n                \n            if save_every != 0 and step % save_every == 0 :\n                model.save(weights_fpath, optimizer)\n\n            msg = f\"| Epoch: {epoch} ({i}/{len(data_loader)}) | \" \\\n                f\"Loss: {avg_loss:.4f} | {speed:.1f} \" \\\n                f\"steps/s | Step: {k}k | \"\n            stream(msg)\n\n\n        gen_testset(model, test_loader, hp.voc_gen_at_checkpoint, hp.voc_gen_batched,\n                    hp.voc_target, hp.voc_overlap, model_dir)\n        print(\"\")\n", "models/vocoder/wavernn/audio.py": "import math\nimport numpy as np\nimport librosa\nimport models.vocoder.wavernn.hparams as hp\nfrom scipy.signal import lfilter\nimport soundfile as sf\n\n\ndef label_2_float(x, bits) :\n    return 2 * x / (2**bits - 1.) - 1.\n\n\ndef float_2_label(x, bits) :\n    assert abs(x).max() <= 1.0\n    x = (x + 1.) * (2**bits - 1) / 2\n    return x.clip(0, 2**bits - 1)\n\n\ndef load_wav(path) :\n    return librosa.load(str(path), sr=hp.sample_rate)[0]\n\n\ndef save_wav(x, path) :\n    sf.write(path, x.astype(np.float32), hp.sample_rate)\n\n\ndef split_signal(x) :\n    unsigned = x + 2**15\n    coarse = unsigned // 256\n    fine = unsigned % 256\n    return coarse, fine\n\n\ndef combine_signal(coarse, fine) :\n    return coarse * 256 + fine - 2**15\n\n\ndef encode_16bits(x) :\n    return np.clip(x * 2**15, -2**15, 2**15 - 1).astype(np.int16)\n\n\nmel_basis = None\n\n\ndef linear_to_mel(spectrogram):\n    global mel_basis\n    if mel_basis is None:\n        mel_basis = build_mel_basis()\n    return np.dot(mel_basis, spectrogram)\n\n\ndef build_mel_basis():\n    return librosa.filters.mel(sr = hp.sample_rate, n_fft = hp.n_fft, n_mels=hp.num_mels, fmin=hp.fmin)\n\n\ndef normalize(S):\n    return np.clip((S - hp.min_level_db) / -hp.min_level_db, 0, 1)\n\n\ndef denormalize(S):\n    return (np.clip(S, 0, 1) * -hp.min_level_db) + hp.min_level_db\n\n\ndef amp_to_db(x):\n    return 20 * np.log10(np.maximum(1e-5, x))\n\n\ndef db_to_amp(x):\n    return np.power(10.0, x * 0.05)\n\n\ndef spectrogram(y):\n    D = stft(y)\n    S = amp_to_db(np.abs(D)) - hp.ref_level_db\n    return normalize(S)\n\n\ndef melspectrogram(y):\n    D = stft(y)\n    S = amp_to_db(linear_to_mel(np.abs(D)))\n    return normalize(S)\n\n\ndef stft(y):\n    return librosa.stft(y=y, n_fft=hp.n_fft, hop_length=hp.hop_length, win_length=hp.win_length)\n\n\ndef pre_emphasis(x):\n    return lfilter([1, -hp.preemphasis], [1], x)\n\n\ndef de_emphasis(x):\n    return lfilter([1], [1, -hp.preemphasis], x)\n\n\ndef encode_mu_law(x, mu) :\n    mu = mu - 1\n    fx = np.sign(x) * np.log(1 + mu * np.abs(x)) / np.log(1 + mu)\n    return np.floor((fx + 1) / 2 * mu + 0.5)\n\n\ndef decode_mu_law(y, mu, from_labels=True) :\n    if from_labels: \n        y = label_2_float(y, math.log2(mu))\n    mu = mu - 1\n    x = np.sign(y) / mu * ((1 + mu) ** np.abs(y) - 1)\n    return x\n\n", "models/vocoder/wavernn/hparams.py": "from models.synthesizer.hparams import hparams as _syn_hp\n\n\n# Audio settings------------------------------------------------------------------------\n# Match the values of the synthesizer\nsample_rate = _syn_hp.sample_rate\nn_fft = _syn_hp.n_fft\nnum_mels = _syn_hp.num_mels\nhop_length = _syn_hp.hop_size\nwin_length = _syn_hp.win_size\nfmin = _syn_hp.fmin\nmin_level_db = _syn_hp.min_level_db\nref_level_db = _syn_hp.ref_level_db\nmel_max_abs_value = _syn_hp.max_abs_value\npreemphasis = _syn_hp.preemphasis\napply_preemphasis = _syn_hp.preemphasize\n\nbits = 9                            # bit depth of signal\nmu_law = True                       # Recommended to suppress noise if using raw bits in hp.voc_mode\n                                    # below\n\n\n# WAVERNN / VOCODER --------------------------------------------------------------------------------\nvoc_mode = 'RAW'                    # either 'RAW' (softmax on raw bits) or 'MOL' (sample from \n# mixture of logistics)\nvoc_upsample_factors = (5, 5, 8)    # NB - this needs to correctly factorise hop_length\nvoc_rnn_dims = 512\nvoc_fc_dims = 512\nvoc_compute_dims = 128\nvoc_res_out_dims = 128\nvoc_res_blocks = 10\n\n# Training\nvoc_batch_size = 100\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5           # number of samples to generate at each checkpoint\nvoc_pad = 2                         # this will pad the input so that the resnet can 'see' wider \n                                    # than input length\nvoc_seq_len = hop_length * 5        # must be a multiple of hop_length\n\n# Generating / Synthesizing\nvoc_gen_batched = True              # very fast (realtime+) single utterance batched generation\nvoc_target = 8000                   # target number of samples to be generated in each batch entry\nvoc_overlap = 400                   # number of samples for crossfading between batches\n", "models/vocoder/wavernn/gen_wavernn.py": "from models.vocoder.wavernn.models.fatchord_version import  WaveRNN\nfrom models.vocoder.wavernn.audio import *\n\n\ndef gen_testset(model: WaveRNN, test_set, samples, batched, target, overlap, save_path):\n    k = model.get_step() // 1000\n\n    for i, (m, x) in enumerate(test_set, 1):\n        if i > samples: \n            break\n\n        print('\\n| Generating: %i/%i' % (i, samples))\n\n        x = x[0].numpy()\n\n        bits = 16 if hp.voc_mode == 'MOL' else hp.bits\n\n        if hp.mu_law and hp.voc_mode != 'MOL' :\n            x = decode_mu_law(x, 2**bits, from_labels=True)\n        else :\n            x = label_2_float(x, bits)\n\n        save_wav(x, save_path.joinpath(\"%dk_steps_%d_target.wav\" % (k, i)))\n        \n        batch_str = \"gen_batched_target%d_overlap%d\" % (target, overlap) if batched else \\\n            \"gen_not_batched\"\n        save_str = save_path.joinpath(\"%dk_steps_%d_%s.wav\" % (k, i, batch_str))\n\n        wav = model.generate(m, batched, target, overlap, hp.mu_law)\n        save_wav(wav, save_str)\n\n", "models/vocoder/wavernn/inference.py": "from models.vocoder.wavernn.models.fatchord_version import WaveRNN\nfrom models.vocoder.wavernn import hparams as hp\nimport torch\n\n\n_model = None   # type: WaveRNN\n\ndef load_model(weights_fpath, verbose=True):\n    global _model, _device\n    \n    if verbose:\n        print(\"Building Wave-RNN\")\n    _model = WaveRNN(\n        rnn_dims=hp.voc_rnn_dims,\n        fc_dims=hp.voc_fc_dims,\n        bits=hp.bits,\n        pad=hp.voc_pad,\n        upsample_factors=hp.voc_upsample_factors,\n        feat_dims=hp.num_mels,\n        compute_dims=hp.voc_compute_dims,\n        res_out_dims=hp.voc_res_out_dims,\n        res_blocks=hp.voc_res_blocks,\n        hop_length=hp.hop_length,\n        sample_rate=hp.sample_rate,\n        mode=hp.voc_mode\n    )\n\n    if torch.cuda.is_available():\n        _model = _model.cuda()\n        _device = torch.device('cuda')\n    else:\n        _device = torch.device('cpu')\n    \n    if verbose:\n        print(\"Loading model weights at %s\" % weights_fpath)\n    checkpoint = torch.load(weights_fpath, _device)\n    _model.load_state_dict(checkpoint['model_state'])\n    _model.eval()\n\n\ndef is_loaded():\n    return _model is not None\n\n\ndef infer_waveform(mel, normalize=True,  batched=True, target=8000, overlap=800, \n                   progress_callback=None):\n    \"\"\"\n    Infers the waveform of a mel spectrogram output by the synthesizer (the format must match \n    that of the synthesizer!)\n    \n    :param normalize:  \n    :param batched: \n    :param target: \n    :param overlap: \n    :return: \n    \"\"\"\n    if _model is None:\n        raise Exception(\"Please load Wave-RNN in memory before using it\")\n    \n    if normalize:\n        mel = mel / hp.mel_max_abs_value\n    mel = torch.from_numpy(mel[None, ...])\n    wav = _model.generate(mel, batched, target, overlap, hp.mu_law, progress_callback)\n    return wav, hp.sample_rate\n", "models/vocoder/wavernn/models/deepmind_version.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils.display import *\nfrom utils.dsp import *\n\n\nclass WaveRNN(nn.Module) :\n    def __init__(self, hidden_size=896, quantisation=256) :\n        super(WaveRNN, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.split_size = hidden_size // 2\n        \n        # The main matmul\n        self.R = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=False)\n        \n        # Output fc layers\n        self.O1 = nn.Linear(self.split_size, self.split_size)\n        self.O2 = nn.Linear(self.split_size, quantisation)\n        self.O3 = nn.Linear(self.split_size, self.split_size)\n        self.O4 = nn.Linear(self.split_size, quantisation)\n        \n        # Input fc layers\n        self.I_coarse = nn.Linear(2, 3 * self.split_size, bias=False)\n        self.I_fine = nn.Linear(3, 3 * self.split_size, bias=False)\n\n        # biases for the gates\n        self.bias_u = nn.Parameter(torch.zeros(self.hidden_size))\n        self.bias_r = nn.Parameter(torch.zeros(self.hidden_size))\n        self.bias_e = nn.Parameter(torch.zeros(self.hidden_size))\n        \n        # display num params\n        self.num_params()\n\n        \n    def forward(self, prev_y, prev_hidden, current_coarse) :\n        \n        # Main matmul - the projection is split 3 ways\n        R_hidden = self.R(prev_hidden)\n        R_u, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n        \n        # Project the prev input \n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n        \n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n        \n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n        \n        # Compute all gates for coarse and fine \n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = torch.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n        \n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n        \n        # Compute outputs \n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n    \n        \n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates \n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input \n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = torch.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = torch.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()        \n            output = combine_signal(coarse, fine)\n        \n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n    \n    def num_params(self) :\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n        print('Trainable Parameters: %.3f million' % parameters)", "models/vocoder/wavernn/models/fatchord_version.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom models.vocoder.distribution import sample_from_discretized_mix_logistic\nfrom models.vocoder.display import *\nfrom models.vocoder.wavernn.audio import *\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.batch_norm1 = nn.BatchNorm1d(dims)\n        self.batch_norm2 = nn.BatchNorm1d(dims)\n\n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.batch_norm1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = self.batch_norm2(x)\n        return x + residual\n\n\nclass MelResNet(nn.Module):\n    def __init__(self, res_blocks, in_dims, compute_dims, res_out_dims, pad):\n        super().__init__()\n        k_size = pad * 2 + 1\n        self.conv_in = nn.Conv1d(in_dims, compute_dims, kernel_size=k_size, bias=False)\n        self.batch_norm = nn.BatchNorm1d(compute_dims)\n        self.layers = nn.ModuleList()\n        for i in range(res_blocks):\n            self.layers.append(ResBlock(compute_dims))\n        self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.batch_norm(x)\n        x = F.relu(x)\n        for f in self.layers: x = f(x)\n        x = self.conv_out(x)\n        return x\n\n\nclass Stretch2d(nn.Module):\n    def __init__(self, x_scale, y_scale):\n        super().__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n\n    def forward(self, x):\n        b, c, h, w = x.size()\n        x = x.unsqueeze(-1).unsqueeze(3)\n        x = x.repeat(1, 1, 1, self.y_scale, 1, self.x_scale)\n        return x.view(b, c, h * self.y_scale, w * self.x_scale)\n\n\nclass UpsampleNetwork(nn.Module):\n    def __init__(self, feat_dims, upsample_scales, compute_dims,\n                 res_blocks, res_out_dims, pad):\n        super().__init__()\n        total_scale = np.cumproduct(upsample_scales)[-1]\n        self.indent = pad * total_scale\n        self.resnet = MelResNet(res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n        self.resnet_stretch = Stretch2d(total_scale, 1)\n        self.up_layers = nn.ModuleList()\n        for scale in upsample_scales:\n            k_size = (1, scale * 2 + 1)\n            padding = (0, scale)\n            stretch = Stretch2d(scale, 1)\n            conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n            conv.weight.data.fill_(1. / k_size[1])\n            self.up_layers.append(stretch)\n            self.up_layers.append(conv)\n\n    def forward(self, m):\n        aux = self.resnet(m).unsqueeze(1)\n        aux = self.resnet_stretch(aux)\n        aux = aux.squeeze(1)\n        m = m.unsqueeze(1)\n        for f in self.up_layers: m = f(m)\n        m = m.squeeze(1)[:, :, self.indent:-self.indent]\n        return m.transpose(1, 2), aux.transpose(1, 2)\n\n\nclass WaveRNN(nn.Module):\n    def __init__(self, rnn_dims, fc_dims, bits, pad, upsample_factors,\n                 feat_dims, compute_dims, res_out_dims, res_blocks,\n                 hop_length, sample_rate, mode='RAW'):\n        super().__init__()\n        self.mode = mode\n        self.pad = pad\n        if self.mode == 'RAW' :\n            self.n_classes = 2 ** bits\n        elif self.mode == 'MOL' :\n            self.n_classes = 30\n        else :\n            RuntimeError(\"Unknown model mode value - \", self.mode)\n\n        self.rnn_dims = rnn_dims\n        self.aux_dims = res_out_dims // 4\n        self.hop_length = hop_length\n        self.sample_rate = sample_rate\n\n        self.upsample = UpsampleNetwork(feat_dims, upsample_factors, compute_dims, res_blocks, res_out_dims, pad)\n        self.I = nn.Linear(feat_dims + self.aux_dims + 1, rnn_dims)\n        self.rnn1 = nn.GRU(rnn_dims, rnn_dims, batch_first=True)\n        self.rnn2 = nn.GRU(rnn_dims + self.aux_dims, rnn_dims, batch_first=True)\n        self.fc1 = nn.Linear(rnn_dims + self.aux_dims, fc_dims)\n        self.fc2 = nn.Linear(fc_dims + self.aux_dims, fc_dims)\n        self.fc3 = nn.Linear(fc_dims, self.n_classes)\n\n        self.step = nn.Parameter(torch.zeros(1).long(), requires_grad=False)\n        self.num_params()\n\n    def forward(self, x, mels):\n        self.step += 1\n        bsize = x.size(0)\n        if torch.cuda.is_available():\n            h1 = torch.zeros(1, bsize, self.rnn_dims).cuda()\n            h2 = torch.zeros(1, bsize, self.rnn_dims).cuda()\n        else:\n            h1 = torch.zeros(1, bsize, self.rnn_dims).cpu()\n            h2 = torch.zeros(1, bsize, self.rnn_dims).cpu()\n        mels, aux = self.upsample(mels)\n\n        aux_idx = [self.aux_dims * i for i in range(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\"Unknown model mode value - \", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n        \n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n        \n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n    def pad_tensor(self, x, pad, side='both'):\n        # NB - this is just a quick method i need right now\n        # i.e., it won't generalise to other shapes/dims\n        b, t, c = x.size()\n        total = t + 2 * pad if side == 'both' else t + pad\n        if torch.cuda.is_available():\n            padded = torch.zeros(b, total, c).cuda()\n        else:\n            padded = torch.zeros(b, total, c).cpu()\n        if side == 'before' or side == 'both':\n            padded[:, pad:pad + t, :] = x\n        elif side == 'after':\n            padded[:, :t, :] = x\n        return padded\n\n    def fold_with_overlap(self, x, target, overlap):\n\n        ''' Fold the tensor with overlap for quick batched inference.\n            Overlap will be used for crossfading in xfade_and_unfold()\n\n        Args:\n            x (tensor)    : Upsampled conditioning features.\n                            shape=(1, timesteps, features)\n            target (int)  : Target timesteps for each index of batch\n            overlap (int) : Timesteps for both xfade and rnn warmup\n\n        Return:\n            (tensor) : shape=(num_folds, target + 2 * overlap, features)\n\n        Details:\n            x = [[h1, h2, ... hn]]\n\n            Where each h is a vector of conditioning features\n\n            Eg: target=2, overlap=1 with x.size(1)=10\n\n            folded = [[h1, h2, h3, h4],\n                      [h4, h5, h6, h7],\n                      [h7, h8, h9, h10]]\n        '''\n\n        _, total_len, features = x.size()\n\n        # Calculate variables needed\n        num_folds = (total_len - overlap) // (target + overlap)\n        extended_len = num_folds * (overlap + target) + overlap\n        remaining = total_len - extended_len\n\n        # Pad if some time steps poking out\n        if remaining != 0:\n            num_folds += 1\n            padding = target + 2 * overlap - remaining\n            x = self.pad_tensor(x, padding, side='after')\n\n        if torch.cuda.is_available():\n            folded = torch.zeros(num_folds, target + 2 * overlap, features).cuda()\n        else:\n            folded = torch.zeros(num_folds, target + 2 * overlap, features).cpu()\n\n        # Get the values for the folded tensor\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            folded[i] = x[:, start:end, :]\n\n        return folded\n\n    def xfade_and_unfold(self, y, target, overlap):\n\n        ''' Applies a crossfade and unfolds into a 1d array.\n\n        Args:\n            y (ndarry)    : Batched sequences of audio samples\n                            shape=(num_folds, target + 2 * overlap)\n                            dtype=np.float64\n            overlap (int) : Timesteps for both xfade and rnn warmup\n\n        Return:\n            (ndarry) : audio samples in a 1d array\n                       shape=(total_len)\n                       dtype=np.float64\n\n        Details:\n            y = [[seq1],\n                 [seq2],\n                 [seq3]]\n\n            Apply a gain envelope at both ends of the sequences\n\n            y = [[seq1_in, seq1_target, seq1_out],\n                 [seq2_in, seq2_target, seq2_out],\n                 [seq3_in, seq3_target, seq3_out]]\n\n            Stagger and add up the groups of samples:\n\n            [seq1_in, seq1_target, (seq1_out + seq2_in), seq2_target, ...]\n\n        '''\n\n        num_folds, length = y.shape\n        target = length - 2 * overlap\n        total_len = num_folds * (target + overlap) + overlap\n\n        # Need some silence for the rnn warmup\n        silence_len = overlap // 2\n        fade_len = overlap - silence_len\n        silence = np.zeros((silence_len), dtype=np.float64)\n\n        # Equal power crossfade\n        t = np.linspace(-1, 1, fade_len, dtype=np.float64)\n        fade_in = np.sqrt(0.5 * (1 + t))\n        fade_out = np.sqrt(0.5 * (1 - t))\n\n        # Concat the silence to the fades\n        fade_in = np.concatenate([silence, fade_in])\n        fade_out = np.concatenate([fade_out, silence])\n\n        # Apply the gain to the overlap samples\n        y[:, :overlap] *= fade_in\n        y[:, -overlap:] *= fade_out\n\n        unfolded = np.zeros((total_len), dtype=np.float64)\n\n        # Loop to add up all the samples\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            unfolded[start:end] += y[i]\n\n        return unfolded\n\n    def get_step(self) :\n        return self.step.data.item()\n\n    def checkpoint(self, model_dir, optimizer) :\n        k_steps = self.get_step() // 1000\n        self.save(model_dir.joinpath(\"checkpoint_%dk_steps.pt\" % k_steps), optimizer)\n\n    def log(self, path, msg) :\n        with open(path, 'a') as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer) :\n        checkpoint = torch.load(path)\n        if \"optimizer_state\" in checkpoint:\n            self.load_state_dict(checkpoint[\"model_state\"])\n            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        else:\n            # Backwards compatibility\n            self.load_state_dict(checkpoint)\n\n    def save(self, path, optimizer) :\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, path)\n\n    def num_params(self, print_out=True):\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n        if print_out :\n            print('Trainable Parameters: %.3fM' % parameters)\n", "models/ppg2mel/preprocess.py": "\nimport os\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport soundfile\nimport resampy\n\nfrom models.ppg_extractor import load_model\nimport encoder.inference as Encoder\nfrom models.encoder.audio import preprocess_wav\nfrom models.encoder import audio\nfrom utils.f0_utils import compute_f0\n\nfrom torch.multiprocessing import Pool, cpu_count\nfrom functools import partial\n\nSAMPLE_RATE=16000\n\ndef _compute_bnf(\n    wav: any,\n    output_fpath: str,\n    device: torch.device,\n    ppg_model_local: any,\n):\n    \"\"\"\n    Compute CTC-Attention Seq2seq ASR encoder bottle-neck features (BNF).\n    \"\"\"\n    ppg_model_local.to(device)\n    wav_tensor = torch.from_numpy(wav).float().to(device).unsqueeze(0)\n    wav_length = torch.LongTensor([wav.shape[0]]).to(device)\n    with torch.no_grad():\n        bnf = ppg_model_local(wav_tensor, wav_length) \n    bnf_npy = bnf.squeeze(0).cpu().numpy()\n    np.save(output_fpath, bnf_npy, allow_pickle=False)\n    return bnf_npy, len(bnf_npy)\n\ndef _compute_f0_from_wav(wav, output_fpath):\n    \"\"\"Compute merged f0 values.\"\"\"\n    f0 = compute_f0(wav, SAMPLE_RATE)\n    np.save(output_fpath, f0, allow_pickle=False)\n    return f0, len(f0)\n\ndef _compute_spkEmbed(wav, output_fpath, encoder_model_local, device):\n    Encoder.set_model(encoder_model_local)\n    # Compute where to split the utterance into partials and pad if necessary\n    wave_slices, mel_slices = Encoder.compute_partial_slices(len(wav), rate=1.3, min_pad_coverage=0.75)\n    max_wave_length = wave_slices[-1].stop\n    if max_wave_length >= len(wav):\n        wav = np.pad(wav, (0, max_wave_length - len(wav)), \"constant\")\n    \n    # Split the utterance into partials\n    frames = audio.wav_to_mel_spectrogram(wav)\n    frames_batch = np.array([frames[s] for s in mel_slices])\n    partial_embeds = Encoder.embed_frames_batch(frames_batch)\n    \n    # Compute the utterance embedding from the partial embeddings\n    raw_embed = np.mean(partial_embeds, axis=0)\n    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n\n    np.save(output_fpath, embed, allow_pickle=False)\n    return embed, len(embed)\n\ndef preprocess_one(wav_path, out_dir, device, ppg_model_local, encoder_model_local):\n    # wav = preprocess_wav(wav_path)\n    # try:\n    wav, sr = soundfile.read(wav_path)\n    if len(wav) < sr:\n        return None, sr, len(wav)\n    if sr != SAMPLE_RATE:\n        wav = resampy.resample(wav, sr, SAMPLE_RATE)\n        sr = SAMPLE_RATE\n    utt_id = os.path.basename(wav_path).rstrip(\".wav\")\n\n    _, length_bnf = _compute_bnf(output_fpath=f\"{out_dir}/bnf/{utt_id}.ling_feat.npy\", wav=wav, device=device, ppg_model_local=ppg_model_local)\n    _, length_f0 = _compute_f0_from_wav(output_fpath=f\"{out_dir}/f0/{utt_id}.f0.npy\", wav=wav)\n    _, length_embed = _compute_spkEmbed(output_fpath=f\"{out_dir}/embed/{utt_id}.npy\",  device=device, encoder_model_local=encoder_model_local, wav=wav)\n\ndef preprocess_dataset(datasets_root, dataset, out_dir, n_processes, ppg_encoder_model_fpath, speaker_encoder_model):\n    # Glob wav files\n    wav_file_list = sorted(Path(f\"{datasets_root}/{dataset}\").glob(\"**/*.wav\"))\n    print(f\"Globbed {len(wav_file_list)} wav files.\")\n\n    out_dir.joinpath(\"bnf\").mkdir(exist_ok=True, parents=True)\n    out_dir.joinpath(\"f0\").mkdir(exist_ok=True, parents=True)\n    out_dir.joinpath(\"embed\").mkdir(exist_ok=True, parents=True)\n    ppg_model_local = load_model(ppg_encoder_model_fpath, \"cpu\")\n    encoder_model_local = Encoder.load_model(speaker_encoder_model, \"cpu\")\n    if n_processes is None:\n        n_processes = cpu_count()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    func = partial(preprocess_one, out_dir=out_dir, ppg_model_local=ppg_model_local, encoder_model_local=encoder_model_local, device=device)\n    job = Pool(n_processes).imap(func, wav_file_list)\n    list(tqdm(job, \"Preprocessing\", len(wav_file_list), unit=\"wav\"))\n\n    # finish processing and mark\n    t_fid_file = out_dir.joinpath(\"train_fidlist.txt\").open(\"w\", encoding=\"utf-8\")\n    d_fid_file = out_dir.joinpath(\"dev_fidlist.txt\").open(\"w\", encoding=\"utf-8\")\n    e_fid_file = out_dir.joinpath(\"eval_fidlist.txt\").open(\"w\", encoding=\"utf-8\")\n    for file in sorted(out_dir.joinpath(\"f0\").glob(\"*.npy\")):\n        id = os.path.basename(file).split(\".f0.npy\")[0]\n        if id.endswith(\"01\"):\n            d_fid_file.write(id + \"\\n\")\n        elif id.endswith(\"09\"):\n            e_fid_file.write(id + \"\\n\")\n        else:\n            t_fid_file.write(id + \"\\n\")\n    t_fid_file.close()\n    d_fid_file.close()\n    e_fid_file.close()\n    return len(wav_file_list)\n", "models/ppg2mel/train.py": "import sys\nimport torch\nimport argparse\nimport numpy as np\nfrom utils.hparams import HpsYaml\nfrom models.ppg2mel.train.train_linglf02mel_seq2seq_oneshotvc import Solver\n\n# For reproducibility, comment these may speed up training\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndef main():\n    # Arguments\n    parser = argparse.ArgumentParser(description=\n            'Training PPG2Mel VC model.')\n    parser.add_argument('--config', type=str, \n                        help='Path to experiment config, e.g., config/vc.yaml')\n    parser.add_argument('--name', default=None, type=str, help='Name for logging.')\n    parser.add_argument('--logdir', default='log/', type=str,\n                        help='Logging path.', required=False)\n    parser.add_argument('--ckpdir', default='ckpt/', type=str,\n                        help='Checkpoint path.', required=False)\n    parser.add_argument('--outdir', default='result/', type=str,\n                        help='Decode output path.', required=False)\n    parser.add_argument('--load', default=None, type=str,\n                        help='Load pre-trained model (for training only)', required=False)\n    parser.add_argument('--warm_start', action='store_true',\n                        help='Load model weights only, ignore specified layers.')\n    parser.add_argument('--seed', default=0, type=int,\n                        help='Random seed for reproducable results.', required=False)\n    parser.add_argument('--njobs', default=8, type=int,\n                        help='Number of threads for dataloader/decoding.', required=False)\n    parser.add_argument('--cpu', action='store_true', help='Disable GPU training.')\n    # parser.add_argument('--no-pin', action='store_true',\n    #                     help='Disable pin-memory for dataloader')\n    parser.add_argument('--no-msg', action='store_true', help='Hide all messages.')\n    \n    ###\n\n    paras = parser.parse_args()\n    setattr(paras, 'gpu', not paras.cpu)\n    setattr(paras, 'pin_memory', not paras.no_pin)\n    setattr(paras, 'verbose', not paras.no_msg)\n    # Make the config dict dot visitable\n    config = HpsYaml(paras.config)\n\n    np.random.seed(paras.seed)\n    torch.manual_seed(paras.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(paras.seed)\n\n    print(\">>> OneShot VC training ...\")\n    mode = \"train\"\n    solver = Solver(config, paras, mode)\n    solver.load_data()\n    solver.set_model()\n    solver.exec()\n    print(\">>> Oneshot VC train finished!\")\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()   \n", "models/ppg2mel/rnn_decoder_mol.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom .utils.mol_attention import MOLAttention\nfrom .utils.basic_layers import Linear\nfrom .utils.vc_utils import get_mask_from_lengths\n\n\nclass DecoderPrenet(nn.Module):\n    def __init__(self, in_dim, sizes):\n        super().__init__()\n        in_sizes = [in_dim] + sizes[:-1]\n        self.layers = nn.ModuleList(\n            [Linear(in_size, out_size, bias=False)\n             for (in_size, out_size) in zip(in_sizes, sizes)])\n\n    def forward(self, x):\n        for linear in self.layers:\n            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n        return x\n\n\nclass Decoder(nn.Module):\n    \"\"\"Mixture of Logistic (MoL) attention-based RNN Decoder.\"\"\"\n    def __init__(\n        self,\n        enc_dim,\n        num_mels,\n        frames_per_step,\n        attention_rnn_dim,\n        decoder_rnn_dim,\n        prenet_dims,\n        num_mixtures,\n        encoder_down_factor=1,\n        num_decoder_rnn_layer=1,\n        use_stop_tokens=False,\n        concat_context_to_last=False,\n    ):\n        super().__init__()\n        self.enc_dim = enc_dim\n        self.encoder_down_factor = encoder_down_factor\n        self.num_mels = num_mels\n        self.frames_per_step = frames_per_step\n        self.attention_rnn_dim = attention_rnn_dim\n        self.decoder_rnn_dim = decoder_rnn_dim\n        self.prenet_dims = prenet_dims\n        self.use_stop_tokens = use_stop_tokens\n        self.num_decoder_rnn_layer = num_decoder_rnn_layer\n        self.concat_context_to_last = concat_context_to_last\n\n        # Mel prenet\n        self.prenet = DecoderPrenet(num_mels, prenet_dims)\n        self.prenet_pitch = DecoderPrenet(num_mels, prenet_dims)\n\n        # Attention RNN\n        self.attention_rnn = nn.LSTMCell(\n            prenet_dims[-1] + enc_dim,\n            attention_rnn_dim\n        )\n        \n        # Attention\n        self.attention_layer = MOLAttention(\n            attention_rnn_dim,\n            r=frames_per_step/encoder_down_factor,\n            M=num_mixtures,\n        )\n\n        # Decoder RNN\n        self.decoder_rnn_layers = nn.ModuleList()\n        for i in range(num_decoder_rnn_layer):\n            if i == 0:\n                self.decoder_rnn_layers.append(\n                    nn.LSTMCell(\n                        enc_dim + attention_rnn_dim,\n                        decoder_rnn_dim))\n            else:\n                self.decoder_rnn_layers.append(\n                    nn.LSTMCell(\n                        decoder_rnn_dim,\n                        decoder_rnn_dim))\n        # self.decoder_rnn = nn.LSTMCell(\n            # 2 * enc_dim + attention_rnn_dim,\n            # decoder_rnn_dim\n        # )\n        if concat_context_to_last:\n            self.linear_projection = Linear(\n                enc_dim + decoder_rnn_dim,\n                num_mels * frames_per_step\n            )\n        else:\n            self.linear_projection = Linear(\n                decoder_rnn_dim,\n                num_mels * frames_per_step\n            )\n\n\n        # Stop-token layer\n        if self.use_stop_tokens:\n            if concat_context_to_last:\n                self.stop_layer = Linear(\n                    enc_dim + decoder_rnn_dim, 1, bias=True, w_init_gain=\"sigmoid\"\n                )\n            else:\n                self.stop_layer = Linear(\n                    decoder_rnn_dim, 1, bias=True, w_init_gain=\"sigmoid\"\n                )\n                \n\n    def get_go_frame(self, memory):\n        B = memory.size(0)\n        go_frame = torch.zeros((B, self.num_mels), dtype=torch.float,\n                               device=memory.device)\n        return go_frame\n\n    def initialize_decoder_states(self, memory, mask):\n        device = next(self.parameters()).device\n        B = memory.size(0)\n        \n        # attention rnn states\n        self.attention_hidden = torch.zeros(\n            (B, self.attention_rnn_dim), device=device)\n        self.attention_cell = torch.zeros(\n            (B, self.attention_rnn_dim), device=device)\n\n        # decoder rnn states\n        self.decoder_hiddens = []\n        self.decoder_cells = []\n        for i in range(self.num_decoder_rnn_layer):\n            self.decoder_hiddens.append(\n                torch.zeros((B, self.decoder_rnn_dim),\n                            device=device)\n            )\n            self.decoder_cells.append(\n                torch.zeros((B, self.decoder_rnn_dim),\n                            device=device)\n            )\n        # self.decoder_hidden = torch.zeros(\n            # (B, self.decoder_rnn_dim), device=device)\n        # self.decoder_cell = torch.zeros(\n            # (B, self.decoder_rnn_dim), device=device)\n        \n        self.attention_context =  torch.zeros(\n            (B, self.enc_dim), device=device)\n\n        self.memory = memory\n        # self.processed_memory = self.attention_layer.memory_layer(memory)\n        self.mask = mask\n\n    def parse_decoder_inputs(self, decoder_inputs):\n        \"\"\"Prepare decoder inputs, i.e. gt mel\n        Args:\n            decoder_inputs:(B, T_out, n_mel_channels) inputs used for teacher-forced training.\n        \"\"\"\n        decoder_inputs = decoder_inputs.reshape(\n            decoder_inputs.size(0),\n            int(decoder_inputs.size(1)/self.frames_per_step), -1)\n        # (B, T_out//r, r*num_mels) -> (T_out//r, B, r*num_mels)\n        decoder_inputs = decoder_inputs.transpose(0, 1)\n        # (T_out//r, B, num_mels)\n        decoder_inputs = decoder_inputs[:,:,-self.num_mels:]\n        return decoder_inputs\n        \n    def parse_decoder_outputs(self, mel_outputs, alignments, stop_outputs):\n        \"\"\" Prepares decoder outputs for output\n        Args:\n            mel_outputs:\n            alignments:\n        \"\"\"\n        # (T_out//r, B, T_enc) -> (B, T_out//r, T_enc)\n        alignments = torch.stack(alignments).transpose(0, 1)\n        # (T_out//r, B) -> (B, T_out//r)\n        if stop_outputs is not None:\n            if alignments.size(0) == 1:\n                stop_outputs = torch.stack(stop_outputs).unsqueeze(0)\n            else:\n                stop_outputs = torch.stack(stop_outputs).transpose(0, 1)\n            stop_outputs = stop_outputs.contiguous()\n        # (T_out//r, B, num_mels*r) -> (B, T_out//r, num_mels*r)\n        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n        # decouple frames per step\n        # (B, T_out, num_mels)\n        mel_outputs = mel_outputs.view(\n            mel_outputs.size(0), -1, self.num_mels)\n        return mel_outputs, alignments, stop_outputs     \n    \n    def attend(self, decoder_input):\n        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n        self.attention_hidden, self.attention_cell = self.attention_rnn(\n            cell_input, (self.attention_hidden, self.attention_cell))\n        self.attention_context, attention_weights = self.attention_layer(\n            self.attention_hidden, self.memory, None, self.mask)\n        \n        decoder_rnn_input = torch.cat(\n            (self.attention_hidden, self.attention_context), -1)\n\n        return decoder_rnn_input, self.attention_context, attention_weights\n\n    def decode(self, decoder_input):\n        for i in range(self.num_decoder_rnn_layer):\n            if i == 0:\n                self.decoder_hiddens[i], self.decoder_cells[i] = self.decoder_rnn_layers[i](\n                    decoder_input, (self.decoder_hiddens[i], self.decoder_cells[i]))\n            else:\n                self.decoder_hiddens[i], self.decoder_cells[i] = self.decoder_rnn_layers[i](\n                    self.decoder_hiddens[i-1], (self.decoder_hiddens[i], self.decoder_cells[i]))\n        return self.decoder_hiddens[-1]\n    \n    def forward(self, memory, mel_inputs, memory_lengths):\n        \"\"\" Decoder forward pass for training\n        Args:\n            memory: (B, T_enc, enc_dim) Encoder outputs\n            decoder_inputs: (B, T, num_mels) Decoder inputs for teacher forcing.\n            memory_lengths: (B, ) Encoder output lengths for attention masking.\n        Returns:\n            mel_outputs: (B, T, num_mels) mel outputs from the decoder\n            alignments: (B, T//r, T_enc) attention weights.\n        \"\"\"\n        # [1, B, num_mels]\n        go_frame = self.get_go_frame(memory).unsqueeze(0)\n        # [T//r, B, num_mels]\n        mel_inputs = self.parse_decoder_inputs(mel_inputs)\n        # [T//r + 1, B, num_mels]\n        mel_inputs = torch.cat((go_frame, mel_inputs), dim=0)\n        # [T//r + 1, B, prenet_dim]\n        decoder_inputs = self.prenet(mel_inputs) \n        # decoder_inputs_pitch = self.prenet_pitch(decoder_inputs__)\n\n        self.initialize_decoder_states(\n            memory, mask=~get_mask_from_lengths(memory_lengths),\n        )\n        \n        self.attention_layer.init_states(memory)\n        # self.attention_layer_pitch.init_states(memory_pitch)\n\n        mel_outputs, alignments = [], []\n        if self.use_stop_tokens:\n            stop_outputs = []\n        else:\n            stop_outputs = None\n        while len(mel_outputs) < decoder_inputs.size(0) - 1:\n            decoder_input = decoder_inputs[len(mel_outputs)]\n            # decoder_input_pitch = decoder_inputs_pitch[len(mel_outputs)]\n\n            decoder_rnn_input, context, attention_weights = self.attend(decoder_input)\n\n            decoder_rnn_output = self.decode(decoder_rnn_input)\n            if self.concat_context_to_last:    \n                decoder_rnn_output = torch.cat(\n                    (decoder_rnn_output, context), dim=1)\n                   \n            mel_output = self.linear_projection(decoder_rnn_output)\n            if self.use_stop_tokens:\n                stop_output = self.stop_layer(decoder_rnn_output)\n                stop_outputs += [stop_output.squeeze()]\n            mel_outputs += [mel_output.squeeze(1)] #? perhaps don't need squeeze\n            alignments += [attention_weights]\n            # alignments_pitch += [attention_weights_pitch]   \n\n        mel_outputs, alignments, stop_outputs = self.parse_decoder_outputs(\n            mel_outputs, alignments, stop_outputs)\n        if stop_outputs is None:\n            return mel_outputs, alignments\n        else:\n            return mel_outputs, stop_outputs, alignments\n\n    def inference(self, memory, stop_threshold=0.5):\n        \"\"\" Decoder inference\n        Args:\n            memory: (1, T_enc, D_enc) Encoder outputs\n        Returns:\n            mel_outputs: mel outputs from the decoder\n            alignments: sequence of attention weights from the decoder\n        \"\"\"\n        # [1, num_mels]\n        decoder_input = self.get_go_frame(memory)\n\n        self.initialize_decoder_states(memory, mask=None)\n\n        self.attention_layer.init_states(memory)\n        \n        mel_outputs, alignments = [], []\n        # NOTE(sx): heuristic \n        max_decoder_step = memory.size(1)*self.encoder_down_factor//self.frames_per_step \n        min_decoder_step = memory.size(1)*self.encoder_down_factor // self.frames_per_step - 5\n        while True:\n            decoder_input = self.prenet(decoder_input)\n\n            decoder_input_final, context, alignment = self.attend(decoder_input)\n\n            #mel_output, stop_output, alignment = self.decode(decoder_input)\n            decoder_rnn_output = self.decode(decoder_input_final)\n            if self.concat_context_to_last:    \n                decoder_rnn_output = torch.cat(\n                    (decoder_rnn_output, context), dim=1)\n            \n            mel_output = self.linear_projection(decoder_rnn_output)\n            stop_output = self.stop_layer(decoder_rnn_output)\n            \n            mel_outputs += [mel_output.squeeze(1)]\n            alignments += [alignment]\n            \n            if torch.sigmoid(stop_output.data) > stop_threshold and len(mel_outputs) >= min_decoder_step:\n                break\n            if len(mel_outputs) >= max_decoder_step:\n                # print(\"Warning! Decoding steps reaches max decoder steps.\")\n                break\n\n            decoder_input = mel_output[:,-self.num_mels:]\n\n\n        mel_outputs, alignments, _  = self.parse_decoder_outputs(\n            mel_outputs, alignments, None)\n\n        return mel_outputs, alignments\n\n    def inference_batched(self, memory, stop_threshold=0.5):\n        \"\"\" Decoder inference\n        Args:\n            memory: (B, T_enc, D_enc) Encoder outputs\n        Returns:\n            mel_outputs: mel outputs from the decoder\n            alignments: sequence of attention weights from the decoder\n        \"\"\"\n        # [1, num_mels]\n        decoder_input = self.get_go_frame(memory)\n\n        self.initialize_decoder_states(memory, mask=None)\n\n        self.attention_layer.init_states(memory)\n        \n        mel_outputs, alignments = [], []\n        stop_outputs = []\n        # NOTE(sx): heuristic \n        max_decoder_step = memory.size(1)*self.encoder_down_factor//self.frames_per_step \n        min_decoder_step = memory.size(1)*self.encoder_down_factor // self.frames_per_step - 5\n        while True:\n            decoder_input = self.prenet(decoder_input)\n\n            decoder_input_final, context, alignment = self.attend(decoder_input)\n\n            #mel_output, stop_output, alignment = self.decode(decoder_input)\n            decoder_rnn_output = self.decode(decoder_input_final)\n            if self.concat_context_to_last:    \n                decoder_rnn_output = torch.cat(\n                    (decoder_rnn_output, context), dim=1)\n            \n            mel_output = self.linear_projection(decoder_rnn_output)\n            # (B, 1)\n            stop_output = self.stop_layer(decoder_rnn_output)\n            stop_outputs += [stop_output.squeeze()]\n            # stop_outputs.append(stop_output) \n\n            mel_outputs += [mel_output.squeeze(1)]\n            alignments += [alignment]\n            # print(stop_output.shape)\n            if torch.all(torch.sigmoid(stop_output.squeeze().data) > stop_threshold) \\\n                    and len(mel_outputs) >= min_decoder_step:\n                break\n            if len(mel_outputs) >= max_decoder_step:\n                # print(\"Warning! Decoding steps reaches max decoder steps.\")\n                break\n\n            decoder_input = mel_output[:,-self.num_mels:]\n\n\n        mel_outputs, alignments, stop_outputs = self.parse_decoder_outputs(\n            mel_outputs, alignments, stop_outputs)\n        mel_outputs_stacked = []\n        for mel, stop_logit in zip(mel_outputs, stop_outputs):\n            idx = np.argwhere(torch.sigmoid(stop_logit.cpu()) > stop_threshold)[0][0].item()\n            mel_outputs_stacked.append(mel[:idx,:])\n        mel_outputs = torch.cat(mel_outputs_stacked, dim=0).unsqueeze(0)\n        return mel_outputs, alignments\n", "models/ppg2mel/__init__.py": "#!/usr/bin/env python3\n\n# Copyright 2020 Songxiang Liu\n# Apache 2.0\n\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\n\nimport numpy as np\n\nfrom .utils.abs_model import AbsMelDecoder\nfrom .rnn_decoder_mol import Decoder\nfrom .utils.cnn_postnet import Postnet\nfrom .utils.vc_utils import get_mask_from_lengths\n\nfrom utils.hparams import HpsYaml\n\nclass MelDecoderMOLv2(AbsMelDecoder):\n    \"\"\"Use an encoder to preprocess ppg.\"\"\"\n    def __init__(\n        self,\n        num_speakers: int,\n        spk_embed_dim: int,\n        bottle_neck_feature_dim: int,\n        encoder_dim: int = 256,\n        encoder_downsample_rates: List = [2, 2],\n        attention_rnn_dim: int = 512,\n        decoder_rnn_dim: int = 512,\n        num_decoder_rnn_layer: int = 1,\n        concat_context_to_last: bool = True,\n        prenet_dims: List = [256, 128],\n        num_mixtures: int = 5,\n        frames_per_step: int = 2,\n        mask_padding: bool = True,\n    ):\n        super().__init__()\n        \n        self.mask_padding = mask_padding\n        self.bottle_neck_feature_dim = bottle_neck_feature_dim\n        self.num_mels = 80\n        self.encoder_down_factor=np.cumprod(encoder_downsample_rates)[-1]\n        self.frames_per_step = frames_per_step\n        self.use_spk_dvec = True\n\n        input_dim = bottle_neck_feature_dim\n        \n        # Downsampling convolution\n        self.bnf_prenet = torch.nn.Sequential(\n            torch.nn.Conv1d(input_dim, encoder_dim, kernel_size=1, bias=False),\n            torch.nn.LeakyReLU(0.1),\n\n            torch.nn.InstanceNorm1d(encoder_dim, affine=False),\n            torch.nn.Conv1d(\n                encoder_dim, encoder_dim, \n                kernel_size=2*encoder_downsample_rates[0], \n                stride=encoder_downsample_rates[0], \n                padding=encoder_downsample_rates[0]//2,\n            ),\n            torch.nn.LeakyReLU(0.1),\n            \n            torch.nn.InstanceNorm1d(encoder_dim, affine=False),\n            torch.nn.Conv1d(\n                encoder_dim, encoder_dim, \n                kernel_size=2*encoder_downsample_rates[1], \n                stride=encoder_downsample_rates[1], \n                padding=encoder_downsample_rates[1]//2,\n            ),\n            torch.nn.LeakyReLU(0.1),\n\n            torch.nn.InstanceNorm1d(encoder_dim, affine=False),\n        )\n        decoder_enc_dim = encoder_dim\n        self.pitch_convs = torch.nn.Sequential(\n            torch.nn.Conv1d(2, encoder_dim, kernel_size=1, bias=False),\n            torch.nn.LeakyReLU(0.1),\n\n            torch.nn.InstanceNorm1d(encoder_dim, affine=False),\n            torch.nn.Conv1d(\n                encoder_dim, encoder_dim, \n                kernel_size=2*encoder_downsample_rates[0], \n                stride=encoder_downsample_rates[0], \n                padding=encoder_downsample_rates[0]//2,\n            ),\n            torch.nn.LeakyReLU(0.1),\n            \n            torch.nn.InstanceNorm1d(encoder_dim, affine=False),\n            torch.nn.Conv1d(\n                encoder_dim, encoder_dim, \n                kernel_size=2*encoder_downsample_rates[1], \n                stride=encoder_downsample_rates[1], \n                padding=encoder_downsample_rates[1]//2,\n            ),\n            torch.nn.LeakyReLU(0.1),\n\n            torch.nn.InstanceNorm1d(encoder_dim, affine=False),\n        )\n        \n        self.reduce_proj = torch.nn.Linear(encoder_dim + spk_embed_dim, encoder_dim)\n\n        # Decoder\n        self.decoder = Decoder(\n            enc_dim=decoder_enc_dim,\n            num_mels=self.num_mels,\n            frames_per_step=frames_per_step,\n            attention_rnn_dim=attention_rnn_dim,\n            decoder_rnn_dim=decoder_rnn_dim,\n            num_decoder_rnn_layer=num_decoder_rnn_layer,\n            prenet_dims=prenet_dims,\n            num_mixtures=num_mixtures,\n            use_stop_tokens=True,\n            concat_context_to_last=concat_context_to_last,\n            encoder_down_factor=self.encoder_down_factor,\n        )\n\n        # Mel-Spec Postnet: some residual CNN layers\n        self.postnet = Postnet()\n    \n    def parse_output(self, outputs, output_lengths=None):\n        if self.mask_padding and output_lengths is not None:\n            mask = ~get_mask_from_lengths(output_lengths, outputs[0].size(1))\n            mask = mask.unsqueeze(2).expand(mask.size(0), mask.size(1), self.num_mels)\n            outputs[0].data.masked_fill_(mask, 0.0)\n            outputs[1].data.masked_fill_(mask, 0.0)\n        return outputs\n\n    def forward(\n        self,\n        bottle_neck_features: torch.Tensor,\n        feature_lengths: torch.Tensor,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        logf0_uv: torch.Tensor = None,\n        spembs: torch.Tensor = None,\n        output_att_ws: bool = False,\n    ):\n        decoder_inputs = self.bnf_prenet(\n            bottle_neck_features.transpose(1, 2)\n        ).transpose(1, 2)\n        logf0_uv = self.pitch_convs(logf0_uv.transpose(1, 2)).transpose(1, 2)\n        decoder_inputs = decoder_inputs + logf0_uv\n            \n        assert spembs is not None\n        spk_embeds = F.normalize(\n            spembs).unsqueeze(1).expand(-1, decoder_inputs.size(1), -1)\n        decoder_inputs = torch.cat([decoder_inputs, spk_embeds], dim=-1)\n        decoder_inputs = self.reduce_proj(decoder_inputs)\n        \n        # (B, num_mels, T_dec)\n        T_dec = torch.div(feature_lengths, int(self.encoder_down_factor), rounding_mode='floor')\n        mel_outputs, predicted_stop, alignments = self.decoder(\n            decoder_inputs, speech, T_dec)\n        ## Post-processing\n        mel_outputs_postnet = self.postnet(mel_outputs.transpose(1, 2)).transpose(1, 2)\n        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n        if output_att_ws: \n            return self.parse_output(\n                [mel_outputs, mel_outputs_postnet, predicted_stop, alignments], speech_lengths)\n        else:\n            return self.parse_output(\n                [mel_outputs, mel_outputs_postnet, predicted_stop], speech_lengths)\n\n        # return mel_outputs, mel_outputs_postnet\n\n    def inference(\n        self,\n        bottle_neck_features: torch.Tensor,\n        logf0_uv: torch.Tensor = None,\n        spembs: torch.Tensor = None,\n    ):\n        decoder_inputs = self.bnf_prenet(bottle_neck_features.transpose(1, 2)).transpose(1, 2)\n        logf0_uv = self.pitch_convs(logf0_uv.transpose(1, 2)).transpose(1, 2)\n        decoder_inputs = decoder_inputs + logf0_uv\n\n        assert spembs is not None\n        spk_embeds = F.normalize(\n            spembs).unsqueeze(1).expand(-1, decoder_inputs.size(1), -1)\n        bottle_neck_features = torch.cat([decoder_inputs, spk_embeds], dim=-1)\n        bottle_neck_features = self.reduce_proj(bottle_neck_features)\n\n        ## Decoder\n        if bottle_neck_features.size(0) > 1:\n            mel_outputs, alignments = self.decoder.inference_batched(bottle_neck_features)\n        else:\n            mel_outputs, alignments = self.decoder.inference(bottle_neck_features,)\n        ## Post-processing\n        mel_outputs_postnet = self.postnet(mel_outputs.transpose(1, 2)).transpose(1, 2)\n        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n        # outputs = mel_outputs_postnet[0]\n        \n        return mel_outputs[0], mel_outputs_postnet[0], alignments[0]\n\ndef load_model(model_file, device=None):\n    # search a config file\n    model_config_fpaths = list(model_file.parent.rglob(\"*.yaml\"))\n    if len(model_config_fpaths) == 0:\n        raise \"No model yaml config found for convertor\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model_config = HpsYaml(model_config_fpaths[0])\n    ppg2mel_model = MelDecoderMOLv2(\n        **model_config[\"model\"]\n    ).to(device)\n    ckpt = torch.load(model_file, map_location=device)\n    ppg2mel_model.load_state_dict(ckpt[\"model\"])\n    ppg2mel_model.eval()\n    return ppg2mel_model\n", "models/ppg2mel/utils/nets_utils.py": "# -*- coding: utf-8 -*-\n\n\"\"\"Network related utility tools.\"\"\"\n\nimport logging\nfrom typing import Dict\n\nimport numpy as np\nimport torch\n\n\ndef to_device(m, x):\n    \"\"\"Send tensor into the device of the module.\n\n    Args:\n        m (torch.nn.Module): Torch module.\n        x (Tensor): Torch tensor.\n\n    Returns:\n        Tensor: Torch tensor located in the same place as torch module.\n\n    \"\"\"\n    assert isinstance(m, torch.nn.Module)\n    device = next(m.parameters()).device\n    return x.to(device)\n\n\ndef pad_list(xs, pad_value):\n    \"\"\"Perform padding for the list of tensors.\n\n    Args:\n        xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)].\n        pad_value (float): Value for padding.\n\n    Returns:\n        Tensor: Padded tensor (B, Tmax, `*`).\n\n    Examples:\n        >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)]\n        >>> x\n        [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])]\n        >>> pad_list(x, 0)\n        tensor([[1., 1., 1., 1.],\n                [1., 1., 0., 0.],\n                [1., 0., 0., 0.]])\n\n    \"\"\"\n    n_batch = len(xs)\n    max_len = max(x.size(0) for x in xs)\n    pad = xs[0].new(n_batch, max_len, *xs[0].size()[1:]).fill_(pad_value)\n\n    for i in range(n_batch):\n        pad[i, :xs[i].size(0)] = xs[i]\n\n    return pad\n\n\ndef make_pad_mask(lengths, xs=None, length_dim=-1):\n    \"\"\"Make mask tensor containing indices of padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor. If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor. See the example.\n\n    Returns:\n        Tensor: Mask tensor containing indices of padded part.\n                dtype=torch.uint8 in PyTorch 1.2-\n                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[0, 0, 0, 0 ,0],\n                 [0, 0, 0, 1, 1],\n                 [0, 0, 1, 1, 1]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0],\n                 [0, 0, 0, 0]],\n                [[0, 0, 0, 1],\n                 [0, 0, 0, 1]],\n                [[0, 0, 1, 1],\n                 [0, 0, 1, 1]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_pad_mask(lengths, xs, 1)\n        tensor([[[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n        >>> make_pad_mask(lengths, xs, 2)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n    \"\"\"\n    if length_dim == 0:\n        raise ValueError('length_dim cannot be 0: {}'.format(length_dim))\n\n    if not isinstance(lengths, list):\n        lengths = lengths.tolist()\n    bs = int(len(lengths))\n    if xs is None:\n        maxlen = int(max(lengths))\n    else:\n        maxlen = xs.size(length_dim)\n\n    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n    mask = seq_range_expand >= seq_length_expand\n\n    if xs is not None:\n        assert xs.size(0) == bs, (xs.size(0), bs)\n\n        if length_dim < 0:\n            length_dim = xs.dim() + length_dim\n        # ind = (:, None, ..., None, :, , None, ..., None)\n        ind = tuple(slice(None) if i in (0, length_dim) else None\n                    for i in range(xs.dim()))\n        mask = mask[ind].expand_as(xs).to(xs.device)\n    return mask\n\n\ndef make_non_pad_mask(lengths, xs=None, length_dim=-1):\n    \"\"\"Make mask tensor containing indices of non-padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor. If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor. See the example.\n\n    Returns:\n        ByteTensor: mask tensor containing indices of padded part.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[1, 1, 1, 1 ,1],\n                 [1, 1, 1, 0, 0],\n                 [1, 1, 0, 0, 0]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1],\n                 [1, 1, 1, 1]],\n                [[1, 1, 1, 0],\n                 [1, 1, 1, 0]],\n                [[1, 1, 0, 0],\n                 [1, 1, 0, 0]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_non_pad_mask(lengths, xs, 1)\n        tensor([[[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)\n        >>> make_non_pad_mask(lengths, xs, 2)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n    \"\"\"\n    return ~make_pad_mask(lengths, xs, length_dim)\n\n\ndef mask_by_length(xs, lengths, fill=0):\n    \"\"\"Mask tensor according to length.\n\n    Args:\n        xs (Tensor): Batch of input tensor (B, `*`).\n        lengths (LongTensor or List): Batch of lengths (B,).\n        fill (int or float): Value to fill masked part.\n\n    Returns:\n        Tensor: Batch of masked input tensor (B, `*`).\n\n    Examples:\n        >>> x = torch.arange(5).repeat(3, 1) + 1\n        >>> x\n        tensor([[1, 2, 3, 4, 5],\n                [1, 2, 3, 4, 5],\n                [1, 2, 3, 4, 5]])\n        >>> lengths = [5, 3, 2]\n        >>> mask_by_length(x, lengths)\n        tensor([[1, 2, 3, 4, 5],\n                [1, 2, 3, 0, 0],\n                [1, 2, 0, 0, 0]])\n\n    \"\"\"\n    assert xs.size(0) == len(lengths)\n    ret = xs.data.new(*xs.size()).fill_(fill)\n    for i, l in enumerate(lengths):\n        ret[i, :l] = xs[i, :l]\n    return ret\n\n\ndef th_accuracy(pad_outputs, pad_targets, ignore_label):\n    \"\"\"Calculate accuracy.\n\n    Args:\n        pad_outputs (Tensor): Prediction tensors (B * Lmax, D).\n        pad_targets (LongTensor): Target label tensors (B, Lmax, D).\n        ignore_label (int): Ignore label id.\n\n    Returns:\n        float: Accuracy value (0.0 - 1.0).\n\n    \"\"\"\n    pad_pred = pad_outputs.view(\n        pad_targets.size(0),\n        pad_targets.size(1),\n        pad_outputs.size(1)).argmax(2)\n    mask = pad_targets != ignore_label\n    numerator = torch.sum(pad_pred.masked_select(mask) == pad_targets.masked_select(mask))\n    denominator = torch.sum(mask)\n    return float(numerator) / float(denominator)\n\n\ndef to_torch_tensor(x):\n    \"\"\"Change to torch.Tensor or ComplexTensor from numpy.ndarray.\n\n    Args:\n        x: Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict.\n\n    Returns:\n        Tensor or ComplexTensor: Type converted inputs.\n\n    Examples:\n        >>> xs = np.ones(3, dtype=np.float32)\n        >>> xs = to_torch_tensor(xs)\n        tensor([1., 1., 1.])\n        >>> xs = torch.ones(3, 4, 5)\n        >>> assert to_torch_tensor(xs) is xs\n        >>> xs = {'real': xs, 'imag': xs}\n        >>> to_torch_tensor(xs)\n        ComplexTensor(\n        Real:\n        tensor([1., 1., 1.])\n        Imag;\n        tensor([1., 1., 1.])\n        )\n\n    \"\"\"\n    # If numpy, change to torch tensor\n    if isinstance(x, np.ndarray):\n        if x.dtype.kind == 'c':\n            # Dynamically importing because torch_complex requires python3\n            from torch_complex.tensor import ComplexTensor\n            return ComplexTensor(x)\n        else:\n            return torch.from_numpy(x)\n\n    # If {'real': ..., 'imag': ...}, convert to ComplexTensor\n    elif isinstance(x, dict):\n        # Dynamically importing because torch_complex requires python3\n        from torch_complex.tensor import ComplexTensor\n\n        if 'real' not in x or 'imag' not in x:\n            raise ValueError(\"has 'real' and 'imag' keys: {}\".format(list(x)))\n        # Relative importing because of using python3 syntax\n        return ComplexTensor(x['real'], x['imag'])\n\n    # If torch.Tensor, as it is\n    elif isinstance(x, torch.Tensor):\n        return x\n\n    else:\n        error = (\"x must be numpy.ndarray, torch.Tensor or a dict like \"\n                 \"{{'real': torch.Tensor, 'imag': torch.Tensor}}, \"\n                 \"but got {}\".format(type(x)))\n        try:\n            from torch_complex.tensor import ComplexTensor\n        except Exception:\n            # If PY2\n            raise ValueError(error)\n        else:\n            # If PY3\n            if isinstance(x, ComplexTensor):\n                return x\n            else:\n                raise ValueError(error)\n\n\ndef get_subsample(train_args, mode, arch):\n    \"\"\"Parse the subsampling factors from the training args for the specified `mode` and `arch`.\n\n    Args:\n        train_args: argument Namespace containing options.\n        mode: one of ('asr', 'mt', 'st')\n        arch: one of ('rnn', 'rnn-t', 'rnn_mix', 'rnn_mulenc', 'transformer')\n\n    Returns:\n        np.ndarray / List[np.ndarray]: subsampling factors.\n    \"\"\"\n    if arch == 'transformer':\n        return np.array([1])\n\n    elif mode == 'mt' and arch == 'rnn':\n        # +1 means input (+1) and layers outputs (train_args.elayer)\n        subsample = np.ones(train_args.elayers + 1, dtype=np.int)\n        logging.warning('Subsampling is not performed for machine translation.')\n        logging.info('subsample: ' + ' '.join([str(x) for x in subsample]))\n        return subsample\n\n    elif (mode == 'asr' and arch in ('rnn', 'rnn-t')) or \\\n         (mode == 'mt' and arch == 'rnn') or \\\n         (mode == 'st' and arch == 'rnn'):\n        subsample = np.ones(train_args.elayers + 1, dtype=np.int)\n        if train_args.etype.endswith(\"p\") and not train_args.etype.startswith(\"vgg\"):\n            ss = train_args.subsample.split(\"_\")\n            for j in range(min(train_args.elayers + 1, len(ss))):\n                subsample[j] = int(ss[j])\n        else:\n            logging.warning(\n                'Subsampling is not performed for vgg*. It is performed in max pooling layers at CNN.')\n        logging.info('subsample: ' + ' '.join([str(x) for x in subsample]))\n        return subsample\n\n    elif mode == 'asr' and arch == 'rnn_mix':\n        subsample = np.ones(train_args.elayers_sd + train_args.elayers + 1, dtype=np.int)\n        if train_args.etype.endswith(\"p\") and not train_args.etype.startswith(\"vgg\"):\n            ss = train_args.subsample.split(\"_\")\n            for j in range(min(train_args.elayers_sd + train_args.elayers + 1, len(ss))):\n                subsample[j] = int(ss[j])\n        else:\n            logging.warning(\n                'Subsampling is not performed for vgg*. It is performed in max pooling layers at CNN.')\n        logging.info('subsample: ' + ' '.join([str(x) for x in subsample]))\n        return subsample\n\n    elif mode == 'asr' and arch == 'rnn_mulenc':\n        subsample_list = []\n        for idx in range(train_args.num_encs):\n            subsample = np.ones(train_args.elayers[idx] + 1, dtype=np.int)\n            if train_args.etype[idx].endswith(\"p\") and not train_args.etype[idx].startswith(\"vgg\"):\n                ss = train_args.subsample[idx].split(\"_\")\n                for j in range(min(train_args.elayers[idx] + 1, len(ss))):\n                    subsample[j] = int(ss[j])\n            else:\n                logging.warning(\n                    'Encoder %d: Subsampling is not performed for vgg*. '\n                    'It is performed in max pooling layers at CNN.', idx + 1)\n            logging.info('subsample: ' + ' '.join([str(x) for x in subsample]))\n            subsample_list.append(subsample)\n        return subsample_list\n\n    else:\n        raise ValueError('Invalid options: mode={}, arch={}'.format(mode, arch))\n\n\ndef rename_state_dict(old_prefix: str, new_prefix: str, state_dict: Dict[str, torch.Tensor]):\n    \"\"\"Replace keys of old prefix with new prefix in state dict.\"\"\"\n    # need this list not to break the dict iterator\n    old_keys = [k for k in state_dict if k.startswith(old_prefix)]\n    if len(old_keys) > 0:\n        logging.warning(f'Rename: {old_prefix} -> {new_prefix}')\n    for k in old_keys:\n        v = state_dict.pop(k)\n        new_k = k.replace(old_prefix, new_prefix)\n        state_dict[new_k] = v\n", "models/ppg2mel/utils/vc_utils.py": "import torch\n\n\ndef gcd(a, b):  \n    \"\"\"Greatest common divisor.\"\"\"\n    a, b = (a, b) if a >=b else (b, a)\n    if a%b == 0:  \n        return b  \n    else :  \n        return gcd(b, a%b) \n\ndef lcm(a, b):\n    \"\"\"Least common multiple\"\"\"\n    return a * b // gcd(a, b)\n\ndef get_mask_from_lengths(lengths, max_len=None):\n    if max_len is None:\n        max_len = torch.max(lengths).item()\n    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n    mask = (ids < lengths.unsqueeze(1)).bool()\n    return mask\n\n", "models/ppg2mel/utils/abs_model.py": "from abc import ABC\nfrom abc import abstractmethod\n\nimport torch\n\nclass AbsMelDecoder(torch.nn.Module, ABC):\n    \"\"\"The abstract PPG-based voice conversion class\n    This \"model\" is one of mediator objects for \"Task\" class.\n\n    \"\"\"\n\n    @abstractmethod\n    def forward(\n        self, \n        bottle_neck_features: torch.Tensor,\n        feature_lengths: torch.Tensor,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        logf0_uv: torch.Tensor = None,\n        spembs: torch.Tensor = None,\n        styleembs: torch.Tensor = None,\n    ) -> torch.Tensor:\n        raise NotImplementedError\n", "models/ppg2mel/utils/mol_attention.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MOLAttention(nn.Module):\n    \"\"\" Discretized Mixture of Logistic (MOL) attention.\n    C.f. Section 5 of \"MelNet: A Generative Model for Audio in the Frequency Domain\" and \n        GMMv2b model in \"Location-relative attention mechanisms for robust long-form speech synthesis\".\n    \"\"\"\n    def __init__(\n        self,\n        query_dim,\n        r=1,\n        M=5,\n    ):\n        \"\"\"\n        Args:\n            query_dim: attention_rnn_dim.\n            M: number of mixtures.\n        \"\"\"\n        super().__init__()\n        if r < 1:\n            self.r = float(r)\n        else:\n            self.r = int(r)\n        self.M = M\n        self.score_mask_value = 0.0 # -float(\"inf\")\n        self.eps = 1e-5\n        # Position arrary for encoder time steps\n        self.J = None\n        # Query layer: [w, sigma,]\n        self.query_layer = torch.nn.Sequential(\n            nn.Linear(query_dim, 256, bias=True),\n            nn.ReLU(),\n            nn.Linear(256, 3*M, bias=True)\n        )\n        self.mu_prev = None\n        self.initialize_bias()\n\n    def initialize_bias(self):\n        \"\"\"Initialize sigma and Delta.\"\"\"\n        # sigma\n        torch.nn.init.constant_(self.query_layer[2].bias[self.M:2*self.M], 1.0)\n        # Delta: softplus(1.8545) = 2.0; softplus(3.9815) = 4.0; softplus(0.5413) = 1.0\n        # softplus(-0.432) = 0.5003\n        if self.r == 2:\n            torch.nn.init.constant_(self.query_layer[2].bias[2*self.M:3*self.M], 1.8545)\n        elif self.r == 4:\n            torch.nn.init.constant_(self.query_layer[2].bias[2*self.M:3*self.M], 3.9815)\n        elif self.r == 1:\n            torch.nn.init.constant_(self.query_layer[2].bias[2*self.M:3*self.M], 0.5413)\n        else:\n            torch.nn.init.constant_(self.query_layer[2].bias[2*self.M:3*self.M], -0.432)\n\n    \n    def init_states(self, memory):\n        \"\"\"Initialize mu_prev and J.\n            This function should be called by the decoder before decoding one batch.\n        Args:\n            memory: (B, T, D_enc) encoder output.\n        \"\"\"\n        B, T_enc, _ = memory.size()\n        device = memory.device\n        self.J = torch.arange(0, T_enc + 2.0).to(device) + 0.5  # NOTE: for discretize usage\n        # self.J = memory.new_tensor(np.arange(T_enc), dtype=torch.float)\n        self.mu_prev = torch.zeros(B, self.M).to(device)\n\n    def forward(self, att_rnn_h, memory, memory_pitch=None, mask=None):\n        \"\"\"\n        att_rnn_h: attetion rnn hidden state.\n        memory: encoder outputs (B, T_enc, D).\n        mask: binary mask for padded data (B, T_enc).\n        \"\"\"\n        # [B, 3M]\n        mixture_params = self.query_layer(att_rnn_h)\n        \n        # [B, M]\n        w_hat = mixture_params[:, :self.M]\n        sigma_hat = mixture_params[:, self.M:2*self.M]\n        Delta_hat = mixture_params[:, 2*self.M:3*self.M]\n        \n        # print(\"w_hat: \", w_hat)\n        # print(\"sigma_hat: \", sigma_hat)\n        # print(\"Delta_hat: \", Delta_hat)\n\n        # Dropout to de-correlate attention heads\n        w_hat = F.dropout(w_hat, p=0.5, training=self.training) # NOTE(sx): needed?\n        \n        # Mixture parameters\n        w = torch.softmax(w_hat, dim=-1) + self.eps\n        sigma = F.softplus(sigma_hat) + self.eps\n        Delta = F.softplus(Delta_hat)\n        mu_cur = self.mu_prev + Delta\n        # print(\"w:\", w)\n        j = self.J[:memory.size(1) + 1]\n\n        # Attention weights\n        # CDF of logistic distribution\n        phi_t = w.unsqueeze(-1) * (1 / (1 + torch.sigmoid(\n            (mu_cur.unsqueeze(-1) - j) / sigma.unsqueeze(-1))))\n        # print(\"phi_t:\", phi_t)\n        \n        # Discretize attention weights\n        # (B, T_enc + 1)\n        alpha_t = torch.sum(phi_t, dim=1)\n        alpha_t = alpha_t[:, 1:] - alpha_t[:, :-1]\n        alpha_t[alpha_t == 0] = self.eps\n        # print(\"alpha_t: \", alpha_t.size())\n        # Apply masking\n        if mask is not None:\n            alpha_t.data.masked_fill_(mask, self.score_mask_value)\n\n        context = torch.bmm(alpha_t.unsqueeze(1), memory).squeeze(1)\n        if memory_pitch is not None:\n            context_pitch = torch.bmm(alpha_t.unsqueeze(1), memory_pitch).squeeze(1)\n\n        self.mu_prev = mu_cur\n        \n        if memory_pitch is not None:\n            return context, context_pitch, alpha_t\n        return context, alpha_t\n\n", "models/ppg2mel/utils/cnn_postnet.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .basic_layers import Linear, Conv1d\n\n\nclass Postnet(nn.Module):\n    \"\"\"Postnet\n        - Five 1-d convolution with 512 channels and kernel size 5\n    \"\"\"\n    def __init__(self, num_mels=80,\n                 num_layers=5,\n                 hidden_dim=512,\n                 kernel_size=5):\n        super(Postnet, self).__init__()\n        self.convolutions = nn.ModuleList()\n\n        self.convolutions.append(\n            nn.Sequential(\n                Conv1d(\n                    num_mels, hidden_dim,\n                    kernel_size=kernel_size, stride=1,\n                    padding=int((kernel_size - 1) / 2),\n                    dilation=1, w_init_gain='tanh'),\n                nn.BatchNorm1d(hidden_dim)))\n\n        for i in range(1, num_layers - 1):\n            self.convolutions.append(\n                nn.Sequential(\n                    Conv1d(\n                        hidden_dim,\n                        hidden_dim,\n                        kernel_size=kernel_size, stride=1,\n                        padding=int((kernel_size - 1) / 2),\n                        dilation=1, w_init_gain='tanh'),\n                    nn.BatchNorm1d(hidden_dim)))\n\n        self.convolutions.append(\n            nn.Sequential(\n                Conv1d(\n                    hidden_dim, num_mels,\n                    kernel_size=kernel_size, stride=1,\n                    padding=int((kernel_size - 1) / 2),\n                    dilation=1, w_init_gain='linear'),\n                nn.BatchNorm1d(num_mels)))\n\n    def forward(self, x):\n        # x: (B, num_mels, T_dec)\n        for i in range(len(self.convolutions) - 1):\n            x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n        return x\n", "models/ppg2mel/utils/basic_layers.py": "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Function\n\ndef tile(x, count, dim=0):\n    \"\"\"\n    Tiles x on dimension dim count times.\n    \"\"\"\n    perm = list(range(len(x.size())))\n    if dim != 0:\n        perm[0], perm[dim] = perm[dim], perm[0]\n        x = x.permute(perm).contiguous()\n    out_size = list(x.size())\n    out_size[0] *= count\n    batch = x.size(0)\n    x = x.view(batch, -1) \\\n         .transpose(0, 1) \\\n         .repeat(count, 1) \\\n         .transpose(0, 1) \\\n         .contiguous() \\\n         .view(*out_size)\n    if dim != 0:\n        x = x.permute(perm).contiguous()\n    return x\n\nclass Linear(torch.nn.Module):\n    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n        super(Linear, self).__init__()\n        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n\n        torch.nn.init.xavier_uniform_(\n            self.linear_layer.weight,\n            gain=torch.nn.init.calculate_gain(w_init_gain))\n\n    def forward(self, x):\n        return self.linear_layer(x)\n\nclass Conv1d(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n                 padding=None, dilation=1, bias=True, w_init_gain='linear', param=None):\n        super(Conv1d, self).__init__()\n        if padding is None:\n            assert(kernel_size % 2 == 1)\n            padding = int(dilation * (kernel_size - 1)/2)\n        \n        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n                                    kernel_size=kernel_size, stride=stride,\n                                    padding=padding, dilation=dilation,\n                                    bias=bias)\n        torch.nn.init.xavier_uniform_(\n            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain, param=param))\n\n    def forward(self, x):\n        # x: BxDxT\n        return self.conv(x)\n\n\n\ndef tile(x, count, dim=0):\n    \"\"\"\n    Tiles x on dimension dim count times.\n    \"\"\"\n    perm = list(range(len(x.size())))\n    if dim != 0:\n        perm[0], perm[dim] = perm[dim], perm[0]\n        x = x.permute(perm).contiguous()\n    out_size = list(x.size())\n    out_size[0] *= count\n    batch = x.size(0)\n    x = x.view(batch, -1) \\\n         .transpose(0, 1) \\\n         .repeat(count, 1) \\\n         .transpose(0, 1) \\\n         .contiguous() \\\n         .view(*out_size)\n    if dim != 0:\n        x = x.permute(perm).contiguous()\n    return x\n", "models/ppg2mel/train/loss.py": "from typing import Dict\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..utils.nets_utils import make_pad_mask\n\n\nclass MaskedMSELoss(nn.Module):\n    def __init__(self, frames_per_step):\n        super().__init__()\n        self.frames_per_step = frames_per_step\n        self.mel_loss_criterion = nn.MSELoss(reduction='none')\n        # self.loss = nn.MSELoss()\n        self.stop_loss_criterion = nn.BCEWithLogitsLoss(reduction='none')   \n\n    def get_mask(self, lengths, max_len=None):\n        # lengths: [B,]\n        if max_len is None:\n            max_len = torch.max(lengths)\n        batch_size = lengths.size(0)\n        seq_range = torch.arange(0, max_len).long()\n        seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len).to(lengths.device)\n        seq_length_expand = lengths.unsqueeze(1).expand_as(seq_range_expand)\n        return (seq_range_expand < seq_length_expand).float()\n\n    def forward(self, mel_pred, mel_pred_postnet, mel_trg, lengths, \n                stop_target, stop_pred):\n        ## process stop_target\n        B = stop_target.size(0)\n        stop_target = stop_target.reshape(B, -1, self.frames_per_step)[:, :, 0]\n        stop_lengths = torch.ceil(lengths.float() / self.frames_per_step).long()\n        stop_mask = self.get_mask(stop_lengths, int(mel_trg.size(1)/self.frames_per_step))\n\n        mel_trg.requires_grad = False\n        # (B, T, 1)\n        mel_mask = self.get_mask(lengths, mel_trg.size(1)).unsqueeze(-1)\n        # (B, T, D)\n        mel_mask = mel_mask.expand_as(mel_trg)\n        mel_loss_pre = (self.mel_loss_criterion(mel_pred, mel_trg) * mel_mask).sum() / mel_mask.sum()\n        mel_loss_post = (self.mel_loss_criterion(mel_pred_postnet, mel_trg) * mel_mask).sum() / mel_mask.sum()\n        \n        mel_loss = mel_loss_pre + mel_loss_post\n\n        # stop token loss\n        stop_loss = torch.sum(self.stop_loss_criterion(stop_pred, stop_target) * stop_mask) / stop_mask.sum()\n        \n        return mel_loss, stop_loss\n", "models/ppg2mel/train/optim.py": "import torch\nimport numpy as np\n\n\nclass Optimizer():\n    def __init__(self, parameters, optimizer, lr, eps, lr_scheduler, \n                **kwargs):\n\n        # Setup torch optimizer\n        self.opt_type = optimizer\n        self.init_lr = lr\n        self.sch_type = lr_scheduler\n        opt = getattr(torch.optim, optimizer)\n        if lr_scheduler == 'warmup':\n            warmup_step = 4000.0\n            init_lr = lr\n            self.lr_scheduler = lambda step: init_lr * warmup_step ** 0.5 * \\\n                np.minimum((step+1)*warmup_step**-1.5, (step+1)**-0.5)\n            self.opt = opt(parameters, lr=1.0)\n        else:\n            self.lr_scheduler = None\n            self.opt = opt(parameters, lr=lr, eps=eps)  # ToDo: 1e-8 better?\n\n    def get_opt_state_dict(self):\n        return self.opt.state_dict()\n\n    def load_opt_state_dict(self, state_dict):\n        self.opt.load_state_dict(state_dict)\n\n    def pre_step(self, step):\n        if self.lr_scheduler is not None:\n            cur_lr = self.lr_scheduler(step)\n            for param_group in self.opt.param_groups:\n                param_group['lr'] = cur_lr\n        else:\n            cur_lr = self.init_lr\n        self.opt.zero_grad()\n        return cur_lr \n \n    def step(self):\n        self.opt.step()\n\n    def create_msg(self):\n        return ['Optim.Info.| Algo. = {}\\t| Lr = {}\\t (schedule = {})'\n                .format(self.opt_type, self.init_lr, self.sch_type)]\n", "models/ppg2mel/train/option.py": "# Default parameters which will be imported by solver\ndefault_hparas = {\n    'GRAD_CLIP': 5.0,          # Grad. clip threshold\n    'PROGRESS_STEP': 100,      # Std. output refresh freq.\n    # Decode steps for objective validation (step = ratio*input_txt_len)\n    'DEV_STEP_RATIO': 1.2,\n    # Number of examples (alignment/text) to show in tensorboard\n    'DEV_N_EXAMPLE': 4,\n    'TB_FLUSH_FREQ': 180       # Update frequency of tensorboard (secs)\n}\n", "models/ppg2mel/train/solver.py": "import os\nimport sys\nimport abc\nimport math\nimport yaml\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom .option import default_hparas\nfrom utils.util import human_format, Timer\n\n\nclass BaseSolver():\n    ''' \n    Prototype Solver for all kinds of tasks\n    Arguments\n        config - yaml-styled config\n        paras  - argparse outcome\n        mode   - \"train\"/\"test\"\n    '''\n\n    def __init__(self, config, paras, mode=\"train\"):\n        # General Settings\n        self.config = config  # load from yaml file\n        self.paras = paras    # command line args  \n        self.mode = mode      # 'train' or 'test'\n        for k, v in default_hparas.items():\n            setattr(self, k, v)\n        self.device = torch.device('cuda') if self.paras.gpu and torch.cuda.is_available() \\\n                    else torch.device('cpu')\n\n        # Name experiment\n        self.exp_name = paras.name\n        if self.exp_name is None:\n            if 'exp_name' in self.config:\n                self.exp_name = self.config.exp_name\n            else:\n                # By default, exp is named after config file\n                self.exp_name = paras.config.split('/')[-1].replace('.yaml', '')\n            if mode == 'train':\n                self.exp_name += '_seed{}'.format(paras.seed)\n                    \n\n        if mode == 'train':\n            # Filepath setup\n            os.makedirs(paras.ckpdir, exist_ok=True)\n            self.ckpdir = os.path.join(paras.ckpdir, self.exp_name)\n            os.makedirs(self.ckpdir, exist_ok=True)\n\n            # Logger settings\n            self.logdir = os.path.join(paras.logdir, self.exp_name)\n            self.log = SummaryWriter(\n                self.logdir, flush_secs=self.TB_FLUSH_FREQ)\n            self.timer = Timer()\n\n            # Hyper-parameters\n            self.step = 0\n            self.valid_step = config.hparas.valid_step\n            self.max_step = config.hparas.max_step\n\n            self.verbose('Exp. name : {}'.format(self.exp_name))\n            self.verbose('Loading data... large corpus may took a while.')\n\n        # elif mode == 'test':\n            # # Output path\n            # os.makedirs(paras.outdir, exist_ok=True)\n            # self.ckpdir = os.path.join(paras.outdir, self.exp_name)\n\n            # Load training config to get acoustic feat and build model\n            # self.src_config = HpsYaml(config.src.config) \n            # self.paras.load = config.src.ckpt\n\n            # self.verbose('Evaluating result of tr. config @ {}'.format(\n                # config.src.config))\n\n    def backward(self, loss):\n        '''\n        Standard backward step with self.timer and debugger\n        Arguments\n            loss - the loss to perform loss.backward()\n        '''\n        self.timer.set()\n        loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            self.model.parameters(), self.GRAD_CLIP)\n        if math.isnan(grad_norm):\n            self.verbose('Error : grad norm is NaN @ step '+str(self.step))\n        else:\n            self.optimizer.step()\n        self.timer.cnt('bw')\n        return grad_norm\n\n    def load_ckpt(self):\n        ''' Load ckpt if --load option is specified '''\n        print(self.paras)\n        if self.paras.load is not None:\n            if self.paras.warm_start:\n                self.verbose(f\"Warm starting model from checkpoint {self.paras.load}.\")\n                ckpt = torch.load(\n                    self.paras.load, map_location=self.device if self.mode == 'train'\n                                                        else 'cpu')\n                model_dict = ckpt['model']\n                if \"ignore_layers\" in self.config.model and len(self.config.model.ignore_layers) > 0:\n                    model_dict = {k:v for k, v in model_dict.items()\n                                  if k not in self.config.model.ignore_layers}\n                    dummy_dict = self.model.state_dict()\n                    dummy_dict.update(model_dict)\n                    model_dict = dummy_dict\n                self.model.load_state_dict(model_dict)\n            else:\n                # Load weights\n                ckpt = torch.load(\n                    self.paras.load, map_location=self.device if self.mode == 'train'\n                                                else 'cpu')\n                self.model.load_state_dict(ckpt['model'])\n\n                # Load task-dependent items\n                if self.mode == 'train':\n                    self.step = ckpt['global_step']\n                    self.optimizer.load_opt_state_dict(ckpt['optimizer'])\n                    self.verbose('Load ckpt from {}, restarting at step {}'.format(\n                        self.paras.load, self.step))\n                else:\n                    for k, v in ckpt.items():\n                        if type(v) is float:\n                            metric, score = k, v\n                    self.model.eval()\n                    self.verbose('Evaluation target = {} (recorded {} = {:.2f} %)'.format(\n                        self.paras.load, metric, score))\n\n    def verbose(self, msg):\n        ''' Verbose function for print information to stdout'''\n        if self.paras.verbose:\n            if type(msg) == list:\n                for m in msg:\n                    print('[INFO]', m.ljust(100))\n            else:\n                print('[INFO]', msg.ljust(100))\n\n    def progress(self, msg):\n        ''' Verbose function for updating progress on stdout (do not include newline) '''\n        if self.paras.verbose:\n            sys.stdout.write(\"\\033[K\")  # Clear line\n            print('[{}] {}'.format(human_format(self.step), msg), end='\\r')\n\n    def write_log(self, log_name, log_dict):\n        '''\n        Write log to TensorBoard\n            log_name  - <str> Name of tensorboard variable \n            log_value - <dict>/<array> Value of variable (e.g. dict of losses), passed if value = None\n        '''\n        if type(log_dict) is dict:\n            log_dict = {key: val for key, val in log_dict.items() if (\n                val is not None and not math.isnan(val))}\n        if log_dict is None:\n            pass\n        elif len(log_dict) > 0:\n            if 'align' in log_name or 'spec' in log_name:\n                img, form = log_dict\n                self.log.add_image(\n                    log_name, img, global_step=self.step, dataformats=form)\n            elif 'text' in log_name or 'hyp' in log_name:\n                self.log.add_text(log_name, log_dict, self.step)\n            else:\n                self.log.add_scalars(log_name, log_dict, self.step)\n\n    def save_checkpoint(self, f_name, metric, score, show_msg=True):\n        '''' \n        Ckpt saver\n            f_name - <str> the name of ckpt file (w/o prefix) to store, overwrite if existed\n            score  - <float> The value of metric used to evaluate model\n        '''\n        ckpt_path = os.path.join(self.ckpdir, f_name)\n        full_dict = {\n            \"model\": self.model.state_dict(),\n            \"optimizer\": self.optimizer.get_opt_state_dict(),\n            \"global_step\": self.step,\n            metric: score\n        }\n\n        torch.save(full_dict, ckpt_path)\n        if show_msg:\n            self.verbose(\"Saved checkpoint (step = {}, {} = {:.2f}) and status @ {}\".\n                         format(human_format(self.step), metric, score, ckpt_path))\n\n\n    # ----------------------------------- Abtract Methods ------------------------------------------ #\n    @abc.abstractmethod\n    def load_data(self):\n        '''\n        Called by main to load all data\n        After this call, data related attributes should be setup (e.g. self.tr_set, self.dev_set)\n        No return value\n        '''\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def set_model(self):\n        '''\n        Called by main to set models\n        After this call, model related attributes should be setup (e.g. self.l2_loss)\n        The followings MUST be setup\n            - self.model (torch.nn.Module)\n            - self.optimizer (src.Optimizer),\n                init. w/ self.optimizer = src.Optimizer(self.model.parameters(),**self.config['hparas'])\n        Loading pre-trained model should also be performed here \n        No return value\n        '''\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def exec(self):\n        '''\n        Called by main to execute training/inference\n        '''\n        raise NotImplementedError\n", "models/ppg2mel/train/train_linglf02mel_seq2seq_oneshotvc.py": "import os, sys\n# sys.path.append('/home/shaunxliu/projects/nnsp')\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport torch\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom .solver import BaseSolver\nfrom utils.data_load import OneshotVcDataset, MultiSpkVcCollate\n# from src.rnn_ppg2mel import BiRnnPpg2MelModel\n# from src.mel_decoder_mol_encAddlf0 import MelDecoderMOL\nfrom .loss import MaskedMSELoss\nfrom .optim import Optimizer\nfrom utils.util import human_format\nfrom models.ppg2mel import MelDecoderMOLv2\n\n\nclass Solver(BaseSolver):\n    \"\"\"Customized Solver.\"\"\"\n    def __init__(self, config, paras, mode):\n        super().__init__(config, paras, mode)\n        self.num_att_plots = 5\n        self.att_ws_dir = f\"{self.logdir}/att_ws\"\n        os.makedirs(self.att_ws_dir, exist_ok=True)\n        self.best_loss = np.inf\n\n    def fetch_data(self, data):\n        \"\"\"Move data to device\"\"\"\n        data = [i.to(self.device) for i in data]\n        return data\n\n    def load_data(self):\n        \"\"\" Load data for training/validation/plotting.\"\"\"\n        train_dataset = OneshotVcDataset(\n            meta_file=self.config.data.train_fid_list,\n            vctk_ppg_dir=self.config.data.vctk_ppg_dir,\n            libri_ppg_dir=self.config.data.libri_ppg_dir,\n            vctk_f0_dir=self.config.data.vctk_f0_dir,\n            libri_f0_dir=self.config.data.libri_f0_dir,\n            vctk_wav_dir=self.config.data.vctk_wav_dir,\n            libri_wav_dir=self.config.data.libri_wav_dir,\n            vctk_spk_dvec_dir=self.config.data.vctk_spk_dvec_dir,\n            libri_spk_dvec_dir=self.config.data.libri_spk_dvec_dir,\n            ppg_file_ext=self.config.data.ppg_file_ext,\n            min_max_norm_mel=self.config.data.min_max_norm_mel,\n            mel_min=self.config.data.mel_min,\n            mel_max=self.config.data.mel_max,\n        )\n        dev_dataset = OneshotVcDataset(\n            meta_file=self.config.data.dev_fid_list,\n            vctk_ppg_dir=self.config.data.vctk_ppg_dir,\n            libri_ppg_dir=self.config.data.libri_ppg_dir,\n            vctk_f0_dir=self.config.data.vctk_f0_dir,\n            libri_f0_dir=self.config.data.libri_f0_dir,\n            vctk_wav_dir=self.config.data.vctk_wav_dir,\n            libri_wav_dir=self.config.data.libri_wav_dir,\n            vctk_spk_dvec_dir=self.config.data.vctk_spk_dvec_dir,\n            libri_spk_dvec_dir=self.config.data.libri_spk_dvec_dir,\n            ppg_file_ext=self.config.data.ppg_file_ext,\n            min_max_norm_mel=self.config.data.min_max_norm_mel,\n            mel_min=self.config.data.mel_min,\n            mel_max=self.config.data.mel_max,\n        )\n        self.train_dataloader = DataLoader(\n            train_dataset,\n            num_workers=self.paras.njobs,\n            shuffle=True,\n            batch_size=self.config.hparas.batch_size,\n            pin_memory=False,\n            drop_last=True,\n            collate_fn=MultiSpkVcCollate(self.config.model.frames_per_step,\n                                        use_spk_dvec=True),\n        )\n        self.dev_dataloader = DataLoader(\n            dev_dataset,\n            num_workers=self.paras.njobs,\n            shuffle=False,\n            batch_size=self.config.hparas.batch_size,\n            pin_memory=False,\n            drop_last=False,\n            collate_fn=MultiSpkVcCollate(self.config.model.frames_per_step,\n                                         use_spk_dvec=True),\n        )\n        self.plot_dataloader = DataLoader(\n            dev_dataset,\n            num_workers=self.paras.njobs,\n            shuffle=False,\n            batch_size=1,\n            pin_memory=False,\n            drop_last=False,\n            collate_fn=MultiSpkVcCollate(self.config.model.frames_per_step,\n                                         use_spk_dvec=True,\n                                         give_uttids=True),\n        )\n        msg = \"Have prepared training set and dev set.\"\n        self.verbose(msg)\n    \n    def load_pretrained_params(self):\n        print(\"Load pretrained model from: \", self.config.data.pretrain_model_file)\n        ignore_layer_prefixes = [\"speaker_embedding_table\"]\n        pretrain_model_file = self.config.data.pretrain_model_file\n        pretrain_ckpt = torch.load(\n            pretrain_model_file, map_location=self.device\n        )[\"model\"]\n        model_dict = self.model.state_dict()\n        print(self.model)\n        \n        # 1. filter out unnecessrary keys\n        for prefix in ignore_layer_prefixes:\n            pretrain_ckpt = {k : v \n                             for k, v in pretrain_ckpt.items() if not k.startswith(prefix) \n                            }\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(pretrain_ckpt)\n\n        # 3. load the new state dict\n        self.model.load_state_dict(model_dict)\n\n    def set_model(self):\n        \"\"\"Setup model and optimizer\"\"\"\n        # Model\n        print(\"[INFO] Model name: \", self.config[\"model_name\"])\n        self.model = MelDecoderMOLv2(\n            **self.config[\"model\"]\n        ).to(self.device)\n        # self.load_pretrained_params()\n\n        # model_params = [{'params': self.model.spk_embedding.weight}]\n        model_params = [{'params': self.model.parameters()}]\n        \n        # Loss criterion\n        self.loss_criterion = MaskedMSELoss(self.config.model.frames_per_step)\n\n        # Optimizer\n        self.optimizer = Optimizer(model_params, **self.config[\"hparas\"])\n        self.verbose(self.optimizer.create_msg())\n\n        # Automatically load pre-trained model if self.paras.load is given\n        self.load_ckpt()\n\n    def exec(self):\n        self.verbose(\"Total training steps {}.\".format(\n            human_format(self.max_step)))\n\n        mel_loss = None\n        n_epochs = 0\n        # Set as current time\n        self.timer.set()\n        \n        while self.step < self.max_step:\n            for data in self.train_dataloader:\n                # Pre-step: updata lr_rate and do zero_grad\n                lr_rate = self.optimizer.pre_step(self.step)\n                total_loss = 0\n                # data to device\n                ppgs, lf0_uvs, mels, in_lengths, \\\n                    out_lengths, spk_ids, stop_tokens = self.fetch_data(data)\n                self.timer.cnt(\"rd\")\n                mel_outputs, mel_outputs_postnet, predicted_stop = self.model(\n                    ppgs,\n                    in_lengths,\n                    mels,\n                    out_lengths,\n                    lf0_uvs,\n                    spk_ids\n                ) \n                mel_loss, stop_loss = self.loss_criterion(\n                    mel_outputs,\n                    mel_outputs_postnet,\n                    mels,\n                    out_lengths,\n                    stop_tokens,\n                    predicted_stop\n                )\n                loss = mel_loss + stop_loss\n\n                self.timer.cnt(\"fw\")\n\n                # Back-prop\n                grad_norm = self.backward(loss)\n                self.step += 1\n\n                # Logger\n                if (self.step == 1) or (self.step % self.PROGRESS_STEP == 0):\n                    self.progress(\"Tr|loss:{:.4f},mel-loss:{:.4f},stop-loss:{:.4f}|Grad.Norm-{:.2f}|{}\"\n                                  .format(loss.cpu().item(), mel_loss.cpu().item(),\n                                    stop_loss.cpu().item(), grad_norm, self.timer.show()))\n                    self.write_log('loss', {'tr/loss': loss,\n                                            'tr/mel-loss': mel_loss,\n                                            'tr/stop-loss': stop_loss})\n\n                # Validation\n                if (self.step == 1) or (self.step % self.valid_step == 0):\n                    self.validate()\n\n                # End of step\n                # https://github.com/pytorch/pytorch/issues/13246#issuecomment-529185354\n                torch.cuda.empty_cache()\n                self.timer.set()\n                if self.step > self.max_step:\n                    break\n            n_epochs += 1\n        self.log.close()\n\n    def validate(self):\n        self.model.eval()\n        dev_loss, dev_mel_loss, dev_stop_loss = 0.0, 0.0, 0.0\n\n        for i, data in enumerate(self.dev_dataloader):\n            self.progress('Valid step - {}/{}'.format(i+1, len(self.dev_dataloader)))\n            # Fetch data\n            ppgs, lf0_uvs, mels, in_lengths, \\\n                out_lengths, spk_ids, stop_tokens = self.fetch_data(data)\n            with torch.no_grad():\n                mel_outputs, mel_outputs_postnet, predicted_stop = self.model(\n                    ppgs,\n                    in_lengths,\n                    mels,\n                    out_lengths,\n                    lf0_uvs,\n                    spk_ids\n                ) \n                mel_loss, stop_loss = self.loss_criterion(\n                    mel_outputs,\n                    mel_outputs_postnet,\n                    mels,\n                    out_lengths,\n                    stop_tokens,\n                    predicted_stop\n                )\n                loss = mel_loss + stop_loss\n\n                dev_loss += loss.cpu().item()\n                dev_mel_loss += mel_loss.cpu().item()\n                dev_stop_loss += stop_loss.cpu().item()\n\n        dev_loss = dev_loss / (i + 1)\n        dev_mel_loss = dev_mel_loss / (i + 1)\n        dev_stop_loss = dev_stop_loss / (i + 1)\n        self.save_checkpoint(f'step_{self.step}.pth', 'loss', dev_loss, show_msg=False)\n        if dev_loss < self.best_loss:\n            self.best_loss = dev_loss\n            self.save_checkpoint(f'best_loss_step_{self.step}.pth', 'loss', dev_loss)\n        self.write_log('loss', {'dv/loss': dev_loss,\n                                'dv/mel-loss': dev_mel_loss,\n                                'dv/stop-loss': dev_stop_loss})\n\n        # plot attention\n        for i, data in enumerate(self.plot_dataloader):\n            if i == self.num_att_plots:\n                break\n            # Fetch data\n            ppgs, lf0_uvs, mels, in_lengths, \\\n                out_lengths, spk_ids, stop_tokens = self.fetch_data(data[:-1])\n            fid = data[-1][0]\n            with torch.no_grad():\n                _, _, _, att_ws = self.model(\n                    ppgs,\n                    in_lengths,\n                    mels,\n                    out_lengths,\n                    lf0_uvs,\n                    spk_ids,\n                    output_att_ws=True\n                )\n                att_ws = att_ws.squeeze(0).cpu().numpy()\n                att_ws = att_ws[None]\n                w, h = plt.figaspect(1.0 / len(att_ws))\n                fig = plt.Figure(figsize=(w * 1.3, h * 1.3))\n                axes = fig.subplots(1, len(att_ws))\n                if len(att_ws) == 1:\n                    axes = [axes]\n\n                for ax, aw in zip(axes, att_ws):\n                    ax.imshow(aw.astype(np.float32), aspect=\"auto\")\n                    ax.set_title(f\"{fid}\")\n                    ax.set_xlabel(\"Input\")\n                    ax.set_ylabel(\"Output\")\n                    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n                    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n                fig_name = f\"{self.att_ws_dir}/{fid}_step{self.step}.png\"\n                fig.savefig(fig_name)\n                \n        # Resume training\n        self.model.train()\n\n", "models/ppg2mel/train/__init__.py": "#", "models/encoder/preprocess.py": "from multiprocess.pool import ThreadPool\nfrom models.encoder.params_data import *\nfrom models.encoder.config import librispeech_datasets, anglophone_nationalites\nfrom datetime import datetime\nfrom models.encoder import audio\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\n\n\nclass DatasetLog:\n    \"\"\"\n    Registers metadata about the dataset in a text file.\n    \"\"\"\n    def __init__(self, root, name):\n        self.text_file = open(Path(root, \"Log_%s.txt\" % name.replace(\"/\", \"_\")), \"w\")\n        self.sample_data = dict()\n        \n        start_time = str(datetime.now().strftime(\"%A %d %B %Y at %H:%M\"))\n        self.write_line(\"Creating dataset %s on %s\" % (name, start_time))\n        self.write_line(\"-----\")\n        self._log_params()\n        \n    def _log_params(self):\n        from models.encoder import params_data\n        self.write_line(\"Parameter values:\")\n        for param_name in (p for p in dir(params_data) if not p.startswith(\"__\")):\n            value = getattr(params_data, param_name)\n            self.write_line(\"\\t%s: %s\" % (param_name, value))\n        self.write_line(\"-----\")\n    \n    def write_line(self, line):\n        self.text_file.write(\"%s\\n\" % line)\n        \n    def add_sample(self, **kwargs):\n        for param_name, value in kwargs.items():\n            if not param_name in self.sample_data:\n                self.sample_data[param_name] = []\n            self.sample_data[param_name].append(value)\n            \n    def finalize(self):\n        self.write_line(\"Statistics:\")\n        for param_name, values in self.sample_data.items():\n            self.write_line(\"\\t%s:\" % param_name)\n            self.write_line(\"\\t\\tmin %.3f, max %.3f\" % (np.min(values), np.max(values)))\n            self.write_line(\"\\t\\tmean %.3f, median %.3f\" % (np.mean(values), np.median(values)))\n        self.write_line(\"-----\")\n        end_time = str(datetime.now().strftime(\"%A %d %B %Y at %H:%M\"))\n        self.write_line(\"Finished on %s\" % end_time)\n        self.text_file.close()\n       \n        \ndef _init_preprocess_dataset(dataset_name, datasets_root, out_dir) -> (Path, DatasetLog):\n    dataset_root = datasets_root.joinpath(dataset_name)\n    if not dataset_root.exists():\n        print(\"Couldn\\'t find %s, skipping this dataset.\" % dataset_root)\n        return None, None\n    return dataset_root, DatasetLog(out_dir, dataset_name)\n\n\ndef _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, extension,\n                             skip_existing, logger):\n    print(\"%s: Preprocessing data for %d speakers.\" % (dataset_name, len(speaker_dirs)))\n    \n    # Function to preprocess utterances for one speaker\n    def preprocess_speaker(speaker_dir: Path):\n        # Give a name to the speaker that includes its dataset\n        speaker_name = \"_\".join(speaker_dir.relative_to(datasets_root).parts)\n        \n        # Create an output directory with that name, as well as a txt file containing a \n        # reference to each source file.\n        speaker_out_dir = out_dir.joinpath(speaker_name)\n        speaker_out_dir.mkdir(exist_ok=True)\n        sources_fpath = speaker_out_dir.joinpath(\"_sources.txt\")\n        \n        # There's a possibility that the preprocessing was interrupted earlier, check if \n        # there already is a sources file.\n        if sources_fpath.exists():\n            try:\n                with sources_fpath.open(\"r\") as sources_file:\n                    existing_fnames = {line.split(\",\")[0] for line in sources_file}\n            except:\n                existing_fnames = {}\n        else:\n            existing_fnames = {}\n        \n        # Gather all audio files for that speaker recursively\n        sources_file = sources_fpath.open(\"a\" if skip_existing else \"w\")\n        for in_fpath in speaker_dir.glob(\"**/*.%s\" % extension):\n            # Check if the target output file already exists\n            out_fname = \"_\".join(in_fpath.relative_to(speaker_dir).parts)\n            out_fname = out_fname.replace(\".%s\" % extension, \".npy\")\n            if skip_existing and out_fname in existing_fnames:\n                continue\n                \n            # Load and preprocess the waveform\n            wav = audio.preprocess_wav(in_fpath)\n            if len(wav) == 0:\n                continue\n            \n            # Create the mel spectrogram, discard those that are too short\n            frames = audio.wav_to_mel_spectrogram(wav)\n            if len(frames) < partials_n_frames:\n                continue\n            \n            out_fpath = speaker_out_dir.joinpath(out_fname)\n            np.save(out_fpath, frames)\n            logger.add_sample(duration=len(wav) / sampling_rate)\n            sources_file.write(\"%s,%s\\n\" % (out_fname, in_fpath))\n        \n        sources_file.close()\n    \n    # Process the utterances for each speaker\n    with ThreadPool(8) as pool:\n        list(tqdm(pool.imap(preprocess_speaker, speaker_dirs), dataset_name, len(speaker_dirs),\n                  unit=\"speakers\"))\n    logger.finalize()\n    print(\"Done preprocessing %s.\\n\" % dataset_name)\n\ndef preprocess_aidatatang_200zh(datasets_root: Path, out_dir: Path, skip_existing=False):\n    dataset_name = \"aidatatang_200zh\"\n    dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)\n    if not dataset_root:\n        return \n    # Preprocess all speakers\n    speaker_dirs = list(dataset_root.joinpath(\"corpus\", \"train\").glob(\"*\"))\n    _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, \"wav\",\n                                skip_existing, logger)\n\ndef preprocess_librispeech(datasets_root: Path, out_dir: Path, skip_existing=False):\n    for dataset_name in librispeech_datasets[\"train\"][\"other\"]:\n        # Initialize the preprocessing\n        dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)\n        if not dataset_root:\n            return \n        \n        # Preprocess all speakers\n        speaker_dirs = list(dataset_root.glob(\"*\"))\n        _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, \"flac\",\n                                 skip_existing, logger)\n\n\ndef preprocess_voxceleb1(datasets_root: Path, out_dir: Path, skip_existing=False):\n    # Initialize the preprocessing\n    dataset_name = \"VoxCeleb1\"\n    dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)\n    if not dataset_root:\n        return\n\n    # Get the contents of the meta file\n    with dataset_root.joinpath(\"vox1_meta.csv\").open(\"r\") as metafile:\n        metadata = [line.split(\"\\t\") for line in metafile][1:]\n    \n    # Select the ID and the nationality, filter out non-anglophone speakers\n    nationalities = {line[0]: line[3] for line in metadata}\n    keep_speaker_ids = [speaker_id for speaker_id, nationality in nationalities.items() if \n                        nationality.lower() in anglophone_nationalites]\n    print(\"VoxCeleb1: using samples from %d (presumed anglophone) speakers out of %d.\" % \n          (len(keep_speaker_ids), len(nationalities)))\n    \n    # Get the speaker directories for anglophone speakers only\n    speaker_dirs = dataset_root.joinpath(\"wav\").glob(\"*\")\n    speaker_dirs = [speaker_dir for speaker_dir in speaker_dirs if\n                    speaker_dir.name in keep_speaker_ids]\n    print(\"VoxCeleb1: found %d anglophone speakers on the disk, %d missing (this is normal).\" % \n          (len(speaker_dirs), len(keep_speaker_ids) - len(speaker_dirs)))\n\n    # Preprocess all speakers\n    _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, \"wav\",\n                             skip_existing, logger)\n\n\ndef preprocess_voxceleb2(datasets_root: Path, out_dir: Path, skip_existing=False):\n    # Initialize the preprocessing\n    dataset_name = \"VoxCeleb2\"\n    dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)\n    if not dataset_root:\n        return\n    \n    # Get the speaker directories\n    # Preprocess all speakers\n    speaker_dirs = list(dataset_root.joinpath(\"dev\", \"aac\").glob(\"*\"))\n    _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, \"m4a\",\n                             skip_existing, logger)\n", "models/encoder/model.py": "from models.encoder.params_model import *\nfrom models.encoder.params_data import *\nfrom scipy.interpolate import interp1d\nfrom sklearn.metrics import roc_curve\nfrom torch.nn.utils import clip_grad_norm_\nfrom scipy.optimize import brentq\nfrom torch import nn\nimport numpy as np\nimport torch\n\n\nclass SpeakerEncoder(nn.Module):\n    def __init__(self, device, loss_device):\n        super().__init__()\n        self.loss_device = loss_device\n        \n        # Network defition\n        self.lstm = nn.LSTM(input_size=mel_n_channels,\n                            hidden_size=model_hidden_size, \n                            num_layers=model_num_layers, \n                            batch_first=True).to(device)\n        self.linear = nn.Linear(in_features=model_hidden_size, \n                                out_features=model_embedding_size).to(device)\n        self.relu = torch.nn.ReLU().to(device)\n        \n        # Cosine similarity scaling (with fixed initial parameter values)\n        self.similarity_weight = nn.Parameter(torch.tensor([10.])).to(loss_device)\n        self.similarity_bias = nn.Parameter(torch.tensor([-5.])).to(loss_device)\n\n        # Loss\n        self.loss_fn = nn.CrossEntropyLoss().to(loss_device)\n        \n    def do_gradient_ops(self):\n        # Gradient scale\n        self.similarity_weight.grad *= 0.01\n        self.similarity_bias.grad *= 0.01\n            \n        # Gradient clipping\n        clip_grad_norm_(self.parameters(), 3, norm_type=2)\n    \n    def forward(self, utterances, hidden_init=None):\n        \"\"\"\n        Computes the embeddings of a batch of utterance spectrograms.\n        \n        :param utterances: batch of mel-scale filterbanks of same duration as a tensor of shape \n        (batch_size, n_frames, n_channels) \n        :param hidden_init: initial hidden state of the LSTM as a tensor of shape (num_layers, \n        batch_size, hidden_size). Will default to a tensor of zeros if None.\n        :return: the embeddings as a tensor of shape (batch_size, embedding_size)\n        \"\"\"\n        # Pass the input through the LSTM layers and retrieve all outputs, the final hidden state\n        # and the final cell state.\n        out, (hidden, cell) = self.lstm(utterances, hidden_init)\n        \n        # We take only the hidden state of the last layer\n        embeds_raw = self.relu(self.linear(hidden[-1]))\n        \n        # L2-normalize it\n        embeds = embeds_raw / (torch.norm(embeds_raw, dim=1, keepdim=True) + 1e-5)        \n\n        return embeds\n    \n    def similarity_matrix(self, embeds):\n        \"\"\"\n        Computes the similarity matrix according the section 2.1 of GE2E.\n\n        :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n        utterances_per_speaker, embedding_size)\n        :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n        utterances_per_speaker, speakers_per_batch)\n        \"\"\"\n        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n        \n        # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n        centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n        # Exclusive centroids (1 per utterance)\n        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n        centroids_excl /= (utterances_per_speaker - 1)\n        centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n        # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n        # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n        # We vectorize the computation for efficiency.\n        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n                                 speakers_per_batch).to(self.loss_device)\n        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n        for j in range(speakers_per_batch):\n            mask = np.where(mask_matrix[j])[0]\n            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n        \n        ## Even more vectorized version (slower maybe because of transpose)\n        # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_speaker\n        #                           ).to(self.loss_device)\n        # eye = np.eye(speakers_per_batch, dtype=np.int)\n        # mask = np.where(1 - eye)\n        # sim_matrix2[mask] = (embeds[mask[0]] * centroids_incl[mask[1]]).sum(dim=2)\n        # mask = np.where(eye)\n        # sim_matrix2[mask] = (embeds * centroids_excl).sum(dim=2)\n        # sim_matrix2 = sim_matrix2.transpose(1, 2)\n        \n        sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias\n        return sim_matrix\n    \n    def loss(self, embeds):\n        \"\"\"\n        Computes the softmax loss according the section 2.1 of GE2E.\n        \n        :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n        utterances_per_speaker, embedding_size)\n        :return: the loss and the EER for this batch of embeddings.\n        \"\"\"\n        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n        \n        # Loss\n        sim_matrix = self.similarity_matrix(embeds)\n        sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker, \n                                         speakers_per_batch))\n        ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n        target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n        loss = self.loss_fn(sim_matrix, target)\n        \n        # EER (not backpropagated)\n        with torch.no_grad():\n            inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int)[0]\n            labels = np.array([inv_argmax(i) for i in ground_truth])\n            preds = sim_matrix.detach().cpu().numpy()\n\n            # Snippet from https://yangcha.github.io/EER-ROC/\n            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())           \n            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n            \n        return loss, eer\n", "models/encoder/train.py": "from models.encoder.visualizations import Visualizations\nfrom models.encoder.data_objects import SpeakerVerificationDataLoader, SpeakerVerificationDataset\nfrom models.encoder.params_model import *\nfrom models.encoder.model import SpeakerEncoder\nfrom utils.profiler import Profiler\nfrom pathlib import Path\nimport torch\n\ndef sync(device: torch.device):\n    # For correct profiling (cuda operations are async)\n    if device.type == \"cuda\":\n        torch.cuda.synchronize(device)\n    \n\ndef train(run_id: str, clean_data_root: Path, models_dir: Path, umap_every: int, save_every: int,\n          backup_every: int, vis_every: int, force_restart: bool, visdom_server: str,\n          no_visdom: bool):\n    # Create a dataset and a dataloader\n    dataset = SpeakerVerificationDataset(clean_data_root)\n    loader = SpeakerVerificationDataLoader(\n        dataset,\n        speakers_per_batch,\n        utterances_per_speaker,\n        num_workers=8,\n    )\n    \n    # Setup the device on which to run the forward pass and the loss. These can be different, \n    # because the forward pass is faster on the GPU whereas the loss is often (depending on your\n    # hyperparameters) faster on the CPU.\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # FIXME: currently, the gradient is None if loss_device is cuda\n    loss_device = torch.device(\"cpu\")\n    \n    # Create the model and the optimizer\n    model = SpeakerEncoder(device, loss_device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_init)\n    init_step = 1\n    \n    # Configure file path for the model\n    state_fpath = models_dir.joinpath(run_id + \".pt\")\n    backup_dir = models_dir.joinpath(run_id + \"_backups\")\n\n    # Load any existing model\n    if not force_restart:\n        if state_fpath.exists():\n            print(\"Found existing model \\\"%s\\\", loading it and resuming training.\" % run_id)\n            checkpoint = torch.load(state_fpath)\n            init_step = checkpoint[\"step\"]\n            model.load_state_dict(checkpoint[\"model_state\"])\n            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n            optimizer.param_groups[0][\"lr\"] = learning_rate_init\n        else:\n            print(\"No model \\\"%s\\\" found, starting training from scratch.\" % run_id)\n    else:\n        print(\"Starting the training from scratch.\")\n    model.train()\n    \n    # Initialize the visualization environment\n    vis = Visualizations(run_id, vis_every, server=visdom_server, disabled=no_visdom)\n    vis.log_dataset(dataset)\n    vis.log_params()\n    device_name = str(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n    vis.log_implementation({\"Device\": device_name})\n    \n    # Training loop\n    profiler = Profiler(summarize_every=10, disabled=False)\n    for step, speaker_batch in enumerate(loader, init_step):\n        profiler.tick(\"Blocking, waiting for batch (threaded)\")\n        \n        # Forward pass\n        inputs = torch.from_numpy(speaker_batch.data).to(device)\n        sync(device)\n        profiler.tick(\"Data to %s\" % device)\n        embeds = model(inputs)\n        sync(device)\n        profiler.tick(\"Forward pass\")\n        embeds_loss = embeds.view((speakers_per_batch, utterances_per_speaker, -1)).to(loss_device)\n        loss, eer = model.loss(embeds_loss)\n        sync(loss_device)\n        profiler.tick(\"Loss\")\n\n        # Backward pass\n        model.zero_grad()\n        loss.backward()\n        profiler.tick(\"Backward pass\")\n        model.do_gradient_ops()\n        optimizer.step()\n        profiler.tick(\"Parameter update\")\n        \n        # Update visualizations\n        # learning_rate = optimizer.param_groups[0][\"lr\"]\n        vis.update(loss.item(), eer, step)\n        \n        # Draw projections and save them to the backup folder\n        if umap_every != 0 and step % umap_every == 0:\n            print(\"Drawing and saving projections (step %d)\" % step)\n            backup_dir.mkdir(exist_ok=True)\n            projection_fpath = backup_dir.joinpath(\"%s_umap_%06d.png\" % (run_id, step))\n            embeds = embeds.detach().cpu().numpy()\n            vis.draw_projections(embeds, utterances_per_speaker, step, projection_fpath)\n            vis.save()\n\n        # Overwrite the latest version of the model\n        if save_every != 0 and step % save_every == 0:\n            print(\"Saving the model (step %d)\" % step)\n            torch.save({\n                \"step\": step + 1,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            }, state_fpath)\n            \n        # Make a backup\n        if backup_every != 0 and step % backup_every == 0:\n            print(\"Making a backup (step %d)\" % step)\n            backup_dir.mkdir(exist_ok=True)\n            backup_fpath = backup_dir.joinpath(\"%s_bak_%06d.pt\" % (run_id, step))\n            torch.save({\n                \"step\": step + 1,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            }, backup_fpath)\n            \n        profiler.tick(\"Extras (visualizations, saving)\")\n", "models/encoder/config.py": "librispeech_datasets = {\n    \"train\": {\n        \"clean\": [\"LibriSpeech/train-clean-100\", \"LibriSpeech/train-clean-360\"],\n        \"other\": [\"LibriSpeech/train-other-500\"]\n    },\n    \"test\": {\n        \"clean\": [\"LibriSpeech/test-clean\"],\n        \"other\": [\"LibriSpeech/test-other\"]\n    },\n    \"dev\": {\n        \"clean\": [\"LibriSpeech/dev-clean\"],\n        \"other\": [\"LibriSpeech/dev-other\"]\n    },\n}\nlibritts_datasets = {\n    \"train\": {\n        \"clean\": [\"LibriTTS/train-clean-100\", \"LibriTTS/train-clean-360\"],\n        \"other\": [\"LibriTTS/train-other-500\"]\n    },\n    \"test\": {\n        \"clean\": [\"LibriTTS/test-clean\"],\n        \"other\": [\"LibriTTS/test-other\"]\n    },\n    \"dev\": {\n        \"clean\": [\"LibriTTS/dev-clean\"],\n        \"other\": [\"LibriTTS/dev-other\"]\n    },\n}\nvoxceleb_datasets = {\n    \"voxceleb1\" : {\n        \"train\": [\"VoxCeleb1/wav\"],\n        \"test\": [\"VoxCeleb1/test_wav\"]\n    },\n    \"voxceleb2\" : {\n        \"train\": [\"VoxCeleb2/dev/aac\"],\n        \"test\": [\"VoxCeleb2/test_wav\"]\n    }\n}\n\nother_datasets = [\n    \"LJSpeech-1.1\",\n    \"VCTK-Corpus/wav48\",\n]\n\nanglophone_nationalites = [\"australia\", \"canada\", \"ireland\", \"uk\", \"usa\"]\n", "models/encoder/visualizations.py": "from models.encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataset\nfrom datetime import datetime\nfrom time import perf_counter as timer\nimport matplotlib.pyplot as plt\nimport numpy as np\n# import webbrowser\nimport visdom\nimport umap\n\ncolormap = np.array([\n    [76, 255, 0],\n    [0, 127, 70],\n    [255, 0, 0],\n    [255, 217, 38],\n    [0, 135, 255],\n    [165, 0, 165],\n    [255, 167, 255],\n    [0, 255, 255],\n    [255, 96, 38],\n    [142, 76, 0],\n    [33, 0, 127],\n    [0, 0, 0],\n    [183, 183, 183],\n], dtype=float) / 255 \n\n\nclass Visualizations:\n    def __init__(self, env_name=None, update_every=10, server=\"http://localhost\", disabled=False):\n        # Tracking data\n        self.last_update_timestamp = timer()\n        self.update_every = update_every\n        self.step_times = []\n        self.losses = []\n        self.eers = []\n        print(\"Updating the visualizations every %d steps.\" % update_every)\n        \n        # If visdom is disabled TODO: use a better paradigm for that\n        self.disabled = disabled    \n        if self.disabled:\n            return \n        \n        # Set the environment name\n        now = str(datetime.now().strftime(\"%d-%m %Hh%M\"))\n        if env_name is None:\n            self.env_name = now\n        else:\n            self.env_name = \"%s (%s)\" % (env_name, now)\n        \n        # Connect to visdom and open the corresponding window in the browser\n        try:\n            self.vis = visdom.Visdom(server, env=self.env_name, raise_exceptions=True)\n        except ConnectionError:\n            raise Exception(\"No visdom server detected. Run the command \\\"visdom\\\" in your CLI to \"\n                            \"start it.\")\n        # webbrowser.open(\"http://localhost:8097/env/\" + self.env_name)\n        \n        # Create the windows\n        self.loss_win = None\n        self.eer_win = None\n        # self.lr_win = None\n        self.implementation_win = None\n        self.projection_win = None\n        self.implementation_string = \"\"\n        \n    def log_params(self):\n        if self.disabled:\n            return \n        from models.encoder import params_data\n        from models.encoder import params_model\n        param_string = \"<b>Model parameters</b>:<br>\"\n        for param_name in (p for p in dir(params_model) if not p.startswith(\"__\")):\n            value = getattr(params_model, param_name)\n            param_string += \"\\t%s: %s<br>\" % (param_name, value)\n        param_string += \"<b>Data parameters</b>:<br>\"\n        for param_name in (p for p in dir(params_data) if not p.startswith(\"__\")):\n            value = getattr(params_data, param_name)\n            param_string += \"\\t%s: %s<br>\" % (param_name, value)\n        self.vis.text(param_string, opts={\"title\": \"Parameters\"})\n        \n    def log_dataset(self, dataset: SpeakerVerificationDataset):\n        if self.disabled:\n            return \n        dataset_string = \"\"\n        dataset_string += \"<b>Speakers</b>: %s\\n\" % len(dataset.speakers)\n        dataset_string += \"\\n\" + dataset.get_logs()\n        dataset_string = dataset_string.replace(\"\\n\", \"<br>\")\n        self.vis.text(dataset_string, opts={\"title\": \"Dataset\"})\n        \n    def log_implementation(self, params):\n        if self.disabled:\n            return \n        implementation_string = \"\"\n        for param, value in params.items():\n            implementation_string += \"<b>%s</b>: %s\\n\" % (param, value)\n            implementation_string = implementation_string.replace(\"\\n\", \"<br>\")\n        self.implementation_string = implementation_string\n        self.implementation_win = self.vis.text(\n            implementation_string, \n            opts={\"title\": \"Training implementation\"}\n        )\n\n    def update(self, loss, eer, step):\n        # Update the tracking data\n        now = timer()\n        self.step_times.append(1000 * (now - self.last_update_timestamp))\n        self.last_update_timestamp = now\n        self.losses.append(loss)\n        self.eers.append(eer)\n        print(\".\", end=\"\")\n        \n        # Update the plots every <update_every> steps\n        if step % self.update_every != 0:\n            return\n        time_string = \"Step time:  mean: %5dms  std: %5dms\" % \\\n                      (int(np.mean(self.step_times)), int(np.std(self.step_times)))\n        print(\"\\nStep %6d   Loss: %.4f   EER: %.4f   %s\" %\n              (step, np.mean(self.losses), np.mean(self.eers), time_string))\n        if not self.disabled:\n            self.loss_win = self.vis.line(\n                [np.mean(self.losses)],\n                [step],\n                win=self.loss_win,\n                update=\"append\" if self.loss_win else None,\n                opts=dict(\n                    legend=[\"Avg. loss\"],\n                    xlabel=\"Step\",\n                    ylabel=\"Loss\",\n                    title=\"Loss\",\n                )\n            )\n            self.eer_win = self.vis.line(\n                [np.mean(self.eers)],\n                [step],\n                win=self.eer_win,\n                update=\"append\" if self.eer_win else None,\n                opts=dict(\n                    legend=[\"Avg. EER\"],\n                    xlabel=\"Step\",\n                    ylabel=\"EER\",\n                    title=\"Equal error rate\"\n                )\n            )\n            if self.implementation_win is not None:\n                self.vis.text(\n                    self.implementation_string + (\"<b>%s</b>\" % time_string), \n                    win=self.implementation_win,\n                    opts={\"title\": \"Training implementation\"},\n                )\n\n        # Reset the tracking\n        self.losses.clear()\n        self.eers.clear()\n        self.step_times.clear()\n        \n    def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None,\n                         max_speakers=10):\n        max_speakers = min(max_speakers, len(colormap))\n        embeds = embeds[:max_speakers * utterances_per_speaker]\n        \n        n_speakers = len(embeds) // utterances_per_speaker\n        ground_truth = np.repeat(np.arange(n_speakers), utterances_per_speaker)\n        colors = [colormap[i] for i in ground_truth]\n        \n        reducer = umap.UMAP()\n        projected = reducer.fit_transform(embeds)\n        plt.scatter(projected[:, 0], projected[:, 1], c=colors)\n        plt.gca().set_aspect(\"equal\", \"datalim\")\n        plt.title(\"UMAP projection (step %d)\" % step)\n        if not self.disabled:\n            self.projection_win = self.vis.matplot(plt, win=self.projection_win)\n        if out_fpath is not None:\n            plt.savefig(out_fpath)\n        plt.clf()\n        \n    def save(self):\n        if not self.disabled:\n            self.vis.save([self.env_name])\n        ", "models/encoder/params_data.py": "\n## Mel-filterbank\nmel_window_length = 25  # In milliseconds\nmel_window_step = 10    # In milliseconds\nmel_n_channels = 40\n\n\n## Audio\nsampling_rate = 16000\n# Number of spectrogram frames in a partial utterance\npartials_n_frames = 160     # 1600 ms\n# Number of spectrogram frames at inference\ninference_n_frames = 80     #  800 ms\n\n\n## Voice Activation Detection\n# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n# This sets the granularity of the VAD. Should not need to be changed.\nvad_window_length = 30  # In milliseconds\n# Number of frames to average together when performing the moving average smoothing.\n# The larger this value, the larger the VAD variations must be to not get smoothed out. \nvad_moving_average_width = 8\n# Maximum number of consecutive silent frames a segment can have.\nvad_max_silence_length = 6\n\n\n## Audio volume normalization\naudio_norm_target_dBFS = -30\n\n", "models/encoder/audio.py": "from scipy.ndimage.morphology import binary_dilation\nfrom models.encoder.params_data import *\nfrom pathlib import Path\nfrom typing import Optional, Union\nfrom warnings import warn\nimport numpy as np\nimport librosa\nimport struct\n\ntry:\n    import webrtcvad\nexcept:\n    warn(\"Unable to import 'webrtcvad'. This package enables noise removal and is recommended.\")\n    webrtcvad=None\n\nint16_max = (2 ** 15) - 1\n\n\ndef preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray],\n                   source_sr: Optional[int] = None,\n                   normalize: Optional[bool] = True,\n                   trim_silence: Optional[bool] = True):\n    \"\"\"\n    Applies the preprocessing operations used in training the Speaker Encoder to a waveform \n    either on disk or in memory. The waveform will be resampled to match the data hyperparameters.\n\n    :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not \n    just .wav), either the waveform as a numpy array of floats.\n    :param source_sr: if passing an audio waveform, the sampling rate of the waveform before \n    preprocessing. After preprocessing, the waveform's sampling rate will match the data \n    hyperparameters. If passing a filepath, the sampling rate will be automatically detected and \n    this argument will be ignored.\n    \"\"\"\n    # Load the wav from disk if needed\n    if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):\n        wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n    else:\n        wav = fpath_or_wav\n    \n    # Resample the wav if needed\n    if source_sr is not None and source_sr != sampling_rate:\n        wav = librosa.resample(wav, orig_sr = source_sr, target_sr = sampling_rate)\n        \n    # Apply the preprocessing: normalize volume and shorten long silences \n    if normalize:\n        wav = normalize_volume(wav, audio_norm_target_dBFS, increase_only=True)\n    if webrtcvad and trim_silence:\n        wav = trim_long_silences(wav)\n    \n    return wav\n\n\ndef wav_to_mel_spectrogram(wav):\n    \"\"\"\n    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.\n    Note: this not a log-mel spectrogram.\n    \"\"\"\n    frames = librosa.feature.melspectrogram(\n        y=wav,\n        sr=sampling_rate,\n        n_fft=int(sampling_rate * mel_window_length / 1000),\n        hop_length=int(sampling_rate * mel_window_step / 1000),\n        n_mels=mel_n_channels\n    )\n    return frames.astype(np.float32).T\n\n\ndef trim_long_silences(wav):\n    \"\"\"\n    Ensures that segments without voice in the waveform remain no longer than a \n    threshold determined by the VAD parameters in params.py.\n\n    :param wav: the raw waveform as a numpy array of floats \n    :return: the same waveform with silences trimmed away (length <= original wav length)\n    \"\"\"\n    # Compute the voice detection window size\n    samples_per_window = (vad_window_length * sampling_rate) // 1000\n    \n    # Trim the end of the audio to have a multiple of the window size\n    wav = wav[:len(wav) - (len(wav) % samples_per_window)]\n    \n    # Convert the float waveform to 16-bit mono PCM\n    pcm_wave = struct.pack(\"%dh\" % len(wav), *(np.round(wav * int16_max)).astype(np.int16))\n    \n    # Perform voice activation detection\n    voice_flags = []\n    vad = webrtcvad.Vad(mode=3)\n    for window_start in range(0, len(wav), samples_per_window):\n        window_end = window_start + samples_per_window\n        voice_flags.append(vad.is_speech(pcm_wave[window_start * 2:window_end * 2],\n                                         sample_rate=sampling_rate))\n    voice_flags = np.array(voice_flags)\n    \n    # Smooth the voice detection with a moving average\n    def moving_average(array, width):\n        array_padded = np.concatenate((np.zeros((width - 1) // 2), array, np.zeros(width // 2)))\n        ret = np.cumsum(array_padded, dtype=float)\n        ret[width:] = ret[width:] - ret[:-width]\n        return ret[width - 1:] / width\n    \n    audio_mask = moving_average(voice_flags, vad_moving_average_width)\n    audio_mask = np.round(audio_mask).astype(bool)\n    \n    # Dilate the voiced regions\n    audio_mask = binary_dilation(audio_mask, np.ones(vad_max_silence_length + 1))\n    audio_mask = np.repeat(audio_mask, samples_per_window)\n    \n    return wav[audio_mask == True]\n\n\ndef normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n", "models/encoder/params_model.py": "\n## Model parameters\nmodel_hidden_size = 256\nmodel_embedding_size = 256\nmodel_num_layers = 3\n\n\n## Training parameters\nlearning_rate_init = 1e-4\nspeakers_per_batch = 64\nutterances_per_speaker = 10\n", "models/encoder/__init__.py": "", "models/encoder/inference.py": "from models.encoder.params_data import *\nfrom models.encoder.model import SpeakerEncoder\nfrom models.encoder.audio import preprocess_wav   # We want to expose this function from here\nfrom matplotlib import cm\nfrom models.encoder import audio\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n_model = None # type: SpeakerEncoder\n_device = None # type: torch.device\n\n\ndef load_model(weights_fpath: Path, device=None):\n    \"\"\"\n    Loads the model in memory. If this function is not explicitely called, it will be run on the \n    first call to embed_frames() with the default weights file.\n    \n    :param weights_fpath: the path to saved model weights.\n    :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\"). The \n    model will be loaded and will run on this device. Outputs will however always be on the cpu. \n    If None, will default to your GPU if it\"s available, otherwise your CPU.\n    \"\"\"\n    # TODO: I think the slow loading of the encoder might have something to do with the device it\n    #   was saved on. Worth investigating.\n    global _model, _device\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, _device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath.name, checkpoint[\"step\"]))\n    return _model\n    \ndef set_model(model, device=None):\n    global _model, _device\n    _model = model\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _device = device\n    _model.to(device)\n\ndef is_loaded():\n    return _model is not None\n\n\ndef embed_frames_batch(frames_batch):\n    \"\"\"\n    Computes embeddings for a batch of mel spectrogram.\n    \n    :param frames_batch: a batch mel of spectrogram as a numpy array of float32 of shape \n    (batch_size, n_frames, n_channels)\n    :return: the embeddings as a numpy array of float32 of shape (batch_size, model_embedding_size)\n    \"\"\"\n    if _model is None:\n        raise Exception(\"Model was not loaded. Call load_model() before inference.\")\n    \n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n    return embed\n\n\ndef compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5, rate=None):\n    \"\"\"\n    Computes where to split an utterance waveform and its corresponding mel spectrogram to obtain \n    partial utterances of <partial_utterance_n_frames> each. Both the waveform and the mel \n    spectrogram slices are returned, so as to make each partial utterance waveform correspond to \n    its spectrogram. This function assumes that the mel spectrogram parameters used are those \n    defined in params_data.py.\n    \n    The returned ranges may be indexing further than the length of the waveform. It is \n    recommended that you pad the waveform with zeros up to wave_slices[-1].stop.\n    \n    :param n_samples: the number of samples in the waveform\n    :param partial_utterance_n_frames: the number of mel spectrogram frames in each partial \n    utterance\n    :param min_pad_coverage: when reaching the last partial utterance, it may or may not have \n    enough frames. If at least <min_pad_coverage> of <partial_utterance_n_frames> are present, \n    then the last partial utterance will be considered, as if we padded the audio. Otherwise, \n    it will be discarded, as if we trimmed the audio. If there aren't enough frames for 1 partial \n    utterance, this parameter is ignored so that the function always returns at least 1 slice.\n    :param overlap: by how much the partial utterance should overlap. If set to 0, the partial \n    utterances are entirely disjoint. \n    :return: the waveform slices and mel spectrogram slices as lists of array slices. Index \n    respectively the waveform and the mel spectrogram with these slices to obtain the partial \n    utterances.\n    \"\"\"\n    assert 0 <= overlap < 1\n    assert 0 < min_pad_coverage <= 1\n    \n    if rate != None:\n        samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n        n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n        frame_step = int(np.round((sampling_rate / rate) / samples_per_frame))\n    else: \n        samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n        n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n        frame_step = max(int(np.round(partial_utterance_n_frames * (1 - overlap))), 1)\n\n    assert 0 < frame_step, \"The rate is too high\"\n    assert frame_step <= partials_n_frames, \"The rate is too low, it should be %f at least\" % \\\n        (sampling_rate / (samples_per_frame * partials_n_frames))\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n        \n    # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    \n    return wav_slices, mel_slices\n\n\ndef embed_utterance(wav, using_partials=True, return_partials=False, **kwargs):\n    \"\"\"\n    Computes an embedding for a single utterance.\n    \n    # TODO: handle multiple wavs to benefit from batching on GPU\n    :param wav: a preprocessed (see audio.py) utterance waveform as a numpy array of float32\n    :param using_partials: if True, then the utterance is split in partial utterances of \n    <partial_utterance_n_frames> frames and the utterance embedding is computed from their \n    normalized average. If False, the utterance is instead computed from feeding the entire \n    spectogram to the network.\n    :param return_partials: if True, the partial embeddings will also be returned along with the \n    wav slices that correspond to the partial embeddings.\n    :param kwargs: additional arguments to compute_partial_splits()\n    :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If \n    <return_partials> is True, the partial utterances as a numpy array of float32 of shape \n    (n_partials, model_embedding_size) and the wav partials as a list of slices will also be \n    returned. If <using_partials> is simultaneously set to False, both these values will be None \n    instead.\n    \"\"\"\n    # Process the entire utterance if not using partials\n    if not using_partials:\n        frames = audio.wav_to_mel_spectrogram(wav)\n        embed = embed_frames_batch(frames[None, ...])[0]\n        if return_partials:\n            return embed, None, None\n        return embed\n    \n    # Compute where to split the utterance into partials and pad if necessary\n    wave_slices, mel_slices = compute_partial_slices(len(wav), **kwargs)\n    max_wave_length = wave_slices[-1].stop\n    if max_wave_length >= len(wav):\n        wav = np.pad(wav, (0, max_wave_length - len(wav)), \"constant\")\n    \n    # Split the utterance into partials\n    frames = audio.wav_to_mel_spectrogram(wav)\n    frames_batch = np.array([frames[s] for s in mel_slices])\n    partial_embeds = embed_frames_batch(frames_batch)\n    \n    # Compute the utterance embedding from the partial embeddings\n    raw_embed = np.mean(partial_embeds, axis=0)\n    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n    \n    if return_partials:\n        return embed, partial_embeds, wave_slices\n    return embed\n\n\ndef embed_speaker(wavs, **kwargs):\n    raise NotImplemented()\n\n\ndef plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30)):\n    if ax is None:\n        ax = plt.gca()\n    \n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, -1)\n    embed = embed.reshape(shape)\n    \n    cmap = cm.get_cmap()\n    mappable = ax.imshow(embed, cmap=cmap)\n    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n    sm = cm.ScalarMappable(cmap=cmap)\n    sm.set_clim(*color_range)\n    \n    ax.set_xticks([]), ax.set_yticks([])\n    ax.set_title(title)\n", "models/encoder/data_objects/speaker_batch.py": "import numpy as np\nfrom typing import List\nfrom models.encoder.data_objects.speaker import Speaker\n\nclass SpeakerBatch:\n    def __init__(self, speakers: List[Speaker], utterances_per_speaker: int, n_frames: int):\n        self.speakers = speakers\n        self.partials = {s: s.random_partial(utterances_per_speaker, n_frames) for s in speakers}\n        \n        # Array of shape (n_speakers * n_utterances, n_frames, mel_n), e.g. for 3 speakers with\n        # 4 utterances each of 160 frames of 40 mel coefficients: (12, 160, 40)\n        self.data = np.array([frames for s in speakers for _, frames, _ in self.partials[s]])\n", "models/encoder/data_objects/speaker_verification_dataset.py": "from models.encoder.data_objects.random_cycler import RandomCycler\nfrom models.encoder.data_objects.speaker_batch import SpeakerBatch\nfrom models.encoder.data_objects.speaker import Speaker\nfrom models.encoder.params_data import partials_n_frames\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\n\n# TODO: improve with a pool of speakers for data efficiency\n\nclass SpeakerVerificationDataset(Dataset):\n    def __init__(self, datasets_root: Path):\n        self.root = datasets_root\n        speaker_dirs = [f for f in self.root.glob(\"*\") if f.is_dir()]\n        if len(speaker_dirs) == 0:\n            raise Exception(\"No speakers found. Make sure you are pointing to the directory \"\n                            \"containing all preprocessed speaker directories.\")\n        self.speakers = [Speaker(speaker_dir) for speaker_dir in speaker_dirs]\n        self.speaker_cycler = RandomCycler(self.speakers)\n\n    def __len__(self):\n        return int(1e10)\n        \n    def __getitem__(self, index):\n        return next(self.speaker_cycler)\n    \n    def get_logs(self):\n        log_string = \"\"\n        for log_fpath in self.root.glob(\"*.txt\"):\n            with log_fpath.open(\"r\") as log_file:\n                log_string += \"\".join(log_file.readlines())\n        return log_string\n    \n    \nclass SpeakerVerificationDataLoader(DataLoader):\n    def __init__(self, dataset, speakers_per_batch, utterances_per_speaker, sampler=None, \n                 batch_sampler=None, num_workers=0, pin_memory=False, timeout=0, \n                 worker_init_fn=None):\n        self.utterances_per_speaker = utterances_per_speaker\n\n        super().__init__(\n            dataset=dataset, \n            batch_size=speakers_per_batch, \n            shuffle=False, \n            sampler=sampler, \n            batch_sampler=batch_sampler, \n            num_workers=num_workers,\n            collate_fn=self.collate, \n            pin_memory=pin_memory, \n            drop_last=False, \n            timeout=timeout, \n            worker_init_fn=worker_init_fn\n        )\n\n    def collate(self, speakers):\n        return SpeakerBatch(speakers, self.utterances_per_speaker, partials_n_frames) \n    ", "models/encoder/data_objects/speaker.py": "from models.encoder.data_objects.random_cycler import RandomCycler\nfrom models.encoder.data_objects.utterance import Utterance\nfrom pathlib import Path\n\n# Contains the set of utterances of a single speaker\nclass Speaker:\n    def __init__(self, root: Path):\n        self.root = root\n        self.name = root.name\n        self.utterances = None\n        self.utterance_cycler = None\n        \n    def _load_utterances(self):\n        with self.root.joinpath(\"_sources.txt\").open(\"r\") as sources_file:\n            sources = [l.split(\",\") for l in sources_file]\n        sources = {frames_fname: wave_fpath for frames_fname, wave_fpath in sources}\n        self.utterances = [Utterance(self.root.joinpath(f), w) for f, w in sources.items()]\n        self.utterance_cycler = RandomCycler(self.utterances)\n               \n    def random_partial(self, count, n_frames):\n        \"\"\"\n        Samples a batch of <count> unique partial utterances from the disk in a way that all \n        utterances come up at least once every two cycles and in a random order every time.\n        \n        :param count: The number of partial utterances to sample from the set of utterances from \n        that speaker. Utterances are guaranteed not to be repeated if <count> is not larger than \n        the number of utterances available.\n        :param n_frames: The number of frames in the partial utterance.\n        :return: A list of tuples (utterance, frames, range) where utterance is an Utterance, \n        frames are the frames of the partial utterances and range is the range of the partial \n        utterance with regard to the complete utterance.\n        \"\"\"\n        if self.utterances is None:\n            self._load_utterances()\n\n        utterances = self.utterance_cycler.sample(count)\n\n        a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n        return a\n", "models/encoder/data_objects/utterance.py": "import numpy as np\n\n\nclass Utterance:\n    def __init__(self, frames_fpath, wave_fpath):\n        self.frames_fpath = frames_fpath\n        self.wave_fpath = wave_fpath\n        \n    def get_frames(self):\n        return np.load(self.frames_fpath)\n\n    def random_partial(self, n_frames):\n        \"\"\"\n        Crops the frames into a partial utterance of n_frames\n        \n        :param n_frames: The number of frames of the partial utterance\n        :return: the partial utterance frames and a tuple indicating the start and end of the \n        partial utterance in the complete utterance.\n        \"\"\"\n        frames = self.get_frames()\n        if frames.shape[0] == n_frames:\n            start = 0\n        else:\n            start = np.random.randint(0, frames.shape[0] - n_frames)\n        end = start + n_frames\n        return frames[start:end], (start, end)", "models/encoder/data_objects/__init__.py": "from models.encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataset\nfrom models.encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataLoader\n", "models/encoder/data_objects/random_cycler.py": "import random\n\nclass RandomCycler:\n    \"\"\"\n    Creates an internal copy of a sequence and allows access to its items in a constrained random \n    order. For a source sequence of n items and one or several consecutive queries of a total \n    of m items, the following guarantees hold (one implies the other):\n        - Each item will be returned between m // n and ((m - 1) // n) + 1 times.\n        - Between two appearances of the same item, there may be at most 2 * (n - 1) other items.\n    \"\"\"\n    \n    def __init__(self, source):\n        if len(source) == 0:\n            raise Exception(\"Can't create RandomCycler from an empty collection\")\n        self.all_items = list(source)\n        self.next_items = []\n    \n    def sample(self, count: int):\n        shuffle = lambda l: random.sample(l, len(l))\n        \n        out = []\n        while count > 0:\n            if count >= len(self.all_items):\n                out.extend(shuffle(list(self.all_items)))\n                count -= len(self.all_items)\n                continue\n            n = min(count, len(self.next_items))\n            out.extend(self.next_items[:n])\n            count -= n\n            self.next_items = self.next_items[n:]\n            if len(self.next_items) == 0:\n                self.next_items = shuffle(list(self.all_items))\n        return out\n    \n    def __next__(self):\n        return self.sample(1)[0]\n\n", "models/ppg_extractor/frontend.py": "import copy\nfrom typing import Tuple\nimport numpy as np\nimport torch\nfrom torch_complex.tensor import ComplexTensor\n\nfrom .log_mel import LogMel\nfrom .stft import Stft\n\n\nclass DefaultFrontend(torch.nn.Module):\n    \"\"\"Conventional frontend structure for ASR\n\n    Stft -> WPE -> MVDR-Beamformer -> Power-spec -> Mel-Fbank -> CMVN\n    \"\"\"\n\n    def __init__(\n        self,\n        fs: 16000,\n        n_fft: int = 1024,\n        win_length: int = 800,\n        hop_length: int = 160,\n        center: bool = True,\n        pad_mode: str = \"reflect\",\n        normalized: bool = False,\n        onesided: bool = True,\n        n_mels: int = 80,\n        fmin: int = None,\n        fmax: int = None,\n        htk: bool = False,\n        norm=1,\n        frontend_conf=None, #Optional[dict] = get_default_kwargs(Frontend),\n        kaldi_padding_mode=False,\n        downsample_rate: int = 1,\n    ):\n        super().__init__()\n        self.downsample_rate = downsample_rate\n\n        # Deepcopy (In general, dict shouldn't be used as default arg)\n        frontend_conf = copy.deepcopy(frontend_conf)\n\n        self.stft = Stft(\n            n_fft=n_fft,\n            win_length=win_length,\n            hop_length=hop_length,\n            center=center,\n            pad_mode=pad_mode,\n            normalized=normalized,\n            onesided=onesided,\n            kaldi_padding_mode=kaldi_padding_mode\n        )\n        if frontend_conf is not None:\n            self.frontend = Frontend(idim=n_fft // 2 + 1, **frontend_conf)\n        else:\n            self.frontend = None\n\n        self.logmel = LogMel(\n            fs=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, htk=htk, norm=norm,\n        )\n        self.n_mels = n_mels\n\n    def output_size(self) -> int:\n        return self.n_mels\n\n    def forward(\n        self, input: torch.Tensor, input_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # 1. Domain-conversion: e.g. Stft: time -> time-freq\n        input_stft, feats_lens = self.stft(input, input_lengths)\n\n        assert input_stft.dim() >= 4, input_stft.shape\n        # \"2\" refers to the real/imag parts of Complex\n        assert input_stft.shape[-1] == 2, input_stft.shape\n\n        # Change torch.Tensor to ComplexTensor\n        # input_stft: (..., F, 2) -> (..., F)\n        input_stft = ComplexTensor(input_stft[..., 0], input_stft[..., 1])\n\n        # 2. [Option] Speech enhancement\n        if self.frontend is not None:\n            assert isinstance(input_stft, ComplexTensor), type(input_stft)\n            # input_stft: (Batch, Length, [Channel], Freq)\n            input_stft, _, mask = self.frontend(input_stft, feats_lens)\n\n        # 3. [Multi channel case]: Select a channel\n        if input_stft.dim() == 4:\n            # h: (B, T, C, F) -> h: (B, T, F)\n            if self.training:\n                # Select 1ch randomly\n                ch = np.random.randint(input_stft.size(2))\n                input_stft = input_stft[:, :, ch, :]\n            else:\n                # Use the first channel\n                input_stft = input_stft[:, :, 0, :]\n\n        # 4. STFT -> Power spectrum\n        # h: ComplexTensor(B, T, F) -> torch.Tensor(B, T, F)\n        input_power = input_stft.real ** 2 + input_stft.imag ** 2\n\n        # 5. Feature transform e.g. Stft -> Log-Mel-Fbank\n        # input_power: (Batch, [Channel,] Length, Freq)\n        #       -> input_feats: (Batch, Length, Dim)\n        input_feats, _ = self.logmel(input_power, feats_lens)\n               \n        # NOTE(sx): pad\n        max_len = input_feats.size(1)\n        if self.downsample_rate > 1 and max_len % self.downsample_rate != 0:\n            padding = self.downsample_rate - max_len % self.downsample_rate\n            # print(\"Logmel: \", input_feats.size())\n            input_feats = torch.nn.functional.pad(input_feats, (0, 0, 0, padding),\n                                                  \"constant\", 0)\n            # print(\"Logmel(after padding): \",input_feats.size())\n            feats_lens[torch.argmax(feats_lens)] = max_len + padding \n\n        return input_feats, feats_lens\n", "models/ppg_extractor/nets_utils.py": "# -*- coding: utf-8 -*-\n\n\"\"\"Network related utility tools.\"\"\"\n\nimport logging\nfrom typing import Dict\n\nimport numpy as np\nimport torch\n\n\ndef to_device(m, x):\n    \"\"\"Send tensor into the device of the module.\n\n    Args:\n        m (torch.nn.Module): Torch module.\n        x (Tensor): Torch tensor.\n\n    Returns:\n        Tensor: Torch tensor located in the same place as torch module.\n\n    \"\"\"\n    assert isinstance(m, torch.nn.Module)\n    device = next(m.parameters()).device\n    return x.to(device)\n\n\ndef pad_list(xs, pad_value):\n    \"\"\"Perform padding for the list of tensors.\n\n    Args:\n        xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)].\n        pad_value (float): Value for padding.\n\n    Returns:\n        Tensor: Padded tensor (B, Tmax, `*`).\n\n    Examples:\n        >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)]\n        >>> x\n        [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])]\n        >>> pad_list(x, 0)\n        tensor([[1., 1., 1., 1.],\n                [1., 1., 0., 0.],\n                [1., 0., 0., 0.]])\n\n    \"\"\"\n    n_batch = len(xs)\n    max_len = max(x.size(0) for x in xs)\n    pad = xs[0].new(n_batch, max_len, *xs[0].size()[1:]).fill_(pad_value)\n\n    for i in range(n_batch):\n        pad[i, :xs[i].size(0)] = xs[i]\n\n    return pad\n\n\ndef make_pad_mask(lengths, xs=None, length_dim=-1):\n    \"\"\"Make mask tensor containing indices of padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor. If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor. See the example.\n\n    Returns:\n        Tensor: Mask tensor containing indices of padded part.\n                dtype=torch.uint8 in PyTorch 1.2-\n                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[0, 0, 0, 0 ,0],\n                 [0, 0, 0, 1, 1],\n                 [0, 0, 1, 1, 1]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0],\n                 [0, 0, 0, 0]],\n                [[0, 0, 0, 1],\n                 [0, 0, 0, 1]],\n                [[0, 0, 1, 1],\n                 [0, 0, 1, 1]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_pad_mask(lengths, xs, 1)\n        tensor([[[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n        >>> make_pad_mask(lengths, xs, 2)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n    \"\"\"\n    if length_dim == 0:\n        raise ValueError('length_dim cannot be 0: {}'.format(length_dim))\n\n    if not isinstance(lengths, list):\n        lengths = lengths.tolist()\n    bs = int(len(lengths))\n    if xs is None:\n        maxlen = int(max(lengths))\n    else:\n        maxlen = xs.size(length_dim)\n\n    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n    mask = seq_range_expand >= seq_length_expand\n\n    if xs is not None:\n        assert xs.size(0) == bs, (xs.size(0), bs)\n\n        if length_dim < 0:\n            length_dim = xs.dim() + length_dim\n        # ind = (:, None, ..., None, :, , None, ..., None)\n        ind = tuple(slice(None) if i in (0, length_dim) else None\n                    for i in range(xs.dim()))\n        mask = mask[ind].expand_as(xs).to(xs.device)\n    return mask\n\n\ndef make_non_pad_mask(lengths, xs=None, length_dim=-1):\n    \"\"\"Make mask tensor containing indices of non-padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor. If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor. See the example.\n\n    Returns:\n        ByteTensor: mask tensor containing indices of padded part.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[1, 1, 1, 1 ,1],\n                 [1, 1, 1, 0, 0],\n                 [1, 1, 0, 0, 0]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1],\n                 [1, 1, 1, 1]],\n                [[1, 1, 1, 0],\n                 [1, 1, 1, 0]],\n                [[1, 1, 0, 0],\n                 [1, 1, 0, 0]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_non_pad_mask(lengths, xs, 1)\n        tensor([[[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)\n        >>> make_non_pad_mask(lengths, xs, 2)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n    \"\"\"\n    return ~make_pad_mask(lengths, xs, length_dim)\n\n\ndef mask_by_length(xs, lengths, fill=0):\n    \"\"\"Mask tensor according to length.\n\n    Args:\n        xs (Tensor): Batch of input tensor (B, `*`).\n        lengths (LongTensor or List): Batch of lengths (B,).\n        fill (int or float): Value to fill masked part.\n\n    Returns:\n        Tensor: Batch of masked input tensor (B, `*`).\n\n    Examples:\n        >>> x = torch.arange(5).repeat(3, 1) + 1\n        >>> x\n        tensor([[1, 2, 3, 4, 5],\n                [1, 2, 3, 4, 5],\n                [1, 2, 3, 4, 5]])\n        >>> lengths = [5, 3, 2]\n        >>> mask_by_length(x, lengths)\n        tensor([[1, 2, 3, 4, 5],\n                [1, 2, 3, 0, 0],\n                [1, 2, 0, 0, 0]])\n\n    \"\"\"\n    assert xs.size(0) == len(lengths)\n    ret = xs.data.new(*xs.size()).fill_(fill)\n    for i, l in enumerate(lengths):\n        ret[i, :l] = xs[i, :l]\n    return ret\n\n\ndef th_accuracy(pad_outputs, pad_targets, ignore_label):\n    \"\"\"Calculate accuracy.\n\n    Args:\n        pad_outputs (Tensor): Prediction tensors (B * Lmax, D).\n        pad_targets (LongTensor): Target label tensors (B, Lmax, D).\n        ignore_label (int): Ignore label id.\n\n    Returns:\n        float: Accuracy value (0.0 - 1.0).\n\n    \"\"\"\n    pad_pred = pad_outputs.view(\n        pad_targets.size(0),\n        pad_targets.size(1),\n        pad_outputs.size(1)).argmax(2)\n    mask = pad_targets != ignore_label\n    numerator = torch.sum(pad_pred.masked_select(mask) == pad_targets.masked_select(mask))\n    denominator = torch.sum(mask)\n    return float(numerator) / float(denominator)\n\n\ndef to_torch_tensor(x):\n    \"\"\"Change to torch.Tensor or ComplexTensor from numpy.ndarray.\n\n    Args:\n        x: Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict.\n\n    Returns:\n        Tensor or ComplexTensor: Type converted inputs.\n\n    Examples:\n        >>> xs = np.ones(3, dtype=np.float32)\n        >>> xs = to_torch_tensor(xs)\n        tensor([1., 1., 1.])\n        >>> xs = torch.ones(3, 4, 5)\n        >>> assert to_torch_tensor(xs) is xs\n        >>> xs = {'real': xs, 'imag': xs}\n        >>> to_torch_tensor(xs)\n        ComplexTensor(\n        Real:\n        tensor([1., 1., 1.])\n        Imag;\n        tensor([1., 1., 1.])\n        )\n\n    \"\"\"\n    # If numpy, change to torch tensor\n    if isinstance(x, np.ndarray):\n        if x.dtype.kind == 'c':\n            # Dynamically importing because torch_complex requires python3\n            from torch_complex.tensor import ComplexTensor\n            return ComplexTensor(x)\n        else:\n            return torch.from_numpy(x)\n\n    # If {'real': ..., 'imag': ...}, convert to ComplexTensor\n    elif isinstance(x, dict):\n        # Dynamically importing because torch_complex requires python3\n        from torch_complex.tensor import ComplexTensor\n\n        if 'real' not in x or 'imag' not in x:\n            raise ValueError(\"has 'real' and 'imag' keys: {}\".format(list(x)))\n        # Relative importing because of using python3 syntax\n        return ComplexTensor(x['real'], x['imag'])\n\n    # If torch.Tensor, as it is\n    elif isinstance(x, torch.Tensor):\n        return x\n\n    else:\n        error = (\"x must be numpy.ndarray, torch.Tensor or a dict like \"\n                 \"{{'real': torch.Tensor, 'imag': torch.Tensor}}, \"\n                 \"but got {}\".format(type(x)))\n        try:\n            from torch_complex.tensor import ComplexTensor\n        except Exception:\n            # If PY2\n            raise ValueError(error)\n        else:\n            # If PY3\n            if isinstance(x, ComplexTensor):\n                return x\n            else:\n                raise ValueError(error)\n\n\ndef get_subsample(train_args, mode, arch):\n    \"\"\"Parse the subsampling factors from the training args for the specified `mode` and `arch`.\n\n    Args:\n        train_args: argument Namespace containing options.\n        mode: one of ('asr', 'mt', 'st')\n        arch: one of ('rnn', 'rnn-t', 'rnn_mix', 'rnn_mulenc', 'transformer')\n\n    Returns:\n        np.ndarray / List[np.ndarray]: subsampling factors.\n    \"\"\"\n    if arch == 'transformer':\n        return np.array([1])\n\n    elif mode == 'mt' and arch == 'rnn':\n        # +1 means input (+1) and layers outputs (train_args.elayer)\n        subsample = np.ones(train_args.elayers + 1, dtype=np.int)\n        logging.warning('Subsampling is not performed for machine translation.')\n        logging.info('subsample: ' + ' '.join([str(x) for x in subsample]))\n        return subsample\n\n    elif (mode == 'asr' and arch in ('rnn', 'rnn-t')) or \\\n         (mode == 'mt' and arch == 'rnn') or \\\n         (mode == 'st' and arch == 'rnn'):\n        subsample = np.ones(train_args.elayers + 1, dtype=np.int)\n        if train_args.etype.endswith(\"p\") and not train_args.etype.startswith(\"vgg\"):\n            ss = train_args.subsample.split(\"_\")\n            for j in range(min(train_args.elayers + 1, len(ss))):\n                subsample[j] = int(ss[j])\n        else:\n            logging.warning(\n                'Subsampling is not performed for vgg*. It is performed in max pooling layers at CNN.')\n        logging.info('subsample: ' + ' '.join([str(x) for x in subsample]))\n        return subsample\n\n    elif mode == 'asr' and arch == 'rnn_mix':\n        subsample = np.ones(train_args.elayers_sd + train_args.elayers + 1, dtype=np.int)\n        if train_args.etype.endswith(\"p\") and not train_args.etype.startswith(\"vgg\"):\n            ss = train_args.subsample.split(\"_\")\n            for j in range(min(train_args.elayers_sd + train_args.elayers + 1, len(ss))):\n                subsample[j] = int(ss[j])\n        else:\n            logging.warning(\n                'Subsampling is not performed for vgg*. It is performed in max pooling layers at CNN.')\n        logging.info('subsample: ' + ' '.join([str(x) for x in subsample]))\n        return subsample\n\n    elif mode == 'asr' and arch == 'rnn_mulenc':\n        subsample_list = []\n        for idx in range(train_args.num_encs):\n            subsample = np.ones(train_args.elayers[idx] + 1, dtype=np.int)\n            if train_args.etype[idx].endswith(\"p\") and not train_args.etype[idx].startswith(\"vgg\"):\n                ss = train_args.subsample[idx].split(\"_\")\n                for j in range(min(train_args.elayers[idx] + 1, len(ss))):\n                    subsample[j] = int(ss[j])\n            else:\n                logging.warning(\n                    'Encoder %d: Subsampling is not performed for vgg*. '\n                    'It is performed in max pooling layers at CNN.', idx + 1)\n            logging.info('subsample: ' + ' '.join([str(x) for x in subsample]))\n            subsample_list.append(subsample)\n        return subsample_list\n\n    else:\n        raise ValueError('Invalid options: mode={}, arch={}'.format(mode, arch))\n\n\ndef rename_state_dict(old_prefix: str, new_prefix: str, state_dict: Dict[str, torch.Tensor]):\n    \"\"\"Replace keys of old prefix with new prefix in state dict.\"\"\"\n    # need this list not to break the dict iterator\n    old_keys = [k for k in state_dict if k.startswith(old_prefix)]\n    if len(old_keys) > 0:\n        logging.warning(f'Rename: {old_prefix} -> {new_prefix}')\n    for k in old_keys:\n        v = state_dict.pop(k)\n        new_k = k.replace(old_prefix, new_prefix)\n        state_dict[new_k] = v\n\ndef get_activation(act):\n    \"\"\"Return activation function.\"\"\"\n    # Lazy load to avoid unused import\n    from .encoder.swish import Swish\n\n    activation_funcs = {\n        \"hardtanh\": torch.nn.Hardtanh,\n        \"relu\": torch.nn.ReLU,\n        \"selu\": torch.nn.SELU,\n        \"swish\": Swish,\n    }\n\n    return activation_funcs[act]()\n", "models/ppg_extractor/stft.py": "from typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport torch\n\nfrom .nets_utils import make_pad_mask\n\n\nclass Stft(torch.nn.Module):\n    def __init__(\n        self,\n        n_fft: int = 512,\n        win_length: Union[int, None] = 512,\n        hop_length: int = 128,\n        center: bool = True,\n        pad_mode: str = \"reflect\",\n        normalized: bool = False,\n        onesided: bool = True,\n        kaldi_padding_mode=False,\n    ):\n        super().__init__()\n        self.n_fft = n_fft\n        if win_length is None:\n            self.win_length = n_fft\n        else:\n            self.win_length = win_length\n        self.hop_length = hop_length\n        self.center = center\n        self.pad_mode = pad_mode\n        self.normalized = normalized\n        self.onesided = onesided\n        self.kaldi_padding_mode = kaldi_padding_mode\n        if self.kaldi_padding_mode:\n            self.win_length = 400\n\n    def extra_repr(self):\n        return (\n            f\"n_fft={self.n_fft}, \"\n            f\"win_length={self.win_length}, \"\n            f\"hop_length={self.hop_length}, \"\n            f\"center={self.center}, \"\n            f\"pad_mode={self.pad_mode}, \"\n            f\"normalized={self.normalized}, \"\n            f\"onesided={self.onesided}\"\n        )\n\n    def forward(\n        self, input: torch.Tensor, ilens: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"STFT forward function.\n\n        Args:\n            input: (Batch, Nsamples) or (Batch, Nsample, Channels)\n            ilens: (Batch)\n        Returns:\n            output: (Batch, Frames, Freq, 2) or (Batch, Frames, Channels, Freq, 2)\n\n        \"\"\"\n        bs = input.size(0)\n        if input.dim() == 3:\n            multi_channel = True\n            # input: (Batch, Nsample, Channels) -> (Batch * Channels, Nsample)\n            input = input.transpose(1, 2).reshape(-1, input.size(1))\n        else:\n            multi_channel = False\n\n        # output: (Batch, Freq, Frames, 2=real_imag)\n        # or (Batch, Channel, Freq, Frames, 2=real_imag)\n        if not self.kaldi_padding_mode:\n            output = torch.stft(\n                input,\n                n_fft=self.n_fft,\n                win_length=self.win_length,\n                hop_length=self.hop_length,\n                center=self.center,\n                pad_mode=self.pad_mode,\n                normalized=self.normalized,\n                onesided=self.onesided,\n                return_complex=False\n            )\n        else:\n            # NOTE(sx): Use Kaldi-fasion padding, maybe wrong\n            num_pads = self.n_fft - self.win_length\n            input = torch.nn.functional.pad(input, (num_pads, 0))\n            output = torch.stft(\n                input,\n                n_fft=self.n_fft,\n                win_length=self.win_length,\n                hop_length=self.hop_length,\n                center=False,\n                pad_mode=self.pad_mode,\n                normalized=self.normalized,\n                onesided=self.onesided,\n                return_complex=False\n            )\n\n        # output: (Batch, Freq, Frames, 2=real_imag)\n        # -> (Batch, Frames, Freq, 2=real_imag)\n        output = output.transpose(1, 2)\n        if multi_channel:\n            # output: (Batch * Channel, Frames, Freq, 2=real_imag)\n            # -> (Batch, Frame, Channel, Freq, 2=real_imag)\n            output = output.view(bs, -1, output.size(1), output.size(2), 2).transpose(\n                1, 2\n            )\n\n        if ilens is not None:\n            if self.center:\n                pad = self.win_length // 2\n                ilens = ilens + 2 * pad\n            olens = torch.div(ilens - self.win_length, self.hop_length, rounding_mode='floor') + 1\n            # olens = ilens - self.win_length // self.hop_length + 1\n            output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)\n        else:\n            olens = None\n\n        return output, olens\n", "models/ppg_extractor/e2e_asr_common.py": "#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Common functions for ASR.\"\"\"\n\nimport argparse\nimport editdistance\nimport json\nimport logging\nimport numpy as np\nimport six\nimport sys\n\nfrom itertools import groupby\n\n\ndef end_detect(ended_hyps, i, M=3, D_end=np.log(1 * np.exp(-10))):\n    \"\"\"End detection.\n\n    desribed in Eq. (50) of S. Watanabe et al\n    \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition\"\n\n    :param ended_hyps:\n    :param i:\n    :param M:\n    :param D_end:\n    :return:\n    \"\"\"\n    if len(ended_hyps) == 0:\n        return False\n    count = 0\n    best_hyp = sorted(ended_hyps, key=lambda x: x['score'], reverse=True)[0]\n    for m in six.moves.range(M):\n        # get ended_hyps with their length is i - m\n        hyp_length = i - m\n        hyps_same_length = [x for x in ended_hyps if len(x['yseq']) == hyp_length]\n        if len(hyps_same_length) > 0:\n            best_hyp_same_length = sorted(hyps_same_length, key=lambda x: x['score'], reverse=True)[0]\n            if best_hyp_same_length['score'] - best_hyp['score'] < D_end:\n                count += 1\n\n    if count == M:\n        return True\n    else:\n        return False\n\n\n# TODO(takaaki-hori): add different smoothing methods\ndef label_smoothing_dist(odim, lsm_type, transcript=None, blank=0):\n    \"\"\"Obtain label distribution for loss smoothing.\n\n    :param odim:\n    :param lsm_type:\n    :param blank:\n    :param transcript:\n    :return:\n    \"\"\"\n    if transcript is not None:\n        with open(transcript, 'rb') as f:\n            trans_json = json.load(f)['utts']\n\n    if lsm_type == 'unigram':\n        assert transcript is not None, 'transcript is required for %s label smoothing' % lsm_type\n        labelcount = np.zeros(odim)\n        for k, v in trans_json.items():\n            ids = np.array([int(n) for n in v['output'][0]['tokenid'].split()])\n            # to avoid an error when there is no text in an uttrance\n            if len(ids) > 0:\n                labelcount[ids] += 1\n        labelcount[odim - 1] = len(transcript)  # count <eos>\n        labelcount[labelcount == 0] = 1  # flooring\n        labelcount[blank] = 0  # remove counts for blank\n        labeldist = labelcount.astype(np.float32) / np.sum(labelcount)\n    else:\n        logging.error(\n            \"Error: unexpected label smoothing type: %s\" % lsm_type)\n        sys.exit()\n\n    return labeldist\n\n\ndef get_vgg2l_odim(idim, in_channel=3, out_channel=128, downsample=True):\n    \"\"\"Return the output size of the VGG frontend.\n\n    :param in_channel: input channel size\n    :param out_channel: output channel size\n    :return: output size\n    :rtype int\n    \"\"\"\n    idim = idim / in_channel\n    if downsample:\n        idim = np.ceil(np.array(idim, dtype=np.float32) / 2)  # 1st max pooling\n        idim = np.ceil(np.array(idim, dtype=np.float32) / 2)  # 2nd max pooling\n    return int(idim) * out_channel  # numer of channels\n\n\nclass ErrorCalculator(object):\n    \"\"\"Calculate CER and WER for E2E_ASR and CTC models during training.\n\n    :param y_hats: numpy array with predicted text\n    :param y_pads: numpy array with true (target) text\n    :param char_list:\n    :param sym_space:\n    :param sym_blank:\n    :return:\n    \"\"\"\n\n    def __init__(self, char_list, sym_space, sym_blank, report_cer=False, report_wer=False,\n                 trans_type=\"char\"):\n        \"\"\"Construct an ErrorCalculator object.\"\"\"\n        super(ErrorCalculator, self).__init__()\n\n        self.report_cer = report_cer\n        self.report_wer = report_wer\n        self.trans_type = trans_type\n        self.char_list = char_list\n        self.space = sym_space\n        self.blank = sym_blank\n        self.idx_blank = self.char_list.index(self.blank)\n        if self.space in self.char_list:\n            self.idx_space = self.char_list.index(self.space)\n        else:\n            self.idx_space = None\n\n    def __call__(self, ys_hat, ys_pad, is_ctc=False):\n        \"\"\"Calculate sentence-level WER/CER score.\n\n        :param torch.Tensor ys_hat: prediction (batch, seqlen)\n        :param torch.Tensor ys_pad: reference (batch, seqlen)\n        :param bool is_ctc: calculate CER score for CTC\n        :return: sentence-level WER score\n        :rtype float\n        :return: sentence-level CER score\n        :rtype float\n        \"\"\"\n        cer, wer = None, None\n        if is_ctc:\n            return self.calculate_cer_ctc(ys_hat, ys_pad)\n        elif not self.report_cer and not self.report_wer:\n            return cer, wer\n\n        seqs_hat, seqs_true = self.convert_to_char(ys_hat, ys_pad)\n        if self.report_cer:\n            cer = self.calculate_cer(seqs_hat, seqs_true)\n\n        if self.report_wer:\n            wer = self.calculate_wer(seqs_hat, seqs_true)\n        return cer, wer\n\n    def calculate_cer_ctc(self, ys_hat, ys_pad):\n        \"\"\"Calculate sentence-level CER score for CTC.\n\n        :param torch.Tensor ys_hat: prediction (batch, seqlen)\n        :param torch.Tensor ys_pad: reference (batch, seqlen)\n        :return: average sentence-level CER score\n        :rtype float\n        \"\"\"\n        cers, char_ref_lens = [], []\n        for i, y in enumerate(ys_hat):\n            y_hat = [x[0] for x in groupby(y)]\n            y_true = ys_pad[i]\n            seq_hat, seq_true = [], []\n            for idx in y_hat:\n                idx = int(idx)\n                if idx != -1 and idx != self.idx_blank and idx != self.idx_space:\n                    seq_hat.append(self.char_list[int(idx)])\n\n            for idx in y_true:\n                idx = int(idx)\n                if idx != -1 and idx != self.idx_blank and idx != self.idx_space:\n                    seq_true.append(self.char_list[int(idx)])\n            if self.trans_type == \"char\":\n                hyp_chars = \"\".join(seq_hat)\n                ref_chars = \"\".join(seq_true)\n            else:\n                hyp_chars = \" \".join(seq_hat)\n                ref_chars = \" \".join(seq_true)\n\n            if len(ref_chars) > 0:\n                cers.append(editdistance.eval(hyp_chars, ref_chars))\n                char_ref_lens.append(len(ref_chars))\n\n        cer_ctc = float(sum(cers)) / sum(char_ref_lens) if cers else None\n        return cer_ctc\n\n    def convert_to_char(self, ys_hat, ys_pad):\n        \"\"\"Convert index to character.\n\n        :param torch.Tensor seqs_hat: prediction (batch, seqlen)\n        :param torch.Tensor seqs_true: reference (batch, seqlen)\n        :return: token list of prediction\n        :rtype list\n        :return: token list of reference\n        :rtype list\n        \"\"\"\n        seqs_hat, seqs_true = [], []\n        for i, y_hat in enumerate(ys_hat):\n            y_true = ys_pad[i]\n            eos_true = np.where(y_true == -1)[0]\n            eos_true = eos_true[0] if len(eos_true) > 0 else len(y_true)\n            # To avoid wrong higher WER than the one obtained from the decoding\n            # eos from y_true is used to mark the eos in y_hat\n            # because of that y_hats has not padded outs with -1.\n            seq_hat = [self.char_list[int(idx)] for idx in y_hat[:eos_true]]\n            seq_true = [self.char_list[int(idx)] for idx in y_true if int(idx) != -1]\n            # seq_hat_text = \"\".join(seq_hat).replace(self.space, ' ')\n            seq_hat_text = \" \".join(seq_hat).replace(self.space, ' ')\n            seq_hat_text = seq_hat_text.replace(self.blank, '')\n            # seq_true_text = \"\".join(seq_true).replace(self.space, ' ')\n            seq_true_text = \" \".join(seq_true).replace(self.space, ' ')\n            seqs_hat.append(seq_hat_text)\n            seqs_true.append(seq_true_text)\n        return seqs_hat, seqs_true\n\n    def calculate_cer(self, seqs_hat, seqs_true):\n        \"\"\"Calculate sentence-level CER score.\n\n        :param list seqs_hat: prediction\n        :param list seqs_true: reference\n        :return: average sentence-level CER score\n        :rtype float\n        \"\"\"\n        char_eds, char_ref_lens = [], []\n        for i, seq_hat_text in enumerate(seqs_hat):\n            seq_true_text = seqs_true[i]\n            hyp_chars = seq_hat_text.replace(' ', '')\n            ref_chars = seq_true_text.replace(' ', '')\n            char_eds.append(editdistance.eval(hyp_chars, ref_chars))\n            char_ref_lens.append(len(ref_chars))\n        return float(sum(char_eds)) / sum(char_ref_lens)\n\n    def calculate_wer(self, seqs_hat, seqs_true):\n        \"\"\"Calculate sentence-level WER score.\n\n        :param list seqs_hat: prediction\n        :param list seqs_true: reference\n        :return: average sentence-level WER score\n        :rtype float\n        \"\"\"\n        word_eds, word_ref_lens = [], []\n        for i, seq_hat_text in enumerate(seqs_hat):\n            seq_true_text = seqs_true[i]\n            hyp_words = seq_hat_text.split()\n            ref_words = seq_true_text.split()\n            word_eds.append(editdistance.eval(hyp_words, ref_words))\n            word_ref_lens.append(len(ref_words))\n        return float(sum(word_eds)) / sum(word_ref_lens)\n\n\nclass ErrorCalculatorTrans(object):\n    \"\"\"Calculate CER and WER for transducer models.\n\n    Args:\n        decoder (nn.Module): decoder module\n        args (Namespace): argument Namespace containing options\n        report_cer (boolean): compute CER option\n        report_wer (boolean): compute WER option\n\n    \"\"\"\n\n    def __init__(self, decoder, args, report_cer=False, report_wer=False):\n        \"\"\"Construct an ErrorCalculator object for transducer model.\"\"\"\n        super(ErrorCalculatorTrans, self).__init__()\n\n        self.dec = decoder\n\n        recog_args = {'beam_size': args.beam_size,\n                      'nbest': args.nbest,\n                      'space': args.sym_space,\n                      'score_norm_transducer': args.score_norm_transducer}\n\n        self.recog_args = argparse.Namespace(**recog_args)\n\n        self.char_list = args.char_list\n        self.space = args.sym_space\n        self.blank = args.sym_blank\n\n        self.report_cer = args.report_cer\n        self.report_wer = args.report_wer\n\n    def __call__(self, hs_pad, ys_pad):\n        \"\"\"Calculate sentence-level WER/CER score for transducer models.\n\n        Args:\n            hs_pad (torch.Tensor): batch of padded input sequence (batch, T, D)\n            ys_pad (torch.Tensor): reference (batch, seqlen)\n\n        Returns:\n            (float): sentence-level CER score\n            (float): sentence-level WER score\n\n        \"\"\"\n        cer, wer = None, None\n\n        if not self.report_cer and not self.report_wer:\n            return cer, wer\n\n        batchsize = int(hs_pad.size(0))\n        batch_nbest = []\n\n        for b in six.moves.range(batchsize):\n            if self.recog_args.beam_size == 1:\n                nbest_hyps = self.dec.recognize(hs_pad[b], self.recog_args)\n            else:\n                nbest_hyps = self.dec.recognize_beam(hs_pad[b], self.recog_args)\n            batch_nbest.append(nbest_hyps)\n\n        ys_hat = [nbest_hyp[0]['yseq'][1:] for nbest_hyp in batch_nbest]\n\n        seqs_hat, seqs_true = self.convert_to_char(ys_hat, ys_pad.cpu())\n\n        if self.report_cer:\n            cer = self.calculate_cer(seqs_hat, seqs_true)\n\n        if self.report_wer:\n            wer = self.calculate_wer(seqs_hat, seqs_true)\n\n        return cer, wer\n\n    def convert_to_char(self, ys_hat, ys_pad):\n        \"\"\"Convert index to character.\n\n        Args:\n            ys_hat (torch.Tensor): prediction (batch, seqlen)\n            ys_pad (torch.Tensor): reference (batch, seqlen)\n\n        Returns:\n            (list): token list of prediction\n            (list): token list of reference\n\n        \"\"\"\n        seqs_hat, seqs_true = [], []\n\n        for i, y_hat in enumerate(ys_hat):\n            y_true = ys_pad[i]\n\n            eos_true = np.where(y_true == -1)[0]\n            eos_true = eos_true[0] if len(eos_true) > 0 else len(y_true)\n\n            seq_hat = [self.char_list[int(idx)] for idx in y_hat[:eos_true]]\n            seq_true = [self.char_list[int(idx)] for idx in y_true if int(idx) != -1]\n\n            seq_hat_text = \"\".join(seq_hat).replace(self.space, ' ')\n            seq_hat_text = seq_hat_text.replace(self.blank, '')\n            seq_true_text = \"\".join(seq_true).replace(self.space, ' ')\n\n            seqs_hat.append(seq_hat_text)\n            seqs_true.append(seq_true_text)\n\n        return seqs_hat, seqs_true\n\n    def calculate_cer(self, seqs_hat, seqs_true):\n        \"\"\"Calculate sentence-level CER score for transducer model.\n\n        Args:\n            seqs_hat (torch.Tensor): prediction (batch, seqlen)\n            seqs_true (torch.Tensor): reference (batch, seqlen)\n\n        Returns:\n            (float): average sentence-level CER score\n\n        \"\"\"\n        char_eds, char_ref_lens = [], []\n\n        for i, seq_hat_text in enumerate(seqs_hat):\n            seq_true_text = seqs_true[i]\n            hyp_chars = seq_hat_text.replace(' ', '')\n            ref_chars = seq_true_text.replace(' ', '')\n\n            char_eds.append(editdistance.eval(hyp_chars, ref_chars))\n            char_ref_lens.append(len(ref_chars))\n\n        return float(sum(char_eds)) / sum(char_ref_lens)\n\n    def calculate_wer(self, seqs_hat, seqs_true):\n        \"\"\"Calculate sentence-level WER score for transducer model.\n\n        Args:\n            seqs_hat (torch.Tensor): prediction (batch, seqlen)\n            seqs_true (torch.Tensor): reference (batch, seqlen)\n\n        Returns:\n            (float): average sentence-level WER score\n\n        \"\"\"\n        word_eds, word_ref_lens = [], []\n\n        for i, seq_hat_text in enumerate(seqs_hat):\n            seq_true_text = seqs_true[i]\n            hyp_words = seq_hat_text.split()\n            ref_words = seq_true_text.split()\n\n            word_eds.append(editdistance.eval(hyp_words, ref_words))\n            word_ref_lens.append(len(ref_words))\n\n        return float(sum(word_eds)) / sum(word_ref_lens)\n", "models/ppg_extractor/log_mel.py": "import librosa\nimport numpy as np\nimport torch\nfrom typing import Tuple\n\nfrom .nets_utils import make_pad_mask\n\n\nclass LogMel(torch.nn.Module):\n    \"\"\"Convert STFT to fbank feats\n\n    The arguments is same as librosa.filters.mel\n\n    Args:\n        fs: number > 0 [scalar] sampling rate of the incoming signal\n        n_fft: int > 0 [scalar] number of FFT components\n        n_mels: int > 0 [scalar] number of Mel bands to generate\n        fmin: float >= 0 [scalar] lowest frequency (in Hz)\n        fmax: float >= 0 [scalar] highest frequency (in Hz).\n            If `None`, use `fmax = fs / 2.0`\n        htk: use HTK formula instead of Slaney\n        norm: {None, 1, np.inf} [scalar]\n            if 1, divide the triangular mel weights by the width of the mel band\n            (area normalization).  Otherwise, leave all the triangles aiming for\n            a peak value of 1.0\n\n    \"\"\"\n\n    def __init__(\n        self,\n        fs: int = 16000,\n        n_fft: int = 512,\n        n_mels: int = 80,\n        fmin: float = 0,\n        fmax: float = None,\n        htk: bool = False,\n        norm=1,\n    ):\n        super().__init__()\n\n        fmax = fs / 2 if fmax is None else fmax\n        _mel_options = dict(\n            sr=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, htk=htk, norm=norm\n        )\n        self.mel_options = _mel_options\n\n        # Note(kamo): The mel matrix of librosa is different from kaldi.\n        melmat = librosa.filters.mel(**_mel_options)\n        # melmat: (D2, D1) -> (D1, D2)\n        self.register_buffer(\"melmat\", torch.from_numpy(melmat.T).float())\n        inv_mel = np.linalg.pinv(melmat)\n        self.register_buffer(\"inv_melmat\", torch.from_numpy(inv_mel.T).float())\n\n    def extra_repr(self):\n        return \", \".join(f\"{k}={v}\" for k, v in self.mel_options.items())\n\n    def forward(\n        self, feat: torch.Tensor, ilens: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # feat: (B, T, D1) x melmat: (D1, D2) -> mel_feat: (B, T, D2)\n        mel_feat = torch.matmul(feat, self.melmat)\n\n        logmel_feat = (mel_feat + 1e-20).log()\n        # Zero padding\n        if ilens is not None:\n            logmel_feat = logmel_feat.masked_fill(\n                make_pad_mask(ilens, logmel_feat, 1), 0.0\n            )\n        else:\n            ilens = feat.new_full(\n                [feat.size(0)], fill_value=feat.size(1), dtype=torch.long\n            )\n        return logmel_feat, ilens\n", "models/ppg_extractor/__init__.py": "import argparse\nimport torch\nfrom pathlib import Path\nimport yaml\n\nfrom .frontend import DefaultFrontend\nfrom .utterance_mvn import UtteranceMVN\nfrom .encoder.conformer_encoder import ConformerEncoder\n\n_model = None # type: PPGModel\n_device = None\n\nclass PPGModel(torch.nn.Module):\n    def __init__(\n        self,\n        frontend,\n        normalizer,\n        encoder,\n    ):\n        super().__init__()\n        self.frontend = frontend\n        self.normalize = normalizer\n        self.encoder = encoder\n\n    def forward(self, speech, speech_lengths):\n        \"\"\"\n\n        Args:\n            speech (tensor): (B, L)\n            speech_lengths (tensor): (B, )\n\n        Returns:\n            bottle_neck_feats (tensor): (B, L//hop_size, 144)\n\n        \"\"\"\n        feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n        feats, feats_lengths = self.normalize(feats, feats_lengths)\n        encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)\n        return encoder_out\n\n    def _extract_feats(\n        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n    ):\n        assert speech_lengths.dim() == 1, speech_lengths.shape\n\n        # for data-parallel\n        speech = speech[:, : speech_lengths.max()]\n\n        if self.frontend is not None:\n            # Frontend\n            #  e.g. STFT and Feature extract\n            #       data_loader may send time-domain signal in this case\n            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)\n            feats, feats_lengths = self.frontend(speech, speech_lengths)\n        else:\n            # No frontend and no feature extract\n            feats, feats_lengths = speech, speech_lengths\n        return feats, feats_lengths\n        \n    def extract_from_wav(self, src_wav):\n        src_wav_tensor = torch.from_numpy(src_wav).unsqueeze(0).float().to(_device)\n        src_wav_lengths = torch.LongTensor([len(src_wav)]).to(_device)\n        return self(src_wav_tensor, src_wav_lengths)\n\n\ndef build_model(args):\n    normalizer = UtteranceMVN(**args.normalize_conf)\n    frontend = DefaultFrontend(**args.frontend_conf)\n    encoder = ConformerEncoder(input_size=80, **args.encoder_conf)\n    model = PPGModel(frontend, normalizer, encoder)\n    \n    return model\n\n\ndef load_model(model_file, device=None):\n    global _model, _device\n    \n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        _device = device\n    # search a config file\n    model_config_fpaths = list(model_file.parent.rglob(\"*.yaml\"))\n    config_file = model_config_fpaths[0]\n    with config_file.open(\"r\", encoding=\"utf-8\") as f:\n        args = yaml.safe_load(f)\n\n    args = argparse.Namespace(**args)\n\n    model = build_model(args)\n    model_state_dict = model.state_dict()\n\n    ckpt_state_dict = torch.load(model_file, map_location=_device)\n    ckpt_state_dict = {k:v for k,v in ckpt_state_dict.items() if 'encoder' in k}\n\n    model_state_dict.update(ckpt_state_dict)\n    model.load_state_dict(model_state_dict)\n\n    _model = model.eval().to(_device)\n    return _model\n\n\n", "models/ppg_extractor/utterance_mvn.py": "from typing import Tuple\n\nimport torch\n\nfrom .nets_utils import make_pad_mask\n\n\nclass UtteranceMVN(torch.nn.Module):\n    def __init__(\n        self, norm_means: bool = True, norm_vars: bool = False, eps: float = 1.0e-20,\n    ):\n        super().__init__()\n        self.norm_means = norm_means\n        self.norm_vars = norm_vars\n        self.eps = eps\n\n    def extra_repr(self):\n        return f\"norm_means={self.norm_means}, norm_vars={self.norm_vars}\"\n\n    def forward(\n        self, x: torch.Tensor, ilens: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward function\n\n        Args:\n            x: (B, L, ...)\n            ilens: (B,)\n\n        \"\"\"\n        return utterance_mvn(\n            x,\n            ilens,\n            norm_means=self.norm_means,\n            norm_vars=self.norm_vars,\n            eps=self.eps,\n        )\n\n\ndef utterance_mvn(\n    x: torch.Tensor,\n    ilens: torch.Tensor = None,\n    norm_means: bool = True,\n    norm_vars: bool = False,\n    eps: float = 1.0e-20,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Apply utterance mean and variance normalization\n\n    Args:\n        x: (B, T, D), assumed zero padded\n        ilens: (B,)\n        norm_means:\n        norm_vars:\n        eps:\n\n    \"\"\"\n    if ilens is None:\n        ilens = x.new_full([x.size(0)], x.size(1))\n    ilens_ = ilens.to(x.device, x.dtype).view(-1, *[1 for _ in range(x.dim() - 1)])\n    # Zero padding\n    if x.requires_grad:\n        x = x.masked_fill(make_pad_mask(ilens, x, 1), 0.0)\n    else:\n        x.masked_fill_(make_pad_mask(ilens, x, 1), 0.0)\n    # mean: (B, 1, D)\n    mean = x.sum(dim=1, keepdim=True) / ilens_\n\n    if norm_means:\n        x -= mean\n\n        if norm_vars:\n            var = x.pow(2).sum(dim=1, keepdim=True) / ilens_\n            std = torch.clamp(var.sqrt(), min=eps)\n            x = x / std.sqrt()\n        return x, ilens\n    else:\n        if norm_vars:\n            y = x - mean\n            y.masked_fill_(make_pad_mask(ilens, y, 1), 0.0)\n            var = y.pow(2).sum(dim=1, keepdim=True) / ilens_\n            std = torch.clamp(var.sqrt(), min=eps)\n            x /= std\n        return x, ilens\n", "models/ppg_extractor/encoders.py": "import logging\nimport six\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_packed_sequence\n\nfrom .e2e_asr_common import get_vgg2l_odim\nfrom .nets_utils import make_pad_mask, to_device\n\n\nclass RNNP(torch.nn.Module):\n    \"\"\"RNN with projection layer module\n\n    :param int idim: dimension of inputs\n    :param int elayers: number of encoder layers\n    :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional)\n    :param int hdim: number of projection units\n    :param np.ndarray subsample: list of subsampling numbers\n    :param float dropout: dropout rate\n    :param str typ: The RNN type\n    \"\"\"\n\n    def __init__(self, idim, elayers, cdim, hdim, subsample, dropout, typ=\"blstm\"):\n        super(RNNP, self).__init__()\n        bidir = typ[0] == \"b\"\n        for i in six.moves.range(elayers):\n            if i == 0:\n                inputdim = idim\n            else:\n                inputdim = hdim\n            rnn = torch.nn.LSTM(inputdim, cdim, dropout=dropout, num_layers=1, bidirectional=bidir,\n                                batch_first=True) if \"lstm\" in typ \\\n                else torch.nn.GRU(inputdim, cdim, dropout=dropout, num_layers=1, bidirectional=bidir, batch_first=True)\n            setattr(self, \"%s%d\" % (\"birnn\" if bidir else \"rnn\", i), rnn)\n            # bottleneck layer to merge\n            if bidir:\n                setattr(self, \"bt%d\" % i, torch.nn.Linear(2 * cdim, hdim))\n            else:\n                setattr(self, \"bt%d\" % i, torch.nn.Linear(cdim, hdim))\n\n        self.elayers = elayers\n        self.cdim = cdim\n        self.subsample = subsample\n        self.typ = typ\n        self.bidir = bidir\n\n    def forward(self, xs_pad, ilens, prev_state=None):\n        \"\"\"RNNP forward\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor prev_state: batch of previous RNN states\n        :return: batch of hidden state sequences (B, Tmax, hdim)\n        :rtype: torch.Tensor\n        \"\"\"\n        logging.debug(self.__class__.__name__ + ' input lengths: ' + str(ilens))\n        elayer_states = []\n        for layer in six.moves.range(self.elayers):\n            xs_pack = pack_padded_sequence(xs_pad, ilens, batch_first=True, enforce_sorted=False)\n            rnn = getattr(self, (\"birnn\" if self.bidir else \"rnn\") + str(layer))\n            rnn.flatten_parameters()\n            if prev_state is not None and rnn.bidirectional:\n                prev_state = reset_backward_rnn_state(prev_state)\n            ys, states = rnn(xs_pack, hx=None if prev_state is None else prev_state[layer])\n            elayer_states.append(states)\n            # ys: utt list of frame x cdim x 2 (2: means bidirectional)\n            ys_pad, ilens = pad_packed_sequence(ys, batch_first=True)\n            sub = self.subsample[layer + 1]\n            if sub > 1:\n                ys_pad = ys_pad[:, ::sub]\n                ilens = [int(i + 1) // sub for i in ilens]\n            # (sum _utt frame_utt) x dim\n            projected = getattr(self, 'bt' + str(layer)\n                                )(ys_pad.contiguous().view(-1, ys_pad.size(2)))\n            if layer == self.elayers - 1:\n                xs_pad = projected.view(ys_pad.size(0), ys_pad.size(1), -1)\n            else:\n                xs_pad = torch.tanh(projected.view(ys_pad.size(0), ys_pad.size(1), -1))\n\n        return xs_pad, ilens, elayer_states  # x: utt list of frame x dim\n\n\nclass RNN(torch.nn.Module):\n    \"\"\"RNN module\n\n    :param int idim: dimension of inputs\n    :param int elayers: number of encoder layers\n    :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional)\n    :param int hdim: number of final projection units\n    :param float dropout: dropout rate\n    :param str typ: The RNN type\n    \"\"\"\n\n    def __init__(self, idim, elayers, cdim, hdim, dropout, typ=\"blstm\"):\n        super(RNN, self).__init__()\n        bidir = typ[0] == \"b\"\n        self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,\n                                   dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\\n            else torch.nn.GRU(idim, cdim, elayers, batch_first=True, dropout=dropout,\n                              bidirectional=bidir)\n        if bidir:\n            self.l_last = torch.nn.Linear(cdim * 2, hdim)\n        else:\n            self.l_last = torch.nn.Linear(cdim, hdim)\n        self.typ = typ\n\n    def forward(self, xs_pad, ilens, prev_state=None):\n        \"\"\"RNN forward\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor prev_state: batch of previous RNN states\n        :return: batch of hidden state sequences (B, Tmax, eprojs)\n        :rtype: torch.Tensor\n        \"\"\"\n        logging.debug(self.__class__.__name__ + ' input lengths: ' + str(ilens))\n        xs_pack = pack_padded_sequence(xs_pad, ilens, batch_first=True)\n        self.nbrnn.flatten_parameters()\n        if prev_state is not None and self.nbrnn.bidirectional:\n            # We assume that when previous state is passed, it means that we're streaming the input\n            # and therefore cannot propagate backward BRNN state (otherwise it goes in the wrong direction)\n            prev_state = reset_backward_rnn_state(prev_state)\n        ys, states = self.nbrnn(xs_pack, hx=prev_state)\n        # ys: utt list of frame x cdim x 2 (2: means bidirectional)\n        ys_pad, ilens = pad_packed_sequence(ys, batch_first=True)\n        # (sum _utt frame_utt) x dim\n        projected = torch.tanh(self.l_last(\n            ys_pad.contiguous().view(-1, ys_pad.size(2))))\n        xs_pad = projected.view(ys_pad.size(0), ys_pad.size(1), -1)\n        return xs_pad, ilens, states  # x: utt list of frame x dim\n\n\ndef reset_backward_rnn_state(states):\n    \"\"\"Sets backward BRNN states to zeroes - useful in processing of sliding windows over the inputs\"\"\"\n    if isinstance(states, (list, tuple)):\n        for state in states:\n            state[1::2] = 0.\n    else:\n        states[1::2] = 0.\n    return states\n\n\nclass VGG2L(torch.nn.Module):\n    \"\"\"VGG-like module\n\n    :param int in_channel: number of input channels\n    \"\"\"\n\n    def __init__(self, in_channel=1, downsample=True):\n        super(VGG2L, self).__init__()\n        # CNN layer (VGG motivated)\n        self.conv1_1 = torch.nn.Conv2d(in_channel, 64, 3, stride=1, padding=1)\n        self.conv1_2 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        self.conv2_1 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n        self.conv2_2 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n\n        self.in_channel = in_channel\n        self.downsample = downsample\n        if downsample:\n            self.stride = 2\n        else:\n            self.stride = 1\n\n    def forward(self, xs_pad, ilens, **kwargs):\n        \"\"\"VGG2L forward\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :return: batch of padded hidden state sequences (B, Tmax // 4, 128 * D // 4) if downsample\n        :rtype: torch.Tensor\n        \"\"\"\n        logging.debug(self.__class__.__name__ + ' input lengths: ' + str(ilens))\n\n        # x: utt x frame x dim\n        # xs_pad = F.pad_sequence(xs_pad)\n\n        # x: utt x 1 (input channel num) x frame x dim\n        xs_pad = xs_pad.view(xs_pad.size(0), xs_pad.size(1), self.in_channel,\n                             xs_pad.size(2) // self.in_channel).transpose(1, 2)\n\n        # NOTE: max_pool1d ?\n        xs_pad = F.relu(self.conv1_1(xs_pad))\n        xs_pad = F.relu(self.conv1_2(xs_pad))\n        if self.downsample:\n            xs_pad = F.max_pool2d(xs_pad, 2, stride=self.stride, ceil_mode=True)\n\n        xs_pad = F.relu(self.conv2_1(xs_pad))\n        xs_pad = F.relu(self.conv2_2(xs_pad))\n        if self.downsample:\n            xs_pad = F.max_pool2d(xs_pad, 2, stride=self.stride, ceil_mode=True)\n        if torch.is_tensor(ilens):\n            ilens = ilens.cpu().numpy()\n        else:\n            ilens = np.array(ilens, dtype=np.float32)\n        if self.downsample:\n            ilens = np.array(np.ceil(ilens / 2), dtype=np.int64)\n            ilens = np.array(\n                np.ceil(np.array(ilens, dtype=np.float32) / 2), dtype=np.int64).tolist()\n\n        # x: utt_list of frame (remove zeropaded frames) x (input channel num x dim)\n        xs_pad = xs_pad.transpose(1, 2)\n        xs_pad = xs_pad.contiguous().view(\n            xs_pad.size(0), xs_pad.size(1), xs_pad.size(2) * xs_pad.size(3))\n        return xs_pad, ilens, None  # no state in this layer\n\n\nclass Encoder(torch.nn.Module):\n    \"\"\"Encoder module\n\n    :param str etype: type of encoder network\n    :param int idim: number of dimensions of encoder network\n    :param int elayers: number of layers of encoder network\n    :param int eunits: number of lstm units of encoder network\n    :param int eprojs: number of projection units of encoder network\n    :param np.ndarray subsample: list of subsampling numbers\n    :param float dropout: dropout rate\n    :param int in_channel: number of input channels\n    \"\"\"\n\n    def __init__(self, etype, idim, elayers, eunits, eprojs, subsample, dropout, in_channel=1):\n        super(Encoder, self).__init__()\n        typ = etype.lstrip(\"vgg\").rstrip(\"p\")\n        if typ not in ['lstm', 'gru', 'blstm', 'bgru']:\n            logging.error(\"Error: need to specify an appropriate encoder architecture\")\n\n        if etype.startswith(\"vgg\"):\n            if etype[-1] == \"p\":\n                self.enc = torch.nn.ModuleList([VGG2L(in_channel),\n                                                RNNP(get_vgg2l_odim(idim, in_channel=in_channel), elayers, eunits,\n                                                     eprojs,\n                                                     subsample, dropout, typ=typ)])\n                logging.info('Use CNN-VGG + ' + typ.upper() + 'P for encoder')\n            else:\n                self.enc = torch.nn.ModuleList([VGG2L(in_channel),\n                                                RNN(get_vgg2l_odim(idim, in_channel=in_channel), elayers, eunits,\n                                                    eprojs,\n                                                    dropout, typ=typ)])\n                logging.info('Use CNN-VGG + ' + typ.upper() + ' for encoder')\n        else:\n            if etype[-1] == \"p\":\n                self.enc = torch.nn.ModuleList(\n                    [RNNP(idim, elayers, eunits, eprojs, subsample, dropout, typ=typ)])\n                logging.info(typ.upper() + ' with every-layer projection for encoder')\n            else:\n                self.enc = torch.nn.ModuleList([RNN(idim, elayers, eunits, eprojs, dropout, typ=typ)])\n                logging.info(typ.upper() + ' without projection for encoder')\n\n    def forward(self, xs_pad, ilens, prev_states=None):\n        \"\"\"Encoder forward\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor prev_state: batch of previous encoder hidden states (?, ...)\n        :return: batch of hidden state sequences (B, Tmax, eprojs)\n        :rtype: torch.Tensor\n        \"\"\"\n        if prev_states is None:\n            prev_states = [None] * len(self.enc)\n        assert len(prev_states) == len(self.enc)\n\n        current_states = []\n        for module, prev_state in zip(self.enc, prev_states):\n            xs_pad, ilens, states = module(xs_pad, ilens, prev_state=prev_state)\n            current_states.append(states)\n\n        # make mask to remove bias value in padded part\n        mask = to_device(self, make_pad_mask(ilens).unsqueeze(-1))\n\n        return xs_pad.masked_fill(mask, 0.0), ilens, current_states\n\n\ndef encoder_for(args, idim, subsample):\n    \"\"\"Instantiates an encoder module given the program arguments\n\n    :param Namespace args: The arguments\n    :param int or List of integer idim: dimension of input, e.g. 83, or\n                                        List of dimensions of inputs, e.g. [83,83]\n    :param List or List of List subsample: subsample factors, e.g. [1,2,2,1,1], or\n                                        List of subsample factors of each encoder. e.g. [[1,2,2,1,1], [1,2,2,1,1]]\n    :rtype torch.nn.Module\n    :return: The encoder module\n    \"\"\"\n    num_encs = getattr(args, \"num_encs\", 1)  # use getattr to keep compatibility\n    if num_encs == 1:\n        # compatible with single encoder asr mode\n        return Encoder(args.etype, idim, args.elayers, args.eunits, args.eprojs, subsample, args.dropout_rate)\n    elif num_encs >= 1:\n        enc_list = torch.nn.ModuleList()\n        for idx in range(num_encs):\n            enc = Encoder(args.etype[idx], idim[idx], args.elayers[idx], args.eunits[idx], args.eprojs, subsample[idx],\n                          args.dropout_rate[idx])\n            enc_list.append(enc)\n        return enc_list\n    else:\n        raise ValueError(\"Number of encoders needs to be more than one. {}\".format(num_encs))\n", "models/ppg_extractor/encoder/attention.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Multi-Head Attention layer definition.\"\"\"\n\nimport math\n\nimport numpy\nimport torch\nfrom torch import nn\n\n\nclass MultiHeadedAttention(nn.Module):\n    \"\"\"Multi-Head Attention layer.\n\n    :param int n_head: the number of head s\n    :param int n_feat: the number of features\n    :param float dropout_rate: dropout rate\n\n    \"\"\"\n\n    def __init__(self, n_head, n_feat, dropout_rate):\n        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n        super(MultiHeadedAttention, self).__init__()\n        assert n_feat % n_head == 0\n        # We assume d_v always equals d_k\n        self.d_k = n_feat // n_head\n        self.h = n_head\n        self.linear_q = nn.Linear(n_feat, n_feat)\n        self.linear_k = nn.Linear(n_feat, n_feat)\n        self.linear_v = nn.Linear(n_feat, n_feat)\n        self.linear_out = nn.Linear(n_feat, n_feat)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n    def forward_qkv(self, query, key, value):\n        \"\"\"Transform query, key and value.\n\n        :param torch.Tensor query: (batch, time1, size)\n        :param torch.Tensor key: (batch, time2, size)\n        :param torch.Tensor value: (batch, time2, size)\n        :return torch.Tensor transformed query, key and value\n\n        \"\"\"\n        n_batch = query.size(0)\n        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n        q = q.transpose(1, 2)  # (batch, head, time1, d_k)\n        k = k.transpose(1, 2)  # (batch, head, time2, d_k)\n        v = v.transpose(1, 2)  # (batch, head, time2, d_k)\n\n        return q, k, v\n\n    def forward_attention(self, value, scores, mask):\n        \"\"\"Compute attention context vector.\n\n        :param torch.Tensor value: (batch, head, time2, size)\n        :param torch.Tensor scores: (batch, head, time1, time2)\n        :param torch.Tensor mask: (batch, 1, time2) or (batch, time1, time2)\n        :return torch.Tensor transformed `value` (batch, time1, d_model)\n            weighted by the attention score (batch, time1, time2)\n\n        \"\"\"\n        n_batch = value.size(0)\n        if mask is not None:\n            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)\n            min_value = float(\n                numpy.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min\n            )\n            scores = scores.masked_fill(mask, min_value)\n            self.attn = torch.softmax(scores, dim=-1).masked_fill(\n                mask, 0.0\n            )  # (batch, head, time1, time2)\n        else:\n            self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n\n        p_attn = self.dropout(self.attn)\n        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)\n        x = (\n            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)\n        )  # (batch, time1, d_model)\n\n        return self.linear_out(x)  # (batch, time1, d_model)\n\n    def forward(self, query, key, value, mask):\n        \"\"\"Compute 'Scaled Dot Product Attention'.\n\n        :param torch.Tensor query: (batch, time1, size)\n        :param torch.Tensor key: (batch, time2, size)\n        :param torch.Tensor value: (batch, time2, size)\n        :param torch.Tensor mask: (batch, 1, time2) or (batch, time1, time2)\n        :param torch.nn.Dropout dropout:\n        :return torch.Tensor: attention output (batch, time1, d_model)\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        return self.forward_attention(v, scores, mask)\n\n\nclass RelPositionMultiHeadedAttention(MultiHeadedAttention):\n    \"\"\"Multi-Head Attention layer with relative position encoding.\n\n    Paper: https://arxiv.org/abs/1901.02860\n\n    :param int n_head: the number of head s\n    :param int n_feat: the number of features\n    :param float dropout_rate: dropout rate\n\n    \"\"\"\n\n    def __init__(self, n_head, n_feat, dropout_rate):\n        \"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"\n        super().__init__(n_head, n_feat, dropout_rate)\n        # linear transformation for positional ecoding\n        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)\n        # these two learnable bias are used in matrix c and matrix d\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        self.pos_bias_u = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        self.pos_bias_v = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        torch.nn.init.xavier_uniform_(self.pos_bias_u)\n        torch.nn.init.xavier_uniform_(self.pos_bias_v)\n\n    def rel_shift(self, x, zero_triu=False):\n        \"\"\"Compute relative positinal encoding.\n\n        :param torch.Tensor x: (batch, time, size)\n        :param bool zero_triu: return the lower triangular part of the matrix\n        \"\"\"\n        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=-1)\n\n        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))\n        x = x_padded[:, :, 1:].view_as(x)\n\n        if zero_triu:\n            ones = torch.ones((x.size(2), x.size(3)))\n            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]\n\n        return x\n\n    def forward(self, query, key, value, pos_emb, mask):\n        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n\n        :param torch.Tensor query: (batch, time1, size)\n        :param torch.Tensor key: (batch, time2, size)\n        :param torch.Tensor value: (batch, time2, size)\n        :param torch.Tensor pos_emb: (batch, time1, size)\n        :param torch.Tensor mask: (batch, time1, time2)\n        :param torch.nn.Dropout dropout:\n        :return torch.Tensor: attention output  (batch, time1, d_model)\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n        q = q.transpose(1, 2)  # (batch, time1, head, d_k)\n\n        n_batch_pos = pos_emb.size(0)\n        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)\n        p = p.transpose(1, 2)  # (batch, head, time1, d_k)\n\n        # (batch, head, time1, d_k)\n        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)\n        # (batch, head, time1, d_k)\n        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)\n\n        # compute attention score\n        # first compute matrix a and matrix c\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        # (batch, head, time1, time2)\n        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n\n        # compute matrix b and matrix d\n        # (batch, head, time1, time2)\n        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n        matrix_bd = self.rel_shift(matrix_bd)\n\n        scores = (matrix_ac + matrix_bd) / math.sqrt(\n            self.d_k\n        )  # (batch, head, time1, time2)\n\n        return self.forward_attention(v, scores, mask)\n", "models/ppg_extractor/encoder/multi_layer_conv.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Layer modules for FFT block in FastSpeech (Feed-forward Transformer).\"\"\"\n\nimport torch\n\n\nclass MultiLayeredConv1d(torch.nn.Module):\n    \"\"\"Multi-layered conv1d for Transformer block.\n\n    This is a module of multi-leyered conv1d designed\n    to replace positionwise feed-forward network\n    in Transforner block, which is introduced in\n    `FastSpeech: Fast, Robust and Controllable Text to Speech`_.\n\n    .. _`FastSpeech: Fast, Robust and Controllable Text to Speech`:\n        https://arxiv.org/pdf/1905.09263.pdf\n\n    \"\"\"\n\n    def __init__(self, in_chans, hidden_chans, kernel_size, dropout_rate):\n        \"\"\"Initialize MultiLayeredConv1d module.\n\n        Args:\n            in_chans (int): Number of input channels.\n            hidden_chans (int): Number of hidden channels.\n            kernel_size (int): Kernel size of conv1d.\n            dropout_rate (float): Dropout rate.\n\n        \"\"\"\n        super(MultiLayeredConv1d, self).__init__()\n        self.w_1 = torch.nn.Conv1d(\n            in_chans,\n            hidden_chans,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n        )\n        self.w_2 = torch.nn.Conv1d(\n            hidden_chans,\n            in_chans,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n        )\n        self.dropout = torch.nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Batch of input tensors (B, ..., in_chans).\n\n        Returns:\n            Tensor: Batch of output tensors (B, ..., hidden_chans).\n\n        \"\"\"\n        x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)\n        return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1)\n\n\nclass Conv1dLinear(torch.nn.Module):\n    \"\"\"Conv1D + Linear for Transformer block.\n\n    A variant of MultiLayeredConv1d, which replaces second conv-layer to linear.\n\n    \"\"\"\n\n    def __init__(self, in_chans, hidden_chans, kernel_size, dropout_rate):\n        \"\"\"Initialize Conv1dLinear module.\n\n        Args:\n            in_chans (int): Number of input channels.\n            hidden_chans (int): Number of hidden channels.\n            kernel_size (int): Kernel size of conv1d.\n            dropout_rate (float): Dropout rate.\n\n        \"\"\"\n        super(Conv1dLinear, self).__init__()\n        self.w_1 = torch.nn.Conv1d(\n            in_chans,\n            hidden_chans,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n        )\n        self.w_2 = torch.nn.Linear(hidden_chans, in_chans)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Batch of input tensors (B, ..., in_chans).\n\n        Returns:\n            Tensor: Batch of output tensors (B, ..., hidden_chans).\n\n        \"\"\"\n        x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)\n        return self.w_2(self.dropout(x))\n", "models/ppg_extractor/encoder/layer_norm.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Layer normalization module.\"\"\"\n\nimport torch\n\n\nclass LayerNorm(torch.nn.LayerNorm):\n    \"\"\"Layer normalization module.\n\n    :param int nout: output dim size\n    :param int dim: dimension to be normalized\n    \"\"\"\n\n    def __init__(self, nout, dim=-1):\n        \"\"\"Construct an LayerNorm object.\"\"\"\n        super(LayerNorm, self).__init__(nout, eps=1e-12)\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"Apply layer normalization.\n\n        :param torch.Tensor x: input tensor\n        :return: layer normalized tensor\n        :rtype torch.Tensor\n        \"\"\"\n        if self.dim == -1:\n            return super(LayerNorm, self).forward(x)\n        return super(LayerNorm, self).forward(x.transpose(1, -1)).transpose(1, -1)\n", "models/ppg_extractor/encoder/convolution.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Johns Hopkins University (Shinji Watanabe)\n#                Northwestern Polytechnical University (Pengcheng Guo)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"ConvolutionModule definition.\"\"\"\n\nfrom torch import nn\n\n\nclass ConvolutionModule(nn.Module):\n    \"\"\"ConvolutionModule in Conformer model.\n\n    :param int channels: channels of cnn\n    :param int kernel_size: kernerl size of cnn\n\n    \"\"\"\n\n    def __init__(self, channels, kernel_size, activation=nn.ReLU(), bias=True):\n        \"\"\"Construct an ConvolutionModule object.\"\"\"\n        super(ConvolutionModule, self).__init__()\n        # kernerl_size should be a odd number for 'SAME' padding\n        assert (kernel_size - 1) % 2 == 0\n\n        self.pointwise_conv1 = nn.Conv1d(\n            channels,\n            2 * channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        self.depthwise_conv = nn.Conv1d(\n            channels,\n            channels,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n            groups=channels,\n            bias=bias,\n        )\n        self.norm = nn.BatchNorm1d(channels)\n        self.pointwise_conv2 = nn.Conv1d(\n            channels,\n            channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        self.activation = activation\n\n    def forward(self, x):\n        \"\"\"Compute convolution module.\n\n        :param torch.Tensor x: (batch, time, size)\n        :return torch.Tensor: convoluted `value` (batch, time, d_model)\n        \"\"\"\n        # exchange the temporal dimension and the feature dimension\n        x = x.transpose(1, 2)\n\n        # GLU mechanism\n        x = self.pointwise_conv1(x)  # (batch, 2*channel, dim)\n        x = nn.functional.glu(x, dim=1)  # (batch, channel, dim)\n\n        # 1D Depthwise Conv\n        x = self.depthwise_conv(x)\n        x = self.activation(self.norm(x))\n\n        x = self.pointwise_conv2(x)\n\n        return x.transpose(1, 2)\n", "models/ppg_extractor/encoder/vgg.py": "\"\"\"VGG2L definition for transformer-transducer.\"\"\"\n\nimport torch\n\n\nclass VGG2L(torch.nn.Module):\n    \"\"\"VGG2L module for transformer-transducer encoder.\"\"\"\n\n    def __init__(self, idim, odim):\n        \"\"\"Construct a VGG2L object.\n\n        Args:\n            idim (int): dimension of inputs\n            odim (int): dimension of outputs\n\n        \"\"\"\n        super(VGG2L, self).__init__()\n\n        self.vgg2l = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 64, 3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(64, 64, 3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((3, 2)),\n            torch.nn.Conv2d(64, 128, 3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2, 2)),\n        )\n\n        self.output = torch.nn.Linear(128 * ((idim // 2) // 2), odim)\n\n    def forward(self, x, x_mask):\n        \"\"\"VGG2L forward for x.\n\n        Args:\n            x (torch.Tensor): input torch (B, T, idim)\n            x_mask (torch.Tensor): (B, 1, T)\n\n        Returns:\n            x (torch.Tensor): input torch (B, sub(T), attention_dim)\n            x_mask (torch.Tensor): (B, 1, sub(T))\n\n        \"\"\"\n        x = x.unsqueeze(1)\n        x = self.vgg2l(x)\n\n        b, c, t, f = x.size()\n\n        x = self.output(x.transpose(1, 2).contiguous().view(b, t, c * f))\n\n        if x_mask is None:\n            return x, None\n        else:\n            x_mask = self.create_new_mask(x_mask, x)\n\n            return x, x_mask\n\n    def create_new_mask(self, x_mask, x):\n        \"\"\"Create a subsampled version of x_mask.\n\n        Args:\n            x_mask (torch.Tensor): (B, 1, T)\n            x (torch.Tensor): (B, sub(T), attention_dim)\n\n        Returns:\n            x_mask (torch.Tensor): (B, 1, sub(T))\n\n        \"\"\"\n        x_t1 = x_mask.size(2) - (x_mask.size(2) % 3)\n        x_mask = x_mask[:, :, :x_t1][:, :, ::3]\n\n        x_t2 = x_mask.size(2) - (x_mask.size(2) % 2)\n        x_mask = x_mask[:, :, :x_t2][:, :, ::2]\n\n        return x_mask\n", "models/ppg_extractor/encoder/encoder_layer.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Encoder self-attention layer definition.\"\"\"\n\nimport torch\n\nfrom torch import nn\n\nfrom .layer_norm import LayerNorm\n\n\nclass EncoderLayer(nn.Module):\n    \"\"\"Encoder layer module.\n\n    :param int size: input dim\n    :param espnet.nets.pytorch_backend.transformer.attention.\n        MultiHeadedAttention self_attn: self attention module\n        RelPositionMultiHeadedAttention self_attn: self attention module\n    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.\n        PositionwiseFeedForward feed_forward:\n        feed forward module\n    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward\n    for macaron style\n    PositionwiseFeedForward feed_forward:\n    feed forward module\n    :param espnet.nets.pytorch_backend.conformer.convolution.\n        ConvolutionModule feed_foreard:\n        feed forward module\n    :param float dropout_rate: dropout rate\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        size,\n        self_attn,\n        feed_forward,\n        feed_forward_macaron,\n        conv_module,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n    ):\n        \"\"\"Construct an EncoderLayer object.\"\"\"\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.feed_forward_macaron = feed_forward_macaron\n        self.conv_module = conv_module\n        self.norm_ff = LayerNorm(size)  # for the FNN module\n        self.norm_mha = LayerNorm(size)  # for the MHA module\n        if feed_forward_macaron is not None:\n            self.norm_ff_macaron = LayerNorm(size)\n            self.ff_scale = 0.5\n        else:\n            self.ff_scale = 1.0\n        if self.conv_module is not None:\n            self.norm_conv = LayerNorm(size)  # for the CNN module\n            self.norm_final = LayerNorm(size)  # for the final output of the block\n        self.dropout = nn.Dropout(dropout_rate)\n        self.size = size\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat_linear = nn.Linear(size + size, size)\n\n    def forward(self, x_input, mask, cache=None):\n        \"\"\"Compute encoded features.\n\n        :param torch.Tensor x_input: encoded source features, w/o pos_emb\n        tuple((batch, max_time_in, size), (1, max_time_in, size))\n        or (batch, max_time_in, size)\n        :param torch.Tensor mask: mask for x (batch, max_time_in)\n        :param torch.Tensor cache: cache for x (batch, max_time_in - 1, size)\n        :rtype: Tuple[torch.Tensor, torch.Tensor]\n        \"\"\"\n        if isinstance(x_input, tuple):\n            x, pos_emb = x_input[0], x_input[1]\n        else:\n            x, pos_emb = x_input, None\n\n        # whether to use macaron style\n        if self.feed_forward_macaron is not None:\n            residual = x\n            if self.normalize_before:\n                x = self.norm_ff_macaron(x)\n            x = residual + self.ff_scale * self.dropout(self.feed_forward_macaron(x))\n            if not self.normalize_before:\n                x = self.norm_ff_macaron(x)\n\n        # multi-headed self-attention module\n        residual = x\n        if self.normalize_before:\n            x = self.norm_mha(x)\n\n        if cache is None:\n            x_q = x\n        else:\n            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)\n            x_q = x[:, -1:, :]\n            residual = residual[:, -1:, :]\n            mask = None if mask is None else mask[:, -1:, :]\n\n        if pos_emb is not None:\n            x_att = self.self_attn(x_q, x, x, pos_emb, mask)\n        else:\n            x_att = self.self_attn(x_q, x, x, mask)\n\n        if self.concat_after:\n            x_concat = torch.cat((x, x_att), dim=-1)\n            x = residual + self.concat_linear(x_concat)\n        else:\n            x = residual + self.dropout(x_att)\n        if not self.normalize_before:\n            x = self.norm_mha(x)\n\n        # convolution module\n        if self.conv_module is not None:\n            residual = x\n            if self.normalize_before:\n                x = self.norm_conv(x)\n            x = residual + self.dropout(self.conv_module(x))\n            if not self.normalize_before:\n                x = self.norm_conv(x)\n\n        # feed forward module\n        residual = x\n        if self.normalize_before:\n            x = self.norm_ff(x)\n        x = residual + self.ff_scale * self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm_ff(x)\n\n        if self.conv_module is not None:\n            x = self.norm_final(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        if pos_emb is not None:\n            return (x, pos_emb), mask\n\n        return x, mask\n", "models/ppg_extractor/encoder/conformer_encoder.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Encoder definition.\"\"\"\n\nimport logging\nimport torch\nfrom typing import Callable\nfrom typing import Collection\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nfrom .convolution import ConvolutionModule\nfrom .encoder_layer import EncoderLayer\nfrom ..nets_utils import get_activation, make_pad_mask\nfrom .vgg import VGG2L\nfrom .attention import MultiHeadedAttention, RelPositionMultiHeadedAttention\nfrom .embedding import PositionalEncoding, ScaledPositionalEncoding, RelPositionalEncoding\nfrom .layer_norm import LayerNorm\nfrom .multi_layer_conv import Conv1dLinear, MultiLayeredConv1d\nfrom .positionwise_feed_forward import PositionwiseFeedForward\nfrom .repeat import repeat\nfrom .subsampling import Conv2dNoSubsampling, Conv2dSubsampling\n\n\nclass ConformerEncoder(torch.nn.Module):\n    \"\"\"Conformer encoder module.\n\n    :param int idim: input dim\n    :param int attention_dim: dimention of attention\n    :param int attention_heads: the number of heads of multi head attention\n    :param int linear_units: the number of units of position-wise feed forward\n    :param int num_blocks: the number of decoder blocks\n    :param float dropout_rate: dropout rate\n    :param float attention_dropout_rate: dropout rate in attention\n    :param float positional_dropout_rate: dropout rate after adding positional encoding\n    :param str or torch.nn.Module input_layer: input layer type\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    :param str positionwise_layer_type: linear of conv1d\n    :param int positionwise_conv_kernel_size: kernel size of positionwise conv1d layer\n    :param str encoder_pos_enc_layer_type: encoder positional encoding layer type\n    :param str encoder_attn_layer_type: encoder attention layer type\n    :param str activation_type: encoder activation function type\n    :param bool macaron_style: whether to use macaron style for positionwise layer\n    :param bool use_cnn_module: whether to use convolution module\n    :param int cnn_module_kernel: kernerl size of convolution module\n    :param int padding_idx: padding_idx for input_layer=embed\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size,\n        attention_dim=256,\n        attention_heads=4,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        attention_dropout_rate=0.0,\n        input_layer=\"conv2d\",\n        normalize_before=True,\n        concat_after=False,\n        positionwise_layer_type=\"linear\",\n        positionwise_conv_kernel_size=1,\n        macaron_style=False,\n        pos_enc_layer_type=\"abs_pos\",\n        selfattention_layer_type=\"selfattn\",\n        activation_type=\"swish\",\n        use_cnn_module=False,\n        cnn_module_kernel=31,\n        padding_idx=-1,\n        no_subsample=False,\n        subsample_by_2=False,\n    ):\n        \"\"\"Construct an Encoder object.\"\"\"\n        super().__init__()\n        \n        self._output_size = attention_dim\n        idim = input_size\n\n        activation = get_activation(activation_type)\n        if pos_enc_layer_type == \"abs_pos\":\n            pos_enc_class = PositionalEncoding\n        elif pos_enc_layer_type == \"scaled_abs_pos\":\n            pos_enc_class = ScaledPositionalEncoding\n        elif pos_enc_layer_type == \"rel_pos\":\n            assert selfattention_layer_type == \"rel_selfattn\"\n            pos_enc_class = RelPositionalEncoding\n        else:\n            raise ValueError(\"unknown pos_enc_layer: \" + pos_enc_layer_type)\n\n        if input_layer == \"linear\":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Linear(idim, attention_dim),\n                torch.nn.LayerNorm(attention_dim),\n                torch.nn.Dropout(dropout_rate),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer == \"conv2d\":\n            logging.info(\"Encoder input layer type: conv2d\")\n            if no_subsample:\n                self.embed = Conv2dNoSubsampling(\n                    idim,\n                    attention_dim,\n                    dropout_rate,\n                    pos_enc_class(attention_dim, positional_dropout_rate),\n                )\n            else:\n                self.embed = Conv2dSubsampling(\n                    idim,\n                    attention_dim,\n                    dropout_rate,\n                    pos_enc_class(attention_dim, positional_dropout_rate),\n                    subsample_by_2,  # NOTE(Sx): added by songxiang\n                )\n        elif input_layer == \"vgg2l\":\n            self.embed = VGG2L(idim, attention_dim)\n        elif input_layer == \"embed\":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif isinstance(input_layer, torch.nn.Module):\n            self.embed = torch.nn.Sequential(\n                input_layer,\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer is None:\n            self.embed = torch.nn.Sequential(\n                pos_enc_class(attention_dim, positional_dropout_rate)\n            )\n        else:\n            raise ValueError(\"unknown input_layer: \" + input_layer)\n        self.normalize_before = normalize_before\n        if positionwise_layer_type == \"linear\":\n            positionwise_layer = PositionwiseFeedForward\n            positionwise_layer_args = (\n                attention_dim,\n                linear_units,\n                dropout_rate,\n                activation,\n            )\n        elif positionwise_layer_type == \"conv1d\":\n            positionwise_layer = MultiLayeredConv1d\n            positionwise_layer_args = (\n                attention_dim,\n                linear_units,\n                positionwise_conv_kernel_size,\n                dropout_rate,\n            )\n        elif positionwise_layer_type == \"conv1d-linear\":\n            positionwise_layer = Conv1dLinear\n            positionwise_layer_args = (\n                attention_dim,\n                linear_units,\n                positionwise_conv_kernel_size,\n                dropout_rate,\n            )\n        else:\n            raise NotImplementedError(\"Support only linear or conv1d.\")\n\n        if selfattention_layer_type == \"selfattn\":\n            logging.info(\"encoder self-attention layer type = self-attention\")\n            encoder_selfattn_layer = MultiHeadedAttention\n            encoder_selfattn_layer_args = (\n                attention_heads,\n                attention_dim,\n                attention_dropout_rate,\n            )\n        elif selfattention_layer_type == \"rel_selfattn\":\n            assert pos_enc_layer_type == \"rel_pos\"\n            encoder_selfattn_layer = RelPositionMultiHeadedAttention\n            encoder_selfattn_layer_args = (\n                attention_heads,\n                attention_dim,\n                attention_dropout_rate,\n            )\n        else:\n            raise ValueError(\"unknown encoder_attn_layer: \" + selfattention_layer_type)\n\n        convolution_layer = ConvolutionModule\n        convolution_layer_args = (attention_dim, cnn_module_kernel, activation)\n\n        self.encoders = repeat(\n            num_blocks,\n            lambda lnum: EncoderLayer(\n                attention_dim,\n                encoder_selfattn_layer(*encoder_selfattn_layer_args),\n                positionwise_layer(*positionwise_layer_args),\n                positionwise_layer(*positionwise_layer_args) if macaron_style else None,\n                convolution_layer(*convolution_layer_args) if use_cnn_module else None,\n                dropout_rate,\n                normalize_before,\n                concat_after,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(attention_dim)\n    \n    def output_size(self) -> int:\n        return self._output_size \n    \n    def forward(\n        self,\n        xs_pad: torch.Tensor,\n        ilens: torch.Tensor,\n        prev_states: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"\n        Args:\n            xs_pad: input tensor (B, L, D)\n            ilens: input lengths (B)\n            prev_states: Not to be used now.\n        Returns:\n            Position embedded tensor and mask\n        \"\"\"\n        masks = (~make_pad_mask(ilens)[:, None, :]).to(xs_pad.device)\n\n        if isinstance(self.embed, (Conv2dSubsampling, Conv2dNoSubsampling, VGG2L)):\n            # print(xs_pad.shape)\n            xs_pad, masks = self.embed(xs_pad, masks)\n            # print(xs_pad[0].size())\n        else:\n            xs_pad = self.embed(xs_pad)\n        xs_pad, masks = self.encoders(xs_pad, masks)\n        if isinstance(xs_pad, tuple):\n            xs_pad = xs_pad[0]\n\n        if self.normalize_before:\n            xs_pad = self.after_norm(xs_pad)\n        olens = masks.squeeze(1).sum(1)\n        return xs_pad, olens, None\n    \n    # def forward(self, xs, masks):\n        # \"\"\"Encode input sequence.\n\n        # :param torch.Tensor xs: input tensor\n        # :param torch.Tensor masks: input mask\n        # :return: position embedded tensor and mask\n        # :rtype Tuple[torch.Tensor, torch.Tensor]:\n        # \"\"\"\n        # if isinstance(self.embed, (Conv2dSubsampling, VGG2L)):\n            # xs, masks = self.embed(xs, masks)\n        # else:\n            # xs = self.embed(xs)\n\n        # xs, masks = self.encoders(xs, masks)\n        # if isinstance(xs, tuple):\n            # xs = xs[0]\n\n        # if self.normalize_before:\n            # xs = self.after_norm(xs)\n        # return xs, masks\n", "models/ppg_extractor/encoder/subsampling.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Subsampling layer definition.\"\"\"\nimport logging\nimport torch\n\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n\n\nclass Conv2dSubsampling(torch.nn.Module):\n    \"\"\"Convolutional 2D subsampling (to 1/4 length or 1/2 length).\n\n    :param int idim: input dim\n    :param int odim: output dim\n    :param flaot dropout_rate: dropout rate\n    :param torch.nn.Module pos_enc: custom position encoding layer\n\n    \"\"\"\n\n    def __init__(self, idim, odim, dropout_rate, pos_enc=None, \n                 subsample_by_2=False,\n        ):\n        \"\"\"Construct an Conv2dSubsampling object.\"\"\"\n        super(Conv2dSubsampling, self).__init__()\n        self.subsample_by_2 = subsample_by_2\n        if subsample_by_2:\n            self.conv = torch.nn.Sequential(\n                torch.nn.Conv2d(1, odim, kernel_size=5, stride=1, padding=2),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(odim, odim, kernel_size=4, stride=2, padding=1),\n                torch.nn.ReLU(),\n            )\n            self.out = torch.nn.Sequential(\n                torch.nn.Linear(odim * (idim // 2), odim),\n                pos_enc if pos_enc is not None else PositionalEncoding(odim, dropout_rate),\n            )\n        else:\n            self.conv = torch.nn.Sequential(\n                torch.nn.Conv2d(1, odim, kernel_size=4, stride=2, padding=1),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(odim, odim, kernel_size=4, stride=2, padding=1),\n                torch.nn.ReLU(),\n            )\n            self.out = torch.nn.Sequential(\n                torch.nn.Linear(odim * (idim // 4), odim),\n                pos_enc if pos_enc is not None else PositionalEncoding(odim, dropout_rate),\n            )\n\n    def forward(self, x, x_mask):\n        \"\"\"Subsample x.\n\n        :param torch.Tensor x: input tensor\n        :param torch.Tensor x_mask: input mask\n        :return: subsampled x and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]\n\n        \"\"\"\n        x = x.unsqueeze(1)  # (b, c, t, f)\n        x = self.conv(x)\n        b, c, t, f = x.size()\n        x = self.out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n        if x_mask is None:\n            return x, None\n        if self.subsample_by_2:\n            return x, x_mask[:, :, ::2]\n        else:\n            return x, x_mask[:, :, ::2][:, :, ::2]\n\n    def __getitem__(self, key):\n        \"\"\"Subsample x.\n\n        When reset_parameters() is called, if use_scaled_pos_enc is used,\n            return the positioning encoding.\n\n        \"\"\"\n        if key != -1:\n            raise NotImplementedError(\"Support only `-1` (for `reset_parameters`).\")\n        return self.out[key]\n\n\nclass Conv2dNoSubsampling(torch.nn.Module):\n    \"\"\"Convolutional 2D without subsampling.\n\n    :param int idim: input dim\n    :param int odim: output dim\n    :param flaot dropout_rate: dropout rate\n    :param torch.nn.Module pos_enc: custom position encoding layer\n\n    \"\"\"\n\n    def __init__(self, idim, odim, dropout_rate, pos_enc=None):\n        \"\"\"Construct an Conv2dSubsampling object.\"\"\"\n        super().__init__()\n        logging.info(\"Encoder does not do down-sample on mel-spectrogram.\")\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(1, odim, kernel_size=5, stride=1, padding=2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(odim, odim, kernel_size=5, stride=1, padding=2),\n            torch.nn.ReLU(),\n        )\n        self.out = torch.nn.Sequential(\n            torch.nn.Linear(odim * idim, odim),\n            pos_enc if pos_enc is not None else PositionalEncoding(odim, dropout_rate),\n        )\n\n    def forward(self, x, x_mask):\n        \"\"\"Subsample x.\n\n        :param torch.Tensor x: input tensor\n        :param torch.Tensor x_mask: input mask\n        :return: subsampled x and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]\n\n        \"\"\"\n        x = x.unsqueeze(1)  # (b, c, t, f)\n        x = self.conv(x)\n        b, c, t, f = x.size()\n        x = self.out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n        if x_mask is None:\n            return x, None\n        return x, x_mask\n\n    def __getitem__(self, key):\n        \"\"\"Subsample x.\n\n        When reset_parameters() is called, if use_scaled_pos_enc is used,\n            return the positioning encoding.\n\n        \"\"\"\n        if key != -1:\n            raise NotImplementedError(\"Support only `-1` (for `reset_parameters`).\")\n        return self.out[key]\n\n\nclass Conv2dSubsampling6(torch.nn.Module):\n    \"\"\"Convolutional 2D subsampling (to 1/6 length).\n\n    :param int idim: input dim\n    :param int odim: output dim\n    :param flaot dropout_rate: dropout rate\n\n    \"\"\"\n\n    def __init__(self, idim, odim, dropout_rate):\n        \"\"\"Construct an Conv2dSubsampling object.\"\"\"\n        super(Conv2dSubsampling6, self).__init__()\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(1, odim, 3, 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(odim, odim, 5, 3),\n            torch.nn.ReLU(),\n        )\n        self.out = torch.nn.Sequential(\n            torch.nn.Linear(odim * (((idim - 1) // 2 - 2) // 3), odim),\n            PositionalEncoding(odim, dropout_rate),\n        )\n\n    def forward(self, x, x_mask):\n        \"\"\"Subsample x.\n\n        :param torch.Tensor x: input tensor\n        :param torch.Tensor x_mask: input mask\n        :return: subsampled x and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]\n        \"\"\"\n        x = x.unsqueeze(1)  # (b, c, t, f)\n        x = self.conv(x)\n        b, c, t, f = x.size()\n        x = self.out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n        if x_mask is None:\n            return x, None\n        return x, x_mask[:, :, :-2:2][:, :, :-4:3]\n\n\nclass Conv2dSubsampling8(torch.nn.Module):\n    \"\"\"Convolutional 2D subsampling (to 1/8 length).\n\n    :param int idim: input dim\n    :param int odim: output dim\n    :param flaot dropout_rate: dropout rate\n\n    \"\"\"\n\n    def __init__(self, idim, odim, dropout_rate):\n        \"\"\"Construct an Conv2dSubsampling object.\"\"\"\n        super(Conv2dSubsampling8, self).__init__()\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(1, odim, 3, 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(odim, odim, 3, 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(odim, odim, 3, 2),\n            torch.nn.ReLU(),\n        )\n        self.out = torch.nn.Sequential(\n            torch.nn.Linear(odim * ((((idim - 1) // 2 - 1) // 2 - 1) // 2), odim),\n            PositionalEncoding(odim, dropout_rate),\n        )\n\n    def forward(self, x, x_mask):\n        \"\"\"Subsample x.\n\n        :param torch.Tensor x: input tensor\n        :param torch.Tensor x_mask: input mask\n        :return: subsampled x and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]\n        \"\"\"\n        x = x.unsqueeze(1)  # (b, c, t, f)\n        x = self.conv(x)\n        b, c, t, f = x.size()\n        x = self.out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n        if x_mask is None:\n            return x, None\n        return x, x_mask[:, :, :-2:2][:, :, :-2:2][:, :, :-2:2]\n", "models/ppg_extractor/encoder/positionwise_feed_forward.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Positionwise feed forward layer definition.\"\"\"\n\nimport torch\n\n\nclass PositionwiseFeedForward(torch.nn.Module):\n    \"\"\"Positionwise feed forward layer.\n\n    :param int idim: input dimenstion\n    :param int hidden_units: number of hidden units\n    :param float dropout_rate: dropout rate\n\n    \"\"\"\n\n    def __init__(self, idim, hidden_units, dropout_rate, activation=torch.nn.ReLU()):\n        \"\"\"Construct an PositionwiseFeedForward object.\"\"\"\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = torch.nn.Linear(idim, hidden_units)\n        self.w_2 = torch.nn.Linear(hidden_units, idim)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n        self.activation = activation\n\n    def forward(self, x):\n        \"\"\"Forward funciton.\"\"\"\n        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n", "models/ppg_extractor/encoder/embedding.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Positonal Encoding Module.\"\"\"\n\nimport math\n\nimport torch\n\n\ndef _pre_hook(\n    state_dict,\n    prefix,\n    local_metadata,\n    strict,\n    missing_keys,\n    unexpected_keys,\n    error_msgs,\n):\n    \"\"\"Perform pre-hook in load_state_dict for backward compatibility.\n\n    Note:\n        We saved self.pe until v.0.5.2 but we have omitted it later.\n        Therefore, we remove the item \"pe\" from `state_dict` for backward compatibility.\n\n    \"\"\"\n    k = prefix + \"pe\"\n    if k in state_dict:\n        state_dict.pop(k)\n\n\nclass PositionalEncoding(torch.nn.Module):\n    \"\"\"Positional encoding.\n\n    :param int d_model: embedding dim\n    :param float dropout_rate: dropout rate\n    :param int max_len: maximum input length\n    :param reverse: whether to reverse the input position\n\n    \"\"\"\n\n    def __init__(self, d_model, dropout_rate, max_len=5000, reverse=False):\n        \"\"\"Construct an PositionalEncoding object.\"\"\"\n        super(PositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.reverse = reverse\n        self.xscale = math.sqrt(self.d_model)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        self.pe = None\n        self.extend_pe(torch.tensor(0.0).expand(1, max_len))\n        self._register_load_state_dict_pre_hook(_pre_hook)\n\n    def extend_pe(self, x):\n        \"\"\"Reset the positional encodings.\"\"\"\n        if self.pe is not None:\n            if self.pe.size(1) >= x.size(1):\n                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n                return\n        pe = torch.zeros(x.size(1), self.d_model)\n        if self.reverse:\n            position = torch.arange(\n                x.size(1) - 1, -1, -1.0, dtype=torch.float32\n            ).unsqueeze(1)\n        else:\n            position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float32)\n            * -(math.log(10000.0) / self.d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.pe = pe.to(device=x.device, dtype=x.dtype)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Add positional encoding.\n\n        Args:\n            x (torch.Tensor): Input. Its shape is (batch, time, ...)\n\n        Returns:\n            torch.Tensor: Encoded tensor. Its shape is (batch, time, ...)\n\n        \"\"\"\n        self.extend_pe(x)\n        x = x * self.xscale + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass ScaledPositionalEncoding(PositionalEncoding):\n    \"\"\"Scaled positional encoding module.\n\n    See also: Sec. 3.2  https://arxiv.org/pdf/1809.08895.pdf\n\n    \"\"\"\n\n    def __init__(self, d_model, dropout_rate, max_len=5000):\n        \"\"\"Initialize class.\n\n        :param int d_model: embedding dim\n        :param float dropout_rate: dropout rate\n        :param int max_len: maximum input length\n\n        \"\"\"\n        super().__init__(d_model=d_model, dropout_rate=dropout_rate, max_len=max_len)\n        self.alpha = torch.nn.Parameter(torch.tensor(1.0))\n\n    def reset_parameters(self):\n        \"\"\"Reset parameters.\"\"\"\n        self.alpha.data = torch.tensor(1.0)\n\n    def forward(self, x):\n        \"\"\"Add positional encoding.\n\n        Args:\n            x (torch.Tensor): Input. Its shape is (batch, time, ...)\n\n        Returns:\n            torch.Tensor: Encoded tensor. Its shape is (batch, time, ...)\n\n        \"\"\"\n        self.extend_pe(x)\n        x = x + self.alpha * self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass RelPositionalEncoding(PositionalEncoding):\n    \"\"\"Relitive positional encoding module.\n\n    See : Appendix B in https://arxiv.org/abs/1901.02860\n\n    :param int d_model: embedding dim\n    :param float dropout_rate: dropout rate\n    :param int max_len: maximum input length\n\n    \"\"\"\n\n    def __init__(self, d_model, dropout_rate, max_len=5000):\n        \"\"\"Initialize class.\n\n        :param int d_model: embedding dim\n        :param float dropout_rate: dropout rate\n        :param int max_len: maximum input length\n\n        \"\"\"\n        super().__init__(d_model, dropout_rate, max_len, reverse=True)\n\n    def forward(self, x):\n        \"\"\"Compute positional encoding.\n\n        Args:\n            x (torch.Tensor): Input. Its shape is (batch, time, ...)\n\n        Returns:\n            torch.Tensor: x. Its shape is (batch, time, ...)\n            torch.Tensor: pos_emb. Its shape is (1, time, ...)\n\n        \"\"\"\n        self.extend_pe(x)\n        x = x * self.xscale\n        pos_emb = self.pe[:, : x.size(1)]\n        return self.dropout(x), self.dropout(pos_emb)\n", "models/ppg_extractor/encoder/__init__.py": "", "models/ppg_extractor/encoder/encoder.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Encoder definition.\"\"\"\n\nimport logging\nimport torch\n\nfrom espnet.nets.pytorch_backend.conformer.convolution import ConvolutionModule\nfrom espnet.nets.pytorch_backend.conformer.encoder_layer import EncoderLayer\nfrom espnet.nets.pytorch_backend.nets_utils import get_activation\nfrom espnet.nets.pytorch_backend.transducer.vgg import VGG2L\nfrom espnet.nets.pytorch_backend.transformer.attention import (\n    MultiHeadedAttention,  # noqa: H301\n    RelPositionMultiHeadedAttention,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.embedding import (\n    PositionalEncoding,  # noqa: H301\n    ScaledPositionalEncoding,  # noqa: H301\n    RelPositionalEncoding,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.pytorch_backend.transformer.multi_layer_conv import Conv1dLinear\nfrom espnet.nets.pytorch_backend.transformer.multi_layer_conv import MultiLayeredConv1d\nfrom espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n    PositionwiseFeedForward,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.repeat import repeat\nfrom espnet.nets.pytorch_backend.transformer.subsampling import Conv2dSubsampling\n\n\nclass Encoder(torch.nn.Module):\n    \"\"\"Conformer encoder module.\n\n    :param int idim: input dim\n    :param int attention_dim: dimention of attention\n    :param int attention_heads: the number of heads of multi head attention\n    :param int linear_units: the number of units of position-wise feed forward\n    :param int num_blocks: the number of decoder blocks\n    :param float dropout_rate: dropout rate\n    :param float attention_dropout_rate: dropout rate in attention\n    :param float positional_dropout_rate: dropout rate after adding positional encoding\n    :param str or torch.nn.Module input_layer: input layer type\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    :param str positionwise_layer_type: linear of conv1d\n    :param int positionwise_conv_kernel_size: kernel size of positionwise conv1d layer\n    :param str encoder_pos_enc_layer_type: encoder positional encoding layer type\n    :param str encoder_attn_layer_type: encoder attention layer type\n    :param str activation_type: encoder activation function type\n    :param bool macaron_style: whether to use macaron style for positionwise layer\n    :param bool use_cnn_module: whether to use convolution module\n    :param int cnn_module_kernel: kernerl size of convolution module\n    :param int padding_idx: padding_idx for input_layer=embed\n    \"\"\"\n\n    def __init__(\n        self,\n        idim,\n        attention_dim=256,\n        attention_heads=4,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        attention_dropout_rate=0.0,\n        input_layer=\"conv2d\",\n        normalize_before=True,\n        concat_after=False,\n        positionwise_layer_type=\"linear\",\n        positionwise_conv_kernel_size=1,\n        macaron_style=False,\n        pos_enc_layer_type=\"abs_pos\",\n        selfattention_layer_type=\"selfattn\",\n        activation_type=\"swish\",\n        use_cnn_module=False,\n        cnn_module_kernel=31,\n        padding_idx=-1,\n    ):\n        \"\"\"Construct an Encoder object.\"\"\"\n        super(Encoder, self).__init__()\n\n        activation = get_activation(activation_type)\n        if pos_enc_layer_type == \"abs_pos\":\n            pos_enc_class = PositionalEncoding\n        elif pos_enc_layer_type == \"scaled_abs_pos\":\n            pos_enc_class = ScaledPositionalEncoding\n        elif pos_enc_layer_type == \"rel_pos\":\n            assert selfattention_layer_type == \"rel_selfattn\"\n            pos_enc_class = RelPositionalEncoding\n        else:\n            raise ValueError(\"unknown pos_enc_layer: \" + pos_enc_layer_type)\n\n        if input_layer == \"linear\":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Linear(idim, attention_dim),\n                torch.nn.LayerNorm(attention_dim),\n                torch.nn.Dropout(dropout_rate),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer == \"conv2d\":\n            self.embed = Conv2dSubsampling(\n                idim,\n                attention_dim,\n                dropout_rate,\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer == \"vgg2l\":\n            self.embed = VGG2L(idim, attention_dim)\n        elif input_layer == \"embed\":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif isinstance(input_layer, torch.nn.Module):\n            self.embed = torch.nn.Sequential(\n                input_layer,\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer is None:\n            self.embed = torch.nn.Sequential(\n                pos_enc_class(attention_dim, positional_dropout_rate)\n            )\n        else:\n            raise ValueError(\"unknown input_layer: \" + input_layer)\n        self.normalize_before = normalize_before\n        if positionwise_layer_type == \"linear\":\n            positionwise_layer = PositionwiseFeedForward\n            positionwise_layer_args = (\n                attention_dim,\n                linear_units,\n                dropout_rate,\n                activation,\n            )\n        elif positionwise_layer_type == \"conv1d\":\n            positionwise_layer = MultiLayeredConv1d\n            positionwise_layer_args = (\n                attention_dim,\n                linear_units,\n                positionwise_conv_kernel_size,\n                dropout_rate,\n            )\n        elif positionwise_layer_type == \"conv1d-linear\":\n            positionwise_layer = Conv1dLinear\n            positionwise_layer_args = (\n                attention_dim,\n                linear_units,\n                positionwise_conv_kernel_size,\n                dropout_rate,\n            )\n        else:\n            raise NotImplementedError(\"Support only linear or conv1d.\")\n\n        if selfattention_layer_type == \"selfattn\":\n            logging.info(\"encoder self-attention layer type = self-attention\")\n            encoder_selfattn_layer = MultiHeadedAttention\n            encoder_selfattn_layer_args = (\n                attention_heads,\n                attention_dim,\n                attention_dropout_rate,\n            )\n        elif selfattention_layer_type == \"rel_selfattn\":\n            assert pos_enc_layer_type == \"rel_pos\"\n            encoder_selfattn_layer = RelPositionMultiHeadedAttention\n            encoder_selfattn_layer_args = (\n                attention_heads,\n                attention_dim,\n                attention_dropout_rate,\n            )\n        else:\n            raise ValueError(\"unknown encoder_attn_layer: \" + selfattention_layer_type)\n\n        convolution_layer = ConvolutionModule\n        convolution_layer_args = (attention_dim, cnn_module_kernel, activation)\n\n        self.encoders = repeat(\n            num_blocks,\n            lambda lnum: EncoderLayer(\n                attention_dim,\n                encoder_selfattn_layer(*encoder_selfattn_layer_args),\n                positionwise_layer(*positionwise_layer_args),\n                positionwise_layer(*positionwise_layer_args) if macaron_style else None,\n                convolution_layer(*convolution_layer_args) if use_cnn_module else None,\n                dropout_rate,\n                normalize_before,\n                concat_after,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(attention_dim)\n\n    def forward(self, xs, masks):\n        \"\"\"Encode input sequence.\n\n        :param torch.Tensor xs: input tensor\n        :param torch.Tensor masks: input mask\n        :return: position embedded tensor and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        if isinstance(self.embed, (Conv2dSubsampling, VGG2L)):\n            xs, masks = self.embed(xs, masks)\n        else:\n            xs = self.embed(xs)\n\n        xs, masks = self.encoders(xs, masks)\n        if isinstance(xs, tuple):\n            xs = xs[0]\n\n        if self.normalize_before:\n            xs = self.after_norm(xs)\n        return xs, masks\n", "models/ppg_extractor/encoder/repeat.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Repeat the same layer definition.\"\"\"\n\nimport torch\n\n\nclass MultiSequential(torch.nn.Sequential):\n    \"\"\"Multi-input multi-output torch.nn.Sequential.\"\"\"\n\n    def forward(self, *args):\n        \"\"\"Repeat.\"\"\"\n        for m in self:\n            args = m(*args)\n        return args\n\n\ndef repeat(N, fn):\n    \"\"\"Repeat module N times.\n\n    :param int N: repeat time\n    :param function fn: function to generate module\n    :return: repeated modules\n    :rtype: MultiSequential\n    \"\"\"\n    return MultiSequential(*[fn(n) for n in range(N)])\n", "models/ppg_extractor/encoder/swish.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Johns Hopkins University (Shinji Watanabe)\n#                Northwestern Polytechnical University (Pengcheng Guo)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Swish() activation function for Conformer.\"\"\"\n\nimport torch\n\n\nclass Swish(torch.nn.Module):\n    \"\"\"Construct an Swish object.\"\"\"\n\n    def forward(self, x):\n        \"\"\"Return Swich activation function.\"\"\"\n        return x * torch.sigmoid(x)\n", "utils/logmmse.py": "# The MIT License (MIT)\n# \n# Copyright (c) 2015 braindead\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n#\n# This code was extracted from the logmmse package (https://pypi.org/project/logmmse/) and I\n# simply modified the interface to meet my needs.\n\n\nimport numpy as np\nimport math\nfrom scipy.special import expn\nfrom collections import namedtuple\n\nNoiseProfile = namedtuple(\"NoiseProfile\", \"sampling_rate window_size len1 len2 win n_fft noise_mu2\")\n\n\ndef profile_noise(noise, sampling_rate, window_size=0):\n    \"\"\"\n    Creates a profile of the noise in a given waveform.\n    \n    :param noise: a waveform containing noise ONLY, as a numpy array of floats or ints. \n    :param sampling_rate: the sampling rate of the audio\n    :param window_size: the size of the window the logmmse algorithm operates on. A default value \n    will be picked if left as 0.\n    :return: a NoiseProfile object\n    \"\"\"\n    noise, dtype = to_float(noise)\n    noise += np.finfo(np.float64).eps\n\n    if window_size == 0:\n        window_size = int(math.floor(0.02 * sampling_rate))\n\n    if window_size % 2 == 1:\n        window_size = window_size + 1\n    \n    perc = 50\n    len1 = int(math.floor(window_size * perc / 100))\n    len2 = int(window_size - len1)\n\n    win = np.hanning(window_size)\n    win = win * len2 / np.sum(win)\n    n_fft = 2 * window_size\n\n    noise_mean = np.zeros(n_fft)\n    n_frames = len(noise) // window_size\n    for j in range(0, window_size * n_frames, window_size):\n        noise_mean += np.absolute(np.fft.fft(win * noise[j:j + window_size], n_fft, axis=0))\n    noise_mu2 = (noise_mean / n_frames) ** 2\n    \n    return NoiseProfile(sampling_rate, window_size, len1, len2, win, n_fft, noise_mu2)\n\n\ndef denoise(wav, noise_profile: NoiseProfile, eta=0.15):\n    \"\"\"\n    Cleans the noise from a speech waveform given a noise profile. The waveform must have the \n    same sampling rate as the one used to create the noise profile. \n    \n    :param wav: a speech waveform as a numpy array of floats or ints.\n    :param noise_profile: a NoiseProfile object that was created from a similar (or a segment of \n    the same) waveform.\n    :param eta: voice threshold for noise update. While the voice activation detection value is \n    below this threshold, the noise profile will be continuously updated throughout the audio. \n    Set to 0 to disable updating the noise profile.\n    :return: the clean wav as a numpy array of floats or ints of the same length.\n    \"\"\"\n    wav, dtype = to_float(wav)\n    wav += np.finfo(np.float64).eps\n    p = noise_profile\n    \n    nframes = int(math.floor(len(wav) / p.len2) - math.floor(p.window_size / p.len2))\n    x_final = np.zeros(nframes * p.len2)\n\n    aa = 0.98\n    mu = 0.98\n    ksi_min = 10 ** (-25 / 10)\n    \n    x_old = np.zeros(p.len1)\n    xk_prev = np.zeros(p.len1)\n    noise_mu2 = p.noise_mu2\n    for k in range(0, nframes * p.len2, p.len2):\n        insign = p.win * wav[k:k + p.window_size]\n\n        spec = np.fft.fft(insign, p.n_fft, axis=0)\n        sig = np.absolute(spec)\n        sig2 = sig ** 2\n\n        gammak = np.minimum(sig2 / noise_mu2, 40)\n\n        if xk_prev.all() == 0:\n            ksi = aa + (1 - aa) * np.maximum(gammak - 1, 0)\n        else:\n            ksi = aa * xk_prev / noise_mu2 + (1 - aa) * np.maximum(gammak - 1, 0)\n            ksi = np.maximum(ksi_min, ksi)\n\n        log_sigma_k = gammak * ksi/(1 + ksi) - np.log(1 + ksi)\n        vad_decision = np.sum(log_sigma_k) / p.window_size\n        if vad_decision < eta:\n            noise_mu2 = mu * noise_mu2 + (1 - mu) * sig2\n\n        a = ksi / (1 + ksi)\n        vk = a * gammak\n        ei_vk = 0.5 * expn(1, np.maximum(vk, 1e-8))\n        hw = a * np.exp(ei_vk)\n        sig = sig * hw\n        xk_prev = sig ** 2\n        xi_w = np.fft.ifft(hw * spec, p.n_fft, axis=0)\n        xi_w = np.real(xi_w)\n\n        x_final[k:k + p.len2] = x_old + xi_w[0:p.len1]\n        x_old = xi_w[p.len1:p.window_size]\n\n    output = from_float(x_final, dtype)\n    output = np.pad(output, (0, len(wav) - len(output)), mode=\"constant\")\n    return output\n\n\n## Alternative VAD algorithm to webrctvad. It has the advantage of not requiring to install that \n## darn package and it also works for any sampling rate. Maybe I'll eventually use it instead of \n## webrctvad\n# def vad(wav, sampling_rate, eta=0.15, window_size=0):\n#     \"\"\"\n#     TODO: fix doc\n#     Creates a profile of the noise in a given waveform.\n# \n#     :param wav: a waveform containing noise ONLY, as a numpy array of floats or ints. \n#     :param sampling_rate: the sampling rate of the audio\n#     :param window_size: the size of the window the logmmse algorithm operates on. A default value \n#     will be picked if left as 0.\n#     :param eta: voice threshold for noise update. While the voice activation detection value is \n#     below this threshold, the noise profile will be continuously updated throughout the audio. \n#     Set to 0 to disable updating the noise profile.\n#     \"\"\"\n#     wav, dtype = to_float(wav)\n#     wav += np.finfo(np.float64).eps\n#     \n#     if window_size == 0:\n#         window_size = int(math.floor(0.02 * sampling_rate))\n#     \n#     if window_size % 2 == 1:\n#         window_size = window_size + 1\n#     \n#     perc = 50\n#     len1 = int(math.floor(window_size * perc / 100))\n#     len2 = int(window_size - len1)\n#     \n#     win = np.hanning(window_size)\n#     win = win * len2 / np.sum(win)\n#     n_fft = 2 * window_size\n#     \n#     wav_mean = np.zeros(n_fft)\n#     n_frames = len(wav) // window_size\n#     for j in range(0, window_size * n_frames, window_size):\n#         wav_mean += np.absolute(np.fft.fft(win * wav[j:j + window_size], n_fft, axis=0))\n#     noise_mu2 = (wav_mean / n_frames) ** 2\n#     \n#     wav, dtype = to_float(wav)\n#     wav += np.finfo(np.float64).eps\n#     \n#     nframes = int(math.floor(len(wav) / len2) - math.floor(window_size / len2))\n#     vad = np.zeros(nframes * len2, dtype=np.bool)\n# \n#     aa = 0.98\n#     mu = 0.98\n#     ksi_min = 10 ** (-25 / 10)\n#     \n#     xk_prev = np.zeros(len1)\n#     noise_mu2 = noise_mu2\n#     for k in range(0, nframes * len2, len2):\n#         insign = win * wav[k:k + window_size]\n#         \n#         spec = np.fft.fft(insign, n_fft, axis=0)\n#         sig = np.absolute(spec)\n#         sig2 = sig ** 2\n#         \n#         gammak = np.minimum(sig2 / noise_mu2, 40)\n#         \n#         if xk_prev.all() == 0:\n#             ksi = aa + (1 - aa) * np.maximum(gammak - 1, 0)\n#         else:\n#             ksi = aa * xk_prev / noise_mu2 + (1 - aa) * np.maximum(gammak - 1, 0)\n#             ksi = np.maximum(ksi_min, ksi)\n#         \n#         log_sigma_k = gammak * ksi / (1 + ksi) - np.log(1 + ksi)\n#         vad_decision = np.sum(log_sigma_k) / window_size\n#         if vad_decision < eta:\n#             noise_mu2 = mu * noise_mu2 + (1 - mu) * sig2\n#         print(vad_decision)\n#         \n#         a = ksi / (1 + ksi)\n#         vk = a * gammak\n#         ei_vk = 0.5 * expn(1, np.maximum(vk, 1e-8))\n#         hw = a * np.exp(ei_vk)\n#         sig = sig * hw\n#         xk_prev = sig ** 2\n#         \n#         vad[k:k + len2] = vad_decision >= eta\n#         \n#     vad = np.pad(vad, (0, len(wav) - len(vad)), mode=\"constant\")\n#     return vad\n\n\ndef to_float(_input):\n    if _input.dtype == np.float64:\n        return _input, _input.dtype\n    elif _input.dtype == np.float32:\n        return _input.astype(np.float64), _input.dtype\n    elif _input.dtype == np.uint8:\n        return (_input - 128) / 128., _input.dtype\n    elif _input.dtype == np.int16:\n        return _input / 32768., _input.dtype\n    elif _input.dtype == np.int32:\n        return _input / 2147483648., _input.dtype\n    raise ValueError('Unsupported wave file format')\n\n\ndef from_float(_input, dtype):\n    if dtype == np.float64:\n        return _input, np.float64\n    elif dtype == np.float32:\n        return _input.astype(np.float32)\n    elif dtype == np.uint8:\n        return ((_input * 128) + 128).astype(np.uint8)\n    elif dtype == np.int16:\n        return (_input * 32768).astype(np.int16)\n    elif dtype == np.int32:\n        print(_input)\n        return (_input * 2147483648).astype(np.int32)\n    raise ValueError('Unsupported wave file format')\n", "utils/loss.py": "import torch\n\n\ndef feature_loss(fmap_r, fmap_g):\n    loss = 0\n    for dr, dg in zip(fmap_r, fmap_g):\n        for rl, gl in zip(dr, dg):\n            loss += torch.mean(torch.abs(rl - gl))\n\n    return loss*2\n\n\ndef discriminator_loss(disc_real_outputs, disc_generated_outputs):\n    loss = 0\n    r_losses = []\n    g_losses = []\n    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n        r_loss = torch.mean((1-dr)**2)\n        g_loss = torch.mean(dg**2)\n        loss += (r_loss + g_loss)\n        r_losses.append(r_loss.item())\n        g_losses.append(g_loss.item())\n\n    return loss, r_losses, g_losses\n\n\ndef generator_loss(disc_outputs):\n    loss = 0\n    gen_losses = []\n    for dg in disc_outputs:\n        l = torch.mean((1-dg)**2)\n        gen_losses.append(l)\n        loss += l\n\n    return loss, gen_losses\n\n\ndef kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n  \"\"\"\n  z_p, logs_q: [b, h, t_t]\n  m_p, logs_p: [b, h, t_t]\n  \"\"\"\n  z_p = z_p.float()\n  logs_q = logs_q.float()\n  m_p = m_p.float()\n  logs_p = logs_p.float()\n  z_mask = z_mask.float()\n\n  kl = logs_p - logs_q - 0.5\n  kl += 0.5 * ((z_p - m_p)**2) * torch.exp(-2. * logs_p)\n  kl = torch.sum(kl * z_mask)\n  l = kl / torch.sum(z_mask)\n  return l\n", "utils/argutils.py": "from pathlib import Path\nimport numpy as np\nimport argparse\n\n_type_priorities = [    # In decreasing order\n    Path,\n    str,\n    int,\n    float,\n    bool,\n]\n\ndef _priority(o):\n    p = next((i for i, t in enumerate(_type_priorities) if type(o) is t), None) \n    if p is not None:\n        return p\n    p = next((i for i, t in enumerate(_type_priorities) if isinstance(o, t)), None) \n    if p is not None:\n        return p\n    return len(_type_priorities)\n\ndef print_args(args: argparse.Namespace, parser=None):\n    args = vars(args)\n    if parser is None:\n        priorities = list(map(_priority, args.values()))\n    else:\n        all_params = [a.dest for g in parser._action_groups for a in g._group_actions ]\n        priority = lambda p: all_params.index(p) if p in all_params else len(all_params)\n        priorities = list(map(priority, args.keys()))\n    \n    pad = max(map(len, args.keys())) + 3\n    indices = np.lexsort((list(args.keys()), priorities))\n    items = list(args.items())\n    \n    print(\"Arguments:\")\n    for i in indices:\n        param, value = items[i]\n        print(\"    {0}:{1}{2}\".format(param, ' ' * (pad - len(param)), value))\n    print(\"\")\n    ", "utils/audio_utils.py": "import numpy as np\nimport torch\nimport torch.utils.data\nfrom scipy.io.wavfile import read\nfrom librosa.filters import mel as librosa_mel_fn\n\nMAX_WAV_VALUE = 32768.0\n\nmel_basis = {}\nhann_window = {}\n\ndef load_wav(full_path):\n    sampling_rate, data = read(full_path)\n    return data, sampling_rate\n\ndef load_wav_to_torch(full_path):\n  sampling_rate, data = read(full_path)\n  return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n\ndef spectrogram(y, n_fft, hop_size, win_size, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n        \n    global hann_window\n    dtype_device = str(y.dtype) + '_' + str(y.device)\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return spec\n\ndef spec_to_mel(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n    global mel_basis\n    dtype_device = str(spec.dtype) + '_' + str(spec.device)\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = _spectral_normalize_torch(spec)\n    return spec\n\n\ndef mel_spectrogram(\n    y, \n    n_fft, \n    num_mels, \n    sampling_rate, \n    hop_size, \n    win_size, \n    fmin, \n    fmax, \n    center=False,\n    output_energy=False,\n):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global mel_basis, hann_window\n    if fmax not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n   \n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)\n    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-6))\n    mel_spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n    mel_spec = _spectral_normalize_torch(mel_spec)\n    if output_energy:\n        energy = torch.norm(spec, dim=1)\n        return mel_spec, energy\n    else:\n        return mel_spec\n\n\ndef _dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef _spectral_normalize_torch(magnitudes):\n    output = _dynamic_range_compression_torch(magnitudes)\n    return output\n", "utils/data_load.py": "import random\nimport numpy as np\nimport torch\nfrom utils.f0_utils import get_cont_lf0\nimport resampy\nfrom .audio_utils import MAX_WAV_VALUE, load_wav, mel_spectrogram\nfrom librosa.util import normalize\nimport os\n\n\nSAMPLE_RATE=16000\n\ndef read_fids(fid_list_f):\n    with open(fid_list_f, 'r') as f:\n        fids = [l.strip().split()[0] for l in f if l.strip()]\n    return fids   \n\nclass OneshotVcDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        meta_file: str,\n        vctk_ppg_dir: str,\n        libri_ppg_dir: str,\n        vctk_f0_dir: str,\n        libri_f0_dir: str,\n        vctk_wav_dir: str,\n        libri_wav_dir: str,\n        vctk_spk_dvec_dir: str,\n        libri_spk_dvec_dir: str,\n        min_max_norm_mel: bool = False,\n        mel_min: float = None,\n        mel_max: float = None,\n        ppg_file_ext: str = \"ling_feat.npy\",\n        f0_file_ext: str = \"f0.npy\",\n        wav_file_ext: str = \"wav\",\n    ):\n        self.fid_list = read_fids(meta_file)\n        self.vctk_ppg_dir = vctk_ppg_dir\n        self.libri_ppg_dir = libri_ppg_dir\n        self.vctk_f0_dir = vctk_f0_dir\n        self.libri_f0_dir = libri_f0_dir\n        self.vctk_wav_dir = vctk_wav_dir\n        self.libri_wav_dir = libri_wav_dir\n        self.vctk_spk_dvec_dir = vctk_spk_dvec_dir\n        self.libri_spk_dvec_dir = libri_spk_dvec_dir\n\n        self.ppg_file_ext = ppg_file_ext\n        self.f0_file_ext = f0_file_ext\n        self.wav_file_ext = wav_file_ext\n\n        self.min_max_norm_mel = min_max_norm_mel\n        if min_max_norm_mel:\n            print(\"[INFO] Min-Max normalize Melspec.\")\n            assert mel_min is not None\n            assert mel_max is not None\n            self.mel_max = mel_max\n            self.mel_min = mel_min\n        \n        random.seed(1234)\n        random.shuffle(self.fid_list)\n        print(f'[INFO] Got {len(self.fid_list)} samples.')\n        \n    def __len__(self):\n        return len(self.fid_list)\n    \n    def get_spk_dvec(self, fid):\n        spk_name = fid\n        if spk_name.startswith(\"p\"):\n            spk_dvec_path = f\"{self.vctk_spk_dvec_dir}{os.sep}{spk_name}.npy\"\n        else:\n            spk_dvec_path = f\"{self.libri_spk_dvec_dir}{os.sep}{spk_name}.npy\"\n        return torch.from_numpy(np.load(spk_dvec_path))\n    \n    def compute_mel(self, wav_path):\n        audio, sr = load_wav(wav_path)\n        if sr != SAMPLE_RATE:\n            audio = resampy.resample(audio, sr, SAMPLE_RATE)\n        audio = audio / MAX_WAV_VALUE\n        audio = normalize(audio) * 0.95\n        audio = torch.FloatTensor(audio).unsqueeze(0)\n        melspec = mel_spectrogram(\n            audio,\n            n_fft=1024,\n            num_mels=80,\n            sampling_rate=SAMPLE_RATE,\n            hop_size=160,\n            win_size=1024,\n            fmin=80,\n            fmax=8000,\n        )\n        return melspec.squeeze(0).numpy().T\n\n    def bin_level_min_max_norm(self, melspec):\n        # frequency bin level min-max normalization to [-4, 4]\n        mel = (melspec - self.mel_min) / (self.mel_max - self.mel_min) * 8.0 - 4.0\n        return np.clip(mel, -4., 4.)   \n\n    def __getitem__(self, index):\n        fid = self.fid_list[index]\n        \n        # 1. Load features\n        if fid.startswith(\"p\"):\n            # vctk\n            sub = fid.split(\"_\")[0]\n            ppg = np.load(f\"{self.vctk_ppg_dir}{os.sep}{fid}.{self.ppg_file_ext}\")\n            f0 = np.load(f\"{self.vctk_f0_dir}{os.sep}{fid}.{self.f0_file_ext}\")\n            mel = self.compute_mel(f\"{self.vctk_wav_dir}{os.sep}{sub}{os.sep}{fid}.{self.wav_file_ext}\")\n        else:\n            # aidatatang\n            sub = fid[5:10]\n            ppg = np.load(f\"{self.libri_ppg_dir}{os.sep}{fid}.{self.ppg_file_ext}\")\n            f0 = np.load(f\"{self.libri_f0_dir}{os.sep}{fid}.{self.f0_file_ext}\")\n            mel = self.compute_mel(f\"{self.libri_wav_dir}{os.sep}{sub}{os.sep}{fid}.{self.wav_file_ext}\")\n        if self.min_max_norm_mel:\n            mel = self.bin_level_min_max_norm(mel)\n        \n        f0, ppg, mel = self._adjust_lengths(f0, ppg, mel, fid)\n        spk_dvec = self.get_spk_dvec(fid)\n\n        # 2. Convert f0 to continuous log-f0 and u/v flags\n        uv, cont_lf0 = get_cont_lf0(f0, 10.0, False)\n        # cont_lf0 = (cont_lf0 - np.amin(cont_lf0)) / (np.amax(cont_lf0) - np.amin(cont_lf0))\n        # cont_lf0 = self.utt_mvn(cont_lf0)\n        lf0_uv = np.concatenate([cont_lf0[:, np.newaxis], uv[:, np.newaxis]], axis=1)\n\n        # uv, cont_f0 = convert_continuous_f0(f0)\n        # cont_f0 = (cont_f0 - np.amin(cont_f0)) / (np.amax(cont_f0) - np.amin(cont_f0))\n        # lf0_uv = np.concatenate([cont_f0[:, np.newaxis], uv[:, np.newaxis]], axis=1)\n        \n        # 3. Convert numpy array to torch.tensor\n        ppg = torch.from_numpy(ppg)\n        lf0_uv = torch.from_numpy(lf0_uv)\n        mel = torch.from_numpy(mel)\n        \n        return (ppg, lf0_uv, mel, spk_dvec, fid)\n\n    def check_lengths(self, f0, ppg, mel, fid):\n        LEN_THRESH = 10\n        assert abs(len(ppg) - len(f0)) <= LEN_THRESH, \\\n            f\"{abs(len(ppg) - len(f0))}: for file {fid}\"\n        assert abs(len(mel) - len(f0)) <= LEN_THRESH, \\\n            f\"{abs(len(mel) - len(f0))}: for file {fid}\"\n    \n    def _adjust_lengths(self, f0, ppg, mel, fid):\n        self.check_lengths(f0, ppg, mel, fid)\n        min_len = min(\n            len(f0),\n            len(ppg),\n            len(mel),\n        )\n        f0 = f0[:min_len]\n        ppg = ppg[:min_len]\n        mel = mel[:min_len]\n        return f0, ppg, mel\n\nclass MultiSpkVcCollate():\n    \"\"\"Zero-pads model inputs and targets based on number of frames per step\n    \"\"\"\n    def __init__(self, n_frames_per_step=1, give_uttids=False,\n                 f02ppg_length_ratio=1, use_spk_dvec=False):\n        self.n_frames_per_step = n_frames_per_step\n        self.give_uttids = give_uttids\n        self.f02ppg_length_ratio = f02ppg_length_ratio\n        self.use_spk_dvec = use_spk_dvec\n\n    def __call__(self, batch):\n        batch_size = len(batch)              \n        # Prepare different features \n        ppgs = [x[0] for x in batch]\n        lf0_uvs = [x[1] for x in batch]\n        mels = [x[2] for x in batch]\n        fids = [x[-1] for x in batch]\n        if len(batch[0]) == 5:\n            spk_ids = [x[3] for x in batch]\n            if self.use_spk_dvec:\n                # use d-vector\n                spk_ids = torch.stack(spk_ids).float()\n            else:\n                # use one-hot ids\n                spk_ids = torch.LongTensor(spk_ids)\n        # Pad features into chunk\n        ppg_lengths = [x.shape[0] for x in ppgs]\n        mel_lengths = [x.shape[0] for x in mels]\n        max_ppg_len = max(ppg_lengths)\n        max_mel_len = max(mel_lengths)\n        if max_mel_len % self.n_frames_per_step != 0:\n            max_mel_len += (self.n_frames_per_step - max_mel_len % self.n_frames_per_step)\n        ppg_dim = ppgs[0].shape[1]\n        mel_dim = mels[0].shape[1]\n        ppgs_padded = torch.FloatTensor(batch_size, max_ppg_len, ppg_dim).zero_()\n        mels_padded = torch.FloatTensor(batch_size, max_mel_len, mel_dim).zero_()\n        lf0_uvs_padded = torch.FloatTensor(batch_size, self.f02ppg_length_ratio * max_ppg_len, 2).zero_()\n        stop_tokens = torch.FloatTensor(batch_size, max_mel_len).zero_()\n        for i in range(batch_size):\n            cur_ppg_len = ppgs[i].shape[0]\n            cur_mel_len = mels[i].shape[0]\n            ppgs_padded[i, :cur_ppg_len, :] = ppgs[i]\n            lf0_uvs_padded[i, :self.f02ppg_length_ratio*cur_ppg_len, :] = lf0_uvs[i]\n            mels_padded[i, :cur_mel_len, :] = mels[i]\n            stop_tokens[i, cur_ppg_len-self.n_frames_per_step:] = 1\n        if len(batch[0]) == 5:\n            ret_tup = (ppgs_padded, lf0_uvs_padded, mels_padded, torch.LongTensor(ppg_lengths), \\\n                torch.LongTensor(mel_lengths), spk_ids, stop_tokens)\n            if self.give_uttids:\n                return ret_tup + (fids, )\n            else:\n                return ret_tup\n        else:\n            ret_tup = (ppgs_padded, lf0_uvs_padded, mels_padded, torch.LongTensor(ppg_lengths), \\\n                torch.LongTensor(mel_lengths), stop_tokens)\n            if self.give_uttids:\n                return ret_tup + (fids, )\n            else:\n                return ret_tup\n", "utils/util.py": "import matplotlib\nfrom torch.nn import functional as F\n\nimport torch\nmatplotlib.use('Agg')\nimport time\n\nclass Timer():\n    ''' Timer for recording training time distribution. '''\n    def __init__(self):\n        self.prev_t = time.time()\n        self.clear()\n\n    def set(self):\n        self.prev_t = time.time()\n\n    def cnt(self, mode):\n        self.time_table[mode] += time.time()-self.prev_t\n        self.set()\n        if mode == 'bw':\n            self.click += 1\n\n    def show(self):\n        total_time = sum(self.time_table.values())\n        self.time_table['avg'] = total_time/self.click\n        self.time_table['rd'] = 100*self.time_table['rd']/total_time\n        self.time_table['fw'] = 100*self.time_table['fw']/total_time\n        self.time_table['bw'] = 100*self.time_table['bw']/total_time\n        msg = '{avg:.3f} sec/step (rd {rd:.1f}% | fw {fw:.1f}% | bw {bw:.1f}%)'.format(\n            **self.time_table)\n        self.clear()\n        return msg\n\n    def clear(self):\n        self.time_table = {'rd': 0, 'fw': 0, 'bw': 0}\n        self.click = 0\n\n# Reference : https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/e2e_asr.py#L168\n\ndef human_format(num):\n    magnitude = 0\n    while num >= 1000:\n        magnitude += 1\n        num /= 1000.0\n    # add more suffixes if you need them\n    return '{:3.1f}{}'.format(num, [' ', 'K', 'M', 'G', 'T', 'P'][magnitude])\n\n\n# provide easy access of attribute from dict, such abc.key \nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\ndef init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\n\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size*dilation - dilation)/2)\n\n\ndef sequence_mask(length, max_length=None):\n  if max_length is None:\n    max_length = length.max()\n  x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n  return x.unsqueeze(0) < length.unsqueeze(1)\n\ndef slice_segments(x, ids_str, segment_size=4):\n  ret = torch.zeros_like(x[:, :, :segment_size])\n  for i in range(x.size(0)):\n    idx_str = ids_str[i]\n    idx_end = idx_str + segment_size\n    ret[i] = x[i, :, idx_str:idx_end]\n  return ret\n\ndef rand_slice_segments(x, x_lengths=None, segment_size=4):\n  b, d, t = x.size()\n  if x_lengths is None:\n    x_lengths = t\n  ids_str_max = x_lengths - segment_size + 1\n  ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n  ret = slice_segments(x, ids_str, segment_size)\n  return ret, ids_str\n\n\ndef convert_pad_shape(pad_shape):\n  l = pad_shape[::-1]\n  pad_shape = [item for sublist in l for item in sublist]\n  return pad_shape\n\ndef generate_path(duration, mask):\n  \"\"\"\n  duration: [b, 1, t_x]\n  mask: [b, 1, t_y, t_x]\n  \"\"\"\n  device = duration.device\n  \n  b, _, t_y, t_x = mask.shape\n  cum_duration = torch.cumsum(duration, -1)\n  \n  cum_duration_flat = cum_duration.view(b * t_x)\n  path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n  path = path.view(b, t_x, t_y)\n  path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n  path = path.unsqueeze(1).transpose(2,3) * mask\n  return path\n\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n  n_channels_int = n_channels[0]\n  in_act = input_a + input_b\n  t_act = torch.tanh(in_act[:, :n_channels_int, :])\n  s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n  acts = t_act * s_act\n  return acts\n\n\ndef subsequent_mask(length):\n  mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n  return mask\n\n\ndef intersperse(lst, item):\n  result = [item] * (len(lst) * 2 + 1)\n  result[1::2] = lst\n  return result\n\n\ndef clip_grad_value_(parameters, clip_value, norm_type=2):\n  if isinstance(parameters, torch.Tensor):\n    parameters = [parameters]\n  parameters = list(filter(lambda p: p.grad is not None, parameters))\n  norm_type = float(norm_type)\n  if clip_value is not None:\n    clip_value = float(clip_value)\n\n  total_norm = 0\n  for p in parameters:\n    param_norm = p.grad.data.norm(norm_type)\n    total_norm += param_norm.item() ** norm_type\n    if clip_value is not None:\n      p.grad.data.clamp_(min=-clip_value, max=clip_value)\n  total_norm = total_norm ** (1. / norm_type)\n  return total_norm\n", "utils/profiler.py": "from time import perf_counter as timer\nfrom collections import OrderedDict\nimport numpy as np\n\n\nclass Profiler:\n    def __init__(self, summarize_every=5, disabled=False):\n        self.last_tick = timer()\n        self.logs = OrderedDict()\n        self.summarize_every = summarize_every\n        self.disabled = disabled\n    \n    def tick(self, name):\n        if self.disabled:\n            return\n        \n        # Log the time needed to execute that function\n        if not name in self.logs:\n            self.logs[name] = []\n        if len(self.logs[name]) >= self.summarize_every:\n            self.summarize()\n            self.purge_logs()\n        self.logs[name].append(timer() - self.last_tick)\n        \n        self.reset_timer()\n        \n    def purge_logs(self):\n        for name in self.logs:\n            self.logs[name].clear()\n    \n    def reset_timer(self):\n        self.last_tick = timer()\n    \n    def summarize(self):\n        n = max(map(len, self.logs.values()))\n        assert n == self.summarize_every\n        print(\"\\nAverage execution time over %d steps:\" % n)\n\n        name_msgs = [\"%s (%d/%d):\" % (name, len(deltas), n) for name, deltas in self.logs.items()]\n        pad = max(map(len, name_msgs))\n        for name_msg, deltas in zip(name_msgs, self.logs.values()):\n            print(\"  %s  mean: %4.0fms   std: %4.0fms\" % \n                  (name_msg.ljust(pad), np.mean(deltas) * 1000, np.std(deltas) * 1000))\n        print(\"\", flush=True)    \n        ", "utils/hparams.py": "import yaml\nimport json\nimport ast\n\ndef load_hparams_json(filename):\n    with open(filename, \"r\") as f:\n        data = f.read()\n    config = json.loads(data)\n\n    hparams = HParams(**config)\n    return hparams\n\n\ndef load_hparams_yaml(filename):\n    stream = open(filename, 'r')\n    docs = yaml.safe_load_all(stream)\n    hparams_dict = dict()\n    for doc in docs:\n        for k, v in doc.items():\n            hparams_dict[k] = v\n    return hparams_dict\n\ndef merge_dict(user, default):\n    if isinstance(user, dict) and isinstance(default, dict):\n        for k, v in default.items():\n            if k not in user:\n                user[k] = v\n            else:\n                user[k] = merge_dict(user[k], v)\n    return user\n\nclass Dotdict(dict):\n    \"\"\"\n    a dictionary that supports dot notation \n    as well as dictionary access notation \n    usage: d = DotDict() or d = DotDict({'val1':'first'})\n    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n    get attributes: d.val2 or d['val2']\n    \"\"\"\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, 'keys'):\n                value = Dotdict(value)\n            self[key] = value\n\nclass HpsYaml(Dotdict):\n    def __init__(self, yaml_file):\n        super(Dotdict, self).__init__()\n        hps = load_hparams_yaml(yaml_file)\n        hp_dict = Dotdict(hps)\n        for k, v in hp_dict.items():\n            setattr(self, k, v)\n\n    __getattr__ = Dotdict.__getitem__\n    __setattr__ = Dotdict.__setitem__\n    __delattr__ = Dotdict.__delitem__\n    \nclass HParams():\n    def __init__(self, **kwargs): \n        for k, v in kwargs.items():\n            if type(v) == dict:\n                v = HParams(**v)\n            self[k] = v\n    def keys(self):\n        return self.__dict__.keys()\n    def __setitem__(self, key, value): setattr(self, key, value)\n    def __getitem__(self, key): return getattr(self, key)\n    def keys(self):  return self.__dict__.keys()\n    def items(self): return self.__dict__.items()\n    def values(self): return self.__dict__.values()\n    def __contains__(self, key): return key in self.__dict__\n    def __repr__(self):\n        return self.__dict__.__repr__()\n\n    def parse(self, string):\n        # Overrides hparams from a comma-separated string of name=value pairs\n        if len(string) > 0:\n            overrides = [s.split(\"=\") for s in string.split(\",\")]\n            keys, values = zip(*overrides)\n            keys = list(map(str.strip, keys))\n            values = list(map(str.strip, values))\n            for k in keys:\n                self.__dict__[k] = ast.literal_eval(values[keys.index(k)])\n        return self\n\n    def loadJson(self, fpath):\n        with fpath.open(\"r\", encoding=\"utf-8\") as f:\n            print(\"\\Loading the json with %s\\n\", fpath)\n            data = json.load(f)\n            for k in data.keys():\n                if k not in [\"tts_schedule\", \"tts_finetune_layers\"]: \n                    v = data[k]\n                    if type(v) == dict:\n                        v = HParams(**v)\n                    self.__dict__[k] = v\n        return self\n\n    def dumpJson(self, fp):\n        print(\"\\Saving the json with %s\\n\", fp)\n        with fp.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(self.__dict__, f)\n        return self\n\n\n\n", "utils/f0_utils.py": "import logging\nimport numpy as np\nimport pyworld\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import firwin, get_window, lfilter\n\ndef compute_mean_std(lf0):\n    nonzero_indices = np.nonzero(lf0)\n    mean = np.mean(lf0[nonzero_indices])\n    std = np.std(lf0[nonzero_indices])\n    return mean, std \n\n\ndef compute_f0(wav, sr=16000, frame_period=10.0):\n    \"\"\"Compute f0 from wav using pyworld harvest algorithm.\"\"\"\n    wav = wav.astype(np.float64)\n    f0, _ = pyworld.harvest(\n        wav, sr, frame_period=frame_period, f0_floor=80.0, f0_ceil=600.0)\n    return f0.astype(np.float32)\n\ndef f02lf0(f0):\n    lf0 = f0.copy()\n    nonzero_indices = np.nonzero(f0)\n    lf0[nonzero_indices] = np.log(f0[nonzero_indices])\n    return lf0\n\ndef get_converted_lf0uv(\n    wav, \n    lf0_mean_trg, \n    lf0_std_trg,\n    convert=True,\n):\n    f0_src = compute_f0(wav)\n    if not convert:\n        uv, cont_lf0 = get_cont_lf0(f0_src)\n        lf0_uv = np.concatenate([cont_lf0[:, np.newaxis], uv[:, np.newaxis]], axis=1)\n        return lf0_uv\n\n    lf0_src = f02lf0(f0_src)\n    lf0_mean_src, lf0_std_src = compute_mean_std(lf0_src)\n    \n    lf0_vc = lf0_src.copy()\n    lf0_vc[lf0_src > 0.0] = (lf0_src[lf0_src > 0.0] - lf0_mean_src) / lf0_std_src * lf0_std_trg + lf0_mean_trg\n    f0_vc = lf0_vc.copy()\n    f0_vc[lf0_src > 0.0] = np.exp(lf0_vc[lf0_src > 0.0])\n    \n    uv, cont_lf0_vc = get_cont_lf0(f0_vc)\n    lf0_uv = np.concatenate([cont_lf0_vc[:, np.newaxis], uv[:, np.newaxis]], axis=1)\n    return lf0_uv\n\ndef low_pass_filter(x, fs, cutoff=70, padding=True):\n    \"\"\"FUNCTION TO APPLY LOW PASS FILTER\n\n    Args:\n        x (ndarray): Waveform sequence\n        fs (int): Sampling frequency\n        cutoff (float): Cutoff frequency of low pass filter\n\n    Return:\n        (ndarray): Low pass filtered waveform sequence\n    \"\"\"\n\n    nyquist = fs // 2\n    norm_cutoff = cutoff / nyquist\n\n    # low cut filter\n    numtaps = 255\n    fil = firwin(numtaps, norm_cutoff)\n    x_pad = np.pad(x, (numtaps, numtaps), 'edge')\n    lpf_x = lfilter(fil, 1, x_pad)\n    lpf_x = lpf_x[numtaps + numtaps // 2: -numtaps // 2]\n\n    return lpf_x\n\n\ndef convert_continuos_f0(f0):\n    \"\"\"CONVERT F0 TO CONTINUOUS F0\n\n    Args:\n        f0 (ndarray): original f0 sequence with the shape (T)\n\n    Return:\n        (ndarray): continuous f0 with the shape (T)\n    \"\"\"\n    # get uv information as binary\n    uv = np.float32(f0 != 0)\n\n    # get start and end of f0\n    if (f0 == 0).all():\n        logging.warn(\"all of the f0 values are 0.\")\n        return uv, f0\n    start_f0 = f0[f0 != 0][0]\n    end_f0 = f0[f0 != 0][-1]\n\n    # padding start and end of f0 sequence\n    start_idx = np.where(f0 == start_f0)[0][0]\n    end_idx = np.where(f0 == end_f0)[0][-1]\n    f0[:start_idx] = start_f0\n    f0[end_idx:] = end_f0\n\n    # get non-zero frame index\n    nz_frames = np.where(f0 != 0)[0]\n\n    # perform linear interpolation\n    f = interp1d(nz_frames, f0[nz_frames])\n    cont_f0 = f(np.arange(0, f0.shape[0]))\n\n    return uv, cont_f0\n\n\ndef get_cont_lf0(f0, frame_period=10.0, lpf=False):\n    uv, cont_f0 = convert_continuos_f0(f0)\n    if lpf:\n        cont_f0_lpf = low_pass_filter(cont_f0, int(1.0 / (frame_period * 0.001)), cutoff=20)\n        cont_lf0_lpf = cont_f0_lpf.copy()\n        nonzero_indices = np.nonzero(cont_lf0_lpf)\n        cont_lf0_lpf[nonzero_indices] = np.log(cont_f0_lpf[nonzero_indices])\n        # cont_lf0_lpf = np.log(cont_f0_lpf)\n        return uv, cont_lf0_lpf \n    else:\n        nonzero_indices = np.nonzero(cont_f0)\n        cont_lf0 = cont_f0.copy()\n        cont_lf0[cont_f0>0] = np.log(cont_f0[cont_f0>0])\n        return uv, cont_lf0\n", "utils/modelutils.py": "from pathlib import Path\n\ndef check_model_paths(encoder_path: Path, synthesizer_path: Path, vocoder_path: Path):\n    # This function tests the model paths and makes sure at least one is valid.\n    if encoder_path.is_file() or encoder_path.is_dir():\n        return\n    if synthesizer_path.is_file() or synthesizer_path.is_dir():\n        return\n    if vocoder_path.is_file() or vocoder_path.is_dir():\n        return\n\n    # If none of the paths exist, remind the user to download models if needed\n    print(\"********************************************************************************\")\n    print(\"Error: Model files not found. Please download the models\")\n    print(\"********************************************************************************\\n\")\n    quit(-1)\n", "utils/__init__.py": "", "control/__init__.py": "", "control/mkgui/preprocess.py": "from pydantic import BaseModel, Field\nimport os\nfrom pathlib import Path\nfrom enum import Enum\nfrom typing import Any, Tuple\n\n\n# Constants\nEXT_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}ppg_extractor\"\nENC_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}encoder\"\n\n\nif os.path.isdir(EXT_MODELS_DIRT):    \n    extractors =  Enum('extractors', list((file.name, file) for file in Path(EXT_MODELS_DIRT).glob(\"**/*.pt\")))\n    print(\"Loaded extractor models: \" + str(len(extractors)))\nelse:\n    raise Exception(f\"Model folder {EXT_MODELS_DIRT} doesn't exist.\")\n\nif os.path.isdir(ENC_MODELS_DIRT):    \n    encoders = Enum('encoders', list((file.name, file) for file in Path(ENC_MODELS_DIRT).glob(\"**/*.pt\")))\n    print(\"Loaded encoders models: \" + str(len(encoders)))\nelse:\n    raise Exception(f\"Model folder {ENC_MODELS_DIRT} doesn't exist.\")\n\nclass Model(str, Enum):\n    VC_PPG2MEL = \"ppg2mel\"\n\nclass Dataset(str, Enum):\n    AIDATATANG_200ZH = \"aidatatang_200zh\"\n    AIDATATANG_200ZH_S = \"aidatatang_200zh_s\"\n\nclass Input(BaseModel):\n    # def render_input_ui(st, input) -> Dict: \n    #     input[\"selected_dataset\"] = st.selectbox(\n    #         '\u9009\u62e9\u6570\u636e\u96c6', \n    #         (\"aidatatang_200zh\", \"aidatatang_200zh_s\")\n    #     )\n    # return input\n    model: Model = Field(\n        Model.VC_PPG2MEL, title=\"\u76ee\u6807\u6a21\u578b\",\n    )\n    dataset: Dataset = Field(\n        Dataset.AIDATATANG_200ZH, title=\"\u6570\u636e\u96c6\u9009\u62e9\",\n    )\n    datasets_root: str = Field(\n        ..., alias=\"\u6570\u636e\u96c6\u6839\u76ee\u5f55\", description=\"\u8f93\u5165\u6570\u636e\u96c6\u6839\u76ee\u5f55\uff08\u76f8\u5bf9/\u7edd\u5bf9\uff09\",\n        format=True,\n        example=\"..\\\\trainning_data\\\\\"\n    )\n    output_root: str = Field(\n        ..., alias=\"\u8f93\u51fa\u6839\u76ee\u5f55\", description=\"\u8f93\u51fa\u7ed3\u679c\u6839\u76ee\u5f55\uff08\u76f8\u5bf9/\u7edd\u5bf9\uff09\",\n        format=True,\n        example=\"..\\\\trainning_data\\\\\"\n    )\n    n_processes: int = Field(   \n        2, alias=\"\u5904\u7406\u7ebf\u7a0b\u6570\", description=\"\u6839\u636eCPU\u7ebf\u7a0b\u6570\u6765\u8bbe\u7f6e\",\n        le=32, ge=1\n    )\n    extractor: extractors = Field(\n        ..., alias=\"\u7279\u5f81\u63d0\u53d6\u6a21\u578b\", \n        description=\"\u9009\u62e9PPG\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u6587\u4ef6.\"\n    )\n    encoder: encoders = Field(\n        ..., alias=\"\u8bed\u97f3\u7f16\u7801\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u7f16\u7801\u6a21\u578b\u6587\u4ef6.\"\n    )\n\nclass AudioEntity(BaseModel):\n    content: bytes\n    mel: Any\n\nclass Output(BaseModel):\n    __root__: Tuple[str, int]\n\n    def render_output_ui(self, streamlit_app, input) -> None:  # type: ignore\n        \"\"\"Custom output UI.\n        If this method is implmeneted, it will be used instead of the default Output UI renderer.\n        \"\"\"\n        sr, count = self.__root__\n        streamlit_app.subheader(f\"Dataset {sr} done processed total of {count}\")\n\ndef preprocess(input: Input) -> Output:\n    \"\"\"Preprocess(\u9884\u5904\u7406)\"\"\"\n    finished = 0\n    if input.model == Model.VC_PPG2MEL:\n        from models.ppg2mel.preprocess import preprocess_dataset\n        finished = preprocess_dataset(\n            datasets_root=Path(input.datasets_root),\n            dataset=input.dataset,\n            out_dir=Path(input.output_root),\n            n_processes=input.n_processes,\n            ppg_encoder_model_fpath=Path(input.extractor.value),\n            speaker_encoder_model=Path(input.encoder.value)\n        )\n    # TODO: pass useful return code\n    return Output(__root__=(input.dataset, finished))", "control/mkgui/train_vc.py": "from pydantic import BaseModel, Field\nimport os\nfrom pathlib import Path\nfrom enum import Enum\nfrom typing import Any, Tuple\nimport numpy as np\nfrom utils.hparams import HpsYaml\nfrom utils.util import AttrDict\nimport torch\n\n# Constants\nEXT_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}ppg_extractor\"\nCONV_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}ppg2mel\"\nENC_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}encoder\"\n\n\nif os.path.isdir(EXT_MODELS_DIRT):    \n    extractors =  Enum('extractors', list((file.name, file) for file in Path(EXT_MODELS_DIRT).glob(\"**/*.pt\")))\n    print(\"Loaded extractor models: \" + str(len(extractors)))\nelse:\n    raise Exception(f\"Model folder {EXT_MODELS_DIRT} doesn't exist.\")\n\nif os.path.isdir(CONV_MODELS_DIRT):    \n    convertors =  Enum('convertors', list((file.name, file) for file in Path(CONV_MODELS_DIRT).glob(\"**/*.pth\")))\n    print(\"Loaded convertor models: \" + str(len(convertors)))\nelse:\n    raise Exception(f\"Model folder {CONV_MODELS_DIRT} doesn't exist.\")\n\nif os.path.isdir(ENC_MODELS_DIRT):    \n    encoders = Enum('encoders', list((file.name, file) for file in Path(ENC_MODELS_DIRT).glob(\"**/*.pt\")))\n    print(\"Loaded encoders models: \" + str(len(encoders)))\nelse:\n    raise Exception(f\"Model folder {ENC_MODELS_DIRT} doesn't exist.\")\n\nclass Model(str, Enum):\n    VC_PPG2MEL = \"ppg2mel\"\n\nclass Dataset(str, Enum):\n    AIDATATANG_200ZH = \"aidatatang_200zh\"\n    AIDATATANG_200ZH_S = \"aidatatang_200zh_s\"\n\nclass Input(BaseModel):\n    # def render_input_ui(st, input) -> Dict: \n    #     input[\"selected_dataset\"] = st.selectbox(\n    #         '\u9009\u62e9\u6570\u636e\u96c6', \n    #         (\"aidatatang_200zh\", \"aidatatang_200zh_s\")\n    #     )\n    # return input\n    model: Model = Field(\n        Model.VC_PPG2MEL, title=\"\u6a21\u578b\u7c7b\u578b\",\n    )\n    # datasets_root: str = Field(\n    #     ..., alias=\"\u9884\u5904\u7406\u6570\u636e\u6839\u76ee\u5f55\", description=\"\u8f93\u5165\u76ee\u5f55\uff08\u76f8\u5bf9/\u7edd\u5bf9\uff09,\u4e0d\u9002\u7528\u4e8eppg2mel\u6a21\u578b\",\n    #     format=True,\n    #     example=\"..\\\\trainning_data\\\\\"\n    # )\n    output_root: str = Field(\n        ..., alias=\"\u8f93\u51fa\u76ee\u5f55(\u53ef\u9009)\", description=\"\u5efa\u8bae\u4e0d\u586b\uff0c\u4fdd\u6301\u9ed8\u8ba4\",\n        format=True,\n        example=\"\"\n    )\n    continue_mode: bool = Field(\n        True, alias=\"\u7ee7\u7eed\u8bad\u7ec3\u6a21\u5f0f\", description=\"\u9009\u62e9\u201c\u662f\u201d\uff0c\u5219\u4ece\u4e0b\u9762\u9009\u62e9\u7684\u6a21\u578b\u4e2d\u7ee7\u7eed\u8bad\u7ec3\",\n    )\n    gpu: bool = Field(\n        True, alias=\"GPU\u8bad\u7ec3\", description=\"\u9009\u62e9\u201c\u662f\u201d\uff0c\u5219\u4f7f\u7528GPU\u8bad\u7ec3\",\n    )\n    verbose: bool = Field(\n        True, alias=\"\u6253\u5370\u8be6\u60c5\", description=\"\u9009\u62e9\u201c\u662f\u201d\uff0c\u8f93\u51fa\u66f4\u591a\u8be6\u60c5\",\n    )\n    # TODO: Move to hiden fields by default\n    convertor: convertors = Field(\n        ..., alias=\"\u8f6c\u6362\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u8f6c\u6362\u6a21\u578b\u6587\u4ef6.\"\n    )\n    extractor: extractors = Field(\n        ..., alias=\"\u7279\u5f81\u63d0\u53d6\u6a21\u578b\", \n        description=\"\u9009\u62e9PPG\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u6587\u4ef6.\"\n    )\n    encoder: encoders = Field(\n        ..., alias=\"\u8bed\u97f3\u7f16\u7801\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u7f16\u7801\u6a21\u578b\u6587\u4ef6.\"\n    )\n    njobs: int = Field(\n        8, alias=\"\u8fdb\u7a0b\u6570\", description=\"\u9002\u7528\u4e8eppg2mel\",\n    )\n    seed: int = Field(\n        default=0, alias=\"\u521d\u59cb\u968f\u673a\u6570\", description=\"\u9002\u7528\u4e8eppg2mel\",\n    )\n    model_name: str = Field(\n        ..., alias=\"\u65b0\u6a21\u578b\u540d\", description=\"\u4ec5\u5728\u91cd\u65b0\u8bad\u7ec3\u65f6\u751f\u6548,\u9009\u4e2d\u7ee7\u7eed\u8bad\u7ec3\u65f6\u65e0\u6548\",\n        example=\"test\"\n    )\n    model_config: str = Field(\n        ..., alias=\"\u65b0\u6a21\u578b\u914d\u7f6e\", description=\"\u4ec5\u5728\u91cd\u65b0\u8bad\u7ec3\u65f6\u751f\u6548,\u9009\u4e2d\u7ee7\u7eed\u8bad\u7ec3\u65f6\u65e0\u6548\",\n        example=\".\\\\ppg2mel\\\\saved_models\\\\seq2seq_mol_ppg2mel_vctk_libri_oneshotvc_r4_normMel_v2\"\n    )\n\nclass AudioEntity(BaseModel):\n    content: bytes\n    mel: Any\n\nclass Output(BaseModel):\n    __root__: Tuple[str, int]\n\n    def render_output_ui(self, streamlit_app, input) -> None:  # type: ignore\n        \"\"\"Custom output UI.\n        If this method is implmeneted, it will be used instead of the default Output UI renderer.\n        \"\"\"\n        sr, count = self.__root__\n        streamlit_app.subheader(f\"Dataset {sr} done processed total of {count}\")\n\ndef train_vc(input: Input) -> Output:\n    \"\"\"Train VC(\u8bad\u7ec3 VC)\"\"\"\n\n    print(\">>> OneShot VC training ...\")\n    params = AttrDict()\n    params.update({\n        \"gpu\": input.gpu,\n        \"cpu\": not input.gpu,\n        \"njobs\": input.njobs,\n        \"seed\": input.seed,\n        \"verbose\": input.verbose,\n        \"load\": input.convertor.value,\n        \"warm_start\": False,\n    })\n    if input.continue_mode: \n        # trace old model and config\n        p = Path(input.convertor.value)\n        params.name = p.parent.name\n        # search a config file\n        model_config_fpaths = list(p.parent.rglob(\"*.yaml\"))\n        if len(model_config_fpaths) == 0:\n            raise \"No model yaml config found for convertor\"\n        config = HpsYaml(model_config_fpaths[0])\n        params.ckpdir = p.parent.parent\n        params.config = model_config_fpaths[0]\n        params.logdir = os.path.join(p.parent, \"log\")\n    else:\n        # Make the config dict dot visitable\n        config = HpsYaml(input.config)    \n    np.random.seed(input.seed)\n    torch.manual_seed(input.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(input.seed)\n    mode = \"train\"\n    from models.ppg2mel.train.train_linglf02mel_seq2seq_oneshotvc import Solver\n    solver = Solver(config, params, mode)\n    solver.load_data()\n    solver.set_model()\n    solver.exec()\n    print(\">>> Oneshot VC train finished!\")\n\n    # TODO: pass useful return code\n    return Output(__root__=(input.dataset, 0))", "control/mkgui/train.py": "from pydantic import BaseModel, Field\nimport os\nfrom pathlib import Path\nfrom enum import Enum\nfrom typing import Any\nfrom models.synthesizer.hparams import hparams\nfrom models.synthesizer.train import train as synt_train\n\n# Constants\nSYN_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}synthesizer\"\nENC_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}encoder\"\n\n\n# EXT_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}ppg_extractor\"\n# CONV_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}ppg2mel\"\n# ENC_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}encoder\"\n\n# Pre-Load models\nif os.path.isdir(SYN_MODELS_DIRT):    \n    synthesizers =  Enum('synthesizers', list((file.name, file) for file in Path(SYN_MODELS_DIRT).glob(\"**/*.pt\")))\n    print(\"Loaded synthesizer models: \" + str(len(synthesizers)))\nelse:\n    raise Exception(f\"Model folder {SYN_MODELS_DIRT} doesn't exist.\")\n\nif os.path.isdir(ENC_MODELS_DIRT):    \n    encoders =  Enum('encoders', list((file.name, file) for file in Path(ENC_MODELS_DIRT).glob(\"**/*.pt\")))\n    print(\"Loaded encoders models: \" + str(len(encoders)))\nelse:\n    raise Exception(f\"Model folder {ENC_MODELS_DIRT} doesn't exist.\")\n\nclass Model(str, Enum):\n    DEFAULT = \"default\"\n\nclass Input(BaseModel):\n    model: Model = Field(\n        Model.DEFAULT, title=\"\u6a21\u578b\u7c7b\u578b\",\n    )\n    # datasets_root: str = Field(\n    #     ..., alias=\"\u9884\u5904\u7406\u6570\u636e\u6839\u76ee\u5f55\", description=\"\u8f93\u5165\u76ee\u5f55\uff08\u76f8\u5bf9/\u7edd\u5bf9\uff09,\u4e0d\u9002\u7528\u4e8eppg2mel\u6a21\u578b\",\n    #     format=True,\n    #     example=\"..\\\\trainning_data\\\\\"\n    # )\n    input_root: str = Field(\n        ..., alias=\"\u8f93\u5165\u76ee\u5f55\", description=\"\u9884\u5904\u7406\u6570\u636e\u6839\u76ee\u5f55\",\n        format=True,\n        example=f\"..{os.sep}audiodata{os.sep}SV2TTS{os.sep}synthesizer\"\n    )\n    run_id: str = Field(\n        \"\", alias=\"\u65b0\u6a21\u578b\u540d/\u8fd0\u884cID\", description=\"\u4f7f\u7528\u65b0ID\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\uff0c\u5426\u5219\u9009\u62e9\u4e0b\u9762\u7684\u6a21\u578b\u8fdb\u884c\u7ee7\u7eed\u8bad\u7ec3\",\n    )\n    synthesizer: synthesizers = Field(\n        ..., alias=\"\u5df2\u6709\u5408\u6210\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u5408\u6210\u6a21\u578b\u6587\u4ef6.\"\n    )\n    gpu: bool = Field(\n        True, alias=\"GPU\u8bad\u7ec3\", description=\"\u9009\u62e9\u201c\u662f\u201d\uff0c\u5219\u4f7f\u7528GPU\u8bad\u7ec3\",\n    )\n    verbose: bool = Field(\n        True, alias=\"\u6253\u5370\u8be6\u60c5\", description=\"\u9009\u62e9\u201c\u662f\u201d\uff0c\u8f93\u51fa\u66f4\u591a\u8be6\u60c5\",\n    )\n    encoder: encoders = Field(\n        ..., alias=\"\u8bed\u97f3\u7f16\u7801\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u7f16\u7801\u6a21\u578b\u6587\u4ef6.\"\n    )\n    save_every: int = Field(\n        1000, alias=\"\u66f4\u65b0\u95f4\u9694\", description=\"\u6bcf\u9694n\u6b65\u5219\u66f4\u65b0\u4e00\u6b21\u6a21\u578b\",\n    )\n    backup_every: int = Field(\n        10000, alias=\"\u4fdd\u5b58\u95f4\u9694\", description=\"\u6bcf\u9694n\u6b65\u5219\u4fdd\u5b58\u4e00\u6b21\u6a21\u578b\",\n    )\n    log_every: int = Field(\n        500, alias=\"\u6253\u5370\u95f4\u9694\", description=\"\u6bcf\u9694n\u6b65\u5219\u6253\u5370\u4e00\u6b21\u8bad\u7ec3\u7edf\u8ba1\",\n    )\n\nclass AudioEntity(BaseModel):\n    content: bytes\n    mel: Any\n\nclass Output(BaseModel):\n    __root__: int\n\n    def render_output_ui(self, streamlit_app) -> None:  # type: ignore\n        \"\"\"Custom output UI.\n        If this method is implmeneted, it will be used instead of the default Output UI renderer.\n        \"\"\"\n        streamlit_app.subheader(f\"Training started with code: {self.__root__}\")\n\ndef train(input: Input) -> Output:\n    \"\"\"Train(\u8bad\u7ec3)\"\"\"\n\n    print(\">>> Start training ...\")\n    force_restart = len(input.run_id) > 0\n    if not force_restart:\n        input.run_id = Path(input.synthesizer.value).name.split('.')[0]\n    \n    synt_train(\n        input.run_id, \n        input.input_root, \n        f\"data{os.sep}ckpt{os.sep}synthesizer\", \n        input.save_every, \n        input.backup_every, \n        input.log_every, \n        force_restart,\n        hparams\n    )\n    return Output(__root__=0)", "control/mkgui/app_vc.py": "import os\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Tuple\n\nimport librosa\nimport matplotlib.pyplot as plt\nimport torch\nfrom pydantic import BaseModel, Field\nfrom scipy.io.wavfile import write\n\nimport models.ppg2mel as Convertor\nimport models.ppg_extractor as Extractor\nfrom control.mkgui.base.components.types import FileContent\nfrom models.encoder import inference as speacker_encoder\nfrom models.synthesizer.inference import Synthesizer\nfrom models.vocoder.hifigan import inference as gan_vocoder\n\n# Constants\nAUDIO_SAMPLES_DIR = f'data{os.sep}samples{os.sep}'\nEXT_MODELS_DIRT = f'data{os.sep}ckpt{os.sep}ppg_extractor'\nCONV_MODELS_DIRT = f'data{os.sep}ckpt{os.sep}ppg2mel'\nVOC_MODELS_DIRT = f'data{os.sep}ckpt{os.sep}vocoder'\nTEMP_SOURCE_AUDIO = f'wavs{os.sep}temp_source.wav'\nTEMP_TARGET_AUDIO = f'wavs{os.sep}temp_target.wav'\nTEMP_RESULT_AUDIO = f'wavs{os.sep}temp_result.wav'\n\n# Load local sample audio as options TODO: load dataset \nif os.path.isdir(AUDIO_SAMPLES_DIR):\n    audio_input_selection = Enum('samples', list((file.name, file) for file in Path(AUDIO_SAMPLES_DIR).glob(\"*.wav\")))\n# Pre-Load models\nif os.path.isdir(EXT_MODELS_DIRT):    \n    extractors =  Enum('extractors', list((file.name, file) for file in Path(EXT_MODELS_DIRT).glob(\"**/*.pt\")))\n    print(\"Loaded extractor models: \" + str(len(extractors)))\nelse:\n    raise Exception(f\"Model folder {EXT_MODELS_DIRT} doesn't exist.\")\n\nif os.path.isdir(CONV_MODELS_DIRT):    \n    convertors =  Enum('convertors', list((file.name, file) for file in Path(CONV_MODELS_DIRT).glob(\"**/*.pth\")))\n    print(\"Loaded convertor models: \" + str(len(convertors)))\nelse:\n    raise Exception(f\"Model folder {CONV_MODELS_DIRT} doesn't exist.\")\n\nif os.path.isdir(VOC_MODELS_DIRT):    \n    vocoders =  Enum('vocoders', list((file.name, file) for file in Path(VOC_MODELS_DIRT).glob(\"**/*gan*.pt\")))\n    print(\"Loaded vocoders models: \" + str(len(vocoders)))\nelse:\n    raise Exception(f\"Model folder {VOC_MODELS_DIRT} doesn't exist.\")\n\nclass Input(BaseModel):\n    local_audio_file: audio_input_selection = Field(\n        ..., alias=\"\u8f93\u5165\u8bed\u97f3\uff08\u672c\u5730wav\uff09\",\n        description=\"\u9009\u62e9\u672c\u5730\u8bed\u97f3\u6587\u4ef6.\"\n    )\n    upload_audio_file: FileContent = Field(default=None, alias=\"\u6216\u4e0a\u4f20\u8bed\u97f3\",\n        description=\"\u62d6\u62fd\u6216\u70b9\u51fb\u4e0a\u4f20.\", mime_type=\"audio/wav\")\n    local_audio_file_target: audio_input_selection = Field(\n        ..., alias=\"\u76ee\u6807\u8bed\u97f3\uff08\u672c\u5730wav\uff09\",\n        description=\"\u9009\u62e9\u672c\u5730\u8bed\u97f3\u6587\u4ef6.\"\n    )\n    upload_audio_file_target: FileContent = Field(default=None, alias=\"\u6216\u4e0a\u4f20\u76ee\u6807\u8bed\u97f3\",\n        description=\"\u62d6\u62fd\u6216\u70b9\u51fb\u4e0a\u4f20.\", mime_type=\"audio/wav\")\n    extractor: extractors = Field(\n        ..., alias=\"\u7f16\u7801\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u7f16\u7801\u6a21\u578b\u6587\u4ef6.\"\n    )\n    convertor: convertors = Field(\n        ..., alias=\"\u8f6c\u6362\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u8f6c\u6362\u6a21\u578b\u6587\u4ef6.\"\n    )\n    vocoder: vocoders = Field(\n        ..., alias=\"\u8bed\u97f3\u89e3\u7801\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u89e3\u7801\u6a21\u578b\u6587\u4ef6(\u76ee\u524d\u53ea\u652f\u6301HifiGan\u7c7b\u578b).\"\n    )\n\nclass AudioEntity(BaseModel):\n    content: bytes\n    mel: Any\n\nclass Output(BaseModel):\n    __root__: Tuple[AudioEntity, AudioEntity, AudioEntity]\n\n    def render_output_ui(self, streamlit_app, input) -> None:  # type: ignore\n        \"\"\"Custom output UI.\n        If this method is implmeneted, it will be used instead of the default Output UI renderer.\n        \"\"\"\n        src, target, result = self.__root__\n        \n        streamlit_app.subheader(\"Synthesized Audio\")\n        streamlit_app.audio(result.content, format=\"audio/wav\")\n\n        fig, ax = plt.subplots()\n        ax.imshow(src.mel, aspect=\"equal\", interpolation=\"none\")\n        ax.set_title(\"mel spectrogram(Source Audio)\")\n        streamlit_app.pyplot(fig)\n        fig, ax = plt.subplots()\n        ax.imshow(target.mel, aspect=\"equal\", interpolation=\"none\")\n        ax.set_title(\"mel spectrogram(Target Audio)\")\n        streamlit_app.pyplot(fig)\n        fig, ax = plt.subplots()\n        ax.imshow(result.mel, aspect=\"equal\", interpolation=\"none\")\n        ax.set_title(\"mel spectrogram(Result Audio)\")\n        streamlit_app.pyplot(fig)\n\ndef convert(input: Input) -> Output:\n    \"\"\"convert(\u8f6c\u6362)\"\"\"\n    # load models\n    extractor = Extractor.load_model(Path(input.extractor.value))\n    convertor = Convertor.load_model(Path(input.convertor.value))\n    # current_synt = Synthesizer(Path(input.synthesizer.value))\n    gan_vocoder.load_model(Path(input.vocoder.value))\n\n    # load file\n    if input.upload_audio_file != None:\n        with open(TEMP_SOURCE_AUDIO, \"w+b\") as f:\n            f.write(input.upload_audio_file.as_bytes())\n            f.seek(0)\n        src_wav, sample_rate = librosa.load(TEMP_SOURCE_AUDIO)\n    else:\n        src_wav, sample_rate  = librosa.load(input.local_audio_file.value)\n        write(TEMP_SOURCE_AUDIO, sample_rate, src_wav) #Make sure we get the correct wav\n\n    if input.upload_audio_file_target != None:\n        with open(TEMP_TARGET_AUDIO, \"w+b\") as f:\n            f.write(input.upload_audio_file_target.as_bytes())\n            f.seek(0)\n        ref_wav, _ = librosa.load(TEMP_TARGET_AUDIO)\n    else:\n        ref_wav, _  = librosa.load(input.local_audio_file_target.value)\n        write(TEMP_TARGET_AUDIO, sample_rate, ref_wav) #Make sure we get the correct wav\n\n    ppg = extractor.extract_from_wav(src_wav)\n    # Import necessary dependency of Voice Conversion\n    from utils.f0_utils import (compute_f0, compute_mean_std, f02lf0,\n                                get_converted_lf0uv)   \n    ref_lf0_mean, ref_lf0_std = compute_mean_std(f02lf0(compute_f0(ref_wav)))\n    speacker_encoder.load_model(Path(f\"data{os.sep}ckpt{os.sep}encoder{os.sep}pretrained_bak_5805000.pt\"))\n    embed = speacker_encoder.embed_utterance(ref_wav)\n    lf0_uv = get_converted_lf0uv(src_wav, ref_lf0_mean, ref_lf0_std, convert=True)\n    min_len = min(ppg.shape[1], len(lf0_uv))\n    ppg = ppg[:, :min_len]\n    lf0_uv = lf0_uv[:min_len]\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _, mel_pred, att_ws = convertor.inference(\n        ppg,\n        logf0_uv=torch.from_numpy(lf0_uv).unsqueeze(0).float().to(device),\n        spembs=torch.from_numpy(embed).unsqueeze(0).to(device),\n    )\n    mel_pred= mel_pred.transpose(0, 1)\n    breaks = [mel_pred.shape[1]]\n    mel_pred= mel_pred.detach().cpu().numpy()\n\n    # synthesize and vocode\n    wav, sample_rate = gan_vocoder.infer_waveform(mel_pred)\n\n    # write and output \n    write(TEMP_RESULT_AUDIO, sample_rate, wav) #Make sure we get the correct wav\n    with open(TEMP_SOURCE_AUDIO, \"rb\") as f:\n        source_file = f.read()\n    with open(TEMP_TARGET_AUDIO, \"rb\") as f:\n        target_file = f.read()\n    with open(TEMP_RESULT_AUDIO, \"rb\") as f:\n        result_file = f.read()\n    \n\n    return Output(__root__=(AudioEntity(content=source_file, mel=Synthesizer.make_spectrogram(src_wav)), AudioEntity(content=target_file, mel=Synthesizer.make_spectrogram(ref_wav)), AudioEntity(content=result_file, mel=Synthesizer.make_spectrogram(wav))))", "control/mkgui/__init__.py": "", "control/mkgui/app.py": "from pydantic import BaseModel, Field\nimport os\nfrom pathlib import Path\nfrom enum import Enum\nfrom models.encoder import inference as encoder\nimport librosa\nfrom scipy.io.wavfile import write\nimport re\nimport numpy as np\nfrom control.mkgui.base.components.types import FileContent\nfrom models.vocoder.hifigan import inference as gan_vocoder\nfrom models.synthesizer.inference import Synthesizer\nfrom typing import Any, Tuple\nimport matplotlib.pyplot as plt\n\n# Constants\nAUDIO_SAMPLES_DIR = f\"data{os.sep}samples{os.sep}\"\nSYN_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}synthesizer\"\nENC_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}encoder\"\nVOC_MODELS_DIRT = f\"data{os.sep}ckpt{os.sep}vocoder\"\nTEMP_SOURCE_AUDIO = f\"wavs{os.sep}temp_source.wav\"\nTEMP_RESULT_AUDIO = f\"wavs{os.sep}temp_result.wav\"\nif not os.path.isdir(\"wavs\"):\n    os.makedirs(\"wavs\")\n\n# Load local sample audio as options TODO: load dataset \nif os.path.isdir(AUDIO_SAMPLES_DIR):\n    audio_input_selection = Enum('samples', list((file.name, file) for file in Path(AUDIO_SAMPLES_DIR).glob(\"*.wav\")))\n# Pre-Load models\nif os.path.isdir(SYN_MODELS_DIRT):    \n    synthesizers =  Enum('synthesizers', list((file.name, file) for file in Path(SYN_MODELS_DIRT).glob(\"**/*.pt\")))\n    print(\"Loaded synthesizer models: \" + str(len(synthesizers)))\nelse:\n    raise Exception(f\"Model folder {SYN_MODELS_DIRT} doesn't exist. \u8bf7\u5c06\u6a21\u578b\u6587\u4ef6\u4f4d\u7f6e\u79fb\u52a8\u5230\u4e0a\u8ff0\u4f4d\u7f6e\u4e2d\u8fdb\u884c\u91cd\u8bd5\uff01\")\n\nif os.path.isdir(ENC_MODELS_DIRT):    \n    encoders =  Enum('encoders', list((file.name, file) for file in Path(ENC_MODELS_DIRT).glob(\"**/*.pt\")))\n    print(\"Loaded encoders models: \" + str(len(encoders)))\nelse:\n    raise Exception(f\"Model folder {ENC_MODELS_DIRT} doesn't exist.\")\n\nif os.path.isdir(VOC_MODELS_DIRT):    \n    vocoders =  Enum('vocoders', list((file.name, file) for file in Path(VOC_MODELS_DIRT).glob(\"**/*gan*.pt\")))\n    print(\"Loaded vocoders models: \" + str(len(synthesizers)))\nelse:\n    raise Exception(f\"Model folder {VOC_MODELS_DIRT} doesn't exist.\")\n\n\nclass Input(BaseModel):\n    message: str = Field(\n        ..., example=\"\u6b22\u8fce\u4f7f\u7528\u5de5\u5177\u7bb1, \u73b0\u5df2\u652f\u6301\u4e2d\u6587\u8f93\u5165\uff01\", alias=\"\u6587\u672c\u5185\u5bb9\"\n    )\n    local_audio_file: audio_input_selection = Field(\n        ..., alias=\"\u9009\u62e9\u8bed\u97f3\uff08\u672c\u5730wav\uff09\",\n        description=\"\u9009\u62e9\u672c\u5730\u8bed\u97f3\u6587\u4ef6.\"\n    )\n    record_audio_file: FileContent = Field(default=None, alias=\"\u5f55\u5236\u8bed\u97f3\",\n        description=\"\u5f55\u97f3.\", is_recorder=True, mime_type=\"audio/wav\")\n    upload_audio_file: FileContent = Field(default=None, alias=\"\u6216\u4e0a\u4f20\u8bed\u97f3\",\n        description=\"\u62d6\u62fd\u6216\u70b9\u51fb\u4e0a\u4f20.\", mime_type=\"audio/wav\")\n    encoder: encoders = Field(\n        ..., alias=\"\u7f16\u7801\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u7f16\u7801\u6a21\u578b\u6587\u4ef6.\"\n    )\n    synthesizer: synthesizers = Field(\n        ..., alias=\"\u5408\u6210\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u5408\u6210\u6a21\u578b\u6587\u4ef6.\"\n    )\n    vocoder: vocoders = Field(\n        ..., alias=\"\u8bed\u97f3\u89e3\u7801\u6a21\u578b\", \n        description=\"\u9009\u62e9\u8bed\u97f3\u89e3\u7801\u6a21\u578b\u6587\u4ef6(\u76ee\u524d\u53ea\u652f\u6301HifiGan\u7c7b\u578b).\"\n    )\n\nclass AudioEntity(BaseModel):\n    content: bytes\n    mel: Any\n\nclass Output(BaseModel):\n    __root__: Tuple[AudioEntity, AudioEntity]\n\n    def render_output_ui(self, streamlit_app, input) -> None:  # type: ignore\n        \"\"\"Custom output UI.\n        If this method is implmeneted, it will be used instead of the default Output UI renderer.\n        \"\"\"\n        src, result = self.__root__\n        \n        streamlit_app.subheader(\"Synthesized Audio\")\n        streamlit_app.audio(result.content, format=\"audio/wav\")\n\n        fig, ax = plt.subplots()\n        ax.imshow(src.mel, aspect=\"equal\", interpolation=\"none\")\n        ax.set_title(\"mel spectrogram(Source Audio)\")\n        streamlit_app.pyplot(fig)\n        fig, ax = plt.subplots()\n        ax.imshow(result.mel, aspect=\"equal\", interpolation=\"none\")\n        ax.set_title(\"mel spectrogram(Result Audio)\")\n        streamlit_app.pyplot(fig)\n\n\ndef synthesize(input: Input) -> Output:\n    \"\"\"synthesize(\u5408\u6210)\"\"\"\n    # load models\n    encoder.load_model(Path(input.encoder.value))\n    current_synt = Synthesizer(Path(input.synthesizer.value))\n    gan_vocoder.load_model(Path(input.vocoder.value))\n\n    # load file\n    if input.record_audio_file != None:\n        with open(TEMP_SOURCE_AUDIO, \"w+b\") as f:\n            f.write(input.record_audio_file.as_bytes())\n            f.seek(0)\n        wav, sample_rate = librosa.load(TEMP_SOURCE_AUDIO)\n    elif input.upload_audio_file != None:\n        with open(TEMP_SOURCE_AUDIO, \"w+b\") as f:\n            f.write(input.upload_audio_file.as_bytes())\n            f.seek(0)\n        wav, sample_rate = librosa.load(TEMP_SOURCE_AUDIO)\n    else:\n        wav, sample_rate  = librosa.load(input.local_audio_file.value)\n        write(TEMP_SOURCE_AUDIO, sample_rate, wav) #Make sure we get the correct wav\n\n    source_spec = Synthesizer.make_spectrogram(wav)\n\n    # preprocess\n    encoder_wav = encoder.preprocess_wav(wav, sample_rate)\n    embed, _, _ = encoder.embed_utterance(encoder_wav, return_partials=True)\n\n    # Load input text\n    texts = filter(None, input.message.split(\"\\n\"))\n    punctuation = '\uff01\uff0c\u3002\u3001,' # punctuate and split/clean text\n    processed_texts = []\n    for text in texts:\n        for processed_text in re.sub(r'[{}]+'.format(punctuation), '\\n', text).split('\\n'):\n            if processed_text:\n                processed_texts.append(processed_text.strip())\n    texts = processed_texts\n\n    # synthesize and vocode\n    embeds = [embed] * len(texts)\n    specs = current_synt.synthesize_spectrograms(texts, embeds)\n    spec = np.concatenate(specs, axis=1)\n    sample_rate = Synthesizer.sample_rate\n    wav, sample_rate = gan_vocoder.infer_waveform(spec)\n\n    # write and output \n    write(TEMP_RESULT_AUDIO, sample_rate, wav) #Make sure we get the correct wav\n    with open(TEMP_SOURCE_AUDIO, \"rb\") as f:\n        source_file = f.read()\n    with open(TEMP_RESULT_AUDIO, \"rb\") as f:\n        result_file = f.read()\n    return Output(__root__=(AudioEntity(content=source_file, mel=source_spec), AudioEntity(content=result_file, mel=spec)))", "control/mkgui/base/core.py": "import importlib\nimport inspect\nimport re\nfrom typing import Any, Callable, Type, Union, get_type_hints\n\nfrom pydantic import BaseModel, parse_raw_as\nfrom pydantic.tools import parse_obj_as\n\n\ndef name_to_title(name: str) -> str:\n    \"\"\"Converts a camelCase or snake_case name to title case.\"\"\"\n    # If camelCase -> convert to snake case\n    name = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    name = re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name).lower()\n    # Convert to title case\n    return name.replace(\"_\", \" \").strip().title()\n\n\ndef is_compatible_type(type: Type) -> bool:\n    \"\"\"Returns `True` if the type is opyrator-compatible.\"\"\"\n    try:\n        if issubclass(type, BaseModel):\n            return True\n    except Exception:\n        pass\n\n    try:\n        # valid list type\n        if type.__origin__ is list and issubclass(type.__args__[0], BaseModel):\n            return True\n    except Exception:\n        pass\n\n    return False\n\n\ndef get_input_type(func: Callable) -> Type:\n    \"\"\"Returns the input type of a given function (callable).\n\n    Args:\n        func: The function for which to get the input type.\n\n    Raises:\n        ValueError: If the function does not have a valid input type annotation.\n    \"\"\"\n    type_hints = get_type_hints(func)\n\n    if \"input\" not in type_hints:\n        raise ValueError(\n            \"The callable MUST have a parameter with the name `input` with typing annotation. \"\n            \"For example: `def my_opyrator(input: InputModel) -> OutputModel:`.\"\n        )\n\n    input_type = type_hints[\"input\"]\n\n    if not is_compatible_type(input_type):\n        raise ValueError(\n            \"The `input` parameter MUST be a subclass of the Pydantic BaseModel or a list of Pydantic models.\"\n        )\n\n    # TODO: return warning if more than one input parameters\n\n    return input_type\n\n\ndef get_output_type(func: Callable) -> Type:\n    \"\"\"Returns the output type of a given function (callable).\n\n    Args:\n        func: The function for which to get the output type.\n\n    Raises:\n        ValueError: If the function does not have a valid output type annotation.\n    \"\"\"\n    type_hints = get_type_hints(func)\n    if \"return\" not in type_hints:\n        raise ValueError(\n            \"The return type of the callable MUST be annotated with type hints.\"\n            \"For example: `def my_opyrator(input: InputModel) -> OutputModel:`.\"\n        )\n\n    output_type = type_hints[\"return\"]\n\n    if not is_compatible_type(output_type):\n        raise ValueError(\n            \"The return value MUST be a subclass of the Pydantic BaseModel or a list of Pydantic models.\"\n        )\n\n    return output_type\n\n\ndef get_callable(import_string: str) -> Callable:\n    \"\"\"Import a callable from an string.\"\"\"\n    callable_seperator = \":\"\n    if callable_seperator not in import_string:\n        # Use dot as seperator\n        callable_seperator = \".\"\n\n    if callable_seperator not in import_string:\n        raise ValueError(\"The callable path MUST specify the function. \")\n\n    mod_name, callable_name = import_string.rsplit(callable_seperator, 1)\n    mod = importlib.import_module(mod_name)\n    return getattr(mod, callable_name)\n\n\nclass Opyrator:\n    def __init__(self, func: Union[Callable, str]) -> None:\n        if isinstance(func, str):\n            # Try to load the function from a string notion\n            self.function = get_callable(func)\n        else:\n            self.function = func\n\n        self._action = \"Execute\"\n        self._input_type = None\n        self._output_type = None\n\n        if not callable(self.function):\n            raise ValueError(\"The provided function parameters is not a callable.\")\n\n        if inspect.isclass(self.function):\n            raise ValueError(\n                \"The provided callable is an uninitialized Class. This is not allowed.\"\n            )\n\n        if inspect.isfunction(self.function):\n            # The provided callable is a function\n            self._input_type = get_input_type(self.function)\n            self._output_type = get_output_type(self.function)\n\n            try:\n                # Get name\n                self._name = name_to_title(self.function.__name__)\n            except Exception:\n                pass\n\n            try:\n                # Get description from function\n                doc_string = inspect.getdoc(self.function)\n                if doc_string:\n                    self._action = doc_string\n            except Exception:\n                pass\n        elif hasattr(self.function, \"__call__\"):\n            # The provided callable is a function\n            self._input_type = get_input_type(self.function.__call__)  # type: ignore\n            self._output_type = get_output_type(self.function.__call__)  # type: ignore\n\n            try:\n                # Get name\n                self._name = name_to_title(type(self.function).__name__)\n            except Exception:\n                pass\n\n            try:\n                # Get action from\n                doc_string = inspect.getdoc(self.function.__call__)  # type: ignore\n                if doc_string:\n                    self._action = doc_string\n\n                if (\n                    not self._action\n                    or self._action == \"Call\"\n                ):\n                    # Get docstring from class instead of __call__ function\n                    doc_string = inspect.getdoc(self.function)\n                    if doc_string:\n                        self._action = doc_string\n            except Exception:\n                pass\n        else:\n            raise ValueError(\"Unknown callable type.\")\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    @property\n    def action(self) -> str:\n        return self._action\n\n    @property\n    def input_type(self) -> Any:\n        return self._input_type\n\n    @property\n    def output_type(self) -> Any:\n        return self._output_type\n\n    def __call__(self, input: Any, **kwargs: Any) -> Any:\n\n        input_obj = input\n\n        if isinstance(input, str):\n            # Allow json input\n            input_obj = parse_raw_as(self.input_type, input)\n\n        if isinstance(input, dict):\n            # Allow dict input\n            input_obj = parse_obj_as(self.input_type, input)\n\n        return self.function(input_obj, **kwargs)\n", "control/mkgui/base/__init__.py": "\nfrom .core import Opyrator\n", "control/mkgui/base/components/outputs.py": "from typing import List\n\nfrom pydantic import BaseModel\n\n\nclass ScoredLabel(BaseModel):\n    label: str\n    score: float\n\n\nclass ClassificationOutput(BaseModel):\n    __root__: List[ScoredLabel]\n\n    def __iter__(self):  # type: ignore\n        return iter(self.__root__)\n\n    def __getitem__(self, item):  # type: ignore\n        return self.__root__[item]\n\n    def render_output_ui(self, streamlit) -> None:  # type: ignore\n        import plotly.express as px\n\n        sorted_predictions = sorted(\n            [prediction.dict() for prediction in self.__root__],\n            key=lambda k: k[\"score\"],\n        )\n\n        num_labels = len(sorted_predictions)\n        if len(sorted_predictions) > 10:\n            num_labels = streamlit.slider(\n                \"Maximum labels to show: \",\n                min_value=1,\n                max_value=len(sorted_predictions),\n                value=len(sorted_predictions),\n            )\n        fig = px.bar(\n            sorted_predictions[len(sorted_predictions) - num_labels :],\n            x=\"score\",\n            y=\"label\",\n            orientation=\"h\",\n        )\n        streamlit.plotly_chart(fig, use_container_width=True)\n        # fig.show()\n", "control/mkgui/base/components/types.py": "import base64\nfrom typing import Any, Dict, overload\n\n\nclass FileContent(str):\n    def as_bytes(self) -> bytes:\n        return base64.b64decode(self, validate=True)\n\n    def as_str(self) -> str:\n        return self.as_bytes().decode()\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        field_schema.update(format=\"byte\")\n\n    @classmethod\n    def __get_validators__(cls) -> Any:  # type: ignore\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Any) -> \"FileContent\":\n        if isinstance(value, FileContent):\n            return value\n        elif isinstance(value, str):\n            return FileContent(value)\n        elif isinstance(value, (bytes, bytearray, memoryview)):\n            return FileContent(base64.b64encode(value).decode())\n        else:\n            raise Exception(\"Wrong type\")\n\n# # \u6682\u65f6\u65e0\u6cd5\u4f7f\u7528\uff0c\u56e0\u4e3a\u6d4f\u89c8\u5668\u4e2d\u6ca1\u6709\u8003\u8651\u9009\u62e9\u6587\u4ef6\u5939\n# class DirectoryContent(FileContent):\n#     @classmethod\n#     def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n#         field_schema.update(format=\"path\")\n\n#     @classmethod\n#     def validate(cls, value: Any) -> \"DirectoryContent\":\n#         if isinstance(value, DirectoryContent):\n#             return value\n#         elif isinstance(value, str):\n#             return DirectoryContent(value)\n#         elif isinstance(value, (bytes, bytearray, memoryview)):\n#             return DirectoryContent(base64.b64encode(value).decode())\n#         else:\n#             raise Exception(\"Wrong type\")\n", "control/mkgui/base/components/__init__.py": "", "control/mkgui/base/ui/schema_utils.py": "from typing import Dict\n\n\ndef resolve_reference(reference: str, references: Dict) -> Dict:\n    return references[reference.split(\"/\")[-1]]\n\n\ndef get_single_reference_item(property: Dict, references: Dict) -> Dict:\n    # Ref can either be directly in the properties or the first element of allOf\n    reference = property.get(\"$ref\")\n    if reference is None:\n        reference = property[\"allOf\"][0][\"$ref\"]\n    return resolve_reference(reference, references)\n\n\ndef is_single_string_property(property: Dict) -> bool:\n    return property.get(\"type\") == \"string\"\n\n\ndef is_single_datetime_property(property: Dict) -> bool:\n    if property.get(\"type\") != \"string\":\n        return False\n    return property.get(\"format\") in [\"date-time\", \"time\", \"date\"]\n\n\ndef is_single_boolean_property(property: Dict) -> bool:\n    return property.get(\"type\") == \"boolean\"\n\n\ndef is_single_number_property(property: Dict) -> bool:\n    return property.get(\"type\") in [\"integer\", \"number\"]\n\n\ndef is_single_file_property(property: Dict) -> bool:\n    if property.get(\"type\") != \"string\":\n        return False\n    # TODO: binary?\n    return property.get(\"format\") == \"byte\"\n\ndef is_single_autio_property(property: Dict) -> bool:\n    if property.get(\"type\") != \"string\":\n        return False\n    # TODO: binary?\n    return property.get(\"format\") == \"bytes\"\n\n\ndef is_single_directory_property(property: Dict) -> bool:\n    if property.get(\"type\") != \"string\":\n        return False\n    return property.get(\"format\") == \"path\"\n\ndef is_multi_enum_property(property: Dict, references: Dict) -> bool:\n    if property.get(\"type\") != \"array\":\n        return False\n\n    if property.get(\"uniqueItems\") is not True:\n        # Only relevant if it is a set or other datastructures with unique items\n        return False\n\n    try:\n        _ = resolve_reference(property[\"items\"][\"$ref\"], references)[\"enum\"]\n        return True\n    except Exception:\n        return False\n\n\ndef is_single_enum_property(property: Dict, references: Dict) -> bool:\n    try:\n        _ = get_single_reference_item(property, references)[\"enum\"]\n        return True\n    except Exception:\n        return False\n\n\ndef is_single_dict_property(property: Dict) -> bool:\n    if property.get(\"type\") != \"object\":\n        return False\n    return \"additionalProperties\" in property\n\n\ndef is_single_reference(property: Dict) -> bool:\n    if property.get(\"type\") is not None:\n        return False\n\n    return bool(property.get(\"$ref\"))\n\n\ndef is_multi_file_property(property: Dict) -> bool:\n    if property.get(\"type\") != \"array\":\n        return False\n\n    if property.get(\"items\") is None:\n        return False\n\n    try:\n        # TODO: binary\n        return property[\"items\"][\"format\"] == \"byte\"\n    except Exception:\n        return False\n\n\ndef is_single_object(property: Dict, references: Dict) -> bool:\n    try:\n        object_reference = get_single_reference_item(property, references)\n        if object_reference[\"type\"] != \"object\":\n            return False\n        return \"properties\" in object_reference\n    except Exception:\n        return False\n\n\ndef is_property_list(property: Dict) -> bool:\n    if property.get(\"type\") != \"array\":\n        return False\n\n    if property.get(\"items\") is None:\n        return False\n\n    try:\n        return property[\"items\"][\"type\"] in [\"string\", \"number\", \"integer\"]\n    except Exception:\n        return False\n\n\ndef is_object_list_property(property: Dict, references: Dict) -> bool:\n    if property.get(\"type\") != \"array\":\n        return False\n\n    try:\n        object_reference = resolve_reference(property[\"items\"][\"$ref\"], references)\n        if object_reference[\"type\"] != \"object\":\n            return False\n        return \"properties\" in object_reference\n    except Exception:\n        return False\n", "control/mkgui/base/ui/streamlit_ui.py": "import datetime\nimport inspect\nimport mimetypes\nimport sys\nfrom os import getcwd, unlink, path\nfrom platform import system\nfrom tempfile import NamedTemporaryFile\nfrom typing import Any, Callable, Dict, List, Type\nfrom PIL import Image\n\nimport pandas as pd\nimport streamlit as st\nfrom fastapi.encoders import jsonable_encoder\nfrom loguru import logger\nfrom pydantic import BaseModel, ValidationError, parse_obj_as\n\nfrom control.mkgui.base import Opyrator\nfrom control.mkgui.base.core import name_to_title\nfrom . import schema_utils\nfrom .streamlit_utils import CUSTOM_STREAMLIT_CSS\n\nSTREAMLIT_RUNNER_SNIPPET = \"\"\"\nfrom control.mkgui.base.ui import render_streamlit_ui\n\nimport streamlit as st\n\n# TODO: Make it configurable\n# Page config can only be setup once\nst.set_page_config(\n    page_title=\"MockingBird\", \n    page_icon=\"\ud83e\uddca\",\n    layout=\"wide\")\n\nrender_streamlit_ui()\n\"\"\"\n\n# with st.spinner(\"Loading MockingBird GUI. Please wait...\"):\n#     opyrator = Opyrator(\"{opyrator_path}\")\n\n\ndef launch_ui(port: int = 8501) -> None:\n    with NamedTemporaryFile(\n        suffix=\".py\", mode=\"w\", encoding=\"utf-8\", delete=False\n    ) as f:\n        f.write(STREAMLIT_RUNNER_SNIPPET)\n        f.seek(0)\n\n        import subprocess\n\n        python_path = f'PYTHONPATH=\"$PYTHONPATH:{getcwd()}\"'\n        if system() == \"Windows\":\n            python_path = f\"set PYTHONPATH=%PYTHONPATH%;{getcwd()} &&\"\n            subprocess.run(\n                f\"\"\"set STREAMLIT_GLOBAL_SHOW_WARNING_ON_DIRECT_EXECUTION=false\"\"\",\n                shell=True,\n            )\n\n        subprocess.run(\n            f\"\"\"{python_path} \"{sys.executable}\" -m streamlit run --server.port={port} --server.headless=True --runner.magicEnabled=False --server.maxUploadSize=50  --browser.gatherUsageStats=False {f.name}\"\"\",\n            shell=True,\n        )\n\n        f.close()\n        unlink(f.name)\n\n\ndef function_has_named_arg(func: Callable, parameter: str) -> bool:\n    try:\n        sig = inspect.signature(func)\n        for param in sig.parameters.values():\n            if param.name == \"input\":\n                return True\n    except Exception:\n        return False\n    return False\n\n\ndef has_output_ui_renderer(data_item: BaseModel) -> bool:\n    return hasattr(data_item, \"render_output_ui\")\n\n\ndef has_input_ui_renderer(input_class: Type[BaseModel]) -> bool:\n    return hasattr(input_class, \"render_input_ui\")\n\n\ndef is_compatible_audio(mime_type: str) -> bool:\n    return mime_type in [\"audio/mpeg\", \"audio/ogg\", \"audio/wav\"]\n\n\ndef is_compatible_image(mime_type: str) -> bool:\n    return mime_type in [\"image/png\", \"image/jpeg\"]\n\n\ndef is_compatible_video(mime_type: str) -> bool:\n    return mime_type in [\"video/mp4\"]\n\n\nclass InputUI:\n    def __init__(self, session_state, input_class: Type[BaseModel]):\n        self._session_state = session_state\n        self._input_class = input_class\n\n        self._schema_properties = input_class.schema(by_alias=True).get(\n            \"properties\", {}\n        )\n        self._schema_references = input_class.schema(by_alias=True).get(\n            \"definitions\", {}\n        )\n\n    def render_ui(self, streamlit_app_root) -> None:\n        if has_input_ui_renderer(self._input_class):\n            # The input model has a rendering function\n            # The rendering also returns the current state of input data\n            self._session_state.input_data = self._input_class.render_input_ui(  # type: ignore\n                st, self._session_state.input_data\n            )\n            return\n\n        # print(self._schema_properties)\n        for property_key in self._schema_properties.keys():\n            property = self._schema_properties[property_key]\n\n            if not property.get(\"title\"):\n                # Set property key as fallback title\n                property[\"title\"] = name_to_title(property_key)\n\n            try:\n                if \"input_data\" in self._session_state:\n                    self._store_value(\n                        property_key,\n                        self._render_property(streamlit_app_root, property_key, property),\n                    )\n            except Exception as e:\n                print(\"Exception!\", e)\n                pass\n\n    def _get_default_streamlit_input_kwargs(self, key: str, property: Dict) -> Dict:\n        streamlit_kwargs = {\n            \"label\": property.get(\"title\"),\n            \"key\": key,\n        }\n\n        if property.get(\"description\"):\n            streamlit_kwargs[\"help\"] = property.get(\"description\")\n        return streamlit_kwargs\n\n    def _store_value(self, key: str, value: Any) -> None:\n        data_element = self._session_state.input_data\n        key_elements = key.split(\".\")\n        for i, key_element in enumerate(key_elements):\n            if i == len(key_elements) - 1:\n                # add value to this element\n                data_element[key_element] = value\n                return\n            if key_element not in data_element:\n                data_element[key_element] = {}\n            data_element = data_element[key_element]\n\n    def _get_value(self, key: str) -> Any:\n        data_element = self._session_state.input_data\n        key_elements = key.split(\".\")\n        for i, key_element in enumerate(key_elements):\n            if i == len(key_elements) - 1:\n                # add value to this element\n                if key_element not in data_element:\n                    return None\n                return data_element[key_element]\n            if key_element not in data_element:\n                data_element[key_element] = {}\n            data_element = data_element[key_element]\n        return None\n\n    def _render_single_datetime_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n        streamlit_kwargs = self._get_default_streamlit_input_kwargs(key, property)\n\n        if property.get(\"format\") == \"time\":\n            if property.get(\"default\"):\n                try:\n                    streamlit_kwargs[\"value\"] = datetime.time.fromisoformat(  # type: ignore\n                        property.get(\"default\")\n                    )\n                except Exception:\n                    pass\n            return streamlit_app.time_input(**streamlit_kwargs)\n        elif property.get(\"format\") == \"date\":\n            if property.get(\"default\"):\n                try:\n                    streamlit_kwargs[\"value\"] = datetime.date.fromisoformat(  # type: ignore\n                        property.get(\"default\")\n                    )\n                except Exception:\n                    pass\n            return streamlit_app.date_input(**streamlit_kwargs)\n        elif property.get(\"format\") == \"date-time\":\n            if property.get(\"default\"):\n                try:\n                    streamlit_kwargs[\"value\"] = datetime.datetime.fromisoformat(  # type: ignore\n                        property.get(\"default\")\n                    )\n                except Exception:\n                    pass\n            with streamlit_app.container():\n                streamlit_app.subheader(streamlit_kwargs.get(\"label\"))\n                if streamlit_kwargs.get(\"description\"):\n                    streamlit_app.text(streamlit_kwargs.get(\"description\"))\n                selected_date = None\n                selected_time = None\n                date_col, time_col = streamlit_app.columns(2)\n                with date_col:\n                    date_kwargs = {\"label\": \"Date\", \"key\": key + \"-date-input\"}\n                    if streamlit_kwargs.get(\"value\"):\n                        try:\n                            date_kwargs[\"value\"] = streamlit_kwargs.get(  # type: ignore\n                                \"value\"\n                            ).date()\n                        except Exception:\n                            pass\n                    selected_date = streamlit_app.date_input(**date_kwargs)\n\n                with time_col:\n                    time_kwargs = {\"label\": \"Time\", \"key\": key + \"-time-input\"}\n                    if streamlit_kwargs.get(\"value\"):\n                        try:\n                            time_kwargs[\"value\"] = streamlit_kwargs.get(  # type: ignore\n                                \"value\"\n                            ).time()\n                        except Exception:\n                            pass\n                    selected_time = streamlit_app.time_input(**time_kwargs)\n                return datetime.datetime.combine(selected_date, selected_time)\n        else:\n            streamlit_app.warning(\n                \"Date format is not supported: \" + str(property.get(\"format\"))\n            )\n\n    def _render_single_file_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n        streamlit_kwargs = self._get_default_streamlit_input_kwargs(key, property)\n        file_extension = None\n        if \"mime_type\" in property:\n            file_extension = mimetypes.guess_extension(property[\"mime_type\"])\n        \n        if \"is_recorder\" in property:\n            from audio_recorder_streamlit import audio_recorder\n            audio_bytes = audio_recorder()\n            if audio_bytes:\n                streamlit_app.audio(audio_bytes, format=\"audio/wav\")\n            return audio_bytes\n        \n        uploaded_file = streamlit_app.file_uploader(\n            **streamlit_kwargs, accept_multiple_files=False, type=file_extension\n        )\n        if uploaded_file is None:\n            return None\n\n        bytes = uploaded_file.getvalue()\n        if property.get(\"mime_type\"):\n            if is_compatible_audio(property[\"mime_type\"]):\n                # Show audio\n                streamlit_app.audio(bytes, format=property.get(\"mime_type\"))\n            if is_compatible_image(property[\"mime_type\"]):\n                # Show image\n                streamlit_app.image(bytes)\n            if is_compatible_video(property[\"mime_type\"]):\n                # Show video\n                streamlit_app.video(bytes, format=property.get(\"mime_type\"))\n        return bytes\n\n    def _render_single_audio_input(\n            self, streamlit_app: st, key: str, property: Dict\n        ) -> Any:\n            # streamlit_kwargs = self._get_default_streamlit_input_kwargs(key, property)\n            from audio_recorder_streamlit import audio_recorder\n            audio_bytes = audio_recorder()\n            if audio_bytes:\n                streamlit_app.audio(audio_bytes, format=\"audio/wav\")\n            return audio_bytes\n\n            # file_extension = None\n            # if \"mime_type\" in property:\n            #     file_extension = mimetypes.guess_extension(property[\"mime_type\"])\n\n            # uploaded_file = streamlit_app.file_uploader(\n            #     **streamlit_kwargs, accept_multiple_files=False, type=file_extension\n            # )\n            # if uploaded_file is None:\n            #     return None\n\n            # bytes = uploaded_file.getvalue()\n            # if property.get(\"mime_type\"):\n            #     if is_compatible_audio(property[\"mime_type\"]):\n            #         # Show audio\n            #         streamlit_app.audio(bytes, format=property.get(\"mime_type\"))\n            #     if is_compatible_image(property[\"mime_type\"]):\n            #         # Show image\n            #         streamlit_app.image(bytes)\n            #     if is_compatible_video(property[\"mime_type\"]):\n            #         # Show video\n            #         streamlit_app.video(bytes, format=property.get(\"mime_type\"))\n            # return bytes\n\n    def _render_single_string_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n        streamlit_kwargs = self._get_default_streamlit_input_kwargs(key, property)\n\n        if property.get(\"default\"):\n            streamlit_kwargs[\"value\"] = property.get(\"default\")\n        elif property.get(\"example\"):\n            # TODO: also use example for other property types\n            # Use example as value if it is provided\n            streamlit_kwargs[\"value\"] = property.get(\"example\")\n\n        if property.get(\"maxLength\") is not None:\n            streamlit_kwargs[\"max_chars\"] = property.get(\"maxLength\")\n\n        if (\n            property.get(\"format\")\n            or (\n                property.get(\"maxLength\") is not None\n                and int(property.get(\"maxLength\")) < 140  # type: ignore\n            )\n            or property.get(\"writeOnly\")\n        ):\n            # If any format is set, use single text input\n            # If max chars is set to less than 140, use single text input\n            # If write only -> password field\n            if property.get(\"writeOnly\"):\n                streamlit_kwargs[\"type\"] = \"password\"\n            return streamlit_app.text_input(**streamlit_kwargs)\n        else:\n            # Otherwise use multiline text area\n            return streamlit_app.text_area(**streamlit_kwargs)\n\n    def _render_multi_enum_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n        streamlit_kwargs = self._get_default_streamlit_input_kwargs(key, property)\n        reference_item = schema_utils.resolve_reference(\n            property[\"items\"][\"$ref\"], self._schema_references\n        )\n        # TODO: how to select defaults\n        return streamlit_app.multiselect(\n            **streamlit_kwargs, options=reference_item[\"enum\"]\n        )\n\n    def _render_single_enum_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n\n        streamlit_kwargs = self._get_default_streamlit_input_kwargs(key, property)\n        reference_item = schema_utils.get_single_reference_item(\n            property, self._schema_references\n        )\n\n        if property.get(\"default\") is not None:\n            try:\n                streamlit_kwargs[\"index\"] = reference_item[\"enum\"].index(\n                    property.get(\"default\")\n                )\n            except Exception:\n                # Use default selection\n                pass\n\n        return streamlit_app.selectbox(\n            **streamlit_kwargs, options=reference_item[\"enum\"]\n        )\n\n    def _render_single_dict_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n\n        # Add title and subheader\n        streamlit_app.subheader(property.get(\"title\"))\n        if property.get(\"description\"):\n            streamlit_app.markdown(property.get(\"description\"))\n\n        streamlit_app.markdown(\"---\")\n\n        current_dict = self._get_value(key)\n        if not current_dict:\n            current_dict = {}\n\n        key_col, value_col = streamlit_app.columns(2)\n\n        with key_col:\n            updated_key = streamlit_app.text_input(\n                \"Key\", value=\"\", key=key + \"-new-key\"\n            )\n\n        with value_col:\n            # TODO: also add boolean?\n            value_kwargs = {\"label\": \"Value\", \"key\": key + \"-new-value\"}\n            if property[\"additionalProperties\"].get(\"type\") == \"integer\":\n                value_kwargs[\"value\"] = 0  # type: ignore\n                updated_value = streamlit_app.number_input(**value_kwargs)\n            elif property[\"additionalProperties\"].get(\"type\") == \"number\":\n                value_kwargs[\"value\"] = 0.0  # type: ignore\n                value_kwargs[\"format\"] = \"%f\"\n                updated_value = streamlit_app.number_input(**value_kwargs)\n            else:\n                value_kwargs[\"value\"] = \"\"\n                updated_value = streamlit_app.text_input(**value_kwargs)\n\n        streamlit_app.markdown(\"---\")\n\n        with streamlit_app.container():\n            clear_col, add_col = streamlit_app.columns([1, 2])\n\n            with clear_col:\n                if streamlit_app.button(\"Clear Items\", key=key + \"-clear-items\"):\n                    current_dict = {}\n\n            with add_col:\n                if (\n                    streamlit_app.button(\"Add Item\", key=key + \"-add-item\")\n                    and updated_key\n                ):\n                    current_dict[updated_key] = updated_value\n\n        streamlit_app.write(current_dict)\n\n        return current_dict\n\n    def _render_single_reference(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n        reference_item = schema_utils.get_single_reference_item(\n            property, self._schema_references\n        )\n        return self._render_property(streamlit_app, key, reference_item)\n\n    def _render_multi_file_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n        streamlit_kwargs = self._get_default_streamlit_input_kwargs(key, property)\n\n        file_extension = None\n        if \"mime_type\" in property:\n            file_extension = mimetypes.guess_extension(property[\"mime_type\"])\n\n        uploaded_files = streamlit_app.file_uploader(\n            **streamlit_kwargs, accept_multiple_files=True, type=file_extension\n        )\n        uploaded_files_bytes = []\n        if uploaded_files:\n            for uploaded_file in uploaded_files:\n                uploaded_files_bytes.append(uploaded_file.read())\n        return uploaded_files_bytes\n\n    def _render_single_boolean_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n        streamlit_kwargs = self._get_default_streamlit_input_kwargs(key, property)\n\n        if property.get(\"default\"):\n            streamlit_kwargs[\"value\"] = property.get(\"default\")\n        return streamlit_app.checkbox(**streamlit_kwargs)\n\n    def _render_single_number_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n        streamlit_kwargs = self._get_default_streamlit_input_kwargs(key, property)\n\n        number_transform = int\n        if property.get(\"type\") == \"number\":\n            number_transform = float  # type: ignore\n            streamlit_kwargs[\"format\"] = \"%f\"\n\n        if \"multipleOf\" in property:\n            # Set stepcount based on multiple of parameter\n            streamlit_kwargs[\"step\"] = number_transform(property[\"multipleOf\"])\n        elif number_transform == int:\n            # Set step size to 1 as default\n            streamlit_kwargs[\"step\"] = 1\n        elif number_transform == float:\n            # Set step size to 0.01 as default\n            # TODO: adapt to default value\n            streamlit_kwargs[\"step\"] = 0.01\n\n        if \"minimum\" in property:\n            streamlit_kwargs[\"min_value\"] = number_transform(property[\"minimum\"])\n        if \"exclusiveMinimum\" in property:\n            streamlit_kwargs[\"min_value\"] = number_transform(\n                property[\"exclusiveMinimum\"] + streamlit_kwargs[\"step\"]\n            )\n        if \"maximum\" in property:\n            streamlit_kwargs[\"max_value\"] = number_transform(property[\"maximum\"])\n\n        if \"exclusiveMaximum\" in property:\n            streamlit_kwargs[\"max_value\"] = number_transform(\n                property[\"exclusiveMaximum\"] - streamlit_kwargs[\"step\"]\n            )\n\n        if property.get(\"default\") is not None:\n            streamlit_kwargs[\"value\"] = number_transform(property.get(\"default\"))  # type: ignore\n        else:\n            if \"min_value\" in streamlit_kwargs:\n                streamlit_kwargs[\"value\"] = streamlit_kwargs[\"min_value\"]\n            elif number_transform == int:\n                streamlit_kwargs[\"value\"] = 0\n            else:\n                # Set default value to step\n                streamlit_kwargs[\"value\"] = number_transform(streamlit_kwargs[\"step\"])\n\n        if \"min_value\" in streamlit_kwargs and \"max_value\" in streamlit_kwargs:\n            # TODO: Only if less than X steps\n            return streamlit_app.slider(**streamlit_kwargs)\n        else:\n            return streamlit_app.number_input(**streamlit_kwargs)\n\n    def _render_object_input(self, streamlit_app: st, key: str, property: Dict) -> Any:\n        properties = property[\"properties\"]\n        object_inputs = {}\n        for property_key in properties:\n            property = properties[property_key]\n            if not property.get(\"title\"):\n                # Set property key as fallback title\n                property[\"title\"] = name_to_title(property_key)\n            # construct full key based on key parts -> required later to get the value\n            full_key = key + \".\" + property_key\n            object_inputs[property_key] = self._render_property(\n                streamlit_app, full_key, property\n            )\n        return object_inputs\n\n    def _render_single_object_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n        # Add title and subheader\n        title = property.get(\"title\")\n        streamlit_app.subheader(title)\n        if property.get(\"description\"):\n            streamlit_app.markdown(property.get(\"description\"))\n\n        object_reference = schema_utils.get_single_reference_item(\n            property, self._schema_references\n        )\n        return self._render_object_input(streamlit_app, key, object_reference)\n\n    def _render_property_list_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n\n        # Add title and subheader\n        streamlit_app.subheader(property.get(\"title\"))\n        if property.get(\"description\"):\n            streamlit_app.markdown(property.get(\"description\"))\n\n        streamlit_app.markdown(\"---\")\n\n        current_list = self._get_value(key)\n        if not current_list:\n            current_list = []\n\n        value_kwargs = {\"label\": \"Value\", \"key\": key + \"-new-value\"}\n        if property[\"items\"][\"type\"] == \"integer\":\n            value_kwargs[\"value\"] = 0  # type: ignore\n            new_value = streamlit_app.number_input(**value_kwargs)\n        elif property[\"items\"][\"type\"] == \"number\":\n            value_kwargs[\"value\"] = 0.0  # type: ignore\n            value_kwargs[\"format\"] = \"%f\"\n            new_value = streamlit_app.number_input(**value_kwargs)\n        else:\n            value_kwargs[\"value\"] = \"\"\n            new_value = streamlit_app.text_input(**value_kwargs)\n\n        streamlit_app.markdown(\"---\")\n\n        with streamlit_app.container():\n            clear_col, add_col = streamlit_app.columns([1, 2])\n\n            with clear_col:\n                if streamlit_app.button(\"Clear Items\", key=key + \"-clear-items\"):\n                    current_list = []\n\n            with add_col:\n                if (\n                    streamlit_app.button(\"Add Item\", key=key + \"-add-item\")\n                    and new_value is not None\n                ):\n                    current_list.append(new_value)\n\n        streamlit_app.write(current_list)\n\n        return current_list\n\n    def _render_object_list_input(\n        self, streamlit_app: st, key: str, property: Dict\n    ) -> Any:\n\n        # TODO: support max_items, and min_items properties\n\n        # Add title and subheader\n        streamlit_app.subheader(property.get(\"title\"))\n        if property.get(\"description\"):\n            streamlit_app.markdown(property.get(\"description\"))\n\n        streamlit_app.markdown(\"---\")\n\n        current_list = self._get_value(key)\n        if not current_list:\n            current_list = []\n\n        object_reference = schema_utils.resolve_reference(\n            property[\"items\"][\"$ref\"], self._schema_references\n        )\n        input_data = self._render_object_input(streamlit_app, key, object_reference)\n\n        streamlit_app.markdown(\"---\")\n\n        with streamlit_app.container():\n            clear_col, add_col = streamlit_app.columns([1, 2])\n\n            with clear_col:\n                if streamlit_app.button(\"Clear Items\", key=key + \"-clear-items\"):\n                    current_list = []\n\n            with add_col:\n                if (\n                    streamlit_app.button(\"Add Item\", key=key + \"-add-item\")\n                    and input_data\n                ):\n                    current_list.append(input_data)\n\n        streamlit_app.write(current_list)\n        return current_list\n\n    def _render_property(self, streamlit_app: st, key: str, property: Dict) -> Any:\n        if schema_utils.is_single_enum_property(property, self._schema_references):\n            return self._render_single_enum_input(streamlit_app, key, property)\n\n        if schema_utils.is_multi_enum_property(property, self._schema_references):\n            return self._render_multi_enum_input(streamlit_app, key, property)\n\n        if schema_utils.is_single_file_property(property):\n            return self._render_single_file_input(streamlit_app, key, property)\n\n        if schema_utils.is_multi_file_property(property):\n            return self._render_multi_file_input(streamlit_app, key, property)\n\n        if schema_utils.is_single_datetime_property(property):\n            return self._render_single_datetime_input(streamlit_app, key, property)\n\n        if schema_utils.is_single_boolean_property(property):\n            return self._render_single_boolean_input(streamlit_app, key, property)\n\n        if schema_utils.is_single_dict_property(property):\n            return self._render_single_dict_input(streamlit_app, key, property)\n\n        if schema_utils.is_single_number_property(property):\n            return self._render_single_number_input(streamlit_app, key, property)\n\n        if schema_utils.is_single_string_property(property):\n            return self._render_single_string_input(streamlit_app, key, property)\n\n        if schema_utils.is_single_object(property, self._schema_references):\n            return self._render_single_object_input(streamlit_app, key, property)\n\n        if schema_utils.is_object_list_property(property, self._schema_references):\n            return self._render_object_list_input(streamlit_app, key, property)\n\n        if schema_utils.is_property_list(property):\n            return self._render_property_list_input(streamlit_app, key, property)\n\n        if schema_utils.is_single_reference(property):\n            return self._render_single_reference(streamlit_app, key, property)\n\n        streamlit_app.warning(\n            \"The type of the following property is currently not supported: \"\n            + str(property.get(\"title\"))\n        )\n        raise Exception(\"Unsupported property\")\n\n\nclass OutputUI:\n    def __init__(self, output_data: Any, input_data: Any):\n        self._output_data = output_data\n        self._input_data = input_data\n\n    def render_ui(self, streamlit_app) -> None:\n        try:\n            if isinstance(self._output_data, BaseModel):\n                self._render_single_output(streamlit_app, self._output_data)\n                return\n            if type(self._output_data) == list:\n                self._render_list_output(streamlit_app, self._output_data)\n                return\n        except Exception as ex:\n            streamlit_app.exception(ex)\n            # Fallback to\n            streamlit_app.json(jsonable_encoder(self._output_data))\n\n    def _render_single_text_property(\n        self, streamlit: st, property_schema: Dict, value: Any\n    ) -> None:\n        # Add title and subheader\n        streamlit.subheader(property_schema.get(\"title\"))\n        if property_schema.get(\"description\"):\n            streamlit.markdown(property_schema.get(\"description\"))\n        if value is None or value == \"\":\n            streamlit.info(\"No value returned!\")\n        else:\n            streamlit.code(str(value), language=\"plain\")\n\n    def _render_single_file_property(\n        self, streamlit: st, property_schema: Dict, value: Any\n    ) -> None:\n        # Add title and subheader\n        streamlit.subheader(property_schema.get(\"title\"))\n        if property_schema.get(\"description\"):\n            streamlit.markdown(property_schema.get(\"description\"))\n        if value is None or value == \"\":\n            streamlit.info(\"No value returned!\")\n        else:\n            # TODO: Detect if it is a FileContent instance\n            # TODO: detect if it is base64\n            file_extension = \"\"\n            if \"mime_type\" in property_schema:\n                mime_type = property_schema[\"mime_type\"]\n                file_extension = mimetypes.guess_extension(mime_type) or \"\"\n\n                if is_compatible_audio(mime_type):\n                    streamlit.audio(value.as_bytes(), format=mime_type)\n                    return\n\n                if is_compatible_image(mime_type):\n                    streamlit.image(value.as_bytes())\n                    return\n\n                if is_compatible_video(mime_type):\n                    streamlit.video(value.as_bytes(), format=mime_type)\n                    return\n\n            filename = (\n                (property_schema[\"title\"] + file_extension)\n                .lower()\n                .strip()\n                .replace(\" \", \"-\")\n            )\n            streamlit.markdown(\n                f'<a href=\"data:application/octet-stream;base64,{value}\" download=\"{filename}\"><input type=\"button\" value=\"Download File\"></a>',\n                unsafe_allow_html=True,\n            )\n\n    def _render_single_complex_property(\n        self, streamlit: st, property_schema: Dict, value: Any\n    ) -> None:\n        # Add title and subheader\n        streamlit.subheader(property_schema.get(\"title\"))\n        if property_schema.get(\"description\"):\n            streamlit.markdown(property_schema.get(\"description\"))\n\n        streamlit.json(jsonable_encoder(value))\n\n    def _render_single_output(self, streamlit: st, output_data: BaseModel) -> None:\n        try:\n            if has_output_ui_renderer(output_data):\n                if function_has_named_arg(output_data.render_output_ui, \"input\"):  # type: ignore\n                    # render method also requests the input data\n                    output_data.render_output_ui(streamlit, input=self._input_data)  # type: ignore\n                else:\n                    output_data.render_output_ui(streamlit)  # type: ignore\n                return\n        except Exception:\n            # Use default auto-generation methods if the custom rendering throws an exception\n            logger.exception(\n                \"Failed to execute custom render_output_ui function. Using auto-generation instead\"\n            )\n\n        model_schema = output_data.schema(by_alias=False)\n        model_properties = model_schema.get(\"properties\")\n        definitions = model_schema.get(\"definitions\")\n\n        if model_properties:\n            for property_key in output_data.__dict__:\n                property_schema = model_properties.get(property_key)\n                if not property_schema.get(\"title\"):\n                    # Set property key as fallback title\n                    property_schema[\"title\"] = property_key\n\n                output_property_value = output_data.__dict__[property_key]\n\n                if has_output_ui_renderer(output_property_value):\n                    output_property_value.render_output_ui(streamlit)  # type: ignore\n                    continue\n\n                if isinstance(output_property_value, BaseModel):\n                    # Render output recursivly\n                    streamlit.subheader(property_schema.get(\"title\"))\n                    if property_schema.get(\"description\"):\n                        streamlit.markdown(property_schema.get(\"description\"))\n                    self._render_single_output(streamlit, output_property_value)\n                    continue\n\n                if property_schema:\n                    if schema_utils.is_single_file_property(property_schema):\n                        self._render_single_file_property(\n                            streamlit, property_schema, output_property_value\n                        )\n                        continue\n\n                    if (\n                        schema_utils.is_single_string_property(property_schema)\n                        or schema_utils.is_single_number_property(property_schema)\n                        or schema_utils.is_single_datetime_property(property_schema)\n                        or schema_utils.is_single_boolean_property(property_schema)\n                    ):\n                        self._render_single_text_property(\n                            streamlit, property_schema, output_property_value\n                        )\n                        continue\n                    if definitions and schema_utils.is_single_enum_property(\n                        property_schema, definitions\n                    ):\n                        self._render_single_text_property(\n                            streamlit, property_schema, output_property_value.value\n                        )\n                        continue\n\n                    # TODO: render dict as table\n\n                    self._render_single_complex_property(\n                        streamlit, property_schema, output_property_value\n                    )\n            return\n\n    def _render_list_output(self, streamlit: st, output_data: List) -> None:\n        try:\n            data_items: List = []\n            for data_item in output_data:\n                if has_output_ui_renderer(data_item):\n                    # Render using the render function\n                    data_item.render_output_ui(streamlit)  # type: ignore\n                    continue\n                data_items.append(data_item.dict())\n            # Try to show as dataframe\n            streamlit.table(pd.DataFrame(data_items))\n        except Exception:\n            # Fallback to\n            streamlit.json(jsonable_encoder(output_data))\n\n\ndef getOpyrator(mode: str) -> Opyrator:\n    if mode == None or mode.startswith('VC'):\n        from control.mkgui.app_vc import convert\n        return  Opyrator(convert)\n    if mode == None or mode.startswith('\u9884\u5904\u7406'):\n        from control.mkgui.preprocess import preprocess\n        return  Opyrator(preprocess)\n    if mode == None or mode.startswith('\u6a21\u578b\u8bad\u7ec3'):\n        from control.mkgui.train import train\n        return  Opyrator(train)\n    if mode == None or mode.startswith('\u6a21\u578b\u8bad\u7ec3(VC)'):\n        from control.mkgui.train_vc import train_vc\n        return  Opyrator(train_vc)\n    from control.mkgui.app import synthesize\n    return Opyrator(synthesize)\n    \ndef render_streamlit_ui() -> None:\n    # init\n    session_state = st.session_state\n    session_state.input_data = {}\n    # Add custom css settings\n    st.markdown(f\"<style>{CUSTOM_STREAMLIT_CSS}</style>\", unsafe_allow_html=True)\n\n    with st.spinner(\"Loading MockingBird GUI. Please wait...\"):\n        session_state.mode = st.sidebar.selectbox(\n            '\u6a21\u5f0f\u9009\u62e9', \n            ( \"AI\u62df\u97f3\", \"VC\u62df\u97f3\", \"\u9884\u5904\u7406\", \"\u6a21\u578b\u8bad\u7ec3\", \"\u6a21\u578b\u8bad\u7ec3(VC)\")\n        )\n        if \"mode\" in session_state:\n            mode = session_state.mode\n        else:\n            mode = \"\"\n        opyrator = getOpyrator(mode)\n    title = opyrator.name + mode\n\n    col1, col2, _ = st.columns(3)\n    col2.title(title)\n    col2.markdown(\"\u6b22\u8fce\u4f7f\u7528MockingBird Web 2\")\n\n    image = Image.open(path.join('control','mkgui', 'static', 'mb.png'))\n    col1.image(image)\n\n    st.markdown(\"---\")\n    left, right = st.columns([0.4, 0.6])\n\n    with left:\n        st.header(\"Control \u63a7\u5236\")\n        # if session_state.mode in [\"AI\u62df\u97f3\", \"VC\u62df\u97f3\"] :\n            # from audiorecorder import audiorecorder\n            # audio = audiorecorder(\"Click to record\", \"Recording...\")\n            # if len(audio) > 0:\n            #     # To play audio in frontend:\n            #     st.audio(audio.tobytes())\n            \n        InputUI(session_state=session_state, input_class=opyrator.input_type).render_ui(st)\n        execute_selected = st.button(opyrator.action)\n        if execute_selected:\n            with st.spinner(\"Executing operation. Please wait...\"):\n                try:\n                    input_data_obj = parse_obj_as(\n                        opyrator.input_type, session_state.input_data\n                    )\n                    session_state.output_data = opyrator(input=input_data_obj)\n                    session_state.latest_operation_input = input_data_obj  # should this really be saved as additional session object?\n                except ValidationError as ex:\n                    st.error(ex)\n                else:\n                    # st.success(\"Operation executed successfully.\")\n                    pass\n\n    with right:\n        st.header(\"Result \u7ed3\u679c\")\n        if 'output_data' in session_state:\n            OutputUI(\n                session_state.output_data, session_state.latest_operation_input\n            ).render_ui(st)\n            if st.button(\"Clear\"):\n            # Clear all state\n                for key in st.session_state.keys():\n                    del st.session_state[key]   \n                session_state.input_data = {}\n                st.experimental_rerun()\n        else:\n            # placeholder\n            st.caption(\"\u8bf7\u4f7f\u7528\u5de6\u4fa7\u63a7\u5236\u677f\u8fdb\u884c\u8f93\u5165\u5e76\u8fd0\u884c\u83b7\u5f97\u7ed3\u679c\")\n        \n   \n", "control/mkgui/base/ui/streamlit_utils.py": "CUSTOM_STREAMLIT_CSS = \"\"\"\ndiv[data-testid=\"stBlock\"] button {\n  width: 100% !important;\n  margin-bottom: 20px !important;\n  border-color: #bfbfbf !important;\n}\nsection[data-testid=\"stSidebar\"] div {\n  max-width: 10rem;\n}\npre code {\n    white-space: pre-wrap;\n}\n\"\"\"\n", "control/mkgui/base/ui/__init__.py": "from .streamlit_ui import render_streamlit_ui\n", "control/mkgui/base/api/fastapi_utils.py": "\"\"\"Collection of utilities for FastAPI apps.\"\"\"\n\nimport inspect\nfrom typing import Any, Type\n\nfrom fastapi import FastAPI, Form\nfrom pydantic import BaseModel\n\n\ndef as_form(cls: Type[BaseModel]) -> Any:\n    \"\"\"Adds an as_form class method to decorated models.\n\n    The as_form class method can be used with FastAPI endpoints\n    \"\"\"\n    new_params = [\n        inspect.Parameter(\n            field.alias,\n            inspect.Parameter.POSITIONAL_ONLY,\n            default=(Form(field.default) if not field.required else Form(...)),\n        )\n        for field in cls.__fields__.values()\n    ]\n\n    async def _as_form(**data):  # type: ignore\n        return cls(**data)\n\n    sig = inspect.signature(_as_form)\n    sig = sig.replace(parameters=new_params)\n    _as_form.__signature__ = sig  # type: ignore\n    setattr(cls, \"as_form\", _as_form)\n    return cls\n\n\ndef patch_fastapi(app: FastAPI) -> None:\n    \"\"\"Patch function to allow relative url resolution.\n\n    This patch is required to make fastapi fully functional with a relative url path.\n    This code snippet can be copy-pasted to any Fastapi application.\n    \"\"\"\n    from fastapi.openapi.docs import get_redoc_html, get_swagger_ui_html\n    from starlette.requests import Request\n    from starlette.responses import HTMLResponse\n\n    async def redoc_ui_html(req: Request) -> HTMLResponse:\n        assert app.openapi_url is not None\n        redoc_ui = get_redoc_html(\n            openapi_url=\"./\" + app.openapi_url.lstrip(\"/\"),\n            title=app.title + \" - Redoc UI\",\n        )\n\n        return HTMLResponse(redoc_ui.body.decode(\"utf-8\"))\n\n    async def swagger_ui_html(req: Request) -> HTMLResponse:\n        assert app.openapi_url is not None\n        swagger_ui = get_swagger_ui_html(\n            openapi_url=\"./\" + app.openapi_url.lstrip(\"/\"),\n            title=app.title + \" - Swagger UI\",\n            oauth2_redirect_url=app.swagger_ui_oauth2_redirect_url,\n        )\n\n        # insert request interceptor to have all request run on relativ path\n        request_interceptor = (\n            \"requestInterceptor: (e)  => {\"\n            \"\\n\\t\\t\\tvar url = window.location.origin + window.location.pathname\"\n            '\\n\\t\\t\\turl = url.substring( 0, url.lastIndexOf( \"/\" ) + 1);'\n            \"\\n\\t\\t\\turl = e.url.replace(/http(s)?:\\/\\/[^/]*\\//i, url);\"  # noqa: W605\n            \"\\n\\t\\t\\te.contextUrl = url\"\n            \"\\n\\t\\t\\te.url = url\"\n            \"\\n\\t\\t\\treturn e;}\"\n        )\n\n        return HTMLResponse(\n            swagger_ui.body.decode(\"utf-8\").replace(\n                \"dom_id: '#swagger-ui',\",\n                \"dom_id: '#swagger-ui',\\n\\t\\t\" + request_interceptor + \",\",\n            )\n        )\n\n    # remove old docs route and add our patched route\n    routes_new = []\n    for app_route in app.routes:\n        if app_route.path == \"/docs\":  # type: ignore\n            continue\n\n        if app_route.path == \"/redoc\":  # type: ignore\n            continue\n\n        routes_new.append(app_route)\n\n    app.router.routes = routes_new\n\n    assert app.docs_url is not None\n    app.add_route(app.docs_url, swagger_ui_html, include_in_schema=False)\n    assert app.redoc_url is not None\n    app.add_route(app.redoc_url, redoc_ui_html, include_in_schema=False)\n\n    # Make graphql realtive\n    from starlette import graphql\n\n    graphql.GRAPHIQL = graphql.GRAPHIQL.replace(\n        \"({{REQUEST_PATH}}\", '(\".\" + {{REQUEST_PATH}}'\n    )\n", "control/mkgui/base/api/__init__.py": "from .fastapi_app import create_api\n", "control/cli/vocoder_train.py": "from utils.argutils import print_args\nfrom models.vocoder.wavernn.train import train\nfrom models.vocoder.hifigan.train import train as train_hifigan\nfrom models.vocoder.fregan.train import train as train_fregan\nfrom utils.util import AttrDict\nfrom pathlib import Path\nimport argparse\nimport json\nimport torch\nimport torch.multiprocessing as mp\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Trains the vocoder from the synthesizer audios and the GTA synthesized mels, \"\n                    \"or ground truth mels.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    \n    parser.add_argument(\"run_id\", type=str, help= \\\n        \"Name for this model instance. If a model state from the same run ID was previously \"\n        \"saved, the training will restart from there. Pass -f to overwrite saved states and \"\n        \"restart from scratch.\")\n    parser.add_argument(\"datasets_root\", type=str, help= \\\n        \"Path to the directory containing your SV2TTS directory. Specifying --syn_dir or --voc_dir \"\n        \"will take priority over this argument.\")\n    parser.add_argument(\"vocoder_type\", type=str, default=\"wavernn\", help= \\\n        \"Choose the vocoder type for train. Defaults to wavernn\"\n        \"Now, Support <hifigan> and <wavernn> for choose\")\n    parser.add_argument(\"--syn_dir\", type=str, default=argparse.SUPPRESS, help= \\\n        \"Path to the synthesizer directory that contains the ground truth mel spectrograms, \"\n        \"the wavs and the embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/.\")\n    parser.add_argument(\"--voc_dir\", type=str, default=argparse.SUPPRESS, help= \\\n        \"Path to the vocoder directory that contains the GTA synthesized mel spectrograms. \"\n        \"Defaults to <datasets_root>/SV2TTS/vocoder/. Unused if --ground_truth is passed.\")\n    parser.add_argument(\"-m\", \"--models_dir\", type=str, default=\"vocoder/saved_models/\", help=\\\n        \"Path to the directory that will contain the saved model weights, as well as backups \"\n        \"of those weights and wavs generated during training.\")\n    parser.add_argument(\"-g\", \"--ground_truth\", action=\"store_true\", help= \\\n        \"Train on ground truth spectrograms (<datasets_root>/SV2TTS/synthesizer/mels).\")\n    parser.add_argument(\"-s\", \"--save_every\", type=int, default=1000, help= \\\n        \"Number of steps between updates of the model on the disk. Set to 0 to never save the \"\n        \"model.\")\n    parser.add_argument(\"-b\", \"--backup_every\", type=int, default=25000, help= \\\n        \"Number of steps between backups of the model. Set to 0 to never make backups of the \"\n        \"model.\")\n    parser.add_argument(\"-f\", \"--force_restart\", action=\"store_true\", help= \\\n        \"Do not load any saved model and restart from scratch.\")\n    parser.add_argument(\"--config\", type=str, default=\"vocoder/hifigan/config_16k_.json\")\n    args = parser.parse_args()\n\n    if not hasattr(args, \"syn_dir\"):\n        args.syn_dir = Path(args.datasets_root, \"SV2TTS\", \"synthesizer\")\n    args.syn_dir = Path(args.syn_dir)\n    if not hasattr(args, \"voc_dir\"):\n        args.voc_dir = Path(args.datasets_root, \"SV2TTS\", \"vocoder\")\n    args.voc_dir = Path(args.voc_dir)\n    del args.datasets_root\n    args.models_dir = Path(args.models_dir)\n    args.models_dir.mkdir(exist_ok=True)\n\n    print_args(args, parser)\n\n    # Process the arguments\n    if args.vocoder_type == \"wavernn\":\n        # Run the training wavernn\n        delattr(args, 'vocoder_type')\n        delattr(args, 'config')\n        train(**vars(args))\n    elif args.vocoder_type == \"hifigan\":\n        with open(args.config) as f:\n            json_config = json.load(f)\n        h = AttrDict(json_config)\n        if h.num_gpus > 1:\n            h.num_gpus = torch.cuda.device_count()\n            h.batch_size = int(h.batch_size / h.num_gpus)\n            print('Batch size per GPU :', h.batch_size)\n            mp.spawn(train_hifigan, nprocs=h.num_gpus, args=(args, h,))\n        else:\n            train_hifigan(0, args, h)\n    elif args.vocoder_type == \"fregan\":\n        with Path('vocoder/fregan/config.json').open() as f:\n            json_config = json.load(f)\n        h = AttrDict(json_config)\n        if h.num_gpus > 1:\n            h.num_gpus = torch.cuda.device_count()\n            h.batch_size = int(h.batch_size / h.num_gpus)\n            print('Batch size per GPU :', h.batch_size)\n            mp.spawn(train_fregan, nprocs=h.num_gpus, args=(args, h,))\n        else:\n            train_fregan(0, args, h)\n\n        ", "control/cli/encoder_train.py": "from utils.argutils import print_args\nfrom models.encoder.train import train\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Trains the speaker encoder. You must have run encoder_preprocess.py first.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    \n    parser.add_argument(\"run_id\", type=str, help= \\\n        \"Name for this model instance. If a model state from the same run ID was previously \"\n        \"saved, the training will restart from there. Pass -f to overwrite saved states and \"\n        \"restart from scratch.\")\n    parser.add_argument(\"clean_data_root\", type=Path, help= \\\n        \"Path to the output directory of encoder_preprocess.py. If you left the default \"\n        \"output directory when preprocessing, it should be <datasets_root>/SV2TTS/encoder/.\")\n    parser.add_argument(\"-m\", \"--models_dir\", type=Path, default=\"encoder/saved_models/\", help=\\\n        \"Path to the output directory that will contain the saved model weights, as well as \"\n        \"backups of those weights and plots generated during training.\")\n    parser.add_argument(\"-v\", \"--vis_every\", type=int, default=10, help= \\\n        \"Number of steps between updates of the loss and the plots.\")\n    parser.add_argument(\"-u\", \"--umap_every\", type=int, default=100, help= \\\n        \"Number of steps between updates of the umap projection. Set to 0 to never update the \"\n        \"projections.\")\n    parser.add_argument(\"-s\", \"--save_every\", type=int, default=500, help= \\\n        \"Number of steps between updates of the model on the disk. Set to 0 to never save the \"\n        \"model.\")\n    parser.add_argument(\"-b\", \"--backup_every\", type=int, default=7500, help= \\\n        \"Number of steps between backups of the model. Set to 0 to never make backups of the \"\n        \"model.\")\n    parser.add_argument(\"-f\", \"--force_restart\", action=\"store_true\", help= \\\n        \"Do not load any saved model.\")\n    parser.add_argument(\"--visdom_server\", type=str, default=\"http://localhost\")\n    parser.add_argument(\"--no_visdom\", action=\"store_true\", help= \\\n        \"Disable visdom.\")\n    args = parser.parse_args()\n    \n    # Process the arguments\n    args.models_dir.mkdir(exist_ok=True)\n    \n    # Run the training\n    print_args(args, parser)\n    train(**vars(args))\n    ", "control/cli/vocoder_preprocess.py": "from models.synthesizer.synthesize import run_synthesis\nfrom models.synthesizer.hparams import hparams\nfrom utils.argutils import print_args\nimport argparse\nimport os\n\n\nif __name__ == \"__main__\":\n    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):\n        pass\n    \n    parser = argparse.ArgumentParser(\n        description=\"Creates ground-truth aligned (GTA) spectrograms from the vocoder.\",\n        formatter_class=MyFormatter\n    )\n    parser.add_argument(\"datasets_root\", type=str, help=\\\n        \"Path to the directory containing your SV2TTS directory. If you specify both --in_dir and \"\n        \"--out_dir, this argument won't be used.\")\n    parser.add_argument(\"-m\", \"--model_dir\", type=str, \n                        default=\"synthesizer/saved_models/mandarin/\", help=\\\n        \"Path to the pretrained model directory.\")\n    parser.add_argument(\"-i\", \"--in_dir\", type=str, default=argparse.SUPPRESS, help= \\\n        \"Path to the synthesizer directory that contains the mel spectrograms, the wavs and the \"\n        \"embeds. Defaults to  <datasets_root>/SV2TTS/synthesizer/.\")\n    parser.add_argument(\"-o\", \"--out_dir\", type=str, default=argparse.SUPPRESS, help= \\\n        \"Path to the output vocoder directory that will contain the ground truth aligned mel \"\n        \"spectrograms. Defaults to <datasets_root>/SV2TTS/vocoder/.\")\n    parser.add_argument(\"--hparams\", default=\"\",\n                        help=\"Hyperparameter overrides as a comma-separated list of name=value \"\n                             \"pairs\")\n    parser.add_argument(\"--no_trim\", action=\"store_true\", help=\\\n        \"Preprocess audio without trimming silences (not recommended).\")\n    parser.add_argument(\"--cpu\", action=\"store_true\", help=\\\n        \"If True, processing is done on CPU, even when a GPU is available.\")\n    args = parser.parse_args()\n    print_args(args, parser)\n    modified_hp = hparams.parse(args.hparams)\n    \n    if not hasattr(args, \"in_dir\"):\n        args.in_dir = os.path.join(args.datasets_root, \"SV2TTS\", \"synthesizer\")\n    if not hasattr(args, \"out_dir\"):\n        args.out_dir = os.path.join(args.datasets_root, \"SV2TTS\", \"vocoder\")\n\n    if args.cpu:\n        # Hide GPUs from Pytorch to force CPU processing\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n    \n    # Verify webrtcvad is available\n    if not args.no_trim:\n        try:\n            import webrtcvad\n        except:\n            raise ModuleNotFoundError(\"Package 'webrtcvad' not found. This package enables \"\n                \"noise removal and is recommended. Please install and try again. If installation fails, \"\n                \"use --no_trim to disable this error message.\")\n    del args.no_trim\n\n    run_synthesis(args.in_dir, args.out_dir, args.model_dir, modified_hp)\n\n", "control/cli/ppg2mel_train.py": "import sys\nimport torch\nimport argparse\nimport numpy as np\nfrom utils.hparams import HpsYaml\nfrom models.ppg2mel.train.train_linglf02mel_seq2seq_oneshotvc import Solver\n\n# For reproducibility, comment these may speed up training\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndef main():\n    # Arguments\n    parser = argparse.ArgumentParser(description=\n            'Training PPG2Mel VC model.')\n    parser.add_argument('--config', type=str, \n                        help='Path to experiment config, e.g., config/vc.yaml')\n    parser.add_argument('--name', default=None, type=str, help='Name for logging.')\n    parser.add_argument('--logdir', default='log/', type=str,\n                        help='Logging path.', required=False)\n    parser.add_argument('--ckpdir', default='ppg2mel/saved_models/', type=str,\n                        help='Checkpoint path.', required=False)\n    parser.add_argument('--outdir', default='result/', type=str,\n                        help='Decode output path.', required=False)\n    parser.add_argument('--load', default=None, type=str,\n                        help='Load pre-trained model (for training only)', required=False)\n    parser.add_argument('--warm_start', action='store_true',\n                        help='Load model weights only, ignore specified layers.')\n    parser.add_argument('--seed', default=0, type=int,\n                        help='Random seed for reproducable results.', required=False)\n    parser.add_argument('--njobs', default=8, type=int,\n                        help='Number of threads for dataloader/decoding.', required=False)\n    parser.add_argument('--cpu', action='store_true', help='Disable GPU training.')\n    parser.add_argument('--no-pin', action='store_true',\n                        help='Disable pin-memory for dataloader')\n    parser.add_argument('--test', action='store_true', help='Test the model.')\n    parser.add_argument('--no-msg', action='store_true', help='Hide all messages.')\n    parser.add_argument('--finetune', action='store_true', help='Finetune model')\n    parser.add_argument('--oneshotvc', action='store_true', help='Oneshot VC model')\n    parser.add_argument('--bilstm', action='store_true', help='BiLSTM VC model')\n    parser.add_argument('--lsa', action='store_true', help='Use location-sensitive attention (LSA)')\n\n    ###\n\n    paras = parser.parse_args()\n    setattr(paras, 'gpu', not paras.cpu)\n    setattr(paras, 'pin_memory', not paras.no_pin)\n    setattr(paras, 'verbose', not paras.no_msg)\n    # Make the config dict dot visitable\n    config = HpsYaml(paras.config)\n\n    np.random.seed(paras.seed)\n    torch.manual_seed(paras.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(paras.seed)\n\n    print(\">>> OneShot VC training ...\")\n    mode = \"train\"\n    solver = Solver(config, paras, mode)\n    solver.load_data()\n    solver.set_model()\n    solver.exec()\n    print(\">>> Oneshot VC train finished!\")\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()   \n", "control/cli/train_ppg2mel.py": "import sys\nimport torch\nimport argparse\nimport numpy as np\nfrom utils.hparams import HpsYaml\nfrom models.ppg2mel.train.train_linglf02mel_seq2seq_oneshotvc import Solver\n\n# For reproducibility, comment these may speed up training\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndef main():\n    # Arguments\n    parser = argparse.ArgumentParser(description=\n            'Training PPG2Mel VC model.')\n    parser.add_argument('--config', type=str, \n                        help='Path to experiment config, e.g., config/vc.yaml')\n    parser.add_argument('--name', default=None, type=str, help='Name for logging.')\n    parser.add_argument('--logdir', default='log/', type=str,\n                        help='Logging path.', required=False)\n    parser.add_argument('--ckpdir', default='ppg2mel/saved_models/', type=str,\n                        help='Checkpoint path.', required=False)\n    parser.add_argument('--outdir', default='result/', type=str,\n                        help='Decode output path.', required=False)\n    parser.add_argument('--load', default=None, type=str,\n                        help='Load pre-trained model (for training only)', required=False)\n    parser.add_argument('--warm_start', action='store_true',\n                        help='Load model weights only, ignore specified layers.')\n    parser.add_argument('--seed', default=0, type=int,\n                        help='Random seed for reproducable results.', required=False)\n    parser.add_argument('--njobs', default=8, type=int,\n                        help='Number of threads for dataloader/decoding.', required=False)\n    parser.add_argument('--cpu', action='store_true', help='Disable GPU training.')\n    parser.add_argument('--no-pin', action='store_true',\n                        help='Disable pin-memory for dataloader')\n    parser.add_argument('--test', action='store_true', help='Test the model.')\n    parser.add_argument('--no-msg', action='store_true', help='Hide all messages.')\n    parser.add_argument('--finetune', action='store_true', help='Finetune model')\n    parser.add_argument('--oneshotvc', action='store_true', help='Oneshot VC model')\n    parser.add_argument('--bilstm', action='store_true', help='BiLSTM VC model')\n    parser.add_argument('--lsa', action='store_true', help='Use location-sensitive attention (LSA)')\n\n    ###\n    paras = parser.parse_args()\n    setattr(paras, 'gpu', not paras.cpu)\n    setattr(paras, 'pin_memory', not paras.no_pin)\n    setattr(paras, 'verbose', not paras.no_msg)\n    # Make the config dict dot visitable\n    config = HpsYaml(paras.config)\n\n    np.random.seed(paras.seed)\n    torch.manual_seed(paras.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(paras.seed)\n\n    print(\">>> OneShot VC training ...\")\n    mode = \"train\"\n    solver = Solver(config, paras, mode)\n    solver.load_data()\n    solver.set_model()\n    solver.exec()\n    print(\">>> Oneshot VC train finished!\")\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()   \n", "control/cli/pre4ppg.py": "from pathlib import Path\nimport argparse\n\nfrom models.ppg2mel.preprocess import preprocess_dataset\nfrom pathlib import Path\nimport argparse\n\nrecognized_datasets = [\n    \"aidatatang_200zh\",\n    \"aidatatang_200zh_s\", #      sample \n]\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Preprocesses audio files from datasets, to be used by the \"\n                    \"ppg2mel model for training.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\"datasets_root\", type=Path, help=\\\n        \"Path to the directory containing your datasets.\")\n    parser.add_argument(\"-d\", \"--dataset\", type=str, default=\"aidatatang_200zh\", help=\\\n        \"Name of the dataset to process, allowing values: aidatatang_200zh.\")\n    parser.add_argument(\"-o\", \"--out_dir\", type=Path, default=argparse.SUPPRESS, help=\\\n        \"Path to the output directory that will contain the mel spectrograms, the audios and the \"\n        \"embeds. Defaults to <datasets_root>/PPGVC/ppg2mel/\")\n    parser.add_argument(\"-n\", \"--n_processes\", type=int, default=8, help=\\\n        \"Number of processes in parallel.\")\n    # parser.add_argument(\"-s\", \"--skip_existing\", action=\"store_true\", help=\\\n    #     \"Whether to overwrite existing files with the same name. Useful if the preprocessing was \"\n    #     \"interrupted. \")\n    # parser.add_argument(\"--hparams\", type=str, default=\"\", help=\\\n    #     \"Hyperparameter overrides as a comma-separated list of name-value pairs\")\n    # parser.add_argument(\"--no_trim\", action=\"store_true\", help=\\\n    #     \"Preprocess audio without trimming silences (not recommended).\")\n    parser.add_argument(\"-pf\", \"--ppg_encoder_model_fpath\", type=Path, default=\"ppg_extractor/saved_models/24epoch.pt\", help=\\\n        \"Path your trained ppg encoder model.\")\n    parser.add_argument(\"-sf\", \"--speaker_encoder_model\", type=Path, default=\"encoder/saved_models/pretrained_bak_5805000.pt\", help=\\\n        \"Path your trained speaker encoder model.\")\n    args = parser.parse_args()\n\n    assert args.dataset in recognized_datasets, 'is not supported, file a issue to propose a new one'\n\n    # Create directories\n    assert args.datasets_root.exists()\n    if not hasattr(args, \"out_dir\"):\n        args.out_dir = args.datasets_root.joinpath(\"PPGVC\", \"ppg2mel\")\n    args.out_dir.mkdir(exist_ok=True, parents=True)\n\n    preprocess_dataset(**vars(args)) \n", "control/cli/encoder_preprocess.py": "import argparse\nfrom pathlib import Path\n\nfrom models.encoder.preprocess import (preprocess_aidatatang_200zh,\n                                preprocess_librispeech, preprocess_voxceleb1,\n                                preprocess_voxceleb2)\nfrom utils.argutils import print_args\n\nif __name__ == \"__main__\":\n    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):\n        pass\n    \n    parser = argparse.ArgumentParser(\n        description=\"Preprocesses audio files from datasets, encodes them as mel spectrograms and \"\n                    \"writes them to the disk. This will allow you to train the encoder. The \"\n                    \"datasets required are at least one of LibriSpeech, VoxCeleb1, VoxCeleb2, aidatatang_200zh. \",\n        formatter_class=MyFormatter\n    )\n    parser.add_argument(\"datasets_root\", type=Path, help=\\\n        \"Path to the directory containing your LibriSpeech/TTS and VoxCeleb datasets.\")\n    parser.add_argument(\"-o\", \"--out_dir\", type=Path, default=argparse.SUPPRESS, help=\\\n        \"Path to the output directory that will contain the mel spectrograms. If left out, \"\n        \"defaults to <datasets_root>/SV2TTS/encoder/\")\n    parser.add_argument(\"-d\", \"--datasets\", type=str, \n                        default=\"librispeech_other,voxceleb1,aidatatang_200zh\", help=\\\n        \"Comma-separated list of the name of the datasets you want to preprocess. Only the train \"\n        \"set of these datasets will be used. Possible names: librispeech_other, voxceleb1, \"\n        \"voxceleb2.\")\n    parser.add_argument(\"-s\", \"--skip_existing\", action=\"store_true\", help=\\\n        \"Whether to skip existing output files with the same name. Useful if this script was \"\n        \"interrupted.\")\n    parser.add_argument(\"--no_trim\", action=\"store_true\", help=\\\n        \"Preprocess audio without trimming silences (not recommended).\")\n    args = parser.parse_args()\n\n    # Verify webrtcvad is available\n    if not args.no_trim:\n        try:\n            import webrtcvad\n        except:\n            raise ModuleNotFoundError(\"Package 'webrtcvad' not found. This package enables \"\n                \"noise removal and is recommended. Please install and try again. If installation fails, \"\n                \"use --no_trim to disable this error message.\")\n    del args.no_trim\n\n    # Process the arguments\n    args.datasets = args.datasets.split(\",\")\n    if not hasattr(args, \"out_dir\"):\n        args.out_dir = args.datasets_root.joinpath(\"SV2TTS\", \"encoder\")\n    assert args.datasets_root.exists()\n    args.out_dir.mkdir(exist_ok=True, parents=True)\n\n    # Preprocess the datasets\n    print_args(args, parser)\n    preprocess_func = {\n        \"librispeech_other\": preprocess_librispeech,\n        \"voxceleb1\": preprocess_voxceleb1,\n        \"voxceleb2\": preprocess_voxceleb2,\n        \"aidatatang_200zh\": preprocess_aidatatang_200zh,\n    }\n    args = vars(args)\n    for dataset in args.pop(\"datasets\"):\n        print(\"Preprocessing %s\" % dataset)\n        preprocess_func[dataset](**args)\n", "control/cli/__init__.py": "", "control/cli/synthesizer_train.py": "from models.synthesizer.hparams import hparams\nfrom models.synthesizer.train import train\nfrom utils.argutils import print_args\nimport argparse\n\ndef new_train():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"run_id\", type=str, help= \\\n        \"Name for this model instance. If a model state from the same run ID was previously \"\n        \"saved, the training will restart from there. Pass -f to overwrite saved states and \"\n        \"restart from scratch.\")\n    parser.add_argument(\"syn_dir\", type=str, default=argparse.SUPPRESS, help= \\\n        \"Path to the synthesizer directory that contains the ground truth mel spectrograms, \"\n        \"the wavs and the embeds.\")\n    parser.add_argument(\"-m\", \"--models_dir\", type=str, default=f\"data/ckpt/synthesizer/\", help=\\\n        \"Path to the output directory that will contain the saved model weights and the logs.\")\n    parser.add_argument(\"-s\", \"--save_every\", type=int, default=1000, help= \\\n        \"Number of steps between updates of the model on the disk. Set to 0 to never save the \"\n        \"model.\")\n    parser.add_argument(\"-b\", \"--backup_every\", type=int, default=25000, help= \\\n        \"Number of steps between backups of the model. Set to 0 to never make backups of the \"\n        \"model.\")\n    parser.add_argument(\"-l\", \"--log_every\", type=int, default=200, help= \\\n        \"Number of steps between summary the training info in tensorboard\")\n    parser.add_argument(\"-f\", \"--force_restart\", action=\"store_true\", help= \\\n        \"Do not load any saved model and restart from scratch.\")\n    parser.add_argument(\"--hparams\", default=\"\",\n                        help=\"Hyperparameter overrides as a comma-separated list of name=value \"\n\t\t\t\t\t\t\t \"pairs\")\n    args, _ = parser.parse_known_args()\n    print_args(args, parser)\n\n    args.hparams = hparams.parse(args.hparams)\n\n    # Run the training\n    train(**vars(args))\n\n\nif __name__ == \"__main__\":\n    new_train()", "control/toolbox/ui.py": "from PyQt5.QtCore import Qt, QStringListModel\nfrom PyQt5 import QtGui\nfrom PyQt5.QtWidgets import *\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\nfrom models.encoder.inference import plot_embedding_as_heatmap\nfrom control.toolbox.utterance import Utterance\nfrom pathlib import Path\nfrom typing import List, Set\nimport sounddevice as sd\nimport soundfile as sf\nimport numpy as np\n# from sklearn.manifold import TSNE         # You can try with TSNE if you like, I prefer UMAP \nfrom time import sleep\nimport umap\nimport sys\nfrom warnings import filterwarnings, warn\nfilterwarnings(\"ignore\")\n\n\ncolormap = np.array([\n    [0, 127, 70],\n    [255, 0, 0],\n    [255, 217, 38],\n    [0, 135, 255],\n    [165, 0, 165],\n    [255, 167, 255],\n    [97, 142, 151],\n    [0, 255, 255],\n    [255, 96, 38],\n    [142, 76, 0],\n    [33, 0, 127],\n    [0, 0, 0],\n    [183, 183, 183],\n    [76, 255, 0],\n], dtype=float) / 255 \n\ndefault_text = \\\n    \"\u6b22\u8fce\u4f7f\u7528\u5de5\u5177\u7bb1, \u73b0\u5df2\u652f\u6301\u4e2d\u6587\u8f93\u5165\uff01\"\n\n\n   \nclass UI(QDialog):\n    min_umap_points = 4\n    max_log_lines = 5\n    max_saved_utterances = 20\n    \n    def draw_utterance(self, utterance: Utterance, which):\n        self.draw_spec(utterance.spec, which)\n        self.draw_embed(utterance.embed, utterance.name, which)\n    \n    def draw_embed(self, embed, name, which):\n        embed_ax, _ = self.current_ax if which == \"current\" else self.gen_ax\n        embed_ax.figure.suptitle(\"\" if embed is None else name)\n        \n        ## Embedding\n        # Clear the plot\n        if len(embed_ax.images) > 0:\n            embed_ax.images[0].colorbar.remove()\n        embed_ax.clear()\n        \n        # Draw the embed\n        if embed is not None:\n            plot_embedding_as_heatmap(embed, embed_ax)\n            embed_ax.set_title(\"embedding\")\n        embed_ax.set_aspect(\"equal\", \"datalim\")\n        embed_ax.set_xticks([])\n        embed_ax.set_yticks([])\n        embed_ax.figure.canvas.draw()\n\n    def draw_spec(self, spec, which):\n        _, spec_ax = self.current_ax if which == \"current\" else self.gen_ax\n\n        ## Spectrogram\n        # Draw the spectrogram\n        spec_ax.clear()\n        if spec is not None:\n            im = spec_ax.imshow(spec, aspect=\"auto\", interpolation=\"none\")\n            # spec_ax.figure.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", \n            # spec_ax=spec_ax)\n            spec_ax.set_title(\"mel spectrogram\")\n    \n        spec_ax.set_xticks([])\n        spec_ax.set_yticks([])\n        spec_ax.figure.canvas.draw()\n        if which != \"current\":\n            self.vocode_button.setDisabled(spec is None)\n\n    def draw_umap_projections(self, utterances: Set[Utterance]):\n        self.umap_ax.clear()\n\n        speakers = np.unique([u.speaker_name for u in utterances])\n        colors = {speaker_name: colormap[i] for i, speaker_name in enumerate(speakers)}\n        embeds = [u.embed for u in utterances]\n\n        # Display a message if there aren't enough points\n        if len(utterances) < self.min_umap_points:\n            self.umap_ax.text(.5, .5, \"Add %d more points to\\ngenerate the projections\" % \n                              (self.min_umap_points - len(utterances)), \n                              horizontalalignment='center', fontsize=15)\n            self.umap_ax.set_title(\"\")\n            \n        # Compute the projections\n        else:\n            if not self.umap_hot:\n                self.log(\n                    \"Drawing UMAP projections for the first time, this will take a few seconds.\")\n                self.umap_hot = True\n            \n            reducer = umap.UMAP(int(np.ceil(np.sqrt(len(embeds)))), metric=\"cosine\")\n            # reducer = TSNE()\n            projections = reducer.fit_transform(embeds)\n            \n            speakers_done = set()\n            for projection, utterance in zip(projections, utterances):\n                color = colors[utterance.speaker_name]\n                mark = \"x\" if \"_gen_\" in utterance.name else \"o\"\n                label = None if utterance.speaker_name in speakers_done else utterance.speaker_name\n                speakers_done.add(utterance.speaker_name)\n                self.umap_ax.scatter(projection[0], projection[1], c=[color], marker=mark,\n                                     label=label)\n            # self.umap_ax.set_title(\"UMAP projections\")\n            self.umap_ax.legend(prop={'size': 10})\n\n        # Draw the plot\n        self.umap_ax.set_aspect(\"equal\", \"datalim\")\n        self.umap_ax.set_xticks([])\n        self.umap_ax.set_yticks([])\n        self.umap_ax.figure.canvas.draw()\n\n    def save_audio_file(self, wav, sample_rate):        \n        dialog = QFileDialog()\n        dialog.setDefaultSuffix(\".wav\")\n        fpath, _ = dialog.getSaveFileName(\n            parent=self,\n            caption=\"Select a path to save the audio file\",\n            filter=\"Audio Files (*.flac *.wav)\"\n        )\n        if fpath:\n            #Default format is wav\n            if Path(fpath).suffix == \"\":\n                fpath += \".wav\"\n            sf.write(fpath, wav, sample_rate)\n\n    def setup_audio_devices(self, sample_rate):\n        input_devices = []\n        output_devices = []\n        for device in sd.query_devices():\n            # Check if valid input\n            try:\n                sd.check_input_settings(device=device[\"name\"], samplerate=sample_rate)\n                input_devices.append(device[\"name\"])\n            except:\n                pass\n\n            # Check if valid output\n            try:\n                sd.check_output_settings(device=device[\"name\"], samplerate=sample_rate)\n                output_devices.append(device[\"name\"])\n            except Exception as e:\n                # Log a warning only if the device is not an input\n                if not device[\"name\"] in input_devices:\n                    warn(\"Unsupported output device %s for the sample rate: %d \\nError: %s\" % (device[\"name\"], sample_rate, str(e)))\n\n        if len(input_devices) == 0:\n            self.log(\"No audio input device detected. Recording may not work.\")\n            self.audio_in_device = None\n        else:\n            self.audio_in_device = input_devices[0]\n\n        if len(output_devices) == 0:\n            self.log(\"No supported output audio devices were found! Audio output may not work.\")\n            self.audio_out_devices_cb.addItems([\"None\"])\n            self.audio_out_devices_cb.setDisabled(True)\n        else:\n            self.audio_out_devices_cb.clear()\n            self.audio_out_devices_cb.addItems(output_devices)\n            self.audio_out_devices_cb.currentTextChanged.connect(self.set_audio_device)\n\n        self.set_audio_device()\n\n    def set_audio_device(self):\n        \n        output_device = self.audio_out_devices_cb.currentText()\n        if output_device == \"None\":\n            output_device = None\n\n        # If None, sounddevice queries portaudio\n        sd.default.device = (self.audio_in_device, output_device)\n    \n    def play(self, wav, sample_rate):\n        try:\n            sd.stop()\n            sd.play(wav, sample_rate)\n        except Exception as e:\n            print(e)\n            self.log(\"Error in audio playback. Try selecting a different audio output device.\")\n            self.log(\"Your device must be connected before you start the toolbox.\")\n        \n    def stop(self):\n        sd.stop()\n\n    def record_one(self, sample_rate, duration):\n        self.record_button.setText(\"Recording...\")\n        self.record_button.setDisabled(True)\n        \n        self.log(\"Recording %d seconds of audio\" % duration)\n        sd.stop()\n        try:\n            wav = sd.rec(duration * sample_rate, sample_rate, 1)\n        except Exception as e:\n            print(e)\n            self.log(\"Could not record anything. Is your recording device enabled?\")\n            self.log(\"Your device must be connected before you start the toolbox.\")\n            return None\n        \n        for i in np.arange(0, duration, 0.1):\n            self.set_loading(i, duration)\n            sleep(0.1)\n        self.set_loading(duration, duration)\n        sd.wait()\n        \n        self.log(\"Done recording.\")\n        self.record_button.setText(\"Record\")\n        self.record_button.setDisabled(False)\n        \n        return wav.squeeze()\n\n    @property        \n    def current_dataset_name(self):\n        return self.dataset_box.currentText()\n\n    @property\n    def current_speaker_name(self):\n        return self.speaker_box.currentText()\n    \n    @property\n    def current_utterance_name(self):\n        return self.utterance_box.currentText()\n    \n    def browse_file(self):\n        fpath = QFileDialog().getOpenFileName(\n            parent=self,\n            caption=\"Select an audio file\",\n            filter=\"Audio Files (*.mp3 *.flac *.wav *.m4a)\"\n        )\n        return Path(fpath[0]) if fpath[0] != \"\" else \"\"\n    \n    @staticmethod\n    def repopulate_box(box, items, random=False):\n        \"\"\"\n        Resets a box and adds a list of items. Pass a list of (item, data) pairs instead to join \n        data to the items\n        \"\"\"\n        box.blockSignals(True)\n        box.clear()\n        for item in items:\n            item = list(item) if isinstance(item, tuple) else [item]\n            box.addItem(str(item[0]), *item[1:])\n        if len(items) > 0:\n            box.setCurrentIndex(np.random.randint(len(items)) if random else 0)\n        box.setDisabled(len(items) == 0)\n        box.blockSignals(False)\n    \n    def populate_browser(self, datasets_root: Path, recognized_datasets: List, level: int,\n                         random=True):\n        # Select a random dataset\n        if level <= 0:\n            if datasets_root is not None:\n                datasets = [datasets_root.joinpath(d) for d in recognized_datasets]\n                datasets = [d.relative_to(datasets_root) for d in datasets if d.exists()]\n                self.browser_load_button.setDisabled(len(datasets) == 0)\n            if datasets_root is None or len(datasets) == 0:\n                msg = \"Warning: you d\" + (\"id not pass a root directory for datasets as argument\" \\\n                    if datasets_root is None else \"o not have any of the recognized datasets\" \\\n                                                  \" in %s \\n\" \\\n                                                  \"Please note use 'E:\\datasets' as root path \" \\\n                                                  \"instead of 'E:\\datasets\\aidatatang_200zh\\corpus\\test' as an example \"   % datasets_root) \n                self.log(msg)\n                msg += \".\\nThe recognized datasets are:\\n\\t%s\\nFeel free to add your own. You \" \\\n                       \"can still use the toolbox by recording samples yourself.\" % \\\n                       (\"\\n\\t\".join(recognized_datasets))\n                print(msg, file=sys.stderr)\n                \n                self.random_utterance_button.setDisabled(True)\n                self.random_speaker_button.setDisabled(True)\n                self.random_dataset_button.setDisabled(True)\n                self.utterance_box.setDisabled(True)\n                self.speaker_box.setDisabled(True)\n                self.dataset_box.setDisabled(True)\n                self.browser_load_button.setDisabled(True)\n                self.auto_next_checkbox.setDisabled(True)\n                return \n            self.repopulate_box(self.dataset_box, datasets, random)\n    \n        # Select a random speaker\n        if level <= 1:\n            speakers_root = datasets_root.joinpath(self.current_dataset_name)\n            speaker_names = [d.stem for d in speakers_root.glob(\"*\") if d.is_dir()]\n            self.repopulate_box(self.speaker_box, speaker_names, random)\n    \n        # Select a random utterance\n        if level <= 2:\n            utterances_root = datasets_root.joinpath(\n                self.current_dataset_name, \n                self.current_speaker_name\n            )\n            utterances = []\n            for extension in ['mp3', 'flac', 'wav', 'm4a']:\n                utterances.extend(Path(utterances_root).glob(\"**/*.%s\" % extension))\n            utterances = [fpath.relative_to(utterances_root) for fpath in utterances]\n            self.repopulate_box(self.utterance_box, utterances, random)\n            \n    def browser_select_next(self):\n        index = (self.utterance_box.currentIndex() + 1) % len(self.utterance_box)\n        self.utterance_box.setCurrentIndex(index)\n\n    @property\n    def current_encoder_fpath(self):\n        return self.encoder_box.itemData(self.encoder_box.currentIndex())\n    \n    @property\n    def current_synthesizer_fpath(self):\n        return self.synthesizer_box.itemData(self.synthesizer_box.currentIndex())\n    \n    @property\n    def current_vocoder_fpath(self):\n        return self.vocoder_box.itemData(self.vocoder_box.currentIndex())\n\n    @property\n    def current_extractor_fpath(self):\n        return self.extractor_box.itemData(self.extractor_box.currentIndex())\n\n    @property\n    def current_convertor_fpath(self):\n        return self.convertor_box.itemData(self.convertor_box.currentIndex())\n\n    def populate_models(self, encoder_models_dir: Path, synthesizer_models_dir: Path, \n                        vocoder_models_dir: Path, extractor_models_dir: Path, convertor_models_dir: Path, vc_mode: bool):\n        # Encoder\n        encoder_fpaths = list(encoder_models_dir.glob(\"*.pt\"))\n        if len(encoder_fpaths) == 0:\n            raise Exception(\"No encoder models found in %s\" % encoder_models_dir)\n        self.repopulate_box(self.encoder_box, [(f.stem, f) for f in encoder_fpaths])\n        \n        if vc_mode:\n            # Extractor\n            extractor_fpaths = list(extractor_models_dir.glob(\"*.pt\"))\n            if len(extractor_fpaths) == 0:\n                self.log(\"No extractor models found in %s\" % extractor_fpaths)\n            self.repopulate_box(self.extractor_box, [(f.stem, f) for f in extractor_fpaths])\n            \n            # Convertor\n            convertor_fpaths = list(convertor_models_dir.glob(\"*.pth\"))\n            if len(convertor_fpaths) == 0:\n                self.log(\"No convertor models found in %s\" % convertor_fpaths)\n            self.repopulate_box(self.convertor_box, [(f.stem, f) for f in convertor_fpaths])\n        else:\n            # Synthesizer\n            synthesizer_fpaths = list(synthesizer_models_dir.glob(\"**/*.pt\"))\n            if len(synthesizer_fpaths) == 0:\n                raise Exception(\"No synthesizer models found in %s\" % synthesizer_models_dir)\n            self.repopulate_box(self.synthesizer_box, [(f.stem, f) for f in synthesizer_fpaths])\n\n        # Vocoder\n        vocoder_fpaths = list(vocoder_models_dir.glob(\"**/*.pt\"))\n        vocoder_items = [(f.stem, f) for f in vocoder_fpaths] + [(\"Griffin-Lim\", None)]\n        self.repopulate_box(self.vocoder_box, vocoder_items)\n\n    @property\n    def selected_utterance(self):\n        return self.utterance_history.itemData(self.utterance_history.currentIndex())\n        \n    def register_utterance(self, utterance: Utterance, vc_mode):\n        self.utterance_history.blockSignals(True)\n        self.utterance_history.insertItem(0, utterance.name, utterance)\n        self.utterance_history.setCurrentIndex(0)\n        self.utterance_history.blockSignals(False)\n        \n        if len(self.utterance_history) > self.max_saved_utterances:\n            self.utterance_history.removeItem(self.max_saved_utterances)\n\n        self.play_button.setDisabled(False)\n        if vc_mode:\n            self.convert_button.setDisabled(False)\n        else:\n            self.generate_button.setDisabled(False)\n            self.synthesize_button.setDisabled(False)\n\n    def log(self, line, mode=\"newline\"):\n        if mode == \"newline\":\n            self.logs.append(line)\n            if len(self.logs) > self.max_log_lines:\n                del self.logs[0]\n        elif mode == \"append\":\n            self.logs[-1] += line\n        elif mode == \"overwrite\":\n            self.logs[-1] = line\n        log_text = '\\n'.join(self.logs)\n        \n        self.log_window.setText(log_text)\n        self.app.processEvents()\n\n    def set_loading(self, value, maximum=1):\n        self.loading_bar.setValue(int(value * 100))\n        self.loading_bar.setMaximum(int(maximum * 100))\n        self.loading_bar.setTextVisible(value != 0)\n        self.app.processEvents()\n\n    def populate_gen_options(self, seed, trim_silences):\n        if seed is not None:\n            self.random_seed_checkbox.setChecked(True)\n            self.seed_textbox.setText(str(seed))\n            self.seed_textbox.setEnabled(True)\n        else:\n            self.random_seed_checkbox.setChecked(False)\n            self.seed_textbox.setText(str(0))\n            self.seed_textbox.setEnabled(False)\n\n        if not trim_silences:\n            self.trim_silences_checkbox.setChecked(False)\n            self.trim_silences_checkbox.setDisabled(True)\n\n    def update_seed_textbox(self):\n        if self.random_seed_checkbox.isChecked():\n            self.seed_textbox.setEnabled(True)\n        else:\n            self.seed_textbox.setEnabled(False)\n\n    def reset_interface(self, vc_mode):\n        self.draw_embed(None, None, \"current\")\n        self.draw_embed(None, None, \"generated\")\n        self.draw_spec(None, \"current\")\n        self.draw_spec(None, \"generated\")\n        self.draw_umap_projections(set())\n        self.set_loading(0)\n        self.play_button.setDisabled(True)\n        if vc_mode:\n            self.convert_button.setDisabled(True)\n        else:\n            self.generate_button.setDisabled(True)\n            self.synthesize_button.setDisabled(True)\n        self.vocode_button.setDisabled(True)\n        self.replay_wav_button.setDisabled(True)\n        self.export_wav_button.setDisabled(True)\n        [self.log(\"\") for _ in range(self.max_log_lines)]\n\n    def __init__(self, vc_mode):\n        ## Initialize the application\n        self.app = QApplication(sys.argv)\n        super().__init__(None)\n        self.setWindowTitle(\"MockingBird GUI\")\n        self.setWindowIcon(QtGui.QIcon('toolbox\\\\assets\\\\mb.png'))\n        self.setWindowFlag(Qt.WindowMinimizeButtonHint, True)\n        self.setWindowFlag(Qt.WindowMaximizeButtonHint, True)\n        \n        \n        ## Main layouts\n        # Root\n        root_layout = QGridLayout()\n        self.setLayout(root_layout)\n        \n        # Browser\n        browser_layout = QGridLayout()\n        root_layout.addLayout(browser_layout, 0, 0, 1, 8)\n        \n        # Generation\n        gen_layout = QVBoxLayout()\n        root_layout.addLayout(gen_layout, 0, 8)\n\n        # Visualizations\n        vis_layout = QVBoxLayout()\n        root_layout.addLayout(vis_layout, 1, 0, 2, 8)\n\n        # Output\n        output_layout = QGridLayout()\n        vis_layout.addLayout(output_layout, 0)\n\n        # Projections\n        self.projections_layout = QVBoxLayout()\n        root_layout.addLayout(self.projections_layout, 1, 8, 2, 2)\n        \n        ## Projections\n        # UMap\n        fig, self.umap_ax = plt.subplots(figsize=(3, 3), facecolor=\"#F0F0F0\")\n        fig.subplots_adjust(left=0.02, bottom=0.02, right=0.98, top=0.98)\n        self.projections_layout.addWidget(FigureCanvas(fig))\n        self.umap_hot = False\n        self.clear_button = QPushButton(\"Clear\")\n        self.projections_layout.addWidget(self.clear_button)\n\n\n        ## Browser\n        # Dataset, speaker and utterance selection\n        i = 0\n        \n        source_groupbox = QGroupBox('Source(\u6e90\u97f3\u9891)')\n        source_layout = QGridLayout()\n        source_groupbox.setLayout(source_layout)\n        browser_layout.addWidget(source_groupbox, i, 0, 1, 5)\n\n        self.dataset_box = QComboBox()\n        source_layout.addWidget(QLabel(\"Dataset(\u6570\u636e\u96c6):\"), i, 0)\n        source_layout.addWidget(self.dataset_box, i, 1)\n        self.random_dataset_button = QPushButton(\"Random\")\n        source_layout.addWidget(self.random_dataset_button, i, 2)\n        i += 1\n        self.speaker_box = QComboBox()\n        source_layout.addWidget(QLabel(\"Speaker(\u8bf4\u8bdd\u8005)\"), i, 0)\n        source_layout.addWidget(self.speaker_box, i, 1)\n        self.random_speaker_button = QPushButton(\"Random\")\n        source_layout.addWidget(self.random_speaker_button, i, 2)\n        i += 1\n        self.utterance_box = QComboBox()\n        source_layout.addWidget(QLabel(\"Utterance(\u97f3\u9891):\"), i, 0)\n        source_layout.addWidget(self.utterance_box, i, 1)\n        self.random_utterance_button = QPushButton(\"Random\")\n        source_layout.addWidget(self.random_utterance_button, i, 2)\n\n        i += 1\n        source_layout.addWidget(QLabel(\"<b>Use(\u4f7f\u7528):</b>\"), i, 0)\n        self.browser_load_button = QPushButton(\"Load Above(\u52a0\u8f7d\u4e0a\u9762)\")\n        source_layout.addWidget(self.browser_load_button, i, 1, 1, 2)\n        self.auto_next_checkbox = QCheckBox(\"Auto select next\")\n        self.auto_next_checkbox.setChecked(True)\n        source_layout.addWidget(self.auto_next_checkbox, i+1, 1)\n        self.browser_browse_button = QPushButton(\"Browse(\u6253\u5f00\u672c\u5730)\")\n        source_layout.addWidget(self.browser_browse_button, i, 3)\n        self.record_button = QPushButton(\"Record(\u5f55\u97f3)\")\n        source_layout.addWidget(self.record_button, i+1, 3)\n        \n        i += 2\n        # Utterance box\n        browser_layout.addWidget(QLabel(\"<b>Current(\u5f53\u524d):</b>\"), i, 0)\n        self.utterance_history = QComboBox()\n        browser_layout.addWidget(self.utterance_history, i, 1)\n        self.play_button = QPushButton(\"Play(\u64ad\u653e)\")\n        browser_layout.addWidget(self.play_button, i, 2)\n        self.stop_button = QPushButton(\"Stop(\u6682\u505c)\")\n        browser_layout.addWidget(self.stop_button, i, 3)\n        if vc_mode:\n            self.load_soruce_button = QPushButton(\"Select(\u9009\u62e9\u4e3a\u88ab\u8f6c\u6362\u7684\u8bed\u97f3\u8f93\u5165)\")\n            browser_layout.addWidget(self.load_soruce_button, i, 4)\n\n        i += 1\n        model_groupbox = QGroupBox('Models(\u6a21\u578b\u9009\u62e9)')\n        model_layout = QHBoxLayout()\n        model_groupbox.setLayout(model_layout)\n        browser_layout.addWidget(model_groupbox, i, 0, 2, 5)\n\n        # Model and audio output selection\n        self.encoder_box = QComboBox()\n        model_layout.addWidget(QLabel(\"Encoder:\"))\n        model_layout.addWidget(self.encoder_box)\n        self.synthesizer_box = QComboBox()\n        if vc_mode:\n            self.extractor_box = QComboBox()\n            model_layout.addWidget(QLabel(\"Extractor:\"))\n            model_layout.addWidget(self.extractor_box)\n            self.convertor_box = QComboBox()\n            model_layout.addWidget(QLabel(\"Convertor:\"))\n            model_layout.addWidget(self.convertor_box)\n        else:\n            model_layout.addWidget(QLabel(\"Synthesizer:\"))\n            model_layout.addWidget(self.synthesizer_box)\n        self.vocoder_box = QComboBox()\n        model_layout.addWidget(QLabel(\"Vocoder:\"))\n        model_layout.addWidget(self.vocoder_box)\n    \n        #Replay & Save Audio\n        i = 0\n        output_layout.addWidget(QLabel(\"<b>Toolbox Output:</b>\"), i, 0)\n        self.waves_cb = QComboBox()\n        self.waves_cb_model = QStringListModel()\n        self.waves_cb.setModel(self.waves_cb_model)\n        self.waves_cb.setToolTip(\"Select one of the last generated waves in this section for replaying or exporting\")\n        output_layout.addWidget(self.waves_cb, i, 1)\n        self.replay_wav_button = QPushButton(\"Replay\")\n        self.replay_wav_button.setToolTip(\"Replay last generated vocoder\")\n        output_layout.addWidget(self.replay_wav_button, i, 2)\n        self.export_wav_button = QPushButton(\"Export\")\n        self.export_wav_button.setToolTip(\"Save last generated vocoder audio in filesystem as a wav file\")\n        output_layout.addWidget(self.export_wav_button, i, 3)\n        self.audio_out_devices_cb=QComboBox()\n        i += 1\n        output_layout.addWidget(QLabel(\"<b>Audio Output</b>\"), i, 0)\n        output_layout.addWidget(self.audio_out_devices_cb, i, 1)\n\n        ## Embed & spectrograms\n        vis_layout.addStretch()\n        # TODO: add spectrograms for source\n        gridspec_kw = {\"width_ratios\": [1, 4]}\n        fig, self.current_ax = plt.subplots(1, 2, figsize=(10, 2.25), facecolor=\"#F0F0F0\", \n                                            gridspec_kw=gridspec_kw)\n        fig.subplots_adjust(left=0, bottom=0.1, right=1, top=0.8)\n        vis_layout.addWidget(FigureCanvas(fig))\n\n        fig, self.gen_ax = plt.subplots(1, 2, figsize=(10, 2.25), facecolor=\"#F0F0F0\", \n                                        gridspec_kw=gridspec_kw)\n        fig.subplots_adjust(left=0, bottom=0.1, right=1, top=0.8)\n        vis_layout.addWidget(FigureCanvas(fig))\n\n        for ax in self.current_ax.tolist() + self.gen_ax.tolist():\n            ax.set_facecolor(\"#F0F0F0\")\n            for side in [\"top\", \"right\", \"bottom\", \"left\"]:\n                ax.spines[side].set_visible(False)\n        \n        ## Generation\n        self.text_prompt = QPlainTextEdit(default_text)\n        gen_layout.addWidget(self.text_prompt, stretch=1)\n        \n        if vc_mode:\n            layout = QHBoxLayout()\n            self.convert_button = QPushButton(\"Extract and Convert\")\n            layout.addWidget(self.convert_button)\n            gen_layout.addLayout(layout)\n        else:\n            self.generate_button = QPushButton(\"Synthesize and vocode\")\n            gen_layout.addWidget(self.generate_button)\n            layout = QHBoxLayout()\n            self.synthesize_button = QPushButton(\"Synthesize only\")\n            layout.addWidget(self.synthesize_button)\n\n        self.vocode_button = QPushButton(\"Vocode only\")\n        layout.addWidget(self.vocode_button)\n        gen_layout.addLayout(layout)\n\n\n        layout_seed = QGridLayout()\n        self.random_seed_checkbox = QCheckBox(\"Random seed:\")\n        self.random_seed_checkbox.setToolTip(\"When checked, makes the synthesizer and vocoder deterministic.\")\n        layout_seed.addWidget(self.random_seed_checkbox, 0, 0)\n        self.seed_textbox = QLineEdit()\n        self.seed_textbox.setMaximumWidth(80)\n        layout_seed.addWidget(self.seed_textbox, 0, 1)\n        self.trim_silences_checkbox = QCheckBox(\"Enhance vocoder output\")\n        self.trim_silences_checkbox.setToolTip(\"When checked, trims excess silence in vocoder output.\"\n            \" This feature requires `webrtcvad` to be installed.\")\n        layout_seed.addWidget(self.trim_silences_checkbox, 0, 2, 1, 2)\n        self.style_slider = QSlider(Qt.Horizontal)\n        self.style_slider.setTickInterval(1)\n        self.style_slider.setFocusPolicy(Qt.NoFocus)\n        self.style_slider.setSingleStep(1)\n        self.style_slider.setRange(-1, 9)\n        self.style_value_label = QLabel(\"-1\")\n        self.style_slider.setValue(-1)\n        layout_seed.addWidget(QLabel(\"Style:\"), 1, 0)\n\n        self.style_slider.valueChanged.connect(lambda s: self.style_value_label.setNum(s))\n        layout_seed.addWidget(self.style_value_label, 1, 1)\n        layout_seed.addWidget(self.style_slider, 1, 3)\n\n        self.token_slider = QSlider(Qt.Horizontal)\n        self.token_slider.setTickInterval(1)\n        self.token_slider.setFocusPolicy(Qt.NoFocus)\n        self.token_slider.setSingleStep(1)\n        self.token_slider.setRange(3, 9)\n        self.token_value_label = QLabel(\"5\")\n        self.token_slider.setValue(4)\n        layout_seed.addWidget(QLabel(\"Accuracy(\u7cbe\u5ea6):\"), 2, 0)\n\n        self.token_slider.valueChanged.connect(lambda s: self.token_value_label.setNum(s))\n        layout_seed.addWidget(self.token_value_label, 2, 1)\n        layout_seed.addWidget(self.token_slider, 2, 3)\n\n        self.length_slider = QSlider(Qt.Horizontal)\n        self.length_slider.setTickInterval(1)\n        self.length_slider.setFocusPolicy(Qt.NoFocus)\n        self.length_slider.setSingleStep(1)\n        self.length_slider.setRange(1, 10)\n        self.length_value_label = QLabel(\"2\")\n        self.length_slider.setValue(2)\n        layout_seed.addWidget(QLabel(\"MaxLength(\u6700\u5927\u53e5\u957f):\"), 3, 0)\n\n        self.length_slider.valueChanged.connect(lambda s: self.length_value_label.setNum(s))\n        layout_seed.addWidget(self.length_value_label, 3, 1)\n        layout_seed.addWidget(self.length_slider, 3, 3)\n\n        gen_layout.addLayout(layout_seed)\n\n        self.loading_bar = QProgressBar()\n        gen_layout.addWidget(self.loading_bar)\n        \n        self.log_window = QLabel()\n        self.log_window.setAlignment(Qt.AlignBottom | Qt.AlignLeft)\n        gen_layout.addWidget(self.log_window)\n        self.logs = []\n        gen_layout.addStretch()\n\n        \n        ## Set the size of the window and of the elements\n        max_size = QDesktopWidget().availableGeometry(self).size() * 0.5\n        self.resize(max_size)\n        \n        ## Finalize the display\n        self.reset_interface(vc_mode)\n        self.show()\n\n    def start(self):\n        self.app.exec_()\n", "control/toolbox/utterance.py": "from collections import namedtuple\n\nUtterance = namedtuple(\"Utterance\", \"name speaker_name wav spec embed partial_embeds synth\")\nUtterance.__eq__ = lambda x, y: x.name == y.name\nUtterance.__hash__ = lambda x: hash(x.name)\n", "control/toolbox/__init__.py": "from control.toolbox.ui import UI\nfrom models.encoder import inference as encoder\nfrom models.synthesizer.inference import Synthesizer\nfrom models.vocoder.wavernn import inference as rnn_vocoder\nfrom models.vocoder.hifigan import inference as gan_vocoder\nfrom models.vocoder.fregan import inference as fgan_vocoder\nfrom pathlib import Path\nfrom time import perf_counter as timer\nfrom control.toolbox.utterance import Utterance\nimport numpy as np\nimport traceback\nimport sys\nimport torch\nimport re\n\n# \u9ed8\u8ba4\u4f7f\u7528wavernn\nvocoder = rnn_vocoder\n\n# Use this directory structure for your datasets, or modify it to fit your needs\nrecognized_datasets = [\n    \"LibriSpeech/dev-clean\",\n    \"LibriSpeech/dev-other\",\n    \"LibriSpeech/test-clean\",\n    \"LibriSpeech/test-other\",\n    \"LibriSpeech/train-clean-100\",\n    \"LibriSpeech/train-clean-360\",\n    \"LibriSpeech/train-other-500\",\n    \"LibriTTS/dev-clean\",\n    \"LibriTTS/dev-other\",\n    \"LibriTTS/test-clean\",\n    \"LibriTTS/test-other\",\n    \"LibriTTS/train-clean-100\",\n    \"LibriTTS/train-clean-360\",\n    \"LibriTTS/train-other-500\",\n    \"LJSpeech-1.1\",\n    \"VoxCeleb1/wav\",\n    \"VoxCeleb1/test_wav\",\n    \"VoxCeleb2/dev/aac\",\n    \"VoxCeleb2/test/aac\",\n    \"VCTK-Corpus/wav48\",\n    \"aidatatang_200zh/corpus/test\",\n    \"aidatatang_200zh/corpus/train\",\n    \"aishell3/test/wav\",\n    \"magicdata/train\",\n]\n\n#Maximum of generated wavs to keep on memory\nMAX_WAVES = 15\n\nclass Toolbox:\n    def __init__(self, datasets_root, enc_models_dir, syn_models_dir, voc_models_dir, extractor_models_dir, convertor_models_dir, seed, no_mp3_support, vc_mode):\n        self.no_mp3_support = no_mp3_support\n        self.vc_mode = vc_mode\n        sys.excepthook = self.excepthook\n        self.datasets_root = datasets_root\n        self.utterances = set()\n        self.current_generated = (None, None, None, None) # speaker_name, spec, breaks, wav\n        \n        self.synthesizer = None # type: Synthesizer\n\n        # for ppg-based voice conversion\n        self.extractor = None \n        self.convertor = None # ppg2mel\n\n        self.current_wav = None\n        self.waves_list = []\n        self.waves_count = 0\n        self.waves_namelist = []\n\n        # Check for webrtcvad (enables removal of silences in vocoder output)\n        try:\n            import webrtcvad\n            self.trim_silences = True\n        except:\n            self.trim_silences = False\n\n        # Initialize the events and the interface\n        self.ui = UI(vc_mode)\n        self.style_idx = 0\n        self.reset_ui(enc_models_dir, syn_models_dir, voc_models_dir, extractor_models_dir, convertor_models_dir, seed)\n        self.setup_events()\n        self.ui.start()\n\n    def excepthook(self, exc_type, exc_value, exc_tb):\n        traceback.print_exception(exc_type, exc_value, exc_tb)\n        self.ui.log(\"Exception: %s\" % exc_value)\n        \n    def setup_events(self):\n        # Dataset, speaker and utterance selection\n        self.ui.browser_load_button.clicked.connect(lambda: self.load_from_browser())\n        random_func = lambda level: lambda: self.ui.populate_browser(self.datasets_root,\n                                                                     recognized_datasets,\n                                                                     level)\n        self.ui.random_dataset_button.clicked.connect(random_func(0))\n        self.ui.random_speaker_button.clicked.connect(random_func(1))\n        self.ui.random_utterance_button.clicked.connect(random_func(2))\n        self.ui.dataset_box.currentIndexChanged.connect(random_func(1))\n        self.ui.speaker_box.currentIndexChanged.connect(random_func(2))\n        \n        # Model selection\n        self.ui.encoder_box.currentIndexChanged.connect(self.init_encoder)\n        def func(): \n            self.synthesizer = None\n        if self.vc_mode:\n            self.ui.extractor_box.currentIndexChanged.connect(self.init_extractor)\n        else:\n            self.ui.synthesizer_box.currentIndexChanged.connect(func)\n\n        self.ui.vocoder_box.currentIndexChanged.connect(self.init_vocoder)\n        \n        # Utterance selection\n        func = lambda: self.load_from_browser(self.ui.browse_file())\n        self.ui.browser_browse_button.clicked.connect(func)\n        func = lambda: self.ui.draw_utterance(self.ui.selected_utterance, \"current\")\n        self.ui.utterance_history.currentIndexChanged.connect(func)\n        func = lambda: self.ui.play(self.ui.selected_utterance.wav, Synthesizer.sample_rate)\n        self.ui.play_button.clicked.connect(func)\n        self.ui.stop_button.clicked.connect(self.ui.stop)\n        self.ui.record_button.clicked.connect(self.record)\n\n        # Source Utterance selection\n        if self.vc_mode:\n            func = lambda: self.load_soruce_button(self.ui.selected_utterance)\n            self.ui.load_soruce_button.clicked.connect(func)\n\n        #Audio\n        self.ui.setup_audio_devices(Synthesizer.sample_rate)\n\n        #Wav playback & save\n        func = lambda: self.replay_last_wav()\n        self.ui.replay_wav_button.clicked.connect(func)\n        func = lambda: self.export_current_wave()\n        self.ui.export_wav_button.clicked.connect(func)\n        self.ui.waves_cb.currentIndexChanged.connect(self.set_current_wav)\n\n        # Generation\n        self.ui.vocode_button.clicked.connect(self.vocode)\n        self.ui.random_seed_checkbox.clicked.connect(self.update_seed_textbox)\n\n        if self.vc_mode:\n            func = lambda: self.convert() or self.vocode()\n            self.ui.convert_button.clicked.connect(func)\n        else:\n            func = lambda: self.synthesize() or self.vocode()\n            self.ui.generate_button.clicked.connect(func)\n            self.ui.synthesize_button.clicked.connect(self.synthesize)\n\n        # UMAP legend\n        self.ui.clear_button.clicked.connect(self.clear_utterances)\n\n    def set_current_wav(self, index):\n        self.current_wav = self.waves_list[index]\n\n    def export_current_wave(self):\n        self.ui.save_audio_file(self.current_wav, Synthesizer.sample_rate)\n\n    def replay_last_wav(self):\n        self.ui.play(self.current_wav, Synthesizer.sample_rate)\n\n    def reset_ui(self, encoder_models_dir, synthesizer_models_dir, vocoder_models_dir, extractor_models_dir, convertor_models_dir, seed):\n        self.ui.populate_browser(self.datasets_root, recognized_datasets, 0, True)\n        self.ui.populate_models(encoder_models_dir, synthesizer_models_dir, vocoder_models_dir, extractor_models_dir, convertor_models_dir, self.vc_mode)\n        self.ui.populate_gen_options(seed, self.trim_silences)\n        \n    def load_from_browser(self, fpath=None):\n        if fpath is None:\n            fpath = Path(self.datasets_root,\n                         self.ui.current_dataset_name,\n                         self.ui.current_speaker_name,\n                         self.ui.current_utterance_name)\n            name = str(fpath.relative_to(self.datasets_root))\n            speaker_name = self.ui.current_dataset_name + '_' + self.ui.current_speaker_name\n            \n            # Select the next utterance\n            if self.ui.auto_next_checkbox.isChecked():\n                self.ui.browser_select_next()\n        elif fpath == \"\":\n            return \n        else:\n            name = fpath.name\n            speaker_name = fpath.parent.name\n\n        if fpath.suffix.lower() == \".mp3\" and self.no_mp3_support:\n                self.ui.log(\"Error: No mp3 file argument was passed but an mp3 file was used\")\n                return\n\n        # Get the wav from the disk. We take the wav with the vocoder/synthesizer format for\n        # playback, so as to have a fair comparison with the generated audio\n        wav = Synthesizer.load_preprocess_wav(fpath)\n        self.ui.log(\"Loaded %s\" % name)\n\n        self.add_real_utterance(wav, name, speaker_name)\n    \n    def load_soruce_button(self, utterance: Utterance):\n        self.selected_source_utterance = utterance\n\n    def record(self):\n        wav = self.ui.record_one(encoder.sampling_rate, 5)\n        if wav is None:\n            return \n        self.ui.play(wav, encoder.sampling_rate)\n\n        speaker_name = \"user01\"\n        name = speaker_name + \"_rec_%05d\" % np.random.randint(100000)\n        self.add_real_utterance(wav, name, speaker_name)\n        \n    def add_real_utterance(self, wav, name, speaker_name):\n        # Compute the mel spectrogram\n        spec = Synthesizer.make_spectrogram(wav)\n        self.ui.draw_spec(spec, \"current\")\n\n        # Compute the embedding\n        if not encoder.is_loaded():\n            self.init_encoder()\n        encoder_wav = encoder.preprocess_wav(wav)\n        embed, partial_embeds, _ = encoder.embed_utterance(encoder_wav, return_partials=True)\n\n        # Add the utterance\n        utterance = Utterance(name, speaker_name, wav, spec, embed, partial_embeds, False)\n        self.utterances.add(utterance)\n        self.ui.register_utterance(utterance, self.vc_mode)\n\n        # Plot it\n        self.ui.draw_embed(embed, name, \"current\")\n        self.ui.draw_umap_projections(self.utterances)\n        \n    def clear_utterances(self):\n        self.utterances.clear()\n        self.ui.draw_umap_projections(self.utterances)\n        \n    def synthesize(self):\n        self.ui.log(\"Generating the mel spectrogram...\")\n        self.ui.set_loading(1)\n        \n        # Update the synthesizer random seed\n        if self.ui.random_seed_checkbox.isChecked():\n            seed = int(self.ui.seed_textbox.text())\n            self.ui.populate_gen_options(seed, self.trim_silences)\n        else:\n            seed = None\n\n        if seed is not None:\n            torch.manual_seed(seed)\n\n        # Synthesize the spectrogram\n        if self.synthesizer is None or seed is not None:\n            self.init_synthesizer()\n\n        texts = self.ui.text_prompt.toPlainText().split(\"\\n\")\n        punctuation = '\uff01\uff0c\u3002\u3001,' # punctuate and split/clean text\n        processed_texts = []\n        for text in texts:\n          for processed_text in re.sub(r'[{}]+'.format(punctuation), '\\n', text).split('\\n'):\n            if processed_text:\n                processed_texts.append(processed_text.strip())\n        texts = processed_texts\n        embed = self.ui.selected_utterance.embed\n        embeds = [embed] * len(texts)\n        min_token = int(self.ui.token_slider.value())\n        specs = self.synthesizer.synthesize_spectrograms(texts, embeds, style_idx=int(self.ui.style_slider.value()), min_stop_token=min_token, steps=int(self.ui.length_slider.value())*200)\n        breaks = [spec.shape[1] for spec in specs]\n        spec = np.concatenate(specs, axis=1)\n        \n        self.ui.draw_spec(spec, \"generated\")\n        self.current_generated = (self.ui.selected_utterance.speaker_name, spec, breaks, None)\n        self.ui.set_loading(0)\n\n    def vocode(self):\n        speaker_name, spec, breaks, _ = self.current_generated\n        assert spec is not None\n\n        # Initialize the vocoder model and make it determinstic, if user provides a seed\n        if self.ui.random_seed_checkbox.isChecked():\n            seed = int(self.ui.seed_textbox.text())\n            self.ui.populate_gen_options(seed, self.trim_silences)\n        else:\n            seed = None\n\n        if seed is not None:\n            torch.manual_seed(seed)\n\n        # Synthesize the waveform\n        if not vocoder.is_loaded() or seed is not None:\n            self.init_vocoder()\n\n        def vocoder_progress(i, seq_len, b_size, gen_rate):\n            real_time_factor = (gen_rate / Synthesizer.sample_rate) * 1000\n            line = \"Waveform generation: %d/%d (batch size: %d, rate: %.1fkHz - %.2fx real time)\" \\\n                   % (i * b_size, seq_len * b_size, b_size, gen_rate, real_time_factor)\n            self.ui.log(line, \"overwrite\")\n            self.ui.set_loading(i, seq_len)\n        if self.ui.current_vocoder_fpath is not None:\n            self.ui.log(\"\")\n            wav, sample_rate = vocoder.infer_waveform(spec, progress_callback=vocoder_progress)\n        else:\n            self.ui.log(\"Waveform generation with Griffin-Lim... \")\n            wav = Synthesizer.griffin_lim(spec)\n        self.ui.set_loading(0)\n        self.ui.log(\" Done!\", \"append\")\n        \n        # Add breaks\n        b_ends = np.cumsum(np.array(breaks) * Synthesizer.hparams.hop_size)\n        b_starts = np.concatenate(([0], b_ends[:-1]))\n        wavs = [wav[start:end] for start, end, in zip(b_starts, b_ends)]\n        breaks = [np.zeros(int(0.15 * sample_rate))] * len(breaks)\n        wav = np.concatenate([i for w, b in zip(wavs, breaks) for i in (w, b)])\n\n        # Trim excessive silences\n        if self.ui.trim_silences_checkbox.isChecked():\n            wav = encoder.preprocess_wav(wav)\n\n        # Play it\n        wav = wav / np.abs(wav).max() * 0.97\n        self.ui.play(wav, sample_rate)\n\n        # Name it (history displayed in combobox)\n        # TODO better naming for the combobox items?\n        wav_name = str(self.waves_count + 1)\n\n        #Update waves combobox\n        self.waves_count += 1\n        if self.waves_count > MAX_WAVES:\n          self.waves_list.pop()\n          self.waves_namelist.pop()\n        self.waves_list.insert(0, wav)\n        self.waves_namelist.insert(0, wav_name)\n\n        self.ui.waves_cb.disconnect()\n        self.ui.waves_cb_model.setStringList(self.waves_namelist)\n        self.ui.waves_cb.setCurrentIndex(0)\n        self.ui.waves_cb.currentIndexChanged.connect(self.set_current_wav)\n\n        # Update current wav\n        self.set_current_wav(0)\n        \n        #Enable replay and save buttons:\n        self.ui.replay_wav_button.setDisabled(False)\n        self.ui.export_wav_button.setDisabled(False)\n\n        # Compute the embedding\n        # TODO: this is problematic with different sampling rates, gotta fix it\n        if not encoder.is_loaded():\n            self.init_encoder()\n        encoder_wav = encoder.preprocess_wav(wav)\n        embed, partial_embeds, _ = encoder.embed_utterance(encoder_wav, return_partials=True)\n        \n        # Add the utterance\n        name = speaker_name + \"_gen_%05d\" % np.random.randint(100000)\n        utterance = Utterance(name, speaker_name, wav, spec, embed, partial_embeds, True)\n        self.utterances.add(utterance)\n        \n        # Plot it\n        self.ui.draw_embed(embed, name, \"generated\")\n        self.ui.draw_umap_projections(self.utterances)\n        \n    def convert(self):\n        self.ui.log(\"Extract PPG and Converting...\")\n        self.ui.set_loading(1)\n        \n        # Init\n        if self.convertor is None:\n            self.init_convertor()\n        if self.extractor is None:\n            self.init_extractor()\n        \n        src_wav = self.selected_source_utterance.wav\n\n        # Compute the ppg\n        if not self.extractor is None:\n            ppg = self.extractor.extract_from_wav(src_wav)\n        \n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        ref_wav = self.ui.selected_utterance.wav\n        # Import necessary dependency of Voice Conversion\n        from utils.f0_utils import compute_f0, f02lf0, compute_mean_std, get_converted_lf0uv   \n        ref_lf0_mean, ref_lf0_std = compute_mean_std(f02lf0(compute_f0(ref_wav)))\n        lf0_uv = get_converted_lf0uv(src_wav, ref_lf0_mean, ref_lf0_std, convert=True)\n        min_len = min(ppg.shape[1], len(lf0_uv))\n        ppg = ppg[:, :min_len]\n        lf0_uv = lf0_uv[:min_len]\n        _, mel_pred, att_ws = self.convertor.inference(\n            ppg,\n            logf0_uv=torch.from_numpy(lf0_uv).unsqueeze(0).float().to(device),\n            spembs=torch.from_numpy(self.ui.selected_utterance.embed).unsqueeze(0).to(device),\n        )\n        mel_pred= mel_pred.transpose(0, 1)\n        breaks = [mel_pred.shape[1]]\n        mel_pred= mel_pred.detach().cpu().numpy()\n        self.ui.draw_spec(mel_pred, \"generated\")\n        self.current_generated = (self.ui.selected_utterance.speaker_name, mel_pred, breaks, None)\n        self.ui.set_loading(0)\n\n    def init_extractor(self):\n        if self.ui.current_extractor_fpath is None:\n            return\n        model_fpath = self.ui.current_extractor_fpath\n        self.ui.log(\"Loading the extractor %s... \" % model_fpath)\n        self.ui.set_loading(1)\n        start = timer()\n        import models.ppg_extractor as extractor\n        self.extractor = extractor.load_model(model_fpath)\n        self.ui.log(\"Done (%dms).\" % int(1000 * (timer() - start)), \"append\")\n        self.ui.set_loading(0)\n\n    def init_convertor(self):\n        if self.ui.current_convertor_fpath is None:\n            return\n        model_fpath = self.ui.current_convertor_fpath\n        self.ui.log(\"Loading the convertor %s... \" % model_fpath)\n        self.ui.set_loading(1)\n        start = timer()\n        import models.ppg2mel as convertor\n        self.convertor = convertor.load_model( model_fpath)\n        self.ui.log(\"Done (%dms).\" % int(1000 * (timer() - start)), \"append\")\n        self.ui.set_loading(0)\n        \n    def init_encoder(self):\n        model_fpath = self.ui.current_encoder_fpath\n        \n        self.ui.log(\"Loading the encoder %s... \" % model_fpath)\n        self.ui.set_loading(1)\n        start = timer()\n        encoder.load_model(model_fpath)\n        self.ui.log(\"Done (%dms).\" % int(1000 * (timer() - start)), \"append\")\n        self.ui.set_loading(0)\n\n    def init_synthesizer(self):\n        model_fpath = self.ui.current_synthesizer_fpath\n\n        self.ui.log(\"Loading the synthesizer %s... \" % model_fpath)\n        self.ui.set_loading(1)\n        start = timer()\n        self.synthesizer = Synthesizer(model_fpath)\n        self.ui.log(\"Done (%dms).\" % int(1000 * (timer() - start)), \"append\")\n        self.ui.set_loading(0)\n           \n    def init_vocoder(self):\n\n        global vocoder\n        model_fpath = self.ui.current_vocoder_fpath\n        # Case of Griffin-lim\n        if model_fpath is None:\n            return \n        # Sekect vocoder based on model name\n        model_config_fpath = None\n        if model_fpath.name is not None and model_fpath.name.find(\"hifigan\") > -1:\n            vocoder = gan_vocoder\n            self.ui.log(\"set hifigan as vocoder\")\n            # search a config file\n            model_config_fpaths = list(model_fpath.parent.rglob(\"*.json\"))\n            if self.vc_mode and self.ui.current_extractor_fpath is None:\n                return\n            if len(model_config_fpaths) > 0:\n                model_config_fpath = model_config_fpaths[0]\n        elif model_fpath.name is not None and model_fpath.name.find(\"fregan\") > -1:\n            vocoder = fgan_vocoder\n            self.ui.log(\"set fregan as vocoder\")\n            # search a config file\n            model_config_fpaths = list(model_fpath.parent.rglob(\"*.json\"))\n            if self.vc_mode and self.ui.current_extractor_fpath is None:\n                return\n            if len(model_config_fpaths) > 0:\n                model_config_fpath = model_config_fpaths[0]\n        else:\n            vocoder = rnn_vocoder\n            self.ui.log(\"set wavernn as vocoder\")\n    \n        self.ui.log(\"Loading the vocoder %s... \" % model_fpath)\n        self.ui.set_loading(1)\n        start = timer()\n        vocoder.load_model(model_fpath, model_config_fpath)\n        self.ui.log(\"Done (%dms).\" % int(1000 * (timer() - start)), \"append\")\n        self.ui.set_loading(0)\n\n    def update_seed_textbox(self):\n       self.ui.update_seed_textbox() \n", "archived_untest_files/demo_cli.py": "from models.encoder.params_model import model_embedding_size as speaker_embedding_size\nfrom utils.argutils import print_args\nfrom utils.modelutils import check_model_paths\nfrom models.synthesizer.inference import Synthesizer\nfrom models.encoder import inference as encoder\nfrom models.vocoder import inference as vocoder\nfrom pathlib import Path\nimport numpy as np\nimport soundfile as sf\nimport librosa\nimport argparse\nimport torch\nimport sys\nimport os\nfrom audioread.exceptions import NoBackendError\n\nif __name__ == '__main__':\n    ## Info & args\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\"-e\", \"--enc_model_fpath\", type=Path, \n                        default=\"encoder/saved_models/pretrained.pt\",\n                        help=\"Path to a saved encoder\")\n    parser.add_argument(\"-s\", \"--syn_model_fpath\", type=Path, \n                        default=\"synthesizer/saved_models/pretrained/pretrained.pt\",\n                        help=\"Path to a saved synthesizer\")\n    parser.add_argument(\"-v\", \"--voc_model_fpath\", type=Path, \n                        default=\"vocoder/saved_models/pretrained/pretrained.pt\",\n                        help=\"Path to a saved vocoder\")\n    parser.add_argument(\"--cpu\", action=\"store_true\", help=\\\n        \"If True, processing is done on CPU, even when a GPU is available.\")\n    parser.add_argument(\"--no_sound\", action=\"store_true\", help=\\\n        \"If True, audio won't be played.\")\n    parser.add_argument(\"--seed\", type=int, default=None, help=\\\n        \"Optional random number seed value to make toolbox deterministic.\")\n    parser.add_argument(\"--no_mp3_support\", action=\"store_true\", help=\\\n        \"If True, disallows loading mp3 files to prevent audioread errors when ffmpeg is not installed.\")\n    args = parser.parse_args()\n    print_args(args, parser)\n    if not args.no_sound:\n        import sounddevice as sd\n\n    if args.cpu:\n        # Hide GPUs from Pytorch to force CPU processing\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n    if not args.no_mp3_support:\n        try:\n            librosa.load(\"samples/1320_00000.mp3\")\n        except NoBackendError:\n            print(\"Librosa will be unable to open mp3 files if additional software is not installed.\\n\"\n                  \"Please install ffmpeg or add the '--no_mp3_support' option to proceed without support for mp3 files.\")\n            exit(-1)\n        \n    print(\"Running a test of your configuration...\\n\")\n        \n    if torch.cuda.is_available():\n        device_id = torch.cuda.current_device()\n        gpu_properties = torch.cuda.get_device_properties(device_id)\n        ## Print some environment information (for debugging purposes)\n        print(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n            \"%.1fGb total memory.\\n\" % \n            (torch.cuda.device_count(),\n            device_id,\n            gpu_properties.name,\n            gpu_properties.major,\n            gpu_properties.minor,\n            gpu_properties.total_memory / 1e9))\n    else:\n        print(\"Using CPU for inference.\\n\")\n    \n    ## Remind the user to download pretrained models if needed\n    check_model_paths(encoder_path=args.enc_model_fpath,\n                      synthesizer_path=args.syn_model_fpath,\n                      vocoder_path=args.voc_model_fpath)\n    \n    ## Load the models one by one.\n    print(\"Preparing the encoder, the synthesizer and the vocoder...\")\n    encoder.load_model(args.enc_model_fpath)\n    synthesizer = Synthesizer(args.syn_model_fpath)\n    vocoder.load_model(args.voc_model_fpath)\n    \n    \n    ## Run a test\n    print(\"Testing your configuration with small inputs.\")\n    # Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder's\n    # sampling rate, which may differ.\n    # If you're unfamiliar with digital audio, know that it is encoded as an array of floats \n    # (or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.\n    # The sampling rate is the number of values (samples) recorded per second, it is set to\n    # 16000 for the encoder. Creating an array of length <sampling_rate> will always correspond \n    # to an audio of 1 second.\n    print(\"\\tTesting the encoder...\")\n    encoder.embed_utterance(np.zeros(encoder.sampling_rate))\n    \n    # Create a dummy embedding. You would normally use the embedding that encoder.embed_utterance\n    # returns, but here we're going to make one ourselves just for the sake of showing that it's\n    # possible.\n    embed = np.random.rand(speaker_embedding_size)\n    # Embeddings are L2-normalized (this isn't important here, but if you want to make your own \n    # embeddings it will be).\n    embed /= np.linalg.norm(embed)\n    # The synthesizer can handle multiple inputs with batching. Let's create another embedding to \n    # illustrate that\n    embeds = [embed, np.zeros(speaker_embedding_size)]\n    texts = [\"test 1\", \"test 2\"]\n    print(\"\\tTesting the synthesizer... (loading the model will output a lot of text)\")\n    mels = synthesizer.synthesize_spectrograms(texts, embeds)\n    \n    # The vocoder synthesizes one waveform at a time, but it's more efficient for long ones. We \n    # can concatenate the mel spectrograms to a single one.\n    mel = np.concatenate(mels, axis=1)\n    # The vocoder can take a callback function to display the generation. More on that later. For \n    # now we'll simply hide it like this:\n    no_action = lambda *args: None\n    print(\"\\tTesting the vocoder...\")\n    # For the sake of making this test short, we'll pass a short target length. The target length \n    # is the length of the wav segments that are processed in parallel. E.g. for audio sampled \n    # at 16000 Hertz, a target length of 8000 means that the target audio will be cut in chunks of\n    # 0.5 seconds which will all be generated together. The parameters here are absurdly short, and \n    # that has a detrimental effect on the quality of the audio. The default parameters are \n    # recommended in general.\n    vocoder.infer_waveform(mel, target=200, overlap=50, progress_callback=no_action)\n    \n    print(\"All test passed! You can now synthesize speech.\\n\\n\")\n    \n    \n    ## Interactive speech generation\n    print(\"This is a GUI-less example of interface to SV2TTS. The purpose of this script is to \"\n          \"show how you can interface this project easily with your own. See the source code for \"\n          \"an explanation of what is happening.\\n\")\n    \n    print(\"Interactive generation loop\")\n    num_generated = 0\n    while True:\n        try:\n            # Get the reference audio filepath\n            message = \"Reference voice: enter an audio filepath of a voice to be cloned (mp3, \" \\\n                      \"wav, m4a, flac, ...):\\n\"\n            in_fpath = Path(input(message).replace(\"\\\"\", \"\").replace(\"\\'\", \"\"))\n\n            if in_fpath.suffix.lower() == \".mp3\" and args.no_mp3_support:\n                print(\"Can't Use mp3 files please try again:\")\n                continue\n            ## Computing the embedding\n            # First, we load the wav using the function that the speaker encoder provides. This is \n            # important: there is preprocessing that must be applied.\n            \n            # The following two methods are equivalent:\n            # - Directly load from the filepath:\n            preprocessed_wav = encoder.preprocess_wav(in_fpath)\n            # - If the wav is already loaded:\n            original_wav, sampling_rate = librosa.load(str(in_fpath))\n            preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n            print(\"Loaded file succesfully\")\n            \n            # Then we derive the embedding. There are many functions and parameters that the \n            # speaker encoder interfaces. These are mostly for in-depth research. You will typically\n            # only use this function (with its default parameters):\n            embed = encoder.embed_utterance(preprocessed_wav)\n            print(\"Created the embedding\")\n            \n            \n            ## Generating the spectrogram\n            text = input(\"Write a sentence (+-20 words) to be synthesized:\\n\")\n            \n            # If seed is specified, reset torch seed and force synthesizer reload\n            if args.seed is not None:\n                torch.manual_seed(args.seed)\n                synthesizer = Synthesizer(args.syn_model_fpath)\n\n            # The synthesizer works in batch, so you need to put your data in a list or numpy array\n            texts = [text]\n            embeds = [embed]\n            # If you know what the attention layer alignments are, you can retrieve them here by\n            # passing return_alignments=True\n            specs = synthesizer.synthesize_spectrograms(texts, embeds)\n            spec = specs[0]\n            print(\"Created the mel spectrogram\")\n            \n            \n            ## Generating the waveform\n            print(\"Synthesizing the waveform:\")\n\n            # If seed is specified, reset torch seed and reload vocoder\n            if args.seed is not None:\n                torch.manual_seed(args.seed)\n                vocoder.load_model(args.voc_model_fpath)\n\n            # Synthesizing the waveform is fairly straightforward. Remember that the longer the\n            # spectrogram, the more time-efficient the vocoder.\n            generated_wav = vocoder.infer_waveform(spec)\n            \n            \n            ## Post-generation\n            # There's a bug with sounddevice that makes the audio cut one second earlier, so we\n            # pad it.\n            generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n\n            # Trim excess silences to compensate for gaps in spectrograms (issue #53)\n            generated_wav = encoder.preprocess_wav(generated_wav)\n            \n            # Play the audio (non-blocking)\n            if not args.no_sound:\n                try:\n                    sd.stop()\n                    sd.play(generated_wav, synthesizer.sample_rate)\n                except sd.PortAudioError as e:\n                    print(\"\\nCaught exception: %s\" % repr(e))\n                    print(\"Continuing without audio playback. Suppress this message with the \\\"--no_sound\\\" flag.\\n\")\n                except:\n                    raise\n                \n            # Save it on the disk\n            filename = \"demo_output_%02d.wav\" % num_generated\n            print(generated_wav.dtype)\n            sf.write(filename, generated_wav.astype(np.float32), synthesizer.sample_rate)\n            num_generated += 1\n            print(\"\\nSaved output as %s\\n\\n\" % filename)\n            \n            \n        except Exception as e:\n            print(\"Caught exception: %s\" % repr(e))\n            print(\"Restarting\\n\")\n", "monotonic_align/setup.py": "from distutils.core import setup\nfrom Cython.Build import cythonize\nimport numpy\n\nsetup(\n  name = 'monotonic_align',\n  ext_modules = cythonize(\"core.pyx\"),\n  include_dirs=[numpy.get_include()]\n)\n", "monotonic_align/__init__.py": "import numpy as np\nimport torch\nfrom .monotonic_align.core import maximum_path_c\n\n\ndef maximum_path(neg_cent, mask):\n  \"\"\" Cython optimized version.\n  neg_cent: [b, t_t, t_s]\n  mask: [b, t_t, t_s]\n  \"\"\"\n  device = neg_cent.device\n  dtype = neg_cent.dtype\n  neg_cent = neg_cent.data.cpu().numpy().astype(np.float32)\n  path = np.zeros(neg_cent.shape, dtype=np.int32)\n\n  t_t_max = mask.sum(1)[:, 0].data.cpu().numpy().astype(np.int32)\n  t_s_max = mask.sum(2)[:, 0].data.cpu().numpy().astype(np.int32)\n  maximum_path_c(path, neg_cent, t_t_max, t_s_max)\n  return torch.from_numpy(path).to(device=device, dtype=dtype)\n"}