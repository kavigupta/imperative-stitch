{"pydantic/functional_validators.py": "\"\"\"This module contains related classes and functions for validation.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport sys\nfrom functools import partialmethod\nfrom types import FunctionType\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar, Union, cast, overload\n\nfrom pydantic_core import core_schema\nfrom pydantic_core import core_schema as _core_schema\nfrom typing_extensions import Annotated, Literal, TypeAlias\n\nfrom . import GetCoreSchemaHandler as _GetCoreSchemaHandler\nfrom ._internal import _core_metadata, _decorators, _generics, _internal_dataclass\nfrom .annotated_handlers import GetCoreSchemaHandler\nfrom .errors import PydanticUserError\n\nif sys.version_info < (3, 11):\n    from typing_extensions import Protocol\nelse:\n    from typing import Protocol\n\n_inspect_validator = _decorators.inspect_validator\n\n\n@dataclasses.dataclass(frozen=True, **_internal_dataclass.slots_true)\nclass AfterValidator:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/validators/#annotated-validators\n\n    A metadata class that indicates that a validation should be applied **after** the inner validation logic.\n\n    Attributes:\n        func: The validator function.\n\n    Example:\n        ```py\n        from typing_extensions import Annotated\n\n        from pydantic import AfterValidator, BaseModel, ValidationError\n\n        MyInt = Annotated[int, AfterValidator(lambda v: v + 1)]\n\n        class Model(BaseModel):\n            a: MyInt\n\n        print(Model(a=1).a)\n        #> 2\n\n        try:\n            Model(a='a')\n        except ValidationError as e:\n            print(e.json(indent=2))\n            '''\n            [\n              {\n                \"type\": \"int_parsing\",\n                \"loc\": [\n                  \"a\"\n                ],\n                \"msg\": \"Input should be a valid integer, unable to parse string as an integer\",\n                \"input\": \"a\",\n                \"url\": \"https://errors.pydantic.dev/2/v/int_parsing\"\n              }\n            ]\n            '''\n        ```\n    \"\"\"\n\n    func: core_schema.NoInfoValidatorFunction | core_schema.WithInfoValidatorFunction\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: _GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source_type)\n        info_arg = _inspect_validator(self.func, 'after')\n        if info_arg:\n            func = cast(core_schema.WithInfoValidatorFunction, self.func)\n            return core_schema.with_info_after_validator_function(func, schema=schema, field_name=handler.field_name)\n        else:\n            func = cast(core_schema.NoInfoValidatorFunction, self.func)\n            return core_schema.no_info_after_validator_function(func, schema=schema)\n\n\n@dataclasses.dataclass(frozen=True, **_internal_dataclass.slots_true)\nclass BeforeValidator:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/validators/#annotated-validators\n\n    A metadata class that indicates that a validation should be applied **before** the inner validation logic.\n\n    Attributes:\n        func: The validator function.\n\n    Example:\n        ```py\n        from typing_extensions import Annotated\n\n        from pydantic import BaseModel, BeforeValidator\n\n        MyInt = Annotated[int, BeforeValidator(lambda v: v + 1)]\n\n        class Model(BaseModel):\n            a: MyInt\n\n        print(Model(a=1).a)\n        #> 2\n\n        try:\n            Model(a='a')\n        except TypeError as e:\n            print(e)\n            #> can only concatenate str (not \"int\") to str\n        ```\n    \"\"\"\n\n    func: core_schema.NoInfoValidatorFunction | core_schema.WithInfoValidatorFunction\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: _GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source_type)\n        info_arg = _inspect_validator(self.func, 'before')\n        if info_arg:\n            func = cast(core_schema.WithInfoValidatorFunction, self.func)\n            return core_schema.with_info_before_validator_function(func, schema=schema, field_name=handler.field_name)\n        else:\n            func = cast(core_schema.NoInfoValidatorFunction, self.func)\n            return core_schema.no_info_before_validator_function(func, schema=schema)\n\n\n@dataclasses.dataclass(frozen=True, **_internal_dataclass.slots_true)\nclass PlainValidator:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/validators/#annotated-validators\n\n    A metadata class that indicates that a validation should be applied **instead** of the inner validation logic.\n\n    Attributes:\n        func: The validator function.\n\n    Example:\n        ```py\n        from typing_extensions import Annotated\n\n        from pydantic import BaseModel, PlainValidator\n\n        MyInt = Annotated[int, PlainValidator(lambda v: int(v) + 1)]\n\n        class Model(BaseModel):\n            a: MyInt\n\n        print(Model(a='1').a)\n        #> 2\n        ```\n    \"\"\"\n\n    func: core_schema.NoInfoValidatorFunction | core_schema.WithInfoValidatorFunction\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: _GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        # Note that for some valid uses of PlainValidator, it is not possible to generate a core schema for the\n        # source_type, so calling `handler(source_type)` will error, which prevents us from generating a proper\n        # serialization schema. To work around this for use cases that will not involve serialization, we simply\n        # catch any PydanticSchemaGenerationError that may be raised while attempting to build the serialization schema\n        # and abort any attempts to handle special serialization.\n        from pydantic import PydanticSchemaGenerationError\n\n        try:\n            schema = handler(source_type)\n            serialization = core_schema.wrap_serializer_function_ser_schema(function=lambda v, h: h(v), schema=schema)\n        except PydanticSchemaGenerationError:\n            serialization = None\n\n        info_arg = _inspect_validator(self.func, 'plain')\n        if info_arg:\n            func = cast(core_schema.WithInfoValidatorFunction, self.func)\n            return core_schema.with_info_plain_validator_function(\n                func, field_name=handler.field_name, serialization=serialization\n            )\n        else:\n            func = cast(core_schema.NoInfoValidatorFunction, self.func)\n            return core_schema.no_info_plain_validator_function(func, serialization=serialization)\n\n\n@dataclasses.dataclass(frozen=True, **_internal_dataclass.slots_true)\nclass WrapValidator:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/validators/#annotated-validators\n\n    A metadata class that indicates that a validation should be applied **around** the inner validation logic.\n\n    Attributes:\n        func: The validator function.\n\n    ```py\n    from datetime import datetime\n\n    from typing_extensions import Annotated\n\n    from pydantic import BaseModel, ValidationError, WrapValidator\n\n    def validate_timestamp(v, handler):\n        if v == 'now':\n            # we don't want to bother with further validation, just return the new value\n            return datetime.now()\n        try:\n            return handler(v)\n        except ValidationError:\n            # validation failed, in this case we want to return a default value\n            return datetime(2000, 1, 1)\n\n    MyTimestamp = Annotated[datetime, WrapValidator(validate_timestamp)]\n\n    class Model(BaseModel):\n        a: MyTimestamp\n\n    print(Model(a='now').a)\n    #> 2032-01-02 03:04:05.000006\n    print(Model(a='invalid').a)\n    #> 2000-01-01 00:00:00\n    ```\n    \"\"\"\n\n    func: core_schema.NoInfoWrapValidatorFunction | core_schema.WithInfoWrapValidatorFunction\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: _GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source_type)\n        info_arg = _inspect_validator(self.func, 'wrap')\n        if info_arg:\n            func = cast(core_schema.WithInfoWrapValidatorFunction, self.func)\n            return core_schema.with_info_wrap_validator_function(func, schema=schema, field_name=handler.field_name)\n        else:\n            func = cast(core_schema.NoInfoWrapValidatorFunction, self.func)\n            return core_schema.no_info_wrap_validator_function(func, schema=schema)\n\n\nif TYPE_CHECKING:\n\n    class _OnlyValueValidatorClsMethod(Protocol):\n        def __call__(self, cls: Any, value: Any, /) -> Any: ...\n\n    class _V2ValidatorClsMethod(Protocol):\n        def __call__(self, cls: Any, value: Any, info: _core_schema.ValidationInfo, /) -> Any: ...\n\n    class _V2WrapValidatorClsMethod(Protocol):\n        def __call__(\n            self,\n            cls: Any,\n            value: Any,\n            handler: _core_schema.ValidatorFunctionWrapHandler,\n            info: _core_schema.ValidationInfo,\n            /,\n        ) -> Any: ...\n\n    _V2Validator = Union[\n        _V2ValidatorClsMethod,\n        _core_schema.WithInfoValidatorFunction,\n        _OnlyValueValidatorClsMethod,\n        _core_schema.NoInfoValidatorFunction,\n    ]\n\n    _V2WrapValidator = Union[\n        _V2WrapValidatorClsMethod,\n        _core_schema.WithInfoWrapValidatorFunction,\n    ]\n\n    _PartialClsOrStaticMethod: TypeAlias = Union[classmethod[Any, Any, Any], staticmethod[Any, Any], partialmethod[Any]]\n\n    _V2BeforeAfterOrPlainValidatorType = TypeVar(\n        '_V2BeforeAfterOrPlainValidatorType',\n        _V2Validator,\n        _PartialClsOrStaticMethod,\n    )\n    _V2WrapValidatorType = TypeVar('_V2WrapValidatorType', _V2WrapValidator, _PartialClsOrStaticMethod)\n\n\n@overload\ndef field_validator(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['before', 'after', 'plain'] = ...,\n    check_fields: bool | None = ...,\n) -> Callable[[_V2BeforeAfterOrPlainValidatorType], _V2BeforeAfterOrPlainValidatorType]: ...\n\n\n@overload\ndef field_validator(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['wrap'],\n    check_fields: bool | None = ...,\n) -> Callable[[_V2WrapValidatorType], _V2WrapValidatorType]: ...\n\n\nFieldValidatorModes: TypeAlias = Literal['before', 'after', 'wrap', 'plain']\n\n\ndef field_validator(\n    field: str,\n    /,\n    *fields: str,\n    mode: FieldValidatorModes = 'after',\n    check_fields: bool | None = None,\n) -> Callable[[Any], Any]:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/validators/#field-validators\n\n    Decorate methods on the class indicating that they should be used to validate fields.\n\n    Example usage:\n    ```py\n    from typing import Any\n\n    from pydantic import (\n        BaseModel,\n        ValidationError,\n        field_validator,\n    )\n\n    class Model(BaseModel):\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def ensure_foobar(cls, v: Any):\n            if 'foobar' not in v:\n                raise ValueError('\"foobar\" not found in a')\n            return v\n\n    print(repr(Model(a='this is foobar good')))\n    #> Model(a='this is foobar good')\n\n    try:\n        Model(a='snap')\n    except ValidationError as exc_info:\n        print(exc_info)\n        '''\n        1 validation error for Model\n        a\n          Value error, \"foobar\" not found in a [type=value_error, input_value='snap', input_type=str]\n        '''\n    ```\n\n    For more in depth examples, see [Field Validators](../concepts/validators.md#field-validators).\n\n    Args:\n        field: The first field the `field_validator` should be called on; this is separate\n            from `fields` to ensure an error is raised if you don't pass at least one.\n        *fields: Additional field(s) the `field_validator` should be called on.\n        mode: Specifies whether to validate the fields before or after validation.\n        check_fields: Whether to check that the fields actually exist on the model.\n\n    Returns:\n        A decorator that can be used to decorate a function to be used as a field_validator.\n\n    Raises:\n        PydanticUserError:\n            - If `@field_validator` is used bare (with no fields).\n            - If the args passed to `@field_validator` as fields are not strings.\n            - If `@field_validator` applied to instance methods.\n    \"\"\"\n    if isinstance(field, FunctionType):\n        raise PydanticUserError(\n            '`@field_validator` should be used with fields and keyword arguments, not bare. '\n            \"E.g. usage should be `@validator('<field_name>', ...)`\",\n            code='validator-no-fields',\n        )\n    fields = field, *fields\n    if not all(isinstance(field, str) for field in fields):\n        raise PydanticUserError(\n            '`@field_validator` fields should be passed as separate string args. '\n            \"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`\",\n            code='validator-invalid-fields',\n        )\n\n    def dec(\n        f: Callable[..., Any] | staticmethod[Any, Any] | classmethod[Any, Any, Any],\n    ) -> _decorators.PydanticDescriptorProxy[Any]:\n        if _decorators.is_instance_method_from_sig(f):\n            raise PydanticUserError(\n                '`@field_validator` cannot be applied to instance methods', code='validator-instance-method'\n            )\n\n        # auto apply the @classmethod decorator\n        f = _decorators.ensure_classmethod_based_on_signature(f)\n\n        dec_info = _decorators.FieldValidatorDecoratorInfo(fields=fields, mode=mode, check_fields=check_fields)\n        return _decorators.PydanticDescriptorProxy(f, dec_info)\n\n    return dec\n\n\n_ModelType = TypeVar('_ModelType')\n_ModelTypeCo = TypeVar('_ModelTypeCo', covariant=True)\n\n\nclass ModelWrapValidatorHandler(_core_schema.ValidatorFunctionWrapHandler, Protocol[_ModelTypeCo]):\n    \"\"\"@model_validator decorated function handler argument type. This is used when `mode='wrap'`.\"\"\"\n\n    def __call__(  # noqa: D102\n        self,\n        value: Any,\n        outer_location: str | int | None = None,\n        /,\n    ) -> _ModelTypeCo:  # pragma: no cover\n        ...\n\n\nclass ModelWrapValidatorWithoutInfo(Protocol[_ModelType]):\n    \"\"\"A @model_validator decorated function signature.\n    This is used when `mode='wrap'` and the function does not have info argument.\n    \"\"\"\n\n    def __call__(  # noqa: D102\n        self,\n        cls: type[_ModelType],\n        # this can be a dict, a model instance\n        # or anything else that gets passed to validate_python\n        # thus validators _must_ handle all cases\n        value: Any,\n        handler: ModelWrapValidatorHandler[_ModelType],\n        /,\n    ) -> _ModelType: ...\n\n\nclass ModelWrapValidator(Protocol[_ModelType]):\n    \"\"\"A @model_validator decorated function signature. This is used when `mode='wrap'`.\"\"\"\n\n    def __call__(  # noqa: D102\n        self,\n        cls: type[_ModelType],\n        # this can be a dict, a model instance\n        # or anything else that gets passed to validate_python\n        # thus validators _must_ handle all cases\n        value: Any,\n        handler: ModelWrapValidatorHandler[_ModelType],\n        info: _core_schema.ValidationInfo,\n        /,\n    ) -> _ModelType: ...\n\n\nclass FreeModelBeforeValidatorWithoutInfo(Protocol):\n    \"\"\"A @model_validator decorated function signature.\n    This is used when `mode='before'` and the function does not have info argument.\n    \"\"\"\n\n    def __call__(  # noqa: D102\n        self,\n        # this can be a dict, a model instance\n        # or anything else that gets passed to validate_python\n        # thus validators _must_ handle all cases\n        value: Any,\n        /,\n    ) -> Any: ...\n\n\nclass ModelBeforeValidatorWithoutInfo(Protocol):\n    \"\"\"A @model_validator decorated function signature.\n    This is used when `mode='before'` and the function does not have info argument.\n    \"\"\"\n\n    def __call__(  # noqa: D102\n        self,\n        cls: Any,\n        # this can be a dict, a model instance\n        # or anything else that gets passed to validate_python\n        # thus validators _must_ handle all cases\n        value: Any,\n        /,\n    ) -> Any: ...\n\n\nclass FreeModelBeforeValidator(Protocol):\n    \"\"\"A `@model_validator` decorated function signature. This is used when `mode='before'`.\"\"\"\n\n    def __call__(  # noqa: D102\n        self,\n        # this can be a dict, a model instance\n        # or anything else that gets passed to validate_python\n        # thus validators _must_ handle all cases\n        value: Any,\n        info: _core_schema.ValidationInfo,\n        /,\n    ) -> Any: ...\n\n\nclass ModelBeforeValidator(Protocol):\n    \"\"\"A `@model_validator` decorated function signature. This is used when `mode='before'`.\"\"\"\n\n    def __call__(  # noqa: D102\n        self,\n        cls: Any,\n        # this can be a dict, a model instance\n        # or anything else that gets passed to validate_python\n        # thus validators _must_ handle all cases\n        value: Any,\n        info: _core_schema.ValidationInfo,\n        /,\n    ) -> Any: ...\n\n\nModelAfterValidatorWithoutInfo = Callable[[_ModelType], _ModelType]\n\"\"\"A `@model_validator` decorated function signature. This is used when `mode='after'` and the function does not\nhave info argument.\n\"\"\"\n\nModelAfterValidator = Callable[[_ModelType, _core_schema.ValidationInfo], _ModelType]\n\"\"\"A `@model_validator` decorated function signature. This is used when `mode='after'`.\"\"\"\n\n_AnyModelWrapValidator = Union[ModelWrapValidator[_ModelType], ModelWrapValidatorWithoutInfo[_ModelType]]\n_AnyModeBeforeValidator = Union[\n    FreeModelBeforeValidator, ModelBeforeValidator, FreeModelBeforeValidatorWithoutInfo, ModelBeforeValidatorWithoutInfo\n]\n_AnyModelAfterValidator = Union[ModelAfterValidator[_ModelType], ModelAfterValidatorWithoutInfo[_ModelType]]\n\n\n@overload\ndef model_validator(\n    *,\n    mode: Literal['wrap'],\n) -> Callable[\n    [_AnyModelWrapValidator[_ModelType]], _decorators.PydanticDescriptorProxy[_decorators.ModelValidatorDecoratorInfo]\n]: ...\n\n\n@overload\ndef model_validator(\n    *,\n    mode: Literal['before'],\n) -> Callable[\n    [_AnyModeBeforeValidator], _decorators.PydanticDescriptorProxy[_decorators.ModelValidatorDecoratorInfo]\n]: ...\n\n\n@overload\ndef model_validator(\n    *,\n    mode: Literal['after'],\n) -> Callable[\n    [_AnyModelAfterValidator[_ModelType]], _decorators.PydanticDescriptorProxy[_decorators.ModelValidatorDecoratorInfo]\n]: ...\n\n\ndef model_validator(\n    *,\n    mode: Literal['wrap', 'before', 'after'],\n) -> Any:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/validators/#model-validators\n\n    Decorate model methods for validation purposes.\n\n    Example usage:\n    ```py\n    from typing_extensions import Self\n\n    from pydantic import BaseModel, ValidationError, model_validator\n\n    class Square(BaseModel):\n        width: float\n        height: float\n\n        @model_validator(mode='after')\n        def verify_square(self) -> Self:\n            if self.width != self.height:\n                raise ValueError('width and height do not match')\n            return self\n\n    s = Square(width=1, height=1)\n    print(repr(s))\n    #> Square(width=1.0, height=1.0)\n\n    try:\n        Square(width=1, height=2)\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Square\n          Value error, width and height do not match [type=value_error, input_value={'width': 1, 'height': 2}, input_type=dict]\n        '''\n    ```\n\n    For more in depth examples, see [Model Validators](../concepts/validators.md#model-validators).\n\n    Args:\n        mode: A required string literal that specifies the validation mode.\n            It can be one of the following: 'wrap', 'before', or 'after'.\n\n    Returns:\n        A decorator that can be used to decorate a function to be used as a model validator.\n    \"\"\"\n\n    def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n        # auto apply the @classmethod decorator\n        f = _decorators.ensure_classmethod_based_on_signature(f)\n        dec_info = _decorators.ModelValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(f, dec_info)\n\n    return dec\n\n\nAnyType = TypeVar('AnyType')\n\n\nif TYPE_CHECKING:\n    # If we add configurable attributes to IsInstance, we'd probably need to stop hiding it from type checkers like this\n    InstanceOf = Annotated[AnyType, ...]  # `IsInstance[Sequence]` will be recognized by type checkers as `Sequence`\n\nelse:\n\n    @dataclasses.dataclass(**_internal_dataclass.slots_true)\n    class InstanceOf:\n        '''Generic type for annotating a type that is an instance of a given class.\n\n        Example:\n            ```py\n            from pydantic import BaseModel, InstanceOf\n\n            class Foo:\n                ...\n\n            class Bar(BaseModel):\n                foo: InstanceOf[Foo]\n\n            Bar(foo=Foo())\n            try:\n                Bar(foo=42)\n            except ValidationError as e:\n                print(e)\n                \"\"\"\n                [\n                \u2502   {\n                \u2502   \u2502   'type': 'is_instance_of',\n                \u2502   \u2502   'loc': ('foo',),\n                \u2502   \u2502   'msg': 'Input should be an instance of Foo',\n                \u2502   \u2502   'input': 42,\n                \u2502   \u2502   'ctx': {'class': 'Foo'},\n                \u2502   \u2502   'url': 'https://errors.pydantic.dev/0.38.0/v/is_instance_of'\n                \u2502   }\n                ]\n                \"\"\"\n            ```\n        '''\n\n        @classmethod\n        def __class_getitem__(cls, item: AnyType) -> AnyType:\n            return Annotated[item, cls()]\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            from pydantic import PydanticSchemaGenerationError\n\n            # use the generic _origin_ as the second argument to isinstance when appropriate\n            instance_of_schema = core_schema.is_instance_schema(_generics.get_origin(source) or source)\n\n            try:\n                # Try to generate the \"standard\" schema, which will be used when loading from JSON\n                original_schema = handler(source)\n            except PydanticSchemaGenerationError:\n                # If that fails, just produce a schema that can validate from python\n                return instance_of_schema\n            else:\n                # Use the \"original\" approach to serialization\n                instance_of_schema['serialization'] = core_schema.wrap_serializer_function_ser_schema(\n                    function=lambda v, h: h(v), schema=original_schema\n                )\n                return core_schema.json_or_python_schema(python_schema=instance_of_schema, json_schema=original_schema)\n\n        __hash__ = object.__hash__\n\n\nif TYPE_CHECKING:\n    SkipValidation = Annotated[AnyType, ...]  # SkipValidation[list[str]] will be treated by type checkers as list[str]\nelse:\n\n    @dataclasses.dataclass(**_internal_dataclass.slots_true)\n    class SkipValidation:\n        \"\"\"If this is applied as an annotation (e.g., via `x: Annotated[int, SkipValidation]`), validation will be\n            skipped. You can also use `SkipValidation[int]` as a shorthand for `Annotated[int, SkipValidation]`.\n\n        This can be useful if you want to use a type annotation for documentation/IDE/type-checking purposes,\n        and know that it is safe to skip validation for one or more of the fields.\n\n        Because this converts the validation schema to `any_schema`, subsequent annotation-applied transformations\n        may not have the expected effects. Therefore, when used, this annotation should generally be the final\n        annotation applied to a type.\n        \"\"\"\n\n        def __class_getitem__(cls, item: Any) -> Any:\n            return Annotated[item, SkipValidation()]\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            original_schema = handler(source)\n            metadata = _core_metadata.build_metadata_dict(js_annotation_functions=[lambda _c, h: h(original_schema)])\n            return core_schema.any_schema(\n                metadata=metadata,\n                serialization=core_schema.wrap_serializer_function_ser_schema(\n                    function=lambda v, h: h(v), schema=original_schema\n                ),\n            )\n\n        __hash__ = object.__hash__\n", "pydantic/generics.py": "\"\"\"The `generics` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/color.py": "\"\"\"Color definitions are used as per the CSS3\n[CSS Color Module Level 3](http://www.w3.org/TR/css3-color/#svg-color) specification.\n\nA few colors have multiple names referring to the sames colors, eg. `grey` and `gray` or `aqua` and `cyan`.\n\nIn these cases the _last_ color when sorted alphabetically takes preferences,\neg. `Color((0, 255, 255)).as_named() == 'cyan'` because \"cyan\" comes after \"aqua\".\n\nWarning: Deprecated\n    The `Color` class is deprecated, use `pydantic_extra_types` instead.\n    See [`pydantic-extra-types.Color`](../usage/types/extra_types/color_types.md)\n    for more information.\n\"\"\"\n\nimport math\nimport re\nfrom colorsys import hls_to_rgb, rgb_to_hls\nfrom typing import Any, Callable, Optional, Tuple, Type, Union, cast\n\nfrom pydantic_core import CoreSchema, PydanticCustomError, core_schema\nfrom typing_extensions import deprecated\n\nfrom ._internal import _repr\nfrom ._internal._schema_generation_shared import GetJsonSchemaHandler as _GetJsonSchemaHandler\nfrom .json_schema import JsonSchemaValue\nfrom .warnings import PydanticDeprecatedSince20\n\nColorTuple = Union[Tuple[int, int, int], Tuple[int, int, int, float]]\nColorType = Union[ColorTuple, str]\nHslColorTuple = Union[Tuple[float, float, float], Tuple[float, float, float, float]]\n\n\nclass RGBA:\n    \"\"\"Internal use only as a representation of a color.\"\"\"\n\n    __slots__ = 'r', 'g', 'b', 'alpha', '_tuple'\n\n    def __init__(self, r: float, g: float, b: float, alpha: Optional[float]):\n        self.r = r\n        self.g = g\n        self.b = b\n        self.alpha = alpha\n\n        self._tuple: Tuple[float, float, float, Optional[float]] = (r, g, b, alpha)\n\n    def __getitem__(self, item: Any) -> Any:\n        return self._tuple[item]\n\n\n# these are not compiled here to avoid import slowdown, they'll be compiled the first time they're used, then cached\n_r_255 = r'(\\d{1,3}(?:\\.\\d+)?)'\n_r_comma = r'\\s*,\\s*'\n_r_alpha = r'(\\d(?:\\.\\d+)?|\\.\\d+|\\d{1,2}%)'\n_r_h = r'(-?\\d+(?:\\.\\d+)?|-?\\.\\d+)(deg|rad|turn)?'\n_r_sl = r'(\\d{1,3}(?:\\.\\d+)?)%'\nr_hex_short = r'\\s*(?:#|0x)?([0-9a-f])([0-9a-f])([0-9a-f])([0-9a-f])?\\s*'\nr_hex_long = r'\\s*(?:#|0x)?([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})?\\s*'\n# CSS3 RGB examples: rgb(0, 0, 0), rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 50%)\nr_rgb = rf'\\s*rgba?\\(\\s*{_r_255}{_r_comma}{_r_255}{_r_comma}{_r_255}(?:{_r_comma}{_r_alpha})?\\s*\\)\\s*'\n# CSS3 HSL examples: hsl(270, 60%, 50%), hsla(270, 60%, 50%, 0.5), hsla(270, 60%, 50%, 50%)\nr_hsl = rf'\\s*hsla?\\(\\s*{_r_h}{_r_comma}{_r_sl}{_r_comma}{_r_sl}(?:{_r_comma}{_r_alpha})?\\s*\\)\\s*'\n# CSS4 RGB examples: rgb(0 0 0), rgb(0 0 0 / 0.5), rgb(0 0 0 / 50%), rgba(0 0 0 / 50%)\nr_rgb_v4_style = rf'\\s*rgba?\\(\\s*{_r_255}\\s+{_r_255}\\s+{_r_255}(?:\\s*/\\s*{_r_alpha})?\\s*\\)\\s*'\n# CSS4 HSL examples: hsl(270 60% 50%), hsl(270 60% 50% / 0.5), hsl(270 60% 50% / 50%), hsla(270 60% 50% / 50%)\nr_hsl_v4_style = rf'\\s*hsla?\\(\\s*{_r_h}\\s+{_r_sl}\\s+{_r_sl}(?:\\s*/\\s*{_r_alpha})?\\s*\\)\\s*'\n\n# colors where the two hex characters are the same, if all colors match this the short version of hex colors can be used\nrepeat_colors = {int(c * 2, 16) for c in '0123456789abcdef'}\nrads = 2 * math.pi\n\n\n@deprecated(\n    'The `Color` class is deprecated, use `pydantic_extra_types` instead. '\n    'See https://docs.pydantic.dev/latest/api/pydantic_extra_types_color/.',\n    category=PydanticDeprecatedSince20,\n)\nclass Color(_repr.Representation):\n    \"\"\"Represents a color.\"\"\"\n\n    __slots__ = '_original', '_rgba'\n\n    def __init__(self, value: ColorType) -> None:\n        self._rgba: RGBA\n        self._original: ColorType\n        if isinstance(value, (tuple, list)):\n            self._rgba = parse_tuple(value)\n        elif isinstance(value, str):\n            self._rgba = parse_str(value)\n        elif isinstance(value, Color):\n            self._rgba = value._rgba\n            value = value._original\n        else:\n            raise PydanticCustomError(\n                'color_error', 'value is not a valid color: value must be a tuple, list or string'\n            )\n\n        # if we've got here value must be a valid color\n        self._original = value\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: core_schema.CoreSchema, handler: _GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = {}\n        field_schema.update(type='string', format='color')\n        return field_schema\n\n    def original(self) -> ColorType:\n        \"\"\"Original value passed to `Color`.\"\"\"\n        return self._original\n\n    def as_named(self, *, fallback: bool = False) -> str:\n        \"\"\"Returns the name of the color if it can be found in `COLORS_BY_VALUE` dictionary,\n        otherwise returns the hexadecimal representation of the color or raises `ValueError`.\n\n        Args:\n            fallback: If True, falls back to returning the hexadecimal representation of\n                the color instead of raising a ValueError when no named color is found.\n\n        Returns:\n            The name of the color, or the hexadecimal representation of the color.\n\n        Raises:\n            ValueError: When no named color is found and fallback is `False`.\n        \"\"\"\n        if self._rgba.alpha is None:\n            rgb = cast(Tuple[int, int, int], self.as_rgb_tuple())\n            try:\n                return COLORS_BY_VALUE[rgb]\n            except KeyError as e:\n                if fallback:\n                    return self.as_hex()\n                else:\n                    raise ValueError('no named color found, use fallback=True, as_hex() or as_rgb()') from e\n        else:\n            return self.as_hex()\n\n    def as_hex(self) -> str:\n        \"\"\"Returns the hexadecimal representation of the color.\n\n        Hex string representing the color can be 3, 4, 6, or 8 characters depending on whether the string\n        a \"short\" representation of the color is possible and whether there's an alpha channel.\n\n        Returns:\n            The hexadecimal representation of the color.\n        \"\"\"\n        values = [float_to_255(c) for c in self._rgba[:3]]\n        if self._rgba.alpha is not None:\n            values.append(float_to_255(self._rgba.alpha))\n\n        as_hex = ''.join(f'{v:02x}' for v in values)\n        if all(c in repeat_colors for c in values):\n            as_hex = ''.join(as_hex[c] for c in range(0, len(as_hex), 2))\n        return '#' + as_hex\n\n    def as_rgb(self) -> str:\n        \"\"\"Color as an `rgb(<r>, <g>, <b>)` or `rgba(<r>, <g>, <b>, <a>)` string.\"\"\"\n        if self._rgba.alpha is None:\n            return f'rgb({float_to_255(self._rgba.r)}, {float_to_255(self._rgba.g)}, {float_to_255(self._rgba.b)})'\n        else:\n            return (\n                f'rgba({float_to_255(self._rgba.r)}, {float_to_255(self._rgba.g)}, {float_to_255(self._rgba.b)}, '\n                f'{round(self._alpha_float(), 2)})'\n            )\n\n    def as_rgb_tuple(self, *, alpha: Optional[bool] = None) -> ColorTuple:\n        \"\"\"Returns the color as an RGB or RGBA tuple.\n\n        Args:\n            alpha: Whether to include the alpha channel. There are three options for this input:\n\n                - `None` (default): Include alpha only if it's set. (e.g. not `None`)\n                - `True`: Always include alpha.\n                - `False`: Always omit alpha.\n\n        Returns:\n            A tuple that contains the values of the red, green, and blue channels in the range 0 to 255.\n                If alpha is included, it is in the range 0 to 1.\n        \"\"\"\n        r, g, b = (float_to_255(c) for c in self._rgba[:3])\n        if alpha is None:\n            if self._rgba.alpha is None:\n                return r, g, b\n            else:\n                return r, g, b, self._alpha_float()\n        elif alpha:\n            return r, g, b, self._alpha_float()\n        else:\n            # alpha is False\n            return r, g, b\n\n    def as_hsl(self) -> str:\n        \"\"\"Color as an `hsl(<h>, <s>, <l>)` or `hsl(<h>, <s>, <l>, <a>)` string.\"\"\"\n        if self._rgba.alpha is None:\n            h, s, li = self.as_hsl_tuple(alpha=False)  # type: ignore\n            return f'hsl({h * 360:0.0f}, {s:0.0%}, {li:0.0%})'\n        else:\n            h, s, li, a = self.as_hsl_tuple(alpha=True)  # type: ignore\n            return f'hsl({h * 360:0.0f}, {s:0.0%}, {li:0.0%}, {round(a, 2)})'\n\n    def as_hsl_tuple(self, *, alpha: Optional[bool] = None) -> HslColorTuple:\n        \"\"\"Returns the color as an HSL or HSLA tuple.\n\n        Args:\n            alpha: Whether to include the alpha channel.\n\n                - `None` (default): Include the alpha channel only if it's set (e.g. not `None`).\n                - `True`: Always include alpha.\n                - `False`: Always omit alpha.\n\n        Returns:\n            The color as a tuple of hue, saturation, lightness, and alpha (if included).\n                All elements are in the range 0 to 1.\n\n        Note:\n            This is HSL as used in HTML and most other places, not HLS as used in Python's `colorsys`.\n        \"\"\"\n        h, l, s = rgb_to_hls(self._rgba.r, self._rgba.g, self._rgba.b)  # noqa: E741\n        if alpha is None:\n            if self._rgba.alpha is None:\n                return h, s, l\n            else:\n                return h, s, l, self._alpha_float()\n        if alpha:\n            return h, s, l, self._alpha_float()\n        else:\n            # alpha is False\n            return h, s, l\n\n    def _alpha_float(self) -> float:\n        return 1 if self._rgba.alpha is None else self._rgba.alpha\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source: Type[Any], handler: Callable[[Any], CoreSchema]\n    ) -> core_schema.CoreSchema:\n        return core_schema.with_info_plain_validator_function(\n            cls._validate, serialization=core_schema.to_string_ser_schema()\n        )\n\n    @classmethod\n    def _validate(cls, __input_value: Any, _: Any) -> 'Color':\n        return cls(__input_value)\n\n    def __str__(self) -> str:\n        return self.as_named(fallback=True)\n\n    def __repr_args__(self) -> '_repr.ReprArgs':\n        return [(None, self.as_named(fallback=True))] + [('rgb', self.as_rgb_tuple())]\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, Color) and self.as_rgb_tuple() == other.as_rgb_tuple()\n\n    def __hash__(self) -> int:\n        return hash(self.as_rgb_tuple())\n\n\ndef parse_tuple(value: Tuple[Any, ...]) -> RGBA:\n    \"\"\"Parse a tuple or list to get RGBA values.\n\n    Args:\n        value: A tuple or list.\n\n    Returns:\n        An `RGBA` tuple parsed from the input tuple.\n\n    Raises:\n        PydanticCustomError: If tuple is not valid.\n    \"\"\"\n    if len(value) == 3:\n        r, g, b = (parse_color_value(v) for v in value)\n        return RGBA(r, g, b, None)\n    elif len(value) == 4:\n        r, g, b = (parse_color_value(v) for v in value[:3])\n        return RGBA(r, g, b, parse_float_alpha(value[3]))\n    else:\n        raise PydanticCustomError('color_error', 'value is not a valid color: tuples must have length 3 or 4')\n\n\ndef parse_str(value: str) -> RGBA:\n    \"\"\"Parse a string representing a color to an RGBA tuple.\n\n    Possible formats for the input string include:\n\n    * named color, see `COLORS_BY_NAME`\n    * hex short eg. `<prefix>fff` (prefix can be `#`, `0x` or nothing)\n    * hex long eg. `<prefix>ffffff` (prefix can be `#`, `0x` or nothing)\n    * `rgb(<r>, <g>, <b>)`\n    * `rgba(<r>, <g>, <b>, <a>)`\n\n    Args:\n        value: A string representing a color.\n\n    Returns:\n        An `RGBA` tuple parsed from the input string.\n\n    Raises:\n        ValueError: If the input string cannot be parsed to an RGBA tuple.\n    \"\"\"\n    value_lower = value.lower()\n    try:\n        r, g, b = COLORS_BY_NAME[value_lower]\n    except KeyError:\n        pass\n    else:\n        return ints_to_rgba(r, g, b, None)\n\n    m = re.fullmatch(r_hex_short, value_lower)\n    if m:\n        *rgb, a = m.groups()\n        r, g, b = (int(v * 2, 16) for v in rgb)\n        if a:\n            alpha: Optional[float] = int(a * 2, 16) / 255\n        else:\n            alpha = None\n        return ints_to_rgba(r, g, b, alpha)\n\n    m = re.fullmatch(r_hex_long, value_lower)\n    if m:\n        *rgb, a = m.groups()\n        r, g, b = (int(v, 16) for v in rgb)\n        if a:\n            alpha = int(a, 16) / 255\n        else:\n            alpha = None\n        return ints_to_rgba(r, g, b, alpha)\n\n    m = re.fullmatch(r_rgb, value_lower) or re.fullmatch(r_rgb_v4_style, value_lower)\n    if m:\n        return ints_to_rgba(*m.groups())  # type: ignore\n\n    m = re.fullmatch(r_hsl, value_lower) or re.fullmatch(r_hsl_v4_style, value_lower)\n    if m:\n        return parse_hsl(*m.groups())  # type: ignore\n\n    raise PydanticCustomError('color_error', 'value is not a valid color: string not recognised as a valid color')\n\n\ndef ints_to_rgba(r: Union[int, str], g: Union[int, str], b: Union[int, str], alpha: Optional[float] = None) -> RGBA:\n    \"\"\"Converts integer or string values for RGB color and an optional alpha value to an `RGBA` object.\n\n    Args:\n        r: An integer or string representing the red color value.\n        g: An integer or string representing the green color value.\n        b: An integer or string representing the blue color value.\n        alpha: A float representing the alpha value. Defaults to None.\n\n    Returns:\n        An instance of the `RGBA` class with the corresponding color and alpha values.\n    \"\"\"\n    return RGBA(parse_color_value(r), parse_color_value(g), parse_color_value(b), parse_float_alpha(alpha))\n\n\ndef parse_color_value(value: Union[int, str], max_val: int = 255) -> float:\n    \"\"\"Parse the color value provided and return a number between 0 and 1.\n\n    Args:\n        value: An integer or string color value.\n        max_val: Maximum range value. Defaults to 255.\n\n    Raises:\n        PydanticCustomError: If the value is not a valid color.\n\n    Returns:\n        A number between 0 and 1.\n    \"\"\"\n    try:\n        color = float(value)\n    except ValueError:\n        raise PydanticCustomError('color_error', 'value is not a valid color: color values must be a valid number')\n    if 0 <= color <= max_val:\n        return color / max_val\n    else:\n        raise PydanticCustomError(\n            'color_error',\n            'value is not a valid color: color values must be in the range 0 to {max_val}',\n            {'max_val': max_val},\n        )\n\n\ndef parse_float_alpha(value: Union[None, str, float, int]) -> Optional[float]:\n    \"\"\"Parse an alpha value checking it's a valid float in the range 0 to 1.\n\n    Args:\n        value: The input value to parse.\n\n    Returns:\n        The parsed value as a float, or `None` if the value was None or equal 1.\n\n    Raises:\n        PydanticCustomError: If the input value cannot be successfully parsed as a float in the expected range.\n    \"\"\"\n    if value is None:\n        return None\n    try:\n        if isinstance(value, str) and value.endswith('%'):\n            alpha = float(value[:-1]) / 100\n        else:\n            alpha = float(value)\n    except ValueError:\n        raise PydanticCustomError('color_error', 'value is not a valid color: alpha values must be a valid float')\n\n    if math.isclose(alpha, 1):\n        return None\n    elif 0 <= alpha <= 1:\n        return alpha\n    else:\n        raise PydanticCustomError('color_error', 'value is not a valid color: alpha values must be in the range 0 to 1')\n\n\ndef parse_hsl(h: str, h_units: str, sat: str, light: str, alpha: Optional[float] = None) -> RGBA:\n    \"\"\"Parse raw hue, saturation, lightness, and alpha values and convert to RGBA.\n\n    Args:\n        h: The hue value.\n        h_units: The unit for hue value.\n        sat: The saturation value.\n        light: The lightness value.\n        alpha: Alpha value.\n\n    Returns:\n        An instance of `RGBA`.\n    \"\"\"\n    s_value, l_value = parse_color_value(sat, 100), parse_color_value(light, 100)\n\n    h_value = float(h)\n    if h_units in {None, 'deg'}:\n        h_value = h_value % 360 / 360\n    elif h_units == 'rad':\n        h_value = h_value % rads / rads\n    else:\n        # turns\n        h_value = h_value % 1\n\n    r, g, b = hls_to_rgb(h_value, l_value, s_value)\n    return RGBA(r, g, b, parse_float_alpha(alpha))\n\n\ndef float_to_255(c: float) -> int:\n    \"\"\"Converts a float value between 0 and 1 (inclusive) to an integer between 0 and 255 (inclusive).\n\n    Args:\n        c: The float value to be converted. Must be between 0 and 1 (inclusive).\n\n    Returns:\n        The integer equivalent of the given float value rounded to the nearest whole number.\n\n    Raises:\n        ValueError: If the given float value is outside the acceptable range of 0 to 1 (inclusive).\n    \"\"\"\n    return int(round(c * 255))\n\n\nCOLORS_BY_NAME = {\n    'aliceblue': (240, 248, 255),\n    'antiquewhite': (250, 235, 215),\n    'aqua': (0, 255, 255),\n    'aquamarine': (127, 255, 212),\n    'azure': (240, 255, 255),\n    'beige': (245, 245, 220),\n    'bisque': (255, 228, 196),\n    'black': (0, 0, 0),\n    'blanchedalmond': (255, 235, 205),\n    'blue': (0, 0, 255),\n    'blueviolet': (138, 43, 226),\n    'brown': (165, 42, 42),\n    'burlywood': (222, 184, 135),\n    'cadetblue': (95, 158, 160),\n    'chartreuse': (127, 255, 0),\n    'chocolate': (210, 105, 30),\n    'coral': (255, 127, 80),\n    'cornflowerblue': (100, 149, 237),\n    'cornsilk': (255, 248, 220),\n    'crimson': (220, 20, 60),\n    'cyan': (0, 255, 255),\n    'darkblue': (0, 0, 139),\n    'darkcyan': (0, 139, 139),\n    'darkgoldenrod': (184, 134, 11),\n    'darkgray': (169, 169, 169),\n    'darkgreen': (0, 100, 0),\n    'darkgrey': (169, 169, 169),\n    'darkkhaki': (189, 183, 107),\n    'darkmagenta': (139, 0, 139),\n    'darkolivegreen': (85, 107, 47),\n    'darkorange': (255, 140, 0),\n    'darkorchid': (153, 50, 204),\n    'darkred': (139, 0, 0),\n    'darksalmon': (233, 150, 122),\n    'darkseagreen': (143, 188, 143),\n    'darkslateblue': (72, 61, 139),\n    'darkslategray': (47, 79, 79),\n    'darkslategrey': (47, 79, 79),\n    'darkturquoise': (0, 206, 209),\n    'darkviolet': (148, 0, 211),\n    'deeppink': (255, 20, 147),\n    'deepskyblue': (0, 191, 255),\n    'dimgray': (105, 105, 105),\n    'dimgrey': (105, 105, 105),\n    'dodgerblue': (30, 144, 255),\n    'firebrick': (178, 34, 34),\n    'floralwhite': (255, 250, 240),\n    'forestgreen': (34, 139, 34),\n    'fuchsia': (255, 0, 255),\n    'gainsboro': (220, 220, 220),\n    'ghostwhite': (248, 248, 255),\n    'gold': (255, 215, 0),\n    'goldenrod': (218, 165, 32),\n    'gray': (128, 128, 128),\n    'green': (0, 128, 0),\n    'greenyellow': (173, 255, 47),\n    'grey': (128, 128, 128),\n    'honeydew': (240, 255, 240),\n    'hotpink': (255, 105, 180),\n    'indianred': (205, 92, 92),\n    'indigo': (75, 0, 130),\n    'ivory': (255, 255, 240),\n    'khaki': (240, 230, 140),\n    'lavender': (230, 230, 250),\n    'lavenderblush': (255, 240, 245),\n    'lawngreen': (124, 252, 0),\n    'lemonchiffon': (255, 250, 205),\n    'lightblue': (173, 216, 230),\n    'lightcoral': (240, 128, 128),\n    'lightcyan': (224, 255, 255),\n    'lightgoldenrodyellow': (250, 250, 210),\n    'lightgray': (211, 211, 211),\n    'lightgreen': (144, 238, 144),\n    'lightgrey': (211, 211, 211),\n    'lightpink': (255, 182, 193),\n    'lightsalmon': (255, 160, 122),\n    'lightseagreen': (32, 178, 170),\n    'lightskyblue': (135, 206, 250),\n    'lightslategray': (119, 136, 153),\n    'lightslategrey': (119, 136, 153),\n    'lightsteelblue': (176, 196, 222),\n    'lightyellow': (255, 255, 224),\n    'lime': (0, 255, 0),\n    'limegreen': (50, 205, 50),\n    'linen': (250, 240, 230),\n    'magenta': (255, 0, 255),\n    'maroon': (128, 0, 0),\n    'mediumaquamarine': (102, 205, 170),\n    'mediumblue': (0, 0, 205),\n    'mediumorchid': (186, 85, 211),\n    'mediumpurple': (147, 112, 219),\n    'mediumseagreen': (60, 179, 113),\n    'mediumslateblue': (123, 104, 238),\n    'mediumspringgreen': (0, 250, 154),\n    'mediumturquoise': (72, 209, 204),\n    'mediumvioletred': (199, 21, 133),\n    'midnightblue': (25, 25, 112),\n    'mintcream': (245, 255, 250),\n    'mistyrose': (255, 228, 225),\n    'moccasin': (255, 228, 181),\n    'navajowhite': (255, 222, 173),\n    'navy': (0, 0, 128),\n    'oldlace': (253, 245, 230),\n    'olive': (128, 128, 0),\n    'olivedrab': (107, 142, 35),\n    'orange': (255, 165, 0),\n    'orangered': (255, 69, 0),\n    'orchid': (218, 112, 214),\n    'palegoldenrod': (238, 232, 170),\n    'palegreen': (152, 251, 152),\n    'paleturquoise': (175, 238, 238),\n    'palevioletred': (219, 112, 147),\n    'papayawhip': (255, 239, 213),\n    'peachpuff': (255, 218, 185),\n    'peru': (205, 133, 63),\n    'pink': (255, 192, 203),\n    'plum': (221, 160, 221),\n    'powderblue': (176, 224, 230),\n    'purple': (128, 0, 128),\n    'red': (255, 0, 0),\n    'rosybrown': (188, 143, 143),\n    'royalblue': (65, 105, 225),\n    'saddlebrown': (139, 69, 19),\n    'salmon': (250, 128, 114),\n    'sandybrown': (244, 164, 96),\n    'seagreen': (46, 139, 87),\n    'seashell': (255, 245, 238),\n    'sienna': (160, 82, 45),\n    'silver': (192, 192, 192),\n    'skyblue': (135, 206, 235),\n    'slateblue': (106, 90, 205),\n    'slategray': (112, 128, 144),\n    'slategrey': (112, 128, 144),\n    'snow': (255, 250, 250),\n    'springgreen': (0, 255, 127),\n    'steelblue': (70, 130, 180),\n    'tan': (210, 180, 140),\n    'teal': (0, 128, 128),\n    'thistle': (216, 191, 216),\n    'tomato': (255, 99, 71),\n    'turquoise': (64, 224, 208),\n    'violet': (238, 130, 238),\n    'wheat': (245, 222, 179),\n    'white': (255, 255, 255),\n    'whitesmoke': (245, 245, 245),\n    'yellow': (255, 255, 0),\n    'yellowgreen': (154, 205, 50),\n}\n\nCOLORS_BY_VALUE = {v: k for k, v in COLORS_BY_NAME.items()}\n", "pydantic/config.py": "\"\"\"Configuration for Pydantic models.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Type, TypeVar, Union\n\nfrom typing_extensions import Literal, TypeAlias, TypedDict\n\nfrom ._migration import getattr_migration\nfrom .aliases import AliasGenerator\nfrom .errors import PydanticUserError\n\nif TYPE_CHECKING:\n    from ._internal._generate_schema import GenerateSchema as _GenerateSchema\n    from .fields import ComputedFieldInfo, FieldInfo\n\n__all__ = ('ConfigDict', 'with_config')\n\n\nJsonValue: TypeAlias = Union[int, float, str, bool, None, List['JsonValue'], 'JsonDict']\nJsonDict: TypeAlias = Dict[str, JsonValue]\n\nJsonEncoder = Callable[[Any], Any]\n\nJsonSchemaExtraCallable: TypeAlias = Union[\n    Callable[[JsonDict], None],\n    Callable[[JsonDict, Type[Any]], None],\n]\n\nExtraValues = Literal['allow', 'ignore', 'forbid']\n\n\nclass ConfigDict(TypedDict, total=False):\n    \"\"\"A TypedDict for configuring Pydantic behaviour.\"\"\"\n\n    title: str | None\n    \"\"\"The title for the generated JSON schema, defaults to the model's name\"\"\"\n\n    model_title_generator: Callable[[type], str] | None\n    \"\"\"A callable that takes a model class and returns the title for it. Defaults to `None`.\"\"\"\n\n    field_title_generator: Callable[[str, FieldInfo | ComputedFieldInfo], str] | None\n    \"\"\"A callable that takes a field's name and info and returns title for it. Defaults to `None`.\"\"\"\n\n    str_to_lower: bool\n    \"\"\"Whether to convert all characters to lowercase for str types. Defaults to `False`.\"\"\"\n\n    str_to_upper: bool\n    \"\"\"Whether to convert all characters to uppercase for str types. Defaults to `False`.\"\"\"\n\n    str_strip_whitespace: bool\n    \"\"\"Whether to strip leading and trailing whitespace for str types.\"\"\"\n\n    str_min_length: int\n    \"\"\"The minimum length for str types. Defaults to `None`.\"\"\"\n\n    str_max_length: int | None\n    \"\"\"The maximum length for str types. Defaults to `None`.\"\"\"\n\n    extra: ExtraValues | None\n    \"\"\"\n    Whether to ignore, allow, or forbid extra attributes during model initialization. Defaults to `'ignore'`.\n\n    You can configure how pydantic handles the attributes that are not defined in the model:\n\n    * `allow` - Allow any extra attributes.\n    * `forbid` - Forbid any extra attributes.\n    * `ignore` - Ignore any extra attributes.\n\n    ```py\n    from pydantic import BaseModel, ConfigDict\n\n\n    class User(BaseModel):\n        model_config = ConfigDict(extra='ignore')  # (1)!\n\n        name: str\n\n\n    user = User(name='John Doe', age=20)  # (2)!\n    print(user)\n    #> name='John Doe'\n    ```\n\n    1. This is the default behaviour.\n    2. The `age` argument is ignored.\n\n    Instead, with `extra='allow'`, the `age` argument is included:\n\n    ```py\n    from pydantic import BaseModel, ConfigDict\n\n\n    class User(BaseModel):\n        model_config = ConfigDict(extra='allow')\n\n        name: str\n\n\n    user = User(name='John Doe', age=20)  # (1)!\n    print(user)\n    #> name='John Doe' age=20\n    ```\n\n    1. The `age` argument is included.\n\n    With `extra='forbid'`, an error is raised:\n\n    ```py\n    from pydantic import BaseModel, ConfigDict, ValidationError\n\n\n    class User(BaseModel):\n        model_config = ConfigDict(extra='forbid')\n\n        name: str\n\n\n    try:\n        User(name='John Doe', age=20)\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for User\n        age\n        Extra inputs are not permitted [type=extra_forbidden, input_value=20, input_type=int]\n        '''\n    ```\n    \"\"\"\n\n    frozen: bool\n    \"\"\"\n    Whether models are faux-immutable, i.e. whether `__setattr__` is allowed, and also generates\n    a `__hash__()` method for the model. This makes instances of the model potentially hashable if all the\n    attributes are hashable. Defaults to `False`.\n\n    Note:\n        On V1, the inverse of this setting was called `allow_mutation`, and was `True` by default.\n    \"\"\"\n\n    populate_by_name: bool\n    \"\"\"\n    Whether an aliased field may be populated by its name as given by the model\n    attribute, as well as the alias. Defaults to `False`.\n\n    Note:\n        The name of this configuration setting was changed in **v2.0** from\n        `allow_population_by_field_name` to `populate_by_name`.\n\n    ```py\n    from pydantic import BaseModel, ConfigDict, Field\n\n\n    class User(BaseModel):\n        model_config = ConfigDict(populate_by_name=True)\n\n        name: str = Field(alias='full_name')  # (1)!\n        age: int\n\n\n    user = User(full_name='John Doe', age=20)  # (2)!\n    print(user)\n    #> name='John Doe' age=20\n    user = User(name='John Doe', age=20)  # (3)!\n    print(user)\n    #> name='John Doe' age=20\n    ```\n\n    1. The field `'name'` has an alias `'full_name'`.\n    2. The model is populated by the alias `'full_name'`.\n    3. The model is populated by the field name `'name'`.\n    \"\"\"\n\n    use_enum_values: bool\n    \"\"\"\n    Whether to populate models with the `value` property of enums, rather than the raw enum.\n    This may be useful if you want to serialize `model.model_dump()` later. Defaults to `False`.\n\n    !!! note\n        If you have an `Optional[Enum]` value that you set a default for, you need to use `validate_default=True`\n        for said Field to ensure that the `use_enum_values` flag takes effect on the default, as extracting an\n        enum's value occurs during validation, not serialization.\n\n    ```py\n    from enum import Enum\n    from typing import Optional\n\n    from pydantic import BaseModel, ConfigDict, Field\n\n\n    class SomeEnum(Enum):\n        FOO = 'foo'\n        BAR = 'bar'\n        BAZ = 'baz'\n\n\n    class SomeModel(BaseModel):\n        model_config = ConfigDict(use_enum_values=True)\n\n        some_enum: SomeEnum\n        another_enum: Optional[SomeEnum] = Field(default=SomeEnum.FOO, validate_default=True)\n\n\n    model1 = SomeModel(some_enum=SomeEnum.BAR)\n    print(model1.model_dump())\n    # {'some_enum': 'bar', 'another_enum': 'foo'}\n\n    model2 = SomeModel(some_enum=SomeEnum.BAR, another_enum=SomeEnum.BAZ)\n    print(model2.model_dump())\n    #> {'some_enum': 'bar', 'another_enum': 'baz'}\n    ```\n    \"\"\"\n\n    validate_assignment: bool\n    \"\"\"\n    Whether to validate the data when the model is changed. Defaults to `False`.\n\n    The default behavior of Pydantic is to validate the data when the model is created.\n\n    In case the user changes the data after the model is created, the model is _not_ revalidated.\n\n    ```py\n    from pydantic import BaseModel\n\n    class User(BaseModel):\n        name: str\n\n    user = User(name='John Doe')  # (1)!\n    print(user)\n    #> name='John Doe'\n    user.name = 123  # (1)!\n    print(user)\n    #> name=123\n    ```\n\n    1. The validation happens only when the model is created.\n    2. The validation does not happen when the data is changed.\n\n    In case you want to revalidate the model when the data is changed, you can use `validate_assignment=True`:\n\n    ```py\n    from pydantic import BaseModel, ValidationError\n\n    class User(BaseModel, validate_assignment=True):  # (1)!\n        name: str\n\n    user = User(name='John Doe')  # (2)!\n    print(user)\n    #> name='John Doe'\n    try:\n        user.name = 123  # (3)!\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for User\n        name\n          Input should be a valid string [type=string_type, input_value=123, input_type=int]\n        '''\n    ```\n\n    1. You can either use class keyword arguments, or `model_config` to set `validate_assignment=True`.\n    2. The validation happens when the model is created.\n    3. The validation _also_ happens when the data is changed.\n    \"\"\"\n\n    arbitrary_types_allowed: bool\n    \"\"\"\n    Whether arbitrary types are allowed for field types. Defaults to `False`.\n\n    ```py\n    from pydantic import BaseModel, ConfigDict, ValidationError\n\n    # This is not a pydantic model, it's an arbitrary class\n    class Pet:\n        def __init__(self, name: str):\n            self.name = name\n\n    class Model(BaseModel):\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n        pet: Pet\n        owner: str\n\n    pet = Pet(name='Hedwig')\n    # A simple check of instance type is used to validate the data\n    model = Model(owner='Harry', pet=pet)\n    print(model)\n    #> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'\n    print(model.pet)\n    #> <__main__.Pet object at 0x0123456789ab>\n    print(model.pet.name)\n    #> Hedwig\n    print(type(model.pet))\n    #> <class '__main__.Pet'>\n    try:\n        # If the value is not an instance of the type, it's invalid\n        Model(owner='Harry', pet='Hedwig')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        pet\n          Input should be an instance of Pet [type=is_instance_of, input_value='Hedwig', input_type=str]\n        '''\n\n    # Nothing in the instance of the arbitrary type is checked\n    # Here name probably should have been a str, but it's not validated\n    pet2 = Pet(name=42)\n    model2 = Model(owner='Harry', pet=pet2)\n    print(model2)\n    #> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'\n    print(model2.pet)\n    #> <__main__.Pet object at 0x0123456789ab>\n    print(model2.pet.name)\n    #> 42\n    print(type(model2.pet))\n    #> <class '__main__.Pet'>\n    ```\n    \"\"\"\n\n    from_attributes: bool\n    \"\"\"\n    Whether to build models and look up discriminators of tagged unions using python object attributes.\n    \"\"\"\n\n    loc_by_alias: bool\n    \"\"\"Whether to use the actual key provided in the data (e.g. alias) for error `loc`s rather than the field's name. Defaults to `True`.\"\"\"\n\n    alias_generator: Callable[[str], str] | AliasGenerator | None\n    \"\"\"\n    A callable that takes a field name and returns an alias for it\n    or an instance of [`AliasGenerator`][pydantic.aliases.AliasGenerator]. Defaults to `None`.\n\n    When using a callable, the alias generator is used for both validation and serialization.\n    If you want to use different alias generators for validation and serialization, you can use\n    [`AliasGenerator`][pydantic.aliases.AliasGenerator] instead.\n\n    If data source field names do not match your code style (e. g. CamelCase fields),\n    you can automatically generate aliases using `alias_generator`. Here's an example with\n    a basic callable:\n\n    ```py\n    from pydantic import BaseModel, ConfigDict\n    from pydantic.alias_generators import to_pascal\n\n    class Voice(BaseModel):\n        model_config = ConfigDict(alias_generator=to_pascal)\n\n        name: str\n        language_code: str\n\n    voice = Voice(Name='Filiz', LanguageCode='tr-TR')\n    print(voice.language_code)\n    #> tr-TR\n    print(voice.model_dump(by_alias=True))\n    #> {'Name': 'Filiz', 'LanguageCode': 'tr-TR'}\n    ```\n\n    If you want to use different alias generators for validation and serialization, you can use\n    [`AliasGenerator`][pydantic.aliases.AliasGenerator].\n\n    ```py\n    from pydantic import AliasGenerator, BaseModel, ConfigDict\n    from pydantic.alias_generators import to_camel, to_pascal\n\n    class Athlete(BaseModel):\n        first_name: str\n        last_name: str\n        sport: str\n\n        model_config = ConfigDict(\n            alias_generator=AliasGenerator(\n                validation_alias=to_camel,\n                serialization_alias=to_pascal,\n            )\n        )\n\n    athlete = Athlete(firstName='John', lastName='Doe', sport='track')\n    print(athlete.model_dump(by_alias=True))\n    #> {'FirstName': 'John', 'LastName': 'Doe', 'Sport': 'track'}\n    ```\n\n    Note:\n        Pydantic offers three built-in alias generators: [`to_pascal`][pydantic.alias_generators.to_pascal],\n        [`to_camel`][pydantic.alias_generators.to_camel], and [`to_snake`][pydantic.alias_generators.to_snake].\n    \"\"\"\n\n    ignored_types: tuple[type, ...]\n    \"\"\"A tuple of types that may occur as values of class attributes without annotations. This is\n    typically used for custom descriptors (classes that behave like `property`). If an attribute is set on a\n    class without an annotation and has a type that is not in this tuple (or otherwise recognized by\n    _pydantic_), an error will be raised. Defaults to `()`.\n    \"\"\"\n\n    allow_inf_nan: bool\n    \"\"\"Whether to allow infinity (`+inf` an `-inf`) and NaN values to float fields. Defaults to `True`.\"\"\"\n\n    json_schema_extra: JsonDict | JsonSchemaExtraCallable | None\n    \"\"\"A dict or callable to provide extra JSON schema properties. Defaults to `None`.\"\"\"\n\n    json_encoders: dict[type[object], JsonEncoder] | None\n    \"\"\"\n    A `dict` of custom JSON encoders for specific types. Defaults to `None`.\n\n    !!! warning \"Deprecated\"\n        This config option is a carryover from v1.\n        We originally planned to remove it in v2 but didn't have a 1:1 replacement so we are keeping it for now.\n        It is still deprecated and will likely be removed in the future.\n    \"\"\"\n\n    # new in V2\n    strict: bool\n    \"\"\"\n    _(new in V2)_ If `True`, strict validation is applied to all fields on the model.\n\n    By default, Pydantic attempts to coerce values to the correct type, when possible.\n\n    There are situations in which you may want to disable this behavior, and instead raise an error if a value's type\n    does not match the field's type annotation.\n\n    To configure strict mode for all fields on a model, you can set `strict=True` on the model.\n\n    ```py\n    from pydantic import BaseModel, ConfigDict\n\n    class Model(BaseModel):\n        model_config = ConfigDict(strict=True)\n\n        name: str\n        age: int\n    ```\n\n    See [Strict Mode](../concepts/strict_mode.md) for more details.\n\n    See the [Conversion Table](../concepts/conversion_table.md) for more details on how Pydantic converts data in both\n    strict and lax modes.\n    \"\"\"\n    # whether instances of models and dataclasses (including subclass instances) should re-validate, default 'never'\n    revalidate_instances: Literal['always', 'never', 'subclass-instances']\n    \"\"\"\n    When and how to revalidate models and dataclasses during validation. Accepts the string\n    values of `'never'`, `'always'` and `'subclass-instances'`. Defaults to `'never'`.\n\n    - `'never'` will not revalidate models and dataclasses during validation\n    - `'always'` will revalidate models and dataclasses during validation\n    - `'subclass-instances'` will revalidate models and dataclasses during validation if the instance is a\n        subclass of the model or dataclass\n\n    By default, model and dataclass instances are not revalidated during validation.\n\n    ```py\n    from typing import List\n\n    from pydantic import BaseModel\n\n    class User(BaseModel, revalidate_instances='never'):  # (1)!\n        hobbies: List[str]\n\n    class SubUser(User):\n        sins: List[str]\n\n    class Transaction(BaseModel):\n        user: User\n\n    my_user = User(hobbies=['reading'])\n    t = Transaction(user=my_user)\n    print(t)\n    #> user=User(hobbies=['reading'])\n\n    my_user.hobbies = [1]  # (2)!\n    t = Transaction(user=my_user)  # (3)!\n    print(t)\n    #> user=User(hobbies=[1])\n\n    my_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\n    t = Transaction(user=my_sub_user)\n    print(t)\n    #> user=SubUser(hobbies=['scuba diving'], sins=['lying'])\n    ```\n\n    1. `revalidate_instances` is set to `'never'` by **default.\n    2. The assignment is not validated, unless you set `validate_assignment` to `True` in the model's config.\n    3. Since `revalidate_instances` is set to `never`, this is not revalidated.\n\n    If you want to revalidate instances during validation, you can set `revalidate_instances` to `'always'`\n    in the model's config.\n\n    ```py\n    from typing import List\n\n    from pydantic import BaseModel, ValidationError\n\n    class User(BaseModel, revalidate_instances='always'):  # (1)!\n        hobbies: List[str]\n\n    class SubUser(User):\n        sins: List[str]\n\n    class Transaction(BaseModel):\n        user: User\n\n    my_user = User(hobbies=['reading'])\n    t = Transaction(user=my_user)\n    print(t)\n    #> user=User(hobbies=['reading'])\n\n    my_user.hobbies = [1]\n    try:\n        t = Transaction(user=my_user)  # (2)!\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Transaction\n        user.hobbies.0\n          Input should be a valid string [type=string_type, input_value=1, input_type=int]\n        '''\n\n    my_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\n    t = Transaction(user=my_sub_user)\n    print(t)  # (3)!\n    #> user=User(hobbies=['scuba diving'])\n    ```\n\n    1. `revalidate_instances` is set to `'always'`.\n    2. The model is revalidated, since `revalidate_instances` is set to `'always'`.\n    3. Using `'never'` we would have gotten `user=SubUser(hobbies=['scuba diving'], sins=['lying'])`.\n\n    It's also possible to set `revalidate_instances` to `'subclass-instances'` to only revalidate instances\n    of subclasses of the model.\n\n    ```py\n    from typing import List\n\n    from pydantic import BaseModel\n\n    class User(BaseModel, revalidate_instances='subclass-instances'):  # (1)!\n        hobbies: List[str]\n\n    class SubUser(User):\n        sins: List[str]\n\n    class Transaction(BaseModel):\n        user: User\n\n    my_user = User(hobbies=['reading'])\n    t = Transaction(user=my_user)\n    print(t)\n    #> user=User(hobbies=['reading'])\n\n    my_user.hobbies = [1]\n    t = Transaction(user=my_user)  # (2)!\n    print(t)\n    #> user=User(hobbies=[1])\n\n    my_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\n    t = Transaction(user=my_sub_user)\n    print(t)  # (3)!\n    #> user=User(hobbies=['scuba diving'])\n    ```\n\n    1. `revalidate_instances` is set to `'subclass-instances'`.\n    2. This is not revalidated, since `my_user` is not a subclass of `User`.\n    3. Using `'never'` we would have gotten `user=SubUser(hobbies=['scuba diving'], sins=['lying'])`.\n    \"\"\"\n\n    ser_json_timedelta: Literal['iso8601', 'float']\n    \"\"\"\n    The format of JSON serialized timedeltas. Accepts the string values of `'iso8601'` and\n    `'float'`. Defaults to `'iso8601'`.\n\n    - `'iso8601'` will serialize timedeltas to ISO 8601 durations.\n    - `'float'` will serialize timedeltas to the total number of seconds.\n    \"\"\"\n\n    ser_json_bytes: Literal['utf8', 'base64']\n    \"\"\"\n    The encoding of JSON serialized bytes. Accepts the string values of `'utf8'` and `'base64'`.\n    Defaults to `'utf8'`.\n\n    - `'utf8'` will serialize bytes to UTF-8 strings.\n    - `'base64'` will serialize bytes to URL safe base64 strings.\n    \"\"\"\n\n    ser_json_inf_nan: Literal['null', 'constants']\n    \"\"\"\n    The encoding of JSON serialized infinity and NaN float values. Accepts the string values of `'null'` and `'constants'`.\n    Defaults to `'null'`.\n\n    - `'null'` will serialize infinity and NaN values as `null`.\n    - `'constants'` will serialize infinity and NaN values as `Infinity` and `NaN`.\n    \"\"\"\n\n    # whether to validate default values during validation, default False\n    validate_default: bool\n    \"\"\"Whether to validate default values during validation. Defaults to `False`.\"\"\"\n\n    validate_return: bool\n    \"\"\"whether to validate the return value from call validators. Defaults to `False`.\"\"\"\n\n    protected_namespaces: tuple[str, ...]\n    \"\"\"\n    A `tuple` of strings that prevent model to have field which conflict with them.\n    Defaults to `('model_', )`).\n\n    Pydantic prevents collisions between model attributes and `BaseModel`'s own methods by\n    namespacing them with the prefix `model_`.\n\n    ```py\n    import warnings\n\n    from pydantic import BaseModel\n\n    warnings.filterwarnings('error')  # Raise warnings as errors\n\n    try:\n\n        class Model(BaseModel):\n            model_prefixed_field: str\n\n    except UserWarning as e:\n        print(e)\n        '''\n        Field \"model_prefixed_field\" has conflict with protected namespace \"model_\".\n\n        You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n        '''\n    ```\n\n    You can customize this behavior using the `protected_namespaces` setting:\n\n    ```py\n    import warnings\n\n    from pydantic import BaseModel, ConfigDict\n\n    warnings.filterwarnings('error')  # Raise warnings as errors\n\n    try:\n\n        class Model(BaseModel):\n            model_prefixed_field: str\n            also_protect_field: str\n\n            model_config = ConfigDict(\n                protected_namespaces=('protect_me_', 'also_protect_')\n            )\n\n    except UserWarning as e:\n        print(e)\n        '''\n        Field \"also_protect_field\" has conflict with protected namespace \"also_protect_\".\n\n        You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('protect_me_',)`.\n        '''\n    ```\n\n    While Pydantic will only emit a warning when an item is in a protected namespace but does not actually have a collision,\n    an error _is_ raised if there is an actual collision with an existing attribute:\n\n    ```py\n    from pydantic import BaseModel\n\n    try:\n\n        class Model(BaseModel):\n            model_validate: str\n\n    except NameError as e:\n        print(e)\n        '''\n        Field \"model_validate\" conflicts with member <bound method BaseModel.model_validate of <class 'pydantic.main.BaseModel'>> of protected namespace \"model_\".\n        '''\n    ```\n    \"\"\"\n\n    hide_input_in_errors: bool\n    \"\"\"\n    Whether to hide inputs when printing errors. Defaults to `False`.\n\n    Pydantic shows the input value and type when it raises `ValidationError` during the validation.\n\n    ```py\n    from pydantic import BaseModel, ValidationError\n\n    class Model(BaseModel):\n        a: str\n\n    try:\n        Model(a=123)\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        a\n          Input should be a valid string [type=string_type, input_value=123, input_type=int]\n        '''\n    ```\n\n    You can hide the input value and type by setting the `hide_input_in_errors` config to `True`.\n\n    ```py\n    from pydantic import BaseModel, ConfigDict, ValidationError\n\n    class Model(BaseModel):\n        a: str\n        model_config = ConfigDict(hide_input_in_errors=True)\n\n    try:\n        Model(a=123)\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        a\n          Input should be a valid string [type=string_type]\n        '''\n    ```\n    \"\"\"\n\n    defer_build: bool\n    \"\"\"\n    Whether to defer model validator and serializer construction until the first model validation. Defaults to False.\n\n    This can be useful to avoid the overhead of building models which are only\n    used nested within other models, or when you want to manually define type namespace via\n    [`Model.model_rebuild(_types_namespace=...)`][pydantic.BaseModel.model_rebuild].\n\n    See also [`experimental_defer_build_mode`][pydantic.config.ConfigDict.experimental_defer_build_mode].\n\n    !!! note\n        `defer_build` does not work by default with FastAPI Pydantic models. By default, the validator and serializer\n        for said models is constructed immediately for FastAPI routes. You also need to define\n        [`experimental_defer_build_mode=('model', 'type_adapter')`][pydantic.config.ConfigDict.experimental_defer_build_mode] with FastAPI\n        models in order for `defer_build=True` to take effect. This additional (experimental) parameter is required for\n        the deferred building due to FastAPI relying on `TypeAdapter`s.\n    \"\"\"\n\n    experimental_defer_build_mode: tuple[Literal['model', 'type_adapter'], ...]\n    \"\"\"\n    Controls when [`defer_build`][pydantic.config.ConfigDict.defer_build] is applicable. Defaults to `('model',)`.\n\n    Due to backwards compatibility reasons [`TypeAdapter`][pydantic.type_adapter.TypeAdapter] does not by default\n    respect `defer_build`. Meaning when `defer_build` is `True` and `experimental_defer_build_mode` is the default `('model',)`\n    then `TypeAdapter` immediately constructs its validator and serializer instead of postponing said construction until\n    the first model validation. Set this to `('model', 'type_adapter')` to make `TypeAdapter` respect the `defer_build`\n    so it postpones validator and serializer construction until the first validation or serialization.\n\n    !!! note\n        The `experimental_defer_build_mode` parameter is named with an underscore to suggest this is an experimental feature. It may\n        be removed or changed in the future in a minor release.\n    \"\"\"\n\n    plugin_settings: dict[str, object] | None\n    \"\"\"A `dict` of settings for plugins. Defaults to `None`.\n\n    See [Pydantic Plugins](../concepts/plugins.md) for details.\n    \"\"\"\n\n    schema_generator: type[_GenerateSchema] | None\n    \"\"\"\n    A custom core schema generator class to use when generating JSON schemas.\n    Useful if you want to change the way types are validated across an entire model/schema. Defaults to `None`.\n\n    The `GenerateSchema` interface is subject to change, currently only the `string_schema` method is public.\n\n    See [#6737](https://github.com/pydantic/pydantic/pull/6737) for details.\n    \"\"\"\n\n    json_schema_serialization_defaults_required: bool\n    \"\"\"\n    Whether fields with default values should be marked as required in the serialization schema. Defaults to `False`.\n\n    This ensures that the serialization schema will reflect the fact a field with a default will always be present\n    when serializing the model, even though it is not required for validation.\n\n    However, there are scenarios where this may be undesirable \u2014 in particular, if you want to share the schema\n    between validation and serialization, and don't mind fields with defaults being marked as not required during\n    serialization. See [#7209](https://github.com/pydantic/pydantic/issues/7209) for more details.\n\n    ```py\n    from pydantic import BaseModel, ConfigDict\n\n    class Model(BaseModel):\n        a: str = 'a'\n\n        model_config = ConfigDict(json_schema_serialization_defaults_required=True)\n\n    print(Model.model_json_schema(mode='validation'))\n    '''\n    {\n        'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},\n        'title': 'Model',\n        'type': 'object',\n    }\n    '''\n    print(Model.model_json_schema(mode='serialization'))\n    '''\n    {\n        'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'title': 'Model',\n        'type': 'object',\n    }\n    '''\n    ```\n    \"\"\"\n\n    json_schema_mode_override: Literal['validation', 'serialization', None]\n    \"\"\"\n    If not `None`, the specified mode will be used to generate the JSON schema regardless of what `mode` was passed to\n    the function call. Defaults to `None`.\n\n    This provides a way to force the JSON schema generation to reflect a specific mode, e.g., to always use the\n    validation schema.\n\n    It can be useful when using frameworks (such as FastAPI) that may generate different schemas for validation\n    and serialization that must both be referenced from the same schema; when this happens, we automatically append\n    `-Input` to the definition reference for the validation schema and `-Output` to the definition reference for the\n    serialization schema. By specifying a `json_schema_mode_override` though, this prevents the conflict between\n    the validation and serialization schemas (since both will use the specified schema), and so prevents the suffixes\n    from being added to the definition references.\n\n    ```py\n    from pydantic import BaseModel, ConfigDict, Json\n\n    class Model(BaseModel):\n        a: Json[int]  # requires a string to validate, but will dump an int\n\n    print(Model.model_json_schema(mode='serialization'))\n    '''\n    {\n        'properties': {'a': {'title': 'A', 'type': 'integer'}},\n        'required': ['a'],\n        'title': 'Model',\n        'type': 'object',\n    }\n    '''\n\n    class ForceInputModel(Model):\n        # the following ensures that even with mode='serialization', we\n        # will get the schema that would be generated for validation.\n        model_config = ConfigDict(json_schema_mode_override='validation')\n\n    print(ForceInputModel.model_json_schema(mode='serialization'))\n    '''\n    {\n        'properties': {\n            'a': {\n                'contentMediaType': 'application/json',\n                'contentSchema': {'type': 'integer'},\n                'title': 'A',\n                'type': 'string',\n            }\n        },\n        'required': ['a'],\n        'title': 'ForceInputModel',\n        'type': 'object',\n    }\n    '''\n    ```\n    \"\"\"\n\n    coerce_numbers_to_str: bool\n    \"\"\"\n    If `True`, enables automatic coercion of any `Number` type to `str` in \"lax\" (non-strict) mode. Defaults to `False`.\n\n    Pydantic doesn't allow number types (`int`, `float`, `Decimal`) to be coerced as type `str` by default.\n\n    ```py\n    from decimal import Decimal\n\n    from pydantic import BaseModel, ConfigDict, ValidationError\n\n    class Model(BaseModel):\n        value: str\n\n    try:\n        print(Model(value=42))\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        value\n          Input should be a valid string [type=string_type, input_value=42, input_type=int]\n        '''\n\n    class Model(BaseModel):\n        model_config = ConfigDict(coerce_numbers_to_str=True)\n\n        value: str\n\n    repr(Model(value=42).value)\n    #> \"42\"\n    repr(Model(value=42.13).value)\n    #> \"42.13\"\n    repr(Model(value=Decimal('42.13')).value)\n    #> \"42.13\"\n    ```\n    \"\"\"\n\n    regex_engine: Literal['rust-regex', 'python-re']\n    \"\"\"\n    The regex engine to be used for pattern validation.\n    Defaults to `'rust-regex'`.\n\n    - `rust-regex` uses the [`regex`](https://docs.rs/regex) Rust crate,\n      which is non-backtracking and therefore more DDoS resistant, but does not support all regex features.\n    - `python-re` use the [`re`](https://docs.python.org/3/library/re.html) module,\n      which supports all regex features, but may be slower.\n\n    !!! note\n        If you use a compiled regex pattern, the python-re engine will be used regardless of this setting.\n        This is so that flags such as `re.IGNORECASE` are respected.\n\n    ```py\n    from pydantic import BaseModel, ConfigDict, Field, ValidationError\n\n    class Model(BaseModel):\n        model_config = ConfigDict(regex_engine='python-re')\n\n        value: str = Field(pattern=r'^abc(?=def)')\n\n    print(Model(value='abcdef').value)\n    #> abcdef\n\n    try:\n        print(Model(value='abxyzcdef'))\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        value\n          String should match pattern '^abc(?=def)' [type=string_pattern_mismatch, input_value='abxyzcdef', input_type=str]\n        '''\n    ```\n    \"\"\"\n\n    validation_error_cause: bool\n    \"\"\"\n    If `True`, Python exceptions that were part of a validation failure will be shown as an exception group as a cause. Can be useful for debugging. Defaults to `False`.\n\n    Note:\n        Python 3.10 and older don't support exception groups natively. <=3.10, backport must be installed: `pip install exceptiongroup`.\n\n    Note:\n        The structure of validation errors are likely to change in future Pydantic versions. Pydantic offers no guarantees about their structure. Should be used for visual traceback debugging only.\n    \"\"\"\n\n    use_attribute_docstrings: bool\n    '''\n    Whether docstrings of attributes (bare string literals immediately following the attribute declaration)\n    should be used for field descriptions. Defaults to `False`.\n\n    Available in Pydantic v2.7+.\n\n    ```py\n    from pydantic import BaseModel, ConfigDict, Field\n\n\n    class Model(BaseModel):\n        model_config = ConfigDict(use_attribute_docstrings=True)\n\n        x: str\n        \"\"\"\n        Example of an attribute docstring\n        \"\"\"\n\n        y: int = Field(description=\"Description in Field\")\n        \"\"\"\n        Description in Field overrides attribute docstring\n        \"\"\"\n\n\n    print(Model.model_fields[\"x\"].description)\n    # > Example of an attribute docstring\n    print(Model.model_fields[\"y\"].description)\n    # > Description in Field\n    ```\n    This requires the source code of the class to be available at runtime.\n\n    !!! warning \"Usage with `TypedDict`\"\n        Due to current limitations, attribute docstrings detection may not work as expected when using `TypedDict`\n        (in particular when multiple `TypedDict` classes have the same name in the same source file). The behavior\n        can be different depending on the Python version used.\n    '''\n\n    cache_strings: bool | Literal['all', 'keys', 'none']\n    \"\"\"\n    Whether to cache strings to avoid constructing new Python objects. Defaults to True.\n\n    Enabling this setting should significantly improve validation performance while increasing memory usage slightly.\n\n    - `True` or `'all'` (the default): cache all strings\n    - `'keys'`: cache only dictionary keys\n    - `False` or `'none'`: no caching\n\n    !!! note\n        `True` or `'all'` is required to cache strings during general validation because\n        validators don't know if they're in a key or a value.\n\n    !!! tip\n        If repeated strings are rare, it's recommended to use `'keys'` or `'none'` to reduce memory usage,\n        as the performance difference is minimal if repeated strings are rare.\n    \"\"\"\n\n\n_TypeT = TypeVar('_TypeT', bound=type)\n\n\ndef with_config(config: ConfigDict) -> Callable[[_TypeT], _TypeT]:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/config/#configuration-with-dataclass-from-the-standard-library-or-typeddict\n\n    A convenience decorator to set a [Pydantic configuration](config.md) on a `TypedDict` or a `dataclass` from the standard library.\n\n    Although the configuration can be set using the `__pydantic_config__` attribute, it does not play well with type checkers,\n    especially with `TypedDict`.\n\n    !!! example \"Usage\"\n\n        ```py\n        from typing_extensions import TypedDict\n\n        from pydantic import ConfigDict, TypeAdapter, with_config\n\n        @with_config(ConfigDict(str_to_lower=True))\n        class Model(TypedDict):\n            x: str\n\n        ta = TypeAdapter(Model)\n\n        print(ta.validate_python({'x': 'ABC'}))\n        #> {'x': 'abc'}\n        ```\n    \"\"\"\n\n    def inner(class_: _TypeT, /) -> _TypeT:\n        # Ideally, we would check for `class_` to either be a `TypedDict` or a stdlib dataclass.\n        # However, the `@with_config` decorator can be applied *after* `@dataclass`. To avoid\n        # common mistakes, we at least check for `class_` to not be a Pydantic model.\n        from ._internal._utils import is_model_class\n\n        if is_model_class(class_):\n            raise PydanticUserError(\n                f'Cannot use `with_config` on {class_.__name__} as it is a Pydantic model',\n                code='with-config-on-model',\n            )\n        class_.__pydantic_config__ = config\n        return class_\n\n    return inner\n\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/type_adapter.py": "\"\"\"Type adapter specification.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport sys\nfrom contextlib import contextmanager\nfrom dataclasses import is_dataclass\nfrom functools import cached_property, wraps\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Generic,\n    Iterable,\n    Iterator,\n    Literal,\n    Set,\n    TypeVar,\n    Union,\n    cast,\n    final,\n    overload,\n)\n\nfrom pydantic_core import CoreSchema, SchemaSerializer, SchemaValidator, Some\nfrom typing_extensions import Concatenate, ParamSpec, is_typeddict\n\nfrom pydantic.errors import PydanticUserError\nfrom pydantic.main import BaseModel\n\nfrom ._internal import _config, _generate_schema, _mock_val_ser, _typing_extra, _utils\nfrom .config import ConfigDict\nfrom .json_schema import (\n    DEFAULT_REF_TEMPLATE,\n    GenerateJsonSchema,\n    JsonSchemaKeyT,\n    JsonSchemaMode,\n    JsonSchemaValue,\n)\nfrom .plugin._schema_validator import create_schema_validator\n\nT = TypeVar('T')\nR = TypeVar('R')\nP = ParamSpec('P')\nTypeAdapterT = TypeVar('TypeAdapterT', bound='TypeAdapter')\n\n\nif TYPE_CHECKING:\n    # should be `set[int] | set[str] | dict[int, IncEx] | dict[str, IncEx] | None`, but mypy can't cope\n    IncEx = Union[Set[int], Set[str], Dict[int, Any], Dict[str, Any]]\n\n\ndef _get_schema(type_: Any, config_wrapper: _config.ConfigWrapper, parent_depth: int) -> CoreSchema:\n    \"\"\"`BaseModel` uses its own `__module__` to find out where it was defined\n    and then looks for symbols to resolve forward references in those globals.\n    On the other hand this function can be called with arbitrary objects,\n    including type aliases, where `__module__` (always `typing.py`) is not useful.\n    So instead we look at the globals in our parent stack frame.\n\n    This works for the case where this function is called in a module that\n    has the target of forward references in its scope, but\n    does not always work for more complex cases.\n\n    For example, take the following:\n\n    a.py\n    ```python\n    from typing import Dict, List\n\n    IntList = List[int]\n    OuterDict = Dict[str, 'IntList']\n    ```\n\n    b.py\n    ```python test=\"skip\"\n    from a import OuterDict\n\n    from pydantic import TypeAdapter\n\n    IntList = int  # replaces the symbol the forward reference is looking for\n    v = TypeAdapter(OuterDict)\n    v({'x': 1})  # should fail but doesn't\n    ```\n\n    If `OuterDict` were a `BaseModel`, this would work because it would resolve\n    the forward reference within the `a.py` namespace.\n    But `TypeAdapter(OuterDict)` can't determine what module `OuterDict` came from.\n\n    In other words, the assumption that _all_ forward references exist in the\n    module we are being called from is not technically always true.\n    Although most of the time it is and it works fine for recursive models and such,\n    `BaseModel`'s behavior isn't perfect either and _can_ break in similar ways,\n    so there is no right or wrong between the two.\n\n    But at the very least this behavior is _subtly_ different from `BaseModel`'s.\n    \"\"\"\n    local_ns = _typing_extra.parent_frame_namespace(parent_depth=parent_depth)\n    global_ns = sys._getframe(max(parent_depth - 1, 1)).f_globals.copy()\n    global_ns.update(local_ns or {})\n    gen = _generate_schema.GenerateSchema(config_wrapper, types_namespace=global_ns, typevars_map={})\n    schema = gen.generate_schema(type_)\n    schema = gen.clean_schema(schema)\n    return schema\n\n\ndef _getattr_no_parents(obj: Any, attribute: str) -> Any:\n    \"\"\"Returns the attribute value without attempting to look up attributes from parent types.\"\"\"\n    if hasattr(obj, '__dict__'):\n        try:\n            return obj.__dict__[attribute]\n        except KeyError:\n            pass\n\n    slots = getattr(obj, '__slots__', None)\n    if slots is not None and attribute in slots:\n        return getattr(obj, attribute)\n    else:\n        raise AttributeError(attribute)\n\n\ndef _type_has_config(type_: Any) -> bool:\n    \"\"\"Returns whether the type has config.\"\"\"\n    type_ = _typing_extra.annotated_type(type_) or type_\n    try:\n        return issubclass(type_, BaseModel) or is_dataclass(type_) or is_typeddict(type_)\n    except TypeError:\n        # type is not a class\n        return False\n\n\n# This is keeping track of the frame depth for the TypeAdapter functions. This is required for _parent_depth used for\n# ForwardRef resolution. We may enter the TypeAdapter schema building via different TypeAdapter functions. Hence, we\n# need to keep track of the frame depth relative to the originally provided _parent_depth.\ndef _frame_depth(\n    depth: int,\n) -> Callable[[Callable[Concatenate[TypeAdapterT, P], R]], Callable[Concatenate[TypeAdapterT, P], R]]:\n    def wrapper(func: Callable[Concatenate[TypeAdapterT, P], R]) -> Callable[Concatenate[TypeAdapterT, P], R]:\n        @wraps(func)\n        def wrapped(self: TypeAdapterT, *args: P.args, **kwargs: P.kwargs) -> R:\n            with self._with_frame_depth(depth + 1):  # depth + 1 for the wrapper function\n                return func(self, *args, **kwargs)\n\n        return wrapped\n\n    return wrapper\n\n\n@final\nclass TypeAdapter(Generic[T]):\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/type_adapter/\n\n    Type adapters provide a flexible way to perform validation and serialization based on a Python type.\n\n    A `TypeAdapter` instance exposes some of the functionality from `BaseModel` instance methods\n    for types that do not have such methods (such as dataclasses, primitive types, and more).\n\n    **Note:** `TypeAdapter` instances are not types, and cannot be used as type annotations for fields.\n\n    **Note:** By default, `TypeAdapter` does not respect the\n    [`defer_build=True`][pydantic.config.ConfigDict.defer_build] setting in the\n    [`model_config`][pydantic.BaseModel.model_config] or in the `TypeAdapter` constructor `config`. You need to also\n    explicitly set [`experimental_defer_build_mode=('model', 'type_adapter')`][pydantic.config.ConfigDict.experimental_defer_build_mode] of the\n    config to defer the model validator and serializer construction. Thus, this feature is opt-in to ensure backwards\n    compatibility.\n\n    Attributes:\n        core_schema: The core schema for the type.\n        validator (SchemaValidator): The schema validator for the type.\n        serializer: The schema serializer for the type.\n    \"\"\"\n\n    @overload\n    def __init__(\n        self,\n        type: type[T],\n        *,\n        config: ConfigDict | None = ...,\n        _parent_depth: int = ...,\n        module: str | None = ...,\n    ) -> None: ...\n\n    # This second overload is for unsupported special forms (such as Annotated, Union, etc.)\n    # Currently there is no way to type this correctly\n    # See https://github.com/python/typing/pull/1618\n    @overload\n    def __init__(\n        self,\n        type: Any,\n        *,\n        config: ConfigDict | None = ...,\n        _parent_depth: int = ...,\n        module: str | None = ...,\n    ) -> None: ...\n\n    def __init__(\n        self,\n        type: Any,\n        *,\n        config: ConfigDict | None = None,\n        _parent_depth: int = 2,\n        module: str | None = None,\n    ) -> None:\n        \"\"\"Initializes the TypeAdapter object.\n\n        Args:\n            type: The type associated with the `TypeAdapter`.\n            config: Configuration for the `TypeAdapter`, should be a dictionary conforming to [`ConfigDict`][pydantic.config.ConfigDict].\n            _parent_depth: depth at which to search the parent namespace to construct the local namespace.\n            module: The module that passes to plugin if provided.\n\n        !!! note\n            You cannot use the `config` argument when instantiating a `TypeAdapter` if the type you're using has its own\n            config that cannot be overridden (ex: `BaseModel`, `TypedDict`, and `dataclass`). A\n            [`type-adapter-config-unused`](../errors/usage_errors.md#type-adapter-config-unused) error will be raised in this case.\n\n        !!! note\n            The `_parent_depth` argument is named with an underscore to suggest its private nature and discourage use.\n            It may be deprecated in a minor version, so we only recommend using it if you're\n            comfortable with potential change in behavior / support.\n\n        ??? tip \"Compatibility with `mypy`\"\n            Depending on the type used, `mypy` might raise an error when instantiating a `TypeAdapter`. As a workaround, you can explicitly\n            annotate your variable:\n\n            ```py\n            from typing import Union\n\n            from pydantic import TypeAdapter\n\n            ta: TypeAdapter[Union[str, int]] = TypeAdapter(Union[str, int])  # type: ignore[arg-type]\n            ```\n\n        Returns:\n            A type adapter configured for the specified `type`.\n        \"\"\"\n        if _type_has_config(type) and config is not None:\n            raise PydanticUserError(\n                'Cannot use `config` when the type is a BaseModel, dataclass or TypedDict.'\n                ' These types can have their own config and setting the config via the `config`'\n                ' parameter to TypeAdapter will not override it, thus the `config` you passed to'\n                ' TypeAdapter becomes meaningless, which is probably not what you want.',\n                code='type-adapter-config-unused',\n            )\n\n        self._type = type\n        self._config = config\n        self._parent_depth = _parent_depth\n        if module is None:\n            f = sys._getframe(1)\n            self._module_name = cast(str, f.f_globals.get('__name__', ''))\n        else:\n            self._module_name = module\n\n        self._core_schema: CoreSchema | None = None\n        self._validator: SchemaValidator | None = None\n        self._serializer: SchemaSerializer | None = None\n\n        if not self._defer_build():\n            # Immediately initialize the core schema, validator and serializer\n            with self._with_frame_depth(1):  # +1 frame depth for this __init__\n                # Model itself may be using deferred building. For backward compatibility we don't rebuild model mocks\n                # here as part of __init__ even though TypeAdapter itself is not using deferred building.\n                self._init_core_attrs(rebuild_mocks=False)\n\n    @contextmanager\n    def _with_frame_depth(self, depth: int) -> Iterator[None]:\n        self._parent_depth += depth\n        try:\n            yield\n        finally:\n            self._parent_depth -= depth\n\n    @_frame_depth(1)\n    def _init_core_attrs(self, rebuild_mocks: bool) -> None:\n        try:\n            self._core_schema = _getattr_no_parents(self._type, '__pydantic_core_schema__')\n            self._validator = _getattr_no_parents(self._type, '__pydantic_validator__')\n            self._serializer = _getattr_no_parents(self._type, '__pydantic_serializer__')\n        except AttributeError:\n            config_wrapper = _config.ConfigWrapper(self._config)\n            core_config = config_wrapper.core_config(None)\n\n            self._core_schema = _get_schema(self._type, config_wrapper, parent_depth=self._parent_depth)\n            self._validator = create_schema_validator(\n                schema=self._core_schema,\n                schema_type=self._type,\n                schema_type_module=self._module_name,\n                schema_type_name=str(self._type),\n                schema_kind='TypeAdapter',\n                config=core_config,\n                plugin_settings=config_wrapper.plugin_settings,\n            )\n            self._serializer = SchemaSerializer(self._core_schema, core_config)\n\n        if rebuild_mocks and isinstance(self._core_schema, _mock_val_ser.MockCoreSchema):\n            self._core_schema.rebuild()\n            self._init_core_attrs(rebuild_mocks=False)\n            assert not isinstance(self._core_schema, _mock_val_ser.MockCoreSchema)\n            assert not isinstance(self._validator, _mock_val_ser.MockValSer)\n            assert not isinstance(self._serializer, _mock_val_ser.MockValSer)\n\n    @cached_property\n    @_frame_depth(2)  # +2 for @cached_property and core_schema(self)\n    def core_schema(self) -> CoreSchema:\n        \"\"\"The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\"\"\"\n        if self._core_schema is None or isinstance(self._core_schema, _mock_val_ser.MockCoreSchema):\n            self._init_core_attrs(rebuild_mocks=True)  # Do not expose MockCoreSchema from public function\n        assert self._core_schema is not None and not isinstance(self._core_schema, _mock_val_ser.MockCoreSchema)\n        return self._core_schema\n\n    @cached_property\n    @_frame_depth(2)  # +2 for @cached_property + validator(self)\n    def validator(self) -> SchemaValidator:\n        \"\"\"The pydantic-core SchemaValidator used to validate instances of the model.\"\"\"\n        if not isinstance(self._validator, SchemaValidator):\n            self._init_core_attrs(rebuild_mocks=True)  # Do not expose MockValSer from public function\n        assert isinstance(self._validator, SchemaValidator)\n        return self._validator\n\n    @cached_property\n    @_frame_depth(2)  # +2 for @cached_property + serializer(self)\n    def serializer(self) -> SchemaSerializer:\n        \"\"\"The pydantic-core SchemaSerializer used to dump instances of the model.\"\"\"\n        if not isinstance(self._serializer, SchemaSerializer):\n            self._init_core_attrs(rebuild_mocks=True)  # Do not expose MockValSer from public function\n        assert isinstance(self._serializer, SchemaSerializer)\n        return self._serializer\n\n    def _defer_build(self) -> bool:\n        config = self._config if self._config is not None else self._model_config()\n        return self._is_defer_build_config(config) if config is not None else False\n\n    def _model_config(self) -> ConfigDict | None:\n        type_: Any = _typing_extra.annotated_type(self._type) or self._type  # Eg FastAPI heavily uses Annotated\n        if _utils.lenient_issubclass(type_, BaseModel):\n            return type_.model_config\n        return getattr(type_, '__pydantic_config__', None)\n\n    @staticmethod\n    def _is_defer_build_config(config: ConfigDict) -> bool:\n        # TODO reevaluate this logic when we have a better understanding of how defer_build should work with TypeAdapter\n        # Should we drop the special experimental_defer_build_mode check?\n        return config.get('defer_build', False) is True and 'type_adapter' in config.get(\n            'experimental_defer_build_mode', tuple()\n        )\n\n    @_frame_depth(1)\n    def validate_python(\n        self,\n        object: Any,\n        /,\n        *,\n        strict: bool | None = None,\n        from_attributes: bool | None = None,\n        context: dict[str, Any] | None = None,\n    ) -> T:\n        \"\"\"Validate a Python object against the model.\n\n        Args:\n            object: The Python object to validate against the model.\n            strict: Whether to strictly check types.\n            from_attributes: Whether to extract data from object attributes.\n            context: Additional context to pass to the validator.\n\n        !!! note\n            When using `TypeAdapter` with a Pydantic `dataclass`, the use of the `from_attributes`\n            argument is not supported.\n\n        Returns:\n            The validated object.\n        \"\"\"\n        return self.validator.validate_python(object, strict=strict, from_attributes=from_attributes, context=context)\n\n    @_frame_depth(1)\n    def validate_json(\n        self, data: str | bytes, /, *, strict: bool | None = None, context: dict[str, Any] | None = None\n    ) -> T:\n        \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/json/#json-parsing\n\n        Validate a JSON string or bytes against the model.\n\n        Args:\n            data: The JSON data to validate against the model.\n            strict: Whether to strictly check types.\n            context: Additional context to use during validation.\n\n        Returns:\n            The validated object.\n        \"\"\"\n        return self.validator.validate_json(data, strict=strict, context=context)\n\n    @_frame_depth(1)\n    def validate_strings(self, obj: Any, /, *, strict: bool | None = None, context: dict[str, Any] | None = None) -> T:\n        \"\"\"Validate object contains string data against the model.\n\n        Args:\n            obj: The object contains string data to validate.\n            strict: Whether to strictly check types.\n            context: Additional context to use during validation.\n\n        Returns:\n            The validated object.\n        \"\"\"\n        return self.validator.validate_strings(obj, strict=strict, context=context)\n\n    @_frame_depth(1)\n    def get_default_value(self, *, strict: bool | None = None, context: dict[str, Any] | None = None) -> Some[T] | None:\n        \"\"\"Get the default value for the wrapped type.\n\n        Args:\n            strict: Whether to strictly check types.\n            context: Additional context to pass to the validator.\n\n        Returns:\n            The default value wrapped in a `Some` if there is one or None if not.\n        \"\"\"\n        return self.validator.get_default_value(strict=strict, context=context)\n\n    @_frame_depth(1)\n    def dump_python(\n        self,\n        instance: T,\n        /,\n        *,\n        mode: Literal['json', 'python'] = 'python',\n        include: IncEx | None = None,\n        exclude: IncEx | None = None,\n        by_alias: bool = False,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        round_trip: bool = False,\n        warnings: bool | Literal['none', 'warn', 'error'] = True,\n        serialize_as_any: bool = False,\n        context: dict[str, Any] | None = None,\n    ) -> Any:\n        \"\"\"Dump an instance of the adapted type to a Python object.\n\n        Args:\n            instance: The Python object to serialize.\n            mode: The output format.\n            include: Fields to include in the output.\n            exclude: Fields to exclude from the output.\n            by_alias: Whether to use alias names for field names.\n            exclude_unset: Whether to exclude unset fields.\n            exclude_defaults: Whether to exclude fields with default values.\n            exclude_none: Whether to exclude fields with None values.\n            round_trip: Whether to output the serialized data in a way that is compatible with deserialization.\n            warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n                \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n            serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n            context: Additional context to pass to the serializer.\n\n        Returns:\n            The serialized object.\n        \"\"\"\n        return self.serializer.to_python(\n            instance,\n            mode=mode,\n            by_alias=by_alias,\n            include=include,\n            exclude=exclude,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            round_trip=round_trip,\n            warnings=warnings,\n            serialize_as_any=serialize_as_any,\n            context=context,\n        )\n\n    @_frame_depth(1)\n    def dump_json(\n        self,\n        instance: T,\n        /,\n        *,\n        indent: int | None = None,\n        include: IncEx | None = None,\n        exclude: IncEx | None = None,\n        by_alias: bool = False,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        round_trip: bool = False,\n        warnings: bool | Literal['none', 'warn', 'error'] = True,\n        serialize_as_any: bool = False,\n        context: dict[str, Any] | None = None,\n    ) -> bytes:\n        \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/json/#json-serialization\n\n        Serialize an instance of the adapted type to JSON.\n\n        Args:\n            instance: The instance to be serialized.\n            indent: Number of spaces for JSON indentation.\n            include: Fields to include.\n            exclude: Fields to exclude.\n            by_alias: Whether to use alias names for field names.\n            exclude_unset: Whether to exclude unset fields.\n            exclude_defaults: Whether to exclude fields with default values.\n            exclude_none: Whether to exclude fields with a value of `None`.\n            round_trip: Whether to serialize and deserialize the instance to ensure round-tripping.\n            warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n                \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n            serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n            context: Additional context to pass to the serializer.\n\n        Returns:\n            The JSON representation of the given instance as bytes.\n        \"\"\"\n        return self.serializer.to_json(\n            instance,\n            indent=indent,\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            round_trip=round_trip,\n            warnings=warnings,\n            serialize_as_any=serialize_as_any,\n            context=context,\n        )\n\n    @_frame_depth(1)\n    def json_schema(\n        self,\n        *,\n        by_alias: bool = True,\n        ref_template: str = DEFAULT_REF_TEMPLATE,\n        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n        mode: JsonSchemaMode = 'validation',\n    ) -> dict[str, Any]:\n        \"\"\"Generate a JSON schema for the adapted type.\n\n        Args:\n            by_alias: Whether to use alias names for field names.\n            ref_template: The format string used for generating $ref strings.\n            schema_generator: The generator class used for creating the schema.\n            mode: The mode to use for schema generation.\n\n        Returns:\n            The JSON schema for the model as a dictionary.\n        \"\"\"\n        schema_generator_instance = schema_generator(by_alias=by_alias, ref_template=ref_template)\n        return schema_generator_instance.generate(self.core_schema, mode=mode)\n\n    @staticmethod\n    def json_schemas(\n        inputs: Iterable[tuple[JsonSchemaKeyT, JsonSchemaMode, TypeAdapter[Any]]],\n        /,\n        *,\n        by_alias: bool = True,\n        title: str | None = None,\n        description: str | None = None,\n        ref_template: str = DEFAULT_REF_TEMPLATE,\n        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n    ) -> tuple[dict[tuple[JsonSchemaKeyT, JsonSchemaMode], JsonSchemaValue], JsonSchemaValue]:\n        \"\"\"Generate a JSON schema including definitions from multiple type adapters.\n\n        Args:\n            inputs: Inputs to schema generation. The first two items will form the keys of the (first)\n                output mapping; the type adapters will provide the core schemas that get converted into\n                definitions in the output JSON schema.\n            by_alias: Whether to use alias names.\n            title: The title for the schema.\n            description: The description for the schema.\n            ref_template: The format string used for generating $ref strings.\n            schema_generator: The generator class used for creating the schema.\n\n        Returns:\n            A tuple where:\n\n                - The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n                    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n                    JsonRef references to definitions that are defined in the second returned element.)\n                - The second element is a JSON schema containing all definitions referenced in the first returned\n                    element, along with the optional title and description keys.\n\n        \"\"\"\n        schema_generator_instance = schema_generator(by_alias=by_alias, ref_template=ref_template)\n\n        inputs_ = []\n        for key, mode, adapter in inputs:\n            with adapter._with_frame_depth(1):  # +1 for json_schemas staticmethod\n                inputs_.append((key, mode, adapter.core_schema))\n\n        json_schemas_map, definitions = schema_generator_instance.generate_definitions(inputs_)\n\n        json_schema: dict[str, Any] = {}\n        if definitions:\n            json_schema['$defs'] = definitions\n        if title:\n            json_schema['title'] = title\n        if description:\n            json_schema['description'] = description\n\n        return json_schemas_map, json_schema\n", "pydantic/env_settings.py": "\"\"\"The `env_settings` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/typing.py": "\"\"\"`typing` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/parse.py": "\"\"\"The `parse` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/schema.py": "\"\"\"The `schema` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/annotated_handlers.py": "\"\"\"Type annotations to use with `__get_pydantic_core_schema__` and `__get_pydantic_json_schema__`.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom typing import TYPE_CHECKING, Any, Union\n\nfrom pydantic_core import core_schema\n\nif TYPE_CHECKING:\n    from .json_schema import JsonSchemaMode, JsonSchemaValue\n\n    CoreSchemaOrField = Union[\n        core_schema.CoreSchema,\n        core_schema.ModelField,\n        core_schema.DataclassField,\n        core_schema.TypedDictField,\n        core_schema.ComputedField,\n    ]\n\n__all__ = 'GetJsonSchemaHandler', 'GetCoreSchemaHandler'\n\n\nclass GetJsonSchemaHandler:\n    \"\"\"Handler to call into the next JSON schema generation function.\n\n    Attributes:\n        mode: Json schema mode, can be `validation` or `serialization`.\n    \"\"\"\n\n    mode: JsonSchemaMode\n\n    def __call__(self, core_schema: CoreSchemaOrField, /) -> JsonSchemaValue:\n        \"\"\"Call the inner handler and get the JsonSchemaValue it returns.\n        This will call the next JSON schema modifying function up until it calls\n        into `pydantic.json_schema.GenerateJsonSchema`, which will raise a\n        `pydantic.errors.PydanticInvalidForJsonSchema` error if it cannot generate\n        a JSON schema.\n\n        Args:\n            core_schema: A `pydantic_core.core_schema.CoreSchema`.\n\n        Returns:\n            JsonSchemaValue: The JSON schema generated by the inner JSON schema modify\n            functions.\n        \"\"\"\n        raise NotImplementedError\n\n    def resolve_ref_schema(self, maybe_ref_json_schema: JsonSchemaValue, /) -> JsonSchemaValue:\n        \"\"\"Get the real schema for a `{\"$ref\": ...}` schema.\n        If the schema given is not a `$ref` schema, it will be returned as is.\n        This means you don't have to check before calling this function.\n\n        Args:\n            maybe_ref_json_schema: A JsonSchemaValue which may be a `$ref` schema.\n\n        Raises:\n            LookupError: If the ref is not found.\n\n        Returns:\n            JsonSchemaValue: A JsonSchemaValue that has no `$ref`.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass GetCoreSchemaHandler:\n    \"\"\"Handler to call into the next CoreSchema schema generation function.\"\"\"\n\n    def __call__(self, source_type: Any, /) -> core_schema.CoreSchema:\n        \"\"\"Call the inner handler and get the CoreSchema it returns.\n        This will call the next CoreSchema modifying function up until it calls\n        into Pydantic's internal schema generation machinery, which will raise a\n        `pydantic.errors.PydanticSchemaGenerationError` error if it cannot generate\n        a CoreSchema for the given source type.\n\n        Args:\n            source_type: The input type.\n\n        Returns:\n            CoreSchema: The `pydantic-core` CoreSchema generated.\n        \"\"\"\n        raise NotImplementedError\n\n    def generate_schema(self, source_type: Any, /) -> core_schema.CoreSchema:\n        \"\"\"Generate a schema unrelated to the current context.\n        Use this function if e.g. you are handling schema generation for a sequence\n        and want to generate a schema for its items.\n        Otherwise, you may end up doing something like applying a `min_length` constraint\n        that was intended for the sequence itself to its items!\n\n        Args:\n            source_type: The input type.\n\n        Returns:\n            CoreSchema: The `pydantic-core` CoreSchema generated.\n        \"\"\"\n        raise NotImplementedError\n\n    def resolve_ref_schema(self, maybe_ref_schema: core_schema.CoreSchema, /) -> core_schema.CoreSchema:\n        \"\"\"Get the real schema for a `definition-ref` schema.\n        If the schema given is not a `definition-ref` schema, it will be returned as is.\n        This means you don't have to check before calling this function.\n\n        Args:\n            maybe_ref_schema: A `CoreSchema`, `ref`-based or not.\n\n        Raises:\n            LookupError: If the `ref` is not found.\n\n        Returns:\n            A concrete `CoreSchema`.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def field_name(self) -> str | None:\n        \"\"\"Get the name of the closest field to this validator.\"\"\"\n        raise NotImplementedError\n\n    def _get_types_namespace(self) -> dict[str, Any] | None:\n        \"\"\"Internal method used during type resolution for serializer annotations.\"\"\"\n        raise NotImplementedError\n", "pydantic/utils.py": "\"\"\"The `utils` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/_migration.py": "import sys\nfrom typing import Any, Callable, Dict\n\nfrom .version import version_short\n\nMOVED_IN_V2 = {\n    'pydantic.utils:version_info': 'pydantic.version:version_info',\n    'pydantic.error_wrappers:ValidationError': 'pydantic:ValidationError',\n    'pydantic.utils:to_camel': 'pydantic.alias_generators:to_pascal',\n    'pydantic.utils:to_lower_camel': 'pydantic.alias_generators:to_camel',\n    'pydantic:PyObject': 'pydantic.types:ImportString',\n    'pydantic.types:PyObject': 'pydantic.types:ImportString',\n    'pydantic.generics:GenericModel': 'pydantic.BaseModel',\n}\n\nDEPRECATED_MOVED_IN_V2 = {\n    'pydantic.tools:schema_of': 'pydantic.deprecated.tools:schema_of',\n    'pydantic.tools:parse_obj_as': 'pydantic.deprecated.tools:parse_obj_as',\n    'pydantic.tools:schema_json_of': 'pydantic.deprecated.tools:schema_json_of',\n    'pydantic.json:pydantic_encoder': 'pydantic.deprecated.json:pydantic_encoder',\n    'pydantic:validate_arguments': 'pydantic.deprecated.decorator:validate_arguments',\n    'pydantic.json:custom_pydantic_encoder': 'pydantic.deprecated.json:custom_pydantic_encoder',\n    'pydantic.json:timedelta_isoformat': 'pydantic.deprecated.json:timedelta_isoformat',\n    'pydantic.decorator:validate_arguments': 'pydantic.deprecated.decorator:validate_arguments',\n    'pydantic.class_validators:validator': 'pydantic.deprecated.class_validators:validator',\n    'pydantic.class_validators:root_validator': 'pydantic.deprecated.class_validators:root_validator',\n    'pydantic.config:BaseConfig': 'pydantic.deprecated.config:BaseConfig',\n    'pydantic.config:Extra': 'pydantic.deprecated.config:Extra',\n}\n\nREDIRECT_TO_V1 = {\n    f'pydantic.utils:{obj}': f'pydantic.v1.utils:{obj}'\n    for obj in (\n        'deep_update',\n        'GetterDict',\n        'lenient_issubclass',\n        'lenient_isinstance',\n        'is_valid_field',\n        'update_not_none',\n        'import_string',\n        'Representation',\n        'ROOT_KEY',\n        'smart_deepcopy',\n        'sequence_like',\n    )\n}\n\n\nREMOVED_IN_V2 = {\n    'pydantic:ConstrainedBytes',\n    'pydantic:ConstrainedDate',\n    'pydantic:ConstrainedDecimal',\n    'pydantic:ConstrainedFloat',\n    'pydantic:ConstrainedFrozenSet',\n    'pydantic:ConstrainedInt',\n    'pydantic:ConstrainedList',\n    'pydantic:ConstrainedSet',\n    'pydantic:ConstrainedStr',\n    'pydantic:JsonWrapper',\n    'pydantic:NoneBytes',\n    'pydantic:NoneStr',\n    'pydantic:NoneStrBytes',\n    'pydantic:Protocol',\n    'pydantic:Required',\n    'pydantic:StrBytes',\n    'pydantic:compiled',\n    'pydantic.config:get_config',\n    'pydantic.config:inherit_config',\n    'pydantic.config:prepare_config',\n    'pydantic:create_model_from_namedtuple',\n    'pydantic:create_model_from_typeddict',\n    'pydantic.dataclasses:create_pydantic_model_from_dataclass',\n    'pydantic.dataclasses:make_dataclass_validator',\n    'pydantic.dataclasses:set_validation',\n    'pydantic.datetime_parse:parse_date',\n    'pydantic.datetime_parse:parse_time',\n    'pydantic.datetime_parse:parse_datetime',\n    'pydantic.datetime_parse:parse_duration',\n    'pydantic.error_wrappers:ErrorWrapper',\n    'pydantic.errors:AnyStrMaxLengthError',\n    'pydantic.errors:AnyStrMinLengthError',\n    'pydantic.errors:ArbitraryTypeError',\n    'pydantic.errors:BoolError',\n    'pydantic.errors:BytesError',\n    'pydantic.errors:CallableError',\n    'pydantic.errors:ClassError',\n    'pydantic.errors:ColorError',\n    'pydantic.errors:ConfigError',\n    'pydantic.errors:DataclassTypeError',\n    'pydantic.errors:DateError',\n    'pydantic.errors:DateNotInTheFutureError',\n    'pydantic.errors:DateNotInThePastError',\n    'pydantic.errors:DateTimeError',\n    'pydantic.errors:DecimalError',\n    'pydantic.errors:DecimalIsNotFiniteError',\n    'pydantic.errors:DecimalMaxDigitsError',\n    'pydantic.errors:DecimalMaxPlacesError',\n    'pydantic.errors:DecimalWholeDigitsError',\n    'pydantic.errors:DictError',\n    'pydantic.errors:DurationError',\n    'pydantic.errors:EmailError',\n    'pydantic.errors:EnumError',\n    'pydantic.errors:EnumMemberError',\n    'pydantic.errors:ExtraError',\n    'pydantic.errors:FloatError',\n    'pydantic.errors:FrozenSetError',\n    'pydantic.errors:FrozenSetMaxLengthError',\n    'pydantic.errors:FrozenSetMinLengthError',\n    'pydantic.errors:HashableError',\n    'pydantic.errors:IPv4AddressError',\n    'pydantic.errors:IPv4InterfaceError',\n    'pydantic.errors:IPv4NetworkError',\n    'pydantic.errors:IPv6AddressError',\n    'pydantic.errors:IPv6InterfaceError',\n    'pydantic.errors:IPv6NetworkError',\n    'pydantic.errors:IPvAnyAddressError',\n    'pydantic.errors:IPvAnyInterfaceError',\n    'pydantic.errors:IPvAnyNetworkError',\n    'pydantic.errors:IntEnumError',\n    'pydantic.errors:IntegerError',\n    'pydantic.errors:InvalidByteSize',\n    'pydantic.errors:InvalidByteSizeUnit',\n    'pydantic.errors:InvalidDiscriminator',\n    'pydantic.errors:InvalidLengthForBrand',\n    'pydantic.errors:JsonError',\n    'pydantic.errors:JsonTypeError',\n    'pydantic.errors:ListError',\n    'pydantic.errors:ListMaxLengthError',\n    'pydantic.errors:ListMinLengthError',\n    'pydantic.errors:ListUniqueItemsError',\n    'pydantic.errors:LuhnValidationError',\n    'pydantic.errors:MissingDiscriminator',\n    'pydantic.errors:MissingError',\n    'pydantic.errors:NoneIsAllowedError',\n    'pydantic.errors:NoneIsNotAllowedError',\n    'pydantic.errors:NotDigitError',\n    'pydantic.errors:NotNoneError',\n    'pydantic.errors:NumberNotGeError',\n    'pydantic.errors:NumberNotGtError',\n    'pydantic.errors:NumberNotLeError',\n    'pydantic.errors:NumberNotLtError',\n    'pydantic.errors:NumberNotMultipleError',\n    'pydantic.errors:PathError',\n    'pydantic.errors:PathNotADirectoryError',\n    'pydantic.errors:PathNotAFileError',\n    'pydantic.errors:PathNotExistsError',\n    'pydantic.errors:PatternError',\n    'pydantic.errors:PyObjectError',\n    'pydantic.errors:PydanticTypeError',\n    'pydantic.errors:PydanticValueError',\n    'pydantic.errors:SequenceError',\n    'pydantic.errors:SetError',\n    'pydantic.errors:SetMaxLengthError',\n    'pydantic.errors:SetMinLengthError',\n    'pydantic.errors:StrError',\n    'pydantic.errors:StrRegexError',\n    'pydantic.errors:StrictBoolError',\n    'pydantic.errors:SubclassError',\n    'pydantic.errors:TimeError',\n    'pydantic.errors:TupleError',\n    'pydantic.errors:TupleLengthError',\n    'pydantic.errors:UUIDError',\n    'pydantic.errors:UUIDVersionError',\n    'pydantic.errors:UrlError',\n    'pydantic.errors:UrlExtraError',\n    'pydantic.errors:UrlHostError',\n    'pydantic.errors:UrlHostTldError',\n    'pydantic.errors:UrlPortError',\n    'pydantic.errors:UrlSchemeError',\n    'pydantic.errors:UrlSchemePermittedError',\n    'pydantic.errors:UrlUserInfoError',\n    'pydantic.errors:WrongConstantError',\n    'pydantic.main:validate_model',\n    'pydantic.networks:stricturl',\n    'pydantic:parse_file_as',\n    'pydantic:parse_raw_as',\n    'pydantic:stricturl',\n    'pydantic.tools:parse_file_as',\n    'pydantic.tools:parse_raw_as',\n    'pydantic.types:ConstrainedBytes',\n    'pydantic.types:ConstrainedDate',\n    'pydantic.types:ConstrainedDecimal',\n    'pydantic.types:ConstrainedFloat',\n    'pydantic.types:ConstrainedFrozenSet',\n    'pydantic.types:ConstrainedInt',\n    'pydantic.types:ConstrainedList',\n    'pydantic.types:ConstrainedSet',\n    'pydantic.types:ConstrainedStr',\n    'pydantic.types:JsonWrapper',\n    'pydantic.types:NoneBytes',\n    'pydantic.types:NoneStr',\n    'pydantic.types:NoneStrBytes',\n    'pydantic.types:StrBytes',\n    'pydantic.typing:evaluate_forwardref',\n    'pydantic.typing:AbstractSetIntStr',\n    'pydantic.typing:AnyCallable',\n    'pydantic.typing:AnyClassMethod',\n    'pydantic.typing:CallableGenerator',\n    'pydantic.typing:DictAny',\n    'pydantic.typing:DictIntStrAny',\n    'pydantic.typing:DictStrAny',\n    'pydantic.typing:IntStr',\n    'pydantic.typing:ListStr',\n    'pydantic.typing:MappingIntStrAny',\n    'pydantic.typing:NoArgAnyCallable',\n    'pydantic.typing:NoneType',\n    'pydantic.typing:ReprArgs',\n    'pydantic.typing:SetStr',\n    'pydantic.typing:StrPath',\n    'pydantic.typing:TupleGenerator',\n    'pydantic.typing:WithArgsTypes',\n    'pydantic.typing:all_literal_values',\n    'pydantic.typing:display_as_type',\n    'pydantic.typing:get_all_type_hints',\n    'pydantic.typing:get_args',\n    'pydantic.typing:get_origin',\n    'pydantic.typing:get_sub_types',\n    'pydantic.typing:is_callable_type',\n    'pydantic.typing:is_classvar',\n    'pydantic.typing:is_finalvar',\n    'pydantic.typing:is_literal_type',\n    'pydantic.typing:is_namedtuple',\n    'pydantic.typing:is_new_type',\n    'pydantic.typing:is_none_type',\n    'pydantic.typing:is_typeddict',\n    'pydantic.typing:is_typeddict_special',\n    'pydantic.typing:is_union',\n    'pydantic.typing:new_type_supertype',\n    'pydantic.typing:resolve_annotations',\n    'pydantic.typing:typing_base',\n    'pydantic.typing:update_field_forward_refs',\n    'pydantic.typing:update_model_forward_refs',\n    'pydantic.utils:ClassAttribute',\n    'pydantic.utils:DUNDER_ATTRIBUTES',\n    'pydantic.utils:PyObjectStr',\n    'pydantic.utils:ValueItems',\n    'pydantic.utils:almost_equal_floats',\n    'pydantic.utils:get_discriminator_alias_and_values',\n    'pydantic.utils:get_model',\n    'pydantic.utils:get_unique_discriminator_alias',\n    'pydantic.utils:in_ipython',\n    'pydantic.utils:is_valid_identifier',\n    'pydantic.utils:path_type',\n    'pydantic.utils:validate_field_name',\n    'pydantic:validate_model',\n}\n\n\ndef getattr_migration(module: str) -> Callable[[str], Any]:\n    \"\"\"Implement PEP 562 for objects that were either moved or removed on the migration\n    to V2.\n\n    Args:\n        module: The module name.\n\n    Returns:\n        A callable that will raise an error if the object is not found.\n    \"\"\"\n    # This avoids circular import with errors.py.\n    from .errors import PydanticImportError\n\n    def wrapper(name: str) -> object:\n        \"\"\"Raise an error if the object is not found, or warn if it was moved.\n\n        In case it was moved, it still returns the object.\n\n        Args:\n            name: The object name.\n\n        Returns:\n            The object.\n        \"\"\"\n        if name == '__path__':\n            raise AttributeError(f'module {module!r} has no attribute {name!r}')\n\n        import warnings\n\n        from ._internal._validators import import_string\n\n        import_path = f'{module}:{name}'\n        if import_path in MOVED_IN_V2.keys():\n            new_location = MOVED_IN_V2[import_path]\n            warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n            return import_string(MOVED_IN_V2[import_path])\n        if import_path in DEPRECATED_MOVED_IN_V2:\n            # skip the warning here because a deprecation warning will be raised elsewhere\n            return import_string(DEPRECATED_MOVED_IN_V2[import_path])\n        if import_path in REDIRECT_TO_V1:\n            new_location = REDIRECT_TO_V1[import_path]\n            warnings.warn(\n                f'`{import_path}` has been removed. We are importing from `{new_location}` instead.'\n                'See the migration guide for more details: https://docs.pydantic.dev/latest/migration/'\n            )\n            return import_string(REDIRECT_TO_V1[import_path])\n        if import_path == 'pydantic:BaseSettings':\n            raise PydanticImportError(\n                '`BaseSettings` has been moved to the `pydantic-settings` package. '\n                f'See https://docs.pydantic.dev/{version_short()}/migration/#basesettings-has-moved-to-pydantic-settings '\n                'for more details.'\n            )\n        if import_path in REMOVED_IN_V2:\n            raise PydanticImportError(f'`{import_path}` has been removed in V2.')\n        globals: Dict[str, Any] = sys.modules[module].__dict__\n        if name in globals:\n            return globals[name]\n        raise AttributeError(f'module {module!r} has no attribute {name!r}')\n\n    return wrapper\n", "pydantic/fields.py": "\"\"\"Defining fields on models.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport inspect\nimport sys\nimport typing\nfrom copy import copy\nfrom dataclasses import Field as DataclassField\nfrom functools import cached_property\nfrom typing import Any, ClassVar\nfrom warnings import warn\n\nimport annotated_types\nimport typing_extensions\nfrom pydantic_core import PydanticUndefined\nfrom typing_extensions import Literal, TypeAlias, Unpack, deprecated\n\nfrom . import types\nfrom ._internal import _decorators, _fields, _generics, _internal_dataclass, _repr, _typing_extra, _utils\nfrom .aliases import AliasChoices, AliasPath\nfrom .config import JsonDict\nfrom .errors import PydanticUserError\nfrom .warnings import PydanticDeprecatedSince20\n\nif typing.TYPE_CHECKING:\n    from ._internal._repr import ReprArgs\nelse:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n__all__ = 'Field', 'PrivateAttr', 'computed_field'\n\n\n_Unset: Any = PydanticUndefined\n\nif sys.version_info >= (3, 13):\n    import warnings\n\n    Deprecated: TypeAlias = warnings.deprecated | deprecated\nelse:\n    Deprecated: TypeAlias = deprecated\n\n\nclass _FromFieldInfoInputs(typing_extensions.TypedDict, total=False):\n    \"\"\"This class exists solely to add type checking for the `**kwargs` in `FieldInfo.from_field`.\"\"\"\n\n    annotation: type[Any] | None\n    default_factory: typing.Callable[[], Any] | None\n    alias: str | None\n    alias_priority: int | None\n    validation_alias: str | AliasPath | AliasChoices | None\n    serialization_alias: str | None\n    title: str | None\n    field_title_generator: typing_extensions.Callable[[str, FieldInfo], str] | None\n    description: str | None\n    examples: list[Any] | None\n    exclude: bool | None\n    gt: annotated_types.SupportsGt | None\n    ge: annotated_types.SupportsGe | None\n    lt: annotated_types.SupportsLt | None\n    le: annotated_types.SupportsLe | None\n    multiple_of: float | None\n    strict: bool | None\n    min_length: int | None\n    max_length: int | None\n    pattern: str | typing.Pattern[str] | None\n    allow_inf_nan: bool | None\n    max_digits: int | None\n    decimal_places: int | None\n    union_mode: Literal['smart', 'left_to_right'] | None\n    discriminator: str | types.Discriminator | None\n    deprecated: Deprecated | str | bool | None\n    json_schema_extra: JsonDict | typing.Callable[[JsonDict], None] | None\n    frozen: bool | None\n    validate_default: bool | None\n    repr: bool\n    init: bool | None\n    init_var: bool | None\n    kw_only: bool | None\n    coerce_numbers_to_str: bool | None\n\n\nclass _FieldInfoInputs(_FromFieldInfoInputs, total=False):\n    \"\"\"This class exists solely to add type checking for the `**kwargs` in `FieldInfo.__init__`.\"\"\"\n\n    default: Any\n\n\nclass FieldInfo(_repr.Representation):\n    \"\"\"This class holds information about a field.\n\n    `FieldInfo` is used for any field definition regardless of whether the [`Field()`][pydantic.fields.Field]\n    function is explicitly used.\n\n    !!! warning\n        You generally shouldn't be creating `FieldInfo` directly, you'll only need to use it when accessing\n        [`BaseModel`][pydantic.main.BaseModel] `.model_fields` internals.\n\n    Attributes:\n        annotation: The type annotation of the field.\n        default: The default value of the field.\n        default_factory: The factory function used to construct the default for the field.\n        alias: The alias name of the field.\n        alias_priority: The priority of the field's alias.\n        validation_alias: The validation alias of the field.\n        serialization_alias: The serialization alias of the field.\n        title: The title of the field.\n        field_title_generator: A callable that takes a field name and returns title for it.\n        description: The description of the field.\n        examples: List of examples of the field.\n        exclude: Whether to exclude the field from the model serialization.\n        discriminator: Field name or Discriminator for discriminating the type in a tagged union.\n        deprecated: A deprecation message, an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport,\n            or a boolean. If `True`, a default deprecation message will be emitted when accessing the field.\n        json_schema_extra: A dict or callable to provide extra JSON schema properties.\n        frozen: Whether the field is frozen.\n        validate_default: Whether to validate the default value of the field.\n        repr: Whether to include the field in representation of the model.\n        init: Whether the field should be included in the constructor of the dataclass.\n        init_var: Whether the field should _only_ be included in the constructor of the dataclass, and not stored.\n        kw_only: Whether the field should be a keyword-only argument in the constructor of the dataclass.\n        metadata: List of metadata constraints.\n    \"\"\"\n\n    annotation: type[Any] | None\n    default: Any\n    default_factory: typing.Callable[[], Any] | None\n    alias: str | None\n    alias_priority: int | None\n    validation_alias: str | AliasPath | AliasChoices | None\n    serialization_alias: str | None\n    title: str | None\n    field_title_generator: typing.Callable[[str, FieldInfo], str] | None\n    description: str | None\n    examples: list[Any] | None\n    exclude: bool | None\n    discriminator: str | types.Discriminator | None\n    deprecated: Deprecated | str | bool | None\n    json_schema_extra: JsonDict | typing.Callable[[JsonDict], None] | None\n    frozen: bool | None\n    validate_default: bool | None\n    repr: bool\n    init: bool | None\n    init_var: bool | None\n    kw_only: bool | None\n    metadata: list[Any]\n\n    __slots__ = (\n        'annotation',\n        'default',\n        'default_factory',\n        'alias',\n        'alias_priority',\n        'validation_alias',\n        'serialization_alias',\n        'title',\n        'field_title_generator',\n        'description',\n        'examples',\n        'exclude',\n        'discriminator',\n        'deprecated',\n        'json_schema_extra',\n        'frozen',\n        'validate_default',\n        'repr',\n        'init',\n        'init_var',\n        'kw_only',\n        'metadata',\n        '_attributes_set',\n    )\n\n    # used to convert kwargs to metadata/constraints,\n    # None has a special meaning - these items are collected into a `PydanticGeneralMetadata`\n    metadata_lookup: ClassVar[dict[str, typing.Callable[[Any], Any] | None]] = {\n        'strict': types.Strict,\n        'gt': annotated_types.Gt,\n        'ge': annotated_types.Ge,\n        'lt': annotated_types.Lt,\n        'le': annotated_types.Le,\n        'multiple_of': annotated_types.MultipleOf,\n        'min_length': annotated_types.MinLen,\n        'max_length': annotated_types.MaxLen,\n        'pattern': None,\n        'allow_inf_nan': None,\n        'max_digits': None,\n        'decimal_places': None,\n        'union_mode': None,\n        'coerce_numbers_to_str': None,\n    }\n\n    def __init__(self, **kwargs: Unpack[_FieldInfoInputs]) -> None:\n        \"\"\"This class should generally not be initialized directly; instead, use the `pydantic.fields.Field` function\n        or one of the constructor classmethods.\n\n        See the signature of `pydantic.fields.Field` for more details about the expected arguments.\n        \"\"\"\n        self._attributes_set = {k: v for k, v in kwargs.items() if v is not _Unset}\n        kwargs = {k: _DefaultValues.get(k) if v is _Unset else v for k, v in kwargs.items()}  # type: ignore\n        self.annotation, annotation_metadata = self._extract_metadata(kwargs.get('annotation'))\n\n        default = kwargs.pop('default', PydanticUndefined)\n        if default is Ellipsis:\n            self.default = PydanticUndefined\n        else:\n            self.default = default\n\n        self.default_factory = kwargs.pop('default_factory', None)\n\n        if self.default is not PydanticUndefined and self.default_factory is not None:\n            raise TypeError('cannot specify both default and default_factory')\n\n        self.alias = kwargs.pop('alias', None)\n        self.validation_alias = kwargs.pop('validation_alias', None)\n        self.serialization_alias = kwargs.pop('serialization_alias', None)\n        alias_is_set = any(alias is not None for alias in (self.alias, self.validation_alias, self.serialization_alias))\n        self.alias_priority = kwargs.pop('alias_priority', None) or 2 if alias_is_set else None\n        self.title = kwargs.pop('title', None)\n        self.field_title_generator = kwargs.pop('field_title_generator', None)\n        self.description = kwargs.pop('description', None)\n        self.examples = kwargs.pop('examples', None)\n        self.exclude = kwargs.pop('exclude', None)\n        self.discriminator = kwargs.pop('discriminator', None)\n        # For compatibility with FastAPI<=0.110.0, we preserve the existing value if it is not overridden\n        self.deprecated = kwargs.pop('deprecated', getattr(self, 'deprecated', None))\n        self.repr = kwargs.pop('repr', True)\n        self.json_schema_extra = kwargs.pop('json_schema_extra', None)\n        self.validate_default = kwargs.pop('validate_default', None)\n        self.frozen = kwargs.pop('frozen', None)\n        # currently only used on dataclasses\n        self.init = kwargs.pop('init', None)\n        self.init_var = kwargs.pop('init_var', None)\n        self.kw_only = kwargs.pop('kw_only', None)\n\n        self.metadata = self._collect_metadata(kwargs) + annotation_metadata  # type: ignore\n\n    @staticmethod\n    def from_field(default: Any = PydanticUndefined, **kwargs: Unpack[_FromFieldInfoInputs]) -> FieldInfo:\n        \"\"\"Create a new `FieldInfo` object with the `Field` function.\n\n        Args:\n            default: The default value for the field. Defaults to Undefined.\n            **kwargs: Additional arguments dictionary.\n\n        Raises:\n            TypeError: If 'annotation' is passed as a keyword argument.\n\n        Returns:\n            A new FieldInfo object with the given parameters.\n\n        Example:\n            This is how you can create a field with default value like this:\n\n            ```python\n            import pydantic\n\n            class MyModel(pydantic.BaseModel):\n                foo: int = pydantic.Field(4)\n            ```\n        \"\"\"\n        if 'annotation' in kwargs:\n            raise TypeError('\"annotation\" is not permitted as a Field keyword argument')\n        return FieldInfo(default=default, **kwargs)\n\n    @staticmethod\n    def from_annotation(annotation: type[Any]) -> FieldInfo:\n        \"\"\"Creates a `FieldInfo` instance from a bare annotation.\n\n        This function is used internally to create a `FieldInfo` from a bare annotation like this:\n\n        ```python\n        import pydantic\n\n        class MyModel(pydantic.BaseModel):\n            foo: int  # <-- like this\n        ```\n\n        We also account for the case where the annotation can be an instance of `Annotated` and where\n        one of the (not first) arguments in `Annotated` is an instance of `FieldInfo`, e.g.:\n\n        ```python\n        import annotated_types\n        from typing_extensions import Annotated\n\n        import pydantic\n\n        class MyModel(pydantic.BaseModel):\n            foo: Annotated[int, annotated_types.Gt(42)]\n            bar: Annotated[int, pydantic.Field(gt=42)]\n        ```\n\n        Args:\n            annotation: An annotation object.\n\n        Returns:\n            An instance of the field metadata.\n        \"\"\"\n        final = False\n        if _typing_extra.is_finalvar(annotation):\n            final = True\n            if annotation is not typing_extensions.Final:\n                annotation = typing_extensions.get_args(annotation)[0]\n\n        if _typing_extra.is_annotated(annotation):\n            first_arg, *extra_args = typing_extensions.get_args(annotation)\n            if _typing_extra.is_finalvar(first_arg):\n                final = True\n            field_info_annotations = [a for a in extra_args if isinstance(a, FieldInfo)]\n            field_info = FieldInfo.merge_field_infos(*field_info_annotations, annotation=first_arg)\n            if field_info:\n                new_field_info = copy(field_info)\n                new_field_info.annotation = first_arg\n                new_field_info.frozen = final or field_info.frozen\n                metadata: list[Any] = []\n                for a in extra_args:\n                    if _typing_extra.is_deprecated_instance(a):\n                        new_field_info.deprecated = a.message\n                    elif not isinstance(a, FieldInfo):\n                        metadata.append(a)\n                    else:\n                        metadata.extend(a.metadata)\n                new_field_info.metadata = metadata\n                return new_field_info\n\n        return FieldInfo(annotation=annotation, frozen=final or None)  # pyright: ignore[reportArgumentType]\n\n    @staticmethod\n    def from_annotated_attribute(annotation: type[Any], default: Any) -> FieldInfo:\n        \"\"\"Create `FieldInfo` from an annotation with a default value.\n\n        This is used in cases like the following:\n\n        ```python\n        import annotated_types\n        from typing_extensions import Annotated\n\n        import pydantic\n\n        class MyModel(pydantic.BaseModel):\n            foo: int = 4  # <-- like this\n            bar: Annotated[int, annotated_types.Gt(4)] = 4  # <-- or this\n            spam: Annotated[int, pydantic.Field(gt=4)] = 4  # <-- or this\n        ```\n\n        Args:\n            annotation: The type annotation of the field.\n            default: The default value of the field.\n\n        Returns:\n            A field object with the passed values.\n        \"\"\"\n        if annotation is default:\n            raise PydanticUserError(\n                'Error when building FieldInfo from annotated attribute. '\n                \"Make sure you don't have any field name clashing with a type annotation \",\n                code='unevaluable-type-annotation',\n            )\n\n        final = False\n        if _typing_extra.is_finalvar(annotation):\n            final = True\n            if annotation is not typing_extensions.Final:\n                annotation = typing_extensions.get_args(annotation)[0]\n\n        if isinstance(default, FieldInfo):\n            default.annotation, annotation_metadata = FieldInfo._extract_metadata(annotation)  # pyright: ignore[reportArgumentType]\n            default.metadata += annotation_metadata\n            default = default.merge_field_infos(\n                *[x for x in annotation_metadata if isinstance(x, FieldInfo)], default, annotation=default.annotation\n            )\n            default.frozen = final or default.frozen\n            return default\n        elif isinstance(default, dataclasses.Field):\n            init_var = False\n            if annotation is dataclasses.InitVar:\n                init_var = True\n                annotation = typing.cast(Any, Any)\n            elif isinstance(annotation, dataclasses.InitVar):\n                init_var = True\n                annotation = annotation.type\n            pydantic_field = FieldInfo._from_dataclass_field(default)\n            pydantic_field.annotation, annotation_metadata = FieldInfo._extract_metadata(annotation)  # pyright: ignore[reportArgumentType]\n            pydantic_field.metadata += annotation_metadata\n            pydantic_field = pydantic_field.merge_field_infos(\n                *[x for x in annotation_metadata if isinstance(x, FieldInfo)],\n                pydantic_field,\n                annotation=pydantic_field.annotation,\n            )\n            pydantic_field.frozen = final or pydantic_field.frozen\n            pydantic_field.init_var = init_var\n            pydantic_field.init = getattr(default, 'init', None)\n            pydantic_field.kw_only = getattr(default, 'kw_only', None)\n            return pydantic_field\n        else:\n            if _typing_extra.is_annotated(annotation):\n                first_arg, *extra_args = typing_extensions.get_args(annotation)\n                field_infos = [a for a in extra_args if isinstance(a, FieldInfo)]\n                field_info = FieldInfo.merge_field_infos(*field_infos, annotation=first_arg, default=default)\n                metadata: list[Any] = []\n                for a in extra_args:\n                    if _typing_extra.is_deprecated_instance(a):\n                        field_info.deprecated = a.message\n                    elif not isinstance(a, FieldInfo):\n                        metadata.append(a)\n                    else:\n                        metadata.extend(a.metadata)\n                field_info.metadata = metadata\n                return field_info\n\n            return FieldInfo(annotation=annotation, default=default, frozen=final or None)  # pyright: ignore[reportArgumentType]\n\n    @staticmethod\n    def merge_field_infos(*field_infos: FieldInfo, **overrides: Any) -> FieldInfo:\n        \"\"\"Merge `FieldInfo` instances keeping only explicitly set attributes.\n\n        Later `FieldInfo` instances override earlier ones.\n\n        Returns:\n            FieldInfo: A merged FieldInfo instance.\n        \"\"\"\n        flattened_field_infos: list[FieldInfo] = []\n        for field_info in field_infos:\n            flattened_field_infos.extend(x for x in field_info.metadata if isinstance(x, FieldInfo))\n            flattened_field_infos.append(field_info)\n        field_infos = tuple(flattened_field_infos)\n        if len(field_infos) == 1:\n            # No merging necessary, but we still need to make a copy and apply the overrides\n            field_info = copy(field_infos[0])\n            field_info._attributes_set.update(overrides)\n\n            default_override = overrides.pop('default', PydanticUndefined)\n            if default_override is Ellipsis:\n                default_override = PydanticUndefined\n            if default_override is not PydanticUndefined:\n                field_info.default = default_override\n\n            for k, v in overrides.items():\n                setattr(field_info, k, v)\n            return field_info  # type: ignore\n\n        new_kwargs: dict[str, Any] = {}\n        metadata = {}\n        for field_info in field_infos:\n            new_kwargs.update(field_info._attributes_set)\n            for x in field_info.metadata:\n                if not isinstance(x, FieldInfo):\n                    metadata[type(x)] = x\n        new_kwargs.update(overrides)\n        field_info = FieldInfo(**new_kwargs)\n        field_info.metadata = list(metadata.values())\n        return field_info\n\n    @staticmethod\n    def _from_dataclass_field(dc_field: DataclassField[Any]) -> FieldInfo:\n        \"\"\"Return a new `FieldInfo` instance from a `dataclasses.Field` instance.\n\n        Args:\n            dc_field: The `dataclasses.Field` instance to convert.\n\n        Returns:\n            The corresponding `FieldInfo` instance.\n\n        Raises:\n            TypeError: If any of the `FieldInfo` kwargs does not match the `dataclass.Field` kwargs.\n        \"\"\"\n        default = dc_field.default\n        if default is dataclasses.MISSING:\n            default = PydanticUndefined\n\n        if dc_field.default_factory is dataclasses.MISSING:\n            default_factory: typing.Callable[[], Any] | None = None\n        else:\n            default_factory = dc_field.default_factory\n\n        # use the `Field` function so in correct kwargs raise the correct `TypeError`\n        dc_field_metadata = {k: v for k, v in dc_field.metadata.items() if k in _FIELD_ARG_NAMES}\n        return Field(default=default, default_factory=default_factory, repr=dc_field.repr, **dc_field_metadata)\n\n    @staticmethod\n    def _extract_metadata(annotation: type[Any] | None) -> tuple[type[Any] | None, list[Any]]:\n        \"\"\"Tries to extract metadata/constraints from an annotation if it uses `Annotated`.\n\n        Args:\n            annotation: The type hint annotation for which metadata has to be extracted.\n\n        Returns:\n            A tuple containing the extracted metadata type and the list of extra arguments.\n        \"\"\"\n        if annotation is not None:\n            if _typing_extra.is_annotated(annotation):\n                first_arg, *extra_args = typing_extensions.get_args(annotation)\n                return first_arg, list(extra_args)\n\n        return annotation, []\n\n    @staticmethod\n    def _collect_metadata(kwargs: dict[str, Any]) -> list[Any]:\n        \"\"\"Collect annotations from kwargs.\n\n        Args:\n            kwargs: Keyword arguments passed to the function.\n\n        Returns:\n            A list of metadata objects - a combination of `annotated_types.BaseMetadata` and\n                `PydanticMetadata`.\n        \"\"\"\n        metadata: list[Any] = []\n        general_metadata = {}\n        for key, value in list(kwargs.items()):\n            try:\n                marker = FieldInfo.metadata_lookup[key]\n            except KeyError:\n                continue\n\n            del kwargs[key]\n            if value is not None:\n                if marker is None:\n                    general_metadata[key] = value\n                else:\n                    metadata.append(marker(value))\n        if general_metadata:\n            metadata.append(_fields.pydantic_general_metadata(**general_metadata))\n        return metadata\n\n    @property\n    def deprecation_message(self) -> str | None:\n        \"\"\"The deprecation message to be emitted, or `None` if not set.\"\"\"\n        if self.deprecated is None:\n            return None\n        if isinstance(self.deprecated, bool):\n            return 'deprecated' if self.deprecated else None\n        return self.deprecated if isinstance(self.deprecated, str) else self.deprecated.message\n\n    def get_default(self, *, call_default_factory: bool = False) -> Any:\n        \"\"\"Get the default value.\n\n        We expose an option for whether to call the default_factory (if present), as calling it may\n        result in side effects that we want to avoid. However, there are times when it really should\n        be called (namely, when instantiating a model via `model_construct`).\n\n        Args:\n            call_default_factory: Whether to call the default_factory or not. Defaults to `False`.\n\n        Returns:\n            The default value, calling the default factory if requested or `None` if not set.\n        \"\"\"\n        if self.default_factory is None:\n            return _utils.smart_deepcopy(self.default)\n        elif call_default_factory:\n            return self.default_factory()\n        else:\n            return None\n\n    def is_required(self) -> bool:\n        \"\"\"Check if the field is required (i.e., does not have a default value or factory).\n\n        Returns:\n            `True` if the field is required, `False` otherwise.\n        \"\"\"\n        return self.default is PydanticUndefined and self.default_factory is None\n\n    def rebuild_annotation(self) -> Any:\n        \"\"\"Attempts to rebuild the original annotation for use in function signatures.\n\n        If metadata is present, it adds it to the original annotation using\n        `Annotated`. Otherwise, it returns the original annotation as-is.\n\n        Note that because the metadata has been flattened, the original annotation\n        may not be reconstructed exactly as originally provided, e.g. if the original\n        type had unrecognized annotations, or was annotated with a call to `pydantic.Field`.\n\n        Returns:\n            The rebuilt annotation.\n        \"\"\"\n        if not self.metadata:\n            return self.annotation\n        else:\n            # Annotated arguments must be a tuple\n            return typing_extensions.Annotated[(self.annotation, *self.metadata)]  # type: ignore\n\n    def apply_typevars_map(self, typevars_map: dict[Any, Any] | None, types_namespace: dict[str, Any] | None) -> None:\n        \"\"\"Apply a `typevars_map` to the annotation.\n\n        This method is used when analyzing parametrized generic types to replace typevars with their concrete types.\n\n        This method applies the `typevars_map` to the annotation in place.\n\n        Args:\n            typevars_map: A dictionary mapping type variables to their concrete types.\n            types_namespace (dict | None): A dictionary containing related types to the annotated type.\n\n        See Also:\n            pydantic._internal._generics.replace_types is used for replacing the typevars with\n                their concrete types.\n        \"\"\"\n        annotation = _typing_extra.eval_type_lenient(self.annotation, types_namespace)\n        self.annotation = _generics.replace_types(annotation, typevars_map)\n\n    def __repr_args__(self) -> ReprArgs:\n        yield 'annotation', _repr.PlainRepr(_repr.display_as_type(self.annotation))\n        yield 'required', self.is_required()\n\n        for s in self.__slots__:\n            if s == '_attributes_set':\n                continue\n            if s == 'annotation':\n                continue\n            elif s == 'metadata' and not self.metadata:\n                continue\n            elif s == 'repr' and self.repr is True:\n                continue\n            if s == 'frozen' and self.frozen is False:\n                continue\n            if s == 'validation_alias' and self.validation_alias == self.alias:\n                continue\n            if s == 'serialization_alias' and self.serialization_alias == self.alias:\n                continue\n            if s == 'default' and self.default is not PydanticUndefined:\n                yield 'default', self.default\n            elif s == 'default_factory' and self.default_factory is not None:\n                yield 'default_factory', _repr.PlainRepr(_repr.display_as_type(self.default_factory))\n            else:\n                value = getattr(self, s)\n                if value is not None and value is not PydanticUndefined:\n                    yield s, value\n\n\nclass _EmptyKwargs(typing_extensions.TypedDict):\n    \"\"\"This class exists solely to ensure that type checking warns about passing `**extra` in `Field`.\"\"\"\n\n\n_DefaultValues = dict(\n    default=...,\n    default_factory=None,\n    alias=None,\n    alias_priority=None,\n    validation_alias=None,\n    serialization_alias=None,\n    title=None,\n    description=None,\n    examples=None,\n    exclude=None,\n    discriminator=None,\n    json_schema_extra=None,\n    frozen=None,\n    validate_default=None,\n    repr=True,\n    init=None,\n    init_var=None,\n    kw_only=None,\n    pattern=None,\n    strict=None,\n    gt=None,\n    ge=None,\n    lt=None,\n    le=None,\n    multiple_of=None,\n    allow_inf_nan=None,\n    max_digits=None,\n    decimal_places=None,\n    min_length=None,\n    max_length=None,\n    coerce_numbers_to_str=None,\n)\n\n\ndef Field(  # noqa: C901\n    default: Any = PydanticUndefined,\n    *,\n    default_factory: typing.Callable[[], Any] | None = _Unset,\n    alias: str | None = _Unset,\n    alias_priority: int | None = _Unset,\n    validation_alias: str | AliasPath | AliasChoices | None = _Unset,\n    serialization_alias: str | None = _Unset,\n    title: str | None = _Unset,\n    field_title_generator: typing_extensions.Callable[[str, FieldInfo], str] | None = _Unset,\n    description: str | None = _Unset,\n    examples: list[Any] | None = _Unset,\n    exclude: bool | None = _Unset,\n    discriminator: str | types.Discriminator | None = _Unset,\n    deprecated: Deprecated | str | bool | None = _Unset,\n    json_schema_extra: JsonDict | typing.Callable[[JsonDict], None] | None = _Unset,\n    frozen: bool | None = _Unset,\n    validate_default: bool | None = _Unset,\n    repr: bool = _Unset,\n    init: bool | None = _Unset,\n    init_var: bool | None = _Unset,\n    kw_only: bool | None = _Unset,\n    pattern: str | typing.Pattern[str] | None = _Unset,\n    strict: bool | None = _Unset,\n    coerce_numbers_to_str: bool | None = _Unset,\n    gt: annotated_types.SupportsGt | None = _Unset,\n    ge: annotated_types.SupportsGe | None = _Unset,\n    lt: annotated_types.SupportsLt | None = _Unset,\n    le: annotated_types.SupportsLe | None = _Unset,\n    multiple_of: float | None = _Unset,\n    allow_inf_nan: bool | None = _Unset,\n    max_digits: int | None = _Unset,\n    decimal_places: int | None = _Unset,\n    min_length: int | None = _Unset,\n    max_length: int | None = _Unset,\n    union_mode: Literal['smart', 'left_to_right'] = _Unset,\n    **extra: Unpack[_EmptyKwargs],\n) -> Any:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/fields\n\n    Create a field for objects that can be configured.\n\n    Used to provide extra information about a field, either for the model schema or complex validation. Some arguments\n    apply only to number fields (`int`, `float`, `Decimal`) and some apply only to `str`.\n\n    Note:\n        - Any `_Unset` objects will be replaced by the corresponding value defined in the `_DefaultValues` dictionary. If a key for the `_Unset` object is not found in the `_DefaultValues` dictionary, it will default to `None`\n\n    Args:\n        default: Default value if the field is not set.\n        default_factory: A callable to generate the default value, such as :func:`~datetime.utcnow`.\n        alias: The name to use for the attribute when validating or serializing by alias.\n            This is often used for things like converting between snake and camel case.\n        alias_priority: Priority of the alias. This affects whether an alias generator is used.\n        validation_alias: Like `alias`, but only affects validation, not serialization.\n        serialization_alias: Like `alias`, but only affects serialization, not validation.\n        title: Human-readable title.\n        field_title_generator: A callable that takes a field name and returns title for it.\n        description: Human-readable description.\n        examples: Example values for this field.\n        exclude: Whether to exclude the field from the model serialization.\n        discriminator: Field name or Discriminator for discriminating the type in a tagged union.\n        deprecated: A deprecation message, an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport,\n            or a boolean. If `True`, a default deprecation message will be emitted when accessing the field.\n        json_schema_extra: A dict or callable to provide extra JSON schema properties.\n        frozen: Whether the field is frozen. If true, attempts to change the value on an instance will raise an error.\n        validate_default: If `True`, apply validation to the default value every time you create an instance.\n            Otherwise, for performance reasons, the default value of the field is trusted and not validated.\n        repr: A boolean indicating whether to include the field in the `__repr__` output.\n        init: Whether the field should be included in the constructor of the dataclass.\n            (Only applies to dataclasses.)\n        init_var: Whether the field should _only_ be included in the constructor of the dataclass.\n            (Only applies to dataclasses.)\n        kw_only: Whether the field should be a keyword-only argument in the constructor of the dataclass.\n            (Only applies to dataclasses.)\n        coerce_numbers_to_str: Whether to enable coercion of any `Number` type to `str` (not applicable in `strict` mode).\n        strict: If `True`, strict validation is applied to the field.\n            See [Strict Mode](../concepts/strict_mode.md) for details.\n        gt: Greater than. If set, value must be greater than this. Only applicable to numbers.\n        ge: Greater than or equal. If set, value must be greater than or equal to this. Only applicable to numbers.\n        lt: Less than. If set, value must be less than this. Only applicable to numbers.\n        le: Less than or equal. If set, value must be less than or equal to this. Only applicable to numbers.\n        multiple_of: Value must be a multiple of this. Only applicable to numbers.\n        min_length: Minimum length for iterables.\n        max_length: Maximum length for iterables.\n        pattern: Pattern for strings (a regular expression).\n        allow_inf_nan: Allow `inf`, `-inf`, `nan`. Only applicable to numbers.\n        max_digits: Maximum number of allow digits for strings.\n        decimal_places: Maximum number of decimal places allowed for numbers.\n        union_mode: The strategy to apply when validating a union. Can be `smart` (the default), or `left_to_right`.\n            See [Union Mode](../concepts/unions.md#union-modes) for details.\n        extra: (Deprecated) Extra fields that will be included in the JSON schema.\n\n            !!! warning Deprecated\n                The `extra` kwargs is deprecated. Use `json_schema_extra` instead.\n\n    Returns:\n        A new [`FieldInfo`][pydantic.fields.FieldInfo]. The return annotation is `Any` so `Field` can be used on\n            type-annotated fields without causing a type error.\n    \"\"\"\n    # Check deprecated and removed params from V1. This logic should eventually be removed.\n    const = extra.pop('const', None)  # type: ignore\n    if const is not None:\n        raise PydanticUserError('`const` is removed, use `Literal` instead', code='removed-kwargs')\n\n    min_items = extra.pop('min_items', None)  # type: ignore\n    if min_items is not None:\n        warn('`min_items` is deprecated and will be removed, use `min_length` instead', DeprecationWarning)\n        if min_length in (None, _Unset):\n            min_length = min_items  # type: ignore\n\n    max_items = extra.pop('max_items', None)  # type: ignore\n    if max_items is not None:\n        warn('`max_items` is deprecated and will be removed, use `max_length` instead', DeprecationWarning)\n        if max_length in (None, _Unset):\n            max_length = max_items  # type: ignore\n\n    unique_items = extra.pop('unique_items', None)  # type: ignore\n    if unique_items is not None:\n        raise PydanticUserError(\n            (\n                '`unique_items` is removed, use `Set` instead'\n                '(this feature is discussed in https://github.com/pydantic/pydantic-core/issues/296)'\n            ),\n            code='removed-kwargs',\n        )\n\n    allow_mutation = extra.pop('allow_mutation', None)  # type: ignore\n    if allow_mutation is not None:\n        warn('`allow_mutation` is deprecated and will be removed. use `frozen` instead', DeprecationWarning)\n        if allow_mutation is False:\n            frozen = True\n\n    regex = extra.pop('regex', None)  # type: ignore\n    if regex is not None:\n        raise PydanticUserError('`regex` is removed. use `pattern` instead', code='removed-kwargs')\n\n    if extra:\n        warn(\n            'Using extra keyword arguments on `Field` is deprecated and will be removed.'\n            ' Use `json_schema_extra` instead.'\n            f' (Extra keys: {\", \".join(k.__repr__() for k in extra.keys())})',\n            DeprecationWarning,\n        )\n        if not json_schema_extra or json_schema_extra is _Unset:\n            json_schema_extra = extra  # type: ignore\n\n    if (\n        validation_alias\n        and validation_alias is not _Unset\n        and not isinstance(validation_alias, (str, AliasChoices, AliasPath))\n    ):\n        raise TypeError('Invalid `validation_alias` type. it should be `str`, `AliasChoices`, or `AliasPath`')\n\n    if serialization_alias in (_Unset, None) and isinstance(alias, str):\n        serialization_alias = alias\n\n    if validation_alias in (_Unset, None):\n        validation_alias = alias\n\n    include = extra.pop('include', None)  # type: ignore\n    if include is not None:\n        warn('`include` is deprecated and does nothing. It will be removed, use `exclude` instead', DeprecationWarning)\n\n    return FieldInfo.from_field(\n        default,\n        default_factory=default_factory,\n        alias=alias,\n        alias_priority=alias_priority,\n        validation_alias=validation_alias,\n        serialization_alias=serialization_alias,\n        title=title,\n        field_title_generator=field_title_generator,\n        description=description,\n        examples=examples,\n        exclude=exclude,\n        discriminator=discriminator,\n        deprecated=deprecated,\n        json_schema_extra=json_schema_extra,\n        frozen=frozen,\n        pattern=pattern,\n        validate_default=validate_default,\n        repr=repr,\n        init=init,\n        init_var=init_var,\n        kw_only=kw_only,\n        coerce_numbers_to_str=coerce_numbers_to_str,\n        strict=strict,\n        gt=gt,\n        ge=ge,\n        lt=lt,\n        le=le,\n        multiple_of=multiple_of,\n        min_length=min_length,\n        max_length=max_length,\n        allow_inf_nan=allow_inf_nan,\n        max_digits=max_digits,\n        decimal_places=decimal_places,\n        union_mode=union_mode,\n    )\n\n\n_FIELD_ARG_NAMES = set(inspect.signature(Field).parameters)\n_FIELD_ARG_NAMES.remove('extra')  # do not include the varkwargs parameter\n\n\nclass ModelPrivateAttr(_repr.Representation):\n    \"\"\"A descriptor for private attributes in class models.\n\n    !!! warning\n        You generally shouldn't be creating `ModelPrivateAttr` instances directly, instead use\n        `pydantic.fields.PrivateAttr`. (This is similar to `FieldInfo` vs. `Field`.)\n\n    Attributes:\n        default: The default value of the attribute if not provided.\n        default_factory: A callable function that generates the default value of the\n            attribute if not provided.\n    \"\"\"\n\n    __slots__ = 'default', 'default_factory'\n\n    def __init__(\n        self, default: Any = PydanticUndefined, *, default_factory: typing.Callable[[], Any] | None = None\n    ) -> None:\n        self.default = default\n        self.default_factory = default_factory\n\n    if not typing.TYPE_CHECKING:\n        # We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access\n\n        def __getattr__(self, item: str) -> Any:\n            \"\"\"This function improves compatibility with custom descriptors by ensuring delegation happens\n            as expected when the default value of a private attribute is a descriptor.\n            \"\"\"\n            if item in {'__get__', '__set__', '__delete__'}:\n                if hasattr(self.default, item):\n                    return getattr(self.default, item)\n            raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n\n    def __set_name__(self, cls: type[Any], name: str) -> None:\n        \"\"\"Preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487.\"\"\"\n        if self.default is PydanticUndefined:\n            return\n        if not hasattr(self.default, '__set_name__'):\n            return\n        set_name = self.default.__set_name__\n        if callable(set_name):\n            set_name(cls, name)\n\n    def get_default(self) -> Any:\n        \"\"\"Retrieve the default value of the object.\n\n        If `self.default_factory` is `None`, the method will return a deep copy of the `self.default` object.\n\n        If `self.default_factory` is not `None`, it will call `self.default_factory` and return the value returned.\n\n        Returns:\n            The default value of the object.\n        \"\"\"\n        return _utils.smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, self.__class__) and (self.default, self.default_factory) == (\n            other.default,\n            other.default_factory,\n        )\n\n\ndef PrivateAttr(\n    default: Any = PydanticUndefined,\n    *,\n    default_factory: typing.Callable[[], Any] | None = None,\n    init: Literal[False] = False,\n) -> Any:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/models/#private-model-attributes\n\n    Indicates that an attribute is intended for private use and not handled during normal validation/serialization.\n\n    Private attributes are not validated by Pydantic, so it's up to you to ensure they are used in a type-safe manner.\n\n    Private attributes are stored in `__private_attributes__` on the model.\n\n    Args:\n        default: The attribute's default value. Defaults to Undefined.\n        default_factory: Callable that will be\n            called when a default value is needed for this attribute.\n            If both `default` and `default_factory` are set, an error will be raised.\n        init: Whether the attribute should be included in the constructor of the dataclass. Always `False`.\n\n    Returns:\n        An instance of [`ModelPrivateAttr`][pydantic.fields.ModelPrivateAttr] class.\n\n    Raises:\n        ValueError: If both `default` and `default_factory` are set.\n    \"\"\"\n    if default is not PydanticUndefined and default_factory is not None:\n        raise TypeError('cannot specify both default and default_factory')\n\n    return ModelPrivateAttr(\n        default,\n        default_factory=default_factory,\n    )\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass ComputedFieldInfo:\n    \"\"\"A container for data from `@computed_field` so that we can access it while building the pydantic-core schema.\n\n    Attributes:\n        decorator_repr: A class variable representing the decorator string, '@computed_field'.\n        wrapped_property: The wrapped computed field property.\n        return_type: The type of the computed field property's return value.\n        alias: The alias of the property to be used during serialization.\n        alias_priority: The priority of the alias. This affects whether an alias generator is used.\n        title: Title of the computed field to include in the serialization JSON schema.\n        field_title_generator: A callable that takes a field name and returns title for it.\n        description: Description of the computed field to include in the serialization JSON schema.\n        deprecated: A deprecation message, an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport,\n            or a boolean. If `True`, a default deprecation message will be emitted when accessing the field.\n        examples: Example values of the computed field to include in the serialization JSON schema.\n        json_schema_extra: A dict or callable to provide extra JSON schema properties.\n        repr: A boolean indicating whether to include the field in the __repr__ output.\n    \"\"\"\n\n    decorator_repr: ClassVar[str] = '@computed_field'\n    wrapped_property: property\n    return_type: Any\n    alias: str | None\n    alias_priority: int | None\n    title: str | None\n    field_title_generator: typing.Callable[[str, ComputedFieldInfo], str] | None\n    description: str | None\n    deprecated: Deprecated | str | bool | None\n    examples: list[Any] | None\n    json_schema_extra: JsonDict | typing.Callable[[JsonDict], None] | None\n    repr: bool\n\n    @property\n    def deprecation_message(self) -> str | None:\n        \"\"\"The deprecation message to be emitted, or `None` if not set.\"\"\"\n        if self.deprecated is None:\n            return None\n        if isinstance(self.deprecated, bool):\n            return 'deprecated' if self.deprecated else None\n        return self.deprecated if isinstance(self.deprecated, str) else self.deprecated.message\n\n\ndef _wrapped_property_is_private(property_: cached_property | property) -> bool:  # type: ignore\n    \"\"\"Returns true if provided property is private, False otherwise.\"\"\"\n    wrapped_name: str = ''\n\n    if isinstance(property_, property):\n        wrapped_name = getattr(property_.fget, '__name__', '')\n    elif isinstance(property_, cached_property):  # type: ignore\n        wrapped_name = getattr(property_.func, '__name__', '')  # type: ignore\n\n    return wrapped_name.startswith('_') and not wrapped_name.startswith('__')\n\n\n# this should really be `property[T], cached_property[T]` but property is not generic unlike cached_property\n# See https://github.com/python/typing/issues/985 and linked issues\nPropertyT = typing.TypeVar('PropertyT')\n\n\n@typing.overload\ndef computed_field(\n    *,\n    alias: str | None = None,\n    alias_priority: int | None = None,\n    title: str | None = None,\n    field_title_generator: typing.Callable[[str, ComputedFieldInfo], str] | None = None,\n    description: str | None = None,\n    deprecated: Deprecated | str | bool | None = None,\n    examples: list[Any] | None = None,\n    json_schema_extra: JsonDict | typing.Callable[[JsonDict], None] | None = None,\n    repr: bool = True,\n    return_type: Any = PydanticUndefined,\n) -> typing.Callable[[PropertyT], PropertyT]: ...\n\n\n@typing.overload\ndef computed_field(__func: PropertyT) -> PropertyT: ...\n\n\ndef computed_field(\n    func: PropertyT | None = None,\n    /,\n    *,\n    alias: str | None = None,\n    alias_priority: int | None = None,\n    title: str | None = None,\n    field_title_generator: typing.Callable[[str, ComputedFieldInfo], str] | None = None,\n    description: str | None = None,\n    deprecated: Deprecated | str | bool | None = None,\n    examples: list[Any] | None = None,\n    json_schema_extra: JsonDict | typing.Callable[[JsonDict], None] | None = None,\n    repr: bool | None = None,\n    return_type: Any = PydanticUndefined,\n) -> PropertyT | typing.Callable[[PropertyT], PropertyT]:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/fields#the-computed_field-decorator\n\n    Decorator to include `property` and `cached_property` when serializing models or dataclasses.\n\n    This is useful for fields that are computed from other fields, or for fields that are expensive to compute and should be cached.\n\n    ```py\n    from pydantic import BaseModel, computed_field\n\n    class Rectangle(BaseModel):\n        width: int\n        length: int\n\n        @computed_field\n        @property\n        def area(self) -> int:\n            return self.width * self.length\n\n    print(Rectangle(width=3, length=2).model_dump())\n    #> {'width': 3, 'length': 2, 'area': 6}\n    ```\n\n    If applied to functions not yet decorated with `@property` or `@cached_property`, the function is\n    automatically wrapped with `property`. Although this is more concise, you will lose IntelliSense in your IDE,\n    and confuse static type checkers, thus explicit use of `@property` is recommended.\n\n    !!! warning \"Mypy Warning\"\n        Even with the `@property` or `@cached_property` applied to your function before `@computed_field`,\n        mypy may throw a `Decorated property not supported` error.\n        See [mypy issue #1362](https://github.com/python/mypy/issues/1362), for more information.\n        To avoid this error message, add `# type: ignore[misc]` to the `@computed_field` line.\n\n        [pyright](https://github.com/microsoft/pyright) supports `@computed_field` without error.\n\n    ```py\n    import random\n\n    from pydantic import BaseModel, computed_field\n\n    class Square(BaseModel):\n        width: float\n\n        @computed_field\n        def area(self) -> float:  # converted to a `property` by `computed_field`\n            return round(self.width**2, 2)\n\n        @area.setter\n        def area(self, new_area: float) -> None:\n            self.width = new_area**0.5\n\n        @computed_field(alias='the magic number', repr=False)\n        def random_number(self) -> int:\n            return random.randint(0, 1_000)\n\n    square = Square(width=1.3)\n\n    # `random_number` does not appear in representation\n    print(repr(square))\n    #> Square(width=1.3, area=1.69)\n\n    print(square.random_number)\n    #> 3\n\n    square.area = 4\n\n    print(square.model_dump_json(by_alias=True))\n    #> {\"width\":2.0,\"area\":4.0,\"the magic number\":3}\n    ```\n\n    !!! warning \"Overriding with `computed_field`\"\n        You can't override a field from a parent class with a `computed_field` in the child class.\n        `mypy` complains about this behavior if allowed, and `dataclasses` doesn't allow this pattern either.\n        See the example below:\n\n    ```py\n    from pydantic import BaseModel, computed_field\n\n    class Parent(BaseModel):\n        a: str\n\n    try:\n\n        class Child(Parent):\n            @computed_field\n            @property\n            def a(self) -> str:\n                return 'new a'\n\n    except ValueError as e:\n        print(repr(e))\n        #> ValueError(\"you can't override a field with a computed field\")\n    ```\n\n    Private properties decorated with `@computed_field` have `repr=False` by default.\n\n    ```py\n    from functools import cached_property\n\n    from pydantic import BaseModel, computed_field\n\n    class Model(BaseModel):\n        foo: int\n\n        @computed_field\n        @cached_property\n        def _private_cached_property(self) -> int:\n            return -self.foo\n\n        @computed_field\n        @property\n        def _private_property(self) -> int:\n            return -self.foo\n\n    m = Model(foo=1)\n    print(repr(m))\n    #> M(foo=1)\n    ```\n\n    Args:\n        func: the function to wrap.\n        alias: alias to use when serializing this computed field, only used when `by_alias=True`\n        alias_priority: priority of the alias. This affects whether an alias generator is used\n        title: Title to use when including this computed field in JSON Schema\n        field_title_generator: A callable that takes a field name and returns title for it.\n        description: Description to use when including this computed field in JSON Schema, defaults to the function's\n            docstring\n        deprecated: A deprecation message (or an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport).\n            to be emitted when accessing the field. Or a boolean. This will automatically be set if the property is decorated with the\n            `deprecated` decorator.\n        examples: Example values to use when including this computed field in JSON Schema\n        json_schema_extra: A dict or callable to provide extra JSON schema properties.\n        repr: whether to include this computed field in model repr.\n            Default is `False` for private properties and `True` for public properties.\n        return_type: optional return for serialization logic to expect when serializing to JSON, if included\n            this must be correct, otherwise a `TypeError` is raised.\n            If you don't include a return type Any is used, which does runtime introspection to handle arbitrary\n            objects.\n\n    Returns:\n        A proxy wrapper for the property.\n    \"\"\"\n\n    def dec(f: Any) -> Any:\n        nonlocal description, deprecated, return_type, alias_priority\n        unwrapped = _decorators.unwrap_wrapped_function(f)\n\n        if description is None and unwrapped.__doc__:\n            description = inspect.cleandoc(unwrapped.__doc__)\n\n        if deprecated is None and hasattr(unwrapped, '__deprecated__'):\n            deprecated = unwrapped.__deprecated__\n\n        # if the function isn't already decorated with `@property` (or another descriptor), then we wrap it now\n        f = _decorators.ensure_property(f)\n        alias_priority = (alias_priority or 2) if alias is not None else None\n\n        if repr is None:\n            repr_: bool = not _wrapped_property_is_private(property_=f)\n        else:\n            repr_ = repr\n\n        dec_info = ComputedFieldInfo(\n            f,\n            return_type,\n            alias,\n            alias_priority,\n            title,\n            field_title_generator,\n            description,\n            deprecated,\n            examples,\n            json_schema_extra,\n            repr_,\n        )\n        return _decorators.PydanticDescriptorProxy(f, dec_info)\n\n    if func is None:\n        return dec\n    else:\n        return dec(func)\n", "pydantic/version.py": "\"\"\"The `version` module holds the version information for Pydantic.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\n__all__ = 'VERSION', 'version_info'\n\nVERSION = '2.8.0a1'\n\"\"\"The version of Pydantic.\"\"\"\n\n\ndef version_short() -> str:\n    \"\"\"Return the `major.minor` part of Pydantic version.\n\n    It returns '2.1' if Pydantic version is '2.1.1'.\n    \"\"\"\n    return '.'.join(VERSION.split('.')[:2])\n\n\ndef version_info() -> str:\n    \"\"\"Return complete version information for Pydantic and its dependencies.\"\"\"\n    import importlib.metadata as importlib_metadata\n    import os\n    import platform\n    import sys\n    from pathlib import Path\n\n    import pydantic_core._pydantic_core as pdc\n\n    from ._internal import _git as git\n\n    # get data about packages that are closely related to pydantic, use pydantic or often conflict with pydantic\n    package_names = {\n        'email-validator',\n        'fastapi',\n        'mypy',\n        'pydantic-extra-types',\n        'pydantic-settings',\n        'pyright',\n        'typing_extensions',\n    }\n    related_packages = []\n\n    for dist in importlib_metadata.distributions():\n        name = dist.metadata['Name']\n        if name in package_names:\n            related_packages.append(f'{name}-{dist.version}')\n\n    pydantic_dir = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n    most_recent_commit = (\n        git.git_revision(pydantic_dir) if git.is_git_repo(pydantic_dir) and git.have_git() else 'unknown'\n    )\n\n    info = {\n        'pydantic version': VERSION,\n        'pydantic-core version': pdc.__version__,\n        'pydantic-core build': getattr(pdc, 'build_info', None) or pdc.build_profile,\n        'install path': Path(__file__).resolve().parent,\n        'python version': sys.version,\n        'platform': platform.platform(),\n        'related packages': ' '.join(related_packages),\n        'commit': most_recent_commit,\n    }\n    return '\\n'.join('{:>30} {}'.format(k + ':', str(v).replace('\\n', ' ')) for k, v in info.items())\n\n\ndef parse_mypy_version(version: str) -> tuple[int, ...]:\n    \"\"\"Parse mypy string version to tuple of ints.\n\n    It parses normal version like `0.930` and extra info followed by a `+` sign\n    like `0.940+dev.04cac4b5d911c4f9529e6ce86a27b44f28846f5d.dirty`.\n\n    Args:\n        version: The mypy version string.\n\n    Returns:\n        A tuple of ints. e.g. (0, 930).\n    \"\"\"\n    return tuple(map(int, version.partition('+')[0].split('.')))\n", "pydantic/tools.py": "\"\"\"The `tools` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/root_model.py": "\"\"\"RootModel class and type definitions.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport typing\nfrom copy import copy, deepcopy\n\nfrom pydantic_core import PydanticUndefined\n\nfrom . import PydanticUserError\nfrom ._internal import _model_construction, _repr\nfrom .main import BaseModel, _object_setattr\n\nif typing.TYPE_CHECKING:\n    from typing import Any\n\n    from typing_extensions import Literal, Self, dataclass_transform\n\n    from .fields import Field as PydanticModelField\n    from .fields import PrivateAttr as PydanticModelPrivateAttr\n\n    # dataclass_transform could be applied to RootModel directly, but `ModelMetaclass`'s dataclass_transform\n    # takes priority (at least with pyright). We trick type checkers into thinking we apply dataclass_transform\n    # on a new metaclass.\n    @dataclass_transform(kw_only_default=False, field_specifiers=(PydanticModelField, PydanticModelPrivateAttr))\n    class _RootModelMetaclass(_model_construction.ModelMetaclass): ...\nelse:\n    _RootModelMetaclass = _model_construction.ModelMetaclass\n\n__all__ = ('RootModel',)\n\nRootModelRootType = typing.TypeVar('RootModelRootType')\n\n\nclass RootModel(BaseModel, typing.Generic[RootModelRootType], metaclass=_RootModelMetaclass):\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/models/#rootmodel-and-custom-root-types\n\n    A Pydantic `BaseModel` for the root object of the model.\n\n    Attributes:\n        root: The root object of the model.\n        __pydantic_root_model__: Whether the model is a RootModel.\n        __pydantic_private__: Private fields in the model.\n        __pydantic_extra__: Extra fields in the model.\n\n    \"\"\"\n\n    __pydantic_root_model__ = True\n    __pydantic_private__ = None\n    __pydantic_extra__ = None\n\n    root: RootModelRootType\n\n    def __init_subclass__(cls, **kwargs):\n        extra = cls.model_config.get('extra')\n        if extra is not None:\n            raise PydanticUserError(\n                \"`RootModel` does not support setting `model_config['extra']`\", code='root-model-extra'\n            )\n        super().__init_subclass__(**kwargs)\n\n    def __init__(self, /, root: RootModelRootType = PydanticUndefined, **data) -> None:  # type: ignore\n        __tracebackhide__ = True\n        if data:\n            if root is not PydanticUndefined:\n                raise ValueError(\n                    '\"RootModel.__init__\" accepts either a single positional argument or arbitrary keyword arguments'\n                )\n            root = data  # type: ignore\n        self.__pydantic_validator__.validate_python(root, self_instance=self)\n\n    __init__.__pydantic_base_init__ = True  # pyright: ignore[reportFunctionMemberAccess]\n\n    @classmethod\n    def model_construct(cls, root: RootModelRootType, _fields_set: set[str] | None = None) -> Self:  # type: ignore\n        \"\"\"Create a new model using the provided root object and update fields set.\n\n        Args:\n            root: The root object of the model.\n            _fields_set: The set of fields to be updated.\n\n        Returns:\n            The new model.\n\n        Raises:\n            NotImplemented: If the model is not a subclass of `RootModel`.\n        \"\"\"\n        return super().model_construct(root=root, _fields_set=_fields_set)\n\n    def __getstate__(self) -> dict[Any, Any]:\n        return {\n            '__dict__': self.__dict__,\n            '__pydantic_fields_set__': self.__pydantic_fields_set__,\n        }\n\n    def __setstate__(self, state: dict[Any, Any]) -> None:\n        _object_setattr(self, '__pydantic_fields_set__', state['__pydantic_fields_set__'])\n        _object_setattr(self, '__dict__', state['__dict__'])\n\n    def __copy__(self) -> Self:\n        \"\"\"Returns a shallow copy of the model.\"\"\"\n        cls = type(self)\n        m = cls.__new__(cls)\n        _object_setattr(m, '__dict__', copy(self.__dict__))\n        _object_setattr(m, '__pydantic_fields_set__', copy(self.__pydantic_fields_set__))\n        return m\n\n    def __deepcopy__(self, memo: dict[int, Any] | None = None) -> Self:\n        \"\"\"Returns a deep copy of the model.\"\"\"\n        cls = type(self)\n        m = cls.__new__(cls)\n        _object_setattr(m, '__dict__', deepcopy(self.__dict__, memo=memo))\n        # This next line doesn't need a deepcopy because __pydantic_fields_set__ is a set[str],\n        # and attempting a deepcopy would be marginally slower.\n        _object_setattr(m, '__pydantic_fields_set__', copy(self.__pydantic_fields_set__))\n        return m\n\n    if typing.TYPE_CHECKING:\n\n        def model_dump(  # type: ignore\n            self,\n            *,\n            mode: Literal['json', 'python'] | str = 'python',\n            include: Any = None,\n            exclude: Any = None,\n            context: dict[str, Any] | None = None,\n            by_alias: bool = False,\n            exclude_unset: bool = False,\n            exclude_defaults: bool = False,\n            exclude_none: bool = False,\n            round_trip: bool = False,\n            warnings: bool | Literal['none', 'warn', 'error'] = True,\n            serialize_as_any: bool = False,\n        ) -> Any:\n            \"\"\"This method is included just to get a more accurate return type for type checkers.\n            It is included in this `if TYPE_CHECKING:` block since no override is actually necessary.\n\n            See the documentation of `BaseModel.model_dump` for more details about the arguments.\n\n            Generally, this method will have a return type of `RootModelRootType`, assuming that `RootModelRootType` is\n            not a `BaseModel` subclass. If `RootModelRootType` is a `BaseModel` subclass, then the return\n            type will likely be `dict[str, Any]`, as `model_dump` calls are recursive. The return type could\n            even be something different, in the case of a custom serializer.\n            Thus, `Any` is used here to catch all of these cases.\n            \"\"\"\n            ...\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, RootModel):\n            return NotImplemented\n        return self.model_fields['root'].annotation == other.model_fields['root'].annotation and super().__eq__(other)\n\n    def __repr_args__(self) -> _repr.ReprArgs:\n        yield 'root', self.root\n", "pydantic/warnings.py": "\"\"\"Pydantic-specific warnings.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom .version import version_short\n\n__all__ = (\n    'PydanticDeprecatedSince20',\n    'PydanticDeprecationWarning',\n    'PydanticDeprecatedSince26',\n    'PydanticExperimentalWarning',\n)\n\n\nclass PydanticDeprecationWarning(DeprecationWarning):\n    \"\"\"A Pydantic specific deprecation warning.\n\n    This warning is raised when using deprecated functionality in Pydantic. It provides information on when the\n    deprecation was introduced and the expected version in which the corresponding functionality will be removed.\n\n    Attributes:\n        message: Description of the warning.\n        since: Pydantic version in what the deprecation was introduced.\n        expected_removal: Pydantic version in what the corresponding functionality expected to be removed.\n    \"\"\"\n\n    message: str\n    since: tuple[int, int]\n    expected_removal: tuple[int, int]\n\n    def __init__(\n        self, message: str, *args: object, since: tuple[int, int], expected_removal: tuple[int, int] | None = None\n    ) -> None:\n        super().__init__(message, *args)\n        self.message = message.rstrip('.')\n        self.since = since\n        self.expected_removal = expected_removal if expected_removal is not None else (since[0] + 1, 0)\n\n    def __str__(self) -> str:\n        message = (\n            f'{self.message}. Deprecated in Pydantic V{self.since[0]}.{self.since[1]}'\n            f' to be removed in V{self.expected_removal[0]}.{self.expected_removal[1]}.'\n        )\n        if self.since == (2, 0):\n            message += f' See Pydantic V2 Migration Guide at https://errors.pydantic.dev/{version_short()}/migration/'\n        return message\n\n\nclass PydanticDeprecatedSince20(PydanticDeprecationWarning):\n    \"\"\"A specific `PydanticDeprecationWarning` subclass defining functionality deprecated since Pydantic 2.0.\"\"\"\n\n    def __init__(self, message: str, *args: object) -> None:\n        super().__init__(message, *args, since=(2, 0), expected_removal=(3, 0))\n\n\nclass PydanticDeprecatedSince26(PydanticDeprecationWarning):\n    \"\"\"A specific `PydanticDeprecationWarning` subclass defining functionality deprecated since Pydantic 2.6.\"\"\"\n\n    def __init__(self, message: str, *args: object) -> None:\n        super().__init__(message, *args, since=(2, 0), expected_removal=(3, 0))\n\n\nclass GenericBeforeBaseModelWarning(Warning):\n    pass\n\n\nclass PydanticExperimentalWarning(Warning):\n    \"\"\"A Pydantic specific experimental functionality warning.\n\n    This warning is raised when using experimental functionality in Pydantic.\n    It is raised to warn users that the functionality may change or be removed in future versions of Pydantic.\n    \"\"\"\n", "pydantic/alias_generators.py": "\"\"\"Alias generators for converting between different capitalization conventions.\"\"\"\n\nimport re\n\n__all__ = ('to_pascal', 'to_camel', 'to_snake')\n\n# TODO: in V3, change the argument names to be more descriptive\n# Generally, don't only convert from snake_case, or name the functions\n# more specifically like snake_to_camel.\n\n\ndef to_pascal(snake: str) -> str:\n    \"\"\"Convert a snake_case string to PascalCase.\n\n    Args:\n        snake: The string to convert.\n\n    Returns:\n        The PascalCase string.\n    \"\"\"\n    camel = snake.title()\n    return re.sub('([0-9A-Za-z])_(?=[0-9A-Z])', lambda m: m.group(1), camel)\n\n\ndef to_camel(snake: str) -> str:\n    \"\"\"Convert a snake_case string to camelCase.\n\n    Args:\n        snake: The string to convert.\n\n    Returns:\n        The converted camelCase string.\n    \"\"\"\n    # If the string is already in camelCase and does not contain a digit followed\n    # by a lowercase letter, return it as it is\n    if re.match('^[a-z]+[A-Za-z0-9]*$', snake) and not re.search(r'\\d[a-z]', snake):\n        return snake\n\n    camel = to_pascal(snake)\n    return re.sub('(^_*[A-Z])', lambda m: m.group(1).lower(), camel)\n\n\ndef to_snake(camel: str) -> str:\n    \"\"\"Convert a PascalCase, camelCase, or kebab-case string to snake_case.\n\n    Args:\n        camel: The string to convert.\n\n    Returns:\n        The converted string in snake_case.\n    \"\"\"\n    snake = re.sub(r'([a-zA-Z])([0-9])', lambda m: f'{m.group(1)}_{m.group(2)}', camel)\n    snake = re.sub(r'([a-z0-9])([A-Z])', lambda m: f'{m.group(1)}_{m.group(2)}', snake)\n    snake = re.sub(r'([A-Z]+)([A-Z][a-z])', lambda m: f'{m.group(1)}_{m.group(2)}', snake)\n    # Replace hyphens with underscores to handle kebab-case\n    snake = snake.replace('-', '_')\n    return snake.lower()\n", "pydantic/json_schema.py": "\"\"\"\nUsage docs: https://docs.pydantic.dev/2.5/concepts/json_schema/\n\nThe `json_schema` module contains classes and functions to allow the way [JSON Schema](https://json-schema.org/)\nis generated to be customized.\n\nIn general you shouldn't need to use this module directly; instead, you can use\n[`BaseModel.model_json_schema`][pydantic.BaseModel.model_json_schema] and\n[`TypeAdapter.json_schema`][pydantic.TypeAdapter.json_schema].\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport inspect\nimport math\nimport re\nimport warnings\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom dataclasses import is_dataclass\nfrom enum import Enum\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Counter,\n    Dict,\n    Hashable,\n    Iterable,\n    NewType,\n    Pattern,\n    Sequence,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport pydantic_core\nfrom pydantic_core import CoreSchema, PydanticOmit, core_schema, to_jsonable_python\nfrom pydantic_core.core_schema import ComputedField\nfrom typing_extensions import Annotated, Literal, TypeAlias, assert_never, deprecated, final\n\nfrom pydantic.warnings import PydanticDeprecatedSince26\n\nfrom ._internal import (\n    _config,\n    _core_metadata,\n    _core_utils,\n    _decorators,\n    _internal_dataclass,\n    _mock_val_ser,\n    _schema_generation_shared,\n    _typing_extra,\n)\nfrom .annotated_handlers import GetJsonSchemaHandler\nfrom .config import JsonDict, JsonSchemaExtraCallable, JsonValue\nfrom .errors import PydanticInvalidForJsonSchema, PydanticSchemaGenerationError, PydanticUserError\n\nif TYPE_CHECKING:\n    from . import ConfigDict\n    from ._internal._core_utils import CoreSchemaField, CoreSchemaOrField\n    from ._internal._dataclasses import PydanticDataclass\n    from ._internal._schema_generation_shared import GetJsonSchemaFunction\n    from .main import BaseModel\n\n\nCoreSchemaOrFieldType = Literal[core_schema.CoreSchemaType, core_schema.CoreSchemaFieldType]\n\"\"\"\nA type alias for defined schema types that represents a union of\n`core_schema.CoreSchemaType` and\n`core_schema.CoreSchemaFieldType`.\n\"\"\"\n\nJsonSchemaValue = Dict[str, Any]\n\"\"\"\nA type alias for a JSON schema value. This is a dictionary of string keys to arbitrary JSON values.\n\"\"\"\n\nJsonSchemaMode = Literal['validation', 'serialization']\n\"\"\"\nA type alias that represents the mode of a JSON schema; either 'validation' or 'serialization'.\n\nFor some types, the inputs to validation differ from the outputs of serialization. For example,\ncomputed fields will only be present when serializing, and should not be provided when\nvalidating. This flag provides a way to indicate whether you want the JSON schema required\nfor validation inputs, or that will be matched by serialization outputs.\n\"\"\"\n\n_MODE_TITLE_MAPPING: dict[JsonSchemaMode, str] = {'validation': 'Input', 'serialization': 'Output'}\n\n\n@deprecated(\n    '`update_json_schema` is deprecated, use a simple `my_dict.update(update_dict)` call instead.',\n    category=None,\n)\ndef update_json_schema(schema: JsonSchemaValue, updates: dict[str, Any]) -> JsonSchemaValue:\n    \"\"\"Update a JSON schema in-place by providing a dictionary of updates.\n\n    This function sets the provided key-value pairs in the schema and returns the updated schema.\n\n    Args:\n        schema: The JSON schema to update.\n        updates: A dictionary of key-value pairs to set in the schema.\n\n    Returns:\n        The updated JSON schema.\n    \"\"\"\n    schema.update(updates)\n    return schema\n\n\nJsonSchemaWarningKind = Literal['skipped-choice', 'non-serializable-default']\n\"\"\"\nA type alias representing the kinds of warnings that can be emitted during JSON schema generation.\n\nSee [`GenerateJsonSchema.render_warning_message`][pydantic.json_schema.GenerateJsonSchema.render_warning_message]\nfor more details.\n\"\"\"\n\n\nclass PydanticJsonSchemaWarning(UserWarning):\n    \"\"\"This class is used to emit warnings produced during JSON schema generation.\n    See the [`GenerateJsonSchema.emit_warning`][pydantic.json_schema.GenerateJsonSchema.emit_warning] and\n    [`GenerateJsonSchema.render_warning_message`][pydantic.json_schema.GenerateJsonSchema.render_warning_message]\n    methods for more details; these can be overridden to control warning behavior.\n    \"\"\"\n\n\n# ##### JSON Schema Generation #####\nDEFAULT_REF_TEMPLATE = '#/$defs/{model}'\n\"\"\"The default format string used to generate reference names.\"\"\"\n\n# There are three types of references relevant to building JSON schemas:\n#   1. core_schema \"ref\" values; these are not exposed as part of the JSON schema\n#       * these might look like the fully qualified path of a model, its id, or something similar\nCoreRef = NewType('CoreRef', str)\n#   2. keys of the \"definitions\" object that will eventually go into the JSON schema\n#       * by default, these look like \"MyModel\", though may change in the presence of collisions\n#       * eventually, we may want to make it easier to modify the way these names are generated\nDefsRef = NewType('DefsRef', str)\n#   3. the values corresponding to the \"$ref\" key in the schema\n#       * By default, these look like \"#/$defs/MyModel\", as in {\"$ref\": \"#/$defs/MyModel\"}\nJsonRef = NewType('JsonRef', str)\n\nCoreModeRef = Tuple[CoreRef, JsonSchemaMode]\nJsonSchemaKeyT = TypeVar('JsonSchemaKeyT', bound=Hashable)\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass _DefinitionsRemapping:\n    defs_remapping: dict[DefsRef, DefsRef]\n    json_remapping: dict[JsonRef, JsonRef]\n\n    @staticmethod\n    def from_prioritized_choices(\n        prioritized_choices: dict[DefsRef, list[DefsRef]],\n        defs_to_json: dict[DefsRef, JsonRef],\n        definitions: dict[DefsRef, JsonSchemaValue],\n    ) -> _DefinitionsRemapping:\n        \"\"\"\n        This function should produce a remapping that replaces complex DefsRef with the simpler ones from the\n        prioritized_choices such that applying the name remapping would result in an equivalent JSON schema.\n        \"\"\"\n        # We need to iteratively simplify the definitions until we reach a fixed point.\n        # The reason for this is that outer definitions may reference inner definitions that get simplified\n        # into an equivalent reference, and the outer definitions won't be equivalent until we've simplified\n        # the inner definitions.\n        copied_definitions = deepcopy(definitions)\n        definitions_schema = {'$defs': copied_definitions}\n        for _iter in range(100):  # prevent an infinite loop in the case of a bug, 100 iterations should be enough\n            # For every possible remapped DefsRef, collect all schemas that that DefsRef might be used for:\n            schemas_for_alternatives: dict[DefsRef, list[JsonSchemaValue]] = defaultdict(list)\n            for defs_ref in copied_definitions:\n                alternatives = prioritized_choices[defs_ref]\n                for alternative in alternatives:\n                    schemas_for_alternatives[alternative].append(copied_definitions[defs_ref])\n\n            # Deduplicate the schemas for each alternative; the idea is that we only want to remap to a new DefsRef\n            # if it introduces no ambiguity, i.e., there is only one distinct schema for that DefsRef.\n            for defs_ref, schemas in schemas_for_alternatives.items():\n                schemas_for_alternatives[defs_ref] = _deduplicate_schemas(schemas_for_alternatives[defs_ref])\n\n            # Build the remapping\n            defs_remapping: dict[DefsRef, DefsRef] = {}\n            json_remapping: dict[JsonRef, JsonRef] = {}\n            for original_defs_ref in definitions:\n                alternatives = prioritized_choices[original_defs_ref]\n                # Pick the first alternative that has only one schema, since that means there is no collision\n                remapped_defs_ref = next(x for x in alternatives if len(schemas_for_alternatives[x]) == 1)\n                defs_remapping[original_defs_ref] = remapped_defs_ref\n                json_remapping[defs_to_json[original_defs_ref]] = defs_to_json[remapped_defs_ref]\n            remapping = _DefinitionsRemapping(defs_remapping, json_remapping)\n            new_definitions_schema = remapping.remap_json_schema({'$defs': copied_definitions})\n            if definitions_schema == new_definitions_schema:\n                # We've reached the fixed point\n                return remapping\n            definitions_schema = new_definitions_schema\n\n        raise PydanticInvalidForJsonSchema('Failed to simplify the JSON schema definitions')\n\n    def remap_defs_ref(self, ref: DefsRef) -> DefsRef:\n        return self.defs_remapping.get(ref, ref)\n\n    def remap_json_ref(self, ref: JsonRef) -> JsonRef:\n        return self.json_remapping.get(ref, ref)\n\n    def remap_json_schema(self, schema: Any) -> Any:\n        \"\"\"\n        Recursively update the JSON schema replacing all $refs\n        \"\"\"\n        if isinstance(schema, str):\n            # Note: this may not really be a JsonRef; we rely on having no collisions between JsonRefs and other strings\n            return self.remap_json_ref(JsonRef(schema))\n        elif isinstance(schema, list):\n            return [self.remap_json_schema(item) for item in schema]\n        elif isinstance(schema, dict):\n            for key, value in schema.items():\n                if key == '$ref' and isinstance(value, str):\n                    schema['$ref'] = self.remap_json_ref(JsonRef(value))\n                elif key == '$defs':\n                    schema['$defs'] = {\n                        self.remap_defs_ref(DefsRef(key)): self.remap_json_schema(value)\n                        for key, value in schema['$defs'].items()\n                    }\n                else:\n                    schema[key] = self.remap_json_schema(value)\n        return schema\n\n\nclass GenerateJsonSchema:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/json_schema/#customizing-the-json-schema-generation-process\n\n    A class for generating JSON schemas.\n\n    This class generates JSON schemas based on configured parameters. The default schema dialect\n    is [https://json-schema.org/draft/2020-12/schema](https://json-schema.org/draft/2020-12/schema).\n    The class uses `by_alias` to configure how fields with\n    multiple names are handled and `ref_template` to format reference names.\n\n    Attributes:\n        schema_dialect: The JSON schema dialect used to generate the schema. See\n            [Declaring a Dialect](https://json-schema.org/understanding-json-schema/reference/schema.html#id4)\n            in the JSON Schema documentation for more information about dialects.\n        ignored_warning_kinds: Warnings to ignore when generating the schema. `self.render_warning_message` will\n            do nothing if its argument `kind` is in `ignored_warning_kinds`;\n            this value can be modified on subclasses to easily control which warnings are emitted.\n        by_alias: Whether to use field aliases when generating the schema.\n        ref_template: The format string used when generating reference names.\n        core_to_json_refs: A mapping of core refs to JSON refs.\n        core_to_defs_refs: A mapping of core refs to definition refs.\n        defs_to_core_refs: A mapping of definition refs to core refs.\n        json_to_defs_refs: A mapping of JSON refs to definition refs.\n        definitions: Definitions in the schema.\n\n    Args:\n        by_alias: Whether to use field aliases in the generated schemas.\n        ref_template: The format string to use when generating reference names.\n\n    Raises:\n        JsonSchemaError: If the instance of the class is inadvertently re-used after generating a schema.\n    \"\"\"\n\n    schema_dialect = 'https://json-schema.org/draft/2020-12/schema'\n\n    # `self.render_warning_message` will do nothing if its argument `kind` is in `ignored_warning_kinds`;\n    # this value can be modified on subclasses to easily control which warnings are emitted\n    ignored_warning_kinds: set[JsonSchemaWarningKind] = {'skipped-choice'}\n\n    def __init__(self, by_alias: bool = True, ref_template: str = DEFAULT_REF_TEMPLATE):\n        self.by_alias = by_alias\n        self.ref_template = ref_template\n\n        self.core_to_json_refs: dict[CoreModeRef, JsonRef] = {}\n        self.core_to_defs_refs: dict[CoreModeRef, DefsRef] = {}\n        self.defs_to_core_refs: dict[DefsRef, CoreModeRef] = {}\n        self.json_to_defs_refs: dict[JsonRef, DefsRef] = {}\n\n        self.definitions: dict[DefsRef, JsonSchemaValue] = {}\n        self._config_wrapper_stack = _config.ConfigWrapperStack(_config.ConfigWrapper({}))\n\n        self._mode: JsonSchemaMode = 'validation'\n\n        # The following includes a mapping of a fully-unique defs ref choice to a list of preferred\n        # alternatives, which are generally simpler, such as only including the class name.\n        # At the end of schema generation, we use these to produce a JSON schema with more human-readable\n        # definitions, which would also work better in a generated OpenAPI client, etc.\n        self._prioritized_defsref_choices: dict[DefsRef, list[DefsRef]] = {}\n        self._collision_counter: dict[str, int] = defaultdict(int)\n        self._collision_index: dict[str, int] = {}\n\n        self._schema_type_to_method = self.build_schema_type_to_method()\n\n        # When we encounter definitions we need to try to build them immediately\n        # so that they are available schemas that reference them\n        # But it's possible that CoreSchema was never going to be used\n        # (e.g. because the CoreSchema that references short circuits is JSON schema generation without needing\n        #  the reference) so instead of failing altogether if we can't build a definition we\n        # store the error raised and re-throw it if we end up needing that def\n        self._core_defs_invalid_for_json_schema: dict[DefsRef, PydanticInvalidForJsonSchema] = {}\n\n        # This changes to True after generating a schema, to prevent issues caused by accidental re-use\n        # of a single instance of a schema generator\n        self._used = False\n\n    @property\n    def _config(self) -> _config.ConfigWrapper:\n        return self._config_wrapper_stack.tail\n\n    @property\n    def mode(self) -> JsonSchemaMode:\n        if self._config.json_schema_mode_override is not None:\n            return self._config.json_schema_mode_override\n        else:\n            return self._mode\n\n    def build_schema_type_to_method(\n        self,\n    ) -> dict[CoreSchemaOrFieldType, Callable[[CoreSchemaOrField], JsonSchemaValue]]:\n        \"\"\"Builds a dictionary mapping fields to methods for generating JSON schemas.\n\n        Returns:\n            A dictionary containing the mapping of `CoreSchemaOrFieldType` to a handler method.\n\n        Raises:\n            TypeError: If no method has been defined for generating a JSON schema for a given pydantic core schema type.\n        \"\"\"\n        mapping: dict[CoreSchemaOrFieldType, Callable[[CoreSchemaOrField], JsonSchemaValue]] = {}\n        core_schema_types: list[CoreSchemaOrFieldType] = _typing_extra.all_literal_values(\n            CoreSchemaOrFieldType  # type: ignore\n        )\n        for key in core_schema_types:\n            method_name = f\"{key.replace('-', '_')}_schema\"\n            try:\n                mapping[key] = getattr(self, method_name)\n            except AttributeError as e:  # pragma: no cover\n                raise TypeError(\n                    f'No method for generating JsonSchema for core_schema.type={key!r} '\n                    f'(expected: {type(self).__name__}.{method_name})'\n                ) from e\n        return mapping\n\n    def generate_definitions(\n        self, inputs: Sequence[tuple[JsonSchemaKeyT, JsonSchemaMode, core_schema.CoreSchema]]\n    ) -> tuple[dict[tuple[JsonSchemaKeyT, JsonSchemaMode], JsonSchemaValue], dict[DefsRef, JsonSchemaValue]]:\n        \"\"\"Generates JSON schema definitions from a list of core schemas, pairing the generated definitions with a\n        mapping that links the input keys to the definition references.\n\n        Args:\n            inputs: A sequence of tuples, where:\n\n                - The first element is a JSON schema key type.\n                - The second element is the JSON mode: either 'validation' or 'serialization'.\n                - The third element is a core schema.\n\n        Returns:\n            A tuple where:\n\n                - The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n                    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n                    JsonRef references to definitions that are defined in the second returned element.)\n                - The second element is a dictionary whose keys are definition references for the JSON schemas\n                    from the first returned element, and whose values are the actual JSON schema definitions.\n\n        Raises:\n            PydanticUserError: Raised if the JSON schema generator has already been used to generate a JSON schema.\n        \"\"\"\n        if self._used:\n            raise PydanticUserError(\n                'This JSON schema generator has already been used to generate a JSON schema. '\n                f'You must create a new instance of {type(self).__name__} to generate a new JSON schema.',\n                code='json-schema-already-used',\n            )\n\n        for key, mode, schema in inputs:\n            self._mode = mode\n            self.generate_inner(schema)\n\n        definitions_remapping = self._build_definitions_remapping()\n\n        json_schemas_map: dict[tuple[JsonSchemaKeyT, JsonSchemaMode], DefsRef] = {}\n        for key, mode, schema in inputs:\n            self._mode = mode\n            json_schema = self.generate_inner(schema)\n            json_schemas_map[(key, mode)] = definitions_remapping.remap_json_schema(json_schema)\n\n        json_schema = {'$defs': self.definitions}\n        json_schema = definitions_remapping.remap_json_schema(json_schema)\n        self._used = True\n        return json_schemas_map, _sort_json_schema(json_schema['$defs'])  # type: ignore\n\n    def generate(self, schema: CoreSchema, mode: JsonSchemaMode = 'validation') -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema for a specified schema in a specified mode.\n\n        Args:\n            schema: A Pydantic model.\n            mode: The mode in which to generate the schema. Defaults to 'validation'.\n\n        Returns:\n            A JSON schema representing the specified schema.\n\n        Raises:\n            PydanticUserError: If the JSON schema generator has already been used to generate a JSON schema.\n        \"\"\"\n        self._mode = mode\n        if self._used:\n            raise PydanticUserError(\n                'This JSON schema generator has already been used to generate a JSON schema. '\n                f'You must create a new instance of {type(self).__name__} to generate a new JSON schema.',\n                code='json-schema-already-used',\n            )\n\n        json_schema: JsonSchemaValue = self.generate_inner(schema)\n        json_ref_counts = self.get_json_ref_counts(json_schema)\n\n        # Remove the top-level $ref if present; note that the _generate method already ensures there are no sibling keys\n        ref = cast(JsonRef, json_schema.get('$ref'))\n        while ref is not None:  # may need to unpack multiple levels\n            ref_json_schema = self.get_schema_from_definitions(ref)\n            if json_ref_counts[ref] > 1 or ref_json_schema is None:\n                # Keep the ref, but use an allOf to remove the top level $ref\n                json_schema = {'allOf': [{'$ref': ref}]}\n            else:\n                # \"Unpack\" the ref since this is the only reference\n                json_schema = ref_json_schema.copy()  # copy to prevent recursive dict reference\n                json_ref_counts[ref] -= 1\n            ref = cast(JsonRef, json_schema.get('$ref'))\n\n        self._garbage_collect_definitions(json_schema)\n        definitions_remapping = self._build_definitions_remapping()\n\n        if self.definitions:\n            json_schema['$defs'] = self.definitions\n\n        json_schema = definitions_remapping.remap_json_schema(json_schema)\n\n        # For now, we will not set the $schema key. However, if desired, this can be easily added by overriding\n        # this method and adding the following line after a call to super().generate(schema):\n        # json_schema['$schema'] = self.schema_dialect\n\n        self._used = True\n        return _sort_json_schema(json_schema)\n\n    def generate_inner(self, schema: CoreSchemaOrField) -> JsonSchemaValue:  # noqa: C901\n        \"\"\"Generates a JSON schema for a given core schema.\n\n        Args:\n            schema: The given core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        # If a schema with the same CoreRef has been handled, just return a reference to it\n        # Note that this assumes that it will _never_ be the case that the same CoreRef is used\n        # on types that should have different JSON schemas\n        if 'ref' in schema:\n            core_ref = CoreRef(schema['ref'])  # type: ignore[typeddict-item]\n            core_mode_ref = (core_ref, self.mode)\n            if core_mode_ref in self.core_to_defs_refs and self.core_to_defs_refs[core_mode_ref] in self.definitions:\n                return {'$ref': self.core_to_json_refs[core_mode_ref]}\n\n        # Generate the JSON schema, accounting for the json_schema_override and core_schema_override\n        metadata_handler = _core_metadata.CoreMetadataHandler(schema)\n\n        def populate_defs(core_schema: CoreSchema, json_schema: JsonSchemaValue) -> JsonSchemaValue:\n            if 'ref' in core_schema:\n                core_ref = CoreRef(core_schema['ref'])  # type: ignore[typeddict-item]\n                defs_ref, ref_json_schema = self.get_cache_defs_ref_schema(core_ref)\n                json_ref = JsonRef(ref_json_schema['$ref'])\n                self.json_to_defs_refs[json_ref] = defs_ref\n                # Replace the schema if it's not a reference to itself\n                # What we want to avoid is having the def be just a ref to itself\n                # which is what would happen if we blindly assigned any\n                if json_schema.get('$ref', None) != json_ref:\n                    self.definitions[defs_ref] = json_schema\n                    self._core_defs_invalid_for_json_schema.pop(defs_ref, None)\n                json_schema = ref_json_schema\n            return json_schema\n\n        def convert_to_all_of(json_schema: JsonSchemaValue) -> JsonSchemaValue:\n            if '$ref' in json_schema and len(json_schema.keys()) > 1:\n                # technically you can't have any other keys next to a \"$ref\"\n                # but it's an easy mistake to make and not hard to correct automatically here\n                json_schema = json_schema.copy()\n                ref = json_schema.pop('$ref')\n                json_schema = {'allOf': [{'$ref': ref}], **json_schema}\n            return json_schema\n\n        def handler_func(schema_or_field: CoreSchemaOrField) -> JsonSchemaValue:\n            \"\"\"Generate a JSON schema based on the input schema.\n\n            Args:\n                schema_or_field: The core schema to generate a JSON schema from.\n\n            Returns:\n                The generated JSON schema.\n\n            Raises:\n                TypeError: If an unexpected schema type is encountered.\n            \"\"\"\n            # Generate the core-schema-type-specific bits of the schema generation:\n            json_schema: JsonSchemaValue | None = None\n            if self.mode == 'serialization' and 'serialization' in schema_or_field:\n                ser_schema = schema_or_field['serialization']  # type: ignore\n                json_schema = self.ser_schema(ser_schema)\n            if json_schema is None:\n                if _core_utils.is_core_schema(schema_or_field) or _core_utils.is_core_schema_field(schema_or_field):\n                    generate_for_schema_type = self._schema_type_to_method[schema_or_field['type']]\n                    json_schema = generate_for_schema_type(schema_or_field)\n                else:\n                    raise TypeError(f'Unexpected schema type: schema={schema_or_field}')\n            if _core_utils.is_core_schema(schema_or_field):\n                json_schema = populate_defs(schema_or_field, json_schema)\n                json_schema = convert_to_all_of(json_schema)\n            return json_schema\n\n        current_handler = _schema_generation_shared.GenerateJsonSchemaHandler(self, handler_func)\n\n        for js_modify_function in metadata_handler.metadata.get('pydantic_js_functions', ()):\n\n            def new_handler_func(\n                schema_or_field: CoreSchemaOrField,\n                current_handler: GetJsonSchemaHandler = current_handler,\n                js_modify_function: GetJsonSchemaFunction = js_modify_function,\n            ) -> JsonSchemaValue:\n                json_schema = js_modify_function(schema_or_field, current_handler)\n                if _core_utils.is_core_schema(schema_or_field):\n                    json_schema = populate_defs(schema_or_field, json_schema)\n                original_schema = current_handler.resolve_ref_schema(json_schema)\n                ref = json_schema.pop('$ref', None)\n                if ref and json_schema:\n                    original_schema.update(json_schema)\n                return original_schema\n\n            current_handler = _schema_generation_shared.GenerateJsonSchemaHandler(self, new_handler_func)\n\n        for js_modify_function in metadata_handler.metadata.get('pydantic_js_annotation_functions', ()):\n\n            def new_handler_func(\n                schema_or_field: CoreSchemaOrField,\n                current_handler: GetJsonSchemaHandler = current_handler,\n                js_modify_function: GetJsonSchemaFunction = js_modify_function,\n            ) -> JsonSchemaValue:\n                json_schema = js_modify_function(schema_or_field, current_handler)\n                if _core_utils.is_core_schema(schema_or_field):\n                    json_schema = populate_defs(schema_or_field, json_schema)\n                    json_schema = convert_to_all_of(json_schema)\n                return json_schema\n\n            current_handler = _schema_generation_shared.GenerateJsonSchemaHandler(self, new_handler_func)\n\n        json_schema = current_handler(schema)\n        if _core_utils.is_core_schema(schema):\n            json_schema = populate_defs(schema, json_schema)\n            json_schema = convert_to_all_of(json_schema)\n        return json_schema\n\n    # ### Schema generation methods\n    def any_schema(self, schema: core_schema.AnySchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches any value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return {}\n\n    def none_schema(self, schema: core_schema.NoneSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches `None`.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return {'type': 'null'}\n\n    def bool_schema(self, schema: core_schema.BoolSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a bool value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return {'type': 'boolean'}\n\n    def int_schema(self, schema: core_schema.IntSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches an int value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema: dict[str, Any] = {'type': 'integer'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.numeric)\n        json_schema = {k: v for k, v in json_schema.items() if v not in {math.inf, -math.inf}}\n        return json_schema\n\n    def float_schema(self, schema: core_schema.FloatSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a float value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema: dict[str, Any] = {'type': 'number'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.numeric)\n        json_schema = {k: v for k, v in json_schema.items() if v not in {math.inf, -math.inf}}\n        return json_schema\n\n    def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a decimal value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema = self.str_schema(core_schema.str_schema())\n        if self.mode == 'validation':\n            multiple_of = schema.get('multiple_of')\n            le = schema.get('le')\n            ge = schema.get('ge')\n            lt = schema.get('lt')\n            gt = schema.get('gt')\n            json_schema = {\n                'anyOf': [\n                    self.float_schema(\n                        core_schema.float_schema(\n                            allow_inf_nan=schema.get('allow_inf_nan'),\n                            multiple_of=None if multiple_of is None else float(multiple_of),\n                            le=None if le is None else float(le),\n                            ge=None if ge is None else float(ge),\n                            lt=None if lt is None else float(lt),\n                            gt=None if gt is None else float(gt),\n                        )\n                    ),\n                    json_schema,\n                ],\n            }\n        return json_schema\n\n    def str_schema(self, schema: core_schema.StringSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a string value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema = {'type': 'string'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.string)\n        if isinstance(json_schema.get('pattern'), Pattern):\n            # TODO: should we add regex flags to the pattern?\n            json_schema['pattern'] = json_schema.get('pattern').pattern  # type: ignore\n        return json_schema\n\n    def bytes_schema(self, schema: core_schema.BytesSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a bytes value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema = {'type': 'string', 'format': 'base64url' if self._config.ser_json_bytes == 'base64' else 'binary'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.bytes)\n        return json_schema\n\n    def date_schema(self, schema: core_schema.DateSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a date value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema = {'type': 'string', 'format': 'date'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.date)\n        return json_schema\n\n    def time_schema(self, schema: core_schema.TimeSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a time value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return {'type': 'string', 'format': 'time'}\n\n    def datetime_schema(self, schema: core_schema.DatetimeSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a datetime value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return {'type': 'string', 'format': 'date-time'}\n\n    def timedelta_schema(self, schema: core_schema.TimedeltaSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a timedelta value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        if self._config.ser_json_timedelta == 'float':\n            return {'type': 'number'}\n        return {'type': 'string', 'format': 'duration'}\n\n    def literal_schema(self, schema: core_schema.LiteralSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a literal value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        expected = [v.value if isinstance(v, Enum) else v for v in schema['expected']]\n        # jsonify the expected values\n        expected = [to_jsonable_python(v) for v in expected]\n\n        result: dict[str, Any] = {'enum': expected}\n        if len(expected) == 1:\n            result['const'] = expected[0]\n\n        types = {type(e) for e in expected}\n        if types == {str}:\n            result['type'] = 'string'\n        elif types == {int}:\n            result['type'] = 'integer'\n        elif types == {float}:\n            result['type'] = 'numeric'\n        elif types == {bool}:\n            result['type'] = 'boolean'\n        elif types == {list}:\n            result['type'] = 'array'\n        elif types == {type(None)}:\n            result['type'] = 'null'\n        return result\n\n    def enum_schema(self, schema: core_schema.EnumSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches an Enum value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        enum_type = schema['cls']\n        description = None if not enum_type.__doc__ else inspect.cleandoc(enum_type.__doc__)\n        if (\n            description == 'An enumeration.'\n        ):  # This is the default value provided by enum.EnumMeta.__new__; don't use it\n            description = None\n        result: dict[str, Any] = {'title': enum_type.__name__, 'description': description}\n        result = {k: v for k, v in result.items() if v is not None}\n\n        expected = [to_jsonable_python(v.value) for v in schema['members']]\n\n        result['enum'] = expected\n        if len(expected) == 1:\n            result['const'] = expected[0]\n\n        types = {type(e) for e in expected}\n        if isinstance(enum_type, str) or types == {str}:\n            result['type'] = 'string'\n        elif isinstance(enum_type, int) or types == {int}:\n            result['type'] = 'integer'\n        elif isinstance(enum_type, float) or types == {float}:\n            result['type'] = 'numeric'\n        elif types == {bool}:\n            result['type'] = 'boolean'\n        elif types == {list}:\n            result['type'] = 'array'\n\n        return result\n\n    def is_instance_schema(self, schema: core_schema.IsInstanceSchema) -> JsonSchemaValue:\n        \"\"\"Handles JSON schema generation for a core schema that checks if a value is an instance of a class.\n\n        Unless overridden in a subclass, this raises an error.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self.handle_invalid_for_json_schema(schema, f'core_schema.IsInstanceSchema ({schema[\"cls\"]})')\n\n    def is_subclass_schema(self, schema: core_schema.IsSubclassSchema) -> JsonSchemaValue:\n        \"\"\"Handles JSON schema generation for a core schema that checks if a value is a subclass of a class.\n\n        For backwards compatibility with v1, this does not raise an error, but can be overridden to change this.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        # Note: This is for compatibility with V1; you can override if you want different behavior.\n        return {}\n\n    def callable_schema(self, schema: core_schema.CallableSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a callable value.\n\n        Unless overridden in a subclass, this raises an error.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self.handle_invalid_for_json_schema(schema, 'core_schema.CallableSchema')\n\n    def list_schema(self, schema: core_schema.ListSchema) -> JsonSchemaValue:\n        \"\"\"Returns a schema that matches a list schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        items_schema = {} if 'items_schema' not in schema else self.generate_inner(schema['items_schema'])\n        json_schema = {'type': 'array', 'items': items_schema}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema\n\n    @deprecated('`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.', category=None)\n    @final\n    def tuple_positional_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        \"\"\"Replaced by `tuple_schema`.\"\"\"\n        warnings.warn(\n            '`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.',\n            PydanticDeprecatedSince26,\n            stacklevel=2,\n        )\n        return self.tuple_schema(schema)\n\n    @deprecated('`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.', category=None)\n    @final\n    def tuple_variable_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        \"\"\"Replaced by `tuple_schema`.\"\"\"\n        warnings.warn(\n            '`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.',\n            PydanticDeprecatedSince26,\n            stacklevel=2,\n        )\n        return self.tuple_schema(schema)\n\n    def tuple_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a tuple schema e.g. `Tuple[int,\n        str, bool]` or `Tuple[int, ...]`.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema: JsonSchemaValue = {'type': 'array'}\n        if 'variadic_item_index' in schema:\n            variadic_item_index = schema['variadic_item_index']\n            if variadic_item_index > 0:\n                json_schema['minItems'] = variadic_item_index\n                json_schema['prefixItems'] = [\n                    self.generate_inner(item) for item in schema['items_schema'][:variadic_item_index]\n                ]\n            if variadic_item_index + 1 == len(schema['items_schema']):\n                # if the variadic item is the last item, then represent it faithfully\n                json_schema['items'] = self.generate_inner(schema['items_schema'][variadic_item_index])\n            else:\n                # otherwise, 'items' represents the schema for the variadic\n                # item plus the suffix, so just allow anything for simplicity\n                # for now\n                json_schema['items'] = True\n        else:\n            prefixItems = [self.generate_inner(item) for item in schema['items_schema']]\n            if prefixItems:\n                json_schema['prefixItems'] = prefixItems\n            json_schema['minItems'] = len(prefixItems)\n            json_schema['maxItems'] = len(prefixItems)\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema\n\n    def set_schema(self, schema: core_schema.SetSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a set schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self._common_set_schema(schema)\n\n    def frozenset_schema(self, schema: core_schema.FrozenSetSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a frozenset schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self._common_set_schema(schema)\n\n    def _common_set_schema(self, schema: core_schema.SetSchema | core_schema.FrozenSetSchema) -> JsonSchemaValue:\n        items_schema = {} if 'items_schema' not in schema else self.generate_inner(schema['items_schema'])\n        json_schema = {'type': 'array', 'uniqueItems': True, 'items': items_schema}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema\n\n    def generator_schema(self, schema: core_schema.GeneratorSchema) -> JsonSchemaValue:\n        \"\"\"Returns a JSON schema that represents the provided GeneratorSchema.\n\n        Args:\n            schema: The schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        items_schema = {} if 'items_schema' not in schema else self.generate_inner(schema['items_schema'])\n        json_schema = {'type': 'array', 'items': items_schema}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema\n\n    def dict_schema(self, schema: core_schema.DictSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a dict schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema: JsonSchemaValue = {'type': 'object'}\n\n        keys_schema = self.generate_inner(schema['keys_schema']).copy() if 'keys_schema' in schema else {}\n        keys_pattern = keys_schema.pop('pattern', None)\n\n        values_schema = self.generate_inner(schema['values_schema']).copy() if 'values_schema' in schema else {}\n        values_schema.pop('title', None)  # don't give a title to the additionalProperties\n        if values_schema or keys_pattern is not None:  # don't add additionalProperties if it's empty\n            if keys_pattern is None:\n                json_schema['additionalProperties'] = values_schema\n            else:\n                json_schema['patternProperties'] = {keys_pattern: values_schema}\n\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.object)\n        return json_schema\n\n    def _function_schema(\n        self,\n        schema: _core_utils.AnyFunctionSchema,\n    ) -> JsonSchemaValue:\n        if _core_utils.is_function_with_inner_schema(schema):\n            # This could be wrong if the function's mode is 'before', but in practice will often be right, and when it\n            # isn't, I think it would be hard to automatically infer what the desired schema should be.\n            return self.generate_inner(schema['schema'])\n\n        # function-plain\n        return self.handle_invalid_for_json_schema(\n            schema, f'core_schema.PlainValidatorFunctionSchema ({schema[\"function\"]})'\n        )\n\n    def function_before_schema(self, schema: core_schema.BeforeValidatorFunctionSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a function-before schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self._function_schema(schema)\n\n    def function_after_schema(self, schema: core_schema.AfterValidatorFunctionSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a function-after schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self._function_schema(schema)\n\n    def function_plain_schema(self, schema: core_schema.PlainValidatorFunctionSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a function-plain schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self._function_schema(schema)\n\n    def function_wrap_schema(self, schema: core_schema.WrapValidatorFunctionSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a function-wrap schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self._function_schema(schema)\n\n    def default_schema(self, schema: core_schema.WithDefaultSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema with a default value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema = self.generate_inner(schema['schema'])\n\n        if 'default' not in schema:\n            return json_schema\n        default = schema['default']\n        # Note: if you want to include the value returned by the default_factory,\n        # override this method and replace the code above with:\n        # if 'default' in schema:\n        #     default = schema['default']\n        # elif 'default_factory' in schema:\n        #     default = schema['default_factory']()\n        # else:\n        #     return json_schema\n\n        # we reflect the application of custom plain, no-info serializers to defaults for\n        # json schemas viewed in serialization mode\n        # TODO: improvements along with https://github.com/pydantic/pydantic/issues/8208\n        # TODO: improve type safety here\n        if self.mode == 'serialization':\n            if (\n                (ser_schema := schema['schema'].get('serialization', {}))\n                and (ser_func := ser_schema.get('function'))\n                and ser_schema.get('type') == 'function-plain'  # type: ignore\n                and ser_schema.get('info_arg') is False  # type: ignore\n            ):\n                default = ser_func(default)  # type: ignore\n\n        try:\n            encoded_default = self.encode_default(default)\n        except pydantic_core.PydanticSerializationError:\n            self.emit_warning(\n                'non-serializable-default',\n                f'Default value {default} is not JSON serializable; excluding default from JSON schema',\n            )\n            # Return the inner schema, as though there was no default\n            return json_schema\n\n        if '$ref' in json_schema:\n            # Since reference schemas do not support child keys, we wrap the reference schema in a single-case allOf:\n            return {'allOf': [json_schema], 'default': encoded_default}\n        else:\n            json_schema['default'] = encoded_default\n            return json_schema\n\n    def nullable_schema(self, schema: core_schema.NullableSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that allows null values.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        null_schema = {'type': 'null'}\n        inner_json_schema = self.generate_inner(schema['schema'])\n\n        if inner_json_schema == null_schema:\n            return null_schema\n        else:\n            # Thanks to the equality check against `null_schema` above, I think 'oneOf' would also be valid here;\n            # I'll use 'anyOf' for now, but it could be changed it if it would work better with some external tooling\n            return self.get_flattened_anyof([inner_json_schema, null_schema])\n\n    def union_schema(self, schema: core_schema.UnionSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that allows values matching any of the given schemas.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        generated: list[JsonSchemaValue] = []\n\n        choices = schema['choices']\n        for choice in choices:\n            # choice will be a tuple if an explicit label was provided\n            choice_schema = choice[0] if isinstance(choice, tuple) else choice\n            try:\n                generated.append(self.generate_inner(choice_schema))\n            except PydanticOmit:\n                continue\n            except PydanticInvalidForJsonSchema as exc:\n                self.emit_warning('skipped-choice', exc.message)\n        if len(generated) == 1:\n            return generated[0]\n        return self.get_flattened_anyof(generated)\n\n    def tagged_union_schema(self, schema: core_schema.TaggedUnionSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that allows values matching any of the given schemas, where\n        the schemas are tagged with a discriminator field that indicates which schema should be used to validate\n        the value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        generated: dict[str, JsonSchemaValue] = {}\n        for k, v in schema['choices'].items():\n            if isinstance(k, Enum):\n                k = k.value\n            try:\n                # Use str(k) since keys must be strings for json; while not technically correct,\n                # it's the closest that can be represented in valid JSON\n                generated[str(k)] = self.generate_inner(v).copy()\n            except PydanticOmit:\n                continue\n            except PydanticInvalidForJsonSchema as exc:\n                self.emit_warning('skipped-choice', exc.message)\n\n        one_of_choices = _deduplicate_schemas(generated.values())\n        json_schema: JsonSchemaValue = {'oneOf': one_of_choices}\n\n        # This reflects the v1 behavior; TODO: we should make it possible to exclude OpenAPI stuff from the JSON schema\n        openapi_discriminator = self._extract_discriminator(schema, one_of_choices)\n        if openapi_discriminator is not None:\n            json_schema['discriminator'] = {\n                'propertyName': openapi_discriminator,\n                'mapping': {k: v.get('$ref', v) for k, v in generated.items()},\n            }\n\n        return json_schema\n\n    def _extract_discriminator(\n        self, schema: core_schema.TaggedUnionSchema, one_of_choices: list[JsonDict]\n    ) -> str | None:\n        \"\"\"Extract a compatible OpenAPI discriminator from the schema and one_of choices that end up in the final\n        schema.\"\"\"\n        openapi_discriminator: str | None = None\n\n        if isinstance(schema['discriminator'], str):\n            return schema['discriminator']\n\n        if isinstance(schema['discriminator'], list):\n            # If the discriminator is a single item list containing a string, that is equivalent to the string case\n            if len(schema['discriminator']) == 1 and isinstance(schema['discriminator'][0], str):\n                return schema['discriminator'][0]\n            # When an alias is used that is different from the field name, the discriminator will be a list of single\n            # str lists, one for the attribute and one for the actual alias. The logic here will work even if there is\n            # more than one possible attribute, and looks for whether a single alias choice is present as a documented\n            # property on all choices. If so, that property will be used as the OpenAPI discriminator.\n            for alias_path in schema['discriminator']:\n                if not isinstance(alias_path, list):\n                    break  # this means that the discriminator is not a list of alias paths\n                if len(alias_path) != 1:\n                    continue  # this means that the \"alias\" does not represent a single field\n                alias = alias_path[0]\n                if not isinstance(alias, str):\n                    continue  # this means that the \"alias\" does not represent a field\n                alias_is_present_on_all_choices = True\n                for choice in one_of_choices:\n                    while '$ref' in choice:\n                        assert isinstance(choice['$ref'], str)\n                        choice = self.get_schema_from_definitions(JsonRef(choice['$ref'])) or {}\n                    properties = choice.get('properties', {})\n                    if not isinstance(properties, dict) or alias not in properties:\n                        alias_is_present_on_all_choices = False\n                        break\n                if alias_is_present_on_all_choices:\n                    openapi_discriminator = alias\n                    break\n        return openapi_discriminator\n\n    def chain_schema(self, schema: core_schema.ChainSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a core_schema.ChainSchema.\n\n        When generating a schema for validation, we return the validation JSON schema for the first step in the chain.\n        For serialization, we return the serialization JSON schema for the last step in the chain.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        step_index = 0 if self.mode == 'validation' else -1  # use first step for validation, last for serialization\n        return self.generate_inner(schema['steps'][step_index])\n\n    def lax_or_strict_schema(self, schema: core_schema.LaxOrStrictSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that allows values matching either the lax schema or the\n        strict schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        # TODO: Need to read the default value off of model config or whatever\n        use_strict = schema.get('strict', False)  # TODO: replace this default False\n        # If your JSON schema fails to generate it is probably\n        # because one of the following two branches failed.\n        if use_strict:\n            return self.generate_inner(schema['strict_schema'])\n        else:\n            return self.generate_inner(schema['lax_schema'])\n\n    def json_or_python_schema(self, schema: core_schema.JsonOrPythonSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that allows values matching either the JSON schema or the\n        Python schema.\n\n        The JSON schema is used instead of the Python schema. If you want to use the Python schema, you should override\n        this method.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self.generate_inner(schema['json_schema'])\n\n    def typed_dict_schema(self, schema: core_schema.TypedDictSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a typed dict.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        total = schema.get('total', True)\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (name, self.field_is_required(field, total), field)\n            for name, field in schema['fields'].items()\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        cls = _get_typed_dict_cls(schema)\n        config = _get_typed_dict_config(cls)\n        with self._config_wrapper_stack.push(config):\n            json_schema = self._named_required_fields_schema(named_required_fields)\n\n        json_schema_extra = config.get('json_schema_extra')\n        extra = schema.get('extra_behavior')\n        if extra is None:\n            extra = config.get('extra', 'ignore')\n\n        if cls is not None:\n            title = config.get('title') or cls.__name__\n            json_schema = self._update_class_schema(json_schema, title, extra, cls, json_schema_extra)\n        else:\n            if extra == 'forbid':\n                json_schema['additionalProperties'] = False\n            elif extra == 'allow':\n                json_schema['additionalProperties'] = True\n\n        return json_schema\n\n    @staticmethod\n    def _name_required_computed_fields(\n        computed_fields: list[ComputedField],\n    ) -> list[tuple[str, bool, core_schema.ComputedField]]:\n        return [(field['property_name'], True, field) for field in computed_fields]\n\n    def _named_required_fields_schema(\n        self, named_required_fields: Sequence[tuple[str, bool, CoreSchemaField]]\n    ) -> JsonSchemaValue:\n        properties: dict[str, JsonSchemaValue] = {}\n        required_fields: list[str] = []\n        for name, required, field in named_required_fields:\n            if self.by_alias:\n                name = self._get_alias_name(field, name)\n            try:\n                field_json_schema = self.generate_inner(field).copy()\n            except PydanticOmit:\n                continue\n            if 'title' not in field_json_schema and self.field_title_should_be_set(field):\n                title = self.get_title_from_name(name)\n                field_json_schema['title'] = title\n            field_json_schema = self.handle_ref_overrides(field_json_schema)\n            properties[name] = field_json_schema\n            if required:\n                required_fields.append(name)\n\n        json_schema = {'type': 'object', 'properties': properties}\n        if required_fields:\n            json_schema['required'] = required_fields\n        return json_schema\n\n    def _get_alias_name(self, field: CoreSchemaField, name: str) -> str:\n        if field['type'] == 'computed-field':\n            alias: Any = field.get('alias', name)\n        elif self.mode == 'validation':\n            alias = field.get('validation_alias', name)\n        else:\n            alias = field.get('serialization_alias', name)\n        if isinstance(alias, str):\n            name = alias\n        elif isinstance(alias, list):\n            alias = cast('list[str] | str', alias)\n            for path in alias:\n                if isinstance(path, list) and len(path) == 1 and isinstance(path[0], str):\n                    # Use the first valid single-item string path; the code that constructs the alias array\n                    # should ensure the first such item is what belongs in the JSON schema\n                    name = path[0]\n                    break\n        else:\n            assert_never(alias)\n        return name\n\n    def typed_dict_field_schema(self, schema: core_schema.TypedDictField) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a typed dict field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self.generate_inner(schema['schema'])\n\n    def dataclass_field_schema(self, schema: core_schema.DataclassField) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self.generate_inner(schema['schema'])\n\n    def model_field_schema(self, schema: core_schema.ModelField) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a model field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self.generate_inner(schema['schema'])\n\n    def computed_field_schema(self, schema: core_schema.ComputedField) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a computed field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self.generate_inner(schema['return_schema'])\n\n    def model_schema(self, schema: core_schema.ModelSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a model.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        # We do not use schema['model'].model_json_schema() here\n        # because it could lead to inconsistent refs handling, etc.\n        cls = cast('type[BaseModel]', schema['cls'])\n        config = cls.model_config\n        title = config.get('title')\n\n        with self._config_wrapper_stack.push(config):\n            json_schema = self.generate_inner(schema['schema'])\n\n        json_schema_extra = config.get('json_schema_extra')\n        if cls.__pydantic_root_model__:\n            root_json_schema_extra = cls.model_fields['root'].json_schema_extra\n            if json_schema_extra and root_json_schema_extra:\n                raise ValueError(\n                    '\"model_config[\\'json_schema_extra\\']\" and \"Field.json_schema_extra\" on \"RootModel.root\"'\n                    ' field must not be set simultaneously'\n                )\n            if root_json_schema_extra:\n                json_schema_extra = root_json_schema_extra\n\n        json_schema = self._update_class_schema(json_schema, title, config.get('extra', None), cls, json_schema_extra)\n\n        return json_schema\n\n    def _update_class_schema(\n        self,\n        json_schema: JsonSchemaValue,\n        title: str | None,\n        extra: Literal['allow', 'ignore', 'forbid'] | None,\n        cls: type[Any],\n        json_schema_extra: JsonDict | JsonSchemaExtraCallable | None,\n    ) -> JsonSchemaValue:\n        if '$ref' in json_schema:\n            schema_to_update = self.get_schema_from_definitions(JsonRef(json_schema['$ref'])) or json_schema\n        else:\n            schema_to_update = json_schema\n\n        if title is not None:\n            # referenced_schema['title'] = title\n            schema_to_update.setdefault('title', title)\n\n        if 'additionalProperties' not in schema_to_update:\n            if extra == 'allow':\n                schema_to_update['additionalProperties'] = True\n            elif extra == 'forbid':\n                schema_to_update['additionalProperties'] = False\n\n        if isinstance(json_schema_extra, (staticmethod, classmethod)):\n            # In older versions of python, this is necessary to ensure staticmethod/classmethods are callable\n            json_schema_extra = json_schema_extra.__get__(cls)\n\n        if isinstance(json_schema_extra, dict):\n            schema_to_update.update(json_schema_extra)\n        elif callable(json_schema_extra):\n            if len(inspect.signature(json_schema_extra).parameters) > 1:\n                json_schema_extra(schema_to_update, cls)  # type: ignore\n            else:\n                json_schema_extra(schema_to_update)  # type: ignore\n        elif json_schema_extra is not None:\n            raise ValueError(\n                f\"model_config['json_schema_extra']={json_schema_extra} should be a dict, callable, or None\"\n            )\n\n        if hasattr(cls, '__deprecated__'):\n            json_schema['deprecated'] = True\n\n        return json_schema\n\n    def resolve_schema_to_update(self, json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        \"\"\"Resolve a JsonSchemaValue to the non-ref schema if it is a $ref schema.\n\n        Args:\n            json_schema: The schema to resolve.\n\n        Returns:\n            The resolved schema.\n        \"\"\"\n        if '$ref' in json_schema:\n            schema_to_update = self.get_schema_from_definitions(JsonRef(json_schema['$ref']))\n            if schema_to_update is None:\n                raise RuntimeError(f'Cannot update undefined schema for $ref={json_schema[\"$ref\"]}')\n            return self.resolve_schema_to_update(schema_to_update)\n        else:\n            schema_to_update = json_schema\n        return schema_to_update\n\n    def model_fields_schema(self, schema: core_schema.ModelFieldsSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a model's fields.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (name, self.field_is_required(field, total=True), field)\n            for name, field in schema['fields'].items()\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        json_schema = self._named_required_fields_schema(named_required_fields)\n        extras_schema = schema.get('extras_schema', None)\n        if extras_schema is not None:\n            schema_to_update = self.resolve_schema_to_update(json_schema)\n            schema_to_update['additionalProperties'] = self.generate_inner(extras_schema)\n        return json_schema\n\n    def field_is_present(self, field: CoreSchemaField) -> bool:\n        \"\"\"Whether the field should be included in the generated JSON schema.\n\n        Args:\n            field: The schema for the field itself.\n\n        Returns:\n            `True` if the field should be included in the generated JSON schema, `False` otherwise.\n        \"\"\"\n        if self.mode == 'serialization':\n            # If you still want to include the field in the generated JSON schema,\n            # override this method and return True\n            return not field.get('serialization_exclude')\n        elif self.mode == 'validation':\n            return True\n        else:\n            assert_never(self.mode)\n\n    def field_is_required(\n        self,\n        field: core_schema.ModelField | core_schema.DataclassField | core_schema.TypedDictField,\n        total: bool,\n    ) -> bool:\n        \"\"\"Whether the field should be marked as required in the generated JSON schema.\n        (Note that this is irrelevant if the field is not present in the JSON schema.).\n\n        Args:\n            field: The schema for the field itself.\n            total: Only applies to `TypedDictField`s.\n                Indicates if the `TypedDict` this field belongs to is total, in which case any fields that don't\n                explicitly specify `required=False` are required.\n\n        Returns:\n            `True` if the field should be marked as required in the generated JSON schema, `False` otherwise.\n        \"\"\"\n        if self.mode == 'serialization' and self._config.json_schema_serialization_defaults_required:\n            return not field.get('serialization_exclude')\n        else:\n            if field['type'] == 'typed-dict-field':\n                return field.get('required', total)\n            else:\n                return field['schema']['type'] != 'default'\n\n    def dataclass_args_schema(self, schema: core_schema.DataclassArgsSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass's constructor arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (field['name'], self.field_is_required(field, total=True), field)\n            for field in schema['fields']\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        return self._named_required_fields_schema(named_required_fields)\n\n    def dataclass_schema(self, schema: core_schema.DataclassSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        cls = schema['cls']\n        config: ConfigDict = getattr(cls, '__pydantic_config__', cast('ConfigDict', {}))\n        title = config.get('title') or cls.__name__\n\n        with self._config_wrapper_stack.push(config):\n            json_schema = self.generate_inner(schema['schema']).copy()\n\n        json_schema_extra = config.get('json_schema_extra')\n        json_schema = self._update_class_schema(json_schema, title, config.get('extra', None), cls, json_schema_extra)\n\n        # Dataclass-specific handling of description\n        if is_dataclass(cls) and not hasattr(cls, '__pydantic_validator__'):\n            # vanilla dataclass; don't use cls.__doc__ as it will contain the class signature by default\n            description = None\n        else:\n            description = None if cls.__doc__ is None else inspect.cleandoc(cls.__doc__)\n        if description:\n            json_schema['description'] = description\n\n        return json_schema\n\n    def arguments_schema(self, schema: core_schema.ArgumentsSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a function's arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        metadata = _core_metadata.CoreMetadataHandler(schema).metadata\n        prefer_positional = metadata.get('pydantic_js_prefer_positional_arguments')\n\n        arguments = schema['arguments_schema']\n        kw_only_arguments = [a for a in arguments if a.get('mode') == 'keyword_only']\n        kw_or_p_arguments = [a for a in arguments if a.get('mode') in {'positional_or_keyword', None}]\n        p_only_arguments = [a for a in arguments if a.get('mode') == 'positional_only']\n        var_args_schema = schema.get('var_args_schema')\n        var_kwargs_schema = schema.get('var_kwargs_schema')\n\n        if prefer_positional:\n            positional_possible = not kw_only_arguments and not var_kwargs_schema\n            if positional_possible:\n                return self.p_arguments_schema(p_only_arguments + kw_or_p_arguments, var_args_schema)\n\n        keyword_possible = not p_only_arguments and not var_args_schema\n        if keyword_possible:\n            return self.kw_arguments_schema(kw_or_p_arguments + kw_only_arguments, var_kwargs_schema)\n\n        if not prefer_positional:\n            positional_possible = not kw_only_arguments and not var_kwargs_schema\n            if positional_possible:\n                return self.p_arguments_schema(p_only_arguments + kw_or_p_arguments, var_args_schema)\n\n        raise PydanticInvalidForJsonSchema(\n            'Unable to generate JSON schema for arguments validator with positional-only and keyword-only arguments'\n        )\n\n    def kw_arguments_schema(\n        self, arguments: list[core_schema.ArgumentsParameter], var_kwargs_schema: CoreSchema | None\n    ) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a function's keyword arguments.\n\n        Args:\n            arguments: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        properties: dict[str, JsonSchemaValue] = {}\n        required: list[str] = []\n        for argument in arguments:\n            name = self.get_argument_name(argument)\n            argument_schema = self.generate_inner(argument['schema']).copy()\n            argument_schema['title'] = self.get_title_from_name(name)\n            properties[name] = argument_schema\n\n            if argument['schema']['type'] != 'default':\n                # This assumes that if the argument has a default value,\n                # the inner schema must be of type WithDefaultSchema.\n                # I believe this is true, but I am not 100% sure\n                required.append(name)\n\n        json_schema: JsonSchemaValue = {'type': 'object', 'properties': properties}\n        if required:\n            json_schema['required'] = required\n\n        if var_kwargs_schema:\n            additional_properties_schema = self.generate_inner(var_kwargs_schema)\n            if additional_properties_schema:\n                json_schema['additionalProperties'] = additional_properties_schema\n        else:\n            json_schema['additionalProperties'] = False\n        return json_schema\n\n    def p_arguments_schema(\n        self, arguments: list[core_schema.ArgumentsParameter], var_args_schema: CoreSchema | None\n    ) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a function's positional arguments.\n\n        Args:\n            arguments: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        prefix_items: list[JsonSchemaValue] = []\n        min_items = 0\n\n        for argument in arguments:\n            name = self.get_argument_name(argument)\n\n            argument_schema = self.generate_inner(argument['schema']).copy()\n            argument_schema['title'] = self.get_title_from_name(name)\n            prefix_items.append(argument_schema)\n\n            if argument['schema']['type'] != 'default':\n                # This assumes that if the argument has a default value,\n                # the inner schema must be of type WithDefaultSchema.\n                # I believe this is true, but I am not 100% sure\n                min_items += 1\n\n        json_schema: JsonSchemaValue = {'type': 'array', 'prefixItems': prefix_items}\n        if min_items:\n            json_schema['minItems'] = min_items\n\n        if var_args_schema:\n            items_schema = self.generate_inner(var_args_schema)\n            if items_schema:\n                json_schema['items'] = items_schema\n        else:\n            json_schema['maxItems'] = len(prefix_items)\n\n        return json_schema\n\n    def get_argument_name(self, argument: core_schema.ArgumentsParameter) -> str:\n        \"\"\"Retrieves the name of an argument.\n\n        Args:\n            argument: The core schema.\n\n        Returns:\n            The name of the argument.\n        \"\"\"\n        name = argument['name']\n        if self.by_alias:\n            alias = argument.get('alias')\n            if isinstance(alias, str):\n                name = alias\n            else:\n                pass  # might want to do something else?\n        return name\n\n    def call_schema(self, schema: core_schema.CallSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a function call.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self.generate_inner(schema['arguments_schema'])\n\n    def custom_error_schema(self, schema: core_schema.CustomErrorSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a custom error.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return self.generate_inner(schema['schema'])\n\n    def json_schema(self, schema: core_schema.JsonSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a JSON object.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        content_core_schema = schema.get('schema') or core_schema.any_schema()\n        content_json_schema = self.generate_inner(content_core_schema)\n        if self.mode == 'validation':\n            return {'type': 'string', 'contentMediaType': 'application/json', 'contentSchema': content_json_schema}\n        else:\n            # self.mode == 'serialization'\n            return content_json_schema\n\n    def url_schema(self, schema: core_schema.UrlSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a URL.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema = {'type': 'string', 'format': 'uri', 'minLength': 1}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.string)\n        return json_schema\n\n    def multi_host_url_schema(self, schema: core_schema.MultiHostUrlSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a URL that can be used with multiple hosts.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        # Note: 'multi-host-uri' is a custom/pydantic-specific format, not part of the JSON Schema spec\n        json_schema = {'type': 'string', 'format': 'multi-host-uri', 'minLength': 1}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.string)\n        return json_schema\n\n    def uuid_schema(self, schema: core_schema.UuidSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a UUID.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        return {'type': 'string', 'format': 'uuid'}\n\n    def definitions_schema(self, schema: core_schema.DefinitionsSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a JSON object with definitions.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        for definition in schema['definitions']:\n            try:\n                self.generate_inner(definition)\n            except PydanticInvalidForJsonSchema as e:\n                core_ref: CoreRef = CoreRef(definition['ref'])  # type: ignore\n                self._core_defs_invalid_for_json_schema[self.get_defs_ref((core_ref, self.mode))] = e\n                continue\n        return self.generate_inner(schema['schema'])\n\n    def definition_ref_schema(self, schema: core_schema.DefinitionReferenceSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that references a definition.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        core_ref = CoreRef(schema['schema_ref'])\n        _, ref_json_schema = self.get_cache_defs_ref_schema(core_ref)\n        return ref_json_schema\n\n    def ser_schema(\n        self, schema: core_schema.SerSchema | core_schema.IncExSeqSerSchema | core_schema.IncExDictSerSchema\n    ) -> JsonSchemaValue | None:\n        \"\"\"Generates a JSON schema that matches a schema that defines a serialized object.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        schema_type = schema['type']\n        if schema_type == 'function-plain' or schema_type == 'function-wrap':\n            # PlainSerializerFunctionSerSchema or WrapSerializerFunctionSerSchema\n            return_schema = schema.get('return_schema')\n            if return_schema is not None:\n                return self.generate_inner(return_schema)\n        elif schema_type == 'format' or schema_type == 'to-string':\n            # FormatSerSchema or ToStringSerSchema\n            return self.str_schema(core_schema.str_schema())\n        elif schema['type'] == 'model':\n            # ModelSerSchema\n            return self.generate_inner(schema['schema'])\n        return None\n\n    # ### Utility methods\n\n    def get_title_from_name(self, name: str) -> str:\n        \"\"\"Retrieves a title from a name.\n\n        Args:\n            name: The name to retrieve a title from.\n\n        Returns:\n            The title.\n        \"\"\"\n        return name.title().replace('_', ' ')\n\n    def field_title_should_be_set(self, schema: CoreSchemaOrField) -> bool:\n        \"\"\"Returns true if a field with the given schema should have a title set based on the field name.\n\n        Intuitively, we want this to return true for schemas that wouldn't otherwise provide their own title\n        (e.g., int, float, str), and false for those that would (e.g., BaseModel subclasses).\n\n        Args:\n            schema: The schema to check.\n\n        Returns:\n            `True` if the field should have a title set, `False` otherwise.\n        \"\"\"\n        if _core_utils.is_core_schema_field(schema):\n            if schema['type'] == 'computed-field':\n                field_schema = schema['return_schema']\n            else:\n                field_schema = schema['schema']\n            return self.field_title_should_be_set(field_schema)\n\n        elif _core_utils.is_core_schema(schema):\n            if schema.get('ref'):  # things with refs, such as models and enums, should not have titles set\n                return False\n            if schema['type'] in {'default', 'nullable', 'definitions'}:\n                return self.field_title_should_be_set(schema['schema'])  # type: ignore[typeddict-item]\n            if _core_utils.is_function_with_inner_schema(schema):\n                return self.field_title_should_be_set(schema['schema'])\n            if schema['type'] == 'definition-ref':\n                # Referenced schemas should not have titles set for the same reason\n                # schemas with refs should not\n                return False\n            return True  # anything else should have title set\n\n        else:\n            raise PydanticInvalidForJsonSchema(f'Unexpected schema type: schema={schema}')  # pragma: no cover\n\n    def normalize_name(self, name: str) -> str:\n        \"\"\"Normalizes a name to be used as a key in a dictionary.\n\n        Args:\n            name: The name to normalize.\n\n        Returns:\n            The normalized name.\n        \"\"\"\n        return re.sub(r'[^a-zA-Z0-9.\\-_]', '_', name).replace('.', '__')\n\n    def get_defs_ref(self, core_mode_ref: CoreModeRef) -> DefsRef:\n        \"\"\"Override this method to change the way that definitions keys are generated from a core reference.\n\n        Args:\n            core_mode_ref: The core reference.\n\n        Returns:\n            The definitions key.\n        \"\"\"\n        # Split the core ref into \"components\"; generic origins and arguments are each separate components\n        core_ref, mode = core_mode_ref\n        components = re.split(r'([\\][,])', core_ref)\n        # Remove IDs from each component\n        components = [x.rsplit(':', 1)[0] for x in components]\n        core_ref_no_id = ''.join(components)\n        # Remove everything before the last period from each \"component\"\n        components = [re.sub(r'(?:[^.[\\]]+\\.)+((?:[^.[\\]]+))', r'\\1', x) for x in components]\n        short_ref = ''.join(components)\n\n        mode_title = _MODE_TITLE_MAPPING[mode]\n\n        # It is important that the generated defs_ref values be such that at least one choice will not\n        # be generated for any other core_ref. Currently, this should be the case because we include\n        # the id of the source type in the core_ref\n        name = DefsRef(self.normalize_name(short_ref))\n        name_mode = DefsRef(self.normalize_name(short_ref) + f'-{mode_title}')\n        module_qualname = DefsRef(self.normalize_name(core_ref_no_id))\n        module_qualname_mode = DefsRef(f'{module_qualname}-{mode_title}')\n        module_qualname_id = DefsRef(self.normalize_name(core_ref))\n        occurrence_index = self._collision_index.get(module_qualname_id)\n        if occurrence_index is None:\n            self._collision_counter[module_qualname] += 1\n            occurrence_index = self._collision_index[module_qualname_id] = self._collision_counter[module_qualname]\n\n        module_qualname_occurrence = DefsRef(f'{module_qualname}__{occurrence_index}')\n        module_qualname_occurrence_mode = DefsRef(f'{module_qualname_mode}__{occurrence_index}')\n\n        self._prioritized_defsref_choices[module_qualname_occurrence_mode] = [\n            name,\n            name_mode,\n            module_qualname,\n            module_qualname_mode,\n            module_qualname_occurrence,\n            module_qualname_occurrence_mode,\n        ]\n\n        return module_qualname_occurrence_mode\n\n    def get_cache_defs_ref_schema(self, core_ref: CoreRef) -> tuple[DefsRef, JsonSchemaValue]:\n        \"\"\"This method wraps the get_defs_ref method with some cache-lookup/population logic,\n        and returns both the produced defs_ref and the JSON schema that will refer to the right definition.\n\n        Args:\n            core_ref: The core reference to get the definitions reference for.\n\n        Returns:\n            A tuple of the definitions reference and the JSON schema that will refer to it.\n        \"\"\"\n        core_mode_ref = (core_ref, self.mode)\n        maybe_defs_ref = self.core_to_defs_refs.get(core_mode_ref)\n        if maybe_defs_ref is not None:\n            json_ref = self.core_to_json_refs[core_mode_ref]\n            return maybe_defs_ref, {'$ref': json_ref}\n\n        defs_ref = self.get_defs_ref(core_mode_ref)\n\n        # populate the ref translation mappings\n        self.core_to_defs_refs[core_mode_ref] = defs_ref\n        self.defs_to_core_refs[defs_ref] = core_mode_ref\n\n        json_ref = JsonRef(self.ref_template.format(model=defs_ref))\n        self.core_to_json_refs[core_mode_ref] = json_ref\n        self.json_to_defs_refs[json_ref] = defs_ref\n        ref_json_schema = {'$ref': json_ref}\n        return defs_ref, ref_json_schema\n\n    def handle_ref_overrides(self, json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        \"\"\"It is not valid for a schema with a top-level $ref to have sibling keys.\n\n        During our own schema generation, we treat sibling keys as overrides to the referenced schema,\n        but this is not how the official JSON schema spec works.\n\n        Because of this, we first remove any sibling keys that are redundant with the referenced schema, then if\n        any remain, we transform the schema from a top-level '$ref' to use allOf to move the $ref out of the top level.\n        (See bottom of https://swagger.io/docs/specification/using-ref/ for a reference about this behavior)\n        \"\"\"\n        if '$ref' in json_schema:\n            # prevent modifications to the input; this copy may be safe to drop if there is significant overhead\n            json_schema = json_schema.copy()\n\n            referenced_json_schema = self.get_schema_from_definitions(JsonRef(json_schema['$ref']))\n            if referenced_json_schema is None:\n                # This can happen when building schemas for models with not-yet-defined references.\n                # It may be a good idea to do a recursive pass at the end of the generation to remove\n                # any redundant override keys.\n                if len(json_schema) > 1:\n                    # Make it an allOf to at least resolve the sibling keys issue\n                    json_schema = json_schema.copy()\n                    json_schema.setdefault('allOf', [])\n                    json_schema['allOf'].append({'$ref': json_schema['$ref']})\n                    del json_schema['$ref']\n\n                return json_schema\n            for k, v in list(json_schema.items()):\n                if k == '$ref':\n                    continue\n                if k in referenced_json_schema and referenced_json_schema[k] == v:\n                    del json_schema[k]  # redundant key\n            if len(json_schema) > 1:\n                # There is a remaining \"override\" key, so we need to move $ref out of the top level\n                json_ref = JsonRef(json_schema['$ref'])\n                del json_schema['$ref']\n                assert 'allOf' not in json_schema  # this should never happen, but just in case\n                json_schema['allOf'] = [{'$ref': json_ref}]\n\n        return json_schema\n\n    def get_schema_from_definitions(self, json_ref: JsonRef) -> JsonSchemaValue | None:\n        def_ref = self.json_to_defs_refs[json_ref]\n        if def_ref in self._core_defs_invalid_for_json_schema:\n            raise self._core_defs_invalid_for_json_schema[def_ref]\n        return self.definitions.get(def_ref, None)\n\n    def encode_default(self, dft: Any) -> Any:\n        \"\"\"Encode a default value to a JSON-serializable value.\n\n        This is used to encode default values for fields in the generated JSON schema.\n\n        Args:\n            dft: The default value to encode.\n\n        Returns:\n            The encoded default value.\n        \"\"\"\n        from .type_adapter import TypeAdapter, _type_has_config\n\n        config = self._config\n        try:\n            default = (\n                dft\n                if _type_has_config(type(dft))\n                else TypeAdapter(type(dft), config=config.config_dict).dump_python(dft, mode='json')\n            )\n        except PydanticSchemaGenerationError:\n            raise pydantic_core.PydanticSerializationError(f'Unable to encode default value {dft}')\n\n        return pydantic_core.to_jsonable_python(\n            default,\n            timedelta_mode=config.ser_json_timedelta,\n            bytes_mode=config.ser_json_bytes,\n        )\n\n    def update_with_validations(\n        self, json_schema: JsonSchemaValue, core_schema: CoreSchema, mapping: dict[str, str]\n    ) -> None:\n        \"\"\"Update the json_schema with the corresponding validations specified in the core_schema,\n        using the provided mapping to translate keys in core_schema to the appropriate keys for a JSON schema.\n\n        Args:\n            json_schema: The JSON schema to update.\n            core_schema: The core schema to get the validations from.\n            mapping: A mapping from core_schema attribute names to the corresponding JSON schema attribute names.\n        \"\"\"\n        for core_key, json_schema_key in mapping.items():\n            if core_key in core_schema:\n                json_schema[json_schema_key] = core_schema[core_key]\n\n    class ValidationsMapping:\n        \"\"\"This class just contains mappings from core_schema attribute names to the corresponding\n        JSON schema attribute names. While I suspect it is unlikely to be necessary, you can in\n        principle override this class in a subclass of GenerateJsonSchema (by inheriting from\n        GenerateJsonSchema.ValidationsMapping) to change these mappings.\n        \"\"\"\n\n        numeric = {\n            'multiple_of': 'multipleOf',\n            'le': 'maximum',\n            'ge': 'minimum',\n            'lt': 'exclusiveMaximum',\n            'gt': 'exclusiveMinimum',\n        }\n        bytes = {\n            'min_length': 'minLength',\n            'max_length': 'maxLength',\n        }\n        string = {\n            'min_length': 'minLength',\n            'max_length': 'maxLength',\n            'pattern': 'pattern',\n        }\n        array = {\n            'min_length': 'minItems',\n            'max_length': 'maxItems',\n        }\n        object = {\n            'min_length': 'minProperties',\n            'max_length': 'maxProperties',\n        }\n        date = {\n            'le': 'maximum',\n            'ge': 'minimum',\n            'lt': 'exclusiveMaximum',\n            'gt': 'exclusiveMinimum',\n        }\n\n    def get_flattened_anyof(self, schemas: list[JsonSchemaValue]) -> JsonSchemaValue:\n        members = []\n        for schema in schemas:\n            if len(schema) == 1 and 'anyOf' in schema:\n                members.extend(schema['anyOf'])\n            else:\n                members.append(schema)\n        members = _deduplicate_schemas(members)\n        if len(members) == 1:\n            return members[0]\n        return {'anyOf': members}\n\n    def get_json_ref_counts(self, json_schema: JsonSchemaValue) -> dict[JsonRef, int]:\n        \"\"\"Get all values corresponding to the key '$ref' anywhere in the json_schema.\"\"\"\n        json_refs: dict[JsonRef, int] = Counter()\n\n        def _add_json_refs(schema: Any) -> None:\n            if isinstance(schema, dict):\n                if '$ref' in schema:\n                    json_ref = JsonRef(schema['$ref'])\n                    if not isinstance(json_ref, str):\n                        return  # in this case, '$ref' might have been the name of a property\n                    already_visited = json_ref in json_refs\n                    json_refs[json_ref] += 1\n                    if already_visited:\n                        return  # prevent recursion on a definition that was already visited\n                    defs_ref = self.json_to_defs_refs[json_ref]\n                    if defs_ref in self._core_defs_invalid_for_json_schema:\n                        raise self._core_defs_invalid_for_json_schema[defs_ref]\n                    _add_json_refs(self.definitions[defs_ref])\n\n                for v in schema.values():\n                    _add_json_refs(v)\n            elif isinstance(schema, list):\n                for v in schema:\n                    _add_json_refs(v)\n\n        _add_json_refs(json_schema)\n        return json_refs\n\n    def handle_invalid_for_json_schema(self, schema: CoreSchemaOrField, error_info: str) -> JsonSchemaValue:\n        raise PydanticInvalidForJsonSchema(f'Cannot generate a JsonSchema for {error_info}')\n\n    def emit_warning(self, kind: JsonSchemaWarningKind, detail: str) -> None:\n        \"\"\"This method simply emits PydanticJsonSchemaWarnings based on handling in the `warning_message` method.\"\"\"\n        message = self.render_warning_message(kind, detail)\n        if message is not None:\n            warnings.warn(message, PydanticJsonSchemaWarning)\n\n    def render_warning_message(self, kind: JsonSchemaWarningKind, detail: str) -> str | None:\n        \"\"\"This method is responsible for ignoring warnings as desired, and for formatting the warning messages.\n\n        You can override the value of `ignored_warning_kinds` in a subclass of GenerateJsonSchema\n        to modify what warnings are generated. If you want more control, you can override this method;\n        just return None in situations where you don't want warnings to be emitted.\n\n        Args:\n            kind: The kind of warning to render. It can be one of the following:\n\n                - 'skipped-choice': A choice field was skipped because it had no valid choices.\n                - 'non-serializable-default': A default value was skipped because it was not JSON-serializable.\n            detail: A string with additional details about the warning.\n\n        Returns:\n            The formatted warning message, or `None` if no warning should be emitted.\n        \"\"\"\n        if kind in self.ignored_warning_kinds:\n            return None\n        return f'{detail} [{kind}]'\n\n    def _build_definitions_remapping(self) -> _DefinitionsRemapping:\n        defs_to_json: dict[DefsRef, JsonRef] = {}\n        for defs_refs in self._prioritized_defsref_choices.values():\n            for defs_ref in defs_refs:\n                json_ref = JsonRef(self.ref_template.format(model=defs_ref))\n                defs_to_json[defs_ref] = json_ref\n\n        return _DefinitionsRemapping.from_prioritized_choices(\n            self._prioritized_defsref_choices, defs_to_json, self.definitions\n        )\n\n    def _garbage_collect_definitions(self, schema: JsonSchemaValue) -> None:\n        visited_defs_refs: set[DefsRef] = set()\n        unvisited_json_refs = _get_all_json_refs(schema)\n        while unvisited_json_refs:\n            next_json_ref = unvisited_json_refs.pop()\n            next_defs_ref = self.json_to_defs_refs[next_json_ref]\n            if next_defs_ref in visited_defs_refs:\n                continue\n            visited_defs_refs.add(next_defs_ref)\n            unvisited_json_refs.update(_get_all_json_refs(self.definitions[next_defs_ref]))\n\n        self.definitions = {k: v for k, v in self.definitions.items() if k in visited_defs_refs}\n\n\n# ##### Start JSON Schema Generation Functions #####\n\n\ndef model_json_schema(\n    cls: type[BaseModel] | type[PydanticDataclass],\n    by_alias: bool = True,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n    mode: JsonSchemaMode = 'validation',\n) -> dict[str, Any]:\n    \"\"\"Utility function to generate a JSON Schema for a model.\n\n    Args:\n        cls: The model class to generate a JSON Schema for.\n        by_alias: If `True` (the default), fields will be serialized according to their alias.\n            If `False`, fields will be serialized according to their attribute name.\n        ref_template: The template to use for generating JSON Schema references.\n        schema_generator: The class to use for generating the JSON Schema.\n        mode: The mode to use for generating the JSON Schema. It can be one of the following:\n\n            - 'validation': Generate a JSON Schema for validating data.\n            - 'serialization': Generate a JSON Schema for serializing data.\n\n    Returns:\n        The generated JSON Schema.\n    \"\"\"\n    from .main import BaseModel\n\n    schema_generator_instance = schema_generator(by_alias=by_alias, ref_template=ref_template)\n\n    if isinstance(cls.__pydantic_core_schema__, _mock_val_ser.MockCoreSchema):\n        cls.__pydantic_core_schema__.rebuild()\n\n    if cls is BaseModel:\n        raise AttributeError('model_json_schema() must be called on a subclass of BaseModel, not BaseModel itself.')\n\n    assert not isinstance(cls.__pydantic_core_schema__, _mock_val_ser.MockCoreSchema), 'this is a bug! please report it'\n    return schema_generator_instance.generate(cls.__pydantic_core_schema__, mode=mode)\n\n\ndef models_json_schema(\n    models: Sequence[tuple[type[BaseModel] | type[PydanticDataclass], JsonSchemaMode]],\n    *,\n    by_alias: bool = True,\n    title: str | None = None,\n    description: str | None = None,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n) -> tuple[dict[tuple[type[BaseModel] | type[PydanticDataclass], JsonSchemaMode], JsonSchemaValue], JsonSchemaValue]:\n    \"\"\"Utility function to generate a JSON Schema for multiple models.\n\n    Args:\n        models: A sequence of tuples of the form (model, mode).\n        by_alias: Whether field aliases should be used as keys in the generated JSON Schema.\n        title: The title of the generated JSON Schema.\n        description: The description of the generated JSON Schema.\n        ref_template: The reference template to use for generating JSON Schema references.\n        schema_generator: The schema generator to use for generating the JSON Schema.\n\n    Returns:\n        A tuple where:\n            - The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n                whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n                JsonRef references to definitions that are defined in the second returned element.)\n            - The second element is a JSON schema containing all definitions referenced in the first returned\n                    element, along with the optional title and description keys.\n    \"\"\"\n    for cls, _ in models:\n        if isinstance(cls.__pydantic_core_schema__, _mock_val_ser.MockCoreSchema):\n            cls.__pydantic_core_schema__.rebuild()\n\n    instance = schema_generator(by_alias=by_alias, ref_template=ref_template)\n    inputs: list[tuple[type[BaseModel] | type[PydanticDataclass], JsonSchemaMode, CoreSchema]] = [\n        (m, mode, m.__pydantic_core_schema__) for m, mode in models\n    ]\n    json_schemas_map, definitions = instance.generate_definitions(inputs)\n\n    json_schema: dict[str, Any] = {}\n    if definitions:\n        json_schema['$defs'] = definitions\n    if title:\n        json_schema['title'] = title\n    if description:\n        json_schema['description'] = description\n\n    return json_schemas_map, json_schema\n\n\n# ##### End JSON Schema Generation Functions #####\n\n\n_HashableJsonValue: TypeAlias = Union[\n    int, float, str, bool, None, Tuple['_HashableJsonValue', ...], Tuple[Tuple[str, '_HashableJsonValue'], ...]\n]\n\n\ndef _deduplicate_schemas(schemas: Iterable[JsonDict]) -> list[JsonDict]:\n    return list({_make_json_hashable(schema): schema for schema in schemas}.values())\n\n\ndef _make_json_hashable(value: JsonValue) -> _HashableJsonValue:\n    if isinstance(value, dict):\n        return tuple(sorted((k, _make_json_hashable(v)) for k, v in value.items()))\n    elif isinstance(value, list):\n        return tuple(_make_json_hashable(v) for v in value)\n    else:\n        return value\n\n\ndef _sort_json_schema(value: JsonSchemaValue, parent_key: str | None = None) -> JsonSchemaValue:\n    if isinstance(value, dict):\n        sorted_dict: dict[str, JsonSchemaValue] = {}\n        keys = value.keys()\n        if (parent_key != 'properties') and (parent_key != 'default'):\n            keys = sorted(keys)\n        for key in keys:\n            sorted_dict[key] = _sort_json_schema(value[key], parent_key=key)\n        return sorted_dict\n    elif isinstance(value, list):\n        sorted_list: list[JsonSchemaValue] = []\n        for item in value:  # type: ignore\n            sorted_list.append(_sort_json_schema(item, parent_key))\n        return sorted_list  # type: ignore\n    else:\n        return value\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass WithJsonSchema:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/json_schema/#withjsonschema-annotation\n\n    Add this as an annotation on a field to override the (base) JSON schema that would be generated for that field.\n    This provides a way to set a JSON schema for types that would otherwise raise errors when producing a JSON schema,\n    such as Callable, or types that have an is-instance core schema, without needing to go so far as creating a\n    custom subclass of pydantic.json_schema.GenerateJsonSchema.\n    Note that any _modifications_ to the schema that would normally be made (such as setting the title for model fields)\n    will still be performed.\n\n    If `mode` is set this will only apply to that schema generation mode, allowing you\n    to set different json schemas for validation and serialization.\n    \"\"\"\n\n    json_schema: JsonSchemaValue | None\n    mode: Literal['validation', 'serialization'] | None = None\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        mode = self.mode or handler.mode\n        if mode != handler.mode:\n            return handler(core_schema)\n        if self.json_schema is None:\n            # This exception is handled in pydantic.json_schema.GenerateJsonSchema._named_required_fields_schema\n            raise PydanticOmit\n        else:\n            return self.json_schema\n\n    def __hash__(self) -> int:\n        return hash(type(self.mode))\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass Examples:\n    \"\"\"Add examples to a JSON schema.\n\n    Examples should be a map of example names (strings)\n    to example values (any valid JSON).\n\n    If `mode` is set this will only apply to that schema generation mode,\n    allowing you to add different examples for validation and serialization.\n    \"\"\"\n\n    examples: dict[str, Any]\n    mode: Literal['validation', 'serialization'] | None = None\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        mode = self.mode or handler.mode\n        json_schema = handler(core_schema)\n        if mode != handler.mode:\n            return json_schema\n        examples = json_schema.get('examples', {})\n        examples.update(to_jsonable_python(self.examples))\n        json_schema['examples'] = examples\n        return json_schema\n\n    def __hash__(self) -> int:\n        return hash(type(self.mode))\n\n\ndef _get_all_json_refs(item: Any) -> set[JsonRef]:\n    \"\"\"Get all the definitions references from a JSON schema.\"\"\"\n    refs: set[JsonRef] = set()\n    stack = [item]\n\n    while stack:\n        current = stack.pop()\n        if isinstance(current, dict):\n            for key, value in current.items():\n                if key == '$ref' and isinstance(value, str):\n                    refs.add(JsonRef(value))\n                elif isinstance(value, dict):\n                    stack.append(value)\n                elif isinstance(value, list):\n                    stack.extend(value)\n        elif isinstance(current, list):\n            stack.extend(current)\n\n    return refs\n\n\nAnyType = TypeVar('AnyType')\n\nif TYPE_CHECKING:\n    SkipJsonSchema = Annotated[AnyType, ...]\nelse:\n\n    @dataclasses.dataclass(**_internal_dataclass.slots_true)\n    class SkipJsonSchema:\n        \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/json_schema/#skipjsonschema-annotation\n\n        Add this as an annotation on a field to skip generating a JSON schema for that field.\n\n        Example:\n            ```py\n            from typing import Union\n\n            from pydantic import BaseModel\n            from pydantic.json_schema import SkipJsonSchema\n\n            from pprint import pprint\n\n\n            class Model(BaseModel):\n                a: Union[int, None] = None  # (1)!\n                b: Union[int, SkipJsonSchema[None]] = None  # (2)!\n                c: SkipJsonSchema[Union[int, None]] = None  # (3)!\n\n\n            pprint(Model.model_json_schema())\n            '''\n            {\n                'properties': {\n                    'a': {\n                        'anyOf': [\n                            {'type': 'integer'},\n                            {'type': 'null'}\n                        ],\n                        'default': None,\n                        'title': 'A'\n                    },\n                    'b': {\n                        'default': None,\n                        'title': 'B',\n                        'type': 'integer'\n                    }\n                },\n                'title': 'Model',\n                'type': 'object'\n            }\n            '''\n            ```\n\n            1. The integer and null types are both included in the schema for `a`.\n            2. The integer type is the only type included in the schema for `b`.\n            3. The entirety of the `c` field is omitted from the schema.\n        \"\"\"\n\n        def __class_getitem__(cls, item: AnyType) -> AnyType:\n            return Annotated[item, cls()]\n\n        def __get_pydantic_json_schema__(\n            self, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            raise PydanticOmit\n\n        def __hash__(self) -> int:\n            return hash(type(self))\n\n\ndef _get_typed_dict_cls(schema: core_schema.TypedDictSchema) -> type[Any] | None:\n    metadata = _core_metadata.CoreMetadataHandler(schema).metadata\n    cls = metadata.get('pydantic_typed_dict_cls')\n    return cls\n\n\ndef _get_typed_dict_config(cls: type[Any] | None) -> ConfigDict:\n    if cls is not None:\n        try:\n            return _decorators.get_attribute_from_bases(cls, '__pydantic_config__')\n        except AttributeError:\n            pass\n    return {}\n", "pydantic/functional_serializers.py": "\"\"\"This module contains related classes and functions for serialization.\"\"\"\n\nfrom __future__ import annotations\n\nimport dataclasses\nfrom functools import partialmethod\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar, Union, overload\n\nfrom pydantic_core import PydanticUndefined, core_schema\nfrom pydantic_core import core_schema as _core_schema\nfrom typing_extensions import Annotated, Literal, TypeAlias\n\nfrom . import PydanticUndefinedAnnotation\nfrom ._internal import _decorators, _internal_dataclass\nfrom .annotated_handlers import GetCoreSchemaHandler\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true, frozen=True)\nclass PlainSerializer:\n    \"\"\"Plain serializers use a function to modify the output of serialization.\n\n    This is particularly helpful when you want to customize the serialization for annotated types.\n    Consider an input of `list`, which will be serialized into a space-delimited string.\n\n    ```python\n    from typing import List\n\n    from typing_extensions import Annotated\n\n    from pydantic import BaseModel, PlainSerializer\n\n    CustomStr = Annotated[\n        List, PlainSerializer(lambda x: ' '.join(x), return_type=str)\n    ]\n\n    class StudentModel(BaseModel):\n        courses: CustomStr\n\n    student = StudentModel(courses=['Math', 'Chemistry', 'English'])\n    print(student.model_dump())\n    #> {'courses': 'Math Chemistry English'}\n    ```\n\n    Attributes:\n        func: The serializer function.\n        return_type: The return type for the function. If omitted it will be inferred from the type annotation.\n        when_used: Determines when this serializer should be used. Accepts a string with values `'always'`,\n            `'unless-none'`, `'json'`, and `'json-unless-none'`. Defaults to 'always'.\n    \"\"\"\n\n    func: core_schema.SerializerFunction\n    return_type: Any = PydanticUndefined\n    when_used: Literal['always', 'unless-none', 'json', 'json-unless-none'] = 'always'\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        \"\"\"Gets the Pydantic core schema.\n\n        Args:\n            source_type: The source type.\n            handler: The `GetCoreSchemaHandler` instance.\n\n        Returns:\n            The Pydantic core schema.\n        \"\"\"\n        schema = handler(source_type)\n        try:\n            return_type = _decorators.get_function_return_type(\n                self.func, self.return_type, handler._get_types_namespace()\n            )\n        except NameError as e:\n            raise PydanticUndefinedAnnotation.from_name_error(e) from e\n        return_schema = None if return_type is PydanticUndefined else handler.generate_schema(return_type)\n        schema['serialization'] = core_schema.plain_serializer_function_ser_schema(\n            function=self.func,\n            info_arg=_decorators.inspect_annotated_serializer(self.func, 'plain'),\n            return_schema=return_schema,\n            when_used=self.when_used,\n        )\n        return schema\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true, frozen=True)\nclass WrapSerializer:\n    \"\"\"Wrap serializers receive the raw inputs along with a handler function that applies the standard serialization\n    logic, and can modify the resulting value before returning it as the final output of serialization.\n\n    For example, here's a scenario in which a wrap serializer transforms timezones to UTC **and** utilizes the existing `datetime` serialization logic.\n\n    ```python\n    from datetime import datetime, timezone\n    from typing import Any, Dict\n\n    from typing_extensions import Annotated\n\n    from pydantic import BaseModel, WrapSerializer\n\n    class EventDatetime(BaseModel):\n        start: datetime\n        end: datetime\n\n    def convert_to_utc(value: Any, handler, info) -> Dict[str, datetime]:\n        # Note that `helper` can actually help serialize the `value` for further custom serialization in case it's a subclass.\n        partial_result = handler(value, info)\n        if info.mode == 'json':\n            return {\n                k: datetime.fromisoformat(v).astimezone(timezone.utc)\n                for k, v in partial_result.items()\n            }\n        return {k: v.astimezone(timezone.utc) for k, v in partial_result.items()}\n\n    UTCEventDatetime = Annotated[EventDatetime, WrapSerializer(convert_to_utc)]\n\n    class EventModel(BaseModel):\n        event_datetime: UTCEventDatetime\n\n    dt = EventDatetime(\n        start='2024-01-01T07:00:00-08:00', end='2024-01-03T20:00:00+06:00'\n    )\n    event = EventModel(event_datetime=dt)\n    print(event.model_dump())\n    '''\n    {\n        'event_datetime': {\n            'start': datetime.datetime(\n                2024, 1, 1, 15, 0, tzinfo=datetime.timezone.utc\n            ),\n            'end': datetime.datetime(\n                2024, 1, 3, 14, 0, tzinfo=datetime.timezone.utc\n            ),\n        }\n    }\n    '''\n\n    print(event.model_dump_json())\n    '''\n    {\"event_datetime\":{\"start\":\"2024-01-01T15:00:00Z\",\"end\":\"2024-01-03T14:00:00Z\"}}\n    '''\n    ```\n\n    Attributes:\n        func: The serializer function to be wrapped.\n        return_type: The return type for the function. If omitted it will be inferred from the type annotation.\n        when_used: Determines when this serializer should be used. Accepts a string with values `'always'`,\n            `'unless-none'`, `'json'`, and `'json-unless-none'`. Defaults to 'always'.\n    \"\"\"\n\n    func: core_schema.WrapSerializerFunction\n    return_type: Any = PydanticUndefined\n    when_used: Literal['always', 'unless-none', 'json', 'json-unless-none'] = 'always'\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        \"\"\"This method is used to get the Pydantic core schema of the class.\n\n        Args:\n            source_type: Source type.\n            handler: Core schema handler.\n\n        Returns:\n            The generated core schema of the class.\n        \"\"\"\n        schema = handler(source_type)\n        try:\n            return_type = _decorators.get_function_return_type(\n                self.func, self.return_type, handler._get_types_namespace()\n            )\n        except NameError as e:\n            raise PydanticUndefinedAnnotation.from_name_error(e) from e\n        return_schema = None if return_type is PydanticUndefined else handler.generate_schema(return_type)\n        schema['serialization'] = core_schema.wrap_serializer_function_ser_schema(\n            function=self.func,\n            info_arg=_decorators.inspect_annotated_serializer(self.func, 'wrap'),\n            return_schema=return_schema,\n            when_used=self.when_used,\n        )\n        return schema\n\n\nif TYPE_CHECKING:\n    _PartialClsOrStaticMethod: TypeAlias = Union[classmethod[Any, Any, Any], staticmethod[Any, Any], partialmethod[Any]]\n    _PlainSerializationFunction = Union[_core_schema.SerializerFunction, _PartialClsOrStaticMethod]\n    _WrapSerializationFunction = Union[_core_schema.WrapSerializerFunction, _PartialClsOrStaticMethod]\n    _PlainSerializeMethodType = TypeVar('_PlainSerializeMethodType', bound=_PlainSerializationFunction)\n    _WrapSerializeMethodType = TypeVar('_WrapSerializeMethodType', bound=_WrapSerializationFunction)\n\n\n@overload\ndef field_serializer(\n    field: str,\n    /,\n    *fields: str,\n    return_type: Any = ...,\n    when_used: Literal['always', 'unless-none', 'json', 'json-unless-none'] = ...,\n    check_fields: bool | None = ...,\n) -> Callable[[_PlainSerializeMethodType], _PlainSerializeMethodType]: ...\n\n\n@overload\ndef field_serializer(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['plain'],\n    return_type: Any = ...,\n    when_used: Literal['always', 'unless-none', 'json', 'json-unless-none'] = ...,\n    check_fields: bool | None = ...,\n) -> Callable[[_PlainSerializeMethodType], _PlainSerializeMethodType]: ...\n\n\n@overload\ndef field_serializer(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['wrap'],\n    return_type: Any = ...,\n    when_used: Literal['always', 'unless-none', 'json', 'json-unless-none'] = ...,\n    check_fields: bool | None = ...,\n) -> Callable[[_WrapSerializeMethodType], _WrapSerializeMethodType]: ...\n\n\ndef field_serializer(\n    *fields: str,\n    mode: Literal['plain', 'wrap'] = 'plain',\n    return_type: Any = PydanticUndefined,\n    when_used: Literal['always', 'unless-none', 'json', 'json-unless-none'] = 'always',\n    check_fields: bool | None = None,\n) -> Callable[[Any], Any]:\n    \"\"\"Decorator that enables custom field serialization.\n\n    In the below example, a field of type `set` is used to mitigate duplication. A `field_serializer` is used to serialize the data as a sorted list.\n\n    ```python\n    from typing import Set\n\n    from pydantic import BaseModel, field_serializer\n\n    class StudentModel(BaseModel):\n        name: str = 'Jane'\n        courses: Set[str]\n\n        @field_serializer('courses', when_used='json')\n        def serialize_courses_in_order(courses: Set[str]):\n            return sorted(courses)\n\n    student = StudentModel(courses={'Math', 'Chemistry', 'English'})\n    print(student.model_dump_json())\n    #> {\"name\":\"Jane\",\"courses\":[\"Chemistry\",\"English\",\"Math\"]}\n    ```\n\n    See [Custom serializers](../concepts/serialization.md#custom-serializers) for more information.\n\n    Four signatures are supported:\n\n    - `(self, value: Any, info: FieldSerializationInfo)`\n    - `(self, value: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo)`\n    - `(value: Any, info: SerializationInfo)`\n    - `(value: Any, nxt: SerializerFunctionWrapHandler, info: SerializationInfo)`\n\n    Args:\n        fields: Which field(s) the method should be called on.\n        mode: The serialization mode.\n\n            - `plain` means the function will be called instead of the default serialization logic,\n            - `wrap` means the function will be called with an argument to optionally call the\n               default serialization logic.\n        return_type: Optional return type for the function, if omitted it will be inferred from the type annotation.\n        when_used: Determines the serializer will be used for serialization.\n        check_fields: Whether to check that the fields actually exist on the model.\n\n    Returns:\n        The decorator function.\n    \"\"\"\n\n    def dec(\n        f: Callable[..., Any] | staticmethod[Any, Any] | classmethod[Any, Any, Any],\n    ) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.FieldSerializerDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            return_type=return_type,\n            when_used=when_used,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, dec_info)\n\n    return dec\n\n\nFuncType = TypeVar('FuncType', bound=Callable[..., Any])\n\n\n@overload\ndef model_serializer(__f: FuncType) -> FuncType: ...\n\n\n@overload\ndef model_serializer(\n    *,\n    mode: Literal['plain', 'wrap'] = ...,\n    when_used: Literal['always', 'unless-none', 'json', 'json-unless-none'] = 'always',\n    return_type: Any = ...,\n) -> Callable[[FuncType], FuncType]: ...\n\n\ndef model_serializer(\n    f: Callable[..., Any] | None = None,\n    /,\n    *,\n    mode: Literal['plain', 'wrap'] = 'plain',\n    when_used: Literal['always', 'unless-none', 'json', 'json-unless-none'] = 'always',\n    return_type: Any = PydanticUndefined,\n) -> Callable[[Any], Any]:\n    \"\"\"Decorator that enables custom model serialization.\n\n    This is useful when a model need to be serialized in a customized manner, allowing for flexibility beyond just specific fields.\n\n    An example would be to serialize temperature to the same temperature scale, such as degrees Celsius.\n\n    ```python\n    from typing import Literal\n\n    from pydantic import BaseModel, model_serializer\n\n    class TemperatureModel(BaseModel):\n        unit: Literal['C', 'F']\n        value: int\n\n        @model_serializer()\n        def serialize_model(self):\n            if self.unit == 'F':\n                return {'unit': 'C', 'value': int((self.value - 32) / 1.8)}\n            return {'unit': self.unit, 'value': self.value}\n\n    temperature = TemperatureModel(unit='F', value=212)\n    print(temperature.model_dump())\n    #> {'unit': 'C', 'value': 100}\n    ```\n\n    See [Custom serializers](../concepts/serialization.md#custom-serializers) for more information.\n\n    Args:\n        f: The function to be decorated.\n        mode: The serialization mode.\n\n            - `'plain'` means the function will be called instead of the default serialization logic\n            - `'wrap'` means the function will be called with an argument to optionally call the default\n                serialization logic.\n        when_used: Determines when this serializer should be used.\n        return_type: The return type for the function. If omitted it will be inferred from the type annotation.\n\n    Returns:\n        The decorator function.\n    \"\"\"\n\n    def dec(f: Callable[..., Any]) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.ModelSerializerDecoratorInfo(mode=mode, return_type=return_type, when_used=when_used)\n        return _decorators.PydanticDescriptorProxy(f, dec_info)\n\n    if f is None:\n        return dec\n    else:\n        return dec(f)  # type: ignore\n\n\nAnyType = TypeVar('AnyType')\n\n\nif TYPE_CHECKING:\n    SerializeAsAny = Annotated[AnyType, ...]  # SerializeAsAny[list[str]] will be treated by type checkers as list[str]\n    \"\"\"Force serialization to ignore whatever is defined in the schema and instead ask the object\n    itself how it should be serialized.\n    In particular, this means that when model subclasses are serialized, fields present in the subclass\n    but not in the original schema will be included.\n    \"\"\"\nelse:\n\n    @dataclasses.dataclass(**_internal_dataclass.slots_true)\n    class SerializeAsAny:  # noqa: D101\n        def __class_getitem__(cls, item: Any) -> Any:\n            return Annotated[item, SerializeAsAny()]\n\n        def __get_pydantic_core_schema__(\n            self, source_type: Any, handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            schema = handler(source_type)\n            schema_to_update = schema\n            while schema_to_update['type'] == 'definitions':\n                schema_to_update = schema_to_update.copy()\n                schema_to_update = schema_to_update['schema']\n            schema_to_update['serialization'] = core_schema.wrap_serializer_function_ser_schema(\n                lambda x, h: h(x), schema=core_schema.any_schema()\n            )\n            return schema\n\n        __hash__ = object.__hash__\n", "pydantic/error_wrappers.py": "\"\"\"The `error_wrappers` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/validate_call_decorator.py": "\"\"\"Decorator for validating function calls.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport functools\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar, overload\n\nfrom ._internal import _validate_call\n\n__all__ = ('validate_call',)\n\nif TYPE_CHECKING:\n    from .config import ConfigDict\n\n    AnyCallableT = TypeVar('AnyCallableT', bound=Callable[..., Any])\n\n\n@overload\ndef validate_call(\n    *, config: ConfigDict | None = None, validate_return: bool = False\n) -> Callable[[AnyCallableT], AnyCallableT]: ...\n\n\n@overload\ndef validate_call(func: AnyCallableT, /) -> AnyCallableT: ...\n\n\ndef validate_call(\n    func: AnyCallableT | None = None,\n    /,\n    *,\n    config: ConfigDict | None = None,\n    validate_return: bool = False,\n) -> AnyCallableT | Callable[[AnyCallableT], AnyCallableT]:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/validation_decorator/\n\n    Returns a decorated wrapper around the function that validates the arguments and, optionally, the return value.\n\n    Usage may be either as a plain decorator `@validate_call` or with arguments `@validate_call(...)`.\n\n    Args:\n        func: The function to be decorated.\n        config: The configuration dictionary.\n        validate_return: Whether to validate the return value.\n\n    Returns:\n        The decorated function.\n    \"\"\"\n\n    def validate(function: AnyCallableT) -> AnyCallableT:\n        if isinstance(function, (classmethod, staticmethod)):\n            name = type(function).__name__\n            raise TypeError(f'The `@{name}` decorator should be applied after `@validate_call` (put `@{name}` on top)')\n        validate_call_wrapper = _validate_call.ValidateCallWrapper(function, config, validate_return)\n\n        @functools.wraps(function)\n        def wrapper_function(*args, **kwargs):\n            return validate_call_wrapper(*args, **kwargs)\n\n        wrapper_function.raw_function = function  # type: ignore\n\n        return wrapper_function  # type: ignore\n\n    if func:\n        return validate(func)\n    else:\n        return validate\n", "pydantic/errors.py": "\"\"\"Pydantic-specific errors.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport re\n\nfrom typing_extensions import Literal, Self\n\nfrom ._migration import getattr_migration\nfrom .version import version_short\n\n__all__ = (\n    'PydanticUserError',\n    'PydanticUndefinedAnnotation',\n    'PydanticImportError',\n    'PydanticSchemaGenerationError',\n    'PydanticInvalidForJsonSchema',\n    'PydanticErrorCodes',\n)\n\n# We use this URL to allow for future flexibility about how we host the docs, while allowing for Pydantic\n# code in the while with \"old\" URLs to still work.\n# 'u' refers to \"user errors\" - e.g. errors caused by developers using pydantic, as opposed to validation errors.\nDEV_ERROR_DOCS_URL = f'https://errors.pydantic.dev/{version_short()}/u/'\nPydanticErrorCodes = Literal[\n    'class-not-fully-defined',\n    'custom-json-schema',\n    'decorator-missing-field',\n    'discriminator-no-field',\n    'discriminator-alias-type',\n    'discriminator-needs-literal',\n    'discriminator-alias',\n    'discriminator-validator',\n    'callable-discriminator-no-tag',\n    'typed-dict-version',\n    'model-field-overridden',\n    'model-field-missing-annotation',\n    'config-both',\n    'removed-kwargs',\n    'invalid-for-json-schema',\n    'json-schema-already-used',\n    'base-model-instantiated',\n    'undefined-annotation',\n    'schema-for-unknown-type',\n    'import-error',\n    'create-model-field-definitions',\n    'create-model-config-base',\n    'validator-no-fields',\n    'validator-invalid-fields',\n    'validator-instance-method',\n    'root-validator-pre-skip',\n    'model-serializer-instance-method',\n    'validator-field-config-info',\n    'validator-v1-signature',\n    'validator-signature',\n    'field-serializer-signature',\n    'model-serializer-signature',\n    'multiple-field-serializers',\n    'invalid_annotated_type',\n    'type-adapter-config-unused',\n    'root-model-extra',\n    'unevaluable-type-annotation',\n    'dataclass-init-false-extra-allow',\n    'clashing-init-and-init-var',\n    'model-config-invalid-field-name',\n    'with-config-on-model',\n    'dataclass-on-model',\n]\n\n\nclass PydanticErrorMixin:\n    \"\"\"A mixin class for common functionality shared by all Pydantic-specific errors.\n\n    Attributes:\n        message: A message describing the error.\n        code: An optional error code from PydanticErrorCodes enum.\n    \"\"\"\n\n    def __init__(self, message: str, *, code: PydanticErrorCodes | None) -> None:\n        self.message = message\n        self.code = code\n\n    def __str__(self) -> str:\n        if self.code is None:\n            return self.message\n        else:\n            return f'{self.message}\\n\\nFor further information visit {DEV_ERROR_DOCS_URL}{self.code}'\n\n\nclass PydanticUserError(PydanticErrorMixin, TypeError):\n    \"\"\"An error raised due to incorrect use of Pydantic.\"\"\"\n\n\nclass PydanticUndefinedAnnotation(PydanticErrorMixin, NameError):\n    \"\"\"A subclass of `NameError` raised when handling undefined annotations during `CoreSchema` generation.\n\n    Attributes:\n        name: Name of the error.\n        message: Description of the error.\n    \"\"\"\n\n    def __init__(self, name: str, message: str) -> None:\n        self.name = name\n        super().__init__(message=message, code='undefined-annotation')\n\n    @classmethod\n    def from_name_error(cls, name_error: NameError) -> Self:\n        \"\"\"Convert a `NameError` to a `PydanticUndefinedAnnotation` error.\n\n        Args:\n            name_error: `NameError` to be converted.\n\n        Returns:\n            Converted `PydanticUndefinedAnnotation` error.\n        \"\"\"\n        try:\n            name = name_error.name  # type: ignore  # python > 3.10\n        except AttributeError:\n            name = re.search(r\".*'(.+?)'\", str(name_error)).group(1)  # type: ignore[union-attr]\n        return cls(name=name, message=str(name_error))\n\n\nclass PydanticImportError(PydanticErrorMixin, ImportError):\n    \"\"\"An error raised when an import fails due to module changes between V1 and V2.\n\n    Attributes:\n        message: Description of the error.\n    \"\"\"\n\n    def __init__(self, message: str) -> None:\n        super().__init__(message, code='import-error')\n\n\nclass PydanticSchemaGenerationError(PydanticUserError):\n    \"\"\"An error raised during failures to generate a `CoreSchema` for some type.\n\n    Attributes:\n        message: Description of the error.\n    \"\"\"\n\n    def __init__(self, message: str) -> None:\n        super().__init__(message, code='schema-for-unknown-type')\n\n\nclass PydanticInvalidForJsonSchema(PydanticUserError):\n    \"\"\"An error raised during failures to generate a JSON schema for some `CoreSchema`.\n\n    Attributes:\n        message: Description of the error.\n    \"\"\"\n\n    def __init__(self, message: str) -> None:\n        super().__init__(message, code='invalid-for-json-schema')\n\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/json.py": "\"\"\"The `json` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/validators.py": "\"\"\"The `validators` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/types.py": "\"\"\"The types module contains custom types used by pydantic.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport base64\nimport dataclasses as _dataclasses\nimport re\nfrom datetime import date, datetime\nfrom decimal import Decimal\nfrom enum import Enum\nfrom pathlib import Path\nfrom types import ModuleType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    FrozenSet,\n    Generic,\n    Hashable,\n    Iterator,\n    List,\n    Pattern,\n    Set,\n    TypeVar,\n    Union,\n    cast,\n    get_args,\n    get_origin,\n)\nfrom uuid import UUID\n\nimport annotated_types\nfrom annotated_types import BaseMetadata, MaxLen, MinLen\nfrom pydantic_core import CoreSchema, PydanticCustomError, core_schema\nfrom typing_extensions import Annotated, Literal, Protocol, TypeAlias, TypeAliasType, deprecated\n\nfrom ._internal import (\n    _core_utils,\n    _fields,\n    _internal_dataclass,\n    _typing_extra,\n    _utils,\n    _validators,\n)\nfrom ._migration import getattr_migration\nfrom .annotated_handlers import GetCoreSchemaHandler, GetJsonSchemaHandler\nfrom .errors import PydanticUserError\nfrom .json_schema import JsonSchemaValue\nfrom .warnings import PydanticDeprecatedSince20\n\n__all__ = (\n    'Strict',\n    'StrictStr',\n    'conbytes',\n    'conlist',\n    'conset',\n    'confrozenset',\n    'constr',\n    'ImportString',\n    'conint',\n    'PositiveInt',\n    'NegativeInt',\n    'NonNegativeInt',\n    'NonPositiveInt',\n    'confloat',\n    'PositiveFloat',\n    'NegativeFloat',\n    'NonNegativeFloat',\n    'NonPositiveFloat',\n    'FiniteFloat',\n    'condecimal',\n    'UUID1',\n    'UUID3',\n    'UUID4',\n    'UUID5',\n    'FilePath',\n    'DirectoryPath',\n    'NewPath',\n    'Json',\n    'Secret',\n    'SecretStr',\n    'SecretBytes',\n    'StrictBool',\n    'StrictBytes',\n    'StrictInt',\n    'StrictFloat',\n    'PaymentCardNumber',\n    'ByteSize',\n    'PastDate',\n    'FutureDate',\n    'PastDatetime',\n    'FutureDatetime',\n    'condate',\n    'AwareDatetime',\n    'NaiveDatetime',\n    'AllowInfNan',\n    'EncoderProtocol',\n    'EncodedBytes',\n    'EncodedStr',\n    'Base64Encoder',\n    'Base64Bytes',\n    'Base64Str',\n    'Base64UrlBytes',\n    'Base64UrlStr',\n    'GetPydanticSchema',\n    'StringConstraints',\n    'Tag',\n    'Discriminator',\n    'JsonValue',\n    'OnErrorOmit',\n)\n\n\nT = TypeVar('T')\n\n\n@_dataclasses.dataclass\nclass Strict(_fields.PydanticMetadata, BaseMetadata):\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/strict_mode/#strict-mode-with-annotated-strict\n\n    A field metadata class to indicate that a field should be validated in strict mode.\n\n    Attributes:\n        strict: Whether to validate the field in strict mode.\n\n    Example:\n        ```python\n        from typing_extensions import Annotated\n\n        from pydantic.types import Strict\n\n        StrictBool = Annotated[bool, Strict()]\n        ```\n    \"\"\"\n\n    strict: bool = True\n\n    def __hash__(self) -> int:\n        return hash(self.strict)\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BOOLEAN TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nStrictBool = Annotated[bool, Strict()]\n\"\"\"A boolean that must be either ``True`` or ``False``.\"\"\"\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ INTEGER TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\ndef conint(\n    *,\n    strict: bool | None = None,\n    gt: int | None = None,\n    ge: int | None = None,\n    lt: int | None = None,\n    le: int | None = None,\n    multiple_of: int | None = None,\n) -> type[int]:\n    \"\"\"\n    !!! warning \"Discouraged\"\n        This function is **discouraged** in favor of using\n        [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with\n        [`Field`][pydantic.fields.Field] instead.\n\n        This function will be **deprecated** in Pydantic 3.0.\n\n        The reason is that `conint` returns a type, which doesn't play well with static analysis tools.\n\n        === \":x: Don't do this\"\n            ```py\n            from pydantic import BaseModel, conint\n\n            class Foo(BaseModel):\n                bar: conint(strict=True, gt=0)\n            ```\n\n        === \":white_check_mark: Do this\"\n            ```py\n            from typing_extensions import Annotated\n\n            from pydantic import BaseModel, Field\n\n            class Foo(BaseModel):\n                bar: Annotated[int, Field(strict=True, gt=0)]\n            ```\n\n    A wrapper around `int` that allows for additional constraints.\n\n    Args:\n        strict: Whether to validate the integer in strict mode. Defaults to `None`.\n        gt: The value must be greater than this.\n        ge: The value must be greater than or equal to this.\n        lt: The value must be less than this.\n        le: The value must be less than or equal to this.\n        multiple_of: The value must be a multiple of this.\n\n    Returns:\n        The wrapped integer type.\n\n    ```py\n    from pydantic import BaseModel, ValidationError, conint\n\n    class ConstrainedExample(BaseModel):\n        constrained_int: conint(gt=1)\n\n    m = ConstrainedExample(constrained_int=2)\n    print(repr(m))\n    #> ConstrainedExample(constrained_int=2)\n\n    try:\n        ConstrainedExample(constrained_int=0)\n    except ValidationError as e:\n        print(e.errors())\n        '''\n        [\n            {\n                'type': 'greater_than',\n                'loc': ('constrained_int',),\n                'msg': 'Input should be greater than 1',\n                'input': 0,\n                'ctx': {'gt': 1},\n                'url': 'https://errors.pydantic.dev/2/v/greater_than',\n            }\n        ]\n        '''\n    ```\n\n    \"\"\"  # noqa: D212\n    return Annotated[  # pyright: ignore[reportReturnType]\n        int,\n        Strict(strict) if strict is not None else None,\n        annotated_types.Interval(gt=gt, ge=ge, lt=lt, le=le),\n        annotated_types.MultipleOf(multiple_of) if multiple_of is not None else None,\n    ]\n\n\nPositiveInt = Annotated[int, annotated_types.Gt(0)]\n\"\"\"An integer that must be greater than zero.\n\n```py\nfrom pydantic import BaseModel, PositiveInt, ValidationError\n\nclass Model(BaseModel):\n    positive_int: PositiveInt\n\nm = Model(positive_int=1)\nprint(repr(m))\n#> Model(positive_int=1)\n\ntry:\n    Model(positive_int=-1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('positive_int',),\n            'msg': 'Input should be greater than 0',\n            'input': -1,\n            'ctx': {'gt': 0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    '''\n```\n\"\"\"\nNegativeInt = Annotated[int, annotated_types.Lt(0)]\n\"\"\"An integer that must be less than zero.\n\n```py\nfrom pydantic import BaseModel, NegativeInt, ValidationError\n\nclass Model(BaseModel):\n    negative_int: NegativeInt\n\nm = Model(negative_int=-1)\nprint(repr(m))\n#> Model(negative_int=-1)\n\ntry:\n    Model(negative_int=1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than',\n            'loc': ('negative_int',),\n            'msg': 'Input should be less than 0',\n            'input': 1,\n            'ctx': {'lt': 0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than',\n        }\n    ]\n    '''\n```\n\"\"\"\nNonPositiveInt = Annotated[int, annotated_types.Le(0)]\n\"\"\"An integer that must be less than or equal to zero.\n\n```py\nfrom pydantic import BaseModel, NonPositiveInt, ValidationError\n\nclass Model(BaseModel):\n    non_positive_int: NonPositiveInt\n\nm = Model(non_positive_int=0)\nprint(repr(m))\n#> Model(non_positive_int=0)\n\ntry:\n    Model(non_positive_int=1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than_equal',\n            'loc': ('non_positive_int',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 1,\n            'ctx': {'le': 0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than_equal',\n        }\n    ]\n    '''\n```\n\"\"\"\nNonNegativeInt = Annotated[int, annotated_types.Ge(0)]\n\"\"\"An integer that must be greater than or equal to zero.\n\n```py\nfrom pydantic import BaseModel, NonNegativeInt, ValidationError\n\nclass Model(BaseModel):\n    non_negative_int: NonNegativeInt\n\nm = Model(non_negative_int=0)\nprint(repr(m))\n#> Model(non_negative_int=0)\n\ntry:\n    Model(non_negative_int=-1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than_equal',\n            'loc': ('non_negative_int',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -1,\n            'ctx': {'ge': 0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',\n        }\n    ]\n    '''\n```\n\"\"\"\nStrictInt = Annotated[int, Strict()]\n\"\"\"An integer that must be validated in strict mode.\n\n```py\nfrom pydantic import BaseModel, StrictInt, ValidationError\n\nclass StrictIntModel(BaseModel):\n    strict_int: StrictInt\n\ntry:\n    StrictIntModel(strict_int=3.14159)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for StrictIntModel\n    strict_int\n      Input should be a valid integer [type=int_type, input_value=3.14159, input_type=float]\n    '''\n```\n\"\"\"\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FLOAT TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n@_dataclasses.dataclass\nclass AllowInfNan(_fields.PydanticMetadata):\n    \"\"\"A field metadata class to indicate that a field should allow ``-inf``, ``inf``, and ``nan``.\"\"\"\n\n    allow_inf_nan: bool = True\n\n    def __hash__(self) -> int:\n        return hash(self.allow_inf_nan)\n\n\ndef confloat(\n    *,\n    strict: bool | None = None,\n    gt: float | None = None,\n    ge: float | None = None,\n    lt: float | None = None,\n    le: float | None = None,\n    multiple_of: float | None = None,\n    allow_inf_nan: bool | None = None,\n) -> type[float]:\n    \"\"\"\n    !!! warning \"Discouraged\"\n        This function is **discouraged** in favor of using\n        [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with\n        [`Field`][pydantic.fields.Field] instead.\n\n        This function will be **deprecated** in Pydantic 3.0.\n\n        The reason is that `confloat` returns a type, which doesn't play well with static analysis tools.\n\n        === \":x: Don't do this\"\n            ```py\n            from pydantic import BaseModel, confloat\n\n            class Foo(BaseModel):\n                bar: confloat(strict=True, gt=0)\n            ```\n\n        === \":white_check_mark: Do this\"\n            ```py\n            from typing_extensions import Annotated\n\n            from pydantic import BaseModel, Field\n\n            class Foo(BaseModel):\n                bar: Annotated[float, Field(strict=True, gt=0)]\n            ```\n\n    A wrapper around `float` that allows for additional constraints.\n\n    Args:\n        strict: Whether to validate the float in strict mode.\n        gt: The value must be greater than this.\n        ge: The value must be greater than or equal to this.\n        lt: The value must be less than this.\n        le: The value must be less than or equal to this.\n        multiple_of: The value must be a multiple of this.\n        allow_inf_nan: Whether to allow `-inf`, `inf`, and `nan`.\n\n    Returns:\n        The wrapped float type.\n\n    ```py\n    from pydantic import BaseModel, ValidationError, confloat\n\n    class ConstrainedExample(BaseModel):\n        constrained_float: confloat(gt=1.0)\n\n    m = ConstrainedExample(constrained_float=1.1)\n    print(repr(m))\n    #> ConstrainedExample(constrained_float=1.1)\n\n    try:\n        ConstrainedExample(constrained_float=0.9)\n    except ValidationError as e:\n        print(e.errors())\n        '''\n        [\n            {\n                'type': 'greater_than',\n                'loc': ('constrained_float',),\n                'msg': 'Input should be greater than 1',\n                'input': 0.9,\n                'ctx': {'gt': 1.0},\n                'url': 'https://errors.pydantic.dev/2/v/greater_than',\n            }\n        ]\n        '''\n    ```\n    \"\"\"  # noqa: D212\n    return Annotated[  # pyright: ignore[reportReturnType]\n        float,\n        Strict(strict) if strict is not None else None,\n        annotated_types.Interval(gt=gt, ge=ge, lt=lt, le=le),\n        annotated_types.MultipleOf(multiple_of) if multiple_of is not None else None,\n        AllowInfNan(allow_inf_nan) if allow_inf_nan is not None else None,\n    ]\n\n\nPositiveFloat = Annotated[float, annotated_types.Gt(0)]\n\"\"\"A float that must be greater than zero.\n\n```py\nfrom pydantic import BaseModel, PositiveFloat, ValidationError\n\nclass Model(BaseModel):\n    positive_float: PositiveFloat\n\nm = Model(positive_float=1.0)\nprint(repr(m))\n#> Model(positive_float=1.0)\n\ntry:\n    Model(positive_float=-1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('positive_float',),\n            'msg': 'Input should be greater than 0',\n            'input': -1.0,\n            'ctx': {'gt': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    '''\n```\n\"\"\"\nNegativeFloat = Annotated[float, annotated_types.Lt(0)]\n\"\"\"A float that must be less than zero.\n\n```py\nfrom pydantic import BaseModel, NegativeFloat, ValidationError\n\nclass Model(BaseModel):\n    negative_float: NegativeFloat\n\nm = Model(negative_float=-1.0)\nprint(repr(m))\n#> Model(negative_float=-1.0)\n\ntry:\n    Model(negative_float=1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than',\n            'loc': ('negative_float',),\n            'msg': 'Input should be less than 0',\n            'input': 1.0,\n            'ctx': {'lt': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than',\n        }\n    ]\n    '''\n```\n\"\"\"\nNonPositiveFloat = Annotated[float, annotated_types.Le(0)]\n\"\"\"A float that must be less than or equal to zero.\n\n```py\nfrom pydantic import BaseModel, NonPositiveFloat, ValidationError\n\nclass Model(BaseModel):\n    non_positive_float: NonPositiveFloat\n\nm = Model(non_positive_float=0.0)\nprint(repr(m))\n#> Model(non_positive_float=0.0)\n\ntry:\n    Model(non_positive_float=1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than_equal',\n            'loc': ('non_positive_float',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 1.0,\n            'ctx': {'le': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than_equal',\n        }\n    ]\n    '''\n```\n\"\"\"\nNonNegativeFloat = Annotated[float, annotated_types.Ge(0)]\n\"\"\"A float that must be greater than or equal to zero.\n\n```py\nfrom pydantic import BaseModel, NonNegativeFloat, ValidationError\n\nclass Model(BaseModel):\n    non_negative_float: NonNegativeFloat\n\nm = Model(non_negative_float=0.0)\nprint(repr(m))\n#> Model(non_negative_float=0.0)\n\ntry:\n    Model(non_negative_float=-1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than_equal',\n            'loc': ('non_negative_float',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -1.0,\n            'ctx': {'ge': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',\n        }\n    ]\n    '''\n```\n\"\"\"\nStrictFloat = Annotated[float, Strict(True)]\n\"\"\"A float that must be validated in strict mode.\n\n```py\nfrom pydantic import BaseModel, StrictFloat, ValidationError\n\nclass StrictFloatModel(BaseModel):\n    strict_float: StrictFloat\n\ntry:\n    StrictFloatModel(strict_float='1.0')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for StrictFloatModel\n    strict_float\n      Input should be a valid number [type=float_type, input_value='1.0', input_type=str]\n    '''\n```\n\"\"\"\nFiniteFloat = Annotated[float, AllowInfNan(False)]\n\"\"\"A float that must be finite (not ``-inf``, ``inf``, or ``nan``).\n\n```py\nfrom pydantic import BaseModel, FiniteFloat\n\nclass Model(BaseModel):\n    finite: FiniteFloat\n\nm = Model(finite=1.0)\nprint(m)\n#> finite=1.0\n```\n\"\"\"\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BYTES TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\ndef conbytes(\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    strict: bool | None = None,\n) -> type[bytes]:\n    \"\"\"A wrapper around `bytes` that allows for additional constraints.\n\n    Args:\n        min_length: The minimum length of the bytes.\n        max_length: The maximum length of the bytes.\n        strict: Whether to validate the bytes in strict mode.\n\n    Returns:\n        The wrapped bytes type.\n    \"\"\"\n    return Annotated[  # pyright: ignore[reportReturnType]\n        bytes,\n        Strict(strict) if strict is not None else None,\n        annotated_types.Len(min_length or 0, max_length),\n    ]\n\n\nStrictBytes = Annotated[bytes, Strict()]\n\"\"\"A bytes that must be validated in strict mode.\"\"\"\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ STRING TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n@_dataclasses.dataclass(frozen=True)\nclass StringConstraints(annotated_types.GroupedMetadata):\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/fields/#string-constraints\n\n    Apply constraints to `str` types.\n\n    Attributes:\n        strip_whitespace: Whether to remove leading and trailing whitespace.\n        to_upper: Whether to convert the string to uppercase.\n        to_lower: Whether to convert the string to lowercase.\n        strict: Whether to validate the string in strict mode.\n        min_length: The minimum length of the string.\n        max_length: The maximum length of the string.\n        pattern: A regex pattern that the string must match.\n    \"\"\"\n\n    strip_whitespace: bool | None = None\n    to_upper: bool | None = None\n    to_lower: bool | None = None\n    strict: bool | None = None\n    min_length: int | None = None\n    max_length: int | None = None\n    pattern: str | Pattern[str] | None = None\n\n    def __iter__(self) -> Iterator[BaseMetadata]:\n        if self.min_length is not None:\n            yield MinLen(self.min_length)\n        if self.max_length is not None:\n            yield MaxLen(self.max_length)\n        if self.strict is not None:\n            yield Strict(self.strict)\n        if (\n            self.strip_whitespace is not None\n            or self.pattern is not None\n            or self.to_lower is not None\n            or self.to_upper is not None\n        ):\n            yield _fields.pydantic_general_metadata(\n                strip_whitespace=self.strip_whitespace,\n                to_upper=self.to_upper,\n                to_lower=self.to_lower,\n                pattern=self.pattern,\n            )\n\n\ndef constr(\n    *,\n    strip_whitespace: bool | None = None,\n    to_upper: bool | None = None,\n    to_lower: bool | None = None,\n    strict: bool | None = None,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    pattern: str | Pattern[str] | None = None,\n) -> type[str]:\n    \"\"\"\n    !!! warning \"Discouraged\"\n        This function is **discouraged** in favor of using\n        [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with\n        [`StringConstraints`][pydantic.types.StringConstraints] instead.\n\n        This function will be **deprecated** in Pydantic 3.0.\n\n        The reason is that `constr` returns a type, which doesn't play well with static analysis tools.\n\n        === \":x: Don't do this\"\n            ```py\n            from pydantic import BaseModel, constr\n\n            class Foo(BaseModel):\n                bar: constr(strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$')\n            ```\n\n        === \":white_check_mark: Do this\"\n            ```py\n            from typing_extensions import Annotated\n\n            from pydantic import BaseModel, StringConstraints\n\n            class Foo(BaseModel):\n                bar: Annotated[str, StringConstraints(strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$')]\n            ```\n\n    A wrapper around `str` that allows for additional constraints.\n\n    ```py\n    from pydantic import BaseModel, constr\n\n    class Foo(BaseModel):\n        bar: constr(strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$')\n\n\n    foo = Foo(bar='  hello  ')\n    print(foo)\n    #> bar='HELLO'\n    ```\n\n    Args:\n        strip_whitespace: Whether to remove leading and trailing whitespace.\n        to_upper: Whether to turn all characters to uppercase.\n        to_lower: Whether to turn all characters to lowercase.\n        strict: Whether to validate the string in strict mode.\n        min_length: The minimum length of the string.\n        max_length: The maximum length of the string.\n        pattern: A regex pattern to validate the string against.\n\n    Returns:\n        The wrapped string type.\n    \"\"\"  # noqa: D212\n    return Annotated[  # pyright: ignore[reportReturnType]\n        str,\n        StringConstraints(\n            strip_whitespace=strip_whitespace,\n            to_upper=to_upper,\n            to_lower=to_lower,\n            strict=strict,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n        ),\n    ]\n\n\nStrictStr = Annotated[str, Strict()]\n\"\"\"A string that must be validated in strict mode.\"\"\"\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~ COLLECTION TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nHashableItemType = TypeVar('HashableItemType', bound=Hashable)\n\n\ndef conset(\n    item_type: type[HashableItemType], *, min_length: int | None = None, max_length: int | None = None\n) -> type[set[HashableItemType]]:\n    \"\"\"A wrapper around `typing.Set` that allows for additional constraints.\n\n    Args:\n        item_type: The type of the items in the set.\n        min_length: The minimum length of the set.\n        max_length: The maximum length of the set.\n\n    Returns:\n        The wrapped set type.\n    \"\"\"\n    return Annotated[Set[item_type], annotated_types.Len(min_length or 0, max_length)]  # pyright: ignore[reportReturnType]\n\n\ndef confrozenset(\n    item_type: type[HashableItemType], *, min_length: int | None = None, max_length: int | None = None\n) -> type[frozenset[HashableItemType]]:\n    \"\"\"A wrapper around `typing.FrozenSet` that allows for additional constraints.\n\n    Args:\n        item_type: The type of the items in the frozenset.\n        min_length: The minimum length of the frozenset.\n        max_length: The maximum length of the frozenset.\n\n    Returns:\n        The wrapped frozenset type.\n    \"\"\"\n    return Annotated[FrozenSet[item_type], annotated_types.Len(min_length or 0, max_length)]  # pyright: ignore[reportReturnType]\n\n\nAnyItemType = TypeVar('AnyItemType')\n\n\ndef conlist(\n    item_type: type[AnyItemType],\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    unique_items: bool | None = None,\n) -> type[list[AnyItemType]]:\n    \"\"\"A wrapper around typing.List that adds validation.\n\n    Args:\n        item_type: The type of the items in the list.\n        min_length: The minimum length of the list. Defaults to None.\n        max_length: The maximum length of the list. Defaults to None.\n        unique_items: Whether the items in the list must be unique. Defaults to None.\n            !!! warning Deprecated\n                The `unique_items` parameter is deprecated, use `Set` instead.\n                See [this issue](https://github.com/pydantic/pydantic-core/issues/296) for more details.\n\n    Returns:\n        The wrapped list type.\n    \"\"\"\n    if unique_items is not None:\n        raise PydanticUserError(\n            (\n                '`unique_items` is removed, use `Set` instead'\n                '(this feature is discussed in https://github.com/pydantic/pydantic-core/issues/296)'\n            ),\n            code='removed-kwargs',\n        )\n    return Annotated[List[item_type], annotated_types.Len(min_length or 0, max_length)]  # pyright: ignore[reportReturnType]\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPORT STRING TYPE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAnyType = TypeVar('AnyType')\nif TYPE_CHECKING:\n    ImportString = Annotated[AnyType, ...]\nelse:\n\n    class ImportString:\n        \"\"\"A type that can be used to import a type from a string.\n\n        `ImportString` expects a string and loads the Python object importable at that dotted path.\n        Attributes of modules may be separated from the module by `:` or `.`, e.g. if `'math:cos'` was provided,\n        the resulting field value would be the function`cos`. If a `.` is used and both an attribute and submodule\n        are present at the same path, the module will be preferred.\n\n        On model instantiation, pointers will be evaluated and imported. There is\n        some nuance to this behavior, demonstrated in the examples below.\n\n        **Good behavior:**\n        ```py\n        from math import cos\n\n        from pydantic import BaseModel, Field, ImportString, ValidationError\n\n\n        class ImportThings(BaseModel):\n            obj: ImportString\n\n\n        # A string value will cause an automatic import\n        my_cos = ImportThings(obj='math.cos')\n\n        # You can use the imported function as you would expect\n        cos_of_0 = my_cos.obj(0)\n        assert cos_of_0 == 1\n\n\n        # A string whose value cannot be imported will raise an error\n        try:\n            ImportThings(obj='foo.bar')\n        except ValidationError as e:\n            print(e)\n            '''\n            1 validation error for ImportThings\n            obj\n            Invalid python path: No module named 'foo.bar' [type=import_error, input_value='foo.bar', input_type=str]\n            '''\n\n\n        # Actual python objects can be assigned as well\n        my_cos = ImportThings(obj=cos)\n        my_cos_2 = ImportThings(obj='math.cos')\n        my_cos_3 = ImportThings(obj='math:cos')\n        assert my_cos == my_cos_2 == my_cos_3\n\n\n        # You can set default field value either as Python object:\n        class ImportThingsDefaultPyObj(BaseModel):\n            obj: ImportString = math.cos\n\n\n        # or as a string value (but only if used with `validate_default=True`)\n        class ImportThingsDefaultString(BaseModel):\n            obj: ImportString = Field(default='math.cos', validate_default=True)\n\n\n        my_cos_default1 = ImportThingsDefaultPyObj()\n        my_cos_default2 = ImportThingsDefaultString()\n        assert my_cos_default1.obj == my_cos_default2.obj == math.cos\n\n\n        # note: this will not work!\n        class ImportThingsMissingValidateDefault(BaseModel):\n            obj: ImportString = 'math.cos'\n\n        my_cos_default3 = ImportThingsMissingValidateDefault()\n        assert my_cos_default3.obj == 'math.cos'  # just string, not evaluated\n        ```\n\n        Serializing an `ImportString` type to json is also possible.\n\n        ```py\n        from pydantic import BaseModel, ImportString\n\n\n        class ImportThings(BaseModel):\n            obj: ImportString\n\n\n        # Create an instance\n        m = ImportThings(obj='math.cos')\n        print(m)\n        #> obj=<built-in function cos>\n        print(m.model_dump_json())\n        #> {\"obj\":\"math.cos\"}\n        ```\n        \"\"\"\n\n        @classmethod\n        def __class_getitem__(cls, item: AnyType) -> AnyType:\n            return Annotated[item, cls()]\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source: type[Any], handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            serializer = core_schema.plain_serializer_function_ser_schema(cls._serialize, when_used='json')\n            if cls is source:\n                # Treat bare usage of ImportString (`schema is None`) as the same as ImportString[Any]\n                return core_schema.no_info_plain_validator_function(\n                    function=_validators.import_string, serialization=serializer\n                )\n            else:\n                return core_schema.no_info_before_validator_function(\n                    function=_validators.import_string, schema=handler(source), serialization=serializer\n                )\n\n        @classmethod\n        def __get_pydantic_json_schema__(cls, cs: CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            return handler(core_schema.str_schema())\n\n        @staticmethod\n        def _serialize(v: Any) -> str:\n            if isinstance(v, ModuleType):\n                return v.__name__\n            elif hasattr(v, '__module__') and hasattr(v, '__name__'):\n                return f'{v.__module__}.{v.__name__}'\n            else:\n                return v\n\n        def __repr__(self) -> str:\n            return 'ImportString'\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DECIMAL TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\ndef condecimal(\n    *,\n    strict: bool | None = None,\n    gt: int | Decimal | None = None,\n    ge: int | Decimal | None = None,\n    lt: int | Decimal | None = None,\n    le: int | Decimal | None = None,\n    multiple_of: int | Decimal | None = None,\n    max_digits: int | None = None,\n    decimal_places: int | None = None,\n    allow_inf_nan: bool | None = None,\n) -> type[Decimal]:\n    \"\"\"\n    !!! warning \"Discouraged\"\n        This function is **discouraged** in favor of using\n        [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with\n        [`Field`][pydantic.fields.Field] instead.\n\n        This function will be **deprecated** in Pydantic 3.0.\n\n        The reason is that `condecimal` returns a type, which doesn't play well with static analysis tools.\n\n        === \":x: Don't do this\"\n            ```py\n            from pydantic import BaseModel, condecimal\n\n            class Foo(BaseModel):\n                bar: condecimal(strict=True, allow_inf_nan=True)\n            ```\n\n        === \":white_check_mark: Do this\"\n            ```py\n            from decimal import Decimal\n\n            from typing_extensions import Annotated\n\n            from pydantic import BaseModel, Field\n\n            class Foo(BaseModel):\n                bar: Annotated[Decimal, Field(strict=True, allow_inf_nan=True)]\n            ```\n\n    A wrapper around Decimal that adds validation.\n\n    Args:\n        strict: Whether to validate the value in strict mode. Defaults to `None`.\n        gt: The value must be greater than this. Defaults to `None`.\n        ge: The value must be greater than or equal to this. Defaults to `None`.\n        lt: The value must be less than this. Defaults to `None`.\n        le: The value must be less than or equal to this. Defaults to `None`.\n        multiple_of: The value must be a multiple of this. Defaults to `None`.\n        max_digits: The maximum number of digits. Defaults to `None`.\n        decimal_places: The number of decimal places. Defaults to `None`.\n        allow_inf_nan: Whether to allow infinity and NaN. Defaults to `None`.\n\n    ```py\n    from decimal import Decimal\n\n    from pydantic import BaseModel, ValidationError, condecimal\n\n    class ConstrainedExample(BaseModel):\n        constrained_decimal: condecimal(gt=Decimal('1.0'))\n\n    m = ConstrainedExample(constrained_decimal=Decimal('1.1'))\n    print(repr(m))\n    #> ConstrainedExample(constrained_decimal=Decimal('1.1'))\n\n    try:\n        ConstrainedExample(constrained_decimal=Decimal('0.9'))\n    except ValidationError as e:\n        print(e.errors())\n        '''\n        [\n            {\n                'type': 'greater_than',\n                'loc': ('constrained_decimal',),\n                'msg': 'Input should be greater than 1.0',\n                'input': Decimal('0.9'),\n                'ctx': {'gt': Decimal('1.0')},\n                'url': 'https://errors.pydantic.dev/2/v/greater_than',\n            }\n        ]\n        '''\n    ```\n    \"\"\"  # noqa: D212\n    return Annotated[  # pyright: ignore[reportReturnType]\n        Decimal,\n        Strict(strict) if strict is not None else None,\n        annotated_types.Interval(gt=gt, ge=ge, lt=lt, le=le),\n        annotated_types.MultipleOf(multiple_of) if multiple_of is not None else None,\n        _fields.pydantic_general_metadata(max_digits=max_digits, decimal_places=decimal_places),\n        AllowInfNan(allow_inf_nan) if allow_inf_nan is not None else None,\n    ]\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ UUID TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n@_dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass UuidVersion:\n    \"\"\"A field metadata class to indicate a [UUID](https://docs.python.org/3/library/uuid.html) version.\"\"\"\n\n    uuid_version: Literal[1, 3, 4, 5]\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = handler(core_schema)\n        field_schema.pop('anyOf', None)  # remove the bytes/str union\n        field_schema.update(type='string', format=f'uuid{self.uuid_version}')\n        return field_schema\n\n    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        if isinstance(self, source):\n            # used directly as a type\n            return core_schema.uuid_schema(version=self.uuid_version)\n        else:\n            # update existing schema with self.uuid_version\n            schema = handler(source)\n            _check_annotated_type(schema['type'], 'uuid', self.__class__.__name__)\n            schema['version'] = self.uuid_version  # type: ignore\n            return schema\n\n    def __hash__(self) -> int:\n        return hash(type(self.uuid_version))\n\n\nUUID1 = Annotated[UUID, UuidVersion(1)]\n\"\"\"A [UUID](https://docs.python.org/3/library/uuid.html) that must be version 1.\n\n```py\nimport uuid\n\nfrom pydantic import UUID1, BaseModel\n\nclass Model(BaseModel):\n    uuid1: UUID1\n\nModel(uuid1=uuid.uuid1())\n```\n\"\"\"\nUUID3 = Annotated[UUID, UuidVersion(3)]\n\"\"\"A [UUID](https://docs.python.org/3/library/uuid.html) that must be version 3.\n\n```py\nimport uuid\n\nfrom pydantic import UUID3, BaseModel\n\nclass Model(BaseModel):\n    uuid3: UUID3\n\nModel(uuid3=uuid.uuid3(uuid.NAMESPACE_DNS, 'pydantic.org'))\n```\n\"\"\"\nUUID4 = Annotated[UUID, UuidVersion(4)]\n\"\"\"A [UUID](https://docs.python.org/3/library/uuid.html) that must be version 4.\n\n```py\nimport uuid\n\nfrom pydantic import UUID4, BaseModel\n\nclass Model(BaseModel):\n    uuid4: UUID4\n\nModel(uuid4=uuid.uuid4())\n```\n\"\"\"\nUUID5 = Annotated[UUID, UuidVersion(5)]\n\"\"\"A [UUID](https://docs.python.org/3/library/uuid.html) that must be version 5.\n\n```py\nimport uuid\n\nfrom pydantic import UUID5, BaseModel\n\nclass Model(BaseModel):\n    uuid5: UUID5\n\nModel(uuid5=uuid.uuid5(uuid.NAMESPACE_DNS, 'pydantic.org'))\n```\n\"\"\"\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PATH TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n@_dataclasses.dataclass\nclass PathType:\n    path_type: Literal['file', 'dir', 'new']\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = handler(core_schema)\n        format_conversion = {'file': 'file-path', 'dir': 'directory-path'}\n        field_schema.update(format=format_conversion.get(self.path_type, 'path'), type='string')\n        return field_schema\n\n    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        function_lookup = {\n            'file': cast(core_schema.WithInfoValidatorFunction, self.validate_file),\n            'dir': cast(core_schema.WithInfoValidatorFunction, self.validate_directory),\n            'new': cast(core_schema.WithInfoValidatorFunction, self.validate_new),\n        }\n\n        return core_schema.with_info_after_validator_function(\n            function_lookup[self.path_type],\n            handler(source),\n        )\n\n    @staticmethod\n    def validate_file(path: Path, _: core_schema.ValidationInfo) -> Path:\n        if path.is_file():\n            return path\n        else:\n            raise PydanticCustomError('path_not_file', 'Path does not point to a file')\n\n    @staticmethod\n    def validate_directory(path: Path, _: core_schema.ValidationInfo) -> Path:\n        if path.is_dir():\n            return path\n        else:\n            raise PydanticCustomError('path_not_directory', 'Path does not point to a directory')\n\n    @staticmethod\n    def validate_new(path: Path, _: core_schema.ValidationInfo) -> Path:\n        if path.exists():\n            raise PydanticCustomError('path_exists', 'Path already exists')\n        elif not path.parent.exists():\n            raise PydanticCustomError('parent_does_not_exist', 'Parent directory does not exist')\n        else:\n            return path\n\n    def __hash__(self) -> int:\n        return hash(type(self.path_type))\n\n\nFilePath = Annotated[Path, PathType('file')]\n\"\"\"A path that must point to a file.\n\n```py\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, FilePath, ValidationError\n\nclass Model(BaseModel):\n    f: FilePath\n\npath = Path('text.txt')\npath.touch()\nm = Model(f='text.txt')\nprint(m.model_dump())\n#> {'f': PosixPath('text.txt')}\npath.unlink()\n\npath = Path('directory')\npath.mkdir(exist_ok=True)\ntry:\n    Model(f='directory')  # directory\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a file [type=path_not_file, input_value='directory', input_type=str]\n    '''\npath.rmdir()\n\ntry:\n    Model(f='not-exists-file')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a file [type=path_not_file, input_value='not-exists-file', input_type=str]\n    '''\n```\n\"\"\"\nDirectoryPath = Annotated[Path, PathType('dir')]\n\"\"\"A path that must point to a directory.\n\n```py\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, DirectoryPath, ValidationError\n\nclass Model(BaseModel):\n    f: DirectoryPath\n\npath = Path('directory/')\npath.mkdir()\nm = Model(f='directory/')\nprint(m.model_dump())\n#> {'f': PosixPath('directory')}\npath.rmdir()\n\npath = Path('file.txt')\npath.touch()\ntry:\n    Model(f='file.txt')  # file\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a directory [type=path_not_directory, input_value='file.txt', input_type=str]\n    '''\npath.unlink()\n\ntry:\n    Model(f='not-exists-directory')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a directory [type=path_not_directory, input_value='not-exists-directory', input_type=str]\n    '''\n```\n\"\"\"\nNewPath = Annotated[Path, PathType('new')]\n\"\"\"A path for a new file or directory that must not already exist. The parent directory must already exist.\"\"\"\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ JSON TYPE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nif TYPE_CHECKING:\n    # Json[list[str]] will be recognized by type checkers as list[str]\n    Json = Annotated[AnyType, ...]\n\nelse:\n\n    class Json:\n        \"\"\"A special type wrapper which loads JSON before parsing.\n\n        You can use the `Json` data type to make Pydantic first load a raw JSON string before\n        validating the loaded data into the parametrized type:\n\n        ```py\n        from typing import Any, List\n\n        from pydantic import BaseModel, Json, ValidationError\n\n\n        class AnyJsonModel(BaseModel):\n            json_obj: Json[Any]\n\n\n        class ConstrainedJsonModel(BaseModel):\n            json_obj: Json[List[int]]\n\n\n        print(AnyJsonModel(json_obj='{\"b\": 1}'))\n        #> json_obj={'b': 1}\n        print(ConstrainedJsonModel(json_obj='[1, 2, 3]'))\n        #> json_obj=[1, 2, 3]\n\n        try:\n            ConstrainedJsonModel(json_obj=12)\n        except ValidationError as e:\n            print(e)\n            '''\n            1 validation error for ConstrainedJsonModel\n            json_obj\n            JSON input should be string, bytes or bytearray [type=json_type, input_value=12, input_type=int]\n            '''\n\n        try:\n            ConstrainedJsonModel(json_obj='[a, b]')\n        except ValidationError as e:\n            print(e)\n            '''\n            1 validation error for ConstrainedJsonModel\n            json_obj\n            Invalid JSON: expected value at line 1 column 2 [type=json_invalid, input_value='[a, b]', input_type=str]\n            '''\n\n        try:\n            ConstrainedJsonModel(json_obj='[\"a\", \"b\"]')\n        except ValidationError as e:\n            print(e)\n            '''\n            2 validation errors for ConstrainedJsonModel\n            json_obj.0\n            Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n            json_obj.1\n            Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='b', input_type=str]\n            '''\n        ```\n\n        When you dump the model using `model_dump` or `model_dump_json`, the dumped value will be the result of validation,\n        not the original JSON string. However, you can use the argument `round_trip=True` to get the original JSON string back:\n\n        ```py\n        from typing import List\n\n        from pydantic import BaseModel, Json\n\n\n        class ConstrainedJsonModel(BaseModel):\n            json_obj: Json[List[int]]\n\n\n        print(ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json())\n        #> {\"json_obj\":[1,2,3]}\n        print(\n            ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json(round_trip=True)\n        )\n        #> {\"json_obj\":\"[1,2,3]\"}\n        ```\n        \"\"\"\n\n        @classmethod\n        def __class_getitem__(cls, item: AnyType) -> AnyType:\n            return Annotated[item, cls()]\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            if cls is source:\n                return core_schema.json_schema(None)\n            else:\n                return core_schema.json_schema(handler(source))\n\n        def __repr__(self) -> str:\n            return 'Json'\n\n        def __hash__(self) -> int:\n            return hash(type(self))\n\n        def __eq__(self, other: Any) -> bool:\n            return type(other) == type(self)\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SECRET TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSecretType = TypeVar('SecretType')\n\n\nclass _SecretBase(Generic[SecretType]):\n    def __init__(self, secret_value: SecretType) -> None:\n        self._secret_value: SecretType = secret_value\n\n    def get_secret_value(self) -> SecretType:\n        \"\"\"Get the secret value.\n\n        Returns:\n            The secret value.\n        \"\"\"\n        return self._secret_value\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, self.__class__) and self.get_secret_value() == other.get_secret_value()\n\n    def __hash__(self) -> int:\n        return hash(self.get_secret_value())\n\n    def __str__(self) -> str:\n        return str(self._display())\n\n    def __repr__(self) -> str:\n        return f'{self.__class__.__name__}({self._display()!r})'\n\n    def _display(self) -> str | bytes:\n        raise NotImplementedError\n\n\nclass Secret(_SecretBase[SecretType]):\n    \"\"\"A generic base class used for defining a field with sensitive information that you do not want to be visible in logging or tracebacks.\n\n    You may either directly parametrize `Secret` with a type, or subclass from `Secret` with a parametrized type. The benefit of subclassing\n    is that you can define a custom `_display` method, which will be used for `repr()` and `str()` methods. The examples below demonstrate both\n    ways of using `Secret` to create a new secret type.\n\n    1. Directly parametrizing `Secret` with a type:\n\n    ```py\n    from pydantic import BaseModel, Secret\n\n    SecretBool = Secret[bool]\n\n    class Model(BaseModel):\n        secret_bool: SecretBool\n\n    m = Model(secret_bool=True)\n    print(m.model_dump())\n    #> {'secret_bool': Secret('**********')}\n\n    print(m.model_dump_json())\n    #> {\"secret_bool\":\"**********\"}\n\n    print(m.secret_bool.get_secret_value())\n    #> True\n    ```\n\n    2. Subclassing from parametrized `Secret`:\n\n    ```py\n    from datetime import date\n\n    from pydantic import BaseModel, Secret\n\n    class SecretDate(Secret[date]):\n        def _display(self) -> str:\n            return '****/**/**'\n\n    class Model(BaseModel):\n        secret_date: SecretDate\n\n    m = Model(secret_date=date(2022, 1, 1))\n    print(m.model_dump())\n    #> {'secret_date': SecretDate('****/**/**')}\n\n    print(m.model_dump_json())\n    #> {\"secret_date\":\"****/**/**\"}\n\n    print(m.secret_date.get_secret_value())\n    #> 2022-01-01\n    ```\n\n    The value returned by the `_display` method will be used for `repr()` and `str()`.\n    \"\"\"\n\n    def _display(self) -> str | bytes:\n        return '**********' if self.get_secret_value() else ''\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        inner_type = None\n        # if origin_type is Secret, then cls is a GenericAlias, and we can extract the inner type directly\n        origin_type = get_origin(source)\n        if origin_type is not None:\n            inner_type = get_args(source)[0]\n        # otherwise, we need to get the inner type from the base class\n        else:\n            bases = getattr(cls, '__orig_bases__', getattr(cls, '__bases__', []))\n            for base in bases:\n                if get_origin(base) is Secret:\n                    inner_type = get_args(base)[0]\n            if bases == [] or inner_type is None:\n                raise TypeError(\n                    f\"Can't get secret type from {cls.__name__}. \"\n                    'Please use Secret[<type>], or subclass from Secret[<type>] instead.'\n                )\n\n        inner_schema = handler.generate_schema(inner_type)  # type: ignore\n\n        def validate_secret_value(value, handler) -> Secret[SecretType]:\n            if isinstance(value, Secret):\n                value = value.get_secret_value()\n            validated_inner = handler(value)\n            return cls(validated_inner)\n\n        def serialize(value: Secret[SecretType], info: core_schema.SerializationInfo) -> str | Secret[SecretType]:\n            if info.mode == 'json':\n                return str(value)\n            else:\n                return value\n\n        return core_schema.json_or_python_schema(\n            python_schema=core_schema.no_info_wrap_validator_function(\n                validate_secret_value,\n                inner_schema,\n            ),\n            json_schema=core_schema.no_info_after_validator_function(lambda x: cls(x), inner_schema),\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                serialize,\n                info_arg=True,\n                when_used='always',\n            ),\n        )\n\n\ndef _secret_display(value: SecretType) -> str:  # type: ignore\n    return '**********' if value else ''\n\n\nclass _SecretField(_SecretBase[SecretType]):\n    _inner_schema: ClassVar[CoreSchema]\n    _error_kind: ClassVar[str]\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        def serialize(\n            value: _SecretField[SecretType], info: core_schema.SerializationInfo\n        ) -> str | _SecretField[SecretType]:\n            if info.mode == 'json':\n                # we want the output to always be string without the `b'` prefix for bytes,\n                # hence we just use `secret_display`\n                return _secret_display(value.get_secret_value())\n            else:\n                return value\n\n        def get_json_schema(_core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            json_schema = handler(cls._inner_schema)\n            _utils.update_not_none(\n                json_schema,\n                type='string',\n                writeOnly=True,\n                format='password',\n            )\n            return json_schema\n\n        json_schema = core_schema.no_info_after_validator_function(\n            source,  # construct the type\n            cls._inner_schema,\n        )\n\n        def get_secret_schema(strict: bool) -> CoreSchema:\n            return core_schema.json_or_python_schema(\n                python_schema=core_schema.union_schema(\n                    [\n                        core_schema.is_instance_schema(source),\n                        json_schema,\n                    ],\n                    custom_error_type=cls._error_kind,\n                    strict=strict,\n                ),\n                json_schema=json_schema,\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    serialize,\n                    info_arg=True,\n                    return_schema=core_schema.str_schema(),\n                    when_used='json',\n                ),\n            )\n\n        return core_schema.lax_or_strict_schema(\n            lax_schema=get_secret_schema(strict=False),\n            strict_schema=get_secret_schema(strict=True),\n            metadata={'pydantic_js_functions': [get_json_schema]},\n        )\n\n\nclass SecretStr(_SecretField[str]):\n    \"\"\"A string used for storing sensitive information that you do not want to be visible in logging or tracebacks.\n\n    When the secret value is nonempty, it is displayed as `'**********'` instead of the underlying value in\n    calls to `repr()` and `str()`. If the value _is_ empty, it is displayed as `''`.\n\n    ```py\n    from pydantic import BaseModel, SecretStr\n\n    class User(BaseModel):\n        username: str\n        password: SecretStr\n\n    user = User(username='scolvin', password='password1')\n\n    print(user)\n    #> username='scolvin' password=SecretStr('**********')\n    print(user.password.get_secret_value())\n    #> password1\n    print((SecretStr('password'), SecretStr('')))\n    #> (SecretStr('**********'), SecretStr(''))\n    ```\n    \"\"\"\n\n    _inner_schema: ClassVar[CoreSchema] = core_schema.str_schema()\n    _error_kind: ClassVar[str] = 'string_type'\n\n    def __len__(self) -> int:\n        return len(self._secret_value)\n\n    def _display(self) -> str:\n        return _secret_display(self._secret_value)\n\n\nclass SecretBytes(_SecretField[bytes]):\n    \"\"\"A bytes used for storing sensitive information that you do not want to be visible in logging or tracebacks.\n\n    It displays `b'**********'` instead of the string value on `repr()` and `str()` calls.\n    When the secret value is nonempty, it is displayed as `b'**********'` instead of the underlying value in\n    calls to `repr()` and `str()`. If the value _is_ empty, it is displayed as `b''`.\n\n    ```py\n    from pydantic import BaseModel, SecretBytes\n\n    class User(BaseModel):\n        username: str\n        password: SecretBytes\n\n    user = User(username='scolvin', password=b'password1')\n    #> username='scolvin' password=SecretBytes(b'**********')\n    print(user.password.get_secret_value())\n    #> b'password1'\n    print((SecretBytes(b'password'), SecretBytes(b'')))\n    #> (SecretBytes(b'**********'), SecretBytes(b''))\n    ```\n    \"\"\"\n\n    _inner_schema: ClassVar[CoreSchema] = core_schema.bytes_schema()\n    _error_kind: ClassVar[str] = 'bytes_type'\n\n    def __len__(self) -> int:\n        return len(self._secret_value)\n\n    def _display(self) -> bytes:\n        return _secret_display(self._secret_value).encode()\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PAYMENT CARD TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass PaymentCardBrand(str, Enum):\n    amex = 'American Express'\n    mastercard = 'Mastercard'\n    visa = 'Visa'\n    other = 'other'\n\n    def __str__(self) -> str:\n        return self.value\n\n\n@deprecated(\n    'The `PaymentCardNumber` class is deprecated, use `pydantic_extra_types` instead. '\n    'See https://docs.pydantic.dev/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.',\n    category=PydanticDeprecatedSince20,\n)\nclass PaymentCardNumber(str):\n    \"\"\"Based on: https://en.wikipedia.org/wiki/Payment_card_number.\"\"\"\n\n    strip_whitespace: ClassVar[bool] = True\n    min_length: ClassVar[int] = 12\n    max_length: ClassVar[int] = 19\n    bin: str\n    last4: str\n    brand: PaymentCardBrand\n\n    def __init__(self, card_number: str):\n        self.validate_digits(card_number)\n\n        card_number = self.validate_luhn_check_digit(card_number)\n\n        self.bin = card_number[:6]\n        self.last4 = card_number[-4:]\n        self.brand = self.validate_brand(card_number)\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        return core_schema.with_info_after_validator_function(\n            cls.validate,\n            core_schema.str_schema(\n                min_length=cls.min_length, max_length=cls.max_length, strip_whitespace=cls.strip_whitespace\n            ),\n        )\n\n    @classmethod\n    def validate(cls, input_value: str, /, _: core_schema.ValidationInfo) -> PaymentCardNumber:\n        \"\"\"Validate the card number and return a `PaymentCardNumber` instance.\"\"\"\n        return cls(input_value)\n\n    @property\n    def masked(self) -> str:\n        \"\"\"Mask all but the last 4 digits of the card number.\n\n        Returns:\n            A masked card number string.\n        \"\"\"\n        num_masked = len(self) - 10  # len(bin) + len(last4) == 10\n        return f'{self.bin}{\"*\" * num_masked}{self.last4}'\n\n    @classmethod\n    def validate_digits(cls, card_number: str) -> None:\n        \"\"\"Validate that the card number is all digits.\"\"\"\n        if not card_number.isdigit():\n            raise PydanticCustomError('payment_card_number_digits', 'Card number is not all digits')\n\n    @classmethod\n    def validate_luhn_check_digit(cls, card_number: str) -> str:\n        \"\"\"Based on: https://en.wikipedia.org/wiki/Luhn_algorithm.\"\"\"\n        sum_ = int(card_number[-1])\n        length = len(card_number)\n        parity = length % 2\n        for i in range(length - 1):\n            digit = int(card_number[i])\n            if i % 2 == parity:\n                digit *= 2\n            if digit > 9:\n                digit -= 9\n            sum_ += digit\n        valid = sum_ % 10 == 0\n        if not valid:\n            raise PydanticCustomError('payment_card_number_luhn', 'Card number is not luhn valid')\n        return card_number\n\n    @staticmethod\n    def validate_brand(card_number: str) -> PaymentCardBrand:\n        \"\"\"Validate length based on BIN for major brands:\n        https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN).\n        \"\"\"\n        if card_number[0] == '4':\n            brand = PaymentCardBrand.visa\n        elif 51 <= int(card_number[:2]) <= 55:\n            brand = PaymentCardBrand.mastercard\n        elif card_number[:2] in {'34', '37'}:\n            brand = PaymentCardBrand.amex\n        else:\n            brand = PaymentCardBrand.other\n\n        required_length: None | int | str = None\n        if brand in PaymentCardBrand.mastercard:\n            required_length = 16\n            valid = len(card_number) == required_length\n        elif brand == PaymentCardBrand.visa:\n            required_length = '13, 16 or 19'\n            valid = len(card_number) in {13, 16, 19}\n        elif brand == PaymentCardBrand.amex:\n            required_length = 15\n            valid = len(card_number) == required_length\n        else:\n            valid = True\n\n        if not valid:\n            raise PydanticCustomError(\n                'payment_card_number_brand',\n                'Length for a {brand} card must be {required_length}',\n                {'brand': brand, 'required_length': required_length},\n            )\n        return brand\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BYTE SIZE TYPE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass ByteSize(int):\n    \"\"\"Converts a string representing a number of bytes with units (such as `'1KB'` or `'11.5MiB'`) into an integer.\n\n    You can use the `ByteSize` data type to (case-insensitively) convert a string representation of a number of bytes into\n    an integer, and also to print out human-readable strings representing a number of bytes.\n\n    In conformance with [IEC 80000-13 Standard](https://en.wikipedia.org/wiki/ISO/IEC_80000) we interpret `'1KB'` to mean 1000 bytes,\n    and `'1KiB'` to mean 1024 bytes. In general, including a middle `'i'` will cause the unit to be interpreted as a power of 2,\n    rather than a power of 10 (so, for example, `'1 MB'` is treated as `1_000_000` bytes, whereas `'1 MiB'` is treated as `1_048_576` bytes).\n\n    !!! info\n        Note that `1b` will be parsed as \"1 byte\" and not \"1 bit\".\n\n    ```py\n    from pydantic import BaseModel, ByteSize\n\n    class MyModel(BaseModel):\n        size: ByteSize\n\n    print(MyModel(size=52000).size)\n    #> 52000\n    print(MyModel(size='3000 KiB').size)\n    #> 3072000\n\n    m = MyModel(size='50 PB')\n    print(m.size.human_readable())\n    #> 44.4PiB\n    print(m.size.human_readable(decimal=True))\n    #> 50.0PB\n    print(m.size.human_readable(separator=' '))\n    #> 44.4 PiB\n\n    print(m.size.to('TiB'))\n    #> 45474.73508864641\n    ```\n    \"\"\"\n\n    byte_sizes = {\n        'b': 1,\n        'kb': 10**3,\n        'mb': 10**6,\n        'gb': 10**9,\n        'tb': 10**12,\n        'pb': 10**15,\n        'eb': 10**18,\n        'kib': 2**10,\n        'mib': 2**20,\n        'gib': 2**30,\n        'tib': 2**40,\n        'pib': 2**50,\n        'eib': 2**60,\n        'bit': 1 / 8,\n        'kbit': 10**3 / 8,\n        'mbit': 10**6 / 8,\n        'gbit': 10**9 / 8,\n        'tbit': 10**12 / 8,\n        'pbit': 10**15 / 8,\n        'ebit': 10**18 / 8,\n        'kibit': 2**10 / 8,\n        'mibit': 2**20 / 8,\n        'gibit': 2**30 / 8,\n        'tibit': 2**40 / 8,\n        'pibit': 2**50 / 8,\n        'eibit': 2**60 / 8,\n    }\n    byte_sizes.update({k.lower()[0]: v for k, v in byte_sizes.items() if 'i' not in k})\n\n    byte_string_pattern = r'^\\s*(\\d*\\.?\\d+)\\s*(\\w+)?'\n    byte_string_re = re.compile(byte_string_pattern, re.IGNORECASE)\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        return core_schema.with_info_after_validator_function(\n            function=cls._validate,\n            schema=core_schema.union_schema(\n                [\n                    core_schema.str_schema(pattern=cls.byte_string_pattern),\n                    core_schema.int_schema(ge=0),\n                ],\n                custom_error_type='byte_size',\n                custom_error_message='could not parse value and unit from byte string',\n            ),\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                int, return_schema=core_schema.int_schema(ge=0)\n            ),\n        )\n\n    @classmethod\n    def _validate(cls, input_value: Any, /, _: core_schema.ValidationInfo) -> ByteSize:\n        try:\n            return cls(int(input_value))\n        except ValueError:\n            pass\n\n        str_match = cls.byte_string_re.match(str(input_value))\n        if str_match is None:\n            raise PydanticCustomError('byte_size', 'could not parse value and unit from byte string')\n\n        scalar, unit = str_match.groups()\n        if unit is None:\n            unit = 'b'\n\n        try:\n            unit_mult = cls.byte_sizes[unit.lower()]\n        except KeyError:\n            raise PydanticCustomError('byte_size_unit', 'could not interpret byte unit: {unit}', {'unit': unit})\n\n        return cls(int(float(scalar) * unit_mult))\n\n    def human_readable(self, decimal: bool = False, separator: str = '') -> str:\n        \"\"\"Converts a byte size to a human readable string.\n\n        Args:\n            decimal: If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units\n                (e.g. 1024 bytes per KiB).\n            separator: A string used to split the value and unit. Defaults to an empty string ('').\n\n        Returns:\n            A human readable string representation of the byte size.\n        \"\"\"\n        if decimal:\n            divisor = 1000\n            units = 'B', 'KB', 'MB', 'GB', 'TB', 'PB'\n            final_unit = 'EB'\n        else:\n            divisor = 1024\n            units = 'B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB'\n            final_unit = 'EiB'\n\n        num = float(self)\n        for unit in units:\n            if abs(num) < divisor:\n                if unit == 'B':\n                    return f'{num:0.0f}{separator}{unit}'\n                else:\n                    return f'{num:0.1f}{separator}{unit}'\n            num /= divisor\n\n        return f'{num:0.1f}{separator}{final_unit}'\n\n    def to(self, unit: str) -> float:\n        \"\"\"Converts a byte size to another unit, including both byte and bit units.\n\n        Args:\n            unit: The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,\n                KiB, MiB, GiB, TiB, PiB, EiB (byte units) and\n                bit, kbit, mbit, gbit, tbit, pbit, ebit,\n                kibit, mibit, gibit, tibit, pibit, eibit (bit units).\n\n        Returns:\n            The byte size in the new unit.\n        \"\"\"\n        try:\n            unit_div = self.byte_sizes[unit.lower()]\n        except KeyError:\n            raise PydanticCustomError('byte_size_unit', 'Could not interpret byte unit: {unit}', {'unit': unit})\n\n        return self / unit_div\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DATE TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\ndef _check_annotated_type(annotated_type: str, expected_type: str, annotation: str) -> None:\n    if annotated_type != expected_type:\n        raise PydanticUserError(f\"'{annotation}' cannot annotate '{annotated_type}'.\", code='invalid_annotated_type')\n\n\nif TYPE_CHECKING:\n    PastDate = Annotated[date, ...]\n    FutureDate = Annotated[date, ...]\nelse:\n\n    class PastDate:\n        \"\"\"A date in the past.\"\"\"\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source: type[Any], handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            if cls is source:\n                # used directly as a type\n                return core_schema.date_schema(now_op='past')\n            else:\n                schema = handler(source)\n                _check_annotated_type(schema['type'], 'date', cls.__name__)\n                schema['now_op'] = 'past'\n                return schema\n\n        def __repr__(self) -> str:\n            return 'PastDate'\n\n    class FutureDate:\n        \"\"\"A date in the future.\"\"\"\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source: type[Any], handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            if cls is source:\n                # used directly as a type\n                return core_schema.date_schema(now_op='future')\n            else:\n                schema = handler(source)\n                _check_annotated_type(schema['type'], 'date', cls.__name__)\n                schema['now_op'] = 'future'\n                return schema\n\n        def __repr__(self) -> str:\n            return 'FutureDate'\n\n\ndef condate(\n    *,\n    strict: bool | None = None,\n    gt: date | None = None,\n    ge: date | None = None,\n    lt: date | None = None,\n    le: date | None = None,\n) -> type[date]:\n    \"\"\"A wrapper for date that adds constraints.\n\n    Args:\n        strict: Whether to validate the date value in strict mode. Defaults to `None`.\n        gt: The value must be greater than this. Defaults to `None`.\n        ge: The value must be greater than or equal to this. Defaults to `None`.\n        lt: The value must be less than this. Defaults to `None`.\n        le: The value must be less than or equal to this. Defaults to `None`.\n\n    Returns:\n        A date type with the specified constraints.\n    \"\"\"\n    return Annotated[  # pyright: ignore[reportReturnType]\n        date,\n        Strict(strict) if strict is not None else None,\n        annotated_types.Interval(gt=gt, ge=ge, lt=lt, le=le),\n    ]\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DATETIME TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nif TYPE_CHECKING:\n    AwareDatetime = Annotated[datetime, ...]\n    NaiveDatetime = Annotated[datetime, ...]\n    PastDatetime = Annotated[datetime, ...]\n    FutureDatetime = Annotated[datetime, ...]\n\nelse:\n\n    class AwareDatetime:\n        \"\"\"A datetime that requires timezone info.\"\"\"\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source: type[Any], handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            if cls is source:\n                # used directly as a type\n                return core_schema.datetime_schema(tz_constraint='aware')\n            else:\n                schema = handler(source)\n                _check_annotated_type(schema['type'], 'datetime', cls.__name__)\n                schema['tz_constraint'] = 'aware'\n                return schema\n\n        def __repr__(self) -> str:\n            return 'AwareDatetime'\n\n    class NaiveDatetime:\n        \"\"\"A datetime that doesn't require timezone info.\"\"\"\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source: type[Any], handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            if cls is source:\n                # used directly as a type\n                return core_schema.datetime_schema(tz_constraint='naive')\n            else:\n                schema = handler(source)\n                _check_annotated_type(schema['type'], 'datetime', cls.__name__)\n                schema['tz_constraint'] = 'naive'\n                return schema\n\n        def __repr__(self) -> str:\n            return 'NaiveDatetime'\n\n    class PastDatetime:\n        \"\"\"A datetime that must be in the past.\"\"\"\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source: type[Any], handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            if cls is source:\n                # used directly as a type\n                return core_schema.datetime_schema(now_op='past')\n            else:\n                schema = handler(source)\n                _check_annotated_type(schema['type'], 'datetime', cls.__name__)\n                schema['now_op'] = 'past'\n                return schema\n\n        def __repr__(self) -> str:\n            return 'PastDatetime'\n\n    class FutureDatetime:\n        \"\"\"A datetime that must be in the future.\"\"\"\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source: type[Any], handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            if cls is source:\n                # used directly as a type\n                return core_schema.datetime_schema(now_op='future')\n            else:\n                schema = handler(source)\n                _check_annotated_type(schema['type'], 'datetime', cls.__name__)\n                schema['now_op'] = 'future'\n                return schema\n\n        def __repr__(self) -> str:\n            return 'FutureDatetime'\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Encoded TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass EncoderProtocol(Protocol):\n    \"\"\"Protocol for encoding and decoding data to and from bytes.\"\"\"\n\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        \"\"\"Decode the data using the encoder.\n\n        Args:\n            data: The data to decode.\n\n        Returns:\n            The decoded data.\n        \"\"\"\n        ...\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        \"\"\"Encode the data using the encoder.\n\n        Args:\n            value: The data to encode.\n\n        Returns:\n            The encoded data.\n        \"\"\"\n        ...\n\n    @classmethod\n    def get_json_format(cls) -> str:\n        \"\"\"Get the JSON format for the encoded data.\n\n        Returns:\n            The JSON format for the encoded data.\n        \"\"\"\n        ...\n\n\nclass Base64Encoder(EncoderProtocol):\n    \"\"\"Standard (non-URL-safe) Base64 encoder.\"\"\"\n\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        \"\"\"Decode the data from base64 encoded bytes to original bytes data.\n\n        Args:\n            data: The data to decode.\n\n        Returns:\n            The decoded data.\n        \"\"\"\n        try:\n            return base64.decodebytes(data)\n        except ValueError as e:\n            raise PydanticCustomError('base64_decode', \"Base64 decoding error: '{error}'\", {'error': str(e)})\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        \"\"\"Encode the data from bytes to a base64 encoded bytes.\n\n        Args:\n            value: The data to encode.\n\n        Returns:\n            The encoded data.\n        \"\"\"\n        return base64.encodebytes(value)\n\n    @classmethod\n    def get_json_format(cls) -> Literal['base64']:\n        \"\"\"Get the JSON format for the encoded data.\n\n        Returns:\n            The JSON format for the encoded data.\n        \"\"\"\n        return 'base64'\n\n\nclass Base64UrlEncoder(EncoderProtocol):\n    \"\"\"URL-safe Base64 encoder.\"\"\"\n\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        \"\"\"Decode the data from base64 encoded bytes to original bytes data.\n\n        Args:\n            data: The data to decode.\n\n        Returns:\n            The decoded data.\n        \"\"\"\n        try:\n            return base64.urlsafe_b64decode(data)\n        except ValueError as e:\n            raise PydanticCustomError('base64_decode', \"Base64 decoding error: '{error}'\", {'error': str(e)})\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        \"\"\"Encode the data from bytes to a base64 encoded bytes.\n\n        Args:\n            value: The data to encode.\n\n        Returns:\n            The encoded data.\n        \"\"\"\n        return base64.urlsafe_b64encode(value)\n\n    @classmethod\n    def get_json_format(cls) -> Literal['base64url']:\n        \"\"\"Get the JSON format for the encoded data.\n\n        Returns:\n            The JSON format for the encoded data.\n        \"\"\"\n        return 'base64url'\n\n\n@_dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass EncodedBytes:\n    \"\"\"A bytes type that is encoded and decoded using the specified encoder.\n\n    `EncodedBytes` needs an encoder that implements `EncoderProtocol` to operate.\n\n    ```py\n    from typing_extensions import Annotated\n\n    from pydantic import BaseModel, EncodedBytes, EncoderProtocol, ValidationError\n\n    class MyEncoder(EncoderProtocol):\n        @classmethod\n        def decode(cls, data: bytes) -> bytes:\n            if data == b'**undecodable**':\n                raise ValueError('Cannot decode data')\n            return data[13:]\n\n        @classmethod\n        def encode(cls, value: bytes) -> bytes:\n            return b'**encoded**: ' + value\n\n        @classmethod\n        def get_json_format(cls) -> str:\n            return 'my-encoder'\n\n    MyEncodedBytes = Annotated[bytes, EncodedBytes(encoder=MyEncoder)]\n\n    class Model(BaseModel):\n        my_encoded_bytes: MyEncodedBytes\n\n    # Initialize the model with encoded data\n    m = Model(my_encoded_bytes=b'**encoded**: some bytes')\n\n    # Access decoded value\n    print(m.my_encoded_bytes)\n    #> b'some bytes'\n\n    # Serialize into the encoded form\n    print(m.model_dump())\n    #> {'my_encoded_bytes': b'**encoded**: some bytes'}\n\n    # Validate encoded data\n    try:\n        Model(my_encoded_bytes=b'**undecodable**')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        my_encoded_bytes\n          Value error, Cannot decode data [type=value_error, input_value=b'**undecodable**', input_type=bytes]\n        '''\n    ```\n    \"\"\"\n\n    encoder: type[EncoderProtocol]\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = handler(core_schema)\n        field_schema.update(type='string', format=self.encoder.get_json_format())\n        return field_schema\n\n    def __get_pydantic_core_schema__(self, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        return core_schema.with_info_after_validator_function(\n            function=self.decode,\n            schema=core_schema.bytes_schema(),\n            serialization=core_schema.plain_serializer_function_ser_schema(function=self.encode),\n        )\n\n    def decode(self, data: bytes, _: core_schema.ValidationInfo) -> bytes:\n        \"\"\"Decode the data using the specified encoder.\n\n        Args:\n            data: The data to decode.\n\n        Returns:\n            The decoded data.\n        \"\"\"\n        return self.encoder.decode(data)\n\n    def encode(self, value: bytes) -> bytes:\n        \"\"\"Encode the data using the specified encoder.\n\n        Args:\n            value: The data to encode.\n\n        Returns:\n            The encoded data.\n        \"\"\"\n        return self.encoder.encode(value)\n\n    def __hash__(self) -> int:\n        return hash(self.encoder)\n\n\n@_dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass EncodedStr(EncodedBytes):\n    \"\"\"A str type that is encoded and decoded using the specified encoder.\n\n    `EncodedStr` needs an encoder that implements `EncoderProtocol` to operate.\n\n    ```py\n    from typing_extensions import Annotated\n\n    from pydantic import BaseModel, EncodedStr, EncoderProtocol, ValidationError\n\n    class MyEncoder(EncoderProtocol):\n        @classmethod\n        def decode(cls, data: bytes) -> bytes:\n            if data == b'**undecodable**':\n                raise ValueError('Cannot decode data')\n            return data[13:]\n\n        @classmethod\n        def encode(cls, value: bytes) -> bytes:\n            return b'**encoded**: ' + value\n\n        @classmethod\n        def get_json_format(cls) -> str:\n            return 'my-encoder'\n\n    MyEncodedStr = Annotated[str, EncodedStr(encoder=MyEncoder)]\n\n    class Model(BaseModel):\n        my_encoded_str: MyEncodedStr\n\n    # Initialize the model with encoded data\n    m = Model(my_encoded_str='**encoded**: some str')\n\n    # Access decoded value\n    print(m.my_encoded_str)\n    #> some str\n\n    # Serialize into the encoded form\n    print(m.model_dump())\n    #> {'my_encoded_str': '**encoded**: some str'}\n\n    # Validate encoded data\n    try:\n        Model(my_encoded_str='**undecodable**')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        my_encoded_str\n          Value error, Cannot decode data [type=value_error, input_value='**undecodable**', input_type=str]\n        '''\n    ```\n    \"\"\"\n\n    def __get_pydantic_core_schema__(self, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        return core_schema.with_info_after_validator_function(\n            function=self.decode_str,\n            schema=super(EncodedStr, self).__get_pydantic_core_schema__(source=source, handler=handler),  # noqa: UP008\n            serialization=core_schema.plain_serializer_function_ser_schema(function=self.encode_str),\n        )\n\n    def decode_str(self, data: bytes, _: core_schema.ValidationInfo) -> str:\n        \"\"\"Decode the data using the specified encoder.\n\n        Args:\n            data: The data to decode.\n\n        Returns:\n            The decoded data.\n        \"\"\"\n        return data.decode()\n\n    def encode_str(self, value: str) -> str:\n        \"\"\"Encode the data using the specified encoder.\n\n        Args:\n            value: The data to encode.\n\n        Returns:\n            The encoded data.\n        \"\"\"\n        return super(EncodedStr, self).encode(value=value.encode()).decode()  # noqa: UP008\n\n    def __hash__(self) -> int:\n        return hash(self.encoder)\n\n\nBase64Bytes = Annotated[bytes, EncodedBytes(encoder=Base64Encoder)]\n\"\"\"A bytes type that is encoded and decoded using the standard (non-URL-safe) base64 encoder.\n\nNote:\n    Under the hood, `Base64Bytes` use standard library `base64.encodebytes` and `base64.decodebytes` functions.\n\n    As a result, attempting to decode url-safe base64 data using the `Base64Bytes` type may fail or produce an incorrect\n    decoding.\n\n```py\nfrom pydantic import Base64Bytes, BaseModel, ValidationError\n\nclass Model(BaseModel):\n    base64_bytes: Base64Bytes\n\n# Initialize the model with base64 data\nm = Model(base64_bytes=b'VGhpcyBpcyB0aGUgd2F5')\n\n# Access decoded value\nprint(m.base64_bytes)\n#> b'This is the way'\n\n# Serialize into the base64 form\nprint(m.model_dump())\n#> {'base64_bytes': b'VGhpcyBpcyB0aGUgd2F5\\n'}\n\n# Validate base64 data\ntry:\n    print(Model(base64_bytes=b'undecodable').base64_bytes)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    base64_bytes\n      Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value=b'undecodable', input_type=bytes]\n    '''\n```\n\"\"\"\nBase64Str = Annotated[str, EncodedStr(encoder=Base64Encoder)]\n\"\"\"A str type that is encoded and decoded using the standard (non-URL-safe) base64 encoder.\n\nNote:\n    Under the hood, `Base64Bytes` use standard library `base64.encodebytes` and `base64.decodebytes` functions.\n\n    As a result, attempting to decode url-safe base64 data using the `Base64Str` type may fail or produce an incorrect\n    decoding.\n\n```py\nfrom pydantic import Base64Str, BaseModel, ValidationError\n\nclass Model(BaseModel):\n    base64_str: Base64Str\n\n# Initialize the model with base64 data\nm = Model(base64_str='VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y')\n\n# Access decoded value\nprint(m.base64_str)\n#> These aren't the droids you're looking for\n\n# Serialize into the base64 form\nprint(m.model_dump())\n#> {'base64_str': 'VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y\\n'}\n\n# Validate base64 data\ntry:\n    print(Model(base64_str='undecodable').base64_str)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    base64_str\n      Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value='undecodable', input_type=str]\n    '''\n```\n\"\"\"\nBase64UrlBytes = Annotated[bytes, EncodedBytes(encoder=Base64UrlEncoder)]\n\"\"\"A bytes type that is encoded and decoded using the URL-safe base64 encoder.\n\nNote:\n    Under the hood, `Base64UrlBytes` use standard library `base64.urlsafe_b64encode` and `base64.urlsafe_b64decode`\n    functions.\n\n    As a result, the `Base64UrlBytes` type can be used to faithfully decode \"vanilla\" base64 data\n    (using `'+'` and `'/'`).\n\n```py\nfrom pydantic import Base64UrlBytes, BaseModel\n\nclass Model(BaseModel):\n    base64url_bytes: Base64UrlBytes\n\n# Initialize the model with base64 data\nm = Model(base64url_bytes=b'SHc_dHc-TXc==')\nprint(m)\n#> base64url_bytes=b'Hw?tw>Mw'\n```\n\"\"\"\nBase64UrlStr = Annotated[str, EncodedStr(encoder=Base64UrlEncoder)]\n\"\"\"A str type that is encoded and decoded using the URL-safe base64 encoder.\n\nNote:\n    Under the hood, `Base64UrlStr` use standard library `base64.urlsafe_b64encode` and `base64.urlsafe_b64decode`\n    functions.\n\n    As a result, the `Base64UrlStr` type can be used to faithfully decode \"vanilla\" base64 data (using `'+'` and `'/'`).\n\n```py\nfrom pydantic import Base64UrlStr, BaseModel\n\nclass Model(BaseModel):\n    base64url_str: Base64UrlStr\n\n# Initialize the model with base64 data\nm = Model(base64url_str='SHc_dHc-TXc==')\nprint(m)\n#> base64url_str='Hw?tw>Mw'\n```\n\"\"\"\n\n\n__getattr__ = getattr_migration(__name__)\n\n\n@_dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass GetPydanticSchema:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/types/#using-getpydanticschema-to-reduce-boilerplate\n\n    A convenience class for creating an annotation that provides pydantic custom type hooks.\n\n    This class is intended to eliminate the need to create a custom \"marker\" which defines the\n     `__get_pydantic_core_schema__` and `__get_pydantic_json_schema__` custom hook methods.\n\n    For example, to have a field treated by type checkers as `int`, but by pydantic as `Any`, you can do:\n    ```python\n    from typing import Any\n\n    from typing_extensions import Annotated\n\n    from pydantic import BaseModel, GetPydanticSchema\n\n    HandleAsAny = GetPydanticSchema(lambda _s, h: h(Any))\n\n    class Model(BaseModel):\n        x: Annotated[int, HandleAsAny]  # pydantic sees `x: Any`\n\n    print(repr(Model(x='abc').x))\n    #> 'abc'\n    ```\n    \"\"\"\n\n    get_pydantic_core_schema: Callable[[Any, GetCoreSchemaHandler], CoreSchema] | None = None\n    get_pydantic_json_schema: Callable[[Any, GetJsonSchemaHandler], JsonSchemaValue] | None = None\n\n    # Note: we may want to consider adding a convenience staticmethod `def for_type(type_: Any) -> GetPydanticSchema:`\n    #   which returns `GetPydanticSchema(lambda _s, h: h(type_))`\n\n    if not TYPE_CHECKING:\n        # We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access\n\n        def __getattr__(self, item: str) -> Any:\n            \"\"\"Use this rather than defining `__get_pydantic_core_schema__` etc. to reduce the number of nested calls.\"\"\"\n            if item == '__get_pydantic_core_schema__' and self.get_pydantic_core_schema:\n                return self.get_pydantic_core_schema\n            elif item == '__get_pydantic_json_schema__' and self.get_pydantic_json_schema:\n                return self.get_pydantic_json_schema\n            else:\n                return object.__getattribute__(self, item)\n\n    __hash__ = object.__hash__\n\n\n@_dataclasses.dataclass(**_internal_dataclass.slots_true, frozen=True)\nclass Tag:\n    \"\"\"Provides a way to specify the expected tag to use for a case of a (callable) discriminated union.\n\n    Also provides a way to label a union case in error messages.\n\n    When using a callable `Discriminator`, attach a `Tag` to each case in the `Union` to specify the tag that\n    should be used to identify that case. For example, in the below example, the `Tag` is used to specify that\n    if `get_discriminator_value` returns `'apple'`, the input should be validated as an `ApplePie`, and if it\n    returns `'pumpkin'`, the input should be validated as a `PumpkinPie`.\n\n    The primary role of the `Tag` here is to map the return value from the callable `Discriminator` function to\n    the appropriate member of the `Union` in question.\n\n    ```py\n    from typing import Any, Union\n\n    from typing_extensions import Annotated, Literal\n\n    from pydantic import BaseModel, Discriminator, Tag\n\n    class Pie(BaseModel):\n        time_to_cook: int\n        num_ingredients: int\n\n    class ApplePie(Pie):\n        fruit: Literal['apple'] = 'apple'\n\n    class PumpkinPie(Pie):\n        filling: Literal['pumpkin'] = 'pumpkin'\n\n    def get_discriminator_value(v: Any) -> str:\n        if isinstance(v, dict):\n            return v.get('fruit', v.get('filling'))\n        return getattr(v, 'fruit', getattr(v, 'filling', None))\n\n    class ThanksgivingDinner(BaseModel):\n        dessert: Annotated[\n            Union[\n                Annotated[ApplePie, Tag('apple')],\n                Annotated[PumpkinPie, Tag('pumpkin')],\n            ],\n            Discriminator(get_discriminator_value),\n        ]\n\n    apple_variation = ThanksgivingDinner.model_validate(\n        {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n    )\n    print(repr(apple_variation))\n    '''\n    ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n    '''\n\n    pumpkin_variation = ThanksgivingDinner.model_validate(\n        {\n            'dessert': {\n                'filling': 'pumpkin',\n                'time_to_cook': 40,\n                'num_ingredients': 6,\n            }\n        }\n    )\n    print(repr(pumpkin_variation))\n    '''\n    ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n    '''\n    ```\n\n    !!! note\n        You must specify a `Tag` for every case in a `Tag` that is associated with a\n        callable `Discriminator`. Failing to do so will result in a `PydanticUserError` with code\n        [`callable-discriminator-no-tag`](../errors/usage_errors.md#callable-discriminator-no-tag).\n\n    See the [Discriminated Unions] concepts docs for more details on how to use `Tag`s.\n\n    [Discriminated Unions]: ../concepts/unions.md#discriminated-unions\n    \"\"\"\n\n    tag: str\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n        schema = handler(source_type)\n        metadata = schema.setdefault('metadata', {})\n        assert isinstance(metadata, dict)\n        metadata[_core_utils.TAGGED_UNION_TAG_KEY] = self.tag\n        return schema\n\n\n@_dataclasses.dataclass(**_internal_dataclass.slots_true, frozen=True)\nclass Discriminator:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/unions/#discriminated-unions-with-callable-discriminator\n\n    Provides a way to use a custom callable as the way to extract the value of a union discriminator.\n\n    This allows you to get validation behavior like you'd get from `Field(discriminator=<field_name>)`,\n    but without needing to have a single shared field across all the union choices. This also makes it\n    possible to handle unions of models and primitive types with discriminated-union-style validation errors.\n    Finally, this allows you to use a custom callable as the way to identify which member of a union a value\n    belongs to, while still seeing all the performance benefits of a discriminated union.\n\n    Consider this example, which is much more performant with the use of `Discriminator` and thus a `TaggedUnion`\n    than it would be as a normal `Union`.\n\n    ```py\n    from typing import Any, Union\n\n    from typing_extensions import Annotated, Literal\n\n    from pydantic import BaseModel, Discriminator, Tag\n\n    class Pie(BaseModel):\n        time_to_cook: int\n        num_ingredients: int\n\n    class ApplePie(Pie):\n        fruit: Literal['apple'] = 'apple'\n\n    class PumpkinPie(Pie):\n        filling: Literal['pumpkin'] = 'pumpkin'\n\n    def get_discriminator_value(v: Any) -> str:\n        if isinstance(v, dict):\n            return v.get('fruit', v.get('filling'))\n        return getattr(v, 'fruit', getattr(v, 'filling', None))\n\n    class ThanksgivingDinner(BaseModel):\n        dessert: Annotated[\n            Union[\n                Annotated[ApplePie, Tag('apple')],\n                Annotated[PumpkinPie, Tag('pumpkin')],\n            ],\n            Discriminator(get_discriminator_value),\n        ]\n\n    apple_variation = ThanksgivingDinner.model_validate(\n        {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n    )\n    print(repr(apple_variation))\n    '''\n    ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n    '''\n\n    pumpkin_variation = ThanksgivingDinner.model_validate(\n        {\n            'dessert': {\n                'filling': 'pumpkin',\n                'time_to_cook': 40,\n                'num_ingredients': 6,\n            }\n        }\n    )\n    print(repr(pumpkin_variation))\n    '''\n    ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n    '''\n    ```\n\n    See the [Discriminated Unions] concepts docs for more details on how to use `Discriminator`s.\n\n    [Discriminated Unions]: ../concepts/unions.md#discriminated-unions\n    \"\"\"\n\n    discriminator: str | Callable[[Any], Hashable]\n    \"\"\"The callable or field name for discriminating the type in a tagged union.\n\n    A `Callable` discriminator must extract the value of the discriminator from the input.\n    A `str` discriminator must be the name of a field to discriminate against.\n    \"\"\"\n    custom_error_type: str | None = None\n    \"\"\"Type to use in [custom errors](../errors/errors.md#custom-errors) replacing the standard discriminated union\n    validation errors.\n    \"\"\"\n    custom_error_message: str | None = None\n    \"\"\"Message to use in custom errors.\"\"\"\n    custom_error_context: dict[str, int | str | float] | None = None\n    \"\"\"Context to use in custom errors.\"\"\"\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n        origin = _typing_extra.get_origin(source_type)\n        if not origin or not _typing_extra.origin_is_union(origin):\n            raise TypeError(f'{type(self).__name__} must be used with a Union type, not {source_type}')\n\n        if isinstance(self.discriminator, str):\n            from pydantic import Field\n\n            return handler(Annotated[source_type, Field(discriminator=self.discriminator)])\n        else:\n            original_schema = handler(source_type)\n            return self._convert_schema(original_schema)\n\n    def _convert_schema(self, original_schema: core_schema.CoreSchema) -> core_schema.TaggedUnionSchema:\n        if original_schema['type'] != 'union':\n            # This likely indicates that the schema was a single-item union that was simplified.\n            # In this case, we do the same thing we do in\n            # `pydantic._internal._discriminated_union._ApplyInferredDiscriminator._apply_to_root`, namely,\n            # package the generated schema back into a single-item union.\n            original_schema = core_schema.union_schema([original_schema])\n\n        tagged_union_choices = {}\n        for i, choice in enumerate(original_schema['choices']):\n            tag = None\n            if isinstance(choice, tuple):\n                choice, tag = choice\n            metadata = choice.get('metadata')\n            if metadata is not None:\n                metadata_tag = metadata.get(_core_utils.TAGGED_UNION_TAG_KEY)\n                if metadata_tag is not None:\n                    tag = metadata_tag\n            if tag is None:\n                raise PydanticUserError(\n                    f'`Tag` not provided for choice {choice} used with `Discriminator`',\n                    code='callable-discriminator-no-tag',\n                )\n            tagged_union_choices[tag] = choice\n\n        # Have to do these verbose checks to ensure falsy values ('' and {}) don't get ignored\n        custom_error_type = self.custom_error_type\n        if custom_error_type is None:\n            custom_error_type = original_schema.get('custom_error_type')\n\n        custom_error_message = self.custom_error_message\n        if custom_error_message is None:\n            custom_error_message = original_schema.get('custom_error_message')\n\n        custom_error_context = self.custom_error_context\n        if custom_error_context is None:\n            custom_error_context = original_schema.get('custom_error_context')\n\n        custom_error_type = original_schema.get('custom_error_type') if custom_error_type is None else custom_error_type\n        return core_schema.tagged_union_schema(\n            tagged_union_choices,\n            self.discriminator,\n            custom_error_type=custom_error_type,\n            custom_error_message=custom_error_message,\n            custom_error_context=custom_error_context,\n            strict=original_schema.get('strict'),\n            ref=original_schema.get('ref'),\n            metadata=original_schema.get('metadata'),\n            serialization=original_schema.get('serialization'),\n        )\n\n\n_JSON_TYPES = {int, float, str, bool, list, dict, type(None)}\n\n\ndef _get_type_name(x: Any) -> str:\n    type_ = type(x)\n    if type_ in _JSON_TYPES:\n        return type_.__name__\n\n    # Handle proper subclasses; note we don't need to handle None or bool here\n    if isinstance(x, int):\n        return 'int'\n    if isinstance(x, float):\n        return 'float'\n    if isinstance(x, str):\n        return 'str'\n    if isinstance(x, list):\n        return 'list'\n    if isinstance(x, dict):\n        return 'dict'\n\n    # Fail by returning the type's actual name\n    return getattr(type_, '__name__', '<no type name>')\n\n\nclass _AllowAnyJson:\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n        python_schema = handler(source_type)\n        return core_schema.json_or_python_schema(json_schema=core_schema.any_schema(), python_schema=python_schema)\n\n\nif TYPE_CHECKING:\n    # This seems to only be necessary for mypy\n    JsonValue: TypeAlias = Union[\n        List['JsonValue'],\n        Dict[str, 'JsonValue'],\n        str,\n        bool,\n        int,\n        float,\n        None,\n    ]\n    \"\"\"A `JsonValue` is used to represent a value that can be serialized to JSON.\n\n    It may be one of:\n\n    * `List['JsonValue']`\n    * `Dict[str, 'JsonValue']`\n    * `str`\n    * `bool`\n    * `int`\n    * `float`\n    * `None`\n\n    The following example demonstrates how to use `JsonValue` to validate JSON data,\n    and what kind of errors to expect when input data is not json serializable.\n\n    ```py\n    import json\n\n    from pydantic import BaseModel, JsonValue, ValidationError\n\n    class Model(BaseModel):\n        j: JsonValue\n\n    valid_json_data = {'j': {'a': {'b': {'c': 1, 'd': [2, None]}}}}\n    invalid_json_data = {'j': {'a': {'b': ...}}}\n\n    print(repr(Model.model_validate(valid_json_data)))\n    #> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})\n    print(repr(Model.model_validate_json(json.dumps(valid_json_data))))\n    #> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})\n\n    try:\n        Model.model_validate(invalid_json_data)\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        j.dict.a.dict.b\n          input was not a valid JSON value [type=invalid-json-value, input_value=Ellipsis, input_type=ellipsis]\n        '''\n    ```\n    \"\"\"\n\nelse:\n    JsonValue = TypeAliasType(\n        'JsonValue',\n        Annotated[\n            Union[\n                Annotated[List['JsonValue'], Tag('list')],\n                Annotated[Dict[str, 'JsonValue'], Tag('dict')],\n                Annotated[str, Tag('str')],\n                Annotated[bool, Tag('bool')],\n                Annotated[int, Tag('int')],\n                Annotated[float, Tag('float')],\n                Annotated[None, Tag('NoneType')],\n            ],\n            Discriminator(\n                _get_type_name,\n                custom_error_type='invalid-json-value',\n                custom_error_message='input was not a valid JSON value',\n            ),\n            _AllowAnyJson,\n        ],\n    )\n\n\nclass _OnErrorOmit:\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n        # there is no actual default value here but we use with_default_schema since it already has the on_error\n        # behavior implemented and it would be no more efficient to implement it on every other validator\n        # or as a standalone validator\n        return core_schema.with_default_schema(schema=handler(source_type), on_error='omit')\n\n\nOnErrorOmit = Annotated[T, _OnErrorOmit]\n\"\"\"\nWhen used as an item in a list, the key type in a dict, optional values of a TypedDict, etc.\nthis annotation omits the item from the iteration if there is any error validating it.\nThat is, instead of a [`ValidationError`][pydantic_core.ValidationError] being propagated up and the entire iterable being discarded\nany invalid items are discarded and the valid ones are returned.\n\"\"\"\n", "pydantic/main.py": "\"\"\"Logic for creating models.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport operator\nimport sys\nimport types\nimport typing\nimport warnings\nfrom copy import copy, deepcopy\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Generator,\n    Literal,\n    Set,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\n\nimport pydantic_core\nimport typing_extensions\nfrom pydantic_core import PydanticUndefined\nfrom typing_extensions import Self, TypeAlias, Unpack\n\nfrom ._internal import (\n    _config,\n    _decorators,\n    _fields,\n    _forward_ref,\n    _generics,\n    _mock_val_ser,\n    _model_construction,\n    _repr,\n    _typing_extra,\n    _utils,\n)\nfrom ._migration import getattr_migration\nfrom .aliases import AliasChoices, AliasPath\nfrom .annotated_handlers import GetCoreSchemaHandler, GetJsonSchemaHandler\nfrom .config import ConfigDict\nfrom .errors import PydanticUndefinedAnnotation, PydanticUserError\nfrom .json_schema import DEFAULT_REF_TEMPLATE, GenerateJsonSchema, JsonSchemaMode, JsonSchemaValue, model_json_schema\nfrom .warnings import PydanticDeprecatedSince20\n\n# Always define certain types that are needed to resolve method type hints/annotations\n# (even when not type checking) via typing.get_type_hints.\nModelT = TypeVar('ModelT', bound='BaseModel')\nTupleGenerator = Generator[Tuple[str, Any], None, None]\n# should be `set[int] | set[str] | dict[int, IncEx] | dict[str, IncEx] | None`, but mypy can't cope\nIncEx: TypeAlias = Union[Set[int], Set[str], Dict[int, Any], Dict[str, Any], None]\n\n\nif TYPE_CHECKING:\n    from inspect import Signature\n    from pathlib import Path\n\n    from pydantic_core import CoreSchema, SchemaSerializer, SchemaValidator\n\n    from ._internal._utils import AbstractSetIntStr, MappingIntStrAny\n    from .deprecated.parse import Protocol as DeprecatedParseProtocol\n    from .fields import ComputedFieldInfo, FieldInfo, ModelPrivateAttr\n    from .fields import PrivateAttr as _PrivateAttr\nelse:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n__all__ = 'BaseModel', 'create_model'\n\n_object_setattr = _model_construction.object_setattr\n\n\nclass BaseModel(metaclass=_model_construction.ModelMetaclass):\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/models/\n\n    A base class for creating Pydantic models.\n\n    Attributes:\n        __class_vars__: The names of classvars defined on the model.\n        __private_attributes__: Metadata about the private attributes of the model.\n        __signature__: The signature for instantiating the model.\n\n        __pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n        __pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n        __pydantic_custom_init__: Whether the model has a custom `__init__` function.\n        __pydantic_decorators__: Metadata containing the decorators defined on the model.\n            This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n        __pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n            __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n        __pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n        __pydantic_post_init__: The name of the post-init method for the model, if defined.\n        __pydantic_root_model__: Whether the model is a `RootModel`.\n        __pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n        __pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n        __pydantic_extra__: An instance attribute with the values of extra fields from validation when\n            `model_config['extra'] == 'allow'`.\n        __pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n        __pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n    \"\"\"\n\n    if TYPE_CHECKING:\n        # Here we provide annotations for the attributes of BaseModel.\n        # Many of these are populated by the metaclass, which is why this section is in a `TYPE_CHECKING` block.\n        # However, for the sake of easy review, we have included type annotations of all class and instance attributes\n        # of `BaseModel` here:\n\n        # Class attributes\n        model_config: ClassVar[ConfigDict]\n        \"\"\"\n        Configuration for the model, should be a dictionary conforming to [`ConfigDict`][pydantic.config.ConfigDict].\n        \"\"\"\n\n        model_fields: ClassVar[dict[str, FieldInfo]]\n        \"\"\"\n        Metadata about the fields defined on the model,\n        mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo].\n\n        This replaces `Model.__fields__` from Pydantic V1.\n        \"\"\"\n\n        model_computed_fields: ClassVar[dict[str, ComputedFieldInfo]]\n        \"\"\"A dictionary of computed field names and their corresponding `ComputedFieldInfo` objects.\"\"\"\n\n        __class_vars__: ClassVar[set[str]]\n        __private_attributes__: ClassVar[dict[str, ModelPrivateAttr]]\n        __signature__: ClassVar[Signature]\n\n        __pydantic_complete__: ClassVar[bool]\n        __pydantic_core_schema__: ClassVar[CoreSchema]\n        __pydantic_custom_init__: ClassVar[bool]\n        __pydantic_decorators__: ClassVar[_decorators.DecoratorInfos]\n        __pydantic_generic_metadata__: ClassVar[_generics.PydanticGenericMetadata]\n        __pydantic_parent_namespace__: ClassVar[dict[str, Any] | None]\n        __pydantic_post_init__: ClassVar[None | Literal['model_post_init']]\n        __pydantic_root_model__: ClassVar[bool]\n        __pydantic_serializer__: ClassVar[SchemaSerializer]\n        __pydantic_validator__: ClassVar[SchemaValidator]\n\n        # Instance attributes\n        __pydantic_extra__: dict[str, Any] | None = _PrivateAttr()\n        __pydantic_fields_set__: set[str] = _PrivateAttr()\n        __pydantic_private__: dict[str, Any] | None = _PrivateAttr()\n\n    else:\n        # `model_fields` and `__pydantic_decorators__` must be set for\n        # pydantic._internal._generate_schema.GenerateSchema.model_schema to work for a plain BaseModel annotation\n        model_fields = {}\n        model_computed_fields = {}\n\n        __pydantic_decorators__ = _decorators.DecoratorInfos()\n        __pydantic_parent_namespace__ = None\n        # Prevent `BaseModel` from being instantiated directly:\n        __pydantic_core_schema__ = _mock_val_ser.MockCoreSchema(\n            'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly',\n            code='base-model-instantiated',\n        )\n        __pydantic_validator__ = _mock_val_ser.MockValSer(\n            'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly',\n            val_or_ser='validator',\n            code='base-model-instantiated',\n        )\n        __pydantic_serializer__ = _mock_val_ser.MockValSer(\n            'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly',\n            val_or_ser='serializer',\n            code='base-model-instantiated',\n        )\n\n    __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n\n    model_config = ConfigDict()\n    __pydantic_complete__ = False\n    __pydantic_root_model__ = False\n\n    def __init__(self, /, **data: Any) -> None:  # type: ignore\n        \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n        Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n        validated to form a valid model.\n\n        `self` is explicitly positional-only to allow `self` as a field name.\n        \"\"\"\n        # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n        __tracebackhide__ = True\n        self.__pydantic_validator__.validate_python(data, self_instance=self)\n\n    # The following line sets a flag that we use to determine when `__init__` gets overridden by the user\n    __init__.__pydantic_base_init__ = True  # pyright: ignore[reportFunctionMemberAccess]\n\n    @property\n    def model_extra(self) -> dict[str, Any] | None:\n        \"\"\"Get extra fields set during validation.\n\n        Returns:\n            A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n        \"\"\"\n        return self.__pydantic_extra__\n\n    @property\n    def model_fields_set(self) -> set[str]:\n        \"\"\"Returns the set of fields that have been explicitly set on this model instance.\n\n        Returns:\n            A set of strings representing the fields that have been set,\n                i.e. that were not filled from defaults.\n        \"\"\"\n        return self.__pydantic_fields_set__\n\n    @classmethod\n    def model_construct(cls, _fields_set: set[str] | None = None, **values: Any) -> Self:  # noqa: C901\n        \"\"\"Creates a new instance of the `Model` class with validated data.\n\n        Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n        Default values are respected, but no other validation is performed.\n\n        !!! note\n            `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n            That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n            and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n            Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n            an error if extra values are passed, but they will be ignored.\n\n        Args:\n            _fields_set: The set of field names accepted for the Model instance.\n            values: Trusted or pre-validated data dictionary.\n\n        Returns:\n            A new instance of the `Model` class with validated data.\n        \"\"\"\n        m = cls.__new__(cls)\n        fields_values: dict[str, Any] = {}\n        fields_set = set()\n\n        for name, field in cls.model_fields.items():\n            if field.alias is not None and field.alias in values:\n                fields_values[name] = values.pop(field.alias)\n                fields_set.add(name)\n\n            if (name not in fields_set) and (field.validation_alias is not None):\n                validation_aliases: list[str | AliasPath] = (\n                    field.validation_alias.choices\n                    if isinstance(field.validation_alias, AliasChoices)\n                    else [field.validation_alias]\n                )\n\n                for alias in validation_aliases:\n                    if isinstance(alias, str) and alias in values:\n                        fields_values[name] = values.pop(alias)\n                        fields_set.add(name)\n                        break\n                    elif isinstance(alias, AliasPath):\n                        value = alias.search_dict_for_path(values)\n                        if value is not PydanticUndefined:\n                            fields_values[name] = value\n                            fields_set.add(name)\n                            break\n\n            if name not in fields_set:\n                if name in values:\n                    fields_values[name] = values.pop(name)\n                    fields_set.add(name)\n                elif not field.is_required():\n                    fields_values[name] = field.get_default(call_default_factory=True)\n        if _fields_set is None:\n            _fields_set = fields_set\n\n        _extra: dict[str, Any] | None = (\n            {k: v for k, v in values.items()} if cls.model_config.get('extra') == 'allow' else None\n        )\n        _object_setattr(m, '__dict__', fields_values)\n        _object_setattr(m, '__pydantic_fields_set__', _fields_set)\n        if not cls.__pydantic_root_model__:\n            _object_setattr(m, '__pydantic_extra__', _extra)\n\n        if cls.__pydantic_post_init__:\n            m.model_post_init(None)\n            # update private attributes with values set\n            if hasattr(m, '__pydantic_private__') and m.__pydantic_private__ is not None:\n                for k, v in values.items():\n                    if k in m.__private_attributes__:\n                        m.__pydantic_private__[k] = v\n\n        elif not cls.__pydantic_root_model__:\n            # Note: if there are any private attributes, cls.__pydantic_post_init__ would exist\n            # Since it doesn't, that means that `__pydantic_private__` should be set to None\n            _object_setattr(m, '__pydantic_private__', None)\n\n        return m\n\n    def model_copy(self, *, update: dict[str, Any] | None = None, deep: bool = False) -> Self:\n        \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/serialization/#model_copy\n\n        Returns a copy of the model.\n\n        Args:\n            update: Values to change/add in the new model. Note: the data is not validated\n                before creating the new model. You should trust this data.\n            deep: Set to `True` to make a deep copy of the model.\n\n        Returns:\n            New model instance.\n        \"\"\"\n        copied = self.__deepcopy__() if deep else self.__copy__()\n        if update:\n            if self.model_config.get('extra') == 'allow':\n                for k, v in update.items():\n                    if k in self.model_fields:\n                        copied.__dict__[k] = v\n                    else:\n                        if copied.__pydantic_extra__ is None:\n                            copied.__pydantic_extra__ = {}\n                        copied.__pydantic_extra__[k] = v\n            else:\n                copied.__dict__.update(update)\n            copied.__pydantic_fields_set__.update(update.keys())\n        return copied\n\n    def model_dump(\n        self,\n        *,\n        mode: Literal['json', 'python'] | str = 'python',\n        include: IncEx = None,\n        exclude: IncEx = None,\n        context: Any | None = None,\n        by_alias: bool = False,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        round_trip: bool = False,\n        warnings: bool | Literal['none', 'warn', 'error'] = True,\n        serialize_as_any: bool = False,\n    ) -> dict[str, Any]:\n        \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/serialization/#modelmodel_dump\n\n        Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\n        Args:\n            mode: The mode in which `to_python` should run.\n                If mode is 'json', the output will only contain JSON serializable types.\n                If mode is 'python', the output may contain non-JSON-serializable Python objects.\n            include: A set of fields to include in the output.\n            exclude: A set of fields to exclude from the output.\n            context: Additional context to pass to the serializer.\n            by_alias: Whether to use the field's alias in the dictionary key if defined.\n            exclude_unset: Whether to exclude fields that have not been explicitly set.\n            exclude_defaults: Whether to exclude fields that are set to their default value.\n            exclude_none: Whether to exclude fields that have a value of `None`.\n            round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n            warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n                \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n            serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n\n        Returns:\n            A dictionary representation of the model.\n        \"\"\"\n        return self.__pydantic_serializer__.to_python(\n            self,\n            mode=mode,\n            by_alias=by_alias,\n            include=include,\n            exclude=exclude,\n            context=context,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            round_trip=round_trip,\n            warnings=warnings,\n            serialize_as_any=serialize_as_any,\n        )\n\n    def model_dump_json(\n        self,\n        *,\n        indent: int | None = None,\n        include: IncEx = None,\n        exclude: IncEx = None,\n        context: Any | None = None,\n        by_alias: bool = False,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        round_trip: bool = False,\n        warnings: bool | Literal['none', 'warn', 'error'] = True,\n        serialize_as_any: bool = False,\n    ) -> str:\n        \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/serialization/#modelmodel_dump_json\n\n        Generates a JSON representation of the model using Pydantic's `to_json` method.\n\n        Args:\n            indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n            include: Field(s) to include in the JSON output.\n            exclude: Field(s) to exclude from the JSON output.\n            context: Additional context to pass to the serializer.\n            by_alias: Whether to serialize using field aliases.\n            exclude_unset: Whether to exclude fields that have not been explicitly set.\n            exclude_defaults: Whether to exclude fields that are set to their default value.\n            exclude_none: Whether to exclude fields that have a value of `None`.\n            round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n            warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n                \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n            serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n\n        Returns:\n            A JSON string representation of the model.\n        \"\"\"\n        return self.__pydantic_serializer__.to_json(\n            self,\n            indent=indent,\n            include=include,\n            exclude=exclude,\n            context=context,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            round_trip=round_trip,\n            warnings=warnings,\n            serialize_as_any=serialize_as_any,\n        ).decode()\n\n    @classmethod\n    def model_json_schema(\n        cls,\n        by_alias: bool = True,\n        ref_template: str = DEFAULT_REF_TEMPLATE,\n        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n        mode: JsonSchemaMode = 'validation',\n    ) -> dict[str, Any]:\n        \"\"\"Generates a JSON schema for a model class.\n\n        Args:\n            by_alias: Whether to use attribute aliases or not.\n            ref_template: The reference template.\n            schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n                `GenerateJsonSchema` with your desired modifications\n            mode: The mode in which to generate the schema.\n\n        Returns:\n            The JSON schema for the given model class.\n        \"\"\"\n        return model_json_schema(\n            cls, by_alias=by_alias, ref_template=ref_template, schema_generator=schema_generator, mode=mode\n        )\n\n    @classmethod\n    def model_parametrized_name(cls, params: tuple[type[Any], ...]) -> str:\n        \"\"\"Compute the class name for parametrizations of generic classes.\n\n        This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n\n        Args:\n            params: Tuple of types of the class. Given a generic class\n                `Model` with 2 type variables and a concrete model `Model[str, int]`,\n                the value `(str, int)` would be passed to `params`.\n\n        Returns:\n            String representing the new class where `params` are passed to `cls` as type variables.\n\n        Raises:\n            TypeError: Raised when trying to generate concrete names for non-generic models.\n        \"\"\"\n        if not issubclass(cls, typing.Generic):\n            raise TypeError('Concrete names should only be generated for generic models.')\n\n        # Any strings received should represent forward references, so we handle them specially below.\n        # If we eventually move toward wrapping them in a ForwardRef in __class_getitem__ in the future,\n        # we may be able to remove this special case.\n        param_names = [param if isinstance(param, str) else _repr.display_as_type(param) for param in params]\n        params_component = ', '.join(param_names)\n        return f'{cls.__name__}[{params_component}]'\n\n    def model_post_init(self, __context: Any) -> None:\n        \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n        This is useful if you want to do some validation that requires the entire model to be initialized.\n        \"\"\"\n        pass\n\n    @classmethod\n    def model_rebuild(\n        cls,\n        *,\n        force: bool = False,\n        raise_errors: bool = True,\n        _parent_namespace_depth: int = 2,\n        _types_namespace: dict[str, Any] | None = None,\n    ) -> bool | None:\n        \"\"\"Try to rebuild the pydantic-core schema for the model.\n\n        This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n        the initial attempt to build the schema, and automatic rebuilding fails.\n\n        Args:\n            force: Whether to force the rebuilding of the model schema, defaults to `False`.\n            raise_errors: Whether to raise errors, defaults to `True`.\n            _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n            _types_namespace: The types namespace, defaults to `None`.\n\n        Returns:\n            Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n            If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n        \"\"\"\n        if not force and cls.__pydantic_complete__:\n            return None\n        else:\n            if '__pydantic_core_schema__' in cls.__dict__:\n                delattr(cls, '__pydantic_core_schema__')  # delete cached value to ensure full rebuild happens\n            if _types_namespace is not None:\n                types_namespace: dict[str, Any] | None = _types_namespace.copy()\n            else:\n                if _parent_namespace_depth > 0:\n                    frame_parent_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth) or {}\n                    cls_parent_ns = (\n                        _model_construction.unpack_lenient_weakvaluedict(cls.__pydantic_parent_namespace__) or {}\n                    )\n                    types_namespace = {**cls_parent_ns, **frame_parent_ns}\n                    cls.__pydantic_parent_namespace__ = _model_construction.build_lenient_weakvaluedict(types_namespace)\n                else:\n                    types_namespace = _model_construction.unpack_lenient_weakvaluedict(\n                        cls.__pydantic_parent_namespace__\n                    )\n\n                types_namespace = _typing_extra.get_cls_types_namespace(cls, types_namespace)\n\n            # manually override defer_build so complete_model_class doesn't skip building the model again\n            config = {**cls.model_config, 'defer_build': False}\n            return _model_construction.complete_model_class(\n                cls,\n                cls.__name__,\n                _config.ConfigWrapper(config, check=False),\n                raise_errors=raise_errors,\n                types_namespace=types_namespace,\n            )\n\n    @classmethod\n    def model_validate(\n        cls,\n        obj: Any,\n        *,\n        strict: bool | None = None,\n        from_attributes: bool | None = None,\n        context: Any | None = None,\n    ) -> Self:\n        \"\"\"Validate a pydantic model instance.\n\n        Args:\n            obj: The object to validate.\n            strict: Whether to enforce types strictly.\n            from_attributes: Whether to extract data from object attributes.\n            context: Additional context to pass to the validator.\n\n        Raises:\n            ValidationError: If the object could not be validated.\n\n        Returns:\n            The validated model instance.\n        \"\"\"\n        # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n        __tracebackhide__ = True\n        return cls.__pydantic_validator__.validate_python(\n            obj, strict=strict, from_attributes=from_attributes, context=context\n        )\n\n    @classmethod\n    def model_validate_json(\n        cls,\n        json_data: str | bytes | bytearray,\n        *,\n        strict: bool | None = None,\n        context: Any | None = None,\n    ) -> Self:\n        \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/json/#json-parsing\n\n        Validate the given JSON data against the Pydantic model.\n\n        Args:\n            json_data: The JSON data to validate.\n            strict: Whether to enforce types strictly.\n            context: Extra variables to pass to the validator.\n\n        Returns:\n            The validated Pydantic model.\n\n        Raises:\n            ValueError: If `json_data` is not a JSON string.\n        \"\"\"\n        # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n        __tracebackhide__ = True\n        return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\n\n    @classmethod\n    def model_validate_strings(\n        cls,\n        obj: Any,\n        *,\n        strict: bool | None = None,\n        context: Any | None = None,\n    ) -> Self:\n        \"\"\"Validate the given object with string data against the Pydantic model.\n\n        Args:\n            obj: The object containing string data to validate.\n            strict: Whether to enforce types strictly.\n            context: Extra variables to pass to the validator.\n\n        Returns:\n            The validated Pydantic model.\n        \"\"\"\n        # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n        __tracebackhide__ = True\n        return cls.__pydantic_validator__.validate_strings(obj, strict=strict, context=context)\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:\n        \"\"\"Hook into generating the model's CoreSchema.\n\n        Args:\n            source: The class we are generating a schema for.\n                This will generally be the same as the `cls` argument if this is a classmethod.\n            handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n\n        Returns:\n            A `pydantic-core` `CoreSchema`.\n        \"\"\"\n        # Only use the cached value from this _exact_ class; we don't want one from a parent class\n        # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\n        schema = cls.__dict__.get('__pydantic_core_schema__')\n        if schema is not None and not isinstance(schema, _mock_val_ser.MockCoreSchema):\n            # Due to the way generic classes are built, it's possible that an invalid schema may be temporarily\n            # set on generic classes. I think we could resolve this to ensure that we get proper schema caching\n            # for generics, but for simplicity for now, we just always rebuild if the class has a generic origin.\n            if not cls.__pydantic_generic_metadata__['origin']:\n                return cls.__pydantic_core_schema__\n\n        return handler(source)\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls,\n        core_schema: CoreSchema,\n        handler: GetJsonSchemaHandler,\n        /,\n    ) -> JsonSchemaValue:\n        \"\"\"Hook into generating the model's JSON schema.\n\n        Args:\n            core_schema: A `pydantic-core` CoreSchema.\n                You can ignore this argument and call the handler with a new CoreSchema,\n                wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n                or just call the handler with the original schema.\n            handler: Call into Pydantic's internal JSON schema generation.\n                This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n                generation fails.\n                Since this gets called by `BaseModel.model_json_schema` you can override the\n                `schema_generator` argument to that function to change JSON schema generation globally\n                for a type.\n\n        Returns:\n            A JSON schema, as a Python object.\n        \"\"\"\n        return handler(core_schema)\n\n    @classmethod\n    def __pydantic_init_subclass__(cls, **kwargs: Any) -> None:\n        \"\"\"This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n        only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n        be present when this is called.\n\n        This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n        and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n        `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n\n        This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n        any kwargs passed to the class definition that aren't used internally by pydantic.\n\n        Args:\n            **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n                by pydantic.\n        \"\"\"\n        pass\n\n    def __class_getitem__(\n        cls, typevar_values: type[Any] | tuple[type[Any], ...]\n    ) -> type[BaseModel] | _forward_ref.PydanticRecursiveRef:\n        cached = _generics.get_cached_generic_type_early(cls, typevar_values)\n        if cached is not None:\n            return cached\n\n        if cls is BaseModel:\n            raise TypeError('Type parameters should be placed on typing.Generic, not BaseModel')\n        if not hasattr(cls, '__parameters__'):\n            raise TypeError(f'{cls} cannot be parametrized because it does not inherit from typing.Generic')\n        if not cls.__pydantic_generic_metadata__['parameters'] and typing.Generic not in cls.__bases__:\n            raise TypeError(f'{cls} is not a generic class')\n\n        if not isinstance(typevar_values, tuple):\n            typevar_values = (typevar_values,)\n        _generics.check_parameters_count(cls, typevar_values)\n\n        # Build map from generic typevars to passed params\n        typevars_map: dict[_typing_extra.TypeVarType, type[Any]] = dict(\n            zip(cls.__pydantic_generic_metadata__['parameters'], typevar_values)\n        )\n\n        if _utils.all_identical(typevars_map.keys(), typevars_map.values()) and typevars_map:\n            submodel = cls  # if arguments are equal to parameters it's the same object\n            _generics.set_cached_generic_type(cls, typevar_values, submodel)\n        else:\n            parent_args = cls.__pydantic_generic_metadata__['args']\n            if not parent_args:\n                args = typevar_values\n            else:\n                args = tuple(_generics.replace_types(arg, typevars_map) for arg in parent_args)\n\n            origin = cls.__pydantic_generic_metadata__['origin'] or cls\n            model_name = origin.model_parametrized_name(args)\n            params = tuple(\n                {param: None for param in _generics.iter_contained_typevars(typevars_map.values())}\n            )  # use dict as ordered set\n\n            with _generics.generic_recursion_self_type(origin, args) as maybe_self_type:\n                if maybe_self_type is not None:\n                    return maybe_self_type\n\n                cached = _generics.get_cached_generic_type_late(cls, typevar_values, origin, args)\n                if cached is not None:\n                    return cached\n\n                # Attempt to rebuild the origin in case new types have been defined\n                try:\n                    # depth 3 gets you above this __class_getitem__ call\n                    origin.model_rebuild(_parent_namespace_depth=3)\n                except PydanticUndefinedAnnotation:\n                    # It's okay if it fails, it just means there are still undefined types\n                    # that could be evaluated later.\n                    # TODO: Make sure validation fails if there are still undefined types, perhaps using MockValidator\n                    pass\n\n                submodel = _generics.create_generic_submodel(model_name, origin, args, params)\n\n                # Update cache\n                _generics.set_cached_generic_type(cls, typevar_values, submodel, origin, args)\n\n        return submodel\n\n    def __copy__(self) -> Self:\n        \"\"\"Returns a shallow copy of the model.\"\"\"\n        cls = type(self)\n        m = cls.__new__(cls)\n        _object_setattr(m, '__dict__', copy(self.__dict__))\n        _object_setattr(m, '__pydantic_extra__', copy(self.__pydantic_extra__))\n        _object_setattr(m, '__pydantic_fields_set__', copy(self.__pydantic_fields_set__))\n\n        if not hasattr(self, '__pydantic_private__') or self.__pydantic_private__ is None:\n            _object_setattr(m, '__pydantic_private__', None)\n        else:\n            _object_setattr(\n                m,\n                '__pydantic_private__',\n                {k: v for k, v in self.__pydantic_private__.items() if v is not PydanticUndefined},\n            )\n\n        return m\n\n    def __deepcopy__(self, memo: dict[int, Any] | None = None) -> Self:\n        \"\"\"Returns a deep copy of the model.\"\"\"\n        cls = type(self)\n        m = cls.__new__(cls)\n        _object_setattr(m, '__dict__', deepcopy(self.__dict__, memo=memo))\n        _object_setattr(m, '__pydantic_extra__', deepcopy(self.__pydantic_extra__, memo=memo))\n        # This next line doesn't need a deepcopy because __pydantic_fields_set__ is a set[str],\n        # and attempting a deepcopy would be marginally slower.\n        _object_setattr(m, '__pydantic_fields_set__', copy(self.__pydantic_fields_set__))\n\n        if not hasattr(self, '__pydantic_private__') or self.__pydantic_private__ is None:\n            _object_setattr(m, '__pydantic_private__', None)\n        else:\n            _object_setattr(\n                m,\n                '__pydantic_private__',\n                deepcopy({k: v for k, v in self.__pydantic_private__.items() if v is not PydanticUndefined}, memo=memo),\n            )\n\n        return m\n\n    if not TYPE_CHECKING:\n        # We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access\n        # The same goes for __setattr__ and __delattr__, see: https://github.com/pydantic/pydantic/issues/8643\n\n        def __getattr__(self, item: str) -> Any:\n            private_attributes = object.__getattribute__(self, '__private_attributes__')\n            if item in private_attributes:\n                attribute = private_attributes[item]\n                if hasattr(attribute, '__get__'):\n                    return attribute.__get__(self, type(self))  # type: ignore\n\n                try:\n                    # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                    return self.__pydantic_private__[item]  # type: ignore\n                except KeyError as exc:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n            else:\n                # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n                # See `BaseModel.__repr_args__` for more details\n                try:\n                    pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n                except AttributeError:\n                    pydantic_extra = None\n\n                if pydantic_extra:\n                    try:\n                        return pydantic_extra[item]\n                    except KeyError as exc:\n                        raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n                else:\n                    if hasattr(self.__class__, item):\n                        return super().__getattribute__(item)  # Raises AttributeError if appropriate\n                    else:\n                        # this is the current error\n                        raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n\n        def __setattr__(self, name: str, value: Any) -> None:\n            if name in self.__class_vars__:\n                raise AttributeError(\n                    f'{name!r} is a ClassVar of `{self.__class__.__name__}` and cannot be set on an instance. '\n                    f'If you want to set a value on the class, use `{self.__class__.__name__}.{name} = value`.'\n                )\n            elif not _fields.is_valid_field_name(name):\n                if self.__pydantic_private__ is None or name not in self.__private_attributes__:\n                    _object_setattr(self, name, value)\n                else:\n                    attribute = self.__private_attributes__[name]\n                    if hasattr(attribute, '__set__'):\n                        attribute.__set__(self, value)  # type: ignore\n                    else:\n                        self.__pydantic_private__[name] = value\n                return\n\n            self._check_frozen(name, value)\n\n            attr = getattr(self.__class__, name, None)\n            if isinstance(attr, property):\n                attr.__set__(self, value)\n            elif self.model_config.get('validate_assignment', None):\n                self.__pydantic_validator__.validate_assignment(self, name, value)\n            elif self.model_config.get('extra') != 'allow' and name not in self.model_fields:\n                # TODO - matching error\n                raise ValueError(f'\"{self.__class__.__name__}\" object has no field \"{name}\"')\n            elif self.model_config.get('extra') == 'allow' and name not in self.model_fields:\n                if self.model_extra and name in self.model_extra:\n                    self.__pydantic_extra__[name] = value  # type: ignore\n                else:\n                    try:\n                        getattr(self, name)\n                    except AttributeError:\n                        # attribute does not already exist on instance, so put it in extra\n                        self.__pydantic_extra__[name] = value  # type: ignore\n                    else:\n                        # attribute _does_ already exist on instance, and was not in extra, so update it\n                        _object_setattr(self, name, value)\n            else:\n                self.__dict__[name] = value\n                self.__pydantic_fields_set__.add(name)\n\n        def __delattr__(self, item: str) -> Any:\n            if item in self.__private_attributes__:\n                attribute = self.__private_attributes__[item]\n                if hasattr(attribute, '__delete__'):\n                    attribute.__delete__(self)  # type: ignore\n                    return\n\n                try:\n                    # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                    del self.__pydantic_private__[item]  # type: ignore\n                    return\n                except KeyError as exc:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n\n            self._check_frozen(item, None)\n\n            if item in self.model_fields:\n                object.__delattr__(self, item)\n            elif self.__pydantic_extra__ is not None and item in self.__pydantic_extra__:\n                del self.__pydantic_extra__[item]\n            else:\n                try:\n                    object.__delattr__(self, item)\n                except AttributeError:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n\n    def _check_frozen(self, name: str, value: Any) -> None:\n        if self.model_config.get('frozen', None):\n            typ = 'frozen_instance'\n        elif getattr(self.model_fields.get(name), 'frozen', False):\n            typ = 'frozen_field'\n        else:\n            return\n        error: pydantic_core.InitErrorDetails = {\n            'type': typ,\n            'loc': (name,),\n            'input': value,\n        }\n        raise pydantic_core.ValidationError.from_exception_data(self.__class__.__name__, [error])\n\n    def __getstate__(self) -> dict[Any, Any]:\n        private = self.__pydantic_private__\n        if private:\n            private = {k: v for k, v in private.items() if v is not PydanticUndefined}\n        return {\n            '__dict__': self.__dict__,\n            '__pydantic_extra__': self.__pydantic_extra__,\n            '__pydantic_fields_set__': self.__pydantic_fields_set__,\n            '__pydantic_private__': private,\n        }\n\n    def __setstate__(self, state: dict[Any, Any]) -> None:\n        _object_setattr(self, '__pydantic_fields_set__', state.get('__pydantic_fields_set__', {}))\n        _object_setattr(self, '__pydantic_extra__', state.get('__pydantic_extra__', {}))\n        _object_setattr(self, '__pydantic_private__', state.get('__pydantic_private__', {}))\n        _object_setattr(self, '__dict__', state.get('__dict__', {}))\n\n    if not TYPE_CHECKING:\n\n        def __eq__(self, other: Any) -> bool:\n            if isinstance(other, BaseModel):\n                # When comparing instances of generic types for equality, as long as all field values are equal,\n                # only require their generic origin types to be equal, rather than exact type equality.\n                # This prevents headaches like MyGeneric(x=1) != MyGeneric[Any](x=1).\n                self_type = self.__pydantic_generic_metadata__['origin'] or self.__class__\n                other_type = other.__pydantic_generic_metadata__['origin'] or other.__class__\n\n                # Perform common checks first\n                if not (\n                    self_type == other_type\n                    and getattr(self, '__pydantic_private__', None) == getattr(other, '__pydantic_private__', None)\n                    and self.__pydantic_extra__ == other.__pydantic_extra__\n                ):\n                    return False\n\n                # We only want to compare pydantic fields but ignoring fields is costly.\n                # We'll perform a fast check first, and fallback only when needed\n                # See GH-7444 and GH-7825 for rationale and a performance benchmark\n\n                # First, do the fast (and sometimes faulty) __dict__ comparison\n                if self.__dict__ == other.__dict__:\n                    # If the check above passes, then pydantic fields are equal, we can return early\n                    return True\n\n                # We don't want to trigger unnecessary costly filtering of __dict__ on all unequal objects, so we return\n                # early if there are no keys to ignore (we would just return False later on anyway)\n                model_fields = type(self).model_fields.keys()\n                if self.__dict__.keys() <= model_fields and other.__dict__.keys() <= model_fields:\n                    return False\n\n                # If we reach here, there are non-pydantic-fields keys, mapped to unequal values, that we need to ignore\n                # Resort to costly filtering of the __dict__ objects\n                # We use operator.itemgetter because it is much faster than dict comprehensions\n                # NOTE: Contrary to standard python class and instances, when the Model class has a default value for an\n                # attribute and the model instance doesn't have a corresponding attribute, accessing the missing attribute\n                # raises an error in BaseModel.__getattr__ instead of returning the class attribute\n                # So we can use operator.itemgetter() instead of operator.attrgetter()\n                getter = operator.itemgetter(*model_fields) if model_fields else lambda _: _utils._SENTINEL\n                try:\n                    return getter(self.__dict__) == getter(other.__dict__)\n                except KeyError:\n                    # In rare cases (such as when using the deprecated BaseModel.copy() method),\n                    # the __dict__ may not contain all model fields, which is how we can get here.\n                    # getter(self.__dict__) is much faster than any 'safe' method that accounts\n                    # for missing keys, and wrapping it in a `try` doesn't slow things down much\n                    # in the common case.\n                    self_fields_proxy = _utils.SafeGetItemProxy(self.__dict__)\n                    other_fields_proxy = _utils.SafeGetItemProxy(other.__dict__)\n                    return getter(self_fields_proxy) == getter(other_fields_proxy)\n\n            # other instance is not a BaseModel\n            else:\n                return NotImplemented  # delegate to the other item in the comparison\n\n    if TYPE_CHECKING:\n        # We put `__init_subclass__` in a TYPE_CHECKING block because, even though we want the type-checking benefits\n        # described in the signature of `__init_subclass__` below, we don't want to modify the default behavior of\n        # subclass initialization.\n\n        def __init_subclass__(cls, **kwargs: Unpack[ConfigDict]):\n            \"\"\"This signature is included purely to help type-checkers check arguments to class declaration, which\n            provides a way to conveniently set model_config key/value pairs.\n\n            ```py\n            from pydantic import BaseModel\n\n            class MyModel(BaseModel, extra='allow'):\n                ...\n            ```\n\n            However, this may be deceiving, since the _actual_ calls to `__init_subclass__` will not receive any\n            of the config arguments, and will only receive any keyword arguments passed during class initialization\n            that are _not_ expected keys in ConfigDict. (This is due to the way `ModelMetaclass.__new__` works.)\n\n            Args:\n                **kwargs: Keyword arguments passed to the class definition, which set model_config\n\n            Note:\n                You may want to override `__pydantic_init_subclass__` instead, which behaves similarly but is called\n                *after* the class is fully initialized.\n            \"\"\"\n\n    def __iter__(self) -> TupleGenerator:\n        \"\"\"So `dict(model)` works.\"\"\"\n        yield from [(k, v) for (k, v) in self.__dict__.items() if not k.startswith('_')]\n        extra = self.__pydantic_extra__\n        if extra:\n            yield from extra.items()\n\n    def __repr__(self) -> str:\n        return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'\n\n    def __repr_args__(self) -> _repr.ReprArgs:\n        for k, v in self.__dict__.items():\n            field = self.model_fields.get(k)\n            if field and field.repr:\n                yield k, v\n\n        # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n        # This can happen if a `ValidationError` is raised during initialization and the instance's\n        # repr is generated as part of the exception handling. Therefore, we use `getattr` here\n        # with a fallback, even though the type hints indicate the attribute will always be present.\n        try:\n            pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n        except AttributeError:\n            pydantic_extra = None\n\n        if pydantic_extra is not None:\n            yield from ((k, v) for k, v in pydantic_extra.items())\n        yield from ((k, getattr(self, k)) for k, v in self.model_computed_fields.items() if v.repr)\n\n    # take logic from `_repr.Representation` without the side effects of inheritance, see #5740\n    __repr_name__ = _repr.Representation.__repr_name__\n    __repr_str__ = _repr.Representation.__repr_str__\n    __pretty__ = _repr.Representation.__pretty__\n    __rich_repr__ = _repr.Representation.__rich_repr__\n\n    def __str__(self) -> str:\n        return self.__repr_str__(' ')\n\n    # ##### Deprecated methods from v1 #####\n    @property\n    @typing_extensions.deprecated(\n        'The `__fields__` attribute is deprecated, use `model_fields` instead.', category=None\n    )\n    def __fields__(self) -> dict[str, FieldInfo]:\n        warnings.warn(\n            'The `__fields__` attribute is deprecated, use `model_fields` instead.', category=PydanticDeprecatedSince20\n        )\n        return self.model_fields\n\n    @property\n    @typing_extensions.deprecated(\n        'The `__fields_set__` attribute is deprecated, use `model_fields_set` instead.',\n        category=None,\n    )\n    def __fields_set__(self) -> set[str]:\n        warnings.warn(\n            'The `__fields_set__` attribute is deprecated, use `model_fields_set` instead.',\n            category=PydanticDeprecatedSince20,\n        )\n        return self.__pydantic_fields_set__\n\n    @typing_extensions.deprecated('The `dict` method is deprecated; use `model_dump` instead.', category=None)\n    def dict(  # noqa: D102\n        self,\n        *,\n        include: IncEx = None,\n        exclude: IncEx = None,\n        by_alias: bool = False,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n    ) -> Dict[str, Any]:  # noqa UP006\n        warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n        return self.model_dump(\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n\n    @typing_extensions.deprecated('The `json` method is deprecated; use `model_dump_json` instead.', category=None)\n    def json(  # noqa: D102\n        self,\n        *,\n        include: IncEx = None,\n        exclude: IncEx = None,\n        by_alias: bool = False,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        encoder: Callable[[Any], Any] | None = PydanticUndefined,  # type: ignore[assignment]\n        models_as_dict: bool = PydanticUndefined,  # type: ignore[assignment]\n        **dumps_kwargs: Any,\n    ) -> str:\n        warnings.warn(\n            'The `json` method is deprecated; use `model_dump_json` instead.', category=PydanticDeprecatedSince20\n        )\n        if encoder is not PydanticUndefined:\n            raise TypeError('The `encoder` argument is no longer supported; use field serializers instead.')\n        if models_as_dict is not PydanticUndefined:\n            raise TypeError('The `models_as_dict` argument is no longer supported; use a model serializer instead.')\n        if dumps_kwargs:\n            raise TypeError('`dumps_kwargs` keyword arguments are no longer supported.')\n        return self.model_dump_json(\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n\n    @classmethod\n    @typing_extensions.deprecated('The `parse_obj` method is deprecated; use `model_validate` instead.', category=None)\n    def parse_obj(cls, obj: Any) -> Self:  # noqa: D102\n        warnings.warn(\n            'The `parse_obj` method is deprecated; use `model_validate` instead.', category=PydanticDeprecatedSince20\n        )\n        return cls.model_validate(obj)\n\n    @classmethod\n    @typing_extensions.deprecated(\n        'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, '\n        'otherwise load the data then use `model_validate` instead.',\n        category=None,\n    )\n    def parse_raw(  # noqa: D102\n        cls,\n        b: str | bytes,\n        *,\n        content_type: str | None = None,\n        encoding: str = 'utf8',\n        proto: DeprecatedParseProtocol | None = None,\n        allow_pickle: bool = False,\n    ) -> Self:  # pragma: no cover\n        warnings.warn(\n            'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, '\n            'otherwise load the data then use `model_validate` instead.',\n            category=PydanticDeprecatedSince20,\n        )\n        from .deprecated import parse\n\n        try:\n            obj = parse.load_str_bytes(\n                b,\n                proto=proto,\n                content_type=content_type,\n                encoding=encoding,\n                allow_pickle=allow_pickle,\n            )\n        except (ValueError, TypeError) as exc:\n            import json\n\n            # try to match V1\n            if isinstance(exc, UnicodeDecodeError):\n                type_str = 'value_error.unicodedecode'\n            elif isinstance(exc, json.JSONDecodeError):\n                type_str = 'value_error.jsondecode'\n            elif isinstance(exc, ValueError):\n                type_str = 'value_error'\n            else:\n                type_str = 'type_error'\n\n            # ctx is missing here, but since we've added `input` to the error, we're not pretending it's the same\n            error: pydantic_core.InitErrorDetails = {\n                # The type: ignore on the next line is to ignore the requirement of LiteralString\n                'type': pydantic_core.PydanticCustomError(type_str, str(exc)),  # type: ignore\n                'loc': ('__root__',),\n                'input': b,\n            }\n            raise pydantic_core.ValidationError.from_exception_data(cls.__name__, [error])\n        return cls.model_validate(obj)\n\n    @classmethod\n    @typing_extensions.deprecated(\n        'The `parse_file` method is deprecated; load the data from file, then if your data is JSON '\n        'use `model_validate_json`, otherwise `model_validate` instead.',\n        category=None,\n    )\n    def parse_file(  # noqa: D102\n        cls,\n        path: str | Path,\n        *,\n        content_type: str | None = None,\n        encoding: str = 'utf8',\n        proto: DeprecatedParseProtocol | None = None,\n        allow_pickle: bool = False,\n    ) -> Self:\n        warnings.warn(\n            'The `parse_file` method is deprecated; load the data from file, then if your data is JSON '\n            'use `model_validate_json`, otherwise `model_validate` instead.',\n            category=PydanticDeprecatedSince20,\n        )\n        from .deprecated import parse\n\n        obj = parse.load_file(\n            path,\n            proto=proto,\n            content_type=content_type,\n            encoding=encoding,\n            allow_pickle=allow_pickle,\n        )\n        return cls.parse_obj(obj)\n\n    @classmethod\n    @typing_extensions.deprecated(\n        'The `from_orm` method is deprecated; set '\n        \"`model_config['from_attributes']=True` and use `model_validate` instead.\",\n        category=None,\n    )\n    def from_orm(cls, obj: Any) -> Self:  # noqa: D102\n        warnings.warn(\n            'The `from_orm` method is deprecated; set '\n            \"`model_config['from_attributes']=True` and use `model_validate` instead.\",\n            category=PydanticDeprecatedSince20,\n        )\n        if not cls.model_config.get('from_attributes', None):\n            raise PydanticUserError(\n                'You must set the config attribute `from_attributes=True` to use from_orm', code=None\n            )\n        return cls.model_validate(obj)\n\n    @classmethod\n    @typing_extensions.deprecated('The `construct` method is deprecated; use `model_construct` instead.', category=None)\n    def construct(cls, _fields_set: set[str] | None = None, **values: Any) -> Self:  # noqa: D102\n        warnings.warn(\n            'The `construct` method is deprecated; use `model_construct` instead.', category=PydanticDeprecatedSince20\n        )\n        return cls.model_construct(_fields_set=_fields_set, **values)\n\n    @typing_extensions.deprecated(\n        'The `copy` method is deprecated; use `model_copy` instead. '\n        'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.',\n        category=None,\n    )\n    def copy(\n        self,\n        *,\n        include: AbstractSetIntStr | MappingIntStrAny | None = None,\n        exclude: AbstractSetIntStr | MappingIntStrAny | None = None,\n        update: Dict[str, Any] | None = None,  # noqa UP006\n        deep: bool = False,\n    ) -> Self:  # pragma: no cover\n        \"\"\"Returns a copy of the model.\n\n        !!! warning \"Deprecated\"\n            This method is now deprecated; use `model_copy` instead.\n\n        If you need `include` or `exclude`, use:\n\n        ```py\n        data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n        data = {**data, **(update or {})}\n        copied = self.model_validate(data)\n        ```\n\n        Args:\n            include: Optional set or mapping specifying which fields to include in the copied model.\n            exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n            update: Optional dictionary of field-value pairs to override field values in the copied model.\n            deep: If True, the values of fields that are Pydantic models will be deep-copied.\n\n        Returns:\n            A copy of the model with included, excluded and updated fields as specified.\n        \"\"\"\n        warnings.warn(\n            'The `copy` method is deprecated; use `model_copy` instead. '\n            'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.',\n            category=PydanticDeprecatedSince20,\n        )\n        from .deprecated import copy_internals\n\n        values = dict(\n            copy_internals._iter(\n                self, to_dict=False, by_alias=False, include=include, exclude=exclude, exclude_unset=False\n            ),\n            **(update or {}),\n        )\n        if self.__pydantic_private__ is None:\n            private = None\n        else:\n            private = {k: v for k, v in self.__pydantic_private__.items() if v is not PydanticUndefined}\n\n        if self.__pydantic_extra__ is None:\n            extra: dict[str, Any] | None = None\n        else:\n            extra = self.__pydantic_extra__.copy()\n            for k in list(self.__pydantic_extra__):\n                if k not in values:  # k was in the exclude\n                    extra.pop(k)\n            for k in list(values):\n                if k in self.__pydantic_extra__:  # k must have come from extra\n                    extra[k] = values.pop(k)\n\n        # new `__pydantic_fields_set__` can have unset optional fields with a set value in `update` kwarg\n        if update:\n            fields_set = self.__pydantic_fields_set__ | update.keys()\n        else:\n            fields_set = set(self.__pydantic_fields_set__)\n\n        # removing excluded fields from `__pydantic_fields_set__`\n        if exclude:\n            fields_set -= set(exclude)\n\n        return copy_internals._copy_and_set_values(self, values, fields_set, extra, private, deep=deep)\n\n    @classmethod\n    @typing_extensions.deprecated('The `schema` method is deprecated; use `model_json_schema` instead.', category=None)\n    def schema(  # noqa: D102\n        cls, by_alias: bool = True, ref_template: str = DEFAULT_REF_TEMPLATE\n    ) -> Dict[str, Any]:  # noqa UP006\n        warnings.warn(\n            'The `schema` method is deprecated; use `model_json_schema` instead.', category=PydanticDeprecatedSince20\n        )\n        return cls.model_json_schema(by_alias=by_alias, ref_template=ref_template)\n\n    @classmethod\n    @typing_extensions.deprecated(\n        'The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead.',\n        category=None,\n    )\n    def schema_json(  # noqa: D102\n        cls, *, by_alias: bool = True, ref_template: str = DEFAULT_REF_TEMPLATE, **dumps_kwargs: Any\n    ) -> str:  # pragma: no cover\n        warnings.warn(\n            'The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead.',\n            category=PydanticDeprecatedSince20,\n        )\n        import json\n\n        from .deprecated.json import pydantic_encoder\n\n        return json.dumps(\n            cls.model_json_schema(by_alias=by_alias, ref_template=ref_template),\n            default=pydantic_encoder,\n            **dumps_kwargs,\n        )\n\n    @classmethod\n    @typing_extensions.deprecated('The `validate` method is deprecated; use `model_validate` instead.', category=None)\n    def validate(cls, value: Any) -> Self:  # noqa: D102\n        warnings.warn(\n            'The `validate` method is deprecated; use `model_validate` instead.', category=PydanticDeprecatedSince20\n        )\n        return cls.model_validate(value)\n\n    @classmethod\n    @typing_extensions.deprecated(\n        'The `update_forward_refs` method is deprecated; use `model_rebuild` instead.',\n        category=None,\n    )\n    def update_forward_refs(cls, **localns: Any) -> None:  # noqa: D102\n        warnings.warn(\n            'The `update_forward_refs` method is deprecated; use `model_rebuild` instead.',\n            category=PydanticDeprecatedSince20,\n        )\n        if localns:  # pragma: no cover\n            raise TypeError('`localns` arguments are not longer accepted.')\n        cls.model_rebuild(force=True)\n\n    @typing_extensions.deprecated(\n        'The private method `_iter` will be removed and should no longer be used.', category=None\n    )\n    def _iter(self, *args: Any, **kwargs: Any) -> Any:\n        warnings.warn(\n            'The private method `_iter` will be removed and should no longer be used.',\n            category=PydanticDeprecatedSince20,\n        )\n        from .deprecated import copy_internals\n\n        return copy_internals._iter(self, *args, **kwargs)\n\n    @typing_extensions.deprecated(\n        'The private method `_copy_and_set_values` will be removed and should no longer be used.',\n        category=None,\n    )\n    def _copy_and_set_values(self, *args: Any, **kwargs: Any) -> Any:\n        warnings.warn(\n            'The private method `_copy_and_set_values` will be removed and should no longer be used.',\n            category=PydanticDeprecatedSince20,\n        )\n        from .deprecated import copy_internals\n\n        return copy_internals._copy_and_set_values(self, *args, **kwargs)\n\n    @classmethod\n    @typing_extensions.deprecated(\n        'The private method `_get_value` will be removed and should no longer be used.',\n        category=None,\n    )\n    def _get_value(cls, *args: Any, **kwargs: Any) -> Any:\n        warnings.warn(\n            'The private method `_get_value` will be removed and should no longer be used.',\n            category=PydanticDeprecatedSince20,\n        )\n        from .deprecated import copy_internals\n\n        return copy_internals._get_value(cls, *args, **kwargs)\n\n    @typing_extensions.deprecated(\n        'The private method `_calculate_keys` will be removed and should no longer be used.',\n        category=None,\n    )\n    def _calculate_keys(self, *args: Any, **kwargs: Any) -> Any:\n        warnings.warn(\n            'The private method `_calculate_keys` will be removed and should no longer be used.',\n            category=PydanticDeprecatedSince20,\n        )\n        from .deprecated import copy_internals\n\n        return copy_internals._calculate_keys(self, *args, **kwargs)\n\n\n@overload\ndef create_model(\n    model_name: str,\n    /,\n    *,\n    __config__: ConfigDict | None = None,\n    __doc__: str | None = None,\n    __base__: None = None,\n    __module__: str = __name__,\n    __validators__: dict[str, Callable[..., Any]] | None = None,\n    __cls_kwargs__: dict[str, Any] | None = None,\n    **field_definitions: Any,\n) -> type[BaseModel]: ...\n\n\n@overload\ndef create_model(\n    model_name: str,\n    /,\n    *,\n    __config__: ConfigDict | None = None,\n    __doc__: str | None = None,\n    __base__: type[ModelT] | tuple[type[ModelT], ...],\n    __module__: str = __name__,\n    __validators__: dict[str, Callable[..., Any]] | None = None,\n    __cls_kwargs__: dict[str, Any] | None = None,\n    **field_definitions: Any,\n) -> type[ModelT]: ...\n\n\ndef create_model(  # noqa: C901\n    model_name: str,\n    /,\n    *,\n    __config__: ConfigDict | None = None,\n    __doc__: str | None = None,\n    __base__: type[ModelT] | tuple[type[ModelT], ...] | None = None,\n    __module__: str | None = None,\n    __validators__: dict[str, Callable[..., Any]] | None = None,\n    __cls_kwargs__: dict[str, Any] | None = None,\n    __slots__: tuple[str, ...] | None = None,\n    **field_definitions: Any,\n) -> type[ModelT]:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/models/#dynamic-model-creation\n\n    Dynamically creates and returns a new Pydantic model, in other words, `create_model` dynamically creates a\n    subclass of [`BaseModel`][pydantic.BaseModel].\n\n    Args:\n        model_name: The name of the newly created model.\n        __config__: The configuration of the new model.\n        __doc__: The docstring of the new model.\n        __base__: The base class or classes for the new model.\n        __module__: The name of the module that the model belongs to;\n            if `None`, the value is taken from `sys._getframe(1)`\n        __validators__: A dictionary of methods that validate fields. The keys are the names of the validation methods to\n            be added to the model, and the values are the validation methods themselves. You can read more about functional\n            validators [here](https://docs.pydantic.dev/2.8/concepts/validators/#field-validators).\n        __cls_kwargs__: A dictionary of keyword arguments for class creation, such as `metaclass`.\n        __slots__: Deprecated. Should not be passed to `create_model`.\n        **field_definitions: Attributes of the new model. They should be passed in the format:\n            `<name>=(<type>, <default value>)`, `<name>=(<type>, <FieldInfo>)`, or `typing.Annotated[<type>, <FieldInfo>]`.\n            Any additional metadata in `typing.Annotated[<type>, <FieldInfo>, ...]` will be ignored.\n\n    Returns:\n        The new [model][pydantic.BaseModel].\n\n    Raises:\n        PydanticUserError: If `__base__` and `__config__` are both passed.\n    \"\"\"\n    if __slots__ is not None:\n        # __slots__ will be ignored from here on\n        warnings.warn('__slots__ should not be passed to create_model', RuntimeWarning)\n\n    if __base__ is not None:\n        if __config__ is not None:\n            raise PydanticUserError(\n                'to avoid confusion `__config__` and `__base__` cannot be used together',\n                code='create-model-config-base',\n            )\n        if not isinstance(__base__, tuple):\n            __base__ = (__base__,)\n    else:\n        __base__ = (cast('type[ModelT]', BaseModel),)\n\n    __cls_kwargs__ = __cls_kwargs__ or {}\n\n    fields = {}\n    annotations = {}\n\n    for f_name, f_def in field_definitions.items():\n        if not _fields.is_valid_field_name(f_name):\n            warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n        if isinstance(f_def, tuple):\n            f_def = cast('tuple[str, Any]', f_def)\n            try:\n                f_annotation, f_value = f_def\n            except ValueError as e:\n                raise PydanticUserError(\n                    'Field definitions should be a `(<type>, <default>)`.',\n                    code='create-model-field-definitions',\n                ) from e\n\n        elif _typing_extra.is_annotated(f_def):\n            (f_annotation, f_value, *_) = typing_extensions.get_args(\n                f_def\n            )  # first two input are expected from Annotated, refer to https://docs.python.org/3/library/typing.html#typing.Annotated\n            from .fields import FieldInfo\n\n            if not isinstance(f_value, FieldInfo):\n                raise PydanticUserError(\n                    'Field definitions should be a Annotated[<type>, <FieldInfo>]',\n                    code='create-model-field-definitions',\n                )\n\n        else:\n            f_annotation, f_value = None, f_def\n\n        if f_annotation:\n            annotations[f_name] = f_annotation\n        fields[f_name] = f_value\n\n    if __module__ is None:\n        f = sys._getframe(1)\n        __module__ = f.f_globals['__name__']\n\n    namespace: dict[str, Any] = {'__annotations__': annotations, '__module__': __module__}\n    if __doc__:\n        namespace.update({'__doc__': __doc__})\n    if __validators__:\n        namespace.update(__validators__)\n    namespace.update(fields)\n    if __config__:\n        namespace['model_config'] = _config.ConfigWrapper(__config__).config_dict\n    resolved_bases = types.resolve_bases(__base__)\n    meta, ns, kwds = types.prepare_class(model_name, resolved_bases, kwds=__cls_kwargs__)\n    if resolved_bases is not __base__:\n        ns['__orig_bases__'] = __base__\n    namespace.update(ns)\n\n    return meta(\n        model_name,\n        resolved_bases,\n        namespace,\n        __pydantic_reset_parent_namespace__=False,\n        _create_model_module=__module__,\n        **kwds,\n    )\n\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/aliases.py": "\"\"\"Support for alias configurations.\"\"\"\n\nfrom __future__ import annotations\n\nimport dataclasses\nfrom typing import Any, Callable, Literal\n\nfrom pydantic_core import PydanticUndefined\n\nfrom ._internal import _internal_dataclass\n\n__all__ = ('AliasGenerator', 'AliasPath', 'AliasChoices')\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass AliasPath:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/alias#aliaspath-and-aliaschoices\n\n    A data class used by `validation_alias` as a convenience to create aliases.\n\n    Attributes:\n        path: A list of string or integer aliases.\n    \"\"\"\n\n    path: list[int | str]\n\n    def __init__(self, first_arg: str, *args: str | int) -> None:\n        self.path = [first_arg] + list(args)\n\n    def convert_to_aliases(self) -> list[str | int]:\n        \"\"\"Converts arguments to a list of string or integer aliases.\n\n        Returns:\n            The list of aliases.\n        \"\"\"\n        return self.path\n\n    def search_dict_for_path(self, d: dict) -> Any:\n        \"\"\"Searches a dictionary for the path specified by the alias.\n\n        Returns:\n            The value at the specified path, or `PydanticUndefined` if the path is not found.\n        \"\"\"\n        v = d\n        for k in self.path:\n            if isinstance(v, str):\n                # disallow indexing into a str, like for AliasPath('x', 0) and x='abc'\n                return PydanticUndefined\n            try:\n                v = v[k]\n            except (KeyError, IndexError, TypeError):\n                return PydanticUndefined\n        return v\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass AliasChoices:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/alias#aliaspath-and-aliaschoices\n\n    A data class used by `validation_alias` as a convenience to create aliases.\n\n    Attributes:\n        choices: A list containing a string or `AliasPath`.\n    \"\"\"\n\n    choices: list[str | AliasPath]\n\n    def __init__(self, first_choice: str | AliasPath, *choices: str | AliasPath) -> None:\n        self.choices = [first_choice] + list(choices)\n\n    def convert_to_aliases(self) -> list[list[str | int]]:\n        \"\"\"Converts arguments to a list of lists containing string or integer aliases.\n\n        Returns:\n            The list of aliases.\n        \"\"\"\n        aliases: list[list[str | int]] = []\n        for c in self.choices:\n            if isinstance(c, AliasPath):\n                aliases.append(c.convert_to_aliases())\n            else:\n                aliases.append([c])\n        return aliases\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass AliasGenerator:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/alias#using-an-aliasgenerator\n\n    A data class used by `alias_generator` as a convenience to create various aliases.\n\n    Attributes:\n        alias: A callable that takes a field name and returns an alias for it.\n        validation_alias: A callable that takes a field name and returns a validation alias for it.\n        serialization_alias: A callable that takes a field name and returns a serialization alias for it.\n    \"\"\"\n\n    alias: Callable[[str], str] | None = None\n    validation_alias: Callable[[str], str | AliasPath | AliasChoices] | None = None\n    serialization_alias: Callable[[str], str] | None = None\n\n    def _generate_alias(\n        self,\n        alias_kind: Literal['alias', 'validation_alias', 'serialization_alias'],\n        allowed_types: tuple[type[str] | type[AliasPath] | type[AliasChoices], ...],\n        field_name: str,\n    ) -> str | AliasPath | AliasChoices | None:\n        \"\"\"Generate an alias of the specified kind. Returns None if the alias generator is None.\n\n        Raises:\n            TypeError: If the alias generator produces an invalid type.\n        \"\"\"\n        alias = None\n        if alias_generator := getattr(self, alias_kind):\n            alias = alias_generator(field_name)\n            if alias and not isinstance(alias, allowed_types):\n                raise TypeError(\n                    f'Invalid `{alias_kind}` type. `{alias_kind}` generator must produce one of `{allowed_types}`'\n                )\n        return alias\n\n    def generate_aliases(self, field_name: str) -> tuple[str | None, str | AliasPath | AliasChoices | None, str | None]:\n        \"\"\"Generate `alias`, `validation_alias`, and `serialization_alias` for a field.\n\n        Returns:\n            A tuple of three aliases - validation, alias, and serialization.\n        \"\"\"\n        alias = self._generate_alias('alias', (str,), field_name)\n        validation_alias = self._generate_alias('validation_alias', (str, AliasChoices, AliasPath), field_name)\n        serialization_alias = self._generate_alias('serialization_alias', (str,), field_name)\n\n        return alias, validation_alias, serialization_alias  # type: ignore\n", "pydantic/dataclasses.py": "\"\"\"Provide an enhanced dataclass that performs validation.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport sys\nimport types\nfrom typing import TYPE_CHECKING, Any, Callable, Generic, NoReturn, TypeVar, overload\n\nfrom typing_extensions import Literal, TypeGuard, dataclass_transform\n\nfrom ._internal import _config, _decorators, _typing_extra\nfrom ._internal import _dataclasses as _pydantic_dataclasses\nfrom ._migration import getattr_migration\nfrom .config import ConfigDict\nfrom .errors import PydanticUserError\nfrom .fields import Field, FieldInfo, PrivateAttr\n\nif TYPE_CHECKING:\n    from ._internal._dataclasses import PydanticDataclass\n\n__all__ = 'dataclass', 'rebuild_dataclass'\n\n_T = TypeVar('_T')\n\nif sys.version_info >= (3, 10):\n\n    @dataclass_transform(field_specifiers=(dataclasses.field, Field, PrivateAttr))\n    @overload\n    def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:  # type: ignore\n        ...\n\n    @dataclass_transform(field_specifiers=(dataclasses.field, Field, PrivateAttr))\n    @overload\n    def dataclass(\n        _cls: type[_T],  # type: ignore\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...\n\nelse:\n\n    @dataclass_transform(field_specifiers=(dataclasses.field, Field, PrivateAttr))\n    @overload\n    def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:  # type: ignore\n        ...\n\n    @dataclass_transform(field_specifiers=(dataclasses.field, Field, PrivateAttr))\n    @overload\n    def dataclass(\n        _cls: type[_T],  # type: ignore\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...\n\n\n@dataclass_transform(field_specifiers=(dataclasses.field, Field, PrivateAttr))\ndef dataclass(  # noqa: C901\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/dataclasses/\n\n    A decorator used to create a Pydantic-enhanced dataclass, similar to the standard Python `dataclass`,\n    but with added validation.\n\n    This function should be used similarly to `dataclasses.dataclass`.\n\n    Args:\n        _cls: The target `dataclass`.\n        init: Included for signature compatibility with `dataclasses.dataclass`, and is passed through to\n            `dataclasses.dataclass` when appropriate. If specified, must be set to `False`, as pydantic inserts its\n            own  `__init__` function.\n        repr: A boolean indicating whether to include the field in the `__repr__` output.\n        eq: Determines if a `__eq__` method should be generated for the class.\n        order: Determines if comparison magic methods should be generated, such as `__lt__`, but not `__eq__`.\n        unsafe_hash: Determines if a `__hash__` method should be included in the class, as in `dataclasses.dataclass`.\n        frozen: Determines if the generated class should be a 'frozen' `dataclass`, which does not allow its\n            attributes to be modified after it has been initialized.\n        config: The Pydantic config to use for the `dataclass`.\n        validate_on_init: A deprecated parameter included for backwards compatibility; in V2, all Pydantic dataclasses\n            are validated on init.\n        kw_only: Determines if `__init__` method parameters must be specified by keyword only. Defaults to `False`.\n        slots: Determines if the generated class should be a 'slots' `dataclass`, which does not allow the addition of\n            new attributes after instantiation.\n\n    Returns:\n        A decorator that accepts a class as its argument and returns a Pydantic `dataclass`.\n\n    Raises:\n        AssertionError: Raised if `init` is not `False` or `validate_on_init` is `False`.\n    \"\"\"\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n\n    if sys.version_info >= (3, 10):\n        kwargs = dict(kw_only=kw_only, slots=slots)\n    else:\n        kwargs = {}\n\n    def make_pydantic_fields_compatible(cls: type[Any]) -> None:\n        \"\"\"Make sure that stdlib `dataclasses` understands `Field` kwargs like `kw_only`\n        To do that, we simply change\n          `x: int = pydantic.Field(..., kw_only=True)`\n        into\n          `x: int = dataclasses.field(default=pydantic.Field(..., kw_only=True), kw_only=True)`\n        \"\"\"\n        for annotation_cls in cls.__mro__:\n            # In Python < 3.9, `__annotations__` might not be present if there are no fields.\n            # we therefore need to use `getattr` to avoid an `AttributeError`.\n            annotations = getattr(annotation_cls, '__annotations__', [])\n            for field_name in annotations:\n                field_value = getattr(cls, field_name, None)\n                # Process only if this is an instance of `FieldInfo`.\n                if not isinstance(field_value, FieldInfo):\n                    continue\n\n                # Initialize arguments for the standard `dataclasses.field`.\n                field_args: dict = {'default': field_value}\n\n                # Handle `kw_only` for Python 3.10+\n                if sys.version_info >= (3, 10) and field_value.kw_only:\n                    field_args['kw_only'] = True\n\n                # Set `repr` attribute if it's explicitly specified to be not `True`.\n                if field_value.repr is not True:\n                    field_args['repr'] = field_value.repr\n\n                setattr(cls, field_name, dataclasses.field(**field_args))\n                # In Python 3.8, dataclasses checks cls.__dict__['__annotations__'] for annotations,\n                # so we must make sure it's initialized before we add to it.\n                if cls.__dict__.get('__annotations__') is None:\n                    cls.__annotations__ = {}\n                cls.__annotations__[field_name] = annotations[field_name]\n\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        \"\"\"Create a Pydantic dataclass from a regular dataclass.\n\n        Args:\n            cls: The class to create the Pydantic dataclass from.\n\n        Returns:\n            A Pydantic dataclass.\n        \"\"\"\n        from ._internal._utils import is_model_class\n\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n\n        original_cls = cls\n\n        config_dict = config\n        if config_dict is None:\n            # if not explicitly provided, read from the type\n            cls_config = getattr(cls, '__pydantic_config__', None)\n            if cls_config is not None:\n                config_dict = cls_config\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls)\n\n        # Keep track of the original __doc__ so that we can restore it after applying the dataclasses decorator\n        # Otherwise, classes with no __doc__ will have their signature added into the JSON schema description,\n        # since dataclasses.dataclass will set this as the __doc__\n        original_doc = cls.__doc__\n\n        if _pydantic_dataclasses.is_builtin_dataclass(cls):\n            # Don't preserve the docstring for vanilla dataclasses, as it may include the signature\n            # This matches v1 behavior, and there was an explicit test for it\n            original_doc = None\n\n            # We don't want to add validation to the existing std lib dataclass, so we will subclass it\n            #   If the class is generic, we need to make sure the subclass also inherits from Generic\n            #   with all the same parameters.\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]  # type: ignore\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n\n        make_pydantic_fields_compatible(cls)\n\n        cls = dataclasses.dataclass(  # type: ignore[call-overload]\n            cls,\n            # the value of init here doesn't affect anything except that it makes it easier to generate a signature\n            init=True,\n            repr=repr,\n            eq=eq,\n            order=order,\n            unsafe_hash=unsafe_hash,\n            frozen=frozen,\n            **kwargs,\n        )\n\n        cls.__pydantic_decorators__ = decorators  # type: ignore\n        cls.__doc__ = original_doc\n        cls.__module__ = original_cls.__module__\n        cls.__qualname__ = original_cls.__qualname__\n        pydantic_complete = _pydantic_dataclasses.complete_dataclass(\n            cls, config_wrapper, raise_errors=False, types_namespace=None\n        )\n        cls.__pydantic_complete__ = pydantic_complete  # type: ignore\n        return cls\n\n    if _cls is None:\n        return create_dataclass\n\n    return create_dataclass(_cls)\n\n\n__getattr__ = getattr_migration(__name__)\n\nif (3, 8) <= sys.version_info < (3, 11):\n    # Monkeypatch dataclasses.InitVar so that typing doesn't error if it occurs as a type when evaluating type hints\n    # Starting in 3.11, typing.get_type_hints will not raise an error if the retrieved type hints are not callable.\n\n    def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        \"\"\"This function does nothing but raise an error that is as similar as possible to what you'd get\n        if you were to try calling `InitVar[int]()` without this monkeypatch. The whole purpose is just\n        to ensure typing._type_check does not error if the type hint evaluates to `InitVar[<parameter>]`.\n        \"\"\"\n        raise TypeError(\"'InitVar' object is not callable\")\n\n    dataclasses.InitVar.__call__ = _call_initvar\n\n\ndef rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: dict[str, Any] | None = None,\n) -> bool | None:\n    \"\"\"Try to rebuild the pydantic-core schema for the dataclass.\n\n    This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n    the initial attempt to build the schema, and automatic rebuilding fails.\n\n    This is analogous to `BaseModel.model_rebuild`.\n\n    Args:\n        cls: The class to rebuild the pydantic-core schema for.\n        force: Whether to force the rebuilding of the schema, defaults to `False`.\n        raise_errors: Whether to raise errors, defaults to `True`.\n        _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n        _types_namespace: The types namespace, defaults to `None`.\n\n    Returns:\n        Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n        If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n    \"\"\"\n    if not force and cls.__pydantic_complete__:\n        return None\n    else:\n        if _types_namespace is not None:\n            types_namespace: dict[str, Any] | None = _types_namespace.copy()\n        else:\n            if _parent_namespace_depth > 0:\n                frame_parent_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth) or {}\n                # Note: we may need to add something similar to cls.__pydantic_parent_namespace__ from BaseModel\n                #   here when implementing handling of recursive generics. See BaseModel.model_rebuild for reference.\n                types_namespace = frame_parent_ns\n            else:\n                types_namespace = {}\n\n            types_namespace = _typing_extra.get_cls_types_namespace(cls, types_namespace)\n        return _pydantic_dataclasses.complete_dataclass(\n            cls,\n            _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n            raise_errors=raise_errors,\n            types_namespace=types_namespace,\n        )\n\n\ndef is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    \"\"\"Whether a class is a pydantic dataclass.\n\n    Args:\n        class_: The class.\n\n    Returns:\n        `True` if the class is a pydantic dataclass, `False` otherwise.\n    \"\"\"\n    try:\n        return '__pydantic_validator__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False\n", "pydantic/class_validators.py": "\"\"\"`class_validators` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/__init__.py": "import typing\n\nfrom ._migration import getattr_migration\nfrom .version import VERSION\n\nif typing.TYPE_CHECKING:\n    # import of virtually everything is supported via `__getattr__` below,\n    # but we need them here for type checking and IDE support\n    import pydantic_core\n    from pydantic_core.core_schema import (\n        FieldSerializationInfo,\n        SerializationInfo,\n        SerializerFunctionWrapHandler,\n        ValidationInfo,\n        ValidatorFunctionWrapHandler,\n    )\n\n    from . import dataclasses\n    from ._internal._generate_schema import GenerateSchema as GenerateSchema\n    from .aliases import AliasChoices, AliasGenerator, AliasPath\n    from .annotated_handlers import GetCoreSchemaHandler, GetJsonSchemaHandler\n    from .config import ConfigDict, with_config\n    from .errors import *\n    from .fields import Field, PrivateAttr, computed_field\n    from .functional_serializers import (\n        PlainSerializer,\n        SerializeAsAny,\n        WrapSerializer,\n        field_serializer,\n        model_serializer,\n    )\n    from .functional_validators import (\n        AfterValidator,\n        BeforeValidator,\n        InstanceOf,\n        PlainValidator,\n        SkipValidation,\n        WrapValidator,\n        field_validator,\n        model_validator,\n    )\n    from .json_schema import WithJsonSchema\n    from .main import *\n    from .networks import *\n    from .type_adapter import TypeAdapter\n    from .types import *\n    from .validate_call_decorator import validate_call\n    from .warnings import (\n        PydanticDeprecatedSince20,\n        PydanticDeprecatedSince26,\n        PydanticDeprecationWarning,\n        PydanticExperimentalWarning,\n    )\n\n    # this encourages pycharm to import `ValidationError` from here, not pydantic_core\n    ValidationError = pydantic_core.ValidationError\n    from .deprecated.class_validators import root_validator, validator\n    from .deprecated.config import BaseConfig, Extra\n    from .deprecated.tools import *\n    from .root_model import RootModel\n\n__version__ = VERSION\n__all__ = (\n    # dataclasses\n    'dataclasses',\n    # functional validators\n    'field_validator',\n    'model_validator',\n    'AfterValidator',\n    'BeforeValidator',\n    'PlainValidator',\n    'WrapValidator',\n    'SkipValidation',\n    'InstanceOf',\n    # JSON Schema\n    'WithJsonSchema',\n    # deprecated V1 functional validators, these are imported via `__getattr__` below\n    'root_validator',\n    'validator',\n    # functional serializers\n    'field_serializer',\n    'model_serializer',\n    'PlainSerializer',\n    'SerializeAsAny',\n    'WrapSerializer',\n    # config\n    'ConfigDict',\n    'with_config',\n    # deprecated V1 config, these are imported via `__getattr__` below\n    'BaseConfig',\n    'Extra',\n    # validate_call\n    'validate_call',\n    # errors\n    'PydanticErrorCodes',\n    'PydanticUserError',\n    'PydanticSchemaGenerationError',\n    'PydanticImportError',\n    'PydanticUndefinedAnnotation',\n    'PydanticInvalidForJsonSchema',\n    # fields\n    'Field',\n    'computed_field',\n    'PrivateAttr',\n    # alias\n    'AliasChoices',\n    'AliasGenerator',\n    'AliasPath',\n    # main\n    'BaseModel',\n    'create_model',\n    # network\n    'AnyUrl',\n    'AnyHttpUrl',\n    'FileUrl',\n    'HttpUrl',\n    'FtpUrl',\n    'WebsocketUrl',\n    'AnyWebsocketUrl',\n    'UrlConstraints',\n    'EmailStr',\n    'NameEmail',\n    'IPvAnyAddress',\n    'IPvAnyInterface',\n    'IPvAnyNetwork',\n    'PostgresDsn',\n    'CockroachDsn',\n    'AmqpDsn',\n    'RedisDsn',\n    'MongoDsn',\n    'KafkaDsn',\n    'NatsDsn',\n    'MySQLDsn',\n    'MariaDBDsn',\n    'ClickHouseDsn',\n    'validate_email',\n    # root_model\n    'RootModel',\n    # deprecated tools, these are imported via `__getattr__` below\n    'parse_obj_as',\n    'schema_of',\n    'schema_json_of',\n    # types\n    'Strict',\n    'StrictStr',\n    'conbytes',\n    'conlist',\n    'conset',\n    'confrozenset',\n    'constr',\n    'StringConstraints',\n    'ImportString',\n    'conint',\n    'PositiveInt',\n    'NegativeInt',\n    'NonNegativeInt',\n    'NonPositiveInt',\n    'confloat',\n    'PositiveFloat',\n    'NegativeFloat',\n    'NonNegativeFloat',\n    'NonPositiveFloat',\n    'FiniteFloat',\n    'condecimal',\n    'condate',\n    'UUID1',\n    'UUID3',\n    'UUID4',\n    'UUID5',\n    'FilePath',\n    'DirectoryPath',\n    'NewPath',\n    'Json',\n    'Secret',\n    'SecretStr',\n    'SecretBytes',\n    'StrictBool',\n    'StrictBytes',\n    'StrictInt',\n    'StrictFloat',\n    'PaymentCardNumber',\n    'ByteSize',\n    'PastDate',\n    'FutureDate',\n    'PastDatetime',\n    'FutureDatetime',\n    'AwareDatetime',\n    'NaiveDatetime',\n    'AllowInfNan',\n    'EncoderProtocol',\n    'EncodedBytes',\n    'EncodedStr',\n    'Base64Encoder',\n    'Base64Bytes',\n    'Base64Str',\n    'Base64UrlBytes',\n    'Base64UrlStr',\n    'GetPydanticSchema',\n    'Tag',\n    'Discriminator',\n    'JsonValue',\n    # type_adapter\n    'TypeAdapter',\n    # version\n    '__version__',\n    'VERSION',\n    # warnings\n    'PydanticDeprecatedSince20',\n    'PydanticDeprecatedSince26',\n    'PydanticDeprecationWarning',\n    'PydanticExperimentalWarning',\n    # annotated handlers\n    'GetCoreSchemaHandler',\n    'GetJsonSchemaHandler',\n    # generate schema from ._internal\n    'GenerateSchema',\n    # pydantic_core\n    'ValidationError',\n    'ValidationInfo',\n    'SerializationInfo',\n    'ValidatorFunctionWrapHandler',\n    'FieldSerializationInfo',\n    'SerializerFunctionWrapHandler',\n    'OnErrorOmit',\n)\n\n# A mapping of {<member name>: (package, <module name>)} defining dynamic imports\n_dynamic_imports: 'dict[str, tuple[str, str]]' = {\n    'dataclasses': (__spec__.parent, '__module__'),\n    # functional validators\n    'field_validator': (__spec__.parent, '.functional_validators'),\n    'model_validator': (__spec__.parent, '.functional_validators'),\n    'AfterValidator': (__spec__.parent, '.functional_validators'),\n    'BeforeValidator': (__spec__.parent, '.functional_validators'),\n    'PlainValidator': (__spec__.parent, '.functional_validators'),\n    'WrapValidator': (__spec__.parent, '.functional_validators'),\n    'SkipValidation': (__spec__.parent, '.functional_validators'),\n    'InstanceOf': (__spec__.parent, '.functional_validators'),\n    # JSON Schema\n    'WithJsonSchema': (__spec__.parent, '.json_schema'),\n    # functional serializers\n    'field_serializer': (__spec__.parent, '.functional_serializers'),\n    'model_serializer': (__spec__.parent, '.functional_serializers'),\n    'PlainSerializer': (__spec__.parent, '.functional_serializers'),\n    'SerializeAsAny': (__spec__.parent, '.functional_serializers'),\n    'WrapSerializer': (__spec__.parent, '.functional_serializers'),\n    # config\n    'ConfigDict': (__spec__.parent, '.config'),\n    'with_config': (__spec__.parent, '.config'),\n    # validate call\n    'validate_call': (__spec__.parent, '.validate_call_decorator'),\n    # errors\n    'PydanticErrorCodes': (__spec__.parent, '.errors'),\n    'PydanticUserError': (__spec__.parent, '.errors'),\n    'PydanticSchemaGenerationError': (__spec__.parent, '.errors'),\n    'PydanticImportError': (__spec__.parent, '.errors'),\n    'PydanticUndefinedAnnotation': (__spec__.parent, '.errors'),\n    'PydanticInvalidForJsonSchema': (__spec__.parent, '.errors'),\n    # fields\n    'Field': (__spec__.parent, '.fields'),\n    'computed_field': (__spec__.parent, '.fields'),\n    'PrivateAttr': (__spec__.parent, '.fields'),\n    # alias\n    'AliasChoices': (__spec__.parent, '.aliases'),\n    'AliasGenerator': (__spec__.parent, '.aliases'),\n    'AliasPath': (__spec__.parent, '.aliases'),\n    # main\n    'BaseModel': (__spec__.parent, '.main'),\n    'create_model': (__spec__.parent, '.main'),\n    # network\n    'AnyUrl': (__spec__.parent, '.networks'),\n    'AnyHttpUrl': (__spec__.parent, '.networks'),\n    'FileUrl': (__spec__.parent, '.networks'),\n    'HttpUrl': (__spec__.parent, '.networks'),\n    'FtpUrl': (__spec__.parent, '.networks'),\n    'WebsocketUrl': (__spec__.parent, '.networks'),\n    'AnyWebsocketUrl': (__spec__.parent, '.networks'),\n    'UrlConstraints': (__spec__.parent, '.networks'),\n    'EmailStr': (__spec__.parent, '.networks'),\n    'NameEmail': (__spec__.parent, '.networks'),\n    'IPvAnyAddress': (__spec__.parent, '.networks'),\n    'IPvAnyInterface': (__spec__.parent, '.networks'),\n    'IPvAnyNetwork': (__spec__.parent, '.networks'),\n    'PostgresDsn': (__spec__.parent, '.networks'),\n    'CockroachDsn': (__spec__.parent, '.networks'),\n    'AmqpDsn': (__spec__.parent, '.networks'),\n    'RedisDsn': (__spec__.parent, '.networks'),\n    'MongoDsn': (__spec__.parent, '.networks'),\n    'KafkaDsn': (__spec__.parent, '.networks'),\n    'NatsDsn': (__spec__.parent, '.networks'),\n    'MySQLDsn': (__spec__.parent, '.networks'),\n    'MariaDBDsn': (__spec__.parent, '.networks'),\n    'ClickHouseDsn': (__spec__.parent, '.networks'),\n    'validate_email': (__spec__.parent, '.networks'),\n    # root_model\n    'RootModel': (__spec__.parent, '.root_model'),\n    # types\n    'Strict': (__spec__.parent, '.types'),\n    'StrictStr': (__spec__.parent, '.types'),\n    'conbytes': (__spec__.parent, '.types'),\n    'conlist': (__spec__.parent, '.types'),\n    'conset': (__spec__.parent, '.types'),\n    'confrozenset': (__spec__.parent, '.types'),\n    'constr': (__spec__.parent, '.types'),\n    'StringConstraints': (__spec__.parent, '.types'),\n    'ImportString': (__spec__.parent, '.types'),\n    'conint': (__spec__.parent, '.types'),\n    'PositiveInt': (__spec__.parent, '.types'),\n    'NegativeInt': (__spec__.parent, '.types'),\n    'NonNegativeInt': (__spec__.parent, '.types'),\n    'NonPositiveInt': (__spec__.parent, '.types'),\n    'confloat': (__spec__.parent, '.types'),\n    'PositiveFloat': (__spec__.parent, '.types'),\n    'NegativeFloat': (__spec__.parent, '.types'),\n    'NonNegativeFloat': (__spec__.parent, '.types'),\n    'NonPositiveFloat': (__spec__.parent, '.types'),\n    'FiniteFloat': (__spec__.parent, '.types'),\n    'condecimal': (__spec__.parent, '.types'),\n    'condate': (__spec__.parent, '.types'),\n    'UUID1': (__spec__.parent, '.types'),\n    'UUID3': (__spec__.parent, '.types'),\n    'UUID4': (__spec__.parent, '.types'),\n    'UUID5': (__spec__.parent, '.types'),\n    'FilePath': (__spec__.parent, '.types'),\n    'DirectoryPath': (__spec__.parent, '.types'),\n    'NewPath': (__spec__.parent, '.types'),\n    'Json': (__spec__.parent, '.types'),\n    'Secret': (__spec__.parent, '.types'),\n    'SecretStr': (__spec__.parent, '.types'),\n    'SecretBytes': (__spec__.parent, '.types'),\n    'StrictBool': (__spec__.parent, '.types'),\n    'StrictBytes': (__spec__.parent, '.types'),\n    'StrictInt': (__spec__.parent, '.types'),\n    'StrictFloat': (__spec__.parent, '.types'),\n    'PaymentCardNumber': (__spec__.parent, '.types'),\n    'ByteSize': (__spec__.parent, '.types'),\n    'PastDate': (__spec__.parent, '.types'),\n    'FutureDate': (__spec__.parent, '.types'),\n    'PastDatetime': (__spec__.parent, '.types'),\n    'FutureDatetime': (__spec__.parent, '.types'),\n    'AwareDatetime': (__spec__.parent, '.types'),\n    'NaiveDatetime': (__spec__.parent, '.types'),\n    'AllowInfNan': (__spec__.parent, '.types'),\n    'EncoderProtocol': (__spec__.parent, '.types'),\n    'EncodedBytes': (__spec__.parent, '.types'),\n    'EncodedStr': (__spec__.parent, '.types'),\n    'Base64Encoder': (__spec__.parent, '.types'),\n    'Base64Bytes': (__spec__.parent, '.types'),\n    'Base64Str': (__spec__.parent, '.types'),\n    'Base64UrlBytes': (__spec__.parent, '.types'),\n    'Base64UrlStr': (__spec__.parent, '.types'),\n    'GetPydanticSchema': (__spec__.parent, '.types'),\n    'Tag': (__spec__.parent, '.types'),\n    'Discriminator': (__spec__.parent, '.types'),\n    'JsonValue': (__spec__.parent, '.types'),\n    'OnErrorOmit': (__spec__.parent, '.types'),\n    # type_adapter\n    'TypeAdapter': (__spec__.parent, '.type_adapter'),\n    # warnings\n    'PydanticDeprecatedSince20': (__spec__.parent, '.warnings'),\n    'PydanticDeprecatedSince26': (__spec__.parent, '.warnings'),\n    'PydanticDeprecationWarning': (__spec__.parent, '.warnings'),\n    'PydanticExperimentalWarning': (__spec__.parent, '.warnings'),\n    # annotated handlers\n    'GetCoreSchemaHandler': (__spec__.parent, '.annotated_handlers'),\n    'GetJsonSchemaHandler': (__spec__.parent, '.annotated_handlers'),\n    # generate schema from ._internal\n    'GenerateSchema': (__spec__.parent, '._internal._generate_schema'),\n    # pydantic_core stuff\n    'ValidationError': ('pydantic_core', '.'),\n    'ValidationInfo': ('pydantic_core', '.core_schema'),\n    'SerializationInfo': ('pydantic_core', '.core_schema'),\n    'ValidatorFunctionWrapHandler': ('pydantic_core', '.core_schema'),\n    'FieldSerializationInfo': ('pydantic_core', '.core_schema'),\n    'SerializerFunctionWrapHandler': ('pydantic_core', '.core_schema'),\n    # deprecated, mostly not included in __all__\n    'root_validator': (__spec__.parent, '.deprecated.class_validators'),\n    'validator': (__spec__.parent, '.deprecated.class_validators'),\n    'BaseConfig': (__spec__.parent, '.deprecated.config'),\n    'Extra': (__spec__.parent, '.deprecated.config'),\n    'parse_obj_as': (__spec__.parent, '.deprecated.tools'),\n    'schema_of': (__spec__.parent, '.deprecated.tools'),\n    'schema_json_of': (__spec__.parent, '.deprecated.tools'),\n    'FieldValidationInfo': ('pydantic_core', '.core_schema'),\n}\n\n_getattr_migration = getattr_migration(__name__)\n\n\ndef __getattr__(attr_name: str) -> object:\n    dynamic_attr = _dynamic_imports.get(attr_name)\n    if dynamic_attr is None:\n        return _getattr_migration(attr_name)\n\n    package, module_name = dynamic_attr\n\n    from importlib import import_module\n\n    if module_name == '__module__':\n        return import_module(f'.{attr_name}', package=package)\n    else:\n        module = import_module(module_name, package=package)\n        return getattr(module, attr_name)\n\n\ndef __dir__() -> 'list[str]':\n    return list(__all__)\n", "pydantic/mypy.py": "\"\"\"This module includes classes and functions designed specifically for use with the mypy plugin.\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\nfrom configparser import ConfigParser\nfrom typing import Any, Callable, Iterator\n\nfrom mypy.errorcodes import ErrorCode\nfrom mypy.expandtype import expand_type, expand_type_by_instance\nfrom mypy.nodes import (\n    ARG_NAMED,\n    ARG_NAMED_OPT,\n    ARG_OPT,\n    ARG_POS,\n    ARG_STAR2,\n    INVARIANT,\n    MDEF,\n    Argument,\n    AssignmentStmt,\n    Block,\n    CallExpr,\n    ClassDef,\n    Context,\n    Decorator,\n    DictExpr,\n    EllipsisExpr,\n    Expression,\n    FuncDef,\n    IfStmt,\n    JsonDict,\n    MemberExpr,\n    NameExpr,\n    PassStmt,\n    PlaceholderNode,\n    RefExpr,\n    Statement,\n    StrExpr,\n    SymbolTableNode,\n    TempNode,\n    TypeAlias,\n    TypeInfo,\n    Var,\n)\nfrom mypy.options import Options\nfrom mypy.plugin import (\n    CheckerPluginInterface,\n    ClassDefContext,\n    FunctionContext,\n    MethodContext,\n    Plugin,\n    ReportConfigContext,\n    SemanticAnalyzerPluginInterface,\n)\nfrom mypy.plugins import dataclasses\nfrom mypy.plugins.common import (\n    deserialize_and_fixup_type,\n)\nfrom mypy.semanal import set_callable_name\nfrom mypy.server.trigger import make_wildcard_trigger\nfrom mypy.state import state\nfrom mypy.typeops import map_type_from_supertype\nfrom mypy.types import (\n    AnyType,\n    CallableType,\n    Instance,\n    NoneType,\n    Overloaded,\n    Type,\n    TypeOfAny,\n    TypeType,\n    TypeVarType,\n    UnionType,\n    get_proper_type,\n)\nfrom mypy.typevars import fill_typevars\nfrom mypy.util import get_unique_redefinition_name\nfrom mypy.version import __version__ as mypy_version\n\nfrom pydantic._internal import _fields\nfrom pydantic.version import parse_mypy_version\n\ntry:\n    from mypy.types import TypeVarDef  # type: ignore[attr-defined]\nexcept ImportError:  # pragma: no cover\n    # Backward-compatible with TypeVarDef from Mypy 0.930.\n    from mypy.types import TypeVarType as TypeVarDef\n\nCONFIGFILE_KEY = 'pydantic-mypy'\nMETADATA_KEY = 'pydantic-mypy-metadata'\nBASEMODEL_FULLNAME = 'pydantic.main.BaseModel'\nBASESETTINGS_FULLNAME = 'pydantic_settings.main.BaseSettings'\nROOT_MODEL_FULLNAME = 'pydantic.root_model.RootModel'\nMODEL_METACLASS_FULLNAME = 'pydantic._internal._model_construction.ModelMetaclass'\nFIELD_FULLNAME = 'pydantic.fields.Field'\nDATACLASS_FULLNAME = 'pydantic.dataclasses.dataclass'\nMODEL_VALIDATOR_FULLNAME = 'pydantic.functional_validators.model_validator'\nDECORATOR_FULLNAMES = {\n    'pydantic.functional_validators.field_validator',\n    'pydantic.functional_validators.model_validator',\n    'pydantic.functional_serializers.serializer',\n    'pydantic.functional_serializers.model_serializer',\n    'pydantic.deprecated.class_validators.validator',\n    'pydantic.deprecated.class_validators.root_validator',\n}\n\n\nMYPY_VERSION_TUPLE = parse_mypy_version(mypy_version)\nBUILTINS_NAME = 'builtins' if MYPY_VERSION_TUPLE >= (0, 930) else '__builtins__'\n\n# Increment version if plugin changes and mypy caches should be invalidated\n__version__ = 2\n\n\ndef plugin(version: str) -> type[Plugin]:\n    \"\"\"`version` is the mypy version string.\n\n    We might want to use this to print a warning if the mypy version being used is\n    newer, or especially older, than we expect (or need).\n\n    Args:\n        version: The mypy version string.\n\n    Return:\n        The Pydantic mypy plugin type.\n    \"\"\"\n    return PydanticPlugin\n\n\nclass PydanticPlugin(Plugin):\n    \"\"\"The Pydantic mypy plugin.\"\"\"\n\n    def __init__(self, options: Options) -> None:\n        self.plugin_config = PydanticPluginConfig(options)\n        self._plugin_data = self.plugin_config.to_data()\n        super().__init__(options)\n\n    def get_base_class_hook(self, fullname: str) -> Callable[[ClassDefContext], bool] | None:\n        \"\"\"Update Pydantic model class.\"\"\"\n        sym = self.lookup_fully_qualified(fullname)\n        if sym and isinstance(sym.node, TypeInfo):  # pragma: no branch\n            # No branching may occur if the mypy cache has not been cleared\n            if any(base.fullname == BASEMODEL_FULLNAME for base in sym.node.mro):\n                return self._pydantic_model_class_maker_callback\n        return None\n\n    def get_metaclass_hook(self, fullname: str) -> Callable[[ClassDefContext], None] | None:\n        \"\"\"Update Pydantic `ModelMetaclass` definition.\"\"\"\n        if fullname == MODEL_METACLASS_FULLNAME:\n            return self._pydantic_model_metaclass_marker_callback\n        return None\n\n    def get_function_hook(self, fullname: str) -> Callable[[FunctionContext], Type] | None:\n        \"\"\"Adjust the return type of the `Field` function.\"\"\"\n        sym = self.lookup_fully_qualified(fullname)\n        if sym and sym.fullname == FIELD_FULLNAME:\n            return self._pydantic_field_callback\n        return None\n\n    def get_method_hook(self, fullname: str) -> Callable[[MethodContext], Type] | None:\n        \"\"\"Adjust return type of `from_orm` method call.\"\"\"\n        if fullname.endswith('.from_orm'):\n            return from_attributes_callback\n        return None\n\n    def get_class_decorator_hook(self, fullname: str) -> Callable[[ClassDefContext], None] | None:\n        \"\"\"Mark pydantic.dataclasses as dataclass.\n\n        Mypy version 1.1.1 added support for `@dataclass_transform` decorator.\n        \"\"\"\n        if fullname == DATACLASS_FULLNAME and MYPY_VERSION_TUPLE < (1, 1):\n            return dataclasses.dataclass_class_maker_callback  # type: ignore[return-value]\n        return None\n\n    def report_config_data(self, ctx: ReportConfigContext) -> dict[str, Any]:\n        \"\"\"Return all plugin config data.\n\n        Used by mypy to determine if cache needs to be discarded.\n        \"\"\"\n        return self._plugin_data\n\n    def _pydantic_model_class_maker_callback(self, ctx: ClassDefContext) -> bool:\n        transformer = PydanticModelTransformer(ctx.cls, ctx.reason, ctx.api, self.plugin_config)\n        return transformer.transform()\n\n    def _pydantic_model_metaclass_marker_callback(self, ctx: ClassDefContext) -> None:\n        \"\"\"Reset dataclass_transform_spec attribute of ModelMetaclass.\n\n        Let the plugin handle it. This behavior can be disabled\n        if 'debug_dataclass_transform' is set to True', for testing purposes.\n        \"\"\"\n        if self.plugin_config.debug_dataclass_transform:\n            return\n        info_metaclass = ctx.cls.info.declared_metaclass\n        assert info_metaclass, \"callback not passed from 'get_metaclass_hook'\"\n        if getattr(info_metaclass.type, 'dataclass_transform_spec', None):\n            info_metaclass.type.dataclass_transform_spec = None\n\n    def _pydantic_field_callback(self, ctx: FunctionContext) -> Type:\n        \"\"\"Extract the type of the `default` argument from the Field function, and use it as the return type.\n\n        In particular:\n        * Check whether the default and default_factory argument is specified.\n        * Output an error if both are specified.\n        * Retrieve the type of the argument which is specified, and use it as return type for the function.\n        \"\"\"\n        default_any_type = ctx.default_return_type\n\n        assert ctx.callee_arg_names[0] == 'default', '\"default\" is no longer first argument in Field()'\n        assert ctx.callee_arg_names[1] == 'default_factory', '\"default_factory\" is no longer second argument in Field()'\n        default_args = ctx.args[0]\n        default_factory_args = ctx.args[1]\n\n        if default_args and default_factory_args:\n            error_default_and_default_factory_specified(ctx.api, ctx.context)\n            return default_any_type\n\n        if default_args:\n            default_type = ctx.arg_types[0][0]\n            default_arg = default_args[0]\n\n            # Fallback to default Any type if the field is required\n            if not isinstance(default_arg, EllipsisExpr):\n                return default_type\n\n        elif default_factory_args:\n            default_factory_type = ctx.arg_types[1][0]\n\n            # Functions which use `ParamSpec` can be overloaded, exposing the callable's types as a parameter\n            # Pydantic calls the default factory without any argument, so we retrieve the first item\n            if isinstance(default_factory_type, Overloaded):\n                default_factory_type = default_factory_type.items[0]\n\n            if isinstance(default_factory_type, CallableType):\n                ret_type = default_factory_type.ret_type\n                # mypy doesn't think `ret_type` has `args`, you'd think mypy should know,\n                # add this check in case it varies by version\n                args = getattr(ret_type, 'args', None)\n                if args:\n                    if all(isinstance(arg, TypeVarType) for arg in args):\n                        # Looks like the default factory is a type like `list` or `dict`, replace all args with `Any`\n                        ret_type.args = tuple(default_any_type for _ in args)  # type: ignore[attr-defined]\n                return ret_type\n\n        return default_any_type\n\n\nclass PydanticPluginConfig:\n    \"\"\"A Pydantic mypy plugin config holder.\n\n    Attributes:\n        init_forbid_extra: Whether to add a `**kwargs` at the end of the generated `__init__` signature.\n        init_typed: Whether to annotate fields in the generated `__init__`.\n        warn_required_dynamic_aliases: Whether to raise required dynamic aliases error.\n        debug_dataclass_transform: Whether to not reset `dataclass_transform_spec` attribute\n            of `ModelMetaclass` for testing purposes.\n    \"\"\"\n\n    __slots__ = (\n        'init_forbid_extra',\n        'init_typed',\n        'warn_required_dynamic_aliases',\n        'debug_dataclass_transform',\n    )\n    init_forbid_extra: bool\n    init_typed: bool\n    warn_required_dynamic_aliases: bool\n    debug_dataclass_transform: bool  # undocumented\n\n    def __init__(self, options: Options) -> None:\n        if options.config_file is None:  # pragma: no cover\n            return\n\n        toml_config = parse_toml(options.config_file)\n        if toml_config is not None:\n            config = toml_config.get('tool', {}).get('pydantic-mypy', {})\n            for key in self.__slots__:\n                setting = config.get(key, False)\n                if not isinstance(setting, bool):\n                    raise ValueError(f'Configuration value must be a boolean for key: {key}')\n                setattr(self, key, setting)\n        else:\n            plugin_config = ConfigParser()\n            plugin_config.read(options.config_file)\n            for key in self.__slots__:\n                setting = plugin_config.getboolean(CONFIGFILE_KEY, key, fallback=False)\n                setattr(self, key, setting)\n\n    def to_data(self) -> dict[str, Any]:\n        \"\"\"Returns a dict of config names to their values.\"\"\"\n        return {key: getattr(self, key) for key in self.__slots__}\n\n\ndef from_attributes_callback(ctx: MethodContext) -> Type:\n    \"\"\"Raise an error if from_attributes is not enabled.\"\"\"\n    model_type: Instance\n    ctx_type = ctx.type\n    if isinstance(ctx_type, TypeType):\n        ctx_type = ctx_type.item\n    if isinstance(ctx_type, CallableType) and isinstance(ctx_type.ret_type, Instance):\n        model_type = ctx_type.ret_type  # called on the class\n    elif isinstance(ctx_type, Instance):\n        model_type = ctx_type  # called on an instance (unusual, but still valid)\n    else:  # pragma: no cover\n        detail = f'ctx.type: {ctx_type} (of type {ctx_type.__class__.__name__})'\n        error_unexpected_behavior(detail, ctx.api, ctx.context)\n        return ctx.default_return_type\n    pydantic_metadata = model_type.type.metadata.get(METADATA_KEY)\n    if pydantic_metadata is None:\n        return ctx.default_return_type\n    from_attributes = pydantic_metadata.get('config', {}).get('from_attributes')\n    if from_attributes is not True:\n        error_from_attributes(model_type.type.name, ctx.api, ctx.context)\n    return ctx.default_return_type\n\n\nclass PydanticModelField:\n    \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        alias: str | None,\n        has_dynamic_alias: bool,\n        has_default: bool,\n        line: int,\n        column: int,\n        type: Type | None,\n        info: TypeInfo,\n    ):\n        self.name = name\n        self.alias = alias\n        self.has_dynamic_alias = has_dynamic_alias\n        self.has_default = has_default\n        self.line = line\n        self.column = column\n        self.type = type\n        self.info = info\n\n    def to_argument(\n        self,\n        current_info: TypeInfo,\n        typed: bool,\n        force_optional: bool,\n        use_alias: bool,\n        api: SemanticAnalyzerPluginInterface,\n        force_typevars_invariant: bool,\n    ) -> Argument:\n        \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.to_argument.\"\"\"\n        variable = self.to_var(current_info, api, use_alias, force_typevars_invariant)\n        type_annotation = self.expand_type(current_info, api) if typed else AnyType(TypeOfAny.explicit)\n        return Argument(\n            variable=variable,\n            type_annotation=type_annotation,\n            initializer=None,\n            kind=ARG_NAMED_OPT if force_optional or self.has_default else ARG_NAMED,\n        )\n\n    def expand_type(\n        self, current_info: TypeInfo, api: SemanticAnalyzerPluginInterface, force_typevars_invariant: bool = False\n    ) -> Type | None:\n        \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.expand_type.\"\"\"\n        # The getattr in the next line is used to prevent errors in legacy versions of mypy without this attribute\n        if force_typevars_invariant:\n            # In some cases, mypy will emit an error \"Cannot use a covariant type variable as a parameter\"\n            # To prevent that, we add an option to replace typevars with invariant ones while building certain\n            # method signatures (in particular, `__init__`). There may be a better way to do this, if this causes\n            # us problems in the future, we should look into why the dataclasses plugin doesn't have this issue.\n            if isinstance(self.type, TypeVarType):\n                modified_type = self.type.copy_modified()\n                modified_type.variance = INVARIANT\n                self.type = modified_type\n\n        if self.type is not None and getattr(self.info, 'self_type', None) is not None:\n            # In general, it is not safe to call `expand_type()` during semantic analyzis,\n            # however this plugin is called very late, so all types should be fully ready.\n            # Also, it is tricky to avoid eager expansion of Self types here (e.g. because\n            # we serialize attributes).\n            with state.strict_optional_set(api.options.strict_optional):\n                filled_with_typevars = fill_typevars(current_info)\n                if force_typevars_invariant:\n                    for arg in filled_with_typevars.args:\n                        if isinstance(arg, TypeVarType):\n                            arg.variance = INVARIANT\n                return expand_type(self.type, {self.info.self_type.id: filled_with_typevars})\n        return self.type\n\n    def to_var(\n        self,\n        current_info: TypeInfo,\n        api: SemanticAnalyzerPluginInterface,\n        use_alias: bool,\n        force_typevars_invariant: bool = False,\n    ) -> Var:\n        \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.to_var.\"\"\"\n        if use_alias and self.alias is not None:\n            name = self.alias\n        else:\n            name = self.name\n\n        return Var(name, self.expand_type(current_info, api, force_typevars_invariant))\n\n    def serialize(self) -> JsonDict:\n        \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.serialize.\"\"\"\n        assert self.type\n        return {\n            'name': self.name,\n            'alias': self.alias,\n            'has_dynamic_alias': self.has_dynamic_alias,\n            'has_default': self.has_default,\n            'line': self.line,\n            'column': self.column,\n            'type': self.type.serialize(),\n        }\n\n    @classmethod\n    def deserialize(cls, info: TypeInfo, data: JsonDict, api: SemanticAnalyzerPluginInterface) -> PydanticModelField:\n        \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.deserialize.\"\"\"\n        data = data.copy()\n        typ = deserialize_and_fixup_type(data.pop('type'), api)\n        return cls(type=typ, info=info, **data)\n\n    def expand_typevar_from_subtype(self, sub_type: TypeInfo, api: SemanticAnalyzerPluginInterface) -> None:\n        \"\"\"Expands type vars in the context of a subtype when an attribute is inherited\n        from a generic super type.\n        \"\"\"\n        if self.type is not None:\n            with state.strict_optional_set(api.options.strict_optional):\n                self.type = map_type_from_supertype(self.type, sub_type, self.info)\n\n\nclass PydanticModelClassVar:\n    \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.\n\n    ClassVars are ignored by subclasses.\n\n    Attributes:\n        name: the ClassVar name\n    \"\"\"\n\n    def __init__(self, name):\n        self.name = name\n\n    @classmethod\n    def deserialize(cls, data: JsonDict) -> PydanticModelClassVar:\n        \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.deserialize.\"\"\"\n        data = data.copy()\n        return cls(**data)\n\n    def serialize(self) -> JsonDict:\n        \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.serialize.\"\"\"\n        return {\n            'name': self.name,\n        }\n\n\nclass PydanticModelTransformer:\n    \"\"\"Transform the BaseModel subclass according to the plugin settings.\n\n    Attributes:\n        tracked_config_fields: A set of field configs that the plugin has to track their value.\n    \"\"\"\n\n    tracked_config_fields: set[str] = {\n        'extra',\n        'frozen',\n        'from_attributes',\n        'populate_by_name',\n        'alias_generator',\n    }\n\n    def __init__(\n        self,\n        cls: ClassDef,\n        reason: Expression | Statement,\n        api: SemanticAnalyzerPluginInterface,\n        plugin_config: PydanticPluginConfig,\n    ) -> None:\n        self._cls = cls\n        self._reason = reason\n        self._api = api\n\n        self.plugin_config = plugin_config\n\n    def transform(self) -> bool:\n        \"\"\"Configures the BaseModel subclass according to the plugin settings.\n\n        In particular:\n\n        * determines the model config and fields,\n        * adds a fields-aware signature for the initializer and construct methods\n        * freezes the class if frozen = True\n        * stores the fields, config, and if the class is settings in the mypy metadata for access by subclasses\n        \"\"\"\n        info = self._cls.info\n        is_root_model = any(ROOT_MODEL_FULLNAME in base.fullname for base in info.mro[:-1])\n        config = self.collect_config()\n        fields, class_vars = self.collect_fields_and_class_vars(config, is_root_model)\n        if fields is None or class_vars is None:\n            # Some definitions are not ready. We need another pass.\n            return False\n        for field in fields:\n            if field.type is None:\n                return False\n\n        is_settings = any(base.fullname == BASESETTINGS_FULLNAME for base in info.mro[:-1])\n        self.add_initializer(fields, config, is_settings, is_root_model)\n        if not is_root_model:\n            self.add_model_construct_method(fields, config, is_settings)\n        self.set_frozen(fields, self._api, frozen=config.frozen is True)\n\n        self.adjust_decorator_signatures()\n\n        info.metadata[METADATA_KEY] = {\n            'fields': {field.name: field.serialize() for field in fields},\n            'class_vars': {class_var.name: class_var.serialize() for class_var in class_vars},\n            'config': config.get_values_dict(),\n        }\n\n        return True\n\n    def adjust_decorator_signatures(self) -> None:\n        \"\"\"When we decorate a function `f` with `pydantic.validator(...)`, `pydantic.field_validator`\n        or `pydantic.serializer(...)`, mypy sees `f` as a regular method taking a `self` instance,\n        even though pydantic internally wraps `f` with `classmethod` if necessary.\n\n        Teach mypy this by marking any function whose outermost decorator is a `validator()`,\n        `field_validator()` or `serializer()` call as a `classmethod`.\n        \"\"\"\n        for name, sym in self._cls.info.names.items():\n            if isinstance(sym.node, Decorator):\n                first_dec = sym.node.original_decorators[0]\n                if (\n                    isinstance(first_dec, CallExpr)\n                    and isinstance(first_dec.callee, NameExpr)\n                    and first_dec.callee.fullname in DECORATOR_FULLNAMES\n                    # @model_validator(mode=\"after\") is an exception, it expects a regular method\n                    and not (\n                        first_dec.callee.fullname == MODEL_VALIDATOR_FULLNAME\n                        and any(\n                            first_dec.arg_names[i] == 'mode' and isinstance(arg, StrExpr) and arg.value == 'after'\n                            for i, arg in enumerate(first_dec.args)\n                        )\n                    )\n                ):\n                    # TODO: Only do this if the first argument of the decorated function is `cls`\n                    sym.node.func.is_class = True\n\n    def collect_config(self) -> ModelConfigData:  # noqa: C901 (ignore complexity)\n        \"\"\"Collects the values of the config attributes that are used by the plugin, accounting for parent classes.\"\"\"\n        cls = self._cls\n        config = ModelConfigData()\n\n        has_config_kwargs = False\n        has_config_from_namespace = False\n\n        # Handle `class MyModel(BaseModel, <name>=<expr>, ...):`\n        for name, expr in cls.keywords.items():\n            config_data = self.get_config_update(name, expr)\n            if config_data:\n                has_config_kwargs = True\n                config.update(config_data)\n\n        # Handle `model_config`\n        stmt: Statement | None = None\n        for stmt in cls.defs.body:\n            if not isinstance(stmt, (AssignmentStmt, ClassDef)):\n                continue\n\n            if isinstance(stmt, AssignmentStmt):\n                lhs = stmt.lvalues[0]\n                if not isinstance(lhs, NameExpr) or lhs.name != 'model_config':\n                    continue\n\n                if isinstance(stmt.rvalue, CallExpr):  # calls to `dict` or `ConfigDict`\n                    for arg_name, arg in zip(stmt.rvalue.arg_names, stmt.rvalue.args):\n                        if arg_name is None:\n                            continue\n                        config.update(self.get_config_update(arg_name, arg, lax_extra=True))\n                elif isinstance(stmt.rvalue, DictExpr):  # dict literals\n                    for key_expr, value_expr in stmt.rvalue.items:\n                        if not isinstance(key_expr, StrExpr):\n                            continue\n                        config.update(self.get_config_update(key_expr.value, value_expr))\n\n            elif isinstance(stmt, ClassDef):\n                if stmt.name != 'Config':  # 'deprecated' Config-class\n                    continue\n                for substmt in stmt.defs.body:\n                    if not isinstance(substmt, AssignmentStmt):\n                        continue\n                    lhs = substmt.lvalues[0]\n                    if not isinstance(lhs, NameExpr):\n                        continue\n                    config.update(self.get_config_update(lhs.name, substmt.rvalue))\n\n            if has_config_kwargs:\n                self._api.fail(\n                    'Specifying config in two places is ambiguous, use either Config attribute or class kwargs',\n                    cls,\n                )\n                break\n\n            has_config_from_namespace = True\n\n        if has_config_kwargs or has_config_from_namespace:\n            if (\n                stmt\n                and config.has_alias_generator\n                and not config.populate_by_name\n                and self.plugin_config.warn_required_dynamic_aliases\n            ):\n                error_required_dynamic_aliases(self._api, stmt)\n\n        for info in cls.info.mro[1:]:  # 0 is the current class\n            if METADATA_KEY not in info.metadata:\n                continue\n\n            # Each class depends on the set of fields in its ancestors\n            self._api.add_plugin_dependency(make_wildcard_trigger(info.fullname))\n            for name, value in info.metadata[METADATA_KEY]['config'].items():\n                config.setdefault(name, value)\n        return config\n\n    def collect_fields_and_class_vars(\n        self, model_config: ModelConfigData, is_root_model: bool\n    ) -> tuple[list[PydanticModelField] | None, list[PydanticModelClassVar] | None]:\n        \"\"\"Collects the fields for the model, accounting for parent classes.\"\"\"\n        cls = self._cls\n\n        # First, collect fields and ClassVars belonging to any class in the MRO, ignoring duplicates.\n        #\n        # We iterate through the MRO in reverse because attrs defined in the parent must appear\n        # earlier in the attributes list than attrs defined in the child. See:\n        # https://docs.python.org/3/library/dataclasses.html#inheritance\n        #\n        # However, we also want fields defined in the subtype to override ones defined\n        # in the parent. We can implement this via a dict without disrupting the attr order\n        # because dicts preserve insertion order in Python 3.7+.\n        found_fields: dict[str, PydanticModelField] = {}\n        found_class_vars: dict[str, PydanticModelClassVar] = {}\n        for info in reversed(cls.info.mro[1:-1]):  # 0 is the current class, -2 is BaseModel, -1 is object\n            # if BASEMODEL_METADATA_TAG_KEY in info.metadata and BASEMODEL_METADATA_KEY not in info.metadata:\n            #     # We haven't processed the base class yet. Need another pass.\n            #     return None, None\n            if METADATA_KEY not in info.metadata:\n                continue\n\n            # Each class depends on the set of attributes in its dataclass ancestors.\n            self._api.add_plugin_dependency(make_wildcard_trigger(info.fullname))\n\n            for name, data in info.metadata[METADATA_KEY]['fields'].items():\n                field = PydanticModelField.deserialize(info, data, self._api)\n                # (The following comment comes directly from the dataclasses plugin)\n                # TODO: We shouldn't be performing type operations during the main\n                #       semantic analysis pass, since some TypeInfo attributes might\n                #       still be in flux. This should be performed in a later phase.\n                field.expand_typevar_from_subtype(cls.info, self._api)\n                found_fields[name] = field\n\n                sym_node = cls.info.names.get(name)\n                if sym_node and sym_node.node and not isinstance(sym_node.node, Var):\n                    self._api.fail(\n                        'BaseModel field may only be overridden by another field',\n                        sym_node.node,\n                    )\n            # Collect ClassVars\n            for name, data in info.metadata[METADATA_KEY]['class_vars'].items():\n                found_class_vars[name] = PydanticModelClassVar.deserialize(data)\n\n        # Second, collect fields and ClassVars belonging to the current class.\n        current_field_names: set[str] = set()\n        current_class_vars_names: set[str] = set()\n        for stmt in self._get_assignment_statements_from_block(cls.defs):\n            maybe_field = self.collect_field_or_class_var_from_stmt(stmt, model_config, found_class_vars)\n            if isinstance(maybe_field, PydanticModelField):\n                lhs = stmt.lvalues[0]\n                if is_root_model and lhs.name != 'root':\n                    error_extra_fields_on_root_model(self._api, stmt)\n                else:\n                    current_field_names.add(lhs.name)\n                    found_fields[lhs.name] = maybe_field\n            elif isinstance(maybe_field, PydanticModelClassVar):\n                lhs = stmt.lvalues[0]\n                current_class_vars_names.add(lhs.name)\n                found_class_vars[lhs.name] = maybe_field\n\n        return list(found_fields.values()), list(found_class_vars.values())\n\n    def _get_assignment_statements_from_if_statement(self, stmt: IfStmt) -> Iterator[AssignmentStmt]:\n        for body in stmt.body:\n            if not body.is_unreachable:\n                yield from self._get_assignment_statements_from_block(body)\n        if stmt.else_body is not None and not stmt.else_body.is_unreachable:\n            yield from self._get_assignment_statements_from_block(stmt.else_body)\n\n    def _get_assignment_statements_from_block(self, block: Block) -> Iterator[AssignmentStmt]:\n        for stmt in block.body:\n            if isinstance(stmt, AssignmentStmt):\n                yield stmt\n            elif isinstance(stmt, IfStmt):\n                yield from self._get_assignment_statements_from_if_statement(stmt)\n\n    def collect_field_or_class_var_from_stmt(  # noqa C901\n        self, stmt: AssignmentStmt, model_config: ModelConfigData, class_vars: dict[str, PydanticModelClassVar]\n    ) -> PydanticModelField | PydanticModelClassVar | None:\n        \"\"\"Get pydantic model field from statement.\n\n        Args:\n            stmt: The statement.\n            model_config: Configuration settings for the model.\n            class_vars: ClassVars already known to be defined on the model.\n\n        Returns:\n            A pydantic model field if it could find the field in statement. Otherwise, `None`.\n        \"\"\"\n        cls = self._cls\n\n        lhs = stmt.lvalues[0]\n        if not isinstance(lhs, NameExpr) or not _fields.is_valid_field_name(lhs.name) or lhs.name == 'model_config':\n            return None\n\n        if not stmt.new_syntax:\n            if (\n                isinstance(stmt.rvalue, CallExpr)\n                and isinstance(stmt.rvalue.callee, CallExpr)\n                and isinstance(stmt.rvalue.callee.callee, NameExpr)\n                and stmt.rvalue.callee.callee.fullname in DECORATOR_FULLNAMES\n            ):\n                # This is a (possibly-reused) validator or serializer, not a field\n                # In particular, it looks something like: my_validator = validator('my_field')(f)\n                # Eventually, we may want to attempt to respect model_config['ignored_types']\n                return None\n\n            if lhs.name in class_vars:\n                # Class vars are not fields and are not required to be annotated\n                return None\n\n            # The assignment does not have an annotation, and it's not anything else we recognize\n            error_untyped_fields(self._api, stmt)\n            return None\n\n        lhs = stmt.lvalues[0]\n        if not isinstance(lhs, NameExpr):\n            return None\n\n        if not _fields.is_valid_field_name(lhs.name) or lhs.name == 'model_config':\n            return None\n\n        sym = cls.info.names.get(lhs.name)\n        if sym is None:  # pragma: no cover\n            # This is likely due to a star import (see the dataclasses plugin for a more detailed explanation)\n            # This is the same logic used in the dataclasses plugin\n            return None\n\n        node = sym.node\n        if isinstance(node, PlaceholderNode):  # pragma: no cover\n            # See the PlaceholderNode docstring for more detail about how this can occur\n            # Basically, it is an edge case when dealing with complex import logic\n\n            # The dataclasses plugin now asserts this cannot happen, but I'd rather not error if it does..\n            return None\n\n        if isinstance(node, TypeAlias):\n            self._api.fail(\n                'Type aliases inside BaseModel definitions are not supported at runtime',\n                node,\n            )\n            # Skip processing this node. This doesn't match the runtime behaviour,\n            # but the only alternative would be to modify the SymbolTable,\n            # and it's a little hairy to do that in a plugin.\n            return None\n\n        if not isinstance(node, Var):  # pragma: no cover\n            # Don't know if this edge case still happens with the `is_valid_field` check above\n            # but better safe than sorry\n\n            # The dataclasses plugin now asserts this cannot happen, but I'd rather not error if it does..\n            return None\n\n        # x: ClassVar[int] is not a field\n        if node.is_classvar:\n            return PydanticModelClassVar(lhs.name)\n\n        # x: InitVar[int] is not supported in BaseModel\n        node_type = get_proper_type(node.type)\n        if isinstance(node_type, Instance) and node_type.type.fullname == 'dataclasses.InitVar':\n            self._api.fail(\n                'InitVar is not supported in BaseModel',\n                node,\n            )\n\n        has_default = self.get_has_default(stmt)\n\n        if sym.type is None and node.is_final and node.is_inferred:\n            # This follows the logic from the dataclasses plugin. The following comment is taken verbatim:\n            #\n            # This is a special case, assignment like x: Final = 42 is classified\n            # annotated above, but mypy strips the `Final` turning it into x = 42.\n            # We do not support inferred types in dataclasses, so we can try inferring\n            # type for simple literals, and otherwise require an explicit type\n            # argument for Final[...].\n            typ = self._api.analyze_simple_literal_type(stmt.rvalue, is_final=True)\n            if typ:\n                node.type = typ\n            else:\n                self._api.fail(\n                    'Need type argument for Final[...] with non-literal default in BaseModel',\n                    stmt,\n                )\n                node.type = AnyType(TypeOfAny.from_error)\n\n        alias, has_dynamic_alias = self.get_alias_info(stmt)\n        if has_dynamic_alias and not model_config.populate_by_name and self.plugin_config.warn_required_dynamic_aliases:\n            error_required_dynamic_aliases(self._api, stmt)\n\n        init_type = self._infer_dataclass_attr_init_type(sym, lhs.name, stmt)\n        return PydanticModelField(\n            name=lhs.name,\n            has_dynamic_alias=has_dynamic_alias,\n            has_default=has_default,\n            alias=alias,\n            line=stmt.line,\n            column=stmt.column,\n            type=init_type,\n            info=cls.info,\n        )\n\n    def _infer_dataclass_attr_init_type(self, sym: SymbolTableNode, name: str, context: Context) -> Type | None:\n        \"\"\"Infer __init__ argument type for an attribute.\n\n        In particular, possibly use the signature of __set__.\n        \"\"\"\n        default = sym.type\n        if sym.implicit:\n            return default\n        t = get_proper_type(sym.type)\n\n        # Perform a simple-minded inference from the signature of __set__, if present.\n        # We can't use mypy.checkmember here, since this plugin runs before type checking.\n        # We only support some basic scanerios here, which is hopefully sufficient for\n        # the vast majority of use cases.\n        if not isinstance(t, Instance):\n            return default\n        setter = t.type.get('__set__')\n        if setter:\n            if isinstance(setter.node, FuncDef):\n                super_info = t.type.get_containing_type_info('__set__')\n                assert super_info\n                if setter.type:\n                    setter_type = get_proper_type(map_type_from_supertype(setter.type, t.type, super_info))\n                else:\n                    return AnyType(TypeOfAny.unannotated)\n                if isinstance(setter_type, CallableType) and setter_type.arg_kinds == [\n                    ARG_POS,\n                    ARG_POS,\n                    ARG_POS,\n                ]:\n                    return expand_type_by_instance(setter_type.arg_types[2], t)\n                else:\n                    self._api.fail(f'Unsupported signature for \"__set__\" in \"{t.type.name}\"', context)\n            else:\n                self._api.fail(f'Unsupported \"__set__\" in \"{t.type.name}\"', context)\n\n        return default\n\n    def add_initializer(\n        self, fields: list[PydanticModelField], config: ModelConfigData, is_settings: bool, is_root_model: bool\n    ) -> None:\n        \"\"\"Adds a fields-aware `__init__` method to the class.\n\n        The added `__init__` will be annotated with types vs. all `Any` depending on the plugin settings.\n        \"\"\"\n        if '__init__' in self._cls.info.names and not self._cls.info.names['__init__'].plugin_generated:\n            return  # Don't generate an __init__ if one already exists\n\n        typed = self.plugin_config.init_typed\n        use_alias = config.populate_by_name is not True\n        requires_dynamic_aliases = bool(config.has_alias_generator and not config.populate_by_name)\n        args = self.get_field_arguments(\n            fields,\n            typed=typed,\n            requires_dynamic_aliases=requires_dynamic_aliases,\n            use_alias=use_alias,\n            is_settings=is_settings,\n            force_typevars_invariant=True,\n        )\n\n        if is_root_model and MYPY_VERSION_TUPLE <= (1, 0, 1):\n            # convert root argument to positional argument\n            # This is needed because mypy support for `dataclass_transform` isn't complete on 1.0.1\n            args[0].kind = ARG_POS if args[0].kind == ARG_NAMED else ARG_OPT\n\n        if is_settings:\n            base_settings_node = self._api.lookup_fully_qualified(BASESETTINGS_FULLNAME).node\n            if '__init__' in base_settings_node.names:\n                base_settings_init_node = base_settings_node.names['__init__'].node\n                if base_settings_init_node is not None and base_settings_init_node.type is not None:\n                    func_type = base_settings_init_node.type\n                    for arg_idx, arg_name in enumerate(func_type.arg_names):\n                        if arg_name.startswith('__') or not arg_name.startswith('_'):\n                            continue\n                        analyzed_variable_type = self._api.anal_type(func_type.arg_types[arg_idx])\n                        variable = Var(arg_name, analyzed_variable_type)\n                        args.append(Argument(variable, analyzed_variable_type, None, ARG_OPT))\n\n        if not self.should_init_forbid_extra(fields, config):\n            var = Var('kwargs')\n            args.append(Argument(var, AnyType(TypeOfAny.explicit), None, ARG_STAR2))\n\n        add_method(self._api, self._cls, '__init__', args=args, return_type=NoneType())\n\n    def add_model_construct_method(\n        self, fields: list[PydanticModelField], config: ModelConfigData, is_settings: bool\n    ) -> None:\n        \"\"\"Adds a fully typed `model_construct` classmethod to the class.\n\n        Similar to the fields-aware __init__ method, but always uses the field names (not aliases),\n        and does not treat settings fields as optional.\n        \"\"\"\n        set_str = self._api.named_type(f'{BUILTINS_NAME}.set', [self._api.named_type(f'{BUILTINS_NAME}.str')])\n        optional_set_str = UnionType([set_str, NoneType()])\n        fields_set_argument = Argument(Var('_fields_set', optional_set_str), optional_set_str, None, ARG_OPT)\n        with state.strict_optional_set(self._api.options.strict_optional):\n            args = self.get_field_arguments(\n                fields, typed=True, requires_dynamic_aliases=False, use_alias=False, is_settings=is_settings\n            )\n        if not self.should_init_forbid_extra(fields, config):\n            var = Var('kwargs')\n            args.append(Argument(var, AnyType(TypeOfAny.explicit), None, ARG_STAR2))\n\n        args = [fields_set_argument] + args\n\n        add_method(\n            self._api,\n            self._cls,\n            'model_construct',\n            args=args,\n            return_type=fill_typevars(self._cls.info),\n            is_classmethod=True,\n        )\n\n    def set_frozen(self, fields: list[PydanticModelField], api: SemanticAnalyzerPluginInterface, frozen: bool) -> None:\n        \"\"\"Marks all fields as properties so that attempts to set them trigger mypy errors.\n\n        This is the same approach used by the attrs and dataclasses plugins.\n        \"\"\"\n        info = self._cls.info\n        for field in fields:\n            sym_node = info.names.get(field.name)\n            if sym_node is not None:\n                var = sym_node.node\n                if isinstance(var, Var):\n                    var.is_property = frozen\n                elif isinstance(var, PlaceholderNode) and not self._api.final_iteration:\n                    # See https://github.com/pydantic/pydantic/issues/5191 to hit this branch for test coverage\n                    self._api.defer()\n                else:  # pragma: no cover\n                    # I don't know whether it's possible to hit this branch, but I've added it for safety\n                    try:\n                        var_str = str(var)\n                    except TypeError:\n                        # This happens for PlaceholderNode; perhaps it will happen for other types in the future..\n                        var_str = repr(var)\n                    detail = f'sym_node.node: {var_str} (of type {var.__class__})'\n                    error_unexpected_behavior(detail, self._api, self._cls)\n            else:\n                var = field.to_var(info, api, use_alias=False)\n                var.info = info\n                var.is_property = frozen\n                var._fullname = info.fullname + '.' + var.name\n                info.names[var.name] = SymbolTableNode(MDEF, var)\n\n    def get_config_update(self, name: str, arg: Expression, lax_extra: bool = False) -> ModelConfigData | None:\n        \"\"\"Determines the config update due to a single kwarg in the ConfigDict definition.\n\n        Warns if a tracked config attribute is set to a value the plugin doesn't know how to interpret (e.g., an int)\n        \"\"\"\n        if name not in self.tracked_config_fields:\n            return None\n        if name == 'extra':\n            if isinstance(arg, StrExpr):\n                forbid_extra = arg.value == 'forbid'\n            elif isinstance(arg, MemberExpr):\n                forbid_extra = arg.name == 'forbid'\n            else:\n                if not lax_extra:\n                    # Only emit an error for other types of `arg` (e.g., `NameExpr`, `ConditionalExpr`, etc.) when\n                    # reading from a config class, etc. If a ConfigDict is used, then we don't want to emit an error\n                    # because you'll get type checking from the ConfigDict itself.\n                    #\n                    # It would be nice if we could introspect the types better otherwise, but I don't know what the API\n                    # is to evaluate an expr into its type and then check if that type is compatible with the expected\n                    # type. Note that you can still get proper type checking via: `model_config = ConfigDict(...)`, just\n                    # if you don't use an explicit string, the plugin won't be able to infer whether extra is forbidden.\n                    error_invalid_config_value(name, self._api, arg)\n                return None\n            return ModelConfigData(forbid_extra=forbid_extra)\n        if name == 'alias_generator':\n            has_alias_generator = True\n            if isinstance(arg, NameExpr) and arg.fullname == 'builtins.None':\n                has_alias_generator = False\n            return ModelConfigData(has_alias_generator=has_alias_generator)\n        if isinstance(arg, NameExpr) and arg.fullname in ('builtins.True', 'builtins.False'):\n            return ModelConfigData(**{name: arg.fullname == 'builtins.True'})\n        error_invalid_config_value(name, self._api, arg)\n        return None\n\n    @staticmethod\n    def get_has_default(stmt: AssignmentStmt) -> bool:\n        \"\"\"Returns a boolean indicating whether the field defined in `stmt` is a required field.\"\"\"\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            # TempNode means annotation-only, so has no default\n            return False\n        if isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME:\n            # The \"default value\" is a call to `Field`; at this point, the field has a default if and only if:\n            # * there is a positional argument that is not `...`\n            # * there is a keyword argument named \"default\" that is not `...`\n            # * there is a \"default_factory\" that is not `None`\n            for arg, name in zip(expr.args, expr.arg_names):\n                # If name is None, then this arg is the default because it is the only positional argument.\n                if name is None or name == 'default':\n                    return arg.__class__ is not EllipsisExpr\n                if name == 'default_factory':\n                    return not (isinstance(arg, NameExpr) and arg.fullname == 'builtins.None')\n            return False\n        # Has no default if the \"default value\" is Ellipsis (i.e., `field_name: Annotation = ...`)\n        return not isinstance(expr, EllipsisExpr)\n\n    @staticmethod\n    def get_alias_info(stmt: AssignmentStmt) -> tuple[str | None, bool]:\n        \"\"\"Returns a pair (alias, has_dynamic_alias), extracted from the declaration of the field defined in `stmt`.\n\n        `has_dynamic_alias` is True if and only if an alias is provided, but not as a string literal.\n        If `has_dynamic_alias` is True, `alias` will be None.\n        \"\"\"\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            # TempNode means annotation-only\n            return None, False\n\n        if not (\n            isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME\n        ):\n            # Assigned value is not a call to pydantic.fields.Field\n            return None, False\n\n        for i, arg_name in enumerate(expr.arg_names):\n            if arg_name != 'alias':\n                continue\n            arg = expr.args[i]\n            if isinstance(arg, StrExpr):\n                return arg.value, False\n            else:\n                return None, True\n        return None, False\n\n    def get_field_arguments(\n        self,\n        fields: list[PydanticModelField],\n        typed: bool,\n        use_alias: bool,\n        requires_dynamic_aliases: bool,\n        is_settings: bool,\n        force_typevars_invariant: bool = False,\n    ) -> list[Argument]:\n        \"\"\"Helper function used during the construction of the `__init__` and `model_construct` method signatures.\n\n        Returns a list of mypy Argument instances for use in the generated signatures.\n        \"\"\"\n        info = self._cls.info\n        arguments = [\n            field.to_argument(\n                info,\n                typed=typed,\n                force_optional=requires_dynamic_aliases or is_settings,\n                use_alias=use_alias,\n                api=self._api,\n                force_typevars_invariant=force_typevars_invariant,\n            )\n            for field in fields\n            if not (use_alias and field.has_dynamic_alias)\n        ]\n        return arguments\n\n    def should_init_forbid_extra(self, fields: list[PydanticModelField], config: ModelConfigData) -> bool:\n        \"\"\"Indicates whether the generated `__init__` should get a `**kwargs` at the end of its signature.\n\n        We disallow arbitrary kwargs if the extra config setting is \"forbid\", or if the plugin config says to,\n        *unless* a required dynamic alias is present (since then we can't determine a valid signature).\n        \"\"\"\n        if not config.populate_by_name:\n            if self.is_dynamic_alias_present(fields, bool(config.has_alias_generator)):\n                return False\n        if config.forbid_extra:\n            return True\n        return self.plugin_config.init_forbid_extra\n\n    @staticmethod\n    def is_dynamic_alias_present(fields: list[PydanticModelField], has_alias_generator: bool) -> bool:\n        \"\"\"Returns whether any fields on the model have a \"dynamic alias\", i.e., an alias that cannot be\n        determined during static analysis.\n        \"\"\"\n        for field in fields:\n            if field.has_dynamic_alias:\n                return True\n        if has_alias_generator:\n            for field in fields:\n                if field.alias is None:\n                    return True\n        return False\n\n\nclass ModelConfigData:\n    \"\"\"Pydantic mypy plugin model config class.\"\"\"\n\n    def __init__(\n        self,\n        forbid_extra: bool | None = None,\n        frozen: bool | None = None,\n        from_attributes: bool | None = None,\n        populate_by_name: bool | None = None,\n        has_alias_generator: bool | None = None,\n    ):\n        self.forbid_extra = forbid_extra\n        self.frozen = frozen\n        self.from_attributes = from_attributes\n        self.populate_by_name = populate_by_name\n        self.has_alias_generator = has_alias_generator\n\n    def get_values_dict(self) -> dict[str, Any]:\n        \"\"\"Returns a dict of Pydantic model config names to their values.\n\n        It includes the config if config value is not `None`.\n        \"\"\"\n        return {k: v for k, v in self.__dict__.items() if v is not None}\n\n    def update(self, config: ModelConfigData | None) -> None:\n        \"\"\"Update Pydantic model config values.\"\"\"\n        if config is None:\n            return\n        for k, v in config.get_values_dict().items():\n            setattr(self, k, v)\n\n    def setdefault(self, key: str, value: Any) -> None:\n        \"\"\"Set default value for Pydantic model config if config value is `None`.\"\"\"\n        if getattr(self, key) is None:\n            setattr(self, key, value)\n\n\nERROR_ORM = ErrorCode('pydantic-orm', 'Invalid from_attributes call', 'Pydantic')\nERROR_CONFIG = ErrorCode('pydantic-config', 'Invalid config value', 'Pydantic')\nERROR_ALIAS = ErrorCode('pydantic-alias', 'Dynamic alias disallowed', 'Pydantic')\nERROR_UNEXPECTED = ErrorCode('pydantic-unexpected', 'Unexpected behavior', 'Pydantic')\nERROR_UNTYPED = ErrorCode('pydantic-field', 'Untyped field disallowed', 'Pydantic')\nERROR_FIELD_DEFAULTS = ErrorCode('pydantic-field', 'Invalid Field defaults', 'Pydantic')\nERROR_EXTRA_FIELD_ROOT_MODEL = ErrorCode('pydantic-field', 'Extra field on RootModel subclass', 'Pydantic')\n\n\ndef error_from_attributes(model_name: str, api: CheckerPluginInterface, context: Context) -> None:\n    \"\"\"Emits an error when the model does not have `from_attributes=True`.\"\"\"\n    api.fail(f'\"{model_name}\" does not have from_attributes=True', context, code=ERROR_ORM)\n\n\ndef error_invalid_config_value(name: str, api: SemanticAnalyzerPluginInterface, context: Context) -> None:\n    \"\"\"Emits an error when the config value is invalid.\"\"\"\n    api.fail(f'Invalid value for \"Config.{name}\"', context, code=ERROR_CONFIG)\n\n\ndef error_required_dynamic_aliases(api: SemanticAnalyzerPluginInterface, context: Context) -> None:\n    \"\"\"Emits required dynamic aliases error.\n\n    This will be called when `warn_required_dynamic_aliases=True`.\n    \"\"\"\n    api.fail('Required dynamic aliases disallowed', context, code=ERROR_ALIAS)\n\n\ndef error_unexpected_behavior(\n    detail: str, api: CheckerPluginInterface | SemanticAnalyzerPluginInterface, context: Context\n) -> None:  # pragma: no cover\n    \"\"\"Emits unexpected behavior error.\"\"\"\n    # Can't think of a good way to test this, but I confirmed it renders as desired by adding to a non-error path\n    link = 'https://github.com/pydantic/pydantic/issues/new/choose'\n    full_message = f'The pydantic mypy plugin ran into unexpected behavior: {detail}\\n'\n    full_message += f'Please consider reporting this bug at {link} so we can try to fix it!'\n    api.fail(full_message, context, code=ERROR_UNEXPECTED)\n\n\ndef error_untyped_fields(api: SemanticAnalyzerPluginInterface, context: Context) -> None:\n    \"\"\"Emits an error when there is an untyped field in the model.\"\"\"\n    api.fail('Untyped fields disallowed', context, code=ERROR_UNTYPED)\n\n\ndef error_extra_fields_on_root_model(api: CheckerPluginInterface, context: Context) -> None:\n    \"\"\"Emits an error when there is more than just a root field defined for a subclass of RootModel.\"\"\"\n    api.fail('Only `root` is allowed as a field of a `RootModel`', context, code=ERROR_EXTRA_FIELD_ROOT_MODEL)\n\n\ndef error_default_and_default_factory_specified(api: CheckerPluginInterface, context: Context) -> None:\n    \"\"\"Emits an error when `Field` has both `default` and `default_factory` together.\"\"\"\n    api.fail('Field default and default_factory cannot be specified together', context, code=ERROR_FIELD_DEFAULTS)\n\n\ndef add_method(\n    api: SemanticAnalyzerPluginInterface | CheckerPluginInterface,\n    cls: ClassDef,\n    name: str,\n    args: list[Argument],\n    return_type: Type,\n    self_type: Type | None = None,\n    tvar_def: TypeVarDef | None = None,\n    is_classmethod: bool = False,\n) -> None:\n    \"\"\"Very closely related to `mypy.plugins.common.add_method_to_class`, with a few pydantic-specific changes.\"\"\"\n    info = cls.info\n\n    # First remove any previously generated methods with the same name\n    # to avoid clashes and problems in the semantic analyzer.\n    if name in info.names:\n        sym = info.names[name]\n        if sym.plugin_generated and isinstance(sym.node, FuncDef):\n            cls.defs.body.remove(sym.node)  # pragma: no cover\n\n    if isinstance(api, SemanticAnalyzerPluginInterface):\n        function_type = api.named_type('builtins.function')\n    else:\n        function_type = api.named_generic_type('builtins.function', [])\n\n    if is_classmethod:\n        self_type = self_type or TypeType(fill_typevars(info))\n        first = [Argument(Var('_cls'), self_type, None, ARG_POS, True)]\n    else:\n        self_type = self_type or fill_typevars(info)\n        # `self` is positional *ONLY* here, but this can't be expressed\n        # fully in the mypy internal API. ARG_POS is the closest we can get.\n        # Using ARG_POS will, however, give mypy errors if a `self` field\n        # is present on a model:\n        #\n        #     Name \"self\" already defined (possibly by an import)  [no-redef]\n        #\n        # As a workaround, we give this argument a name that will\n        # never conflict. By its positional nature, this name will not\n        # be used or exposed to users.\n        first = [Argument(Var('__pydantic_self__'), self_type, None, ARG_POS)]\n    args = first + args\n\n    arg_types, arg_names, arg_kinds = [], [], []\n    for arg in args:\n        assert arg.type_annotation, 'All arguments must be fully typed.'\n        arg_types.append(arg.type_annotation)\n        arg_names.append(arg.variable.name)\n        arg_kinds.append(arg.kind)\n\n    signature = CallableType(arg_types, arg_kinds, arg_names, return_type, function_type)\n    if tvar_def:\n        signature.variables = [tvar_def]\n\n    func = FuncDef(name, args, Block([PassStmt()]))\n    func.info = info\n    func.type = set_callable_name(signature, func)\n    func.is_class = is_classmethod\n    func._fullname = info.fullname + '.' + name\n    func.line = info.line\n\n    # NOTE: we would like the plugin generated node to dominate, but we still\n    # need to keep any existing definitions so they get semantically analyzed.\n    if name in info.names:\n        # Get a nice unique name instead.\n        r_name = get_unique_redefinition_name(name, info.names)\n        info.names[r_name] = info.names[name]\n\n    # Add decorator for is_classmethod\n    # The dataclasses plugin claims this is unnecessary for classmethods, but not including it results in a\n    # signature incompatible with the superclass, which causes mypy errors to occur for every subclass of BaseModel.\n    if is_classmethod:\n        func.is_decorated = True\n        v = Var(name, func.type)\n        v.info = info\n        v._fullname = func._fullname\n        v.is_classmethod = True\n        dec = Decorator(func, [NameExpr('classmethod')], v)\n        dec.line = info.line\n        sym = SymbolTableNode(MDEF, dec)\n    else:\n        sym = SymbolTableNode(MDEF, func)\n    sym.plugin_generated = True\n    info.names[name] = sym\n\n    info.defn.defs.body.append(func)\n\n\ndef parse_toml(config_file: str) -> dict[str, Any] | None:\n    \"\"\"Returns a dict of config keys to values.\n\n    It reads configs from toml file and returns `None` if the file is not a toml file.\n    \"\"\"\n    if not config_file.endswith('.toml'):\n        return None\n\n    if sys.version_info >= (3, 11):\n        import tomllib as toml_\n    else:\n        try:\n            import tomli as toml_\n        except ImportError:  # pragma: no cover\n            import warnings\n\n            warnings.warn('No TOML parser installed, cannot read configuration from `pyproject.toml`.')\n            return None\n\n    with open(config_file, 'rb') as rf:\n        return toml_.load(rf)\n", "pydantic/networks.py": "\"\"\"The networks module contains types for common network-related fields.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses as _dataclasses\nimport re\nfrom importlib.metadata import version\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom typing import TYPE_CHECKING, Any\n\nfrom pydantic_core import MultiHostUrl, PydanticCustomError, Url, core_schema\nfrom typing_extensions import Annotated, Self, TypeAlias\n\nfrom ._internal import _fields, _repr, _schema_generation_shared\nfrom ._migration import getattr_migration\nfrom .annotated_handlers import GetCoreSchemaHandler\nfrom .json_schema import JsonSchemaValue\n\nif TYPE_CHECKING:\n    import email_validator\n\n    NetworkType: TypeAlias = 'str | bytes | int | tuple[str | bytes | int, str | int]'\n\nelse:\n    email_validator = None\n\n\n__all__ = [\n    'AnyUrl',\n    'AnyHttpUrl',\n    'FileUrl',\n    'FtpUrl',\n    'HttpUrl',\n    'WebsocketUrl',\n    'AnyWebsocketUrl',\n    'UrlConstraints',\n    'EmailStr',\n    'NameEmail',\n    'IPvAnyAddress',\n    'IPvAnyInterface',\n    'IPvAnyNetwork',\n    'PostgresDsn',\n    'CockroachDsn',\n    'AmqpDsn',\n    'RedisDsn',\n    'MongoDsn',\n    'KafkaDsn',\n    'NatsDsn',\n    'validate_email',\n    'MySQLDsn',\n    'MariaDBDsn',\n    'ClickHouseDsn',\n]\n\n\n@_dataclasses.dataclass\nclass UrlConstraints(_fields.PydanticMetadata):\n    \"\"\"Url constraints.\n\n    Attributes:\n        max_length: The maximum length of the url. Defaults to `None`.\n        allowed_schemes: The allowed schemes. Defaults to `None`.\n        host_required: Whether the host is required. Defaults to `None`.\n        default_host: The default host. Defaults to `None`.\n        default_port: The default port. Defaults to `None`.\n        default_path: The default path. Defaults to `None`.\n    \"\"\"\n\n    max_length: int | None = None\n    allowed_schemes: list[str] | None = None\n    host_required: bool | None = None\n    default_host: str | None = None\n    default_port: int | None = None\n    default_path: str | None = None\n\n    def __hash__(self) -> int:\n        return hash(\n            (\n                self.max_length,\n                tuple(self.allowed_schemes) if self.allowed_schemes is not None else None,\n                self.host_required,\n                self.default_host,\n                self.default_port,\n                self.default_path,\n            )\n        )\n\n\nAnyUrl = Url\n\"\"\"Base type for all URLs.\n\n* Any scheme allowed\n* Top-level domain (TLD) not required\n* Host required\n\nAssuming an input URL of `http://samuel:pass@example.com:8000/the/path/?query=here#fragment=is;this=bit`,\nthe types export the following properties:\n\n- `scheme`: the URL scheme (`http`), always set.\n- `host`: the URL host (`example.com`), always set.\n- `username`: optional username if included (`samuel`).\n- `password`: optional password if included (`pass`).\n- `port`: optional port (`8000`).\n- `path`: optional path (`/the/path/`).\n- `query`: optional URL query (for example, `GET` arguments or \"search string\", such as `query=here`).\n- `fragment`: optional fragment (`fragment=is;this=bit`).\n\"\"\"\nAnyHttpUrl = Annotated[Url, UrlConstraints(allowed_schemes=['http', 'https'])]\n\"\"\"A type that will accept any http or https URL.\n\n* TLD not required\n* Host required\n\"\"\"\nHttpUrl = Annotated[Url, UrlConstraints(max_length=2083, allowed_schemes=['http', 'https'])]\n\"\"\"A type that will accept any http or https URL.\n\n* TLD not required\n* Host required\n* Max length 2083\n\n```py\nfrom pydantic import BaseModel, HttpUrl, ValidationError\n\nclass MyModel(BaseModel):\n    url: HttpUrl\n\nm = MyModel(url='http://www.example.com')  # (1)!\nprint(m.url)\n#> http://www.example.com/\n\ntry:\n    MyModel(url='ftp://invalid.url')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for MyModel\n    url\n      URL scheme should be 'http' or 'https' [type=url_scheme, input_value='ftp://invalid.url', input_type=str]\n    '''\n\ntry:\n    MyModel(url='not a url')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for MyModel\n    url\n      Input should be a valid URL, relative URL without a base [type=url_parsing, input_value='not a url', input_type=str]\n    '''\n```\n\n1. Note: mypy would prefer `m = MyModel(url=HttpUrl('http://www.example.com'))`, but Pydantic will convert the string to an HttpUrl instance anyway.\n\n\"International domains\" (e.g. a URL where the host or TLD includes non-ascii characters) will be encoded via\n[punycode](https://en.wikipedia.org/wiki/Punycode) (see\n[this article](https://www.xudongz.com/blog/2017/idn-phishing/) for a good description of why this is important):\n\n```py\nfrom pydantic import BaseModel, HttpUrl\n\nclass MyModel(BaseModel):\n    url: HttpUrl\n\nm1 = MyModel(url='http://puny\u00a3code.com')\nprint(m1.url)\n#> http://xn--punycode-eja.com/\nm2 = MyModel(url='https://www.\u0430\u0440\u0440\u04cf\u0435.com/')\nprint(m2.url)\n#> https://www.xn--80ak6aa92e.com/\nm3 = MyModel(url='https://www.example.\u73e0\u5b9d/')\nprint(m3.url)\n#> https://www.example.xn--pbt977c/\n```\n\n\n!!! warning \"Underscores in Hostnames\"\n    In Pydantic, underscores are allowed in all parts of a domain except the TLD.\n    Technically this might be wrong - in theory the hostname cannot have underscores, but subdomains can.\n\n    To explain this; consider the following two cases:\n\n    - `exam_ple.co.uk`: the hostname is `exam_ple`, which should not be allowed since it contains an underscore.\n    - `foo_bar.example.com` the hostname is `example`, which should be allowed since the underscore is in the subdomain.\n\n    Without having an exhaustive list of TLDs, it would be impossible to differentiate between these two. Therefore\n    underscores are allowed, but you can always do further validation in a validator if desired.\n\n    Also, Chrome, Firefox, and Safari all currently accept `http://exam_ple.com` as a URL, so we're in good\n    (or at least big) company.\n\"\"\"\nAnyWebsocketUrl = Annotated[Url, UrlConstraints(allowed_schemes=['ws', 'wss'])]\n\"\"\"A type that will accept any ws or wss URL.\n\n* TLD not required\n* Host required\n\"\"\"\nWebsocketUrl = Annotated[Url, UrlConstraints(max_length=2083, allowed_schemes=['ws', 'wss'])]\n\"\"\"A type that will accept any ws or wss URL.\n\n* TLD not required\n* Host required\n* Max length 2083\n\"\"\"\nFileUrl = Annotated[Url, UrlConstraints(allowed_schemes=['file'])]\n\"\"\"A type that will accept any file URL.\n\n* Host not required\n\"\"\"\nFtpUrl = Annotated[Url, UrlConstraints(allowed_schemes=['ftp'])]\n\"\"\"A type that will accept ftp URL.\n\n* TLD not required\n* Host required\n\"\"\"\nPostgresDsn = Annotated[\n    MultiHostUrl,\n    UrlConstraints(\n        host_required=True,\n        allowed_schemes=[\n            'postgres',\n            'postgresql',\n            'postgresql+asyncpg',\n            'postgresql+pg8000',\n            'postgresql+psycopg',\n            'postgresql+psycopg2',\n            'postgresql+psycopg2cffi',\n            'postgresql+py-postgresql',\n            'postgresql+pygresql',\n        ],\n    ),\n]\n\"\"\"A type that will accept any Postgres DSN.\n\n* User info required\n* TLD not required\n* Host required\n* Supports multiple hosts\n\nIf further validation is required, these properties can be used by validators to enforce specific behaviour:\n\n```py\nfrom pydantic import (\n    BaseModel,\n    HttpUrl,\n    PostgresDsn,\n    ValidationError,\n    field_validator,\n)\n\nclass MyModel(BaseModel):\n    url: HttpUrl\n\nm = MyModel(url='http://www.example.com')\n\n# the repr() method for a url will display all properties of the url\nprint(repr(m.url))\n#> Url('http://www.example.com/')\nprint(m.url.scheme)\n#> http\nprint(m.url.host)\n#> www.example.com\nprint(m.url.port)\n#> 80\n\nclass MyDatabaseModel(BaseModel):\n    db: PostgresDsn\n\n    @field_validator('db')\n    def check_db_name(cls, v):\n        assert v.path and len(v.path) > 1, 'database must be provided'\n        return v\n\nm = MyDatabaseModel(db='postgres://user:pass@localhost:5432/foobar')\nprint(m.db)\n#> postgres://user:pass@localhost:5432/foobar\n\ntry:\n    MyDatabaseModel(db='postgres://user:pass@localhost:5432')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for MyDatabaseModel\n    db\n      Assertion failed, database must be provided\n    assert (None)\n     +  where None = MultiHostUrl('postgres://user:pass@localhost:5432').path [type=assertion_error, input_value='postgres://user:pass@localhost:5432', input_type=str]\n    '''\n```\n\"\"\"\n\nCockroachDsn = Annotated[\n    Url,\n    UrlConstraints(\n        host_required=True,\n        allowed_schemes=[\n            'cockroachdb',\n            'cockroachdb+psycopg2',\n            'cockroachdb+asyncpg',\n        ],\n    ),\n]\n\"\"\"A type that will accept any Cockroach DSN.\n\n* User info required\n* TLD not required\n* Host required\n\"\"\"\nAmqpDsn = Annotated[Url, UrlConstraints(allowed_schemes=['amqp', 'amqps'])]\n\"\"\"A type that will accept any AMQP DSN.\n\n* User info required\n* TLD not required\n* Host required\n\"\"\"\nRedisDsn = Annotated[\n    Url,\n    UrlConstraints(allowed_schemes=['redis', 'rediss'], default_host='localhost', default_port=6379, default_path='/0'),\n]\n\"\"\"A type that will accept any Redis DSN.\n\n* User info required\n* TLD not required\n* Host required (e.g., `rediss://:pass@localhost`)\n\"\"\"\nMongoDsn = Annotated[MultiHostUrl, UrlConstraints(allowed_schemes=['mongodb', 'mongodb+srv'], default_port=27017)]\n\"\"\"A type that will accept any MongoDB DSN.\n\n* User info not required\n* Database name not required\n* Port not required\n* User info may be passed without user part (e.g., `mongodb://mongodb0.example.com:27017`).\n\"\"\"\nKafkaDsn = Annotated[Url, UrlConstraints(allowed_schemes=['kafka'], default_host='localhost', default_port=9092)]\n\"\"\"A type that will accept any Kafka DSN.\n\n* User info required\n* TLD not required\n* Host required\n\"\"\"\nNatsDsn = Annotated[\n    MultiHostUrl, UrlConstraints(allowed_schemes=['nats', 'tls', 'ws'], default_host='localhost', default_port=4222)\n]\n\"\"\"A type that will accept any NATS DSN.\n\nNATS is a connective technology built for the ever increasingly hyper-connected world.\nIt is a single technology that enables applications to securely communicate across\nany combination of cloud vendors, on-premise, edge, web and mobile, and devices.\nMore: https://nats.io\n\"\"\"\nMySQLDsn = Annotated[\n    Url,\n    UrlConstraints(\n        allowed_schemes=[\n            'mysql',\n            'mysql+mysqlconnector',\n            'mysql+aiomysql',\n            'mysql+asyncmy',\n            'mysql+mysqldb',\n            'mysql+pymysql',\n            'mysql+cymysql',\n            'mysql+pyodbc',\n        ],\n        default_port=3306,\n    ),\n]\n\"\"\"A type that will accept any MySQL DSN.\n\n* User info required\n* TLD not required\n* Host required\n\"\"\"\nMariaDBDsn = Annotated[\n    Url,\n    UrlConstraints(\n        allowed_schemes=['mariadb', 'mariadb+mariadbconnector', 'mariadb+pymysql'],\n        default_port=3306,\n    ),\n]\n\"\"\"A type that will accept any MariaDB DSN.\n\n* User info required\n* TLD not required\n* Host required\n\"\"\"\nClickHouseDsn = Annotated[\n    Url,\n    UrlConstraints(\n        allowed_schemes=['clickhouse+native', 'clickhouse+asynch'],\n        default_host='localhost',\n        default_port=9000,\n    ),\n]\n\"\"\"A type that will accept any ClickHouse DSN.\n\n* User info required\n* TLD not required\n* Host required\n\"\"\"\n\n\ndef import_email_validator() -> None:\n    global email_validator\n    try:\n        import email_validator\n    except ImportError as e:\n        raise ImportError('email-validator is not installed, run `pip install pydantic[email]`') from e\n    if not version('email-validator').partition('.')[0] == '2':\n        raise ImportError('email-validator version >= 2.0 required, run pip install -U email-validator')\n\n\nif TYPE_CHECKING:\n    EmailStr = Annotated[str, ...]\nelse:\n\n    class EmailStr:\n        \"\"\"\n        Info:\n            To use this type, you need to install the optional\n            [`email-validator`](https://github.com/JoshData/python-email-validator) package:\n\n            ```bash\n            pip install email-validator\n            ```\n\n        Validate email addresses.\n\n        ```py\n        from pydantic import BaseModel, EmailStr\n\n        class Model(BaseModel):\n            email: EmailStr\n\n        print(Model(email='contact@mail.com'))\n        #> email='contact@mail.com'\n        ```\n        \"\"\"  # noqa: D212\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            _source: type[Any],\n            _handler: GetCoreSchemaHandler,\n        ) -> core_schema.CoreSchema:\n            import_email_validator()\n            return core_schema.no_info_after_validator_function(cls._validate, core_schema.str_schema())\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = handler(core_schema)\n            field_schema.update(type='string', format='email')\n            return field_schema\n\n        @classmethod\n        def _validate(cls, input_value: str, /) -> str:\n            return validate_email(input_value)[1]\n\n\nclass NameEmail(_repr.Representation):\n    \"\"\"\n    Info:\n        To use this type, you need to install the optional\n        [`email-validator`](https://github.com/JoshData/python-email-validator) package:\n\n        ```bash\n        pip install email-validator\n        ```\n\n    Validate a name and email address combination, as specified by\n    [RFC 5322](https://datatracker.ietf.org/doc/html/rfc5322#section-3.4).\n\n    The `NameEmail` has two properties: `name` and `email`.\n    In case the `name` is not provided, it's inferred from the email address.\n\n    ```py\n    from pydantic import BaseModel, NameEmail\n\n    class User(BaseModel):\n        email: NameEmail\n\n    user = User(email='Fred Bloggs <fred.bloggs@example.com>')\n    print(user.email)\n    #> Fred Bloggs <fred.bloggs@example.com>\n    print(user.email.name)\n    #> Fred Bloggs\n\n    user = User(email='fred.bloggs@example.com')\n    print(user.email)\n    #> fred.bloggs <fred.bloggs@example.com>\n    print(user.email.name)\n    #> fred.bloggs\n    ```\n    \"\"\"  # noqa: D212\n\n    __slots__ = 'name', 'email'\n\n    def __init__(self, name: str, email: str):\n        self.name = name\n        self.email = email\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, NameEmail) and (self.name, self.email) == (other.name, other.email)\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = handler(core_schema)\n        field_schema.update(type='string', format='name-email')\n        return field_schema\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls,\n        _source: type[Any],\n        _handler: GetCoreSchemaHandler,\n    ) -> core_schema.CoreSchema:\n        import_email_validator()\n\n        return core_schema.no_info_after_validator_function(\n            cls._validate,\n            core_schema.json_or_python_schema(\n                json_schema=core_schema.str_schema(),\n                python_schema=core_schema.union_schema(\n                    [core_schema.is_instance_schema(cls), core_schema.str_schema()],\n                    custom_error_type='name_email_type',\n                    custom_error_message='Input is not a valid NameEmail',\n                ),\n                serialization=core_schema.to_string_ser_schema(),\n            ),\n        )\n\n    @classmethod\n    def _validate(cls, input_value: Self | str, /) -> Self:\n        if isinstance(input_value, str):\n            name, email = validate_email(input_value)\n            return cls(name, email)\n        else:\n            return input_value\n\n    def __str__(self) -> str:\n        if '@' in self.name:\n            return f'\"{self.name}\" <{self.email}>'\n\n        return f'{self.name} <{self.email}>'\n\n\nclass IPvAnyAddress:\n    \"\"\"Validate an IPv4 or IPv6 address.\n\n    ```py\n    from pydantic import BaseModel\n    from pydantic.networks import IPvAnyAddress\n\n    class IpModel(BaseModel):\n        ip: IPvAnyAddress\n\n    print(IpModel(ip='127.0.0.1'))\n    #> ip=IPv4Address('127.0.0.1')\n\n    try:\n        IpModel(ip='http://www.example.com')\n    except ValueError as e:\n        print(e.errors())\n        '''\n        [\n            {\n                'type': 'ip_any_address',\n                'loc': ('ip',),\n                'msg': 'value is not a valid IPv4 or IPv6 address',\n                'input': 'http://www.example.com',\n            }\n        ]\n        '''\n    ```\n    \"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, value: Any) -> IPv4Address | IPv6Address:\n        \"\"\"Validate an IPv4 or IPv6 address.\"\"\"\n        try:\n            return IPv4Address(value)\n        except ValueError:\n            pass\n\n        try:\n            return IPv6Address(value)\n        except ValueError:\n            raise PydanticCustomError('ip_any_address', 'value is not a valid IPv4 or IPv6 address')\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = {}\n        field_schema.update(type='string', format='ipvanyaddress')\n        return field_schema\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls,\n        _source: type[Any],\n        _handler: GetCoreSchemaHandler,\n    ) -> core_schema.CoreSchema:\n        return core_schema.no_info_plain_validator_function(\n            cls._validate, serialization=core_schema.to_string_ser_schema()\n        )\n\n    @classmethod\n    def _validate(cls, input_value: Any, /) -> IPv4Address | IPv6Address:\n        return cls(input_value)  # type: ignore[return-value]\n\n\nclass IPvAnyInterface:\n    \"\"\"Validate an IPv4 or IPv6 interface.\"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, value: NetworkType) -> IPv4Interface | IPv6Interface:\n        \"\"\"Validate an IPv4 or IPv6 interface.\"\"\"\n        try:\n            return IPv4Interface(value)\n        except ValueError:\n            pass\n\n        try:\n            return IPv6Interface(value)\n        except ValueError:\n            raise PydanticCustomError('ip_any_interface', 'value is not a valid IPv4 or IPv6 interface')\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = {}\n        field_schema.update(type='string', format='ipvanyinterface')\n        return field_schema\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls,\n        _source: type[Any],\n        _handler: GetCoreSchemaHandler,\n    ) -> core_schema.CoreSchema:\n        return core_schema.no_info_plain_validator_function(\n            cls._validate, serialization=core_schema.to_string_ser_schema()\n        )\n\n    @classmethod\n    def _validate(cls, input_value: NetworkType, /) -> IPv4Interface | IPv6Interface:\n        return cls(input_value)  # type: ignore[return-value]\n\n\nIPvAnyNetworkType: TypeAlias = 'IPv4Network | IPv6Network'\n\nif TYPE_CHECKING:\n    IPvAnyNetwork = IPvAnyNetworkType\nelse:\n\n    class IPvAnyNetwork:\n        \"\"\"Validate an IPv4 or IPv6 network.\"\"\"\n\n        __slots__ = ()\n\n        def __new__(cls, value: NetworkType) -> IPvAnyNetworkType:\n            \"\"\"Validate an IPv4 or IPv6 network.\"\"\"\n            # Assume IP Network is defined with a default value for `strict` argument.\n            # Define your own class if you want to specify network address check strictness.\n            try:\n                return IPv4Network(value)\n            except ValueError:\n                pass\n\n            try:\n                return IPv6Network(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_network', 'value is not a valid IPv4 or IPv6 network')\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = {}\n            field_schema.update(type='string', format='ipvanynetwork')\n            return field_schema\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            _source: type[Any],\n            _handler: GetCoreSchemaHandler,\n        ) -> core_schema.CoreSchema:\n            return core_schema.no_info_plain_validator_function(\n                cls._validate, serialization=core_schema.to_string_ser_schema()\n            )\n\n        @classmethod\n        def _validate(cls, input_value: NetworkType, /) -> IPvAnyNetworkType:\n            return cls(input_value)  # type: ignore[return-value]\n\n\ndef _build_pretty_email_regex() -> re.Pattern[str]:\n    name_chars = r'[\\w!#$%&\\'*+\\-/=?^_`{|}~]'\n    unquoted_name_group = rf'((?:{name_chars}+\\s+)*{name_chars}+)'\n    quoted_name_group = r'\"((?:[^\"]|\\\")+)\"'\n    email_group = r'<\\s*(.+)\\s*>'\n    return re.compile(rf'\\s*(?:{unquoted_name_group}|{quoted_name_group})?\\s*{email_group}\\s*')\n\n\npretty_email_regex = _build_pretty_email_regex()\n\nMAX_EMAIL_LENGTH = 2048\n\"\"\"Maximum length for an email.\nA somewhat arbitrary but very generous number compared to what is allowed by most implementations.\n\"\"\"\n\n\ndef validate_email(value: str) -> tuple[str, str]:\n    \"\"\"Email address validation using [email-validator](https://pypi.org/project/email-validator/).\n\n    Note:\n        Note that:\n\n        * Raw IP address (literal) domain parts are not allowed.\n        * `\"John Doe <local_part@domain.com>\"` style \"pretty\" email addresses are processed.\n        * Spaces are striped from the beginning and end of addresses, but no error is raised.\n    \"\"\"\n    if email_validator is None:\n        import_email_validator()\n\n    if len(value) > MAX_EMAIL_LENGTH:\n        raise PydanticCustomError(\n            'value_error',\n            'value is not a valid email address: {reason}',\n            {'reason': f'Length must not exceed {MAX_EMAIL_LENGTH} characters'},\n        )\n\n    m = pretty_email_regex.fullmatch(value)\n    name: str | None = None\n    if m:\n        unquoted_name, quoted_name, value = m.groups()\n        name = unquoted_name or quoted_name\n\n    email = value.strip()\n\n    try:\n        parts = email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise PydanticCustomError(\n            'value_error', 'value is not a valid email address: {reason}', {'reason': str(e.args[0])}\n        ) from e\n\n    email = parts.normalized\n    assert email is not None\n    name = name or parts.local_part\n    return name, email\n\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/decorator.py": "\"\"\"The `decorator` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/datetime_parse.py": "\"\"\"The `datetime_parse` module is a backport module from V1.\"\"\"\n\nfrom ._migration import getattr_migration\n\n__getattr__ = getattr_migration(__name__)\n", "pydantic/experimental/pipeline.py": "\"\"\"Experimental pipeline API functionality. Be careful with this API, it's subject to change.\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nimport operator\nimport re\nimport sys\nfrom collections import deque\nfrom collections.abc import Container\nfrom dataclasses import dataclass\nfrom decimal import Decimal\nfrom functools import cached_property, partial\nfrom typing import TYPE_CHECKING, Any, Callable, Generic, Pattern, Protocol, TypeVar, Union, overload\n\nimport annotated_types\nfrom typing_extensions import Annotated\n\nif TYPE_CHECKING:\n    from pydantic_core import core_schema as cs\n\n    from pydantic import GetCoreSchemaHandler\n\nfrom pydantic._internal._internal_dataclass import slots_true as _slots_true\n\nif sys.version_info < (3, 10):\n    EllipsisType = type(Ellipsis)\nelse:\n    from types import EllipsisType\n\n__all__ = ['validate_as', 'validate_as_deferred', 'transform']\n\n_slots_frozen = {**_slots_true, 'frozen': True}\n\n\n@dataclass(**_slots_frozen)\nclass _ValidateAs:\n    tp: type[Any]\n    strict: bool = False\n\n\n@dataclass\nclass _ValidateAsDefer:\n    func: Callable[[], type[Any]]\n\n    @cached_property\n    def tp(self) -> type[Any]:\n        return self.func()\n\n\n@dataclass(**_slots_frozen)\nclass _Transform:\n    func: Callable[[Any], Any]\n\n\n@dataclass(**_slots_frozen)\nclass _PipelineOr:\n    left: _Pipeline[Any, Any]\n    right: _Pipeline[Any, Any]\n\n\n@dataclass(**_slots_frozen)\nclass _PipelineAnd:\n    left: _Pipeline[Any, Any]\n    right: _Pipeline[Any, Any]\n\n\n@dataclass(**_slots_frozen)\nclass _Eq:\n    value: Any\n\n\n@dataclass(**_slots_frozen)\nclass _NotEq:\n    value: Any\n\n\n@dataclass(**_slots_frozen)\nclass _In:\n    values: Container[Any]\n\n\n@dataclass(**_slots_frozen)\nclass _NotIn:\n    values: Container[Any]\n\n\n_ConstraintAnnotation = Union[\n    annotated_types.Le,\n    annotated_types.Ge,\n    annotated_types.Lt,\n    annotated_types.Gt,\n    annotated_types.Len,\n    annotated_types.MultipleOf,\n    annotated_types.Timezone,\n    annotated_types.Interval,\n    annotated_types.Predicate,\n    # common predicates not included in annotated_types\n    _Eq,\n    _NotEq,\n    _In,\n    _NotIn,\n    # regular expressions\n    Pattern[str],\n]\n\n\n@dataclass(**_slots_frozen)\nclass _Constraint:\n    constraint: _ConstraintAnnotation\n\n\n_Step = Union[_ValidateAs, _ValidateAsDefer, _Transform, _PipelineOr, _PipelineAnd, _Constraint]\n\n_InT = TypeVar('_InT')\n_OutT = TypeVar('_OutT')\n_NewOutT = TypeVar('_NewOutT')\n\n\nclass _FieldTypeMarker:\n    pass\n\n\n# TODO: ultimately, make this public, see https://github.com/pydantic/pydantic/pull/9459#discussion_r1628197626\n# Also, make this frozen eventually, but that doesn't work right now because of the generic base\n# Which attempts to modify __orig_base__ and such.\n# We could go with a manual freeze, but that seems overkill for now.\n@dataclass(**_slots_true)\nclass _Pipeline(Generic[_InT, _OutT]):\n    \"\"\"Abstract representation of a chain of validation, transformation, and parsing steps.\"\"\"\n\n    _steps: tuple[_Step, ...]\n\n    def transform(\n        self,\n        func: Callable[[_OutT], _NewOutT],\n    ) -> _Pipeline[_InT, _NewOutT]:\n        \"\"\"Transform the output of the previous step.\n\n        If used as the first step in a pipeline, the type of the field is used.\n        That is, the transformation is applied to after the value is parsed to the field's type.\n        \"\"\"\n        return _Pipeline[_InT, _NewOutT](self._steps + (_Transform(func),))\n\n    @overload\n    def validate_as(self, tp: type[_NewOutT], *, strict: bool = ...) -> _Pipeline[_InT, _NewOutT]: ...\n\n    @overload\n    def validate_as(self, tp: EllipsisType, *, strict: bool = ...) -> _Pipeline[_InT, Any]:  # type: ignore\n        ...\n\n    def validate_as(self, tp: type[_NewOutT] | EllipsisType, *, strict: bool = False) -> _Pipeline[_InT, Any]:  # type: ignore\n        \"\"\"Validate / parse the input into a new type.\n\n        If no type is provided, the type of the field is used.\n\n        Types are parsed in Pydantic's `lax` mode by default,\n        but you can enable `strict` mode by passing `strict=True`.\n        \"\"\"\n        if isinstance(tp, EllipsisType):\n            return _Pipeline[_InT, Any](self._steps + (_ValidateAs(_FieldTypeMarker, strict=strict),))\n        return _Pipeline[_InT, _NewOutT](self._steps + (_ValidateAs(tp, strict=strict),))\n\n    def validate_as_deferred(self, func: Callable[[], type[_NewOutT]]) -> _Pipeline[_InT, _NewOutT]:\n        \"\"\"Parse the input into a new type, deferring resolution of the type until the current class\n        is fully defined.\n\n        This is useful when you need to reference the class in it's own type annotations.\n        \"\"\"\n        return _Pipeline[_InT, _NewOutT](self._steps + (_ValidateAsDefer(func),))\n\n    # constraints\n    @overload\n    def constrain(self: _Pipeline[_InT, _NewOutGe], constraint: annotated_types.Ge) -> _Pipeline[_InT, _NewOutGe]: ...\n\n    @overload\n    def constrain(self: _Pipeline[_InT, _NewOutGt], constraint: annotated_types.Gt) -> _Pipeline[_InT, _NewOutGt]: ...\n\n    @overload\n    def constrain(self: _Pipeline[_InT, _NewOutLe], constraint: annotated_types.Le) -> _Pipeline[_InT, _NewOutLe]: ...\n\n    @overload\n    def constrain(self: _Pipeline[_InT, _NewOutLt], constraint: annotated_types.Lt) -> _Pipeline[_InT, _NewOutLt]: ...\n\n    @overload\n    def constrain(\n        self: _Pipeline[_InT, _NewOutLen], constraint: annotated_types.Len\n    ) -> _Pipeline[_InT, _NewOutLen]: ...\n\n    @overload\n    def constrain(\n        self: _Pipeline[_InT, _NewOutT], constraint: annotated_types.MultipleOf\n    ) -> _Pipeline[_InT, _NewOutT]: ...\n\n    @overload\n    def constrain(\n        self: _Pipeline[_InT, _NewOutDatetime], constraint: annotated_types.Timezone\n    ) -> _Pipeline[_InT, _NewOutDatetime]: ...\n\n    @overload\n    def constrain(self: _Pipeline[_InT, _OutT], constraint: annotated_types.Predicate) -> _Pipeline[_InT, _OutT]: ...\n\n    @overload\n    def constrain(\n        self: _Pipeline[_InT, _NewOutInterval], constraint: annotated_types.Interval\n    ) -> _Pipeline[_InT, _NewOutInterval]: ...\n\n    @overload\n    def constrain(self: _Pipeline[_InT, _OutT], constraint: _Eq) -> _Pipeline[_InT, _OutT]: ...\n\n    @overload\n    def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotEq) -> _Pipeline[_InT, _OutT]: ...\n\n    @overload\n    def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...\n\n    @overload\n    def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotIn) -> _Pipeline[_InT, _OutT]: ...\n\n    @overload\n    def constrain(self: _Pipeline[_InT, _NewOutT], constraint: Pattern[str]) -> _Pipeline[_InT, _NewOutT]: ...\n\n    def constrain(self, constraint: _ConstraintAnnotation) -> Any:\n        \"\"\"Constrain a value to meet a certain condition.\n\n        We support most conditions from `annotated_types`, as well as regular expressions.\n\n        Most of the time you'll be calling a shortcut method like `gt`, `lt`, `len`, etc\n        so you don't need to call this directly.\n        \"\"\"\n        return _Pipeline[_InT, _OutT](self._steps + (_Constraint(constraint),))\n\n    def predicate(self: _Pipeline[_InT, _NewOutT], func: Callable[[_NewOutT], bool]) -> _Pipeline[_InT, _NewOutT]:\n        \"\"\"Constrain a value to meet a certain predicate.\"\"\"\n        return self.constrain(annotated_types.Predicate(func))\n\n    def gt(self: _Pipeline[_InT, _NewOutGt], gt: _NewOutGt) -> _Pipeline[_InT, _NewOutGt]:\n        \"\"\"Constrain a value to be greater than a certain value.\"\"\"\n        return self.constrain(annotated_types.Gt(gt))\n\n    def lt(self: _Pipeline[_InT, _NewOutLt], lt: _NewOutLt) -> _Pipeline[_InT, _NewOutLt]:\n        \"\"\"Constrain a value to be less than a certain value.\"\"\"\n        return self.constrain(annotated_types.Lt(lt))\n\n    def ge(self: _Pipeline[_InT, _NewOutGe], ge: _NewOutGe) -> _Pipeline[_InT, _NewOutGe]:\n        \"\"\"Constrain a value to be greater than or equal to a certain value.\"\"\"\n        return self.constrain(annotated_types.Ge(ge))\n\n    def le(self: _Pipeline[_InT, _NewOutLe], le: _NewOutLe) -> _Pipeline[_InT, _NewOutLe]:\n        \"\"\"Constrain a value to be less than or equal to a certain value.\"\"\"\n        return self.constrain(annotated_types.Le(le))\n\n    def len(self: _Pipeline[_InT, _NewOutLen], min_len: int, max_len: int | None = None) -> _Pipeline[_InT, _NewOutLen]:\n        \"\"\"Constrain a value to have a certain length.\"\"\"\n        return self.constrain(annotated_types.Len(min_len, max_len))\n\n    @overload\n    def multiple_of(self: _Pipeline[_InT, _NewOutDiv], multiple_of: _NewOutDiv) -> _Pipeline[_InT, _NewOutDiv]: ...\n\n    @overload\n    def multiple_of(self: _Pipeline[_InT, _NewOutMod], multiple_of: _NewOutMod) -> _Pipeline[_InT, _NewOutMod]: ...\n\n    def multiple_of(self: _Pipeline[_InT, Any], multiple_of: Any) -> _Pipeline[_InT, Any]:\n        \"\"\"Constrain a value to be a multiple of a certain number.\"\"\"\n        return self.constrain(annotated_types.MultipleOf(multiple_of))\n\n    def eq(self: _Pipeline[_InT, _OutT], value: _OutT) -> _Pipeline[_InT, _OutT]:\n        \"\"\"Constrain a value to be equal to a certain value.\"\"\"\n        return self.constrain(_Eq(value))\n\n    def not_eq(self: _Pipeline[_InT, _OutT], value: _OutT) -> _Pipeline[_InT, _OutT]:\n        \"\"\"Constrain a value to not be equal to a certain value.\"\"\"\n        return self.constrain(_NotEq(value))\n\n    def in_(self: _Pipeline[_InT, _OutT], values: Container[_OutT]) -> _Pipeline[_InT, _OutT]:\n        \"\"\"Constrain a value to be in a certain set.\"\"\"\n        return self.constrain(_In(values))\n\n    def not_in(self: _Pipeline[_InT, _OutT], values: Container[_OutT]) -> _Pipeline[_InT, _OutT]:\n        \"\"\"Constrain a value to not be in a certain set.\"\"\"\n        return self.constrain(_NotIn(values))\n\n    # timezone methods\n    def datetime_tz_naive(self: _Pipeline[_InT, datetime.datetime]) -> _Pipeline[_InT, datetime.datetime]:\n        return self.constrain(annotated_types.Timezone(None))\n\n    def datetime_tz_aware(self: _Pipeline[_InT, datetime.datetime]) -> _Pipeline[_InT, datetime.datetime]:\n        return self.constrain(annotated_types.Timezone(...))\n\n    def datetime_tz(\n        self: _Pipeline[_InT, datetime.datetime], tz: datetime.tzinfo\n    ) -> _Pipeline[_InT, datetime.datetime]:\n        return self.constrain(annotated_types.Timezone(tz))  # type: ignore\n\n    def datetime_with_tz(\n        self: _Pipeline[_InT, datetime.datetime], tz: datetime.tzinfo | None\n    ) -> _Pipeline[_InT, datetime.datetime]:\n        return self.transform(partial(datetime.datetime.replace, tzinfo=tz))\n\n    # string methods\n    def str_lower(self: _Pipeline[_InT, str]) -> _Pipeline[_InT, str]:\n        return self.transform(str.lower)\n\n    def str_upper(self: _Pipeline[_InT, str]) -> _Pipeline[_InT, str]:\n        return self.transform(str.upper)\n\n    def str_title(self: _Pipeline[_InT, str]) -> _Pipeline[_InT, str]:\n        return self.transform(str.title)\n\n    def str_strip(self: _Pipeline[_InT, str]) -> _Pipeline[_InT, str]:\n        return self.transform(str.strip)\n\n    def str_pattern(self: _Pipeline[_InT, str], pattern: str) -> _Pipeline[_InT, str]:\n        return self.constrain(re.compile(pattern))\n\n    def str_contains(self: _Pipeline[_InT, str], substring: str) -> _Pipeline[_InT, str]:\n        return self.predicate(lambda v: substring in v)\n\n    def str_starts_with(self: _Pipeline[_InT, str], prefix: str) -> _Pipeline[_InT, str]:\n        return self.predicate(lambda v: v.startswith(prefix))\n\n    def str_ends_with(self: _Pipeline[_InT, str], suffix: str) -> _Pipeline[_InT, str]:\n        return self.predicate(lambda v: v.endswith(suffix))\n\n    # operators\n    def otherwise(self, other: _Pipeline[_OtherIn, _OtherOut]) -> _Pipeline[_InT | _OtherIn, _OutT | _OtherOut]:\n        \"\"\"Combine two validation chains, returning the result of the first chain if it succeeds, and the second chain if it fails.\"\"\"\n        return _Pipeline((_PipelineOr(self, other),))\n\n    __or__ = otherwise\n\n    def then(self, other: _Pipeline[_OutT, _OtherOut]) -> _Pipeline[_InT, _OtherOut]:\n        \"\"\"Pipe the result of one validation chain into another.\"\"\"\n        return _Pipeline((_PipelineAnd(self, other),))\n\n    __and__ = then\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> cs.CoreSchema:\n        from pydantic_core import core_schema as cs\n\n        queue = deque(self._steps)\n\n        s = None\n\n        while queue:\n            step = queue.popleft()\n            s = _apply_step(step, s, handler, source_type)\n\n        s = s or cs.any_schema()\n        return s\n\n    def __supports_type__(self, _: _OutT) -> bool:\n        raise NotImplementedError\n\n\nvalidate_as = _Pipeline[Any, Any](()).validate_as\nvalidate_as_deferred = _Pipeline[Any, Any](()).validate_as_deferred\ntransform = _Pipeline[Any, Any]((_ValidateAs(_FieldTypeMarker),)).transform\n\n\ndef _check_func(\n    func: Callable[[Any], bool], predicate_err: str | Callable[[], str], s: cs.CoreSchema | None\n) -> cs.CoreSchema:\n    from pydantic_core import core_schema as cs\n\n    def handler(v: Any) -> Any:\n        if func(v):\n            return v\n        raise ValueError(f'Expected {predicate_err if isinstance(predicate_err, str) else predicate_err()}')\n\n    if s is None:\n        return cs.no_info_plain_validator_function(handler)\n    else:\n        return cs.no_info_after_validator_function(handler, s)\n\n\ndef _apply_step(step: _Step, s: cs.CoreSchema | None, handler: GetCoreSchemaHandler, source_type: Any) -> cs.CoreSchema:\n    from pydantic_core import core_schema as cs\n\n    if isinstance(step, _ValidateAs):\n        s = _apply_parse(s, step.tp, step.strict, handler, source_type)\n    elif isinstance(step, _ValidateAsDefer):\n        s = _apply_parse(s, step.tp, False, handler, source_type)\n    elif isinstance(step, _Transform):\n        s = _apply_transform(s, step.func, handler)\n    elif isinstance(step, _Constraint):\n        s = _apply_constraint(s, step.constraint)\n    elif isinstance(step, _PipelineOr):\n        s = cs.union_schema([handler(step.left), handler(step.right)])\n    else:\n        assert isinstance(step, _PipelineAnd)\n        s = cs.chain_schema([handler(step.left), handler(step.right)])\n    return s\n\n\ndef _apply_parse(\n    s: cs.CoreSchema | None,\n    tp: type[Any],\n    strict: bool,\n    handler: GetCoreSchemaHandler,\n    source_type: Any,\n) -> cs.CoreSchema:\n    from pydantic_core import core_schema as cs\n\n    from pydantic import Strict\n\n    if tp is _FieldTypeMarker:\n        return handler(source_type)\n\n    if strict:\n        tp = Annotated[tp, Strict()]  # type: ignore\n\n    if s and s['type'] == 'any':\n        return handler(tp)\n    else:\n        return cs.chain_schema([s, handler(tp)]) if s else handler(tp)\n\n\ndef _apply_transform(\n    s: cs.CoreSchema | None, func: Callable[[Any], Any], handler: GetCoreSchemaHandler\n) -> cs.CoreSchema:\n    from pydantic_core import core_schema as cs\n\n    if s is None:\n        return cs.no_info_plain_validator_function(func)\n\n    if s['type'] == 'str':\n        if func is str.strip:\n            s = s.copy()\n            s['strip_whitespace'] = True\n            return s\n        elif func is str.lower:\n            s = s.copy()\n            s['to_lower'] = True\n            return s\n        elif func is str.upper:\n            s = s.copy()\n            s['to_upper'] = True\n            return s\n\n    return cs.no_info_after_validator_function(func, s)\n\n\ndef _apply_constraint(  # noqa: C901\n    s: cs.CoreSchema | None, constraint: _ConstraintAnnotation\n) -> cs.CoreSchema:\n    \"\"\"Apply a single constraint to a schema.\"\"\"\n    if isinstance(constraint, annotated_types.Gt):\n        gt = constraint.gt\n        if s and s['type'] in {'int', 'float', 'decimal'}:\n            s = s.copy()\n            if s['type'] == 'int' and isinstance(gt, int):\n                s['gt'] = gt\n            elif s['type'] == 'float' and isinstance(gt, float):\n                s['gt'] = gt\n            elif s['type'] == 'decimal' and isinstance(gt, Decimal):\n                s['gt'] = gt\n        else:\n\n            def check_gt(v: Any) -> bool:\n                return v > gt\n\n            s = _check_func(check_gt, f'> {gt}', s)\n    elif isinstance(constraint, annotated_types.Ge):\n        ge = constraint.ge\n        if s and s['type'] in {'int', 'float', 'decimal'}:\n            s = s.copy()\n            if s['type'] == 'int' and isinstance(ge, int):\n                s['ge'] = ge\n            elif s['type'] == 'float' and isinstance(ge, float):\n                s['ge'] = ge\n            elif s['type'] == 'decimal' and isinstance(ge, Decimal):\n                s['ge'] = ge\n\n        def check_ge(v: Any) -> bool:\n            return v >= ge\n\n        s = _check_func(check_ge, f'>= {ge}', s)\n    elif isinstance(constraint, annotated_types.Lt):\n        lt = constraint.lt\n        if s and s['type'] in {'int', 'float', 'decimal'}:\n            s = s.copy()\n            if s['type'] == 'int' and isinstance(lt, int):\n                s['lt'] = lt\n            elif s['type'] == 'float' and isinstance(lt, float):\n                s['lt'] = lt\n            elif s['type'] == 'decimal' and isinstance(lt, Decimal):\n                s['lt'] = lt\n\n        def check_lt(v: Any) -> bool:\n            return v < lt\n\n        s = _check_func(check_lt, f'< {lt}', s)\n    elif isinstance(constraint, annotated_types.Le):\n        le = constraint.le\n        if s and s['type'] in {'int', 'float', 'decimal'}:\n            s = s.copy()\n            if s['type'] == 'int' and isinstance(le, int):\n                s['le'] = le\n            elif s['type'] == 'float' and isinstance(le, float):\n                s['le'] = le\n            elif s['type'] == 'decimal' and isinstance(le, Decimal):\n                s['le'] = le\n\n        def check_le(v: Any) -> bool:\n            return v <= le\n\n        s = _check_func(check_le, f'<= {le}', s)\n    elif isinstance(constraint, annotated_types.Len):\n        min_len = constraint.min_length\n        max_len = constraint.max_length\n\n        if s and s['type'] in {'str', 'list', 'tuple', 'set', 'frozenset', 'dict'}:\n            assert (\n                s['type'] == 'str'\n                or s['type'] == 'list'\n                or s['type'] == 'tuple'\n                or s['type'] == 'set'\n                or s['type'] == 'dict'\n                or s['type'] == 'frozenset'\n            )\n            s = s.copy()\n            if min_len != 0:\n                s['min_length'] = min_len\n            if max_len is not None:\n                s['max_length'] = max_len\n\n        def check_len(v: Any) -> bool:\n            if max_len is not None:\n                return (min_len <= len(v)) and (len(v) <= max_len)\n            return min_len <= len(v)\n\n        s = _check_func(check_len, f'length >= {min_len} and length <= {max_len}', s)\n    elif isinstance(constraint, annotated_types.MultipleOf):\n        multiple_of = constraint.multiple_of\n        if s and s['type'] in {'int', 'float', 'decimal'}:\n            s = s.copy()\n            if s['type'] == 'int' and isinstance(multiple_of, int):\n                s['multiple_of'] = multiple_of\n            elif s['type'] == 'float' and isinstance(multiple_of, float):\n                s['multiple_of'] = multiple_of\n            elif s['type'] == 'decimal' and isinstance(multiple_of, Decimal):\n                s['multiple_of'] = multiple_of\n\n        def check_multiple_of(v: Any) -> bool:\n            return v % multiple_of == 0\n\n        s = _check_func(check_multiple_of, f'% {multiple_of} == 0', s)\n    elif isinstance(constraint, annotated_types.Timezone):\n        tz = constraint.tz\n\n        if tz is ...:\n            if s and s['type'] == 'datetime':\n                s = s.copy()\n                s['tz_constraint'] = 'aware'\n            else:\n\n                def check_tz_aware(v: object) -> bool:\n                    assert isinstance(v, datetime.datetime)\n                    return v.tzinfo is not None\n\n                s = _check_func(check_tz_aware, 'timezone aware', s)\n        elif tz is None:\n            if s and s['type'] == 'datetime':\n                s = s.copy()\n                s['tz_constraint'] = 'naive'\n            else:\n\n                def check_tz_naive(v: object) -> bool:\n                    assert isinstance(v, datetime.datetime)\n                    return v.tzinfo is None\n\n                s = _check_func(check_tz_naive, 'timezone naive', s)\n        else:\n            raise NotImplementedError('Constraining to a specific timezone is not yet supported')\n    elif isinstance(constraint, annotated_types.Interval):\n        if constraint.ge:\n            s = _apply_constraint(s, annotated_types.Ge(constraint.ge))\n        if constraint.gt:\n            s = _apply_constraint(s, annotated_types.Gt(constraint.gt))\n        if constraint.le:\n            s = _apply_constraint(s, annotated_types.Le(constraint.le))\n        if constraint.lt:\n            s = _apply_constraint(s, annotated_types.Lt(constraint.lt))\n        assert s is not None\n    elif isinstance(constraint, annotated_types.Predicate):\n        func = constraint.func\n\n        if func.__name__ == '<lambda>':\n            # attempt to extract the source code for a lambda function\n            # to use as the function name in error messages\n            # TODO: is there a better way? should we just not do this?\n            import inspect\n\n            try:\n                # remove ')' suffix, can use removesuffix once we drop 3.8\n                source = inspect.getsource(func).strip()\n                if source.endswith(')'):\n                    source = source[:-1]\n                lambda_source_code = '`' + ''.join(''.join(source.split('lambda ')[1:]).split(':')[1:]).strip() + '`'\n            except OSError:\n                # stringified annotations\n                lambda_source_code = 'lambda'\n\n            s = _check_func(func, lambda_source_code, s)\n        else:\n            s = _check_func(func, func.__name__, s)\n    elif isinstance(constraint, _NotEq):\n        value = constraint.value\n\n        def check_not_eq(v: Any) -> bool:\n            return operator.__ne__(v, value)\n\n        s = _check_func(check_not_eq, f'!= {value}', s)\n    elif isinstance(constraint, _Eq):\n        value = constraint.value\n\n        def check_eq(v: Any) -> bool:\n            return operator.__eq__(v, value)\n\n        s = _check_func(check_eq, f'== {value}', s)\n    elif isinstance(constraint, _In):\n        values = constraint.values\n\n        def check_in(v: Any) -> bool:\n            return operator.__contains__(values, v)\n\n        s = _check_func(check_in, f'in {values}', s)\n    elif isinstance(constraint, _NotIn):\n        values = constraint.values\n\n        def check_not_in(v: Any) -> bool:\n            return operator.__not__(operator.__contains__(values, v))\n\n        s = _check_func(check_not_in, f'not in {values}', s)\n    else:\n        assert isinstance(constraint, Pattern)\n        if s and s['type'] == 'str':\n            s = s.copy()\n            s['pattern'] = constraint.pattern\n        else:\n\n            def check_pattern(v: object) -> bool:\n                assert isinstance(v, str)\n                return constraint.match(v) is not None\n\n            s = _check_func(check_pattern, f'~ {constraint.pattern}', s)\n    return s\n\n\nclass _SupportsRange(annotated_types.SupportsLe, annotated_types.SupportsGe, Protocol):\n    pass\n\n\nclass _SupportsLen(Protocol):\n    def __len__(self) -> int: ...\n\n\n_NewOutGt = TypeVar('_NewOutGt', bound=annotated_types.SupportsGt)\n_NewOutGe = TypeVar('_NewOutGe', bound=annotated_types.SupportsGe)\n_NewOutLt = TypeVar('_NewOutLt', bound=annotated_types.SupportsLt)\n_NewOutLe = TypeVar('_NewOutLe', bound=annotated_types.SupportsLe)\n_NewOutLen = TypeVar('_NewOutLen', bound=_SupportsLen)\n_NewOutDiv = TypeVar('_NewOutDiv', bound=annotated_types.SupportsDiv)\n_NewOutMod = TypeVar('_NewOutMod', bound=annotated_types.SupportsMod)\n_NewOutDatetime = TypeVar('_NewOutDatetime', bound=datetime.datetime)\n_NewOutInterval = TypeVar('_NewOutInterval', bound=_SupportsRange)\n_OtherIn = TypeVar('_OtherIn')\n_OtherOut = TypeVar('_OtherOut')\n", "pydantic/experimental/__init__.py": "\"\"\"The \"experimental\" module of pydantic contains potential new features that are subject to change.\"\"\"\n\nimport warnings\n\nfrom pydantic.warnings import PydanticExperimentalWarning\n\nwarnings.warn(\n    'This module is experimental, its contents are subject to change and deprecation.',\n    category=PydanticExperimentalWarning,\n)\n", "pydantic/v1/generics.py": "import sys\nimport types\nimport typing\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    Dict,\n    ForwardRef,\n    Generic,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom weakref import WeakKeyDictionary, WeakValueDictionary\n\nfrom typing_extensions import Annotated, Literal as ExtLiteral\n\nfrom pydantic.v1.class_validators import gather_all_validators\nfrom pydantic.v1.fields import DeferredType\nfrom pydantic.v1.main import BaseModel, create_model\nfrom pydantic.v1.types import JsonWrapper\nfrom pydantic.v1.typing import display_as_type, get_all_type_hints, get_args, get_origin, typing_base\nfrom pydantic.v1.utils import all_identical, lenient_issubclass\n\nif sys.version_info >= (3, 10):\n    from typing import _UnionGenericAlias\nif sys.version_info >= (3, 8):\n    from typing import Literal\n\nGenericModelT = TypeVar('GenericModelT', bound='GenericModel')\nTypeVarType = Any  # since mypy doesn't allow the use of TypeVar as a type\n\nCacheKey = Tuple[Type[Any], Any, Tuple[Any, ...]]\nParametrization = Mapping[TypeVarType, Type[Any]]\n\n# weak dictionaries allow the dynamically created parametrized versions of generic models to get collected\n# once they are no longer referenced by the caller.\nif sys.version_info >= (3, 9):  # Typing for weak dictionaries available at 3.9\n    GenericTypesCache = WeakValueDictionary[CacheKey, Type[BaseModel]]\n    AssignedParameters = WeakKeyDictionary[Type[BaseModel], Parametrization]\nelse:\n    GenericTypesCache = WeakValueDictionary\n    AssignedParameters = WeakKeyDictionary\n\n# _generic_types_cache is a Mapping from __class_getitem__ arguments to the parametrized version of generic models.\n# This ensures multiple calls of e.g. A[B] return always the same class.\n_generic_types_cache = GenericTypesCache()\n\n# _assigned_parameters is a Mapping from parametrized version of generic models to assigned types of parametrizations\n# as captured during construction of the class (not instances).\n# E.g., for generic model `Model[A, B]`, when parametrized model `Model[int, str]` is created,\n# `Model[int, str]`: {A: int, B: str}` will be stored in `_assigned_parameters`.\n# (This information is only otherwise available after creation from the class name string).\n_assigned_parameters = AssignedParameters()\n\n\nclass GenericModel(BaseModel):\n    __slots__ = ()\n    __concrete__: ClassVar[bool] = False\n\n    if TYPE_CHECKING:\n        # Putting this in a TYPE_CHECKING block allows us to replace `if Generic not in cls.__bases__` with\n        # `not hasattr(cls, \"__parameters__\")`. This means we don't need to force non-concrete subclasses of\n        # `GenericModel` to also inherit from `Generic`, which would require changes to the use of `create_model` below.\n        __parameters__: ClassVar[Tuple[TypeVarType, ...]]\n\n    # Setting the return type as Type[Any] instead of Type[BaseModel] prevents PyCharm warnings\n    def __class_getitem__(cls: Type[GenericModelT], params: Union[Type[Any], Tuple[Type[Any], ...]]) -> Type[Any]:\n        \"\"\"Instantiates a new class from a generic class `cls` and type variables `params`.\n\n        :param params: Tuple of types the class . Given a generic class\n            `Model` with 2 type variables and a concrete model `Model[str, int]`,\n            the value `(str, int)` would be passed to `params`.\n        :return: New model class inheriting from `cls` with instantiated\n            types described by `params`. If no parameters are given, `cls` is\n            returned as is.\n\n        \"\"\"\n\n        def _cache_key(_params: Any) -> CacheKey:\n            args = get_args(_params)\n            # python returns a list for Callables, which is not hashable\n            if len(args) == 2 and isinstance(args[0], list):\n                args = (tuple(args[0]), args[1])\n            return cls, _params, args\n\n        cached = _generic_types_cache.get(_cache_key(params))\n        if cached is not None:\n            return cached\n        if cls.__concrete__ and Generic not in cls.__bases__:\n            raise TypeError('Cannot parameterize a concrete instantiation of a generic model')\n        if not isinstance(params, tuple):\n            params = (params,)\n        if cls is GenericModel and any(isinstance(param, TypeVar) for param in params):\n            raise TypeError('Type parameters should be placed on typing.Generic, not GenericModel')\n        if not hasattr(cls, '__parameters__'):\n            raise TypeError(f'Type {cls.__name__} must inherit from typing.Generic before being parameterized')\n\n        check_parameters_count(cls, params)\n        # Build map from generic typevars to passed params\n        typevars_map: Dict[TypeVarType, Type[Any]] = dict(zip(cls.__parameters__, params))\n        if all_identical(typevars_map.keys(), typevars_map.values()) and typevars_map:\n            return cls  # if arguments are equal to parameters it's the same object\n\n        # Create new model with original model as parent inserting fields with DeferredType.\n        model_name = cls.__concrete_name__(params)\n        validators = gather_all_validators(cls)\n\n        type_hints = get_all_type_hints(cls).items()\n        instance_type_hints = {k: v for k, v in type_hints if get_origin(v) is not ClassVar}\n\n        fields = {k: (DeferredType(), cls.__fields__[k].field_info) for k in instance_type_hints if k in cls.__fields__}\n\n        model_module, called_globally = get_caller_frame_info()\n        created_model = cast(\n            Type[GenericModel],  # casting ensures mypy is aware of the __concrete__ and __parameters__ attributes\n            create_model(\n                model_name,\n                __module__=model_module or cls.__module__,\n                __base__=(cls,) + tuple(cls.__parameterized_bases__(typevars_map)),\n                __config__=None,\n                __validators__=validators,\n                __cls_kwargs__=None,\n                **fields,\n            ),\n        )\n\n        _assigned_parameters[created_model] = typevars_map\n\n        if called_globally:  # create global reference and therefore allow pickling\n            object_by_reference = None\n            reference_name = model_name\n            reference_module_globals = sys.modules[created_model.__module__].__dict__\n            while object_by_reference is not created_model:\n                object_by_reference = reference_module_globals.setdefault(reference_name, created_model)\n                reference_name += '_'\n\n        created_model.Config = cls.Config\n\n        # Find any typevars that are still present in the model.\n        # If none are left, the model is fully \"concrete\", otherwise the new\n        # class is a generic class as well taking the found typevars as\n        # parameters.\n        new_params = tuple(\n            {param: None for param in iter_contained_typevars(typevars_map.values())}\n        )  # use dict as ordered set\n        created_model.__concrete__ = not new_params\n        if new_params:\n            created_model.__parameters__ = new_params\n\n        # Save created model in cache so we don't end up creating duplicate\n        # models that should be identical.\n        _generic_types_cache[_cache_key(params)] = created_model\n        if len(params) == 1:\n            _generic_types_cache[_cache_key(params[0])] = created_model\n\n        # Recursively walk class type hints and replace generic typevars\n        # with concrete types that were passed.\n        _prepare_model_fields(created_model, fields, instance_type_hints, typevars_map)\n\n        return created_model\n\n    @classmethod\n    def __concrete_name__(cls: Type[Any], params: Tuple[Type[Any], ...]) -> str:\n        \"\"\"Compute class name for child classes.\n\n        :param params: Tuple of types the class . Given a generic class\n            `Model` with 2 type variables and a concrete model `Model[str, int]`,\n            the value `(str, int)` would be passed to `params`.\n        :return: String representing a the new class where `params` are\n            passed to `cls` as type variables.\n\n        This method can be overridden to achieve a custom naming scheme for GenericModels.\n        \"\"\"\n        param_names = [display_as_type(param) for param in params]\n        params_component = ', '.join(param_names)\n        return f'{cls.__name__}[{params_component}]'\n\n    @classmethod\n    def __parameterized_bases__(cls, typevars_map: Parametrization) -> Iterator[Type[Any]]:\n        \"\"\"\n        Returns unbound bases of cls parameterised to given type variables\n\n        :param typevars_map: Dictionary of type applications for binding subclasses.\n            Given a generic class `Model` with 2 type variables [S, T]\n            and a concrete model `Model[str, int]`,\n            the value `{S: str, T: int}` would be passed to `typevars_map`.\n        :return: an iterator of generic sub classes, parameterised by `typevars_map`\n            and other assigned parameters of `cls`\n\n        e.g.:\n        ```\n        class A(GenericModel, Generic[T]):\n            ...\n\n        class B(A[V], Generic[V]):\n            ...\n\n        assert A[int] in B.__parameterized_bases__({V: int})\n        ```\n        \"\"\"\n\n        def build_base_model(\n            base_model: Type[GenericModel], mapped_types: Parametrization\n        ) -> Iterator[Type[GenericModel]]:\n            base_parameters = tuple(mapped_types[param] for param in base_model.__parameters__)\n            parameterized_base = base_model.__class_getitem__(base_parameters)\n            if parameterized_base is base_model or parameterized_base is cls:\n                # Avoid duplication in MRO\n                return\n            yield parameterized_base\n\n        for base_model in cls.__bases__:\n            if not issubclass(base_model, GenericModel):\n                # not a class that can be meaningfully parameterized\n                continue\n            elif not getattr(base_model, '__parameters__', None):\n                # base_model is \"GenericModel\"  (and has no __parameters__)\n                # or\n                # base_model is already concrete, and will be included transitively via cls.\n                continue\n            elif cls in _assigned_parameters:\n                if base_model in _assigned_parameters:\n                    # cls is partially parameterised but not from base_model\n                    # e.g. cls = B[S], base_model = A[S]\n                    # B[S][int] should subclass A[int],  (and will be transitively via B[int])\n                    # but it's not viable to consistently subclass types with arbitrary construction\n                    # So don't attempt to include A[S][int]\n                    continue\n                else:  # base_model not in _assigned_parameters:\n                    # cls is partially parameterized, base_model is original generic\n                    # e.g.  cls = B[str, T], base_model = B[S, T]\n                    # Need to determine the mapping for the base_model parameters\n                    mapped_types: Parametrization = {\n                        key: typevars_map.get(value, value) for key, value in _assigned_parameters[cls].items()\n                    }\n                    yield from build_base_model(base_model, mapped_types)\n            else:\n                # cls is base generic, so base_class has a distinct base\n                # can construct the Parameterised base model using typevars_map directly\n                yield from build_base_model(base_model, typevars_map)\n\n\ndef replace_types(type_: Any, type_map: Mapping[Any, Any]) -> Any:\n    \"\"\"Return type with all occurrences of `type_map` keys recursively replaced with their values.\n\n    :param type_: Any type, class or generic alias\n    :param type_map: Mapping from `TypeVar` instance to concrete types.\n    :return: New type representing the basic structure of `type_` with all\n        `typevar_map` keys recursively replaced.\n\n    >>> replace_types(Tuple[str, Union[List[str], float]], {str: int})\n    Tuple[int, Union[List[int], float]]\n\n    \"\"\"\n    if not type_map:\n        return type_\n\n    type_args = get_args(type_)\n    origin_type = get_origin(type_)\n\n    if origin_type is Annotated:\n        annotated_type, *annotations = type_args\n        return Annotated[replace_types(annotated_type, type_map), tuple(annotations)]\n\n    if (origin_type is ExtLiteral) or (sys.version_info >= (3, 8) and origin_type is Literal):\n        return type_map.get(type_, type_)\n    # Having type args is a good indicator that this is a typing module\n    # class instantiation or a generic alias of some sort.\n    if type_args:\n        resolved_type_args = tuple(replace_types(arg, type_map) for arg in type_args)\n        if all_identical(type_args, resolved_type_args):\n            # If all arguments are the same, there is no need to modify the\n            # type or create a new object at all\n            return type_\n        if (\n            origin_type is not None\n            and isinstance(type_, typing_base)\n            and not isinstance(origin_type, typing_base)\n            and getattr(type_, '_name', None) is not None\n        ):\n            # In python < 3.9 generic aliases don't exist so any of these like `list`,\n            # `type` or `collections.abc.Callable` need to be translated.\n            # See: https://www.python.org/dev/peps/pep-0585\n            origin_type = getattr(typing, type_._name)\n        assert origin_type is not None\n        # PEP-604 syntax (Ex.: list | str) is represented with a types.UnionType object that does not have __getitem__.\n        # We also cannot use isinstance() since we have to compare types.\n        if sys.version_info >= (3, 10) and origin_type is types.UnionType:  # noqa: E721\n            return _UnionGenericAlias(origin_type, resolved_type_args)\n        return origin_type[resolved_type_args]\n\n    # We handle pydantic generic models separately as they don't have the same\n    # semantics as \"typing\" classes or generic aliases\n    if not origin_type and lenient_issubclass(type_, GenericModel) and not type_.__concrete__:\n        type_args = type_.__parameters__\n        resolved_type_args = tuple(replace_types(t, type_map) for t in type_args)\n        if all_identical(type_args, resolved_type_args):\n            return type_\n        return type_[resolved_type_args]\n\n    # Handle special case for typehints that can have lists as arguments.\n    # `typing.Callable[[int, str], int]` is an example for this.\n    if isinstance(type_, (List, list)):\n        resolved_list = list(replace_types(element, type_map) for element in type_)\n        if all_identical(type_, resolved_list):\n            return type_\n        return resolved_list\n\n    # For JsonWrapperValue, need to handle its inner type to allow correct parsing\n    # of generic Json arguments like Json[T]\n    if not origin_type and lenient_issubclass(type_, JsonWrapper):\n        type_.inner_type = replace_types(type_.inner_type, type_map)\n        return type_\n\n    # If all else fails, we try to resolve the type directly and otherwise just\n    # return the input with no modifications.\n    new_type = type_map.get(type_, type_)\n    # Convert string to ForwardRef\n    if isinstance(new_type, str):\n        return ForwardRef(new_type)\n    else:\n        return new_type\n\n\ndef check_parameters_count(cls: Type[GenericModel], parameters: Tuple[Any, ...]) -> None:\n    actual = len(parameters)\n    expected = len(cls.__parameters__)\n    if actual != expected:\n        description = 'many' if actual > expected else 'few'\n        raise TypeError(f'Too {description} parameters for {cls.__name__}; actual {actual}, expected {expected}')\n\n\nDictValues: Type[Any] = {}.values().__class__\n\n\ndef iter_contained_typevars(v: Any) -> Iterator[TypeVarType]:\n    \"\"\"Recursively iterate through all subtypes and type args of `v` and yield any typevars that are found.\"\"\"\n    if isinstance(v, TypeVar):\n        yield v\n    elif hasattr(v, '__parameters__') and not get_origin(v) and lenient_issubclass(v, GenericModel):\n        yield from v.__parameters__\n    elif isinstance(v, (DictValues, list)):\n        for var in v:\n            yield from iter_contained_typevars(var)\n    else:\n        args = get_args(v)\n        for arg in args:\n            yield from iter_contained_typevars(arg)\n\n\ndef get_caller_frame_info() -> Tuple[Optional[str], bool]:\n    \"\"\"\n    Used inside a function to check whether it was called globally\n\n    Will only work against non-compiled code, therefore used only in pydantic.generics\n\n    :returns Tuple[module_name, called_globally]\n    \"\"\"\n    try:\n        previous_caller_frame = sys._getframe(2)\n    except ValueError as e:\n        raise RuntimeError('This function must be used inside another function') from e\n    except AttributeError:  # sys module does not have _getframe function, so there's nothing we can do about it\n        return None, False\n    frame_globals = previous_caller_frame.f_globals\n    return frame_globals.get('__name__'), previous_caller_frame.f_locals is frame_globals\n\n\ndef _prepare_model_fields(\n    created_model: Type[GenericModel],\n    fields: Mapping[str, Any],\n    instance_type_hints: Mapping[str, type],\n    typevars_map: Mapping[Any, type],\n) -> None:\n    \"\"\"\n    Replace DeferredType fields with concrete type hints and prepare them.\n    \"\"\"\n\n    for key, field in created_model.__fields__.items():\n        if key not in fields:\n            assert field.type_.__class__ is not DeferredType\n            # https://github.com/nedbat/coveragepy/issues/198\n            continue  # pragma: no cover\n\n        assert field.type_.__class__ is DeferredType, field.type_.__class__\n\n        field_type_hint = instance_type_hints[key]\n        concrete_type = replace_types(field_type_hint, typevars_map)\n        field.type_ = concrete_type\n        field.outer_type_ = concrete_type\n        field.prepare()\n        created_model.__annotations__[key] = concrete_type\n", "pydantic/v1/color.py": "\"\"\"\nColor definitions are  used as per CSS3 specification:\nhttp://www.w3.org/TR/css3-color/#svg-color\n\nA few colors have multiple names referring to the sames colors, eg. `grey` and `gray` or `aqua` and `cyan`.\n\nIn these cases the LAST color when sorted alphabetically takes preferences,\neg. Color((0, 255, 255)).as_named() == 'cyan' because \"cyan\" comes after \"aqua\".\n\"\"\"\nimport math\nimport re\nfrom colorsys import hls_to_rgb, rgb_to_hls\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, Union, cast\n\nfrom pydantic.v1.errors import ColorError\nfrom pydantic.v1.utils import Representation, almost_equal_floats\n\nif TYPE_CHECKING:\n    from pydantic.v1.typing import CallableGenerator, ReprArgs\n\nColorTuple = Union[Tuple[int, int, int], Tuple[int, int, int, float]]\nColorType = Union[ColorTuple, str]\nHslColorTuple = Union[Tuple[float, float, float], Tuple[float, float, float, float]]\n\n\nclass RGBA:\n    \"\"\"\n    Internal use only as a representation of a color.\n    \"\"\"\n\n    __slots__ = 'r', 'g', 'b', 'alpha', '_tuple'\n\n    def __init__(self, r: float, g: float, b: float, alpha: Optional[float]):\n        self.r = r\n        self.g = g\n        self.b = b\n        self.alpha = alpha\n\n        self._tuple: Tuple[float, float, float, Optional[float]] = (r, g, b, alpha)\n\n    def __getitem__(self, item: Any) -> Any:\n        return self._tuple[item]\n\n\n# these are not compiled here to avoid import slowdown, they'll be compiled the first time they're used, then cached\nr_hex_short = r'\\s*(?:#|0x)?([0-9a-f])([0-9a-f])([0-9a-f])([0-9a-f])?\\s*'\nr_hex_long = r'\\s*(?:#|0x)?([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})?\\s*'\n_r_255 = r'(\\d{1,3}(?:\\.\\d+)?)'\n_r_comma = r'\\s*,\\s*'\nr_rgb = fr'\\s*rgb\\(\\s*{_r_255}{_r_comma}{_r_255}{_r_comma}{_r_255}\\)\\s*'\n_r_alpha = r'(\\d(?:\\.\\d+)?|\\.\\d+|\\d{1,2}%)'\nr_rgba = fr'\\s*rgba\\(\\s*{_r_255}{_r_comma}{_r_255}{_r_comma}{_r_255}{_r_comma}{_r_alpha}\\s*\\)\\s*'\n_r_h = r'(-?\\d+(?:\\.\\d+)?|-?\\.\\d+)(deg|rad|turn)?'\n_r_sl = r'(\\d{1,3}(?:\\.\\d+)?)%'\nr_hsl = fr'\\s*hsl\\(\\s*{_r_h}{_r_comma}{_r_sl}{_r_comma}{_r_sl}\\s*\\)\\s*'\nr_hsla = fr'\\s*hsl\\(\\s*{_r_h}{_r_comma}{_r_sl}{_r_comma}{_r_sl}{_r_comma}{_r_alpha}\\s*\\)\\s*'\n\n# colors where the two hex characters are the same, if all colors match this the short version of hex colors can be used\nrepeat_colors = {int(c * 2, 16) for c in '0123456789abcdef'}\nrads = 2 * math.pi\n\n\nclass Color(Representation):\n    __slots__ = '_original', '_rgba'\n\n    def __init__(self, value: ColorType) -> None:\n        self._rgba: RGBA\n        self._original: ColorType\n        if isinstance(value, (tuple, list)):\n            self._rgba = parse_tuple(value)\n        elif isinstance(value, str):\n            self._rgba = parse_str(value)\n        elif isinstance(value, Color):\n            self._rgba = value._rgba\n            value = value._original\n        else:\n            raise ColorError(reason='value must be a tuple, list or string')\n\n        # if we've got here value must be a valid color\n        self._original = value\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        field_schema.update(type='string', format='color')\n\n    def original(self) -> ColorType:\n        \"\"\"\n        Original value passed to Color\n        \"\"\"\n        return self._original\n\n    def as_named(self, *, fallback: bool = False) -> str:\n        if self._rgba.alpha is None:\n            rgb = cast(Tuple[int, int, int], self.as_rgb_tuple())\n            try:\n                return COLORS_BY_VALUE[rgb]\n            except KeyError as e:\n                if fallback:\n                    return self.as_hex()\n                else:\n                    raise ValueError('no named color found, use fallback=True, as_hex() or as_rgb()') from e\n        else:\n            return self.as_hex()\n\n    def as_hex(self) -> str:\n        \"\"\"\n        Hex string representing the color can be 3, 4, 6 or 8 characters depending on whether the string\n        a \"short\" representation of the color is possible and whether there's an alpha channel.\n        \"\"\"\n        values = [float_to_255(c) for c in self._rgba[:3]]\n        if self._rgba.alpha is not None:\n            values.append(float_to_255(self._rgba.alpha))\n\n        as_hex = ''.join(f'{v:02x}' for v in values)\n        if all(c in repeat_colors for c in values):\n            as_hex = ''.join(as_hex[c] for c in range(0, len(as_hex), 2))\n        return '#' + as_hex\n\n    def as_rgb(self) -> str:\n        \"\"\"\n        Color as an rgb(<r>, <g>, <b>) or rgba(<r>, <g>, <b>, <a>) string.\n        \"\"\"\n        if self._rgba.alpha is None:\n            return f'rgb({float_to_255(self._rgba.r)}, {float_to_255(self._rgba.g)}, {float_to_255(self._rgba.b)})'\n        else:\n            return (\n                f'rgba({float_to_255(self._rgba.r)}, {float_to_255(self._rgba.g)}, {float_to_255(self._rgba.b)}, '\n                f'{round(self._alpha_float(), 2)})'\n            )\n\n    def as_rgb_tuple(self, *, alpha: Optional[bool] = None) -> ColorTuple:\n        \"\"\"\n        Color as an RGB or RGBA tuple; red, green and blue are in the range 0 to 255, alpha if included is\n        in the range 0 to 1.\n\n        :param alpha: whether to include the alpha channel, options are\n          None - (default) include alpha only if it's set (e.g. not None)\n          True - always include alpha,\n          False - always omit alpha,\n        \"\"\"\n        r, g, b = (float_to_255(c) for c in self._rgba[:3])\n        if alpha is None:\n            if self._rgba.alpha is None:\n                return r, g, b\n            else:\n                return r, g, b, self._alpha_float()\n        elif alpha:\n            return r, g, b, self._alpha_float()\n        else:\n            # alpha is False\n            return r, g, b\n\n    def as_hsl(self) -> str:\n        \"\"\"\n        Color as an hsl(<h>, <s>, <l>) or hsl(<h>, <s>, <l>, <a>) string.\n        \"\"\"\n        if self._rgba.alpha is None:\n            h, s, li = self.as_hsl_tuple(alpha=False)  # type: ignore\n            return f'hsl({h * 360:0.0f}, {s:0.0%}, {li:0.0%})'\n        else:\n            h, s, li, a = self.as_hsl_tuple(alpha=True)  # type: ignore\n            return f'hsl({h * 360:0.0f}, {s:0.0%}, {li:0.0%}, {round(a, 2)})'\n\n    def as_hsl_tuple(self, *, alpha: Optional[bool] = None) -> HslColorTuple:\n        \"\"\"\n        Color as an HSL or HSLA tuple, e.g. hue, saturation, lightness and optionally alpha; all elements are in\n        the range 0 to 1.\n\n        NOTE: this is HSL as used in HTML and most other places, not HLS as used in python's colorsys.\n\n        :param alpha: whether to include the alpha channel, options are\n          None - (default) include alpha only if it's set (e.g. not None)\n          True - always include alpha,\n          False - always omit alpha,\n        \"\"\"\n        h, l, s = rgb_to_hls(self._rgba.r, self._rgba.g, self._rgba.b)\n        if alpha is None:\n            if self._rgba.alpha is None:\n                return h, s, l\n            else:\n                return h, s, l, self._alpha_float()\n        if alpha:\n            return h, s, l, self._alpha_float()\n        else:\n            # alpha is False\n            return h, s, l\n\n    def _alpha_float(self) -> float:\n        return 1 if self._rgba.alpha is None else self._rgba.alpha\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls\n\n    def __str__(self) -> str:\n        return self.as_named(fallback=True)\n\n    def __repr_args__(self) -> 'ReprArgs':\n        return [(None, self.as_named(fallback=True))] + [('rgb', self.as_rgb_tuple())]  # type: ignore\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, Color) and self.as_rgb_tuple() == other.as_rgb_tuple()\n\n    def __hash__(self) -> int:\n        return hash(self.as_rgb_tuple())\n\n\ndef parse_tuple(value: Tuple[Any, ...]) -> RGBA:\n    \"\"\"\n    Parse a tuple or list as a color.\n    \"\"\"\n    if len(value) == 3:\n        r, g, b = (parse_color_value(v) for v in value)\n        return RGBA(r, g, b, None)\n    elif len(value) == 4:\n        r, g, b = (parse_color_value(v) for v in value[:3])\n        return RGBA(r, g, b, parse_float_alpha(value[3]))\n    else:\n        raise ColorError(reason='tuples must have length 3 or 4')\n\n\ndef parse_str(value: str) -> RGBA:\n    \"\"\"\n    Parse a string to an RGBA tuple, trying the following formats (in this order):\n    * named color, see COLORS_BY_NAME below\n    * hex short eg. `<prefix>fff` (prefix can be `#`, `0x` or nothing)\n    * hex long eg. `<prefix>ffffff` (prefix can be `#`, `0x` or nothing)\n    * `rgb(<r>, <g>, <b>) `\n    * `rgba(<r>, <g>, <b>, <a>)`\n    \"\"\"\n    value_lower = value.lower()\n    try:\n        r, g, b = COLORS_BY_NAME[value_lower]\n    except KeyError:\n        pass\n    else:\n        return ints_to_rgba(r, g, b, None)\n\n    m = re.fullmatch(r_hex_short, value_lower)\n    if m:\n        *rgb, a = m.groups()\n        r, g, b = (int(v * 2, 16) for v in rgb)\n        if a:\n            alpha: Optional[float] = int(a * 2, 16) / 255\n        else:\n            alpha = None\n        return ints_to_rgba(r, g, b, alpha)\n\n    m = re.fullmatch(r_hex_long, value_lower)\n    if m:\n        *rgb, a = m.groups()\n        r, g, b = (int(v, 16) for v in rgb)\n        if a:\n            alpha = int(a, 16) / 255\n        else:\n            alpha = None\n        return ints_to_rgba(r, g, b, alpha)\n\n    m = re.fullmatch(r_rgb, value_lower)\n    if m:\n        return ints_to_rgba(*m.groups(), None)  # type: ignore\n\n    m = re.fullmatch(r_rgba, value_lower)\n    if m:\n        return ints_to_rgba(*m.groups())  # type: ignore\n\n    m = re.fullmatch(r_hsl, value_lower)\n    if m:\n        h, h_units, s, l_ = m.groups()\n        return parse_hsl(h, h_units, s, l_)\n\n    m = re.fullmatch(r_hsla, value_lower)\n    if m:\n        h, h_units, s, l_, a = m.groups()\n        return parse_hsl(h, h_units, s, l_, parse_float_alpha(a))\n\n    raise ColorError(reason='string not recognised as a valid color')\n\n\ndef ints_to_rgba(r: Union[int, str], g: Union[int, str], b: Union[int, str], alpha: Optional[float]) -> RGBA:\n    return RGBA(parse_color_value(r), parse_color_value(g), parse_color_value(b), parse_float_alpha(alpha))\n\n\ndef parse_color_value(value: Union[int, str], max_val: int = 255) -> float:\n    \"\"\"\n    Parse a value checking it's a valid int in the range 0 to max_val and divide by max_val to give a number\n    in the range 0 to 1\n    \"\"\"\n    try:\n        color = float(value)\n    except ValueError:\n        raise ColorError(reason='color values must be a valid number')\n    if 0 <= color <= max_val:\n        return color / max_val\n    else:\n        raise ColorError(reason=f'color values must be in the range 0 to {max_val}')\n\n\ndef parse_float_alpha(value: Union[None, str, float, int]) -> Optional[float]:\n    \"\"\"\n    Parse a value checking it's a valid float in the range 0 to 1\n    \"\"\"\n    if value is None:\n        return None\n    try:\n        if isinstance(value, str) and value.endswith('%'):\n            alpha = float(value[:-1]) / 100\n        else:\n            alpha = float(value)\n    except ValueError:\n        raise ColorError(reason='alpha values must be a valid float')\n\n    if almost_equal_floats(alpha, 1):\n        return None\n    elif 0 <= alpha <= 1:\n        return alpha\n    else:\n        raise ColorError(reason='alpha values must be in the range 0 to 1')\n\n\ndef parse_hsl(h: str, h_units: str, sat: str, light: str, alpha: Optional[float] = None) -> RGBA:\n    \"\"\"\n    Parse raw hue, saturation, lightness and alpha values and convert to RGBA.\n    \"\"\"\n    s_value, l_value = parse_color_value(sat, 100), parse_color_value(light, 100)\n\n    h_value = float(h)\n    if h_units in {None, 'deg'}:\n        h_value = h_value % 360 / 360\n    elif h_units == 'rad':\n        h_value = h_value % rads / rads\n    else:\n        # turns\n        h_value = h_value % 1\n\n    r, g, b = hls_to_rgb(h_value, l_value, s_value)\n    return RGBA(r, g, b, alpha)\n\n\ndef float_to_255(c: float) -> int:\n    return int(round(c * 255))\n\n\nCOLORS_BY_NAME = {\n    'aliceblue': (240, 248, 255),\n    'antiquewhite': (250, 235, 215),\n    'aqua': (0, 255, 255),\n    'aquamarine': (127, 255, 212),\n    'azure': (240, 255, 255),\n    'beige': (245, 245, 220),\n    'bisque': (255, 228, 196),\n    'black': (0, 0, 0),\n    'blanchedalmond': (255, 235, 205),\n    'blue': (0, 0, 255),\n    'blueviolet': (138, 43, 226),\n    'brown': (165, 42, 42),\n    'burlywood': (222, 184, 135),\n    'cadetblue': (95, 158, 160),\n    'chartreuse': (127, 255, 0),\n    'chocolate': (210, 105, 30),\n    'coral': (255, 127, 80),\n    'cornflowerblue': (100, 149, 237),\n    'cornsilk': (255, 248, 220),\n    'crimson': (220, 20, 60),\n    'cyan': (0, 255, 255),\n    'darkblue': (0, 0, 139),\n    'darkcyan': (0, 139, 139),\n    'darkgoldenrod': (184, 134, 11),\n    'darkgray': (169, 169, 169),\n    'darkgreen': (0, 100, 0),\n    'darkgrey': (169, 169, 169),\n    'darkkhaki': (189, 183, 107),\n    'darkmagenta': (139, 0, 139),\n    'darkolivegreen': (85, 107, 47),\n    'darkorange': (255, 140, 0),\n    'darkorchid': (153, 50, 204),\n    'darkred': (139, 0, 0),\n    'darksalmon': (233, 150, 122),\n    'darkseagreen': (143, 188, 143),\n    'darkslateblue': (72, 61, 139),\n    'darkslategray': (47, 79, 79),\n    'darkslategrey': (47, 79, 79),\n    'darkturquoise': (0, 206, 209),\n    'darkviolet': (148, 0, 211),\n    'deeppink': (255, 20, 147),\n    'deepskyblue': (0, 191, 255),\n    'dimgray': (105, 105, 105),\n    'dimgrey': (105, 105, 105),\n    'dodgerblue': (30, 144, 255),\n    'firebrick': (178, 34, 34),\n    'floralwhite': (255, 250, 240),\n    'forestgreen': (34, 139, 34),\n    'fuchsia': (255, 0, 255),\n    'gainsboro': (220, 220, 220),\n    'ghostwhite': (248, 248, 255),\n    'gold': (255, 215, 0),\n    'goldenrod': (218, 165, 32),\n    'gray': (128, 128, 128),\n    'green': (0, 128, 0),\n    'greenyellow': (173, 255, 47),\n    'grey': (128, 128, 128),\n    'honeydew': (240, 255, 240),\n    'hotpink': (255, 105, 180),\n    'indianred': (205, 92, 92),\n    'indigo': (75, 0, 130),\n    'ivory': (255, 255, 240),\n    'khaki': (240, 230, 140),\n    'lavender': (230, 230, 250),\n    'lavenderblush': (255, 240, 245),\n    'lawngreen': (124, 252, 0),\n    'lemonchiffon': (255, 250, 205),\n    'lightblue': (173, 216, 230),\n    'lightcoral': (240, 128, 128),\n    'lightcyan': (224, 255, 255),\n    'lightgoldenrodyellow': (250, 250, 210),\n    'lightgray': (211, 211, 211),\n    'lightgreen': (144, 238, 144),\n    'lightgrey': (211, 211, 211),\n    'lightpink': (255, 182, 193),\n    'lightsalmon': (255, 160, 122),\n    'lightseagreen': (32, 178, 170),\n    'lightskyblue': (135, 206, 250),\n    'lightslategray': (119, 136, 153),\n    'lightslategrey': (119, 136, 153),\n    'lightsteelblue': (176, 196, 222),\n    'lightyellow': (255, 255, 224),\n    'lime': (0, 255, 0),\n    'limegreen': (50, 205, 50),\n    'linen': (250, 240, 230),\n    'magenta': (255, 0, 255),\n    'maroon': (128, 0, 0),\n    'mediumaquamarine': (102, 205, 170),\n    'mediumblue': (0, 0, 205),\n    'mediumorchid': (186, 85, 211),\n    'mediumpurple': (147, 112, 219),\n    'mediumseagreen': (60, 179, 113),\n    'mediumslateblue': (123, 104, 238),\n    'mediumspringgreen': (0, 250, 154),\n    'mediumturquoise': (72, 209, 204),\n    'mediumvioletred': (199, 21, 133),\n    'midnightblue': (25, 25, 112),\n    'mintcream': (245, 255, 250),\n    'mistyrose': (255, 228, 225),\n    'moccasin': (255, 228, 181),\n    'navajowhite': (255, 222, 173),\n    'navy': (0, 0, 128),\n    'oldlace': (253, 245, 230),\n    'olive': (128, 128, 0),\n    'olivedrab': (107, 142, 35),\n    'orange': (255, 165, 0),\n    'orangered': (255, 69, 0),\n    'orchid': (218, 112, 214),\n    'palegoldenrod': (238, 232, 170),\n    'palegreen': (152, 251, 152),\n    'paleturquoise': (175, 238, 238),\n    'palevioletred': (219, 112, 147),\n    'papayawhip': (255, 239, 213),\n    'peachpuff': (255, 218, 185),\n    'peru': (205, 133, 63),\n    'pink': (255, 192, 203),\n    'plum': (221, 160, 221),\n    'powderblue': (176, 224, 230),\n    'purple': (128, 0, 128),\n    'red': (255, 0, 0),\n    'rosybrown': (188, 143, 143),\n    'royalblue': (65, 105, 225),\n    'saddlebrown': (139, 69, 19),\n    'salmon': (250, 128, 114),\n    'sandybrown': (244, 164, 96),\n    'seagreen': (46, 139, 87),\n    'seashell': (255, 245, 238),\n    'sienna': (160, 82, 45),\n    'silver': (192, 192, 192),\n    'skyblue': (135, 206, 235),\n    'slateblue': (106, 90, 205),\n    'slategray': (112, 128, 144),\n    'slategrey': (112, 128, 144),\n    'snow': (255, 250, 250),\n    'springgreen': (0, 255, 127),\n    'steelblue': (70, 130, 180),\n    'tan': (210, 180, 140),\n    'teal': (0, 128, 128),\n    'thistle': (216, 191, 216),\n    'tomato': (255, 99, 71),\n    'turquoise': (64, 224, 208),\n    'violet': (238, 130, 238),\n    'wheat': (245, 222, 179),\n    'white': (255, 255, 255),\n    'whitesmoke': (245, 245, 245),\n    'yellow': (255, 255, 0),\n    'yellowgreen': (154, 205, 50),\n}\n\nCOLORS_BY_VALUE = {v: k for k, v in COLORS_BY_NAME.items()}\n", "pydantic/v1/config.py": "import json\nfrom enum import Enum\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, ForwardRef, Optional, Tuple, Type, Union\n\nfrom typing_extensions import Literal, Protocol\n\nfrom pydantic.v1.typing import AnyArgTCallable, AnyCallable\nfrom pydantic.v1.utils import GetterDict\nfrom pydantic.v1.version import compiled\n\nif TYPE_CHECKING:\n    from typing import overload\n\n    from pydantic.v1.fields import ModelField\n    from pydantic.v1.main import BaseModel\n\n    ConfigType = Type['BaseConfig']\n\n    class SchemaExtraCallable(Protocol):\n        @overload\n        def __call__(self, schema: Dict[str, Any]) -> None:\n            pass\n\n        @overload\n        def __call__(self, schema: Dict[str, Any], model_class: Type[BaseModel]) -> None:\n            pass\n\nelse:\n    SchemaExtraCallable = Callable[..., None]\n\n__all__ = 'BaseConfig', 'ConfigDict', 'get_config', 'Extra', 'inherit_config', 'prepare_config'\n\n\nclass Extra(str, Enum):\n    allow = 'allow'\n    ignore = 'ignore'\n    forbid = 'forbid'\n\n\n# https://github.com/cython/cython/issues/4003\n# Fixed in Cython 3 and Pydantic v1 won't support Cython 3.\n# Pydantic v2 doesn't depend on Cython at all.\nif not compiled:\n    from typing_extensions import TypedDict\n\n    class ConfigDict(TypedDict, total=False):\n        title: Optional[str]\n        anystr_lower: bool\n        anystr_strip_whitespace: bool\n        min_anystr_length: int\n        max_anystr_length: Optional[int]\n        validate_all: bool\n        extra: Extra\n        allow_mutation: bool\n        frozen: bool\n        allow_population_by_field_name: bool\n        use_enum_values: bool\n        fields: Dict[str, Union[str, Dict[str, str]]]\n        validate_assignment: bool\n        error_msg_templates: Dict[str, str]\n        arbitrary_types_allowed: bool\n        orm_mode: bool\n        getter_dict: Type[GetterDict]\n        alias_generator: Optional[Callable[[str], str]]\n        keep_untouched: Tuple[type, ...]\n        schema_extra: Union[Dict[str, object], 'SchemaExtraCallable']\n        json_loads: Callable[[str], object]\n        json_dumps: AnyArgTCallable[str]\n        json_encoders: Dict[Type[object], AnyCallable]\n        underscore_attrs_are_private: bool\n        allow_inf_nan: bool\n        copy_on_model_validation: Literal['none', 'deep', 'shallow']\n        # whether dataclass `__post_init__` should be run after validation\n        post_init_call: Literal['before_validation', 'after_validation']\n\nelse:\n    ConfigDict = dict  # type: ignore\n\n\nclass BaseConfig:\n    title: Optional[str] = None\n    anystr_lower: bool = False\n    anystr_upper: bool = False\n    anystr_strip_whitespace: bool = False\n    min_anystr_length: int = 0\n    max_anystr_length: Optional[int] = None\n    validate_all: bool = False\n    extra: Extra = Extra.ignore\n    allow_mutation: bool = True\n    frozen: bool = False\n    allow_population_by_field_name: bool = False\n    use_enum_values: bool = False\n    fields: Dict[str, Union[str, Dict[str, str]]] = {}\n    validate_assignment: bool = False\n    error_msg_templates: Dict[str, str] = {}\n    arbitrary_types_allowed: bool = False\n    orm_mode: bool = False\n    getter_dict: Type[GetterDict] = GetterDict\n    alias_generator: Optional[Callable[[str], str]] = None\n    keep_untouched: Tuple[type, ...] = ()\n    schema_extra: Union[Dict[str, Any], 'SchemaExtraCallable'] = {}\n    json_loads: Callable[[str], Any] = json.loads\n    json_dumps: Callable[..., str] = json.dumps\n    json_encoders: Dict[Union[Type[Any], str, ForwardRef], AnyCallable] = {}\n    underscore_attrs_are_private: bool = False\n    allow_inf_nan: bool = True\n\n    # whether inherited models as fields should be reconstructed as base model,\n    # and whether such a copy should be shallow or deep\n    copy_on_model_validation: Literal['none', 'deep', 'shallow'] = 'shallow'\n\n    # whether `Union` should check all allowed types before even trying to coerce\n    smart_union: bool = False\n    # whether dataclass `__post_init__` should be run before or after validation\n    post_init_call: Literal['before_validation', 'after_validation'] = 'before_validation'\n\n    @classmethod\n    def get_field_info(cls, name: str) -> Dict[str, Any]:\n        \"\"\"\n        Get properties of FieldInfo from the `fields` property of the config class.\n        \"\"\"\n\n        fields_value = cls.fields.get(name)\n\n        if isinstance(fields_value, str):\n            field_info: Dict[str, Any] = {'alias': fields_value}\n        elif isinstance(fields_value, dict):\n            field_info = fields_value\n        else:\n            field_info = {}\n\n        if 'alias' in field_info:\n            field_info.setdefault('alias_priority', 2)\n\n        if field_info.get('alias_priority', 0) <= 1 and cls.alias_generator:\n            alias = cls.alias_generator(name)\n            if not isinstance(alias, str):\n                raise TypeError(f'Config.alias_generator must return str, not {alias.__class__}')\n            field_info.update(alias=alias, alias_priority=1)\n        return field_info\n\n    @classmethod\n    def prepare_field(cls, field: 'ModelField') -> None:\n        \"\"\"\n        Optional hook to check or modify fields during model creation.\n        \"\"\"\n        pass\n\n\ndef get_config(config: Union[ConfigDict, Type[object], None]) -> Type[BaseConfig]:\n    if config is None:\n        return BaseConfig\n\n    else:\n        config_dict = (\n            config\n            if isinstance(config, dict)\n            else {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n        )\n\n        class Config(BaseConfig):\n            ...\n\n        for k, v in config_dict.items():\n            setattr(Config, k, v)\n        return Config\n\n\ndef inherit_config(self_config: 'ConfigType', parent_config: 'ConfigType', **namespace: Any) -> 'ConfigType':\n    if not self_config:\n        base_classes: Tuple['ConfigType', ...] = (parent_config,)\n    elif self_config == parent_config:\n        base_classes = (self_config,)\n    else:\n        base_classes = self_config, parent_config\n\n    namespace['json_encoders'] = {\n        **getattr(parent_config, 'json_encoders', {}),\n        **getattr(self_config, 'json_encoders', {}),\n        **namespace.get('json_encoders', {}),\n    }\n\n    return type('Config', base_classes, namespace)\n\n\ndef prepare_config(config: Type[BaseConfig], cls_name: str) -> None:\n    if not isinstance(config.extra, Extra):\n        try:\n            config.extra = Extra(config.extra)\n        except ValueError:\n            raise ValueError(f'\"{cls_name}\": {config.extra} is not a valid value for \"extra\"')\n", "pydantic/v1/env_settings.py": "import os\nimport warnings\nfrom pathlib import Path\nfrom typing import AbstractSet, Any, Callable, ClassVar, Dict, List, Mapping, Optional, Tuple, Type, Union\n\nfrom pydantic.v1.config import BaseConfig, Extra\nfrom pydantic.v1.fields import ModelField\nfrom pydantic.v1.main import BaseModel\nfrom pydantic.v1.types import JsonWrapper\nfrom pydantic.v1.typing import StrPath, display_as_type, get_origin, is_union\nfrom pydantic.v1.utils import deep_update, lenient_issubclass, path_type, sequence_like\n\nenv_file_sentinel = str(object())\n\nSettingsSourceCallable = Callable[['BaseSettings'], Dict[str, Any]]\nDotenvType = Union[StrPath, List[StrPath], Tuple[StrPath, ...]]\n\n\nclass SettingsError(ValueError):\n    pass\n\n\nclass BaseSettings(BaseModel):\n    \"\"\"\n    Base class for settings, allowing values to be overridden by environment variables.\n\n    This is useful in production for secrets you do not wish to save in code, it plays nicely with docker(-compose),\n    Heroku and any 12 factor app design.\n    \"\"\"\n\n    def __init__(\n        __pydantic_self__,\n        _env_file: Optional[DotenvType] = env_file_sentinel,\n        _env_file_encoding: Optional[str] = None,\n        _env_nested_delimiter: Optional[str] = None,\n        _secrets_dir: Optional[StrPath] = None,\n        **values: Any,\n    ) -> None:\n        # Uses something other than `self` the first arg to allow \"self\" as a settable attribute\n        super().__init__(\n            **__pydantic_self__._build_values(\n                values,\n                _env_file=_env_file,\n                _env_file_encoding=_env_file_encoding,\n                _env_nested_delimiter=_env_nested_delimiter,\n                _secrets_dir=_secrets_dir,\n            )\n        )\n\n    def _build_values(\n        self,\n        init_kwargs: Dict[str, Any],\n        _env_file: Optional[DotenvType] = None,\n        _env_file_encoding: Optional[str] = None,\n        _env_nested_delimiter: Optional[str] = None,\n        _secrets_dir: Optional[StrPath] = None,\n    ) -> Dict[str, Any]:\n        # Configure built-in sources\n        init_settings = InitSettingsSource(init_kwargs=init_kwargs)\n        env_settings = EnvSettingsSource(\n            env_file=(_env_file if _env_file != env_file_sentinel else self.__config__.env_file),\n            env_file_encoding=(\n                _env_file_encoding if _env_file_encoding is not None else self.__config__.env_file_encoding\n            ),\n            env_nested_delimiter=(\n                _env_nested_delimiter if _env_nested_delimiter is not None else self.__config__.env_nested_delimiter\n            ),\n            env_prefix_len=len(self.__config__.env_prefix),\n        )\n        file_secret_settings = SecretsSettingsSource(secrets_dir=_secrets_dir or self.__config__.secrets_dir)\n        # Provide a hook to set built-in sources priority and add / remove sources\n        sources = self.__config__.customise_sources(\n            init_settings=init_settings, env_settings=env_settings, file_secret_settings=file_secret_settings\n        )\n        if sources:\n            return deep_update(*reversed([source(self) for source in sources]))\n        else:\n            # no one should mean to do this, but I think returning an empty dict is marginally preferable\n            # to an informative error and much better than a confusing error\n            return {}\n\n    class Config(BaseConfig):\n        env_prefix: str = ''\n        env_file: Optional[DotenvType] = None\n        env_file_encoding: Optional[str] = None\n        env_nested_delimiter: Optional[str] = None\n        secrets_dir: Optional[StrPath] = None\n        validate_all: bool = True\n        extra: Extra = Extra.forbid\n        arbitrary_types_allowed: bool = True\n        case_sensitive: bool = False\n\n        @classmethod\n        def prepare_field(cls, field: ModelField) -> None:\n            env_names: Union[List[str], AbstractSet[str]]\n            field_info_from_config = cls.get_field_info(field.name)\n\n            env = field_info_from_config.get('env') or field.field_info.extra.get('env')\n            if env is None:\n                if field.has_alias:\n                    warnings.warn(\n                        'aliases are no longer used by BaseSettings to define which environment variables to read. '\n                        'Instead use the \"env\" field setting. '\n                        'See https://pydantic-docs.helpmanual.io/usage/settings/#environment-variable-names',\n                        FutureWarning,\n                    )\n                env_names = {cls.env_prefix + field.name}\n            elif isinstance(env, str):\n                env_names = {env}\n            elif isinstance(env, (set, frozenset)):\n                env_names = env\n            elif sequence_like(env):\n                env_names = list(env)\n            else:\n                raise TypeError(f'invalid field env: {env!r} ({display_as_type(env)}); should be string, list or set')\n\n            if not cls.case_sensitive:\n                env_names = env_names.__class__(n.lower() for n in env_names)\n            field.field_info.extra['env_names'] = env_names\n\n        @classmethod\n        def customise_sources(\n            cls,\n            init_settings: SettingsSourceCallable,\n            env_settings: SettingsSourceCallable,\n            file_secret_settings: SettingsSourceCallable,\n        ) -> Tuple[SettingsSourceCallable, ...]:\n            return init_settings, env_settings, file_secret_settings\n\n        @classmethod\n        def parse_env_var(cls, field_name: str, raw_val: str) -> Any:\n            return cls.json_loads(raw_val)\n\n    # populated by the metaclass using the Config class defined above, annotated here to help IDEs only\n    __config__: ClassVar[Type[Config]]\n\n\nclass InitSettingsSource:\n    __slots__ = ('init_kwargs',)\n\n    def __init__(self, init_kwargs: Dict[str, Any]):\n        self.init_kwargs = init_kwargs\n\n    def __call__(self, settings: BaseSettings) -> Dict[str, Any]:\n        return self.init_kwargs\n\n    def __repr__(self) -> str:\n        return f'InitSettingsSource(init_kwargs={self.init_kwargs!r})'\n\n\nclass EnvSettingsSource:\n    __slots__ = ('env_file', 'env_file_encoding', 'env_nested_delimiter', 'env_prefix_len')\n\n    def __init__(\n        self,\n        env_file: Optional[DotenvType],\n        env_file_encoding: Optional[str],\n        env_nested_delimiter: Optional[str] = None,\n        env_prefix_len: int = 0,\n    ):\n        self.env_file: Optional[DotenvType] = env_file\n        self.env_file_encoding: Optional[str] = env_file_encoding\n        self.env_nested_delimiter: Optional[str] = env_nested_delimiter\n        self.env_prefix_len: int = env_prefix_len\n\n    def __call__(self, settings: BaseSettings) -> Dict[str, Any]:  # noqa C901\n        \"\"\"\n        Build environment variables suitable for passing to the Model.\n        \"\"\"\n        d: Dict[str, Any] = {}\n\n        if settings.__config__.case_sensitive:\n            env_vars: Mapping[str, Optional[str]] = os.environ\n        else:\n            env_vars = {k.lower(): v for k, v in os.environ.items()}\n\n        dotenv_vars = self._read_env_files(settings.__config__.case_sensitive)\n        if dotenv_vars:\n            env_vars = {**dotenv_vars, **env_vars}\n\n        for field in settings.__fields__.values():\n            env_val: Optional[str] = None\n            for env_name in field.field_info.extra['env_names']:\n                env_val = env_vars.get(env_name)\n                if env_val is not None:\n                    break\n\n            is_complex, allow_parse_failure = self.field_is_complex(field)\n            if is_complex:\n                if env_val is None:\n                    # field is complex but no value found so far, try explode_env_vars\n                    env_val_built = self.explode_env_vars(field, env_vars)\n                    if env_val_built:\n                        d[field.alias] = env_val_built\n                else:\n                    # field is complex and there's a value, decode that as JSON, then add explode_env_vars\n                    try:\n                        env_val = settings.__config__.parse_env_var(field.name, env_val)\n                    except ValueError as e:\n                        if not allow_parse_failure:\n                            raise SettingsError(f'error parsing env var \"{env_name}\"') from e\n\n                    if isinstance(env_val, dict):\n                        d[field.alias] = deep_update(env_val, self.explode_env_vars(field, env_vars))\n                    else:\n                        d[field.alias] = env_val\n            elif env_val is not None:\n                # simplest case, field is not complex, we only need to add the value if it was found\n                d[field.alias] = env_val\n\n        return d\n\n    def _read_env_files(self, case_sensitive: bool) -> Dict[str, Optional[str]]:\n        env_files = self.env_file\n        if env_files is None:\n            return {}\n\n        if isinstance(env_files, (str, os.PathLike)):\n            env_files = [env_files]\n\n        dotenv_vars = {}\n        for env_file in env_files:\n            env_path = Path(env_file).expanduser()\n            if env_path.is_file():\n                dotenv_vars.update(\n                    read_env_file(env_path, encoding=self.env_file_encoding, case_sensitive=case_sensitive)\n                )\n\n        return dotenv_vars\n\n    def field_is_complex(self, field: ModelField) -> Tuple[bool, bool]:\n        \"\"\"\n        Find out if a field is complex, and if so whether JSON errors should be ignored\n        \"\"\"\n        if lenient_issubclass(field.annotation, JsonWrapper):\n            return False, False\n\n        if field.is_complex():\n            allow_parse_failure = False\n        elif is_union(get_origin(field.type_)) and field.sub_fields and any(f.is_complex() for f in field.sub_fields):\n            allow_parse_failure = True\n        else:\n            return False, False\n\n        return True, allow_parse_failure\n\n    def explode_env_vars(self, field: ModelField, env_vars: Mapping[str, Optional[str]]) -> Dict[str, Any]:\n        \"\"\"\n        Process env_vars and extract the values of keys containing env_nested_delimiter into nested dictionaries.\n\n        This is applied to a single field, hence filtering by env_var prefix.\n        \"\"\"\n        prefixes = [f'{env_name}{self.env_nested_delimiter}' for env_name in field.field_info.extra['env_names']]\n        result: Dict[str, Any] = {}\n        for env_name, env_val in env_vars.items():\n            if not any(env_name.startswith(prefix) for prefix in prefixes):\n                continue\n            # we remove the prefix before splitting in case the prefix has characters in common with the delimiter\n            env_name_without_prefix = env_name[self.env_prefix_len :]\n            _, *keys, last_key = env_name_without_prefix.split(self.env_nested_delimiter)\n            env_var = result\n            for key in keys:\n                env_var = env_var.setdefault(key, {})\n            env_var[last_key] = env_val\n\n        return result\n\n    def __repr__(self) -> str:\n        return (\n            f'EnvSettingsSource(env_file={self.env_file!r}, env_file_encoding={self.env_file_encoding!r}, '\n            f'env_nested_delimiter={self.env_nested_delimiter!r})'\n        )\n\n\nclass SecretsSettingsSource:\n    __slots__ = ('secrets_dir',)\n\n    def __init__(self, secrets_dir: Optional[StrPath]):\n        self.secrets_dir: Optional[StrPath] = secrets_dir\n\n    def __call__(self, settings: BaseSettings) -> Dict[str, Any]:\n        \"\"\"\n        Build fields from \"secrets\" files.\n        \"\"\"\n        secrets: Dict[str, Optional[str]] = {}\n\n        if self.secrets_dir is None:\n            return secrets\n\n        secrets_path = Path(self.secrets_dir).expanduser()\n\n        if not secrets_path.exists():\n            warnings.warn(f'directory \"{secrets_path}\" does not exist')\n            return secrets\n\n        if not secrets_path.is_dir():\n            raise SettingsError(f'secrets_dir must reference a directory, not a {path_type(secrets_path)}')\n\n        for field in settings.__fields__.values():\n            for env_name in field.field_info.extra['env_names']:\n                path = find_case_path(secrets_path, env_name, settings.__config__.case_sensitive)\n                if not path:\n                    # path does not exist, we currently don't return a warning for this\n                    continue\n\n                if path.is_file():\n                    secret_value = path.read_text().strip()\n                    if field.is_complex():\n                        try:\n                            secret_value = settings.__config__.parse_env_var(field.name, secret_value)\n                        except ValueError as e:\n                            raise SettingsError(f'error parsing env var \"{env_name}\"') from e\n\n                    secrets[field.alias] = secret_value\n                else:\n                    warnings.warn(\n                        f'attempted to load secret file \"{path}\" but found a {path_type(path)} instead.',\n                        stacklevel=4,\n                    )\n        return secrets\n\n    def __repr__(self) -> str:\n        return f'SecretsSettingsSource(secrets_dir={self.secrets_dir!r})'\n\n\ndef read_env_file(\n    file_path: StrPath, *, encoding: str = None, case_sensitive: bool = False\n) -> Dict[str, Optional[str]]:\n    try:\n        from dotenv import dotenv_values\n    except ImportError as e:\n        raise ImportError('python-dotenv is not installed, run `pip install pydantic[dotenv]`') from e\n\n    file_vars: Dict[str, Optional[str]] = dotenv_values(file_path, encoding=encoding or 'utf8')\n    if not case_sensitive:\n        return {k.lower(): v for k, v in file_vars.items()}\n    else:\n        return file_vars\n\n\ndef find_case_path(dir_path: Path, file_name: str, case_sensitive: bool) -> Optional[Path]:\n    \"\"\"\n    Find a file within path's directory matching filename, optionally ignoring case.\n    \"\"\"\n    for f in dir_path.iterdir():\n        if f.name == file_name:\n            return f\n        elif not case_sensitive and f.name.lower() == file_name.lower():\n            return f\n    return None\n", "pydantic/v1/typing.py": "import sys\nimport typing\nfrom collections.abc import Callable\nfrom os import PathLike\nfrom typing import (  # type: ignore\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    Callable as TypingCallable,\n    ClassVar,\n    Dict,\n    ForwardRef,\n    Generator,\n    Iterable,\n    List,\n    Mapping,\n    NewType,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    _eval_type,\n    cast,\n    get_type_hints,\n)\n\nfrom typing_extensions import (\n    Annotated,\n    Final,\n    Literal,\n    NotRequired as TypedDictNotRequired,\n    Required as TypedDictRequired,\n)\n\ntry:\n    from typing import _TypingBase as typing_base  # type: ignore\nexcept ImportError:\n    from typing import _Final as typing_base  # type: ignore\n\ntry:\n    from typing import GenericAlias as TypingGenericAlias  # type: ignore\nexcept ImportError:\n    # python < 3.9 does not have GenericAlias (list[int], tuple[str, ...] and so on)\n    TypingGenericAlias = ()\n\ntry:\n    from types import UnionType as TypesUnionType  # type: ignore\nexcept ImportError:\n    # python < 3.10 does not have UnionType (str | int, byte | bool and so on)\n    TypesUnionType = ()\n\n\nif sys.version_info < (3, 9):\n\n    def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -> Any:\n        return type_._evaluate(globalns, localns)\n\nelse:\n\n    def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -> Any:\n        # Even though it is the right signature for python 3.9, mypy complains with\n        # `error: Too many arguments for \"_evaluate\" of \"ForwardRef\"` hence the cast...\n        # Python 3.13/3.12.4+ made `recursive_guard` a kwarg, so name it explicitly to avoid:\n        # TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\n        return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n\n\nif sys.version_info < (3, 9):\n    # Ensure we always get all the whole `Annotated` hint, not just the annotated type.\n    # For 3.7 to 3.8, `get_type_hints` doesn't recognize `typing_extensions.Annotated`,\n    # so it already returns the full annotation\n    get_all_type_hints = get_type_hints\n\nelse:\n\n    def get_all_type_hints(obj: Any, globalns: Any = None, localns: Any = None) -> Any:\n        return get_type_hints(obj, globalns, localns, include_extras=True)\n\n\n_T = TypeVar('_T')\n\nAnyCallable = TypingCallable[..., Any]\nNoArgAnyCallable = TypingCallable[[], Any]\n\n# workaround for https://github.com/python/mypy/issues/9496\nAnyArgTCallable = TypingCallable[..., _T]\n\n\n# Annotated[...] is implemented by returning an instance of one of these classes, depending on\n# python/typing_extensions version.\nAnnotatedTypeNames = {'AnnotatedMeta', '_AnnotatedAlias'}\n\n\nLITERAL_TYPES: Set[Any] = {Literal}\nif hasattr(typing, 'Literal'):\n    LITERAL_TYPES.add(typing.Literal)\n\n\nif sys.version_info < (3, 8):\n\n    def get_origin(t: Type[Any]) -> Optional[Type[Any]]:\n        if type(t).__name__ in AnnotatedTypeNames:\n            # weirdly this is a runtime requirement, as well as for mypy\n            return cast(Type[Any], Annotated)\n        return getattr(t, '__origin__', None)\n\nelse:\n    from typing import get_origin as _typing_get_origin\n\n    def get_origin(tp: Type[Any]) -> Optional[Type[Any]]:\n        \"\"\"\n        We can't directly use `typing.get_origin` since we need a fallback to support\n        custom generic classes like `ConstrainedList`\n        It should be useless once https://github.com/cython/cython/issues/3537 is\n        solved and https://github.com/pydantic/pydantic/pull/1753 is merged.\n        \"\"\"\n        if type(tp).__name__ in AnnotatedTypeNames:\n            return cast(Type[Any], Annotated)  # mypy complains about _SpecialForm\n        return _typing_get_origin(tp) or getattr(tp, '__origin__', None)\n\n\nif sys.version_info < (3, 8):\n    from typing import _GenericAlias\n\n    def get_args(t: Type[Any]) -> Tuple[Any, ...]:\n        \"\"\"Compatibility version of get_args for python 3.7.\n\n        Mostly compatible with the python 3.8 `typing` module version\n        and able to handle almost all use cases.\n        \"\"\"\n        if type(t).__name__ in AnnotatedTypeNames:\n            return t.__args__ + t.__metadata__\n        if isinstance(t, _GenericAlias):\n            res = t.__args__\n            if t.__origin__ is Callable and res and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return getattr(t, '__args__', ())\n\nelse:\n    from typing import get_args as _typing_get_args\n\n    def _generic_get_args(tp: Type[Any]) -> Tuple[Any, ...]:\n        \"\"\"\n        In python 3.9, `typing.Dict`, `typing.List`, ...\n        do have an empty `__args__` by default (instead of the generic ~T for example).\n        In order to still support `Dict` for example and consider it as `Dict[Any, Any]`,\n        we retrieve the `_nparams` value that tells us how many parameters it needs.\n        \"\"\"\n        if hasattr(tp, '_nparams'):\n            return (Any,) * tp._nparams\n        # Special case for `tuple[()]`, which used to return ((),) with `typing.Tuple`\n        # in python 3.10- but now returns () for `tuple` and `Tuple`.\n        # This will probably be clarified in pydantic v2\n        try:\n            if tp == Tuple[()] or sys.version_info >= (3, 9) and tp == tuple[()]:  # type: ignore[misc]\n                return ((),)\n        # there is a TypeError when compiled with cython\n        except TypeError:  # pragma: no cover\n            pass\n        return ()\n\n    def get_args(tp: Type[Any]) -> Tuple[Any, ...]:\n        \"\"\"Get type arguments with all substitutions performed.\n\n        For unions, basic simplifications used by Union constructor are performed.\n        Examples::\n            get_args(Dict[str, int]) == (str, int)\n            get_args(int) == ()\n            get_args(Union[int, Union[T, int], str][int]) == (int, str)\n            get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n            get_args(Callable[[], T][int]) == ([], int)\n        \"\"\"\n        if type(tp).__name__ in AnnotatedTypeNames:\n            return tp.__args__ + tp.__metadata__\n        # the fallback is needed for the same reasons as `get_origin` (see above)\n        return _typing_get_args(tp) or getattr(tp, '__args__', ()) or _generic_get_args(tp)\n\n\nif sys.version_info < (3, 9):\n\n    def convert_generics(tp: Type[Any]) -> Type[Any]:\n        \"\"\"Python 3.9 and older only supports generics from `typing` module.\n        They convert strings to ForwardRef automatically.\n\n        Examples::\n            typing.List['Hero'] == typing.List[ForwardRef('Hero')]\n        \"\"\"\n        return tp\n\nelse:\n    from typing import _UnionGenericAlias  # type: ignore\n\n    from typing_extensions import _AnnotatedAlias\n\n    def convert_generics(tp: Type[Any]) -> Type[Any]:\n        \"\"\"\n        Recursively searches for `str` type hints and replaces them with ForwardRef.\n\n        Examples::\n            convert_generics(list['Hero']) == list[ForwardRef('Hero')]\n            convert_generics(dict['Hero', 'Team']) == dict[ForwardRef('Hero'), ForwardRef('Team')]\n            convert_generics(typing.Dict['Hero', 'Team']) == typing.Dict[ForwardRef('Hero'), ForwardRef('Team')]\n            convert_generics(list[str | 'Hero'] | int) == list[str | ForwardRef('Hero')] | int\n        \"\"\"\n        origin = get_origin(tp)\n        if not origin or not hasattr(tp, '__args__'):\n            return tp\n\n        args = get_args(tp)\n\n        # typing.Annotated needs special treatment\n        if origin is Annotated:\n            return _AnnotatedAlias(convert_generics(args[0]), args[1:])\n\n        # recursively replace `str` instances inside of `GenericAlias` with `ForwardRef(arg)`\n        converted = tuple(\n            ForwardRef(arg) if isinstance(arg, str) and isinstance(tp, TypingGenericAlias) else convert_generics(arg)\n            for arg in args\n        )\n\n        if converted == args:\n            return tp\n        elif isinstance(tp, TypingGenericAlias):\n            return TypingGenericAlias(origin, converted)\n        elif isinstance(tp, TypesUnionType):\n            # recreate types.UnionType (PEP604, Python >= 3.10)\n            return _UnionGenericAlias(origin, converted)\n        else:\n            try:\n                setattr(tp, '__args__', converted)\n            except AttributeError:\n                pass\n            return tp\n\n\nif sys.version_info < (3, 10):\n\n    def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union\n\n    WithArgsTypes = (TypingGenericAlias,)\n\nelse:\n    import types\n    import typing\n\n    def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union or tp is types.UnionType  # noqa: E721\n\n    WithArgsTypes = (typing._GenericAlias, types.GenericAlias, types.UnionType)\n\n\nStrPath = Union[str, PathLike]\n\n\nif TYPE_CHECKING:\n    from pydantic.v1.fields import ModelField\n\n    TupleGenerator = Generator[Tuple[str, Any], None, None]\n    DictStrAny = Dict[str, Any]\n    DictAny = Dict[Any, Any]\n    SetStr = Set[str]\n    ListStr = List[str]\n    IntStr = Union[int, str]\n    AbstractSetIntStr = AbstractSet[IntStr]\n    DictIntStrAny = Dict[IntStr, Any]\n    MappingIntStrAny = Mapping[IntStr, Any]\n    CallableGenerator = Generator[AnyCallable, None, None]\n    ReprArgs = Sequence[Tuple[Optional[str], Any]]\n\n    MYPY = False\n    if MYPY:\n        AnyClassMethod = classmethod[Any]\n    else:\n        # classmethod[TargetType, CallableParamSpecType, CallableReturnType]\n        AnyClassMethod = classmethod[Any, Any, Any]\n\n__all__ = (\n    'AnyCallable',\n    'NoArgAnyCallable',\n    'NoneType',\n    'is_none_type',\n    'display_as_type',\n    'resolve_annotations',\n    'is_callable_type',\n    'is_literal_type',\n    'all_literal_values',\n    'is_namedtuple',\n    'is_typeddict',\n    'is_typeddict_special',\n    'is_new_type',\n    'new_type_supertype',\n    'is_classvar',\n    'is_finalvar',\n    'update_field_forward_refs',\n    'update_model_forward_refs',\n    'TupleGenerator',\n    'DictStrAny',\n    'DictAny',\n    'SetStr',\n    'ListStr',\n    'IntStr',\n    'AbstractSetIntStr',\n    'DictIntStrAny',\n    'CallableGenerator',\n    'ReprArgs',\n    'AnyClassMethod',\n    'CallableGenerator',\n    'WithArgsTypes',\n    'get_args',\n    'get_origin',\n    'get_sub_types',\n    'typing_base',\n    'get_all_type_hints',\n    'is_union',\n    'StrPath',\n    'MappingIntStrAny',\n)\n\n\nNoneType = None.__class__\n\n\nNONE_TYPES: Tuple[Any, Any, Any] = (None, NoneType, Literal[None])\n\n\nif sys.version_info < (3, 8):\n    # Even though this implementation is slower, we need it for python 3.7:\n    # In python 3.7 \"Literal\" is not a builtin type and uses a different\n    # mechanism.\n    # for this reason `Literal[None] is Literal[None]` evaluates to `False`,\n    # breaking the faster implementation used for the other python versions.\n\n    def is_none_type(type_: Any) -> bool:\n        return type_ in NONE_TYPES\n\nelif sys.version_info[:2] == (3, 8):\n\n    def is_none_type(type_: Any) -> bool:\n        for none_type in NONE_TYPES:\n            if type_ is none_type:\n                return True\n        # With python 3.8, specifically 3.8.10, Literal \"is\" check sare very flakey\n        # can change on very subtle changes like use of types in other modules,\n        # hopefully this check avoids that issue.\n        if is_literal_type(type_):  # pragma: no cover\n            return all_literal_values(type_) == (None,)\n        return False\n\nelse:\n\n    def is_none_type(type_: Any) -> bool:\n        return type_ in NONE_TYPES\n\n\ndef display_as_type(v: Type[Any]) -> str:\n    if not isinstance(v, typing_base) and not isinstance(v, WithArgsTypes) and not isinstance(v, type):\n        v = v.__class__\n\n    if is_union(get_origin(v)):\n        return f'Union[{\", \".join(map(display_as_type, get_args(v)))}]'\n\n    if isinstance(v, WithArgsTypes):\n        # Generic alias are constructs like `list[int]`\n        return str(v).replace('typing.', '')\n\n    try:\n        return v.__name__\n    except AttributeError:\n        # happens with typing objects\n        return str(v).replace('typing.', '')\n\n\ndef resolve_annotations(raw_annotations: Dict[str, Type[Any]], module_name: Optional[str]) -> Dict[str, Type[Any]]:\n    \"\"\"\n    Partially taken from typing.get_type_hints.\n\n    Resolve string or ForwardRef annotations into type objects if possible.\n    \"\"\"\n    base_globals: Optional[Dict[str, Any]] = None\n    if module_name:\n        try:\n            module = sys.modules[module_name]\n        except KeyError:\n            # happens occasionally, see https://github.com/pydantic/pydantic/issues/2363\n            pass\n        else:\n            base_globals = module.__dict__\n\n    annotations = {}\n    for name, value in raw_annotations.items():\n        if isinstance(value, str):\n            if (3, 10) > sys.version_info >= (3, 9, 8) or sys.version_info >= (3, 10, 1):\n                value = ForwardRef(value, is_argument=False, is_class=True)\n            else:\n                value = ForwardRef(value, is_argument=False)\n        try:\n            value = _eval_type(value, base_globals, None)\n        except NameError:\n            # this is ok, it can be fixed with update_forward_refs\n            pass\n        annotations[name] = value\n    return annotations\n\n\ndef is_callable_type(type_: Type[Any]) -> bool:\n    return type_ is Callable or get_origin(type_) is Callable\n\n\ndef is_literal_type(type_: Type[Any]) -> bool:\n    return Literal is not None and get_origin(type_) in LITERAL_TYPES\n\n\ndef literal_values(type_: Type[Any]) -> Tuple[Any, ...]:\n    return get_args(type_)\n\n\ndef all_literal_values(type_: Type[Any]) -> Tuple[Any, ...]:\n    \"\"\"\n    This method is used to retrieve all Literal values as\n    Literal can be used recursively (see https://www.python.org/dev/peps/pep-0586)\n    e.g. `Literal[Literal[Literal[1, 2, 3], \"foo\"], 5, None]`\n    \"\"\"\n    if not is_literal_type(type_):\n        return (type_,)\n\n    values = literal_values(type_)\n    return tuple(x for value in values for x in all_literal_values(value))\n\n\ndef is_namedtuple(type_: Type[Any]) -> bool:\n    \"\"\"\n    Check if a given class is a named tuple.\n    It can be either a `typing.NamedTuple` or `collections.namedtuple`\n    \"\"\"\n    from pydantic.v1.utils import lenient_issubclass\n\n    return lenient_issubclass(type_, tuple) and hasattr(type_, '_fields')\n\n\ndef is_typeddict(type_: Type[Any]) -> bool:\n    \"\"\"\n    Check if a given class is a typed dict (from `typing` or `typing_extensions`)\n    In 3.10, there will be a public method (https://docs.python.org/3.10/library/typing.html#typing.is_typeddict)\n    \"\"\"\n    from pydantic.v1.utils import lenient_issubclass\n\n    return lenient_issubclass(type_, dict) and hasattr(type_, '__total__')\n\n\ndef _check_typeddict_special(type_: Any) -> bool:\n    return type_ is TypedDictRequired or type_ is TypedDictNotRequired\n\n\ndef is_typeddict_special(type_: Any) -> bool:\n    \"\"\"\n    Check if type is a TypedDict special form (Required or NotRequired).\n    \"\"\"\n    return _check_typeddict_special(type_) or _check_typeddict_special(get_origin(type_))\n\n\ntest_type = NewType('test_type', str)\n\n\ndef is_new_type(type_: Type[Any]) -> bool:\n    \"\"\"\n    Check whether type_ was created using typing.NewType\n    \"\"\"\n    return isinstance(type_, test_type.__class__) and hasattr(type_, '__supertype__')  # type: ignore\n\n\ndef new_type_supertype(type_: Type[Any]) -> Type[Any]:\n    while hasattr(type_, '__supertype__'):\n        type_ = type_.__supertype__\n    return type_\n\n\ndef _check_classvar(v: Optional[Type[Any]]) -> bool:\n    if v is None:\n        return False\n\n    return v.__class__ == ClassVar.__class__ and getattr(v, '_name', None) == 'ClassVar'\n\n\ndef _check_finalvar(v: Optional[Type[Any]]) -> bool:\n    \"\"\"\n    Check if a given type is a `typing.Final` type.\n    \"\"\"\n    if v is None:\n        return False\n\n    return v.__class__ == Final.__class__ and (sys.version_info < (3, 8) or getattr(v, '_name', None) == 'Final')\n\n\ndef is_classvar(ann_type: Type[Any]) -> bool:\n    if _check_classvar(ann_type) or _check_classvar(get_origin(ann_type)):\n        return True\n\n    # this is an ugly workaround for class vars that contain forward references and are therefore themselves\n    # forward references, see #3679\n    if ann_type.__class__ == ForwardRef and ann_type.__forward_arg__.startswith('ClassVar['):\n        return True\n\n    return False\n\n\ndef is_finalvar(ann_type: Type[Any]) -> bool:\n    return _check_finalvar(ann_type) or _check_finalvar(get_origin(ann_type))\n\n\ndef update_field_forward_refs(field: 'ModelField', globalns: Any, localns: Any) -> None:\n    \"\"\"\n    Try to update ForwardRefs on fields based on this ModelField, globalns and localns.\n    \"\"\"\n    prepare = False\n    if field.type_.__class__ == ForwardRef:\n        prepare = True\n        field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\n    if field.outer_type_.__class__ == ForwardRef:\n        prepare = True\n        field.outer_type_ = evaluate_forwardref(field.outer_type_, globalns, localns or None)\n    if prepare:\n        field.prepare()\n\n    if field.sub_fields:\n        for sub_f in field.sub_fields:\n            update_field_forward_refs(sub_f, globalns=globalns, localns=localns)\n\n    if field.discriminator_key is not None:\n        field.prepare_discriminated_union_sub_fields()\n\n\ndef update_model_forward_refs(\n    model: Type[Any],\n    fields: Iterable['ModelField'],\n    json_encoders: Dict[Union[Type[Any], str, ForwardRef], AnyCallable],\n    localns: 'DictStrAny',\n    exc_to_suppress: Tuple[Type[BaseException], ...] = (),\n) -> None:\n    \"\"\"\n    Try to update model fields ForwardRefs based on model and localns.\n    \"\"\"\n    if model.__module__ in sys.modules:\n        globalns = sys.modules[model.__module__].__dict__.copy()\n    else:\n        globalns = {}\n\n    globalns.setdefault(model.__name__, model)\n\n    for f in fields:\n        try:\n            update_field_forward_refs(f, globalns=globalns, localns=localns)\n        except exc_to_suppress:\n            pass\n\n    for key in set(json_encoders.keys()):\n        if isinstance(key, str):\n            fr: ForwardRef = ForwardRef(key)\n        elif isinstance(key, ForwardRef):\n            fr = key\n        else:\n            continue\n\n        try:\n            new_key = evaluate_forwardref(fr, globalns, localns or None)\n        except exc_to_suppress:  # pragma: no cover\n            continue\n\n        json_encoders[new_key] = json_encoders.pop(key)\n\n\ndef get_class(type_: Type[Any]) -> Union[None, bool, Type[Any]]:\n    \"\"\"\n    Tries to get the class of a Type[T] annotation. Returns True if Type is used\n    without brackets. Otherwise returns None.\n    \"\"\"\n    if type_ is type:\n        return True\n\n    if get_origin(type_) is None:\n        return None\n\n    args = get_args(type_)\n    if not args or not isinstance(args[0], type):\n        return True\n    else:\n        return args[0]\n\n\ndef get_sub_types(tp: Any) -> List[Any]:\n    \"\"\"\n    Return all the types that are allowed by type `tp`\n    `tp` can be a `Union` of allowed types or an `Annotated` type\n    \"\"\"\n    origin = get_origin(tp)\n    if origin is Annotated:\n        return get_sub_types(get_args(tp)[0])\n    elif is_union(origin):\n        return [x for t in get_args(tp) for x in get_sub_types(t)]\n    else:\n        return [tp]\n", "pydantic/v1/parse.py": "import json\nimport pickle\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Callable, Union\n\nfrom pydantic.v1.types import StrBytes\n\n\nclass Protocol(str, Enum):\n    json = 'json'\n    pickle = 'pickle'\n\n\ndef load_str_bytes(\n    b: StrBytes,\n    *,\n    content_type: str = None,\n    encoding: str = 'utf8',\n    proto: Protocol = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    if proto is None and content_type:\n        if content_type.endswith(('json', 'javascript')):\n            pass\n        elif allow_pickle and content_type.endswith('pickle'):\n            proto = Protocol.pickle\n        else:\n            raise TypeError(f'Unknown content-type: {content_type}')\n\n    proto = proto or Protocol.json\n\n    if proto == Protocol.json:\n        if isinstance(b, bytes):\n            b = b.decode(encoding)\n        return json_loads(b)\n    elif proto == Protocol.pickle:\n        if not allow_pickle:\n            raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n        bb = b if isinstance(b, bytes) else b.encode()\n        return pickle.loads(bb)\n    else:\n        raise TypeError(f'Unknown protocol: {proto}')\n\n\ndef load_file(\n    path: Union[str, Path],\n    *,\n    content_type: str = None,\n    encoding: str = 'utf8',\n    proto: Protocol = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    path = Path(path)\n    b = path.read_bytes()\n    if content_type is None:\n        if path.suffix in ('.js', '.json'):\n            proto = Protocol.json\n        elif path.suffix == '.pkl':\n            proto = Protocol.pickle\n\n    return load_str_bytes(\n        b, proto=proto, content_type=content_type, encoding=encoding, allow_pickle=allow_pickle, json_loads=json_loads\n    )\n", "pydantic/v1/schema.py": "import re\nimport warnings\nfrom collections import defaultdict\nfrom dataclasses import is_dataclass\nfrom datetime import date, datetime, time, timedelta\nfrom decimal import Decimal\nfrom enum import Enum\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom pathlib import Path\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    ForwardRef,\n    FrozenSet,\n    Generic,\n    Iterable,\n    List,\n    Optional,\n    Pattern,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom uuid import UUID\n\nfrom typing_extensions import Annotated, Literal\n\nfrom pydantic.v1.fields import (\n    MAPPING_LIKE_SHAPES,\n    SHAPE_DEQUE,\n    SHAPE_FROZENSET,\n    SHAPE_GENERIC,\n    SHAPE_ITERABLE,\n    SHAPE_LIST,\n    SHAPE_SEQUENCE,\n    SHAPE_SET,\n    SHAPE_SINGLETON,\n    SHAPE_TUPLE,\n    SHAPE_TUPLE_ELLIPSIS,\n    FieldInfo,\n    ModelField,\n)\nfrom pydantic.v1.json import pydantic_encoder\nfrom pydantic.v1.networks import AnyUrl, EmailStr\nfrom pydantic.v1.types import (\n    ConstrainedDecimal,\n    ConstrainedFloat,\n    ConstrainedFrozenSet,\n    ConstrainedInt,\n    ConstrainedList,\n    ConstrainedSet,\n    ConstrainedStr,\n    SecretBytes,\n    SecretStr,\n    StrictBytes,\n    StrictStr,\n    conbytes,\n    condecimal,\n    confloat,\n    confrozenset,\n    conint,\n    conlist,\n    conset,\n    constr,\n)\nfrom pydantic.v1.typing import (\n    all_literal_values,\n    get_args,\n    get_origin,\n    get_sub_types,\n    is_callable_type,\n    is_literal_type,\n    is_namedtuple,\n    is_none_type,\n    is_union,\n)\nfrom pydantic.v1.utils import ROOT_KEY, get_model, lenient_issubclass\n\nif TYPE_CHECKING:\n    from pydantic.v1.dataclasses import Dataclass\n    from pydantic.v1.main import BaseModel\n\ndefault_prefix = '#/definitions/'\ndefault_ref_template = '#/definitions/{model}'\n\nTypeModelOrEnum = Union[Type['BaseModel'], Type[Enum]]\nTypeModelSet = Set[TypeModelOrEnum]\n\n\ndef _apply_modify_schema(\n    modify_schema: Callable[..., None], field: Optional[ModelField], field_schema: Dict[str, Any]\n) -> None:\n    from inspect import signature\n\n    sig = signature(modify_schema)\n    args = set(sig.parameters.keys())\n    if 'field' in args or 'kwargs' in args:\n        modify_schema(field_schema, field=field)\n    else:\n        modify_schema(field_schema)\n\n\ndef schema(\n    models: Sequence[Union[Type['BaseModel'], Type['Dataclass']]],\n    *,\n    by_alias: bool = True,\n    title: Optional[str] = None,\n    description: Optional[str] = None,\n    ref_prefix: Optional[str] = None,\n    ref_template: str = default_ref_template,\n) -> Dict[str, Any]:\n    \"\"\"\n    Process a list of models and generate a single JSON Schema with all of them defined in the ``definitions``\n    top-level JSON key, including their sub-models.\n\n    :param models: a list of models to include in the generated JSON Schema\n    :param by_alias: generate the schemas using the aliases defined, if any\n    :param title: title for the generated schema that includes the definitions\n    :param description: description for the generated schema\n    :param ref_prefix: the JSON Pointer prefix for schema references with ``$ref``, if None, will be set to the\n      default of ``#/definitions/``. Update it if you want the schemas to reference the definitions somewhere\n      else, e.g. for OpenAPI use ``#/components/schemas/``. The resulting generated schemas will still be at the\n      top-level key ``definitions``, so you can extract them from there. But all the references will have the set\n      prefix.\n    :param ref_template: Use a ``string.format()`` template for ``$ref`` instead of a prefix. This can be useful\n      for references that cannot be represented by ``ref_prefix`` such as a definition stored in another file. For\n      a sibling json file in a ``/schemas`` directory use ``\"/schemas/${model}.json#\"``.\n    :return: dict with the JSON Schema with a ``definitions`` top-level key including the schema definitions for\n      the models and sub-models passed in ``models``.\n    \"\"\"\n    clean_models = [get_model(model) for model in models]\n    flat_models = get_flat_models_from_models(clean_models)\n    model_name_map = get_model_name_map(flat_models)\n    definitions = {}\n    output_schema: Dict[str, Any] = {}\n    if title:\n        output_schema['title'] = title\n    if description:\n        output_schema['description'] = description\n    for model in clean_models:\n        m_schema, m_definitions, m_nested_models = model_process_schema(\n            model,\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            ref_prefix=ref_prefix,\n            ref_template=ref_template,\n        )\n        definitions.update(m_definitions)\n        model_name = model_name_map[model]\n        definitions[model_name] = m_schema\n    if definitions:\n        output_schema['definitions'] = definitions\n    return output_schema\n\n\ndef model_schema(\n    model: Union[Type['BaseModel'], Type['Dataclass']],\n    by_alias: bool = True,\n    ref_prefix: Optional[str] = None,\n    ref_template: str = default_ref_template,\n) -> Dict[str, Any]:\n    \"\"\"\n    Generate a JSON Schema for one model. With all the sub-models defined in the ``definitions`` top-level\n    JSON key.\n\n    :param model: a Pydantic model (a class that inherits from BaseModel)\n    :param by_alias: generate the schemas using the aliases defined, if any\n    :param ref_prefix: the JSON Pointer prefix for schema references with ``$ref``, if None, will be set to the\n      default of ``#/definitions/``. Update it if you want the schemas to reference the definitions somewhere\n      else, e.g. for OpenAPI use ``#/components/schemas/``. The resulting generated schemas will still be at the\n      top-level key ``definitions``, so you can extract them from there. But all the references will have the set\n      prefix.\n    :param ref_template: Use a ``string.format()`` template for ``$ref`` instead of a prefix. This can be useful for\n      references that cannot be represented by ``ref_prefix`` such as a definition stored in another file. For a\n      sibling json file in a ``/schemas`` directory use ``\"/schemas/${model}.json#\"``.\n    :return: dict with the JSON Schema for the passed ``model``\n    \"\"\"\n    model = get_model(model)\n    flat_models = get_flat_models_from_model(model)\n    model_name_map = get_model_name_map(flat_models)\n    model_name = model_name_map[model]\n    m_schema, m_definitions, nested_models = model_process_schema(\n        model, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix, ref_template=ref_template\n    )\n    if model_name in nested_models:\n        # model_name is in Nested models, it has circular references\n        m_definitions[model_name] = m_schema\n        m_schema = get_schema_ref(model_name, ref_prefix, ref_template, False)\n    if m_definitions:\n        m_schema.update({'definitions': m_definitions})\n    return m_schema\n\n\ndef get_field_info_schema(field: ModelField, schema_overrides: bool = False) -> Tuple[Dict[str, Any], bool]:\n    # If no title is explicitly set, we don't set title in the schema for enums.\n    # The behaviour is the same as `BaseModel` reference, where the default title\n    # is in the definitions part of the schema.\n    schema_: Dict[str, Any] = {}\n    if field.field_info.title or not lenient_issubclass(field.type_, Enum):\n        schema_['title'] = field.field_info.title or field.alias.title().replace('_', ' ')\n\n    if field.field_info.title:\n        schema_overrides = True\n\n    if field.field_info.description:\n        schema_['description'] = field.field_info.description\n        schema_overrides = True\n\n    if not field.required and field.default is not None and not is_callable_type(field.outer_type_):\n        schema_['default'] = encode_default(field.default)\n        schema_overrides = True\n\n    return schema_, schema_overrides\n\n\ndef field_schema(\n    field: ModelField,\n    *,\n    by_alias: bool = True,\n    model_name_map: Dict[TypeModelOrEnum, str],\n    ref_prefix: Optional[str] = None,\n    ref_template: str = default_ref_template,\n    known_models: Optional[TypeModelSet] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any], Set[str]]:\n    \"\"\"\n    Process a Pydantic field and return a tuple with a JSON Schema for it as the first item.\n    Also return a dictionary of definitions with models as keys and their schemas as values. If the passed field\n    is a model and has sub-models, and those sub-models don't have overrides (as ``title``, ``default``, etc), they\n    will be included in the definitions and referenced in the schema instead of included recursively.\n\n    :param field: a Pydantic ``ModelField``\n    :param by_alias: use the defined alias (if any) in the returned schema\n    :param model_name_map: used to generate the JSON Schema references to other models included in the definitions\n    :param ref_prefix: the JSON Pointer prefix to use for references to other schemas, if None, the default of\n      #/definitions/ will be used\n    :param ref_template: Use a ``string.format()`` template for ``$ref`` instead of a prefix. This can be useful for\n      references that cannot be represented by ``ref_prefix`` such as a definition stored in another file. For a\n      sibling json file in a ``/schemas`` directory use ``\"/schemas/${model}.json#\"``.\n    :param known_models: used to solve circular references\n    :return: tuple of the schema for this field and additional definitions\n    \"\"\"\n    s, schema_overrides = get_field_info_schema(field)\n\n    validation_schema = get_field_schema_validations(field)\n    if validation_schema:\n        s.update(validation_schema)\n        schema_overrides = True\n\n    f_schema, f_definitions, f_nested_models = field_type_schema(\n        field,\n        by_alias=by_alias,\n        model_name_map=model_name_map,\n        schema_overrides=schema_overrides,\n        ref_prefix=ref_prefix,\n        ref_template=ref_template,\n        known_models=known_models or set(),\n    )\n\n    # $ref will only be returned when there are no schema_overrides\n    if '$ref' in f_schema:\n        return f_schema, f_definitions, f_nested_models\n    else:\n        s.update(f_schema)\n        return s, f_definitions, f_nested_models\n\n\nnumeric_types = (int, float, Decimal)\n_str_types_attrs: Tuple[Tuple[str, Union[type, Tuple[type, ...]], str], ...] = (\n    ('max_length', numeric_types, 'maxLength'),\n    ('min_length', numeric_types, 'minLength'),\n    ('regex', str, 'pattern'),\n)\n\n_numeric_types_attrs: Tuple[Tuple[str, Union[type, Tuple[type, ...]], str], ...] = (\n    ('gt', numeric_types, 'exclusiveMinimum'),\n    ('lt', numeric_types, 'exclusiveMaximum'),\n    ('ge', numeric_types, 'minimum'),\n    ('le', numeric_types, 'maximum'),\n    ('multiple_of', numeric_types, 'multipleOf'),\n)\n\n\ndef get_field_schema_validations(field: ModelField) -> Dict[str, Any]:\n    \"\"\"\n    Get the JSON Schema validation keywords for a ``field`` with an annotation of\n    a Pydantic ``FieldInfo`` with validation arguments.\n    \"\"\"\n    f_schema: Dict[str, Any] = {}\n\n    if lenient_issubclass(field.type_, Enum):\n        # schema is already updated by `enum_process_schema`; just update with field extra\n        if field.field_info.extra:\n            f_schema.update(field.field_info.extra)\n        return f_schema\n\n    if lenient_issubclass(field.type_, (str, bytes)):\n        for attr_name, t, keyword in _str_types_attrs:\n            attr = getattr(field.field_info, attr_name, None)\n            if isinstance(attr, t):\n                f_schema[keyword] = attr\n    if lenient_issubclass(field.type_, numeric_types) and not issubclass(field.type_, bool):\n        for attr_name, t, keyword in _numeric_types_attrs:\n            attr = getattr(field.field_info, attr_name, None)\n            if isinstance(attr, t):\n                f_schema[keyword] = attr\n    if field.field_info is not None and field.field_info.const:\n        f_schema['const'] = field.default\n    if field.field_info.extra:\n        f_schema.update(field.field_info.extra)\n    modify_schema = getattr(field.outer_type_, '__modify_schema__', None)\n    if modify_schema:\n        _apply_modify_schema(modify_schema, field, f_schema)\n    return f_schema\n\n\ndef get_model_name_map(unique_models: TypeModelSet) -> Dict[TypeModelOrEnum, str]:\n    \"\"\"\n    Process a set of models and generate unique names for them to be used as keys in the JSON Schema\n    definitions. By default the names are the same as the class name. But if two models in different Python\n    modules have the same name (e.g. \"users.Model\" and \"items.Model\"), the generated names will be\n    based on the Python module path for those conflicting models to prevent name collisions.\n\n    :param unique_models: a Python set of models\n    :return: dict mapping models to names\n    \"\"\"\n    name_model_map = {}\n    conflicting_names: Set[str] = set()\n    for model in unique_models:\n        model_name = normalize_name(model.__name__)\n        if model_name in conflicting_names:\n            model_name = get_long_model_name(model)\n            name_model_map[model_name] = model\n        elif model_name in name_model_map:\n            conflicting_names.add(model_name)\n            conflicting_model = name_model_map.pop(model_name)\n            name_model_map[get_long_model_name(conflicting_model)] = conflicting_model\n            name_model_map[get_long_model_name(model)] = model\n        else:\n            name_model_map[model_name] = model\n    return {v: k for k, v in name_model_map.items()}\n\n\ndef get_flat_models_from_model(model: Type['BaseModel'], known_models: Optional[TypeModelSet] = None) -> TypeModelSet:\n    \"\"\"\n    Take a single ``model`` and generate a set with itself and all the sub-models in the tree. I.e. if you pass\n    model ``Foo`` (subclass of Pydantic ``BaseModel``) as ``model``, and it has a field of type ``Bar`` (also\n    subclass of ``BaseModel``) and that model ``Bar`` has a field of type ``Baz`` (also subclass of ``BaseModel``),\n    the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param model: a Pydantic ``BaseModel`` subclass\n    :param known_models: used to solve circular references\n    :return: a set with the initial model and all its sub-models\n    \"\"\"\n    known_models = known_models or set()\n    flat_models: TypeModelSet = set()\n    flat_models.add(model)\n    known_models |= flat_models\n    fields = cast(Sequence[ModelField], model.__fields__.values())\n    flat_models |= get_flat_models_from_fields(fields, known_models=known_models)\n    return flat_models\n\n\ndef get_flat_models_from_field(field: ModelField, known_models: TypeModelSet) -> TypeModelSet:\n    \"\"\"\n    Take a single Pydantic ``ModelField`` (from a model) that could have been declared as a subclass of BaseModel\n    (so, it could be a submodel), and generate a set with its model and all the sub-models in the tree.\n    I.e. if you pass a field that was declared to be of type ``Foo`` (subclass of BaseModel) as ``field``, and that\n    model ``Foo`` has a field of type ``Bar`` (also subclass of ``BaseModel``) and that model ``Bar`` has a field of\n    type ``Baz`` (also subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param field: a Pydantic ``ModelField``\n    :param known_models: used to solve circular references\n    :return: a set with the model used in the declaration for this field, if any, and all its sub-models\n    \"\"\"\n    from pydantic.v1.main import BaseModel\n\n    flat_models: TypeModelSet = set()\n\n    field_type = field.type_\n    if lenient_issubclass(getattr(field_type, '__pydantic_model__', None), BaseModel):\n        field_type = field_type.__pydantic_model__\n\n    if field.sub_fields and not lenient_issubclass(field_type, BaseModel):\n        flat_models |= get_flat_models_from_fields(field.sub_fields, known_models=known_models)\n    elif lenient_issubclass(field_type, BaseModel) and field_type not in known_models:\n        flat_models |= get_flat_models_from_model(field_type, known_models=known_models)\n    elif lenient_issubclass(field_type, Enum):\n        flat_models.add(field_type)\n    return flat_models\n\n\ndef get_flat_models_from_fields(fields: Sequence[ModelField], known_models: TypeModelSet) -> TypeModelSet:\n    \"\"\"\n    Take a list of Pydantic  ``ModelField``s (from a model) that could have been declared as subclasses of ``BaseModel``\n    (so, any of them could be a submodel), and generate a set with their models and all the sub-models in the tree.\n    I.e. if you pass a the fields of a model ``Foo`` (subclass of ``BaseModel``) as ``fields``, and on of them has a\n    field of type ``Bar`` (also subclass of ``BaseModel``) and that model ``Bar`` has a field of type ``Baz`` (also\n    subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param fields: a list of Pydantic ``ModelField``s\n    :param known_models: used to solve circular references\n    :return: a set with any model declared in the fields, and all their sub-models\n    \"\"\"\n    flat_models: TypeModelSet = set()\n    for field in fields:\n        flat_models |= get_flat_models_from_field(field, known_models=known_models)\n    return flat_models\n\n\ndef get_flat_models_from_models(models: Sequence[Type['BaseModel']]) -> TypeModelSet:\n    \"\"\"\n    Take a list of ``models`` and generate a set with them and all their sub-models in their trees. I.e. if you pass\n    a list of two models, ``Foo`` and ``Bar``, both subclasses of Pydantic ``BaseModel`` as models, and ``Bar`` has\n    a field of type ``Baz`` (also subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n    \"\"\"\n    flat_models: TypeModelSet = set()\n    for model in models:\n        flat_models |= get_flat_models_from_model(model)\n    return flat_models\n\n\ndef get_long_model_name(model: TypeModelOrEnum) -> str:\n    return f'{model.__module__}__{model.__qualname__}'.replace('.', '__')\n\n\ndef field_type_schema(\n    field: ModelField,\n    *,\n    by_alias: bool,\n    model_name_map: Dict[TypeModelOrEnum, str],\n    ref_template: str,\n    schema_overrides: bool = False,\n    ref_prefix: Optional[str] = None,\n    known_models: TypeModelSet,\n) -> Tuple[Dict[str, Any], Dict[str, Any], Set[str]]:\n    \"\"\"\n    Used by ``field_schema()``, you probably should be using that function.\n\n    Take a single ``field`` and generate the schema for its type only, not including additional\n    information as title, etc. Also return additional schema definitions, from sub-models.\n    \"\"\"\n    from pydantic.v1.main import BaseModel  # noqa: F811\n\n    definitions = {}\n    nested_models: Set[str] = set()\n    f_schema: Dict[str, Any]\n    if field.shape in {\n        SHAPE_LIST,\n        SHAPE_TUPLE_ELLIPSIS,\n        SHAPE_SEQUENCE,\n        SHAPE_SET,\n        SHAPE_FROZENSET,\n        SHAPE_ITERABLE,\n        SHAPE_DEQUE,\n    }:\n        items_schema, f_definitions, f_nested_models = field_singleton_schema(\n            field,\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            ref_prefix=ref_prefix,\n            ref_template=ref_template,\n            known_models=known_models,\n        )\n        definitions.update(f_definitions)\n        nested_models.update(f_nested_models)\n        f_schema = {'type': 'array', 'items': items_schema}\n        if field.shape in {SHAPE_SET, SHAPE_FROZENSET}:\n            f_schema['uniqueItems'] = True\n\n    elif field.shape in MAPPING_LIKE_SHAPES:\n        f_schema = {'type': 'object'}\n        key_field = cast(ModelField, field.key_field)\n        regex = getattr(key_field.type_, 'regex', None)\n        items_schema, f_definitions, f_nested_models = field_singleton_schema(\n            field,\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            ref_prefix=ref_prefix,\n            ref_template=ref_template,\n            known_models=known_models,\n        )\n        definitions.update(f_definitions)\n        nested_models.update(f_nested_models)\n        if regex:\n            # Dict keys have a regex pattern\n            # items_schema might be a schema or empty dict, add it either way\n            f_schema['patternProperties'] = {ConstrainedStr._get_pattern(regex): items_schema}\n        if items_schema:\n            # The dict values are not simply Any, so they need a schema\n            f_schema['additionalProperties'] = items_schema\n    elif field.shape == SHAPE_TUPLE or (field.shape == SHAPE_GENERIC and not issubclass(field.type_, BaseModel)):\n        sub_schema = []\n        sub_fields = cast(List[ModelField], field.sub_fields)\n        for sf in sub_fields:\n            sf_schema, sf_definitions, sf_nested_models = field_type_schema(\n                sf,\n                by_alias=by_alias,\n                model_name_map=model_name_map,\n                ref_prefix=ref_prefix,\n                ref_template=ref_template,\n                known_models=known_models,\n            )\n            definitions.update(sf_definitions)\n            nested_models.update(sf_nested_models)\n            sub_schema.append(sf_schema)\n\n        sub_fields_len = len(sub_fields)\n        if field.shape == SHAPE_GENERIC:\n            all_of_schemas = sub_schema[0] if sub_fields_len == 1 else {'type': 'array', 'items': sub_schema}\n            f_schema = {'allOf': [all_of_schemas]}\n        else:\n            f_schema = {\n                'type': 'array',\n                'minItems': sub_fields_len,\n                'maxItems': sub_fields_len,\n            }\n            if sub_fields_len >= 1:\n                f_schema['items'] = sub_schema\n    else:\n        assert field.shape in {SHAPE_SINGLETON, SHAPE_GENERIC}, field.shape\n        f_schema, f_definitions, f_nested_models = field_singleton_schema(\n            field,\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            schema_overrides=schema_overrides,\n            ref_prefix=ref_prefix,\n            ref_template=ref_template,\n            known_models=known_models,\n        )\n        definitions.update(f_definitions)\n        nested_models.update(f_nested_models)\n\n    # check field type to avoid repeated calls to the same __modify_schema__ method\n    if field.type_ != field.outer_type_:\n        if field.shape == SHAPE_GENERIC:\n            field_type = field.type_\n        else:\n            field_type = field.outer_type_\n        modify_schema = getattr(field_type, '__modify_schema__', None)\n        if modify_schema:\n            _apply_modify_schema(modify_schema, field, f_schema)\n    return f_schema, definitions, nested_models\n\n\ndef model_process_schema(\n    model: TypeModelOrEnum,\n    *,\n    by_alias: bool = True,\n    model_name_map: Dict[TypeModelOrEnum, str],\n    ref_prefix: Optional[str] = None,\n    ref_template: str = default_ref_template,\n    known_models: Optional[TypeModelSet] = None,\n    field: Optional[ModelField] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any], Set[str]]:\n    \"\"\"\n    Used by ``model_schema()``, you probably should be using that function.\n\n    Take a single ``model`` and generate its schema. Also return additional schema definitions, from sub-models. The\n    sub-models of the returned schema will be referenced, but their definitions will not be included in the schema. All\n    the definitions are returned as the second value.\n    \"\"\"\n    from inspect import getdoc, signature\n\n    known_models = known_models or set()\n    if lenient_issubclass(model, Enum):\n        model = cast(Type[Enum], model)\n        s = enum_process_schema(model, field=field)\n        return s, {}, set()\n    model = cast(Type['BaseModel'], model)\n    s = {'title': model.__config__.title or model.__name__}\n    doc = getdoc(model)\n    if doc:\n        s['description'] = doc\n    known_models.add(model)\n    m_schema, m_definitions, nested_models = model_type_schema(\n        model,\n        by_alias=by_alias,\n        model_name_map=model_name_map,\n        ref_prefix=ref_prefix,\n        ref_template=ref_template,\n        known_models=known_models,\n    )\n    s.update(m_schema)\n    schema_extra = model.__config__.schema_extra\n    if callable(schema_extra):\n        if len(signature(schema_extra).parameters) == 1:\n            schema_extra(s)\n        else:\n            schema_extra(s, model)\n    else:\n        s.update(schema_extra)\n    return s, m_definitions, nested_models\n\n\ndef model_type_schema(\n    model: Type['BaseModel'],\n    *,\n    by_alias: bool,\n    model_name_map: Dict[TypeModelOrEnum, str],\n    ref_template: str,\n    ref_prefix: Optional[str] = None,\n    known_models: TypeModelSet,\n) -> Tuple[Dict[str, Any], Dict[str, Any], Set[str]]:\n    \"\"\"\n    You probably should be using ``model_schema()``, this function is indirectly used by that function.\n\n    Take a single ``model`` and generate the schema for its type only, not including additional\n    information as title, etc. Also return additional schema definitions, from sub-models.\n    \"\"\"\n    properties = {}\n    required = []\n    definitions: Dict[str, Any] = {}\n    nested_models: Set[str] = set()\n    for k, f in model.__fields__.items():\n        try:\n            f_schema, f_definitions, f_nested_models = field_schema(\n                f,\n                by_alias=by_alias,\n                model_name_map=model_name_map,\n                ref_prefix=ref_prefix,\n                ref_template=ref_template,\n                known_models=known_models,\n            )\n        except SkipField as skip:\n            warnings.warn(skip.message, UserWarning)\n            continue\n        definitions.update(f_definitions)\n        nested_models.update(f_nested_models)\n        if by_alias:\n            properties[f.alias] = f_schema\n            if f.required:\n                required.append(f.alias)\n        else:\n            properties[k] = f_schema\n            if f.required:\n                required.append(k)\n    if ROOT_KEY in properties:\n        out_schema = properties[ROOT_KEY]\n        out_schema['title'] = model.__config__.title or model.__name__\n    else:\n        out_schema = {'type': 'object', 'properties': properties}\n        if required:\n            out_schema['required'] = required\n    if model.__config__.extra == 'forbid':\n        out_schema['additionalProperties'] = False\n    return out_schema, definitions, nested_models\n\n\ndef enum_process_schema(enum: Type[Enum], *, field: Optional[ModelField] = None) -> Dict[str, Any]:\n    \"\"\"\n    Take a single `enum` and generate its schema.\n\n    This is similar to the `model_process_schema` function, but applies to ``Enum`` objects.\n    \"\"\"\n    import inspect\n\n    schema_: Dict[str, Any] = {\n        'title': enum.__name__,\n        # Python assigns all enums a default docstring value of 'An enumeration', so\n        # all enums will have a description field even if not explicitly provided.\n        'description': inspect.cleandoc(enum.__doc__ or 'An enumeration.'),\n        # Add enum values and the enum field type to the schema.\n        'enum': [item.value for item in cast(Iterable[Enum], enum)],\n    }\n\n    add_field_type_to_schema(enum, schema_)\n\n    modify_schema = getattr(enum, '__modify_schema__', None)\n    if modify_schema:\n        _apply_modify_schema(modify_schema, field, schema_)\n\n    return schema_\n\n\ndef field_singleton_sub_fields_schema(\n    field: ModelField,\n    *,\n    by_alias: bool,\n    model_name_map: Dict[TypeModelOrEnum, str],\n    ref_template: str,\n    schema_overrides: bool = False,\n    ref_prefix: Optional[str] = None,\n    known_models: TypeModelSet,\n) -> Tuple[Dict[str, Any], Dict[str, Any], Set[str]]:\n    \"\"\"\n    This function is indirectly used by ``field_schema()``, you probably should be using that function.\n\n    Take a list of Pydantic ``ModelField`` from the declaration of a type with parameters, and generate their\n    schema. I.e., fields used as \"type parameters\", like ``str`` and ``int`` in ``Tuple[str, int]``.\n    \"\"\"\n    sub_fields = cast(List[ModelField], field.sub_fields)\n    definitions = {}\n    nested_models: Set[str] = set()\n    if len(sub_fields) == 1:\n        return field_type_schema(\n            sub_fields[0],\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            schema_overrides=schema_overrides,\n            ref_prefix=ref_prefix,\n            ref_template=ref_template,\n            known_models=known_models,\n        )\n    else:\n        s: Dict[str, Any] = {}\n        # https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#discriminator-object\n        field_has_discriminator: bool = field.discriminator_key is not None\n        if field_has_discriminator:\n            assert field.sub_fields_mapping is not None\n\n            discriminator_models_refs: Dict[str, Union[str, Dict[str, Any]]] = {}\n\n            for discriminator_value, sub_field in field.sub_fields_mapping.items():\n                if isinstance(discriminator_value, Enum):\n                    discriminator_value = str(discriminator_value.value)\n                # sub_field is either a `BaseModel` or directly an `Annotated` `Union` of many\n                if is_union(get_origin(sub_field.type_)):\n                    sub_models = get_sub_types(sub_field.type_)\n                    discriminator_models_refs[discriminator_value] = {\n                        model_name_map[sub_model]: get_schema_ref(\n                            model_name_map[sub_model], ref_prefix, ref_template, False\n                        )\n                        for sub_model in sub_models\n                    }\n                else:\n                    sub_field_type = sub_field.type_\n                    if hasattr(sub_field_type, '__pydantic_model__'):\n                        sub_field_type = sub_field_type.__pydantic_model__\n\n                    discriminator_model_name = model_name_map[sub_field_type]\n                    discriminator_model_ref = get_schema_ref(discriminator_model_name, ref_prefix, ref_template, False)\n                    discriminator_models_refs[discriminator_value] = discriminator_model_ref['$ref']\n\n            s['discriminator'] = {\n                'propertyName': field.discriminator_alias,\n                'mapping': discriminator_models_refs,\n            }\n\n        sub_field_schemas = []\n        for sf in sub_fields:\n            sub_schema, sub_definitions, sub_nested_models = field_type_schema(\n                sf,\n                by_alias=by_alias,\n                model_name_map=model_name_map,\n                schema_overrides=schema_overrides,\n                ref_prefix=ref_prefix,\n                ref_template=ref_template,\n                known_models=known_models,\n            )\n            definitions.update(sub_definitions)\n            if schema_overrides and 'allOf' in sub_schema:\n                # if the sub_field is a referenced schema we only need the referenced\n                # object. Otherwise we will end up with several allOf inside anyOf/oneOf.\n                # See https://github.com/pydantic/pydantic/issues/1209\n                sub_schema = sub_schema['allOf'][0]\n\n            if sub_schema.keys() == {'discriminator', 'oneOf'}:\n                # we don't want discriminator information inside oneOf choices, this is dealt with elsewhere\n                sub_schema.pop('discriminator')\n            sub_field_schemas.append(sub_schema)\n            nested_models.update(sub_nested_models)\n        s['oneOf' if field_has_discriminator else 'anyOf'] = sub_field_schemas\n        return s, definitions, nested_models\n\n\n# Order is important, e.g. subclasses of str must go before str\n# this is used only for standard library types, custom types should use __modify_schema__ instead\nfield_class_to_schema: Tuple[Tuple[Any, Dict[str, Any]], ...] = (\n    (Path, {'type': 'string', 'format': 'path'}),\n    (datetime, {'type': 'string', 'format': 'date-time'}),\n    (date, {'type': 'string', 'format': 'date'}),\n    (time, {'type': 'string', 'format': 'time'}),\n    (timedelta, {'type': 'number', 'format': 'time-delta'}),\n    (IPv4Network, {'type': 'string', 'format': 'ipv4network'}),\n    (IPv6Network, {'type': 'string', 'format': 'ipv6network'}),\n    (IPv4Interface, {'type': 'string', 'format': 'ipv4interface'}),\n    (IPv6Interface, {'type': 'string', 'format': 'ipv6interface'}),\n    (IPv4Address, {'type': 'string', 'format': 'ipv4'}),\n    (IPv6Address, {'type': 'string', 'format': 'ipv6'}),\n    (Pattern, {'type': 'string', 'format': 'regex'}),\n    (str, {'type': 'string'}),\n    (bytes, {'type': 'string', 'format': 'binary'}),\n    (bool, {'type': 'boolean'}),\n    (int, {'type': 'integer'}),\n    (float, {'type': 'number'}),\n    (Decimal, {'type': 'number'}),\n    (UUID, {'type': 'string', 'format': 'uuid'}),\n    (dict, {'type': 'object'}),\n    (list, {'type': 'array', 'items': {}}),\n    (tuple, {'type': 'array', 'items': {}}),\n    (set, {'type': 'array', 'items': {}, 'uniqueItems': True}),\n    (frozenset, {'type': 'array', 'items': {}, 'uniqueItems': True}),\n)\n\njson_scheme = {'type': 'string', 'format': 'json-string'}\n\n\ndef add_field_type_to_schema(field_type: Any, schema_: Dict[str, Any]) -> None:\n    \"\"\"\n    Update the given `schema` with the type-specific metadata for the given `field_type`.\n\n    This function looks through `field_class_to_schema` for a class that matches the given `field_type`,\n    and then modifies the given `schema` with the information from that type.\n    \"\"\"\n    for type_, t_schema in field_class_to_schema:\n        # Fallback for `typing.Pattern` and `re.Pattern` as they are not a valid class\n        if lenient_issubclass(field_type, type_) or field_type is type_ is Pattern:\n            schema_.update(t_schema)\n            break\n\n\ndef get_schema_ref(name: str, ref_prefix: Optional[str], ref_template: str, schema_overrides: bool) -> Dict[str, Any]:\n    if ref_prefix:\n        schema_ref = {'$ref': ref_prefix + name}\n    else:\n        schema_ref = {'$ref': ref_template.format(model=name)}\n    return {'allOf': [schema_ref]} if schema_overrides else schema_ref\n\n\ndef field_singleton_schema(  # noqa: C901 (ignore complexity)\n    field: ModelField,\n    *,\n    by_alias: bool,\n    model_name_map: Dict[TypeModelOrEnum, str],\n    ref_template: str,\n    schema_overrides: bool = False,\n    ref_prefix: Optional[str] = None,\n    known_models: TypeModelSet,\n) -> Tuple[Dict[str, Any], Dict[str, Any], Set[str]]:\n    \"\"\"\n    This function is indirectly used by ``field_schema()``, you should probably be using that function.\n\n    Take a single Pydantic ``ModelField``, and return its schema and any additional definitions from sub-models.\n    \"\"\"\n    from pydantic.v1.main import BaseModel\n\n    definitions: Dict[str, Any] = {}\n    nested_models: Set[str] = set()\n    field_type = field.type_\n\n    # Recurse into this field if it contains sub_fields and is NOT a\n    # BaseModel OR that BaseModel is a const\n    if field.sub_fields and (\n        (field.field_info and field.field_info.const) or not lenient_issubclass(field_type, BaseModel)\n    ):\n        return field_singleton_sub_fields_schema(\n            field,\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            schema_overrides=schema_overrides,\n            ref_prefix=ref_prefix,\n            ref_template=ref_template,\n            known_models=known_models,\n        )\n    if field_type is Any or field_type is object or field_type.__class__ == TypeVar or get_origin(field_type) is type:\n        return {}, definitions, nested_models  # no restrictions\n    if is_none_type(field_type):\n        return {'type': 'null'}, definitions, nested_models\n    if is_callable_type(field_type):\n        raise SkipField(f'Callable {field.name} was excluded from schema since JSON schema has no equivalent type.')\n    f_schema: Dict[str, Any] = {}\n    if field.field_info is not None and field.field_info.const:\n        f_schema['const'] = field.default\n\n    if is_literal_type(field_type):\n        values = tuple(x.value if isinstance(x, Enum) else x for x in all_literal_values(field_type))\n\n        if len({v.__class__ for v in values}) > 1:\n            return field_schema(\n                multitypes_literal_field_for_schema(values, field),\n                by_alias=by_alias,\n                model_name_map=model_name_map,\n                ref_prefix=ref_prefix,\n                ref_template=ref_template,\n                known_models=known_models,\n            )\n\n        # All values have the same type\n        field_type = values[0].__class__\n        f_schema['enum'] = list(values)\n        add_field_type_to_schema(field_type, f_schema)\n    elif lenient_issubclass(field_type, Enum):\n        enum_name = model_name_map[field_type]\n        f_schema, schema_overrides = get_field_info_schema(field, schema_overrides)\n        f_schema.update(get_schema_ref(enum_name, ref_prefix, ref_template, schema_overrides))\n        definitions[enum_name] = enum_process_schema(field_type, field=field)\n    elif is_namedtuple(field_type):\n        sub_schema, *_ = model_process_schema(\n            field_type.__pydantic_model__,\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            ref_prefix=ref_prefix,\n            ref_template=ref_template,\n            known_models=known_models,\n            field=field,\n        )\n        items_schemas = list(sub_schema['properties'].values())\n        f_schema.update(\n            {\n                'type': 'array',\n                'items': items_schemas,\n                'minItems': len(items_schemas),\n                'maxItems': len(items_schemas),\n            }\n        )\n    elif not hasattr(field_type, '__pydantic_model__'):\n        add_field_type_to_schema(field_type, f_schema)\n\n        modify_schema = getattr(field_type, '__modify_schema__', None)\n        if modify_schema:\n            _apply_modify_schema(modify_schema, field, f_schema)\n\n    if f_schema:\n        return f_schema, definitions, nested_models\n\n    # Handle dataclass-based models\n    if lenient_issubclass(getattr(field_type, '__pydantic_model__', None), BaseModel):\n        field_type = field_type.__pydantic_model__\n\n    if issubclass(field_type, BaseModel):\n        model_name = model_name_map[field_type]\n        if field_type not in known_models:\n            sub_schema, sub_definitions, sub_nested_models = model_process_schema(\n                field_type,\n                by_alias=by_alias,\n                model_name_map=model_name_map,\n                ref_prefix=ref_prefix,\n                ref_template=ref_template,\n                known_models=known_models,\n                field=field,\n            )\n            definitions.update(sub_definitions)\n            definitions[model_name] = sub_schema\n            nested_models.update(sub_nested_models)\n        else:\n            nested_models.add(model_name)\n        schema_ref = get_schema_ref(model_name, ref_prefix, ref_template, schema_overrides)\n        return schema_ref, definitions, nested_models\n\n    # For generics with no args\n    args = get_args(field_type)\n    if args is not None and not args and Generic in field_type.__bases__:\n        return f_schema, definitions, nested_models\n\n    raise ValueError(f'Value not declarable with JSON Schema, field: {field}')\n\n\ndef multitypes_literal_field_for_schema(values: Tuple[Any, ...], field: ModelField) -> ModelField:\n    \"\"\"\n    To support `Literal` with values of different types, we split it into multiple `Literal` with same type\n    e.g. `Literal['qwe', 'asd', 1, 2]` becomes `Union[Literal['qwe', 'asd'], Literal[1, 2]]`\n    \"\"\"\n    literal_distinct_types = defaultdict(list)\n    for v in values:\n        literal_distinct_types[v.__class__].append(v)\n    distinct_literals = (Literal[tuple(same_type_values)] for same_type_values in literal_distinct_types.values())\n\n    return ModelField(\n        name=field.name,\n        type_=Union[tuple(distinct_literals)],  # type: ignore\n        class_validators=field.class_validators,\n        model_config=field.model_config,\n        default=field.default,\n        required=field.required,\n        alias=field.alias,\n        field_info=field.field_info,\n    )\n\n\ndef encode_default(dft: Any) -> Any:\n    from pydantic.v1.main import BaseModel\n\n    if isinstance(dft, BaseModel) or is_dataclass(dft):\n        dft = cast('dict[str, Any]', pydantic_encoder(dft))\n\n    if isinstance(dft, dict):\n        return {encode_default(k): encode_default(v) for k, v in dft.items()}\n    elif isinstance(dft, Enum):\n        return dft.value\n    elif isinstance(dft, (int, float, str)):\n        return dft\n    elif isinstance(dft, (list, tuple)):\n        t = dft.__class__\n        seq_args = (encode_default(v) for v in dft)\n        return t(*seq_args) if is_namedtuple(t) else t(seq_args)\n    elif dft is None:\n        return None\n    else:\n        return pydantic_encoder(dft)\n\n\n_map_types_constraint: Dict[Any, Callable[..., type]] = {int: conint, float: confloat, Decimal: condecimal}\n\n\ndef get_annotation_from_field_info(\n    annotation: Any, field_info: FieldInfo, field_name: str, validate_assignment: bool = False\n) -> Type[Any]:\n    \"\"\"\n    Get an annotation with validation implemented for numbers and strings based on the field_info.\n    :param annotation: an annotation from a field specification, as ``str``, ``ConstrainedStr``\n    :param field_info: an instance of FieldInfo, possibly with declarations for validations and JSON Schema\n    :param field_name: name of the field for use in error messages\n    :param validate_assignment: default False, flag for BaseModel Config value of validate_assignment\n    :return: the same ``annotation`` if unmodified or a new annotation with validation in place\n    \"\"\"\n    constraints = field_info.get_constraints()\n    used_constraints: Set[str] = set()\n    if constraints:\n        annotation, used_constraints = get_annotation_with_constraints(annotation, field_info)\n    if validate_assignment:\n        used_constraints.add('allow_mutation')\n\n    unused_constraints = constraints - used_constraints\n    if unused_constraints:\n        raise ValueError(\n            f'On field \"{field_name}\" the following field constraints are set but not enforced: '\n            f'{\", \".join(unused_constraints)}. '\n            f'\\nFor more details see https://docs.pydantic.dev/usage/schema/#unenforced-field-constraints'\n        )\n\n    return annotation\n\n\ndef get_annotation_with_constraints(annotation: Any, field_info: FieldInfo) -> Tuple[Type[Any], Set[str]]:  # noqa: C901\n    \"\"\"\n    Get an annotation with used constraints implemented for numbers and strings based on the field_info.\n\n    :param annotation: an annotation from a field specification, as ``str``, ``ConstrainedStr``\n    :param field_info: an instance of FieldInfo, possibly with declarations for validations and JSON Schema\n    :return: the same ``annotation`` if unmodified or a new annotation along with the used constraints.\n    \"\"\"\n    used_constraints: Set[str] = set()\n\n    def go(type_: Any) -> Type[Any]:\n        if (\n            is_literal_type(type_)\n            or isinstance(type_, ForwardRef)\n            or lenient_issubclass(type_, (ConstrainedList, ConstrainedSet, ConstrainedFrozenSet))\n        ):\n            return type_\n        origin = get_origin(type_)\n        if origin is not None:\n            args: Tuple[Any, ...] = get_args(type_)\n            if any(isinstance(a, ForwardRef) for a in args):\n                # forward refs cause infinite recursion below\n                return type_\n\n            if origin is Annotated:\n                return go(args[0])\n            if is_union(origin):\n                return Union[tuple(go(a) for a in args)]  # type: ignore\n\n            if issubclass(origin, List) and (\n                field_info.min_items is not None\n                or field_info.max_items is not None\n                or field_info.unique_items is not None\n            ):\n                used_constraints.update({'min_items', 'max_items', 'unique_items'})\n                return conlist(\n                    go(args[0]),\n                    min_items=field_info.min_items,\n                    max_items=field_info.max_items,\n                    unique_items=field_info.unique_items,\n                )\n\n            if issubclass(origin, Set) and (field_info.min_items is not None or field_info.max_items is not None):\n                used_constraints.update({'min_items', 'max_items'})\n                return conset(go(args[0]), min_items=field_info.min_items, max_items=field_info.max_items)\n\n            if issubclass(origin, FrozenSet) and (field_info.min_items is not None or field_info.max_items is not None):\n                used_constraints.update({'min_items', 'max_items'})\n                return confrozenset(go(args[0]), min_items=field_info.min_items, max_items=field_info.max_items)\n\n            for t in (Tuple, List, Set, FrozenSet, Sequence):\n                if issubclass(origin, t):  # type: ignore\n                    return t[tuple(go(a) for a in args)]  # type: ignore\n\n            if issubclass(origin, Dict):\n                return Dict[args[0], go(args[1])]  # type: ignore\n\n        attrs: Optional[Tuple[str, ...]] = None\n        constraint_func: Optional[Callable[..., type]] = None\n        if isinstance(type_, type):\n            if issubclass(type_, (SecretStr, SecretBytes)):\n                attrs = ('max_length', 'min_length')\n\n                def constraint_func(**kw: Any) -> Type[Any]:  # noqa: F811\n                    return type(type_.__name__, (type_,), kw)\n\n            elif issubclass(type_, str) and not issubclass(type_, (EmailStr, AnyUrl)):\n                attrs = ('max_length', 'min_length', 'regex')\n                if issubclass(type_, StrictStr):\n\n                    def constraint_func(**kw: Any) -> Type[Any]:\n                        return type(type_.__name__, (type_,), kw)\n\n                else:\n                    constraint_func = constr\n            elif issubclass(type_, bytes):\n                attrs = ('max_length', 'min_length', 'regex')\n                if issubclass(type_, StrictBytes):\n\n                    def constraint_func(**kw: Any) -> Type[Any]:\n                        return type(type_.__name__, (type_,), kw)\n\n                else:\n                    constraint_func = conbytes\n            elif issubclass(type_, numeric_types) and not issubclass(\n                type_,\n                (\n                    ConstrainedInt,\n                    ConstrainedFloat,\n                    ConstrainedDecimal,\n                    ConstrainedList,\n                    ConstrainedSet,\n                    ConstrainedFrozenSet,\n                    bool,\n                ),\n            ):\n                # Is numeric type\n                attrs = ('gt', 'lt', 'ge', 'le', 'multiple_of')\n                if issubclass(type_, float):\n                    attrs += ('allow_inf_nan',)\n                if issubclass(type_, Decimal):\n                    attrs += ('max_digits', 'decimal_places')\n                numeric_type = next(t for t in numeric_types if issubclass(type_, t))  # pragma: no branch\n                constraint_func = _map_types_constraint[numeric_type]\n\n        if attrs:\n            used_constraints.update(set(attrs))\n            kwargs = {\n                attr_name: attr\n                for attr_name, attr in ((attr_name, getattr(field_info, attr_name)) for attr_name in attrs)\n                if attr is not None\n            }\n            if kwargs:\n                constraint_func = cast(Callable[..., type], constraint_func)\n                return constraint_func(**kwargs)\n        return type_\n\n    return go(annotation), used_constraints\n\n\ndef normalize_name(name: str) -> str:\n    \"\"\"\n    Normalizes the given name. This can be applied to either a model *or* enum.\n    \"\"\"\n    return re.sub(r'[^a-zA-Z0-9.\\-_]', '_', name)\n\n\nclass SkipField(Exception):\n    \"\"\"\n    Utility exception used to exclude fields from schema.\n    \"\"\"\n\n    def __init__(self, message: str) -> None:\n        self.message = message\n", "pydantic/v1/utils.py": "import keyword\nimport warnings\nimport weakref\nfrom collections import OrderedDict, defaultdict, deque\nfrom copy import deepcopy\nfrom itertools import islice, zip_longest\nfrom types import BuiltinFunctionType, CodeType, FunctionType, GeneratorType, LambdaType, ModuleType\nfrom typing import (\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    Callable,\n    Collection,\n    Dict,\n    Generator,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    NoReturn,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nfrom typing_extensions import Annotated\n\nfrom pydantic.v1.errors import ConfigError\nfrom pydantic.v1.typing import (\n    NoneType,\n    WithArgsTypes,\n    all_literal_values,\n    display_as_type,\n    get_args,\n    get_origin,\n    is_literal_type,\n    is_union,\n)\nfrom pydantic.v1.version import version_info\n\nif TYPE_CHECKING:\n    from inspect import Signature\n    from pathlib import Path\n\n    from pydantic.v1.config import BaseConfig\n    from pydantic.v1.dataclasses import Dataclass\n    from pydantic.v1.fields import ModelField\n    from pydantic.v1.main import BaseModel\n    from pydantic.v1.typing import AbstractSetIntStr, DictIntStrAny, IntStr, MappingIntStrAny, ReprArgs\n\n    RichReprResult = Iterable[Union[Any, Tuple[Any], Tuple[str, Any], Tuple[str, Any, Any]]]\n\n__all__ = (\n    'import_string',\n    'sequence_like',\n    'validate_field_name',\n    'lenient_isinstance',\n    'lenient_issubclass',\n    'in_ipython',\n    'is_valid_identifier',\n    'deep_update',\n    'update_not_none',\n    'almost_equal_floats',\n    'get_model',\n    'to_camel',\n    'is_valid_field',\n    'smart_deepcopy',\n    'PyObjectStr',\n    'Representation',\n    'GetterDict',\n    'ValueItems',\n    'version_info',  # required here to match behaviour in v1.3\n    'ClassAttribute',\n    'path_type',\n    'ROOT_KEY',\n    'get_unique_discriminator_alias',\n    'get_discriminator_alias_and_values',\n    'DUNDER_ATTRIBUTES',\n)\n\nROOT_KEY = '__root__'\n# these are types that are returned unchanged by deepcopy\nIMMUTABLE_NON_COLLECTIONS_TYPES: Set[Type[Any]] = {\n    int,\n    float,\n    complex,\n    str,\n    bool,\n    bytes,\n    type,\n    NoneType,\n    FunctionType,\n    BuiltinFunctionType,\n    LambdaType,\n    weakref.ref,\n    CodeType,\n    # note: including ModuleType will differ from behaviour of deepcopy by not producing error.\n    # It might be not a good idea in general, but considering that this function used only internally\n    # against default values of fields, this will allow to actually have a field with module as default value\n    ModuleType,\n    NotImplemented.__class__,\n    Ellipsis.__class__,\n}\n\n# these are types that if empty, might be copied with simple copy() instead of deepcopy()\nBUILTIN_COLLECTIONS: Set[Type[Any]] = {\n    list,\n    set,\n    tuple,\n    frozenset,\n    dict,\n    OrderedDict,\n    defaultdict,\n    deque,\n}\n\n\ndef import_string(dotted_path: str) -> Any:\n    \"\"\"\n    Stolen approximately from django. Import a dotted module path and return the attribute/class designated by the\n    last name in the path. Raise ImportError if the import fails.\n    \"\"\"\n    from importlib import import_module\n\n    try:\n        module_path, class_name = dotted_path.strip(' ').rsplit('.', 1)\n    except ValueError as e:\n        raise ImportError(f'\"{dotted_path}\" doesn\\'t look like a module path') from e\n\n    module = import_module(module_path)\n    try:\n        return getattr(module, class_name)\n    except AttributeError as e:\n        raise ImportError(f'Module \"{module_path}\" does not define a \"{class_name}\" attribute') from e\n\n\ndef truncate(v: Union[str], *, max_len: int = 80) -> str:\n    \"\"\"\n    Truncate a value and add a unicode ellipsis (three dots) to the end if it was too long\n    \"\"\"\n    warnings.warn('`truncate` is no-longer used by pydantic and is deprecated', DeprecationWarning)\n    if isinstance(v, str) and len(v) > (max_len - 2):\n        # -3 so quote + string + \u2026 + quote has correct length\n        return (v[: (max_len - 3)] + '\u2026').__repr__()\n    try:\n        v = v.__repr__()\n    except TypeError:\n        v = v.__class__.__repr__(v)  # in case v is a type\n    if len(v) > max_len:\n        v = v[: max_len - 1] + '\u2026'\n    return v\n\n\ndef sequence_like(v: Any) -> bool:\n    return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n\n\ndef validate_field_name(bases: List[Type['BaseModel']], field_name: str) -> None:\n    \"\"\"\n    Ensure that the field's name does not shadow an existing attribute of the model.\n    \"\"\"\n    for base in bases:\n        if getattr(base, field_name, None):\n            raise NameError(\n                f'Field name \"{field_name}\" shadows a BaseModel attribute; '\n                f'use a different field name with \"alias=\\'{field_name}\\'\".'\n            )\n\n\ndef lenient_isinstance(o: Any, class_or_tuple: Union[Type[Any], Tuple[Type[Any], ...], None]) -> bool:\n    try:\n        return isinstance(o, class_or_tuple)  # type: ignore[arg-type]\n    except TypeError:\n        return False\n\n\ndef lenient_issubclass(cls: Any, class_or_tuple: Union[Type[Any], Tuple[Type[Any], ...], None]) -> bool:\n    try:\n        return isinstance(cls, type) and issubclass(cls, class_or_tuple)  # type: ignore[arg-type]\n    except TypeError:\n        if isinstance(cls, WithArgsTypes):\n            return False\n        raise  # pragma: no cover\n\n\ndef in_ipython() -> bool:\n    \"\"\"\n    Check whether we're in an ipython environment, including jupyter notebooks.\n    \"\"\"\n    try:\n        eval('__IPYTHON__')\n    except NameError:\n        return False\n    else:  # pragma: no cover\n        return True\n\n\ndef is_valid_identifier(identifier: str) -> bool:\n    \"\"\"\n    Checks that a string is a valid identifier and not a Python keyword.\n    :param identifier: The identifier to test.\n    :return: True if the identifier is valid.\n    \"\"\"\n    return identifier.isidentifier() and not keyword.iskeyword(identifier)\n\n\nKeyType = TypeVar('KeyType')\n\n\ndef deep_update(mapping: Dict[KeyType, Any], *updating_mappings: Dict[KeyType, Any]) -> Dict[KeyType, Any]:\n    updated_mapping = mapping.copy()\n    for updating_mapping in updating_mappings:\n        for k, v in updating_mapping.items():\n            if k in updated_mapping and isinstance(updated_mapping[k], dict) and isinstance(v, dict):\n                updated_mapping[k] = deep_update(updated_mapping[k], v)\n            else:\n                updated_mapping[k] = v\n    return updated_mapping\n\n\ndef update_not_none(mapping: Dict[Any, Any], **update: Any) -> None:\n    mapping.update({k: v for k, v in update.items() if v is not None})\n\n\ndef almost_equal_floats(value_1: float, value_2: float, *, delta: float = 1e-8) -> bool:\n    \"\"\"\n    Return True if two floats are almost equal\n    \"\"\"\n    return abs(value_1 - value_2) <= delta\n\n\ndef generate_model_signature(\n    init: Callable[..., None], fields: Dict[str, 'ModelField'], config: Type['BaseConfig']\n) -> 'Signature':\n    \"\"\"\n    Generate signature for model based on its fields\n    \"\"\"\n    from inspect import Parameter, Signature, signature\n\n    from pydantic.v1.config import Extra\n\n    present_params = signature(init).parameters.values()\n    merged_params: Dict[str, Parameter] = {}\n    var_kw = None\n    use_var_kw = False\n\n    for param in islice(present_params, 1, None):  # skip self arg\n        if param.kind is param.VAR_KEYWORD:\n            var_kw = param\n            continue\n        merged_params[param.name] = param\n\n    if var_kw:  # if custom init has no var_kw, fields which are not declared in it cannot be passed through\n        allow_names = config.allow_population_by_field_name\n        for field_name, field in fields.items():\n            param_name = field.alias\n            if field_name in merged_params or param_name in merged_params:\n                continue\n            elif not is_valid_identifier(param_name):\n                if allow_names and is_valid_identifier(field_name):\n                    param_name = field_name\n                else:\n                    use_var_kw = True\n                    continue\n\n            # TODO: replace annotation with actual expected types once #1055 solved\n            kwargs = {'default': field.default} if not field.required else {}\n            merged_params[param_name] = Parameter(\n                param_name, Parameter.KEYWORD_ONLY, annotation=field.annotation, **kwargs\n            )\n\n    if config.extra is Extra.allow:\n        use_var_kw = True\n\n    if var_kw and use_var_kw:\n        # Make sure the parameter for extra kwargs\n        # does not have the same name as a field\n        default_model_signature = [\n            ('__pydantic_self__', Parameter.POSITIONAL_OR_KEYWORD),\n            ('data', Parameter.VAR_KEYWORD),\n        ]\n        if [(p.name, p.kind) for p in present_params] == default_model_signature:\n            # if this is the standard model signature, use extra_data as the extra args name\n            var_kw_name = 'extra_data'\n        else:\n            # else start from var_kw\n            var_kw_name = var_kw.name\n\n        # generate a name that's definitely unique\n        while var_kw_name in fields:\n            var_kw_name += '_'\n        merged_params[var_kw_name] = var_kw.replace(name=var_kw_name)\n\n    return Signature(parameters=list(merged_params.values()), return_annotation=None)\n\n\ndef get_model(obj: Union[Type['BaseModel'], Type['Dataclass']]) -> Type['BaseModel']:\n    from pydantic.v1.main import BaseModel\n\n    try:\n        model_cls = obj.__pydantic_model__  # type: ignore\n    except AttributeError:\n        model_cls = obj\n\n    if not issubclass(model_cls, BaseModel):\n        raise TypeError('Unsupported type, must be either BaseModel or dataclass')\n    return model_cls\n\n\ndef to_camel(string: str) -> str:\n    return ''.join(word.capitalize() for word in string.split('_'))\n\n\ndef to_lower_camel(string: str) -> str:\n    if len(string) >= 1:\n        pascal_string = to_camel(string)\n        return pascal_string[0].lower() + pascal_string[1:]\n    return string.lower()\n\n\nT = TypeVar('T')\n\n\ndef unique_list(\n    input_list: Union[List[T], Tuple[T, ...]],\n    *,\n    name_factory: Callable[[T], str] = str,\n) -> List[T]:\n    \"\"\"\n    Make a list unique while maintaining order.\n    We update the list if another one with the same name is set\n    (e.g. root validator overridden in subclass)\n    \"\"\"\n    result: List[T] = []\n    result_names: List[str] = []\n    for v in input_list:\n        v_name = name_factory(v)\n        if v_name not in result_names:\n            result_names.append(v_name)\n            result.append(v)\n        else:\n            result[result_names.index(v_name)] = v\n\n    return result\n\n\nclass PyObjectStr(str):\n    \"\"\"\n    String class where repr doesn't include quotes. Useful with Representation when you want to return a string\n    representation of something that valid (or pseudo-valid) python.\n    \"\"\"\n\n    def __repr__(self) -> str:\n        return str(self)\n\n\nclass Representation:\n    \"\"\"\n    Mixin to provide __str__, __repr__, and __pretty__ methods. See #884 for more details.\n\n    __pretty__ is used by [devtools](https://python-devtools.helpmanual.io/) to provide human readable representations\n    of objects.\n    \"\"\"\n\n    __slots__: Tuple[str, ...] = tuple()\n\n    def __repr_args__(self) -> 'ReprArgs':\n        \"\"\"\n        Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n\n        Can either return:\n        * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n        * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n        \"\"\"\n        attrs = ((s, getattr(self, s)) for s in self.__slots__)\n        return [(a, v) for a, v in attrs if v is not None]\n\n    def __repr_name__(self) -> str:\n        \"\"\"\n        Name of the instance's class, used in __repr__.\n        \"\"\"\n        return self.__class__.__name__\n\n    def __repr_str__(self, join_str: str) -> str:\n        return join_str.join(repr(v) if a is None else f'{a}={v!r}' for a, v in self.__repr_args__())\n\n    def __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, None, None]:\n        \"\"\"\n        Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n        \"\"\"\n        yield self.__repr_name__() + '('\n        yield 1\n        for name, value in self.__repr_args__():\n            if name is not None:\n                yield name + '='\n            yield fmt(value)\n            yield ','\n            yield 0\n        yield -1\n        yield ')'\n\n    def __str__(self) -> str:\n        return self.__repr_str__(' ')\n\n    def __repr__(self) -> str:\n        return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'\n\n    def __rich_repr__(self) -> 'RichReprResult':\n        \"\"\"Get fields for Rich library\"\"\"\n        for name, field_repr in self.__repr_args__():\n            if name is None:\n                yield field_repr\n            else:\n                yield name, field_repr\n\n\nclass GetterDict(Representation):\n    \"\"\"\n    Hack to make object's smell just enough like dicts for validate_model.\n\n    We can't inherit from Mapping[str, Any] because it upsets cython so we have to implement all methods ourselves.\n    \"\"\"\n\n    __slots__ = ('_obj',)\n\n    def __init__(self, obj: Any):\n        self._obj = obj\n\n    def __getitem__(self, key: str) -> Any:\n        try:\n            return getattr(self._obj, key)\n        except AttributeError as e:\n            raise KeyError(key) from e\n\n    def get(self, key: Any, default: Any = None) -> Any:\n        return getattr(self._obj, key, default)\n\n    def extra_keys(self) -> Set[Any]:\n        \"\"\"\n        We don't want to get any other attributes of obj if the model didn't explicitly ask for them\n        \"\"\"\n        return set()\n\n    def keys(self) -> List[Any]:\n        \"\"\"\n        Keys of the pseudo dictionary, uses a list not set so order information can be maintained like python\n        dictionaries.\n        \"\"\"\n        return list(self)\n\n    def values(self) -> List[Any]:\n        return [self[k] for k in self]\n\n    def items(self) -> Iterator[Tuple[str, Any]]:\n        for k in self:\n            yield k, self.get(k)\n\n    def __iter__(self) -> Iterator[str]:\n        for name in dir(self._obj):\n            if not name.startswith('_'):\n                yield name\n\n    def __len__(self) -> int:\n        return sum(1 for _ in self)\n\n    def __contains__(self, item: Any) -> bool:\n        return item in self.keys()\n\n    def __eq__(self, other: Any) -> bool:\n        return dict(self) == dict(other.items())\n\n    def __repr_args__(self) -> 'ReprArgs':\n        return [(None, dict(self))]\n\n    def __repr_name__(self) -> str:\n        return f'GetterDict[{display_as_type(self._obj)}]'\n\n\nclass ValueItems(Representation):\n    \"\"\"\n    Class for more convenient calculation of excluded or included fields on values.\n    \"\"\"\n\n    __slots__ = ('_items', '_type')\n\n    def __init__(self, value: Any, items: Union['AbstractSetIntStr', 'MappingIntStrAny']) -> None:\n        items = self._coerce_items(items)\n\n        if isinstance(value, (list, tuple)):\n            items = self._normalize_indexes(items, len(value))\n\n        self._items: 'MappingIntStrAny' = items\n\n    def is_excluded(self, item: Any) -> bool:\n        \"\"\"\n        Check if item is fully excluded.\n\n        :param item: key or index of a value\n        \"\"\"\n        return self.is_true(self._items.get(item))\n\n    def is_included(self, item: Any) -> bool:\n        \"\"\"\n        Check if value is contained in self._items\n\n        :param item: key or index of value\n        \"\"\"\n        return item in self._items\n\n    def for_element(self, e: 'IntStr') -> Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']]:\n        \"\"\"\n        :param e: key or index of element on value\n        :return: raw values for element if self._items is dict and contain needed element\n        \"\"\"\n\n        item = self._items.get(e)\n        return item if not self.is_true(item) else None\n\n    def _normalize_indexes(self, items: 'MappingIntStrAny', v_length: int) -> 'DictIntStrAny':\n        \"\"\"\n        :param items: dict or set of indexes which will be normalized\n        :param v_length: length of sequence indexes of which will be\n\n        >>> self._normalize_indexes({0: True, -2: True, -1: True}, 4)\n        {0: True, 2: True, 3: True}\n        >>> self._normalize_indexes({'__all__': True}, 4)\n        {0: True, 1: True, 2: True, 3: True}\n        \"\"\"\n\n        normalized_items: 'DictIntStrAny' = {}\n        all_items = None\n        for i, v in items.items():\n            if not (isinstance(v, Mapping) or isinstance(v, AbstractSet) or self.is_true(v)):\n                raise TypeError(f'Unexpected type of exclude value for index \"{i}\" {v.__class__}')\n            if i == '__all__':\n                all_items = self._coerce_value(v)\n                continue\n            if not isinstance(i, int):\n                raise TypeError(\n                    'Excluding fields from a sequence of sub-models or dicts must be performed index-wise: '\n                    'expected integer keys or keyword \"__all__\"'\n                )\n            normalized_i = v_length + i if i < 0 else i\n            normalized_items[normalized_i] = self.merge(v, normalized_items.get(normalized_i))\n\n        if not all_items:\n            return normalized_items\n        if self.is_true(all_items):\n            for i in range(v_length):\n                normalized_items.setdefault(i, ...)\n            return normalized_items\n        for i in range(v_length):\n            normalized_item = normalized_items.setdefault(i, {})\n            if not self.is_true(normalized_item):\n                normalized_items[i] = self.merge(all_items, normalized_item)\n        return normalized_items\n\n    @classmethod\n    def merge(cls, base: Any, override: Any, intersect: bool = False) -> Any:\n        \"\"\"\n        Merge a ``base`` item with an ``override`` item.\n\n        Both ``base`` and ``override`` are converted to dictionaries if possible.\n        Sets are converted to dictionaries with the sets entries as keys and\n        Ellipsis as values.\n\n        Each key-value pair existing in ``base`` is merged with ``override``,\n        while the rest of the key-value pairs are updated recursively with this function.\n\n        Merging takes place based on the \"union\" of keys if ``intersect`` is\n        set to ``False`` (default) and on the intersection of keys if\n        ``intersect`` is set to ``True``.\n        \"\"\"\n        override = cls._coerce_value(override)\n        base = cls._coerce_value(base)\n        if override is None:\n            return base\n        if cls.is_true(base) or base is None:\n            return override\n        if cls.is_true(override):\n            return base if intersect else override\n\n        # intersection or union of keys while preserving ordering:\n        if intersect:\n            merge_keys = [k for k in base if k in override] + [k for k in override if k in base]\n        else:\n            merge_keys = list(base) + [k for k in override if k not in base]\n\n        merged: 'DictIntStrAny' = {}\n        for k in merge_keys:\n            merged_item = cls.merge(base.get(k), override.get(k), intersect=intersect)\n            if merged_item is not None:\n                merged[k] = merged_item\n\n        return merged\n\n    @staticmethod\n    def _coerce_items(items: Union['AbstractSetIntStr', 'MappingIntStrAny']) -> 'MappingIntStrAny':\n        if isinstance(items, Mapping):\n            pass\n        elif isinstance(items, AbstractSet):\n            items = dict.fromkeys(items, ...)\n        else:\n            class_name = getattr(items, '__class__', '???')\n            assert_never(\n                items,\n                f'Unexpected type of exclude value {class_name}',\n            )\n        return items\n\n    @classmethod\n    def _coerce_value(cls, value: Any) -> Any:\n        if value is None or cls.is_true(value):\n            return value\n        return cls._coerce_items(value)\n\n    @staticmethod\n    def is_true(v: Any) -> bool:\n        return v is True or v is ...\n\n    def __repr_args__(self) -> 'ReprArgs':\n        return [(None, self._items)]\n\n\nclass ClassAttribute:\n    \"\"\"\n    Hide class attribute from its instances\n    \"\"\"\n\n    __slots__ = (\n        'name',\n        'value',\n    )\n\n    def __init__(self, name: str, value: Any) -> None:\n        self.name = name\n        self.value = value\n\n    def __get__(self, instance: Any, owner: Type[Any]) -> None:\n        if instance is None:\n            return self.value\n        raise AttributeError(f'{self.name!r} attribute of {owner.__name__!r} is class-only')\n\n\npath_types = {\n    'is_dir': 'directory',\n    'is_file': 'file',\n    'is_mount': 'mount point',\n    'is_symlink': 'symlink',\n    'is_block_device': 'block device',\n    'is_char_device': 'char device',\n    'is_fifo': 'FIFO',\n    'is_socket': 'socket',\n}\n\n\ndef path_type(p: 'Path') -> str:\n    \"\"\"\n    Find out what sort of thing a path is.\n    \"\"\"\n    assert p.exists(), 'path does not exist'\n    for method, name in path_types.items():\n        if getattr(p, method)():\n            return name\n\n    return 'unknown'\n\n\nObj = TypeVar('Obj')\n\n\ndef smart_deepcopy(obj: Obj) -> Obj:\n    \"\"\"\n    Return type as is for immutable built-in types\n    Use obj.copy() for built-in empty collections\n    Use copy.deepcopy() for non-empty collections and unknown objects\n    \"\"\"\n\n    obj_type = obj.__class__\n    if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n        return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n    try:\n        if not obj and obj_type in BUILTIN_COLLECTIONS:\n            # faster way for empty collections, no need to copy its members\n            return obj if obj_type is tuple else obj.copy()  # type: ignore  # tuple doesn't have copy method\n    except (TypeError, ValueError, RuntimeError):\n        # do we really dare to catch ALL errors? Seems a bit risky\n        pass\n\n    return deepcopy(obj)  # slowest way when we actually might need a deepcopy\n\n\ndef is_valid_field(name: str) -> bool:\n    if not name.startswith('_'):\n        return True\n    return ROOT_KEY == name\n\n\nDUNDER_ATTRIBUTES = {\n    '__annotations__',\n    '__classcell__',\n    '__doc__',\n    '__module__',\n    '__orig_bases__',\n    '__orig_class__',\n    '__qualname__',\n}\n\n\ndef is_valid_private_name(name: str) -> bool:\n    return not is_valid_field(name) and name not in DUNDER_ATTRIBUTES\n\n\n_EMPTY = object()\n\n\ndef all_identical(left: Iterable[Any], right: Iterable[Any]) -> bool:\n    \"\"\"\n    Check that the items of `left` are the same objects as those in `right`.\n\n    >>> a, b = object(), object()\n    >>> all_identical([a, b, a], [a, b, a])\n    True\n    >>> all_identical([a, b, [a]], [a, b, [a]])  # new list object, while \"equal\" is not \"identical\"\n    False\n    \"\"\"\n    for left_item, right_item in zip_longest(left, right, fillvalue=_EMPTY):\n        if left_item is not right_item:\n            return False\n    return True\n\n\ndef assert_never(obj: NoReturn, msg: str) -> NoReturn:\n    \"\"\"\n    Helper to make sure that we have covered all possible types.\n\n    This is mostly useful for ``mypy``, docs:\n    https://mypy.readthedocs.io/en/latest/literal_types.html#exhaustive-checks\n    \"\"\"\n    raise TypeError(msg)\n\n\ndef get_unique_discriminator_alias(all_aliases: Collection[str], discriminator_key: str) -> str:\n    \"\"\"Validate that all aliases are the same and if that's the case return the alias\"\"\"\n    unique_aliases = set(all_aliases)\n    if len(unique_aliases) > 1:\n        raise ConfigError(\n            f'Aliases for discriminator {discriminator_key!r} must be the same (got {\", \".join(sorted(all_aliases))})'\n        )\n    return unique_aliases.pop()\n\n\ndef get_discriminator_alias_and_values(tp: Any, discriminator_key: str) -> Tuple[str, Tuple[str, ...]]:\n    \"\"\"\n    Get alias and all valid values in the `Literal` type of the discriminator field\n    `tp` can be a `BaseModel` class or directly an `Annotated` `Union` of many.\n    \"\"\"\n    is_root_model = getattr(tp, '__custom_root_type__', False)\n\n    if get_origin(tp) is Annotated:\n        tp = get_args(tp)[0]\n\n    if hasattr(tp, '__pydantic_model__'):\n        tp = tp.__pydantic_model__\n\n    if is_union(get_origin(tp)):\n        alias, all_values = _get_union_alias_and_all_values(tp, discriminator_key)\n        return alias, tuple(v for values in all_values for v in values)\n    elif is_root_model:\n        union_type = tp.__fields__[ROOT_KEY].type_\n        alias, all_values = _get_union_alias_and_all_values(union_type, discriminator_key)\n\n        if len(set(all_values)) > 1:\n            raise ConfigError(\n                f'Field {discriminator_key!r} is not the same for all submodels of {display_as_type(tp)!r}'\n            )\n\n        return alias, all_values[0]\n\n    else:\n        try:\n            t_discriminator_type = tp.__fields__[discriminator_key].type_\n        except AttributeError as e:\n            raise TypeError(f'Type {tp.__name__!r} is not a valid `BaseModel` or `dataclass`') from e\n        except KeyError as e:\n            raise ConfigError(f'Model {tp.__name__!r} needs a discriminator field for key {discriminator_key!r}') from e\n\n        if not is_literal_type(t_discriminator_type):\n            raise ConfigError(f'Field {discriminator_key!r} of model {tp.__name__!r} needs to be a `Literal`')\n\n        return tp.__fields__[discriminator_key].alias, all_literal_values(t_discriminator_type)\n\n\ndef _get_union_alias_and_all_values(\n    union_type: Type[Any], discriminator_key: str\n) -> Tuple[str, Tuple[Tuple[str, ...], ...]]:\n    zipped_aliases_values = [get_discriminator_alias_and_values(t, discriminator_key) for t in get_args(union_type)]\n    # unzip: [('alias_a',('v1', 'v2)), ('alias_b', ('v3',))] => [('alias_a', 'alias_b'), (('v1', 'v2'), ('v3',))]\n    all_aliases, all_values = zip(*zipped_aliases_values)\n    return get_unique_discriminator_alias(all_aliases, discriminator_key), all_values\n", "pydantic/v1/_hypothesis_plugin.py": "\"\"\"\nRegister Hypothesis strategies for Pydantic custom types.\n\nThis enables fully-automatic generation of test data for most Pydantic classes.\n\nNote that this module has *no* runtime impact on Pydantic itself; instead it\nis registered as a setuptools entry point and Hypothesis will import it if\nPydantic is installed.  See also:\n\nhttps://hypothesis.readthedocs.io/en/latest/strategies.html#registering-strategies-via-setuptools-entry-points\nhttps://hypothesis.readthedocs.io/en/latest/data.html#hypothesis.strategies.register_type_strategy\nhttps://hypothesis.readthedocs.io/en/latest/strategies.html#interaction-with-pytest-cov\nhttps://docs.pydantic.dev/usage/types/#pydantic-types\n\nNote that because our motivation is to *improve user experience*, the strategies\nare always sound (never generate invalid data) but sacrifice completeness for\nmaintainability (ie may be unable to generate some tricky but valid data).\n\nFinally, this module makes liberal use of `# type: ignore[<code>]` pragmas.\nThis is because Hypothesis annotates `register_type_strategy()` with\n`(T, SearchStrategy[T])`, but in most cases we register e.g. `ConstrainedInt`\nto generate instances of the builtin `int` type which match the constraints.\n\"\"\"\n\nimport contextlib\nimport datetime\nimport ipaddress\nimport json\nimport math\nfrom fractions import Fraction\nfrom typing import Callable, Dict, Type, Union, cast, overload\n\nimport hypothesis.strategies as st\n\nimport pydantic\nimport pydantic.color\nimport pydantic.types\nfrom pydantic.v1.utils import lenient_issubclass\n\n# FilePath and DirectoryPath are explicitly unsupported, as we'd have to create\n# them on-disk, and that's unsafe in general without being told *where* to do so.\n#\n# URLs are unsupported because it's easy for users to define their own strategy for\n# \"normal\" URLs, and hard for us to define a general strategy which includes \"weird\"\n# URLs but doesn't also have unpredictable performance problems.\n#\n# conlist() and conset() are unsupported for now, because the workarounds for\n# Cython and Hypothesis to handle parametrized generic types are incompatible.\n# We are rethinking Hypothesis compatibility in Pydantic v2.\n\n# Emails\ntry:\n    import email_validator\nexcept ImportError:  # pragma: no cover\n    pass\nelse:\n\n    def is_valid_email(s: str) -> bool:\n        # Hypothesis' st.emails() occasionally generates emails like 0@A0--0.ac\n        # that are invalid according to email-validator, so we filter those out.\n        try:\n            email_validator.validate_email(s, check_deliverability=False)\n            return True\n        except email_validator.EmailNotValidError:  # pragma: no cover\n            return False\n\n    # Note that these strategies deliberately stay away from any tricky Unicode\n    # or other encoding issues; we're just trying to generate *something* valid.\n    st.register_type_strategy(pydantic.EmailStr, st.emails().filter(is_valid_email))  # type: ignore[arg-type]\n    st.register_type_strategy(\n        pydantic.NameEmail,\n        st.builds(\n            '{} <{}>'.format,  # type: ignore[arg-type]\n            st.from_regex('[A-Za-z0-9_]+( [A-Za-z0-9_]+){0,5}', fullmatch=True),\n            st.emails().filter(is_valid_email),\n        ),\n    )\n\n# PyObject - dotted names, in this case taken from the math module.\nst.register_type_strategy(\n    pydantic.PyObject,  # type: ignore[arg-type]\n    st.sampled_from(\n        [cast(pydantic.PyObject, f'math.{name}') for name in sorted(vars(math)) if not name.startswith('_')]\n    ),\n)\n\n# CSS3 Colors; as name, hex, rgb(a) tuples or strings, or hsl strings\n_color_regexes = (\n    '|'.join(\n        (\n            pydantic.color.r_hex_short,\n            pydantic.color.r_hex_long,\n            pydantic.color.r_rgb,\n            pydantic.color.r_rgba,\n            pydantic.color.r_hsl,\n            pydantic.color.r_hsla,\n        )\n    )\n    # Use more precise regex patterns to avoid value-out-of-range errors\n    .replace(pydantic.color._r_sl, r'(?:(\\d\\d?(?:\\.\\d+)?|100(?:\\.0+)?)%)')\n    .replace(pydantic.color._r_alpha, r'(?:(0(?:\\.\\d+)?|1(?:\\.0+)?|\\.\\d+|\\d{1,2}%))')\n    .replace(pydantic.color._r_255, r'(?:((?:\\d|\\d\\d|[01]\\d\\d|2[0-4]\\d|25[0-4])(?:\\.\\d+)?|255(?:\\.0+)?))')\n)\nst.register_type_strategy(\n    pydantic.color.Color,\n    st.one_of(\n        st.sampled_from(sorted(pydantic.color.COLORS_BY_NAME)),\n        st.tuples(\n            st.integers(0, 255),\n            st.integers(0, 255),\n            st.integers(0, 255),\n            st.none() | st.floats(0, 1) | st.floats(0, 100).map('{}%'.format),\n        ),\n        st.from_regex(_color_regexes, fullmatch=True),\n    ),\n)\n\n\n# Card numbers, valid according to the Luhn algorithm\n\n\ndef add_luhn_digit(card_number: str) -> str:\n    # See https://en.wikipedia.org/wiki/Luhn_algorithm\n    for digit in '0123456789':\n        with contextlib.suppress(Exception):\n            pydantic.PaymentCardNumber.validate_luhn_check_digit(card_number + digit)\n            return card_number + digit\n    raise AssertionError('Unreachable')  # pragma: no cover\n\n\ncard_patterns = (\n    # Note that these patterns omit the Luhn check digit; that's added by the function above\n    '4[0-9]{14}',  # Visa\n    '5[12345][0-9]{13}',  # Mastercard\n    '3[47][0-9]{12}',  # American Express\n    '[0-26-9][0-9]{10,17}',  # other (incomplete to avoid overlap)\n)\nst.register_type_strategy(\n    pydantic.PaymentCardNumber,\n    st.from_regex('|'.join(card_patterns), fullmatch=True).map(add_luhn_digit),  # type: ignore[arg-type]\n)\n\n# UUIDs\nst.register_type_strategy(pydantic.UUID1, st.uuids(version=1))\nst.register_type_strategy(pydantic.UUID3, st.uuids(version=3))\nst.register_type_strategy(pydantic.UUID4, st.uuids(version=4))\nst.register_type_strategy(pydantic.UUID5, st.uuids(version=5))\n\n# Secrets\nst.register_type_strategy(pydantic.SecretBytes, st.binary().map(pydantic.SecretBytes))\nst.register_type_strategy(pydantic.SecretStr, st.text().map(pydantic.SecretStr))\n\n# IP addresses, networks, and interfaces\nst.register_type_strategy(pydantic.IPvAnyAddress, st.ip_addresses())  # type: ignore[arg-type]\nst.register_type_strategy(\n    pydantic.IPvAnyInterface,\n    st.from_type(ipaddress.IPv4Interface) | st.from_type(ipaddress.IPv6Interface),  # type: ignore[arg-type]\n)\nst.register_type_strategy(\n    pydantic.IPvAnyNetwork,\n    st.from_type(ipaddress.IPv4Network) | st.from_type(ipaddress.IPv6Network),  # type: ignore[arg-type]\n)\n\n# We hook into the con***() functions and the ConstrainedNumberMeta metaclass,\n# so here we only have to register subclasses for other constrained types which\n# don't go via those mechanisms.  Then there are the registration hooks below.\nst.register_type_strategy(pydantic.StrictBool, st.booleans())\nst.register_type_strategy(pydantic.StrictStr, st.text())\n\n\n# FutureDate, PastDate\nst.register_type_strategy(pydantic.FutureDate, st.dates(min_value=datetime.date.today() + datetime.timedelta(days=1)))\nst.register_type_strategy(pydantic.PastDate, st.dates(max_value=datetime.date.today() - datetime.timedelta(days=1)))\n\n\n# Constrained-type resolver functions\n#\n# For these ones, we actually want to inspect the type in order to work out a\n# satisfying strategy.  First up, the machinery for tracking resolver functions:\n\nRESOLVERS: Dict[type, Callable[[type], st.SearchStrategy]] = {}  # type: ignore[type-arg]\n\n\n@overload\ndef _registered(typ: Type[pydantic.types.T]) -> Type[pydantic.types.T]:\n    pass\n\n\n@overload\ndef _registered(typ: pydantic.types.ConstrainedNumberMeta) -> pydantic.types.ConstrainedNumberMeta:\n    pass\n\n\ndef _registered(\n    typ: Union[Type[pydantic.types.T], pydantic.types.ConstrainedNumberMeta]\n) -> Union[Type[pydantic.types.T], pydantic.types.ConstrainedNumberMeta]:\n    # This function replaces the version in `pydantic.types`, in order to\n    # effect the registration of new constrained types so that Hypothesis\n    # can generate valid examples.\n    pydantic.types._DEFINED_TYPES.add(typ)\n    for supertype, resolver in RESOLVERS.items():\n        if issubclass(typ, supertype):\n            st.register_type_strategy(typ, resolver(typ))  # type: ignore\n            return typ\n    raise NotImplementedError(f'Unknown type {typ!r} has no resolver to register')  # pragma: no cover\n\n\ndef resolves(\n    typ: Union[type, pydantic.types.ConstrainedNumberMeta]\n) -> Callable[[Callable[..., st.SearchStrategy]], Callable[..., st.SearchStrategy]]:  # type: ignore[type-arg]\n    def inner(f):  # type: ignore\n        assert f not in RESOLVERS\n        RESOLVERS[typ] = f\n        return f\n\n    return inner\n\n\n# Type-to-strategy resolver functions\n\n\n@resolves(pydantic.JsonWrapper)\ndef resolve_json(cls):  # type: ignore[no-untyped-def]\n    try:\n        inner = st.none() if cls.inner_type is None else st.from_type(cls.inner_type)\n    except Exception:  # pragma: no cover\n        finite = st.floats(allow_infinity=False, allow_nan=False)\n        inner = st.recursive(\n            base=st.one_of(st.none(), st.booleans(), st.integers(), finite, st.text()),\n            extend=lambda x: st.lists(x) | st.dictionaries(st.text(), x),  # type: ignore\n        )\n    inner_type = getattr(cls, 'inner_type', None)\n    return st.builds(\n        cls.inner_type.json if lenient_issubclass(inner_type, pydantic.BaseModel) else json.dumps,\n        inner,\n        ensure_ascii=st.booleans(),\n        indent=st.none() | st.integers(0, 16),\n        sort_keys=st.booleans(),\n    )\n\n\n@resolves(pydantic.ConstrainedBytes)\ndef resolve_conbytes(cls):  # type: ignore[no-untyped-def]  # pragma: no cover\n    min_size = cls.min_length or 0\n    max_size = cls.max_length\n    if not cls.strip_whitespace:\n        return st.binary(min_size=min_size, max_size=max_size)\n    # Fun with regex to ensure we neither start nor end with whitespace\n    repeats = '{{{},{}}}'.format(\n        min_size - 2 if min_size > 2 else 0,\n        max_size - 2 if (max_size or 0) > 2 else '',\n    )\n    if min_size >= 2:\n        pattern = rf'\\W.{repeats}\\W'\n    elif min_size == 1:\n        pattern = rf'\\W(.{repeats}\\W)?'\n    else:\n        assert min_size == 0\n        pattern = rf'(\\W(.{repeats}\\W)?)?'\n    return st.from_regex(pattern.encode(), fullmatch=True)\n\n\n@resolves(pydantic.ConstrainedDecimal)\ndef resolve_condecimal(cls):  # type: ignore[no-untyped-def]\n    min_value = cls.ge\n    max_value = cls.le\n    if cls.gt is not None:\n        assert min_value is None, 'Set `gt` or `ge`, but not both'\n        min_value = cls.gt\n    if cls.lt is not None:\n        assert max_value is None, 'Set `lt` or `le`, but not both'\n        max_value = cls.lt\n    s = st.decimals(min_value, max_value, allow_nan=False, places=cls.decimal_places)\n    if cls.lt is not None:\n        s = s.filter(lambda d: d < cls.lt)\n    if cls.gt is not None:\n        s = s.filter(lambda d: cls.gt < d)\n    return s\n\n\n@resolves(pydantic.ConstrainedFloat)\ndef resolve_confloat(cls):  # type: ignore[no-untyped-def]\n    min_value = cls.ge\n    max_value = cls.le\n    exclude_min = False\n    exclude_max = False\n\n    if cls.gt is not None:\n        assert min_value is None, 'Set `gt` or `ge`, but not both'\n        min_value = cls.gt\n        exclude_min = True\n    if cls.lt is not None:\n        assert max_value is None, 'Set `lt` or `le`, but not both'\n        max_value = cls.lt\n        exclude_max = True\n\n    if cls.multiple_of is None:\n        return st.floats(min_value, max_value, exclude_min=exclude_min, exclude_max=exclude_max, allow_nan=False)\n\n    if min_value is not None:\n        min_value = math.ceil(min_value / cls.multiple_of)\n        if exclude_min:\n            min_value = min_value + 1\n    if max_value is not None:\n        assert max_value >= cls.multiple_of, 'Cannot build model with max value smaller than multiple of'\n        max_value = math.floor(max_value / cls.multiple_of)\n        if exclude_max:\n            max_value = max_value - 1\n\n    return st.integers(min_value, max_value).map(lambda x: x * cls.multiple_of)\n\n\n@resolves(pydantic.ConstrainedInt)\ndef resolve_conint(cls):  # type: ignore[no-untyped-def]\n    min_value = cls.ge\n    max_value = cls.le\n    if cls.gt is not None:\n        assert min_value is None, 'Set `gt` or `ge`, but not both'\n        min_value = cls.gt + 1\n    if cls.lt is not None:\n        assert max_value is None, 'Set `lt` or `le`, but not both'\n        max_value = cls.lt - 1\n\n    if cls.multiple_of is None or cls.multiple_of == 1:\n        return st.integers(min_value, max_value)\n\n    # These adjustments and the .map handle integer-valued multiples, while the\n    # .filter handles trickier cases as for confloat.\n    if min_value is not None:\n        min_value = math.ceil(Fraction(min_value) / Fraction(cls.multiple_of))\n    if max_value is not None:\n        max_value = math.floor(Fraction(max_value) / Fraction(cls.multiple_of))\n    return st.integers(min_value, max_value).map(lambda x: x * cls.multiple_of)\n\n\n@resolves(pydantic.ConstrainedDate)\ndef resolve_condate(cls):  # type: ignore[no-untyped-def]\n    if cls.ge is not None:\n        assert cls.gt is None, 'Set `gt` or `ge`, but not both'\n        min_value = cls.ge\n    elif cls.gt is not None:\n        min_value = cls.gt + datetime.timedelta(days=1)\n    else:\n        min_value = datetime.date.min\n    if cls.le is not None:\n        assert cls.lt is None, 'Set `lt` or `le`, but not both'\n        max_value = cls.le\n    elif cls.lt is not None:\n        max_value = cls.lt - datetime.timedelta(days=1)\n    else:\n        max_value = datetime.date.max\n    return st.dates(min_value, max_value)\n\n\n@resolves(pydantic.ConstrainedStr)\ndef resolve_constr(cls):  # type: ignore[no-untyped-def]  # pragma: no cover\n    min_size = cls.min_length or 0\n    max_size = cls.max_length\n\n    if cls.regex is None and not cls.strip_whitespace:\n        return st.text(min_size=min_size, max_size=max_size)\n\n    if cls.regex is not None:\n        strategy = st.from_regex(cls.regex)\n        if cls.strip_whitespace:\n            strategy = strategy.filter(lambda s: s == s.strip())\n    elif cls.strip_whitespace:\n        repeats = '{{{},{}}}'.format(\n            min_size - 2 if min_size > 2 else 0,\n            max_size - 2 if (max_size or 0) > 2 else '',\n        )\n        if min_size >= 2:\n            strategy = st.from_regex(rf'\\W.{repeats}\\W')\n        elif min_size == 1:\n            strategy = st.from_regex(rf'\\W(.{repeats}\\W)?')\n        else:\n            assert min_size == 0\n            strategy = st.from_regex(rf'(\\W(.{repeats}\\W)?)?')\n\n    if min_size == 0 and max_size is None:\n        return strategy\n    elif max_size is None:\n        return strategy.filter(lambda s: min_size <= len(s))\n    return strategy.filter(lambda s: min_size <= len(s) <= max_size)\n\n\n# Finally, register all previously-defined types, and patch in our new function\nfor typ in list(pydantic.types._DEFINED_TYPES):\n    _registered(typ)\npydantic.types._registered = _registered\nst.register_type_strategy(pydantic.Json, resolve_json)\n", "pydantic/v1/fields.py": "import copy\nimport re\nfrom collections import Counter as CollectionCounter, defaultdict, deque\nfrom collections.abc import Callable, Hashable as CollectionsHashable, Iterable as CollectionsIterable\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Counter,\n    DefaultDict,\n    Deque,\n    Dict,\n    ForwardRef,\n    FrozenSet,\n    Generator,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Pattern,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nfrom typing_extensions import Annotated, Final\n\nfrom pydantic.v1 import errors as errors_\nfrom pydantic.v1.class_validators import Validator, make_generic_validator, prep_validators\nfrom pydantic.v1.error_wrappers import ErrorWrapper\nfrom pydantic.v1.errors import ConfigError, InvalidDiscriminator, MissingDiscriminator, NoneIsNotAllowedError\nfrom pydantic.v1.types import Json, JsonWrapper\nfrom pydantic.v1.typing import (\n    NoArgAnyCallable,\n    convert_generics,\n    display_as_type,\n    get_args,\n    get_origin,\n    is_finalvar,\n    is_literal_type,\n    is_new_type,\n    is_none_type,\n    is_typeddict,\n    is_typeddict_special,\n    is_union,\n    new_type_supertype,\n)\nfrom pydantic.v1.utils import (\n    PyObjectStr,\n    Representation,\n    ValueItems,\n    get_discriminator_alias_and_values,\n    get_unique_discriminator_alias,\n    lenient_isinstance,\n    lenient_issubclass,\n    sequence_like,\n    smart_deepcopy,\n)\nfrom pydantic.v1.validators import constant_validator, dict_validator, find_validators, validate_json\n\nRequired: Any = Ellipsis\n\nT = TypeVar('T')\n\n\nclass UndefinedType:\n    def __repr__(self) -> str:\n        return 'PydanticUndefined'\n\n    def __copy__(self: T) -> T:\n        return self\n\n    def __reduce__(self) -> str:\n        return 'Undefined'\n\n    def __deepcopy__(self: T, _: Any) -> T:\n        return self\n\n\nUndefined = UndefinedType()\n\nif TYPE_CHECKING:\n    from pydantic.v1.class_validators import ValidatorsList\n    from pydantic.v1.config import BaseConfig\n    from pydantic.v1.error_wrappers import ErrorList\n    from pydantic.v1.types import ModelOrDc\n    from pydantic.v1.typing import AbstractSetIntStr, MappingIntStrAny, ReprArgs\n\n    ValidateReturn = Tuple[Optional[Any], Optional[ErrorList]]\n    LocStr = Union[Tuple[Union[int, str], ...], str]\n    BoolUndefined = Union[bool, UndefinedType]\n\n\nclass FieldInfo(Representation):\n    \"\"\"\n    Captures extra information about a field.\n    \"\"\"\n\n    __slots__ = (\n        'default',\n        'default_factory',\n        'alias',\n        'alias_priority',\n        'title',\n        'description',\n        'exclude',\n        'include',\n        'const',\n        'gt',\n        'ge',\n        'lt',\n        'le',\n        'multiple_of',\n        'allow_inf_nan',\n        'max_digits',\n        'decimal_places',\n        'min_items',\n        'max_items',\n        'unique_items',\n        'min_length',\n        'max_length',\n        'allow_mutation',\n        'repr',\n        'regex',\n        'discriminator',\n        'extra',\n    )\n\n    # field constraints with the default value, it's also used in update_from_config below\n    __field_constraints__ = {\n        'min_length': None,\n        'max_length': None,\n        'regex': None,\n        'gt': None,\n        'lt': None,\n        'ge': None,\n        'le': None,\n        'multiple_of': None,\n        'allow_inf_nan': None,\n        'max_digits': None,\n        'decimal_places': None,\n        'min_items': None,\n        'max_items': None,\n        'unique_items': None,\n        'allow_mutation': True,\n    }\n\n    def __init__(self, default: Any = Undefined, **kwargs: Any) -> None:\n        self.default = default\n        self.default_factory = kwargs.pop('default_factory', None)\n        self.alias = kwargs.pop('alias', None)\n        self.alias_priority = kwargs.pop('alias_priority', 2 if self.alias is not None else None)\n        self.title = kwargs.pop('title', None)\n        self.description = kwargs.pop('description', None)\n        self.exclude = kwargs.pop('exclude', None)\n        self.include = kwargs.pop('include', None)\n        self.const = kwargs.pop('const', None)\n        self.gt = kwargs.pop('gt', None)\n        self.ge = kwargs.pop('ge', None)\n        self.lt = kwargs.pop('lt', None)\n        self.le = kwargs.pop('le', None)\n        self.multiple_of = kwargs.pop('multiple_of', None)\n        self.allow_inf_nan = kwargs.pop('allow_inf_nan', None)\n        self.max_digits = kwargs.pop('max_digits', None)\n        self.decimal_places = kwargs.pop('decimal_places', None)\n        self.min_items = kwargs.pop('min_items', None)\n        self.max_items = kwargs.pop('max_items', None)\n        self.unique_items = kwargs.pop('unique_items', None)\n        self.min_length = kwargs.pop('min_length', None)\n        self.max_length = kwargs.pop('max_length', None)\n        self.allow_mutation = kwargs.pop('allow_mutation', True)\n        self.regex = kwargs.pop('regex', None)\n        self.discriminator = kwargs.pop('discriminator', None)\n        self.repr = kwargs.pop('repr', True)\n        self.extra = kwargs\n\n    def __repr_args__(self) -> 'ReprArgs':\n        field_defaults_to_hide: Dict[str, Any] = {\n            'repr': True,\n            **self.__field_constraints__,\n        }\n\n        attrs = ((s, getattr(self, s)) for s in self.__slots__)\n        return [(a, v) for a, v in attrs if v != field_defaults_to_hide.get(a, None)]\n\n    def get_constraints(self) -> Set[str]:\n        \"\"\"\n        Gets the constraints set on the field by comparing the constraint value with its default value\n\n        :return: the constraints set on field_info\n        \"\"\"\n        return {attr for attr, default in self.__field_constraints__.items() if getattr(self, attr) != default}\n\n    def update_from_config(self, from_config: Dict[str, Any]) -> None:\n        \"\"\"\n        Update this FieldInfo based on a dict from get_field_info, only fields which have not been set are dated.\n        \"\"\"\n        for attr_name, value in from_config.items():\n            try:\n                current_value = getattr(self, attr_name)\n            except AttributeError:\n                # attr_name is not an attribute of FieldInfo, it should therefore be added to extra\n                # (except if extra already has this value!)\n                self.extra.setdefault(attr_name, value)\n            else:\n                if current_value is self.__field_constraints__.get(attr_name, None):\n                    setattr(self, attr_name, value)\n                elif attr_name == 'exclude':\n                    self.exclude = ValueItems.merge(value, current_value)\n                elif attr_name == 'include':\n                    self.include = ValueItems.merge(value, current_value, intersect=True)\n\n    def _validate(self) -> None:\n        if self.default is not Undefined and self.default_factory is not None:\n            raise ValueError('cannot specify both default and default_factory')\n\n\ndef Field(\n    default: Any = Undefined,\n    *,\n    default_factory: Optional[NoArgAnyCallable] = None,\n    alias: Optional[str] = None,\n    title: Optional[str] = None,\n    description: Optional[str] = None,\n    exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny', Any]] = None,\n    include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny', Any]] = None,\n    const: Optional[bool] = None,\n    gt: Optional[float] = None,\n    ge: Optional[float] = None,\n    lt: Optional[float] = None,\n    le: Optional[float] = None,\n    multiple_of: Optional[float] = None,\n    allow_inf_nan: Optional[bool] = None,\n    max_digits: Optional[int] = None,\n    decimal_places: Optional[int] = None,\n    min_items: Optional[int] = None,\n    max_items: Optional[int] = None,\n    unique_items: Optional[bool] = None,\n    min_length: Optional[int] = None,\n    max_length: Optional[int] = None,\n    allow_mutation: bool = True,\n    regex: Optional[str] = None,\n    discriminator: Optional[str] = None,\n    repr: bool = True,\n    **extra: Any,\n) -> Any:\n    \"\"\"\n    Used to provide extra information about a field, either for the model schema or complex validation. Some arguments\n    apply only to number fields (``int``, ``float``, ``Decimal``) and some apply only to ``str``.\n\n    :param default: since this is replacing the field\u2019s default, its first argument is used\n      to set the default, use ellipsis (``...``) to indicate the field is required\n    :param default_factory: callable that will be called when a default value is needed for this field\n      If both `default` and `default_factory` are set, an error is raised.\n    :param alias: the public name of the field\n    :param title: can be any string, used in the schema\n    :param description: can be any string, used in the schema\n    :param exclude: exclude this field while dumping.\n      Takes same values as the ``include`` and ``exclude`` arguments on the ``.dict`` method.\n    :param include: include this field while dumping.\n      Takes same values as the ``include`` and ``exclude`` arguments on the ``.dict`` method.\n    :param const: this field is required and *must* take it's default value\n    :param gt: only applies to numbers, requires the field to be \"greater than\". The schema\n      will have an ``exclusiveMinimum`` validation keyword\n    :param ge: only applies to numbers, requires the field to be \"greater than or equal to\". The\n      schema will have a ``minimum`` validation keyword\n    :param lt: only applies to numbers, requires the field to be \"less than\". The schema\n      will have an ``exclusiveMaximum`` validation keyword\n    :param le: only applies to numbers, requires the field to be \"less than or equal to\". The\n      schema will have a ``maximum`` validation keyword\n    :param multiple_of: only applies to numbers, requires the field to be \"a multiple of\". The\n      schema will have a ``multipleOf`` validation keyword\n    :param allow_inf_nan: only applies to numbers, allows the field to be NaN or infinity (+inf or -inf),\n        which is a valid Python float. Default True, set to False for compatibility with JSON.\n    :param max_digits: only applies to Decimals, requires the field to have a maximum number\n      of digits within the decimal. It does not include a zero before the decimal point or trailing decimal zeroes.\n    :param decimal_places: only applies to Decimals, requires the field to have at most a number of decimal places\n      allowed. It does not include trailing decimal zeroes.\n    :param min_items: only applies to lists, requires the field to have a minimum number of\n      elements. The schema will have a ``minItems`` validation keyword\n    :param max_items: only applies to lists, requires the field to have a maximum number of\n      elements. The schema will have a ``maxItems`` validation keyword\n    :param unique_items: only applies to lists, requires the field not to have duplicated\n      elements. The schema will have a ``uniqueItems`` validation keyword\n    :param min_length: only applies to strings, requires the field to have a minimum length. The\n      schema will have a ``minLength`` validation keyword\n    :param max_length: only applies to strings, requires the field to have a maximum length. The\n      schema will have a ``maxLength`` validation keyword\n    :param allow_mutation: a boolean which defaults to True. When False, the field raises a TypeError if the field is\n      assigned on an instance.  The BaseModel Config must set validate_assignment to True\n    :param regex: only applies to strings, requires the field match against a regular expression\n      pattern string. The schema will have a ``pattern`` validation keyword\n    :param discriminator: only useful with a (discriminated a.k.a. tagged) `Union` of sub models with a common field.\n      The `discriminator` is the name of this common field to shorten validation and improve generated schema\n    :param repr: show this field in the representation\n    :param **extra: any additional keyword arguments will be added as is to the schema\n    \"\"\"\n    field_info = FieldInfo(\n        default,\n        default_factory=default_factory,\n        alias=alias,\n        title=title,\n        description=description,\n        exclude=exclude,\n        include=include,\n        const=const,\n        gt=gt,\n        ge=ge,\n        lt=lt,\n        le=le,\n        multiple_of=multiple_of,\n        allow_inf_nan=allow_inf_nan,\n        max_digits=max_digits,\n        decimal_places=decimal_places,\n        min_items=min_items,\n        max_items=max_items,\n        unique_items=unique_items,\n        min_length=min_length,\n        max_length=max_length,\n        allow_mutation=allow_mutation,\n        regex=regex,\n        discriminator=discriminator,\n        repr=repr,\n        **extra,\n    )\n    field_info._validate()\n    return field_info\n\n\n# used to be an enum but changed to int's for small performance improvement as less access overhead\nSHAPE_SINGLETON = 1\nSHAPE_LIST = 2\nSHAPE_SET = 3\nSHAPE_MAPPING = 4\nSHAPE_TUPLE = 5\nSHAPE_TUPLE_ELLIPSIS = 6\nSHAPE_SEQUENCE = 7\nSHAPE_FROZENSET = 8\nSHAPE_ITERABLE = 9\nSHAPE_GENERIC = 10\nSHAPE_DEQUE = 11\nSHAPE_DICT = 12\nSHAPE_DEFAULTDICT = 13\nSHAPE_COUNTER = 14\nSHAPE_NAME_LOOKUP = {\n    SHAPE_LIST: 'List[{}]',\n    SHAPE_SET: 'Set[{}]',\n    SHAPE_TUPLE_ELLIPSIS: 'Tuple[{}, ...]',\n    SHAPE_SEQUENCE: 'Sequence[{}]',\n    SHAPE_FROZENSET: 'FrozenSet[{}]',\n    SHAPE_ITERABLE: 'Iterable[{}]',\n    SHAPE_DEQUE: 'Deque[{}]',\n    SHAPE_DICT: 'Dict[{}]',\n    SHAPE_DEFAULTDICT: 'DefaultDict[{}]',\n    SHAPE_COUNTER: 'Counter[{}]',\n}\n\nMAPPING_LIKE_SHAPES: Set[int] = {SHAPE_DEFAULTDICT, SHAPE_DICT, SHAPE_MAPPING, SHAPE_COUNTER}\n\n\nclass ModelField(Representation):\n    __slots__ = (\n        'type_',\n        'outer_type_',\n        'annotation',\n        'sub_fields',\n        'sub_fields_mapping',\n        'key_field',\n        'validators',\n        'pre_validators',\n        'post_validators',\n        'default',\n        'default_factory',\n        'required',\n        'final',\n        'model_config',\n        'name',\n        'alias',\n        'has_alias',\n        'field_info',\n        'discriminator_key',\n        'discriminator_alias',\n        'validate_always',\n        'allow_none',\n        'shape',\n        'class_validators',\n        'parse_json',\n    )\n\n    def __init__(\n        self,\n        *,\n        name: str,\n        type_: Type[Any],\n        class_validators: Optional[Dict[str, Validator]],\n        model_config: Type['BaseConfig'],\n        default: Any = None,\n        default_factory: Optional[NoArgAnyCallable] = None,\n        required: 'BoolUndefined' = Undefined,\n        final: bool = False,\n        alias: Optional[str] = None,\n        field_info: Optional[FieldInfo] = None,\n    ) -> None:\n        self.name: str = name\n        self.has_alias: bool = alias is not None\n        self.alias: str = alias if alias is not None else name\n        self.annotation = type_\n        self.type_: Any = convert_generics(type_)\n        self.outer_type_: Any = type_\n        self.class_validators = class_validators or {}\n        self.default: Any = default\n        self.default_factory: Optional[NoArgAnyCallable] = default_factory\n        self.required: 'BoolUndefined' = required\n        self.final: bool = final\n        self.model_config = model_config\n        self.field_info: FieldInfo = field_info or FieldInfo(default)\n        self.discriminator_key: Optional[str] = self.field_info.discriminator\n        self.discriminator_alias: Optional[str] = self.discriminator_key\n\n        self.allow_none: bool = False\n        self.validate_always: bool = False\n        self.sub_fields: Optional[List[ModelField]] = None\n        self.sub_fields_mapping: Optional[Dict[str, 'ModelField']] = None  # used for discriminated union\n        self.key_field: Optional[ModelField] = None\n        self.validators: 'ValidatorsList' = []\n        self.pre_validators: Optional['ValidatorsList'] = None\n        self.post_validators: Optional['ValidatorsList'] = None\n        self.parse_json: bool = False\n        self.shape: int = SHAPE_SINGLETON\n        self.model_config.prepare_field(self)\n        self.prepare()\n\n    def get_default(self) -> Any:\n        return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n\n    @staticmethod\n    def _get_field_info(\n        field_name: str, annotation: Any, value: Any, config: Type['BaseConfig']\n    ) -> Tuple[FieldInfo, Any]:\n        \"\"\"\n        Get a FieldInfo from a root typing.Annotated annotation, value, or config default.\n\n        The FieldInfo may be set in typing.Annotated or the value, but not both. If neither contain\n        a FieldInfo, a new one will be created using the config.\n\n        :param field_name: name of the field for use in error messages\n        :param annotation: a type hint such as `str` or `Annotated[str, Field(..., min_length=5)]`\n        :param value: the field's assigned value\n        :param config: the model's config object\n        :return: the FieldInfo contained in the `annotation`, the value, or a new one from the config.\n        \"\"\"\n        field_info_from_config = config.get_field_info(field_name)\n\n        field_info = None\n        if get_origin(annotation) is Annotated:\n            field_infos = [arg for arg in get_args(annotation)[1:] if isinstance(arg, FieldInfo)]\n            if len(field_infos) > 1:\n                raise ValueError(f'cannot specify multiple `Annotated` `Field`s for {field_name!r}')\n            field_info = next(iter(field_infos), None)\n            if field_info is not None:\n                field_info = copy.copy(field_info)\n                field_info.update_from_config(field_info_from_config)\n                if field_info.default not in (Undefined, Required):\n                    raise ValueError(f'`Field` default cannot be set in `Annotated` for {field_name!r}')\n                if value is not Undefined and value is not Required:\n                    # check also `Required` because of `validate_arguments` that sets `...` as default value\n                    field_info.default = value\n\n        if isinstance(value, FieldInfo):\n            if field_info is not None:\n                raise ValueError(f'cannot specify `Annotated` and value `Field`s together for {field_name!r}')\n            field_info = value\n            field_info.update_from_config(field_info_from_config)\n        elif field_info is None:\n            field_info = FieldInfo(value, **field_info_from_config)\n        value = None if field_info.default_factory is not None else field_info.default\n        field_info._validate()\n        return field_info, value\n\n    @classmethod\n    def infer(\n        cls,\n        *,\n        name: str,\n        value: Any,\n        annotation: Any,\n        class_validators: Optional[Dict[str, Validator]],\n        config: Type['BaseConfig'],\n    ) -> 'ModelField':\n        from pydantic.v1.schema import get_annotation_from_field_info\n\n        field_info, value = cls._get_field_info(name, annotation, value, config)\n        required: 'BoolUndefined' = Undefined\n        if value is Required:\n            required = True\n            value = None\n        elif value is not Undefined:\n            required = False\n        annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\n\n        return cls(\n            name=name,\n            type_=annotation,\n            alias=field_info.alias,\n            class_validators=class_validators,\n            default=value,\n            default_factory=field_info.default_factory,\n            required=required,\n            model_config=config,\n            field_info=field_info,\n        )\n\n    def set_config(self, config: Type['BaseConfig']) -> None:\n        self.model_config = config\n        info_from_config = config.get_field_info(self.name)\n        config.prepare_field(self)\n        new_alias = info_from_config.get('alias')\n        new_alias_priority = info_from_config.get('alias_priority') or 0\n        if new_alias and new_alias_priority >= (self.field_info.alias_priority or 0):\n            self.field_info.alias = new_alias\n            self.field_info.alias_priority = new_alias_priority\n            self.alias = new_alias\n        new_exclude = info_from_config.get('exclude')\n        if new_exclude is not None:\n            self.field_info.exclude = ValueItems.merge(self.field_info.exclude, new_exclude)\n        new_include = info_from_config.get('include')\n        if new_include is not None:\n            self.field_info.include = ValueItems.merge(self.field_info.include, new_include, intersect=True)\n\n    @property\n    def alt_alias(self) -> bool:\n        return self.name != self.alias\n\n    def prepare(self) -> None:\n        \"\"\"\n        Prepare the field but inspecting self.default, self.type_ etc.\n\n        Note: this method is **not** idempotent (because _type_analysis is not idempotent),\n        e.g. calling it it multiple times may modify the field and configure it incorrectly.\n        \"\"\"\n        self._set_default_and_type()\n        if self.type_.__class__ is ForwardRef or self.type_.__class__ is DeferredType:\n            # self.type_ is currently a ForwardRef and there's nothing we can do now,\n            # user will need to call model.update_forward_refs()\n            return\n\n        self._type_analysis()\n        if self.required is Undefined:\n            self.required = True\n        if self.default is Undefined and self.default_factory is None:\n            self.default = None\n        self.populate_validators()\n\n    def _set_default_and_type(self) -> None:\n        \"\"\"\n        Set the default value, infer the type if needed and check if `None` value is valid.\n        \"\"\"\n        if self.default_factory is not None:\n            if self.type_ is Undefined:\n                raise errors_.ConfigError(\n                    f'you need to set the type of field {self.name!r} when using `default_factory`'\n                )\n            return\n\n        default_value = self.get_default()\n\n        if default_value is not None and self.type_ is Undefined:\n            self.type_ = default_value.__class__\n            self.outer_type_ = self.type_\n            self.annotation = self.type_\n\n        if self.type_ is Undefined:\n            raise errors_.ConfigError(f'unable to infer type for attribute \"{self.name}\"')\n\n        if self.required is False and default_value is None:\n            self.allow_none = True\n\n    def _type_analysis(self) -> None:  # noqa: C901 (ignore complexity)\n        # typing interface is horrible, we have to do some ugly checks\n        if lenient_issubclass(self.type_, JsonWrapper):\n            self.type_ = self.type_.inner_type\n            self.parse_json = True\n        elif lenient_issubclass(self.type_, Json):\n            self.type_ = Any\n            self.parse_json = True\n        elif isinstance(self.type_, TypeVar):\n            if self.type_.__bound__:\n                self.type_ = self.type_.__bound__\n            elif self.type_.__constraints__:\n                self.type_ = Union[self.type_.__constraints__]\n            else:\n                self.type_ = Any\n        elif is_new_type(self.type_):\n            self.type_ = new_type_supertype(self.type_)\n\n        if self.type_ is Any or self.type_ is object:\n            if self.required is Undefined:\n                self.required = False\n            self.allow_none = True\n            return\n        elif self.type_ is Pattern or self.type_ is re.Pattern:\n            # python 3.7 only, Pattern is a typing object but without sub fields\n            return\n        elif is_literal_type(self.type_):\n            return\n        elif is_typeddict(self.type_):\n            return\n\n        if is_finalvar(self.type_):\n            self.final = True\n\n            if self.type_ is Final:\n                self.type_ = Any\n            else:\n                self.type_ = get_args(self.type_)[0]\n\n            self._type_analysis()\n            return\n\n        origin = get_origin(self.type_)\n\n        if origin is Annotated or is_typeddict_special(origin):\n            self.type_ = get_args(self.type_)[0]\n            self._type_analysis()\n            return\n\n        if self.discriminator_key is not None and not is_union(origin):\n            raise TypeError('`discriminator` can only be used with `Union` type with more than one variant')\n\n        # add extra check for `collections.abc.Hashable` for python 3.10+ where origin is not `None`\n        if origin is None or origin is CollectionsHashable:\n            # field is not \"typing\" object eg. Union, Dict, List etc.\n            # allow None for virtual superclasses of NoneType, e.g. Hashable\n            if isinstance(self.type_, type) and isinstance(None, self.type_):\n                self.allow_none = True\n            return\n        elif origin is Callable:\n            return\n        elif is_union(origin):\n            types_ = []\n            for type_ in get_args(self.type_):\n                if is_none_type(type_) or type_ is Any or type_ is object:\n                    if self.required is Undefined:\n                        self.required = False\n                    self.allow_none = True\n                if is_none_type(type_):\n                    continue\n                types_.append(type_)\n\n            if len(types_) == 1:\n                # Optional[]\n                self.type_ = types_[0]\n                # this is the one case where the \"outer type\" isn't just the original type\n                self.outer_type_ = self.type_\n                # re-run to correctly interpret the new self.type_\n                self._type_analysis()\n            else:\n                self.sub_fields = [self._create_sub_type(t, f'{self.name}_{display_as_type(t)}') for t in types_]\n\n                if self.discriminator_key is not None:\n                    self.prepare_discriminated_union_sub_fields()\n            return\n        elif issubclass(origin, Tuple):  # type: ignore\n            # origin == Tuple without item type\n            args = get_args(self.type_)\n            if not args:  # plain tuple\n                self.type_ = Any\n                self.shape = SHAPE_TUPLE_ELLIPSIS\n            elif len(args) == 2 and args[1] is Ellipsis:  # e.g. Tuple[int, ...]\n                self.type_ = args[0]\n                self.shape = SHAPE_TUPLE_ELLIPSIS\n                self.sub_fields = [self._create_sub_type(args[0], f'{self.name}_0')]\n            elif args == ((),):  # Tuple[()] means empty tuple\n                self.shape = SHAPE_TUPLE\n                self.type_ = Any\n                self.sub_fields = []\n            else:\n                self.shape = SHAPE_TUPLE\n                self.sub_fields = [self._create_sub_type(t, f'{self.name}_{i}') for i, t in enumerate(args)]\n            return\n        elif issubclass(origin, List):\n            # Create self validators\n            get_validators = getattr(self.type_, '__get_validators__', None)\n            if get_validators:\n                self.class_validators.update(\n                    {f'list_{i}': Validator(validator, pre=True) for i, validator in enumerate(get_validators())}\n                )\n\n            self.type_ = get_args(self.type_)[0]\n            self.shape = SHAPE_LIST\n        elif issubclass(origin, Set):\n            # Create self validators\n            get_validators = getattr(self.type_, '__get_validators__', None)\n            if get_validators:\n                self.class_validators.update(\n                    {f'set_{i}': Validator(validator, pre=True) for i, validator in enumerate(get_validators())}\n                )\n\n            self.type_ = get_args(self.type_)[0]\n            self.shape = SHAPE_SET\n        elif issubclass(origin, FrozenSet):\n            # Create self validators\n            get_validators = getattr(self.type_, '__get_validators__', None)\n            if get_validators:\n                self.class_validators.update(\n                    {f'frozenset_{i}': Validator(validator, pre=True) for i, validator in enumerate(get_validators())}\n                )\n\n            self.type_ = get_args(self.type_)[0]\n            self.shape = SHAPE_FROZENSET\n        elif issubclass(origin, Deque):\n            self.type_ = get_args(self.type_)[0]\n            self.shape = SHAPE_DEQUE\n        elif issubclass(origin, Sequence):\n            self.type_ = get_args(self.type_)[0]\n            self.shape = SHAPE_SEQUENCE\n        # priority to most common mapping: dict\n        elif origin is dict or origin is Dict:\n            self.key_field = self._create_sub_type(get_args(self.type_)[0], 'key_' + self.name, for_keys=True)\n            self.type_ = get_args(self.type_)[1]\n            self.shape = SHAPE_DICT\n        elif issubclass(origin, DefaultDict):\n            self.key_field = self._create_sub_type(get_args(self.type_)[0], 'key_' + self.name, for_keys=True)\n            self.type_ = get_args(self.type_)[1]\n            self.shape = SHAPE_DEFAULTDICT\n        elif issubclass(origin, Counter):\n            self.key_field = self._create_sub_type(get_args(self.type_)[0], 'key_' + self.name, for_keys=True)\n            self.type_ = int\n            self.shape = SHAPE_COUNTER\n        elif issubclass(origin, Mapping):\n            self.key_field = self._create_sub_type(get_args(self.type_)[0], 'key_' + self.name, for_keys=True)\n            self.type_ = get_args(self.type_)[1]\n            self.shape = SHAPE_MAPPING\n        # Equality check as almost everything inherits form Iterable, including str\n        # check for Iterable and CollectionsIterable, as it could receive one even when declared with the other\n        elif origin in {Iterable, CollectionsIterable}:\n            self.type_ = get_args(self.type_)[0]\n            self.shape = SHAPE_ITERABLE\n            self.sub_fields = [self._create_sub_type(self.type_, f'{self.name}_type')]\n        elif issubclass(origin, Type):  # type: ignore\n            return\n        elif hasattr(origin, '__get_validators__') or self.model_config.arbitrary_types_allowed:\n            # Is a Pydantic-compatible generic that handles itself\n            # or we have arbitrary_types_allowed = True\n            self.shape = SHAPE_GENERIC\n            self.sub_fields = [self._create_sub_type(t, f'{self.name}_{i}') for i, t in enumerate(get_args(self.type_))]\n            self.type_ = origin\n            return\n        else:\n            raise TypeError(f'Fields of type \"{origin}\" are not supported.')\n\n        # type_ has been refined eg. as the type of a List and sub_fields needs to be populated\n        self.sub_fields = [self._create_sub_type(self.type_, '_' + self.name)]\n\n    def prepare_discriminated_union_sub_fields(self) -> None:\n        \"\"\"\n        Prepare the mapping <discriminator key> -> <ModelField> and update `sub_fields`\n        Note that this process can be aborted if a `ForwardRef` is encountered\n        \"\"\"\n        assert self.discriminator_key is not None\n\n        if self.type_.__class__ is DeferredType:\n            return\n\n        assert self.sub_fields is not None\n        sub_fields_mapping: Dict[str, 'ModelField'] = {}\n        all_aliases: Set[str] = set()\n\n        for sub_field in self.sub_fields:\n            t = sub_field.type_\n            if t.__class__ is ForwardRef:\n                # Stopping everything...will need to call `update_forward_refs`\n                return\n\n            alias, discriminator_values = get_discriminator_alias_and_values(t, self.discriminator_key)\n            all_aliases.add(alias)\n            for discriminator_value in discriminator_values:\n                sub_fields_mapping[discriminator_value] = sub_field\n\n        self.sub_fields_mapping = sub_fields_mapping\n        self.discriminator_alias = get_unique_discriminator_alias(all_aliases, self.discriminator_key)\n\n    def _create_sub_type(self, type_: Type[Any], name: str, *, for_keys: bool = False) -> 'ModelField':\n        if for_keys:\n            class_validators = None\n        else:\n            # validators for sub items should not have `each_item` as we want to check only the first sublevel\n            class_validators = {\n                k: Validator(\n                    func=v.func,\n                    pre=v.pre,\n                    each_item=False,\n                    always=v.always,\n                    check_fields=v.check_fields,\n                    skip_on_failure=v.skip_on_failure,\n                )\n                for k, v in self.class_validators.items()\n                if v.each_item\n            }\n\n        field_info, _ = self._get_field_info(name, type_, None, self.model_config)\n\n        return self.__class__(\n            type_=type_,\n            name=name,\n            class_validators=class_validators,\n            model_config=self.model_config,\n            field_info=field_info,\n        )\n\n    def populate_validators(self) -> None:\n        \"\"\"\n        Prepare self.pre_validators, self.validators, and self.post_validators based on self.type_'s  __get_validators__\n        and class validators. This method should be idempotent, e.g. it should be safe to call multiple times\n        without mis-configuring the field.\n        \"\"\"\n        self.validate_always = getattr(self.type_, 'validate_always', False) or any(\n            v.always for v in self.class_validators.values()\n        )\n\n        class_validators_ = self.class_validators.values()\n        if not self.sub_fields or self.shape == SHAPE_GENERIC:\n            get_validators = getattr(self.type_, '__get_validators__', None)\n            v_funcs = (\n                *[v.func for v in class_validators_ if v.each_item and v.pre],\n                *(get_validators() if get_validators else list(find_validators(self.type_, self.model_config))),\n                *[v.func for v in class_validators_ if v.each_item and not v.pre],\n            )\n            self.validators = prep_validators(v_funcs)\n\n        self.pre_validators = []\n        self.post_validators = []\n\n        if self.field_info and self.field_info.const:\n            self.post_validators.append(make_generic_validator(constant_validator))\n\n        if class_validators_:\n            self.pre_validators += prep_validators(v.func for v in class_validators_ if not v.each_item and v.pre)\n            self.post_validators += prep_validators(v.func for v in class_validators_ if not v.each_item and not v.pre)\n\n        if self.parse_json:\n            self.pre_validators.append(make_generic_validator(validate_json))\n\n        self.pre_validators = self.pre_validators or None\n        self.post_validators = self.post_validators or None\n\n    def validate(\n        self, v: Any, values: Dict[str, Any], *, loc: 'LocStr', cls: Optional['ModelOrDc'] = None\n    ) -> 'ValidateReturn':\n        assert self.type_.__class__ is not DeferredType\n\n        if self.type_.__class__ is ForwardRef:\n            assert cls is not None\n            raise ConfigError(\n                f'field \"{self.name}\" not yet prepared so type is still a ForwardRef, '\n                f'you might need to call {cls.__name__}.update_forward_refs().'\n            )\n\n        errors: Optional['ErrorList']\n        if self.pre_validators:\n            v, errors = self._apply_validators(v, values, loc, cls, self.pre_validators)\n            if errors:\n                return v, errors\n\n        if v is None:\n            if is_none_type(self.type_):\n                # keep validating\n                pass\n            elif self.allow_none:\n                if self.post_validators:\n                    return self._apply_validators(v, values, loc, cls, self.post_validators)\n                else:\n                    return None, None\n            else:\n                return v, ErrorWrapper(NoneIsNotAllowedError(), loc)\n\n        if self.shape == SHAPE_SINGLETON:\n            v, errors = self._validate_singleton(v, values, loc, cls)\n        elif self.shape in MAPPING_LIKE_SHAPES:\n            v, errors = self._validate_mapping_like(v, values, loc, cls)\n        elif self.shape == SHAPE_TUPLE:\n            v, errors = self._validate_tuple(v, values, loc, cls)\n        elif self.shape == SHAPE_ITERABLE:\n            v, errors = self._validate_iterable(v, values, loc, cls)\n        elif self.shape == SHAPE_GENERIC:\n            v, errors = self._apply_validators(v, values, loc, cls, self.validators)\n        else:\n            #  sequence, list, set, generator, tuple with ellipsis, frozen set\n            v, errors = self._validate_sequence_like(v, values, loc, cls)\n\n        if not errors and self.post_validators:\n            v, errors = self._apply_validators(v, values, loc, cls, self.post_validators)\n        return v, errors\n\n    def _validate_sequence_like(  # noqa: C901 (ignore complexity)\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        \"\"\"\n        Validate sequence-like containers: lists, tuples, sets and generators\n        Note that large if-else blocks are necessary to enable Cython\n        optimization, which is why we disable the complexity check above.\n        \"\"\"\n        if not sequence_like(v):\n            e: errors_.PydanticTypeError\n            if self.shape == SHAPE_LIST:\n                e = errors_.ListError()\n            elif self.shape in (SHAPE_TUPLE, SHAPE_TUPLE_ELLIPSIS):\n                e = errors_.TupleError()\n            elif self.shape == SHAPE_SET:\n                e = errors_.SetError()\n            elif self.shape == SHAPE_FROZENSET:\n                e = errors_.FrozenSetError()\n            else:\n                e = errors_.SequenceError()\n            return v, ErrorWrapper(e, loc)\n\n        loc = loc if isinstance(loc, tuple) else (loc,)\n        result = []\n        errors: List[ErrorList] = []\n        for i, v_ in enumerate(v):\n            v_loc = *loc, i\n            r, ee = self._validate_singleton(v_, values, v_loc, cls)\n            if ee:\n                errors.append(ee)\n            else:\n                result.append(r)\n\n        if errors:\n            return v, errors\n\n        converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n\n        if self.shape == SHAPE_SET:\n            converted = set(result)\n        elif self.shape == SHAPE_FROZENSET:\n            converted = frozenset(result)\n        elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n            converted = tuple(result)\n        elif self.shape == SHAPE_DEQUE:\n            converted = deque(result, maxlen=getattr(v, 'maxlen', None))\n        elif self.shape == SHAPE_SEQUENCE:\n            if isinstance(v, tuple):\n                converted = tuple(result)\n            elif isinstance(v, set):\n                converted = set(result)\n            elif isinstance(v, Generator):\n                converted = iter(result)\n            elif isinstance(v, deque):\n                converted = deque(result, maxlen=getattr(v, 'maxlen', None))\n        return converted, None\n\n    def _validate_iterable(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        \"\"\"\n        Validate Iterables.\n\n        This intentionally doesn't validate values to allow infinite generators.\n        \"\"\"\n\n        try:\n            iterable = iter(v)\n        except TypeError:\n            return v, ErrorWrapper(errors_.IterableError(), loc)\n        return iterable, None\n\n    def _validate_tuple(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        e: Optional[Exception] = None\n        if not sequence_like(v):\n            e = errors_.TupleError()\n        else:\n            actual_length, expected_length = len(v), len(self.sub_fields)  # type: ignore\n            if actual_length != expected_length:\n                e = errors_.TupleLengthError(actual_length=actual_length, expected_length=expected_length)\n\n        if e:\n            return v, ErrorWrapper(e, loc)\n\n        loc = loc if isinstance(loc, tuple) else (loc,)\n        result = []\n        errors: List[ErrorList] = []\n        for i, (v_, field) in enumerate(zip(v, self.sub_fields)):  # type: ignore\n            v_loc = *loc, i\n            r, ee = field.validate(v_, values, loc=v_loc, cls=cls)\n            if ee:\n                errors.append(ee)\n            else:\n                result.append(r)\n\n        if errors:\n            return v, errors\n        else:\n            return tuple(result), None\n\n    def _validate_mapping_like(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        try:\n            v_iter = dict_validator(v)\n        except TypeError as exc:\n            return v, ErrorWrapper(exc, loc)\n\n        loc = loc if isinstance(loc, tuple) else (loc,)\n        result, errors = {}, []\n        for k, v_ in v_iter.items():\n            v_loc = *loc, '__key__'\n            key_result, key_errors = self.key_field.validate(k, values, loc=v_loc, cls=cls)  # type: ignore\n            if key_errors:\n                errors.append(key_errors)\n                continue\n\n            v_loc = *loc, k\n            value_result, value_errors = self._validate_singleton(v_, values, v_loc, cls)\n            if value_errors:\n                errors.append(value_errors)\n                continue\n\n            result[key_result] = value_result\n        if errors:\n            return v, errors\n        elif self.shape == SHAPE_DICT:\n            return result, None\n        elif self.shape == SHAPE_DEFAULTDICT:\n            return defaultdict(self.type_, result), None\n        elif self.shape == SHAPE_COUNTER:\n            return CollectionCounter(result), None\n        else:\n            return self._get_mapping_value(v, result), None\n\n    def _get_mapping_value(self, original: T, converted: Dict[Any, Any]) -> Union[T, Dict[Any, Any]]:\n        \"\"\"\n        When type is `Mapping[KT, KV]` (or another unsupported mapping), we try to avoid\n        coercing to `dict` unwillingly.\n        \"\"\"\n        original_cls = original.__class__\n\n        if original_cls == dict or original_cls == Dict:\n            return converted\n        elif original_cls in {defaultdict, DefaultDict}:\n            return defaultdict(self.type_, converted)\n        else:\n            try:\n                # Counter, OrderedDict, UserDict, ...\n                return original_cls(converted)  # type: ignore\n            except TypeError:\n                raise RuntimeError(f'Could not convert dictionary to {original_cls.__name__!r}') from None\n\n    def _validate_singleton(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        if self.sub_fields:\n            if self.discriminator_key is not None:\n                return self._validate_discriminated_union(v, values, loc, cls)\n\n            errors = []\n\n            if self.model_config.smart_union and is_union(get_origin(self.type_)):\n                # 1st pass: check if the value is an exact instance of one of the Union types\n                # (e.g. to avoid coercing a bool into an int)\n                for field in self.sub_fields:\n                    if v.__class__ is field.outer_type_:\n                        return v, None\n\n                # 2nd pass: check if the value is an instance of any subclass of the Union types\n                for field in self.sub_fields:\n                    # This whole logic will be improved later on to support more complex `isinstance` checks\n                    # It will probably be done once a strict mode is added and be something like:\n                    # ```\n                    #     value, error = field.validate(v, values, strict=True)\n                    #     if error is None:\n                    #         return value, None\n                    # ```\n                    try:\n                        if isinstance(v, field.outer_type_):\n                            return v, None\n                    except TypeError:\n                        # compound type\n                        if lenient_isinstance(v, get_origin(field.outer_type_)):\n                            value, error = field.validate(v, values, loc=loc, cls=cls)\n                            if not error:\n                                return value, None\n\n            # 1st pass by default or 3rd pass with `smart_union` enabled:\n            # check if the value can be coerced into one of the Union types\n            for field in self.sub_fields:\n                value, error = field.validate(v, values, loc=loc, cls=cls)\n                if error:\n                    errors.append(error)\n                else:\n                    return value, None\n            return v, errors\n        else:\n            return self._apply_validators(v, values, loc, cls, self.validators)\n\n    def _validate_discriminated_union(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        assert self.discriminator_key is not None\n        assert self.discriminator_alias is not None\n\n        try:\n            try:\n                discriminator_value = v[self.discriminator_alias]\n            except KeyError:\n                if self.model_config.allow_population_by_field_name:\n                    discriminator_value = v[self.discriminator_key]\n                else:\n                    raise\n        except KeyError:\n            return v, ErrorWrapper(MissingDiscriminator(discriminator_key=self.discriminator_key), loc)\n        except TypeError:\n            try:\n                # BaseModel or dataclass\n                discriminator_value = getattr(v, self.discriminator_key)\n            except (AttributeError, TypeError):\n                return v, ErrorWrapper(MissingDiscriminator(discriminator_key=self.discriminator_key), loc)\n\n        if self.sub_fields_mapping is None:\n            assert cls is not None\n            raise ConfigError(\n                f'field \"{self.name}\" not yet prepared so type is still a ForwardRef, '\n                f'you might need to call {cls.__name__}.update_forward_refs().'\n            )\n\n        try:\n            sub_field = self.sub_fields_mapping[discriminator_value]\n        except (KeyError, TypeError):\n            # KeyError: `discriminator_value` is not in the dictionary.\n            # TypeError: `discriminator_value` is unhashable.\n            assert self.sub_fields_mapping is not None\n            return v, ErrorWrapper(\n                InvalidDiscriminator(\n                    discriminator_key=self.discriminator_key,\n                    discriminator_value=discriminator_value,\n                    allowed_values=list(self.sub_fields_mapping),\n                ),\n                loc,\n            )\n        else:\n            if not isinstance(loc, tuple):\n                loc = (loc,)\n            return sub_field.validate(v, values, loc=(*loc, display_as_type(sub_field.type_)), cls=cls)\n\n    def _apply_validators(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc'], validators: 'ValidatorsList'\n    ) -> 'ValidateReturn':\n        for validator in validators:\n            try:\n                v = validator(cls, v, values, self, self.model_config)\n            except (ValueError, TypeError, AssertionError) as exc:\n                return v, ErrorWrapper(exc, loc)\n        return v, None\n\n    def is_complex(self) -> bool:\n        \"\"\"\n        Whether the field is \"complex\" eg. env variables should be parsed as JSON.\n        \"\"\"\n        from pydantic.v1.main import BaseModel\n\n        return (\n            self.shape != SHAPE_SINGLETON\n            or hasattr(self.type_, '__pydantic_model__')\n            or lenient_issubclass(self.type_, (BaseModel, list, set, frozenset, dict))\n        )\n\n    def _type_display(self) -> PyObjectStr:\n        t = display_as_type(self.type_)\n\n        if self.shape in MAPPING_LIKE_SHAPES:\n            t = f'Mapping[{display_as_type(self.key_field.type_)}, {t}]'  # type: ignore\n        elif self.shape == SHAPE_TUPLE:\n            t = 'Tuple[{}]'.format(', '.join(display_as_type(f.type_) for f in self.sub_fields))  # type: ignore\n        elif self.shape == SHAPE_GENERIC:\n            assert self.sub_fields\n            t = '{}[{}]'.format(\n                display_as_type(self.type_), ', '.join(display_as_type(f.type_) for f in self.sub_fields)\n            )\n        elif self.shape != SHAPE_SINGLETON:\n            t = SHAPE_NAME_LOOKUP[self.shape].format(t)\n\n        if self.allow_none and (self.shape != SHAPE_SINGLETON or not self.sub_fields):\n            t = f'Optional[{t}]'\n        return PyObjectStr(t)\n\n    def __repr_args__(self) -> 'ReprArgs':\n        args = [('name', self.name), ('type', self._type_display()), ('required', self.required)]\n\n        if not self.required:\n            if self.default_factory is not None:\n                args.append(('default_factory', f'<function {self.default_factory.__name__}>'))\n            else:\n                args.append(('default', self.default))\n\n        if self.alt_alias:\n            args.append(('alias', self.alias))\n        return args\n\n\nclass ModelPrivateAttr(Representation):\n    __slots__ = ('default', 'default_factory')\n\n    def __init__(self, default: Any = Undefined, *, default_factory: Optional[NoArgAnyCallable] = None) -> None:\n        self.default = default\n        self.default_factory = default_factory\n\n    def get_default(self) -> Any:\n        return smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, self.__class__) and (self.default, self.default_factory) == (\n            other.default,\n            other.default_factory,\n        )\n\n\ndef PrivateAttr(\n    default: Any = Undefined,\n    *,\n    default_factory: Optional[NoArgAnyCallable] = None,\n) -> Any:\n    \"\"\"\n    Indicates that attribute is only used internally and never mixed with regular fields.\n\n    Types or values of private attrs are not checked by pydantic and it's up to you to keep them relevant.\n\n    Private attrs are stored in model __slots__.\n\n    :param default: the attribute\u2019s default value\n    :param default_factory: callable that will be called when a default value is needed for this attribute\n      If both `default` and `default_factory` are set, an error is raised.\n    \"\"\"\n    if default is not Undefined and default_factory is not None:\n        raise ValueError('cannot specify both default and default_factory')\n\n    return ModelPrivateAttr(\n        default,\n        default_factory=default_factory,\n    )\n\n\nclass DeferredType:\n    \"\"\"\n    Used to postpone field preparation, while creating recursive generic models.\n    \"\"\"\n\n\ndef is_finalvar_with_default_val(type_: Type[Any], val: Any) -> bool:\n    return is_finalvar(type_) and val is not Undefined and not isinstance(val, FieldInfo)\n", "pydantic/v1/version.py": "__all__ = 'compiled', 'VERSION', 'version_info'\n\nVERSION = '1.10.17'\n\ntry:\n    import cython  # type: ignore\nexcept ImportError:\n    compiled: bool = False\nelse:  # pragma: no cover\n    try:\n        compiled = cython.compiled\n    except AttributeError:\n        compiled = False\n\n\ndef version_info() -> str:\n    import platform\n    import sys\n    from importlib import import_module\n    from pathlib import Path\n\n    optional_deps = []\n    for p in ('devtools', 'dotenv', 'email-validator', 'typing-extensions'):\n        try:\n            import_module(p.replace('-', '_'))\n        except ImportError:\n            continue\n        optional_deps.append(p)\n\n    info = {\n        'pydantic version': VERSION,\n        'pydantic compiled': compiled,\n        'install path': Path(__file__).resolve().parent,\n        'python version': sys.version,\n        'platform': platform.platform(),\n        'optional deps. installed': optional_deps,\n    }\n    return '\\n'.join('{:>30} {}'.format(k + ':', str(v).replace('\\n', ' ')) for k, v in info.items())\n", "pydantic/v1/tools.py": "import json\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, Optional, Type, TypeVar, Union\n\nfrom pydantic.v1.parse import Protocol, load_file, load_str_bytes\nfrom pydantic.v1.types import StrBytes\nfrom pydantic.v1.typing import display_as_type\n\n__all__ = ('parse_file_as', 'parse_obj_as', 'parse_raw_as', 'schema_of', 'schema_json_of')\n\nNameFactory = Union[str, Callable[[Type[Any]], str]]\n\nif TYPE_CHECKING:\n    from pydantic.v1.typing import DictStrAny\n\n\ndef _generate_parsing_type_name(type_: Any) -> str:\n    return f'ParsingModel[{display_as_type(type_)}]'\n\n\n@lru_cache(maxsize=2048)\ndef _get_parsing_type(type_: Any, *, type_name: Optional[NameFactory] = None) -> Any:\n    from pydantic.v1.main import create_model\n\n    if type_name is None:\n        type_name = _generate_parsing_type_name\n    if not isinstance(type_name, str):\n        type_name = type_name(type_)\n    return create_model(type_name, __root__=(type_, ...))\n\n\nT = TypeVar('T')\n\n\ndef parse_obj_as(type_: Type[T], obj: Any, *, type_name: Optional[NameFactory] = None) -> T:\n    model_type = _get_parsing_type(type_, type_name=type_name)  # type: ignore[arg-type]\n    return model_type(__root__=obj).__root__\n\n\ndef parse_file_as(\n    type_: Type[T],\n    path: Union[str, Path],\n    *,\n    content_type: str = None,\n    encoding: str = 'utf8',\n    proto: Protocol = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n    type_name: Optional[NameFactory] = None,\n) -> T:\n    obj = load_file(\n        path,\n        proto=proto,\n        content_type=content_type,\n        encoding=encoding,\n        allow_pickle=allow_pickle,\n        json_loads=json_loads,\n    )\n    return parse_obj_as(type_, obj, type_name=type_name)\n\n\ndef parse_raw_as(\n    type_: Type[T],\n    b: StrBytes,\n    *,\n    content_type: str = None,\n    encoding: str = 'utf8',\n    proto: Protocol = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n    type_name: Optional[NameFactory] = None,\n) -> T:\n    obj = load_str_bytes(\n        b,\n        proto=proto,\n        content_type=content_type,\n        encoding=encoding,\n        allow_pickle=allow_pickle,\n        json_loads=json_loads,\n    )\n    return parse_obj_as(type_, obj, type_name=type_name)\n\n\ndef schema_of(type_: Any, *, title: Optional[NameFactory] = None, **schema_kwargs: Any) -> 'DictStrAny':\n    \"\"\"Generate a JSON schema (as dict) for the passed model or dynamically generated one\"\"\"\n    return _get_parsing_type(type_, type_name=title).schema(**schema_kwargs)\n\n\ndef schema_json_of(type_: Any, *, title: Optional[NameFactory] = None, **schema_json_kwargs: Any) -> str:\n    \"\"\"Generate a JSON schema (as JSON) for the passed model or dynamically generated one\"\"\"\n    return _get_parsing_type(type_, type_name=title).schema_json(**schema_json_kwargs)\n", "pydantic/v1/error_wrappers.py": "import json\nfrom typing import TYPE_CHECKING, Any, Dict, Generator, List, Optional, Sequence, Tuple, Type, Union\n\nfrom pydantic.v1.json import pydantic_encoder\nfrom pydantic.v1.utils import Representation\n\nif TYPE_CHECKING:\n    from typing_extensions import TypedDict\n\n    from pydantic.v1.config import BaseConfig\n    from pydantic.v1.types import ModelOrDc\n    from pydantic.v1.typing import ReprArgs\n\n    Loc = Tuple[Union[int, str], ...]\n\n    class _ErrorDictRequired(TypedDict):\n        loc: Loc\n        msg: str\n        type: str\n\n    class ErrorDict(_ErrorDictRequired, total=False):\n        ctx: Dict[str, Any]\n\n\n__all__ = 'ErrorWrapper', 'ValidationError'\n\n\nclass ErrorWrapper(Representation):\n    __slots__ = 'exc', '_loc'\n\n    def __init__(self, exc: Exception, loc: Union[str, 'Loc']) -> None:\n        self.exc = exc\n        self._loc = loc\n\n    def loc_tuple(self) -> 'Loc':\n        if isinstance(self._loc, tuple):\n            return self._loc\n        else:\n            return (self._loc,)\n\n    def __repr_args__(self) -> 'ReprArgs':\n        return [('exc', self.exc), ('loc', self.loc_tuple())]\n\n\n# ErrorList is something like Union[List[Union[List[ErrorWrapper], ErrorWrapper]], ErrorWrapper]\n# but recursive, therefore just use:\nErrorList = Union[Sequence[Any], ErrorWrapper]\n\n\nclass ValidationError(Representation, ValueError):\n    __slots__ = 'raw_errors', 'model', '_error_cache'\n\n    def __init__(self, errors: Sequence[ErrorList], model: 'ModelOrDc') -> None:\n        self.raw_errors = errors\n        self.model = model\n        self._error_cache: Optional[List['ErrorDict']] = None\n\n    def errors(self) -> List['ErrorDict']:\n        if self._error_cache is None:\n            try:\n                config = self.model.__config__  # type: ignore\n            except AttributeError:\n                config = self.model.__pydantic_model__.__config__  # type: ignore\n            self._error_cache = list(flatten_errors(self.raw_errors, config))\n        return self._error_cache\n\n    def json(self, *, indent: Union[None, int, str] = 2) -> str:\n        return json.dumps(self.errors(), indent=indent, default=pydantic_encoder)\n\n    def __str__(self) -> str:\n        errors = self.errors()\n        no_errors = len(errors)\n        return (\n            f'{no_errors} validation error{\"\" if no_errors == 1 else \"s\"} for {self.model.__name__}\\n'\n            f'{display_errors(errors)}'\n        )\n\n    def __repr_args__(self) -> 'ReprArgs':\n        return [('model', self.model.__name__), ('errors', self.errors())]\n\n\ndef display_errors(errors: List['ErrorDict']) -> str:\n    return '\\n'.join(f'{_display_error_loc(e)}\\n  {e[\"msg\"]} ({_display_error_type_and_ctx(e)})' for e in errors)\n\n\ndef _display_error_loc(error: 'ErrorDict') -> str:\n    return ' -> '.join(str(e) for e in error['loc'])\n\n\ndef _display_error_type_and_ctx(error: 'ErrorDict') -> str:\n    t = 'type=' + error['type']\n    ctx = error.get('ctx')\n    if ctx:\n        return t + ''.join(f'; {k}={v}' for k, v in ctx.items())\n    else:\n        return t\n\n\ndef flatten_errors(\n    errors: Sequence[Any], config: Type['BaseConfig'], loc: Optional['Loc'] = None\n) -> Generator['ErrorDict', None, None]:\n    for error in errors:\n        if isinstance(error, ErrorWrapper):\n            if loc:\n                error_loc = loc + error.loc_tuple()\n            else:\n                error_loc = error.loc_tuple()\n\n            if isinstance(error.exc, ValidationError):\n                yield from flatten_errors(error.exc.raw_errors, config, error_loc)\n            else:\n                yield error_dict(error.exc, config, error_loc)\n        elif isinstance(error, list):\n            yield from flatten_errors(error, config, loc=loc)\n        else:\n            raise RuntimeError(f'Unknown error object: {error}')\n\n\ndef error_dict(exc: Exception, config: Type['BaseConfig'], loc: 'Loc') -> 'ErrorDict':\n    type_ = get_exc_type(exc.__class__)\n    msg_template = config.error_msg_templates.get(type_) or getattr(exc, 'msg_template', None)\n    ctx = exc.__dict__\n    if msg_template:\n        msg = msg_template.format(**ctx)\n    else:\n        msg = str(exc)\n\n    d: 'ErrorDict' = {'loc': loc, 'msg': msg, 'type': type_}\n\n    if ctx:\n        d['ctx'] = ctx\n\n    return d\n\n\n_EXC_TYPE_CACHE: Dict[Type[Exception], str] = {}\n\n\ndef get_exc_type(cls: Type[Exception]) -> str:\n    # slightly more efficient than using lru_cache since we don't need to worry about the cache filling up\n    try:\n        return _EXC_TYPE_CACHE[cls]\n    except KeyError:\n        r = _get_exc_type(cls)\n        _EXC_TYPE_CACHE[cls] = r\n        return r\n\n\ndef _get_exc_type(cls: Type[Exception]) -> str:\n    if issubclass(cls, AssertionError):\n        return 'assertion_error'\n\n    base_name = 'type_error' if issubclass(cls, TypeError) else 'value_error'\n    if cls in (TypeError, ValueError):\n        # just TypeError or ValueError, no extra code\n        return base_name\n\n    # if it's not a TypeError or ValueError, we just take the lowercase of the exception name\n    # no chaining or snake case logic, use \"code\" for more complex error types.\n    code = getattr(cls, 'code', None) or cls.__name__.replace('Error', '').lower()\n    return base_name + '.' + code\n", "pydantic/v1/errors.py": "from decimal import Decimal\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, Sequence, Set, Tuple, Type, Union\n\nfrom pydantic.v1.typing import display_as_type\n\nif TYPE_CHECKING:\n    from pydantic.v1.typing import DictStrAny\n\n# explicitly state exports to avoid \"from pydantic.v1.errors import *\" also importing Decimal, Path etc.\n__all__ = (\n    'PydanticTypeError',\n    'PydanticValueError',\n    'ConfigError',\n    'MissingError',\n    'ExtraError',\n    'NoneIsNotAllowedError',\n    'NoneIsAllowedError',\n    'WrongConstantError',\n    'NotNoneError',\n    'BoolError',\n    'BytesError',\n    'DictError',\n    'EmailError',\n    'UrlError',\n    'UrlSchemeError',\n    'UrlSchemePermittedError',\n    'UrlUserInfoError',\n    'UrlHostError',\n    'UrlHostTldError',\n    'UrlPortError',\n    'UrlExtraError',\n    'EnumError',\n    'IntEnumError',\n    'EnumMemberError',\n    'IntegerError',\n    'FloatError',\n    'PathError',\n    'PathNotExistsError',\n    'PathNotAFileError',\n    'PathNotADirectoryError',\n    'PyObjectError',\n    'SequenceError',\n    'ListError',\n    'SetError',\n    'FrozenSetError',\n    'TupleError',\n    'TupleLengthError',\n    'ListMinLengthError',\n    'ListMaxLengthError',\n    'ListUniqueItemsError',\n    'SetMinLengthError',\n    'SetMaxLengthError',\n    'FrozenSetMinLengthError',\n    'FrozenSetMaxLengthError',\n    'AnyStrMinLengthError',\n    'AnyStrMaxLengthError',\n    'StrError',\n    'StrRegexError',\n    'NumberNotGtError',\n    'NumberNotGeError',\n    'NumberNotLtError',\n    'NumberNotLeError',\n    'NumberNotMultipleError',\n    'DecimalError',\n    'DecimalIsNotFiniteError',\n    'DecimalMaxDigitsError',\n    'DecimalMaxPlacesError',\n    'DecimalWholeDigitsError',\n    'DateTimeError',\n    'DateError',\n    'DateNotInThePastError',\n    'DateNotInTheFutureError',\n    'TimeError',\n    'DurationError',\n    'HashableError',\n    'UUIDError',\n    'UUIDVersionError',\n    'ArbitraryTypeError',\n    'ClassError',\n    'SubclassError',\n    'JsonError',\n    'JsonTypeError',\n    'PatternError',\n    'DataclassTypeError',\n    'CallableError',\n    'IPvAnyAddressError',\n    'IPvAnyInterfaceError',\n    'IPvAnyNetworkError',\n    'IPv4AddressError',\n    'IPv6AddressError',\n    'IPv4NetworkError',\n    'IPv6NetworkError',\n    'IPv4InterfaceError',\n    'IPv6InterfaceError',\n    'ColorError',\n    'StrictBoolError',\n    'NotDigitError',\n    'LuhnValidationError',\n    'InvalidLengthForBrand',\n    'InvalidByteSize',\n    'InvalidByteSizeUnit',\n    'MissingDiscriminator',\n    'InvalidDiscriminator',\n)\n\n\ndef cls_kwargs(cls: Type['PydanticErrorMixin'], ctx: 'DictStrAny') -> 'PydanticErrorMixin':\n    \"\"\"\n    For built-in exceptions like ValueError or TypeError, we need to implement\n    __reduce__ to override the default behaviour (instead of __getstate__/__setstate__)\n    By default pickle protocol 2 calls `cls.__new__(cls, *args)`.\n    Since we only use kwargs, we need a little constructor to change that.\n    Note: the callable can't be a lambda as pickle looks in the namespace to find it\n    \"\"\"\n    return cls(**ctx)\n\n\nclass PydanticErrorMixin:\n    code: str\n    msg_template: str\n\n    def __init__(self, **ctx: Any) -> None:\n        self.__dict__ = ctx\n\n    def __str__(self) -> str:\n        return self.msg_template.format(**self.__dict__)\n\n    def __reduce__(self) -> Tuple[Callable[..., 'PydanticErrorMixin'], Tuple[Type['PydanticErrorMixin'], 'DictStrAny']]:\n        return cls_kwargs, (self.__class__, self.__dict__)\n\n\nclass PydanticTypeError(PydanticErrorMixin, TypeError):\n    pass\n\n\nclass PydanticValueError(PydanticErrorMixin, ValueError):\n    pass\n\n\nclass ConfigError(RuntimeError):\n    pass\n\n\nclass MissingError(PydanticValueError):\n    msg_template = 'field required'\n\n\nclass ExtraError(PydanticValueError):\n    msg_template = 'extra fields not permitted'\n\n\nclass NoneIsNotAllowedError(PydanticTypeError):\n    code = 'none.not_allowed'\n    msg_template = 'none is not an allowed value'\n\n\nclass NoneIsAllowedError(PydanticTypeError):\n    code = 'none.allowed'\n    msg_template = 'value is not none'\n\n\nclass WrongConstantError(PydanticValueError):\n    code = 'const'\n\n    def __str__(self) -> str:\n        permitted = ', '.join(repr(v) for v in self.permitted)  # type: ignore\n        return f'unexpected value; permitted: {permitted}'\n\n\nclass NotNoneError(PydanticTypeError):\n    code = 'not_none'\n    msg_template = 'value is not None'\n\n\nclass BoolError(PydanticTypeError):\n    msg_template = 'value could not be parsed to a boolean'\n\n\nclass BytesError(PydanticTypeError):\n    msg_template = 'byte type expected'\n\n\nclass DictError(PydanticTypeError):\n    msg_template = 'value is not a valid dict'\n\n\nclass EmailError(PydanticValueError):\n    msg_template = 'value is not a valid email address'\n\n\nclass UrlError(PydanticValueError):\n    code = 'url'\n\n\nclass UrlSchemeError(UrlError):\n    code = 'url.scheme'\n    msg_template = 'invalid or missing URL scheme'\n\n\nclass UrlSchemePermittedError(UrlError):\n    code = 'url.scheme'\n    msg_template = 'URL scheme not permitted'\n\n    def __init__(self, allowed_schemes: Set[str]):\n        super().__init__(allowed_schemes=allowed_schemes)\n\n\nclass UrlUserInfoError(UrlError):\n    code = 'url.userinfo'\n    msg_template = 'userinfo required in URL but missing'\n\n\nclass UrlHostError(UrlError):\n    code = 'url.host'\n    msg_template = 'URL host invalid'\n\n\nclass UrlHostTldError(UrlError):\n    code = 'url.host'\n    msg_template = 'URL host invalid, top level domain required'\n\n\nclass UrlPortError(UrlError):\n    code = 'url.port'\n    msg_template = 'URL port invalid, port cannot exceed 65535'\n\n\nclass UrlExtraError(UrlError):\n    code = 'url.extra'\n    msg_template = 'URL invalid, extra characters found after valid URL: {extra!r}'\n\n\nclass EnumMemberError(PydanticTypeError):\n    code = 'enum'\n\n    def __str__(self) -> str:\n        permitted = ', '.join(repr(v.value) for v in self.enum_values)  # type: ignore\n        return f'value is not a valid enumeration member; permitted: {permitted}'\n\n\nclass IntegerError(PydanticTypeError):\n    msg_template = 'value is not a valid integer'\n\n\nclass FloatError(PydanticTypeError):\n    msg_template = 'value is not a valid float'\n\n\nclass PathError(PydanticTypeError):\n    msg_template = 'value is not a valid path'\n\n\nclass _PathValueError(PydanticValueError):\n    def __init__(self, *, path: Path) -> None:\n        super().__init__(path=str(path))\n\n\nclass PathNotExistsError(_PathValueError):\n    code = 'path.not_exists'\n    msg_template = 'file or directory at path \"{path}\" does not exist'\n\n\nclass PathNotAFileError(_PathValueError):\n    code = 'path.not_a_file'\n    msg_template = 'path \"{path}\" does not point to a file'\n\n\nclass PathNotADirectoryError(_PathValueError):\n    code = 'path.not_a_directory'\n    msg_template = 'path \"{path}\" does not point to a directory'\n\n\nclass PyObjectError(PydanticTypeError):\n    msg_template = 'ensure this value contains valid import path or valid callable: {error_message}'\n\n\nclass SequenceError(PydanticTypeError):\n    msg_template = 'value is not a valid sequence'\n\n\nclass IterableError(PydanticTypeError):\n    msg_template = 'value is not a valid iterable'\n\n\nclass ListError(PydanticTypeError):\n    msg_template = 'value is not a valid list'\n\n\nclass SetError(PydanticTypeError):\n    msg_template = 'value is not a valid set'\n\n\nclass FrozenSetError(PydanticTypeError):\n    msg_template = 'value is not a valid frozenset'\n\n\nclass DequeError(PydanticTypeError):\n    msg_template = 'value is not a valid deque'\n\n\nclass TupleError(PydanticTypeError):\n    msg_template = 'value is not a valid tuple'\n\n\nclass TupleLengthError(PydanticValueError):\n    code = 'tuple.length'\n    msg_template = 'wrong tuple length {actual_length}, expected {expected_length}'\n\n    def __init__(self, *, actual_length: int, expected_length: int) -> None:\n        super().__init__(actual_length=actual_length, expected_length=expected_length)\n\n\nclass ListMinLengthError(PydanticValueError):\n    code = 'list.min_items'\n    msg_template = 'ensure this value has at least {limit_value} items'\n\n    def __init__(self, *, limit_value: int) -> None:\n        super().__init__(limit_value=limit_value)\n\n\nclass ListMaxLengthError(PydanticValueError):\n    code = 'list.max_items'\n    msg_template = 'ensure this value has at most {limit_value} items'\n\n    def __init__(self, *, limit_value: int) -> None:\n        super().__init__(limit_value=limit_value)\n\n\nclass ListUniqueItemsError(PydanticValueError):\n    code = 'list.unique_items'\n    msg_template = 'the list has duplicated items'\n\n\nclass SetMinLengthError(PydanticValueError):\n    code = 'set.min_items'\n    msg_template = 'ensure this value has at least {limit_value} items'\n\n    def __init__(self, *, limit_value: int) -> None:\n        super().__init__(limit_value=limit_value)\n\n\nclass SetMaxLengthError(PydanticValueError):\n    code = 'set.max_items'\n    msg_template = 'ensure this value has at most {limit_value} items'\n\n    def __init__(self, *, limit_value: int) -> None:\n        super().__init__(limit_value=limit_value)\n\n\nclass FrozenSetMinLengthError(PydanticValueError):\n    code = 'frozenset.min_items'\n    msg_template = 'ensure this value has at least {limit_value} items'\n\n    def __init__(self, *, limit_value: int) -> None:\n        super().__init__(limit_value=limit_value)\n\n\nclass FrozenSetMaxLengthError(PydanticValueError):\n    code = 'frozenset.max_items'\n    msg_template = 'ensure this value has at most {limit_value} items'\n\n    def __init__(self, *, limit_value: int) -> None:\n        super().__init__(limit_value=limit_value)\n\n\nclass AnyStrMinLengthError(PydanticValueError):\n    code = 'any_str.min_length'\n    msg_template = 'ensure this value has at least {limit_value} characters'\n\n    def __init__(self, *, limit_value: int) -> None:\n        super().__init__(limit_value=limit_value)\n\n\nclass AnyStrMaxLengthError(PydanticValueError):\n    code = 'any_str.max_length'\n    msg_template = 'ensure this value has at most {limit_value} characters'\n\n    def __init__(self, *, limit_value: int) -> None:\n        super().__init__(limit_value=limit_value)\n\n\nclass StrError(PydanticTypeError):\n    msg_template = 'str type expected'\n\n\nclass StrRegexError(PydanticValueError):\n    code = 'str.regex'\n    msg_template = 'string does not match regex \"{pattern}\"'\n\n    def __init__(self, *, pattern: str) -> None:\n        super().__init__(pattern=pattern)\n\n\nclass _NumberBoundError(PydanticValueError):\n    def __init__(self, *, limit_value: Union[int, float, Decimal]) -> None:\n        super().__init__(limit_value=limit_value)\n\n\nclass NumberNotGtError(_NumberBoundError):\n    code = 'number.not_gt'\n    msg_template = 'ensure this value is greater than {limit_value}'\n\n\nclass NumberNotGeError(_NumberBoundError):\n    code = 'number.not_ge'\n    msg_template = 'ensure this value is greater than or equal to {limit_value}'\n\n\nclass NumberNotLtError(_NumberBoundError):\n    code = 'number.not_lt'\n    msg_template = 'ensure this value is less than {limit_value}'\n\n\nclass NumberNotLeError(_NumberBoundError):\n    code = 'number.not_le'\n    msg_template = 'ensure this value is less than or equal to {limit_value}'\n\n\nclass NumberNotFiniteError(PydanticValueError):\n    code = 'number.not_finite_number'\n    msg_template = 'ensure this value is a finite number'\n\n\nclass NumberNotMultipleError(PydanticValueError):\n    code = 'number.not_multiple'\n    msg_template = 'ensure this value is a multiple of {multiple_of}'\n\n    def __init__(self, *, multiple_of: Union[int, float, Decimal]) -> None:\n        super().__init__(multiple_of=multiple_of)\n\n\nclass DecimalError(PydanticTypeError):\n    msg_template = 'value is not a valid decimal'\n\n\nclass DecimalIsNotFiniteError(PydanticValueError):\n    code = 'decimal.not_finite'\n    msg_template = 'value is not a valid decimal'\n\n\nclass DecimalMaxDigitsError(PydanticValueError):\n    code = 'decimal.max_digits'\n    msg_template = 'ensure that there are no more than {max_digits} digits in total'\n\n    def __init__(self, *, max_digits: int) -> None:\n        super().__init__(max_digits=max_digits)\n\n\nclass DecimalMaxPlacesError(PydanticValueError):\n    code = 'decimal.max_places'\n    msg_template = 'ensure that there are no more than {decimal_places} decimal places'\n\n    def __init__(self, *, decimal_places: int) -> None:\n        super().__init__(decimal_places=decimal_places)\n\n\nclass DecimalWholeDigitsError(PydanticValueError):\n    code = 'decimal.whole_digits'\n    msg_template = 'ensure that there are no more than {whole_digits} digits before the decimal point'\n\n    def __init__(self, *, whole_digits: int) -> None:\n        super().__init__(whole_digits=whole_digits)\n\n\nclass DateTimeError(PydanticValueError):\n    msg_template = 'invalid datetime format'\n\n\nclass DateError(PydanticValueError):\n    msg_template = 'invalid date format'\n\n\nclass DateNotInThePastError(PydanticValueError):\n    code = 'date.not_in_the_past'\n    msg_template = 'date is not in the past'\n\n\nclass DateNotInTheFutureError(PydanticValueError):\n    code = 'date.not_in_the_future'\n    msg_template = 'date is not in the future'\n\n\nclass TimeError(PydanticValueError):\n    msg_template = 'invalid time format'\n\n\nclass DurationError(PydanticValueError):\n    msg_template = 'invalid duration format'\n\n\nclass HashableError(PydanticTypeError):\n    msg_template = 'value is not a valid hashable'\n\n\nclass UUIDError(PydanticTypeError):\n    msg_template = 'value is not a valid uuid'\n\n\nclass UUIDVersionError(PydanticValueError):\n    code = 'uuid.version'\n    msg_template = 'uuid version {required_version} expected'\n\n    def __init__(self, *, required_version: int) -> None:\n        super().__init__(required_version=required_version)\n\n\nclass ArbitraryTypeError(PydanticTypeError):\n    code = 'arbitrary_type'\n    msg_template = 'instance of {expected_arbitrary_type} expected'\n\n    def __init__(self, *, expected_arbitrary_type: Type[Any]) -> None:\n        super().__init__(expected_arbitrary_type=display_as_type(expected_arbitrary_type))\n\n\nclass ClassError(PydanticTypeError):\n    code = 'class'\n    msg_template = 'a class is expected'\n\n\nclass SubclassError(PydanticTypeError):\n    code = 'subclass'\n    msg_template = 'subclass of {expected_class} expected'\n\n    def __init__(self, *, expected_class: Type[Any]) -> None:\n        super().__init__(expected_class=display_as_type(expected_class))\n\n\nclass JsonError(PydanticValueError):\n    msg_template = 'Invalid JSON'\n\n\nclass JsonTypeError(PydanticTypeError):\n    code = 'json'\n    msg_template = 'JSON object must be str, bytes or bytearray'\n\n\nclass PatternError(PydanticValueError):\n    code = 'regex_pattern'\n    msg_template = 'Invalid regular expression'\n\n\nclass DataclassTypeError(PydanticTypeError):\n    code = 'dataclass'\n    msg_template = 'instance of {class_name}, tuple or dict expected'\n\n\nclass CallableError(PydanticTypeError):\n    msg_template = '{value} is not callable'\n\n\nclass EnumError(PydanticTypeError):\n    code = 'enum_instance'\n    msg_template = '{value} is not a valid Enum instance'\n\n\nclass IntEnumError(PydanticTypeError):\n    code = 'int_enum_instance'\n    msg_template = '{value} is not a valid IntEnum instance'\n\n\nclass IPvAnyAddressError(PydanticValueError):\n    msg_template = 'value is not a valid IPv4 or IPv6 address'\n\n\nclass IPvAnyInterfaceError(PydanticValueError):\n    msg_template = 'value is not a valid IPv4 or IPv6 interface'\n\n\nclass IPvAnyNetworkError(PydanticValueError):\n    msg_template = 'value is not a valid IPv4 or IPv6 network'\n\n\nclass IPv4AddressError(PydanticValueError):\n    msg_template = 'value is not a valid IPv4 address'\n\n\nclass IPv6AddressError(PydanticValueError):\n    msg_template = 'value is not a valid IPv6 address'\n\n\nclass IPv4NetworkError(PydanticValueError):\n    msg_template = 'value is not a valid IPv4 network'\n\n\nclass IPv6NetworkError(PydanticValueError):\n    msg_template = 'value is not a valid IPv6 network'\n\n\nclass IPv4InterfaceError(PydanticValueError):\n    msg_template = 'value is not a valid IPv4 interface'\n\n\nclass IPv6InterfaceError(PydanticValueError):\n    msg_template = 'value is not a valid IPv6 interface'\n\n\nclass ColorError(PydanticValueError):\n    msg_template = 'value is not a valid color: {reason}'\n\n\nclass StrictBoolError(PydanticValueError):\n    msg_template = 'value is not a valid boolean'\n\n\nclass NotDigitError(PydanticValueError):\n    code = 'payment_card_number.digits'\n    msg_template = 'card number is not all digits'\n\n\nclass LuhnValidationError(PydanticValueError):\n    code = 'payment_card_number.luhn_check'\n    msg_template = 'card number is not luhn valid'\n\n\nclass InvalidLengthForBrand(PydanticValueError):\n    code = 'payment_card_number.invalid_length_for_brand'\n    msg_template = 'Length for a {brand} card must be {required_length}'\n\n\nclass InvalidByteSize(PydanticValueError):\n    msg_template = 'could not parse value and unit from byte string'\n\n\nclass InvalidByteSizeUnit(PydanticValueError):\n    msg_template = 'could not interpret byte unit: {unit}'\n\n\nclass MissingDiscriminator(PydanticValueError):\n    code = 'discriminated_union.missing_discriminator'\n    msg_template = 'Discriminator {discriminator_key!r} is missing in value'\n\n\nclass InvalidDiscriminator(PydanticValueError):\n    code = 'discriminated_union.invalid_discriminator'\n    msg_template = (\n        'No match for discriminator {discriminator_key!r} and value {discriminator_value!r} '\n        '(allowed values: {allowed_values})'\n    )\n\n    def __init__(self, *, discriminator_key: str, discriminator_value: Any, allowed_values: Sequence[Any]) -> None:\n        super().__init__(\n            discriminator_key=discriminator_key,\n            discriminator_value=discriminator_value,\n            allowed_values=', '.join(map(repr, allowed_values)),\n        )\n", "pydantic/v1/json.py": "import datetime\nfrom collections import deque\nfrom decimal import Decimal\nfrom enum import Enum\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom pathlib import Path\nfrom re import Pattern\nfrom types import GeneratorType\nfrom typing import Any, Callable, Dict, Type, Union\nfrom uuid import UUID\n\nfrom pydantic.v1.color import Color\nfrom pydantic.v1.networks import NameEmail\nfrom pydantic.v1.types import SecretBytes, SecretStr\n\n__all__ = 'pydantic_encoder', 'custom_pydantic_encoder', 'timedelta_isoformat'\n\n\ndef isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()\n\n\ndef decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    \"\"\"\n    Encodes a Decimal as int of there's no exponent, otherwise float\n\n    This is useful when we use ConstrainedDecimal to represent Numeric(x,0)\n    where a integer (but not int typed) is used. Encoding this as a float\n    results in failed round-tripping between encode and parse.\n    Our Id type is a prime example of this.\n\n    >>> decimal_encoder(Decimal(\"1.0\"))\n    1.0\n\n    >>> decimal_encoder(Decimal(\"1\"))\n    1\n    \"\"\"\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)\n\n\nENCODERS_BY_TYPE: Dict[Type[Any], Callable[[Any], Any]] = {\n    bytes: lambda o: o.decode(),\n    Color: str,\n    datetime.date: isoformat,\n    datetime.datetime: isoformat,\n    datetime.time: isoformat,\n    datetime.timedelta: lambda td: td.total_seconds(),\n    Decimal: decimal_encoder,\n    Enum: lambda o: o.value,\n    frozenset: list,\n    deque: list,\n    GeneratorType: list,\n    IPv4Address: str,\n    IPv4Interface: str,\n    IPv4Network: str,\n    IPv6Address: str,\n    IPv6Interface: str,\n    IPv6Network: str,\n    NameEmail: str,\n    Path: str,\n    Pattern: lambda o: o.pattern,\n    SecretBytes: str,\n    SecretStr: str,\n    set: list,\n    UUID: str,\n}\n\n\ndef pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n\n    from pydantic.v1.main import BaseModel\n\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n\n    # Check the class type and its superclasses for a matching encoder\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:  # We have exited the for loop without finding a suitable encoder\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")\n\n\ndef custom_pydantic_encoder(type_encoders: Dict[Any, Callable[[Type[Any]], Any]], obj: Any) -> Any:\n    # Check the class type and its superclasses for a matching encoder\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n\n        return encoder(obj)\n    else:  # We have exited the for loop without finding a suitable encoder\n        return pydantic_encoder(obj)\n\n\ndef timedelta_isoformat(td: datetime.timedelta) -> str:\n    \"\"\"\n    ISO 8601 encoding for Python timedelta object.\n    \"\"\"\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'\n", "pydantic/v1/validators.py": "import math\nimport re\nfrom collections import OrderedDict, deque\nfrom collections.abc import Hashable as CollectionsHashable\nfrom datetime import date, datetime, time, timedelta\nfrom decimal import Decimal, DecimalException\nfrom enum import Enum, IntEnum\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom pathlib import Path\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Deque,\n    Dict,\n    ForwardRef,\n    FrozenSet,\n    Generator,\n    Hashable,\n    List,\n    NamedTuple,\n    Pattern,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\nfrom uuid import UUID\n\nfrom pydantic.v1 import errors\nfrom pydantic.v1.datetime_parse import parse_date, parse_datetime, parse_duration, parse_time\nfrom pydantic.v1.typing import (\n    AnyCallable,\n    all_literal_values,\n    display_as_type,\n    get_class,\n    is_callable_type,\n    is_literal_type,\n    is_namedtuple,\n    is_none_type,\n    is_typeddict,\n)\nfrom pydantic.v1.utils import almost_equal_floats, lenient_issubclass, sequence_like\n\nif TYPE_CHECKING:\n    from typing_extensions import Literal, TypedDict\n\n    from pydantic.v1.config import BaseConfig\n    from pydantic.v1.fields import ModelField\n    from pydantic.v1.types import ConstrainedDecimal, ConstrainedFloat, ConstrainedInt\n\n    ConstrainedNumber = Union[ConstrainedDecimal, ConstrainedFloat, ConstrainedInt]\n    AnyOrderedDict = OrderedDict[Any, Any]\n    Number = Union[int, float, Decimal]\n    StrBytes = Union[str, bytes]\n\n\ndef str_validator(v: Any) -> Union[str]:\n    if isinstance(v, str):\n        if isinstance(v, Enum):\n            return v.value\n        else:\n            return v\n    elif isinstance(v, (float, int, Decimal)):\n        # is there anything else we want to add here? If you think so, create an issue.\n        return str(v)\n    elif isinstance(v, (bytes, bytearray)):\n        return v.decode()\n    else:\n        raise errors.StrError()\n\n\ndef strict_str_validator(v: Any) -> Union[str]:\n    if isinstance(v, str) and not isinstance(v, Enum):\n        return v\n    raise errors.StrError()\n\n\ndef bytes_validator(v: Any) -> Union[bytes]:\n    if isinstance(v, bytes):\n        return v\n    elif isinstance(v, bytearray):\n        return bytes(v)\n    elif isinstance(v, str):\n        return v.encode()\n    elif isinstance(v, (float, int, Decimal)):\n        return str(v).encode()\n    else:\n        raise errors.BytesError()\n\n\ndef strict_bytes_validator(v: Any) -> Union[bytes]:\n    if isinstance(v, bytes):\n        return v\n    elif isinstance(v, bytearray):\n        return bytes(v)\n    else:\n        raise errors.BytesError()\n\n\nBOOL_FALSE = {0, '0', 'off', 'f', 'false', 'n', 'no'}\nBOOL_TRUE = {1, '1', 'on', 't', 'true', 'y', 'yes'}\n\n\ndef bool_validator(v: Any) -> bool:\n    if v is True or v is False:\n        return v\n    if isinstance(v, bytes):\n        v = v.decode()\n    if isinstance(v, str):\n        v = v.lower()\n    try:\n        if v in BOOL_TRUE:\n            return True\n        if v in BOOL_FALSE:\n            return False\n    except TypeError:\n        raise errors.BoolError()\n    raise errors.BoolError()\n\n\n# matches the default limit cpython, see https://github.com/python/cpython/pull/96500\nmax_str_int = 4_300\n\n\ndef int_validator(v: Any) -> int:\n    if isinstance(v, int) and not (v is True or v is False):\n        return v\n\n    # see https://github.com/pydantic/pydantic/issues/1477 and in turn, https://github.com/python/cpython/issues/95778\n    # this check should be unnecessary once patch releases are out for 3.7, 3.8, 3.9 and 3.10\n    # but better to check here until then.\n    # NOTICE: this does not fully protect user from the DOS risk since the standard library JSON implementation\n    # (and other std lib modules like xml) use `int()` and are likely called before this, the best workaround is to\n    # 1. update to the latest patch release of python once released, 2. use a different JSON library like ujson\n    if isinstance(v, (str, bytes, bytearray)) and len(v) > max_str_int:\n        raise errors.IntegerError()\n\n    try:\n        return int(v)\n    except (TypeError, ValueError, OverflowError):\n        raise errors.IntegerError()\n\n\ndef strict_int_validator(v: Any) -> int:\n    if isinstance(v, int) and not (v is True or v is False):\n        return v\n    raise errors.IntegerError()\n\n\ndef float_validator(v: Any) -> float:\n    if isinstance(v, float):\n        return v\n\n    try:\n        return float(v)\n    except (TypeError, ValueError):\n        raise errors.FloatError()\n\n\ndef strict_float_validator(v: Any) -> float:\n    if isinstance(v, float):\n        return v\n    raise errors.FloatError()\n\n\ndef float_finite_validator(v: 'Number', field: 'ModelField', config: 'BaseConfig') -> 'Number':\n    allow_inf_nan = getattr(field.type_, 'allow_inf_nan', None)\n    if allow_inf_nan is None:\n        allow_inf_nan = config.allow_inf_nan\n\n    if allow_inf_nan is False and (math.isnan(v) or math.isinf(v)):\n        raise errors.NumberNotFiniteError()\n    return v\n\n\ndef number_multiple_validator(v: 'Number', field: 'ModelField') -> 'Number':\n    field_type: ConstrainedNumber = field.type_\n    if field_type.multiple_of is not None:\n        mod = float(v) / float(field_type.multiple_of) % 1\n        if not almost_equal_floats(mod, 0.0) and not almost_equal_floats(mod, 1.0):\n            raise errors.NumberNotMultipleError(multiple_of=field_type.multiple_of)\n    return v\n\n\ndef number_size_validator(v: 'Number', field: 'ModelField') -> 'Number':\n    field_type: ConstrainedNumber = field.type_\n    if field_type.gt is not None and not v > field_type.gt:\n        raise errors.NumberNotGtError(limit_value=field_type.gt)\n    elif field_type.ge is not None and not v >= field_type.ge:\n        raise errors.NumberNotGeError(limit_value=field_type.ge)\n\n    if field_type.lt is not None and not v < field_type.lt:\n        raise errors.NumberNotLtError(limit_value=field_type.lt)\n    if field_type.le is not None and not v <= field_type.le:\n        raise errors.NumberNotLeError(limit_value=field_type.le)\n\n    return v\n\n\ndef constant_validator(v: 'Any', field: 'ModelField') -> 'Any':\n    \"\"\"Validate ``const`` fields.\n\n    The value provided for a ``const`` field must be equal to the default value\n    of the field. This is to support the keyword of the same name in JSON\n    Schema.\n    \"\"\"\n    if v != field.default:\n        raise errors.WrongConstantError(given=v, permitted=[field.default])\n\n    return v\n\n\ndef anystr_length_validator(v: 'StrBytes', config: 'BaseConfig') -> 'StrBytes':\n    v_len = len(v)\n\n    min_length = config.min_anystr_length\n    if v_len < min_length:\n        raise errors.AnyStrMinLengthError(limit_value=min_length)\n\n    max_length = config.max_anystr_length\n    if max_length is not None and v_len > max_length:\n        raise errors.AnyStrMaxLengthError(limit_value=max_length)\n\n    return v\n\n\ndef anystr_strip_whitespace(v: 'StrBytes') -> 'StrBytes':\n    return v.strip()\n\n\ndef anystr_upper(v: 'StrBytes') -> 'StrBytes':\n    return v.upper()\n\n\ndef anystr_lower(v: 'StrBytes') -> 'StrBytes':\n    return v.lower()\n\n\ndef ordered_dict_validator(v: Any) -> 'AnyOrderedDict':\n    if isinstance(v, OrderedDict):\n        return v\n\n    try:\n        return OrderedDict(v)\n    except (TypeError, ValueError):\n        raise errors.DictError()\n\n\ndef dict_validator(v: Any) -> Dict[Any, Any]:\n    if isinstance(v, dict):\n        return v\n\n    try:\n        return dict(v)\n    except (TypeError, ValueError):\n        raise errors.DictError()\n\n\ndef list_validator(v: Any) -> List[Any]:\n    if isinstance(v, list):\n        return v\n    elif sequence_like(v):\n        return list(v)\n    else:\n        raise errors.ListError()\n\n\ndef tuple_validator(v: Any) -> Tuple[Any, ...]:\n    if isinstance(v, tuple):\n        return v\n    elif sequence_like(v):\n        return tuple(v)\n    else:\n        raise errors.TupleError()\n\n\ndef set_validator(v: Any) -> Set[Any]:\n    if isinstance(v, set):\n        return v\n    elif sequence_like(v):\n        return set(v)\n    else:\n        raise errors.SetError()\n\n\ndef frozenset_validator(v: Any) -> FrozenSet[Any]:\n    if isinstance(v, frozenset):\n        return v\n    elif sequence_like(v):\n        return frozenset(v)\n    else:\n        raise errors.FrozenSetError()\n\n\ndef deque_validator(v: Any) -> Deque[Any]:\n    if isinstance(v, deque):\n        return v\n    elif sequence_like(v):\n        return deque(v)\n    else:\n        raise errors.DequeError()\n\n\ndef enum_member_validator(v: Any, field: 'ModelField', config: 'BaseConfig') -> Enum:\n    try:\n        enum_v = field.type_(v)\n    except ValueError:\n        # field.type_ should be an enum, so will be iterable\n        raise errors.EnumMemberError(enum_values=list(field.type_))\n    return enum_v.value if config.use_enum_values else enum_v\n\n\ndef uuid_validator(v: Any, field: 'ModelField') -> UUID:\n    try:\n        if isinstance(v, str):\n            v = UUID(v)\n        elif isinstance(v, (bytes, bytearray)):\n            try:\n                v = UUID(v.decode())\n            except ValueError:\n                # 16 bytes in big-endian order as the bytes argument fail\n                # the above check\n                v = UUID(bytes=v)\n    except ValueError:\n        raise errors.UUIDError()\n\n    if not isinstance(v, UUID):\n        raise errors.UUIDError()\n\n    required_version = getattr(field.type_, '_required_version', None)\n    if required_version and v.version != required_version:\n        raise errors.UUIDVersionError(required_version=required_version)\n\n    return v\n\n\ndef decimal_validator(v: Any) -> Decimal:\n    if isinstance(v, Decimal):\n        return v\n    elif isinstance(v, (bytes, bytearray)):\n        v = v.decode()\n\n    v = str(v).strip()\n\n    try:\n        v = Decimal(v)\n    except DecimalException:\n        raise errors.DecimalError()\n\n    if not v.is_finite():\n        raise errors.DecimalIsNotFiniteError()\n\n    return v\n\n\ndef hashable_validator(v: Any) -> Hashable:\n    if isinstance(v, Hashable):\n        return v\n\n    raise errors.HashableError()\n\n\ndef ip_v4_address_validator(v: Any) -> IPv4Address:\n    if isinstance(v, IPv4Address):\n        return v\n\n    try:\n        return IPv4Address(v)\n    except ValueError:\n        raise errors.IPv4AddressError()\n\n\ndef ip_v6_address_validator(v: Any) -> IPv6Address:\n    if isinstance(v, IPv6Address):\n        return v\n\n    try:\n        return IPv6Address(v)\n    except ValueError:\n        raise errors.IPv6AddressError()\n\n\ndef ip_v4_network_validator(v: Any) -> IPv4Network:\n    \"\"\"\n    Assume IPv4Network initialised with a default ``strict`` argument\n\n    See more:\n    https://docs.python.org/library/ipaddress.html#ipaddress.IPv4Network\n    \"\"\"\n    if isinstance(v, IPv4Network):\n        return v\n\n    try:\n        return IPv4Network(v)\n    except ValueError:\n        raise errors.IPv4NetworkError()\n\n\ndef ip_v6_network_validator(v: Any) -> IPv6Network:\n    \"\"\"\n    Assume IPv6Network initialised with a default ``strict`` argument\n\n    See more:\n    https://docs.python.org/library/ipaddress.html#ipaddress.IPv6Network\n    \"\"\"\n    if isinstance(v, IPv6Network):\n        return v\n\n    try:\n        return IPv6Network(v)\n    except ValueError:\n        raise errors.IPv6NetworkError()\n\n\ndef ip_v4_interface_validator(v: Any) -> IPv4Interface:\n    if isinstance(v, IPv4Interface):\n        return v\n\n    try:\n        return IPv4Interface(v)\n    except ValueError:\n        raise errors.IPv4InterfaceError()\n\n\ndef ip_v6_interface_validator(v: Any) -> IPv6Interface:\n    if isinstance(v, IPv6Interface):\n        return v\n\n    try:\n        return IPv6Interface(v)\n    except ValueError:\n        raise errors.IPv6InterfaceError()\n\n\ndef path_validator(v: Any) -> Path:\n    if isinstance(v, Path):\n        return v\n\n    try:\n        return Path(v)\n    except TypeError:\n        raise errors.PathError()\n\n\ndef path_exists_validator(v: Any) -> Path:\n    if not v.exists():\n        raise errors.PathNotExistsError(path=v)\n\n    return v\n\n\ndef callable_validator(v: Any) -> AnyCallable:\n    \"\"\"\n    Perform a simple check if the value is callable.\n\n    Note: complete matching of argument type hints and return types is not performed\n    \"\"\"\n    if callable(v):\n        return v\n\n    raise errors.CallableError(value=v)\n\n\ndef enum_validator(v: Any) -> Enum:\n    if isinstance(v, Enum):\n        return v\n\n    raise errors.EnumError(value=v)\n\n\ndef int_enum_validator(v: Any) -> IntEnum:\n    if isinstance(v, IntEnum):\n        return v\n\n    raise errors.IntEnumError(value=v)\n\n\ndef make_literal_validator(type_: Any) -> Callable[[Any], Any]:\n    permitted_choices = all_literal_values(type_)\n\n    # To have a O(1) complexity and still return one of the values set inside the `Literal`,\n    # we create a dict with the set values (a set causes some problems with the way intersection works).\n    # In some cases the set value and checked value can indeed be different (see `test_literal_validator_str_enum`)\n    allowed_choices = {v: v for v in permitted_choices}\n\n    def literal_validator(v: Any) -> Any:\n        try:\n            return allowed_choices[v]\n        except (KeyError, TypeError):\n            raise errors.WrongConstantError(given=v, permitted=permitted_choices)\n\n    return literal_validator\n\n\ndef constr_length_validator(v: 'StrBytes', field: 'ModelField', config: 'BaseConfig') -> 'StrBytes':\n    v_len = len(v)\n\n    min_length = field.type_.min_length if field.type_.min_length is not None else config.min_anystr_length\n    if v_len < min_length:\n        raise errors.AnyStrMinLengthError(limit_value=min_length)\n\n    max_length = field.type_.max_length if field.type_.max_length is not None else config.max_anystr_length\n    if max_length is not None and v_len > max_length:\n        raise errors.AnyStrMaxLengthError(limit_value=max_length)\n\n    return v\n\n\ndef constr_strip_whitespace(v: 'StrBytes', field: 'ModelField', config: 'BaseConfig') -> 'StrBytes':\n    strip_whitespace = field.type_.strip_whitespace or config.anystr_strip_whitespace\n    if strip_whitespace:\n        v = v.strip()\n\n    return v\n\n\ndef constr_upper(v: 'StrBytes', field: 'ModelField', config: 'BaseConfig') -> 'StrBytes':\n    upper = field.type_.to_upper or config.anystr_upper\n    if upper:\n        v = v.upper()\n\n    return v\n\n\ndef constr_lower(v: 'StrBytes', field: 'ModelField', config: 'BaseConfig') -> 'StrBytes':\n    lower = field.type_.to_lower or config.anystr_lower\n    if lower:\n        v = v.lower()\n    return v\n\n\ndef validate_json(v: Any, config: 'BaseConfig') -> Any:\n    if v is None:\n        # pass None through to other validators\n        return v\n    try:\n        return config.json_loads(v)  # type: ignore\n    except ValueError:\n        raise errors.JsonError()\n    except TypeError:\n        raise errors.JsonTypeError()\n\n\nT = TypeVar('T')\n\n\ndef make_arbitrary_type_validator(type_: Type[T]) -> Callable[[T], T]:\n    def arbitrary_type_validator(v: Any) -> T:\n        if isinstance(v, type_):\n            return v\n        raise errors.ArbitraryTypeError(expected_arbitrary_type=type_)\n\n    return arbitrary_type_validator\n\n\ndef make_class_validator(type_: Type[T]) -> Callable[[Any], Type[T]]:\n    def class_validator(v: Any) -> Type[T]:\n        if lenient_issubclass(v, type_):\n            return v\n        raise errors.SubclassError(expected_class=type_)\n\n    return class_validator\n\n\ndef any_class_validator(v: Any) -> Type[T]:\n    if isinstance(v, type):\n        return v\n    raise errors.ClassError()\n\n\ndef none_validator(v: Any) -> 'Literal[None]':\n    if v is None:\n        return v\n    raise errors.NotNoneError()\n\n\ndef pattern_validator(v: Any) -> Pattern[str]:\n    if isinstance(v, Pattern):\n        return v\n\n    str_value = str_validator(v)\n\n    try:\n        return re.compile(str_value)\n    except re.error:\n        raise errors.PatternError()\n\n\nNamedTupleT = TypeVar('NamedTupleT', bound=NamedTuple)\n\n\ndef make_namedtuple_validator(\n    namedtuple_cls: Type[NamedTupleT], config: Type['BaseConfig']\n) -> Callable[[Tuple[Any, ...]], NamedTupleT]:\n    from pydantic.v1.annotated_types import create_model_from_namedtuple\n\n    NamedTupleModel = create_model_from_namedtuple(\n        namedtuple_cls,\n        __config__=config,\n        __module__=namedtuple_cls.__module__,\n    )\n    namedtuple_cls.__pydantic_model__ = NamedTupleModel  # type: ignore[attr-defined]\n\n    def namedtuple_validator(values: Tuple[Any, ...]) -> NamedTupleT:\n        annotations = NamedTupleModel.__annotations__\n\n        if len(values) > len(annotations):\n            raise errors.ListMaxLengthError(limit_value=len(annotations))\n\n        dict_values: Dict[str, Any] = dict(zip(annotations, values))\n        validated_dict_values: Dict[str, Any] = dict(NamedTupleModel(**dict_values))\n        return namedtuple_cls(**validated_dict_values)\n\n    return namedtuple_validator\n\n\ndef make_typeddict_validator(\n    typeddict_cls: Type['TypedDict'], config: Type['BaseConfig']  # type: ignore[valid-type]\n) -> Callable[[Any], Dict[str, Any]]:\n    from pydantic.v1.annotated_types import create_model_from_typeddict\n\n    TypedDictModel = create_model_from_typeddict(\n        typeddict_cls,\n        __config__=config,\n        __module__=typeddict_cls.__module__,\n    )\n    typeddict_cls.__pydantic_model__ = TypedDictModel  # type: ignore[attr-defined]\n\n    def typeddict_validator(values: 'TypedDict') -> Dict[str, Any]:  # type: ignore[valid-type]\n        return TypedDictModel.parse_obj(values).dict(exclude_unset=True)\n\n    return typeddict_validator\n\n\nclass IfConfig:\n    def __init__(self, validator: AnyCallable, *config_attr_names: str, ignored_value: Any = False) -> None:\n        self.validator = validator\n        self.config_attr_names = config_attr_names\n        self.ignored_value = ignored_value\n\n    def check(self, config: Type['BaseConfig']) -> bool:\n        return any(getattr(config, name) not in {None, self.ignored_value} for name in self.config_attr_names)\n\n\n# order is important here, for example: bool is a subclass of int so has to come first, datetime before date same,\n# IPv4Interface before IPv4Address, etc\n_VALIDATORS: List[Tuple[Type[Any], List[Any]]] = [\n    (IntEnum, [int_validator, enum_member_validator]),\n    (Enum, [enum_member_validator]),\n    (\n        str,\n        [\n            str_validator,\n            IfConfig(anystr_strip_whitespace, 'anystr_strip_whitespace'),\n            IfConfig(anystr_upper, 'anystr_upper'),\n            IfConfig(anystr_lower, 'anystr_lower'),\n            IfConfig(anystr_length_validator, 'min_anystr_length', 'max_anystr_length'),\n        ],\n    ),\n    (\n        bytes,\n        [\n            bytes_validator,\n            IfConfig(anystr_strip_whitespace, 'anystr_strip_whitespace'),\n            IfConfig(anystr_upper, 'anystr_upper'),\n            IfConfig(anystr_lower, 'anystr_lower'),\n            IfConfig(anystr_length_validator, 'min_anystr_length', 'max_anystr_length'),\n        ],\n    ),\n    (bool, [bool_validator]),\n    (int, [int_validator]),\n    (float, [float_validator, IfConfig(float_finite_validator, 'allow_inf_nan', ignored_value=True)]),\n    (Path, [path_validator]),\n    (datetime, [parse_datetime]),\n    (date, [parse_date]),\n    (time, [parse_time]),\n    (timedelta, [parse_duration]),\n    (OrderedDict, [ordered_dict_validator]),\n    (dict, [dict_validator]),\n    (list, [list_validator]),\n    (tuple, [tuple_validator]),\n    (set, [set_validator]),\n    (frozenset, [frozenset_validator]),\n    (deque, [deque_validator]),\n    (UUID, [uuid_validator]),\n    (Decimal, [decimal_validator]),\n    (IPv4Interface, [ip_v4_interface_validator]),\n    (IPv6Interface, [ip_v6_interface_validator]),\n    (IPv4Address, [ip_v4_address_validator]),\n    (IPv6Address, [ip_v6_address_validator]),\n    (IPv4Network, [ip_v4_network_validator]),\n    (IPv6Network, [ip_v6_network_validator]),\n]\n\n\ndef find_validators(  # noqa: C901 (ignore complexity)\n    type_: Type[Any], config: Type['BaseConfig']\n) -> Generator[AnyCallable, None, None]:\n    from pydantic.v1.dataclasses import is_builtin_dataclass, make_dataclass_validator\n\n    if type_ is Any or type_ is object:\n        return\n    type_type = type_.__class__\n    if type_type == ForwardRef or type_type == TypeVar:\n        return\n\n    if is_none_type(type_):\n        yield none_validator\n        return\n    if type_ is Pattern or type_ is re.Pattern:\n        yield pattern_validator\n        return\n    if type_ is Hashable or type_ is CollectionsHashable:\n        yield hashable_validator\n        return\n    if is_callable_type(type_):\n        yield callable_validator\n        return\n    if is_literal_type(type_):\n        yield make_literal_validator(type_)\n        return\n    if is_builtin_dataclass(type_):\n        yield from make_dataclass_validator(type_, config)\n        return\n    if type_ is Enum:\n        yield enum_validator\n        return\n    if type_ is IntEnum:\n        yield int_enum_validator\n        return\n    if is_namedtuple(type_):\n        yield tuple_validator\n        yield make_namedtuple_validator(type_, config)\n        return\n    if is_typeddict(type_):\n        yield make_typeddict_validator(type_, config)\n        return\n\n    class_ = get_class(type_)\n    if class_ is not None:\n        if class_ is not Any and isinstance(class_, type):\n            yield make_class_validator(class_)\n        else:\n            yield any_class_validator\n        return\n\n    for val_type, validators in _VALIDATORS:\n        try:\n            if issubclass(type_, val_type):\n                for v in validators:\n                    if isinstance(v, IfConfig):\n                        if v.check(config):\n                            yield v.validator\n                    else:\n                        yield v\n                return\n        except TypeError:\n            raise RuntimeError(f'error checking inheritance of {type_!r} (type: {display_as_type(type_)})')\n\n    if config.arbitrary_types_allowed:\n        yield make_arbitrary_type_validator(type_)\n    else:\n        raise RuntimeError(f'no validator found for {type_}, see `arbitrary_types_allowed` in Config')\n", "pydantic/v1/types.py": "import abc\nimport math\nimport re\nimport warnings\nfrom datetime import date\nfrom decimal import Decimal, InvalidOperation\nfrom enum import Enum\nfrom pathlib import Path\nfrom types import new_class\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    FrozenSet,\n    List,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\nfrom uuid import UUID\nfrom weakref import WeakSet\n\nfrom pydantic.v1 import errors\nfrom pydantic.v1.datetime_parse import parse_date\nfrom pydantic.v1.utils import import_string, update_not_none\nfrom pydantic.v1.validators import (\n    bytes_validator,\n    constr_length_validator,\n    constr_lower,\n    constr_strip_whitespace,\n    constr_upper,\n    decimal_validator,\n    float_finite_validator,\n    float_validator,\n    frozenset_validator,\n    int_validator,\n    list_validator,\n    number_multiple_validator,\n    number_size_validator,\n    path_exists_validator,\n    path_validator,\n    set_validator,\n    str_validator,\n    strict_bytes_validator,\n    strict_float_validator,\n    strict_int_validator,\n    strict_str_validator,\n)\n\n__all__ = [\n    'NoneStr',\n    'NoneBytes',\n    'StrBytes',\n    'NoneStrBytes',\n    'StrictStr',\n    'ConstrainedBytes',\n    'conbytes',\n    'ConstrainedList',\n    'conlist',\n    'ConstrainedSet',\n    'conset',\n    'ConstrainedFrozenSet',\n    'confrozenset',\n    'ConstrainedStr',\n    'constr',\n    'PyObject',\n    'ConstrainedInt',\n    'conint',\n    'PositiveInt',\n    'NegativeInt',\n    'NonNegativeInt',\n    'NonPositiveInt',\n    'ConstrainedFloat',\n    'confloat',\n    'PositiveFloat',\n    'NegativeFloat',\n    'NonNegativeFloat',\n    'NonPositiveFloat',\n    'FiniteFloat',\n    'ConstrainedDecimal',\n    'condecimal',\n    'UUID1',\n    'UUID3',\n    'UUID4',\n    'UUID5',\n    'FilePath',\n    'DirectoryPath',\n    'Json',\n    'JsonWrapper',\n    'SecretField',\n    'SecretStr',\n    'SecretBytes',\n    'StrictBool',\n    'StrictBytes',\n    'StrictInt',\n    'StrictFloat',\n    'PaymentCardNumber',\n    'ByteSize',\n    'PastDate',\n    'FutureDate',\n    'ConstrainedDate',\n    'condate',\n]\n\nNoneStr = Optional[str]\nNoneBytes = Optional[bytes]\nStrBytes = Union[str, bytes]\nNoneStrBytes = Optional[StrBytes]\nOptionalInt = Optional[int]\nOptionalIntFloat = Union[OptionalInt, float]\nOptionalIntFloatDecimal = Union[OptionalIntFloat, Decimal]\nOptionalDate = Optional[date]\nStrIntFloat = Union[str, int, float]\n\nif TYPE_CHECKING:\n    from typing_extensions import Annotated\n\n    from pydantic.v1.dataclasses import Dataclass\n    from pydantic.v1.main import BaseModel\n    from pydantic.v1.typing import CallableGenerator\n\n    ModelOrDc = Type[Union[BaseModel, Dataclass]]\n\nT = TypeVar('T')\n_DEFINED_TYPES: 'WeakSet[type]' = WeakSet()\n\n\n@overload\ndef _registered(typ: Type[T]) -> Type[T]:\n    pass\n\n\n@overload\ndef _registered(typ: 'ConstrainedNumberMeta') -> 'ConstrainedNumberMeta':\n    pass\n\n\ndef _registered(typ: Union[Type[T], 'ConstrainedNumberMeta']) -> Union[Type[T], 'ConstrainedNumberMeta']:\n    # In order to generate valid examples of constrained types, Hypothesis needs\n    # to inspect the type object - so we keep a weakref to each contype object\n    # until it can be registered.  When (or if) our Hypothesis plugin is loaded,\n    # it monkeypatches this function.\n    # If Hypothesis is never used, the total effect is to keep a weak reference\n    # which has minimal memory usage and doesn't even affect garbage collection.\n    _DEFINED_TYPES.add(typ)\n    return typ\n\n\nclass ConstrainedNumberMeta(type):\n    def __new__(cls, name: str, bases: Any, dct: Dict[str, Any]) -> 'ConstrainedInt':  # type: ignore\n        new_cls = cast('ConstrainedInt', type.__new__(cls, name, bases, dct))\n\n        if new_cls.gt is not None and new_cls.ge is not None:\n            raise errors.ConfigError('bounds gt and ge cannot be specified at the same time')\n        if new_cls.lt is not None and new_cls.le is not None:\n            raise errors.ConfigError('bounds lt and le cannot be specified at the same time')\n\n        return _registered(new_cls)  # type: ignore\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BOOLEAN TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nif TYPE_CHECKING:\n    StrictBool = bool\nelse:\n\n    class StrictBool(int):\n        \"\"\"\n        StrictBool to allow for bools which are not type-coerced.\n        \"\"\"\n\n        @classmethod\n        def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n            field_schema.update(type='boolean')\n\n        @classmethod\n        def __get_validators__(cls) -> 'CallableGenerator':\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, value: Any) -> bool:\n            \"\"\"\n            Ensure that we only allow bools.\n            \"\"\"\n            if isinstance(value, bool):\n                return value\n\n            raise errors.StrictBoolError()\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ INTEGER TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass ConstrainedInt(int, metaclass=ConstrainedNumberMeta):\n    strict: bool = False\n    gt: OptionalInt = None\n    ge: OptionalInt = None\n    lt: OptionalInt = None\n    le: OptionalInt = None\n    multiple_of: OptionalInt = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            exclusiveMinimum=cls.gt,\n            exclusiveMaximum=cls.lt,\n            minimum=cls.ge,\n            maximum=cls.le,\n            multipleOf=cls.multiple_of,\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield strict_int_validator if cls.strict else int_validator\n        yield number_size_validator\n        yield number_multiple_validator\n\n\ndef conint(\n    *,\n    strict: bool = False,\n    gt: Optional[int] = None,\n    ge: Optional[int] = None,\n    lt: Optional[int] = None,\n    le: Optional[int] = None,\n    multiple_of: Optional[int] = None,\n) -> Type[int]:\n    # use kwargs then define conf in a dict to aid with IDE type hinting\n    namespace = dict(strict=strict, gt=gt, ge=ge, lt=lt, le=le, multiple_of=multiple_of)\n    return type('ConstrainedIntValue', (ConstrainedInt,), namespace)\n\n\nif TYPE_CHECKING:\n    PositiveInt = int\n    NegativeInt = int\n    NonPositiveInt = int\n    NonNegativeInt = int\n    StrictInt = int\nelse:\n\n    class PositiveInt(ConstrainedInt):\n        gt = 0\n\n    class NegativeInt(ConstrainedInt):\n        lt = 0\n\n    class NonPositiveInt(ConstrainedInt):\n        le = 0\n\n    class NonNegativeInt(ConstrainedInt):\n        ge = 0\n\n    class StrictInt(ConstrainedInt):\n        strict = True\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FLOAT TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass ConstrainedFloat(float, metaclass=ConstrainedNumberMeta):\n    strict: bool = False\n    gt: OptionalIntFloat = None\n    ge: OptionalIntFloat = None\n    lt: OptionalIntFloat = None\n    le: OptionalIntFloat = None\n    multiple_of: OptionalIntFloat = None\n    allow_inf_nan: Optional[bool] = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            exclusiveMinimum=cls.gt,\n            exclusiveMaximum=cls.lt,\n            minimum=cls.ge,\n            maximum=cls.le,\n            multipleOf=cls.multiple_of,\n        )\n        # Modify constraints to account for differences between IEEE floats and JSON\n        if field_schema.get('exclusiveMinimum') == -math.inf:\n            del field_schema['exclusiveMinimum']\n        if field_schema.get('minimum') == -math.inf:\n            del field_schema['minimum']\n        if field_schema.get('exclusiveMaximum') == math.inf:\n            del field_schema['exclusiveMaximum']\n        if field_schema.get('maximum') == math.inf:\n            del field_schema['maximum']\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield strict_float_validator if cls.strict else float_validator\n        yield number_size_validator\n        yield number_multiple_validator\n        yield float_finite_validator\n\n\ndef confloat(\n    *,\n    strict: bool = False,\n    gt: float = None,\n    ge: float = None,\n    lt: float = None,\n    le: float = None,\n    multiple_of: float = None,\n    allow_inf_nan: Optional[bool] = None,\n) -> Type[float]:\n    # use kwargs then define conf in a dict to aid with IDE type hinting\n    namespace = dict(strict=strict, gt=gt, ge=ge, lt=lt, le=le, multiple_of=multiple_of, allow_inf_nan=allow_inf_nan)\n    return type('ConstrainedFloatValue', (ConstrainedFloat,), namespace)\n\n\nif TYPE_CHECKING:\n    PositiveFloat = float\n    NegativeFloat = float\n    NonPositiveFloat = float\n    NonNegativeFloat = float\n    StrictFloat = float\n    FiniteFloat = float\nelse:\n\n    class PositiveFloat(ConstrainedFloat):\n        gt = 0\n\n    class NegativeFloat(ConstrainedFloat):\n        lt = 0\n\n    class NonPositiveFloat(ConstrainedFloat):\n        le = 0\n\n    class NonNegativeFloat(ConstrainedFloat):\n        ge = 0\n\n    class StrictFloat(ConstrainedFloat):\n        strict = True\n\n    class FiniteFloat(ConstrainedFloat):\n        allow_inf_nan = False\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BYTES TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass ConstrainedBytes(bytes):\n    strip_whitespace = False\n    to_upper = False\n    to_lower = False\n    min_length: OptionalInt = None\n    max_length: OptionalInt = None\n    strict: bool = False\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(field_schema, minLength=cls.min_length, maxLength=cls.max_length)\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield strict_bytes_validator if cls.strict else bytes_validator\n        yield constr_strip_whitespace\n        yield constr_upper\n        yield constr_lower\n        yield constr_length_validator\n\n\ndef conbytes(\n    *,\n    strip_whitespace: bool = False,\n    to_upper: bool = False,\n    to_lower: bool = False,\n    min_length: Optional[int] = None,\n    max_length: Optional[int] = None,\n    strict: bool = False,\n) -> Type[bytes]:\n    # use kwargs then define conf in a dict to aid with IDE type hinting\n    namespace = dict(\n        strip_whitespace=strip_whitespace,\n        to_upper=to_upper,\n        to_lower=to_lower,\n        min_length=min_length,\n        max_length=max_length,\n        strict=strict,\n    )\n    return _registered(type('ConstrainedBytesValue', (ConstrainedBytes,), namespace))\n\n\nif TYPE_CHECKING:\n    StrictBytes = bytes\nelse:\n\n    class StrictBytes(ConstrainedBytes):\n        strict = True\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ STRING TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass ConstrainedStr(str):\n    strip_whitespace = False\n    to_upper = False\n    to_lower = False\n    min_length: OptionalInt = None\n    max_length: OptionalInt = None\n    curtail_length: OptionalInt = None\n    regex: Optional[Union[str, Pattern[str]]] = None\n    strict = False\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            minLength=cls.min_length,\n            maxLength=cls.max_length,\n            pattern=cls.regex and cls._get_pattern(cls.regex),\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield strict_str_validator if cls.strict else str_validator\n        yield constr_strip_whitespace\n        yield constr_upper\n        yield constr_lower\n        yield constr_length_validator\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Union[str]) -> Union[str]:\n        if cls.curtail_length and len(value) > cls.curtail_length:\n            value = value[: cls.curtail_length]\n\n        if cls.regex:\n            if not re.match(cls.regex, value):\n                raise errors.StrRegexError(pattern=cls._get_pattern(cls.regex))\n\n        return value\n\n    @staticmethod\n    def _get_pattern(regex: Union[str, Pattern[str]]) -> str:\n        return regex if isinstance(regex, str) else regex.pattern\n\n\ndef constr(\n    *,\n    strip_whitespace: bool = False,\n    to_upper: bool = False,\n    to_lower: bool = False,\n    strict: bool = False,\n    min_length: Optional[int] = None,\n    max_length: Optional[int] = None,\n    curtail_length: Optional[int] = None,\n    regex: Optional[str] = None,\n) -> Type[str]:\n    # use kwargs then define conf in a dict to aid with IDE type hinting\n    namespace = dict(\n        strip_whitespace=strip_whitespace,\n        to_upper=to_upper,\n        to_lower=to_lower,\n        strict=strict,\n        min_length=min_length,\n        max_length=max_length,\n        curtail_length=curtail_length,\n        regex=regex and re.compile(regex),\n    )\n    return _registered(type('ConstrainedStrValue', (ConstrainedStr,), namespace))\n\n\nif TYPE_CHECKING:\n    StrictStr = str\nelse:\n\n    class StrictStr(ConstrainedStr):\n        strict = True\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SET TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# This types superclass should be Set[T], but cython chokes on that...\nclass ConstrainedSet(set):  # type: ignore\n    # Needed for pydantic to detect that this is a set\n    __origin__ = set\n    __args__: Set[Type[T]]  # type: ignore\n\n    min_items: Optional[int] = None\n    max_items: Optional[int] = None\n    item_type: Type[T]  # type: ignore\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.set_length_validator\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(field_schema, minItems=cls.min_items, maxItems=cls.max_items)\n\n    @classmethod\n    def set_length_validator(cls, v: 'Optional[Set[T]]') -> 'Optional[Set[T]]':\n        if v is None:\n            return None\n\n        v = set_validator(v)\n        v_len = len(v)\n\n        if cls.min_items is not None and v_len < cls.min_items:\n            raise errors.SetMinLengthError(limit_value=cls.min_items)\n\n        if cls.max_items is not None and v_len > cls.max_items:\n            raise errors.SetMaxLengthError(limit_value=cls.max_items)\n\n        return v\n\n\ndef conset(item_type: Type[T], *, min_items: Optional[int] = None, max_items: Optional[int] = None) -> Type[Set[T]]:\n    # __args__ is needed to conform to typing generics api\n    namespace = {'min_items': min_items, 'max_items': max_items, 'item_type': item_type, '__args__': [item_type]}\n    # We use new_class to be able to deal with Generic types\n    return new_class('ConstrainedSetValue', (ConstrainedSet,), {}, lambda ns: ns.update(namespace))\n\n\n# This types superclass should be FrozenSet[T], but cython chokes on that...\nclass ConstrainedFrozenSet(frozenset):  # type: ignore\n    # Needed for pydantic to detect that this is a set\n    __origin__ = frozenset\n    __args__: FrozenSet[Type[T]]  # type: ignore\n\n    min_items: Optional[int] = None\n    max_items: Optional[int] = None\n    item_type: Type[T]  # type: ignore\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.frozenset_length_validator\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(field_schema, minItems=cls.min_items, maxItems=cls.max_items)\n\n    @classmethod\n    def frozenset_length_validator(cls, v: 'Optional[FrozenSet[T]]') -> 'Optional[FrozenSet[T]]':\n        if v is None:\n            return None\n\n        v = frozenset_validator(v)\n        v_len = len(v)\n\n        if cls.min_items is not None and v_len < cls.min_items:\n            raise errors.FrozenSetMinLengthError(limit_value=cls.min_items)\n\n        if cls.max_items is not None and v_len > cls.max_items:\n            raise errors.FrozenSetMaxLengthError(limit_value=cls.max_items)\n\n        return v\n\n\ndef confrozenset(\n    item_type: Type[T], *, min_items: Optional[int] = None, max_items: Optional[int] = None\n) -> Type[FrozenSet[T]]:\n    # __args__ is needed to conform to typing generics api\n    namespace = {'min_items': min_items, 'max_items': max_items, 'item_type': item_type, '__args__': [item_type]}\n    # We use new_class to be able to deal with Generic types\n    return new_class('ConstrainedFrozenSetValue', (ConstrainedFrozenSet,), {}, lambda ns: ns.update(namespace))\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LIST TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# This types superclass should be List[T], but cython chokes on that...\nclass ConstrainedList(list):  # type: ignore\n    # Needed for pydantic to detect that this is a list\n    __origin__ = list\n    __args__: Tuple[Type[T], ...]  # type: ignore\n\n    min_items: Optional[int] = None\n    max_items: Optional[int] = None\n    unique_items: Optional[bool] = None\n    item_type: Type[T]  # type: ignore\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.list_length_validator\n        if cls.unique_items:\n            yield cls.unique_items_validator\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(field_schema, minItems=cls.min_items, maxItems=cls.max_items, uniqueItems=cls.unique_items)\n\n    @classmethod\n    def list_length_validator(cls, v: 'Optional[List[T]]') -> 'Optional[List[T]]':\n        if v is None:\n            return None\n\n        v = list_validator(v)\n        v_len = len(v)\n\n        if cls.min_items is not None and v_len < cls.min_items:\n            raise errors.ListMinLengthError(limit_value=cls.min_items)\n\n        if cls.max_items is not None and v_len > cls.max_items:\n            raise errors.ListMaxLengthError(limit_value=cls.max_items)\n\n        return v\n\n    @classmethod\n    def unique_items_validator(cls, v: 'Optional[List[T]]') -> 'Optional[List[T]]':\n        if v is None:\n            return None\n\n        for i, value in enumerate(v, start=1):\n            if value in v[i:]:\n                raise errors.ListUniqueItemsError()\n\n        return v\n\n\ndef conlist(\n    item_type: Type[T], *, min_items: Optional[int] = None, max_items: Optional[int] = None, unique_items: bool = None\n) -> Type[List[T]]:\n    # __args__ is needed to conform to typing generics api\n    namespace = dict(\n        min_items=min_items, max_items=max_items, unique_items=unique_items, item_type=item_type, __args__=(item_type,)\n    )\n    # We use new_class to be able to deal with Generic types\n    return new_class('ConstrainedListValue', (ConstrainedList,), {}, lambda ns: ns.update(namespace))\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PYOBJECT TYPE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nif TYPE_CHECKING:\n    PyObject = Callable[..., Any]\nelse:\n\n    class PyObject:\n        validate_always = True\n\n        @classmethod\n        def __get_validators__(cls) -> 'CallableGenerator':\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, value: Any) -> Any:\n            if isinstance(value, Callable):\n                return value\n\n            try:\n                value = str_validator(value)\n            except errors.StrError:\n                raise errors.PyObjectError(error_message='value is neither a valid import path not a valid callable')\n\n            try:\n                return import_string(value)\n            except ImportError as e:\n                raise errors.PyObjectError(error_message=str(e))\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DECIMAL TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass ConstrainedDecimal(Decimal, metaclass=ConstrainedNumberMeta):\n    gt: OptionalIntFloatDecimal = None\n    ge: OptionalIntFloatDecimal = None\n    lt: OptionalIntFloatDecimal = None\n    le: OptionalIntFloatDecimal = None\n    max_digits: OptionalInt = None\n    decimal_places: OptionalInt = None\n    multiple_of: OptionalIntFloatDecimal = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            exclusiveMinimum=cls.gt,\n            exclusiveMaximum=cls.lt,\n            minimum=cls.ge,\n            maximum=cls.le,\n            multipleOf=cls.multiple_of,\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield decimal_validator\n        yield number_size_validator\n        yield number_multiple_validator\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Decimal) -> Decimal:\n        try:\n            normalized_value = value.normalize()\n        except InvalidOperation:\n            normalized_value = value\n        digit_tuple, exponent = normalized_value.as_tuple()[1:]\n        if exponent in {'F', 'n', 'N'}:\n            raise errors.DecimalIsNotFiniteError()\n\n        if exponent >= 0:\n            # A positive exponent adds that many trailing zeros.\n            digits = len(digit_tuple) + exponent\n            decimals = 0\n        else:\n            # If the absolute value of the negative exponent is larger than the\n            # number of digits, then it's the same as the number of digits,\n            # because it'll consume all of the digits in digit_tuple and then\n            # add abs(exponent) - len(digit_tuple) leading zeros after the\n            # decimal point.\n            if abs(exponent) > len(digit_tuple):\n                digits = decimals = abs(exponent)\n            else:\n                digits = len(digit_tuple)\n                decimals = abs(exponent)\n        whole_digits = digits - decimals\n\n        if cls.max_digits is not None and digits > cls.max_digits:\n            raise errors.DecimalMaxDigitsError(max_digits=cls.max_digits)\n\n        if cls.decimal_places is not None and decimals > cls.decimal_places:\n            raise errors.DecimalMaxPlacesError(decimal_places=cls.decimal_places)\n\n        if cls.max_digits is not None and cls.decimal_places is not None:\n            expected = cls.max_digits - cls.decimal_places\n            if whole_digits > expected:\n                raise errors.DecimalWholeDigitsError(whole_digits=expected)\n\n        return value\n\n\ndef condecimal(\n    *,\n    gt: Decimal = None,\n    ge: Decimal = None,\n    lt: Decimal = None,\n    le: Decimal = None,\n    max_digits: Optional[int] = None,\n    decimal_places: Optional[int] = None,\n    multiple_of: Decimal = None,\n) -> Type[Decimal]:\n    # use kwargs then define conf in a dict to aid with IDE type hinting\n    namespace = dict(\n        gt=gt, ge=ge, lt=lt, le=le, max_digits=max_digits, decimal_places=decimal_places, multiple_of=multiple_of\n    )\n    return type('ConstrainedDecimalValue', (ConstrainedDecimal,), namespace)\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ UUID TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nif TYPE_CHECKING:\n    UUID1 = UUID\n    UUID3 = UUID\n    UUID4 = UUID\n    UUID5 = UUID\nelse:\n\n    class UUID1(UUID):\n        _required_version = 1\n\n        @classmethod\n        def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n            field_schema.update(type='string', format=f'uuid{cls._required_version}')\n\n    class UUID3(UUID1):\n        _required_version = 3\n\n    class UUID4(UUID1):\n        _required_version = 4\n\n    class UUID5(UUID1):\n        _required_version = 5\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PATH TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nif TYPE_CHECKING:\n    FilePath = Path\n    DirectoryPath = Path\nelse:\n\n    class FilePath(Path):\n        @classmethod\n        def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n            field_schema.update(format='file-path')\n\n        @classmethod\n        def __get_validators__(cls) -> 'CallableGenerator':\n            yield path_validator\n            yield path_exists_validator\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, value: Path) -> Path:\n            if not value.is_file():\n                raise errors.PathNotAFileError(path=value)\n\n            return value\n\n    class DirectoryPath(Path):\n        @classmethod\n        def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n            field_schema.update(format='directory-path')\n\n        @classmethod\n        def __get_validators__(cls) -> 'CallableGenerator':\n            yield path_validator\n            yield path_exists_validator\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, value: Path) -> Path:\n            if not value.is_dir():\n                raise errors.PathNotADirectoryError(path=value)\n\n            return value\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ JSON TYPE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass JsonWrapper:\n    pass\n\n\nclass JsonMeta(type):\n    def __getitem__(self, t: Type[Any]) -> Type[JsonWrapper]:\n        if t is Any:\n            return Json  # allow Json[Any] to replecate plain Json\n        return _registered(type('JsonWrapperValue', (JsonWrapper,), {'inner_type': t}))\n\n\nif TYPE_CHECKING:\n    Json = Annotated[T, ...]  # Json[list[str]] will be recognized by type checkers as list[str]\n\nelse:\n\n    class Json(metaclass=JsonMeta):\n        @classmethod\n        def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n            field_schema.update(type='string', format='json-string')\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SECRET TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass SecretField(abc.ABC):\n    \"\"\"\n    Note: this should be implemented as a generic like `SecretField(ABC, Generic[T])`,\n          the `__init__()` should be part of the abstract class and the\n          `get_secret_value()` method should use the generic `T` type.\n\n          However Cython doesn't support very well generics at the moment and\n          the generated code fails to be imported (see\n          https://github.com/cython/cython/issues/2753).\n    \"\"\"\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, self.__class__) and self.get_secret_value() == other.get_secret_value()\n\n    def __str__(self) -> str:\n        return '**********' if self.get_secret_value() else ''\n\n    def __hash__(self) -> int:\n        return hash(self.get_secret_value())\n\n    @abc.abstractmethod\n    def get_secret_value(self) -> Any:  # pragma: no cover\n        ...\n\n\nclass SecretStr(SecretField):\n    min_length: OptionalInt = None\n    max_length: OptionalInt = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            type='string',\n            writeOnly=True,\n            format='password',\n            minLength=cls.min_length,\n            maxLength=cls.max_length,\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n        yield constr_length_validator\n\n    @classmethod\n    def validate(cls, value: Any) -> 'SecretStr':\n        if isinstance(value, cls):\n            return value\n        value = str_validator(value)\n        return cls(value)\n\n    def __init__(self, value: str):\n        self._secret_value = value\n\n    def __repr__(self) -> str:\n        return f\"SecretStr('{self}')\"\n\n    def __len__(self) -> int:\n        return len(self._secret_value)\n\n    def display(self) -> str:\n        warnings.warn('`secret_str.display()` is deprecated, use `str(secret_str)` instead', DeprecationWarning)\n        return str(self)\n\n    def get_secret_value(self) -> str:\n        return self._secret_value\n\n\nclass SecretBytes(SecretField):\n    min_length: OptionalInt = None\n    max_length: OptionalInt = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            type='string',\n            writeOnly=True,\n            format='password',\n            minLength=cls.min_length,\n            maxLength=cls.max_length,\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n        yield constr_length_validator\n\n    @classmethod\n    def validate(cls, value: Any) -> 'SecretBytes':\n        if isinstance(value, cls):\n            return value\n        value = bytes_validator(value)\n        return cls(value)\n\n    def __init__(self, value: bytes):\n        self._secret_value = value\n\n    def __repr__(self) -> str:\n        return f\"SecretBytes(b'{self}')\"\n\n    def __len__(self) -> int:\n        return len(self._secret_value)\n\n    def display(self) -> str:\n        warnings.warn('`secret_bytes.display()` is deprecated, use `str(secret_bytes)` instead', DeprecationWarning)\n        return str(self)\n\n    def get_secret_value(self) -> bytes:\n        return self._secret_value\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PAYMENT CARD TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass PaymentCardBrand(str, Enum):\n    # If you add another card type, please also add it to the\n    # Hypothesis strategy in `pydantic._hypothesis_plugin`.\n    amex = 'American Express'\n    mastercard = 'Mastercard'\n    visa = 'Visa'\n    other = 'other'\n\n    def __str__(self) -> str:\n        return self.value\n\n\nclass PaymentCardNumber(str):\n    \"\"\"\n    Based on: https://en.wikipedia.org/wiki/Payment_card_number\n    \"\"\"\n\n    strip_whitespace: ClassVar[bool] = True\n    min_length: ClassVar[int] = 12\n    max_length: ClassVar[int] = 19\n    bin: str\n    last4: str\n    brand: PaymentCardBrand\n\n    def __init__(self, card_number: str):\n        self.bin = card_number[:6]\n        self.last4 = card_number[-4:]\n        self.brand = self._get_brand(card_number)\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield str_validator\n        yield constr_strip_whitespace\n        yield constr_length_validator\n        yield cls.validate_digits\n        yield cls.validate_luhn_check_digit\n        yield cls\n        yield cls.validate_length_for_brand\n\n    @property\n    def masked(self) -> str:\n        num_masked = len(self) - 10  # len(bin) + len(last4) == 10\n        return f'{self.bin}{\"*\" * num_masked}{self.last4}'\n\n    @classmethod\n    def validate_digits(cls, card_number: str) -> str:\n        if not card_number.isdigit():\n            raise errors.NotDigitError\n        return card_number\n\n    @classmethod\n    def validate_luhn_check_digit(cls, card_number: str) -> str:\n        \"\"\"\n        Based on: https://en.wikipedia.org/wiki/Luhn_algorithm\n        \"\"\"\n        sum_ = int(card_number[-1])\n        length = len(card_number)\n        parity = length % 2\n        for i in range(length - 1):\n            digit = int(card_number[i])\n            if i % 2 == parity:\n                digit *= 2\n            if digit > 9:\n                digit -= 9\n            sum_ += digit\n        valid = sum_ % 10 == 0\n        if not valid:\n            raise errors.LuhnValidationError\n        return card_number\n\n    @classmethod\n    def validate_length_for_brand(cls, card_number: 'PaymentCardNumber') -> 'PaymentCardNumber':\n        \"\"\"\n        Validate length based on BIN for major brands:\n        https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN)\n        \"\"\"\n        required_length: Union[None, int, str] = None\n        if card_number.brand in PaymentCardBrand.mastercard:\n            required_length = 16\n            valid = len(card_number) == required_length\n        elif card_number.brand == PaymentCardBrand.visa:\n            required_length = '13, 16 or 19'\n            valid = len(card_number) in {13, 16, 19}\n        elif card_number.brand == PaymentCardBrand.amex:\n            required_length = 15\n            valid = len(card_number) == required_length\n        else:\n            valid = True\n        if not valid:\n            raise errors.InvalidLengthForBrand(brand=card_number.brand, required_length=required_length)\n        return card_number\n\n    @staticmethod\n    def _get_brand(card_number: str) -> PaymentCardBrand:\n        if card_number[0] == '4':\n            brand = PaymentCardBrand.visa\n        elif 51 <= int(card_number[:2]) <= 55:\n            brand = PaymentCardBrand.mastercard\n        elif card_number[:2] in {'34', '37'}:\n            brand = PaymentCardBrand.amex\n        else:\n            brand = PaymentCardBrand.other\n        return brand\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BYTE SIZE TYPE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBYTE_SIZES = {\n    'b': 1,\n    'kb': 10**3,\n    'mb': 10**6,\n    'gb': 10**9,\n    'tb': 10**12,\n    'pb': 10**15,\n    'eb': 10**18,\n    'kib': 2**10,\n    'mib': 2**20,\n    'gib': 2**30,\n    'tib': 2**40,\n    'pib': 2**50,\n    'eib': 2**60,\n}\nBYTE_SIZES.update({k.lower()[0]: v for k, v in BYTE_SIZES.items() if 'i' not in k})\nbyte_string_re = re.compile(r'^\\s*(\\d*\\.?\\d+)\\s*(\\w+)?', re.IGNORECASE)\n\n\nclass ByteSize(int):\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v: StrIntFloat) -> 'ByteSize':\n        try:\n            return cls(int(v))\n        except ValueError:\n            pass\n\n        str_match = byte_string_re.match(str(v))\n        if str_match is None:\n            raise errors.InvalidByteSize()\n\n        scalar, unit = str_match.groups()\n        if unit is None:\n            unit = 'b'\n\n        try:\n            unit_mult = BYTE_SIZES[unit.lower()]\n        except KeyError:\n            raise errors.InvalidByteSizeUnit(unit=unit)\n\n        return cls(int(float(scalar) * unit_mult))\n\n    def human_readable(self, decimal: bool = False) -> str:\n        if decimal:\n            divisor = 1000\n            units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n            final_unit = 'EB'\n        else:\n            divisor = 1024\n            units = ['B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB']\n            final_unit = 'EiB'\n\n        num = float(self)\n        for unit in units:\n            if abs(num) < divisor:\n                return f'{num:0.1f}{unit}'\n            num /= divisor\n\n        return f'{num:0.1f}{final_unit}'\n\n    def to(self, unit: str) -> float:\n        try:\n            unit_div = BYTE_SIZES[unit.lower()]\n        except KeyError:\n            raise errors.InvalidByteSizeUnit(unit=unit)\n\n        return self / unit_div\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DATE TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nif TYPE_CHECKING:\n    PastDate = date\n    FutureDate = date\nelse:\n\n    class PastDate(date):\n        @classmethod\n        def __get_validators__(cls) -> 'CallableGenerator':\n            yield parse_date\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, value: date) -> date:\n            if value >= date.today():\n                raise errors.DateNotInThePastError()\n\n            return value\n\n    class FutureDate(date):\n        @classmethod\n        def __get_validators__(cls) -> 'CallableGenerator':\n            yield parse_date\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, value: date) -> date:\n            if value <= date.today():\n                raise errors.DateNotInTheFutureError()\n\n            return value\n\n\nclass ConstrainedDate(date, metaclass=ConstrainedNumberMeta):\n    gt: OptionalDate = None\n    ge: OptionalDate = None\n    lt: OptionalDate = None\n    le: OptionalDate = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(field_schema, exclusiveMinimum=cls.gt, exclusiveMaximum=cls.lt, minimum=cls.ge, maximum=cls.le)\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield parse_date\n        yield number_size_validator\n\n\ndef condate(\n    *,\n    gt: date = None,\n    ge: date = None,\n    lt: date = None,\n    le: date = None,\n) -> Type[date]:\n    # use kwargs then define conf in a dict to aid with IDE type hinting\n    namespace = dict(gt=gt, ge=ge, lt=lt, le=le)\n    return type('ConstrainedDateValue', (ConstrainedDate,), namespace)\n", "pydantic/v1/annotated_types.py": "import sys\nfrom typing import TYPE_CHECKING, Any, Dict, FrozenSet, NamedTuple, Type\n\nfrom pydantic.v1.fields import Required\nfrom pydantic.v1.main import BaseModel, create_model\nfrom pydantic.v1.typing import is_typeddict, is_typeddict_special\n\nif TYPE_CHECKING:\n    from typing_extensions import TypedDict\n\nif sys.version_info < (3, 11):\n\n    def is_legacy_typeddict(typeddict_cls: Type['TypedDict']) -> bool:  # type: ignore[valid-type]\n        return is_typeddict(typeddict_cls) and type(typeddict_cls).__module__ == 'typing'\n\nelse:\n\n    def is_legacy_typeddict(_: Any) -> Any:\n        return False\n\n\ndef create_model_from_typeddict(\n    # Mypy bug: `Type[TypedDict]` is resolved as `Any` https://github.com/python/mypy/issues/11030\n    typeddict_cls: Type['TypedDict'],  # type: ignore[valid-type]\n    **kwargs: Any,\n) -> Type['BaseModel']:\n    \"\"\"\n    Create a `BaseModel` based on the fields of a `TypedDict`.\n    Since `typing.TypedDict` in Python 3.8 does not store runtime information about optional keys,\n    we raise an error if this happens (see https://bugs.python.org/issue38834).\n    \"\"\"\n    field_definitions: Dict[str, Any]\n\n    # Best case scenario: with python 3.9+ or when `TypedDict` is imported from `typing_extensions`\n    if not hasattr(typeddict_cls, '__required_keys__'):\n        raise TypeError(\n            'You should use `typing_extensions.TypedDict` instead of `typing.TypedDict` with Python < 3.9.2. '\n            'Without it, there is no way to differentiate required and optional fields when subclassed.'\n        )\n\n    if is_legacy_typeddict(typeddict_cls) and any(\n        is_typeddict_special(t) for t in typeddict_cls.__annotations__.values()\n    ):\n        raise TypeError(\n            'You should use `typing_extensions.TypedDict` instead of `typing.TypedDict` with Python < 3.11. '\n            'Without it, there is no way to reflect Required/NotRequired keys.'\n        )\n\n    required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]\n    field_definitions = {\n        field_name: (field_type, Required if field_name in required_keys else None)\n        for field_name, field_type in typeddict_cls.__annotations__.items()\n    }\n\n    return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)\n\n\ndef create_model_from_namedtuple(namedtuple_cls: Type['NamedTuple'], **kwargs: Any) -> Type['BaseModel']:\n    \"\"\"\n    Create a `BaseModel` based on the fields of a named tuple.\n    A named tuple can be created with `typing.NamedTuple` and declared annotations\n    but also with `collections.namedtuple`, in this case we consider all fields\n    to have type `Any`.\n    \"\"\"\n    # With python 3.10+, `__annotations__` always exists but can be empty hence the `getattr... or...` logic\n    namedtuple_annotations: Dict[str, Type[Any]] = getattr(namedtuple_cls, '__annotations__', None) or {\n        k: Any for k in namedtuple_cls._fields\n    }\n    field_definitions: Dict[str, Any] = {\n        field_name: (field_type, Required) for field_name, field_type in namedtuple_annotations.items()\n    }\n    return create_model(namedtuple_cls.__name__, **kwargs, **field_definitions)\n", "pydantic/v1/main.py": "import warnings\nfrom abc import ABCMeta\nfrom copy import deepcopy\nfrom enum import Enum\nfrom functools import partial\nfrom pathlib import Path\nfrom types import FunctionType, prepare_class, resolve_bases\nfrom typing import (\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    List,\n    Mapping,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n    no_type_check,\n    overload,\n)\n\nfrom typing_extensions import dataclass_transform\n\nfrom pydantic.v1.class_validators import ValidatorGroup, extract_root_validators, extract_validators, inherit_validators\nfrom pydantic.v1.config import BaseConfig, Extra, inherit_config, prepare_config\nfrom pydantic.v1.error_wrappers import ErrorWrapper, ValidationError\nfrom pydantic.v1.errors import ConfigError, DictError, ExtraError, MissingError\nfrom pydantic.v1.fields import (\n    MAPPING_LIKE_SHAPES,\n    Field,\n    ModelField,\n    ModelPrivateAttr,\n    PrivateAttr,\n    Undefined,\n    is_finalvar_with_default_val,\n)\nfrom pydantic.v1.json import custom_pydantic_encoder, pydantic_encoder\nfrom pydantic.v1.parse import Protocol, load_file, load_str_bytes\nfrom pydantic.v1.schema import default_ref_template, model_schema\nfrom pydantic.v1.types import PyObject, StrBytes\nfrom pydantic.v1.typing import (\n    AnyCallable,\n    get_args,\n    get_origin,\n    is_classvar,\n    is_namedtuple,\n    is_union,\n    resolve_annotations,\n    update_model_forward_refs,\n)\nfrom pydantic.v1.utils import (\n    DUNDER_ATTRIBUTES,\n    ROOT_KEY,\n    ClassAttribute,\n    GetterDict,\n    Representation,\n    ValueItems,\n    generate_model_signature,\n    is_valid_field,\n    is_valid_private_name,\n    lenient_issubclass,\n    sequence_like,\n    smart_deepcopy,\n    unique_list,\n    validate_field_name,\n)\n\nif TYPE_CHECKING:\n    from inspect import Signature\n\n    from pydantic.v1.class_validators import ValidatorListDict\n    from pydantic.v1.types import ModelOrDc\n    from pydantic.v1.typing import (\n        AbstractSetIntStr,\n        AnyClassMethod,\n        CallableGenerator,\n        DictAny,\n        DictStrAny,\n        MappingIntStrAny,\n        ReprArgs,\n        SetStr,\n        TupleGenerator,\n    )\n\n    Model = TypeVar('Model', bound='BaseModel')\n\n__all__ = 'BaseModel', 'create_model', 'validate_model'\n\n_T = TypeVar('_T')\n\n\ndef validate_custom_root_type(fields: Dict[str, ModelField]) -> None:\n    if len(fields) > 1:\n        raise ValueError(f'{ROOT_KEY} cannot be mixed with other fields')\n\n\ndef generate_hash_function(frozen: bool) -> Optional[Callable[[Any], int]]:\n    def hash_function(self_: Any) -> int:\n        return hash(self_.__class__) + hash(tuple(self_.__dict__.values()))\n\n    return hash_function if frozen else None\n\n\n# If a field is of type `Callable`, its default value should be a function and cannot to ignored.\nANNOTATED_FIELD_UNTOUCHED_TYPES: Tuple[Any, ...] = (property, type, classmethod, staticmethod)\n# When creating a `BaseModel` instance, we bypass all the methods, properties... added to the model\nUNTOUCHED_TYPES: Tuple[Any, ...] = (FunctionType,) + ANNOTATED_FIELD_UNTOUCHED_TYPES\n# Note `ModelMetaclass` refers to `BaseModel`, but is also used to *create* `BaseModel`, so we need to add this extra\n# (somewhat hacky) boolean to keep track of whether we've created the `BaseModel` class yet, and therefore whether it's\n# safe to refer to it. If it *hasn't* been created, we assume that the `__new__` call we're in the middle of is for\n# the `BaseModel` class, since that's defined immediately after the metaclass.\n_is_base_model_class_defined = False\n\n\n@dataclass_transform(kw_only_default=True, field_specifiers=(Field,))\nclass ModelMetaclass(ABCMeta):\n    @no_type_check  # noqa C901\n    def __new__(mcs, name, bases, namespace, **kwargs):  # noqa C901\n        fields: Dict[str, ModelField] = {}\n        config = BaseConfig\n        validators: 'ValidatorListDict' = {}\n\n        pre_root_validators, post_root_validators = [], []\n        private_attributes: Dict[str, ModelPrivateAttr] = {}\n        base_private_attributes: Dict[str, ModelPrivateAttr] = {}\n        slots: SetStr = namespace.get('__slots__', ())\n        slots = {slots} if isinstance(slots, str) else set(slots)\n        class_vars: SetStr = set()\n        hash_func: Optional[Callable[[Any], int]] = None\n\n        for base in reversed(bases):\n            if _is_base_model_class_defined and issubclass(base, BaseModel) and base != BaseModel:\n                fields.update(smart_deepcopy(base.__fields__))\n                config = inherit_config(base.__config__, config)\n                validators = inherit_validators(base.__validators__, validators)\n                pre_root_validators += base.__pre_root_validators__\n                post_root_validators += base.__post_root_validators__\n                base_private_attributes.update(base.__private_attributes__)\n                class_vars.update(base.__class_vars__)\n                hash_func = base.__hash__\n\n        resolve_forward_refs = kwargs.pop('__resolve_forward_refs__', True)\n        allowed_config_kwargs: SetStr = {\n            key\n            for key in dir(config)\n            if not (key.startswith('__') and key.endswith('__'))  # skip dunder methods and attributes\n        }\n        config_kwargs = {key: kwargs.pop(key) for key in kwargs.keys() & allowed_config_kwargs}\n        config_from_namespace = namespace.get('Config')\n        if config_kwargs and config_from_namespace:\n            raise TypeError('Specifying config in two places is ambiguous, use either Config attribute or class kwargs')\n        config = inherit_config(config_from_namespace, config, **config_kwargs)\n\n        validators = inherit_validators(extract_validators(namespace), validators)\n        vg = ValidatorGroup(validators)\n\n        for f in fields.values():\n            f.set_config(config)\n            extra_validators = vg.get_validators(f.name)\n            if extra_validators:\n                f.class_validators.update(extra_validators)\n                # re-run prepare to add extra validators\n                f.populate_validators()\n\n        prepare_config(config, name)\n\n        untouched_types = ANNOTATED_FIELD_UNTOUCHED_TYPES\n\n        def is_untouched(v: Any) -> bool:\n            return isinstance(v, untouched_types) or v.__class__.__name__ == 'cython_function_or_method'\n\n        if (namespace.get('__module__'), namespace.get('__qualname__')) != ('pydantic.main', 'BaseModel'):\n            annotations = resolve_annotations(namespace.get('__annotations__', {}), namespace.get('__module__', None))\n            # annotation only fields need to come first in fields\n            for ann_name, ann_type in annotations.items():\n                if is_classvar(ann_type):\n                    class_vars.add(ann_name)\n                elif is_finalvar_with_default_val(ann_type, namespace.get(ann_name, Undefined)):\n                    class_vars.add(ann_name)\n                elif is_valid_field(ann_name):\n                    validate_field_name(bases, ann_name)\n                    value = namespace.get(ann_name, Undefined)\n                    allowed_types = get_args(ann_type) if is_union(get_origin(ann_type)) else (ann_type,)\n                    if (\n                        is_untouched(value)\n                        and ann_type != PyObject\n                        and not any(\n                            lenient_issubclass(get_origin(allowed_type), Type) for allowed_type in allowed_types\n                        )\n                    ):\n                        continue\n                    fields[ann_name] = ModelField.infer(\n                        name=ann_name,\n                        value=value,\n                        annotation=ann_type,\n                        class_validators=vg.get_validators(ann_name),\n                        config=config,\n                    )\n                elif ann_name not in namespace and config.underscore_attrs_are_private:\n                    private_attributes[ann_name] = PrivateAttr()\n\n            untouched_types = UNTOUCHED_TYPES + config.keep_untouched\n            for var_name, value in namespace.items():\n                can_be_changed = var_name not in class_vars and not is_untouched(value)\n                if isinstance(value, ModelPrivateAttr):\n                    if not is_valid_private_name(var_name):\n                        raise NameError(\n                            f'Private attributes \"{var_name}\" must not be a valid field name; '\n                            f'Use sunder or dunder names, e. g. \"_{var_name}\" or \"__{var_name}__\"'\n                        )\n                    private_attributes[var_name] = value\n                elif config.underscore_attrs_are_private and is_valid_private_name(var_name) and can_be_changed:\n                    private_attributes[var_name] = PrivateAttr(default=value)\n                elif is_valid_field(var_name) and var_name not in annotations and can_be_changed:\n                    validate_field_name(bases, var_name)\n                    inferred = ModelField.infer(\n                        name=var_name,\n                        value=value,\n                        annotation=annotations.get(var_name, Undefined),\n                        class_validators=vg.get_validators(var_name),\n                        config=config,\n                    )\n                    if var_name in fields:\n                        if lenient_issubclass(inferred.type_, fields[var_name].type_):\n                            inferred.type_ = fields[var_name].type_\n                        else:\n                            raise TypeError(\n                                f'The type of {name}.{var_name} differs from the new default value; '\n                                f'if you wish to change the type of this field, please use a type annotation'\n                            )\n                    fields[var_name] = inferred\n\n        _custom_root_type = ROOT_KEY in fields\n        if _custom_root_type:\n            validate_custom_root_type(fields)\n        vg.check_for_unused()\n        if config.json_encoders:\n            json_encoder = partial(custom_pydantic_encoder, config.json_encoders)\n        else:\n            json_encoder = pydantic_encoder\n        pre_rv_new, post_rv_new = extract_root_validators(namespace)\n\n        if hash_func is None:\n            hash_func = generate_hash_function(config.frozen)\n\n        exclude_from_namespace = fields | private_attributes.keys() | {'__slots__'}\n        new_namespace = {\n            '__config__': config,\n            '__fields__': fields,\n            '__exclude_fields__': {\n                name: field.field_info.exclude for name, field in fields.items() if field.field_info.exclude is not None\n            }\n            or None,\n            '__include_fields__': {\n                name: field.field_info.include for name, field in fields.items() if field.field_info.include is not None\n            }\n            or None,\n            '__validators__': vg.validators,\n            '__pre_root_validators__': unique_list(\n                pre_root_validators + pre_rv_new,\n                name_factory=lambda v: v.__name__,\n            ),\n            '__post_root_validators__': unique_list(\n                post_root_validators + post_rv_new,\n                name_factory=lambda skip_on_failure_and_v: skip_on_failure_and_v[1].__name__,\n            ),\n            '__schema_cache__': {},\n            '__json_encoder__': staticmethod(json_encoder),\n            '__custom_root_type__': _custom_root_type,\n            '__private_attributes__': {**base_private_attributes, **private_attributes},\n            '__slots__': slots | private_attributes.keys(),\n            '__hash__': hash_func,\n            '__class_vars__': class_vars,\n            **{n: v for n, v in namespace.items() if n not in exclude_from_namespace},\n        }\n\n        cls = super().__new__(mcs, name, bases, new_namespace, **kwargs)\n        # set __signature__ attr only for model class, but not for its instances\n        cls.__signature__ = ClassAttribute('__signature__', generate_model_signature(cls.__init__, fields, config))\n        if resolve_forward_refs:\n            cls.__try_update_forward_refs__()\n\n        # preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\n        # for attributes not in `new_namespace` (e.g. private attributes)\n        for name, obj in namespace.items():\n            if name not in new_namespace:\n                set_name = getattr(obj, '__set_name__', None)\n                if callable(set_name):\n                    set_name(cls, name)\n\n        return cls\n\n    def __instancecheck__(self, instance: Any) -> bool:\n        \"\"\"\n        Avoid calling ABC _abc_subclasscheck unless we're pretty sure.\n\n        See #3829 and python/cpython#92810\n        \"\"\"\n        return hasattr(instance, '__fields__') and super().__instancecheck__(instance)\n\n\nobject_setattr = object.__setattr__\n\n\nclass BaseModel(Representation, metaclass=ModelMetaclass):\n    if TYPE_CHECKING:\n        # populated by the metaclass, defined here to help IDEs only\n        __fields__: ClassVar[Dict[str, ModelField]] = {}\n        __include_fields__: ClassVar[Optional[Mapping[str, Any]]] = None\n        __exclude_fields__: ClassVar[Optional[Mapping[str, Any]]] = None\n        __validators__: ClassVar[Dict[str, AnyCallable]] = {}\n        __pre_root_validators__: ClassVar[List[AnyCallable]]\n        __post_root_validators__: ClassVar[List[Tuple[bool, AnyCallable]]]\n        __config__: ClassVar[Type[BaseConfig]] = BaseConfig\n        __json_encoder__: ClassVar[Callable[[Any], Any]] = lambda x: x\n        __schema_cache__: ClassVar['DictAny'] = {}\n        __custom_root_type__: ClassVar[bool] = False\n        __signature__: ClassVar['Signature']\n        __private_attributes__: ClassVar[Dict[str, ModelPrivateAttr]]\n        __class_vars__: ClassVar[SetStr]\n        __fields_set__: ClassVar[SetStr] = set()\n\n    Config = BaseConfig\n    __slots__ = ('__dict__', '__fields_set__')\n    __doc__ = ''  # Null out the Representation docstring\n\n    def __init__(__pydantic_self__, **data: Any) -> None:\n        \"\"\"\n        Create a new model by parsing and validating input data from keyword arguments.\n\n        Raises ValidationError if the input data cannot be parsed to form a valid model.\n        \"\"\"\n        # Uses something other than `self` the first arg to allow \"self\" as a settable attribute\n        values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n        if validation_error:\n            raise validation_error\n        try:\n            object_setattr(__pydantic_self__, '__dict__', values)\n        except TypeError as e:\n            raise TypeError(\n                'Model values must be a dict; you may not have returned a dictionary from a root validator'\n            ) from e\n        object_setattr(__pydantic_self__, '__fields_set__', fields_set)\n        __pydantic_self__._init_private_attributes()\n\n    @no_type_check\n    def __setattr__(self, name, value):  # noqa: C901 (ignore complexity)\n        if name in self.__private_attributes__ or name in DUNDER_ATTRIBUTES:\n            return object_setattr(self, name, value)\n\n        if self.__config__.extra is not Extra.allow and name not in self.__fields__:\n            raise ValueError(f'\"{self.__class__.__name__}\" object has no field \"{name}\"')\n        elif not self.__config__.allow_mutation or self.__config__.frozen:\n            raise TypeError(f'\"{self.__class__.__name__}\" is immutable and does not support item assignment')\n        elif name in self.__fields__ and self.__fields__[name].final:\n            raise TypeError(\n                f'\"{self.__class__.__name__}\" object \"{name}\" field is final and does not support reassignment'\n            )\n        elif self.__config__.validate_assignment:\n            new_values = {**self.__dict__, name: value}\n\n            for validator in self.__pre_root_validators__:\n                try:\n                    new_values = validator(self.__class__, new_values)\n                except (ValueError, TypeError, AssertionError) as exc:\n                    raise ValidationError([ErrorWrapper(exc, loc=ROOT_KEY)], self.__class__)\n\n            known_field = self.__fields__.get(name, None)\n            if known_field:\n                # We want to\n                # - make sure validators are called without the current value for this field inside `values`\n                # - keep other values (e.g. submodels) untouched (using `BaseModel.dict()` will change them into dicts)\n                # - keep the order of the fields\n                if not known_field.field_info.allow_mutation:\n                    raise TypeError(f'\"{known_field.name}\" has allow_mutation set to False and cannot be assigned')\n                dict_without_original_value = {k: v for k, v in self.__dict__.items() if k != name}\n                value, error_ = known_field.validate(value, dict_without_original_value, loc=name, cls=self.__class__)\n                if error_:\n                    raise ValidationError([error_], self.__class__)\n                else:\n                    new_values[name] = value\n\n            errors = []\n            for skip_on_failure, validator in self.__post_root_validators__:\n                if skip_on_failure and errors:\n                    continue\n                try:\n                    new_values = validator(self.__class__, new_values)\n                except (ValueError, TypeError, AssertionError) as exc:\n                    errors.append(ErrorWrapper(exc, loc=ROOT_KEY))\n            if errors:\n                raise ValidationError(errors, self.__class__)\n\n            # update the whole __dict__ as other values than just `value`\n            # may be changed (e.g. with `root_validator`)\n            object_setattr(self, '__dict__', new_values)\n        else:\n            self.__dict__[name] = value\n\n        self.__fields_set__.add(name)\n\n    def __getstate__(self) -> 'DictAny':\n        private_attrs = ((k, getattr(self, k, Undefined)) for k in self.__private_attributes__)\n        return {\n            '__dict__': self.__dict__,\n            '__fields_set__': self.__fields_set__,\n            '__private_attribute_values__': {k: v for k, v in private_attrs if v is not Undefined},\n        }\n\n    def __setstate__(self, state: 'DictAny') -> None:\n        object_setattr(self, '__dict__', state['__dict__'])\n        object_setattr(self, '__fields_set__', state['__fields_set__'])\n        for name, value in state.get('__private_attribute_values__', {}).items():\n            object_setattr(self, name, value)\n\n    def _init_private_attributes(self) -> None:\n        for name, private_attr in self.__private_attributes__.items():\n            default = private_attr.get_default()\n            if default is not Undefined:\n                object_setattr(self, name, default)\n\n    def dict(\n        self,\n        *,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        by_alias: bool = False,\n        skip_defaults: Optional[bool] = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n    ) -> 'DictStrAny':\n        \"\"\"\n        Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\n        \"\"\"\n        if skip_defaults is not None:\n            warnings.warn(\n                f'{self.__class__.__name__}.dict(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\n                DeprecationWarning,\n            )\n            exclude_unset = skip_defaults\n\n        return dict(\n            self._iter(\n                to_dict=True,\n                by_alias=by_alias,\n                include=include,\n                exclude=exclude,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        )\n\n    def json(\n        self,\n        *,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        by_alias: bool = False,\n        skip_defaults: Optional[bool] = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        encoder: Optional[Callable[[Any], Any]] = None,\n        models_as_dict: bool = True,\n        **dumps_kwargs: Any,\n    ) -> str:\n        \"\"\"\n        Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n\n        `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n        \"\"\"\n        if skip_defaults is not None:\n            warnings.warn(\n                f'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\n                DeprecationWarning,\n            )\n            exclude_unset = skip_defaults\n        encoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n\n        # We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n        # because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n        # This allows users to write custom JSON encoders for given `BaseModel` classes.\n        data = dict(\n            self._iter(\n                to_dict=models_as_dict,\n                by_alias=by_alias,\n                include=include,\n                exclude=exclude,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        )\n        if self.__custom_root_type__:\n            data = data[ROOT_KEY]\n        return self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n\n    @classmethod\n    def _enforce_dict_if_root(cls, obj: Any) -> Any:\n        if cls.__custom_root_type__ and (\n            not (isinstance(obj, dict) and obj.keys() == {ROOT_KEY})\n            and not (isinstance(obj, BaseModel) and obj.__fields__.keys() == {ROOT_KEY})\n            or cls.__fields__[ROOT_KEY].shape in MAPPING_LIKE_SHAPES\n        ):\n            return {ROOT_KEY: obj}\n        else:\n            return obj\n\n    @classmethod\n    def parse_obj(cls: Type['Model'], obj: Any) -> 'Model':\n        obj = cls._enforce_dict_if_root(obj)\n        if not isinstance(obj, dict):\n            try:\n                obj = dict(obj)\n            except (TypeError, ValueError) as e:\n                exc = TypeError(f'{cls.__name__} expected dict not {obj.__class__.__name__}')\n                raise ValidationError([ErrorWrapper(exc, loc=ROOT_KEY)], cls) from e\n        return cls(**obj)\n\n    @classmethod\n    def parse_raw(\n        cls: Type['Model'],\n        b: StrBytes,\n        *,\n        content_type: str = None,\n        encoding: str = 'utf8',\n        proto: Protocol = None,\n        allow_pickle: bool = False,\n    ) -> 'Model':\n        try:\n            obj = load_str_bytes(\n                b,\n                proto=proto,\n                content_type=content_type,\n                encoding=encoding,\n                allow_pickle=allow_pickle,\n                json_loads=cls.__config__.json_loads,\n            )\n        except (ValueError, TypeError, UnicodeDecodeError) as e:\n            raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n        return cls.parse_obj(obj)\n\n    @classmethod\n    def parse_file(\n        cls: Type['Model'],\n        path: Union[str, Path],\n        *,\n        content_type: str = None,\n        encoding: str = 'utf8',\n        proto: Protocol = None,\n        allow_pickle: bool = False,\n    ) -> 'Model':\n        obj = load_file(\n            path,\n            proto=proto,\n            content_type=content_type,\n            encoding=encoding,\n            allow_pickle=allow_pickle,\n            json_loads=cls.__config__.json_loads,\n        )\n        return cls.parse_obj(obj)\n\n    @classmethod\n    def from_orm(cls: Type['Model'], obj: Any) -> 'Model':\n        if not cls.__config__.orm_mode:\n            raise ConfigError('You must have the config attribute orm_mode=True to use from_orm')\n        obj = {ROOT_KEY: obj} if cls.__custom_root_type__ else cls._decompose_class(obj)\n        m = cls.__new__(cls)\n        values, fields_set, validation_error = validate_model(cls, obj)\n        if validation_error:\n            raise validation_error\n        object_setattr(m, '__dict__', values)\n        object_setattr(m, '__fields_set__', fields_set)\n        m._init_private_attributes()\n        return m\n\n    @classmethod\n    def construct(cls: Type['Model'], _fields_set: Optional['SetStr'] = None, **values: Any) -> 'Model':\n        \"\"\"\n        Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n        Default values are respected, but no other validation is performed.\n        Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n        \"\"\"\n        m = cls.__new__(cls)\n        fields_values: Dict[str, Any] = {}\n        for name, field in cls.__fields__.items():\n            if field.alt_alias and field.alias in values:\n                fields_values[name] = values[field.alias]\n            elif name in values:\n                fields_values[name] = values[name]\n            elif not field.required:\n                fields_values[name] = field.get_default()\n        fields_values.update(values)\n        object_setattr(m, '__dict__', fields_values)\n        if _fields_set is None:\n            _fields_set = set(values.keys())\n        object_setattr(m, '__fields_set__', _fields_set)\n        m._init_private_attributes()\n        return m\n\n    def _copy_and_set_values(self: 'Model', values: 'DictStrAny', fields_set: 'SetStr', *, deep: bool) -> 'Model':\n        if deep:\n            # chances of having empty dict here are quite low for using smart_deepcopy\n            values = deepcopy(values)\n\n        cls = self.__class__\n        m = cls.__new__(cls)\n        object_setattr(m, '__dict__', values)\n        object_setattr(m, '__fields_set__', fields_set)\n        for name in self.__private_attributes__:\n            value = getattr(self, name, Undefined)\n            if value is not Undefined:\n                if deep:\n                    value = deepcopy(value)\n                object_setattr(m, name, value)\n\n        return m\n\n    def copy(\n        self: 'Model',\n        *,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        update: Optional['DictStrAny'] = None,\n        deep: bool = False,\n    ) -> 'Model':\n        \"\"\"\n        Duplicate a model, optionally choose which fields to include, exclude and change.\n\n        :param include: fields to include in new model\n        :param exclude: fields to exclude from new model, as with values this takes precedence over include\n        :param update: values to change/add in the new model. Note: the data is not validated before creating\n            the new model: you should trust this data\n        :param deep: set to `True` to make a deep copy of the model\n        :return: new model instance\n        \"\"\"\n\n        values = dict(\n            self._iter(to_dict=False, by_alias=False, include=include, exclude=exclude, exclude_unset=False),\n            **(update or {}),\n        )\n\n        # new `__fields_set__` can have unset optional fields with a set value in `update` kwarg\n        if update:\n            fields_set = self.__fields_set__ | update.keys()\n        else:\n            fields_set = set(self.__fields_set__)\n\n        return self._copy_and_set_values(values, fields_set, deep=deep)\n\n    @classmethod\n    def schema(cls, by_alias: bool = True, ref_template: str = default_ref_template) -> 'DictStrAny':\n        cached = cls.__schema_cache__.get((by_alias, ref_template))\n        if cached is not None:\n            return cached\n        s = model_schema(cls, by_alias=by_alias, ref_template=ref_template)\n        cls.__schema_cache__[(by_alias, ref_template)] = s\n        return s\n\n    @classmethod\n    def schema_json(\n        cls, *, by_alias: bool = True, ref_template: str = default_ref_template, **dumps_kwargs: Any\n    ) -> str:\n        from pydantic.v1.json import pydantic_encoder\n\n        return cls.__config__.json_dumps(\n            cls.schema(by_alias=by_alias, ref_template=ref_template), default=pydantic_encoder, **dumps_kwargs\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n\n    @classmethod\n    def validate(cls: Type['Model'], value: Any) -> 'Model':\n        if isinstance(value, cls):\n            copy_on_model_validation = cls.__config__.copy_on_model_validation\n            # whether to deep or shallow copy the model on validation, None means do not copy\n            deep_copy: Optional[bool] = None\n            if copy_on_model_validation not in {'deep', 'shallow', 'none'}:\n                # Warn about deprecated behavior\n                warnings.warn(\n                    \"`copy_on_model_validation` should be a string: 'deep', 'shallow' or 'none'\", DeprecationWarning\n                )\n                if copy_on_model_validation:\n                    deep_copy = False\n\n            if copy_on_model_validation == 'shallow':\n                # shallow copy\n                deep_copy = False\n            elif copy_on_model_validation == 'deep':\n                # deep copy\n                deep_copy = True\n\n            if deep_copy is None:\n                return value\n            else:\n                return value._copy_and_set_values(value.__dict__, value.__fields_set__, deep=deep_copy)\n\n        value = cls._enforce_dict_if_root(value)\n\n        if isinstance(value, dict):\n            return cls(**value)\n        elif cls.__config__.orm_mode:\n            return cls.from_orm(value)\n        else:\n            try:\n                value_as_dict = dict(value)\n            except (TypeError, ValueError) as e:\n                raise DictError() from e\n            return cls(**value_as_dict)\n\n    @classmethod\n    def _decompose_class(cls: Type['Model'], obj: Any) -> GetterDict:\n        if isinstance(obj, GetterDict):\n            return obj\n        return cls.__config__.getter_dict(obj)\n\n    @classmethod\n    @no_type_check\n    def _get_value(\n        cls,\n        v: Any,\n        to_dict: bool,\n        by_alias: bool,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']],\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']],\n        exclude_unset: bool,\n        exclude_defaults: bool,\n        exclude_none: bool,\n    ) -> Any:\n        if isinstance(v, BaseModel):\n            if to_dict:\n                v_dict = v.dict(\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    include=include,\n                    exclude=exclude,\n                    exclude_none=exclude_none,\n                )\n                if ROOT_KEY in v_dict:\n                    return v_dict[ROOT_KEY]\n                return v_dict\n            else:\n                return v.copy(include=include, exclude=exclude)\n\n        value_exclude = ValueItems(v, exclude) if exclude else None\n        value_include = ValueItems(v, include) if include else None\n\n        if isinstance(v, dict):\n            return {\n                k_: cls._get_value(\n                    v_,\n                    to_dict=to_dict,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    include=value_include and value_include.for_element(k_),\n                    exclude=value_exclude and value_exclude.for_element(k_),\n                    exclude_none=exclude_none,\n                )\n                for k_, v_ in v.items()\n                if (not value_exclude or not value_exclude.is_excluded(k_))\n                and (not value_include or value_include.is_included(k_))\n            }\n\n        elif sequence_like(v):\n            seq_args = (\n                cls._get_value(\n                    v_,\n                    to_dict=to_dict,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    include=value_include and value_include.for_element(i),\n                    exclude=value_exclude and value_exclude.for_element(i),\n                    exclude_none=exclude_none,\n                )\n                for i, v_ in enumerate(v)\n                if (not value_exclude or not value_exclude.is_excluded(i))\n                and (not value_include or value_include.is_included(i))\n            )\n\n            return v.__class__(*seq_args) if is_namedtuple(v.__class__) else v.__class__(seq_args)\n\n        elif isinstance(v, Enum) and getattr(cls.Config, 'use_enum_values', False):\n            return v.value\n\n        else:\n            return v\n\n    @classmethod\n    def __try_update_forward_refs__(cls, **localns: Any) -> None:\n        \"\"\"\n        Same as update_forward_refs but will not raise exception\n        when forward references are not defined.\n        \"\"\"\n        update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))\n\n    @classmethod\n    def update_forward_refs(cls, **localns: Any) -> None:\n        \"\"\"\n        Try to update ForwardRefs on fields based on this Model, globalns and localns.\n        \"\"\"\n        update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns)\n\n    def __iter__(self) -> 'TupleGenerator':\n        \"\"\"\n        so `dict(model)` works\n        \"\"\"\n        yield from self.__dict__.items()\n\n    def _iter(\n        self,\n        to_dict: bool = False,\n        by_alias: bool = False,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n    ) -> 'TupleGenerator':\n        # Merge field set excludes with explicit exclude parameter with explicit overriding field set options.\n        # The extra \"is not None\" guards are not logically necessary but optimizes performance for the simple case.\n        if exclude is not None or self.__exclude_fields__ is not None:\n            exclude = ValueItems.merge(self.__exclude_fields__, exclude)\n\n        if include is not None or self.__include_fields__ is not None:\n            include = ValueItems.merge(self.__include_fields__, include, intersect=True)\n\n        allowed_keys = self._calculate_keys(\n            include=include, exclude=exclude, exclude_unset=exclude_unset  # type: ignore\n        )\n        if allowed_keys is None and not (to_dict or by_alias or exclude_unset or exclude_defaults or exclude_none):\n            # huge boost for plain _iter()\n            yield from self.__dict__.items()\n            return\n\n        value_exclude = ValueItems(self, exclude) if exclude is not None else None\n        value_include = ValueItems(self, include) if include is not None else None\n\n        for field_key, v in self.__dict__.items():\n            if (allowed_keys is not None and field_key not in allowed_keys) or (exclude_none and v is None):\n                continue\n\n            if exclude_defaults:\n                model_field = self.__fields__.get(field_key)\n                if not getattr(model_field, 'required', True) and getattr(model_field, 'default', _missing) == v:\n                    continue\n\n            if by_alias and field_key in self.__fields__:\n                dict_key = self.__fields__[field_key].alias\n            else:\n                dict_key = field_key\n\n            if to_dict or value_include or value_exclude:\n                v = self._get_value(\n                    v,\n                    to_dict=to_dict,\n                    by_alias=by_alias,\n                    include=value_include and value_include.for_element(field_key),\n                    exclude=value_exclude and value_exclude.for_element(field_key),\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    exclude_none=exclude_none,\n                )\n            yield dict_key, v\n\n    def _calculate_keys(\n        self,\n        include: Optional['MappingIntStrAny'],\n        exclude: Optional['MappingIntStrAny'],\n        exclude_unset: bool,\n        update: Optional['DictStrAny'] = None,\n    ) -> Optional[AbstractSet[str]]:\n        if include is None and exclude is None and exclude_unset is False:\n            return None\n\n        keys: AbstractSet[str]\n        if exclude_unset:\n            keys = self.__fields_set__.copy()\n        else:\n            keys = self.__dict__.keys()\n\n        if include is not None:\n            keys &= include.keys()\n\n        if update:\n            keys -= update.keys()\n\n        if exclude:\n            keys -= {k for k, v in exclude.items() if ValueItems.is_true(v)}\n\n        return keys\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, BaseModel):\n            return self.dict() == other.dict()\n        else:\n            return self.dict() == other\n\n    def __repr_args__(self) -> 'ReprArgs':\n        return [\n            (k, v)\n            for k, v in self.__dict__.items()\n            if k not in DUNDER_ATTRIBUTES and (k not in self.__fields__ or self.__fields__[k].field_info.repr)\n        ]\n\n\n_is_base_model_class_defined = True\n\n\n@overload\ndef create_model(\n    __model_name: str,\n    *,\n    __config__: Optional[Type[BaseConfig]] = None,\n    __base__: None = None,\n    __module__: str = __name__,\n    __validators__: Dict[str, 'AnyClassMethod'] = None,\n    __cls_kwargs__: Dict[str, Any] = None,\n    **field_definitions: Any,\n) -> Type['BaseModel']:\n    ...\n\n\n@overload\ndef create_model(\n    __model_name: str,\n    *,\n    __config__: Optional[Type[BaseConfig]] = None,\n    __base__: Union[Type['Model'], Tuple[Type['Model'], ...]],\n    __module__: str = __name__,\n    __validators__: Dict[str, 'AnyClassMethod'] = None,\n    __cls_kwargs__: Dict[str, Any] = None,\n    **field_definitions: Any,\n) -> Type['Model']:\n    ...\n\n\ndef create_model(\n    __model_name: str,\n    *,\n    __config__: Optional[Type[BaseConfig]] = None,\n    __base__: Union[None, Type['Model'], Tuple[Type['Model'], ...]] = None,\n    __module__: str = __name__,\n    __validators__: Dict[str, 'AnyClassMethod'] = None,\n    __cls_kwargs__: Dict[str, Any] = None,\n    __slots__: Optional[Tuple[str, ...]] = None,\n    **field_definitions: Any,\n) -> Type['Model']:\n    \"\"\"\n    Dynamically create a model.\n    :param __model_name: name of the created model\n    :param __config__: config class to use for the new model\n    :param __base__: base class for the new model to inherit from\n    :param __module__: module of the created model\n    :param __validators__: a dict of method names and @validator class methods\n    :param __cls_kwargs__: a dict for class creation\n    :param __slots__: Deprecated, `__slots__` should not be passed to `create_model`\n    :param field_definitions: fields of the model (or extra fields if a base is supplied)\n        in the format `<name>=(<type>, <default default>)` or `<name>=<default value>, e.g.\n        `foobar=(str, ...)` or `foobar=123`, or, for complex use-cases, in the format\n        `<name>=<Field>` or `<name>=(<type>, <FieldInfo>)`, e.g.\n        `foo=Field(datetime, default_factory=datetime.utcnow, alias='bar')` or\n        `foo=(str, FieldInfo(title='Foo'))`\n    \"\"\"\n    if __slots__ is not None:\n        # __slots__ will be ignored from here on\n        warnings.warn('__slots__ should not be passed to create_model', RuntimeWarning)\n\n    if __base__ is not None:\n        if __config__ is not None:\n            raise ConfigError('to avoid confusion __config__ and __base__ cannot be used together')\n        if not isinstance(__base__, tuple):\n            __base__ = (__base__,)\n    else:\n        __base__ = (cast(Type['Model'], BaseModel),)\n\n    __cls_kwargs__ = __cls_kwargs__ or {}\n\n    fields = {}\n    annotations = {}\n\n    for f_name, f_def in field_definitions.items():\n        if not is_valid_field(f_name):\n            warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n        if isinstance(f_def, tuple):\n            try:\n                f_annotation, f_value = f_def\n            except ValueError as e:\n                raise ConfigError(\n                    'field definitions should either be a tuple of (<type>, <default>) or just a '\n                    'default value, unfortunately this means tuples as '\n                    'default values are not allowed'\n                ) from e\n        else:\n            f_annotation, f_value = None, f_def\n\n        if f_annotation:\n            annotations[f_name] = f_annotation\n        fields[f_name] = f_value\n\n    namespace: 'DictStrAny' = {'__annotations__': annotations, '__module__': __module__}\n    if __validators__:\n        namespace.update(__validators__)\n    namespace.update(fields)\n    if __config__:\n        namespace['Config'] = inherit_config(__config__, BaseConfig)\n    resolved_bases = resolve_bases(__base__)\n    meta, ns, kwds = prepare_class(__model_name, resolved_bases, kwds=__cls_kwargs__)\n    if resolved_bases is not __base__:\n        ns['__orig_bases__'] = __base__\n    namespace.update(ns)\n    return meta(__model_name, resolved_bases, namespace, **kwds)\n\n\n_missing = object()\n\n\ndef validate_model(  # noqa: C901 (ignore complexity)\n    model: Type[BaseModel], input_data: 'DictStrAny', cls: 'ModelOrDc' = None\n) -> Tuple['DictStrAny', 'SetStr', Optional[ValidationError]]:\n    \"\"\"\n    validate data against a model.\n    \"\"\"\n    values = {}\n    errors = []\n    # input_data names, possibly alias\n    names_used = set()\n    # field names, never aliases\n    fields_set = set()\n    config = model.__config__\n    check_extra = config.extra is not Extra.ignore\n    cls_ = cls or model\n\n    for validator in model.__pre_root_validators__:\n        try:\n            input_data = validator(cls_, input_data)\n        except (ValueError, TypeError, AssertionError) as exc:\n            return {}, set(), ValidationError([ErrorWrapper(exc, loc=ROOT_KEY)], cls_)\n\n    for name, field in model.__fields__.items():\n        value = input_data.get(field.alias, _missing)\n        using_name = False\n        if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n            value = input_data.get(field.name, _missing)\n            using_name = True\n\n        if value is _missing:\n            if field.required:\n                errors.append(ErrorWrapper(MissingError(), loc=field.alias))\n                continue\n\n            value = field.get_default()\n\n            if not config.validate_all and not field.validate_always:\n                values[name] = value\n                continue\n        else:\n            fields_set.add(name)\n            if check_extra:\n                names_used.add(field.name if using_name else field.alias)\n\n        v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n        if isinstance(errors_, ErrorWrapper):\n            errors.append(errors_)\n        elif isinstance(errors_, list):\n            errors.extend(errors_)\n        else:\n            values[name] = v_\n\n    if check_extra:\n        if isinstance(input_data, GetterDict):\n            extra = input_data.extra_keys() - names_used\n        else:\n            extra = input_data.keys() - names_used\n        if extra:\n            fields_set |= extra\n            if config.extra is Extra.allow:\n                for f in extra:\n                    values[f] = input_data[f]\n            else:\n                for f in sorted(extra):\n                    errors.append(ErrorWrapper(ExtraError(), loc=f))\n\n    for skip_on_failure, validator in model.__post_root_validators__:\n        if skip_on_failure and errors:\n            continue\n        try:\n            values = validator(cls_, values)\n        except (ValueError, TypeError, AssertionError) as exc:\n            errors.append(ErrorWrapper(exc, loc=ROOT_KEY))\n\n    if errors:\n        return values, fields_set, ValidationError(errors, cls_)\n    else:\n        return values, fields_set, None\n", "pydantic/v1/dataclasses.py": "\"\"\"\nThe main purpose is to enhance stdlib dataclasses by adding validation\nA pydantic dataclass can be generated from scratch or from a stdlib one.\n\nBehind the scene, a pydantic dataclass is just like a regular one on which we attach\na `BaseModel` and magic methods to trigger the validation of the data.\n`__init__` and `__post_init__` are hence overridden and have extra logic to be\nable to validate input data.\n\nWhen a pydantic dataclass is generated from scratch, it's just a plain dataclass\nwith validation triggered at initialization\n\nThe tricky part if for stdlib dataclasses that are converted after into pydantic ones e.g.\n\n```py\n@dataclasses.dataclass\nclass M:\n    x: int\n\nValidatedM = pydantic.dataclasses.dataclass(M)\n```\n\nWe indeed still want to support equality, hashing, repr, ... as if it was the stdlib one!\n\n```py\nassert isinstance(ValidatedM(x=1), M)\nassert ValidatedM(x=1) == M(x=1)\n```\n\nThis means we **don't want to create a new dataclass that inherits from it**\nThe trick is to create a wrapper around `M` that will act as a proxy to trigger\nvalidation without altering default `M` behaviour.\n\"\"\"\nimport copy\nimport dataclasses\nimport sys\nfrom contextlib import contextmanager\nfrom functools import wraps\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n    # cached_property available only for python3.8+\n    pass\n\nfrom typing import TYPE_CHECKING, Any, Callable, ClassVar, Dict, Generator, Optional, Type, TypeVar, Union, overload\n\nfrom typing_extensions import dataclass_transform\n\nfrom pydantic.v1.class_validators import gather_all_validators\nfrom pydantic.v1.config import BaseConfig, ConfigDict, Extra, get_config\nfrom pydantic.v1.error_wrappers import ValidationError\nfrom pydantic.v1.errors import DataclassTypeError\nfrom pydantic.v1.fields import Field, FieldInfo, Required, Undefined\nfrom pydantic.v1.main import create_model, validate_model\nfrom pydantic.v1.utils import ClassAttribute\n\nif TYPE_CHECKING:\n    from pydantic.v1.main import BaseModel\n    from pydantic.v1.typing import CallableGenerator, NoArgAnyCallable\n\n    DataclassT = TypeVar('DataclassT', bound='Dataclass')\n\n    DataclassClassOrWrapper = Union[Type['Dataclass'], 'DataclassProxy']\n\n    class Dataclass:\n        # stdlib attributes\n        __dataclass_fields__: ClassVar[Dict[str, Any]]\n        __dataclass_params__: ClassVar[Any]  # in reality `dataclasses._DataclassParams`\n        __post_init__: ClassVar[Callable[..., None]]\n\n        # Added by pydantic\n        __pydantic_run_validation__: ClassVar[bool]\n        __post_init_post_parse__: ClassVar[Callable[..., None]]\n        __pydantic_initialised__: ClassVar[bool]\n        __pydantic_model__: ClassVar[Type[BaseModel]]\n        __pydantic_validate_values__: ClassVar[Callable[['Dataclass'], None]]\n        __pydantic_has_field_info_default__: ClassVar[bool]  # whether a `pydantic.Field` is used as default value\n\n        def __init__(self, *args: object, **kwargs: object) -> None:\n            pass\n\n        @classmethod\n        def __get_validators__(cls: Type['Dataclass']) -> 'CallableGenerator':\n            pass\n\n        @classmethod\n        def __validate__(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n            pass\n\n\n__all__ = [\n    'dataclass',\n    'set_validation',\n    'create_pydantic_model_from_dataclass',\n    'is_builtin_dataclass',\n    'make_dataclass_validator',\n]\n\n_T = TypeVar('_T')\n\nif sys.version_info >= (3, 10):\n\n    @dataclass_transform(field_specifiers=(dataclasses.field, Field))\n    @overload\n    def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...\n\n    @dataclass_transform(field_specifiers=(dataclasses.field, Field))\n    @overload\n    def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...\n\nelse:\n\n    @dataclass_transform(field_specifiers=(dataclasses.field, Field))\n    @overload\n    def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...\n\n    @dataclass_transform(field_specifiers=(dataclasses.field, Field))\n    @overload\n    def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...\n\n\n@dataclass_transform(field_specifiers=(dataclasses.field, Field))\ndef dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    \"\"\"\n    Like the python standard lib dataclasses but with type validation.\n    The result is either a pydantic dataclass that will validate input data\n    or a wrapper that will trigger validation around a stdlib dataclass\n    to avoid modifying it directly\n    \"\"\"\n    the_config = get_config(config)\n\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''  # needs to be done before generating dataclass\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(  # type: ignore\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n\n    if _cls is None:\n        return wrap\n\n    return wrap(_cls)\n\n\n@contextmanager\ndef set_validation(cls: Type['DataclassT'], value: bool) -> Generator[Type['DataclassT'], None, None]:\n    original_run_validation = cls.__pydantic_run_validation__\n    try:\n        cls.__pydantic_run_validation__ = value\n        yield cls\n    finally:\n        cls.__pydantic_run_validation__ = original_run_validation\n\n\nclass DataclassProxy:\n    __slots__ = '__dataclass__'\n\n    def __init__(self, dc_cls: Type['Dataclass']) -> None:\n        object.__setattr__(self, '__dataclass__', dc_cls)\n\n    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n        with set_validation(self.__dataclass__, True):\n            return self.__dataclass__(*args, **kwargs)\n\n    def __getattr__(self, name: str) -> Any:\n        return getattr(self.__dataclass__, name)\n\n    def __setattr__(self, __name: str, __value: Any) -> None:\n        return setattr(self.__dataclass__, __name, __value)\n\n    def __instancecheck__(self, instance: Any) -> bool:\n        return isinstance(instance, self.__dataclass__)\n\n    def __copy__(self) -> 'DataclassProxy':\n        return DataclassProxy(copy.copy(self.__dataclass__))\n\n    def __deepcopy__(self, memo: Any) -> 'DataclassProxy':\n        return DataclassProxy(copy.deepcopy(self.__dataclass__, memo))\n\n\ndef _add_pydantic_validation_attributes(  # noqa: C901 (ignore complexity)\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    \"\"\"\n    We need to replace the right method. If no `__post_init__` has been set in the stdlib dataclass\n    it won't even exist (code is generated on the fly by `dataclasses`)\n    By default, we run validation after `__init__` or `__post_init__` if defined\n    \"\"\"\n    init = dc_cls.__init__\n\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n\n        else:\n            init(self, *args, **kwargs)\n\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__  # type: ignore[attr-defined]\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n\n    else:\n\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n\n            if hasattr(self, '__post_init_post_parse__'):\n                # We need to find again the initvars. To do that we use `__dataclass_fields__` instead of\n                # public method `dataclasses.fields`\n\n                # get all initvars and their default values\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:  # type: ignore[attr-defined]\n                        try:\n                            # set arg value by default\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n\n                self.__post_init_post_parse__(**initvars_and_values)\n\n        setattr(dc_cls, '__init__', new_init)\n\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)\n\n\ndef _get_validators(cls: 'DataclassClassOrWrapper') -> 'CallableGenerator':\n    yield cls.__validate__\n\n\ndef _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)\n\n\ndef create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n\n        field_definitions[field.name] = (field.type, field_info)\n\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model\n\n\nif sys.version_info >= (3, 8):\n\n    def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)\n\nelse:\n\n    def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False\n\n\ndef _dataclass_validate_values(self: 'Dataclass') -> None:\n    # validation errors can occur if this function is called twice on an already initialised dataclass.\n    # for example if Extra.forbid is enabled, it would consider __pydantic_initialised__ an invalid extra property\n    if getattr(self, '__pydantic_initialised__'):\n        return\n    if getattr(self, '__pydantic_has_field_info_default__', False):\n        # We need to remove `FieldInfo` values since they are not valid as input\n        # It's ok to do that because they are obviously the default values!\n        input_data = {\n            k: v\n            for k, v in self.__dict__.items()\n            if not (isinstance(v, FieldInfo) or _is_field_cached_property(self, k))\n        }\n    else:\n        input_data = {k: v for k, v in self.__dict__.items() if not _is_field_cached_property(self, k)}\n    d, _, validation_error = validate_model(self.__pydantic_model__, input_data, cls=self.__class__)\n    if validation_error:\n        raise validation_error\n    self.__dict__.update(d)\n    object.__setattr__(self, '__pydantic_initialised__', True)\n\n\ndef _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n\n    object.__setattr__(self, name, value)\n\n\ndef is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    \"\"\"\n    Whether a class is a stdlib dataclass\n    (useful to discriminated a pydantic dataclass that is actually a wrapper around a stdlib dataclass)\n\n    we check that\n    - `_cls` is a dataclass\n    - `_cls` is not a processed pydantic dataclass (with a basemodel attached)\n    - `_cls` is not a pydantic dataclass inheriting directly from a stdlib dataclass\n    e.g.\n    ```\n    @dataclasses.dataclass\n    class A:\n        x: int\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        y: int\n    ```\n    In this case, when we first check `B`, we make an extra check and look at the annotations ('y'),\n    which won't be a superset of all the dataclass fields (only the stdlib fields i.e. 'x')\n    \"\"\"\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )\n\n\ndef make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    \"\"\"\n    Create a pydantic.dataclass from a builtin dataclass to add type validation\n    and yield the validators\n    It retrieves the parameters of the dataclass and forwards them to the newly created dataclass\n    \"\"\"\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))\n", "pydantic/v1/class_validators.py": "import warnings\nfrom collections import ChainMap\nfrom functools import partial, partialmethod, wraps\nfrom itertools import chain\nfrom types import FunctionType\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, Iterable, List, Optional, Set, Tuple, Type, Union, overload\n\nfrom pydantic.v1.errors import ConfigError\nfrom pydantic.v1.typing import AnyCallable\nfrom pydantic.v1.utils import ROOT_KEY, in_ipython\n\nif TYPE_CHECKING:\n    from pydantic.v1.typing import AnyClassMethod\n\n\nclass Validator:\n    __slots__ = 'func', 'pre', 'each_item', 'always', 'check_fields', 'skip_on_failure'\n\n    def __init__(\n        self,\n        func: AnyCallable,\n        pre: bool = False,\n        each_item: bool = False,\n        always: bool = False,\n        check_fields: bool = False,\n        skip_on_failure: bool = False,\n    ):\n        self.func = func\n        self.pre = pre\n        self.each_item = each_item\n        self.always = always\n        self.check_fields = check_fields\n        self.skip_on_failure = skip_on_failure\n\n\nif TYPE_CHECKING:\n    from inspect import Signature\n\n    from pydantic.v1.config import BaseConfig\n    from pydantic.v1.fields import ModelField\n    from pydantic.v1.types import ModelOrDc\n\n    ValidatorCallable = Callable[[Optional[ModelOrDc], Any, Dict[str, Any], ModelField, Type[BaseConfig]], Any]\n    ValidatorsList = List[ValidatorCallable]\n    ValidatorListDict = Dict[str, List[Validator]]\n\n_FUNCS: Set[str] = set()\nVALIDATOR_CONFIG_KEY = '__validator_config__'\nROOT_VALIDATOR_CONFIG_KEY = '__root_validator_config__'\n\n\ndef validator(\n    *fields: str,\n    pre: bool = False,\n    each_item: bool = False,\n    always: bool = False,\n    check_fields: bool = True,\n    whole: Optional[bool] = None,\n    allow_reuse: bool = False,\n) -> Callable[[AnyCallable], 'AnyClassMethod']:\n    \"\"\"\n    Decorate methods on the class indicating that they should be used to validate fields\n    :param fields: which field(s) the method should be called on\n    :param pre: whether or not this validator should be called before the standard validators (else after)\n    :param each_item: for complex objects (sets, lists etc.) whether to validate individual elements rather than the\n      whole object\n    :param always: whether this method and other validators should be called even if the value is missing\n    :param check_fields: whether to check that the fields actually exist on the model\n    :param allow_reuse: whether to track and raise an error if another validator refers to the decorated function\n    \"\"\"\n    if not fields:\n        raise ConfigError('validator with no fields specified')\n    elif isinstance(fields[0], FunctionType):\n        raise ConfigError(\n            \"validators should be used with fields and keyword arguments, not bare. \"  # noqa: Q000\n            \"E.g. usage should be `@validator('<field_name>', ...)`\"\n        )\n    elif not all(isinstance(field, str) for field in fields):\n        raise ConfigError(\n            \"validator fields should be passed as separate string args. \"  # noqa: Q000\n            \"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`\"\n        )\n\n    if whole is not None:\n        warnings.warn(\n            'The \"whole\" keyword argument is deprecated, use \"each_item\" (inverse meaning, default False) instead',\n            DeprecationWarning,\n        )\n        assert each_item is False, '\"each_item\" and \"whole\" conflict, remove \"whole\"'\n        each_item = not whole\n\n    def dec(f: AnyCallable) -> 'AnyClassMethod':\n        f_cls = _prepare_validator(f, allow_reuse)\n        setattr(\n            f_cls,\n            VALIDATOR_CONFIG_KEY,\n            (\n                fields,\n                Validator(func=f_cls.__func__, pre=pre, each_item=each_item, always=always, check_fields=check_fields),\n            ),\n        )\n        return f_cls\n\n    return dec\n\n\n@overload\ndef root_validator(_func: AnyCallable) -> 'AnyClassMethod':\n    ...\n\n\n@overload\ndef root_validator(\n    *, pre: bool = False, allow_reuse: bool = False, skip_on_failure: bool = False\n) -> Callable[[AnyCallable], 'AnyClassMethod']:\n    ...\n\n\ndef root_validator(\n    _func: Optional[AnyCallable] = None, *, pre: bool = False, allow_reuse: bool = False, skip_on_failure: bool = False\n) -> Union['AnyClassMethod', Callable[[AnyCallable], 'AnyClassMethod']]:\n    \"\"\"\n    Decorate methods on a model indicating that they should be used to validate (and perhaps modify) data either\n    before or after standard model parsing/validation is performed.\n    \"\"\"\n    if _func:\n        f_cls = _prepare_validator(_func, allow_reuse)\n        setattr(\n            f_cls, ROOT_VALIDATOR_CONFIG_KEY, Validator(func=f_cls.__func__, pre=pre, skip_on_failure=skip_on_failure)\n        )\n        return f_cls\n\n    def dec(f: AnyCallable) -> 'AnyClassMethod':\n        f_cls = _prepare_validator(f, allow_reuse)\n        setattr(\n            f_cls, ROOT_VALIDATOR_CONFIG_KEY, Validator(func=f_cls.__func__, pre=pre, skip_on_failure=skip_on_failure)\n        )\n        return f_cls\n\n    return dec\n\n\ndef _prepare_validator(function: AnyCallable, allow_reuse: bool) -> 'AnyClassMethod':\n    \"\"\"\n    Avoid validators with duplicated names since without this, validators can be overwritten silently\n    which generally isn't the intended behaviour, don't run in ipython (see #312) or if allow_reuse is False.\n    \"\"\"\n    f_cls = function if isinstance(function, classmethod) else classmethod(function)\n    if not in_ipython() and not allow_reuse:\n        ref = (\n            getattr(f_cls.__func__, '__module__', '<No __module__>')\n            + '.'\n            + getattr(f_cls.__func__, '__qualname__', f'<No __qualname__: id:{id(f_cls.__func__)}>')\n        )\n        if ref in _FUNCS:\n            raise ConfigError(f'duplicate validator function \"{ref}\"; if this is intended, set `allow_reuse=True`')\n        _FUNCS.add(ref)\n    return f_cls\n\n\nclass ValidatorGroup:\n    def __init__(self, validators: 'ValidatorListDict') -> None:\n        self.validators = validators\n        self.used_validators = {'*'}\n\n    def get_validators(self, name: str) -> Optional[Dict[str, Validator]]:\n        self.used_validators.add(name)\n        validators = self.validators.get(name, [])\n        if name != ROOT_KEY:\n            validators += self.validators.get('*', [])\n        if validators:\n            return {getattr(v.func, '__name__', f'<No __name__: id:{id(v.func)}>'): v for v in validators}\n        else:\n            return None\n\n    def check_for_unused(self) -> None:\n        unused_validators = set(\n            chain.from_iterable(\n                (\n                    getattr(v.func, '__name__', f'<No __name__: id:{id(v.func)}>')\n                    for v in self.validators[f]\n                    if v.check_fields\n                )\n                for f in (self.validators.keys() - self.used_validators)\n            )\n        )\n        if unused_validators:\n            fn = ', '.join(unused_validators)\n            raise ConfigError(\n                f\"Validators defined with incorrect fields: {fn} \"  # noqa: Q000\n                f\"(use check_fields=False if you're inheriting from the model and intended this)\"\n            )\n\n\ndef extract_validators(namespace: Dict[str, Any]) -> Dict[str, List[Validator]]:\n    validators: Dict[str, List[Validator]] = {}\n    for var_name, value in namespace.items():\n        validator_config = getattr(value, VALIDATOR_CONFIG_KEY, None)\n        if validator_config:\n            fields, v = validator_config\n            for field in fields:\n                if field in validators:\n                    validators[field].append(v)\n                else:\n                    validators[field] = [v]\n    return validators\n\n\ndef extract_root_validators(namespace: Dict[str, Any]) -> Tuple[List[AnyCallable], List[Tuple[bool, AnyCallable]]]:\n    from inspect import signature\n\n    pre_validators: List[AnyCallable] = []\n    post_validators: List[Tuple[bool, AnyCallable]] = []\n    for name, value in namespace.items():\n        validator_config: Optional[Validator] = getattr(value, ROOT_VALIDATOR_CONFIG_KEY, None)\n        if validator_config:\n            sig = signature(validator_config.func)\n            args = list(sig.parameters.keys())\n            if args[0] == 'self':\n                raise ConfigError(\n                    f'Invalid signature for root validator {name}: {sig}, \"self\" not permitted as first argument, '\n                    f'should be: (cls, values).'\n                )\n            if len(args) != 2:\n                raise ConfigError(f'Invalid signature for root validator {name}: {sig}, should be: (cls, values).')\n            # check function signature\n            if validator_config.pre:\n                pre_validators.append(validator_config.func)\n            else:\n                post_validators.append((validator_config.skip_on_failure, validator_config.func))\n    return pre_validators, post_validators\n\n\ndef inherit_validators(base_validators: 'ValidatorListDict', validators: 'ValidatorListDict') -> 'ValidatorListDict':\n    for field, field_validators in base_validators.items():\n        if field not in validators:\n            validators[field] = []\n        validators[field] += field_validators\n    return validators\n\n\ndef make_generic_validator(validator: AnyCallable) -> 'ValidatorCallable':\n    \"\"\"\n    Make a generic function which calls a validator with the right arguments.\n\n    Unfortunately other approaches (eg. return a partial of a function that builds the arguments) is slow,\n    hence this laborious way of doing things.\n\n    It's done like this so validators don't all need **kwargs in their signature, eg. any combination of\n    the arguments \"values\", \"fields\" and/or \"config\" are permitted.\n    \"\"\"\n    from inspect import signature\n\n    if not isinstance(validator, (partial, partialmethod)):\n        # This should be the default case, so overhead is reduced\n        sig = signature(validator)\n        args = list(sig.parameters.keys())\n    else:\n        # Fix the generated argument lists of partial methods\n        sig = signature(validator.func)\n        args = [\n            k\n            for k in signature(validator.func).parameters.keys()\n            if k not in validator.args | validator.keywords.keys()\n        ]\n\n    first_arg = args.pop(0)\n    if first_arg == 'self':\n        raise ConfigError(\n            f'Invalid signature for validator {validator}: {sig}, \"self\" not permitted as first argument, '\n            f'should be: (cls, value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n        )\n    elif first_arg == 'cls':\n        # assume the second argument is value\n        return wraps(validator)(_generic_validator_cls(validator, sig, set(args[1:])))\n    else:\n        # assume the first argument was value which has already been removed\n        return wraps(validator)(_generic_validator_basic(validator, sig, set(args)))\n\n\ndef prep_validators(v_funcs: Iterable[AnyCallable]) -> 'ValidatorsList':\n    return [make_generic_validator(f) for f in v_funcs if f]\n\n\nall_kwargs = {'values', 'field', 'config'}\n\n\ndef _generic_validator_cls(validator: AnyCallable, sig: 'Signature', args: Set[str]) -> 'ValidatorCallable':\n    # assume the first argument is value\n    has_kwargs = False\n    if 'kwargs' in args:\n        has_kwargs = True\n        args -= {'kwargs'}\n\n    if not args.issubset(all_kwargs):\n        raise ConfigError(\n            f'Invalid signature for validator {validator}: {sig}, should be: '\n            f'(cls, value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n        )\n\n    if has_kwargs:\n        return lambda cls, v, values, field, config: validator(cls, v, values=values, field=field, config=config)\n    elif args == set():\n        return lambda cls, v, values, field, config: validator(cls, v)\n    elif args == {'values'}:\n        return lambda cls, v, values, field, config: validator(cls, v, values=values)\n    elif args == {'field'}:\n        return lambda cls, v, values, field, config: validator(cls, v, field=field)\n    elif args == {'config'}:\n        return lambda cls, v, values, field, config: validator(cls, v, config=config)\n    elif args == {'values', 'field'}:\n        return lambda cls, v, values, field, config: validator(cls, v, values=values, field=field)\n    elif args == {'values', 'config'}:\n        return lambda cls, v, values, field, config: validator(cls, v, values=values, config=config)\n    elif args == {'field', 'config'}:\n        return lambda cls, v, values, field, config: validator(cls, v, field=field, config=config)\n    else:\n        # args == {'values', 'field', 'config'}\n        return lambda cls, v, values, field, config: validator(cls, v, values=values, field=field, config=config)\n\n\ndef _generic_validator_basic(validator: AnyCallable, sig: 'Signature', args: Set[str]) -> 'ValidatorCallable':\n    has_kwargs = False\n    if 'kwargs' in args:\n        has_kwargs = True\n        args -= {'kwargs'}\n\n    if not args.issubset(all_kwargs):\n        raise ConfigError(\n            f'Invalid signature for validator {validator}: {sig}, should be: '\n            f'(value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n        )\n\n    if has_kwargs:\n        return lambda cls, v, values, field, config: validator(v, values=values, field=field, config=config)\n    elif args == set():\n        return lambda cls, v, values, field, config: validator(v)\n    elif args == {'values'}:\n        return lambda cls, v, values, field, config: validator(v, values=values)\n    elif args == {'field'}:\n        return lambda cls, v, values, field, config: validator(v, field=field)\n    elif args == {'config'}:\n        return lambda cls, v, values, field, config: validator(v, config=config)\n    elif args == {'values', 'field'}:\n        return lambda cls, v, values, field, config: validator(v, values=values, field=field)\n    elif args == {'values', 'config'}:\n        return lambda cls, v, values, field, config: validator(v, values=values, config=config)\n    elif args == {'field', 'config'}:\n        return lambda cls, v, values, field, config: validator(v, field=field, config=config)\n    else:\n        # args == {'values', 'field', 'config'}\n        return lambda cls, v, values, field, config: validator(v, values=values, field=field, config=config)\n\n\ndef gather_all_validators(type_: 'ModelOrDc') -> Dict[str, 'AnyClassMethod']:\n    all_attributes = ChainMap(*[cls.__dict__ for cls in type_.__mro__])  # type: ignore[arg-type,var-annotated]\n    return {\n        k: v\n        for k, v in all_attributes.items()\n        if hasattr(v, VALIDATOR_CONFIG_KEY) or hasattr(v, ROOT_VALIDATOR_CONFIG_KEY)\n    }\n", "pydantic/v1/__init__.py": "# flake8: noqa\nfrom pydantic.v1 import dataclasses\nfrom pydantic.v1.annotated_types import create_model_from_namedtuple, create_model_from_typeddict\nfrom pydantic.v1.class_validators import root_validator, validator\nfrom pydantic.v1.config import BaseConfig, ConfigDict, Extra\nfrom pydantic.v1.decorator import validate_arguments\nfrom pydantic.v1.env_settings import BaseSettings\nfrom pydantic.v1.error_wrappers import ValidationError\nfrom pydantic.v1.errors import *\nfrom pydantic.v1.fields import Field, PrivateAttr, Required\nfrom pydantic.v1.main import *\nfrom pydantic.v1.networks import *\nfrom pydantic.v1.parse import Protocol\nfrom pydantic.v1.tools import *\nfrom pydantic.v1.types import *\nfrom pydantic.v1.version import VERSION, compiled\n\n__version__ = VERSION\n\n# WARNING __all__ from pydantic.errors is not included here, it will be removed as an export here in v2\n# please use \"from pydantic.v1.errors import ...\" instead\n__all__ = [\n    # annotated types utils\n    'create_model_from_namedtuple',\n    'create_model_from_typeddict',\n    # dataclasses\n    'dataclasses',\n    # class_validators\n    'root_validator',\n    'validator',\n    # config\n    'BaseConfig',\n    'ConfigDict',\n    'Extra',\n    # decorator\n    'validate_arguments',\n    # env_settings\n    'BaseSettings',\n    # error_wrappers\n    'ValidationError',\n    # fields\n    'Field',\n    'Required',\n    # main\n    'BaseModel',\n    'create_model',\n    'validate_model',\n    # network\n    'AnyUrl',\n    'AnyHttpUrl',\n    'FileUrl',\n    'HttpUrl',\n    'stricturl',\n    'EmailStr',\n    'NameEmail',\n    'IPvAnyAddress',\n    'IPvAnyInterface',\n    'IPvAnyNetwork',\n    'PostgresDsn',\n    'CockroachDsn',\n    'AmqpDsn',\n    'RedisDsn',\n    'MongoDsn',\n    'KafkaDsn',\n    'validate_email',\n    # parse\n    'Protocol',\n    # tools\n    'parse_file_as',\n    'parse_obj_as',\n    'parse_raw_as',\n    'schema_of',\n    'schema_json_of',\n    # types\n    'NoneStr',\n    'NoneBytes',\n    'StrBytes',\n    'NoneStrBytes',\n    'StrictStr',\n    'ConstrainedBytes',\n    'conbytes',\n    'ConstrainedList',\n    'conlist',\n    'ConstrainedSet',\n    'conset',\n    'ConstrainedFrozenSet',\n    'confrozenset',\n    'ConstrainedStr',\n    'constr',\n    'PyObject',\n    'ConstrainedInt',\n    'conint',\n    'PositiveInt',\n    'NegativeInt',\n    'NonNegativeInt',\n    'NonPositiveInt',\n    'ConstrainedFloat',\n    'confloat',\n    'PositiveFloat',\n    'NegativeFloat',\n    'NonNegativeFloat',\n    'NonPositiveFloat',\n    'FiniteFloat',\n    'ConstrainedDecimal',\n    'condecimal',\n    'ConstrainedDate',\n    'condate',\n    'UUID1',\n    'UUID3',\n    'UUID4',\n    'UUID5',\n    'FilePath',\n    'DirectoryPath',\n    'Json',\n    'JsonWrapper',\n    'SecretField',\n    'SecretStr',\n    'SecretBytes',\n    'StrictBool',\n    'StrictBytes',\n    'StrictInt',\n    'StrictFloat',\n    'PaymentCardNumber',\n    'PrivateAttr',\n    'ByteSize',\n    'PastDate',\n    'FutureDate',\n    # version\n    'compiled',\n    'VERSION',\n]\n", "pydantic/v1/mypy.py": "import sys\nfrom configparser import ConfigParser\nfrom typing import Any, Callable, Dict, List, Optional, Set, Tuple, Type as TypingType, Union\n\nfrom mypy.errorcodes import ErrorCode\nfrom mypy.nodes import (\n    ARG_NAMED,\n    ARG_NAMED_OPT,\n    ARG_OPT,\n    ARG_POS,\n    ARG_STAR2,\n    MDEF,\n    Argument,\n    AssignmentStmt,\n    Block,\n    CallExpr,\n    ClassDef,\n    Context,\n    Decorator,\n    EllipsisExpr,\n    FuncBase,\n    FuncDef,\n    JsonDict,\n    MemberExpr,\n    NameExpr,\n    PassStmt,\n    PlaceholderNode,\n    RefExpr,\n    StrExpr,\n    SymbolNode,\n    SymbolTableNode,\n    TempNode,\n    TypeInfo,\n    TypeVarExpr,\n    Var,\n)\nfrom mypy.options import Options\nfrom mypy.plugin import (\n    CheckerPluginInterface,\n    ClassDefContext,\n    FunctionContext,\n    MethodContext,\n    Plugin,\n    ReportConfigContext,\n    SemanticAnalyzerPluginInterface,\n)\nfrom mypy.plugins import dataclasses\nfrom mypy.semanal import set_callable_name  # type: ignore\nfrom mypy.server.trigger import make_wildcard_trigger\nfrom mypy.types import (\n    AnyType,\n    CallableType,\n    Instance,\n    NoneType,\n    Overloaded,\n    ProperType,\n    Type,\n    TypeOfAny,\n    TypeType,\n    TypeVarId,\n    TypeVarType,\n    UnionType,\n    get_proper_type,\n)\nfrom mypy.typevars import fill_typevars\nfrom mypy.util import get_unique_redefinition_name\nfrom mypy.version import __version__ as mypy_version\n\nfrom pydantic.v1.utils import is_valid_field\n\ntry:\n    from mypy.types import TypeVarDef  # type: ignore[attr-defined]\nexcept ImportError:  # pragma: no cover\n    # Backward-compatible with TypeVarDef from Mypy 0.910.\n    from mypy.types import TypeVarType as TypeVarDef\n\nCONFIGFILE_KEY = 'pydantic-mypy'\nMETADATA_KEY = 'pydantic-mypy-metadata'\n_NAMESPACE = __name__[:-5]  # 'pydantic' in 1.10.X, 'pydantic.v1' in v2.X\nBASEMODEL_FULLNAME = f'{_NAMESPACE}.main.BaseModel'\nBASESETTINGS_FULLNAME = f'{_NAMESPACE}.env_settings.BaseSettings'\nMODEL_METACLASS_FULLNAME = f'{_NAMESPACE}.main.ModelMetaclass'\nFIELD_FULLNAME = f'{_NAMESPACE}.fields.Field'\nDATACLASS_FULLNAME = f'{_NAMESPACE}.dataclasses.dataclass'\n\n\ndef parse_mypy_version(version: str) -> Tuple[int, ...]:\n    return tuple(map(int, version.partition('+')[0].split('.')))\n\n\nMYPY_VERSION_TUPLE = parse_mypy_version(mypy_version)\nBUILTINS_NAME = 'builtins' if MYPY_VERSION_TUPLE >= (0, 930) else '__builtins__'\n\n# Increment version if plugin changes and mypy caches should be invalidated\n__version__ = 2\n\n\ndef plugin(version: str) -> 'TypingType[Plugin]':\n    \"\"\"\n    `version` is the mypy version string\n\n    We might want to use this to print a warning if the mypy version being used is\n    newer, or especially older, than we expect (or need).\n    \"\"\"\n    return PydanticPlugin\n\n\nclass PydanticPlugin(Plugin):\n    def __init__(self, options: Options) -> None:\n        self.plugin_config = PydanticPluginConfig(options)\n        self._plugin_data = self.plugin_config.to_data()\n        super().__init__(options)\n\n    def get_base_class_hook(self, fullname: str) -> 'Optional[Callable[[ClassDefContext], None]]':\n        sym = self.lookup_fully_qualified(fullname)\n        if sym and isinstance(sym.node, TypeInfo):  # pragma: no branch\n            # No branching may occur if the mypy cache has not been cleared\n            if any(get_fullname(base) == BASEMODEL_FULLNAME for base in sym.node.mro):\n                return self._pydantic_model_class_maker_callback\n        return None\n\n    def get_metaclass_hook(self, fullname: str) -> Optional[Callable[[ClassDefContext], None]]:\n        if fullname == MODEL_METACLASS_FULLNAME:\n            return self._pydantic_model_metaclass_marker_callback\n        return None\n\n    def get_function_hook(self, fullname: str) -> 'Optional[Callable[[FunctionContext], Type]]':\n        sym = self.lookup_fully_qualified(fullname)\n        if sym and sym.fullname == FIELD_FULLNAME:\n            return self._pydantic_field_callback\n        return None\n\n    def get_method_hook(self, fullname: str) -> Optional[Callable[[MethodContext], Type]]:\n        if fullname.endswith('.from_orm'):\n            return from_orm_callback\n        return None\n\n    def get_class_decorator_hook(self, fullname: str) -> Optional[Callable[[ClassDefContext], None]]:\n        \"\"\"Mark pydantic.dataclasses as dataclass.\n\n        Mypy version 1.1.1 added support for `@dataclass_transform` decorator.\n        \"\"\"\n        if fullname == DATACLASS_FULLNAME and MYPY_VERSION_TUPLE < (1, 1):\n            return dataclasses.dataclass_class_maker_callback  # type: ignore[return-value]\n        return None\n\n    def report_config_data(self, ctx: ReportConfigContext) -> Dict[str, Any]:\n        \"\"\"Return all plugin config data.\n\n        Used by mypy to determine if cache needs to be discarded.\n        \"\"\"\n        return self._plugin_data\n\n    def _pydantic_model_class_maker_callback(self, ctx: ClassDefContext) -> None:\n        transformer = PydanticModelTransformer(ctx, self.plugin_config)\n        transformer.transform()\n\n    def _pydantic_model_metaclass_marker_callback(self, ctx: ClassDefContext) -> None:\n        \"\"\"Reset dataclass_transform_spec attribute of ModelMetaclass.\n\n        Let the plugin handle it. This behavior can be disabled\n        if 'debug_dataclass_transform' is set to True', for testing purposes.\n        \"\"\"\n        if self.plugin_config.debug_dataclass_transform:\n            return\n        info_metaclass = ctx.cls.info.declared_metaclass\n        assert info_metaclass, \"callback not passed from 'get_metaclass_hook'\"\n        if getattr(info_metaclass.type, 'dataclass_transform_spec', None):\n            info_metaclass.type.dataclass_transform_spec = None  # type: ignore[attr-defined]\n\n    def _pydantic_field_callback(self, ctx: FunctionContext) -> 'Type':\n        \"\"\"\n        Extract the type of the `default` argument from the Field function, and use it as the return type.\n\n        In particular:\n        * Check whether the default and default_factory argument is specified.\n        * Output an error if both are specified.\n        * Retrieve the type of the argument which is specified, and use it as return type for the function.\n        \"\"\"\n        default_any_type = ctx.default_return_type\n\n        assert ctx.callee_arg_names[0] == 'default', '\"default\" is no longer first argument in Field()'\n        assert ctx.callee_arg_names[1] == 'default_factory', '\"default_factory\" is no longer second argument in Field()'\n        default_args = ctx.args[0]\n        default_factory_args = ctx.args[1]\n\n        if default_args and default_factory_args:\n            error_default_and_default_factory_specified(ctx.api, ctx.context)\n            return default_any_type\n\n        if default_args:\n            default_type = ctx.arg_types[0][0]\n            default_arg = default_args[0]\n\n            # Fallback to default Any type if the field is required\n            if not isinstance(default_arg, EllipsisExpr):\n                return default_type\n\n        elif default_factory_args:\n            default_factory_type = ctx.arg_types[1][0]\n\n            # Functions which use `ParamSpec` can be overloaded, exposing the callable's types as a parameter\n            # Pydantic calls the default factory without any argument, so we retrieve the first item\n            if isinstance(default_factory_type, Overloaded):\n                if MYPY_VERSION_TUPLE > (0, 910):\n                    default_factory_type = default_factory_type.items[0]\n                else:\n                    # Mypy0.910 exposes the items of overloaded types in a function\n                    default_factory_type = default_factory_type.items()[0]  # type: ignore[operator]\n\n            if isinstance(default_factory_type, CallableType):\n                ret_type = default_factory_type.ret_type\n                # mypy doesn't think `ret_type` has `args`, you'd think mypy should know,\n                # add this check in case it varies by version\n                args = getattr(ret_type, 'args', None)\n                if args:\n                    if all(isinstance(arg, TypeVarType) for arg in args):\n                        # Looks like the default factory is a type like `list` or `dict`, replace all args with `Any`\n                        ret_type.args = tuple(default_any_type for _ in args)  # type: ignore[attr-defined]\n                return ret_type\n\n        return default_any_type\n\n\nclass PydanticPluginConfig:\n    __slots__ = (\n        'init_forbid_extra',\n        'init_typed',\n        'warn_required_dynamic_aliases',\n        'warn_untyped_fields',\n        'debug_dataclass_transform',\n    )\n    init_forbid_extra: bool\n    init_typed: bool\n    warn_required_dynamic_aliases: bool\n    warn_untyped_fields: bool\n    debug_dataclass_transform: bool  # undocumented\n\n    def __init__(self, options: Options) -> None:\n        if options.config_file is None:  # pragma: no cover\n            return\n\n        toml_config = parse_toml(options.config_file)\n        if toml_config is not None:\n            config = toml_config.get('tool', {}).get('pydantic-mypy', {})\n            for key in self.__slots__:\n                setting = config.get(key, False)\n                if not isinstance(setting, bool):\n                    raise ValueError(f'Configuration value must be a boolean for key: {key}')\n                setattr(self, key, setting)\n        else:\n            plugin_config = ConfigParser()\n            plugin_config.read(options.config_file)\n            for key in self.__slots__:\n                setting = plugin_config.getboolean(CONFIGFILE_KEY, key, fallback=False)\n                setattr(self, key, setting)\n\n    def to_data(self) -> Dict[str, Any]:\n        return {key: getattr(self, key) for key in self.__slots__}\n\n\ndef from_orm_callback(ctx: MethodContext) -> Type:\n    \"\"\"\n    Raise an error if orm_mode is not enabled\n    \"\"\"\n    model_type: Instance\n    ctx_type = ctx.type\n    if isinstance(ctx_type, TypeType):\n        ctx_type = ctx_type.item\n    if isinstance(ctx_type, CallableType) and isinstance(ctx_type.ret_type, Instance):\n        model_type = ctx_type.ret_type  # called on the class\n    elif isinstance(ctx_type, Instance):\n        model_type = ctx_type  # called on an instance (unusual, but still valid)\n    else:  # pragma: no cover\n        detail = f'ctx.type: {ctx_type} (of type {ctx_type.__class__.__name__})'\n        error_unexpected_behavior(detail, ctx.api, ctx.context)\n        return ctx.default_return_type\n    pydantic_metadata = model_type.type.metadata.get(METADATA_KEY)\n    if pydantic_metadata is None:\n        return ctx.default_return_type\n    orm_mode = pydantic_metadata.get('config', {}).get('orm_mode')\n    if orm_mode is not True:\n        error_from_orm(get_name(model_type.type), ctx.api, ctx.context)\n    return ctx.default_return_type\n\n\nclass PydanticModelTransformer:\n    tracked_config_fields: Set[str] = {\n        'extra',\n        'allow_mutation',\n        'frozen',\n        'orm_mode',\n        'allow_population_by_field_name',\n        'alias_generator',\n    }\n\n    def __init__(self, ctx: ClassDefContext, plugin_config: PydanticPluginConfig) -> None:\n        self._ctx = ctx\n        self.plugin_config = plugin_config\n\n    def transform(self) -> None:\n        \"\"\"\n        Configures the BaseModel subclass according to the plugin settings.\n\n        In particular:\n        * determines the model config and fields,\n        * adds a fields-aware signature for the initializer and construct methods\n        * freezes the class if allow_mutation = False or frozen = True\n        * stores the fields, config, and if the class is settings in the mypy metadata for access by subclasses\n        \"\"\"\n        ctx = self._ctx\n        info = ctx.cls.info\n\n        self.adjust_validator_signatures()\n        config = self.collect_config()\n        fields = self.collect_fields(config)\n        is_settings = any(get_fullname(base) == BASESETTINGS_FULLNAME for base in info.mro[:-1])\n        self.add_initializer(fields, config, is_settings)\n        self.add_construct_method(fields)\n        self.set_frozen(fields, frozen=config.allow_mutation is False or config.frozen is True)\n        info.metadata[METADATA_KEY] = {\n            'fields': {field.name: field.serialize() for field in fields},\n            'config': config.set_values_dict(),\n        }\n\n    def adjust_validator_signatures(self) -> None:\n        \"\"\"When we decorate a function `f` with `pydantic.validator(...), mypy sees\n        `f` as a regular method taking a `self` instance, even though pydantic\n        internally wraps `f` with `classmethod` if necessary.\n\n        Teach mypy this by marking any function whose outermost decorator is a\n        `validator()` call as a classmethod.\n        \"\"\"\n        for name, sym in self._ctx.cls.info.names.items():\n            if isinstance(sym.node, Decorator):\n                first_dec = sym.node.original_decorators[0]\n                if (\n                    isinstance(first_dec, CallExpr)\n                    and isinstance(first_dec.callee, NameExpr)\n                    and first_dec.callee.fullname == f'{_NAMESPACE}.class_validators.validator'\n                ):\n                    sym.node.func.is_class = True\n\n    def collect_config(self) -> 'ModelConfigData':\n        \"\"\"\n        Collects the values of the config attributes that are used by the plugin, accounting for parent classes.\n        \"\"\"\n        ctx = self._ctx\n        cls = ctx.cls\n        config = ModelConfigData()\n        for stmt in cls.defs.body:\n            if not isinstance(stmt, ClassDef):\n                continue\n            if stmt.name == 'Config':\n                for substmt in stmt.defs.body:\n                    if not isinstance(substmt, AssignmentStmt):\n                        continue\n                    config.update(self.get_config_update(substmt))\n                if (\n                    config.has_alias_generator\n                    and not config.allow_population_by_field_name\n                    and self.plugin_config.warn_required_dynamic_aliases\n                ):\n                    error_required_dynamic_aliases(ctx.api, stmt)\n        for info in cls.info.mro[1:]:  # 0 is the current class\n            if METADATA_KEY not in info.metadata:\n                continue\n\n            # Each class depends on the set of fields in its ancestors\n            ctx.api.add_plugin_dependency(make_wildcard_trigger(get_fullname(info)))\n            for name, value in info.metadata[METADATA_KEY]['config'].items():\n                config.setdefault(name, value)\n        return config\n\n    def collect_fields(self, model_config: 'ModelConfigData') -> List['PydanticModelField']:\n        \"\"\"\n        Collects the fields for the model, accounting for parent classes\n        \"\"\"\n        # First, collect fields belonging to the current class.\n        ctx = self._ctx\n        cls = self._ctx.cls\n        fields = []  # type: List[PydanticModelField]\n        known_fields = set()  # type: Set[str]\n        for stmt in cls.defs.body:\n            if not isinstance(stmt, AssignmentStmt):  # `and stmt.new_syntax` to require annotation\n                continue\n\n            lhs = stmt.lvalues[0]\n            if not isinstance(lhs, NameExpr) or not is_valid_field(lhs.name):\n                continue\n\n            if not stmt.new_syntax and self.plugin_config.warn_untyped_fields:\n                error_untyped_fields(ctx.api, stmt)\n\n            # if lhs.name == '__config__':  # BaseConfig not well handled; I'm not sure why yet\n            #     continue\n\n            sym = cls.info.names.get(lhs.name)\n            if sym is None:  # pragma: no cover\n                # This is likely due to a star import (see the dataclasses plugin for a more detailed explanation)\n                # This is the same logic used in the dataclasses plugin\n                continue\n\n            node = sym.node\n            if isinstance(node, PlaceholderNode):  # pragma: no cover\n                # See the PlaceholderNode docstring for more detail about how this can occur\n                # Basically, it is an edge case when dealing with complex import logic\n                # This is the same logic used in the dataclasses plugin\n                continue\n            if not isinstance(node, Var):  # pragma: no cover\n                # Don't know if this edge case still happens with the `is_valid_field` check above\n                # but better safe than sorry\n                continue\n\n            # x: ClassVar[int] is ignored by dataclasses.\n            if node.is_classvar:\n                continue\n\n            is_required = self.get_is_required(cls, stmt, lhs)\n            alias, has_dynamic_alias = self.get_alias_info(stmt)\n            if (\n                has_dynamic_alias\n                and not model_config.allow_population_by_field_name\n                and self.plugin_config.warn_required_dynamic_aliases\n            ):\n                error_required_dynamic_aliases(ctx.api, stmt)\n            fields.append(\n                PydanticModelField(\n                    name=lhs.name,\n                    is_required=is_required,\n                    alias=alias,\n                    has_dynamic_alias=has_dynamic_alias,\n                    line=stmt.line,\n                    column=stmt.column,\n                )\n            )\n            known_fields.add(lhs.name)\n        all_fields = fields.copy()\n        for info in cls.info.mro[1:]:  # 0 is the current class, -2 is BaseModel, -1 is object\n            if METADATA_KEY not in info.metadata:\n                continue\n\n            superclass_fields = []\n            # Each class depends on the set of fields in its ancestors\n            ctx.api.add_plugin_dependency(make_wildcard_trigger(get_fullname(info)))\n\n            for name, data in info.metadata[METADATA_KEY]['fields'].items():\n                if name not in known_fields:\n                    field = PydanticModelField.deserialize(info, data)\n                    known_fields.add(name)\n                    superclass_fields.append(field)\n                else:\n                    (field,) = (a for a in all_fields if a.name == name)\n                    all_fields.remove(field)\n                    superclass_fields.append(field)\n            all_fields = superclass_fields + all_fields\n        return all_fields\n\n    def add_initializer(self, fields: List['PydanticModelField'], config: 'ModelConfigData', is_settings: bool) -> None:\n        \"\"\"\n        Adds a fields-aware `__init__` method to the class.\n\n        The added `__init__` will be annotated with types vs. all `Any` depending on the plugin settings.\n        \"\"\"\n        ctx = self._ctx\n        typed = self.plugin_config.init_typed\n        use_alias = config.allow_population_by_field_name is not True\n        force_all_optional = is_settings or bool(\n            config.has_alias_generator and not config.allow_population_by_field_name\n        )\n        init_arguments = self.get_field_arguments(\n            fields, typed=typed, force_all_optional=force_all_optional, use_alias=use_alias\n        )\n        if not self.should_init_forbid_extra(fields, config):\n            var = Var('kwargs')\n            init_arguments.append(Argument(var, AnyType(TypeOfAny.explicit), None, ARG_STAR2))\n\n        if '__init__' not in ctx.cls.info.names:\n            add_method(ctx, '__init__', init_arguments, NoneType())\n\n    def add_construct_method(self, fields: List['PydanticModelField']) -> None:\n        \"\"\"\n        Adds a fully typed `construct` classmethod to the class.\n\n        Similar to the fields-aware __init__ method, but always uses the field names (not aliases),\n        and does not treat settings fields as optional.\n        \"\"\"\n        ctx = self._ctx\n        set_str = ctx.api.named_type(f'{BUILTINS_NAME}.set', [ctx.api.named_type(f'{BUILTINS_NAME}.str')])\n        optional_set_str = UnionType([set_str, NoneType()])\n        fields_set_argument = Argument(Var('_fields_set', optional_set_str), optional_set_str, None, ARG_OPT)\n        construct_arguments = self.get_field_arguments(fields, typed=True, force_all_optional=False, use_alias=False)\n        construct_arguments = [fields_set_argument] + construct_arguments\n\n        obj_type = ctx.api.named_type(f'{BUILTINS_NAME}.object')\n        self_tvar_name = '_PydanticBaseModel'  # Make sure it does not conflict with other names in the class\n        tvar_fullname = ctx.cls.fullname + '.' + self_tvar_name\n        if MYPY_VERSION_TUPLE >= (1, 4):\n            tvd = TypeVarType(\n                self_tvar_name,\n                tvar_fullname,\n                TypeVarId(-1),\n                [],\n                obj_type,\n                AnyType(TypeOfAny.from_omitted_generics),  # type: ignore[arg-type]\n            )\n            self_tvar_expr = TypeVarExpr(\n                self_tvar_name,\n                tvar_fullname,\n                [],\n                obj_type,\n                AnyType(TypeOfAny.from_omitted_generics),  # type: ignore[arg-type]\n            )\n        else:\n            tvd = TypeVarDef(self_tvar_name, tvar_fullname, -1, [], obj_type)\n            self_tvar_expr = TypeVarExpr(self_tvar_name, tvar_fullname, [], obj_type)\n        ctx.cls.info.names[self_tvar_name] = SymbolTableNode(MDEF, self_tvar_expr)\n\n        # Backward-compatible with TypeVarDef from Mypy 0.910.\n        if isinstance(tvd, TypeVarType):\n            self_type = tvd\n        else:\n            self_type = TypeVarType(tvd)\n\n        add_method(\n            ctx,\n            'construct',\n            construct_arguments,\n            return_type=self_type,\n            self_type=self_type,\n            tvar_def=tvd,\n            is_classmethod=True,\n        )\n\n    def set_frozen(self, fields: List['PydanticModelField'], frozen: bool) -> None:\n        \"\"\"\n        Marks all fields as properties so that attempts to set them trigger mypy errors.\n\n        This is the same approach used by the attrs and dataclasses plugins.\n        \"\"\"\n        ctx = self._ctx\n        info = ctx.cls.info\n        for field in fields:\n            sym_node = info.names.get(field.name)\n            if sym_node is not None:\n                var = sym_node.node\n                if isinstance(var, Var):\n                    var.is_property = frozen\n                elif isinstance(var, PlaceholderNode) and not ctx.api.final_iteration:\n                    # See https://github.com/pydantic/pydantic/issues/5191 to hit this branch for test coverage\n                    ctx.api.defer()\n                else:  # pragma: no cover\n                    # I don't know whether it's possible to hit this branch, but I've added it for safety\n                    try:\n                        var_str = str(var)\n                    except TypeError:\n                        # This happens for PlaceholderNode; perhaps it will happen for other types in the future..\n                        var_str = repr(var)\n                    detail = f'sym_node.node: {var_str} (of type {var.__class__})'\n                    error_unexpected_behavior(detail, ctx.api, ctx.cls)\n            else:\n                var = field.to_var(info, use_alias=False)\n                var.info = info\n                var.is_property = frozen\n                var._fullname = get_fullname(info) + '.' + get_name(var)\n                info.names[get_name(var)] = SymbolTableNode(MDEF, var)\n\n    def get_config_update(self, substmt: AssignmentStmt) -> Optional['ModelConfigData']:\n        \"\"\"\n        Determines the config update due to a single statement in the Config class definition.\n\n        Warns if a tracked config attribute is set to a value the plugin doesn't know how to interpret (e.g., an int)\n        \"\"\"\n        lhs = substmt.lvalues[0]\n        if not (isinstance(lhs, NameExpr) and lhs.name in self.tracked_config_fields):\n            return None\n        if lhs.name == 'extra':\n            if isinstance(substmt.rvalue, StrExpr):\n                forbid_extra = substmt.rvalue.value == 'forbid'\n            elif isinstance(substmt.rvalue, MemberExpr):\n                forbid_extra = substmt.rvalue.name == 'forbid'\n            else:\n                error_invalid_config_value(lhs.name, self._ctx.api, substmt)\n                return None\n            return ModelConfigData(forbid_extra=forbid_extra)\n        if lhs.name == 'alias_generator':\n            has_alias_generator = True\n            if isinstance(substmt.rvalue, NameExpr) and substmt.rvalue.fullname == 'builtins.None':\n                has_alias_generator = False\n            return ModelConfigData(has_alias_generator=has_alias_generator)\n        if isinstance(substmt.rvalue, NameExpr) and substmt.rvalue.fullname in ('builtins.True', 'builtins.False'):\n            return ModelConfigData(**{lhs.name: substmt.rvalue.fullname == 'builtins.True'})\n        error_invalid_config_value(lhs.name, self._ctx.api, substmt)\n        return None\n\n    @staticmethod\n    def get_is_required(cls: ClassDef, stmt: AssignmentStmt, lhs: NameExpr) -> bool:\n        \"\"\"\n        Returns a boolean indicating whether the field defined in `stmt` is a required field.\n        \"\"\"\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            # TempNode means annotation-only, so only non-required if Optional\n            value_type = get_proper_type(cls.info[lhs.name].type)\n            return not PydanticModelTransformer.type_has_implicit_default(value_type)\n        if isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME:\n            # The \"default value\" is a call to `Field`; at this point, the field is\n            # only required if default is Ellipsis (i.e., `field_name: Annotation = Field(...)`) or if default_factory\n            # is specified.\n            for arg, name in zip(expr.args, expr.arg_names):\n                # If name is None, then this arg is the default because it is the only positional argument.\n                if name is None or name == 'default':\n                    return arg.__class__ is EllipsisExpr\n                if name == 'default_factory':\n                    return False\n            # In this case, default and default_factory are not specified, so we need to look at the annotation\n            value_type = get_proper_type(cls.info[lhs.name].type)\n            return not PydanticModelTransformer.type_has_implicit_default(value_type)\n        # Only required if the \"default value\" is Ellipsis (i.e., `field_name: Annotation = ...`)\n        return isinstance(expr, EllipsisExpr)\n\n    @staticmethod\n    def type_has_implicit_default(type_: Optional[ProperType]) -> bool:\n        \"\"\"\n        Returns True if the passed type will be given an implicit default value.\n\n        In pydantic v1, this is the case for Optional types and Any (with default value None).\n        \"\"\"\n        if isinstance(type_, AnyType):\n            # Annotated as Any\n            return True\n        if isinstance(type_, UnionType) and any(\n            isinstance(item, NoneType) or isinstance(item, AnyType) for item in type_.items\n        ):\n            # Annotated as Optional, or otherwise having NoneType or AnyType in the union\n            return True\n        return False\n\n    @staticmethod\n    def get_alias_info(stmt: AssignmentStmt) -> Tuple[Optional[str], bool]:\n        \"\"\"\n        Returns a pair (alias, has_dynamic_alias), extracted from the declaration of the field defined in `stmt`.\n\n        `has_dynamic_alias` is True if and only if an alias is provided, but not as a string literal.\n        If `has_dynamic_alias` is True, `alias` will be None.\n        \"\"\"\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            # TempNode means annotation-only\n            return None, False\n\n        if not (\n            isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME\n        ):\n            # Assigned value is not a call to pydantic.fields.Field\n            return None, False\n\n        for i, arg_name in enumerate(expr.arg_names):\n            if arg_name != 'alias':\n                continue\n            arg = expr.args[i]\n            if isinstance(arg, StrExpr):\n                return arg.value, False\n            else:\n                return None, True\n        return None, False\n\n    def get_field_arguments(\n        self, fields: List['PydanticModelField'], typed: bool, force_all_optional: bool, use_alias: bool\n    ) -> List[Argument]:\n        \"\"\"\n        Helper function used during the construction of the `__init__` and `construct` method signatures.\n\n        Returns a list of mypy Argument instances for use in the generated signatures.\n        \"\"\"\n        info = self._ctx.cls.info\n        arguments = [\n            field.to_argument(info, typed=typed, force_optional=force_all_optional, use_alias=use_alias)\n            for field in fields\n            if not (use_alias and field.has_dynamic_alias)\n        ]\n        return arguments\n\n    def should_init_forbid_extra(self, fields: List['PydanticModelField'], config: 'ModelConfigData') -> bool:\n        \"\"\"\n        Indicates whether the generated `__init__` should get a `**kwargs` at the end of its signature\n\n        We disallow arbitrary kwargs if the extra config setting is \"forbid\", or if the plugin config says to,\n        *unless* a required dynamic alias is present (since then we can't determine a valid signature).\n        \"\"\"\n        if not config.allow_population_by_field_name:\n            if self.is_dynamic_alias_present(fields, bool(config.has_alias_generator)):\n                return False\n        if config.forbid_extra:\n            return True\n        return self.plugin_config.init_forbid_extra\n\n    @staticmethod\n    def is_dynamic_alias_present(fields: List['PydanticModelField'], has_alias_generator: bool) -> bool:\n        \"\"\"\n        Returns whether any fields on the model have a \"dynamic alias\", i.e., an alias that cannot be\n        determined during static analysis.\n        \"\"\"\n        for field in fields:\n            if field.has_dynamic_alias:\n                return True\n        if has_alias_generator:\n            for field in fields:\n                if field.alias is None:\n                    return True\n        return False\n\n\nclass PydanticModelField:\n    def __init__(\n        self, name: str, is_required: bool, alias: Optional[str], has_dynamic_alias: bool, line: int, column: int\n    ):\n        self.name = name\n        self.is_required = is_required\n        self.alias = alias\n        self.has_dynamic_alias = has_dynamic_alias\n        self.line = line\n        self.column = column\n\n    def to_var(self, info: TypeInfo, use_alias: bool) -> Var:\n        name = self.name\n        if use_alias and self.alias is not None:\n            name = self.alias\n        return Var(name, info[self.name].type)\n\n    def to_argument(self, info: TypeInfo, typed: bool, force_optional: bool, use_alias: bool) -> Argument:\n        if typed and info[self.name].type is not None:\n            type_annotation = info[self.name].type\n        else:\n            type_annotation = AnyType(TypeOfAny.explicit)\n        return Argument(\n            variable=self.to_var(info, use_alias),\n            type_annotation=type_annotation,\n            initializer=None,\n            kind=ARG_NAMED_OPT if force_optional or not self.is_required else ARG_NAMED,\n        )\n\n    def serialize(self) -> JsonDict:\n        return self.__dict__\n\n    @classmethod\n    def deserialize(cls, info: TypeInfo, data: JsonDict) -> 'PydanticModelField':\n        return cls(**data)\n\n\nclass ModelConfigData:\n    def __init__(\n        self,\n        forbid_extra: Optional[bool] = None,\n        allow_mutation: Optional[bool] = None,\n        frozen: Optional[bool] = None,\n        orm_mode: Optional[bool] = None,\n        allow_population_by_field_name: Optional[bool] = None,\n        has_alias_generator: Optional[bool] = None,\n    ):\n        self.forbid_extra = forbid_extra\n        self.allow_mutation = allow_mutation\n        self.frozen = frozen\n        self.orm_mode = orm_mode\n        self.allow_population_by_field_name = allow_population_by_field_name\n        self.has_alias_generator = has_alias_generator\n\n    def set_values_dict(self) -> Dict[str, Any]:\n        return {k: v for k, v in self.__dict__.items() if v is not None}\n\n    def update(self, config: Optional['ModelConfigData']) -> None:\n        if config is None:\n            return\n        for k, v in config.set_values_dict().items():\n            setattr(self, k, v)\n\n    def setdefault(self, key: str, value: Any) -> None:\n        if getattr(self, key) is None:\n            setattr(self, key, value)\n\n\nERROR_ORM = ErrorCode('pydantic-orm', 'Invalid from_orm call', 'Pydantic')\nERROR_CONFIG = ErrorCode('pydantic-config', 'Invalid config value', 'Pydantic')\nERROR_ALIAS = ErrorCode('pydantic-alias', 'Dynamic alias disallowed', 'Pydantic')\nERROR_UNEXPECTED = ErrorCode('pydantic-unexpected', 'Unexpected behavior', 'Pydantic')\nERROR_UNTYPED = ErrorCode('pydantic-field', 'Untyped field disallowed', 'Pydantic')\nERROR_FIELD_DEFAULTS = ErrorCode('pydantic-field', 'Invalid Field defaults', 'Pydantic')\n\n\ndef error_from_orm(model_name: str, api: CheckerPluginInterface, context: Context) -> None:\n    api.fail(f'\"{model_name}\" does not have orm_mode=True', context, code=ERROR_ORM)\n\n\ndef error_invalid_config_value(name: str, api: SemanticAnalyzerPluginInterface, context: Context) -> None:\n    api.fail(f'Invalid value for \"Config.{name}\"', context, code=ERROR_CONFIG)\n\n\ndef error_required_dynamic_aliases(api: SemanticAnalyzerPluginInterface, context: Context) -> None:\n    api.fail('Required dynamic aliases disallowed', context, code=ERROR_ALIAS)\n\n\ndef error_unexpected_behavior(\n    detail: str, api: Union[CheckerPluginInterface, SemanticAnalyzerPluginInterface], context: Context\n) -> None:  # pragma: no cover\n    # Can't think of a good way to test this, but I confirmed it renders as desired by adding to a non-error path\n    link = 'https://github.com/pydantic/pydantic/issues/new/choose'\n    full_message = f'The pydantic mypy plugin ran into unexpected behavior: {detail}\\n'\n    full_message += f'Please consider reporting this bug at {link} so we can try to fix it!'\n    api.fail(full_message, context, code=ERROR_UNEXPECTED)\n\n\ndef error_untyped_fields(api: SemanticAnalyzerPluginInterface, context: Context) -> None:\n    api.fail('Untyped fields disallowed', context, code=ERROR_UNTYPED)\n\n\ndef error_default_and_default_factory_specified(api: CheckerPluginInterface, context: Context) -> None:\n    api.fail('Field default and default_factory cannot be specified together', context, code=ERROR_FIELD_DEFAULTS)\n\n\ndef add_method(\n    ctx: ClassDefContext,\n    name: str,\n    args: List[Argument],\n    return_type: Type,\n    self_type: Optional[Type] = None,\n    tvar_def: Optional[TypeVarDef] = None,\n    is_classmethod: bool = False,\n    is_new: bool = False,\n    # is_staticmethod: bool = False,\n) -> None:\n    \"\"\"\n    Adds a new method to a class.\n\n    This can be dropped if/when https://github.com/python/mypy/issues/7301 is merged\n    \"\"\"\n    info = ctx.cls.info\n\n    # First remove any previously generated methods with the same name\n    # to avoid clashes and problems in the semantic analyzer.\n    if name in info.names:\n        sym = info.names[name]\n        if sym.plugin_generated and isinstance(sym.node, FuncDef):\n            ctx.cls.defs.body.remove(sym.node)  # pragma: no cover\n\n    self_type = self_type or fill_typevars(info)\n    if is_classmethod or is_new:\n        first = [Argument(Var('_cls'), TypeType.make_normalized(self_type), None, ARG_POS)]\n    # elif is_staticmethod:\n    #     first = []\n    else:\n        self_type = self_type or fill_typevars(info)\n        first = [Argument(Var('__pydantic_self__'), self_type, None, ARG_POS)]\n    args = first + args\n    arg_types, arg_names, arg_kinds = [], [], []\n    for arg in args:\n        assert arg.type_annotation, 'All arguments must be fully typed.'\n        arg_types.append(arg.type_annotation)\n        arg_names.append(get_name(arg.variable))\n        arg_kinds.append(arg.kind)\n\n    function_type = ctx.api.named_type(f'{BUILTINS_NAME}.function')\n    signature = CallableType(arg_types, arg_kinds, arg_names, return_type, function_type)\n    if tvar_def:\n        signature.variables = [tvar_def]\n\n    func = FuncDef(name, args, Block([PassStmt()]))\n    func.info = info\n    func.type = set_callable_name(signature, func)\n    func.is_class = is_classmethod\n    # func.is_static = is_staticmethod\n    func._fullname = get_fullname(info) + '.' + name\n    func.line = info.line\n\n    # NOTE: we would like the plugin generated node to dominate, but we still\n    # need to keep any existing definitions so they get semantically analyzed.\n    if name in info.names:\n        # Get a nice unique name instead.\n        r_name = get_unique_redefinition_name(name, info.names)\n        info.names[r_name] = info.names[name]\n\n    if is_classmethod:  # or is_staticmethod:\n        func.is_decorated = True\n        v = Var(name, func.type)\n        v.info = info\n        v._fullname = func._fullname\n        # if is_classmethod:\n        v.is_classmethod = True\n        dec = Decorator(func, [NameExpr('classmethod')], v)\n        # else:\n        #     v.is_staticmethod = True\n        #     dec = Decorator(func, [NameExpr('staticmethod')], v)\n\n        dec.line = info.line\n        sym = SymbolTableNode(MDEF, dec)\n    else:\n        sym = SymbolTableNode(MDEF, func)\n    sym.plugin_generated = True\n\n    info.names[name] = sym\n    info.defn.defs.body.append(func)\n\n\ndef get_fullname(x: Union[FuncBase, SymbolNode]) -> str:\n    \"\"\"\n    Used for compatibility with mypy 0.740; can be dropped once support for 0.740 is dropped.\n    \"\"\"\n    fn = x.fullname\n    if callable(fn):  # pragma: no cover\n        return fn()\n    return fn\n\n\ndef get_name(x: Union[FuncBase, SymbolNode]) -> str:\n    \"\"\"\n    Used for compatibility with mypy 0.740; can be dropped once support for 0.740 is dropped.\n    \"\"\"\n    fn = x.name\n    if callable(fn):  # pragma: no cover\n        return fn()\n    return fn\n\n\ndef parse_toml(config_file: str) -> Optional[Dict[str, Any]]:\n    if not config_file.endswith('.toml'):\n        return None\n\n    read_mode = 'rb'\n    if sys.version_info >= (3, 11):\n        import tomllib as toml_\n    else:\n        try:\n            import tomli as toml_\n        except ImportError:\n            # older versions of mypy have toml as a dependency, not tomli\n            read_mode = 'r'\n            try:\n                import toml as toml_  # type: ignore[no-redef]\n            except ImportError:  # pragma: no cover\n                import warnings\n\n                warnings.warn('No TOML parser installed, cannot read configuration from `pyproject.toml`.')\n                return None\n\n    with open(config_file, read_mode) as rf:\n        return toml_.load(rf)  # type: ignore[arg-type]\n", "pydantic/v1/networks.py": "import re\nfrom ipaddress import (\n    IPv4Address,\n    IPv4Interface,\n    IPv4Network,\n    IPv6Address,\n    IPv6Interface,\n    IPv6Network,\n    _BaseAddress,\n    _BaseNetwork,\n)\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Collection,\n    Dict,\n    Generator,\n    List,\n    Match,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Type,\n    Union,\n    cast,\n    no_type_check,\n)\n\nfrom pydantic.v1 import errors\nfrom pydantic.v1.utils import Representation, update_not_none\nfrom pydantic.v1.validators import constr_length_validator, str_validator\n\nif TYPE_CHECKING:\n    import email_validator\n    from typing_extensions import TypedDict\n\n    from pydantic.v1.config import BaseConfig\n    from pydantic.v1.fields import ModelField\n    from pydantic.v1.typing import AnyCallable\n\n    CallableGenerator = Generator[AnyCallable, None, None]\n\n    class Parts(TypedDict, total=False):\n        scheme: str\n        user: Optional[str]\n        password: Optional[str]\n        ipv4: Optional[str]\n        ipv6: Optional[str]\n        domain: Optional[str]\n        port: Optional[str]\n        path: Optional[str]\n        query: Optional[str]\n        fragment: Optional[str]\n\n    class HostParts(TypedDict, total=False):\n        host: str\n        tld: Optional[str]\n        host_type: Optional[str]\n        port: Optional[str]\n        rebuild: bool\n\nelse:\n    email_validator = None\n\n    class Parts(dict):\n        pass\n\n\nNetworkType = Union[str, bytes, int, Tuple[Union[str, bytes, int], Union[str, int]]]\n\n__all__ = [\n    'AnyUrl',\n    'AnyHttpUrl',\n    'FileUrl',\n    'HttpUrl',\n    'stricturl',\n    'EmailStr',\n    'NameEmail',\n    'IPvAnyAddress',\n    'IPvAnyInterface',\n    'IPvAnyNetwork',\n    'PostgresDsn',\n    'CockroachDsn',\n    'AmqpDsn',\n    'RedisDsn',\n    'MongoDsn',\n    'KafkaDsn',\n    'validate_email',\n]\n\n_url_regex_cache = None\n_multi_host_url_regex_cache = None\n_ascii_domain_regex_cache = None\n_int_domain_regex_cache = None\n_host_regex_cache = None\n\n_host_regex = (\n    r'(?:'\n    r'(?P<ipv4>(?:\\d{1,3}\\.){3}\\d{1,3})(?=$|[/:#?])|'  # ipv4\n    r'(?P<ipv6>\\[[A-F0-9]*:[A-F0-9:]+\\])(?=$|[/:#?])|'  # ipv6\n    r'(?P<domain>[^\\s/:?#]+)'  # domain, validation occurs later\n    r')?'\n    r'(?::(?P<port>\\d+))?'  # port\n)\n_scheme_regex = r'(?:(?P<scheme>[a-z][a-z0-9+\\-.]+)://)?'  # scheme https://tools.ietf.org/html/rfc3986#appendix-A\n_user_info_regex = r'(?:(?P<user>[^\\s:/]*)(?::(?P<password>[^\\s/]*))?@)?'\n_path_regex = r'(?P<path>/[^\\s?#]*)?'\n_query_regex = r'(?:\\?(?P<query>[^\\s#]*))?'\n_fragment_regex = r'(?:#(?P<fragment>[^\\s#]*))?'\n\n\ndef url_regex() -> Pattern[str]:\n    global _url_regex_cache\n    if _url_regex_cache is None:\n        _url_regex_cache = re.compile(\n            rf'{_scheme_regex}{_user_info_regex}{_host_regex}{_path_regex}{_query_regex}{_fragment_regex}',\n            re.IGNORECASE,\n        )\n    return _url_regex_cache\n\n\ndef multi_host_url_regex() -> Pattern[str]:\n    \"\"\"\n    Compiled multi host url regex.\n\n    Additionally to `url_regex` it allows to match multiple hosts.\n    E.g. host1.db.net,host2.db.net\n    \"\"\"\n    global _multi_host_url_regex_cache\n    if _multi_host_url_regex_cache is None:\n        _multi_host_url_regex_cache = re.compile(\n            rf'{_scheme_regex}{_user_info_regex}'\n            r'(?P<hosts>([^/]*))'  # validation occurs later\n            rf'{_path_regex}{_query_regex}{_fragment_regex}',\n            re.IGNORECASE,\n        )\n    return _multi_host_url_regex_cache\n\n\ndef ascii_domain_regex() -> Pattern[str]:\n    global _ascii_domain_regex_cache\n    if _ascii_domain_regex_cache is None:\n        ascii_chunk = r'[_0-9a-z](?:[-_0-9a-z]{0,61}[_0-9a-z])?'\n        ascii_domain_ending = r'(?P<tld>\\.[a-z]{2,63})?\\.?'\n        _ascii_domain_regex_cache = re.compile(\n            fr'(?:{ascii_chunk}\\.)*?{ascii_chunk}{ascii_domain_ending}', re.IGNORECASE\n        )\n    return _ascii_domain_regex_cache\n\n\ndef int_domain_regex() -> Pattern[str]:\n    global _int_domain_regex_cache\n    if _int_domain_regex_cache is None:\n        int_chunk = r'[_0-9a-\\U00040000](?:[-_0-9a-\\U00040000]{0,61}[_0-9a-\\U00040000])?'\n        int_domain_ending = r'(?P<tld>(\\.[^\\W\\d_]{2,63})|(\\.(?:xn--)[_0-9a-z-]{2,63}))?\\.?'\n        _int_domain_regex_cache = re.compile(fr'(?:{int_chunk}\\.)*?{int_chunk}{int_domain_ending}', re.IGNORECASE)\n    return _int_domain_regex_cache\n\n\ndef host_regex() -> Pattern[str]:\n    global _host_regex_cache\n    if _host_regex_cache is None:\n        _host_regex_cache = re.compile(\n            _host_regex,\n            re.IGNORECASE,\n        )\n    return _host_regex_cache\n\n\nclass AnyUrl(str):\n    strip_whitespace = True\n    min_length = 1\n    max_length = 2**16\n    allowed_schemes: Optional[Collection[str]] = None\n    tld_required: bool = False\n    user_required: bool = False\n    host_required: bool = True\n    hidden_parts: Set[str] = set()\n\n    __slots__ = ('scheme', 'user', 'password', 'host', 'tld', 'host_type', 'port', 'path', 'query', 'fragment')\n\n    @no_type_check\n    def __new__(cls, url: Optional[str], **kwargs) -> object:\n        return str.__new__(cls, cls.build(**kwargs) if url is None else url)\n\n    def __init__(\n        self,\n        url: str,\n        *,\n        scheme: str,\n        user: Optional[str] = None,\n        password: Optional[str] = None,\n        host: Optional[str] = None,\n        tld: Optional[str] = None,\n        host_type: str = 'domain',\n        port: Optional[str] = None,\n        path: Optional[str] = None,\n        query: Optional[str] = None,\n        fragment: Optional[str] = None,\n    ) -> None:\n        str.__init__(url)\n        self.scheme = scheme\n        self.user = user\n        self.password = password\n        self.host = host\n        self.tld = tld\n        self.host_type = host_type\n        self.port = port\n        self.path = path\n        self.query = query\n        self.fragment = fragment\n\n    @classmethod\n    def build(\n        cls,\n        *,\n        scheme: str,\n        user: Optional[str] = None,\n        password: Optional[str] = None,\n        host: str,\n        port: Optional[str] = None,\n        path: Optional[str] = None,\n        query: Optional[str] = None,\n        fragment: Optional[str] = None,\n        **_kwargs: str,\n    ) -> str:\n        parts = Parts(\n            scheme=scheme,\n            user=user,\n            password=password,\n            host=host,\n            port=port,\n            path=path,\n            query=query,\n            fragment=fragment,\n            **_kwargs,  # type: ignore[misc]\n        )\n\n        url = scheme + '://'\n        if user:\n            url += user\n        if password:\n            url += ':' + password\n        if user or password:\n            url += '@'\n        url += host\n        if port and ('port' not in cls.hidden_parts or cls.get_default_parts(parts).get('port') != port):\n            url += ':' + port\n        if path:\n            url += path\n        if query:\n            url += '?' + query\n        if fragment:\n            url += '#' + fragment\n        return url\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(field_schema, minLength=cls.min_length, maxLength=cls.max_length, format='uri')\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Any, field: 'ModelField', config: 'BaseConfig') -> 'AnyUrl':\n        if value.__class__ == cls:\n            return value\n        value = str_validator(value)\n        if cls.strip_whitespace:\n            value = value.strip()\n        url: str = cast(str, constr_length_validator(value, field, config))\n\n        m = cls._match_url(url)\n        # the regex should always match, if it doesn't please report with details of the URL tried\n        assert m, 'URL regex failed unexpectedly'\n\n        original_parts = cast('Parts', m.groupdict())\n        parts = cls.apply_default_parts(original_parts)\n        parts = cls.validate_parts(parts)\n\n        if m.end() != len(url):\n            raise errors.UrlExtraError(extra=url[m.end() :])\n\n        return cls._build_url(m, url, parts)\n\n    @classmethod\n    def _build_url(cls, m: Match[str], url: str, parts: 'Parts') -> 'AnyUrl':\n        \"\"\"\n        Validate hosts and build the AnyUrl object. Split from `validate` so this method\n        can be altered in `MultiHostDsn`.\n        \"\"\"\n        host, tld, host_type, rebuild = cls.validate_host(parts)\n\n        return cls(\n            None if rebuild else url,\n            scheme=parts['scheme'],\n            user=parts['user'],\n            password=parts['password'],\n            host=host,\n            tld=tld,\n            host_type=host_type,\n            port=parts['port'],\n            path=parts['path'],\n            query=parts['query'],\n            fragment=parts['fragment'],\n        )\n\n    @staticmethod\n    def _match_url(url: str) -> Optional[Match[str]]:\n        return url_regex().match(url)\n\n    @staticmethod\n    def _validate_port(port: Optional[str]) -> None:\n        if port is not None and int(port) > 65_535:\n            raise errors.UrlPortError()\n\n    @classmethod\n    def validate_parts(cls, parts: 'Parts', validate_port: bool = True) -> 'Parts':\n        \"\"\"\n        A method used to validate parts of a URL.\n        Could be overridden to set default values for parts if missing\n        \"\"\"\n        scheme = parts['scheme']\n        if scheme is None:\n            raise errors.UrlSchemeError()\n\n        if cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\n            raise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\n\n        if validate_port:\n            cls._validate_port(parts['port'])\n\n        user = parts['user']\n        if cls.user_required and user is None:\n            raise errors.UrlUserInfoError()\n\n        return parts\n\n    @classmethod\n    def validate_host(cls, parts: 'Parts') -> Tuple[str, Optional[str], str, bool]:\n        tld, host_type, rebuild = None, None, False\n        for f in ('domain', 'ipv4', 'ipv6'):\n            host = parts[f]  # type: ignore[literal-required]\n            if host:\n                host_type = f\n                break\n\n        if host is None:\n            if cls.host_required:\n                raise errors.UrlHostError()\n        elif host_type == 'domain':\n            is_international = False\n            d = ascii_domain_regex().fullmatch(host)\n            if d is None:\n                d = int_domain_regex().fullmatch(host)\n                if d is None:\n                    raise errors.UrlHostError()\n                is_international = True\n\n            tld = d.group('tld')\n            if tld is None and not is_international:\n                d = int_domain_regex().fullmatch(host)\n                assert d is not None\n                tld = d.group('tld')\n                is_international = True\n\n            if tld is not None:\n                tld = tld[1:]\n            elif cls.tld_required:\n                raise errors.UrlHostTldError()\n\n            if is_international:\n                host_type = 'int_domain'\n                rebuild = True\n                host = host.encode('idna').decode('ascii')\n                if tld is not None:\n                    tld = tld.encode('idna').decode('ascii')\n\n        return host, tld, host_type, rebuild  # type: ignore\n\n    @staticmethod\n    def get_default_parts(parts: 'Parts') -> 'Parts':\n        return {}\n\n    @classmethod\n    def apply_default_parts(cls, parts: 'Parts') -> 'Parts':\n        for key, value in cls.get_default_parts(parts).items():\n            if not parts[key]:  # type: ignore[literal-required]\n                parts[key] = value  # type: ignore[literal-required]\n        return parts\n\n    def __repr__(self) -> str:\n        extra = ', '.join(f'{n}={getattr(self, n)!r}' for n in self.__slots__ if getattr(self, n) is not None)\n        return f'{self.__class__.__name__}({super().__repr__()}, {extra})'\n\n\nclass AnyHttpUrl(AnyUrl):\n    allowed_schemes = {'http', 'https'}\n\n    __slots__ = ()\n\n\nclass HttpUrl(AnyHttpUrl):\n    tld_required = True\n    # https://stackoverflow.com/questions/417142/what-is-the-maximum-length-of-a-url-in-different-browsers\n    max_length = 2083\n    hidden_parts = {'port'}\n\n    @staticmethod\n    def get_default_parts(parts: 'Parts') -> 'Parts':\n        return {'port': '80' if parts['scheme'] == 'http' else '443'}\n\n\nclass FileUrl(AnyUrl):\n    allowed_schemes = {'file'}\n    host_required = False\n\n    __slots__ = ()\n\n\nclass MultiHostDsn(AnyUrl):\n    __slots__ = AnyUrl.__slots__ + ('hosts',)\n\n    def __init__(self, *args: Any, hosts: Optional[List['HostParts']] = None, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.hosts = hosts\n\n    @staticmethod\n    def _match_url(url: str) -> Optional[Match[str]]:\n        return multi_host_url_regex().match(url)\n\n    @classmethod\n    def validate_parts(cls, parts: 'Parts', validate_port: bool = True) -> 'Parts':\n        return super().validate_parts(parts, validate_port=False)\n\n    @classmethod\n    def _build_url(cls, m: Match[str], url: str, parts: 'Parts') -> 'MultiHostDsn':\n        hosts_parts: List['HostParts'] = []\n        host_re = host_regex()\n        for host in m.groupdict()['hosts'].split(','):\n            d: Parts = host_re.match(host).groupdict()  # type: ignore\n            host, tld, host_type, rebuild = cls.validate_host(d)\n            port = d.get('port')\n            cls._validate_port(port)\n            hosts_parts.append(\n                {\n                    'host': host,\n                    'host_type': host_type,\n                    'tld': tld,\n                    'rebuild': rebuild,\n                    'port': port,\n                }\n            )\n\n        if len(hosts_parts) > 1:\n            return cls(\n                None if any([hp['rebuild'] for hp in hosts_parts]) else url,\n                scheme=parts['scheme'],\n                user=parts['user'],\n                password=parts['password'],\n                path=parts['path'],\n                query=parts['query'],\n                fragment=parts['fragment'],\n                host_type=None,\n                hosts=hosts_parts,\n            )\n        else:\n            # backwards compatibility with single host\n            host_part = hosts_parts[0]\n            return cls(\n                None if host_part['rebuild'] else url,\n                scheme=parts['scheme'],\n                user=parts['user'],\n                password=parts['password'],\n                host=host_part['host'],\n                tld=host_part['tld'],\n                host_type=host_part['host_type'],\n                port=host_part.get('port'),\n                path=parts['path'],\n                query=parts['query'],\n                fragment=parts['fragment'],\n            )\n\n\nclass PostgresDsn(MultiHostDsn):\n    allowed_schemes = {\n        'postgres',\n        'postgresql',\n        'postgresql+asyncpg',\n        'postgresql+pg8000',\n        'postgresql+psycopg',\n        'postgresql+psycopg2',\n        'postgresql+psycopg2cffi',\n        'postgresql+py-postgresql',\n        'postgresql+pygresql',\n    }\n    user_required = True\n\n    __slots__ = ()\n\n\nclass CockroachDsn(AnyUrl):\n    allowed_schemes = {\n        'cockroachdb',\n        'cockroachdb+psycopg2',\n        'cockroachdb+asyncpg',\n    }\n    user_required = True\n\n\nclass AmqpDsn(AnyUrl):\n    allowed_schemes = {'amqp', 'amqps'}\n    host_required = False\n\n\nclass RedisDsn(AnyUrl):\n    __slots__ = ()\n    allowed_schemes = {'redis', 'rediss'}\n    host_required = False\n\n    @staticmethod\n    def get_default_parts(parts: 'Parts') -> 'Parts':\n        return {\n            'domain': 'localhost' if not (parts['ipv4'] or parts['ipv6']) else '',\n            'port': '6379',\n            'path': '/0',\n        }\n\n\nclass MongoDsn(AnyUrl):\n    allowed_schemes = {'mongodb'}\n\n    # TODO: Needed to generic \"Parts\" for \"Replica Set\", \"Sharded Cluster\", and other mongodb deployment modes\n    @staticmethod\n    def get_default_parts(parts: 'Parts') -> 'Parts':\n        return {\n            'port': '27017',\n        }\n\n\nclass KafkaDsn(AnyUrl):\n    allowed_schemes = {'kafka'}\n\n    @staticmethod\n    def get_default_parts(parts: 'Parts') -> 'Parts':\n        return {\n            'domain': 'localhost',\n            'port': '9092',\n        }\n\n\ndef stricturl(\n    *,\n    strip_whitespace: bool = True,\n    min_length: int = 1,\n    max_length: int = 2**16,\n    tld_required: bool = True,\n    host_required: bool = True,\n    allowed_schemes: Optional[Collection[str]] = None,\n) -> Type[AnyUrl]:\n    # use kwargs then define conf in a dict to aid with IDE type hinting\n    namespace = dict(\n        strip_whitespace=strip_whitespace,\n        min_length=min_length,\n        max_length=max_length,\n        tld_required=tld_required,\n        host_required=host_required,\n        allowed_schemes=allowed_schemes,\n    )\n    return type('UrlValue', (AnyUrl,), namespace)\n\n\ndef import_email_validator() -> None:\n    global email_validator\n    try:\n        import email_validator\n    except ImportError as e:\n        raise ImportError('email-validator is not installed, run `pip install pydantic[email]`') from e\n\n\nclass EmailStr(str):\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        field_schema.update(type='string', format='email')\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        # included here and below so the error happens straight away\n        import_email_validator()\n\n        yield str_validator\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Union[str]) -> str:\n        return validate_email(value)[1]\n\n\nclass NameEmail(Representation):\n    __slots__ = 'name', 'email'\n\n    def __init__(self, name: str, email: str):\n        self.name = name\n        self.email = email\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, NameEmail) and (self.name, self.email) == (other.name, other.email)\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        field_schema.update(type='string', format='name-email')\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        import_email_validator()\n\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Any) -> 'NameEmail':\n        if value.__class__ == cls:\n            return value\n        value = str_validator(value)\n        return cls(*validate_email(value))\n\n    def __str__(self) -> str:\n        return f'{self.name} <{self.email}>'\n\n\nclass IPvAnyAddress(_BaseAddress):\n    __slots__ = ()\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        field_schema.update(type='string', format='ipvanyaddress')\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Union[str, bytes, int]) -> Union[IPv4Address, IPv6Address]:\n        try:\n            return IPv4Address(value)\n        except ValueError:\n            pass\n\n        try:\n            return IPv6Address(value)\n        except ValueError:\n            raise errors.IPvAnyAddressError()\n\n\nclass IPvAnyInterface(_BaseAddress):\n    __slots__ = ()\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        field_schema.update(type='string', format='ipvanyinterface')\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: NetworkType) -> Union[IPv4Interface, IPv6Interface]:\n        try:\n            return IPv4Interface(value)\n        except ValueError:\n            pass\n\n        try:\n            return IPv6Interface(value)\n        except ValueError:\n            raise errors.IPvAnyInterfaceError()\n\n\nclass IPvAnyNetwork(_BaseNetwork):  # type: ignore\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        field_schema.update(type='string', format='ipvanynetwork')\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: NetworkType) -> Union[IPv4Network, IPv6Network]:\n        # Assume IP Network is defined with a default value for ``strict`` argument.\n        # Define your own class if you want to specify network address check strictness.\n        try:\n            return IPv4Network(value)\n        except ValueError:\n            pass\n\n        try:\n            return IPv6Network(value)\n        except ValueError:\n            raise errors.IPvAnyNetworkError()\n\n\npretty_email_regex = re.compile(r'([\\w ]*?) *<(.*)> *')\nMAX_EMAIL_LENGTH = 2048\n\"\"\"Maximum length for an email.\nA somewhat arbitrary but very generous number compared to what is allowed by most implementations.\n\"\"\"\n\n\ndef validate_email(value: Union[str]) -> Tuple[str, str]:\n    \"\"\"\n    Email address validation using https://pypi.org/project/email-validator/\n    Notes:\n    * raw ip address (literal) domain parts are not allowed.\n    * \"John Doe <local_part@domain.com>\" style \"pretty\" email addresses are processed\n    * spaces are striped from the beginning and end of addresses but no error is raised\n    \"\"\"\n    if email_validator is None:\n        import_email_validator()\n\n    if len(value) > MAX_EMAIL_LENGTH:\n        raise errors.EmailError()\n\n    m = pretty_email_regex.fullmatch(value)\n    name: Union[str, None] = None\n    if m:\n        name, value = m.groups()\n    email = value.strip()\n    try:\n        parts = email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise errors.EmailError from e\n\n    if hasattr(parts, 'normalized'):\n        # email-validator >= 2\n        email = parts.normalized\n        assert email is not None\n        name = name or parts.local_part\n        return name, email\n    else:\n        # email-validator >1, <2\n        at_index = email.index('@')\n        local_part = email[:at_index]  # RFC 5321, local part must be case-sensitive.\n        global_part = email[at_index:].lower()\n\n        return name or local_part, local_part + global_part\n", "pydantic/v1/decorator.py": "from functools import wraps\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Mapping, Optional, Tuple, Type, TypeVar, Union, overload\n\nfrom pydantic.v1 import validator\nfrom pydantic.v1.config import Extra\nfrom pydantic.v1.errors import ConfigError\nfrom pydantic.v1.main import BaseModel, create_model\nfrom pydantic.v1.typing import get_all_type_hints\nfrom pydantic.v1.utils import to_camel\n\n__all__ = ('validate_arguments',)\n\nif TYPE_CHECKING:\n    from pydantic.v1.typing import AnyCallable\n\n    AnyCallableT = TypeVar('AnyCallableT', bound=AnyCallable)\n    ConfigType = Union[None, Type[Any], Dict[str, Any]]\n\n\n@overload\ndef validate_arguments(func: None = None, *, config: 'ConfigType' = None) -> Callable[['AnyCallableT'], 'AnyCallableT']:\n    ...\n\n\n@overload\ndef validate_arguments(func: 'AnyCallableT') -> 'AnyCallableT':\n    ...\n\n\ndef validate_arguments(func: Optional['AnyCallableT'] = None, *, config: 'ConfigType' = None) -> Any:\n    \"\"\"\n    Decorator to validate the arguments passed to a function.\n    \"\"\"\n\n    def validate(_func: 'AnyCallable') -> 'AnyCallable':\n        vd = ValidatedFunction(_func, config)\n\n        @wraps(_func)\n        def wrapper_function(*args: Any, **kwargs: Any) -> Any:\n            return vd.call(*args, **kwargs)\n\n        wrapper_function.vd = vd  # type: ignore\n        wrapper_function.validate = vd.init_model_instance  # type: ignore\n        wrapper_function.raw_function = vd.raw_function  # type: ignore\n        wrapper_function.model = vd.model  # type: ignore\n        return wrapper_function\n\n    if func:\n        return validate(func)\n    else:\n        return validate\n\n\nALT_V_ARGS = 'v__args'\nALT_V_KWARGS = 'v__kwargs'\nV_POSITIONAL_ONLY_NAME = 'v__positional_only'\nV_DUPLICATE_KWARGS = 'v__duplicate_kwargs'\n\n\nclass ValidatedFunction:\n    def __init__(self, function: 'AnyCallableT', config: 'ConfigType'):  # noqa C901\n        from inspect import Parameter, signature\n\n        parameters: Mapping[str, Parameter] = signature(function).parameters\n\n        if parameters.keys() & {ALT_V_ARGS, ALT_V_KWARGS, V_POSITIONAL_ONLY_NAME, V_DUPLICATE_KWARGS}:\n            raise ConfigError(\n                f'\"{ALT_V_ARGS}\", \"{ALT_V_KWARGS}\", \"{V_POSITIONAL_ONLY_NAME}\" and \"{V_DUPLICATE_KWARGS}\" '\n                f'are not permitted as argument names when using the \"{validate_arguments.__name__}\" decorator'\n            )\n\n        self.raw_function = function\n        self.arg_mapping: Dict[int, str] = {}\n        self.positional_only_args = set()\n        self.v_args_name = 'args'\n        self.v_kwargs_name = 'kwargs'\n\n        type_hints = get_all_type_hints(function)\n        takes_args = False\n        takes_kwargs = False\n        fields: Dict[str, Tuple[Any, Any]] = {}\n        for i, (name, p) in enumerate(parameters.items()):\n            if p.annotation is p.empty:\n                annotation = Any\n            else:\n                annotation = type_hints[name]\n\n            default = ... if p.default is p.empty else p.default\n            if p.kind == Parameter.POSITIONAL_ONLY:\n                self.arg_mapping[i] = name\n                fields[name] = annotation, default\n                fields[V_POSITIONAL_ONLY_NAME] = List[str], None\n                self.positional_only_args.add(name)\n            elif p.kind == Parameter.POSITIONAL_OR_KEYWORD:\n                self.arg_mapping[i] = name\n                fields[name] = annotation, default\n                fields[V_DUPLICATE_KWARGS] = List[str], None\n            elif p.kind == Parameter.KEYWORD_ONLY:\n                fields[name] = annotation, default\n            elif p.kind == Parameter.VAR_POSITIONAL:\n                self.v_args_name = name\n                fields[name] = Tuple[annotation, ...], None\n                takes_args = True\n            else:\n                assert p.kind == Parameter.VAR_KEYWORD, p.kind\n                self.v_kwargs_name = name\n                fields[name] = Dict[str, annotation], None  # type: ignore\n                takes_kwargs = True\n\n        # these checks avoid a clash between \"args\" and a field with that name\n        if not takes_args and self.v_args_name in fields:\n            self.v_args_name = ALT_V_ARGS\n\n        # same with \"kwargs\"\n        if not takes_kwargs and self.v_kwargs_name in fields:\n            self.v_kwargs_name = ALT_V_KWARGS\n\n        if not takes_args:\n            # we add the field so validation below can raise the correct exception\n            fields[self.v_args_name] = List[Any], None\n\n        if not takes_kwargs:\n            # same with kwargs\n            fields[self.v_kwargs_name] = Dict[Any, Any], None\n\n        self.create_model(fields, takes_args, takes_kwargs, config)\n\n    def init_model_instance(self, *args: Any, **kwargs: Any) -> BaseModel:\n        values = self.build_values(args, kwargs)\n        return self.model(**values)\n\n    def call(self, *args: Any, **kwargs: Any) -> Any:\n        m = self.init_model_instance(*args, **kwargs)\n        return self.execute(m)\n\n    def build_values(self, args: Tuple[Any, ...], kwargs: Dict[str, Any]) -> Dict[str, Any]:\n        values: Dict[str, Any] = {}\n        if args:\n            arg_iter = enumerate(args)\n            while True:\n                try:\n                    i, a = next(arg_iter)\n                except StopIteration:\n                    break\n                arg_name = self.arg_mapping.get(i)\n                if arg_name is not None:\n                    values[arg_name] = a\n                else:\n                    values[self.v_args_name] = [a] + [a for _, a in arg_iter]\n                    break\n\n        var_kwargs: Dict[str, Any] = {}\n        wrong_positional_args = []\n        duplicate_kwargs = []\n        fields_alias = [\n            field.alias\n            for name, field in self.model.__fields__.items()\n            if name not in (self.v_args_name, self.v_kwargs_name)\n        ]\n        non_var_fields = set(self.model.__fields__) - {self.v_args_name, self.v_kwargs_name}\n        for k, v in kwargs.items():\n            if k in non_var_fields or k in fields_alias:\n                if k in self.positional_only_args:\n                    wrong_positional_args.append(k)\n                if k in values:\n                    duplicate_kwargs.append(k)\n                values[k] = v\n            else:\n                var_kwargs[k] = v\n\n        if var_kwargs:\n            values[self.v_kwargs_name] = var_kwargs\n        if wrong_positional_args:\n            values[V_POSITIONAL_ONLY_NAME] = wrong_positional_args\n        if duplicate_kwargs:\n            values[V_DUPLICATE_KWARGS] = duplicate_kwargs\n        return values\n\n    def execute(self, m: BaseModel) -> Any:\n        d = {k: v for k, v in m._iter() if k in m.__fields_set__ or m.__fields__[k].default_factory}\n        var_kwargs = d.pop(self.v_kwargs_name, {})\n\n        if self.v_args_name in d:\n            args_: List[Any] = []\n            in_kwargs = False\n            kwargs = {}\n            for name, value in d.items():\n                if in_kwargs:\n                    kwargs[name] = value\n                elif name == self.v_args_name:\n                    args_ += value\n                    in_kwargs = True\n                else:\n                    args_.append(value)\n            return self.raw_function(*args_, **kwargs, **var_kwargs)\n        elif self.positional_only_args:\n            args_ = []\n            kwargs = {}\n            for name, value in d.items():\n                if name in self.positional_only_args:\n                    args_.append(value)\n                else:\n                    kwargs[name] = value\n            return self.raw_function(*args_, **kwargs, **var_kwargs)\n        else:\n            return self.raw_function(**d, **var_kwargs)\n\n    def create_model(self, fields: Dict[str, Any], takes_args: bool, takes_kwargs: bool, config: 'ConfigType') -> None:\n        pos_args = len(self.arg_mapping)\n\n        class CustomConfig:\n            pass\n\n        if not TYPE_CHECKING:  # pragma: no branch\n            if isinstance(config, dict):\n                CustomConfig = type('Config', (), config)  # noqa: F811\n            elif config is not None:\n                CustomConfig = config  # noqa: F811\n\n        if hasattr(CustomConfig, 'fields') or hasattr(CustomConfig, 'alias_generator'):\n            raise ConfigError(\n                'Setting the \"fields\" and \"alias_generator\" property on custom Config for '\n                '@validate_arguments is not yet supported, please remove.'\n            )\n\n        class DecoratorBaseModel(BaseModel):\n            @validator(self.v_args_name, check_fields=False, allow_reuse=True)\n            def check_args(cls, v: Optional[List[Any]]) -> Optional[List[Any]]:\n                if takes_args or v is None:\n                    return v\n\n                raise TypeError(f'{pos_args} positional arguments expected but {pos_args + len(v)} given')\n\n            @validator(self.v_kwargs_name, check_fields=False, allow_reuse=True)\n            def check_kwargs(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\n                if takes_kwargs or v is None:\n                    return v\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v.keys()))\n                raise TypeError(f'unexpected keyword argument{plural}: {keys}')\n\n            @validator(V_POSITIONAL_ONLY_NAME, check_fields=False, allow_reuse=True)\n            def check_positional_only(cls, v: Optional[List[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'positional-only argument{plural} passed as keyword argument{plural}: {keys}')\n\n            @validator(V_DUPLICATE_KWARGS, check_fields=False, allow_reuse=True)\n            def check_duplicate_kwargs(cls, v: Optional[List[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'multiple values for argument{plural}: {keys}')\n\n            class Config(CustomConfig):\n                extra = getattr(CustomConfig, 'extra', Extra.forbid)\n\n        self.model = create_model(to_camel(self.raw_function.__name__), __base__=DecoratorBaseModel, **fields)\n", "pydantic/v1/datetime_parse.py": "\"\"\"\nFunctions to parse datetime objects.\n\nWe're using regular expressions rather than time.strptime because:\n- They provide both validation and parsing.\n- They're more flexible for datetimes.\n- The date/datetime/time constructors produce friendlier error messages.\n\nStolen from https://raw.githubusercontent.com/django/django/main/django/utils/dateparse.py at\n9718fa2e8abe430c3526a9278dd976443d4ae3c6\n\nChanged to:\n* use standard python datetime types not django.utils.timezone\n* raise ValueError when regex doesn't match rather than returning None\n* support parsing unix timestamps for dates and datetimes\n\"\"\"\nimport re\nfrom datetime import date, datetime, time, timedelta, timezone\nfrom typing import Dict, Optional, Type, Union\n\nfrom pydantic.v1 import errors\n\ndate_expr = r'(?P<year>\\d{4})-(?P<month>\\d{1,2})-(?P<day>\\d{1,2})'\ntime_expr = (\n    r'(?P<hour>\\d{1,2}):(?P<minute>\\d{1,2})'\n    r'(?::(?P<second>\\d{1,2})(?:\\.(?P<microsecond>\\d{1,6})\\d{0,6})?)?'\n    r'(?P<tzinfo>Z|[+-]\\d{2}(?::?\\d{2})?)?$'\n)\n\ndate_re = re.compile(f'{date_expr}$')\ntime_re = re.compile(time_expr)\ndatetime_re = re.compile(f'{date_expr}[T ]{time_expr}')\n\nstandard_duration_re = re.compile(\n    r'^'\n    r'(?:(?P<days>-?\\d+) (days?, )?)?'\n    r'((?:(?P<hours>-?\\d+):)(?=\\d+:\\d+))?'\n    r'(?:(?P<minutes>-?\\d+):)?'\n    r'(?P<seconds>-?\\d+)'\n    r'(?:\\.(?P<microseconds>\\d{1,6})\\d{0,6})?'\n    r'$'\n)\n\n# Support the sections of ISO 8601 date representation that are accepted by timedelta\niso8601_duration_re = re.compile(\n    r'^(?P<sign>[-+]?)'\n    r'P'\n    r'(?:(?P<days>\\d+(.\\d+)?)D)?'\n    r'(?:T'\n    r'(?:(?P<hours>\\d+(.\\d+)?)H)?'\n    r'(?:(?P<minutes>\\d+(.\\d+)?)M)?'\n    r'(?:(?P<seconds>\\d+(.\\d+)?)S)?'\n    r')?'\n    r'$'\n)\n\nEPOCH = datetime(1970, 1, 1)\n# if greater than this, the number is in ms, if less than or equal it's in seconds\n# (in seconds this is 11th October 2603, in ms it's 20th August 1970)\nMS_WATERSHED = int(2e10)\n# slightly more than datetime.max in ns - (datetime.max - EPOCH).total_seconds() * 1e9\nMAX_NUMBER = int(3e20)\nStrBytesIntFloat = Union[str, bytes, int, float]\n\n\ndef get_numeric(value: StrBytesIntFloat, native_expected_type: str) -> Union[None, int, float]:\n    if isinstance(value, (int, float)):\n        return value\n    try:\n        return float(value)\n    except ValueError:\n        return None\n    except TypeError:\n        raise TypeError(f'invalid type; expected {native_expected_type}, string, bytes, int or float')\n\n\ndef from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    if seconds > MAX_NUMBER:\n        return datetime.max\n    elif seconds < -MAX_NUMBER:\n        return datetime.min\n\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)\n\n\ndef _parse_timezone(value: Optional[str], error: Type[Exception]) -> Union[None, int, timezone]:\n    if value == 'Z':\n        return timezone.utc\n    elif value is not None:\n        offset_mins = int(value[-2:]) if len(value) > 3 else 0\n        offset = 60 * int(value[1:3]) + offset_mins\n        if value[0] == '-':\n            offset = -offset\n        try:\n            return timezone(timedelta(minutes=offset))\n        except ValueError:\n            raise error()\n    else:\n        return None\n\n\ndef parse_date(value: Union[date, StrBytesIntFloat]) -> date:\n    \"\"\"\n    Parse a date/int/float/string and return a datetime.date.\n\n    Raise ValueError if the input is well formatted but not a valid date.\n    Raise ValueError if the input isn't well formatted.\n    \"\"\"\n    if isinstance(value, date):\n        if isinstance(value, datetime):\n            return value.date()\n        else:\n            return value\n\n    number = get_numeric(value, 'date')\n    if number is not None:\n        return from_unix_seconds(number).date()\n\n    if isinstance(value, bytes):\n        value = value.decode()\n\n    match = date_re.match(value)  # type: ignore\n    if match is None:\n        raise errors.DateError()\n\n    kw = {k: int(v) for k, v in match.groupdict().items()}\n\n    try:\n        return date(**kw)\n    except ValueError:\n        raise errors.DateError()\n\n\ndef parse_time(value: Union[time, StrBytesIntFloat]) -> time:\n    \"\"\"\n    Parse a time/string and return a datetime.time.\n\n    Raise ValueError if the input is well formatted but not a valid time.\n    Raise ValueError if the input isn't well formatted, in particular if it contains an offset.\n    \"\"\"\n    if isinstance(value, time):\n        return value\n\n    number = get_numeric(value, 'time')\n    if number is not None:\n        if number >= 86400:\n            # doesn't make sense since the time time loop back around to 0\n            raise errors.TimeError()\n        return (datetime.min + timedelta(seconds=number)).time()\n\n    if isinstance(value, bytes):\n        value = value.decode()\n\n    match = time_re.match(value)  # type: ignore\n    if match is None:\n        raise errors.TimeError()\n\n    kw = match.groupdict()\n    if kw['microsecond']:\n        kw['microsecond'] = kw['microsecond'].ljust(6, '0')\n\n    tzinfo = _parse_timezone(kw.pop('tzinfo'), errors.TimeError)\n    kw_: Dict[str, Union[None, int, timezone]] = {k: int(v) for k, v in kw.items() if v is not None}\n    kw_['tzinfo'] = tzinfo\n\n    try:\n        return time(**kw_)  # type: ignore\n    except ValueError:\n        raise errors.TimeError()\n\n\ndef parse_datetime(value: Union[datetime, StrBytesIntFloat]) -> datetime:\n    \"\"\"\n    Parse a datetime/int/float/string and return a datetime.datetime.\n\n    This function supports time zone offsets. When the input contains one,\n    the output uses a timezone with a fixed offset from UTC.\n\n    Raise ValueError if the input is well formatted but not a valid datetime.\n    Raise ValueError if the input isn't well formatted.\n    \"\"\"\n    if isinstance(value, datetime):\n        return value\n\n    number = get_numeric(value, 'datetime')\n    if number is not None:\n        return from_unix_seconds(number)\n\n    if isinstance(value, bytes):\n        value = value.decode()\n\n    match = datetime_re.match(value)  # type: ignore\n    if match is None:\n        raise errors.DateTimeError()\n\n    kw = match.groupdict()\n    if kw['microsecond']:\n        kw['microsecond'] = kw['microsecond'].ljust(6, '0')\n\n    tzinfo = _parse_timezone(kw.pop('tzinfo'), errors.DateTimeError)\n    kw_: Dict[str, Union[None, int, timezone]] = {k: int(v) for k, v in kw.items() if v is not None}\n    kw_['tzinfo'] = tzinfo\n\n    try:\n        return datetime(**kw_)  # type: ignore\n    except ValueError:\n        raise errors.DateTimeError()\n\n\ndef parse_duration(value: StrBytesIntFloat) -> timedelta:\n    \"\"\"\n    Parse a duration int/float/string and return a datetime.timedelta.\n\n    The preferred format for durations in Django is '%d %H:%M:%S.%f'.\n\n    Also supports ISO 8601 representation.\n    \"\"\"\n    if isinstance(value, timedelta):\n        return value\n\n    if isinstance(value, (int, float)):\n        # below code requires a string\n        value = f'{value:f}'\n    elif isinstance(value, bytes):\n        value = value.decode()\n\n    try:\n        match = standard_duration_re.match(value) or iso8601_duration_re.match(value)\n    except TypeError:\n        raise TypeError('invalid type; expected timedelta, string, bytes, int or float')\n\n    if not match:\n        raise errors.DurationError()\n\n    kw = match.groupdict()\n    sign = -1 if kw.pop('sign', '+') == '-' else 1\n    if kw.get('microseconds'):\n        kw['microseconds'] = kw['microseconds'].ljust(6, '0')\n\n    if kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-'):\n        kw['microseconds'] = '-' + kw['microseconds']\n\n    kw_ = {k: float(v) for k, v in kw.items() if v is not None}\n\n    return sign * timedelta(**kw_)\n", "pydantic/_internal/_core_metadata.py": "from __future__ import annotations as _annotations\n\nimport typing\nfrom typing import Any\n\nimport typing_extensions\n\nif typing.TYPE_CHECKING:\n    from ._schema_generation_shared import (\n        CoreSchemaOrField as CoreSchemaOrField,\n    )\n    from ._schema_generation_shared import (\n        GetJsonSchemaFunction,\n    )\n\n\nclass CoreMetadata(typing_extensions.TypedDict, total=False):\n    \"\"\"A `TypedDict` for holding the metadata dict of the schema.\n\n    Attributes:\n        pydantic_js_functions: List of JSON schema functions.\n        pydantic_js_prefer_positional_arguments: Whether JSON schema generator will\n            prefer positional over keyword arguments for an 'arguments' schema.\n    \"\"\"\n\n    pydantic_js_functions: list[GetJsonSchemaFunction]\n    pydantic_js_annotation_functions: list[GetJsonSchemaFunction]\n\n    # If `pydantic_js_prefer_positional_arguments` is True, the JSON schema generator will\n    # prefer positional over keyword arguments for an 'arguments' schema.\n    pydantic_js_prefer_positional_arguments: bool | None\n\n    pydantic_typed_dict_cls: type[Any] | None  # TODO: Consider moving this into the pydantic-core TypedDictSchema\n\n\nclass CoreMetadataHandler:\n    \"\"\"Because the metadata field in pydantic_core is of type `Any`, we can't assume much about its contents.\n\n    This class is used to interact with the metadata field on a CoreSchema object in a consistent\n    way throughout pydantic.\n    \"\"\"\n\n    __slots__ = ('_schema',)\n\n    def __init__(self, schema: CoreSchemaOrField):\n        self._schema = schema\n\n        metadata = schema.get('metadata')\n        if metadata is None:\n            schema['metadata'] = CoreMetadata()\n        elif not isinstance(metadata, dict):\n            raise TypeError(f'CoreSchema metadata should be a dict; got {metadata!r}.')\n\n    @property\n    def metadata(self) -> CoreMetadata:\n        \"\"\"Retrieves the metadata dict from the schema, initializing it to a dict if it is None\n        and raises an error if it is not a dict.\n        \"\"\"\n        metadata = self._schema.get('metadata')\n        if metadata is None:\n            self._schema['metadata'] = metadata = CoreMetadata()\n        if not isinstance(metadata, dict):\n            raise TypeError(f'CoreSchema metadata should be a dict; got {metadata!r}.')\n        return metadata\n\n\ndef build_metadata_dict(\n    *,  # force keyword arguments to make it easier to modify this signature in a backwards-compatible way\n    js_functions: list[GetJsonSchemaFunction] | None = None,\n    js_annotation_functions: list[GetJsonSchemaFunction] | None = None,\n    js_prefer_positional_arguments: bool | None = None,\n    typed_dict_cls: type[Any] | None = None,\n    initial_metadata: Any | None = None,\n) -> Any:\n    \"\"\"Builds a dict to use as the metadata field of a CoreSchema object in a manner that is consistent\n    with the CoreMetadataHandler class.\n    \"\"\"\n    if initial_metadata is not None and not isinstance(initial_metadata, dict):\n        raise TypeError(f'CoreSchema metadata should be a dict; got {initial_metadata!r}.')\n\n    metadata = CoreMetadata(\n        pydantic_js_functions=js_functions or [],\n        pydantic_js_annotation_functions=js_annotation_functions or [],\n        pydantic_js_prefer_positional_arguments=js_prefer_positional_arguments,\n        pydantic_typed_dict_cls=typed_dict_cls,\n    )\n    metadata = {k: v for k, v in metadata.items() if v is not None}\n\n    if initial_metadata is not None:\n        metadata = {**initial_metadata, **metadata}\n\n    return metadata\n", "pydantic/_internal/_git.py": "\"\"\"Git utilities, adopted from mypy's git utilities (https://github.com/python/mypy/blob/master/mypy/git.py).\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport subprocess\n\n\ndef is_git_repo(dir: str) -> bool:\n    \"\"\"Is the given directory version-controlled with git?\"\"\"\n    return os.path.exists(os.path.join(dir, '.git'))\n\n\ndef have_git() -> bool:\n    \"\"\"Can we run the git executable?\"\"\"\n    try:\n        subprocess.check_output(['git', '--help'])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n    except OSError:\n        return False\n\n\ndef git_revision(dir: str) -> str:\n    \"\"\"Get the SHA-1 of the HEAD of a git repository.\"\"\"\n    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], cwd=dir).decode('utf-8').strip()\n", "pydantic/_internal/_signature.py": "from __future__ import annotations\n\nimport dataclasses\nfrom inspect import Parameter, Signature, signature\nfrom typing import TYPE_CHECKING, Any, Callable\n\nfrom pydantic_core import PydanticUndefined\n\nfrom ._config import ConfigWrapper\nfrom ._utils import is_valid_identifier\n\nif TYPE_CHECKING:\n    from ..fields import FieldInfo\n\n\ndef _field_name_for_signature(field_name: str, field_info: FieldInfo) -> str:\n    \"\"\"Extract the correct name to use for the field when generating a signature.\n\n    Assuming the field has a valid alias, this will return the alias. Otherwise, it will return the field name.\n    First priority is given to the validation_alias, then the alias, then the field name.\n\n    Args:\n        field_name: The name of the field\n        field_info: The corresponding FieldInfo object.\n\n    Returns:\n        The correct name to use when generating a signature.\n    \"\"\"\n\n    def _alias_if_valid(x: Any) -> str | None:\n        \"\"\"Return the alias if it is a valid alias and identifier, else None.\"\"\"\n        return x if isinstance(x, str) and is_valid_identifier(x) else None\n\n    return _alias_if_valid(field_info.alias) or _alias_if_valid(field_info.validation_alias) or field_name\n\n\ndef _process_param_defaults(param: Parameter) -> Parameter:\n    \"\"\"Modify the signature for a parameter in a dataclass where the default value is a FieldInfo instance.\n\n    Args:\n        param (Parameter): The parameter\n\n    Returns:\n        Parameter: The custom processed parameter\n    \"\"\"\n    from ..fields import FieldInfo\n\n    param_default = param.default\n    if isinstance(param_default, FieldInfo):\n        annotation = param.annotation\n        # Replace the annotation if appropriate\n        # inspect does \"clever\" things to show annotations as strings because we have\n        # `from __future__ import annotations` in main, we don't want that\n        if annotation == 'Any':\n            annotation = Any\n\n        # Replace the field default\n        default = param_default.default\n        if default is PydanticUndefined:\n            if param_default.default_factory is PydanticUndefined:\n                default = Signature.empty\n            else:\n                # this is used by dataclasses to indicate a factory exists:\n                default = dataclasses._HAS_DEFAULT_FACTORY  # type: ignore\n        return param.replace(\n            annotation=annotation, name=_field_name_for_signature(param.name, param_default), default=default\n        )\n    return param\n\n\ndef _generate_signature_parameters(  # noqa: C901 (ignore complexity, could use a refactor)\n    init: Callable[..., None],\n    fields: dict[str, FieldInfo],\n    config_wrapper: ConfigWrapper,\n) -> dict[str, Parameter]:\n    \"\"\"Generate a mapping of parameter names to Parameter objects for a pydantic BaseModel or dataclass.\"\"\"\n    from itertools import islice\n\n    present_params = signature(init).parameters.values()\n    merged_params: dict[str, Parameter] = {}\n    var_kw = None\n    use_var_kw = False\n\n    for param in islice(present_params, 1, None):  # skip self arg\n        # inspect does \"clever\" things to show annotations as strings because we have\n        # `from __future__ import annotations` in main, we don't want that\n        if fields.get(param.name):\n            # exclude params with init=False\n            if getattr(fields[param.name], 'init', True) is False:\n                continue\n            param = param.replace(name=_field_name_for_signature(param.name, fields[param.name]))\n        if param.annotation == 'Any':\n            param = param.replace(annotation=Any)\n        if param.kind is param.VAR_KEYWORD:\n            var_kw = param\n            continue\n        merged_params[param.name] = param\n\n    if var_kw:  # if custom init has no var_kw, fields which are not declared in it cannot be passed through\n        allow_names = config_wrapper.populate_by_name\n        for field_name, field in fields.items():\n            # when alias is a str it should be used for signature generation\n            param_name = _field_name_for_signature(field_name, field)\n\n            if field_name in merged_params or param_name in merged_params:\n                continue\n\n            if not is_valid_identifier(param_name):\n                if allow_names:\n                    param_name = field_name\n                else:\n                    use_var_kw = True\n                    continue\n\n            kwargs = {} if field.is_required() else {'default': field.get_default(call_default_factory=False)}\n            merged_params[param_name] = Parameter(\n                param_name, Parameter.KEYWORD_ONLY, annotation=field.rebuild_annotation(), **kwargs\n            )\n\n    if config_wrapper.extra == 'allow':\n        use_var_kw = True\n\n    if var_kw and use_var_kw:\n        # Make sure the parameter for extra kwargs\n        # does not have the same name as a field\n        default_model_signature = [\n            ('self', Parameter.POSITIONAL_ONLY),\n            ('data', Parameter.VAR_KEYWORD),\n        ]\n        if [(p.name, p.kind) for p in present_params] == default_model_signature:\n            # if this is the standard model signature, use extra_data as the extra args name\n            var_kw_name = 'extra_data'\n        else:\n            # else start from var_kw\n            var_kw_name = var_kw.name\n\n        # generate a name that's definitely unique\n        while var_kw_name in fields:\n            var_kw_name += '_'\n        merged_params[var_kw_name] = var_kw.replace(name=var_kw_name)\n\n    return merged_params\n\n\ndef generate_pydantic_signature(\n    init: Callable[..., None], fields: dict[str, FieldInfo], config_wrapper: ConfigWrapper, is_dataclass: bool = False\n) -> Signature:\n    \"\"\"Generate signature for a pydantic BaseModel or dataclass.\n\n    Args:\n        init: The class init.\n        fields: The model fields.\n        config_wrapper: The config wrapper instance.\n        is_dataclass: Whether the model is a dataclass.\n\n    Returns:\n        The dataclass/BaseModel subclass signature.\n    \"\"\"\n    merged_params = _generate_signature_parameters(init, fields, config_wrapper)\n\n    if is_dataclass:\n        merged_params = {k: _process_param_defaults(v) for k, v in merged_params.items()}\n\n    return Signature(parameters=list(merged_params.values()), return_annotation=None)\n", "pydantic/_internal/_std_types_schema.py": "\"\"\"Logic for generating pydantic-core schemas for standard library types.\n\nImport of this module is deferred since it contains imports of many standard library modules.\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport collections\nimport collections.abc\nimport dataclasses\nimport decimal\nimport inspect\nimport os\nimport typing\nfrom enum import Enum\nfrom functools import partial\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom operator import attrgetter\nfrom typing import Any, Callable, Iterable, Literal, Tuple, TypeVar\n\nimport typing_extensions\nfrom pydantic_core import (\n    CoreSchema,\n    MultiHostUrl,\n    PydanticCustomError,\n    PydanticOmit,\n    Url,\n    core_schema,\n)\nfrom typing_extensions import get_args, get_origin\n\nfrom pydantic.errors import PydanticSchemaGenerationError\nfrom pydantic.fields import FieldInfo\nfrom pydantic.types import Strict\n\nfrom ..config import ConfigDict\nfrom ..json_schema import JsonSchemaValue\nfrom . import _known_annotated_metadata, _typing_extra, _validators\nfrom ._core_utils import get_type_ref\nfrom ._internal_dataclass import slots_true\nfrom ._schema_generation_shared import GetCoreSchemaHandler, GetJsonSchemaHandler\n\nif typing.TYPE_CHECKING:\n    from ._generate_schema import GenerateSchema\n\n    StdSchemaFunction = Callable[[GenerateSchema, type[Any]], core_schema.CoreSchema]\n\n\n@dataclasses.dataclass(**slots_true)\nclass SchemaTransformer:\n    get_core_schema: Callable[[Any, GetCoreSchemaHandler], CoreSchema]\n    get_json_schema: Callable[[CoreSchema, GetJsonSchemaHandler], JsonSchemaValue]\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n        return self.get_core_schema(source_type, handler)\n\n    def __get_pydantic_json_schema__(self, schema: CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n        return self.get_json_schema(schema, handler)\n\n\ndef get_enum_core_schema(enum_type: type[Enum], config: ConfigDict) -> CoreSchema:\n    cases: list[Any] = list(enum_type.__members__.values())\n\n    enum_ref = get_type_ref(enum_type)\n    description = None if not enum_type.__doc__ else inspect.cleandoc(enum_type.__doc__)\n    if description == 'An enumeration.':  # This is the default value provided by enum.EnumMeta.__new__; don't use it\n        description = None\n    js_updates = {'title': enum_type.__name__, 'description': description}\n    js_updates = {k: v for k, v in js_updates.items() if v is not None}\n\n    sub_type: Literal['str', 'int', 'float'] | None = None\n    if issubclass(enum_type, int):\n        sub_type = 'int'\n        value_ser_type: core_schema.SerSchema = core_schema.simple_ser_schema('int')\n    elif issubclass(enum_type, str):\n        # this handles `StrEnum` (3.11 only), and also `Foobar(str, Enum)`\n        sub_type = 'str'\n        value_ser_type = core_schema.simple_ser_schema('str')\n    elif issubclass(enum_type, float):\n        sub_type = 'float'\n        value_ser_type = core_schema.simple_ser_schema('float')\n    else:\n        # TODO this is an ugly hack, how do we trigger an Any schema for serialization?\n        value_ser_type = core_schema.plain_serializer_function_ser_schema(lambda x: x)\n\n    if cases:\n\n        def get_json_schema(schema: CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            json_schema = handler(schema)\n            original_schema = handler.resolve_ref_schema(json_schema)\n            original_schema.update(js_updates)\n            return json_schema\n\n        # we don't want to add the missing to the schema if it's the default one\n        default_missing = getattr(enum_type._missing_, '__func__', None) == Enum._missing_.__func__  # type: ignore\n        enum_schema = core_schema.enum_schema(\n            enum_type,\n            cases,\n            sub_type=sub_type,\n            missing=None if default_missing else enum_type._missing_,\n            ref=enum_ref,\n            metadata={'pydantic_js_functions': [get_json_schema]},\n        )\n\n        if config.get('use_enum_values', False):\n            enum_schema = core_schema.no_info_after_validator_function(\n                attrgetter('value'), enum_schema, serialization=value_ser_type\n            )\n\n        return enum_schema\n\n    else:\n\n        def get_json_schema_no_cases(_, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            json_schema = handler(core_schema.enum_schema(enum_type, cases, sub_type=sub_type, ref=enum_ref))\n            original_schema = handler.resolve_ref_schema(json_schema)\n            original_schema.update(js_updates)\n            return json_schema\n\n        # Use an isinstance check for enums with no cases.\n        # The most important use case for this is creating TypeVar bounds for generics that should\n        # be restricted to enums. This is more consistent than it might seem at first, since you can only\n        # subclass enum.Enum (or subclasses of enum.Enum) if all parent classes have no cases.\n        # We use the get_json_schema function when an Enum subclass has been declared with no cases\n        # so that we can still generate a valid json schema.\n        return core_schema.is_instance_schema(\n            enum_type,\n            metadata={'pydantic_js_functions': [get_json_schema_no_cases]},\n        )\n\n\n@dataclasses.dataclass(**slots_true)\nclass InnerSchemaValidator:\n    \"\"\"Use a fixed CoreSchema, avoiding interference from outward annotations.\"\"\"\n\n    core_schema: CoreSchema\n    js_schema: JsonSchemaValue | None = None\n    js_core_schema: CoreSchema | None = None\n    js_schema_update: JsonSchemaValue | None = None\n\n    def __get_pydantic_json_schema__(self, _schema: CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n        if self.js_schema is not None:\n            return self.js_schema\n        js_schema = handler(self.js_core_schema or self.core_schema)\n        if self.js_schema_update is not None:\n            js_schema.update(self.js_schema_update)\n        return js_schema\n\n    def __get_pydantic_core_schema__(self, _source_type: Any, _handler: GetCoreSchemaHandler) -> CoreSchema:\n        return self.core_schema\n\n\ndef decimal_prepare_pydantic_annotations(\n    source: Any, annotations: Iterable[Any], config: ConfigDict\n) -> tuple[Any, list[Any]] | None:\n    if source is not decimal.Decimal:\n        return None\n\n    metadata, remaining_annotations = _known_annotated_metadata.collect_known_metadata(annotations)\n\n    config_allow_inf_nan = config.get('allow_inf_nan')\n    if config_allow_inf_nan is not None:\n        metadata.setdefault('allow_inf_nan', config_allow_inf_nan)\n\n    _known_annotated_metadata.check_metadata(\n        metadata, {*_known_annotated_metadata.FLOAT_CONSTRAINTS, 'max_digits', 'decimal_places'}, decimal.Decimal\n    )\n    return source, [InnerSchemaValidator(core_schema.decimal_schema(**metadata)), *remaining_annotations]\n\n\ndef datetime_prepare_pydantic_annotations(\n    source_type: Any, annotations: Iterable[Any], _config: ConfigDict\n) -> tuple[Any, list[Any]] | None:\n    import datetime\n\n    metadata, remaining_annotations = _known_annotated_metadata.collect_known_metadata(annotations)\n    if source_type is datetime.date:\n        sv = InnerSchemaValidator(core_schema.date_schema(**metadata))\n    elif source_type is datetime.datetime:\n        sv = InnerSchemaValidator(core_schema.datetime_schema(**metadata))\n    elif source_type is datetime.time:\n        sv = InnerSchemaValidator(core_schema.time_schema(**metadata))\n    elif source_type is datetime.timedelta:\n        sv = InnerSchemaValidator(core_schema.timedelta_schema(**metadata))\n    else:\n        return None\n    # check now that we know the source type is correct\n    _known_annotated_metadata.check_metadata(metadata, _known_annotated_metadata.DATE_TIME_CONSTRAINTS, source_type)\n    return (source_type, [sv, *remaining_annotations])\n\n\ndef uuid_prepare_pydantic_annotations(\n    source_type: Any, annotations: Iterable[Any], _config: ConfigDict\n) -> tuple[Any, list[Any]] | None:\n    # UUIDs have no constraints - they are fixed length, constructing a UUID instance checks the length\n\n    from uuid import UUID\n\n    if source_type is not UUID:\n        return None\n\n    return (source_type, [InnerSchemaValidator(core_schema.uuid_schema()), *annotations])\n\n\ndef path_schema_prepare_pydantic_annotations(\n    source_type: Any, annotations: Iterable[Any], _config: ConfigDict\n) -> tuple[Any, list[Any]] | None:\n    import pathlib\n\n    if source_type not in {\n        os.PathLike,\n        pathlib.Path,\n        pathlib.PurePath,\n        pathlib.PosixPath,\n        pathlib.PurePosixPath,\n        pathlib.PureWindowsPath,\n    }:\n        return None\n\n    metadata, remaining_annotations = _known_annotated_metadata.collect_known_metadata(annotations)\n    _known_annotated_metadata.check_metadata(metadata, _known_annotated_metadata.STR_CONSTRAINTS, source_type)\n\n    construct_path = pathlib.PurePath if source_type is os.PathLike else source_type\n\n    def path_validator(input_value: str) -> os.PathLike[Any]:\n        try:\n            return construct_path(input_value)\n        except TypeError as e:\n            raise PydanticCustomError('path_type', 'Input is not a valid path') from e\n\n    constrained_str_schema = core_schema.str_schema(**metadata)\n\n    instance_schema = core_schema.json_or_python_schema(\n        json_schema=core_schema.no_info_after_validator_function(path_validator, constrained_str_schema),\n        python_schema=core_schema.is_instance_schema(source_type),\n    )\n\n    strict: bool | None = None\n    for annotation in annotations:\n        if isinstance(annotation, Strict):\n            strict = annotation.strict\n\n    schema = core_schema.lax_or_strict_schema(\n        lax_schema=core_schema.union_schema(\n            [\n                instance_schema,\n                core_schema.no_info_after_validator_function(path_validator, constrained_str_schema),\n            ],\n            custom_error_type='path_type',\n            custom_error_message='Input is not a valid path',\n            strict=True,\n        ),\n        strict_schema=instance_schema,\n        serialization=core_schema.to_string_ser_schema(),\n        strict=strict,\n    )\n\n    return (\n        source_type,\n        [\n            InnerSchemaValidator(schema, js_core_schema=constrained_str_schema, js_schema_update={'format': 'path'}),\n            *remaining_annotations,\n        ],\n    )\n\n\ndef dequeue_validator(\n    input_value: Any, handler: core_schema.ValidatorFunctionWrapHandler, maxlen: None | int\n) -> collections.deque[Any]:\n    if isinstance(input_value, collections.deque):\n        maxlens = [v for v in (input_value.maxlen, maxlen) if v is not None]\n        if maxlens:\n            maxlen = min(maxlens)\n        return collections.deque(handler(input_value), maxlen=maxlen)\n    else:\n        return collections.deque(handler(input_value), maxlen=maxlen)\n\n\ndef serialize_sequence_via_list(\n    v: Any, handler: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo\n) -> Any:\n    items: list[Any] = []\n\n    mapped_origin = SEQUENCE_ORIGIN_MAP.get(type(v), None)\n    if mapped_origin is None:\n        # we shouldn't hit this branch, should probably add a serialization error or something\n        return v\n\n    for index, item in enumerate(v):\n        try:\n            v = handler(item, index)\n        except PydanticOmit:\n            pass\n        else:\n            items.append(v)\n\n    if info.mode_is_json():\n        return items\n    else:\n        return mapped_origin(items)\n\n\n@dataclasses.dataclass(**slots_true)\nclass SequenceValidator:\n    mapped_origin: type[Any]\n    item_source_type: type[Any]\n    min_length: int | None = None\n    max_length: int | None = None\n    strict: bool | None = None\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n        if self.item_source_type is Any:\n            items_schema = None\n        else:\n            items_schema = handler.generate_schema(self.item_source_type)\n\n        metadata = {'min_length': self.min_length, 'max_length': self.max_length, 'strict': self.strict}\n\n        if self.mapped_origin in (list, set, frozenset):\n            if self.mapped_origin is list:\n                constrained_schema = core_schema.list_schema(items_schema, **metadata)\n            elif self.mapped_origin is set:\n                constrained_schema = core_schema.set_schema(items_schema, **metadata)\n            else:\n                assert self.mapped_origin is frozenset  # safety check in case we forget to add a case\n                constrained_schema = core_schema.frozenset_schema(items_schema, **metadata)\n\n            schema = constrained_schema\n        else:\n            # safety check in case we forget to add a case\n            assert self.mapped_origin in (collections.deque, collections.Counter)\n\n            if self.mapped_origin is collections.deque:\n                # if we have a MaxLen annotation might as well set that as the default maxlen on the deque\n                # this lets us re-use existing metadata annotations to let users set the maxlen on a dequeue\n                # that e.g. comes from JSON\n                coerce_instance_wrap = partial(\n                    core_schema.no_info_wrap_validator_function,\n                    partial(dequeue_validator, maxlen=metadata.get('max_length', None)),\n                )\n            else:\n                coerce_instance_wrap = partial(core_schema.no_info_after_validator_function, self.mapped_origin)\n\n            # we have to use a lax list schema here, because we need to validate the deque's\n            # items via a list schema, but it's ok if the deque itself is not a list (same for Counter)\n            metadata_with_strict_override = {**metadata, 'strict': False}\n            constrained_schema = core_schema.list_schema(items_schema, **metadata_with_strict_override)\n\n            check_instance = core_schema.json_or_python_schema(\n                json_schema=core_schema.list_schema(),\n                python_schema=core_schema.is_instance_schema(self.mapped_origin),\n            )\n\n            serialization = core_schema.wrap_serializer_function_ser_schema(\n                serialize_sequence_via_list, schema=items_schema or core_schema.any_schema(), info_arg=True\n            )\n\n            strict = core_schema.chain_schema([check_instance, coerce_instance_wrap(constrained_schema)])\n\n            if metadata.get('strict', False):\n                schema = strict\n            else:\n                lax = coerce_instance_wrap(constrained_schema)\n                schema = core_schema.lax_or_strict_schema(lax_schema=lax, strict_schema=strict)\n            schema['serialization'] = serialization\n\n        return schema\n\n\nSEQUENCE_ORIGIN_MAP: dict[Any, Any] = {\n    typing.Deque: collections.deque,\n    collections.deque: collections.deque,\n    list: list,\n    typing.List: list,\n    set: set,\n    typing.AbstractSet: set,\n    typing.Set: set,\n    frozenset: frozenset,\n    typing.FrozenSet: frozenset,\n    typing.Sequence: list,\n    typing.MutableSequence: list,\n    typing.MutableSet: set,\n    # this doesn't handle subclasses of these\n    # parametrized typing.Set creates one of these\n    collections.abc.MutableSet: set,\n    collections.abc.Set: frozenset,\n}\n\n\ndef identity(s: CoreSchema) -> CoreSchema:\n    return s\n\n\ndef sequence_like_prepare_pydantic_annotations(\n    source_type: Any, annotations: Iterable[Any], _config: ConfigDict\n) -> tuple[Any, list[Any]] | None:\n    origin: Any = get_origin(source_type)\n\n    mapped_origin = SEQUENCE_ORIGIN_MAP.get(origin, None) if origin else SEQUENCE_ORIGIN_MAP.get(source_type, None)\n    if mapped_origin is None:\n        return None\n\n    args = get_args(source_type)\n\n    if not args:\n        args = typing.cast(Tuple[Any], (Any,))\n    elif len(args) != 1:\n        raise ValueError('Expected sequence to have exactly 1 generic parameter')\n\n    item_source_type = args[0]\n\n    metadata, remaining_annotations = _known_annotated_metadata.collect_known_metadata(annotations)\n    _known_annotated_metadata.check_metadata(metadata, _known_annotated_metadata.SEQUENCE_CONSTRAINTS, source_type)\n\n    return (source_type, [SequenceValidator(mapped_origin, item_source_type, **metadata), *remaining_annotations])\n\n\nMAPPING_ORIGIN_MAP: dict[Any, Any] = {\n    typing.DefaultDict: collections.defaultdict,\n    collections.defaultdict: collections.defaultdict,\n    collections.OrderedDict: collections.OrderedDict,\n    typing_extensions.OrderedDict: collections.OrderedDict,\n    dict: dict,\n    typing.Dict: dict,\n    collections.Counter: collections.Counter,\n    typing.Counter: collections.Counter,\n    # this doesn't handle subclasses of these\n    typing.Mapping: dict,\n    typing.MutableMapping: dict,\n    # parametrized typing.{Mutable}Mapping creates one of these\n    collections.abc.MutableMapping: dict,\n    collections.abc.Mapping: dict,\n}\n\n\ndef defaultdict_validator(\n    input_value: Any, handler: core_schema.ValidatorFunctionWrapHandler, default_default_factory: Callable[[], Any]\n) -> collections.defaultdict[Any, Any]:\n    if isinstance(input_value, collections.defaultdict):\n        default_factory = input_value.default_factory\n        return collections.defaultdict(default_factory, handler(input_value))\n    else:\n        return collections.defaultdict(default_default_factory, handler(input_value))\n\n\ndef get_defaultdict_default_default_factory(values_source_type: Any) -> Callable[[], Any]:\n    def infer_default() -> Callable[[], Any]:\n        allowed_default_types: dict[Any, Any] = {\n            typing.Tuple: tuple,\n            tuple: tuple,\n            collections.abc.Sequence: tuple,\n            collections.abc.MutableSequence: list,\n            typing.List: list,\n            list: list,\n            typing.Sequence: list,\n            typing.Set: set,\n            set: set,\n            typing.MutableSet: set,\n            collections.abc.MutableSet: set,\n            collections.abc.Set: frozenset,\n            typing.MutableMapping: dict,\n            typing.Mapping: dict,\n            collections.abc.Mapping: dict,\n            collections.abc.MutableMapping: dict,\n            float: float,\n            int: int,\n            str: str,\n            bool: bool,\n        }\n        values_type_origin = get_origin(values_source_type) or values_source_type\n        instructions = 'set using `DefaultDict[..., Annotated[..., Field(default_factory=...)]]`'\n        if isinstance(values_type_origin, TypeVar):\n\n            def type_var_default_factory() -> None:\n                raise RuntimeError(\n                    'Generic defaultdict cannot be used without a concrete value type or an'\n                    ' explicit default factory, ' + instructions\n                )\n\n            return type_var_default_factory\n        elif values_type_origin not in allowed_default_types:\n            # a somewhat subjective set of types that have reasonable default values\n            allowed_msg = ', '.join([t.__name__ for t in set(allowed_default_types.values())])\n            raise PydanticSchemaGenerationError(\n                f'Unable to infer a default factory for keys of type {values_source_type}.'\n                f' Only {allowed_msg} are supported, other types require an explicit default factory'\n                ' ' + instructions\n            )\n        return allowed_default_types[values_type_origin]\n\n    # Assume Annotated[..., Field(...)]\n    if _typing_extra.is_annotated(values_source_type):\n        field_info = next((v for v in get_args(values_source_type) if isinstance(v, FieldInfo)), None)\n    else:\n        field_info = None\n    if field_info and field_info.default_factory:\n        default_default_factory = field_info.default_factory\n    else:\n        default_default_factory = infer_default()\n    return default_default_factory\n\n\n@dataclasses.dataclass(**slots_true)\nclass MappingValidator:\n    mapped_origin: type[Any]\n    keys_source_type: type[Any]\n    values_source_type: type[Any]\n    min_length: int | None = None\n    max_length: int | None = None\n    strict: bool = False\n\n    def serialize_mapping_via_dict(self, v: Any, handler: core_schema.SerializerFunctionWrapHandler) -> Any:\n        return handler(v)\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n        if self.keys_source_type is Any:\n            keys_schema = None\n        else:\n            keys_schema = handler.generate_schema(self.keys_source_type)\n        if self.values_source_type is Any:\n            values_schema = None\n        else:\n            values_schema = handler.generate_schema(self.values_source_type)\n\n        metadata = {'min_length': self.min_length, 'max_length': self.max_length, 'strict': self.strict}\n\n        if self.mapped_origin is dict:\n            schema = core_schema.dict_schema(keys_schema, values_schema, **metadata)\n        else:\n            constrained_schema = core_schema.dict_schema(keys_schema, values_schema, **metadata)\n            check_instance = core_schema.json_or_python_schema(\n                json_schema=core_schema.dict_schema(),\n                python_schema=core_schema.is_instance_schema(self.mapped_origin),\n            )\n\n            if self.mapped_origin is collections.defaultdict:\n                default_default_factory = get_defaultdict_default_default_factory(self.values_source_type)\n                coerce_instance_wrap = partial(\n                    core_schema.no_info_wrap_validator_function,\n                    partial(defaultdict_validator, default_default_factory=default_default_factory),\n                )\n            else:\n                coerce_instance_wrap = partial(core_schema.no_info_after_validator_function, self.mapped_origin)\n\n            serialization = core_schema.wrap_serializer_function_ser_schema(\n                self.serialize_mapping_via_dict,\n                schema=core_schema.dict_schema(\n                    keys_schema or core_schema.any_schema(), values_schema or core_schema.any_schema()\n                ),\n                info_arg=False,\n            )\n\n            strict = core_schema.chain_schema([check_instance, coerce_instance_wrap(constrained_schema)])\n\n            if metadata.get('strict', False):\n                schema = strict\n            else:\n                lax = coerce_instance_wrap(constrained_schema)\n                schema = core_schema.lax_or_strict_schema(lax_schema=lax, strict_schema=strict)\n                schema['serialization'] = serialization\n\n        return schema\n\n\ndef mapping_like_prepare_pydantic_annotations(\n    source_type: Any, annotations: Iterable[Any], _config: ConfigDict\n) -> tuple[Any, list[Any]] | None:\n    origin: Any = get_origin(source_type)\n\n    mapped_origin = MAPPING_ORIGIN_MAP.get(origin, None) if origin else MAPPING_ORIGIN_MAP.get(source_type, None)\n    if mapped_origin is None:\n        return None\n\n    args = get_args(source_type)\n\n    if not args:\n        args = typing.cast(Tuple[Any, Any], (Any, Any))\n    elif mapped_origin is collections.Counter:\n        # a single generic\n        if len(args) != 1:\n            raise ValueError('Expected Counter to have exactly 1 generic parameter')\n        args = (args[0], int)  # keys are always an int\n    elif len(args) != 2:\n        raise ValueError('Expected mapping to have exactly 2 generic parameters')\n\n    keys_source_type, values_source_type = args\n\n    metadata, remaining_annotations = _known_annotated_metadata.collect_known_metadata(annotations)\n    _known_annotated_metadata.check_metadata(metadata, _known_annotated_metadata.SEQUENCE_CONSTRAINTS, source_type)\n\n    return (\n        source_type,\n        [\n            MappingValidator(mapped_origin, keys_source_type, values_source_type, **metadata),\n            *remaining_annotations,\n        ],\n    )\n\n\ndef ip_prepare_pydantic_annotations(\n    source_type: Any, annotations: Iterable[Any], _config: ConfigDict\n) -> tuple[Any, list[Any]] | None:\n    def make_strict_ip_schema(tp: type[Any]) -> CoreSchema:\n        return core_schema.json_or_python_schema(\n            json_schema=core_schema.no_info_after_validator_function(tp, core_schema.str_schema()),\n            python_schema=core_schema.is_instance_schema(tp),\n        )\n\n    if source_type is IPv4Address:\n        return source_type, [\n            SchemaTransformer(\n                lambda _1, _2: core_schema.lax_or_strict_schema(\n                    lax_schema=core_schema.no_info_plain_validator_function(_validators.ip_v4_address_validator),\n                    strict_schema=make_strict_ip_schema(IPv4Address),\n                    serialization=core_schema.to_string_ser_schema(),\n                ),\n                lambda _1, _2: {'type': 'string', 'format': 'ipv4'},\n            ),\n            *annotations,\n        ]\n    if source_type is IPv4Network:\n        return source_type, [\n            SchemaTransformer(\n                lambda _1, _2: core_schema.lax_or_strict_schema(\n                    lax_schema=core_schema.no_info_plain_validator_function(_validators.ip_v4_network_validator),\n                    strict_schema=make_strict_ip_schema(IPv4Network),\n                    serialization=core_schema.to_string_ser_schema(),\n                ),\n                lambda _1, _2: {'type': 'string', 'format': 'ipv4network'},\n            ),\n            *annotations,\n        ]\n    if source_type is IPv4Interface:\n        return source_type, [\n            SchemaTransformer(\n                lambda _1, _2: core_schema.lax_or_strict_schema(\n                    lax_schema=core_schema.no_info_plain_validator_function(_validators.ip_v4_interface_validator),\n                    strict_schema=make_strict_ip_schema(IPv4Interface),\n                    serialization=core_schema.to_string_ser_schema(),\n                ),\n                lambda _1, _2: {'type': 'string', 'format': 'ipv4interface'},\n            ),\n            *annotations,\n        ]\n\n    if source_type is IPv6Address:\n        return source_type, [\n            SchemaTransformer(\n                lambda _1, _2: core_schema.lax_or_strict_schema(\n                    lax_schema=core_schema.no_info_plain_validator_function(_validators.ip_v6_address_validator),\n                    strict_schema=make_strict_ip_schema(IPv6Address),\n                    serialization=core_schema.to_string_ser_schema(),\n                ),\n                lambda _1, _2: {'type': 'string', 'format': 'ipv6'},\n            ),\n            *annotations,\n        ]\n    if source_type is IPv6Network:\n        return source_type, [\n            SchemaTransformer(\n                lambda _1, _2: core_schema.lax_or_strict_schema(\n                    lax_schema=core_schema.no_info_plain_validator_function(_validators.ip_v6_network_validator),\n                    strict_schema=make_strict_ip_schema(IPv6Network),\n                    serialization=core_schema.to_string_ser_schema(),\n                ),\n                lambda _1, _2: {'type': 'string', 'format': 'ipv6network'},\n            ),\n            *annotations,\n        ]\n    if source_type is IPv6Interface:\n        return source_type, [\n            SchemaTransformer(\n                lambda _1, _2: core_schema.lax_or_strict_schema(\n                    lax_schema=core_schema.no_info_plain_validator_function(_validators.ip_v6_interface_validator),\n                    strict_schema=make_strict_ip_schema(IPv6Interface),\n                    serialization=core_schema.to_string_ser_schema(),\n                ),\n                lambda _1, _2: {'type': 'string', 'format': 'ipv6interface'},\n            ),\n            *annotations,\n        ]\n\n    return None\n\n\ndef url_prepare_pydantic_annotations(\n    source_type: Any, annotations: Iterable[Any], _config: ConfigDict\n) -> tuple[Any, list[Any]] | None:\n    if source_type is Url:\n        return source_type, [\n            SchemaTransformer(\n                lambda _1, _2: core_schema.url_schema(),\n                lambda cs, handler: handler(cs),\n            ),\n            *annotations,\n        ]\n    if source_type is MultiHostUrl:\n        return source_type, [\n            SchemaTransformer(\n                lambda _1, _2: core_schema.multi_host_url_schema(),\n                lambda cs, handler: handler(cs),\n            ),\n            *annotations,\n        ]\n\n\nPREPARE_METHODS: tuple[Callable[[Any, Iterable[Any], ConfigDict], tuple[Any, list[Any]] | None], ...] = (\n    decimal_prepare_pydantic_annotations,\n    sequence_like_prepare_pydantic_annotations,\n    datetime_prepare_pydantic_annotations,\n    uuid_prepare_pydantic_annotations,\n    path_schema_prepare_pydantic_annotations,\n    mapping_like_prepare_pydantic_annotations,\n    ip_prepare_pydantic_annotations,\n    url_prepare_pydantic_annotations,\n)\n", "pydantic/_internal/_typing_extra.py": "\"\"\"Logic for interacting with type annotations, mostly extensions, shims and hacks to wrap python's typing module.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport re\nimport sys\nimport types\nimport typing\nimport warnings\nfrom collections.abc import Callable\nfrom functools import partial\nfrom types import GetSetDescriptorType\nfrom typing import TYPE_CHECKING, Any, Final\n\nfrom typing_extensions import Annotated, Literal, TypeAliasType, TypeGuard, deprecated, get_args, get_origin\n\nif TYPE_CHECKING:\n    from ._dataclasses import StandardDataclass\n\ntry:\n    from typing import _TypingBase  # type: ignore[attr-defined]\nexcept ImportError:\n    from typing import _Final as _TypingBase  # type: ignore[attr-defined]\n\ntyping_base = _TypingBase\n\n\nif sys.version_info < (3, 9):\n    # python < 3.9 does not have GenericAlias (list[int], tuple[str, ...] and so on)\n    TypingGenericAlias = ()\nelse:\n    from typing import GenericAlias as TypingGenericAlias  # type: ignore\n\n\nif sys.version_info < (3, 11):\n    from typing_extensions import NotRequired, Required\nelse:\n    from typing import NotRequired, Required  # noqa: F401\n\n\nif sys.version_info < (3, 10):\n\n    def origin_is_union(tp: type[Any] | None) -> bool:\n        return tp is typing.Union\n\n    WithArgsTypes = (TypingGenericAlias,)\n\nelse:\n\n    def origin_is_union(tp: type[Any] | None) -> bool:\n        return tp is typing.Union or tp is types.UnionType\n\n    WithArgsTypes = typing._GenericAlias, types.GenericAlias, types.UnionType  # type: ignore[attr-defined]\n\n\nif sys.version_info < (3, 10):\n    NoneType = type(None)\n    EllipsisType = type(Ellipsis)\nelse:\n    from types import NoneType as NoneType\n\n\nLITERAL_TYPES: set[Any] = {Literal}\nif hasattr(typing, 'Literal'):\n    LITERAL_TYPES.add(typing.Literal)  # type: ignore\n\n# Check if `deprecated` is a type to prevent errors when using typing_extensions < 4.9.0\nDEPRECATED_TYPES: tuple[Any, ...] = (deprecated,) if isinstance(deprecated, type) else ()\nif hasattr(warnings, 'deprecated'):\n    DEPRECATED_TYPES = (*DEPRECATED_TYPES, warnings.deprecated)  # type: ignore\n\nNONE_TYPES: tuple[Any, ...] = (None, NoneType, *(tp[None] for tp in LITERAL_TYPES))\n\n\nTypeVarType = Any  # since mypy doesn't allow the use of TypeVar as a type\n\n\ndef is_none_type(type_: Any) -> bool:\n    return type_ in NONE_TYPES\n\n\ndef is_callable_type(type_: type[Any]) -> bool:\n    return type_ is Callable or get_origin(type_) is Callable\n\n\ndef is_literal_type(type_: type[Any]) -> bool:\n    return Literal is not None and get_origin(type_) in LITERAL_TYPES\n\n\ndef is_deprecated_instance(instance: Any) -> TypeGuard[deprecated]:\n    return isinstance(instance, DEPRECATED_TYPES)\n\n\ndef literal_values(type_: type[Any]) -> tuple[Any, ...]:\n    return get_args(type_)\n\n\ndef all_literal_values(type_: type[Any]) -> list[Any]:\n    \"\"\"This method is used to retrieve all Literal values as\n    Literal can be used recursively (see https://www.python.org/dev/peps/pep-0586)\n    e.g. `Literal[Literal[Literal[1, 2, 3], \"foo\"], 5, None]`.\n    \"\"\"\n    if not is_literal_type(type_):\n        return [type_]\n\n    values = literal_values(type_)\n    return list(x for value in values for x in all_literal_values(value))\n\n\ndef is_annotated(ann_type: Any) -> bool:\n    from ._utils import lenient_issubclass\n\n    origin = get_origin(ann_type)\n    return origin is not None and lenient_issubclass(origin, Annotated)\n\n\ndef annotated_type(type_: Any) -> Any | None:\n    return get_args(type_)[0] if is_annotated(type_) else None\n\n\ndef is_namedtuple(type_: type[Any]) -> bool:\n    \"\"\"Check if a given class is a named tuple.\n    It can be either a `typing.NamedTuple` or `collections.namedtuple`.\n    \"\"\"\n    from ._utils import lenient_issubclass\n\n    return lenient_issubclass(type_, tuple) and hasattr(type_, '_fields')\n\n\ntest_new_type = typing.NewType('test_new_type', str)\n\n\ndef is_new_type(type_: type[Any]) -> bool:\n    \"\"\"Check whether type_ was created using typing.NewType.\n\n    Can't use isinstance because it fails <3.10.\n    \"\"\"\n    return isinstance(type_, test_new_type.__class__) and hasattr(type_, '__supertype__')  # type: ignore[arg-type]\n\n\ndef _check_classvar(v: type[Any] | None) -> bool:\n    if v is None:\n        return False\n\n    return v.__class__ == typing.ClassVar.__class__ and getattr(v, '_name', None) == 'ClassVar'\n\n\ndef is_classvar(ann_type: type[Any]) -> bool:\n    if _check_classvar(ann_type) or _check_classvar(get_origin(ann_type)):\n        return True\n\n    # this is an ugly workaround for class vars that contain forward references and are therefore themselves\n    # forward references, see #3679\n    if ann_type.__class__ == typing.ForwardRef and re.match(\n        r'(\\w+\\.)?ClassVar\\[',\n        ann_type.__forward_arg__,  # type: ignore\n    ):\n        return True\n\n    return False\n\n\ndef _check_finalvar(v: type[Any] | None) -> bool:\n    \"\"\"Check if a given type is a `typing.Final` type.\"\"\"\n    if v is None:\n        return False\n\n    return v.__class__ == Final.__class__ and (sys.version_info < (3, 8) or getattr(v, '_name', None) == 'Final')\n\n\ndef is_finalvar(ann_type: Any) -> bool:\n    return _check_finalvar(ann_type) or _check_finalvar(get_origin(ann_type))\n\n\ndef parent_frame_namespace(*, parent_depth: int = 2) -> dict[str, Any] | None:\n    \"\"\"We allow use of items in parent namespace to get around the issue with `get_type_hints` only looking in the\n    global module namespace. See https://github.com/pydantic/pydantic/issues/2678#issuecomment-1008139014 -> Scope\n    and suggestion at the end of the next comment by @gvanrossum.\n\n    WARNING 1: it matters exactly where this is called. By default, this function will build a namespace from the\n    parent of where it is called.\n\n    WARNING 2: this only looks in the parent namespace, not other parents since (AFAIK) there's no way to collect a\n    dict of exactly what's in scope. Using `f_back` would work sometimes but would be very wrong and confusing in many\n    other cases. See https://discuss.python.org/t/is-there-a-way-to-access-parent-nested-namespaces/20659.\n    \"\"\"\n    frame = sys._getframe(parent_depth)\n    # if f_back is None, it's the global module namespace and we don't need to include it here\n    if frame.f_back is None:\n        return None\n    else:\n        return frame.f_locals\n\n\ndef add_module_globals(obj: Any, globalns: dict[str, Any] | None = None) -> dict[str, Any]:\n    module_name = getattr(obj, '__module__', None)\n    if module_name:\n        try:\n            module_globalns = sys.modules[module_name].__dict__\n        except KeyError:\n            # happens occasionally, see https://github.com/pydantic/pydantic/issues/2363\n            pass\n        else:\n            if globalns:\n                return {**module_globalns, **globalns}\n            else:\n                # copy module globals to make sure it can't be updated later\n                return module_globalns.copy()\n\n    return globalns or {}\n\n\ndef get_cls_types_namespace(cls: type[Any], parent_namespace: dict[str, Any] | None = None) -> dict[str, Any]:\n    ns = add_module_globals(cls, parent_namespace)\n    ns[cls.__name__] = cls\n    return ns\n\n\ndef get_cls_type_hints_lenient(obj: Any, globalns: dict[str, Any] | None = None) -> dict[str, Any]:\n    \"\"\"Collect annotations from a class, including those from parent classes.\n\n    Unlike `typing.get_type_hints`, this function will not error if a forward reference is not resolvable.\n    \"\"\"\n    hints = {}\n    for base in reversed(obj.__mro__):\n        ann = base.__dict__.get('__annotations__')\n        localns = dict(vars(base))\n        if ann is not None and ann is not GetSetDescriptorType:\n            for name, value in ann.items():\n                hints[name] = eval_type_lenient(value, globalns, localns)\n    return hints\n\n\ndef eval_type_lenient(value: Any, globalns: dict[str, Any] | None = None, localns: dict[str, Any] | None = None) -> Any:\n    \"\"\"Behaves like typing._eval_type, except it won't raise an error if a forward reference can't be resolved.\"\"\"\n    if value is None:\n        value = NoneType\n    elif isinstance(value, str):\n        value = _make_forward_ref(value, is_argument=False, is_class=True)\n\n    try:\n        return eval_type_backport(value, globalns, localns)\n    except NameError:\n        # the point of this function is to be tolerant to this case\n        return value\n\n\ndef eval_type_backport(\n    value: Any, globalns: dict[str, Any] | None = None, localns: dict[str, Any] | None = None\n) -> Any:\n    \"\"\"Like `typing._eval_type`, but falls back to the `eval_type_backport` package if it's\n    installed to let older Python versions use newer typing features.\n    Specifically, this transforms `X | Y` into `typing.Union[X, Y]`\n    and `list[X]` into `typing.List[X]` etc. (for all the types made generic in PEP 585)\n    if the original syntax is not supported in the current Python version.\n    \"\"\"\n    try:\n        return typing._eval_type(  # type: ignore\n            value, globalns, localns\n        )\n    except TypeError as e:\n        if not (isinstance(value, typing.ForwardRef) and is_backport_fixable_error(e)):\n            raise\n        try:\n            from eval_type_backport import eval_type_backport\n        except ImportError:\n            raise TypeError(\n                f'You have a type annotation {value.__forward_arg__!r} '\n                f'which makes use of newer typing features than are supported in your version of Python. '\n                f'To handle this error, you should either remove the use of new syntax '\n                f'or install the `eval_type_backport` package.'\n            ) from e\n\n        return eval_type_backport(value, globalns, localns, try_default=False)\n\n\ndef is_backport_fixable_error(e: TypeError) -> bool:\n    msg = str(e)\n    return msg.startswith('unsupported operand type(s) for |: ') or \"' object is not subscriptable\" in msg\n\n\ndef get_function_type_hints(\n    function: Callable[..., Any], *, include_keys: set[str] | None = None, types_namespace: dict[str, Any] | None = None\n) -> dict[str, Any]:\n    \"\"\"Like `typing.get_type_hints`, but doesn't convert `X` to `Optional[X]` if the default value is `None`, also\n    copes with `partial`.\n    \"\"\"\n    try:\n        if isinstance(function, partial):\n            annotations = function.func.__annotations__\n        else:\n            annotations = function.__annotations__\n    except AttributeError:\n        type_hints = get_type_hints(function)\n        if isinstance(function, type):\n            # `type[...]` is a callable, which returns an instance of itself.\n            # At some point, we might even look into the return type of `__new__`\n            # if it returns something else.\n            type_hints.setdefault('return', function)\n        return type_hints\n\n    globalns = add_module_globals(function)\n    type_hints = {}\n    for name, value in annotations.items():\n        if include_keys is not None and name not in include_keys:\n            continue\n        if value is None:\n            value = NoneType\n        elif isinstance(value, str):\n            value = _make_forward_ref(value)\n\n        type_hints[name] = eval_type_backport(value, globalns, types_namespace)\n\n    return type_hints\n\n\nif sys.version_info < (3, 9, 8) or (3, 10) <= sys.version_info < (3, 10, 1):\n\n    def _make_forward_ref(\n        arg: Any,\n        is_argument: bool = True,\n        *,\n        is_class: bool = False,\n    ) -> typing.ForwardRef:\n        \"\"\"Wrapper for ForwardRef that accounts for the `is_class` argument missing in older versions.\n        The `module` argument is omitted as it breaks <3.9.8, =3.10.0 and isn't used in the calls below.\n\n        See https://github.com/python/cpython/pull/28560 for some background.\n        The backport happened on 3.9.8, see:\n        https://github.com/pydantic/pydantic/discussions/6244#discussioncomment-6275458,\n        and on 3.10.1 for the 3.10 branch, see:\n        https://github.com/pydantic/pydantic/issues/6912\n\n        Implemented as EAFP with memory.\n        \"\"\"\n        return typing.ForwardRef(arg, is_argument)\n\nelse:\n    _make_forward_ref = typing.ForwardRef\n\n\nif sys.version_info >= (3, 10):\n    get_type_hints = typing.get_type_hints\n\nelse:\n    \"\"\"\n    For older versions of python, we have a custom implementation of `get_type_hints` which is a close as possible to\n    the implementation in CPython 3.10.8.\n    \"\"\"\n\n    @typing.no_type_check\n    def get_type_hints(  # noqa: C901\n        obj: Any,\n        globalns: dict[str, Any] | None = None,\n        localns: dict[str, Any] | None = None,\n        include_extras: bool = False,\n    ) -> dict[str, Any]:  # pragma: no cover\n        \"\"\"Taken verbatim from python 3.10.8 unchanged, except:\n        * type annotations of the function definition above.\n        * prefixing `typing.` where appropriate\n        * Use `_make_forward_ref` instead of `typing.ForwardRef` to handle the `is_class` argument.\n\n        https://github.com/python/cpython/blob/aaaf5174241496afca7ce4d4584570190ff972fe/Lib/typing.py#L1773-L1875\n\n        DO NOT CHANGE THIS METHOD UNLESS ABSOLUTELY NECESSARY.\n        ======================================================\n\n        Return type hints for an object.\n\n        This is often the same as obj.__annotations__, but it handles\n        forward references encoded as string literals, adds Optional[t] if a\n        default value equal to None is set and recursively replaces all\n        'Annotated[T, ...]' with 'T' (unless 'include_extras=True').\n\n        The argument may be a module, class, method, or function. The annotations\n        are returned as a dictionary. For classes, annotations include also\n        inherited members.\n\n        TypeError is raised if the argument is not of a type that can contain\n        annotations, and an empty dictionary is returned if no annotations are\n        present.\n\n        BEWARE -- the behavior of globalns and localns is counterintuitive\n        (unless you are familiar with how eval() and exec() work).  The\n        search order is locals first, then globals.\n\n        - If no dict arguments are passed, an attempt is made to use the\n          globals from obj (or the respective module's globals for classes),\n          and these are also used as the locals.  If the object does not appear\n          to have globals, an empty dictionary is used.  For classes, the search\n          order is globals first then locals.\n\n        - If one dict argument is passed, it is used for both globals and\n          locals.\n\n        - If two dict arguments are passed, they specify globals and\n          locals, respectively.\n        \"\"\"\n        if getattr(obj, '__no_type_check__', None):\n            return {}\n        # Classes require a special treatment.\n        if isinstance(obj, type):\n            hints = {}\n            for base in reversed(obj.__mro__):\n                if globalns is None:\n                    base_globals = getattr(sys.modules.get(base.__module__, None), '__dict__', {})\n                else:\n                    base_globals = globalns\n                ann = base.__dict__.get('__annotations__', {})\n                if isinstance(ann, types.GetSetDescriptorType):\n                    ann = {}\n                base_locals = dict(vars(base)) if localns is None else localns\n                if localns is None and globalns is None:\n                    # This is surprising, but required.  Before Python 3.10,\n                    # get_type_hints only evaluated the globalns of\n                    # a class.  To maintain backwards compatibility, we reverse\n                    # the globalns and localns order so that eval() looks into\n                    # *base_globals* first rather than *base_locals*.\n                    # This only affects ForwardRefs.\n                    base_globals, base_locals = base_locals, base_globals\n                for name, value in ann.items():\n                    if value is None:\n                        value = type(None)\n                    if isinstance(value, str):\n                        value = _make_forward_ref(value, is_argument=False, is_class=True)\n\n                    value = eval_type_backport(value, base_globals, base_locals)\n                    hints[name] = value\n            if not include_extras and hasattr(typing, '_strip_annotations'):\n                return {\n                    k: typing._strip_annotations(t)  # type: ignore\n                    for k, t in hints.items()\n                }\n            else:\n                return hints\n\n        if globalns is None:\n            if isinstance(obj, types.ModuleType):\n                globalns = obj.__dict__\n            else:\n                nsobj = obj\n                # Find globalns for the unwrapped object.\n                while hasattr(nsobj, '__wrapped__'):\n                    nsobj = nsobj.__wrapped__\n                globalns = getattr(nsobj, '__globals__', {})\n            if localns is None:\n                localns = globalns\n        elif localns is None:\n            localns = globalns\n        hints = getattr(obj, '__annotations__', None)\n        if hints is None:\n            # Return empty annotations for something that _could_ have them.\n            if isinstance(obj, typing._allowed_types):  # type: ignore\n                return {}\n            else:\n                raise TypeError(f'{obj!r} is not a module, class, method, ' 'or function.')\n        defaults = typing._get_defaults(obj)  # type: ignore\n        hints = dict(hints)\n        for name, value in hints.items():\n            if value is None:\n                value = type(None)\n            if isinstance(value, str):\n                # class-level forward refs were handled above, this must be either\n                # a module-level annotation or a function argument annotation\n\n                value = _make_forward_ref(\n                    value,\n                    is_argument=not isinstance(obj, types.ModuleType),\n                    is_class=False,\n                )\n            value = eval_type_backport(value, globalns, localns)\n            if name in defaults and defaults[name] is None:\n                value = typing.Optional[value]\n            hints[name] = value\n        return hints if include_extras else {k: typing._strip_annotations(t) for k, t in hints.items()}  # type: ignore\n\n\ndef is_dataclass(_cls: type[Any]) -> TypeGuard[type[StandardDataclass]]:\n    # The dataclasses.is_dataclass function doesn't seem to provide TypeGuard functionality,\n    # so I created this convenience function\n    return dataclasses.is_dataclass(_cls)\n\n\ndef origin_is_type_alias_type(origin: Any) -> TypeGuard[TypeAliasType]:\n    return isinstance(origin, TypeAliasType)\n\n\nif sys.version_info >= (3, 10):\n\n    def is_generic_alias(type_: type[Any]) -> bool:\n        return isinstance(type_, (types.GenericAlias, typing._GenericAlias))  # type: ignore[attr-defined]\n\nelse:\n\n    def is_generic_alias(type_: type[Any]) -> bool:\n        return isinstance(type_, typing._GenericAlias)  # type: ignore\n\n\ndef is_self_type(tp: Any) -> bool:\n    \"\"\"Check if a given class is a Self type (from `typing` or `typing_extensions`)\"\"\"\n    return isinstance(tp, typing_base) and getattr(tp, '_name', None) == 'Self'\n", "pydantic/_internal/_schema_generation_shared.py": "\"\"\"Types and utility functions used by various other internal tools.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Callable\n\nfrom pydantic_core import core_schema\nfrom typing_extensions import Literal\n\nfrom ..annotated_handlers import GetCoreSchemaHandler, GetJsonSchemaHandler\n\nif TYPE_CHECKING:\n    from ..json_schema import GenerateJsonSchema, JsonSchemaValue\n    from ._core_utils import CoreSchemaOrField\n    from ._generate_schema import GenerateSchema\n\n    GetJsonSchemaFunction = Callable[[CoreSchemaOrField, GetJsonSchemaHandler], JsonSchemaValue]\n    HandlerOverride = Callable[[CoreSchemaOrField], JsonSchemaValue]\n\n\nclass GenerateJsonSchemaHandler(GetJsonSchemaHandler):\n    \"\"\"JsonSchemaHandler implementation that doesn't do ref unwrapping by default.\n\n    This is used for any Annotated metadata so that we don't end up with conflicting\n    modifications to the definition schema.\n\n    Used internally by Pydantic, please do not rely on this implementation.\n    See `GetJsonSchemaHandler` for the handler API.\n    \"\"\"\n\n    def __init__(self, generate_json_schema: GenerateJsonSchema, handler_override: HandlerOverride | None) -> None:\n        self.generate_json_schema = generate_json_schema\n        self.handler = handler_override or generate_json_schema.generate_inner\n        self.mode = generate_json_schema.mode\n\n    def __call__(self, core_schema: CoreSchemaOrField, /) -> JsonSchemaValue:\n        return self.handler(core_schema)\n\n    def resolve_ref_schema(self, maybe_ref_json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        \"\"\"Resolves `$ref` in the json schema.\n\n        This returns the input json schema if there is no `$ref` in json schema.\n\n        Args:\n            maybe_ref_json_schema: The input json schema that may contains `$ref`.\n\n        Returns:\n            Resolved json schema.\n\n        Raises:\n            LookupError: If it can't find the definition for `$ref`.\n        \"\"\"\n        if '$ref' not in maybe_ref_json_schema:\n            return maybe_ref_json_schema\n        ref = maybe_ref_json_schema['$ref']\n        json_schema = self.generate_json_schema.get_schema_from_definitions(ref)\n        if json_schema is None:\n            raise LookupError(\n                f'Could not find a ref for {ref}.'\n                ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n            )\n        return json_schema\n\n\nclass CallbackGetCoreSchemaHandler(GetCoreSchemaHandler):\n    \"\"\"Wrapper to use an arbitrary function as a `GetCoreSchemaHandler`.\n\n    Used internally by Pydantic, please do not rely on this implementation.\n    See `GetCoreSchemaHandler` for the handler API.\n    \"\"\"\n\n    def __init__(\n        self,\n        handler: Callable[[Any], core_schema.CoreSchema],\n        generate_schema: GenerateSchema,\n        ref_mode: Literal['to-def', 'unpack'] = 'to-def',\n    ) -> None:\n        self._handler = handler\n        self._generate_schema = generate_schema\n        self._ref_mode = ref_mode\n\n    def __call__(self, source_type: Any, /) -> core_schema.CoreSchema:\n        schema = self._handler(source_type)\n        ref = schema.get('ref')\n        if self._ref_mode == 'to-def':\n            if ref is not None:\n                self._generate_schema.defs.definitions[ref] = schema\n                return core_schema.definition_reference_schema(ref)\n            return schema\n        else:  # ref_mode = 'unpack\n            return self.resolve_ref_schema(schema)\n\n    def _get_types_namespace(self) -> dict[str, Any] | None:\n        return self._generate_schema._types_namespace\n\n    def generate_schema(self, source_type: Any, /) -> core_schema.CoreSchema:\n        return self._generate_schema.generate_schema(source_type)\n\n    @property\n    def field_name(self) -> str | None:\n        return self._generate_schema.field_name_stack.get()\n\n    def resolve_ref_schema(self, maybe_ref_schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        \"\"\"Resolves reference in the core schema.\n\n        Args:\n            maybe_ref_schema: The input core schema that may contains reference.\n\n        Returns:\n            Resolved core schema.\n\n        Raises:\n            LookupError: If it can't find the definition for reference.\n        \"\"\"\n        if maybe_ref_schema['type'] == 'definition-ref':\n            ref = maybe_ref_schema['schema_ref']\n            if ref not in self._generate_schema.defs.definitions:\n                raise LookupError(\n                    f'Could not find a ref for {ref}.'\n                    ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n                )\n            return self._generate_schema.defs.definitions[ref]\n        elif maybe_ref_schema['type'] == 'definitions':\n            return self.resolve_ref_schema(maybe_ref_schema['schema'])\n        return maybe_ref_schema\n", "pydantic/_internal/_validators.py": "\"\"\"Validator functions for standard library types.\n\nImport of this module is deferred since it contains imports of many standard library modules.\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport math\nimport re\nimport typing\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom typing import Any\n\nfrom pydantic_core import PydanticCustomError, core_schema\nfrom pydantic_core._pydantic_core import PydanticKnownError\n\n\ndef sequence_validator(\n    input_value: typing.Sequence[Any],\n    /,\n    validator: core_schema.ValidatorFunctionWrapHandler,\n) -> typing.Sequence[Any]:\n    \"\"\"Validator for `Sequence` types, isinstance(v, Sequence) has already been called.\"\"\"\n    value_type = type(input_value)\n\n    # We don't accept any plain string as a sequence\n    # Relevant issue: https://github.com/pydantic/pydantic/issues/5595\n    if issubclass(value_type, (str, bytes)):\n        raise PydanticCustomError(\n            'sequence_str',\n            \"'{type_name}' instances are not allowed as a Sequence value\",\n            {'type_name': value_type.__name__},\n        )\n\n    # TODO: refactor sequence validation to validate with either a list or a tuple\n    # schema, depending on the type of the value.\n    # Additionally, we should be able to remove one of either this validator or the\n    # SequenceValidator in _std_types_schema.py (preferably this one, while porting over some logic).\n    # Effectively, a refactor for sequence validation is needed.\n    if value_type == tuple:\n        input_value = list(input_value)\n\n    v_list = validator(input_value)\n\n    # the rest of the logic is just re-creating the original type from `v_list`\n    if value_type == list:\n        return v_list\n    elif issubclass(value_type, range):\n        # return the list as we probably can't re-create the range\n        return v_list\n    elif value_type == tuple:\n        return tuple(v_list)\n    else:\n        # best guess at how to re-create the original type, more custom construction logic might be required\n        return value_type(v_list)  # type: ignore[call-arg]\n\n\ndef import_string(value: Any) -> Any:\n    if isinstance(value, str):\n        try:\n            return _import_string_logic(value)\n        except ImportError as e:\n            raise PydanticCustomError('import_error', 'Invalid python path: {error}', {'error': str(e)}) from e\n    else:\n        # otherwise we just return the value and let the next validator do the rest of the work\n        return value\n\n\ndef _import_string_logic(dotted_path: str) -> Any:\n    \"\"\"Inspired by uvicorn \u2014 dotted paths should include a colon before the final item if that item is not a module.\n    (This is necessary to distinguish between a submodule and an attribute when there is a conflict.).\n\n    If the dotted path does not include a colon and the final item is not a valid module, importing as an attribute\n    rather than a submodule will be attempted automatically.\n\n    So, for example, the following values of `dotted_path` result in the following returned values:\n    * 'collections': <module 'collections'>\n    * 'collections.abc': <module 'collections.abc'>\n    * 'collections.abc:Mapping': <class 'collections.abc.Mapping'>\n    * `collections.abc.Mapping`: <class 'collections.abc.Mapping'> (though this is a bit slower than the previous line)\n\n    An error will be raised under any of the following scenarios:\n    * `dotted_path` contains more than one colon (e.g., 'collections:abc:Mapping')\n    * the substring of `dotted_path` before the colon is not a valid module in the environment (e.g., '123:Mapping')\n    * the substring of `dotted_path` after the colon is not an attribute of the module (e.g., 'collections:abc123')\n    \"\"\"\n    from importlib import import_module\n\n    components = dotted_path.strip().split(':')\n    if len(components) > 2:\n        raise ImportError(f\"Import strings should have at most one ':'; received {dotted_path!r}\")\n\n    module_path = components[0]\n    if not module_path:\n        raise ImportError(f'Import strings should have a nonempty module name; received {dotted_path!r}')\n\n    try:\n        module = import_module(module_path)\n    except ModuleNotFoundError as e:\n        if '.' in module_path:\n            # Check if it would be valid if the final item was separated from its module with a `:`\n            maybe_module_path, maybe_attribute = dotted_path.strip().rsplit('.', 1)\n            try:\n                return _import_string_logic(f'{maybe_module_path}:{maybe_attribute}')\n            except ImportError:\n                pass\n            raise ImportError(f'No module named {module_path!r}') from e\n        raise e\n\n    if len(components) > 1:\n        attribute = components[1]\n        try:\n            return getattr(module, attribute)\n        except AttributeError as e:\n            raise ImportError(f'cannot import name {attribute!r} from {module_path!r}') from e\n    else:\n        return module\n\n\ndef pattern_either_validator(input_value: Any, /) -> typing.Pattern[Any]:\n    if isinstance(input_value, typing.Pattern):\n        return input_value\n    elif isinstance(input_value, (str, bytes)):\n        # todo strict mode\n        return compile_pattern(input_value)  # type: ignore\n    else:\n        raise PydanticCustomError('pattern_type', 'Input should be a valid pattern')\n\n\ndef pattern_str_validator(input_value: Any, /) -> typing.Pattern[str]:\n    if isinstance(input_value, typing.Pattern):\n        if isinstance(input_value.pattern, str):\n            return input_value\n        else:\n            raise PydanticCustomError('pattern_str_type', 'Input should be a string pattern')\n    elif isinstance(input_value, str):\n        return compile_pattern(input_value)\n    elif isinstance(input_value, bytes):\n        raise PydanticCustomError('pattern_str_type', 'Input should be a string pattern')\n    else:\n        raise PydanticCustomError('pattern_type', 'Input should be a valid pattern')\n\n\ndef pattern_bytes_validator(input_value: Any, /) -> typing.Pattern[bytes]:\n    if isinstance(input_value, typing.Pattern):\n        if isinstance(input_value.pattern, bytes):\n            return input_value\n        else:\n            raise PydanticCustomError('pattern_bytes_type', 'Input should be a bytes pattern')\n    elif isinstance(input_value, bytes):\n        return compile_pattern(input_value)\n    elif isinstance(input_value, str):\n        raise PydanticCustomError('pattern_bytes_type', 'Input should be a bytes pattern')\n    else:\n        raise PydanticCustomError('pattern_type', 'Input should be a valid pattern')\n\n\nPatternType = typing.TypeVar('PatternType', str, bytes)\n\n\ndef compile_pattern(pattern: PatternType) -> typing.Pattern[PatternType]:\n    try:\n        return re.compile(pattern)\n    except re.error:\n        raise PydanticCustomError('pattern_regex', 'Input should be a valid regular expression')\n\n\ndef ip_v4_address_validator(input_value: Any, /) -> IPv4Address:\n    if isinstance(input_value, IPv4Address):\n        return input_value\n\n    try:\n        return IPv4Address(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v4_address', 'Input is not a valid IPv4 address')\n\n\ndef ip_v6_address_validator(input_value: Any, /) -> IPv6Address:\n    if isinstance(input_value, IPv6Address):\n        return input_value\n\n    try:\n        return IPv6Address(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v6_address', 'Input is not a valid IPv6 address')\n\n\ndef ip_v4_network_validator(input_value: Any, /) -> IPv4Network:\n    \"\"\"Assume IPv4Network initialised with a default `strict` argument.\n\n    See more:\n    https://docs.python.org/library/ipaddress.html#ipaddress.IPv4Network\n    \"\"\"\n    if isinstance(input_value, IPv4Network):\n        return input_value\n\n    try:\n        return IPv4Network(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v4_network', 'Input is not a valid IPv4 network')\n\n\ndef ip_v6_network_validator(input_value: Any, /) -> IPv6Network:\n    \"\"\"Assume IPv6Network initialised with a default `strict` argument.\n\n    See more:\n    https://docs.python.org/library/ipaddress.html#ipaddress.IPv6Network\n    \"\"\"\n    if isinstance(input_value, IPv6Network):\n        return input_value\n\n    try:\n        return IPv6Network(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v6_network', 'Input is not a valid IPv6 network')\n\n\ndef ip_v4_interface_validator(input_value: Any, /) -> IPv4Interface:\n    if isinstance(input_value, IPv4Interface):\n        return input_value\n\n    try:\n        return IPv4Interface(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v4_interface', 'Input is not a valid IPv4 interface')\n\n\ndef ip_v6_interface_validator(input_value: Any, /) -> IPv6Interface:\n    if isinstance(input_value, IPv6Interface):\n        return input_value\n\n    try:\n        return IPv6Interface(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v6_interface', 'Input is not a valid IPv6 interface')\n\n\ndef greater_than_validator(x: Any, gt: Any) -> Any:\n    if not (x > gt):\n        raise PydanticKnownError('greater_than', {'gt': gt})\n    return x\n\n\ndef greater_than_or_equal_validator(x: Any, ge: Any) -> Any:\n    if not (x >= ge):\n        raise PydanticKnownError('greater_than_equal', {'ge': ge})\n    return x\n\n\ndef less_than_validator(x: Any, lt: Any) -> Any:\n    if not (x < lt):\n        raise PydanticKnownError('less_than', {'lt': lt})\n    return x\n\n\ndef less_than_or_equal_validator(x: Any, le: Any) -> Any:\n    if not (x <= le):\n        raise PydanticKnownError('less_than_equal', {'le': le})\n    return x\n\n\ndef multiple_of_validator(x: Any, multiple_of: Any) -> Any:\n    if not (x % multiple_of == 0):\n        raise PydanticKnownError('multiple_of', {'multiple_of': multiple_of})\n    return x\n\n\ndef min_length_validator(x: Any, min_length: Any) -> Any:\n    if not (len(x) >= min_length):\n        raise PydanticKnownError(\n            'too_short',\n            {'field_type': 'Value', 'min_length': min_length, 'actual_length': len(x)},\n        )\n    return x\n\n\ndef max_length_validator(x: Any, max_length: Any) -> Any:\n    if len(x) > max_length:\n        raise PydanticKnownError(\n            'too_long',\n            {'field_type': 'Value', 'max_length': max_length, 'actual_length': len(x)},\n        )\n    return x\n\n\ndef forbid_inf_nan_check(x: Any) -> Any:\n    if not math.isfinite(x):\n        raise PydanticKnownError('finite_number')\n    return x\n", "pydantic/_internal/_known_annotated_metadata.py": "from __future__ import annotations\n\nfrom collections import defaultdict\nfrom copy import copy\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Any, Callable, Iterable\n\nfrom pydantic_core import CoreSchema, PydanticCustomError, to_jsonable_python\nfrom pydantic_core import core_schema as cs\n\nfrom ._fields import PydanticMetadata\n\nif TYPE_CHECKING:\n    from ..annotated_handlers import GetJsonSchemaHandler\n\n\nSTRICT = {'strict'}\nSEQUENCE_CONSTRAINTS = {'min_length', 'max_length'}\nINEQUALITY = {'le', 'ge', 'lt', 'gt'}\nNUMERIC_CONSTRAINTS = {'multiple_of', 'allow_inf_nan', *INEQUALITY}\n\nSTR_CONSTRAINTS = {\n    *SEQUENCE_CONSTRAINTS,\n    *STRICT,\n    'strip_whitespace',\n    'to_lower',\n    'to_upper',\n    'pattern',\n    'coerce_numbers_to_str',\n}\nBYTES_CONSTRAINTS = {*SEQUENCE_CONSTRAINTS, *STRICT}\n\nLIST_CONSTRAINTS = {*SEQUENCE_CONSTRAINTS, *STRICT}\nTUPLE_CONSTRAINTS = {*SEQUENCE_CONSTRAINTS, *STRICT}\nSET_CONSTRAINTS = {*SEQUENCE_CONSTRAINTS, *STRICT}\nDICT_CONSTRAINTS = {*SEQUENCE_CONSTRAINTS, *STRICT}\nGENERATOR_CONSTRAINTS = {*SEQUENCE_CONSTRAINTS, *STRICT}\n\nFLOAT_CONSTRAINTS = {*NUMERIC_CONSTRAINTS, *STRICT}\nINT_CONSTRAINTS = {*NUMERIC_CONSTRAINTS, *STRICT}\nBOOL_CONSTRAINTS = STRICT\nUUID_CONSTRAINTS = STRICT\n\nDATE_TIME_CONSTRAINTS = {*NUMERIC_CONSTRAINTS, *STRICT}\nTIMEDELTA_CONSTRAINTS = {*NUMERIC_CONSTRAINTS, *STRICT}\nTIME_CONSTRAINTS = {*NUMERIC_CONSTRAINTS, *STRICT}\nLAX_OR_STRICT_CONSTRAINTS = STRICT\nENUM_CONSTRAINTS = STRICT\n\nUNION_CONSTRAINTS = {'union_mode'}\nURL_CONSTRAINTS = {\n    'max_length',\n    'allowed_schemes',\n    'host_required',\n    'default_host',\n    'default_port',\n    'default_path',\n}\n\nTEXT_SCHEMA_TYPES = ('str', 'bytes', 'url', 'multi-host-url')\nSEQUENCE_SCHEMA_TYPES = ('list', 'tuple', 'set', 'frozenset', 'generator', *TEXT_SCHEMA_TYPES)\nNUMERIC_SCHEMA_TYPES = ('float', 'int', 'date', 'time', 'timedelta', 'datetime')\n\nCONSTRAINTS_TO_ALLOWED_SCHEMAS: dict[str, set[str]] = defaultdict(set)\nfor constraint in STR_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(TEXT_SCHEMA_TYPES)\nfor constraint in BYTES_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('bytes',))\nfor constraint in LIST_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('list',))\nfor constraint in TUPLE_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('tuple',))\nfor constraint in SET_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('set', 'frozenset'))\nfor constraint in DICT_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('dict',))\nfor constraint in GENERATOR_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('generator',))\nfor constraint in FLOAT_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('float',))\nfor constraint in INT_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('int',))\nfor constraint in DATE_TIME_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('date', 'time', 'datetime'))\nfor constraint in TIMEDELTA_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('timedelta',))\nfor constraint in TIME_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('time',))\nfor schema_type in (*TEXT_SCHEMA_TYPES, *SEQUENCE_SCHEMA_TYPES, *NUMERIC_SCHEMA_TYPES, 'typed-dict', 'model'):\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS['strict'].add(schema_type)\nfor constraint in UNION_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('union',))\nfor constraint in URL_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('url', 'multi-host-url'))\nfor constraint in BOOL_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('bool',))\nfor constraint in UUID_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('uuid',))\nfor constraint in LAX_OR_STRICT_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('lax-or-strict',))\nfor constraint in ENUM_CONSTRAINTS:\n    CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint].update(('enum',))\n\n\ndef add_js_update_schema(s: cs.CoreSchema, f: Callable[[], dict[str, Any]]) -> None:\n    def update_js_schema(s: cs.CoreSchema, handler: GetJsonSchemaHandler) -> dict[str, Any]:\n        js_schema = handler(s)\n        js_schema.update(f())\n        return js_schema\n\n    if 'metadata' in s:\n        metadata = s['metadata']\n        if 'pydantic_js_functions' in s:\n            metadata['pydantic_js_functions'].append(update_js_schema)\n        else:\n            metadata['pydantic_js_functions'] = [update_js_schema]\n    else:\n        s['metadata'] = {'pydantic_js_functions': [update_js_schema]}\n\n\ndef as_jsonable_value(v: Any) -> Any:\n    if type(v) not in (int, str, float, bytes, bool, type(None)):\n        return to_jsonable_python(v)\n    return v\n\n\ndef expand_grouped_metadata(annotations: Iterable[Any]) -> Iterable[Any]:\n    \"\"\"Expand the annotations.\n\n    Args:\n        annotations: An iterable of annotations.\n\n    Returns:\n        An iterable of expanded annotations.\n\n    Example:\n        ```py\n        from annotated_types import Ge, Len\n\n        from pydantic._internal._known_annotated_metadata import expand_grouped_metadata\n\n        print(list(expand_grouped_metadata([Ge(4), Len(5)])))\n        #> [Ge(ge=4), MinLen(min_length=5)]\n        ```\n    \"\"\"\n    import annotated_types as at\n\n    from pydantic.fields import FieldInfo  # circular import\n\n    for annotation in annotations:\n        if isinstance(annotation, at.GroupedMetadata):\n            yield from annotation\n        elif isinstance(annotation, FieldInfo):\n            yield from annotation.metadata\n            # this is a bit problematic in that it results in duplicate metadata\n            # all of our \"consumers\" can handle it, but it is not ideal\n            # we probably should split up FieldInfo into:\n            # - annotated types metadata\n            # - individual metadata known only to Pydantic\n            annotation = copy(annotation)\n            annotation.metadata = []\n            yield annotation\n        else:\n            yield annotation\n\n\ndef apply_known_metadata(annotation: Any, schema: CoreSchema) -> CoreSchema | None:  # noqa: C901\n    \"\"\"Apply `annotation` to `schema` if it is an annotation we know about (Gt, Le, etc.).\n    Otherwise return `None`.\n\n    This does not handle all known annotations. If / when it does, it can always\n    return a CoreSchema and return the unmodified schema if the annotation should be ignored.\n\n    Assumes that GroupedMetadata has already been expanded via `expand_grouped_metadata`.\n\n    Args:\n        annotation: The annotation.\n        schema: The schema.\n\n    Returns:\n        An updated schema with annotation if it is an annotation we know about, `None` otherwise.\n\n    Raises:\n        PydanticCustomError: If `Predicate` fails.\n    \"\"\"\n    import annotated_types as at\n\n    from . import _validators\n\n    COMPARISON_VALIDATORS = {\n        'gt': _validators.greater_than_validator,\n        'ge': _validators.greater_than_or_equal_validator,\n        'lt': _validators.less_than_validator,\n        'le': _validators.less_than_or_equal_validator,\n        'multiple_of': _validators.multiple_of_validator,\n        'min_length': _validators.min_length_validator,\n        'max_length': _validators.max_length_validator,\n    }\n\n    CONSTRAINT_STR_FROM_ANNOTATED_TYPE = {\n        at.Gt: 'gt',\n        at.Ge: 'ge',\n        at.Lt: 'lt',\n        at.Le: 'le',\n        at.MultipleOf: 'multiple_of',\n        at.MinLen: 'min_length',\n        at.MaxLen: 'max_length',\n    }\n\n    schema = schema.copy()\n    schema_update, other_metadata = collect_known_metadata([annotation])\n    schema_type = schema['type']\n\n    chain_schema_steps: list[CoreSchema] = []\n\n    for constraint, value in schema_update.items():\n        if constraint not in CONSTRAINTS_TO_ALLOWED_SCHEMAS:\n            raise ValueError(f'Unknown constraint {constraint}')\n        allowed_schemas = CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint]\n\n        # if it becomes necessary to handle more than one constraint\n        # in this recursive case with function-after or function-wrap, we should refactor\n        if schema_type in {'function-before', 'function-wrap', 'function-after'} and constraint == 'strict':\n            schema['schema'] = apply_known_metadata(annotation, schema['schema'])  # type: ignore  # schema is function-after schema\n            return schema\n\n        if schema_type in allowed_schemas:\n            if constraint == 'union_mode' and schema_type == 'union':\n                schema['mode'] = value  # type: ignore  # schema is UnionSchema\n            else:\n                schema[constraint] = value\n            continue\n\n        if constraint in {'pattern', 'strip_whitespace', 'to_lower', 'to_upper', 'coerce_numbers_to_str'}:\n            chain_schema_steps.append(cs.str_schema(**{constraint: value}))\n        elif constraint in {'gt', 'ge', 'lt', 'le', 'multiple_of', 'min_length', 'max_length'}:\n            if constraint == 'multiple_of':\n                json_schema_constraint = 'multiple_of'\n            elif constraint in {'min_length', 'max_length'}:\n                if schema['type'] == 'list' or (\n                    schema['type'] == 'json-or-python' and schema['json_schema']['type'] == 'list'\n                ):\n                    json_schema_constraint = 'minItems' if constraint == 'min_length' else 'maxItems'\n                else:\n                    json_schema_constraint = 'minLength' if constraint == 'min_length' else 'maxLength'\n            else:\n                json_schema_constraint = constraint\n\n            schema = cs.no_info_after_validator_function(\n                partial(COMPARISON_VALIDATORS[constraint], **{constraint: value}), schema\n            )\n\n            add_js_update_schema(schema, lambda: {json_schema_constraint: as_jsonable_value(value)})\n        elif constraint == 'allow_inf_nan' and value is False:\n            schema = cs.no_info_after_validator_function(\n                _validators.forbid_inf_nan_check,\n                schema,\n            )\n        else:\n            raise RuntimeError(f'Unable to apply constraint {constraint} to schema {schema_type}')\n\n    for annotation in other_metadata:\n        if isinstance(annotation, (at.Gt, at.Ge, at.Lt, at.Le, at.MultipleOf, at.MinLen, at.MaxLen)):\n            constraint = CONSTRAINT_STR_FROM_ANNOTATED_TYPE[type(annotation)]\n            schema = cs.no_info_after_validator_function(\n                partial(COMPARISON_VALIDATORS[constraint], {constraint: getattr(annotation, constraint)}), schema\n            )\n        elif isinstance(annotation, at.Predicate):\n            predicate_name = f'{annotation.func.__qualname__} ' if hasattr(annotation.func, '__qualname__') else ''\n\n            def val_func(v: Any) -> Any:\n                # annotation.func may also raise an exception, let it pass through\n                if not annotation.func(v):\n                    raise PydanticCustomError(\n                        'predicate_failed',\n                        f'Predicate {predicate_name}failed',  # type: ignore\n                    )\n                return v\n\n            return cs.no_info_after_validator_function(val_func, schema)\n        # ignore any other unknown metadata\n        return None\n\n    if chain_schema_steps:\n        chain_schema_steps = [schema] + chain_schema_steps\n        return cs.chain_schema(chain_schema_steps)\n\n    return schema\n\n\ndef collect_known_metadata(annotations: Iterable[Any]) -> tuple[dict[str, Any], list[Any]]:\n    \"\"\"Split `annotations` into known metadata and unknown annotations.\n\n    Args:\n        annotations: An iterable of annotations.\n\n    Returns:\n        A tuple contains a dict of known metadata and a list of unknown annotations.\n\n    Example:\n        ```py\n        from annotated_types import Gt, Len\n\n        from pydantic._internal._known_annotated_metadata import collect_known_metadata\n\n        print(collect_known_metadata([Gt(1), Len(42), ...]))\n        #> ({'gt': 1, 'min_length': 42}, [Ellipsis])\n        ```\n    \"\"\"\n    import annotated_types as at\n\n    annotations = expand_grouped_metadata(annotations)\n\n    res: dict[str, Any] = {}\n    remaining: list[Any] = []\n    for annotation in annotations:\n        # isinstance(annotation, PydanticMetadata) also covers ._fields:_PydanticGeneralMetadata\n        if isinstance(annotation, PydanticMetadata):\n            res.update(annotation.__dict__)\n        # we don't use dataclasses.asdict because that recursively calls asdict on the field values\n        elif isinstance(annotation, at.MinLen):\n            res.update({'min_length': annotation.min_length})\n        elif isinstance(annotation, at.MaxLen):\n            res.update({'max_length': annotation.max_length})\n        elif isinstance(annotation, at.Gt):\n            res.update({'gt': annotation.gt})\n        elif isinstance(annotation, at.Ge):\n            res.update({'ge': annotation.ge})\n        elif isinstance(annotation, at.Lt):\n            res.update({'lt': annotation.lt})\n        elif isinstance(annotation, at.Le):\n            res.update({'le': annotation.le})\n        elif isinstance(annotation, at.MultipleOf):\n            res.update({'multiple_of': annotation.multiple_of})\n        elif isinstance(annotation, type) and issubclass(annotation, PydanticMetadata):\n            # also support PydanticMetadata classes being used without initialisation,\n            # e.g. `Annotated[int, Strict]` as well as `Annotated[int, Strict()]`\n            res.update({k: v for k, v in vars(annotation).items() if not k.startswith('_')})\n        else:\n            remaining.append(annotation)\n    # Nones can sneak in but pydantic-core will reject them\n    # it'd be nice to clean things up so we don't put in None (we probably don't _need_ to, it was just easier)\n    # but this is simple enough to kick that can down the road\n    res = {k: v for k, v in res.items() if v is not None}\n    return res, remaining\n\n\ndef check_metadata(metadata: dict[str, Any], allowed: Iterable[str], source_type: Any) -> None:\n    \"\"\"A small utility function to validate that the given metadata can be applied to the target.\n    More than saving lines of code, this gives us a consistent error message for all of our internal implementations.\n\n    Args:\n        metadata: A dict of metadata.\n        allowed: An iterable of allowed metadata.\n        source_type: The source type.\n\n    Raises:\n        TypeError: If there is metadatas that can't be applied on source type.\n    \"\"\"\n    unknown = metadata.keys() - set(allowed)\n    if unknown:\n        raise TypeError(\n            f'The following constraints cannot be applied to {source_type!r}: {\", \".join([f\"{k!r}\" for k in unknown])}'\n        )\n", "pydantic/_internal/_forward_ref.py": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom typing import Union\n\n\n@dataclass\nclass PydanticRecursiveRef:\n    type_ref: str\n\n    __name__ = 'PydanticRecursiveRef'\n    __hash__ = object.__hash__\n\n    def __call__(self) -> None:\n        \"\"\"Defining __call__ is necessary for the `typing` module to let you use an instance of\n        this class as the result of resolving a standard ForwardRef.\n        \"\"\"\n\n    def __or__(self, other):\n        return Union[self, other]  # type: ignore\n\n    def __ror__(self, other):\n        return Union[other, self]  # type: ignore\n", "pydantic/_internal/_decorators_v1.py": "\"\"\"Logic for V1 validators, e.g. `@validator` and `@root_validator`.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom inspect import Parameter, signature\nfrom typing import Any, Dict, Tuple, Union, cast\n\nfrom pydantic_core import core_schema\nfrom typing_extensions import Protocol\n\nfrom ..errors import PydanticUserError\nfrom ._decorators import can_be_positional\n\n\nclass V1OnlyValueValidator(Protocol):\n    \"\"\"A simple validator, supported for V1 validators and V2 validators.\"\"\"\n\n    def __call__(self, __value: Any) -> Any: ...\n\n\nclass V1ValidatorWithValues(Protocol):\n    \"\"\"A validator with `values` argument, supported for V1 validators and V2 validators.\"\"\"\n\n    def __call__(self, __value: Any, values: dict[str, Any]) -> Any: ...\n\n\nclass V1ValidatorWithValuesKwOnly(Protocol):\n    \"\"\"A validator with keyword only `values` argument, supported for V1 validators and V2 validators.\"\"\"\n\n    def __call__(self, __value: Any, *, values: dict[str, Any]) -> Any: ...\n\n\nclass V1ValidatorWithKwargs(Protocol):\n    \"\"\"A validator with `kwargs` argument, supported for V1 validators and V2 validators.\"\"\"\n\n    def __call__(self, __value: Any, **kwargs: Any) -> Any: ...\n\n\nclass V1ValidatorWithValuesAndKwargs(Protocol):\n    \"\"\"A validator with `values` and `kwargs` arguments, supported for V1 validators and V2 validators.\"\"\"\n\n    def __call__(self, __value: Any, values: dict[str, Any], **kwargs: Any) -> Any: ...\n\n\nV1Validator = Union[\n    V1ValidatorWithValues, V1ValidatorWithValuesKwOnly, V1ValidatorWithKwargs, V1ValidatorWithValuesAndKwargs\n]\n\n\ndef can_be_keyword(param: Parameter) -> bool:\n    return param.kind in (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY)\n\n\ndef make_generic_v1_field_validator(validator: V1Validator) -> core_schema.WithInfoValidatorFunction:\n    \"\"\"Wrap a V1 style field validator for V2 compatibility.\n\n    Args:\n        validator: The V1 style field validator.\n\n    Returns:\n        A wrapped V2 style field validator.\n\n    Raises:\n        PydanticUserError: If the signature is not supported or the parameters are\n            not available in Pydantic V2.\n    \"\"\"\n    sig = signature(validator)\n\n    needs_values_kw = False\n\n    for param_num, (param_name, parameter) in enumerate(sig.parameters.items()):\n        if can_be_keyword(parameter) and param_name in ('field', 'config'):\n            raise PydanticUserError(\n                'The `field` and `config` parameters are not available in Pydantic V2, '\n                'please use the `info` parameter instead.',\n                code='validator-field-config-info',\n            )\n        if parameter.kind is Parameter.VAR_KEYWORD:\n            needs_values_kw = True\n        elif can_be_keyword(parameter) and param_name == 'values':\n            needs_values_kw = True\n        elif can_be_positional(parameter) and param_num == 0:\n            # value\n            continue\n        elif parameter.default is Parameter.empty:  # ignore params with defaults e.g. bound by functools.partial\n            raise PydanticUserError(\n                f'Unsupported signature for V1 style validator {validator}: {sig} is not supported.',\n                code='validator-v1-signature',\n            )\n\n    if needs_values_kw:\n        # (v, **kwargs), (v, values, **kwargs), (v, *, values, **kwargs) or (v, *, values)\n        val1 = cast(V1ValidatorWithValues, validator)\n\n        def wrapper1(value: Any, info: core_schema.ValidationInfo) -> Any:\n            return val1(value, values=info.data)\n\n        return wrapper1\n    else:\n        val2 = cast(V1OnlyValueValidator, validator)\n\n        def wrapper2(value: Any, _: core_schema.ValidationInfo) -> Any:\n            return val2(value)\n\n        return wrapper2\n\n\nRootValidatorValues = Dict[str, Any]\n# technically tuple[model_dict, model_extra, fields_set] | tuple[dataclass_dict, init_vars]\nRootValidatorFieldsTuple = Tuple[Any, ...]\n\n\nclass V1RootValidatorFunction(Protocol):\n    \"\"\"A simple root validator, supported for V1 validators and V2 validators.\"\"\"\n\n    def __call__(self, __values: RootValidatorValues) -> RootValidatorValues: ...\n\n\nclass V2CoreBeforeRootValidator(Protocol):\n    \"\"\"V2 validator with mode='before'.\"\"\"\n\n    def __call__(self, __values: RootValidatorValues, __info: core_schema.ValidationInfo) -> RootValidatorValues: ...\n\n\nclass V2CoreAfterRootValidator(Protocol):\n    \"\"\"V2 validator with mode='after'.\"\"\"\n\n    def __call__(\n        self, __fields_tuple: RootValidatorFieldsTuple, __info: core_schema.ValidationInfo\n    ) -> RootValidatorFieldsTuple: ...\n\n\ndef make_v1_generic_root_validator(\n    validator: V1RootValidatorFunction, pre: bool\n) -> V2CoreBeforeRootValidator | V2CoreAfterRootValidator:\n    \"\"\"Wrap a V1 style root validator for V2 compatibility.\n\n    Args:\n        validator: The V1 style field validator.\n        pre: Whether the validator is a pre validator.\n\n    Returns:\n        A wrapped V2 style validator.\n    \"\"\"\n    if pre is True:\n        # mode='before' for pydantic-core\n        def _wrapper1(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n            return validator(values)\n\n        return _wrapper1\n\n    # mode='after' for pydantic-core\n    def _wrapper2(fields_tuple: RootValidatorFieldsTuple, _: core_schema.ValidationInfo) -> RootValidatorFieldsTuple:\n        if len(fields_tuple) == 2:\n            # dataclass, this is easy\n            values, init_vars = fields_tuple\n            values = validator(values)\n            return values, init_vars\n        else:\n            # ugly hack: to match v1 behaviour, we merge values and model_extra, then split them up based on fields\n            # afterwards\n            model_dict, model_extra, fields_set = fields_tuple\n            if model_extra:\n                fields = set(model_dict.keys())\n                model_dict.update(model_extra)\n                model_dict_new = validator(model_dict)\n                for k in list(model_dict_new.keys()):\n                    if k not in fields:\n                        model_extra[k] = model_dict_new.pop(k)\n            else:\n                model_dict_new = validator(model_dict)\n            return model_dict_new, model_extra, fields_set\n\n    return _wrapper2\n", "pydantic/_internal/_mock_val_ser.py": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Callable, Generic, Iterator, Mapping, TypeVar\n\nfrom pydantic_core import CoreSchema, SchemaSerializer, SchemaValidator\nfrom typing_extensions import Literal\n\nfrom ..errors import PydanticErrorCodes, PydanticUserError\n\nif TYPE_CHECKING:\n    from ..dataclasses import PydanticDataclass\n    from ..main import BaseModel\n\n\nValSer = TypeVar('ValSer', SchemaValidator, SchemaSerializer)\nT = TypeVar('T')\n\n\nclass MockCoreSchema(Mapping[str, Any]):\n    \"\"\"Mocker for `pydantic_core.CoreSchema` which optionally attempts to\n    rebuild the thing it's mocking when one of its methods is accessed and raises an error if that fails.\n    \"\"\"\n\n    __slots__ = '_error_message', '_code', '_attempt_rebuild', '_built_memo'\n\n    def __init__(\n        self,\n        error_message: str,\n        *,\n        code: PydanticErrorCodes,\n        attempt_rebuild: Callable[[], CoreSchema | None] | None = None,\n    ) -> None:\n        self._error_message = error_message\n        self._code: PydanticErrorCodes = code\n        self._attempt_rebuild = attempt_rebuild\n        self._built_memo: CoreSchema | None = None\n\n    def __getitem__(self, key: str) -> Any:\n        return self._get_built().__getitem__(key)\n\n    def __len__(self) -> int:\n        return self._get_built().__len__()\n\n    def __iter__(self) -> Iterator[str]:\n        return self._get_built().__iter__()\n\n    def _get_built(self) -> CoreSchema:\n        if self._built_memo is not None:\n            return self._built_memo\n\n        if self._attempt_rebuild:\n            schema = self._attempt_rebuild()\n            if schema is not None:\n                self._built_memo = schema\n                return schema\n        raise PydanticUserError(self._error_message, code=self._code)\n\n    def rebuild(self) -> CoreSchema | None:\n        self._built_memo = None\n        if self._attempt_rebuild:\n            val_ser = self._attempt_rebuild()\n            if val_ser is not None:\n                return val_ser\n            else:\n                raise PydanticUserError(self._error_message, code=self._code)\n        return None\n\n\nclass MockValSer(Generic[ValSer]):\n    \"\"\"Mocker for `pydantic_core.SchemaValidator` or `pydantic_core.SchemaSerializer` which optionally attempts to\n    rebuild the thing it's mocking when one of its methods is accessed and raises an error if that fails.\n    \"\"\"\n\n    __slots__ = '_error_message', '_code', '_val_or_ser', '_attempt_rebuild'\n\n    def __init__(\n        self,\n        error_message: str,\n        *,\n        code: PydanticErrorCodes,\n        val_or_ser: Literal['validator', 'serializer'],\n        attempt_rebuild: Callable[[], ValSer | None] | None = None,\n    ) -> None:\n        self._error_message = error_message\n        self._val_or_ser = SchemaValidator if val_or_ser == 'validator' else SchemaSerializer\n        self._code: PydanticErrorCodes = code\n        self._attempt_rebuild = attempt_rebuild\n\n    def __getattr__(self, item: str) -> None:\n        __tracebackhide__ = True\n        if self._attempt_rebuild:\n            val_ser = self._attempt_rebuild()\n            if val_ser is not None:\n                return getattr(val_ser, item)\n\n        # raise an AttributeError if `item` doesn't exist\n        getattr(self._val_or_ser, item)\n        raise PydanticUserError(self._error_message, code=self._code)\n\n    def rebuild(self) -> ValSer | None:\n        if self._attempt_rebuild:\n            val_ser = self._attempt_rebuild()\n            if val_ser is not None:\n                return val_ser\n            else:\n                raise PydanticUserError(self._error_message, code=self._code)\n        return None\n\n\ndef set_model_mocks(cls: type[BaseModel], cls_name: str, undefined_name: str = 'all referenced types') -> None:\n    \"\"\"Set `__pydantic_validator__` and `__pydantic_serializer__` to `MockValSer`s on a model.\n\n    Args:\n        cls: The model class to set the mocks on\n        cls_name: Name of the model class, used in error messages\n        undefined_name: Name of the undefined thing, used in error messages\n    \"\"\"\n    undefined_type_error_message = (\n        f'`{cls_name}` is not fully defined; you should define {undefined_name},'\n        f' then call `{cls_name}.model_rebuild()`.'\n    )\n\n    def attempt_rebuild_fn(attr_fn: Callable[[type[BaseModel]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if cls.model_rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            else:\n                return None\n\n        return handler\n\n    cls.__pydantic_core_schema__ = MockCoreSchema(  # type: ignore[assignment]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_core_schema__),\n    )\n    cls.__pydantic_validator__ = MockValSer(  # type: ignore[assignment]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='validator',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_validator__),\n    )\n    cls.__pydantic_serializer__ = MockValSer(  # type: ignore[assignment]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='serializer',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_serializer__),\n    )\n\n\ndef set_dataclass_mocks(\n    cls: type[PydanticDataclass], cls_name: str, undefined_name: str = 'all referenced types'\n) -> None:\n    \"\"\"Set `__pydantic_validator__` and `__pydantic_serializer__` to `MockValSer`s on a dataclass.\n\n    Args:\n        cls: The model class to set the mocks on\n        cls_name: Name of the model class, used in error messages\n        undefined_name: Name of the undefined thing, used in error messages\n    \"\"\"\n    from ..dataclasses import rebuild_dataclass\n\n    undefined_type_error_message = (\n        f'`{cls_name}` is not fully defined; you should define {undefined_name},'\n        f' then call `pydantic.dataclasses.rebuild_dataclass({cls_name})`.'\n    )\n\n    def attempt_rebuild_fn(attr_fn: Callable[[type[PydanticDataclass]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if rebuild_dataclass(cls, raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            else:\n                return None\n\n        return handler\n\n    cls.__pydantic_core_schema__ = MockCoreSchema(  # type: ignore[assignment]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_core_schema__),\n    )\n    cls.__pydantic_validator__ = MockValSer(  # type: ignore[assignment]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='validator',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_validator__),\n    )\n    cls.__pydantic_serializer__ = MockValSer(  # type: ignore[assignment]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='validator',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_serializer__),\n    )\n", "pydantic/_internal/_core_utils.py": "from __future__ import annotations\n\nimport os\nfrom collections import defaultdict\nfrom typing import (\n    Any,\n    Callable,\n    Hashable,\n    TypeVar,\n    Union,\n)\n\nfrom pydantic_core import CoreSchema, core_schema\nfrom pydantic_core import validate_core_schema as _validate_core_schema\nfrom typing_extensions import TypeAliasType, TypeGuard, get_args, get_origin\n\nfrom . import _repr\nfrom ._typing_extra import is_generic_alias\n\nAnyFunctionSchema = Union[\n    core_schema.AfterValidatorFunctionSchema,\n    core_schema.BeforeValidatorFunctionSchema,\n    core_schema.WrapValidatorFunctionSchema,\n    core_schema.PlainValidatorFunctionSchema,\n]\n\n\nFunctionSchemaWithInnerSchema = Union[\n    core_schema.AfterValidatorFunctionSchema,\n    core_schema.BeforeValidatorFunctionSchema,\n    core_schema.WrapValidatorFunctionSchema,\n]\n\nCoreSchemaField = Union[\n    core_schema.ModelField, core_schema.DataclassField, core_schema.TypedDictField, core_schema.ComputedField\n]\nCoreSchemaOrField = Union[core_schema.CoreSchema, CoreSchemaField]\n\n_CORE_SCHEMA_FIELD_TYPES = {'typed-dict-field', 'dataclass-field', 'model-field', 'computed-field'}\n_FUNCTION_WITH_INNER_SCHEMA_TYPES = {'function-before', 'function-after', 'function-wrap'}\n_LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES = {'list', 'set', 'frozenset'}\n\nTAGGED_UNION_TAG_KEY = 'pydantic.internal.tagged_union_tag'\n\"\"\"\nUsed in a `Tag` schema to specify the tag used for a discriminated union.\n\"\"\"\nHAS_INVALID_SCHEMAS_METADATA_KEY = 'pydantic.internal.invalid'\n\"\"\"Used to mark a schema that is invalid because it refers to a definition that was not yet defined when the\nschema was first encountered.\n\"\"\"\n\n\ndef is_core_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchema]:\n    return schema['type'] not in _CORE_SCHEMA_FIELD_TYPES\n\n\ndef is_core_schema_field(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return schema['type'] in _CORE_SCHEMA_FIELD_TYPES\n\n\ndef is_function_with_inner_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[FunctionSchemaWithInnerSchema]:\n    return schema['type'] in _FUNCTION_WITH_INNER_SCHEMA_TYPES\n\n\ndef is_list_like_schema_with_items_schema(\n    schema: CoreSchema,\n) -> TypeGuard[core_schema.ListSchema | core_schema.SetSchema | core_schema.FrozenSetSchema]:\n    return schema['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n\n\ndef get_type_ref(type_: type[Any], args_override: tuple[type[Any], ...] | None = None) -> str:\n    \"\"\"Produces the ref to be used for this type by pydantic_core's core schemas.\n\n    This `args_override` argument was added for the purpose of creating valid recursive references\n    when creating generic models without needing to create a concrete class.\n    \"\"\"\n    origin = get_origin(type_) or type_\n\n    args = get_args(type_) if is_generic_alias(type_) else (args_override or ())\n    generic_metadata = getattr(type_, '__pydantic_generic_metadata__', None)\n    if generic_metadata:\n        origin = generic_metadata['origin'] or origin\n        args = generic_metadata['args'] or args\n\n    module_name = getattr(origin, '__module__', '<No __module__>')\n    if isinstance(origin, TypeAliasType):\n        type_ref = f'{module_name}.{origin.__name__}:{id(origin)}'\n    else:\n        try:\n            qualname = getattr(origin, '__qualname__', f'<No __qualname__: {origin}>')\n        except Exception:\n            qualname = getattr(origin, '__qualname__', '<No __qualname__>')\n        type_ref = f'{module_name}.{qualname}:{id(origin)}'\n\n    arg_refs: list[str] = []\n    for arg in args:\n        if isinstance(arg, str):\n            # Handle string literals as a special case; we may be able to remove this special handling if we\n            # wrap them in a ForwardRef at some point.\n            arg_ref = f'{arg}:str-{id(arg)}'\n        else:\n            arg_ref = f'{_repr.display_as_type(arg)}:{id(arg)}'\n        arg_refs.append(arg_ref)\n    if arg_refs:\n        type_ref = f'{type_ref}[{\",\".join(arg_refs)}]'\n    return type_ref\n\n\ndef get_ref(s: core_schema.CoreSchema) -> None | str:\n    \"\"\"Get the ref from the schema if it has one.\n    This exists just for type checking to work correctly.\n    \"\"\"\n    return s.get('ref', None)\n\n\ndef collect_definitions(schema: core_schema.CoreSchema) -> dict[str, core_schema.CoreSchema]:\n    defs: dict[str, CoreSchema] = {}\n\n    def _record_valid_refs(s: core_schema.CoreSchema, recurse: Recurse) -> core_schema.CoreSchema:\n        ref = get_ref(s)\n        if ref:\n            defs[ref] = s\n        return recurse(s, _record_valid_refs)\n\n    walk_core_schema(schema, _record_valid_refs)\n\n    return defs\n\n\ndef define_expected_missing_refs(\n    schema: core_schema.CoreSchema, allowed_missing_refs: set[str]\n) -> core_schema.CoreSchema | None:\n    if not allowed_missing_refs:\n        # in this case, there are no missing refs to potentially substitute, so there's no need to walk the schema\n        # this is a common case (will be hit for all non-generic models), so it's worth optimizing for\n        return None\n\n    refs = collect_definitions(schema).keys()\n\n    expected_missing_refs = allowed_missing_refs.difference(refs)\n    if expected_missing_refs:\n        definitions: list[core_schema.CoreSchema] = [\n            # TODO: Replace this with a (new) CoreSchema that, if present at any level, makes validation fail\n            #   Issue: https://github.com/pydantic/pydantic-core/issues/619\n            core_schema.none_schema(ref=ref, metadata={HAS_INVALID_SCHEMAS_METADATA_KEY: True})\n            for ref in expected_missing_refs\n        ]\n        return core_schema.definitions_schema(schema, definitions)\n    return None\n\n\ndef collect_invalid_schemas(schema: core_schema.CoreSchema) -> bool:\n    invalid = False\n\n    def _is_schema_valid(s: core_schema.CoreSchema, recurse: Recurse) -> core_schema.CoreSchema:\n        nonlocal invalid\n        if 'metadata' in s:\n            metadata = s['metadata']\n            if HAS_INVALID_SCHEMAS_METADATA_KEY in metadata:\n                invalid = metadata[HAS_INVALID_SCHEMAS_METADATA_KEY]\n                return s\n        return recurse(s, _is_schema_valid)\n\n    walk_core_schema(schema, _is_schema_valid)\n    return invalid\n\n\nT = TypeVar('T')\n\n\nRecurse = Callable[[core_schema.CoreSchema, 'Walk'], core_schema.CoreSchema]\nWalk = Callable[[core_schema.CoreSchema, Recurse], core_schema.CoreSchema]\n\n# TODO: Should we move _WalkCoreSchema into pydantic_core proper?\n#   Issue: https://github.com/pydantic/pydantic-core/issues/615\n\n\nclass _WalkCoreSchema:\n    def __init__(self):\n        self._schema_type_to_method = self._build_schema_type_to_method()\n\n    def _build_schema_type_to_method(self) -> dict[core_schema.CoreSchemaType, Recurse]:\n        mapping: dict[core_schema.CoreSchemaType, Recurse] = {}\n        key: core_schema.CoreSchemaType\n        for key in get_args(core_schema.CoreSchemaType):\n            method_name = f\"handle_{key.replace('-', '_')}_schema\"\n            mapping[key] = getattr(self, method_name, self._handle_other_schemas)\n        return mapping\n\n    def walk(self, schema: core_schema.CoreSchema, f: Walk) -> core_schema.CoreSchema:\n        return f(schema, self._walk)\n\n    def _walk(self, schema: core_schema.CoreSchema, f: Walk) -> core_schema.CoreSchema:\n        schema = self._schema_type_to_method[schema['type']](schema.copy(), f)\n        ser_schema: core_schema.SerSchema | None = schema.get('serialization')  # type: ignore\n        if ser_schema:\n            schema['serialization'] = self._handle_ser_schemas(ser_schema, f)\n        return schema\n\n    def _handle_other_schemas(self, schema: core_schema.CoreSchema, f: Walk) -> core_schema.CoreSchema:\n        sub_schema = schema.get('schema', None)\n        if sub_schema is not None:\n            schema['schema'] = self.walk(sub_schema, f)  # type: ignore\n        return schema\n\n    def _handle_ser_schemas(self, ser_schema: core_schema.SerSchema, f: Walk) -> core_schema.SerSchema:\n        schema: core_schema.CoreSchema | None = ser_schema.get('schema', None)\n        if schema is not None:\n            ser_schema['schema'] = self.walk(schema, f)  # type: ignore\n        return_schema: core_schema.CoreSchema | None = ser_schema.get('return_schema', None)\n        if return_schema is not None:\n            ser_schema['return_schema'] = self.walk(return_schema, f)  # type: ignore\n        return ser_schema\n\n    def handle_definitions_schema(self, schema: core_schema.DefinitionsSchema, f: Walk) -> core_schema.CoreSchema:\n        new_definitions: list[core_schema.CoreSchema] = []\n        for definition in schema['definitions']:\n            if 'schema_ref' in definition and 'ref' in definition:\n                # This indicates a purposely indirect reference\n                # We want to keep such references around for implications related to JSON schema, etc.:\n                new_definitions.append(definition)\n                # However, we still need to walk the referenced definition:\n                self.walk(definition, f)\n                continue\n\n            updated_definition = self.walk(definition, f)\n            if 'ref' in updated_definition:\n                # If the updated definition schema doesn't have a 'ref', it shouldn't go in the definitions\n                # This is most likely to happen due to replacing something with a definition reference, in\n                # which case it should certainly not go in the definitions list\n                new_definitions.append(updated_definition)\n        new_inner_schema = self.walk(schema['schema'], f)\n\n        if not new_definitions and len(schema) == 3:\n            # This means we'd be returning a \"trivial\" definitions schema that just wrapped the inner schema\n            return new_inner_schema\n\n        new_schema = schema.copy()\n        new_schema['schema'] = new_inner_schema\n        new_schema['definitions'] = new_definitions\n        return new_schema\n\n    def handle_list_schema(self, schema: core_schema.ListSchema, f: Walk) -> core_schema.CoreSchema:\n        items_schema = schema.get('items_schema')\n        if items_schema is not None:\n            schema['items_schema'] = self.walk(items_schema, f)\n        return schema\n\n    def handle_set_schema(self, schema: core_schema.SetSchema, f: Walk) -> core_schema.CoreSchema:\n        items_schema = schema.get('items_schema')\n        if items_schema is not None:\n            schema['items_schema'] = self.walk(items_schema, f)\n        return schema\n\n    def handle_frozenset_schema(self, schema: core_schema.FrozenSetSchema, f: Walk) -> core_schema.CoreSchema:\n        items_schema = schema.get('items_schema')\n        if items_schema is not None:\n            schema['items_schema'] = self.walk(items_schema, f)\n        return schema\n\n    def handle_generator_schema(self, schema: core_schema.GeneratorSchema, f: Walk) -> core_schema.CoreSchema:\n        items_schema = schema.get('items_schema')\n        if items_schema is not None:\n            schema['items_schema'] = self.walk(items_schema, f)\n        return schema\n\n    def handle_tuple_schema(self, schema: core_schema.TupleSchema, f: Walk) -> core_schema.CoreSchema:\n        schema['items_schema'] = [self.walk(v, f) for v in schema['items_schema']]\n        return schema\n\n    def handle_dict_schema(self, schema: core_schema.DictSchema, f: Walk) -> core_schema.CoreSchema:\n        keys_schema = schema.get('keys_schema')\n        if keys_schema is not None:\n            schema['keys_schema'] = self.walk(keys_schema, f)\n        values_schema = schema.get('values_schema')\n        if values_schema:\n            schema['values_schema'] = self.walk(values_schema, f)\n        return schema\n\n    def handle_function_schema(self, schema: AnyFunctionSchema, f: Walk) -> core_schema.CoreSchema:\n        if not is_function_with_inner_schema(schema):\n            return schema\n        schema['schema'] = self.walk(schema['schema'], f)\n        return schema\n\n    def handle_union_schema(self, schema: core_schema.UnionSchema, f: Walk) -> core_schema.CoreSchema:\n        new_choices: list[CoreSchema | tuple[CoreSchema, str]] = []\n        for v in schema['choices']:\n            if isinstance(v, tuple):\n                new_choices.append((self.walk(v[0], f), v[1]))\n            else:\n                new_choices.append(self.walk(v, f))\n        schema['choices'] = new_choices\n        return schema\n\n    def handle_tagged_union_schema(self, schema: core_schema.TaggedUnionSchema, f: Walk) -> core_schema.CoreSchema:\n        new_choices: dict[Hashable, core_schema.CoreSchema] = {}\n        for k, v in schema['choices'].items():\n            new_choices[k] = v if isinstance(v, (str, int)) else self.walk(v, f)\n        schema['choices'] = new_choices\n        return schema\n\n    def handle_chain_schema(self, schema: core_schema.ChainSchema, f: Walk) -> core_schema.CoreSchema:\n        schema['steps'] = [self.walk(v, f) for v in schema['steps']]\n        return schema\n\n    def handle_lax_or_strict_schema(self, schema: core_schema.LaxOrStrictSchema, f: Walk) -> core_schema.CoreSchema:\n        schema['lax_schema'] = self.walk(schema['lax_schema'], f)\n        schema['strict_schema'] = self.walk(schema['strict_schema'], f)\n        return schema\n\n    def handle_json_or_python_schema(self, schema: core_schema.JsonOrPythonSchema, f: Walk) -> core_schema.CoreSchema:\n        schema['json_schema'] = self.walk(schema['json_schema'], f)\n        schema['python_schema'] = self.walk(schema['python_schema'], f)\n        return schema\n\n    def handle_model_fields_schema(self, schema: core_schema.ModelFieldsSchema, f: Walk) -> core_schema.CoreSchema:\n        extras_schema = schema.get('extras_schema')\n        if extras_schema is not None:\n            schema['extras_schema'] = self.walk(extras_schema, f)\n        replaced_fields: dict[str, core_schema.ModelField] = {}\n        replaced_computed_fields: list[core_schema.ComputedField] = []\n        for computed_field in schema.get('computed_fields', ()):\n            replaced_field = computed_field.copy()\n            replaced_field['return_schema'] = self.walk(computed_field['return_schema'], f)\n            replaced_computed_fields.append(replaced_field)\n        if replaced_computed_fields:\n            schema['computed_fields'] = replaced_computed_fields\n        for k, v in schema['fields'].items():\n            replaced_field = v.copy()\n            replaced_field['schema'] = self.walk(v['schema'], f)\n            replaced_fields[k] = replaced_field\n        schema['fields'] = replaced_fields\n        return schema\n\n    def handle_typed_dict_schema(self, schema: core_schema.TypedDictSchema, f: Walk) -> core_schema.CoreSchema:\n        extras_schema = schema.get('extras_schema')\n        if extras_schema is not None:\n            schema['extras_schema'] = self.walk(extras_schema, f)\n        replaced_computed_fields: list[core_schema.ComputedField] = []\n        for computed_field in schema.get('computed_fields', ()):\n            replaced_field = computed_field.copy()\n            replaced_field['return_schema'] = self.walk(computed_field['return_schema'], f)\n            replaced_computed_fields.append(replaced_field)\n        if replaced_computed_fields:\n            schema['computed_fields'] = replaced_computed_fields\n        replaced_fields: dict[str, core_schema.TypedDictField] = {}\n        for k, v in schema['fields'].items():\n            replaced_field = v.copy()\n            replaced_field['schema'] = self.walk(v['schema'], f)\n            replaced_fields[k] = replaced_field\n        schema['fields'] = replaced_fields\n        return schema\n\n    def handle_dataclass_args_schema(self, schema: core_schema.DataclassArgsSchema, f: Walk) -> core_schema.CoreSchema:\n        replaced_fields: list[core_schema.DataclassField] = []\n        replaced_computed_fields: list[core_schema.ComputedField] = []\n        for computed_field in schema.get('computed_fields', ()):\n            replaced_field = computed_field.copy()\n            replaced_field['return_schema'] = self.walk(computed_field['return_schema'], f)\n            replaced_computed_fields.append(replaced_field)\n        if replaced_computed_fields:\n            schema['computed_fields'] = replaced_computed_fields\n        for field in schema['fields']:\n            replaced_field = field.copy()\n            replaced_field['schema'] = self.walk(field['schema'], f)\n            replaced_fields.append(replaced_field)\n        schema['fields'] = replaced_fields\n        return schema\n\n    def handle_arguments_schema(self, schema: core_schema.ArgumentsSchema, f: Walk) -> core_schema.CoreSchema:\n        replaced_arguments_schema: list[core_schema.ArgumentsParameter] = []\n        for param in schema['arguments_schema']:\n            replaced_param = param.copy()\n            replaced_param['schema'] = self.walk(param['schema'], f)\n            replaced_arguments_schema.append(replaced_param)\n        schema['arguments_schema'] = replaced_arguments_schema\n        if 'var_args_schema' in schema:\n            schema['var_args_schema'] = self.walk(schema['var_args_schema'], f)\n        if 'var_kwargs_schema' in schema:\n            schema['var_kwargs_schema'] = self.walk(schema['var_kwargs_schema'], f)\n        return schema\n\n    def handle_call_schema(self, schema: core_schema.CallSchema, f: Walk) -> core_schema.CoreSchema:\n        schema['arguments_schema'] = self.walk(schema['arguments_schema'], f)\n        if 'return_schema' in schema:\n            schema['return_schema'] = self.walk(schema['return_schema'], f)\n        return schema\n\n\n_dispatch = _WalkCoreSchema().walk\n\n\ndef walk_core_schema(schema: core_schema.CoreSchema, f: Walk) -> core_schema.CoreSchema:\n    \"\"\"Recursively traverse a CoreSchema.\n\n    Args:\n        schema (core_schema.CoreSchema): The CoreSchema to process, it will not be modified.\n        f (Walk): A function to apply. This function takes two arguments:\n          1. The current CoreSchema that is being processed\n             (not the same one you passed into this function, one level down).\n          2. The \"next\" `f` to call. This lets you for example use `f=functools.partial(some_method, some_context)`\n             to pass data down the recursive calls without using globals or other mutable state.\n\n    Returns:\n        core_schema.CoreSchema: A processed CoreSchema.\n    \"\"\"\n    return f(schema.copy(), _dispatch)\n\n\ndef simplify_schema_references(schema: core_schema.CoreSchema) -> core_schema.CoreSchema:  # noqa: C901\n    definitions: dict[str, core_schema.CoreSchema] = {}\n    ref_counts: dict[str, int] = defaultdict(int)\n    involved_in_recursion: dict[str, bool] = {}\n    current_recursion_ref_count: dict[str, int] = defaultdict(int)\n\n    def collect_refs(s: core_schema.CoreSchema, recurse: Recurse) -> core_schema.CoreSchema:\n        if s['type'] == 'definitions':\n            for definition in s['definitions']:\n                ref = get_ref(definition)\n                assert ref is not None\n                if ref not in definitions:\n                    definitions[ref] = definition\n                recurse(definition, collect_refs)\n            return recurse(s['schema'], collect_refs)\n        else:\n            ref = get_ref(s)\n            if ref is not None:\n                new = recurse(s, collect_refs)\n                new_ref = get_ref(new)\n                if new_ref:\n                    definitions[new_ref] = new\n                return core_schema.definition_reference_schema(schema_ref=ref)\n            else:\n                return recurse(s, collect_refs)\n\n    schema = walk_core_schema(schema, collect_refs)\n\n    def count_refs(s: core_schema.CoreSchema, recurse: Recurse) -> core_schema.CoreSchema:\n        if s['type'] != 'definition-ref':\n            return recurse(s, count_refs)\n        ref = s['schema_ref']\n        ref_counts[ref] += 1\n\n        if ref_counts[ref] >= 2:\n            # If this model is involved in a recursion this should be detected\n            # on its second encounter, we can safely stop the walk here.\n            if current_recursion_ref_count[ref] != 0:\n                involved_in_recursion[ref] = True\n            return s\n\n        current_recursion_ref_count[ref] += 1\n        recurse(definitions[ref], count_refs)\n        current_recursion_ref_count[ref] -= 1\n        return s\n\n    schema = walk_core_schema(schema, count_refs)\n\n    assert all(c == 0 for c in current_recursion_ref_count.values()), 'this is a bug! please report it'\n\n    def can_be_inlined(s: core_schema.DefinitionReferenceSchema, ref: str) -> bool:\n        if ref_counts[ref] > 1:\n            return False\n        if involved_in_recursion.get(ref, False):\n            return False\n        if 'serialization' in s:\n            return False\n        if 'metadata' in s:\n            metadata = s['metadata']\n            for k in (\n                'pydantic_js_functions',\n                'pydantic_js_annotation_functions',\n                'pydantic.internal.union_discriminator',\n            ):\n                if k in metadata:\n                    # we need to keep this as a ref\n                    return False\n        return True\n\n    def inline_refs(s: core_schema.CoreSchema, recurse: Recurse) -> core_schema.CoreSchema:\n        if s['type'] == 'definition-ref':\n            ref = s['schema_ref']\n            # Check if the reference is only used once, not involved in recursion and does not have\n            # any extra keys (like 'serialization')\n            if can_be_inlined(s, ref):\n                # Inline the reference by replacing the reference with the actual schema\n                new = definitions.pop(ref)\n                ref_counts[ref] -= 1  # because we just replaced it!\n                # put all other keys that were on the def-ref schema into the inlined version\n                # in particular this is needed for `serialization`\n                if 'serialization' in s:\n                    new['serialization'] = s['serialization']\n                s = recurse(new, inline_refs)\n                return s\n            else:\n                return recurse(s, inline_refs)\n        else:\n            return recurse(s, inline_refs)\n\n    schema = walk_core_schema(schema, inline_refs)\n\n    def_values = [v for v in definitions.values() if ref_counts[v['ref']] > 0]  # type: ignore\n\n    if def_values:\n        schema = core_schema.definitions_schema(schema=schema, definitions=def_values)\n    return schema\n\n\ndef _strip_metadata(schema: CoreSchema) -> CoreSchema:\n    def strip_metadata(s: CoreSchema, recurse: Recurse) -> CoreSchema:\n        s = s.copy()\n        s.pop('metadata', None)\n        if s['type'] == 'model-fields':\n            s = s.copy()\n            s['fields'] = {k: v.copy() for k, v in s['fields'].items()}\n            for field_name, field_schema in s['fields'].items():\n                field_schema.pop('metadata', None)\n                s['fields'][field_name] = field_schema\n            computed_fields = s.get('computed_fields', None)\n            if computed_fields:\n                s['computed_fields'] = [cf.copy() for cf in computed_fields]\n                for cf in computed_fields:\n                    cf.pop('metadata', None)\n            else:\n                s.pop('computed_fields', None)\n        elif s['type'] == 'model':\n            # remove some defaults\n            if s.get('custom_init', True) is False:\n                s.pop('custom_init')\n            if s.get('root_model', True) is False:\n                s.pop('root_model')\n            if {'title'}.issuperset(s.get('config', {}).keys()):\n                s.pop('config', None)\n\n        return recurse(s, strip_metadata)\n\n    return walk_core_schema(schema, strip_metadata)\n\n\ndef pretty_print_core_schema(\n    schema: CoreSchema,\n    include_metadata: bool = False,\n) -> None:\n    \"\"\"Pretty print a CoreSchema using rich.\n    This is intended for debugging purposes.\n\n    Args:\n        schema: The CoreSchema to print.\n        include_metadata: Whether to include metadata in the output. Defaults to `False`.\n    \"\"\"\n    from rich import print  # type: ignore  # install it manually in your dev env\n\n    if not include_metadata:\n        schema = _strip_metadata(schema)\n\n    return print(schema)\n\n\ndef validate_core_schema(schema: CoreSchema) -> CoreSchema:\n    if 'PYDANTIC_SKIP_VALIDATING_CORE_SCHEMAS' in os.environ:\n        return schema\n    return _validate_core_schema(schema)\n", "pydantic/_internal/_generics.py": "from __future__ import annotations\n\nimport sys\nimport types\nimport typing\nfrom collections import ChainMap\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom types import prepare_class\nfrom typing import TYPE_CHECKING, Any, Iterator, List, Mapping, MutableMapping, Tuple, TypeVar\nfrom weakref import WeakValueDictionary\n\nimport typing_extensions\n\nfrom ._core_utils import get_type_ref\nfrom ._forward_ref import PydanticRecursiveRef\nfrom ._typing_extra import TypeVarType, typing_base\nfrom ._utils import all_identical, is_model_class\n\nif sys.version_info >= (3, 10):\n    from typing import _UnionGenericAlias  # type: ignore[attr-defined]\n\nif TYPE_CHECKING:\n    from ..main import BaseModel\n\nGenericTypesCacheKey = Tuple[Any, Any, Tuple[Any, ...]]\n\n# Note: We want to remove LimitedDict, but to do this, we'd need to improve the handling of generics caching.\n#   Right now, to handle recursive generics, we some types must remain cached for brief periods without references.\n#   By chaining the WeakValuesDict with a LimitedDict, we have a way to retain caching for all types with references,\n#   while also retaining a limited number of types even without references. This is generally enough to build\n#   specific recursive generic models without losing required items out of the cache.\n\nKT = TypeVar('KT')\nVT = TypeVar('VT')\n_LIMITED_DICT_SIZE = 100\nif TYPE_CHECKING:\n\n    class LimitedDict(dict, MutableMapping[KT, VT]):\n        def __init__(self, size_limit: int = _LIMITED_DICT_SIZE): ...\n\nelse:\n\n    class LimitedDict(dict):\n        \"\"\"Limit the size/length of a dict used for caching to avoid unlimited increase in memory usage.\n\n        Since the dict is ordered, and we always remove elements from the beginning, this is effectively a FIFO cache.\n        \"\"\"\n\n        def __init__(self, size_limit: int = _LIMITED_DICT_SIZE):\n            self.size_limit = size_limit\n            super().__init__()\n\n        def __setitem__(self, key: Any, value: Any, /) -> None:\n            super().__setitem__(key, value)\n            if len(self) > self.size_limit:\n                excess = len(self) - self.size_limit + self.size_limit // 10\n                to_remove = list(self.keys())[:excess]\n                for k in to_remove:\n                    del self[k]\n\n\n# weak dictionaries allow the dynamically created parametrized versions of generic models to get collected\n# once they are no longer referenced by the caller.\nif sys.version_info >= (3, 9):  # Typing for weak dictionaries available at 3.9\n    GenericTypesCache = WeakValueDictionary[GenericTypesCacheKey, 'type[BaseModel]']\nelse:\n    GenericTypesCache = WeakValueDictionary\n\nif TYPE_CHECKING:\n\n    class DeepChainMap(ChainMap[KT, VT]):  # type: ignore\n        ...\n\nelse:\n\n    class DeepChainMap(ChainMap):\n        \"\"\"Variant of ChainMap that allows direct updates to inner scopes.\n\n        Taken from https://docs.python.org/3/library/collections.html#collections.ChainMap,\n        with some light modifications for this use case.\n        \"\"\"\n\n        def clear(self) -> None:\n            for mapping in self.maps:\n                mapping.clear()\n\n        def __setitem__(self, key: KT, value: VT) -> None:\n            for mapping in self.maps:\n                mapping[key] = value\n\n        def __delitem__(self, key: KT) -> None:\n            hit = False\n            for mapping in self.maps:\n                if key in mapping:\n                    del mapping[key]\n                    hit = True\n            if not hit:\n                raise KeyError(key)\n\n\n# Despite the fact that LimitedDict _seems_ no longer necessary, I'm very nervous to actually remove it\n# and discover later on that we need to re-add all this infrastructure...\n# _GENERIC_TYPES_CACHE = DeepChainMap(GenericTypesCache(), LimitedDict())\n\n_GENERIC_TYPES_CACHE = GenericTypesCache()\n\n\nclass PydanticGenericMetadata(typing_extensions.TypedDict):\n    origin: type[BaseModel] | None  # analogous to typing._GenericAlias.__origin__\n    args: tuple[Any, ...]  # analogous to typing._GenericAlias.__args__\n    parameters: tuple[type[Any], ...]  # analogous to typing.Generic.__parameters__\n\n\ndef create_generic_submodel(\n    model_name: str, origin: type[BaseModel], args: tuple[Any, ...], params: tuple[Any, ...]\n) -> type[BaseModel]:\n    \"\"\"Dynamically create a submodel of a provided (generic) BaseModel.\n\n    This is used when producing concrete parametrizations of generic models. This function\n    only *creates* the new subclass; the schema/validators/serialization must be updated to\n    reflect a concrete parametrization elsewhere.\n\n    Args:\n        model_name: The name of the newly created model.\n        origin: The base class for the new model to inherit from.\n        args: A tuple of generic metadata arguments.\n        params: A tuple of generic metadata parameters.\n\n    Returns:\n        The created submodel.\n    \"\"\"\n    namespace: dict[str, Any] = {'__module__': origin.__module__}\n    bases = (origin,)\n    meta, ns, kwds = prepare_class(model_name, bases)\n    namespace.update(ns)\n    created_model = meta(\n        model_name,\n        bases,\n        namespace,\n        __pydantic_generic_metadata__={\n            'origin': origin,\n            'args': args,\n            'parameters': params,\n        },\n        __pydantic_reset_parent_namespace__=False,\n        **kwds,\n    )\n\n    model_module, called_globally = _get_caller_frame_info(depth=3)\n    if called_globally:  # create global reference and therefore allow pickling\n        object_by_reference = None\n        reference_name = model_name\n        reference_module_globals = sys.modules[created_model.__module__].__dict__\n        while object_by_reference is not created_model:\n            object_by_reference = reference_module_globals.setdefault(reference_name, created_model)\n            reference_name += '_'\n\n    return created_model\n\n\ndef _get_caller_frame_info(depth: int = 2) -> tuple[str | None, bool]:\n    \"\"\"Used inside a function to check whether it was called globally.\n\n    Args:\n        depth: The depth to get the frame.\n\n    Returns:\n        A tuple contains `module_name` and `called_globally`.\n\n    Raises:\n        RuntimeError: If the function is not called inside a function.\n    \"\"\"\n    try:\n        previous_caller_frame = sys._getframe(depth)\n    except ValueError as e:\n        raise RuntimeError('This function must be used inside another function') from e\n    except AttributeError:  # sys module does not have _getframe function, so there's nothing we can do about it\n        return None, False\n    frame_globals = previous_caller_frame.f_globals\n    return frame_globals.get('__name__'), previous_caller_frame.f_locals is frame_globals\n\n\nDictValues: type[Any] = {}.values().__class__\n\n\ndef iter_contained_typevars(v: Any) -> Iterator[TypeVarType]:\n    \"\"\"Recursively iterate through all subtypes and type args of `v` and yield any typevars that are found.\n\n    This is inspired as an alternative to directly accessing the `__parameters__` attribute of a GenericAlias,\n    since __parameters__ of (nested) generic BaseModel subclasses won't show up in that list.\n    \"\"\"\n    if isinstance(v, TypeVar):\n        yield v\n    elif is_model_class(v):\n        yield from v.__pydantic_generic_metadata__['parameters']\n    elif isinstance(v, (DictValues, list)):\n        for var in v:\n            yield from iter_contained_typevars(var)\n    else:\n        args = get_args(v)\n        for arg in args:\n            yield from iter_contained_typevars(arg)\n\n\ndef get_args(v: Any) -> Any:\n    pydantic_generic_metadata: PydanticGenericMetadata | None = getattr(v, '__pydantic_generic_metadata__', None)\n    if pydantic_generic_metadata:\n        return pydantic_generic_metadata.get('args')\n    return typing_extensions.get_args(v)\n\n\ndef get_origin(v: Any) -> Any:\n    pydantic_generic_metadata: PydanticGenericMetadata | None = getattr(v, '__pydantic_generic_metadata__', None)\n    if pydantic_generic_metadata:\n        return pydantic_generic_metadata.get('origin')\n    return typing_extensions.get_origin(v)\n\n\ndef get_standard_typevars_map(cls: type[Any]) -> dict[TypeVarType, Any] | None:\n    \"\"\"Package a generic type's typevars and parametrization (if present) into a dictionary compatible with the\n    `replace_types` function. Specifically, this works with standard typing generics and typing._GenericAlias.\n    \"\"\"\n    origin = get_origin(cls)\n    if origin is None:\n        return None\n    if not hasattr(origin, '__parameters__'):\n        return None\n\n    # In this case, we know that cls is a _GenericAlias, and origin is the generic type\n    # So it is safe to access cls.__args__ and origin.__parameters__\n    args: tuple[Any, ...] = cls.__args__  # type: ignore\n    parameters: tuple[TypeVarType, ...] = origin.__parameters__\n    return dict(zip(parameters, args))\n\n\ndef get_model_typevars_map(cls: type[BaseModel]) -> dict[TypeVarType, Any] | None:\n    \"\"\"Package a generic BaseModel's typevars and concrete parametrization (if present) into a dictionary compatible\n    with the `replace_types` function.\n\n    Since BaseModel.__class_getitem__ does not produce a typing._GenericAlias, and the BaseModel generic info is\n    stored in the __pydantic_generic_metadata__ attribute, we need special handling here.\n    \"\"\"\n    # TODO: This could be unified with `get_standard_typevars_map` if we stored the generic metadata\n    #   in the __origin__, __args__, and __parameters__ attributes of the model.\n    generic_metadata = cls.__pydantic_generic_metadata__\n    origin = generic_metadata['origin']\n    args = generic_metadata['args']\n    return dict(zip(iter_contained_typevars(origin), args))\n\n\ndef replace_types(type_: Any, type_map: Mapping[Any, Any] | None) -> Any:\n    \"\"\"Return type with all occurrences of `type_map` keys recursively replaced with their values.\n\n    Args:\n        type_: The class or generic alias.\n        type_map: Mapping from `TypeVar` instance to concrete types.\n\n    Returns:\n        A new type representing the basic structure of `type_` with all\n        `typevar_map` keys recursively replaced.\n\n    Example:\n        ```py\n        from typing import List, Tuple, Union\n\n        from pydantic._internal._generics import replace_types\n\n        replace_types(Tuple[str, Union[List[str], float]], {str: int})\n        #> Tuple[int, Union[List[int], float]]\n        ```\n    \"\"\"\n    if not type_map:\n        return type_\n\n    type_args = get_args(type_)\n    origin_type = get_origin(type_)\n\n    if origin_type is typing_extensions.Annotated:\n        annotated_type, *annotations = type_args\n        annotated = replace_types(annotated_type, type_map)\n        for annotation in annotations:\n            annotated = typing_extensions.Annotated[annotated, annotation]\n        return annotated\n\n    # Having type args is a good indicator that this is a typing module\n    # class instantiation or a generic alias of some sort.\n    if type_args:\n        resolved_type_args = tuple(replace_types(arg, type_map) for arg in type_args)\n        if all_identical(type_args, resolved_type_args):\n            # If all arguments are the same, there is no need to modify the\n            # type or create a new object at all\n            return type_\n        if (\n            origin_type is not None\n            and isinstance(type_, typing_base)\n            and not isinstance(origin_type, typing_base)\n            and getattr(type_, '_name', None) is not None\n        ):\n            # In python < 3.9 generic aliases don't exist so any of these like `list`,\n            # `type` or `collections.abc.Callable` need to be translated.\n            # See: https://www.python.org/dev/peps/pep-0585\n            origin_type = getattr(typing, type_._name)\n        assert origin_type is not None\n        # PEP-604 syntax (Ex.: list | str) is represented with a types.UnionType object that does not have __getitem__.\n        # We also cannot use isinstance() since we have to compare types.\n        if sys.version_info >= (3, 10) and origin_type is types.UnionType:\n            return _UnionGenericAlias(origin_type, resolved_type_args)\n        # NotRequired[T] and Required[T] don't support tuple type resolved_type_args, hence the condition below\n        return origin_type[resolved_type_args[0] if len(resolved_type_args) == 1 else resolved_type_args]\n\n    # We handle pydantic generic models separately as they don't have the same\n    # semantics as \"typing\" classes or generic aliases\n\n    if not origin_type and is_model_class(type_):\n        parameters = type_.__pydantic_generic_metadata__['parameters']\n        if not parameters:\n            return type_\n        resolved_type_args = tuple(replace_types(t, type_map) for t in parameters)\n        if all_identical(parameters, resolved_type_args):\n            return type_\n        return type_[resolved_type_args]\n\n    # Handle special case for typehints that can have lists as arguments.\n    # `typing.Callable[[int, str], int]` is an example for this.\n    if isinstance(type_, (List, list)):\n        resolved_list = list(replace_types(element, type_map) for element in type_)\n        if all_identical(type_, resolved_list):\n            return type_\n        return resolved_list\n\n    # If all else fails, we try to resolve the type directly and otherwise just\n    # return the input with no modifications.\n    return type_map.get(type_, type_)\n\n\ndef has_instance_in_type(type_: Any, isinstance_target: Any) -> bool:\n    \"\"\"Checks if the type, or any of its arbitrary nested args, satisfy\n    `isinstance(<type>, isinstance_target)`.\n    \"\"\"\n    if isinstance(type_, isinstance_target):\n        return True\n\n    type_args = get_args(type_)\n    origin_type = get_origin(type_)\n\n    if origin_type is typing_extensions.Annotated:\n        annotated_type, *annotations = type_args\n        return has_instance_in_type(annotated_type, isinstance_target)\n\n    # Having type args is a good indicator that this is a typing module\n    # class instantiation or a generic alias of some sort.\n    if any(has_instance_in_type(a, isinstance_target) for a in type_args):\n        return True\n\n    # Handle special case for typehints that can have lists as arguments.\n    # `typing.Callable[[int, str], int]` is an example for this.\n    if isinstance(type_, (List, list)) and not isinstance(type_, typing_extensions.ParamSpec):\n        if any(has_instance_in_type(element, isinstance_target) for element in type_):\n            return True\n\n    return False\n\n\ndef check_parameters_count(cls: type[BaseModel], parameters: tuple[Any, ...]) -> None:\n    \"\"\"Check the generic model parameters count is equal.\n\n    Args:\n        cls: The generic model.\n        parameters: A tuple of passed parameters to the generic model.\n\n    Raises:\n        TypeError: If the passed parameters count is not equal to generic model parameters count.\n    \"\"\"\n    actual = len(parameters)\n    expected = len(cls.__pydantic_generic_metadata__['parameters'])\n    if actual != expected:\n        description = 'many' if actual > expected else 'few'\n        raise TypeError(f'Too {description} parameters for {cls}; actual {actual}, expected {expected}')\n\n\n_generic_recursion_cache: ContextVar[set[str] | None] = ContextVar('_generic_recursion_cache', default=None)\n\n\n@contextmanager\ndef generic_recursion_self_type(\n    origin: type[BaseModel], args: tuple[Any, ...]\n) -> Iterator[PydanticRecursiveRef | None]:\n    \"\"\"This contextmanager should be placed around the recursive calls used to build a generic type,\n    and accept as arguments the generic origin type and the type arguments being passed to it.\n\n    If the same origin and arguments are observed twice, it implies that a self-reference placeholder\n    can be used while building the core schema, and will produce a schema_ref that will be valid in the\n    final parent schema.\n    \"\"\"\n    previously_seen_type_refs = _generic_recursion_cache.get()\n    if previously_seen_type_refs is None:\n        previously_seen_type_refs = set()\n        token = _generic_recursion_cache.set(previously_seen_type_refs)\n    else:\n        token = None\n\n    try:\n        type_ref = get_type_ref(origin, args_override=args)\n        if type_ref in previously_seen_type_refs:\n            self_type = PydanticRecursiveRef(type_ref=type_ref)\n            yield self_type\n        else:\n            previously_seen_type_refs.add(type_ref)\n            yield None\n    finally:\n        if token:\n            _generic_recursion_cache.reset(token)\n\n\ndef recursively_defined_type_refs() -> set[str]:\n    visited = _generic_recursion_cache.get()\n    if not visited:\n        return set()  # not in a generic recursion, so there are no types\n\n    return visited.copy()  # don't allow modifications\n\n\ndef get_cached_generic_type_early(parent: type[BaseModel], typevar_values: Any) -> type[BaseModel] | None:\n    \"\"\"The use of a two-stage cache lookup approach was necessary to have the highest performance possible for\n    repeated calls to `__class_getitem__` on generic types (which may happen in tighter loops during runtime),\n    while still ensuring that certain alternative parametrizations ultimately resolve to the same type.\n\n    As a concrete example, this approach was necessary to make Model[List[T]][int] equal to Model[List[int]].\n    The approach could be modified to not use two different cache keys at different points, but the\n    _early_cache_key is optimized to be as quick to compute as possible (for repeated-access speed), and the\n    _late_cache_key is optimized to be as \"correct\" as possible, so that two types that will ultimately be the\n    same after resolving the type arguments will always produce cache hits.\n\n    If we wanted to move to only using a single cache key per type, we would either need to always use the\n    slower/more computationally intensive logic associated with _late_cache_key, or would need to accept\n    that Model[List[T]][int] is a different type than Model[List[T]][int]. Because we rely on subclass relationships\n    during validation, I think it is worthwhile to ensure that types that are functionally equivalent are actually\n    equal.\n    \"\"\"\n    return _GENERIC_TYPES_CACHE.get(_early_cache_key(parent, typevar_values))\n\n\ndef get_cached_generic_type_late(\n    parent: type[BaseModel], typevar_values: Any, origin: type[BaseModel], args: tuple[Any, ...]\n) -> type[BaseModel] | None:\n    \"\"\"See the docstring of `get_cached_generic_type_early` for more information about the two-stage cache lookup.\"\"\"\n    cached = _GENERIC_TYPES_CACHE.get(_late_cache_key(origin, args, typevar_values))\n    if cached is not None:\n        set_cached_generic_type(parent, typevar_values, cached, origin, args)\n    return cached\n\n\ndef set_cached_generic_type(\n    parent: type[BaseModel],\n    typevar_values: tuple[Any, ...],\n    type_: type[BaseModel],\n    origin: type[BaseModel] | None = None,\n    args: tuple[Any, ...] | None = None,\n) -> None:\n    \"\"\"See the docstring of `get_cached_generic_type_early` for more information about why items are cached with\n    two different keys.\n    \"\"\"\n    _GENERIC_TYPES_CACHE[_early_cache_key(parent, typevar_values)] = type_\n    if len(typevar_values) == 1:\n        _GENERIC_TYPES_CACHE[_early_cache_key(parent, typevar_values[0])] = type_\n    if origin and args:\n        _GENERIC_TYPES_CACHE[_late_cache_key(origin, args, typevar_values)] = type_\n\n\ndef _union_orderings_key(typevar_values: Any) -> Any:\n    \"\"\"This is intended to help differentiate between Union types with the same arguments in different order.\n\n    Thanks to caching internal to the `typing` module, it is not possible to distinguish between\n    List[Union[int, float]] and List[Union[float, int]] (and similarly for other \"parent\" origins besides List)\n    because `typing` considers Union[int, float] to be equal to Union[float, int].\n\n    However, you _can_ distinguish between (top-level) Union[int, float] vs. Union[float, int].\n    Because we parse items as the first Union type that is successful, we get slightly more consistent behavior\n    if we make an effort to distinguish the ordering of items in a union. It would be best if we could _always_\n    get the exact-correct order of items in the union, but that would require a change to the `typing` module itself.\n    (See https://github.com/python/cpython/issues/86483 for reference.)\n    \"\"\"\n    if isinstance(typevar_values, tuple):\n        args_data = []\n        for value in typevar_values:\n            args_data.append(_union_orderings_key(value))\n        return tuple(args_data)\n    elif typing_extensions.get_origin(typevar_values) is typing.Union:\n        return get_args(typevar_values)\n    else:\n        return ()\n\n\ndef _early_cache_key(cls: type[BaseModel], typevar_values: Any) -> GenericTypesCacheKey:\n    \"\"\"This is intended for minimal computational overhead during lookups of cached types.\n\n    Note that this is overly simplistic, and it's possible that two different cls/typevar_values\n    inputs would ultimately result in the same type being created in BaseModel.__class_getitem__.\n    To handle this, we have a fallback _late_cache_key that is checked later if the _early_cache_key\n    lookup fails, and should result in a cache hit _precisely_ when the inputs to __class_getitem__\n    would result in the same type.\n    \"\"\"\n    return cls, typevar_values, _union_orderings_key(typevar_values)\n\n\ndef _late_cache_key(origin: type[BaseModel], args: tuple[Any, ...], typevar_values: Any) -> GenericTypesCacheKey:\n    \"\"\"This is intended for use later in the process of creating a new type, when we have more information\n    about the exact args that will be passed. If it turns out that a different set of inputs to\n    __class_getitem__ resulted in the same inputs to the generic type creation process, we can still\n    return the cached type, and update the cache with the _early_cache_key as well.\n    \"\"\"\n    # The _union_orderings_key is placed at the start here to ensure there cannot be a collision with an\n    # _early_cache_key, as that function will always produce a BaseModel subclass as the first item in the key,\n    # whereas this function will always produce a tuple as the first item in the key.\n    return _union_orderings_key(typevar_values), origin, args\n", "pydantic/_internal/_model_construction.py": "\"\"\"Private logic for creating models.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport builtins\nimport operator\nimport typing\nimport warnings\nimport weakref\nfrom abc import ABCMeta\nfrom functools import partial\nfrom types import FunctionType\nfrom typing import Any, Callable, Generic, NoReturn\n\nimport typing_extensions\nfrom pydantic_core import PydanticUndefined, SchemaSerializer\nfrom typing_extensions import dataclass_transform, deprecated\n\nfrom ..errors import PydanticUndefinedAnnotation, PydanticUserError\nfrom ..plugin._schema_validator import create_schema_validator\nfrom ..warnings import GenericBeforeBaseModelWarning, PydanticDeprecatedSince20\nfrom ._config import ConfigWrapper\nfrom ._decorators import DecoratorInfos, PydanticDescriptorProxy, get_attribute_from_bases, unwrap_wrapped_function\nfrom ._fields import collect_model_fields, is_valid_field_name, is_valid_privateattr_name\nfrom ._generate_schema import GenerateSchema\nfrom ._generics import PydanticGenericMetadata, get_model_typevars_map\nfrom ._mock_val_ser import set_model_mocks\nfrom ._schema_generation_shared import CallbackGetCoreSchemaHandler\nfrom ._signature import generate_pydantic_signature\nfrom ._typing_extra import get_cls_types_namespace, is_annotated, is_classvar, parent_frame_namespace\nfrom ._utils import ClassAttribute, SafeGetItemProxy\nfrom ._validate_call import ValidateCallWrapper\n\nif typing.TYPE_CHECKING:\n    from ..fields import Field as PydanticModelField\n    from ..fields import FieldInfo, ModelPrivateAttr\n    from ..fields import PrivateAttr as PydanticModelPrivateAttr\n    from ..main import BaseModel\nelse:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n    PydanticModelField = object()\n    PydanticModelPrivateAttr = object()\n\nobject_setattr = object.__setattr__\n\n\nclass _ModelNamespaceDict(dict):\n    \"\"\"A dictionary subclass that intercepts attribute setting on model classes and\n    warns about overriding of decorators.\n    \"\"\"\n\n    def __setitem__(self, k: str, v: object) -> None:\n        existing: Any = self.get(k, None)\n        if existing and v is not existing and isinstance(existing, PydanticDescriptorProxy):\n            warnings.warn(f'`{k}` overrides an existing Pydantic `{existing.decorator_info.decorator_repr}` decorator')\n\n        return super().__setitem__(k, v)\n\n\n@dataclass_transform(kw_only_default=True, field_specifiers=(PydanticModelField, PydanticModelPrivateAttr))\nclass ModelMetaclass(ABCMeta):\n    def __new__(\n        mcs,\n        cls_name: str,\n        bases: tuple[type[Any], ...],\n        namespace: dict[str, Any],\n        __pydantic_generic_metadata__: PydanticGenericMetadata | None = None,\n        __pydantic_reset_parent_namespace__: bool = True,\n        _create_model_module: str | None = None,\n        **kwargs: Any,\n    ) -> type:\n        \"\"\"Metaclass for creating Pydantic models.\n\n        Args:\n            cls_name: The name of the class to be created.\n            bases: The base classes of the class to be created.\n            namespace: The attribute dictionary of the class to be created.\n            __pydantic_generic_metadata__: Metadata for generic models.\n            __pydantic_reset_parent_namespace__: Reset parent namespace.\n            _create_model_module: The module of the class to be created, if created by `create_model`.\n            **kwargs: Catch-all for any other keyword arguments.\n\n        Returns:\n            The new class created by the metaclass.\n        \"\"\"\n        # Note `ModelMetaclass` refers to `BaseModel`, but is also used to *create* `BaseModel`, so we rely on the fact\n        # that `BaseModel` itself won't have any bases, but any subclass of it will, to determine whether the `__new__`\n        # call we're in the middle of is for the `BaseModel` class.\n        if bases:\n            base_field_names, class_vars, base_private_attributes = mcs._collect_bases_data(bases)\n\n            config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)\n            namespace['model_config'] = config_wrapper.config_dict\n            private_attributes = inspect_namespace(\n                namespace, config_wrapper.ignored_types, class_vars, base_field_names\n            )\n            if private_attributes or base_private_attributes:\n                original_model_post_init = get_model_post_init(namespace, bases)\n                if original_model_post_init is not None:\n                    # if there are private_attributes and a model_post_init function, we handle both\n\n                    def wrapped_model_post_init(self: BaseModel, context: Any, /) -> None:\n                        \"\"\"We need to both initialize private attributes and call the user-defined model_post_init\n                        method.\n                        \"\"\"\n                        init_private_attributes(self, context)\n                        original_model_post_init(self, context)\n\n                    namespace['model_post_init'] = wrapped_model_post_init\n                else:\n                    namespace['model_post_init'] = init_private_attributes\n\n            namespace['__class_vars__'] = class_vars\n            namespace['__private_attributes__'] = {**base_private_attributes, **private_attributes}\n\n            cls: type[BaseModel] = super().__new__(mcs, cls_name, bases, namespace, **kwargs)  # type: ignore\n\n            from ..main import BaseModel\n\n            mro = cls.__mro__\n            if Generic in mro and mro.index(Generic) < mro.index(BaseModel):\n                warnings.warn(\n                    GenericBeforeBaseModelWarning(\n                        'Classes should inherit from `BaseModel` before generic classes (e.g. `typing.Generic[T]`) '\n                        'for pydantic generics to work properly.'\n                    ),\n                    stacklevel=2,\n                )\n\n            cls.__pydantic_custom_init__ = not getattr(cls.__init__, '__pydantic_base_init__', False)\n            cls.__pydantic_post_init__ = None if cls.model_post_init is BaseModel.model_post_init else 'model_post_init'\n\n            cls.__pydantic_decorators__ = DecoratorInfos.build(cls)\n\n            # Use the getattr below to grab the __parameters__ from the `typing.Generic` parent class\n            if __pydantic_generic_metadata__:\n                cls.__pydantic_generic_metadata__ = __pydantic_generic_metadata__\n            else:\n                parent_parameters = getattr(cls, '__pydantic_generic_metadata__', {}).get('parameters', ())\n                parameters = getattr(cls, '__parameters__', None) or parent_parameters\n                if parameters and parent_parameters and not all(x in parameters for x in parent_parameters):\n                    from ..root_model import RootModelRootType\n\n                    missing_parameters = tuple(x for x in parameters if x not in parent_parameters)\n                    if RootModelRootType in parent_parameters and RootModelRootType not in parameters:\n                        # This is a special case where the user has subclassed `RootModel`, but has not parametrized\n                        # RootModel with the generic type identifiers being used. Ex:\n                        # class MyModel(RootModel, Generic[T]):\n                        #    root: T\n                        # Should instead just be:\n                        # class MyModel(RootModel[T]):\n                        #   root: T\n                        parameters_str = ', '.join([x.__name__ for x in missing_parameters])\n                        error_message = (\n                            f'{cls.__name__} is a subclass of `RootModel`, but does not include the generic type identifier(s) '\n                            f'{parameters_str} in its parameters. '\n                            f'You should parametrize RootModel directly, e.g., `class {cls.__name__}(RootModel[{parameters_str}]): ...`.'\n                        )\n                    else:\n                        combined_parameters = parent_parameters + missing_parameters\n                        parameters_str = ', '.join([str(x) for x in combined_parameters])\n                        generic_type_label = f'typing.Generic[{parameters_str}]'\n                        error_message = (\n                            f'All parameters must be present on typing.Generic;'\n                            f' you should inherit from {generic_type_label}.'\n                        )\n                        if Generic not in bases:  # pragma: no cover\n                            # We raise an error here not because it is desirable, but because some cases are mishandled.\n                            # It would be nice to remove this error and still have things behave as expected, it's just\n                            # challenging because we are using a custom `__class_getitem__` to parametrize generic models,\n                            # and not returning a typing._GenericAlias from it.\n                            bases_str = ', '.join([x.__name__ for x in bases] + [generic_type_label])\n                            error_message += (\n                                f' Note: `typing.Generic` must go last: `class {cls.__name__}({bases_str}): ...`)'\n                            )\n                    raise TypeError(error_message)\n\n                cls.__pydantic_generic_metadata__ = {\n                    'origin': None,\n                    'args': (),\n                    'parameters': parameters,\n                }\n\n            cls.__pydantic_complete__ = False  # Ensure this specific class gets completed\n\n            # preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\n            # for attributes not in `new_namespace` (e.g. private attributes)\n            for name, obj in private_attributes.items():\n                obj.__set_name__(cls, name)\n\n            if __pydantic_reset_parent_namespace__:\n                cls.__pydantic_parent_namespace__ = build_lenient_weakvaluedict(parent_frame_namespace())\n            parent_namespace = getattr(cls, '__pydantic_parent_namespace__', None)\n            if isinstance(parent_namespace, dict):\n                parent_namespace = unpack_lenient_weakvaluedict(parent_namespace)\n\n            types_namespace = get_cls_types_namespace(cls, parent_namespace)\n            set_model_fields(cls, bases, config_wrapper, types_namespace)\n\n            if config_wrapper.frozen and '__hash__' not in namespace:\n                set_default_hash_func(cls, bases)\n\n            complete_model_class(\n                cls,\n                cls_name,\n                config_wrapper,\n                raise_errors=False,\n                types_namespace=types_namespace,\n                create_model_module=_create_model_module,\n            )\n\n            # If this is placed before the complete_model_class call above,\n            # the generic computed fields return type is set to PydanticUndefined\n            cls.model_computed_fields = {k: v.info for k, v in cls.__pydantic_decorators__.computed_fields.items()}\n\n            set_deprecated_descriptors(cls)\n\n            # using super(cls, cls) on the next line ensures we only call the parent class's __pydantic_init_subclass__\n            # I believe the `type: ignore` is only necessary because mypy doesn't realize that this code branch is\n            # only hit for _proper_ subclasses of BaseModel\n            super(cls, cls).__pydantic_init_subclass__(**kwargs)  # type: ignore[misc]\n            return cls\n        else:\n            # this is the BaseModel class itself being created, no logic required\n            return super().__new__(mcs, cls_name, bases, namespace, **kwargs)\n\n    if not typing.TYPE_CHECKING:  # pragma: no branch\n        # We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access\n\n        def __getattr__(self, item: str) -> Any:\n            \"\"\"This is necessary to keep attribute access working for class attribute access.\"\"\"\n            private_attributes = self.__dict__.get('__private_attributes__')\n            if private_attributes and item in private_attributes:\n                return private_attributes[item]\n            raise AttributeError(item)\n\n    @classmethod\n    def __prepare__(cls, *args: Any, **kwargs: Any) -> dict[str, object]:\n        return _ModelNamespaceDict()\n\n    def __instancecheck__(self, instance: Any) -> bool:\n        \"\"\"Avoid calling ABC _abc_subclasscheck unless we're pretty sure.\n\n        See #3829 and python/cpython#92810\n        \"\"\"\n        return hasattr(instance, '__pydantic_validator__') and super().__instancecheck__(instance)\n\n    @staticmethod\n    def _collect_bases_data(bases: tuple[type[Any], ...]) -> tuple[set[str], set[str], dict[str, ModelPrivateAttr]]:\n        from ..main import BaseModel\n\n        field_names: set[str] = set()\n        class_vars: set[str] = set()\n        private_attributes: dict[str, ModelPrivateAttr] = {}\n        for base in bases:\n            if issubclass(base, BaseModel) and base is not BaseModel:\n                # model_fields might not be defined yet in the case of generics, so we use getattr here:\n                field_names.update(getattr(base, 'model_fields', {}).keys())\n                class_vars.update(base.__class_vars__)\n                private_attributes.update(base.__private_attributes__)\n        return field_names, class_vars, private_attributes\n\n    @property\n    @deprecated('The `__fields__` attribute is deprecated, use `model_fields` instead.', category=None)\n    def __fields__(self) -> dict[str, FieldInfo]:\n        warnings.warn(\n            'The `__fields__` attribute is deprecated, use `model_fields` instead.', PydanticDeprecatedSince20\n        )\n        return self.model_fields  # type: ignore\n\n    def __dir__(self) -> list[str]:\n        attributes = list(super().__dir__())\n        if '__fields__' in attributes:\n            attributes.remove('__fields__')\n        return attributes\n\n\ndef init_private_attributes(self: BaseModel, context: Any, /) -> None:\n    \"\"\"This function is meant to behave like a BaseModel method to initialise private attributes.\n\n    It takes context as an argument since that's what pydantic-core passes when calling it.\n\n    Args:\n        self: The BaseModel instance.\n        context: The context.\n    \"\"\"\n    if getattr(self, '__pydantic_private__', None) is None:\n        pydantic_private = {}\n        for name, private_attr in self.__private_attributes__.items():\n            default = private_attr.get_default()\n            if default is not PydanticUndefined:\n                pydantic_private[name] = default\n        object_setattr(self, '__pydantic_private__', pydantic_private)\n\n\ndef get_model_post_init(namespace: dict[str, Any], bases: tuple[type[Any], ...]) -> Callable[..., Any] | None:\n    \"\"\"Get the `model_post_init` method from the namespace or the class bases, or `None` if not defined.\"\"\"\n    if 'model_post_init' in namespace:\n        return namespace['model_post_init']\n\n    from ..main import BaseModel\n\n    model_post_init = get_attribute_from_bases(bases, 'model_post_init')\n    if model_post_init is not BaseModel.model_post_init:\n        return model_post_init\n\n\ndef inspect_namespace(  # noqa C901\n    namespace: dict[str, Any],\n    ignored_types: tuple[type[Any], ...],\n    base_class_vars: set[str],\n    base_class_fields: set[str],\n) -> dict[str, ModelPrivateAttr]:\n    \"\"\"Iterate over the namespace and:\n    * gather private attributes\n    * check for items which look like fields but are not (e.g. have no annotation) and warn.\n\n    Args:\n        namespace: The attribute dictionary of the class to be created.\n        ignored_types: A tuple of ignore types.\n        base_class_vars: A set of base class class variables.\n        base_class_fields: A set of base class fields.\n\n    Returns:\n        A dict contains private attributes info.\n\n    Raises:\n        TypeError: If there is a `__root__` field in model.\n        NameError: If private attribute name is invalid.\n        PydanticUserError:\n            - If a field does not have a type annotation.\n            - If a field on base class was overridden by a non-annotated attribute.\n    \"\"\"\n    from ..fields import FieldInfo, ModelPrivateAttr, PrivateAttr\n\n    all_ignored_types = ignored_types + default_ignored_types()\n\n    private_attributes: dict[str, ModelPrivateAttr] = {}\n    raw_annotations = namespace.get('__annotations__', {})\n\n    if '__root__' in raw_annotations or '__root__' in namespace:\n        raise TypeError(\"To define root models, use `pydantic.RootModel` rather than a field called '__root__'\")\n\n    ignored_names: set[str] = set()\n    for var_name, value in list(namespace.items()):\n        if var_name == 'model_config' or var_name == '__pydantic_extra__':\n            continue\n        elif (\n            isinstance(value, type)\n            and value.__module__ == namespace['__module__']\n            and '__qualname__' in namespace\n            and value.__qualname__.startswith(namespace['__qualname__'])\n        ):\n            # `value` is a nested type defined in this namespace; don't error\n            continue\n        elif isinstance(value, all_ignored_types) or value.__class__.__module__ == 'functools':\n            ignored_names.add(var_name)\n            continue\n        elif isinstance(value, ModelPrivateAttr):\n            if var_name.startswith('__'):\n                raise NameError(\n                    'Private attributes must not use dunder names;'\n                    f' use a single underscore prefix instead of {var_name!r}.'\n                )\n            elif is_valid_field_name(var_name):\n                raise NameError(\n                    'Private attributes must not use valid field names;'\n                    f' use sunder names, e.g. {\"_\" + var_name!r} instead of {var_name!r}.'\n                )\n            private_attributes[var_name] = value\n            del namespace[var_name]\n        elif isinstance(value, FieldInfo) and not is_valid_field_name(var_name):\n            suggested_name = var_name.lstrip('_') or 'my_field'  # don't suggest '' for all-underscore name\n            raise NameError(\n                f'Fields must not use names with leading underscores;'\n                f' e.g., use {suggested_name!r} instead of {var_name!r}.'\n            )\n\n        elif var_name.startswith('__'):\n            continue\n        elif is_valid_privateattr_name(var_name):\n            if var_name not in raw_annotations or not is_classvar(raw_annotations[var_name]):\n                private_attributes[var_name] = PrivateAttr(default=value)\n                del namespace[var_name]\n        elif var_name in base_class_vars:\n            continue\n        elif var_name not in raw_annotations:\n            if var_name in base_class_fields:\n                raise PydanticUserError(\n                    f'Field {var_name!r} defined on a base class was overridden by a non-annotated attribute. '\n                    f'All field definitions, including overrides, require a type annotation.',\n                    code='model-field-overridden',\n                )\n            elif isinstance(value, FieldInfo):\n                raise PydanticUserError(\n                    f'Field {var_name!r} requires a type annotation', code='model-field-missing-annotation'\n                )\n            else:\n                raise PydanticUserError(\n                    f'A non-annotated attribute was detected: `{var_name} = {value!r}`. All model fields require a '\n                    f'type annotation; if `{var_name}` is not meant to be a field, you may be able to resolve this '\n                    f\"error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\",\n                    code='model-field-missing-annotation',\n                )\n\n    for ann_name, ann_type in raw_annotations.items():\n        if (\n            is_valid_privateattr_name(ann_name)\n            and ann_name not in private_attributes\n            and ann_name not in ignored_names\n            and not is_classvar(ann_type)\n            and ann_type not in all_ignored_types\n            and getattr(ann_type, '__module__', None) != 'functools'\n        ):\n            if is_annotated(ann_type):\n                _, *metadata = typing_extensions.get_args(ann_type)\n                private_attr = next((v for v in metadata if isinstance(v, ModelPrivateAttr)), None)\n                if private_attr is not None:\n                    private_attributes[ann_name] = private_attr\n                    continue\n            private_attributes[ann_name] = PrivateAttr()\n\n    return private_attributes\n\n\ndef set_default_hash_func(cls: type[BaseModel], bases: tuple[type[Any], ...]) -> None:\n    base_hash_func = get_attribute_from_bases(bases, '__hash__')\n    new_hash_func = make_hash_func(cls)\n    if base_hash_func in {None, object.__hash__} or getattr(base_hash_func, '__code__', None) == new_hash_func.__code__:\n        # If `__hash__` is some default, we generate a hash function.\n        # It will be `None` if not overridden from BaseModel.\n        # It may be `object.__hash__` if there is another\n        # parent class earlier in the bases which doesn't override `__hash__` (e.g. `typing.Generic`).\n        # It may be a value set by `set_default_hash_func` if `cls` is a subclass of another frozen model.\n        # In the last case we still need a new hash function to account for new `model_fields`.\n        cls.__hash__ = new_hash_func\n\n\ndef make_hash_func(cls: type[BaseModel]) -> Any:\n    getter = operator.itemgetter(*cls.model_fields.keys()) if cls.model_fields else lambda _: 0\n\n    def hash_func(self: Any) -> int:\n        try:\n            return hash(getter(self.__dict__))\n        except KeyError:\n            # In rare cases (such as when using the deprecated copy method), the __dict__ may not contain\n            # all model fields, which is how we can get here.\n            # getter(self.__dict__) is much faster than any 'safe' method that accounts for missing keys,\n            # and wrapping it in a `try` doesn't slow things down much in the common case.\n            return hash(getter(SafeGetItemProxy(self.__dict__)))\n\n    return hash_func\n\n\ndef set_model_fields(\n    cls: type[BaseModel], bases: tuple[type[Any], ...], config_wrapper: ConfigWrapper, types_namespace: dict[str, Any]\n) -> None:\n    \"\"\"Collect and set `cls.model_fields` and `cls.__class_vars__`.\n\n    Args:\n        cls: BaseModel or dataclass.\n        bases: Parents of the class, generally `cls.__bases__`.\n        config_wrapper: The config wrapper instance.\n        types_namespace: Optional extra namespace to look for types in.\n    \"\"\"\n    typevars_map = get_model_typevars_map(cls)\n    fields, class_vars = collect_model_fields(cls, bases, config_wrapper, types_namespace, typevars_map=typevars_map)\n\n    cls.model_fields = fields\n    cls.__class_vars__.update(class_vars)\n\n    for k in class_vars:\n        # Class vars should not be private attributes\n        #     We remove them _here_ and not earlier because we rely on inspecting the class to determine its classvars,\n        #     but private attributes are determined by inspecting the namespace _prior_ to class creation.\n        #     In the case that a classvar with a leading-'_' is defined via a ForwardRef (e.g., when using\n        #     `__future__.annotations`), we want to remove the private attribute which was detected _before_ we knew it\n        #     evaluated to a classvar\n\n        value = cls.__private_attributes__.pop(k, None)\n        if value is not None and value.default is not PydanticUndefined:\n            setattr(cls, k, value.default)\n\n\ndef complete_model_class(\n    cls: type[BaseModel],\n    cls_name: str,\n    config_wrapper: ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    types_namespace: dict[str, Any] | None,\n    create_model_module: str | None = None,\n) -> bool:\n    \"\"\"Finish building a model class.\n\n    This logic must be called after class has been created since validation functions must be bound\n    and `get_type_hints` requires a class object.\n\n    Args:\n        cls: BaseModel or dataclass.\n        cls_name: The model or dataclass name.\n        config_wrapper: The config wrapper instance.\n        raise_errors: Whether to raise errors.\n        types_namespace: Optional extra namespace to look for types in.\n        create_model_module: The module of the class to be created, if created by `create_model`.\n\n    Returns:\n        `True` if the model is successfully completed, else `False`.\n\n    Raises:\n        PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n            and `raise_errors=True`.\n    \"\"\"\n    typevars_map = get_model_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        types_namespace,\n        typevars_map,\n    )\n\n    handler = CallbackGetCoreSchemaHandler(\n        partial(gen_schema.generate_schema, from_dunder_get_core_schema=False),\n        gen_schema,\n        ref_mode='unpack',\n    )\n\n    if config_wrapper.defer_build and 'model' in config_wrapper.experimental_defer_build_mode:\n        set_model_mocks(cls, cls_name)\n        return False\n\n    try:\n        schema = cls.__get_pydantic_core_schema__(cls, handler)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_model_mocks(cls, cls_name, f'`{e.name}`')\n        return False\n\n    core_config = config_wrapper.core_config(cls)\n\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except gen_schema.CollectedInvalid:\n        set_model_mocks(cls, cls_name)\n        return False\n\n    # debug(schema)\n    cls.__pydantic_core_schema__ = schema\n\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema,\n        cls,\n        create_model_module or cls.__module__,\n        cls.__qualname__,\n        'create_model' if create_model_module else 'BaseModel',\n        core_config,\n        config_wrapper.plugin_settings,\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n\n    # set __signature__ attr only for model class, but not for its instances\n    cls.__signature__ = ClassAttribute(\n        '__signature__',\n        generate_pydantic_signature(init=cls.__init__, fields=cls.model_fields, config_wrapper=config_wrapper),\n    )\n    return True\n\n\ndef set_deprecated_descriptors(cls: type[BaseModel]) -> None:\n    \"\"\"Set data descriptors on the class for deprecated fields.\"\"\"\n    for field, field_info in cls.model_fields.items():\n        if (msg := field_info.deprecation_message) is not None:\n            desc = _DeprecatedFieldDescriptor(msg)\n            desc.__set_name__(cls, field)\n            setattr(cls, field, desc)\n\n    for field, computed_field_info in cls.model_computed_fields.items():\n        if (\n            (msg := computed_field_info.deprecation_message) is not None\n            # Avoid having two warnings emitted:\n            and not hasattr(unwrap_wrapped_function(computed_field_info.wrapped_property), '__deprecated__')\n        ):\n            desc = _DeprecatedFieldDescriptor(msg, computed_field_info.wrapped_property)\n            desc.__set_name__(cls, field)\n            setattr(cls, field, desc)\n\n\nclass _DeprecatedFieldDescriptor:\n    \"\"\"Data descriptor used to emit a runtime deprecation warning before accessing a deprecated field.\n\n    Attributes:\n        msg: The deprecation message to be emitted.\n        wrapped_property: The property instance if the deprecated field is a computed field, or `None`.\n        field_name: The name of the field being deprecated.\n    \"\"\"\n\n    field_name: str\n\n    def __init__(self, msg: str, wrapped_property: property | None = None) -> None:\n        self.msg = msg\n        self.wrapped_property = wrapped_property\n\n    def __set_name__(self, cls: type[BaseModel], name: str) -> None:\n        self.field_name = name\n\n    def __get__(self, obj: BaseModel | None, obj_type: type[BaseModel] | None = None) -> Any:\n        if obj is None:\n            raise AttributeError(self.field_name)\n\n        warnings.warn(self.msg, builtins.DeprecationWarning, stacklevel=2)\n\n        if self.wrapped_property is not None:\n            return self.wrapped_property.__get__(obj, obj_type)\n        return obj.__dict__[self.field_name]\n\n    # Defined to take precedence over the instance's dictionary\n    # Note that it will not be called when setting a value on a model instance\n    # as `BaseModel.__setattr__` is defined and takes priority.\n    def __set__(self, obj: Any, value: Any) -> NoReturn:\n        raise AttributeError(self.field_name)\n\n\nclass _PydanticWeakRef:\n    \"\"\"Wrapper for `weakref.ref` that enables `pickle` serialization.\n\n    Cloudpickle fails to serialize `weakref.ref` objects due to an arcane error related\n    to abstract base classes (`abc.ABC`). This class works around the issue by wrapping\n    `weakref.ref` instead of subclassing it.\n\n    See https://github.com/pydantic/pydantic/issues/6763 for context.\n\n    Semantics:\n        - If not pickled, behaves the same as a `weakref.ref`.\n        - If pickled along with the referenced object, the same `weakref.ref` behavior\n          will be maintained between them after unpickling.\n        - If pickled without the referenced object, after unpickling the underlying\n          reference will be cleared (`__call__` will always return `None`).\n    \"\"\"\n\n    def __init__(self, obj: Any):\n        if obj is None:\n            # The object will be `None` upon deserialization if the serialized weakref\n            # had lost its underlying object.\n            self._wr = None\n        else:\n            self._wr = weakref.ref(obj)\n\n    def __call__(self) -> Any:\n        if self._wr is None:\n            return None\n        else:\n            return self._wr()\n\n    def __reduce__(self) -> tuple[Callable, tuple[weakref.ReferenceType | None]]:\n        return _PydanticWeakRef, (self(),)\n\n\ndef build_lenient_weakvaluedict(d: dict[str, Any] | None) -> dict[str, Any] | None:\n    \"\"\"Takes an input dictionary, and produces a new value that (invertibly) replaces the values with weakrefs.\n\n    We can't just use a WeakValueDictionary because many types (including int, str, etc.) can't be stored as values\n    in a WeakValueDictionary.\n\n    The `unpack_lenient_weakvaluedict` function can be used to reverse this operation.\n    \"\"\"\n    if d is None:\n        return None\n    result = {}\n    for k, v in d.items():\n        try:\n            proxy = _PydanticWeakRef(v)\n        except TypeError:\n            proxy = v\n        result[k] = proxy\n    return result\n\n\ndef unpack_lenient_weakvaluedict(d: dict[str, Any] | None) -> dict[str, Any] | None:\n    \"\"\"Inverts the transform performed by `build_lenient_weakvaluedict`.\"\"\"\n    if d is None:\n        return None\n\n    result = {}\n    for k, v in d.items():\n        if isinstance(v, _PydanticWeakRef):\n            v = v()\n            if v is not None:\n                result[k] = v\n        else:\n            result[k] = v\n    return result\n\n\ndef default_ignored_types() -> tuple[type[Any], ...]:\n    from ..fields import ComputedFieldInfo\n\n    return (\n        FunctionType,\n        property,\n        classmethod,\n        staticmethod,\n        PydanticDescriptorProxy,\n        ComputedFieldInfo,\n        ValidateCallWrapper,\n    )\n", "pydantic/_internal/_config.py": "from __future__ import annotations as _annotations\n\nimport warnings\nfrom contextlib import contextmanager\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    cast,\n)\n\nfrom pydantic_core import core_schema\nfrom typing_extensions import (\n    Literal,\n    Self,\n)\n\nfrom ..aliases import AliasGenerator\nfrom ..config import ConfigDict, ExtraValues, JsonDict, JsonEncoder, JsonSchemaExtraCallable\nfrom ..errors import PydanticUserError\nfrom ..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\nif TYPE_CHECKING:\n    from .._internal._schema_generation_shared import GenerateSchema\n    from ..fields import ComputedFieldInfo, FieldInfo\n\nDEPRECATION_MESSAGE = 'Support for class-based `config` is deprecated, use ConfigDict instead.'\n\n\nclass ConfigWrapper:\n    \"\"\"Internal wrapper for Config which exposes ConfigDict items as attributes.\"\"\"\n\n    __slots__ = ('config_dict',)\n\n    config_dict: ConfigDict\n\n    # all annotations are copied directly from ConfigDict, and should be kept up to date, a test will fail if they\n    # stop matching\n    title: str | None\n    str_to_lower: bool\n    str_to_upper: bool\n    str_strip_whitespace: bool\n    str_min_length: int\n    str_max_length: int | None\n    extra: ExtraValues | None\n    frozen: bool\n    populate_by_name: bool\n    use_enum_values: bool\n    validate_assignment: bool\n    arbitrary_types_allowed: bool\n    from_attributes: bool\n    # whether to use the actual key provided in the data (e.g. alias or first alias for \"field required\" errors) instead of field_names\n    # to construct error `loc`s, default `True`\n    loc_by_alias: bool\n    alias_generator: Callable[[str], str] | AliasGenerator | None\n    model_title_generator: Callable[[type], str] | None\n    field_title_generator: Callable[[str, FieldInfo | ComputedFieldInfo], str] | None\n    ignored_types: tuple[type, ...]\n    allow_inf_nan: bool\n    json_schema_extra: JsonDict | JsonSchemaExtraCallable | None\n    json_encoders: dict[type[object], JsonEncoder] | None\n\n    # new in V2\n    strict: bool\n    # whether instances of models and dataclasses (including subclass instances) should re-validate, default 'never'\n    revalidate_instances: Literal['always', 'never', 'subclass-instances']\n    ser_json_timedelta: Literal['iso8601', 'float']\n    ser_json_bytes: Literal['utf8', 'base64']\n    ser_json_inf_nan: Literal['null', 'constants']\n    # whether to validate default values during validation, default False\n    validate_default: bool\n    validate_return: bool\n    protected_namespaces: tuple[str, ...]\n    hide_input_in_errors: bool\n    defer_build: bool\n    experimental_defer_build_mode: tuple[Literal['model', 'type_adapter'], ...]\n    plugin_settings: dict[str, object] | None\n    schema_generator: type[GenerateSchema] | None\n    json_schema_serialization_defaults_required: bool\n    json_schema_mode_override: Literal['validation', 'serialization', None]\n    coerce_numbers_to_str: bool\n    regex_engine: Literal['rust-regex', 'python-re']\n    validation_error_cause: bool\n    use_attribute_docstrings: bool\n    cache_strings: bool | Literal['all', 'keys', 'none']\n\n    def __init__(self, config: ConfigDict | dict[str, Any] | type[Any] | None, *, check: bool = True):\n        if check:\n            self.config_dict = prepare_config(config)\n        else:\n            self.config_dict = cast(ConfigDict, config)\n\n    @classmethod\n    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:\n        \"\"\"Build a new `ConfigWrapper` instance for a `BaseModel`.\n\n        The config wrapper built based on (in descending order of priority):\n        - options from `kwargs`\n        - options from the `namespace`\n        - options from the base classes (`bases`)\n\n        Args:\n            bases: A tuple of base classes.\n            namespace: The namespace of the class being created.\n            kwargs: The kwargs passed to the class being created.\n\n        Returns:\n            A `ConfigWrapper` instance for `BaseModel`.\n        \"\"\"\n        config_new = ConfigDict()\n        for base in bases:\n            config = getattr(base, 'model_config', None)\n            if config:\n                config_new.update(config.copy())\n\n        config_class_from_namespace = namespace.get('Config')\n        config_dict_from_namespace = namespace.get('model_config')\n\n        raw_annotations = namespace.get('__annotations__', {})\n        if raw_annotations.get('model_config') and not config_dict_from_namespace:\n            raise PydanticUserError(\n                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',\n                code='model-config-invalid-field-name',\n            )\n\n        if config_class_from_namespace and config_dict_from_namespace:\n            raise PydanticUserError('\"Config\" and \"model_config\" cannot be used together', code='config-both')\n\n        config_from_namespace = config_dict_from_namespace or prepare_config(config_class_from_namespace)\n\n        config_new.update(config_from_namespace)\n\n        for k in list(kwargs.keys()):\n            if k in config_keys:\n                config_new[k] = kwargs.pop(k)\n\n        return cls(config_new)\n\n    # we don't show `__getattr__` to type checkers so missing attributes cause errors\n    if not TYPE_CHECKING:  # pragma: no branch\n\n        def __getattr__(self, name: str) -> Any:\n            try:\n                return self.config_dict[name]\n            except KeyError:\n                try:\n                    return config_defaults[name]\n                except KeyError:\n                    raise AttributeError(f'Config has no attribute {name!r}') from None\n\n    def core_config(self, obj: Any) -> core_schema.CoreConfig:\n        \"\"\"Create a pydantic-core config, `obj` is just used to populate `title` if not set in config.\n\n        Pass `obj=None` if you do not want to attempt to infer the `title`.\n\n        We don't use getattr here since we don't want to populate with defaults.\n\n        Args:\n            obj: An object used to populate `title` if not set in config.\n\n        Returns:\n            A `CoreConfig` object created from config.\n        \"\"\"\n\n        def dict_not_none(**kwargs: Any) -> Any:\n            return {k: v for k, v in kwargs.items() if v is not None}\n\n        core_config = core_schema.CoreConfig(\n            **dict_not_none(\n                title=self.config_dict.get('title') or (obj and obj.__name__),\n                extra_fields_behavior=self.config_dict.get('extra'),\n                allow_inf_nan=self.config_dict.get('allow_inf_nan'),\n                populate_by_name=self.config_dict.get('populate_by_name'),\n                str_strip_whitespace=self.config_dict.get('str_strip_whitespace'),\n                str_to_lower=self.config_dict.get('str_to_lower'),\n                str_to_upper=self.config_dict.get('str_to_upper'),\n                strict=self.config_dict.get('strict'),\n                ser_json_timedelta=self.config_dict.get('ser_json_timedelta'),\n                ser_json_bytes=self.config_dict.get('ser_json_bytes'),\n                ser_json_inf_nan=self.config_dict.get('ser_json_inf_nan'),\n                from_attributes=self.config_dict.get('from_attributes'),\n                loc_by_alias=self.config_dict.get('loc_by_alias'),\n                revalidate_instances=self.config_dict.get('revalidate_instances'),\n                validate_default=self.config_dict.get('validate_default'),\n                str_max_length=self.config_dict.get('str_max_length'),\n                str_min_length=self.config_dict.get('str_min_length'),\n                hide_input_in_errors=self.config_dict.get('hide_input_in_errors'),\n                coerce_numbers_to_str=self.config_dict.get('coerce_numbers_to_str'),\n                regex_engine=self.config_dict.get('regex_engine'),\n                validation_error_cause=self.config_dict.get('validation_error_cause'),\n                cache_strings=self.config_dict.get('cache_strings'),\n            )\n        )\n        return core_config\n\n    def __repr__(self):\n        c = ', '.join(f'{k}={v!r}' for k, v in self.config_dict.items())\n        return f'ConfigWrapper({c})'\n\n\nclass ConfigWrapperStack:\n    \"\"\"A stack of `ConfigWrapper` instances.\"\"\"\n\n    def __init__(self, config_wrapper: ConfigWrapper):\n        self._config_wrapper_stack: list[ConfigWrapper] = [config_wrapper]\n\n    @property\n    def tail(self) -> ConfigWrapper:\n        return self._config_wrapper_stack[-1]\n\n    @contextmanager\n    def push(self, config_wrapper: ConfigWrapper | ConfigDict | None):\n        if config_wrapper is None:\n            yield\n            return\n\n        if not isinstance(config_wrapper, ConfigWrapper):\n            config_wrapper = ConfigWrapper(config_wrapper, check=False)\n\n        self._config_wrapper_stack.append(config_wrapper)\n        try:\n            yield\n        finally:\n            self._config_wrapper_stack.pop()\n\n\nconfig_defaults = ConfigDict(\n    title=None,\n    str_to_lower=False,\n    str_to_upper=False,\n    str_strip_whitespace=False,\n    str_min_length=0,\n    str_max_length=None,\n    # let the model / dataclass decide how to handle it\n    extra=None,\n    frozen=False,\n    populate_by_name=False,\n    use_enum_values=False,\n    validate_assignment=False,\n    arbitrary_types_allowed=False,\n    from_attributes=False,\n    loc_by_alias=True,\n    alias_generator=None,\n    model_title_generator=None,\n    field_title_generator=None,\n    ignored_types=(),\n    allow_inf_nan=True,\n    json_schema_extra=None,\n    strict=False,\n    revalidate_instances='never',\n    ser_json_timedelta='iso8601',\n    ser_json_bytes='utf8',\n    ser_json_inf_nan='null',\n    validate_default=False,\n    validate_return=False,\n    protected_namespaces=('model_',),\n    hide_input_in_errors=False,\n    json_encoders=None,\n    defer_build=False,\n    experimental_defer_build_mode=('model',),\n    plugin_settings=None,\n    schema_generator=None,\n    json_schema_serialization_defaults_required=False,\n    json_schema_mode_override=None,\n    coerce_numbers_to_str=False,\n    regex_engine='rust-regex',\n    validation_error_cause=False,\n    use_attribute_docstrings=False,\n    cache_strings=True,\n)\n\n\ndef prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    \"\"\"Create a `ConfigDict` instance from an existing dict, a class (e.g. old class-based config) or None.\n\n    Args:\n        config: The input config.\n\n    Returns:\n        A ConfigDict object created from config.\n    \"\"\"\n    if config is None:\n        return ConfigDict()\n\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n        config = {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n\n    config_dict = cast(ConfigDict, config)\n    check_deprecated(config_dict)\n    return config_dict\n\n\nconfig_keys = set(ConfigDict.__annotations__.keys())\n\n\nV2_REMOVED_KEYS = {\n    'allow_mutation',\n    'error_msg_templates',\n    'fields',\n    'getter_dict',\n    'smart_union',\n    'underscore_attrs_are_private',\n    'json_loads',\n    'json_dumps',\n    'copy_on_model_validation',\n    'post_init_call',\n}\nV2_RENAMED_KEYS = {\n    'allow_population_by_field_name': 'populate_by_name',\n    'anystr_lower': 'str_to_lower',\n    'anystr_strip_whitespace': 'str_strip_whitespace',\n    'anystr_upper': 'str_to_upper',\n    'keep_untouched': 'ignored_types',\n    'max_anystr_length': 'str_max_length',\n    'min_anystr_length': 'str_min_length',\n    'orm_mode': 'from_attributes',\n    'schema_extra': 'json_schema_extra',\n    'validate_all': 'validate_default',\n}\n\n\ndef check_deprecated(config_dict: ConfigDict) -> None:\n    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        config_dict: The input config.\n    \"\"\"\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)\n", "pydantic/_internal/_docs_extraction.py": "\"\"\"Utilities related to attribute docstring extraction.\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport inspect\nimport textwrap\nfrom typing import Any\n\n\nclass DocstringVisitor(ast.NodeVisitor):\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.target: str | None = None\n        self.attrs: dict[str, str] = {}\n        self.previous_node_type: type[ast.AST] | None = None\n\n    def visit(self, node: ast.AST) -> Any:\n        node_result = super().visit(node)\n        self.previous_node_type = type(node)\n        return node_result\n\n    def visit_AnnAssign(self, node: ast.AnnAssign) -> Any:\n        if isinstance(node.target, ast.Name):\n            self.target = node.target.id\n\n    def visit_Expr(self, node: ast.Expr) -> Any:\n        if (\n            isinstance(node.value, ast.Constant)\n            and isinstance(node.value.value, str)\n            and self.previous_node_type is ast.AnnAssign\n        ):\n            docstring = inspect.cleandoc(node.value.value)\n            if self.target:\n                self.attrs[self.target] = docstring\n            self.target = None\n\n\ndef _dedent_source_lines(source: list[str]) -> str:\n    # Required for nested class definitions, e.g. in a function block\n    dedent_source = textwrap.dedent(''.join(source))\n    if dedent_source.startswith((' ', '\\t')):\n        # We are in the case where there's a dedented (usually multiline) string\n        # at a lower indentation level than the class itself. We wrap our class\n        # in a function as a workaround.\n        dedent_source = f'def dedent_workaround():\\n{dedent_source}'\n    return dedent_source\n\n\ndef _extract_source_from_frame(cls: type[Any]) -> list[str] | None:\n    frame = inspect.currentframe()\n\n    while frame:\n        if inspect.getmodule(frame) is inspect.getmodule(cls):\n            lnum = frame.f_lineno\n            try:\n                lines, _ = inspect.findsource(frame)\n            except OSError:\n                # Source can't be retrieved (maybe because running in an interactive terminal),\n                # we don't want to error here.\n                pass\n            else:\n                block_lines = inspect.getblock(lines[lnum - 1 :])\n                dedent_source = _dedent_source_lines(block_lines)\n                try:\n                    block_tree = ast.parse(dedent_source)\n                except SyntaxError:\n                    pass\n                else:\n                    stmt = block_tree.body[0]\n                    if isinstance(stmt, ast.FunctionDef) and stmt.name == 'dedent_workaround':\n                        # `_dedent_source_lines` wrapped the class around the workaround function\n                        stmt = stmt.body[0]\n                    if isinstance(stmt, ast.ClassDef) and stmt.name == cls.__name__:\n                        return block_lines\n\n        frame = frame.f_back\n\n\ndef extract_docstrings_from_cls(cls: type[Any], use_inspect: bool = False) -> dict[str, str]:\n    \"\"\"Map model attributes and their corresponding docstring.\n\n    Args:\n        cls: The class of the Pydantic model to inspect.\n        use_inspect: Whether to skip usage of frames to find the object and use\n            the `inspect` module instead.\n\n    Returns:\n        A mapping containing attribute names and their corresponding docstring.\n    \"\"\"\n    if use_inspect:\n        # Might not work as expected if two classes have the same name in the same source file.\n        try:\n            source, _ = inspect.getsourcelines(cls)\n        except OSError:\n            return {}\n    else:\n        source = _extract_source_from_frame(cls)\n\n    if not source:\n        return {}\n\n    dedent_source = _dedent_source_lines(source)\n\n    visitor = DocstringVisitor()\n    visitor.visit(ast.parse(dedent_source))\n    return visitor.attrs\n", "pydantic/_internal/_utils.py": "\"\"\"Bucket of reusable internal utilities.\n\nThis should be reduced as much as possible with functions only used in one place, moved to that place.\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport keyword\nimport typing\nimport weakref\nfrom collections import OrderedDict, defaultdict, deque\nfrom copy import deepcopy\nfrom itertools import zip_longest\nfrom types import BuiltinFunctionType, CodeType, FunctionType, GeneratorType, LambdaType, ModuleType\nfrom typing import Any, Mapping, TypeVar\n\nfrom typing_extensions import TypeAlias, TypeGuard\n\nfrom . import _repr, _typing_extra\n\nif typing.TYPE_CHECKING:\n    MappingIntStrAny: TypeAlias = 'typing.Mapping[int, Any] | typing.Mapping[str, Any]'\n    AbstractSetIntStr: TypeAlias = 'typing.AbstractSet[int] | typing.AbstractSet[str]'\n    from ..main import BaseModel\n\n\n# these are types that are returned unchanged by deepcopy\nIMMUTABLE_NON_COLLECTIONS_TYPES: set[type[Any]] = {\n    int,\n    float,\n    complex,\n    str,\n    bool,\n    bytes,\n    type,\n    _typing_extra.NoneType,\n    FunctionType,\n    BuiltinFunctionType,\n    LambdaType,\n    weakref.ref,\n    CodeType,\n    # note: including ModuleType will differ from behaviour of deepcopy by not producing error.\n    # It might be not a good idea in general, but considering that this function used only internally\n    # against default values of fields, this will allow to actually have a field with module as default value\n    ModuleType,\n    NotImplemented.__class__,\n    Ellipsis.__class__,\n}\n\n# these are types that if empty, might be copied with simple copy() instead of deepcopy()\nBUILTIN_COLLECTIONS: set[type[Any]] = {\n    list,\n    set,\n    tuple,\n    frozenset,\n    dict,\n    OrderedDict,\n    defaultdict,\n    deque,\n}\n\n\ndef sequence_like(v: Any) -> bool:\n    return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))\n\n\ndef lenient_isinstance(o: Any, class_or_tuple: type[Any] | tuple[type[Any], ...] | None) -> bool:  # pragma: no cover\n    try:\n        return isinstance(o, class_or_tuple)  # type: ignore[arg-type]\n    except TypeError:\n        return False\n\n\ndef lenient_issubclass(cls: Any, class_or_tuple: Any) -> bool:  # pragma: no cover\n    try:\n        return isinstance(cls, type) and issubclass(cls, class_or_tuple)\n    except TypeError:\n        if isinstance(cls, _typing_extra.WithArgsTypes):\n            return False\n        raise  # pragma: no cover\n\n\ndef is_model_class(cls: Any) -> TypeGuard[type[BaseModel]]:\n    \"\"\"Returns true if cls is a _proper_ subclass of BaseModel, and provides proper type-checking,\n    unlike raw calls to lenient_issubclass.\n    \"\"\"\n    from ..main import BaseModel\n\n    return lenient_issubclass(cls, BaseModel) and cls is not BaseModel\n\n\ndef is_valid_identifier(identifier: str) -> bool:\n    \"\"\"Checks that a string is a valid identifier and not a Python keyword.\n    :param identifier: The identifier to test.\n    :return: True if the identifier is valid.\n    \"\"\"\n    return identifier.isidentifier() and not keyword.iskeyword(identifier)\n\n\nKeyType = TypeVar('KeyType')\n\n\ndef deep_update(mapping: dict[KeyType, Any], *updating_mappings: dict[KeyType, Any]) -> dict[KeyType, Any]:\n    updated_mapping = mapping.copy()\n    for updating_mapping in updating_mappings:\n        for k, v in updating_mapping.items():\n            if k in updated_mapping and isinstance(updated_mapping[k], dict) and isinstance(v, dict):\n                updated_mapping[k] = deep_update(updated_mapping[k], v)\n            else:\n                updated_mapping[k] = v\n    return updated_mapping\n\n\ndef update_not_none(mapping: dict[Any, Any], **update: Any) -> None:\n    mapping.update({k: v for k, v in update.items() if v is not None})\n\n\nT = TypeVar('T')\n\n\ndef unique_list(\n    input_list: list[T] | tuple[T, ...],\n    *,\n    name_factory: typing.Callable[[T], str] = str,\n) -> list[T]:\n    \"\"\"Make a list unique while maintaining order.\n    We update the list if another one with the same name is set\n    (e.g. model validator overridden in subclass).\n    \"\"\"\n    result: list[T] = []\n    result_names: list[str] = []\n    for v in input_list:\n        v_name = name_factory(v)\n        if v_name not in result_names:\n            result_names.append(v_name)\n            result.append(v)\n        else:\n            result[result_names.index(v_name)] = v\n\n    return result\n\n\nclass ValueItems(_repr.Representation):\n    \"\"\"Class for more convenient calculation of excluded or included fields on values.\"\"\"\n\n    __slots__ = ('_items', '_type')\n\n    def __init__(self, value: Any, items: AbstractSetIntStr | MappingIntStrAny) -> None:\n        items = self._coerce_items(items)\n\n        if isinstance(value, (list, tuple)):\n            items = self._normalize_indexes(items, len(value))  # type: ignore\n\n        self._items: MappingIntStrAny = items  # type: ignore\n\n    def is_excluded(self, item: Any) -> bool:\n        \"\"\"Check if item is fully excluded.\n\n        :param item: key or index of a value\n        \"\"\"\n        return self.is_true(self._items.get(item))\n\n    def is_included(self, item: Any) -> bool:\n        \"\"\"Check if value is contained in self._items.\n\n        :param item: key or index of value\n        \"\"\"\n        return item in self._items\n\n    def for_element(self, e: int | str) -> AbstractSetIntStr | MappingIntStrAny | None:\n        \"\"\":param e: key or index of element on value\n        :return: raw values for element if self._items is dict and contain needed element\n        \"\"\"\n        item = self._items.get(e)  # type: ignore\n        return item if not self.is_true(item) else None\n\n    def _normalize_indexes(self, items: MappingIntStrAny, v_length: int) -> dict[int | str, Any]:\n        \"\"\":param items: dict or set of indexes which will be normalized\n        :param v_length: length of sequence indexes of which will be\n\n        >>> self._normalize_indexes({0: True, -2: True, -1: True}, 4)\n        {0: True, 2: True, 3: True}\n        >>> self._normalize_indexes({'__all__': True}, 4)\n        {0: True, 1: True, 2: True, 3: True}\n        \"\"\"\n        normalized_items: dict[int | str, Any] = {}\n        all_items = None\n        for i, v in items.items():\n            if not (isinstance(v, typing.Mapping) or isinstance(v, typing.AbstractSet) or self.is_true(v)):\n                raise TypeError(f'Unexpected type of exclude value for index \"{i}\" {v.__class__}')\n            if i == '__all__':\n                all_items = self._coerce_value(v)\n                continue\n            if not isinstance(i, int):\n                raise TypeError(\n                    'Excluding fields from a sequence of sub-models or dicts must be performed index-wise: '\n                    'expected integer keys or keyword \"__all__\"'\n                )\n            normalized_i = v_length + i if i < 0 else i\n            normalized_items[normalized_i] = self.merge(v, normalized_items.get(normalized_i))\n\n        if not all_items:\n            return normalized_items\n        if self.is_true(all_items):\n            for i in range(v_length):\n                normalized_items.setdefault(i, ...)\n            return normalized_items\n        for i in range(v_length):\n            normalized_item = normalized_items.setdefault(i, {})\n            if not self.is_true(normalized_item):\n                normalized_items[i] = self.merge(all_items, normalized_item)\n        return normalized_items\n\n    @classmethod\n    def merge(cls, base: Any, override: Any, intersect: bool = False) -> Any:\n        \"\"\"Merge a `base` item with an `override` item.\n\n        Both `base` and `override` are converted to dictionaries if possible.\n        Sets are converted to dictionaries with the sets entries as keys and\n        Ellipsis as values.\n\n        Each key-value pair existing in `base` is merged with `override`,\n        while the rest of the key-value pairs are updated recursively with this function.\n\n        Merging takes place based on the \"union\" of keys if `intersect` is\n        set to `False` (default) and on the intersection of keys if\n        `intersect` is set to `True`.\n        \"\"\"\n        override = cls._coerce_value(override)\n        base = cls._coerce_value(base)\n        if override is None:\n            return base\n        if cls.is_true(base) or base is None:\n            return override\n        if cls.is_true(override):\n            return base if intersect else override\n\n        # intersection or union of keys while preserving ordering:\n        if intersect:\n            merge_keys = [k for k in base if k in override] + [k for k in override if k in base]\n        else:\n            merge_keys = list(base) + [k for k in override if k not in base]\n\n        merged: dict[int | str, Any] = {}\n        for k in merge_keys:\n            merged_item = cls.merge(base.get(k), override.get(k), intersect=intersect)\n            if merged_item is not None:\n                merged[k] = merged_item\n\n        return merged\n\n    @staticmethod\n    def _coerce_items(items: AbstractSetIntStr | MappingIntStrAny) -> MappingIntStrAny:\n        if isinstance(items, typing.Mapping):\n            pass\n        elif isinstance(items, typing.AbstractSet):\n            items = dict.fromkeys(items, ...)  # type: ignore\n        else:\n            class_name = getattr(items, '__class__', '???')\n            raise TypeError(f'Unexpected type of exclude value {class_name}')\n        return items  # type: ignore\n\n    @classmethod\n    def _coerce_value(cls, value: Any) -> Any:\n        if value is None or cls.is_true(value):\n            return value\n        return cls._coerce_items(value)\n\n    @staticmethod\n    def is_true(v: Any) -> bool:\n        return v is True or v is ...\n\n    def __repr_args__(self) -> _repr.ReprArgs:\n        return [(None, self._items)]\n\n\nif typing.TYPE_CHECKING:\n\n    def ClassAttribute(name: str, value: T) -> T: ...\n\nelse:\n\n    class ClassAttribute:\n        \"\"\"Hide class attribute from its instances.\"\"\"\n\n        __slots__ = 'name', 'value'\n\n        def __init__(self, name: str, value: Any) -> None:\n            self.name = name\n            self.value = value\n\n        def __get__(self, instance: Any, owner: type[Any]) -> None:\n            if instance is None:\n                return self.value\n            raise AttributeError(f'{self.name!r} attribute of {owner.__name__!r} is class-only')\n\n\nObj = TypeVar('Obj')\n\n\ndef smart_deepcopy(obj: Obj) -> Obj:\n    \"\"\"Return type as is for immutable built-in types\n    Use obj.copy() for built-in empty collections\n    Use copy.deepcopy() for non-empty collections and unknown objects.\n    \"\"\"\n    obj_type = obj.__class__\n    if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n        return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n    try:\n        if not obj and obj_type in BUILTIN_COLLECTIONS:\n            # faster way for empty collections, no need to copy its members\n            return obj if obj_type is tuple else obj.copy()  # tuple doesn't have copy method  # type: ignore\n    except (TypeError, ValueError, RuntimeError):\n        # do we really dare to catch ALL errors? Seems a bit risky\n        pass\n\n    return deepcopy(obj)  # slowest way when we actually might need a deepcopy\n\n\n_SENTINEL = object()\n\n\ndef all_identical(left: typing.Iterable[Any], right: typing.Iterable[Any]) -> bool:\n    \"\"\"Check that the items of `left` are the same objects as those in `right`.\n\n    >>> a, b = object(), object()\n    >>> all_identical([a, b, a], [a, b, a])\n    True\n    >>> all_identical([a, b, [a]], [a, b, [a]])  # new list object, while \"equal\" is not \"identical\"\n    False\n    \"\"\"\n    for left_item, right_item in zip_longest(left, right, fillvalue=_SENTINEL):\n        if left_item is not right_item:\n            return False\n    return True\n\n\n@dataclasses.dataclass(frozen=True)\nclass SafeGetItemProxy:\n    \"\"\"Wrapper redirecting `__getitem__` to `get` with a sentinel value as default\n\n    This makes is safe to use in `operator.itemgetter` when some keys may be missing\n    \"\"\"\n\n    # Define __slots__manually for performances\n    # @dataclasses.dataclass() only support slots=True in python>=3.10\n    __slots__ = ('wrapped',)\n\n    wrapped: Mapping[str, Any]\n\n    def __getitem__(self, key: str, /) -> Any:\n        return self.wrapped.get(key, _SENTINEL)\n\n    # required to pass the object to operator.itemgetter() instances due to a quirk of typeshed\n    # https://github.com/python/mypy/issues/13713\n    # https://github.com/python/typeshed/pull/8785\n    # Since this is typing-only, hide it in a typing.TYPE_CHECKING block\n    if typing.TYPE_CHECKING:\n\n        def __contains__(self, key: str, /) -> bool:\n            return self.wrapped.__contains__(key)\n", "pydantic/_internal/_dataclasses.py": "\"\"\"Private logic for creating pydantic dataclasses.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport typing\nimport warnings\nfrom functools import partial, wraps\nfrom typing import Any, Callable, ClassVar\n\nfrom pydantic_core import (\n    ArgsKwargs,\n    SchemaSerializer,\n    SchemaValidator,\n    core_schema,\n)\nfrom typing_extensions import TypeGuard\n\nfrom ..errors import PydanticUndefinedAnnotation\nfrom ..fields import FieldInfo\nfrom ..plugin._schema_validator import create_schema_validator\nfrom ..warnings import PydanticDeprecatedSince20\nfrom . import _config, _decorators, _typing_extra\nfrom ._fields import collect_dataclass_fields\nfrom ._generate_schema import GenerateSchema\nfrom ._generics import get_standard_typevars_map\nfrom ._mock_val_ser import set_dataclass_mocks\nfrom ._schema_generation_shared import CallbackGetCoreSchemaHandler\nfrom ._signature import generate_pydantic_signature\n\nif typing.TYPE_CHECKING:\n    from ..config import ConfigDict\n\n    class StandardDataclass(typing.Protocol):\n        __dataclass_fields__: ClassVar[dict[str, Any]]\n        __dataclass_params__: ClassVar[Any]  # in reality `dataclasses._DataclassParams`\n        __post_init__: ClassVar[Callable[..., None]]\n\n        def __init__(self, *args: object, **kwargs: object) -> None:\n            pass\n\n    class PydanticDataclass(StandardDataclass, typing.Protocol):\n        \"\"\"A protocol containing attributes only available once a class has been decorated as a Pydantic dataclass.\n\n        Attributes:\n            __pydantic_config__: Pydantic-specific configuration settings for the dataclass.\n            __pydantic_complete__: Whether dataclass building is completed, or if there are still undefined fields.\n            __pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n            __pydantic_decorators__: Metadata containing the decorators defined on the dataclass.\n            __pydantic_fields__: Metadata about the fields defined on the dataclass.\n            __pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the dataclass.\n            __pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the dataclass.\n        \"\"\"\n\n        __pydantic_config__: ClassVar[ConfigDict]\n        __pydantic_complete__: ClassVar[bool]\n        __pydantic_core_schema__: ClassVar[core_schema.CoreSchema]\n        __pydantic_decorators__: ClassVar[_decorators.DecoratorInfos]\n        __pydantic_fields__: ClassVar[dict[str, FieldInfo]]\n        __pydantic_serializer__: ClassVar[SchemaSerializer]\n        __pydantic_validator__: ClassVar[SchemaValidator]\n\nelse:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n\ndef set_dataclass_fields(\n    cls: type[StandardDataclass],\n    types_namespace: dict[str, Any] | None = None,\n    config_wrapper: _config.ConfigWrapper | None = None,\n) -> None:\n    \"\"\"Collect and set `cls.__pydantic_fields__`.\n\n    Args:\n        cls: The class.\n        types_namespace: The types namespace, defaults to `None`.\n        config_wrapper: The config wrapper instance, defaults to `None`.\n    \"\"\"\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(cls, types_namespace, typevars_map=typevars_map, config_wrapper=config_wrapper)\n\n    cls.__pydantic_fields__ = fields  # type: ignore\n\n\ndef complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    types_namespace: dict[str, Any] | None,\n) -> bool:\n    \"\"\"Finish building a pydantic dataclass.\n\n    This logic is called on a class which has already been wrapped in `dataclasses.dataclass()`.\n\n    This is somewhat analogous to `pydantic._internal._model_construction.complete_model_class`.\n\n    Args:\n        cls: The class.\n        config_wrapper: The config wrapper instance.\n        raise_errors: Whether to raise errors, defaults to `True`.\n        types_namespace: The types namespace.\n\n    Returns:\n        `True` if building a pydantic dataclass is successfully completed, `False` otherwise.\n\n    Raises:\n        PydanticUndefinedAnnotation: If `raise_error` is `True` and there is an undefined annotations.\n    \"\"\"\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called', DeprecationWarning\n        )\n\n    if types_namespace is None:\n        types_namespace = _typing_extra.get_cls_types_namespace(cls)\n\n    set_dataclass_fields(cls, types_namespace, config_wrapper=config_wrapper)\n\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        types_namespace,\n        typevars_map,\n    )\n\n    # This needs to be called before we change the __init__\n    sig = generate_pydantic_signature(\n        init=cls.__init__,\n        fields=cls.__pydantic_fields__,  # type: ignore\n        config_wrapper=config_wrapper,\n        is_dataclass=True,\n    )\n\n    # dataclass.__init__ must be defined here so its `__qualname__` can be changed since functions can't be copied.\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n\n    cls.__init__ = __init__  # type: ignore\n    cls.__pydantic_config__ = config_wrapper.config_dict  # type: ignore\n    cls.__signature__ = sig  # type: ignore\n    get_core_schema = getattr(cls, '__get_pydantic_core_schema__', None)\n    try:\n        if get_core_schema:\n            schema = get_core_schema(\n                cls,\n                CallbackGetCoreSchemaHandler(\n                    partial(gen_schema.generate_schema, from_dunder_get_core_schema=False),\n                    gen_schema,\n                    ref_mode='unpack',\n                ),\n            )\n        else:\n            schema = gen_schema.generate_schema(cls, from_dunder_get_core_schema=False)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, cls.__name__, f'`{e.name}`')\n        return False\n\n    core_config = config_wrapper.core_config(cls)\n\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except gen_schema.CollectedInvalid:\n        set_dataclass_mocks(cls, cls.__name__, 'all referenced types')\n        return False\n\n    # We are about to set all the remaining required properties expected for this cast;\n    # __pydantic_decorators__ and __pydantic_fields__ should already be set\n    cls = typing.cast('type[PydanticDataclass]', cls)\n    # debug(schema)\n\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = validator = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n\n    if config_wrapper.validate_assignment:\n\n        @wraps(cls.__setattr__)\n        def validated_setattr(instance: Any, field: str, value: str, /) -> None:\n            validator.validate_assignment(instance, field, value)\n\n        cls.__setattr__ = validated_setattr.__get__(None, cls)  # type: ignore\n\n    return True\n\n\ndef is_builtin_dataclass(_cls: type[Any]) -> TypeGuard[type[StandardDataclass]]:\n    \"\"\"Returns True if a class is a stdlib dataclass and *not* a pydantic dataclass.\n\n    We check that\n    - `_cls` is a dataclass\n    - `_cls` does not inherit from a processed pydantic dataclass (and thus have a `__pydantic_validator__`)\n    - `_cls` does not have any annotations that are not dataclass fields\n    e.g.\n    ```py\n    import dataclasses\n\n    import pydantic.dataclasses\n\n    @dataclasses.dataclass\n    class A:\n        x: int\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        y: int\n    ```\n    In this case, when we first check `B`, we make an extra check and look at the annotations ('y'),\n    which won't be a superset of all the dataclass fields (only the stdlib fields i.e. 'x')\n\n    Args:\n        cls: The class.\n\n    Returns:\n        `True` if the class is a stdlib dataclass, `False` otherwise.\n    \"\"\"\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_validator__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )\n", "pydantic/_internal/_discriminated_union.py": "from __future__ import annotations as _annotations\n\nfrom typing import TYPE_CHECKING, Any, Hashable, Sequence\n\nfrom pydantic_core import CoreSchema, core_schema\n\nfrom ..errors import PydanticUserError\nfrom . import _core_utils\nfrom ._core_utils import (\n    CoreSchemaField,\n    collect_definitions,\n)\n\nif TYPE_CHECKING:\n    from ..types import Discriminator\n\nCORE_SCHEMA_METADATA_DISCRIMINATOR_PLACEHOLDER_KEY = 'pydantic.internal.union_discriminator'\n\n\nclass MissingDefinitionForUnionRef(Exception):\n    \"\"\"Raised when applying a discriminated union discriminator to a schema\n    requires a definition that is not yet defined\n    \"\"\"\n\n    def __init__(self, ref: str) -> None:\n        self.ref = ref\n        super().__init__(f'Missing definition for ref {self.ref!r}')\n\n\ndef set_discriminator_in_metadata(schema: CoreSchema, discriminator: Any) -> None:\n    schema.setdefault('metadata', {})\n    metadata = schema.get('metadata')\n    assert metadata is not None\n    metadata[CORE_SCHEMA_METADATA_DISCRIMINATOR_PLACEHOLDER_KEY] = discriminator\n\n\ndef apply_discriminators(schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n    # We recursively walk through the `schema` passed to `apply_discriminators`, applying discriminators\n    # where necessary at each level. During this recursion, we allow references to be resolved from the definitions\n    # that are originally present on the original, outermost `schema`. Before `apply_discriminators` is called,\n    # `simplify_schema_references` is called on the schema (in the `clean_schema` function),\n    # which often puts the definitions in the outermost schema.\n    global_definitions: dict[str, CoreSchema] = collect_definitions(schema)\n\n    def inner(s: core_schema.CoreSchema, recurse: _core_utils.Recurse) -> core_schema.CoreSchema:\n        nonlocal global_definitions\n\n        s = recurse(s, inner)\n        if s['type'] == 'tagged-union':\n            return s\n\n        metadata = s.get('metadata', {})\n        discriminator = metadata.pop(CORE_SCHEMA_METADATA_DISCRIMINATOR_PLACEHOLDER_KEY, None)\n        if discriminator is not None:\n            s = apply_discriminator(s, discriminator, global_definitions)\n        return s\n\n    return _core_utils.walk_core_schema(schema, inner)\n\n\ndef apply_discriminator(\n    schema: core_schema.CoreSchema,\n    discriminator: str | Discriminator,\n    definitions: dict[str, core_schema.CoreSchema] | None = None,\n) -> core_schema.CoreSchema:\n    \"\"\"Applies the discriminator and returns a new core schema.\n\n    Args:\n        schema: The input schema.\n        discriminator: The name of the field which will serve as the discriminator.\n        definitions: A mapping of schema ref to schema.\n\n    Returns:\n        The new core schema.\n\n    Raises:\n        TypeError:\n            - If `discriminator` is used with invalid union variant.\n            - If `discriminator` is used with `Union` type with one variant.\n            - If `discriminator` value mapped to multiple choices.\n        MissingDefinitionForUnionRef:\n            If the definition for ref is missing.\n        PydanticUserError:\n            - If a model in union doesn't have a discriminator field.\n            - If discriminator field has a non-string alias.\n            - If discriminator fields have different aliases.\n            - If discriminator field not of type `Literal`.\n    \"\"\"\n    from ..types import Discriminator\n\n    if isinstance(discriminator, Discriminator):\n        if isinstance(discriminator.discriminator, str):\n            discriminator = discriminator.discriminator\n        else:\n            return discriminator._convert_schema(schema)\n\n    return _ApplyInferredDiscriminator(discriminator, definitions or {}).apply(schema)\n\n\nclass _ApplyInferredDiscriminator:\n    \"\"\"This class is used to convert an input schema containing a union schema into one where that union is\n    replaced with a tagged-union, with all the associated debugging and performance benefits.\n\n    This is done by:\n    * Validating that the input schema is compatible with the provided discriminator\n    * Introspecting the schema to determine which discriminator values should map to which union choices\n    * Handling various edge cases such as 'definitions', 'default', 'nullable' schemas, and more\n\n    I have chosen to implement the conversion algorithm in this class, rather than a function,\n    to make it easier to maintain state while recursively walking the provided CoreSchema.\n    \"\"\"\n\n    def __init__(self, discriminator: str, definitions: dict[str, core_schema.CoreSchema]):\n        # `discriminator` should be the name of the field which will serve as the discriminator.\n        # It must be the python name of the field, and *not* the field's alias. Note that as of now,\n        # all members of a discriminated union _must_ use a field with the same name as the discriminator.\n        # This may change if/when we expose a way to manually specify the TaggedUnionSchema's choices.\n        self.discriminator = discriminator\n\n        # `definitions` should contain a mapping of schema ref to schema for all schemas which might\n        # be referenced by some choice\n        self.definitions = definitions\n\n        # `_discriminator_alias` will hold the value, if present, of the alias for the discriminator\n        #\n        # Note: following the v1 implementation, we currently disallow the use of different aliases\n        # for different choices. This is not a limitation of pydantic_core, but if we try to handle\n        # this, the inference logic gets complicated very quickly, and could result in confusing\n        # debugging challenges for users making subtle mistakes.\n        #\n        # Rather than trying to do the most powerful inference possible, I think we should eventually\n        # expose a way to more-manually control the way the TaggedUnionSchema is constructed through\n        # the use of a new type which would be placed as an Annotation on the Union type. This would\n        # provide the full flexibility/power of pydantic_core's TaggedUnionSchema where necessary for\n        # more complex cases, without over-complicating the inference logic for the common cases.\n        self._discriminator_alias: str | None = None\n\n        # `_should_be_nullable` indicates whether the converted union has `None` as an allowed value.\n        # If `None` is an acceptable value of the (possibly-wrapped) union, we ignore it while\n        # constructing the TaggedUnionSchema, but set the `_should_be_nullable` attribute to True.\n        # Once we have constructed the TaggedUnionSchema, if `_should_be_nullable` is True, we ensure\n        # that the final schema gets wrapped as a NullableSchema. This has the same semantics on the\n        # python side, but resolves the issue that `None` cannot correspond to any discriminator values.\n        self._should_be_nullable = False\n\n        # `_is_nullable` is used to track if the final produced schema will definitely be nullable;\n        # we set it to True if the input schema is wrapped in a nullable schema that we know will be preserved\n        # as an indication that, even if None is discovered as one of the union choices, we will not need to wrap\n        # the final value in another nullable schema.\n        #\n        # This is more complicated than just checking for the final outermost schema having type 'nullable' thanks\n        # to the possible presence of other wrapper schemas such as DefinitionsSchema, WithDefaultSchema, etc.\n        self._is_nullable = False\n\n        # `_choices_to_handle` serves as a stack of choices to add to the tagged union. Initially, choices\n        # from the union in the wrapped schema will be appended to this list, and the recursive choice-handling\n        # algorithm may add more choices to this stack as (nested) unions are encountered.\n        self._choices_to_handle: list[core_schema.CoreSchema] = []\n\n        # `_tagged_union_choices` is built during the call to `apply`, and will hold the choices to be included\n        # in the output TaggedUnionSchema that will replace the union from the input schema\n        self._tagged_union_choices: dict[Hashable, core_schema.CoreSchema] = {}\n\n        # `_used` is changed to True after applying the discriminator to prevent accidental re-use\n        self._used = False\n\n    def apply(self, schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        \"\"\"Return a new CoreSchema based on `schema` that uses a tagged-union with the discriminator provided\n        to this class.\n\n        Args:\n            schema: The input schema.\n\n        Returns:\n            The new core schema.\n\n        Raises:\n            TypeError:\n                - If `discriminator` is used with invalid union variant.\n                - If `discriminator` is used with `Union` type with one variant.\n                - If `discriminator` value mapped to multiple choices.\n            ValueError:\n                If the definition for ref is missing.\n            PydanticUserError:\n                - If a model in union doesn't have a discriminator field.\n                - If discriminator field has a non-string alias.\n                - If discriminator fields have different aliases.\n                - If discriminator field not of type `Literal`.\n        \"\"\"\n        assert not self._used\n        schema = self._apply_to_root(schema)\n        if self._should_be_nullable and not self._is_nullable:\n            schema = core_schema.nullable_schema(schema)\n        self._used = True\n        return schema\n\n    def _apply_to_root(self, schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        \"\"\"This method handles the outer-most stage of recursion over the input schema:\n        unwrapping nullable or definitions schemas, and calling the `_handle_choice`\n        method iteratively on the choices extracted (recursively) from the possibly-wrapped union.\n        \"\"\"\n        if schema['type'] == 'nullable':\n            self._is_nullable = True\n            wrapped = self._apply_to_root(schema['schema'])\n            nullable_wrapper = schema.copy()\n            nullable_wrapper['schema'] = wrapped\n            return nullable_wrapper\n\n        if schema['type'] == 'definitions':\n            wrapped = self._apply_to_root(schema['schema'])\n            definitions_wrapper = schema.copy()\n            definitions_wrapper['schema'] = wrapped\n            return definitions_wrapper\n\n        if schema['type'] != 'union':\n            # If the schema is not a union, it probably means it just had a single member and\n            # was flattened by pydantic_core.\n            # However, it still may make sense to apply the discriminator to this schema,\n            # as a way to get discriminated-union-style error messages, so we allow this here.\n            schema = core_schema.union_schema([schema])\n\n        # Reverse the choices list before extending the stack so that they get handled in the order they occur\n        choices_schemas = [v[0] if isinstance(v, tuple) else v for v in schema['choices'][::-1]]\n        self._choices_to_handle.extend(choices_schemas)\n        while self._choices_to_handle:\n            choice = self._choices_to_handle.pop()\n            self._handle_choice(choice)\n\n        if self._discriminator_alias is not None and self._discriminator_alias != self.discriminator:\n            # * We need to annotate `discriminator` as a union here to handle both branches of this conditional\n            # * We need to annotate `discriminator` as list[list[str | int]] and not list[list[str]] due to the\n            #   invariance of list, and because list[list[str | int]] is the type of the discriminator argument\n            #   to tagged_union_schema below\n            # * See the docstring of pydantic_core.core_schema.tagged_union_schema for more details about how to\n            #   interpret the value of the discriminator argument to tagged_union_schema. (The list[list[str]] here\n            #   is the appropriate way to provide a list of fallback attributes to check for a discriminator value.)\n            discriminator: str | list[list[str | int]] = [[self.discriminator], [self._discriminator_alias]]\n        else:\n            discriminator = self.discriminator\n        return core_schema.tagged_union_schema(\n            choices=self._tagged_union_choices,\n            discriminator=discriminator,\n            custom_error_type=schema.get('custom_error_type'),\n            custom_error_message=schema.get('custom_error_message'),\n            custom_error_context=schema.get('custom_error_context'),\n            strict=False,\n            from_attributes=True,\n            ref=schema.get('ref'),\n            metadata=schema.get('metadata'),\n            serialization=schema.get('serialization'),\n        )\n\n    def _handle_choice(self, choice: core_schema.CoreSchema) -> None:\n        \"\"\"This method handles the \"middle\" stage of recursion over the input schema.\n        Specifically, it is responsible for handling each choice of the outermost union\n        (and any \"coalesced\" choices obtained from inner unions).\n\n        Here, \"handling\" entails:\n        * Coalescing nested unions and compatible tagged-unions\n        * Tracking the presence of 'none' and 'nullable' schemas occurring as choices\n        * Validating that each allowed discriminator value maps to a unique choice\n        * Updating the _tagged_union_choices mapping that will ultimately be used to build the TaggedUnionSchema.\n        \"\"\"\n        if choice['type'] == 'definition-ref':\n            if choice['schema_ref'] not in self.definitions:\n                raise MissingDefinitionForUnionRef(choice['schema_ref'])\n\n        if choice['type'] == 'none':\n            self._should_be_nullable = True\n        elif choice['type'] == 'definitions':\n            self._handle_choice(choice['schema'])\n        elif choice['type'] == 'nullable':\n            self._should_be_nullable = True\n            self._handle_choice(choice['schema'])  # unwrap the nullable schema\n        elif choice['type'] == 'union':\n            # Reverse the choices list before extending the stack so that they get handled in the order they occur\n            choices_schemas = [v[0] if isinstance(v, tuple) else v for v in choice['choices'][::-1]]\n            self._choices_to_handle.extend(choices_schemas)\n        elif choice['type'] not in {\n            'model',\n            'typed-dict',\n            'tagged-union',\n            'lax-or-strict',\n            'dataclass',\n            'dataclass-args',\n            'definition-ref',\n        } and not _core_utils.is_function_with_inner_schema(choice):\n            # We should eventually handle 'definition-ref' as well\n            raise TypeError(\n                f'{choice[\"type\"]!r} is not a valid discriminated union variant;'\n                ' should be a `BaseModel` or `dataclass`'\n            )\n        else:\n            if choice['type'] == 'tagged-union' and self._is_discriminator_shared(choice):\n                # In this case, this inner tagged-union is compatible with the outer tagged-union,\n                # and its choices can be coalesced into the outer TaggedUnionSchema.\n                subchoices = [x for x in choice['choices'].values() if not isinstance(x, (str, int))]\n                # Reverse the choices list before extending the stack so that they get handled in the order they occur\n                self._choices_to_handle.extend(subchoices[::-1])\n                return\n\n            inferred_discriminator_values = self._infer_discriminator_values_for_choice(choice, source_name=None)\n            self._set_unique_choice_for_values(choice, inferred_discriminator_values)\n\n    def _is_discriminator_shared(self, choice: core_schema.TaggedUnionSchema) -> bool:\n        \"\"\"This method returns a boolean indicating whether the discriminator for the `choice`\n        is the same as that being used for the outermost tagged union. This is used to\n        determine whether this TaggedUnionSchema choice should be \"coalesced\" into the top level,\n        or whether it should be treated as a separate (nested) choice.\n        \"\"\"\n        inner_discriminator = choice['discriminator']\n        return inner_discriminator == self.discriminator or (\n            isinstance(inner_discriminator, list)\n            and (self.discriminator in inner_discriminator or [self.discriminator] in inner_discriminator)\n        )\n\n    def _infer_discriminator_values_for_choice(  # noqa C901\n        self, choice: core_schema.CoreSchema, source_name: str | None\n    ) -> list[str | int]:\n        \"\"\"This function recurses over `choice`, extracting all discriminator values that should map to this choice.\n\n        `model_name` is accepted for the purpose of producing useful error messages.\n        \"\"\"\n        if choice['type'] == 'definitions':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=source_name)\n        elif choice['type'] == 'function-plain':\n            raise TypeError(\n                f'{choice[\"type\"]!r} is not a valid discriminated union variant;'\n                ' should be a `BaseModel` or `dataclass`'\n            )\n        elif _core_utils.is_function_with_inner_schema(choice):\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=source_name)\n        elif choice['type'] == 'lax-or-strict':\n            return sorted(\n                set(\n                    self._infer_discriminator_values_for_choice(choice['lax_schema'], source_name=None)\n                    + self._infer_discriminator_values_for_choice(choice['strict_schema'], source_name=None)\n                )\n            )\n\n        elif choice['type'] == 'tagged-union':\n            values: list[str | int] = []\n            # Ignore str/int \"choices\" since these are just references to other choices\n            subchoices = [x for x in choice['choices'].values() if not isinstance(x, (str, int))]\n            for subchoice in subchoices:\n                subchoice_values = self._infer_discriminator_values_for_choice(subchoice, source_name=None)\n                values.extend(subchoice_values)\n            return values\n\n        elif choice['type'] == 'union':\n            values = []\n            for subchoice in choice['choices']:\n                subchoice_schema = subchoice[0] if isinstance(subchoice, tuple) else subchoice\n                subchoice_values = self._infer_discriminator_values_for_choice(subchoice_schema, source_name=None)\n                values.extend(subchoice_values)\n            return values\n\n        elif choice['type'] == 'nullable':\n            self._should_be_nullable = True\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=None)\n\n        elif choice['type'] == 'model':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=choice['cls'].__name__)\n\n        elif choice['type'] == 'dataclass':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=choice['cls'].__name__)\n\n        elif choice['type'] == 'model-fields':\n            return self._infer_discriminator_values_for_model_choice(choice, source_name=source_name)\n\n        elif choice['type'] == 'dataclass-args':\n            return self._infer_discriminator_values_for_dataclass_choice(choice, source_name=source_name)\n\n        elif choice['type'] == 'typed-dict':\n            return self._infer_discriminator_values_for_typed_dict_choice(choice, source_name=source_name)\n\n        elif choice['type'] == 'definition-ref':\n            schema_ref = choice['schema_ref']\n            if schema_ref not in self.definitions:\n                raise MissingDefinitionForUnionRef(schema_ref)\n            return self._infer_discriminator_values_for_choice(self.definitions[schema_ref], source_name=source_name)\n        else:\n            raise TypeError(\n                f'{choice[\"type\"]!r} is not a valid discriminated union variant;'\n                ' should be a `BaseModel` or `dataclass`'\n            )\n\n    def _infer_discriminator_values_for_typed_dict_choice(\n        self, choice: core_schema.TypedDictSchema, source_name: str | None = None\n    ) -> list[str | int]:\n        \"\"\"This method just extracts the _infer_discriminator_values_for_choice logic specific to TypedDictSchema\n        for the sake of readability.\n        \"\"\"\n        source = 'TypedDict' if source_name is None else f'TypedDict {source_name!r}'\n        field = choice['fields'].get(self.discriminator)\n        if field is None:\n            raise PydanticUserError(\n                f'{source} needs a discriminator field for key {self.discriminator!r}', code='discriminator-no-field'\n            )\n        return self._infer_discriminator_values_for_field(field, source)\n\n    def _infer_discriminator_values_for_model_choice(\n        self, choice: core_schema.ModelFieldsSchema, source_name: str | None = None\n    ) -> list[str | int]:\n        source = 'ModelFields' if source_name is None else f'Model {source_name!r}'\n        field = choice['fields'].get(self.discriminator)\n        if field is None:\n            raise PydanticUserError(\n                f'{source} needs a discriminator field for key {self.discriminator!r}', code='discriminator-no-field'\n            )\n        return self._infer_discriminator_values_for_field(field, source)\n\n    def _infer_discriminator_values_for_dataclass_choice(\n        self, choice: core_schema.DataclassArgsSchema, source_name: str | None = None\n    ) -> list[str | int]:\n        source = 'DataclassArgs' if source_name is None else f'Dataclass {source_name!r}'\n        for field in choice['fields']:\n            if field['name'] == self.discriminator:\n                break\n        else:\n            raise PydanticUserError(\n                f'{source} needs a discriminator field for key {self.discriminator!r}', code='discriminator-no-field'\n            )\n        return self._infer_discriminator_values_for_field(field, source)\n\n    def _infer_discriminator_values_for_field(self, field: CoreSchemaField, source: str) -> list[str | int]:\n        if field['type'] == 'computed-field':\n            # This should never occur as a discriminator, as it is only relevant to serialization\n            return []\n        alias = field.get('validation_alias', self.discriminator)\n        if not isinstance(alias, str):\n            raise PydanticUserError(\n                f'Alias {alias!r} is not supported in a discriminated union', code='discriminator-alias-type'\n            )\n        if self._discriminator_alias is None:\n            self._discriminator_alias = alias\n        elif self._discriminator_alias != alias:\n            raise PydanticUserError(\n                f'Aliases for discriminator {self.discriminator!r} must be the same '\n                f'(got {alias}, {self._discriminator_alias})',\n                code='discriminator-alias',\n            )\n        return self._infer_discriminator_values_for_inner_schema(field['schema'], source)\n\n    def _infer_discriminator_values_for_inner_schema(\n        self, schema: core_schema.CoreSchema, source: str\n    ) -> list[str | int]:\n        \"\"\"When inferring discriminator values for a field, we typically extract the expected values from a literal\n        schema. This function does that, but also handles nested unions and defaults.\n        \"\"\"\n        if schema['type'] == 'literal':\n            return schema['expected']\n\n        elif schema['type'] == 'union':\n            # Generally when multiple values are allowed they should be placed in a single `Literal`, but\n            # we add this case to handle the situation where a field is annotated as a `Union` of `Literal`s.\n            # For example, this lets us handle `Union[Literal['key'], Union[Literal['Key'], Literal['KEY']]]`\n            values: list[Any] = []\n            for choice in schema['choices']:\n                choice_schema = choice[0] if isinstance(choice, tuple) else choice\n                choice_values = self._infer_discriminator_values_for_inner_schema(choice_schema, source)\n                values.extend(choice_values)\n            return values\n\n        elif schema['type'] == 'default':\n            # This will happen if the field has a default value; we ignore it while extracting the discriminator values\n            return self._infer_discriminator_values_for_inner_schema(schema['schema'], source)\n\n        elif schema['type'] == 'function-after':\n            # After validators don't affect the discriminator values\n            return self._infer_discriminator_values_for_inner_schema(schema['schema'], source)\n\n        elif schema['type'] in {'function-before', 'function-wrap', 'function-plain'}:\n            validator_type = repr(schema['type'].split('-')[1])\n            raise PydanticUserError(\n                f'Cannot use a mode={validator_type} validator in the'\n                f' discriminator field {self.discriminator!r} of {source}',\n                code='discriminator-validator',\n            )\n\n        else:\n            raise PydanticUserError(\n                f'{source} needs field {self.discriminator!r} to be of type `Literal`',\n                code='discriminator-needs-literal',\n            )\n\n    def _set_unique_choice_for_values(self, choice: core_schema.CoreSchema, values: Sequence[str | int]) -> None:\n        \"\"\"This method updates `self.tagged_union_choices` so that all provided (discriminator) `values` map to the\n        provided `choice`, validating that none of these values already map to another (different) choice.\n        \"\"\"\n        for discriminator_value in values:\n            if discriminator_value in self._tagged_union_choices:\n                # It is okay if `value` is already in tagged_union_choices as long as it maps to the same value.\n                # Because tagged_union_choices may map values to other values, we need to walk the choices dict\n                # until we get to a \"real\" choice, and confirm that is equal to the one assigned.\n                existing_choice = self._tagged_union_choices[discriminator_value]\n                if existing_choice != choice:\n                    raise TypeError(\n                        f'Value {discriminator_value!r} for discriminator '\n                        f'{self.discriminator!r} mapped to multiple choices'\n                    )\n            else:\n                self._tagged_union_choices[discriminator_value] = choice\n", "pydantic/_internal/_generate_schema.py": "\"\"\"Convert python types to pydantic-core schema.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport collections.abc\nimport dataclasses\nimport inspect\nimport re\nimport sys\nimport typing\nimport warnings\nfrom contextlib import ExitStack, contextmanager\nfrom copy import copy, deepcopy\nfrom enum import Enum\nfrom functools import partial\nfrom inspect import Parameter, _ParameterKind, signature\nfrom itertools import chain\nfrom operator import attrgetter\nfrom types import FunctionType, LambdaType, MethodType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Final,\n    ForwardRef,\n    Iterable,\n    Iterator,\n    Mapping,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\nfrom warnings import warn\n\nfrom pydantic_core import CoreSchema, PydanticUndefined, core_schema, to_jsonable_python\nfrom typing_extensions import Annotated, Literal, TypeAliasType, TypedDict, get_args, get_origin, is_typeddict\n\nfrom ..aliases import AliasGenerator\nfrom ..annotated_handlers import GetCoreSchemaHandler, GetJsonSchemaHandler\nfrom ..config import ConfigDict, JsonDict, JsonEncoder\nfrom ..errors import PydanticSchemaGenerationError, PydanticUndefinedAnnotation, PydanticUserError\nfrom ..json_schema import JsonSchemaValue\nfrom ..version import version_short\nfrom ..warnings import PydanticDeprecatedSince20\nfrom . import _core_utils, _decorators, _discriminated_union, _known_annotated_metadata, _typing_extra\nfrom ._config import ConfigWrapper, ConfigWrapperStack\nfrom ._core_metadata import CoreMetadataHandler, build_metadata_dict\nfrom ._core_utils import (\n    CoreSchemaOrField,\n    collect_invalid_schemas,\n    define_expected_missing_refs,\n    get_ref,\n    get_type_ref,\n    is_function_with_inner_schema,\n    is_list_like_schema_with_items_schema,\n    simplify_schema_references,\n    validate_core_schema,\n)\nfrom ._decorators import (\n    Decorator,\n    DecoratorInfos,\n    FieldSerializerDecoratorInfo,\n    FieldValidatorDecoratorInfo,\n    ModelSerializerDecoratorInfo,\n    ModelValidatorDecoratorInfo,\n    RootValidatorDecoratorInfo,\n    ValidatorDecoratorInfo,\n    get_attribute_from_bases,\n    inspect_field_serializer,\n    inspect_model_serializer,\n    inspect_validator,\n)\nfrom ._docs_extraction import extract_docstrings_from_cls\nfrom ._fields import collect_dataclass_fields, get_type_hints_infer_globalns\nfrom ._forward_ref import PydanticRecursiveRef\nfrom ._generics import get_standard_typevars_map, has_instance_in_type, recursively_defined_type_refs, replace_types\nfrom ._mock_val_ser import MockCoreSchema\nfrom ._schema_generation_shared import CallbackGetCoreSchemaHandler\nfrom ._typing_extra import is_finalvar, is_self_type\nfrom ._utils import lenient_issubclass\n\nif TYPE_CHECKING:\n    from ..fields import ComputedFieldInfo, FieldInfo\n    from ..main import BaseModel\n    from ..types import Discriminator\n    from ..validators import FieldValidatorModes\n    from ._dataclasses import StandardDataclass\n    from ._schema_generation_shared import GetJsonSchemaFunction\n\n_SUPPORTS_TYPEDDICT = sys.version_info >= (3, 12)\n_AnnotatedType = type(Annotated[int, 123])\n\nFieldDecoratorInfo = Union[ValidatorDecoratorInfo, FieldValidatorDecoratorInfo, FieldSerializerDecoratorInfo]\nFieldDecoratorInfoType = TypeVar('FieldDecoratorInfoType', bound=FieldDecoratorInfo)\nAnyFieldDecorator = Union[\n    Decorator[ValidatorDecoratorInfo],\n    Decorator[FieldValidatorDecoratorInfo],\n    Decorator[FieldSerializerDecoratorInfo],\n]\n\nModifyCoreSchemaWrapHandler = GetCoreSchemaHandler\nGetCoreSchemaFunction = Callable[[Any, ModifyCoreSchemaWrapHandler], core_schema.CoreSchema]\n\nTUPLE_TYPES: list[type] = [tuple, typing.Tuple]\nLIST_TYPES: list[type] = [list, typing.List, collections.abc.MutableSequence]\nSET_TYPES: list[type] = [set, typing.Set, collections.abc.MutableSet]\nFROZEN_SET_TYPES: list[type] = [frozenset, typing.FrozenSet, collections.abc.Set]\nDICT_TYPES: list[type] = [dict, typing.Dict, collections.abc.MutableMapping, collections.abc.Mapping]\n\n\ndef check_validator_fields_against_field_name(\n    info: FieldDecoratorInfo,\n    field: str,\n) -> bool:\n    \"\"\"Check if field name is in validator fields.\n\n    Args:\n        info: The field info.\n        field: The field name to check.\n\n    Returns:\n        `True` if field name is in validator fields, `False` otherwise.\n    \"\"\"\n    if '*' in info.fields:\n        return True\n    for v_field_name in info.fields:\n        if v_field_name == field:\n            return True\n    return False\n\n\ndef check_decorator_fields_exist(decorators: Iterable[AnyFieldDecorator], fields: Iterable[str]) -> None:\n    \"\"\"Check if the defined fields in decorators exist in `fields` param.\n\n    It ignores the check for a decorator if the decorator has `*` as field or `check_fields=False`.\n\n    Args:\n        decorators: An iterable of decorators.\n        fields: An iterable of fields name.\n\n    Raises:\n        PydanticUserError: If one of the field names does not exist in `fields` param.\n    \"\"\"\n    fields = set(fields)\n    for dec in decorators:\n        if '*' in dec.info.fields:\n            continue\n        if dec.info.check_fields is False:\n            continue\n        for field in dec.info.fields:\n            if field not in fields:\n                raise PydanticUserError(\n                    f'Decorators defined with incorrect fields: {dec.cls_ref}.{dec.cls_var_name}'\n                    \" (use check_fields=False if you're inheriting from the model and intended this)\",\n                    code='decorator-missing-field',\n                )\n\n\ndef filter_field_decorator_info_by_field(\n    validator_functions: Iterable[Decorator[FieldDecoratorInfoType]], field: str\n) -> list[Decorator[FieldDecoratorInfoType]]:\n    return [dec for dec in validator_functions if check_validator_fields_against_field_name(dec.info, field)]\n\n\ndef apply_each_item_validators(\n    schema: core_schema.CoreSchema,\n    each_item_validators: list[Decorator[ValidatorDecoratorInfo]],\n    field_name: str | None,\n) -> core_schema.CoreSchema:\n    # This V1 compatibility shim should eventually be removed\n\n    # push down any `each_item=True` validators\n    # note that this won't work for any Annotated types that get wrapped by a function validator\n    # but that's okay because that didn't exist in V1\n    if schema['type'] == 'nullable':\n        schema['schema'] = apply_each_item_validators(schema['schema'], each_item_validators, field_name)\n        return schema\n    elif schema['type'] == 'tuple':\n        if (variadic_item_index := schema.get('variadic_item_index')) is not None:\n            schema['items_schema'][variadic_item_index] = apply_validators(\n                schema['items_schema'][variadic_item_index], each_item_validators, field_name\n            )\n    elif is_list_like_schema_with_items_schema(schema):\n        inner_schema = schema.get('items_schema', None)\n        if inner_schema is None:\n            inner_schema = core_schema.any_schema()\n        schema['items_schema'] = apply_validators(inner_schema, each_item_validators, field_name)\n    elif schema['type'] == 'dict':\n        # push down any `each_item=True` validators onto dict _values_\n        # this is super arbitrary but it's the V1 behavior\n        inner_schema = schema.get('values_schema', None)\n        if inner_schema is None:\n            inner_schema = core_schema.any_schema()\n        schema['values_schema'] = apply_validators(inner_schema, each_item_validators, field_name)\n    elif each_item_validators:\n        raise TypeError(\n            f\"`@validator(..., each_item=True)` cannot be applied to fields with a schema of {schema['type']}\"\n        )\n    return schema\n\n\ndef modify_model_json_schema(\n    schema_or_field: CoreSchemaOrField,\n    handler: GetJsonSchemaHandler,\n    *,\n    cls: Any,\n    title: str | None = None,\n) -> JsonSchemaValue:\n    \"\"\"Add title and description for model-like classes' JSON schema.\n\n    Args:\n        schema_or_field: The schema data to generate a JSON schema from.\n        handler: The `GetCoreSchemaHandler` instance.\n        cls: The model-like class.\n        title: The title to set for the model's schema, defaults to the model's name\n\n    Returns:\n        JsonSchemaValue: The updated JSON schema.\n    \"\"\"\n    from ..dataclasses import is_pydantic_dataclass\n    from ..main import BaseModel\n    from ..root_model import RootModel\n    from ._dataclasses import is_builtin_dataclass\n\n    json_schema = handler(schema_or_field)\n    original_schema = handler.resolve_ref_schema(json_schema)\n    # Preserve the fact that definitions schemas should never have sibling keys:\n    if '$ref' in original_schema:\n        ref = original_schema['$ref']\n        original_schema.clear()\n        original_schema['allOf'] = [{'$ref': ref}]\n    if title is not None:\n        original_schema['title'] = title\n    elif 'title' not in original_schema:\n        original_schema['title'] = cls.__name__\n    # BaseModel + Dataclass; don't use cls.__doc__ as it will contain the verbose class signature by default\n    docstring = None if cls is BaseModel or is_builtin_dataclass(cls) or is_pydantic_dataclass(cls) else cls.__doc__\n    if docstring and 'description' not in original_schema:\n        original_schema['description'] = inspect.cleandoc(docstring)\n    elif issubclass(cls, RootModel) and cls.model_fields['root'].description:\n        original_schema['description'] = cls.model_fields['root'].description\n    return json_schema\n\n\nJsonEncoders = Dict[Type[Any], JsonEncoder]\n\n\ndef _add_custom_serialization_from_json_encoders(\n    json_encoders: JsonEncoders | None, tp: Any, schema: CoreSchema\n) -> CoreSchema:\n    \"\"\"Iterate over the json_encoders and add the first matching encoder to the schema.\n\n    Args:\n        json_encoders: A dictionary of types and their encoder functions.\n        tp: The type to check for a matching encoder.\n        schema: The schema to add the encoder to.\n    \"\"\"\n    if not json_encoders:\n        return schema\n    if 'serialization' in schema:\n        return schema\n    # Check the class type and its superclasses for a matching encoder\n    # Decimal.__class__.__mro__ (and probably other cases) doesn't include Decimal itself\n    # if the type is a GenericAlias (e.g. from list[int]) we need to use __class__ instead of .__mro__\n    for base in (tp, *getattr(tp, '__mro__', tp.__class__.__mro__)[:-1]):\n        encoder = json_encoders.get(base)\n        if encoder is None:\n            continue\n\n        warnings.warn(\n            f'`json_encoders` is deprecated. See https://docs.pydantic.dev/{version_short()}/concepts/serialization/#custom-serializers for alternatives',\n            PydanticDeprecatedSince20,\n        )\n\n        # TODO: in theory we should check that the schema accepts a serialization key\n        schema['serialization'] = core_schema.plain_serializer_function_ser_schema(encoder, when_used='json')\n        return schema\n\n    return schema\n\n\nTypesNamespace = Union[Dict[str, Any], None]\n\n\nclass TypesNamespaceStack:\n    \"\"\"A stack of types namespaces.\"\"\"\n\n    def __init__(self, types_namespace: TypesNamespace):\n        self._types_namespace_stack: list[TypesNamespace] = [types_namespace]\n\n    @property\n    def tail(self) -> TypesNamespace:\n        return self._types_namespace_stack[-1]\n\n    @contextmanager\n    def push(self, for_type: type[Any]):\n        types_namespace = {**_typing_extra.get_cls_types_namespace(for_type), **(self.tail or {})}\n        self._types_namespace_stack.append(types_namespace)\n        try:\n            yield\n        finally:\n            self._types_namespace_stack.pop()\n\n\ndef _get_first_non_null(a: Any, b: Any) -> Any:\n    \"\"\"Return the first argument if it is not None, otherwise return the second argument.\n\n    Use case: serialization_alias (argument a) and alias (argument b) are both defined, and serialization_alias is ''.\n    This function will return serialization_alias, which is the first argument, even though it is an empty string.\n    \"\"\"\n    return a if a is not None else b\n\n\nclass GenerateSchema:\n    \"\"\"Generate core schema for a Pydantic model, dataclass and types like `str`, `datetime`, ... .\"\"\"\n\n    __slots__ = (\n        '_config_wrapper_stack',\n        '_types_namespace_stack',\n        '_typevars_map',\n        'field_name_stack',\n        'model_type_stack',\n        'defs',\n    )\n\n    def __init__(\n        self,\n        config_wrapper: ConfigWrapper,\n        types_namespace: dict[str, Any] | None,\n        typevars_map: dict[Any, Any] | None = None,\n    ) -> None:\n        # we need a stack for recursing into child models\n        self._config_wrapper_stack = ConfigWrapperStack(config_wrapper)\n        self._types_namespace_stack = TypesNamespaceStack(types_namespace)\n        self._typevars_map = typevars_map\n        self.field_name_stack = _FieldNameStack()\n        self.model_type_stack = _ModelTypeStack()\n        self.defs = _Definitions()\n\n    @classmethod\n    def __from_parent(\n        cls,\n        config_wrapper_stack: ConfigWrapperStack,\n        types_namespace_stack: TypesNamespaceStack,\n        model_type_stack: _ModelTypeStack,\n        typevars_map: dict[Any, Any] | None,\n        defs: _Definitions,\n    ) -> GenerateSchema:\n        obj = cls.__new__(cls)\n        obj._config_wrapper_stack = config_wrapper_stack\n        obj._types_namespace_stack = types_namespace_stack\n        obj.model_type_stack = model_type_stack\n        obj._typevars_map = typevars_map\n        obj.field_name_stack = _FieldNameStack()\n        obj.defs = defs\n        return obj\n\n    @property\n    def _config_wrapper(self) -> ConfigWrapper:\n        return self._config_wrapper_stack.tail\n\n    @property\n    def _types_namespace(self) -> dict[str, Any] | None:\n        return self._types_namespace_stack.tail\n\n    @property\n    def _current_generate_schema(self) -> GenerateSchema:\n        cls = self._config_wrapper.schema_generator or GenerateSchema\n        return cls.__from_parent(\n            self._config_wrapper_stack,\n            self._types_namespace_stack,\n            self.model_type_stack,\n            self._typevars_map,\n            self.defs,\n        )\n\n    @property\n    def _arbitrary_types(self) -> bool:\n        return self._config_wrapper.arbitrary_types_allowed\n\n    def str_schema(self) -> CoreSchema:\n        \"\"\"Generate a CoreSchema for `str`\"\"\"\n        return core_schema.str_schema()\n\n    # the following methods can be overridden but should be considered\n    # unstable / private APIs\n    def _list_schema(self, tp: Any, items_type: Any) -> CoreSchema:\n        return core_schema.list_schema(self.generate_schema(items_type))\n\n    def _dict_schema(self, tp: Any, keys_type: Any, values_type: Any) -> CoreSchema:\n        return core_schema.dict_schema(self.generate_schema(keys_type), self.generate_schema(values_type))\n\n    def _set_schema(self, tp: Any, items_type: Any) -> CoreSchema:\n        return core_schema.set_schema(self.generate_schema(items_type))\n\n    def _frozenset_schema(self, tp: Any, items_type: Any) -> CoreSchema:\n        return core_schema.frozenset_schema(self.generate_schema(items_type))\n\n    def _arbitrary_type_schema(self, tp: Any) -> CoreSchema:\n        if not isinstance(tp, type):\n            warn(\n                f'{tp!r} is not a Python type (it may be an instance of an object),'\n                ' Pydantic will allow any object with no validation since we cannot even'\n                ' enforce that the input is an instance of the given type.'\n                ' To get rid of this error wrap the type with `pydantic.SkipValidation`.',\n                UserWarning,\n            )\n            return core_schema.any_schema()\n        return core_schema.is_instance_schema(tp)\n\n    def _unknown_type_schema(self, obj: Any) -> CoreSchema:\n        raise PydanticSchemaGenerationError(\n            f'Unable to generate pydantic-core schema for {obj!r}. '\n            'Set `arbitrary_types_allowed=True` in the model_config to ignore this error'\n            ' or implement `__get_pydantic_core_schema__` on your type to fully support it.'\n            '\\n\\nIf you got this error by calling handler(<some type>) within'\n            ' `__get_pydantic_core_schema__` then you likely need to call'\n            ' `handler.generate_schema(<some type>)` since we do not call'\n            ' `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.'\n        )\n\n    def _apply_discriminator_to_union(\n        self, schema: CoreSchema, discriminator: str | Discriminator | None\n    ) -> CoreSchema:\n        if discriminator is None:\n            return schema\n        try:\n            return _discriminated_union.apply_discriminator(\n                schema,\n                discriminator,\n            )\n        except _discriminated_union.MissingDefinitionForUnionRef:\n            # defer until defs are resolved\n            _discriminated_union.set_discriminator_in_metadata(\n                schema,\n                discriminator,\n            )\n            return schema\n\n    class CollectedInvalid(Exception):\n        pass\n\n    def clean_schema(self, schema: CoreSchema) -> CoreSchema:\n        schema = self.collect_definitions(schema)\n        schema = simplify_schema_references(schema)\n        if collect_invalid_schemas(schema):\n            raise self.CollectedInvalid()\n        schema = _discriminated_union.apply_discriminators(schema)\n        schema = validate_core_schema(schema)\n        return schema\n\n    def collect_definitions(self, schema: CoreSchema) -> CoreSchema:\n        ref = cast('str | None', schema.get('ref', None))\n        if ref:\n            self.defs.definitions[ref] = schema\n        if 'ref' in schema:\n            schema = core_schema.definition_reference_schema(schema['ref'])\n        return core_schema.definitions_schema(\n            schema,\n            list(self.defs.definitions.values()),\n        )\n\n    def _add_js_function(self, metadata_schema: CoreSchema, js_function: Callable[..., Any]) -> None:\n        metadata = CoreMetadataHandler(metadata_schema).metadata\n        pydantic_js_functions = metadata.setdefault('pydantic_js_functions', [])\n        # because of how we generate core schemas for nested generic models\n        # we can end up adding `BaseModel.__get_pydantic_json_schema__` multiple times\n        # this check may fail to catch duplicates if the function is a `functools.partial`\n        # or something like that\n        # but if it does it'll fail by inserting the duplicate\n        if js_function not in pydantic_js_functions:\n            pydantic_js_functions.append(js_function)\n\n    def generate_schema(\n        self,\n        obj: Any,\n        from_dunder_get_core_schema: bool = True,\n    ) -> core_schema.CoreSchema:\n        \"\"\"Generate core schema.\n\n        Args:\n            obj: The object to generate core schema for.\n            from_dunder_get_core_schema: Whether to generate schema from either the\n                `__get_pydantic_core_schema__` function or `__pydantic_core_schema__` property.\n\n        Returns:\n            The generated core schema.\n\n        Raises:\n            PydanticUndefinedAnnotation:\n                If it is not possible to evaluate forward reference.\n            PydanticSchemaGenerationError:\n                If it is not possible to generate pydantic-core schema.\n            TypeError:\n                - If `alias_generator` returns a disallowed type (must be str, AliasPath or AliasChoices).\n                - If V1 style validator with `each_item=True` applied on a wrong field.\n            PydanticUserError:\n                - If `typing.TypedDict` is used instead of `typing_extensions.TypedDict` on Python < 3.12.\n                - If `__modify_schema__` method is used instead of `__get_pydantic_json_schema__`.\n        \"\"\"\n        schema: CoreSchema | None = None\n\n        if from_dunder_get_core_schema:\n            from_property = self._generate_schema_from_property(obj, obj)\n            if from_property is not None:\n                schema = from_property\n\n        if schema is None:\n            schema = self._generate_schema_inner(obj)\n\n        metadata_js_function = _extract_get_pydantic_json_schema(obj, schema)\n        if metadata_js_function is not None:\n            metadata_schema = resolve_original_schema(schema, self.defs.definitions)\n            if metadata_schema:\n                self._add_js_function(metadata_schema, metadata_js_function)\n\n        schema = _add_custom_serialization_from_json_encoders(self._config_wrapper.json_encoders, obj, schema)\n\n        return schema\n\n    def _model_schema(self, cls: type[BaseModel]) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a Pydantic model.\"\"\"\n        with self.defs.get_schema_or_ref(cls) as (model_ref, maybe_schema):\n            if maybe_schema is not None:\n                return maybe_schema\n\n            fields = cls.model_fields\n            decorators = cls.__pydantic_decorators__\n            computed_fields = decorators.computed_fields\n            check_decorator_fields_exist(\n                chain(\n                    decorators.field_validators.values(),\n                    decorators.field_serializers.values(),\n                    decorators.validators.values(),\n                ),\n                {*fields.keys(), *computed_fields.keys()},\n            )\n            config_wrapper = ConfigWrapper(cls.model_config, check=False)\n            core_config = config_wrapper.core_config(cls)\n            title = self._get_model_title_from_config(cls, config_wrapper)\n            metadata = build_metadata_dict(js_functions=[partial(modify_model_json_schema, cls=cls, title=title)])\n\n            model_validators = decorators.model_validators.values()\n\n            extras_schema = None\n            if core_config.get('extra_fields_behavior') == 'allow':\n                assert cls.__mro__[0] is cls\n                assert cls.__mro__[-1] is object\n                for candidate_cls in cls.__mro__[:-1]:\n                    extras_annotation = getattr(candidate_cls, '__annotations__', {}).get('__pydantic_extra__', None)\n                    if extras_annotation is not None:\n                        if isinstance(extras_annotation, str):\n                            extras_annotation = _typing_extra.eval_type_backport(\n                                _typing_extra._make_forward_ref(extras_annotation, is_argument=False, is_class=True),\n                                self._types_namespace,\n                            )\n                        tp = get_origin(extras_annotation)\n                        if tp not in (Dict, dict):\n                            raise PydanticSchemaGenerationError(\n                                'The type annotation for `__pydantic_extra__` must be `Dict[str, ...]`'\n                            )\n                        extra_items_type = self._get_args_resolving_forward_refs(\n                            extras_annotation,\n                            required=True,\n                        )[1]\n                        if extra_items_type is not Any:\n                            extras_schema = self.generate_schema(extra_items_type)\n                            break\n\n            with self._config_wrapper_stack.push(config_wrapper), self._types_namespace_stack.push(cls):\n                self = self._current_generate_schema\n                if cls.__pydantic_root_model__:\n                    root_field = self._common_field_schema('root', fields['root'], decorators)\n                    inner_schema = root_field['schema']\n                    inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n                    model_schema = core_schema.model_schema(\n                        cls,\n                        inner_schema,\n                        custom_init=getattr(cls, '__pydantic_custom_init__', None),\n                        root_model=True,\n                        post_init=getattr(cls, '__pydantic_post_init__', None),\n                        config=core_config,\n                        ref=model_ref,\n                        metadata=metadata,\n                    )\n                else:\n                    fields_schema: core_schema.CoreSchema = core_schema.model_fields_schema(\n                        {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n                        computed_fields=[\n                            self._computed_field_schema(d, decorators.field_serializers)\n                            for d in computed_fields.values()\n                        ],\n                        extras_schema=extras_schema,\n                        model_name=cls.__name__,\n                    )\n                    inner_schema = apply_validators(fields_schema, decorators.root_validators.values(), None)\n                    new_inner_schema = define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n                    if new_inner_schema is not None:\n                        inner_schema = new_inner_schema\n                    inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n\n                    model_schema = core_schema.model_schema(\n                        cls,\n                        inner_schema,\n                        custom_init=getattr(cls, '__pydantic_custom_init__', None),\n                        root_model=False,\n                        post_init=getattr(cls, '__pydantic_post_init__', None),\n                        config=core_config,\n                        ref=model_ref,\n                        metadata=metadata,\n                    )\n\n                schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n                schema = apply_model_validators(schema, model_validators, 'outer')\n                self.defs.definitions[model_ref] = schema\n                return core_schema.definition_reference_schema(model_ref)\n\n    @staticmethod\n    def _get_model_title_from_config(\n        model: type[BaseModel | StandardDataclass], config_wrapper: ConfigWrapper | None = None\n    ) -> str | None:\n        \"\"\"Get the title of a model if `model_title_generator` or `title` are set in the config, else return None\"\"\"\n        if config_wrapper is None:\n            return None\n\n        if config_wrapper.title:\n            return config_wrapper.title\n\n        model_title_generator = config_wrapper.model_title_generator\n        if model_title_generator:\n            title = model_title_generator(model)\n            if not isinstance(title, str):\n                raise TypeError(f'model_title_generator {model_title_generator} must return str, not {title.__class__}')\n            return title\n\n        return None\n\n    def _unpack_refs_defs(self, schema: CoreSchema) -> CoreSchema:\n        \"\"\"Unpack all 'definitions' schemas into `GenerateSchema.defs.definitions`\n        and return the inner schema.\n        \"\"\"\n\n        def get_ref(s: CoreSchema) -> str:\n            return s['ref']  # type: ignore\n\n        if schema['type'] == 'definitions':\n            self.defs.definitions.update({get_ref(s): s for s in schema['definitions']})\n            schema = schema['schema']\n        return schema\n\n    def _generate_schema_from_property(self, obj: Any, source: Any) -> core_schema.CoreSchema | None:\n        \"\"\"Try to generate schema from either the `__get_pydantic_core_schema__` function or\n        `__pydantic_core_schema__` property.\n\n        Note: `__get_pydantic_core_schema__` takes priority so it can\n        decide whether to use a `__pydantic_core_schema__` attribute, or generate a fresh schema.\n        \"\"\"\n        # avoid calling `__get_pydantic_core_schema__` if we've already visited this object\n        if is_self_type(obj):\n            obj = self.model_type_stack.get()\n        with self.defs.get_schema_or_ref(obj) as (_, maybe_schema):\n            if maybe_schema is not None:\n                return maybe_schema\n        if obj is source:\n            ref_mode = 'unpack'\n        else:\n            ref_mode = 'to-def'\n\n        schema: CoreSchema\n\n        if (get_schema := getattr(obj, '__get_pydantic_core_schema__', None)) is not None:\n            if len(inspect.signature(get_schema).parameters) == 1:\n                # (source) -> CoreSchema\n                schema = get_schema(source)\n            else:\n                schema = get_schema(\n                    source, CallbackGetCoreSchemaHandler(self._generate_schema_inner, self, ref_mode=ref_mode)\n                )\n        # fmt: off\n        elif (\n            (existing_schema := getattr(obj, '__pydantic_core_schema__', None)) is not None\n            and not isinstance(existing_schema, MockCoreSchema)\n            and existing_schema.get('cls', None) == obj\n        ):\n            schema = existing_schema\n        # fmt: on\n        elif (validators := getattr(obj, '__get_validators__', None)) is not None:\n            warn(\n                '`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n                PydanticDeprecatedSince20,\n            )\n            schema = core_schema.chain_schema([core_schema.with_info_plain_validator_function(v) for v in validators()])\n        else:\n            # we have no existing schema information on the property, exit early so that we can go generate a schema\n            return None\n\n        schema = self._unpack_refs_defs(schema)\n\n        if is_function_with_inner_schema(schema):\n            ref = schema['schema'].pop('ref', None)  # pyright: ignore[reportCallIssue, reportArgumentType]\n            if ref:\n                schema['ref'] = ref\n        else:\n            ref = get_ref(schema)\n\n        if ref:\n            self.defs.definitions[ref] = schema\n            return core_schema.definition_reference_schema(ref)\n\n        return schema\n\n    def _resolve_forward_ref(self, obj: Any) -> Any:\n        # we assume that types_namespace has the target of forward references in its scope,\n        # but this could fail, for example, if calling Validator on an imported type which contains\n        # forward references to other types only defined in the module from which it was imported\n        # `Validator(SomeImportedTypeAliasWithAForwardReference)`\n        # or the equivalent for BaseModel\n        # class Model(BaseModel):\n        #   x: SomeImportedTypeAliasWithAForwardReference\n        try:\n            obj = _typing_extra.eval_type_backport(obj, globalns=self._types_namespace)\n        except NameError as e:\n            raise PydanticUndefinedAnnotation.from_name_error(e) from e\n\n        # if obj is still a ForwardRef, it means we can't evaluate it, raise PydanticUndefinedAnnotation\n        if isinstance(obj, ForwardRef):\n            raise PydanticUndefinedAnnotation(obj.__forward_arg__, f'Unable to evaluate forward reference {obj}')\n\n        if self._typevars_map:\n            obj = replace_types(obj, self._typevars_map)\n\n        return obj\n\n    @overload\n    def _get_args_resolving_forward_refs(self, obj: Any, required: Literal[True]) -> tuple[Any, ...]: ...\n\n    @overload\n    def _get_args_resolving_forward_refs(self, obj: Any) -> tuple[Any, ...] | None: ...\n\n    def _get_args_resolving_forward_refs(self, obj: Any, required: bool = False) -> tuple[Any, ...] | None:\n        args = get_args(obj)\n        if args:\n            args = tuple([self._resolve_forward_ref(a) if isinstance(a, ForwardRef) else a for a in args])\n        elif required:  # pragma: no cover\n            raise TypeError(f'Expected {obj} to have generic parameters but it had none')\n        return args\n\n    def _get_first_arg_or_any(self, obj: Any) -> Any:\n        args = self._get_args_resolving_forward_refs(obj)\n        if not args:\n            return Any\n        return args[0]\n\n    def _get_first_two_args_or_any(self, obj: Any) -> tuple[Any, Any]:\n        args = self._get_args_resolving_forward_refs(obj)\n        if not args:\n            return (Any, Any)\n        if len(args) < 2:\n            origin = get_origin(obj)\n            raise TypeError(f'Expected two type arguments for {origin}, got 1')\n        return args[0], args[1]\n\n    def _generate_schema_inner(self, obj: Any) -> core_schema.CoreSchema:\n        if isinstance(obj, _AnnotatedType):\n            return self._annotated_schema(obj)\n\n        if isinstance(obj, dict):\n            # we assume this is already a valid schema\n            return obj  # type: ignore[return-value]\n\n        if isinstance(obj, str):\n            obj = ForwardRef(obj)\n\n        if isinstance(obj, ForwardRef):\n            return self.generate_schema(self._resolve_forward_ref(obj))\n\n        from ..main import BaseModel\n\n        if lenient_issubclass(obj, BaseModel):\n            with self.model_type_stack.push(obj):\n                return self._model_schema(obj)\n\n        if isinstance(obj, PydanticRecursiveRef):\n            return core_schema.definition_reference_schema(schema_ref=obj.type_ref)\n\n        return self.match_type(obj)\n\n    def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901\n        \"\"\"Main mapping of types to schemas.\n\n        The general structure is a series of if statements starting with the simple cases\n        (non-generic primitive types) and then handling generics and other more complex cases.\n\n        Each case either generates a schema directly, calls into a public user-overridable method\n        (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some\n        boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).\n\n        The idea is that we'll evolve this into adding more and more user facing methods over time\n        as they get requested and we figure out what the right API for them is.\n        \"\"\"\n        if obj is str:\n            return self.str_schema()\n        elif obj is bytes:\n            return core_schema.bytes_schema()\n        elif obj is int:\n            return core_schema.int_schema()\n        elif obj is float:\n            return core_schema.float_schema()\n        elif obj is bool:\n            return core_schema.bool_schema()\n        elif obj is Any or obj is object:\n            return core_schema.any_schema()\n        elif obj is None or obj is _typing_extra.NoneType:\n            return core_schema.none_schema()\n        elif obj in TUPLE_TYPES:\n            return self._tuple_schema(obj)\n        elif obj in LIST_TYPES:\n            return self._list_schema(obj, self._get_first_arg_or_any(obj))\n        elif obj in SET_TYPES:\n            return self._set_schema(obj, self._get_first_arg_or_any(obj))\n        elif obj in FROZEN_SET_TYPES:\n            return self._frozenset_schema(obj, self._get_first_arg_or_any(obj))\n        elif obj in DICT_TYPES:\n            return self._dict_schema(obj, *self._get_first_two_args_or_any(obj))\n        elif isinstance(obj, TypeAliasType):\n            return self._type_alias_type_schema(obj)\n        elif obj == type:\n            return self._type_schema()\n        elif _typing_extra.is_callable_type(obj):\n            return core_schema.callable_schema()\n        elif _typing_extra.is_literal_type(obj):\n            return self._literal_schema(obj)\n        elif is_typeddict(obj):\n            return self._typed_dict_schema(obj, None)\n        elif _typing_extra.is_namedtuple(obj):\n            return self._namedtuple_schema(obj, None)\n        elif _typing_extra.is_new_type(obj):\n            # NewType, can't use isinstance because it fails <3.10\n            return self.generate_schema(obj.__supertype__)\n        elif obj == re.Pattern:\n            return self._pattern_schema(obj)\n        elif obj is collections.abc.Hashable or obj is typing.Hashable:\n            return self._hashable_schema()\n        elif isinstance(obj, typing.TypeVar):\n            return self._unsubstituted_typevar_schema(obj)\n        elif is_finalvar(obj):\n            if obj is Final:\n                return core_schema.any_schema()\n            return self.generate_schema(\n                self._get_first_arg_or_any(obj),\n            )\n        elif isinstance(obj, (FunctionType, LambdaType, MethodType, partial)):\n            return self._callable_schema(obj)\n        elif inspect.isclass(obj) and issubclass(obj, Enum):\n            from ._std_types_schema import get_enum_core_schema\n\n            return get_enum_core_schema(obj, self._config_wrapper.config_dict)\n\n        if _typing_extra.is_dataclass(obj):\n            return self._dataclass_schema(obj, None)\n        res = self._get_prepare_pydantic_annotations_for_known_type(obj, ())\n        if res is not None:\n            source_type, annotations = res\n            return self._apply_annotations(source_type, annotations)\n\n        origin = get_origin(obj)\n        if origin is not None:\n            return self._match_generic_type(obj, origin)\n\n        if self._arbitrary_types:\n            return self._arbitrary_type_schema(obj)\n        return self._unknown_type_schema(obj)\n\n    def _match_generic_type(self, obj: Any, origin: Any) -> CoreSchema:  # noqa: C901\n        if isinstance(origin, TypeAliasType):\n            return self._type_alias_type_schema(obj)\n\n        # Need to handle generic dataclasses before looking for the schema properties because attribute accesses\n        # on _GenericAlias delegate to the origin type, so lose the information about the concrete parametrization\n        # As a result, currently, there is no way to cache the schema for generic dataclasses. This may be possible\n        # to resolve by modifying the value returned by `Generic.__class_getitem__`, but that is a dangerous game.\n        if _typing_extra.is_dataclass(origin):\n            return self._dataclass_schema(obj, origin)\n        if _typing_extra.is_namedtuple(origin):\n            return self._namedtuple_schema(obj, origin)\n\n        from_property = self._generate_schema_from_property(origin, obj)\n        if from_property is not None:\n            return from_property\n\n        if _typing_extra.origin_is_union(origin):\n            return self._union_schema(obj)\n        elif origin in TUPLE_TYPES:\n            return self._tuple_schema(obj)\n        elif origin in LIST_TYPES:\n            return self._list_schema(obj, self._get_first_arg_or_any(obj))\n        elif origin in SET_TYPES:\n            return self._set_schema(obj, self._get_first_arg_or_any(obj))\n        elif origin in FROZEN_SET_TYPES:\n            return self._frozenset_schema(obj, self._get_first_arg_or_any(obj))\n        elif origin in DICT_TYPES:\n            return self._dict_schema(obj, *self._get_first_two_args_or_any(obj))\n        elif is_typeddict(origin):\n            return self._typed_dict_schema(obj, origin)\n        elif origin in (typing.Type, type):\n            return self._subclass_schema(obj)\n        elif origin in {typing.Sequence, collections.abc.Sequence}:\n            return self._sequence_schema(obj)\n        elif origin in {typing.Iterable, collections.abc.Iterable, typing.Generator, collections.abc.Generator}:\n            return self._iterable_schema(obj)\n        elif origin in (re.Pattern, typing.Pattern):\n            return self._pattern_schema(obj)\n\n        if self._arbitrary_types:\n            return self._arbitrary_type_schema(origin)\n        return self._unknown_type_schema(obj)\n\n    def _generate_td_field_schema(\n        self,\n        name: str,\n        field_info: FieldInfo,\n        decorators: DecoratorInfos,\n        *,\n        required: bool = True,\n    ) -> core_schema.TypedDictField:\n        \"\"\"Prepare a TypedDictField to represent a model or typeddict field.\"\"\"\n        common_field = self._common_field_schema(name, field_info, decorators)\n        return core_schema.typed_dict_field(\n            common_field['schema'],\n            required=False if not field_info.is_required() else required,\n            serialization_exclude=common_field['serialization_exclude'],\n            validation_alias=common_field['validation_alias'],\n            serialization_alias=common_field['serialization_alias'],\n            metadata=common_field['metadata'],\n        )\n\n    def _generate_md_field_schema(\n        self,\n        name: str,\n        field_info: FieldInfo,\n        decorators: DecoratorInfos,\n    ) -> core_schema.ModelField:\n        \"\"\"Prepare a ModelField to represent a model field.\"\"\"\n        common_field = self._common_field_schema(name, field_info, decorators)\n        return core_schema.model_field(\n            common_field['schema'],\n            serialization_exclude=common_field['serialization_exclude'],\n            validation_alias=common_field['validation_alias'],\n            serialization_alias=common_field['serialization_alias'],\n            frozen=common_field['frozen'],\n            metadata=common_field['metadata'],\n        )\n\n    def _generate_dc_field_schema(\n        self,\n        name: str,\n        field_info: FieldInfo,\n        decorators: DecoratorInfos,\n    ) -> core_schema.DataclassField:\n        \"\"\"Prepare a DataclassField to represent the parameter/field, of a dataclass.\"\"\"\n        common_field = self._common_field_schema(name, field_info, decorators)\n        return core_schema.dataclass_field(\n            name,\n            common_field['schema'],\n            init=field_info.init,\n            init_only=field_info.init_var or None,\n            kw_only=None if field_info.kw_only else False,\n            serialization_exclude=common_field['serialization_exclude'],\n            validation_alias=common_field['validation_alias'],\n            serialization_alias=common_field['serialization_alias'],\n            frozen=common_field['frozen'],\n            metadata=common_field['metadata'],\n        )\n\n    @staticmethod\n    def _apply_alias_generator_to_field_info(\n        alias_generator: Callable[[str], str] | AliasGenerator, field_info: FieldInfo, field_name: str\n    ) -> None:\n        \"\"\"Apply an alias_generator to aliases on a FieldInfo instance if appropriate.\n\n        Args:\n            alias_generator: A callable that takes a string and returns a string, or an AliasGenerator instance.\n            field_info: The FieldInfo instance to which the alias_generator is (maybe) applied.\n            field_name: The name of the field from which to generate the alias.\n        \"\"\"\n        # Apply an alias_generator if\n        # 1. An alias is not specified\n        # 2. An alias is specified, but the priority is <= 1\n        if (\n            field_info.alias_priority is None\n            or field_info.alias_priority <= 1\n            or field_info.alias is None\n            or field_info.validation_alias is None\n            or field_info.serialization_alias is None\n        ):\n            alias, validation_alias, serialization_alias = None, None, None\n\n            if isinstance(alias_generator, AliasGenerator):\n                alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n            elif isinstance(alias_generator, Callable):\n                alias = alias_generator(field_name)\n                if not isinstance(alias, str):\n                    raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n\n            # if priority is not set, we set to 1\n            # which supports the case where the alias_generator from a child class is used\n            # to generate an alias for a field in a parent class\n            if field_info.alias_priority is None or field_info.alias_priority <= 1:\n                field_info.alias_priority = 1\n\n            # if the priority is 1, then we set the aliases to the generated alias\n            if field_info.alias_priority == 1:\n                field_info.serialization_alias = _get_first_non_null(serialization_alias, alias)\n                field_info.validation_alias = _get_first_non_null(validation_alias, alias)\n                field_info.alias = alias\n\n            # if any of the aliases are not set, then we set them to the corresponding generated alias\n            if field_info.alias is None:\n                field_info.alias = alias\n            if field_info.serialization_alias is None:\n                field_info.serialization_alias = _get_first_non_null(serialization_alias, alias)\n            if field_info.validation_alias is None:\n                field_info.validation_alias = _get_first_non_null(validation_alias, alias)\n\n    @staticmethod\n    def _apply_alias_generator_to_computed_field_info(\n        alias_generator: Callable[[str], str] | AliasGenerator,\n        computed_field_info: ComputedFieldInfo,\n        computed_field_name: str,\n    ):\n        \"\"\"Apply an alias_generator to alias on a ComputedFieldInfo instance if appropriate.\n\n        Args:\n            alias_generator: A callable that takes a string and returns a string, or an AliasGenerator instance.\n            computed_field_info: The ComputedFieldInfo instance to which the alias_generator is (maybe) applied.\n            computed_field_name: The name of the computed field from which to generate the alias.\n        \"\"\"\n        # Apply an alias_generator if\n        # 1. An alias is not specified\n        # 2. An alias is specified, but the priority is <= 1\n\n        if (\n            computed_field_info.alias_priority is None\n            or computed_field_info.alias_priority <= 1\n            or computed_field_info.alias is None\n        ):\n            alias, validation_alias, serialization_alias = None, None, None\n\n            if isinstance(alias_generator, AliasGenerator):\n                alias, validation_alias, serialization_alias = alias_generator.generate_aliases(computed_field_name)\n            elif isinstance(alias_generator, Callable):\n                alias = alias_generator(computed_field_name)\n                if not isinstance(alias, str):\n                    raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n\n            # if priority is not set, we set to 1\n            # which supports the case where the alias_generator from a child class is used\n            # to generate an alias for a field in a parent class\n            if computed_field_info.alias_priority is None or computed_field_info.alias_priority <= 1:\n                computed_field_info.alias_priority = 1\n\n            # if the priority is 1, then we set the aliases to the generated alias\n            # note that we use the serialization_alias with priority over alias, as computed_field\n            # aliases are used for serialization only (not validation)\n            if computed_field_info.alias_priority == 1:\n                computed_field_info.alias = _get_first_non_null(serialization_alias, alias)\n\n    @staticmethod\n    def _apply_field_title_generator_to_field_info(\n        config_wrapper: ConfigWrapper, field_info: FieldInfo | ComputedFieldInfo, field_name: str\n    ) -> None:\n        \"\"\"Apply a field_title_generator on a FieldInfo or ComputedFieldInfo instance if appropriate\n        Args:\n            config_wrapper: The config of the model\n            field_info: The FieldInfo or ComputedField instance to which the title_generator is (maybe) applied.\n            field_name: The name of the field from which to generate the title.\n        \"\"\"\n        field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n\n        if field_title_generator is None:\n            return\n\n        if field_info.title is None:\n            title = field_title_generator(field_name, field_info)  # type: ignore\n            if not isinstance(title, str):\n                raise TypeError(f'field_title_generator {field_title_generator} must return str, not {title.__class__}')\n\n            field_info.title = title\n\n    def _common_field_schema(  # C901\n        self, name: str, field_info: FieldInfo, decorators: DecoratorInfos\n    ) -> _CommonField:\n        # Update FieldInfo annotation if appropriate:\n        from .. import AliasChoices, AliasPath\n        from ..fields import FieldInfo\n\n        if has_instance_in_type(field_info.annotation, (ForwardRef, str)):\n            types_namespace = self._types_namespace\n            if self._typevars_map:\n                types_namespace = (types_namespace or {}).copy()\n                # Ensure that typevars get mapped to their concrete types:\n                types_namespace.update({k.__name__: v for k, v in self._typevars_map.items()})\n\n            evaluated = _typing_extra.eval_type_lenient(field_info.annotation, types_namespace)\n            if evaluated is not field_info.annotation and not has_instance_in_type(evaluated, PydanticRecursiveRef):\n                new_field_info = FieldInfo.from_annotation(evaluated)\n                field_info.annotation = new_field_info.annotation\n\n                # Handle any field info attributes that may have been obtained from now-resolved annotations\n                for k, v in new_field_info._attributes_set.items():\n                    # If an attribute is already set, it means it was set by assigning to a call to Field (or just a\n                    # default value), and that should take the highest priority. So don't overwrite existing attributes.\n                    # We skip over \"attributes\" that are present in the metadata_lookup dict because these won't\n                    # actually end up as attributes of the `FieldInfo` instance.\n                    if k not in field_info._attributes_set and k not in field_info.metadata_lookup:\n                        setattr(field_info, k, v)\n\n                # Finally, ensure the field info also reflects all the `_attributes_set` that are actually metadata.\n                field_info.metadata = [*new_field_info.metadata, *field_info.metadata]\n\n        source_type, annotations = field_info.annotation, field_info.metadata\n\n        def set_discriminator(schema: CoreSchema) -> CoreSchema:\n            schema = self._apply_discriminator_to_union(schema, field_info.discriminator)\n            return schema\n\n        with self.field_name_stack.push(name):\n            if field_info.discriminator is not None:\n                schema = self._apply_annotations(source_type, annotations, transform_inner_schema=set_discriminator)\n            else:\n                schema = self._apply_annotations(\n                    source_type,\n                    annotations,\n                )\n\n        # This V1 compatibility shim should eventually be removed\n        # push down any `each_item=True` validators\n        # note that this won't work for any Annotated types that get wrapped by a function validator\n        # but that's okay because that didn't exist in V1\n        this_field_validators = filter_field_decorator_info_by_field(decorators.validators.values(), name)\n        if _validators_require_validate_default(this_field_validators):\n            field_info.validate_default = True\n        each_item_validators = [v for v in this_field_validators if v.info.each_item is True]\n        this_field_validators = [v for v in this_field_validators if v not in each_item_validators]\n        schema = apply_each_item_validators(schema, each_item_validators, name)\n\n        schema = apply_validators(schema, filter_field_decorator_info_by_field(this_field_validators, name), name)\n        schema = apply_validators(\n            schema, filter_field_decorator_info_by_field(decorators.field_validators.values(), name), name\n        )\n\n        # the default validator needs to go outside of any other validators\n        # so that it is the topmost validator for the field validator\n        # which uses it to check if the field has a default value or not\n        if not field_info.is_required():\n            schema = wrap_default(field_info, schema)\n\n        schema = self._apply_field_serializers(\n            schema, filter_field_decorator_info_by_field(decorators.field_serializers.values(), name)\n        )\n        self._apply_field_title_generator_to_field_info(self._config_wrapper, field_info, name)\n\n        json_schema_updates = {\n            'title': field_info.title,\n            'description': field_info.description,\n            'deprecated': bool(field_info.deprecated) or field_info.deprecated == '' or None,\n            'examples': to_jsonable_python(field_info.examples),\n        }\n        json_schema_updates = {k: v for k, v in json_schema_updates.items() if v is not None}\n\n        json_schema_extra = field_info.json_schema_extra\n\n        metadata = build_metadata_dict(\n            js_annotation_functions=[get_json_schema_update_func(json_schema_updates, json_schema_extra)]\n        )\n\n        alias_generator = self._config_wrapper.alias_generator\n        if alias_generator is not None:\n            self._apply_alias_generator_to_field_info(alias_generator, field_info, name)\n\n        if isinstance(field_info.validation_alias, (AliasChoices, AliasPath)):\n            validation_alias = field_info.validation_alias.convert_to_aliases()\n        else:\n            validation_alias = field_info.validation_alias\n\n        return _common_field(\n            schema,\n            serialization_exclude=True if field_info.exclude else None,\n            validation_alias=validation_alias,\n            serialization_alias=field_info.serialization_alias,\n            frozen=field_info.frozen,\n            metadata=metadata,\n        )\n\n    def _union_schema(self, union_type: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a Union.\"\"\"\n        args = self._get_args_resolving_forward_refs(union_type, required=True)\n        choices: list[CoreSchema] = []\n        nullable = False\n        for arg in args:\n            if arg is None or arg is _typing_extra.NoneType:\n                nullable = True\n            else:\n                choices.append(self.generate_schema(arg))\n\n        if len(choices) == 1:\n            s = choices[0]\n        else:\n            choices_with_tags: list[CoreSchema | tuple[CoreSchema, str]] = []\n            for choice in choices:\n                tag = choice.get('metadata', {}).get(_core_utils.TAGGED_UNION_TAG_KEY)\n                if tag is not None:\n                    choices_with_tags.append((choice, tag))\n                else:\n                    choices_with_tags.append(choice)\n            s = core_schema.union_schema(choices_with_tags)\n\n        if nullable:\n            s = core_schema.nullable_schema(s)\n        return s\n\n    def _type_alias_type_schema(\n        self,\n        obj: Any,  # TypeAliasType\n    ) -> CoreSchema:\n        with self.defs.get_schema_or_ref(obj) as (ref, maybe_schema):\n            if maybe_schema is not None:\n                return maybe_schema\n\n            origin = get_origin(obj) or obj\n\n            annotation = origin.__value__\n            typevars_map = get_standard_typevars_map(obj)\n\n            with self._types_namespace_stack.push(origin):\n                annotation = _typing_extra.eval_type_lenient(annotation, self._types_namespace)\n                annotation = replace_types(annotation, typevars_map)\n                schema = self.generate_schema(annotation)\n                assert schema['type'] != 'definitions'\n                schema['ref'] = ref  # type: ignore\n            self.defs.definitions[ref] = schema\n            return core_schema.definition_reference_schema(ref)\n\n    def _literal_schema(self, literal_type: Any) -> CoreSchema:\n        \"\"\"Generate schema for a Literal.\"\"\"\n        expected = _typing_extra.all_literal_values(literal_type)\n        assert expected, f'literal \"expected\" cannot be empty, obj={literal_type}'\n        return core_schema.literal_schema(expected)\n\n    def _typed_dict_schema(self, typed_dict_cls: Any, origin: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a TypedDict.\n\n        It is not possible to track required/optional keys in TypedDict without __required_keys__\n        since TypedDict.__new__ erases the base classes (it replaces them with just `dict`)\n        and thus we can track usage of total=True/False\n        __required_keys__ was added in Python 3.9\n        (https://github.com/miss-islington/cpython/blob/1e9939657dd1f8eb9f596f77c1084d2d351172fc/Doc/library/typing.rst?plain=1#L1546-L1548)\n        however it is buggy\n        (https://github.com/python/typing_extensions/blob/ac52ac5f2cb0e00e7988bae1e2a1b8257ac88d6d/src/typing_extensions.py#L657-L666).\n\n        On 3.11 but < 3.12 TypedDict does not preserve inheritance information.\n\n        Hence to avoid creating validators that do not do what users expect we only\n        support typing.TypedDict on Python >= 3.12 or typing_extension.TypedDict on all versions\n        \"\"\"\n        from ..fields import FieldInfo\n\n        with self.model_type_stack.push(typed_dict_cls), self.defs.get_schema_or_ref(typed_dict_cls) as (\n            typed_dict_ref,\n            maybe_schema,\n        ):\n            if maybe_schema is not None:\n                return maybe_schema\n\n            typevars_map = get_standard_typevars_map(typed_dict_cls)\n            if origin is not None:\n                typed_dict_cls = origin\n\n            if not _SUPPORTS_TYPEDDICT and type(typed_dict_cls).__module__ == 'typing':\n                raise PydanticUserError(\n                    'Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.',\n                    code='typed-dict-version',\n                )\n\n            try:\n                config: ConfigDict | None = get_attribute_from_bases(typed_dict_cls, '__pydantic_config__')\n            except AttributeError:\n                config = None\n\n            with self._config_wrapper_stack.push(config), self._types_namespace_stack.push(typed_dict_cls):\n                core_config = self._config_wrapper.core_config(typed_dict_cls)\n\n                self = self._current_generate_schema\n\n                required_keys: frozenset[str] = typed_dict_cls.__required_keys__\n\n                fields: dict[str, core_schema.TypedDictField] = {}\n\n                decorators = DecoratorInfos.build(typed_dict_cls)\n\n                if self._config_wrapper.use_attribute_docstrings:\n                    field_docstrings = extract_docstrings_from_cls(typed_dict_cls, use_inspect=True)\n                else:\n                    field_docstrings = None\n\n                for field_name, annotation in get_type_hints_infer_globalns(\n                    typed_dict_cls, localns=self._types_namespace, include_extras=True\n                ).items():\n                    annotation = replace_types(annotation, typevars_map)\n                    required = field_name in required_keys\n\n                    if get_origin(annotation) == _typing_extra.Required:\n                        required = True\n                        annotation = self._get_args_resolving_forward_refs(\n                            annotation,\n                            required=True,\n                        )[0]\n                    elif get_origin(annotation) == _typing_extra.NotRequired:\n                        required = False\n                        annotation = self._get_args_resolving_forward_refs(\n                            annotation,\n                            required=True,\n                        )[0]\n\n                    field_info = FieldInfo.from_annotation(annotation)\n                    if (\n                        field_docstrings is not None\n                        and field_info.description is None\n                        and field_name in field_docstrings\n                    ):\n                        field_info.description = field_docstrings[field_name]\n                    self._apply_field_title_generator_to_field_info(self._config_wrapper, field_info, field_name)\n                    fields[field_name] = self._generate_td_field_schema(\n                        field_name, field_info, decorators, required=required\n                    )\n\n                title = self._get_model_title_from_config(typed_dict_cls, ConfigWrapper(config))\n                metadata = build_metadata_dict(\n                    js_functions=[partial(modify_model_json_schema, cls=typed_dict_cls, title=title)],\n                    typed_dict_cls=typed_dict_cls,\n                )\n                td_schema = core_schema.typed_dict_schema(\n                    fields,\n                    computed_fields=[\n                        self._computed_field_schema(d, decorators.field_serializers)\n                        for d in decorators.computed_fields.values()\n                    ],\n                    ref=typed_dict_ref,\n                    metadata=metadata,\n                    config=core_config,\n                )\n\n                schema = self._apply_model_serializers(td_schema, decorators.model_serializers.values())\n                schema = apply_model_validators(schema, decorators.model_validators.values(), 'all')\n                self.defs.definitions[typed_dict_ref] = schema\n                return core_schema.definition_reference_schema(typed_dict_ref)\n\n    def _namedtuple_schema(self, namedtuple_cls: Any, origin: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a NamedTuple.\"\"\"\n        with self.model_type_stack.push(namedtuple_cls), self.defs.get_schema_or_ref(namedtuple_cls) as (\n            namedtuple_ref,\n            maybe_schema,\n        ):\n            if maybe_schema is not None:\n                return maybe_schema\n            typevars_map = get_standard_typevars_map(namedtuple_cls)\n            if origin is not None:\n                namedtuple_cls = origin\n\n            annotations: dict[str, Any] = get_type_hints_infer_globalns(\n                namedtuple_cls, include_extras=True, localns=self._types_namespace\n            )\n            if not annotations:\n                # annotations is empty, happens if namedtuple_cls defined via collections.namedtuple(...)\n                annotations = {k: Any for k in namedtuple_cls._fields}\n\n            if typevars_map:\n                annotations = {\n                    field_name: replace_types(annotation, typevars_map)\n                    for field_name, annotation in annotations.items()\n                }\n\n            arguments_schema = core_schema.arguments_schema(\n                [\n                    self._generate_parameter_schema(\n                        field_name, annotation, default=namedtuple_cls._field_defaults.get(field_name, Parameter.empty)\n                    )\n                    for field_name, annotation in annotations.items()\n                ],\n                metadata=build_metadata_dict(js_prefer_positional_arguments=True),\n            )\n            return core_schema.call_schema(arguments_schema, namedtuple_cls, ref=namedtuple_ref)\n\n    def _generate_parameter_schema(\n        self,\n        name: str,\n        annotation: type[Any],\n        default: Any = Parameter.empty,\n        mode: Literal['positional_only', 'positional_or_keyword', 'keyword_only'] | None = None,\n    ) -> core_schema.ArgumentsParameter:\n        \"\"\"Prepare a ArgumentsParameter to represent a field in a namedtuple or function signature.\"\"\"\n        from ..fields import FieldInfo\n\n        if default is Parameter.empty:\n            field = FieldInfo.from_annotation(annotation)\n        else:\n            field = FieldInfo.from_annotated_attribute(annotation, default)\n        assert field.annotation is not None, 'field.annotation should not be None when generating a schema'\n        source_type, annotations = field.annotation, field.metadata\n        with self.field_name_stack.push(name):\n            schema = self._apply_annotations(source_type, annotations)\n\n        if not field.is_required():\n            schema = wrap_default(field, schema)\n\n        parameter_schema = core_schema.arguments_parameter(name, schema)\n        if mode is not None:\n            parameter_schema['mode'] = mode\n        if field.alias is not None:\n            parameter_schema['alias'] = field.alias\n        else:\n            alias_generator = self._config_wrapper.alias_generator\n            if isinstance(alias_generator, AliasGenerator) and alias_generator.alias is not None:\n                parameter_schema['alias'] = alias_generator.alias(name)\n            elif isinstance(alias_generator, Callable):\n                parameter_schema['alias'] = alias_generator(name)\n        return parameter_schema\n\n    def _tuple_schema(self, tuple_type: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a Tuple, e.g. `tuple[int, str]` or `tuple[int, ...]`.\"\"\"\n        # TODO: do we really need to resolve type vars here?\n        typevars_map = get_standard_typevars_map(tuple_type)\n        params = self._get_args_resolving_forward_refs(tuple_type)\n\n        if typevars_map and params:\n            params = tuple(replace_types(param, typevars_map) for param in params)\n\n        # NOTE: subtle difference: `tuple[()]` gives `params=()`, whereas `typing.Tuple[()]` gives `params=((),)`\n        # This is only true for <3.11, on Python 3.11+ `typing.Tuple[()]` gives `params=()`\n        if not params:\n            if tuple_type in TUPLE_TYPES:\n                return core_schema.tuple_schema([core_schema.any_schema()], variadic_item_index=0)\n            else:\n                # special case for `tuple[()]` which means `tuple[]` - an empty tuple\n                return core_schema.tuple_schema([])\n        elif params[-1] is Ellipsis:\n            if len(params) == 2:\n                return core_schema.tuple_schema([self.generate_schema(params[0])], variadic_item_index=0)\n            else:\n                # TODO: something like https://github.com/pydantic/pydantic/issues/5952\n                raise ValueError('Variable tuples can only have one type')\n        elif len(params) == 1 and params[0] == ():\n            # special case for `Tuple[()]` which means `Tuple[]` - an empty tuple\n            # NOTE: This conditional can be removed when we drop support for Python 3.10.\n            return core_schema.tuple_schema([])\n        else:\n            return core_schema.tuple_schema([self.generate_schema(param) for param in params])\n\n    def _type_schema(self) -> core_schema.CoreSchema:\n        return core_schema.custom_error_schema(\n            core_schema.is_instance_schema(type),\n            custom_error_type='is_type',\n            custom_error_message='Input should be a type',\n        )\n\n    def _union_is_subclass_schema(self, union_type: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for `Type[Union[X, ...]]`.\"\"\"\n        args = self._get_args_resolving_forward_refs(union_type, required=True)\n        return core_schema.union_schema([self.generate_schema(typing.Type[args]) for args in args])\n\n    def _subclass_schema(self, type_: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a Type, e.g. `Type[int]`.\"\"\"\n        type_param = self._get_first_arg_or_any(type_)\n        if type_param == Any:\n            return self._type_schema()\n        elif isinstance(type_param, typing.TypeVar):\n            if type_param.__bound__:\n                if _typing_extra.origin_is_union(get_origin(type_param.__bound__)):\n                    return self._union_is_subclass_schema(type_param.__bound__)\n                return core_schema.is_subclass_schema(type_param.__bound__)\n            elif type_param.__constraints__:\n                return core_schema.union_schema(\n                    [self.generate_schema(typing.Type[c]) for c in type_param.__constraints__]\n                )\n            else:\n                return self._type_schema()\n        elif _typing_extra.origin_is_union(get_origin(type_param)):\n            return self._union_is_subclass_schema(type_param)\n        else:\n            return core_schema.is_subclass_schema(type_param)\n\n    def _sequence_schema(self, sequence_type: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a Sequence, e.g. `Sequence[int]`.\"\"\"\n        from ._std_types_schema import serialize_sequence_via_list\n\n        item_type = self._get_first_arg_or_any(sequence_type)\n        item_type_schema = self.generate_schema(item_type)\n        list_schema = core_schema.list_schema(item_type_schema)\n\n        python_schema = core_schema.is_instance_schema(typing.Sequence, cls_repr='Sequence')\n        if item_type != Any:\n            from ._validators import sequence_validator\n\n            python_schema = core_schema.chain_schema(\n                [python_schema, core_schema.no_info_wrap_validator_function(sequence_validator, list_schema)],\n            )\n\n        serialization = core_schema.wrap_serializer_function_ser_schema(\n            serialize_sequence_via_list, schema=item_type_schema, info_arg=True\n        )\n        return core_schema.json_or_python_schema(\n            json_schema=list_schema, python_schema=python_schema, serialization=serialization\n        )\n\n    def _iterable_schema(self, type_: Any) -> core_schema.GeneratorSchema:\n        \"\"\"Generate a schema for an `Iterable`.\"\"\"\n        item_type = self._get_first_arg_or_any(type_)\n\n        return core_schema.generator_schema(self.generate_schema(item_type))\n\n    def _pattern_schema(self, pattern_type: Any) -> core_schema.CoreSchema:\n        from . import _validators\n\n        metadata = build_metadata_dict(js_functions=[lambda _1, _2: {'type': 'string', 'format': 'regex'}])\n        ser = core_schema.plain_serializer_function_ser_schema(\n            attrgetter('pattern'), when_used='json', return_schema=core_schema.str_schema()\n        )\n        if pattern_type == typing.Pattern or pattern_type == re.Pattern:\n            # bare type\n            return core_schema.no_info_plain_validator_function(\n                _validators.pattern_either_validator, serialization=ser, metadata=metadata\n            )\n\n        param = self._get_args_resolving_forward_refs(\n            pattern_type,\n            required=True,\n        )[0]\n        if param == str:\n            return core_schema.no_info_plain_validator_function(\n                _validators.pattern_str_validator, serialization=ser, metadata=metadata\n            )\n        elif param == bytes:\n            return core_schema.no_info_plain_validator_function(\n                _validators.pattern_bytes_validator, serialization=ser, metadata=metadata\n            )\n        else:\n            raise PydanticSchemaGenerationError(f'Unable to generate pydantic-core schema for {pattern_type!r}.')\n\n    def _hashable_schema(self) -> core_schema.CoreSchema:\n        return core_schema.custom_error_schema(\n            core_schema.is_instance_schema(collections.abc.Hashable),\n            custom_error_type='is_hashable',\n            custom_error_message='Input should be hashable',\n        )\n\n    def _dataclass_schema(\n        self, dataclass: type[StandardDataclass], origin: type[StandardDataclass] | None\n    ) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a dataclass.\"\"\"\n        with self.model_type_stack.push(dataclass), self.defs.get_schema_or_ref(dataclass) as (\n            dataclass_ref,\n            maybe_schema,\n        ):\n            if maybe_schema is not None:\n                return maybe_schema\n\n            typevars_map = get_standard_typevars_map(dataclass)\n            if origin is not None:\n                dataclass = origin\n\n            with ExitStack() as dataclass_bases_stack:\n                # Pushing a namespace prioritises items already in the stack, so iterate though the MRO forwards\n                for dataclass_base in dataclass.__mro__:\n                    if dataclasses.is_dataclass(dataclass_base):\n                        dataclass_bases_stack.enter_context(self._types_namespace_stack.push(dataclass_base))\n\n                # Pushing a config overwrites the previous config, so iterate though the MRO backwards\n                config = None\n                for dataclass_base in reversed(dataclass.__mro__):\n                    if dataclasses.is_dataclass(dataclass_base):\n                        config = getattr(dataclass_base, '__pydantic_config__', None)\n                        dataclass_bases_stack.enter_context(self._config_wrapper_stack.push(config))\n\n                core_config = self._config_wrapper.core_config(dataclass)\n\n                self = self._current_generate_schema\n\n                from ..dataclasses import is_pydantic_dataclass\n\n                if is_pydantic_dataclass(dataclass):\n                    fields = deepcopy(dataclass.__pydantic_fields__)\n                    if typevars_map:\n                        for field in fields.values():\n                            field.apply_typevars_map(typevars_map, self._types_namespace)\n                else:\n                    fields = collect_dataclass_fields(\n                        dataclass,\n                        self._types_namespace,\n                        typevars_map=typevars_map,\n                    )\n\n                # disallow combination of init=False on a dataclass field and extra='allow' on a dataclass\n                if self._config_wrapper_stack.tail.extra == 'allow':\n                    # disallow combination of init=False on a dataclass field and extra='allow' on a dataclass\n                    for field_name, field in fields.items():\n                        if field.init is False:\n                            raise PydanticUserError(\n                                f'Field {field_name} has `init=False` and dataclass has config setting `extra=\"allow\"`. '\n                                f'This combination is not allowed.',\n                                code='dataclass-init-false-extra-allow',\n                            )\n\n                decorators = dataclass.__dict__.get('__pydantic_decorators__') or DecoratorInfos.build(dataclass)\n                # Move kw_only=False args to the start of the list, as this is how vanilla dataclasses work.\n                # Note that when kw_only is missing or None, it is treated as equivalent to kw_only=True\n                args = sorted(\n                    (self._generate_dc_field_schema(k, v, decorators) for k, v in fields.items()),\n                    key=lambda a: a.get('kw_only') is not False,\n                )\n                has_post_init = hasattr(dataclass, '__post_init__')\n                has_slots = hasattr(dataclass, '__slots__')\n\n                args_schema = core_schema.dataclass_args_schema(\n                    dataclass.__name__,\n                    args,\n                    computed_fields=[\n                        self._computed_field_schema(d, decorators.field_serializers)\n                        for d in decorators.computed_fields.values()\n                    ],\n                    collect_init_only=has_post_init,\n                )\n\n                inner_schema = apply_validators(args_schema, decorators.root_validators.values(), None)\n\n                model_validators = decorators.model_validators.values()\n                inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n\n                title = self._get_model_title_from_config(dataclass, ConfigWrapper(config))\n                metadata = build_metadata_dict(\n                    js_functions=[partial(modify_model_json_schema, cls=dataclass, title=title)]\n                )\n\n                dc_schema = core_schema.dataclass_schema(\n                    dataclass,\n                    inner_schema,\n                    post_init=has_post_init,\n                    ref=dataclass_ref,\n                    fields=[field.name for field in dataclasses.fields(dataclass)],\n                    slots=has_slots,\n                    config=core_config,\n                    metadata=metadata,\n                )\n                schema = self._apply_model_serializers(dc_schema, decorators.model_serializers.values())\n                schema = apply_model_validators(schema, model_validators, 'outer')\n                self.defs.definitions[dataclass_ref] = schema\n                return core_schema.definition_reference_schema(dataclass_ref)\n\n            # Type checkers seem to assume ExitStack may suppress exceptions and therefore\n            # control flow can exit the `with` block without returning.\n            assert False, 'Unreachable'\n\n    def _callable_schema(self, function: Callable[..., Any]) -> core_schema.CallSchema:\n        \"\"\"Generate schema for a Callable.\n\n        TODO support functional validators once we support them in Config\n        \"\"\"\n        sig = signature(function)\n\n        type_hints = _typing_extra.get_function_type_hints(function)\n\n        mode_lookup: dict[_ParameterKind, Literal['positional_only', 'positional_or_keyword', 'keyword_only']] = {\n            Parameter.POSITIONAL_ONLY: 'positional_only',\n            Parameter.POSITIONAL_OR_KEYWORD: 'positional_or_keyword',\n            Parameter.KEYWORD_ONLY: 'keyword_only',\n        }\n\n        arguments_list: list[core_schema.ArgumentsParameter] = []\n        var_args_schema: core_schema.CoreSchema | None = None\n        var_kwargs_schema: core_schema.CoreSchema | None = None\n\n        for name, p in sig.parameters.items():\n            if p.annotation is sig.empty:\n                annotation = typing.cast(Any, Any)\n            else:\n                annotation = type_hints[name]\n\n            parameter_mode = mode_lookup.get(p.kind)\n            if parameter_mode is not None:\n                arg_schema = self._generate_parameter_schema(name, annotation, p.default, parameter_mode)\n                arguments_list.append(arg_schema)\n            elif p.kind == Parameter.VAR_POSITIONAL:\n                var_args_schema = self.generate_schema(annotation)\n            else:\n                assert p.kind == Parameter.VAR_KEYWORD, p.kind\n                var_kwargs_schema = self.generate_schema(annotation)\n\n        return_schema: core_schema.CoreSchema | None = None\n        config_wrapper = self._config_wrapper\n        if config_wrapper.validate_return:\n            return_hint = type_hints.get('return')\n            if return_hint is not None:\n                return_schema = self.generate_schema(return_hint)\n\n        return core_schema.call_schema(\n            core_schema.arguments_schema(\n                arguments_list,\n                var_args_schema=var_args_schema,\n                var_kwargs_schema=var_kwargs_schema,\n                populate_by_name=config_wrapper.populate_by_name,\n            ),\n            function,\n            return_schema=return_schema,\n        )\n\n    def _unsubstituted_typevar_schema(self, typevar: typing.TypeVar) -> core_schema.CoreSchema:\n        assert isinstance(typevar, typing.TypeVar)\n\n        bound = typevar.__bound__\n        constraints = typevar.__constraints__\n\n        try:\n            typevar_has_default = typevar.has_default()  # type: ignore\n        except AttributeError:\n            # could still have a default if it's an old version of typing_extensions.TypeVar\n            typevar_has_default = getattr(typevar, '__default__', None) is not None\n\n        if (bound is not None) + (len(constraints) != 0) + typevar_has_default > 1:\n            raise NotImplementedError(\n                'Pydantic does not support mixing more than one of TypeVar bounds, constraints and defaults'\n            )\n\n        if typevar_has_default:\n            return self.generate_schema(typevar.__default__)  # type: ignore\n        elif constraints:\n            return self._union_schema(typing.Union[constraints])  # type: ignore\n        elif bound:\n            schema = self.generate_schema(bound)\n            schema['serialization'] = core_schema.wrap_serializer_function_ser_schema(\n                lambda x, h: h(x), schema=core_schema.any_schema()\n            )\n            return schema\n        else:\n            return core_schema.any_schema()\n\n    def _computed_field_schema(\n        self,\n        d: Decorator[ComputedFieldInfo],\n        field_serializers: dict[str, Decorator[FieldSerializerDecoratorInfo]],\n    ) -> core_schema.ComputedField:\n        try:\n            return_type = _decorators.get_function_return_type(d.func, d.info.return_type, self._types_namespace)\n        except NameError as e:\n            raise PydanticUndefinedAnnotation.from_name_error(e) from e\n        if return_type is PydanticUndefined:\n            raise PydanticUserError(\n                'Computed field is missing return type annotation or specifying `return_type`'\n                ' to the `@computed_field` decorator (e.g. `@computed_field(return_type=int|str)`)',\n                code='model-field-missing-annotation',\n            )\n\n        return_type = replace_types(return_type, self._typevars_map)\n        # Create a new ComputedFieldInfo so that different type parametrizations of the same\n        # generic model's computed field can have different return types.\n        d.info = dataclasses.replace(d.info, return_type=return_type)\n        return_type_schema = self.generate_schema(return_type)\n        # Apply serializers to computed field if there exist\n        return_type_schema = self._apply_field_serializers(\n            return_type_schema,\n            filter_field_decorator_info_by_field(field_serializers.values(), d.cls_var_name),\n            computed_field=True,\n        )\n\n        alias_generator = self._config_wrapper.alias_generator\n        if alias_generator is not None:\n            self._apply_alias_generator_to_computed_field_info(\n                alias_generator=alias_generator, computed_field_info=d.info, computed_field_name=d.cls_var_name\n            )\n        self._apply_field_title_generator_to_field_info(self._config_wrapper, d.info, d.cls_var_name)\n\n        def set_computed_field_metadata(schema: CoreSchemaOrField, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            json_schema = handler(schema)\n\n            json_schema['readOnly'] = True\n\n            title = d.info.title\n            if title is not None:\n                json_schema['title'] = title\n\n            description = d.info.description\n            if description is not None:\n                json_schema['description'] = description\n\n            if d.info.deprecated or d.info.deprecated == '':\n                json_schema['deprecated'] = True\n\n            examples = d.info.examples\n            if examples is not None:\n                json_schema['examples'] = to_jsonable_python(examples)\n\n            json_schema_extra = d.info.json_schema_extra\n            if json_schema_extra is not None:\n                add_json_schema_extra(json_schema, json_schema_extra)\n\n            return json_schema\n\n        metadata = build_metadata_dict(js_annotation_functions=[set_computed_field_metadata])\n        return core_schema.computed_field(\n            d.cls_var_name, return_schema=return_type_schema, alias=d.info.alias, metadata=metadata\n        )\n\n    def _annotated_schema(self, annotated_type: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for an Annotated type, e.g. `Annotated[int, Field(...)]` or `Annotated[int, Gt(0)]`.\"\"\"\n        from ..fields import FieldInfo\n\n        source_type, *annotations = self._get_args_resolving_forward_refs(\n            annotated_type,\n            required=True,\n        )\n        schema = self._apply_annotations(source_type, annotations)\n        # put the default validator last so that TypeAdapter.get_default_value() works\n        # even if there are function validators involved\n        for annotation in annotations:\n            if isinstance(annotation, FieldInfo):\n                schema = wrap_default(annotation, schema)\n        return schema\n\n    def _get_prepare_pydantic_annotations_for_known_type(\n        self, obj: Any, annotations: tuple[Any, ...]\n    ) -> tuple[Any, list[Any]] | None:\n        from ._std_types_schema import PREPARE_METHODS\n\n        # Check for hashability\n        try:\n            hash(obj)\n        except TypeError:\n            # obj is definitely not a known type if this fails\n            return None\n\n        for gen in PREPARE_METHODS:\n            res = gen(obj, annotations, self._config_wrapper.config_dict)\n            if res is not None:\n                return res\n\n        return None\n\n    def _apply_annotations(\n        self,\n        source_type: Any,\n        annotations: list[Any],\n        transform_inner_schema: Callable[[CoreSchema], CoreSchema] = lambda x: x,\n    ) -> CoreSchema:\n        \"\"\"Apply arguments from `Annotated` or from `FieldInfo` to a schema.\n\n        This gets called by `GenerateSchema._annotated_schema` but differs from it in that it does\n        not expect `source_type` to be an `Annotated` object, it expects it to be  the first argument of that\n        (in other words, `GenerateSchema._annotated_schema` just unpacks `Annotated`, this process it).\n        \"\"\"\n        annotations = list(_known_annotated_metadata.expand_grouped_metadata(annotations))\n        res = self._get_prepare_pydantic_annotations_for_known_type(source_type, tuple(annotations))\n        if res is not None:\n            source_type, annotations = res\n\n        pydantic_js_annotation_functions: list[GetJsonSchemaFunction] = []\n\n        def inner_handler(obj: Any) -> CoreSchema:\n            from_property = self._generate_schema_from_property(obj, source_type)\n            if from_property is None:\n                schema = self._generate_schema_inner(obj)\n            else:\n                schema = from_property\n            metadata_js_function = _extract_get_pydantic_json_schema(obj, schema)\n            if metadata_js_function is not None:\n                metadata_schema = resolve_original_schema(schema, self.defs.definitions)\n                if metadata_schema is not None:\n                    self._add_js_function(metadata_schema, metadata_js_function)\n            return transform_inner_schema(schema)\n\n        get_inner_schema = CallbackGetCoreSchemaHandler(inner_handler, self)\n\n        for annotation in annotations:\n            if annotation is None:\n                continue\n            get_inner_schema = self._get_wrapped_inner_schema(\n                get_inner_schema, annotation, pydantic_js_annotation_functions\n            )\n\n        schema = get_inner_schema(source_type)\n        if pydantic_js_annotation_functions:\n            metadata = CoreMetadataHandler(schema).metadata\n            metadata.setdefault('pydantic_js_annotation_functions', []).extend(pydantic_js_annotation_functions)\n        return _add_custom_serialization_from_json_encoders(self._config_wrapper.json_encoders, source_type, schema)\n\n    def _apply_single_annotation(self, schema: core_schema.CoreSchema, metadata: Any) -> core_schema.CoreSchema:\n        from ..fields import FieldInfo\n\n        if isinstance(metadata, FieldInfo):\n            for field_metadata in metadata.metadata:\n                schema = self._apply_single_annotation(schema, field_metadata)\n\n            if metadata.discriminator is not None:\n                schema = self._apply_discriminator_to_union(schema, metadata.discriminator)\n            return schema\n\n        if schema['type'] == 'nullable':\n            # for nullable schemas, metadata is automatically applied to the inner schema\n            inner = schema.get('schema', core_schema.any_schema())\n            inner = self._apply_single_annotation(inner, metadata)\n            if inner:\n                schema['schema'] = inner\n            return schema\n\n        original_schema = schema\n        ref = schema.get('ref', None)\n        if ref is not None:\n            schema = schema.copy()\n            new_ref = ref + f'_{repr(metadata)}'\n            if new_ref in self.defs.definitions:\n                return self.defs.definitions[new_ref]\n            schema['ref'] = new_ref  # type: ignore\n        elif schema['type'] == 'definition-ref':\n            ref = schema['schema_ref']\n            if ref in self.defs.definitions:\n                schema = self.defs.definitions[ref].copy()\n                new_ref = ref + f'_{repr(metadata)}'\n                if new_ref in self.defs.definitions:\n                    return self.defs.definitions[new_ref]\n                schema['ref'] = new_ref  # type: ignore\n\n        maybe_updated_schema = _known_annotated_metadata.apply_known_metadata(metadata, schema.copy())\n\n        if maybe_updated_schema is not None:\n            return maybe_updated_schema\n        return original_schema\n\n    def _apply_single_annotation_json_schema(\n        self, schema: core_schema.CoreSchema, metadata: Any\n    ) -> core_schema.CoreSchema:\n        from ..fields import FieldInfo\n\n        if isinstance(metadata, FieldInfo):\n            for field_metadata in metadata.metadata:\n                schema = self._apply_single_annotation_json_schema(schema, field_metadata)\n            json_schema_update: JsonSchemaValue = {}\n            if metadata.title:\n                json_schema_update['title'] = metadata.title\n            if metadata.description:\n                json_schema_update['description'] = metadata.description\n            if metadata.examples:\n                json_schema_update['examples'] = to_jsonable_python(metadata.examples)\n\n            json_schema_extra = metadata.json_schema_extra\n            if json_schema_update or json_schema_extra:\n                CoreMetadataHandler(schema).metadata.setdefault('pydantic_js_annotation_functions', []).append(\n                    get_json_schema_update_func(json_schema_update, json_schema_extra)\n                )\n        return schema\n\n    def _get_wrapped_inner_schema(\n        self,\n        get_inner_schema: GetCoreSchemaHandler,\n        annotation: Any,\n        pydantic_js_annotation_functions: list[GetJsonSchemaFunction],\n    ) -> CallbackGetCoreSchemaHandler:\n        metadata_get_schema: GetCoreSchemaFunction = getattr(annotation, '__get_pydantic_core_schema__', None) or (\n            lambda source, handler: handler(source)\n        )\n\n        def new_handler(source: Any) -> core_schema.CoreSchema:\n            schema = metadata_get_schema(source, get_inner_schema)\n            schema = self._apply_single_annotation(schema, annotation)\n            schema = self._apply_single_annotation_json_schema(schema, annotation)\n\n            metadata_js_function = _extract_get_pydantic_json_schema(annotation, schema)\n            if metadata_js_function is not None:\n                pydantic_js_annotation_functions.append(metadata_js_function)\n            return schema\n\n        return CallbackGetCoreSchemaHandler(new_handler, self)\n\n    def _apply_field_serializers(\n        self,\n        schema: core_schema.CoreSchema,\n        serializers: list[Decorator[FieldSerializerDecoratorInfo]],\n        computed_field: bool = False,\n    ) -> core_schema.CoreSchema:\n        \"\"\"Apply field serializers to a schema.\"\"\"\n        if serializers:\n            schema = copy(schema)\n            if schema['type'] == 'definitions':\n                inner_schema = schema['schema']\n                schema['schema'] = self._apply_field_serializers(inner_schema, serializers)\n                return schema\n            else:\n                ref = typing.cast('str|None', schema.get('ref', None))\n                if ref is not None:\n                    schema = core_schema.definition_reference_schema(ref)\n\n            # use the last serializer to make it easy to override a serializer set on a parent model\n            serializer = serializers[-1]\n            is_field_serializer, info_arg = inspect_field_serializer(\n                serializer.func, serializer.info.mode, computed_field=computed_field\n            )\n\n            try:\n                return_type = _decorators.get_function_return_type(\n                    serializer.func, serializer.info.return_type, self._types_namespace\n                )\n            except NameError as e:\n                raise PydanticUndefinedAnnotation.from_name_error(e) from e\n\n            if return_type is PydanticUndefined:\n                return_schema = None\n            else:\n                return_schema = self.generate_schema(return_type)\n\n            if serializer.info.mode == 'wrap':\n                schema['serialization'] = core_schema.wrap_serializer_function_ser_schema(\n                    serializer.func,\n                    is_field_serializer=is_field_serializer,\n                    info_arg=info_arg,\n                    return_schema=return_schema,\n                    when_used=serializer.info.when_used,\n                )\n            else:\n                assert serializer.info.mode == 'plain'\n                schema['serialization'] = core_schema.plain_serializer_function_ser_schema(\n                    serializer.func,\n                    is_field_serializer=is_field_serializer,\n                    info_arg=info_arg,\n                    return_schema=return_schema,\n                    when_used=serializer.info.when_used,\n                )\n        return schema\n\n    def _apply_model_serializers(\n        self, schema: core_schema.CoreSchema, serializers: Iterable[Decorator[ModelSerializerDecoratorInfo]]\n    ) -> core_schema.CoreSchema:\n        \"\"\"Apply model serializers to a schema.\"\"\"\n        ref: str | None = schema.pop('ref', None)  # type: ignore\n        if serializers:\n            serializer = list(serializers)[-1]\n            info_arg = inspect_model_serializer(serializer.func, serializer.info.mode)\n\n            try:\n                return_type = _decorators.get_function_return_type(\n                    serializer.func, serializer.info.return_type, self._types_namespace\n                )\n            except NameError as e:\n                raise PydanticUndefinedAnnotation.from_name_error(e) from e\n            if return_type is PydanticUndefined:\n                return_schema = None\n            else:\n                return_schema = self.generate_schema(return_type)\n\n            if serializer.info.mode == 'wrap':\n                ser_schema: core_schema.SerSchema = core_schema.wrap_serializer_function_ser_schema(\n                    serializer.func,\n                    info_arg=info_arg,\n                    return_schema=return_schema,\n                    when_used=serializer.info.when_used,\n                )\n            else:\n                # plain\n                ser_schema = core_schema.plain_serializer_function_ser_schema(\n                    serializer.func,\n                    info_arg=info_arg,\n                    return_schema=return_schema,\n                    when_used=serializer.info.when_used,\n                )\n            schema['serialization'] = ser_schema\n        if ref:\n            schema['ref'] = ref  # type: ignore\n        return schema\n\n\n_VALIDATOR_F_MATCH: Mapping[\n    tuple[FieldValidatorModes, Literal['no-info', 'with-info']],\n    Callable[[Callable[..., Any], core_schema.CoreSchema, str | None], core_schema.CoreSchema],\n] = {\n    ('before', 'no-info'): lambda f, schema, _: core_schema.no_info_before_validator_function(f, schema),\n    ('after', 'no-info'): lambda f, schema, _: core_schema.no_info_after_validator_function(f, schema),\n    ('plain', 'no-info'): lambda f, _1, _2: core_schema.no_info_plain_validator_function(f),\n    ('wrap', 'no-info'): lambda f, schema, _: core_schema.no_info_wrap_validator_function(f, schema),\n    ('before', 'with-info'): lambda f, schema, field_name: core_schema.with_info_before_validator_function(\n        f, schema, field_name=field_name\n    ),\n    ('after', 'with-info'): lambda f, schema, field_name: core_schema.with_info_after_validator_function(\n        f, schema, field_name=field_name\n    ),\n    ('plain', 'with-info'): lambda f, _, field_name: core_schema.with_info_plain_validator_function(\n        f, field_name=field_name\n    ),\n    ('wrap', 'with-info'): lambda f, schema, field_name: core_schema.with_info_wrap_validator_function(\n        f, schema, field_name=field_name\n    ),\n}\n\n\ndef apply_validators(\n    schema: core_schema.CoreSchema,\n    validators: Iterable[Decorator[RootValidatorDecoratorInfo]]\n    | Iterable[Decorator[ValidatorDecoratorInfo]]\n    | Iterable[Decorator[FieldValidatorDecoratorInfo]],\n    field_name: str | None,\n) -> core_schema.CoreSchema:\n    \"\"\"Apply validators to a schema.\n\n    Args:\n        schema: The schema to apply validators on.\n        validators: An iterable of validators.\n        field_name: The name of the field if validators are being applied to a model field.\n\n    Returns:\n        The updated schema.\n    \"\"\"\n    for validator in validators:\n        info_arg = inspect_validator(validator.func, validator.info.mode)\n        val_type = 'with-info' if info_arg else 'no-info'\n\n        schema = _VALIDATOR_F_MATCH[(validator.info.mode, val_type)](validator.func, schema, field_name)\n    return schema\n\n\ndef _validators_require_validate_default(validators: Iterable[Decorator[ValidatorDecoratorInfo]]) -> bool:\n    \"\"\"In v1, if any of the validators for a field had `always=True`, the default value would be validated.\n\n    This serves as an auxiliary function for re-implementing that logic, by looping over a provided\n    collection of (v1-style) ValidatorDecoratorInfo's and checking if any of them have `always=True`.\n\n    We should be able to drop this function and the associated logic calling it once we drop support\n    for v1-style validator decorators. (Or we can extend it and keep it if we add something equivalent\n    to the v1-validator `always` kwarg to `field_validator`.)\n    \"\"\"\n    for validator in validators:\n        if validator.info.always:\n            return True\n    return False\n\n\ndef apply_model_validators(\n    schema: core_schema.CoreSchema,\n    validators: Iterable[Decorator[ModelValidatorDecoratorInfo]],\n    mode: Literal['inner', 'outer', 'all'],\n) -> core_schema.CoreSchema:\n    \"\"\"Apply model validators to a schema.\n\n    If mode == 'inner', only \"before\" validators are applied\n    If mode == 'outer', validators other than \"before\" are applied\n    If mode == 'all', all validators are applied\n\n    Args:\n        schema: The schema to apply validators on.\n        validators: An iterable of validators.\n        mode: The validator mode.\n\n    Returns:\n        The updated schema.\n    \"\"\"\n    ref: str | None = schema.pop('ref', None)  # type: ignore\n    for validator in validators:\n        if mode == 'inner' and validator.info.mode != 'before':\n            continue\n        if mode == 'outer' and validator.info.mode == 'before':\n            continue\n        info_arg = inspect_validator(validator.func, validator.info.mode)\n        if validator.info.mode == 'wrap':\n            if info_arg:\n                schema = core_schema.with_info_wrap_validator_function(function=validator.func, schema=schema)\n            else:\n                schema = core_schema.no_info_wrap_validator_function(function=validator.func, schema=schema)\n        elif validator.info.mode == 'before':\n            if info_arg:\n                schema = core_schema.with_info_before_validator_function(function=validator.func, schema=schema)\n            else:\n                schema = core_schema.no_info_before_validator_function(function=validator.func, schema=schema)\n        else:\n            assert validator.info.mode == 'after'\n            if info_arg:\n                schema = core_schema.with_info_after_validator_function(function=validator.func, schema=schema)\n            else:\n                schema = core_schema.no_info_after_validator_function(function=validator.func, schema=schema)\n    if ref:\n        schema['ref'] = ref  # type: ignore\n    return schema\n\n\ndef wrap_default(field_info: FieldInfo, schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n    \"\"\"Wrap schema with default schema if default value or `default_factory` are available.\n\n    Args:\n        field_info: The field info object.\n        schema: The schema to apply default on.\n\n    Returns:\n        Updated schema by default value or `default_factory`.\n    \"\"\"\n    if field_info.default_factory:\n        return core_schema.with_default_schema(\n            schema, default_factory=field_info.default_factory, validate_default=field_info.validate_default\n        )\n    elif field_info.default is not PydanticUndefined:\n        return core_schema.with_default_schema(\n            schema, default=field_info.default, validate_default=field_info.validate_default\n        )\n    else:\n        return schema\n\n\ndef _extract_get_pydantic_json_schema(tp: Any, schema: CoreSchema) -> GetJsonSchemaFunction | None:\n    \"\"\"Extract `__get_pydantic_json_schema__` from a type, handling the deprecated `__modify_schema__`.\"\"\"\n    js_modify_function = getattr(tp, '__get_pydantic_json_schema__', None)\n\n    if hasattr(tp, '__modify_schema__'):\n        from pydantic import BaseModel  # circular reference\n\n        has_custom_v2_modify_js_func = (\n            js_modify_function is not None\n            and BaseModel.__get_pydantic_json_schema__.__func__  # type: ignore\n            not in (js_modify_function, getattr(js_modify_function, '__func__', None))\n        )\n\n        if not has_custom_v2_modify_js_func:\n            cls_name = getattr(tp, '__name__', None)\n            raise PydanticUserError(\n                f'The `__modify_schema__` method is not supported in Pydantic v2. '\n                f'Use `__get_pydantic_json_schema__` instead{f\" in class `{cls_name}`\" if cls_name else \"\"}.',\n                code='custom-json-schema',\n            )\n\n    # handle GenericAlias' but ignore Annotated which \"lies\" about its origin (in this case it would be `int`)\n    if hasattr(tp, '__origin__') and not isinstance(tp, type(Annotated[int, 'placeholder'])):\n        return _extract_get_pydantic_json_schema(tp.__origin__, schema)\n\n    if js_modify_function is None:\n        return None\n\n    return js_modify_function\n\n\ndef get_json_schema_update_func(\n    json_schema_update: JsonSchemaValue, json_schema_extra: JsonDict | typing.Callable[[JsonDict], None] | None\n) -> GetJsonSchemaFunction:\n    def json_schema_update_func(\n        core_schema_or_field: CoreSchemaOrField, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        json_schema = {**handler(core_schema_or_field), **json_schema_update}\n        add_json_schema_extra(json_schema, json_schema_extra)\n        return json_schema\n\n    return json_schema_update_func\n\n\ndef add_json_schema_extra(\n    json_schema: JsonSchemaValue, json_schema_extra: JsonDict | typing.Callable[[JsonDict], None] | None\n):\n    if isinstance(json_schema_extra, dict):\n        json_schema.update(to_jsonable_python(json_schema_extra))\n    elif callable(json_schema_extra):\n        json_schema_extra(json_schema)\n\n\nclass _CommonField(TypedDict):\n    schema: core_schema.CoreSchema\n    validation_alias: str | list[str | int] | list[list[str | int]] | None\n    serialization_alias: str | None\n    serialization_exclude: bool | None\n    frozen: bool | None\n    metadata: dict[str, Any]\n\n\ndef _common_field(\n    schema: core_schema.CoreSchema,\n    *,\n    validation_alias: str | list[str | int] | list[list[str | int]] | None = None,\n    serialization_alias: str | None = None,\n    serialization_exclude: bool | None = None,\n    frozen: bool | None = None,\n    metadata: Any = None,\n) -> _CommonField:\n    return {\n        'schema': schema,\n        'validation_alias': validation_alias,\n        'serialization_alias': serialization_alias,\n        'serialization_exclude': serialization_exclude,\n        'frozen': frozen,\n        'metadata': metadata,\n    }\n\n\nclass _Definitions:\n    \"\"\"Keeps track of references and definitions.\"\"\"\n\n    def __init__(self) -> None:\n        self.seen: set[str] = set()\n        self.definitions: dict[str, core_schema.CoreSchema] = {}\n\n    @contextmanager\n    def get_schema_or_ref(self, tp: Any) -> Iterator[tuple[str, None] | tuple[str, CoreSchema]]:\n        \"\"\"Get a definition for `tp` if one exists.\n\n        If a definition exists, a tuple of `(ref_string, CoreSchema)` is returned.\n        If no definition exists yet, a tuple of `(ref_string, None)` is returned.\n\n        Note that the returned `CoreSchema` will always be a `DefinitionReferenceSchema`,\n        not the actual definition itself.\n\n        This should be called for any type that can be identified by reference.\n        This includes any recursive types.\n\n        At present the following types can be named/recursive:\n\n        - BaseModel\n        - Dataclasses\n        - TypedDict\n        - TypeAliasType\n        \"\"\"\n        ref = get_type_ref(tp)\n        # return the reference if we're either (1) in a cycle or (2) it was already defined\n        if ref in self.seen or ref in self.definitions:\n            yield (ref, core_schema.definition_reference_schema(ref))\n        else:\n            self.seen.add(ref)\n            try:\n                yield (ref, None)\n            finally:\n                self.seen.discard(ref)\n\n\ndef resolve_original_schema(schema: CoreSchema, definitions: dict[str, CoreSchema]) -> CoreSchema | None:\n    if schema['type'] == 'definition-ref':\n        return definitions.get(schema['schema_ref'], None)\n    elif schema['type'] == 'definitions':\n        return schema['schema']\n    else:\n        return schema\n\n\nclass _FieldNameStack:\n    __slots__ = ('_stack',)\n\n    def __init__(self) -> None:\n        self._stack: list[str] = []\n\n    @contextmanager\n    def push(self, field_name: str) -> Iterator[None]:\n        self._stack.append(field_name)\n        yield\n        self._stack.pop()\n\n    def get(self) -> str | None:\n        if self._stack:\n            return self._stack[-1]\n        else:\n            return None\n\n\nclass _ModelTypeStack:\n    __slots__ = ('_stack',)\n\n    def __init__(self) -> None:\n        self._stack: list[type] = []\n\n    @contextmanager\n    def push(self, type_obj: type) -> Iterator[None]:\n        self._stack.append(type_obj)\n        yield\n        self._stack.pop()\n\n    def get(self) -> type | None:\n        if self._stack:\n            return self._stack[-1]\n        else:\n            return None\n", "pydantic/_internal/_internal_dataclass.py": "import sys\n\n# `slots` is available on Python >= 3.10\nif sys.version_info >= (3, 10):\n    slots_true = {'slots': True}\nelse:\n    slots_true = {}\n", "pydantic/_internal/__init__.py": "", "pydantic/_internal/_fields.py": "\"\"\"Private logic related to fields (the `Field()` function and `FieldInfo` class), and arguments to `Annotated`.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport sys\nimport warnings\nfrom copy import copy\nfrom functools import lru_cache\nfrom typing import TYPE_CHECKING, Any\n\nfrom pydantic_core import PydanticUndefined\n\nfrom pydantic.errors import PydanticUserError\n\nfrom . import _typing_extra\nfrom ._config import ConfigWrapper\nfrom ._docs_extraction import extract_docstrings_from_cls\nfrom ._repr import Representation\nfrom ._typing_extra import get_cls_type_hints_lenient, get_type_hints, is_classvar, is_finalvar\n\nif TYPE_CHECKING:\n    from annotated_types import BaseMetadata\n\n    from ..fields import FieldInfo\n    from ..main import BaseModel\n    from ._dataclasses import StandardDataclass\n    from ._decorators import DecoratorInfos\n\n\ndef get_type_hints_infer_globalns(\n    obj: Any,\n    localns: dict[str, Any] | None = None,\n    include_extras: bool = False,\n) -> dict[str, Any]:\n    \"\"\"Gets type hints for an object by inferring the global namespace.\n\n    It uses the `typing.get_type_hints`, The only thing that we do here is fetching\n    global namespace from `obj.__module__` if it is not `None`.\n\n    Args:\n        obj: The object to get its type hints.\n        localns: The local namespaces.\n        include_extras: Whether to recursively include annotation metadata.\n\n    Returns:\n        The object type hints.\n    \"\"\"\n    module_name = getattr(obj, '__module__', None)\n    globalns: dict[str, Any] | None = None\n    if module_name:\n        try:\n            globalns = sys.modules[module_name].__dict__\n        except KeyError:\n            # happens occasionally, see https://github.com/pydantic/pydantic/issues/2363\n            pass\n    return get_type_hints(obj, globalns=globalns, localns=localns, include_extras=include_extras)\n\n\nclass PydanticMetadata(Representation):\n    \"\"\"Base class for annotation markers like `Strict`.\"\"\"\n\n    __slots__ = ()\n\n\ndef pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    \"\"\"Create a new `_PydanticGeneralMetadata` class with the given metadata.\n\n    Args:\n        **metadata: The metadata to add.\n\n    Returns:\n        The new `_PydanticGeneralMetadata` class.\n    \"\"\"\n    return _general_metadata_cls()(metadata)  # type: ignore\n\n\n@lru_cache(maxsize=None)\ndef _general_metadata_cls() -> type[BaseMetadata]:\n    \"\"\"Do it this way to avoid importing `annotated_types` at import time.\"\"\"\n    from annotated_types import BaseMetadata\n\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        \"\"\"Pydantic general metadata like `max_digits`.\"\"\"\n\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n\n    return _PydanticGeneralMetadata  # type: ignore\n\n\ndef _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], config_wrapper: ConfigWrapper) -> None:\n    if config_wrapper.use_attribute_docstrings:\n        fields_docs = extract_docstrings_from_cls(cls)\n        for ann_name, field_info in fields.items():\n            if field_info.description is None and ann_name in fields_docs:\n                field_info.description = fields_docs[ann_name]\n\n\ndef collect_model_fields(  # noqa: C901\n    cls: type[BaseModel],\n    bases: tuple[type[Any], ...],\n    config_wrapper: ConfigWrapper,\n    types_namespace: dict[str, Any] | None,\n    *,\n    typevars_map: dict[Any, Any] | None = None,\n) -> tuple[dict[str, FieldInfo], set[str]]:\n    \"\"\"Collect the fields of a nascent pydantic model.\n\n    Also collect the names of any ClassVars present in the type hints.\n\n    The returned value is a tuple of two items: the fields dict, and the set of ClassVar names.\n\n    Args:\n        cls: BaseModel or dataclass.\n        bases: Parents of the class, generally `cls.__bases__`.\n        config_wrapper: The config wrapper instance.\n        types_namespace: Optional extra namespace to look for types in.\n        typevars_map: A dictionary mapping type variables to their concrete types.\n\n    Returns:\n        A tuple contains fields and class variables.\n\n    Raises:\n        NameError:\n            - If there is a conflict between a field name and protected namespaces.\n            - If there is a field other than `root` in `RootModel`.\n            - If a field shadows an attribute in the parent model.\n    \"\"\"\n    from ..fields import FieldInfo\n\n    type_hints = get_cls_type_hints_lenient(cls, types_namespace)\n\n    # https://docs.python.org/3/howto/annotations.html#accessing-the-annotations-dict-of-an-object-in-python-3-9-and-older\n    # annotations is only used for finding fields in parent classes\n    annotations = cls.__dict__.get('__annotations__', {})\n    fields: dict[str, FieldInfo] = {}\n\n    class_vars: set[str] = set()\n    for ann_name, ann_type in type_hints.items():\n        if ann_name == 'model_config':\n            # We never want to treat `model_config` as a field\n            # Note: we may need to change this logic if/when we introduce a `BareModel` class with no\n            # protected namespaces (where `model_config` might be allowed as a field name)\n            continue\n        for protected_namespace in config_wrapper.protected_namespaces:\n            if ann_name.startswith(protected_namespace):\n                for b in bases:\n                    if hasattr(b, ann_name):\n                        from ..main import BaseModel\n\n                        if not (issubclass(b, BaseModel) and ann_name in b.model_fields):\n                            raise NameError(\n                                f'Field \"{ann_name}\" conflicts with member {getattr(b, ann_name)}'\n                                f' of protected namespace \"{protected_namespace}\".'\n                            )\n                else:\n                    valid_namespaces = tuple(\n                        x for x in config_wrapper.protected_namespaces if not ann_name.startswith(x)\n                    )\n                    warnings.warn(\n                        f'Field \"{ann_name}\" has conflict with protected namespace \"{protected_namespace}\".'\n                        '\\n\\nYou may be able to resolve this warning by setting'\n                        f\" `model_config['protected_namespaces'] = {valid_namespaces}`.\",\n                        UserWarning,\n                    )\n        if is_classvar(ann_type):\n            class_vars.add(ann_name)\n            continue\n        if _is_finalvar_with_default_val(ann_type, getattr(cls, ann_name, PydanticUndefined)):\n            class_vars.add(ann_name)\n            continue\n        if not is_valid_field_name(ann_name):\n            continue\n        if cls.__pydantic_root_model__ and ann_name != 'root':\n            raise NameError(\n                f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n            )\n\n        # when building a generic model with `MyModel[int]`, the generic_origin check makes sure we don't get\n        # \"... shadows an attribute\" warnings\n        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n        for base in bases:\n            dataclass_fields = {\n                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n            }\n            if hasattr(base, ann_name):\n                if base is generic_origin:\n                    # Don't warn when \"shadowing\" of attributes in parametrized generics\n                    continue\n\n                if ann_name in dataclass_fields:\n                    # Don't warn when inheriting stdlib dataclasses whose fields are \"shadowed\" by defaults being set\n                    # on the class instance.\n                    continue\n\n                if ann_name not in annotations:\n                    # Don't warn when a field exists in a parent class but has not been defined in the current class\n                    continue\n\n                warnings.warn(\n                    f'Field name \"{ann_name}\" in \"{cls.__qualname__}\" shadows an attribute in parent '\n                    f'\"{base.__qualname__}\"',\n                    UserWarning,\n                )\n\n        try:\n            default = getattr(cls, ann_name, PydanticUndefined)\n            if default is PydanticUndefined:\n                raise AttributeError\n        except AttributeError:\n            if ann_name in annotations:\n                field_info = FieldInfo.from_annotation(ann_type)\n            else:\n                # if field has no default value and is not in __annotations__ this means that it is\n                # defined in a base class and we can take it from there\n                model_fields_lookup: dict[str, FieldInfo] = {}\n                for x in cls.__bases__[::-1]:\n                    model_fields_lookup.update(getattr(x, 'model_fields', {}))\n                if ann_name in model_fields_lookup:\n                    # The field was present on one of the (possibly multiple) base classes\n                    # copy the field to make sure typevar substitutions don't cause issues with the base classes\n                    field_info = copy(model_fields_lookup[ann_name])\n                else:\n                    # The field was not found on any base classes; this seems to be caused by fields not getting\n                    # generated thanks to models not being fully defined while initializing recursive models.\n                    # Nothing stops us from just creating a new FieldInfo for this type hint, so we do this.\n                    field_info = FieldInfo.from_annotation(ann_type)\n        else:\n            _warn_on_nested_alias_in_annotation(ann_type, ann_name)\n            field_info = FieldInfo.from_annotated_attribute(ann_type, default)\n            # attributes which are fields are removed from the class namespace:\n            # 1. To match the behaviour of annotation-only fields\n            # 2. To avoid false positives in the NameError check above\n            try:\n                delattr(cls, ann_name)\n            except AttributeError:\n                pass  # indicates the attribute was on a parent class\n\n        # Use cls.__dict__['__pydantic_decorators__'] instead of cls.__pydantic_decorators__\n        # to make sure the decorators have already been built for this exact class\n        decorators: DecoratorInfos = cls.__dict__['__pydantic_decorators__']\n        if ann_name in decorators.computed_fields:\n            raise ValueError(\"you can't override a field with a computed field\")\n        fields[ann_name] = field_info\n\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map, types_namespace)\n\n    _update_fields_from_docstrings(cls, fields, config_wrapper)\n\n    return fields, class_vars\n\n\ndef _warn_on_nested_alias_in_annotation(ann_type: type[Any], ann_name: str):\n    from ..fields import FieldInfo\n\n    if hasattr(ann_type, '__args__'):\n        for anno_arg in ann_type.__args__:\n            if _typing_extra.is_annotated(anno_arg):\n                for anno_type_arg in _typing_extra.get_args(anno_arg):\n                    if isinstance(anno_type_arg, FieldInfo) and anno_type_arg.alias is not None:\n                        warnings.warn(\n                            f'`alias` specification on field \"{ann_name}\" must be set on outermost annotation to take effect.',\n                            UserWarning,\n                        )\n                        break\n\n\ndef _is_finalvar_with_default_val(type_: type[Any], val: Any) -> bool:\n    from ..fields import FieldInfo\n\n    if not is_finalvar(type_):\n        return False\n    elif val is PydanticUndefined:\n        return False\n    elif isinstance(val, FieldInfo) and (val.default is PydanticUndefined and val.default_factory is None):\n        return False\n    else:\n        return True\n\n\ndef collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    types_namespace: dict[str, Any] | None,\n    *,\n    typevars_map: dict[Any, Any] | None = None,\n    config_wrapper: ConfigWrapper | None = None,\n) -> dict[str, FieldInfo]:\n    \"\"\"Collect the fields of a dataclass.\n\n    Args:\n        cls: dataclass.\n        types_namespace: Optional extra namespace to look for types in.\n        typevars_map: A dictionary mapping type variables to their concrete types.\n        config_wrapper: The config wrapper instance.\n\n    Returns:\n        The dataclass fields.\n    \"\"\"\n    from ..fields import FieldInfo\n\n    fields: dict[str, FieldInfo] = {}\n    dataclass_fields: dict[str, dataclasses.Field] = cls.__dataclass_fields__\n    cls_localns = dict(vars(cls))  # this matches get_cls_type_hints_lenient, but all tests pass with `= None` instead\n\n    source_module = sys.modules.get(cls.__module__)\n    if source_module is not None:\n        types_namespace = {**source_module.__dict__, **(types_namespace or {})}\n\n    for ann_name, dataclass_field in dataclass_fields.items():\n        ann_type = _typing_extra.eval_type_lenient(dataclass_field.type, types_namespace, cls_localns)\n        if is_classvar(ann_type):\n            continue\n\n        if (\n            not dataclass_field.init\n            and dataclass_field.default == dataclasses.MISSING\n            and dataclass_field.default_factory == dataclasses.MISSING\n        ):\n            # TODO: We should probably do something with this so that validate_assignment behaves properly\n            #   Issue: https://github.com/pydantic/pydantic/issues/5470\n            continue\n\n        if isinstance(dataclass_field.default, FieldInfo):\n            if dataclass_field.default.init_var:\n                if dataclass_field.default.init is False:\n                    raise PydanticUserError(\n                        f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                        code='clashing-init-and-init-var',\n                    )\n\n                # TODO: same note as above re validate_assignment\n                continue\n            field_info = FieldInfo.from_annotated_attribute(ann_type, dataclass_field.default)\n        else:\n            field_info = FieldInfo.from_annotated_attribute(ann_type, dataclass_field)\n\n        fields[ann_name] = field_info\n\n        if field_info.default is not PydanticUndefined and isinstance(getattr(cls, ann_name, field_info), FieldInfo):\n            # We need this to fix the default when the \"default\" from __dataclass_fields__ is a pydantic.FieldInfo\n            setattr(cls, ann_name, field_info.default)\n\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map, types_namespace)\n\n    if config_wrapper is not None:\n        _update_fields_from_docstrings(cls, fields, config_wrapper)\n\n    return fields\n\n\ndef is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')\n\n\ndef is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')\n", "pydantic/_internal/_repr.py": "\"\"\"Tools to provide pretty/human-readable display of objects.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport types\nimport typing\nfrom typing import Any\n\nimport typing_extensions\n\nfrom . import _typing_extra\n\nif typing.TYPE_CHECKING:\n    ReprArgs: typing_extensions.TypeAlias = 'typing.Iterable[tuple[str | None, Any]]'\n    RichReprResult: typing_extensions.TypeAlias = (\n        'typing.Iterable[Any | tuple[Any] | tuple[str, Any] | tuple[str, Any, Any]]'\n    )\n\n\nclass PlainRepr(str):\n    \"\"\"String class where repr doesn't include quotes. Useful with Representation when you want to return a string\n    representation of something that is valid (or pseudo-valid) python.\n    \"\"\"\n\n    def __repr__(self) -> str:\n        return str(self)\n\n\nclass Representation:\n    # Mixin to provide `__str__`, `__repr__`, and `__pretty__` and `__rich_repr__` methods.\n    # `__pretty__` is used by [devtools](https://python-devtools.helpmanual.io/).\n    # `__rich_repr__` is used by [rich](https://rich.readthedocs.io/en/stable/pretty.html).\n    # (this is not a docstring to avoid adding a docstring to classes which inherit from Representation)\n\n    # we don't want to use a type annotation here as it can break get_type_hints\n    __slots__ = tuple()  # type: typing.Collection[str]\n\n    def __repr_args__(self) -> ReprArgs:\n        \"\"\"Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n\n        Can either return:\n        * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n        * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n        \"\"\"\n        attrs_names = self.__slots__\n        if not attrs_names and hasattr(self, '__dict__'):\n            attrs_names = self.__dict__.keys()\n        attrs = ((s, getattr(self, s)) for s in attrs_names)\n        return [(a, v) for a, v in attrs if v is not None]\n\n    def __repr_name__(self) -> str:\n        \"\"\"Name of the instance's class, used in __repr__.\"\"\"\n        return self.__class__.__name__\n\n    def __repr_str__(self, join_str: str) -> str:\n        return join_str.join(repr(v) if a is None else f'{a}={v!r}' for a, v in self.__repr_args__())\n\n    def __pretty__(self, fmt: typing.Callable[[Any], Any], **kwargs: Any) -> typing.Generator[Any, None, None]:\n        \"\"\"Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\"\"\"\n        yield self.__repr_name__() + '('\n        yield 1\n        for name, value in self.__repr_args__():\n            if name is not None:\n                yield name + '='\n            yield fmt(value)\n            yield ','\n            yield 0\n        yield -1\n        yield ')'\n\n    def __rich_repr__(self) -> RichReprResult:\n        \"\"\"Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\"\"\"\n        for name, field_repr in self.__repr_args__():\n            if name is None:\n                yield field_repr\n            else:\n                yield name, field_repr\n\n    def __str__(self) -> str:\n        return self.__repr_str__(' ')\n\n    def __repr__(self) -> str:\n        return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'\n\n\ndef display_as_type(obj: Any) -> str:\n    \"\"\"Pretty representation of a type, should be as close as possible to the original type definition string.\n\n    Takes some logic from `typing._type_repr`.\n    \"\"\"\n    if isinstance(obj, types.FunctionType):\n        return obj.__name__\n    elif obj is ...:\n        return '...'\n    elif isinstance(obj, Representation):\n        return repr(obj)\n    elif isinstance(obj, typing_extensions.TypeAliasType):\n        return str(obj)\n\n    if not isinstance(obj, (_typing_extra.typing_base, _typing_extra.WithArgsTypes, type)):\n        obj = obj.__class__\n\n    if _typing_extra.origin_is_union(typing_extensions.get_origin(obj)):\n        args = ', '.join(map(display_as_type, typing_extensions.get_args(obj)))\n        return f'Union[{args}]'\n    elif isinstance(obj, _typing_extra.WithArgsTypes):\n        if typing_extensions.get_origin(obj) == typing_extensions.Literal:\n            args = ', '.join(map(repr, typing_extensions.get_args(obj)))\n        else:\n            args = ', '.join(map(display_as_type, typing_extensions.get_args(obj)))\n        try:\n            return f'{obj.__qualname__}[{args}]'\n        except AttributeError:\n            return str(obj)  # handles TypeAliasType in 3.12\n    elif isinstance(obj, type):\n        return obj.__qualname__\n    else:\n        return repr(obj).replace('typing.', '').replace('typing_extensions.', '')\n", "pydantic/_internal/_decorators.py": "\"\"\"Logic related to validators applied to models etc. via the `@field_validator` and `@model_validator` decorators.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom functools import cached_property, partial, partialmethod\nfrom inspect import Parameter, Signature, isdatadescriptor, ismethoddescriptor, signature\nfrom itertools import islice\nfrom typing import TYPE_CHECKING, Any, Callable, ClassVar, Generic, Iterable, TypeVar, Union\n\nfrom pydantic_core import PydanticUndefined, core_schema\nfrom typing_extensions import Literal, TypeAlias, is_typeddict\n\nfrom ..errors import PydanticUserError\nfrom ._core_utils import get_type_ref\nfrom ._internal_dataclass import slots_true\nfrom ._typing_extra import get_function_type_hints\n\nif TYPE_CHECKING:\n    from ..fields import ComputedFieldInfo\n    from ..functional_validators import FieldValidatorModes\n\n\n@dataclass(**slots_true)\nclass ValidatorDecoratorInfo:\n    \"\"\"A container for data from `@validator` so that we can access it\n    while building the pydantic-core schema.\n\n    Attributes:\n        decorator_repr: A class variable representing the decorator string, '@validator'.\n        fields: A tuple of field names the validator should be called on.\n        mode: The proposed validator mode.\n        each_item: For complex objects (sets, lists etc.) whether to validate individual\n            elements rather than the whole object.\n        always: Whether this method and other validators should be called even if the value is missing.\n        check_fields: Whether to check that the fields actually exist on the model.\n    \"\"\"\n\n    decorator_repr: ClassVar[str] = '@validator'\n\n    fields: tuple[str, ...]\n    mode: Literal['before', 'after']\n    each_item: bool\n    always: bool\n    check_fields: bool | None\n\n\n@dataclass(**slots_true)\nclass FieldValidatorDecoratorInfo:\n    \"\"\"A container for data from `@field_validator` so that we can access it\n    while building the pydantic-core schema.\n\n    Attributes:\n        decorator_repr: A class variable representing the decorator string, '@field_validator'.\n        fields: A tuple of field names the validator should be called on.\n        mode: The proposed validator mode.\n        check_fields: Whether to check that the fields actually exist on the model.\n    \"\"\"\n\n    decorator_repr: ClassVar[str] = '@field_validator'\n\n    fields: tuple[str, ...]\n    mode: FieldValidatorModes\n    check_fields: bool | None\n\n\n@dataclass(**slots_true)\nclass RootValidatorDecoratorInfo:\n    \"\"\"A container for data from `@root_validator` so that we can access it\n    while building the pydantic-core schema.\n\n    Attributes:\n        decorator_repr: A class variable representing the decorator string, '@root_validator'.\n        mode: The proposed validator mode.\n    \"\"\"\n\n    decorator_repr: ClassVar[str] = '@root_validator'\n    mode: Literal['before', 'after']\n\n\n@dataclass(**slots_true)\nclass FieldSerializerDecoratorInfo:\n    \"\"\"A container for data from `@field_serializer` so that we can access it\n    while building the pydantic-core schema.\n\n    Attributes:\n        decorator_repr: A class variable representing the decorator string, '@field_serializer'.\n        fields: A tuple of field names the serializer should be called on.\n        mode: The proposed serializer mode.\n        return_type: The type of the serializer's return value.\n        when_used: The serialization condition. Accepts a string with values `'always'`, `'unless-none'`, `'json'`,\n            and `'json-unless-none'`.\n        check_fields: Whether to check that the fields actually exist on the model.\n    \"\"\"\n\n    decorator_repr: ClassVar[str] = '@field_serializer'\n    fields: tuple[str, ...]\n    mode: Literal['plain', 'wrap']\n    return_type: Any\n    when_used: core_schema.WhenUsed\n    check_fields: bool | None\n\n\n@dataclass(**slots_true)\nclass ModelSerializerDecoratorInfo:\n    \"\"\"A container for data from `@model_serializer` so that we can access it\n    while building the pydantic-core schema.\n\n    Attributes:\n        decorator_repr: A class variable representing the decorator string, '@model_serializer'.\n        mode: The proposed serializer mode.\n        return_type: The type of the serializer's return value.\n        when_used: The serialization condition. Accepts a string with values `'always'`, `'unless-none'`, `'json'`,\n            and `'json-unless-none'`.\n    \"\"\"\n\n    decorator_repr: ClassVar[str] = '@model_serializer'\n    mode: Literal['plain', 'wrap']\n    return_type: Any\n    when_used: core_schema.WhenUsed\n\n\n@dataclass(**slots_true)\nclass ModelValidatorDecoratorInfo:\n    \"\"\"A container for data from `@model_validator` so that we can access it\n    while building the pydantic-core schema.\n\n    Attributes:\n        decorator_repr: A class variable representing the decorator string, '@model_serializer'.\n        mode: The proposed serializer mode.\n    \"\"\"\n\n    decorator_repr: ClassVar[str] = '@model_validator'\n    mode: Literal['wrap', 'before', 'after']\n\n\nDecoratorInfo: TypeAlias = \"\"\"Union[\n    ValidatorDecoratorInfo,\n    FieldValidatorDecoratorInfo,\n    RootValidatorDecoratorInfo,\n    FieldSerializerDecoratorInfo,\n    ModelSerializerDecoratorInfo,\n    ModelValidatorDecoratorInfo,\n    ComputedFieldInfo,\n]\"\"\"\n\nReturnType = TypeVar('ReturnType')\nDecoratedType: TypeAlias = (\n    'Union[classmethod[Any, Any, ReturnType], staticmethod[Any, ReturnType], Callable[..., ReturnType], property]'\n)\n\n\n@dataclass  # can't use slots here since we set attributes on `__post_init__`\nclass PydanticDescriptorProxy(Generic[ReturnType]):\n    \"\"\"Wrap a classmethod, staticmethod, property or unbound function\n    and act as a descriptor that allows us to detect decorated items\n    from the class' attributes.\n\n    This class' __get__ returns the wrapped item's __get__ result,\n    which makes it transparent for classmethods and staticmethods.\n\n    Attributes:\n        wrapped: The decorator that has to be wrapped.\n        decorator_info: The decorator info.\n        shim: A wrapper function to wrap V1 style function.\n    \"\"\"\n\n    wrapped: DecoratedType[ReturnType]\n    decorator_info: DecoratorInfo\n    shim: Callable[[Callable[..., Any]], Callable[..., Any]] | None = None\n\n    def __post_init__(self):\n        for attr in 'setter', 'deleter':\n            if hasattr(self.wrapped, attr):\n                f = partial(self._call_wrapped_attr, name=attr)\n                setattr(self, attr, f)\n\n    def _call_wrapped_attr(self, func: Callable[[Any], None], *, name: str) -> PydanticDescriptorProxy[ReturnType]:\n        self.wrapped = getattr(self.wrapped, name)(func)\n        return self\n\n    def __get__(self, obj: object | None, obj_type: type[object] | None = None) -> PydanticDescriptorProxy[ReturnType]:\n        try:\n            return self.wrapped.__get__(obj, obj_type)\n        except AttributeError:\n            # not a descriptor, e.g. a partial object\n            return self.wrapped  # type: ignore[return-value]\n\n    def __set_name__(self, instance: Any, name: str) -> None:\n        if hasattr(self.wrapped, '__set_name__'):\n            self.wrapped.__set_name__(instance, name)  # pyright: ignore[reportFunctionMemberAccess]\n\n    def __getattr__(self, __name: str) -> Any:\n        \"\"\"Forward checks for __isabstractmethod__ and such.\"\"\"\n        return getattr(self.wrapped, __name)\n\n\nDecoratorInfoType = TypeVar('DecoratorInfoType', bound=DecoratorInfo)\n\n\n@dataclass(**slots_true)\nclass Decorator(Generic[DecoratorInfoType]):\n    \"\"\"A generic container class to join together the decorator metadata\n    (metadata from decorator itself, which we have when the\n    decorator is called but not when we are building the core-schema)\n    and the bound function (which we have after the class itself is created).\n\n    Attributes:\n        cls_ref: The class ref.\n        cls_var_name: The decorated function name.\n        func: The decorated function.\n        shim: A wrapper function to wrap V1 style function.\n        info: The decorator info.\n    \"\"\"\n\n    cls_ref: str\n    cls_var_name: str\n    func: Callable[..., Any]\n    shim: Callable[[Any], Any] | None\n    info: DecoratorInfoType\n\n    @staticmethod\n    def build(\n        cls_: Any,\n        *,\n        cls_var_name: str,\n        shim: Callable[[Any], Any] | None,\n        info: DecoratorInfoType,\n    ) -> Decorator[DecoratorInfoType]:\n        \"\"\"Build a new decorator.\n\n        Args:\n            cls_: The class.\n            cls_var_name: The decorated function name.\n            shim: A wrapper function to wrap V1 style function.\n            info: The decorator info.\n\n        Returns:\n            The new decorator instance.\n        \"\"\"\n        func = get_attribute_from_bases(cls_, cls_var_name)\n        if shim is not None:\n            func = shim(func)\n        func = unwrap_wrapped_function(func, unwrap_partial=False)\n        if not callable(func):\n            # This branch will get hit for classmethod properties\n            attribute = get_attribute_from_base_dicts(cls_, cls_var_name)  # prevents the binding call to `__get__`\n            if isinstance(attribute, PydanticDescriptorProxy):\n                func = unwrap_wrapped_function(attribute.wrapped)\n        return Decorator(\n            cls_ref=get_type_ref(cls_),\n            cls_var_name=cls_var_name,\n            func=func,\n            shim=shim,\n            info=info,\n        )\n\n    def bind_to_cls(self, cls: Any) -> Decorator[DecoratorInfoType]:\n        \"\"\"Bind the decorator to a class.\n\n        Args:\n            cls: the class.\n\n        Returns:\n            The new decorator instance.\n        \"\"\"\n        return self.build(\n            cls,\n            cls_var_name=self.cls_var_name,\n            shim=self.shim,\n            info=self.info,\n        )\n\n\ndef get_bases(tp: type[Any]) -> tuple[type[Any], ...]:\n    \"\"\"Get the base classes of a class or typeddict.\n\n    Args:\n        tp: The type or class to get the bases.\n\n    Returns:\n        The base classes.\n    \"\"\"\n    if is_typeddict(tp):\n        return tp.__orig_bases__  # type: ignore\n    try:\n        return tp.__bases__\n    except AttributeError:\n        return ()\n\n\ndef mro(tp: type[Any]) -> tuple[type[Any], ...]:\n    \"\"\"Calculate the Method Resolution Order of bases using the C3 algorithm.\n\n    See https://www.python.org/download/releases/2.3/mro/\n    \"\"\"\n    # try to use the existing mro, for performance mainly\n    # but also because it helps verify the implementation below\n    if not is_typeddict(tp):\n        try:\n            return tp.__mro__\n        except AttributeError:\n            # GenericAlias and some other cases\n            pass\n\n    bases = get_bases(tp)\n    return (tp,) + mro_for_bases(bases)\n\n\ndef mro_for_bases(bases: tuple[type[Any], ...]) -> tuple[type[Any], ...]:\n    def merge_seqs(seqs: list[deque[type[Any]]]) -> Iterable[type[Any]]:\n        while True:\n            non_empty = [seq for seq in seqs if seq]\n            if not non_empty:\n                # Nothing left to process, we're done.\n                return\n            candidate: type[Any] | None = None\n            for seq in non_empty:  # Find merge candidates among seq heads.\n                candidate = seq[0]\n                not_head = [s for s in non_empty if candidate in islice(s, 1, None)]\n                if not_head:\n                    # Reject the candidate.\n                    candidate = None\n                else:\n                    break\n            if not candidate:\n                raise TypeError('Inconsistent hierarchy, no C3 MRO is possible')\n            yield candidate\n            for seq in non_empty:\n                # Remove candidate.\n                if seq[0] == candidate:\n                    seq.popleft()\n\n    seqs = [deque(mro(base)) for base in bases] + [deque(bases)]\n    return tuple(merge_seqs(seqs))\n\n\n_sentinel = object()\n\n\ndef get_attribute_from_bases(tp: type[Any] | tuple[type[Any], ...], name: str) -> Any:\n    \"\"\"Get the attribute from the next class in the MRO that has it,\n    aiming to simulate calling the method on the actual class.\n\n    The reason for iterating over the mro instead of just getting\n    the attribute (which would do that for us) is to support TypedDict,\n    which lacks a real __mro__, but can have a virtual one constructed\n    from its bases (as done here).\n\n    Args:\n        tp: The type or class to search for the attribute. If a tuple, this is treated as a set of base classes.\n        name: The name of the attribute to retrieve.\n\n    Returns:\n        Any: The attribute value, if found.\n\n    Raises:\n        AttributeError: If the attribute is not found in any class in the MRO.\n    \"\"\"\n    if isinstance(tp, tuple):\n        for base in mro_for_bases(tp):\n            attribute = base.__dict__.get(name, _sentinel)\n            if attribute is not _sentinel:\n                attribute_get = getattr(attribute, '__get__', None)\n                if attribute_get is not None:\n                    return attribute_get(None, tp)\n                return attribute\n        raise AttributeError(f'{name} not found in {tp}')\n    else:\n        try:\n            return getattr(tp, name)\n        except AttributeError:\n            return get_attribute_from_bases(mro(tp), name)\n\n\ndef get_attribute_from_base_dicts(tp: type[Any], name: str) -> Any:\n    \"\"\"Get an attribute out of the `__dict__` following the MRO.\n    This prevents the call to `__get__` on the descriptor, and allows\n    us to get the original function for classmethod properties.\n\n    Args:\n        tp: The type or class to search for the attribute.\n        name: The name of the attribute to retrieve.\n\n    Returns:\n        Any: The attribute value, if found.\n\n    Raises:\n        KeyError: If the attribute is not found in any class's `__dict__` in the MRO.\n    \"\"\"\n    for base in reversed(mro(tp)):\n        if name in base.__dict__:\n            return base.__dict__[name]\n    return tp.__dict__[name]  # raise the error\n\n\n@dataclass(**slots_true)\nclass DecoratorInfos:\n    \"\"\"Mapping of name in the class namespace to decorator info.\n\n    note that the name in the class namespace is the function or attribute name\n    not the field name!\n    \"\"\"\n\n    validators: dict[str, Decorator[ValidatorDecoratorInfo]] = field(default_factory=dict)\n    field_validators: dict[str, Decorator[FieldValidatorDecoratorInfo]] = field(default_factory=dict)\n    root_validators: dict[str, Decorator[RootValidatorDecoratorInfo]] = field(default_factory=dict)\n    field_serializers: dict[str, Decorator[FieldSerializerDecoratorInfo]] = field(default_factory=dict)\n    model_serializers: dict[str, Decorator[ModelSerializerDecoratorInfo]] = field(default_factory=dict)\n    model_validators: dict[str, Decorator[ModelValidatorDecoratorInfo]] = field(default_factory=dict)\n    computed_fields: dict[str, Decorator[ComputedFieldInfo]] = field(default_factory=dict)\n\n    @staticmethod\n    def build(model_dc: type[Any]) -> DecoratorInfos:  # noqa: C901 (ignore complexity)\n        \"\"\"We want to collect all DecFunc instances that exist as\n        attributes in the namespace of the class (a BaseModel or dataclass)\n        that called us\n        But we want to collect these in the order of the bases\n        So instead of getting them all from the leaf class (the class that called us),\n        we traverse the bases from root (the oldest ancestor class) to leaf\n        and collect all of the instances as we go, taking care to replace\n        any duplicate ones with the last one we see to mimic how function overriding\n        works with inheritance.\n        If we do replace any functions we put the replacement into the position\n        the replaced function was in; that is, we maintain the order.\n        \"\"\"\n        # reminder: dicts are ordered and replacement does not alter the order\n        res = DecoratorInfos()\n        for base in reversed(mro(model_dc)[1:]):\n            existing: DecoratorInfos | None = base.__dict__.get('__pydantic_decorators__')\n            if existing is None:\n                existing = DecoratorInfos.build(base)\n            res.validators.update({k: v.bind_to_cls(model_dc) for k, v in existing.validators.items()})\n            res.field_validators.update({k: v.bind_to_cls(model_dc) for k, v in existing.field_validators.items()})\n            res.root_validators.update({k: v.bind_to_cls(model_dc) for k, v in existing.root_validators.items()})\n            res.field_serializers.update({k: v.bind_to_cls(model_dc) for k, v in existing.field_serializers.items()})\n            res.model_serializers.update({k: v.bind_to_cls(model_dc) for k, v in existing.model_serializers.items()})\n            res.model_validators.update({k: v.bind_to_cls(model_dc) for k, v in existing.model_validators.items()})\n            res.computed_fields.update({k: v.bind_to_cls(model_dc) for k, v in existing.computed_fields.items()})\n\n        to_replace: list[tuple[str, Any]] = []\n\n        for var_name, var_value in vars(model_dc).items():\n            if isinstance(var_value, PydanticDescriptorProxy):\n                info = var_value.decorator_info\n                if isinstance(info, ValidatorDecoratorInfo):\n                    res.validators[var_name] = Decorator.build(\n                        model_dc, cls_var_name=var_name, shim=var_value.shim, info=info\n                    )\n                elif isinstance(info, FieldValidatorDecoratorInfo):\n                    res.field_validators[var_name] = Decorator.build(\n                        model_dc, cls_var_name=var_name, shim=var_value.shim, info=info\n                    )\n                elif isinstance(info, RootValidatorDecoratorInfo):\n                    res.root_validators[var_name] = Decorator.build(\n                        model_dc, cls_var_name=var_name, shim=var_value.shim, info=info\n                    )\n                elif isinstance(info, FieldSerializerDecoratorInfo):\n                    # check whether a serializer function is already registered for fields\n                    for field_serializer_decorator in res.field_serializers.values():\n                        # check that each field has at most one serializer function.\n                        # serializer functions for the same field in subclasses are allowed,\n                        # and are treated as overrides\n                        if field_serializer_decorator.cls_var_name == var_name:\n                            continue\n                        for f in info.fields:\n                            if f in field_serializer_decorator.info.fields:\n                                raise PydanticUserError(\n                                    'Multiple field serializer functions were defined '\n                                    f'for field {f!r}, this is not allowed.',\n                                    code='multiple-field-serializers',\n                                )\n                    res.field_serializers[var_name] = Decorator.build(\n                        model_dc, cls_var_name=var_name, shim=var_value.shim, info=info\n                    )\n                elif isinstance(info, ModelValidatorDecoratorInfo):\n                    res.model_validators[var_name] = Decorator.build(\n                        model_dc, cls_var_name=var_name, shim=var_value.shim, info=info\n                    )\n                elif isinstance(info, ModelSerializerDecoratorInfo):\n                    res.model_serializers[var_name] = Decorator.build(\n                        model_dc, cls_var_name=var_name, shim=var_value.shim, info=info\n                    )\n                else:\n                    from ..fields import ComputedFieldInfo\n\n                    isinstance(var_value, ComputedFieldInfo)\n                    res.computed_fields[var_name] = Decorator.build(\n                        model_dc, cls_var_name=var_name, shim=None, info=info\n                    )\n                to_replace.append((var_name, var_value.wrapped))\n        if to_replace:\n            # If we can save `__pydantic_decorators__` on the class we'll be able to check for it above\n            # so then we don't need to re-process the type, which means we can discard our descriptor wrappers\n            # and replace them with the thing they are wrapping (see the other setattr call below)\n            # which allows validator class methods to also function as regular class methods\n            setattr(model_dc, '__pydantic_decorators__', res)\n            for name, value in to_replace:\n                setattr(model_dc, name, value)\n        return res\n\n\ndef inspect_validator(validator: Callable[..., Any], mode: FieldValidatorModes) -> bool:\n    \"\"\"Look at a field or model validator function and determine whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        validator: The validator function to inspect.\n        mode: The proposed validator mode.\n\n    Returns:\n        Whether the validator takes an info argument.\n    \"\"\"\n    try:\n        sig = signature(validator)\n    except (ValueError, TypeError):\n        # `inspect.signature` might not be able to infer a signature, e.g. with C objects.\n        # In this case, we assume no info argument is present:\n        return False\n    n_positional = count_positional_required_params(sig)\n    if mode == 'wrap':\n        if n_positional == 3:\n            return True\n        elif n_positional == 2:\n            return False\n    else:\n        assert mode in {'before', 'after', 'plain'}, f\"invalid mode: {mode!r}, expected 'before', 'after' or 'plain\"\n        if n_positional == 2:\n            return True\n        elif n_positional == 1:\n            return False\n\n    raise PydanticUserError(\n        f'Unrecognized field_validator function signature for {validator} with `mode={mode}`:{sig}',\n        code='validator-signature',\n    )\n\n\ndef inspect_field_serializer(\n    serializer: Callable[..., Any], mode: Literal['plain', 'wrap'], computed_field: bool = False\n) -> tuple[bool, bool]:\n    \"\"\"Look at a field serializer function and determine if it is a field serializer,\n    and whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        serializer: The serializer function to inspect.\n        mode: The serializer mode, either 'plain' or 'wrap'.\n        computed_field: When serializer is applied on computed_field. It doesn't require\n            info signature.\n\n    Returns:\n        Tuple of (is_field_serializer, info_arg).\n    \"\"\"\n    try:\n        sig = signature(serializer)\n    except (ValueError, TypeError):\n        # `inspect.signature` might not be able to infer a signature, e.g. with C objects.\n        # In this case, we assume no info argument is present and this is not a method:\n        return (False, False)\n\n    first = next(iter(sig.parameters.values()), None)\n    is_field_serializer = first is not None and first.name == 'self'\n\n    n_positional = count_positional_required_params(sig)\n    if is_field_serializer:\n        # -1 to correct for self parameter\n        info_arg = _serializer_info_arg(mode, n_positional - 1)\n    else:\n        info_arg = _serializer_info_arg(mode, n_positional)\n\n    if info_arg is None:\n        raise PydanticUserError(\n            f'Unrecognized field_serializer function signature for {serializer} with `mode={mode}`:{sig}',\n            code='field-serializer-signature',\n        )\n    if info_arg and computed_field:\n        raise PydanticUserError(\n            'field_serializer on computed_field does not use info signature', code='field-serializer-signature'\n        )\n\n    else:\n        return is_field_serializer, info_arg\n\n\ndef inspect_annotated_serializer(serializer: Callable[..., Any], mode: Literal['plain', 'wrap']) -> bool:\n    \"\"\"Look at a serializer function used via `Annotated` and determine whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        serializer: The serializer function to check.\n        mode: The serializer mode, either 'plain' or 'wrap'.\n\n    Returns:\n        info_arg\n    \"\"\"\n    try:\n        sig = signature(serializer)\n    except (ValueError, TypeError):\n        # `inspect.signature` might not be able to infer a signature, e.g. with C objects.\n        # In this case, we assume no info argument is present:\n        return False\n    info_arg = _serializer_info_arg(mode, count_positional_required_params(sig))\n    if info_arg is None:\n        raise PydanticUserError(\n            f'Unrecognized field_serializer function signature for {serializer} with `mode={mode}`:{sig}',\n            code='field-serializer-signature',\n        )\n    else:\n        return info_arg\n\n\ndef inspect_model_serializer(serializer: Callable[..., Any], mode: Literal['plain', 'wrap']) -> bool:\n    \"\"\"Look at a model serializer function and determine whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        serializer: The serializer function to check.\n        mode: The serializer mode, either 'plain' or 'wrap'.\n\n    Returns:\n        `info_arg` - whether the function expects an info argument.\n    \"\"\"\n    if isinstance(serializer, (staticmethod, classmethod)) or not is_instance_method_from_sig(serializer):\n        raise PydanticUserError(\n            '`@model_serializer` must be applied to instance methods', code='model-serializer-instance-method'\n        )\n\n    sig = signature(serializer)\n    info_arg = _serializer_info_arg(mode, count_positional_required_params(sig))\n    if info_arg is None:\n        raise PydanticUserError(\n            f'Unrecognized model_serializer function signature for {serializer} with `mode={mode}`:{sig}',\n            code='model-serializer-signature',\n        )\n    else:\n        return info_arg\n\n\ndef _serializer_info_arg(mode: Literal['plain', 'wrap'], n_positional: int) -> bool | None:\n    if mode == 'plain':\n        if n_positional == 1:\n            # (input_value: Any, /) -> Any\n            return False\n        elif n_positional == 2:\n            # (model: Any, input_value: Any, /) -> Any\n            return True\n    else:\n        assert mode == 'wrap', f\"invalid mode: {mode!r}, expected 'plain' or 'wrap'\"\n        if n_positional == 2:\n            # (input_value: Any, serializer: SerializerFunctionWrapHandler, /) -> Any\n            return False\n        elif n_positional == 3:\n            # (input_value: Any, serializer: SerializerFunctionWrapHandler, info: SerializationInfo, /) -> Any\n            return True\n\n    return None\n\n\nAnyDecoratorCallable: TypeAlias = (\n    'Union[classmethod[Any, Any, Any], staticmethod[Any, Any], partialmethod[Any], Callable[..., Any]]'\n)\n\n\ndef is_instance_method_from_sig(function: AnyDecoratorCallable) -> bool:\n    \"\"\"Whether the function is an instance method.\n\n    It will consider a function as instance method if the first parameter of\n    function is `self`.\n\n    Args:\n        function: The function to check.\n\n    Returns:\n        `True` if the function is an instance method, `False` otherwise.\n    \"\"\"\n    sig = signature(unwrap_wrapped_function(function))\n    first = next(iter(sig.parameters.values()), None)\n    if first and first.name == 'self':\n        return True\n    return False\n\n\ndef ensure_classmethod_based_on_signature(function: AnyDecoratorCallable) -> Any:\n    \"\"\"Apply the `@classmethod` decorator on the function.\n\n    Args:\n        function: The function to apply the decorator on.\n\n    Return:\n        The `@classmethod` decorator applied function.\n    \"\"\"\n    if not isinstance(\n        unwrap_wrapped_function(function, unwrap_class_static_method=False), classmethod\n    ) and _is_classmethod_from_sig(function):\n        return classmethod(function)  # type: ignore[arg-type]\n    return function\n\n\ndef _is_classmethod_from_sig(function: AnyDecoratorCallable) -> bool:\n    sig = signature(unwrap_wrapped_function(function))\n    first = next(iter(sig.parameters.values()), None)\n    if first and first.name == 'cls':\n        return True\n    return False\n\n\ndef unwrap_wrapped_function(\n    func: Any,\n    *,\n    unwrap_partial: bool = True,\n    unwrap_class_static_method: bool = True,\n) -> Any:\n    \"\"\"Recursively unwraps a wrapped function until the underlying function is reached.\n    This handles property, functools.partial, functools.partialmethod, staticmethod and classmethod.\n\n    Args:\n        func: The function to unwrap.\n        unwrap_partial: If True (default), unwrap partial and partialmethod decorators, otherwise don't.\n            decorators.\n        unwrap_class_static_method: If True (default), also unwrap classmethod and staticmethod\n            decorators. If False, only unwrap partial and partialmethod decorators.\n\n    Returns:\n        The underlying function of the wrapped function.\n    \"\"\"\n    all: set[Any] = {property, cached_property}\n\n    if unwrap_partial:\n        all.update({partial, partialmethod})\n\n    if unwrap_class_static_method:\n        all.update({staticmethod, classmethod})\n\n    while isinstance(func, tuple(all)):\n        if unwrap_class_static_method and isinstance(func, (classmethod, staticmethod)):\n            func = func.__func__\n        elif isinstance(func, (partial, partialmethod)):\n            func = func.func\n        elif isinstance(func, property):\n            func = func.fget  # arbitrary choice, convenient for computed fields\n        else:\n            # Make coverage happy as it can only get here in the last possible case\n            assert isinstance(func, cached_property)\n            func = func.func  # type: ignore\n\n    return func\n\n\ndef get_function_return_type(\n    func: Any, explicit_return_type: Any, types_namespace: dict[str, Any] | None = None\n) -> Any:\n    \"\"\"Get the function return type.\n\n    It gets the return type from the type annotation if `explicit_return_type` is `None`.\n    Otherwise, it returns `explicit_return_type`.\n\n    Args:\n        func: The function to get its return type.\n        explicit_return_type: The explicit return type.\n        types_namespace: The types namespace, defaults to `None`.\n\n    Returns:\n        The function return type.\n    \"\"\"\n    if explicit_return_type is PydanticUndefined:\n        # try to get it from the type annotation\n        hints = get_function_type_hints(\n            unwrap_wrapped_function(func), include_keys={'return'}, types_namespace=types_namespace\n        )\n        return hints.get('return', PydanticUndefined)\n    else:\n        return explicit_return_type\n\n\ndef count_positional_required_params(sig: Signature) -> int:\n    \"\"\"Get the number of positional (required) arguments of a signature.\n\n    This function should only be used to inspect signatures of validation and serialization functions.\n    The first argument (the value being serialized or validated) is counted as a required argument\n    even if a default value exists.\n\n    Returns:\n        The number of positional arguments of a signature.\n    \"\"\"\n    parameters = list(sig.parameters.values())\n    return sum(\n        1\n        for param in parameters\n        if can_be_positional(param)\n        # First argument is the value being validated/serialized, and can have a default value\n        # (e.g. `float`, which has signature `(x=0, /)`). We assume other parameters (the info arg\n        # for instance) should be required, and thus without any default value.\n        and (param.default is Parameter.empty or param == parameters[0])\n    )\n\n\ndef can_be_positional(param: Parameter) -> bool:\n    return param.kind in (Parameter.POSITIONAL_ONLY, Parameter.POSITIONAL_OR_KEYWORD)\n\n\ndef ensure_property(f: Any) -> Any:\n    \"\"\"Ensure that a function is a `property` or `cached_property`, or is a valid descriptor.\n\n    Args:\n        f: The function to check.\n\n    Returns:\n        The function, or a `property` or `cached_property` instance wrapping the function.\n    \"\"\"\n    if ismethoddescriptor(f) or isdatadescriptor(f):\n        return f\n    else:\n        return property(f)\n", "pydantic/_internal/_validate_call.py": "from __future__ import annotations as _annotations\n\nimport inspect\nfrom functools import partial\nfrom typing import Any, Awaitable, Callable\n\nimport pydantic_core\n\nfrom ..config import ConfigDict\nfrom ..plugin._schema_validator import create_schema_validator\nfrom . import _generate_schema, _typing_extra\nfrom ._config import ConfigWrapper\n\n\nclass ValidateCallWrapper:\n    \"\"\"This is a wrapper around a function that validates the arguments passed to it, and optionally the return value.\"\"\"\n\n    __slots__ = (\n        '__pydantic_validator__',\n        '__name__',\n        '__qualname__',\n        '__annotations__',\n        '__dict__',  # required for __module__\n    )\n\n    def __init__(self, function: Callable[..., Any], config: ConfigDict | None, validate_return: bool):\n        if isinstance(function, partial):\n            func = function.func\n            schema_type = func\n            self.__name__ = f'partial({func.__name__})'\n            self.__qualname__ = f'partial({func.__qualname__})'\n            self.__module__ = func.__module__\n        else:\n            schema_type = function\n            self.__name__ = function.__name__\n            self.__qualname__ = function.__qualname__\n            self.__module__ = function.__module__\n\n        namespace = _typing_extra.add_module_globals(function, None)\n        config_wrapper = ConfigWrapper(config)\n        gen_schema = _generate_schema.GenerateSchema(config_wrapper, namespace)\n        schema = gen_schema.clean_schema(gen_schema.generate_schema(function))\n        core_config = config_wrapper.core_config(self)\n\n        self.__pydantic_validator__ = create_schema_validator(\n            schema,\n            schema_type,\n            self.__module__,\n            self.__qualname__,\n            'validate_call',\n            core_config,\n            config_wrapper.plugin_settings,\n        )\n\n        if validate_return:\n            signature = inspect.signature(function)\n            return_type = signature.return_annotation if signature.return_annotation is not signature.empty else Any\n            gen_schema = _generate_schema.GenerateSchema(config_wrapper, namespace)\n            schema = gen_schema.clean_schema(gen_schema.generate_schema(return_type))\n            validator = create_schema_validator(\n                schema,\n                schema_type,\n                self.__module__,\n                self.__qualname__,\n                'validate_call',\n                core_config,\n                config_wrapper.plugin_settings,\n            )\n            if inspect.iscoroutinefunction(function):\n\n                async def return_val_wrapper(aw: Awaitable[Any]) -> None:\n                    return validator.validate_python(await aw)\n\n                self.__return_pydantic_validator__ = return_val_wrapper\n            else:\n                self.__return_pydantic_validator__ = validator.validate_python\n        else:\n            self.__return_pydantic_validator__ = None\n\n    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n        res = self.__pydantic_validator__.validate_python(pydantic_core.ArgsKwargs(args, kwargs))\n        if self.__return_pydantic_validator__:\n            return self.__return_pydantic_validator__(res)\n        return res\n", "pydantic/deprecated/config.py": "from __future__ import annotations as _annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any\n\nfrom typing_extensions import Literal, deprecated\n\nfrom .._internal import _config\nfrom ..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n__all__ = 'BaseConfig', 'Extra'\n\n\nclass _ConfigMetaclass(type):\n    def __getattr__(self, item: str) -> Any:\n        try:\n            obj = _config.config_defaults[item]\n            warnings.warn(_config.DEPRECATION_MESSAGE, DeprecationWarning)\n            return obj\n        except KeyError as exc:\n            raise AttributeError(f\"type object '{self.__name__}' has no attribute {exc}\") from exc\n\n\n@deprecated('BaseConfig is deprecated. Use the `pydantic.ConfigDict` instead.', category=PydanticDeprecatedSince20)\nclass BaseConfig(metaclass=_ConfigMetaclass):\n    \"\"\"This class is only retained for backwards compatibility.\n\n    !!! Warning \"Deprecated\"\n        BaseConfig is deprecated. Use the [`pydantic.ConfigDict`][pydantic.ConfigDict] instead.\n    \"\"\"\n\n    def __getattr__(self, item: str) -> Any:\n        try:\n            obj = super().__getattribute__(item)\n            warnings.warn(_config.DEPRECATION_MESSAGE, DeprecationWarning)\n            return obj\n        except AttributeError as exc:\n            try:\n                return getattr(type(self), item)\n            except AttributeError:\n                # re-raising changes the displayed text to reflect that `self` is not a type\n                raise AttributeError(str(exc)) from exc\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        warnings.warn(_config.DEPRECATION_MESSAGE, DeprecationWarning)\n        return super().__init_subclass__(**kwargs)\n\n\nclass _ExtraMeta(type):\n    def __getattribute__(self, __name: str) -> Any:\n        # The @deprecated decorator accesses other attributes, so we only emit a warning for the expected ones\n        if __name in {'allow', 'ignore', 'forbid'}:\n            warnings.warn(\n                \"`pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`)\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        return super().__getattribute__(__name)\n\n\n@deprecated(\n    \"Extra is deprecated. Use literal values instead (e.g. `extra='allow'`)\", category=PydanticDeprecatedSince20\n)\nclass Extra(metaclass=_ExtraMeta):\n    allow: Literal['allow'] = 'allow'\n    ignore: Literal['ignore'] = 'ignore'\n    forbid: Literal['forbid'] = 'forbid'\n", "pydantic/deprecated/parse.py": "from __future__ import annotations\n\nimport json\nimport pickle\nimport warnings\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable\n\nfrom typing_extensions import deprecated\n\nfrom ..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n\nclass Protocol(str, Enum):\n    json = 'json'\n    pickle = 'pickle'\n\n\n@deprecated('`load_str_bytes` is deprecated.', category=None)\ndef load_str_bytes(\n    b: str | bytes,\n    *,\n    content_type: str | None = None,\n    encoding: str = 'utf8',\n    proto: Protocol | None = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    warnings.warn('`load_str_bytes` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    if proto is None and content_type:\n        if content_type.endswith(('json', 'javascript')):\n            pass\n        elif allow_pickle and content_type.endswith('pickle'):\n            proto = Protocol.pickle\n        else:\n            raise TypeError(f'Unknown content-type: {content_type}')\n\n    proto = proto or Protocol.json\n\n    if proto == Protocol.json:\n        if isinstance(b, bytes):\n            b = b.decode(encoding)\n        return json_loads(b)  # type: ignore\n    elif proto == Protocol.pickle:\n        if not allow_pickle:\n            raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n        bb = b if isinstance(b, bytes) else b.encode()  # type: ignore\n        return pickle.loads(bb)\n    else:\n        raise TypeError(f'Unknown protocol: {proto}')\n\n\n@deprecated('`load_file` is deprecated.', category=None)\ndef load_file(\n    path: str | Path,\n    *,\n    content_type: str | None = None,\n    encoding: str = 'utf8',\n    proto: Protocol | None = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    warnings.warn('`load_file` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    path = Path(path)\n    b = path.read_bytes()\n    if content_type is None:\n        if path.suffix in ('.js', '.json'):\n            proto = Protocol.json\n        elif path.suffix == '.pkl':\n            proto = Protocol.pickle\n\n    return load_str_bytes(\n        b, proto=proto, content_type=content_type, encoding=encoding, allow_pickle=allow_pickle, json_loads=json_loads\n    )\n", "pydantic/deprecated/tools.py": "from __future__ import annotations\n\nimport json\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Callable, Type, TypeVar, Union\n\nfrom typing_extensions import deprecated\n\nfrom ..json_schema import DEFAULT_REF_TEMPLATE, GenerateJsonSchema\nfrom ..type_adapter import TypeAdapter\nfrom ..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n__all__ = 'parse_obj_as', 'schema_of', 'schema_json_of'\n\nNameFactory = Union[str, Callable[[Type[Any]], str]]\n\n\nT = TypeVar('T')\n\n\n@deprecated(\n    '`parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead.',\n    category=None,\n)\ndef parse_obj_as(type_: type[T], obj: Any, type_name: NameFactory | None = None) -> T:\n    warnings.warn(\n        '`parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    if type_name is not None:  # pragma: no cover\n        warnings.warn(\n            'The type_name parameter is deprecated. parse_obj_as no longer creates temporary models',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    return TypeAdapter(type_).validate_python(obj)\n\n\n@deprecated(\n    '`schema_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n    category=None,\n)\ndef schema_of(\n    type_: Any,\n    *,\n    title: NameFactory | None = None,\n    by_alias: bool = True,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n) -> dict[str, Any]:\n    \"\"\"Generate a JSON schema (as dict) for the passed model or dynamically generated one.\"\"\"\n    warnings.warn(\n        '`schema_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    res = TypeAdapter(type_).json_schema(\n        by_alias=by_alias,\n        schema_generator=schema_generator,\n        ref_template=ref_template,\n    )\n    if title is not None:\n        if isinstance(title, str):\n            res['title'] = title\n        else:\n            warnings.warn(\n                'Passing a callable for the `title` parameter is deprecated and no longer supported',\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            res['title'] = title(type_)\n    return res\n\n\n@deprecated(\n    '`schema_json_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n    category=None,\n)\ndef schema_json_of(\n    type_: Any,\n    *,\n    title: NameFactory | None = None,\n    by_alias: bool = True,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n    **dumps_kwargs: Any,\n) -> str:\n    \"\"\"Generate a JSON schema (as JSON) for the passed model or dynamically generated one.\"\"\"\n    warnings.warn(\n        '`schema_json_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    return json.dumps(\n        schema_of(type_, title=title, by_alias=by_alias, ref_template=ref_template, schema_generator=schema_generator),\n        **dumps_kwargs,\n    )\n", "pydantic/deprecated/json.py": "import datetime\nimport warnings\nfrom collections import deque\nfrom decimal import Decimal\nfrom enum import Enum\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom pathlib import Path\nfrom re import Pattern\nfrom types import GeneratorType\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, Type, Union\nfrom uuid import UUID\n\nfrom typing_extensions import deprecated\n\nfrom ..color import Color\nfrom ..networks import NameEmail\nfrom ..types import SecretBytes, SecretStr\nfrom ..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n__all__ = 'pydantic_encoder', 'custom_pydantic_encoder', 'timedelta_isoformat'\n\n\ndef isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()\n\n\ndef decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    \"\"\"Encodes a Decimal as int of there's no exponent, otherwise float.\n\n    This is useful when we use ConstrainedDecimal to represent Numeric(x,0)\n    where a integer (but not int typed) is used. Encoding this as a float\n    results in failed round-tripping between encode and parse.\n    Our Id type is a prime example of this.\n\n    >>> decimal_encoder(Decimal(\"1.0\"))\n    1.0\n\n    >>> decimal_encoder(Decimal(\"1\"))\n    1\n    \"\"\"\n    exponent = dec_value.as_tuple().exponent\n    if isinstance(exponent, int) and exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)\n\n\nENCODERS_BY_TYPE: Dict[Type[Any], Callable[[Any], Any]] = {\n    bytes: lambda o: o.decode(),\n    Color: str,\n    datetime.date: isoformat,\n    datetime.datetime: isoformat,\n    datetime.time: isoformat,\n    datetime.timedelta: lambda td: td.total_seconds(),\n    Decimal: decimal_encoder,\n    Enum: lambda o: o.value,\n    frozenset: list,\n    deque: list,\n    GeneratorType: list,\n    IPv4Address: str,\n    IPv4Interface: str,\n    IPv4Network: str,\n    IPv6Address: str,\n    IPv6Interface: str,\n    IPv6Network: str,\n    NameEmail: str,\n    Path: str,\n    Pattern: lambda o: o.pattern,\n    SecretBytes: str,\n    SecretStr: str,\n    set: list,\n    UUID: str,\n}\n\n\n@deprecated(\n    '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n    category=None,\n)\ndef pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n\n    from ..main import BaseModel\n\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)  # type: ignore\n\n    # Check the class type and its superclasses for a matching encoder\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:  # We have exited the for loop without finding a suitable encoder\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")\n\n\n# TODO: Add a suggested migration path once there is a way to use custom encoders\n@deprecated(\n    '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n    category=None,\n)\ndef custom_pydantic_encoder(type_encoders: Dict[Any, Callable[[Type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    # Check the class type and its superclasses for a matching encoder\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n\n        return encoder(obj)\n    else:  # We have exited the for loop without finding a suitable encoder\n        return pydantic_encoder(obj)\n\n\n@deprecated('`timedelta_isoformat` is deprecated.', category=None)\ndef timedelta_isoformat(td: datetime.timedelta) -> str:\n    \"\"\"ISO 8601 encoding for Python timedelta object.\"\"\"\n    warnings.warn('`timedelta_isoformat` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'\n", "pydantic/deprecated/class_validators.py": "\"\"\"Old `@validator` and `@root_validator` function validators from V1.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom functools import partial, partialmethod\nfrom types import FunctionType\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar, Union, overload\nfrom warnings import warn\n\nfrom typing_extensions import Literal, Protocol, TypeAlias, deprecated\n\nfrom .._internal import _decorators, _decorators_v1\nfrom ..errors import PydanticUserError\nfrom ..warnings import PydanticDeprecatedSince20\n\n_ALLOW_REUSE_WARNING_MESSAGE = '`allow_reuse` is deprecated and will be ignored; it should no longer be necessary'\n\n\nif TYPE_CHECKING:\n\n    class _OnlyValueValidatorClsMethod(Protocol):\n        def __call__(self, __cls: Any, __value: Any) -> Any: ...\n\n    class _V1ValidatorWithValuesClsMethod(Protocol):\n        def __call__(self, __cls: Any, __value: Any, values: dict[str, Any]) -> Any: ...\n\n    class _V1ValidatorWithValuesKwOnlyClsMethod(Protocol):\n        def __call__(self, __cls: Any, __value: Any, *, values: dict[str, Any]) -> Any: ...\n\n    class _V1ValidatorWithKwargsClsMethod(Protocol):\n        def __call__(self, __cls: Any, **kwargs: Any) -> Any: ...\n\n    class _V1ValidatorWithValuesAndKwargsClsMethod(Protocol):\n        def __call__(self, __cls: Any, values: dict[str, Any], **kwargs: Any) -> Any: ...\n\n    class _V1RootValidatorClsMethod(Protocol):\n        def __call__(\n            self, __cls: Any, __values: _decorators_v1.RootValidatorValues\n        ) -> _decorators_v1.RootValidatorValues: ...\n\n    V1Validator = Union[\n        _OnlyValueValidatorClsMethod,\n        _V1ValidatorWithValuesClsMethod,\n        _V1ValidatorWithValuesKwOnlyClsMethod,\n        _V1ValidatorWithKwargsClsMethod,\n        _V1ValidatorWithValuesAndKwargsClsMethod,\n        _decorators_v1.V1ValidatorWithValues,\n        _decorators_v1.V1ValidatorWithValuesKwOnly,\n        _decorators_v1.V1ValidatorWithKwargs,\n        _decorators_v1.V1ValidatorWithValuesAndKwargs,\n    ]\n\n    V1RootValidator = Union[\n        _V1RootValidatorClsMethod,\n        _decorators_v1.V1RootValidatorFunction,\n    ]\n\n    _PartialClsOrStaticMethod: TypeAlias = Union[classmethod[Any, Any, Any], staticmethod[Any, Any], partialmethod[Any]]\n\n    # Allow both a V1 (assumed pre=False) or V2 (assumed mode='after') validator\n    # We lie to type checkers and say we return the same thing we get\n    # but in reality we return a proxy object that _mostly_ behaves like the wrapped thing\n    _V1ValidatorType = TypeVar('_V1ValidatorType', V1Validator, _PartialClsOrStaticMethod)\n    _V1RootValidatorFunctionType = TypeVar(\n        '_V1RootValidatorFunctionType',\n        _decorators_v1.V1RootValidatorFunction,\n        _V1RootValidatorClsMethod,\n        _PartialClsOrStaticMethod,\n    )\nelse:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n\n@deprecated(\n    'Pydantic V1 style `@validator` validators are deprecated.'\n    ' You should migrate to Pydantic V2 style `@field_validator` validators,'\n    ' see the migration guide for more details',\n    category=None,\n)\ndef validator(\n    __field: str,\n    *fields: str,\n    pre: bool = False,\n    each_item: bool = False,\n    always: bool = False,\n    check_fields: bool | None = None,\n    allow_reuse: bool = False,\n) -> Callable[[_V1ValidatorType], _V1ValidatorType]:\n    \"\"\"Decorate methods on the class indicating that they should be used to validate fields.\n\n    Args:\n        __field (str): The first field the validator should be called on; this is separate\n            from `fields` to ensure an error is raised if you don't pass at least one.\n        *fields (str): Additional field(s) the validator should be called on.\n        pre (bool, optional): Whether this validator should be called before the standard\n            validators (else after). Defaults to False.\n        each_item (bool, optional): For complex objects (sets, lists etc.) whether to validate\n            individual elements rather than the whole object. Defaults to False.\n        always (bool, optional): Whether this method and other validators should be called even if\n            the value is missing. Defaults to False.\n        check_fields (bool | None, optional): Whether to check that the fields actually exist on the model.\n            Defaults to None.\n        allow_reuse (bool, optional): Whether to track and raise an error if another validator refers to\n            the decorated function. Defaults to False.\n\n    Returns:\n        Callable: A decorator that can be used to decorate a\n            function to be used as a validator.\n    \"\"\"\n    warn(\n        'Pydantic V1 style `@validator` validators are deprecated.'\n        ' You should migrate to Pydantic V2 style `@field_validator` validators,'\n        ' see the migration guide for more details',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    if allow_reuse is True:  # pragma: no cover\n        warn(_ALLOW_REUSE_WARNING_MESSAGE, DeprecationWarning)\n    fields = tuple((__field, *fields))\n    if isinstance(fields[0], FunctionType):\n        raise PydanticUserError(\n            '`@validator` should be used with fields and keyword arguments, not bare. '\n            \"E.g. usage should be `@validator('<field_name>', ...)`\",\n            code='validator-no-fields',\n        )\n    elif not all(isinstance(field, str) for field in fields):\n        raise PydanticUserError(\n            '`@validator` fields should be passed as separate string args. '\n            \"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`\",\n            code='validator-invalid-fields',\n        )\n\n    mode: Literal['before', 'after'] = 'before' if pre is True else 'after'\n\n    def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n        if _decorators.is_instance_method_from_sig(f):\n            raise PydanticUserError(\n                '`@validator` cannot be applied to instance methods', code='validator-instance-method'\n            )\n        # auto apply the @classmethod decorator\n        f = _decorators.ensure_classmethod_based_on_signature(f)\n        wrap = _decorators_v1.make_generic_v1_field_validator\n        validator_wrapper_info = _decorators.ValidatorDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            each_item=each_item,\n            always=always,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, validator_wrapper_info, shim=wrap)\n\n    return dec  # type: ignore[return-value]\n\n\n@overload\ndef root_validator(\n    *,\n    # if you don't specify `pre` the default is `pre=False`\n    # which means you need to specify `skip_on_failure=True`\n    skip_on_failure: Literal[True],\n    allow_reuse: bool = ...,\n) -> Callable[\n    [_V1RootValidatorFunctionType],\n    _V1RootValidatorFunctionType,\n]: ...\n\n\n@overload\ndef root_validator(\n    *,\n    # if you specify `pre=True` then you don't need to specify\n    # `skip_on_failure`, in fact it is not allowed as an argument!\n    pre: Literal[True],\n    allow_reuse: bool = ...,\n) -> Callable[\n    [_V1RootValidatorFunctionType],\n    _V1RootValidatorFunctionType,\n]: ...\n\n\n@overload\ndef root_validator(\n    *,\n    # if you explicitly specify `pre=False` then you\n    # MUST specify `skip_on_failure=True`\n    pre: Literal[False],\n    skip_on_failure: Literal[True],\n    allow_reuse: bool = ...,\n) -> Callable[\n    [_V1RootValidatorFunctionType],\n    _V1RootValidatorFunctionType,\n]: ...\n\n\n@deprecated(\n    'Pydantic V1 style `@root_validator` validators are deprecated.'\n    ' You should migrate to Pydantic V2 style `@model_validator` validators,'\n    ' see the migration guide for more details',\n    category=None,\n)\ndef root_validator(\n    *__args,\n    pre: bool = False,\n    skip_on_failure: bool = False,\n    allow_reuse: bool = False,\n) -> Any:\n    \"\"\"Decorate methods on a model indicating that they should be used to validate (and perhaps\n    modify) data either before or after standard model parsing/validation is performed.\n\n    Args:\n        pre (bool, optional): Whether this validator should be called before the standard\n            validators (else after). Defaults to False.\n        skip_on_failure (bool, optional): Whether to stop validation and return as soon as a\n            failure is encountered. Defaults to False.\n        allow_reuse (bool, optional): Whether to track and raise an error if another validator\n            refers to the decorated function. Defaults to False.\n\n    Returns:\n        Any: A decorator that can be used to decorate a function to be used as a root_validator.\n    \"\"\"\n    warn(\n        'Pydantic V1 style `@root_validator` validators are deprecated.'\n        ' You should migrate to Pydantic V2 style `@model_validator` validators,'\n        ' see the migration guide for more details',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    if __args:\n        # Ensure a nice error is raised if someone attempts to use the bare decorator\n        return root_validator()(*__args)  # type: ignore\n\n    if allow_reuse is True:  # pragma: no cover\n        warn(_ALLOW_REUSE_WARNING_MESSAGE, DeprecationWarning)\n    mode: Literal['before', 'after'] = 'before' if pre is True else 'after'\n    if pre is False and skip_on_failure is not True:\n        raise PydanticUserError(\n            'If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.'\n            ' Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.',\n            code='root-validator-pre-skip',\n        )\n\n    wrap = partial(_decorators_v1.make_v1_generic_root_validator, pre=pre)\n\n    def dec(f: Callable[..., Any] | classmethod[Any, Any, Any] | staticmethod[Any, Any]) -> Any:\n        if _decorators.is_instance_method_from_sig(f):\n            raise TypeError('`@root_validator` cannot be applied to instance methods')\n        # auto apply the @classmethod decorator\n        res = _decorators.ensure_classmethod_based_on_signature(f)\n        dec_info = _decorators.RootValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(res, dec_info, shim=wrap)\n\n    return dec\n", "pydantic/deprecated/__init__.py": "", "pydantic/deprecated/decorator.py": "import warnings\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Mapping, Optional, Tuple, Type, TypeVar, Union, overload\n\nfrom typing_extensions import deprecated\n\nfrom .._internal import _config, _typing_extra\nfrom ..alias_generators import to_pascal\nfrom ..errors import PydanticUserError\nfrom ..functional_validators import field_validator\nfrom ..main import BaseModel, create_model\nfrom ..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n__all__ = ('validate_arguments',)\n\nif TYPE_CHECKING:\n    AnyCallable = Callable[..., Any]\n\n    AnyCallableT = TypeVar('AnyCallableT', bound=AnyCallable)\n    ConfigType = Union[None, Type[Any], Dict[str, Any]]\n\n\n@overload\ndef validate_arguments(\n    func: None = None, *, config: 'ConfigType' = None\n) -> Callable[['AnyCallableT'], 'AnyCallableT']: ...\n\n\n@overload\ndef validate_arguments(func: 'AnyCallableT') -> 'AnyCallableT': ...\n\n\n@deprecated(\n    'The `validate_arguments` method is deprecated; use `validate_call` instead.',\n    category=None,\n)\ndef validate_arguments(func: Optional['AnyCallableT'] = None, *, config: 'ConfigType' = None) -> Any:\n    \"\"\"Decorator to validate the arguments passed to a function.\"\"\"\n    warnings.warn(\n        'The `validate_arguments` method is deprecated; use `validate_call` instead.',\n        PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n\n    def validate(_func: 'AnyCallable') -> 'AnyCallable':\n        vd = ValidatedFunction(_func, config)\n\n        @wraps(_func)\n        def wrapper_function(*args: Any, **kwargs: Any) -> Any:\n            return vd.call(*args, **kwargs)\n\n        wrapper_function.vd = vd  # type: ignore\n        wrapper_function.validate = vd.init_model_instance  # type: ignore\n        wrapper_function.raw_function = vd.raw_function  # type: ignore\n        wrapper_function.model = vd.model  # type: ignore\n        return wrapper_function\n\n    if func:\n        return validate(func)\n    else:\n        return validate\n\n\nALT_V_ARGS = 'v__args'\nALT_V_KWARGS = 'v__kwargs'\nV_POSITIONAL_ONLY_NAME = 'v__positional_only'\nV_DUPLICATE_KWARGS = 'v__duplicate_kwargs'\n\n\nclass ValidatedFunction:\n    def __init__(self, function: 'AnyCallable', config: 'ConfigType'):\n        from inspect import Parameter, signature\n\n        parameters: Mapping[str, Parameter] = signature(function).parameters\n\n        if parameters.keys() & {ALT_V_ARGS, ALT_V_KWARGS, V_POSITIONAL_ONLY_NAME, V_DUPLICATE_KWARGS}:\n            raise PydanticUserError(\n                f'\"{ALT_V_ARGS}\", \"{ALT_V_KWARGS}\", \"{V_POSITIONAL_ONLY_NAME}\" and \"{V_DUPLICATE_KWARGS}\" '\n                f'are not permitted as argument names when using the \"{validate_arguments.__name__}\" decorator',\n                code=None,\n            )\n\n        self.raw_function = function\n        self.arg_mapping: Dict[int, str] = {}\n        self.positional_only_args: set[str] = set()\n        self.v_args_name = 'args'\n        self.v_kwargs_name = 'kwargs'\n\n        type_hints = _typing_extra.get_type_hints(function, include_extras=True)\n        takes_args = False\n        takes_kwargs = False\n        fields: Dict[str, Tuple[Any, Any]] = {}\n        for i, (name, p) in enumerate(parameters.items()):\n            if p.annotation is p.empty:\n                annotation = Any\n            else:\n                annotation = type_hints[name]\n\n            default = ... if p.default is p.empty else p.default\n            if p.kind == Parameter.POSITIONAL_ONLY:\n                self.arg_mapping[i] = name\n                fields[name] = annotation, default\n                fields[V_POSITIONAL_ONLY_NAME] = List[str], None\n                self.positional_only_args.add(name)\n            elif p.kind == Parameter.POSITIONAL_OR_KEYWORD:\n                self.arg_mapping[i] = name\n                fields[name] = annotation, default\n                fields[V_DUPLICATE_KWARGS] = List[str], None\n            elif p.kind == Parameter.KEYWORD_ONLY:\n                fields[name] = annotation, default\n            elif p.kind == Parameter.VAR_POSITIONAL:\n                self.v_args_name = name\n                fields[name] = Tuple[annotation, ...], None\n                takes_args = True\n            else:\n                assert p.kind == Parameter.VAR_KEYWORD, p.kind\n                self.v_kwargs_name = name\n                fields[name] = Dict[str, annotation], None\n                takes_kwargs = True\n\n        # these checks avoid a clash between \"args\" and a field with that name\n        if not takes_args and self.v_args_name in fields:\n            self.v_args_name = ALT_V_ARGS\n\n        # same with \"kwargs\"\n        if not takes_kwargs and self.v_kwargs_name in fields:\n            self.v_kwargs_name = ALT_V_KWARGS\n\n        if not takes_args:\n            # we add the field so validation below can raise the correct exception\n            fields[self.v_args_name] = List[Any], None\n\n        if not takes_kwargs:\n            # same with kwargs\n            fields[self.v_kwargs_name] = Dict[Any, Any], None\n\n        self.create_model(fields, takes_args, takes_kwargs, config)\n\n    def init_model_instance(self, *args: Any, **kwargs: Any) -> BaseModel:\n        values = self.build_values(args, kwargs)\n        return self.model(**values)\n\n    def call(self, *args: Any, **kwargs: Any) -> Any:\n        m = self.init_model_instance(*args, **kwargs)\n        return self.execute(m)\n\n    def build_values(self, args: Tuple[Any, ...], kwargs: Dict[str, Any]) -> Dict[str, Any]:\n        values: Dict[str, Any] = {}\n        if args:\n            arg_iter = enumerate(args)\n            while True:\n                try:\n                    i, a = next(arg_iter)\n                except StopIteration:\n                    break\n                arg_name = self.arg_mapping.get(i)\n                if arg_name is not None:\n                    values[arg_name] = a\n                else:\n                    values[self.v_args_name] = [a] + [a for _, a in arg_iter]\n                    break\n\n        var_kwargs: Dict[str, Any] = {}\n        wrong_positional_args = []\n        duplicate_kwargs = []\n        fields_alias = [\n            field.alias\n            for name, field in self.model.model_fields.items()\n            if name not in (self.v_args_name, self.v_kwargs_name)\n        ]\n        non_var_fields = set(self.model.model_fields) - {self.v_args_name, self.v_kwargs_name}\n        for k, v in kwargs.items():\n            if k in non_var_fields or k in fields_alias:\n                if k in self.positional_only_args:\n                    wrong_positional_args.append(k)\n                if k in values:\n                    duplicate_kwargs.append(k)\n                values[k] = v\n            else:\n                var_kwargs[k] = v\n\n        if var_kwargs:\n            values[self.v_kwargs_name] = var_kwargs\n        if wrong_positional_args:\n            values[V_POSITIONAL_ONLY_NAME] = wrong_positional_args\n        if duplicate_kwargs:\n            values[V_DUPLICATE_KWARGS] = duplicate_kwargs\n        return values\n\n    def execute(self, m: BaseModel) -> Any:\n        d = {k: v for k, v in m.__dict__.items() if k in m.__pydantic_fields_set__ or m.model_fields[k].default_factory}\n        var_kwargs = d.pop(self.v_kwargs_name, {})\n\n        if self.v_args_name in d:\n            args_: List[Any] = []\n            in_kwargs = False\n            kwargs = {}\n            for name, value in d.items():\n                if in_kwargs:\n                    kwargs[name] = value\n                elif name == self.v_args_name:\n                    args_ += value\n                    in_kwargs = True\n                else:\n                    args_.append(value)\n            return self.raw_function(*args_, **kwargs, **var_kwargs)\n        elif self.positional_only_args:\n            args_ = []\n            kwargs = {}\n            for name, value in d.items():\n                if name in self.positional_only_args:\n                    args_.append(value)\n                else:\n                    kwargs[name] = value\n            return self.raw_function(*args_, **kwargs, **var_kwargs)\n        else:\n            return self.raw_function(**d, **var_kwargs)\n\n    def create_model(self, fields: Dict[str, Any], takes_args: bool, takes_kwargs: bool, config: 'ConfigType') -> None:\n        pos_args = len(self.arg_mapping)\n\n        config_wrapper = _config.ConfigWrapper(config)\n\n        if config_wrapper.alias_generator:\n            raise PydanticUserError(\n                'Setting the \"alias_generator\" property on custom Config for '\n                '@validate_arguments is not yet supported, please remove.',\n                code=None,\n            )\n        if config_wrapper.extra is None:\n            config_wrapper.config_dict['extra'] = 'forbid'\n\n        class DecoratorBaseModel(BaseModel):\n            @field_validator(self.v_args_name, check_fields=False)\n            @classmethod\n            def check_args(cls, v: Optional[List[Any]]) -> Optional[List[Any]]:\n                if takes_args or v is None:\n                    return v\n\n                raise TypeError(f'{pos_args} positional arguments expected but {pos_args + len(v)} given')\n\n            @field_validator(self.v_kwargs_name, check_fields=False)\n            @classmethod\n            def check_kwargs(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\n                if takes_kwargs or v is None:\n                    return v\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v.keys()))\n                raise TypeError(f'unexpected keyword argument{plural}: {keys}')\n\n            @field_validator(V_POSITIONAL_ONLY_NAME, check_fields=False)\n            @classmethod\n            def check_positional_only(cls, v: Optional[List[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'positional-only argument{plural} passed as keyword argument{plural}: {keys}')\n\n            @field_validator(V_DUPLICATE_KWARGS, check_fields=False)\n            @classmethod\n            def check_duplicate_kwargs(cls, v: Optional[List[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'multiple values for argument{plural}: {keys}')\n\n            model_config = config_wrapper.config_dict\n\n        self.model = create_model(to_pascal(self.raw_function.__name__), __base__=DecoratorBaseModel, **fields)\n", "pydantic/deprecated/copy_internals.py": "from __future__ import annotations as _annotations\n\nimport typing\nfrom copy import deepcopy\nfrom enum import Enum\nfrom typing import Any, Tuple\n\nimport typing_extensions\n\nfrom .._internal import (\n    _model_construction,\n    _typing_extra,\n    _utils,\n)\n\nif typing.TYPE_CHECKING:\n    from .. import BaseModel\n    from .._internal._utils import AbstractSetIntStr, MappingIntStrAny\n\n    AnyClassMethod = classmethod[Any, Any, Any]\n    TupleGenerator = typing.Generator[Tuple[str, Any], None, None]\n    Model = typing.TypeVar('Model', bound='BaseModel')\n    # should be `set[int] | set[str] | dict[int, IncEx] | dict[str, IncEx] | None`, but mypy can't cope\n    IncEx: typing_extensions.TypeAlias = 'set[int] | set[str] | dict[int, Any] | dict[str, Any] | None'\n\n_object_setattr = _model_construction.object_setattr\n\n\ndef _iter(\n    self: BaseModel,\n    to_dict: bool = False,\n    by_alias: bool = False,\n    include: AbstractSetIntStr | MappingIntStrAny | None = None,\n    exclude: AbstractSetIntStr | MappingIntStrAny | None = None,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n) -> TupleGenerator:\n    # Merge field set excludes with explicit exclude parameter with explicit overriding field set options.\n    # The extra \"is not None\" guards are not logically necessary but optimizes performance for the simple case.\n    if exclude is not None:\n        exclude = _utils.ValueItems.merge(\n            {k: v.exclude for k, v in self.model_fields.items() if v.exclude is not None}, exclude\n        )\n\n    if include is not None:\n        include = _utils.ValueItems.merge({k: True for k in self.model_fields}, include, intersect=True)\n\n    allowed_keys = _calculate_keys(self, include=include, exclude=exclude, exclude_unset=exclude_unset)  # type: ignore\n    if allowed_keys is None and not (to_dict or by_alias or exclude_unset or exclude_defaults or exclude_none):\n        # huge boost for plain _iter()\n        yield from self.__dict__.items()\n        if self.__pydantic_extra__:\n            yield from self.__pydantic_extra__.items()\n        return\n\n    value_exclude = _utils.ValueItems(self, exclude) if exclude is not None else None\n    value_include = _utils.ValueItems(self, include) if include is not None else None\n\n    if self.__pydantic_extra__ is None:\n        items = self.__dict__.items()\n    else:\n        items = list(self.__dict__.items()) + list(self.__pydantic_extra__.items())\n\n    for field_key, v in items:\n        if (allowed_keys is not None and field_key not in allowed_keys) or (exclude_none and v is None):\n            continue\n\n        if exclude_defaults:\n            try:\n                field = self.model_fields[field_key]\n            except KeyError:\n                pass\n            else:\n                if not field.is_required() and field.default == v:\n                    continue\n\n        if by_alias and field_key in self.model_fields:\n            dict_key = self.model_fields[field_key].alias or field_key\n        else:\n            dict_key = field_key\n\n        if to_dict or value_include or value_exclude:\n            v = _get_value(\n                type(self),\n                v,\n                to_dict=to_dict,\n                by_alias=by_alias,\n                include=value_include and value_include.for_element(field_key),\n                exclude=value_exclude and value_exclude.for_element(field_key),\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        yield dict_key, v\n\n\ndef _copy_and_set_values(\n    self: Model,\n    values: dict[str, Any],\n    fields_set: set[str],\n    extra: dict[str, Any] | None = None,\n    private: dict[str, Any] | None = None,\n    *,\n    deep: bool,  # UP006\n) -> Model:\n    if deep:\n        # chances of having empty dict here are quite low for using smart_deepcopy\n        values = deepcopy(values)\n        extra = deepcopy(extra)\n        private = deepcopy(private)\n\n    cls = self.__class__\n    m = cls.__new__(cls)\n    _object_setattr(m, '__dict__', values)\n    _object_setattr(m, '__pydantic_extra__', extra)\n    _object_setattr(m, '__pydantic_fields_set__', fields_set)\n    _object_setattr(m, '__pydantic_private__', private)\n\n    return m\n\n\n@typing.no_type_check\ndef _get_value(\n    cls: type[BaseModel],\n    v: Any,\n    to_dict: bool,\n    by_alias: bool,\n    include: AbstractSetIntStr | MappingIntStrAny | None,\n    exclude: AbstractSetIntStr | MappingIntStrAny | None,\n    exclude_unset: bool,\n    exclude_defaults: bool,\n    exclude_none: bool,\n) -> Any:\n    from .. import BaseModel\n\n    if isinstance(v, BaseModel):\n        if to_dict:\n            return v.model_dump(\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                include=include,  # type: ignore\n                exclude=exclude,  # type: ignore\n                exclude_none=exclude_none,\n            )\n        else:\n            return v.copy(include=include, exclude=exclude)\n\n    value_exclude = _utils.ValueItems(v, exclude) if exclude else None\n    value_include = _utils.ValueItems(v, include) if include else None\n\n    if isinstance(v, dict):\n        return {\n            k_: _get_value(\n                cls,\n                v_,\n                to_dict=to_dict,\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                include=value_include and value_include.for_element(k_),\n                exclude=value_exclude and value_exclude.for_element(k_),\n                exclude_none=exclude_none,\n            )\n            for k_, v_ in v.items()\n            if (not value_exclude or not value_exclude.is_excluded(k_))\n            and (not value_include or value_include.is_included(k_))\n        }\n\n    elif _utils.sequence_like(v):\n        seq_args = (\n            _get_value(\n                cls,\n                v_,\n                to_dict=to_dict,\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                include=value_include and value_include.for_element(i),\n                exclude=value_exclude and value_exclude.for_element(i),\n                exclude_none=exclude_none,\n            )\n            for i, v_ in enumerate(v)\n            if (not value_exclude or not value_exclude.is_excluded(i))\n            and (not value_include or value_include.is_included(i))\n        )\n\n        return v.__class__(*seq_args) if _typing_extra.is_namedtuple(v.__class__) else v.__class__(seq_args)\n\n    elif isinstance(v, Enum) and getattr(cls.model_config, 'use_enum_values', False):\n        return v.value\n\n    else:\n        return v\n\n\ndef _calculate_keys(\n    self: BaseModel,\n    include: MappingIntStrAny | None,\n    exclude: MappingIntStrAny | None,\n    exclude_unset: bool,\n    update: typing.Dict[str, Any] | None = None,  # noqa UP006\n) -> typing.AbstractSet[str] | None:\n    if include is None and exclude is None and exclude_unset is False:\n        return None\n\n    keys: typing.AbstractSet[str]\n    if exclude_unset:\n        keys = self.__pydantic_fields_set__.copy()\n    else:\n        keys = set(self.__dict__.keys())\n        keys = keys | (self.__pydantic_extra__ or {}).keys()\n\n    if include is not None:\n        keys &= include.keys()\n\n    if update:\n        keys -= update.keys()\n\n    if exclude:\n        keys -= {k for k, v in exclude.items() if _utils.ValueItems.is_true(v)}\n\n    return keys\n", "pydantic/plugin/_loader.py": "from __future__ import annotations\n\nimport importlib.metadata as importlib_metadata\nimport os\nimport warnings\nfrom typing import TYPE_CHECKING, Final, Iterable\n\nif TYPE_CHECKING:\n    from . import PydanticPluginProtocol\n\n\nPYDANTIC_ENTRY_POINT_GROUP: Final[str] = 'pydantic'\n\n# cache of plugins\n_plugins: dict[str, PydanticPluginProtocol] | None = None\n# return no plugins while loading plugins to avoid recursion and errors while import plugins\n# this means that if plugins use pydantic\n_loading_plugins: bool = False\n\n\ndef get_plugins() -> Iterable[PydanticPluginProtocol]:\n    \"\"\"Load plugins for Pydantic.\n\n    Inspired by: https://github.com/pytest-dev/pluggy/blob/1.3.0/src/pluggy/_manager.py#L376-L402\n    \"\"\"\n    disabled_plugins = os.getenv('PYDANTIC_DISABLE_PLUGINS')\n    global _plugins, _loading_plugins\n    if _loading_plugins:\n        # this happens when plugins themselves use pydantic, we return no plugins\n        return ()\n    elif disabled_plugins in ('__all__', '1', 'true'):\n        return ()\n    elif _plugins is None:\n        _plugins = {}\n        # set _loading_plugins so any plugins that use pydantic don't themselves use plugins\n        _loading_plugins = True\n        try:\n            for dist in importlib_metadata.distributions():\n                for entry_point in dist.entry_points:\n                    if entry_point.group != PYDANTIC_ENTRY_POINT_GROUP:\n                        continue\n                    if entry_point.value in _plugins:\n                        continue\n                    if disabled_plugins is not None and entry_point.name in disabled_plugins.split(','):\n                        continue\n                    try:\n                        _plugins[entry_point.value] = entry_point.load()\n                    except (ImportError, AttributeError) as e:\n                        warnings.warn(\n                            f'{e.__class__.__name__} while loading the `{entry_point.name}` Pydantic plugin, '\n                            f'this plugin will not be installed.\\n\\n{e!r}'\n                        )\n        finally:\n            _loading_plugins = False\n\n    return _plugins.values()\n", "pydantic/plugin/_schema_validator.py": "\"\"\"Pluggable schema validator for pydantic.\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nfrom typing import TYPE_CHECKING, Any, Callable, Iterable, TypeVar\n\nfrom pydantic_core import CoreConfig, CoreSchema, SchemaValidator, ValidationError\nfrom typing_extensions import Literal, ParamSpec\n\nif TYPE_CHECKING:\n    from . import BaseValidateHandlerProtocol, PydanticPluginProtocol, SchemaKind, SchemaTypePath\n\n\nP = ParamSpec('P')\nR = TypeVar('R')\nEvent = Literal['on_validate_python', 'on_validate_json', 'on_validate_strings']\nevents: list[Event] = list(Event.__args__)  # type: ignore\n\n\ndef create_schema_validator(\n    schema: CoreSchema,\n    schema_type: Any,\n    schema_type_module: str,\n    schema_type_name: str,\n    schema_kind: SchemaKind,\n    config: CoreConfig | None = None,\n    plugin_settings: dict[str, Any] | None = None,\n) -> SchemaValidator:\n    \"\"\"Create a `SchemaValidator` or `PluggableSchemaValidator` if plugins are installed.\n\n    Returns:\n        If plugins are installed then return `PluggableSchemaValidator`, otherwise return `SchemaValidator`.\n    \"\"\"\n    from . import SchemaTypePath\n    from ._loader import get_plugins\n\n    plugins = get_plugins()\n    if plugins:\n        return PluggableSchemaValidator(\n            schema,\n            schema_type,\n            SchemaTypePath(schema_type_module, schema_type_name),\n            schema_kind,\n            config,\n            plugins,\n            plugin_settings or {},\n        )  # type: ignore\n    else:\n        return SchemaValidator(schema, config)\n\n\nclass PluggableSchemaValidator:\n    \"\"\"Pluggable schema validator.\"\"\"\n\n    __slots__ = '_schema_validator', 'validate_json', 'validate_python', 'validate_strings'\n\n    def __init__(\n        self,\n        schema: CoreSchema,\n        schema_type: Any,\n        schema_type_path: SchemaTypePath,\n        schema_kind: SchemaKind,\n        config: CoreConfig | None,\n        plugins: Iterable[PydanticPluginProtocol],\n        plugin_settings: dict[str, Any],\n    ) -> None:\n        self._schema_validator = SchemaValidator(schema, config)\n\n        python_event_handlers: list[BaseValidateHandlerProtocol] = []\n        json_event_handlers: list[BaseValidateHandlerProtocol] = []\n        strings_event_handlers: list[BaseValidateHandlerProtocol] = []\n        for plugin in plugins:\n            try:\n                p, j, s = plugin.new_schema_validator(\n                    schema, schema_type, schema_type_path, schema_kind, config, plugin_settings\n                )\n            except TypeError as e:  # pragma: no cover\n                raise TypeError(f'Error using plugin `{plugin.__module__}:{plugin.__class__.__name__}`: {e}') from e\n            if p is not None:\n                python_event_handlers.append(p)\n            if j is not None:\n                json_event_handlers.append(j)\n            if s is not None:\n                strings_event_handlers.append(s)\n\n        self.validate_python = build_wrapper(self._schema_validator.validate_python, python_event_handlers)\n        self.validate_json = build_wrapper(self._schema_validator.validate_json, json_event_handlers)\n        self.validate_strings = build_wrapper(self._schema_validator.validate_strings, strings_event_handlers)\n\n    def __getattr__(self, name: str) -> Any:\n        return getattr(self._schema_validator, name)\n\n\ndef build_wrapper(func: Callable[P, R], event_handlers: list[BaseValidateHandlerProtocol]) -> Callable[P, R]:\n    if not event_handlers:\n        return func\n    else:\n        on_enters = tuple(h.on_enter for h in event_handlers if filter_handlers(h, 'on_enter'))\n        on_successes = tuple(h.on_success for h in event_handlers if filter_handlers(h, 'on_success'))\n        on_errors = tuple(h.on_error for h in event_handlers if filter_handlers(h, 'on_error'))\n        on_exceptions = tuple(h.on_exception for h in event_handlers if filter_handlers(h, 'on_exception'))\n\n        @functools.wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n            for on_enter_handler in on_enters:\n                on_enter_handler(*args, **kwargs)\n\n            try:\n                result = func(*args, **kwargs)\n            except ValidationError as error:\n                for on_error_handler in on_errors:\n                    on_error_handler(error)\n                raise\n            except Exception as exception:\n                for on_exception_handler in on_exceptions:\n                    on_exception_handler(exception)\n                raise\n            else:\n                for on_success_handler in on_successes:\n                    on_success_handler(result)\n                return result\n\n        return wrapper\n\n\ndef filter_handlers(handler_cls: BaseValidateHandlerProtocol, method_name: str) -> bool:\n    \"\"\"Filter out handler methods which are not implemented by the plugin directly - e.g. are missing\n    or are inherited from the protocol.\n    \"\"\"\n    handler = getattr(handler_cls, method_name, None)\n    if handler is None:\n        return False\n    elif handler.__module__ == 'pydantic.plugin':\n        # this is the original handler, from the protocol due to runtime inheritance\n        # we don't want to call it\n        return False\n    else:\n        return True\n", "pydantic/plugin/__init__.py": "\"\"\"Usage docs: https://docs.pydantic.dev/2.8/concepts/plugins#build-a-plugin\n\nPlugin interface for Pydantic plugins, and related types.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Callable, NamedTuple\n\nfrom pydantic_core import CoreConfig, CoreSchema, ValidationError\nfrom typing_extensions import Literal, Protocol, TypeAlias\n\n__all__ = (\n    'PydanticPluginProtocol',\n    'BaseValidateHandlerProtocol',\n    'ValidatePythonHandlerProtocol',\n    'ValidateJsonHandlerProtocol',\n    'ValidateStringsHandlerProtocol',\n    'NewSchemaReturns',\n    'SchemaTypePath',\n    'SchemaKind',\n)\n\nNewSchemaReturns: TypeAlias = 'tuple[ValidatePythonHandlerProtocol | None, ValidateJsonHandlerProtocol | None, ValidateStringsHandlerProtocol | None]'\n\n\nclass SchemaTypePath(NamedTuple):\n    \"\"\"Path defining where `schema_type` was defined, or where `TypeAdapter` was called.\"\"\"\n\n    module: str\n    name: str\n\n\nSchemaKind: TypeAlias = Literal['BaseModel', 'TypeAdapter', 'dataclass', 'create_model', 'validate_call']\n\n\nclass PydanticPluginProtocol(Protocol):\n    \"\"\"Protocol defining the interface for Pydantic plugins.\"\"\"\n\n    def new_schema_validator(\n        self,\n        schema: CoreSchema,\n        schema_type: Any,\n        schema_type_path: SchemaTypePath,\n        schema_kind: SchemaKind,\n        config: CoreConfig | None,\n        plugin_settings: dict[str, object],\n    ) -> tuple[\n        ValidatePythonHandlerProtocol | None, ValidateJsonHandlerProtocol | None, ValidateStringsHandlerProtocol | None\n    ]:\n        \"\"\"This method is called for each plugin every time a new [`SchemaValidator`][pydantic_core.SchemaValidator]\n        is created.\n\n        It should return an event handler for each of the three validation methods, or `None` if the plugin does not\n        implement that method.\n\n        Args:\n            schema: The schema to validate against.\n            schema_type: The original type which the schema was created from, e.g. the model class.\n            schema_type_path: Path defining where `schema_type` was defined, or where `TypeAdapter` was called.\n            schema_kind: The kind of schema to validate against.\n            config: The config to use for validation.\n            plugin_settings: Any plugin settings.\n\n        Returns:\n            A tuple of optional event handlers for each of the three validation methods -\n                `validate_python`, `validate_json`, `validate_strings`.\n        \"\"\"\n        raise NotImplementedError('Pydantic plugins should implement `new_schema_validator`.')\n\n\nclass BaseValidateHandlerProtocol(Protocol):\n    \"\"\"Base class for plugin callbacks protocols.\n\n    You shouldn't implement this protocol directly, instead use one of the subclasses with adds the correctly\n    typed `on_error` method.\n    \"\"\"\n\n    on_enter: Callable[..., None]\n    \"\"\"`on_enter` is changed to be more specific on all subclasses\"\"\"\n\n    def on_success(self, result: Any) -> None:\n        \"\"\"Callback to be notified of successful validation.\n\n        Args:\n            result: The result of the validation.\n        \"\"\"\n        return\n\n    def on_error(self, error: ValidationError) -> None:\n        \"\"\"Callback to be notified of validation errors.\n\n        Args:\n            error: The validation error.\n        \"\"\"\n        return\n\n    def on_exception(self, exception: Exception) -> None:\n        \"\"\"Callback to be notified of validation exceptions.\n\n        Args:\n            exception: The exception raised during validation.\n        \"\"\"\n        return\n\n\nclass ValidatePythonHandlerProtocol(BaseValidateHandlerProtocol, Protocol):\n    \"\"\"Event handler for `SchemaValidator.validate_python`.\"\"\"\n\n    def on_enter(\n        self,\n        input: Any,\n        *,\n        strict: bool | None = None,\n        from_attributes: bool | None = None,\n        context: dict[str, Any] | None = None,\n        self_instance: Any | None = None,\n    ) -> None:\n        \"\"\"Callback to be notified of validation start, and create an instance of the event handler.\n\n        Args:\n            input: The input to be validated.\n            strict: Whether to validate the object in strict mode.\n            from_attributes: Whether to validate objects as inputs by extracting attributes.\n            context: The context to use for validation, this is passed to functional validators.\n            self_instance: An instance of a model to set attributes on from validation, this is used when running\n                validation from the `__init__` method of a model.\n        \"\"\"\n        pass\n\n\nclass ValidateJsonHandlerProtocol(BaseValidateHandlerProtocol, Protocol):\n    \"\"\"Event handler for `SchemaValidator.validate_json`.\"\"\"\n\n    def on_enter(\n        self,\n        input: str | bytes | bytearray,\n        *,\n        strict: bool | None = None,\n        context: dict[str, Any] | None = None,\n        self_instance: Any | None = None,\n    ) -> None:\n        \"\"\"Callback to be notified of validation start, and create an instance of the event handler.\n\n        Args:\n            input: The JSON data to be validated.\n            strict: Whether to validate the object in strict mode.\n            context: The context to use for validation, this is passed to functional validators.\n            self_instance: An instance of a model to set attributes on from validation, this is used when running\n                validation from the `__init__` method of a model.\n        \"\"\"\n        pass\n\n\nStringInput: TypeAlias = 'dict[str, StringInput]'\n\n\nclass ValidateStringsHandlerProtocol(BaseValidateHandlerProtocol, Protocol):\n    \"\"\"Event handler for `SchemaValidator.validate_strings`.\"\"\"\n\n    def on_enter(\n        self, input: StringInput, *, strict: bool | None = None, context: dict[str, Any] | None = None\n    ) -> None:\n        \"\"\"Callback to be notified of validation start, and create an instance of the event handler.\n\n        Args:\n            input: The string data to be validated.\n            strict: Whether to validate the object in strict mode.\n            context: The context to use for validation, this is passed to functional validators.\n        \"\"\"\n        pass\n", ".github/actions/people/people.py": "\"\"\"Use the github API to get lists of people who have contributed in various ways to Pydantic.\n\nThis logic is inspired by that of @tiangolo's\n[FastAPI people script](https://github.com/tiangolo/fastapi/blob/master/.github/actions/people/app/main.py).\n\"\"\"\nimport requests\nimport logging\nimport subprocess\nimport sys\nfrom collections import Counter\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\nfrom typing import Any, Container, Dict, List, Set, Union\n\nimport yaml\nfrom github import Github\nfrom pydantic import BaseModel, SecretStr\nfrom pydantic_settings import BaseSettings\n\ngithub_graphql_url = \"https://api.github.com/graphql\"\n\ndiscussions_query = \"\"\"\nquery Q($after: String) {\n  repository(name: \"pydantic\", owner: \"samuelcolvin\") {\n    discussions(first: 100, after: $after) {\n      edges {\n        cursor\n        node {\n          number\n          author {\n            login\n            avatarUrl\n            url\n          }\n          title\n          createdAt\n          comments(first: 100) {\n            nodes {\n              createdAt\n              author {\n                login\n                avatarUrl\n                url\n              }\n              isAnswer\n              replies(first: 10) {\n                nodes {\n                  createdAt\n                  author {\n                    login\n                    avatarUrl\n                    url\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\"\"\"\n\nissues_query = \"\"\"\nquery Q($after: String) {\n  repository(name: \"pydantic\", owner: \"samuelcolvin\") {\n    issues(first: 100, after: $after) {\n      edges {\n        cursor\n        node {\n          number\n          author {\n            login\n            avatarUrl\n            url\n          }\n          title\n          createdAt\n          state\n          comments(first: 100) {\n            nodes {\n              createdAt\n              author {\n                login\n                avatarUrl\n                url\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\"\"\"\n\nprs_query = \"\"\"\nquery Q($after: String) {\n  repository(name: \"pydantic\", owner: \"samuelcolvin\") {\n    pullRequests(first: 100, after: $after) {\n      edges {\n        cursor\n        node {\n          number\n          labels(first: 100) {\n            nodes {\n              name\n            }\n          }\n          author {\n            login\n            avatarUrl\n            url\n          }\n          title\n          createdAt\n          state\n          comments(first: 100) {\n            nodes {\n              createdAt\n              author {\n                login\n                avatarUrl\n                url\n              }\n            }\n          }\n          reviews(first:100) {\n            nodes {\n              author {\n                login\n                avatarUrl\n                url\n              }\n              state\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\"\"\"\n\n\nclass Author(BaseModel):\n    login: str\n    avatarUrl: str\n    url: str\n\n\n# Issues and Discussions\n\n\nclass CommentsNode(BaseModel):\n    createdAt: datetime\n    author: Union[Author, None] = None\n\n\nclass Replies(BaseModel):\n    nodes: List[CommentsNode]\n\n\nclass DiscussionsCommentsNode(CommentsNode):\n    replies: Replies\n\n\nclass Comments(BaseModel):\n    nodes: List[CommentsNode]\n\n\nclass DiscussionsComments(BaseModel):\n    nodes: List[DiscussionsCommentsNode]\n\n\nclass IssuesNode(BaseModel):\n    number: int\n    author: Union[Author, None] = None\n    title: str\n    createdAt: datetime\n    state: str\n    comments: Comments\n\n\nclass DiscussionsNode(BaseModel):\n    number: int\n    author: Union[Author, None] = None\n    title: str\n    createdAt: datetime\n    comments: DiscussionsComments\n\n\nclass IssuesEdge(BaseModel):\n    cursor: str\n    node: IssuesNode\n\n\nclass DiscussionsEdge(BaseModel):\n    cursor: str\n    node: DiscussionsNode\n\n\nclass Issues(BaseModel):\n    edges: List[IssuesEdge]\n\n\nclass Discussions(BaseModel):\n    edges: List[DiscussionsEdge]\n\n\nclass IssuesRepository(BaseModel):\n    issues: Issues\n\n\nclass DiscussionsRepository(BaseModel):\n    discussions: Discussions\n\n\nclass IssuesResponseData(BaseModel):\n    repository: IssuesRepository\n\n\nclass DiscussionsResponseData(BaseModel):\n    repository: DiscussionsRepository\n\n\nclass IssuesResponse(BaseModel):\n    data: IssuesResponseData\n\n\nclass DiscussionsResponse(BaseModel):\n    data: DiscussionsResponseData\n\n\n# PRs\n\n\nclass LabelNode(BaseModel):\n    name: str\n\n\nclass Labels(BaseModel):\n    nodes: List[LabelNode]\n\n\nclass ReviewNode(BaseModel):\n    author: Union[Author, None] = None\n    state: str\n\n\nclass Reviews(BaseModel):\n    nodes: List[ReviewNode]\n\n\nclass PullRequestNode(BaseModel):\n    number: int\n    labels: Labels\n    author: Union[Author, None] = None\n    title: str\n    createdAt: datetime\n    state: str\n    comments: Comments\n    reviews: Reviews\n\n\nclass PullRequestEdge(BaseModel):\n    cursor: str\n    node: PullRequestNode\n\n\nclass PullRequests(BaseModel):\n    edges: List[PullRequestEdge]\n\n\nclass PRsRepository(BaseModel):\n    pullRequests: PullRequests\n\n\nclass PRsResponseData(BaseModel):\n    repository: PRsRepository\n\n\nclass PRsResponse(BaseModel):\n    data: PRsResponseData\n\n\nclass Settings(BaseSettings):\n    input_token: SecretStr\n    github_repository: str = \"pydantic/pydantic\"\n    request_timeout: int = 30\n\n\ndef get_graphql_response(\n    *,\n    settings: Settings,\n    query: str,\n    after: Union[str, None] = None,\n) -> Dict[str, Any]:\n    headers = {\"Authorization\": f\"token {settings.input_token.get_secret_value()}\"}\n    variables = {\"after\": after}\n    response = requests.post(\n        github_graphql_url,\n        headers=headers,\n        timeout=settings.request_timeout,\n        json={\"query\": query, \"variables\": variables, \"operationName\": \"Q\"},\n    )\n    if response.status_code != 200:\n        logging.error(\n            f\"Response was not 200, after: {after}\"\n        )\n        logging.error(response.text)\n        raise RuntimeError(response.text)\n    data = response.json()\n    if \"errors\" in data:\n        logging.error(f\"Errors in response, after: {after}\")\n        logging.error(data[\"errors\"])\n        logging.error(response.text)\n        raise RuntimeError(response.text)\n    return data\n\n\ndef get_graphql_issue_edges(*, settings: Settings, after: Union[str, None] = None):\n    data = get_graphql_response(settings=settings, query=issues_query, after=after)\n    graphql_response = IssuesResponse.model_validate(data)\n    return graphql_response.data.repository.issues.edges\n\n\ndef get_graphql_question_discussion_edges(\n    *,\n    settings: Settings,\n    after: Union[str, None] = None,\n):\n    data = get_graphql_response(\n        settings=settings,\n        query=discussions_query,\n        after=after,\n    )\n    graphql_response = DiscussionsResponse.model_validate(data)\n    return graphql_response.data.repository.discussions.edges\n\n\ndef get_graphql_pr_edges(*, settings: Settings, after: Union[str, None] = None):\n    data = get_graphql_response(settings=settings, query=prs_query, after=after)\n    graphql_response = PRsResponse.model_validate(data)\n    return graphql_response.data.repository.pullRequests.edges\n\n\ndef get_issues_experts(settings: Settings):\n    issue_nodes: List[IssuesNode] = []\n    issue_edges = get_graphql_issue_edges(settings=settings)\n\n    while issue_edges:\n        for edge in issue_edges:\n            issue_nodes.append(edge.node)\n        last_edge = issue_edges[-1]\n        issue_edges = get_graphql_issue_edges(settings=settings, after=last_edge.cursor)\n\n    commentors = Counter()\n    last_month_commentors = Counter()\n    authors: Dict[str, Author] = {}\n\n    now = datetime.now(tz=timezone.utc)\n    one_month_ago = now - timedelta(days=30)\n\n    for issue in issue_nodes:\n        issue_author_name = None\n        if issue.author:\n            authors[issue.author.login] = issue.author\n            issue_author_name = issue.author.login\n        issue_commentors = set()\n        for comment in issue.comments.nodes:\n            if comment.author:\n                authors[comment.author.login] = comment.author\n                if comment.author.login != issue_author_name:\n                    issue_commentors.add(comment.author.login)\n        for author_name in issue_commentors:\n            commentors[author_name] += 1\n            if issue.createdAt > one_month_ago:\n                last_month_commentors[author_name] += 1\n\n    return commentors, last_month_commentors, authors\n\n\ndef get_discussions_experts(settings: Settings):\n    discussion_nodes: List[DiscussionsNode] = []\n    discussion_edges = get_graphql_question_discussion_edges(settings=settings)\n\n    while discussion_edges:\n        for discussion_edge in discussion_edges:\n            discussion_nodes.append(discussion_edge.node)\n        last_edge = discussion_edges[-1]\n        discussion_edges = get_graphql_question_discussion_edges(\n            settings=settings, after=last_edge.cursor\n        )\n\n    commentors = Counter()\n    last_month_commentors = Counter()\n    authors: Dict[str, Author] = {}\n\n    now = datetime.now(tz=timezone.utc)\n    one_month_ago = now - timedelta(days=30)\n\n    for discussion in discussion_nodes:\n        discussion_author_name = None\n        if discussion.author:\n            authors[discussion.author.login] = discussion.author\n            discussion_author_name = discussion.author.login\n        discussion_commentors = set()\n        for comment in discussion.comments.nodes:\n            if comment.author:\n                authors[comment.author.login] = comment.author\n                if comment.author.login != discussion_author_name:\n                    discussion_commentors.add(comment.author.login)\n            for reply in comment.replies.nodes:\n                if reply.author:\n                    authors[reply.author.login] = reply.author\n                    if reply.author.login != discussion_author_name:\n                        discussion_commentors.add(reply.author.login)\n        for author_name in discussion_commentors:\n            commentors[author_name] += 1\n            if discussion.createdAt > one_month_ago:\n                last_month_commentors[author_name] += 1\n    return commentors, last_month_commentors, authors\n\n\ndef get_experts(settings: Settings):\n    # Migrated to only use GitHub Discussions\n    # (\n    #     issues_commentors,\n    #     issues_last_month_commentors,\n    #     issues_authors,\n    # ) = get_issues_experts(settings=settings)\n    (\n        discussions_commentors,\n        discussions_last_month_commentors,\n        discussions_authors,\n    ) = get_discussions_experts(settings=settings)\n    # commentors = issues_commentors + discussions_commentors\n    commentors = discussions_commentors\n    # last_month_commentors = (\n    #     issues_last_month_commentors + discussions_last_month_commentors\n    # )\n    last_month_commentors = discussions_last_month_commentors\n    # authors = {**issues_authors, **discussions_authors}\n    authors = {**discussions_authors}\n    return commentors, last_month_commentors, authors\n\n\ndef get_contributors(settings: Settings):\n    pr_nodes: List[PullRequestNode] = []\n    pr_edges = get_graphql_pr_edges(settings=settings)\n\n    while pr_edges:\n        for edge in pr_edges:\n            pr_nodes.append(edge.node)\n        last_edge = pr_edges[-1]\n        pr_edges = get_graphql_pr_edges(settings=settings, after=last_edge.cursor)\n\n    contributors = Counter()\n    commentors = Counter()\n    reviewers = Counter()\n    authors: Dict[str, Author] = {}\n\n    for pr in pr_nodes:\n        author_name = None\n        if pr.author:\n            authors[pr.author.login] = pr.author\n            author_name = pr.author.login\n        pr_commentors: Set[str] = set()\n        pr_reviewers: Set[str] = set()\n        for comment in pr.comments.nodes:\n            if comment.author:\n                authors[comment.author.login] = comment.author\n                if comment.author.login == author_name:\n                    continue\n                pr_commentors.add(comment.author.login)\n        for author_name in pr_commentors:\n            commentors[author_name] += 1\n        for review in pr.reviews.nodes:\n            if review.author:\n                authors[review.author.login] = review.author\n                pr_reviewers.add(review.author.login)\n        for reviewer in pr_reviewers:\n            reviewers[reviewer] += 1\n        if pr.state == \"MERGED\" and pr.author:\n            contributors[pr.author.login] += 1\n    return contributors, commentors, reviewers, authors\n\n\ndef get_top_users(\n    *,\n    counter: Counter,\n    min_count: int,\n    authors: Dict[str, Author],\n    skip_users: Container[str],\n):\n    users = []\n    for commentor, count in counter.most_common(50):\n        if commentor in skip_users:\n            continue\n        if count >= min_count:\n            author = authors[commentor]\n            users.append(\n                {\n                    \"login\": commentor,\n                    \"count\": count,\n                    \"avatarUrl\": author.avatarUrl,\n                    \"url\": author.url,\n                }\n            )\n    return users\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    settings = Settings()\n    logging.info(f\"Using config: {settings.model_dump_json()}\")\n    g = Github(settings.input_token.get_secret_value())\n    repo = g.get_repo(settings.github_repository)\n    question_commentors, question_last_month_commentors, question_authors = get_experts(\n        settings=settings\n    )\n    contributors, pr_commentors, reviewers, pr_authors = get_contributors(\n        settings=settings\n    )\n    authors = {**question_authors, **pr_authors}\n    maintainers_logins = {'samuelcolvin', 'adriangb', 'dmontagu', 'hramezani', 'Kludex', 'davidhewitt', 'sydney-runkle', 'alexmojaki'}\n    bot_names = {\"codecov\", \"github-actions\", \"pre-commit-ci\", \"dependabot\"}\n    maintainers = []\n    for login in maintainers_logins:\n        user = authors[login]\n        maintainers.append(\n            {\n                \"login\": login,\n                \"answers\": question_commentors[login],\n                \"prs\": contributors[login],\n                \"avatarUrl\": user.avatarUrl,\n                \"url\": user.url,\n            }\n        )\n\n    min_count_expert = 10\n    min_count_last_month = 3\n    min_count_contributor = 4\n    min_count_reviewer = 4\n    skip_users = maintainers_logins | bot_names\n    experts = get_top_users(\n        counter=question_commentors,\n        min_count=min_count_expert,\n        authors=authors,\n        skip_users=skip_users,\n    )\n    last_month_active = get_top_users(\n        counter=question_last_month_commentors,\n        min_count=min_count_last_month,\n        authors=authors,\n        skip_users=skip_users,\n    )\n    top_contributors = get_top_users(\n        counter=contributors,\n        min_count=min_count_contributor,\n        authors=authors,\n        skip_users=skip_users,\n    )\n    top_reviewers = get_top_users(\n        counter=reviewers,\n        min_count=min_count_reviewer,\n        authors=authors,\n        skip_users=skip_users,\n    )\n\n    extra_experts = [\n        {\n            \"login\": \"ybressler\",\n            \"count\": None,\n            \"avatarUrl\": \"https://avatars.githubusercontent.com/u/40807730?v=4\",\n            \"url\": \"https://github.com/ybressler\"\n        },\n    ]\n    expert_logins = {e[\"login\"] for e in experts}\n    experts.extend([expert for expert in extra_experts if expert[\"login\"] not in expert_logins])\n\n    people = {\n        \"maintainers\": maintainers,\n        \"experts\": experts,\n        \"last_month_active\": last_month_active,\n        \"top_contributors\": top_contributors,\n        \"top_reviewers\": top_reviewers,\n    }\n    people_path = Path(\"./docs/plugins/people.yml\")\n    people_old_content = people_path.read_text(encoding=\"utf-8\")\n    new_people_content = yaml.dump(\n        people, sort_keys=False, width=200, allow_unicode=True\n    )\n    if (\n        people_old_content == new_people_content\n    ):\n        logging.info(\"The Pydantic People data hasn't changed, finishing.\")\n        sys.exit(0)\n    people_path.write_text(new_people_content, encoding=\"utf-8\")\n\n    logging.info(\"Setting up GitHub Actions git user\")\n    subprocess.run([\"git\", \"config\", \"user.name\", \"github-actions\"], check=True)\n    subprocess.run(\n        [\"git\", \"config\", \"user.email\", \"github-actions@github.com\"], check=True\n    )\n\n    branch_name = \"pydantic-people-update\"\n    logging.info(f\"Creating a new branch {branch_name}\")\n    subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n    logging.info(\"Adding updated file\")\n    subprocess.run(\n        [\"git\", \"add\", str(people_path)], check=True\n    )\n    logging.info(\"Committing updated file\")\n    message = \"\ud83d\udc65 Update Pydantic People\"\n    result = subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n    logging.info(\"Pushing branch\")\n    subprocess.run([\"git\", \"push\", \"origin\", branch_name], check=True)\n    logging.info(\"Creating PR\")\n    pr = repo.create_pull(title=message, body=message, base=\"main\", head=branch_name)\n    logging.info(f\"Created PR: {pr.number}\")\n    logging.info(\"Finished\")\n", "docs/plugins/conversion_table.py": "from __future__ import annotations as _annotations\n\nimport collections\nimport typing\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom datetime import date, datetime, time, timedelta\nfrom decimal import Decimal\nfrom enum import Enum, IntEnum\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom pathlib import Path\nfrom typing import Any, Iterable, Mapping, Pattern, Sequence, Type\nfrom uuid import UUID\n\nfrom pydantic_core import CoreSchema, core_schema\nfrom typing_extensions import TypedDict\n\nfrom pydantic import ByteSize, InstanceOf\n\n\n@dataclass\nclass Row:\n    field_type: type[Any] | str\n    input_type: type[Any] | str\n    python_input: bool = False\n    json_input: bool = False\n    strict: bool = False\n    condition: str | None = None\n    valid_examples: list[Any] | None = None\n    invalid_examples: list[Any] | None = None\n    core_schemas: list[type[CoreSchema]] | None = None\n\n    @property\n    def field_type_str(self) -> str:\n        return f'{self.field_type.__name__}' if hasattr(self.field_type, '__name__') else f'{self.field_type}'\n\n    @property\n    def input_type_str(self) -> str:\n        return f'{self.input_type.__name__}' if hasattr(self.input_type, '__name__') else f'{self.input_type}'\n\n    @property\n    def input_source_str(self) -> str:\n        if self.python_input:\n            if self.json_input:\n                return 'Python & JSON'\n            else:\n                return 'Python'\n        elif self.json_input:\n            return 'JSON'\n        else:\n            return ''\n\n\n@dataclass\nclass ConversionTable:\n    rows: list[Row]\n\n    col_names = [\n        'Field Type',\n        'Input',\n        'Strict',\n        'Input Source',\n        'Conditions',\n    ]\n    open_nowrap_span = '<span style=\"white-space: nowrap;\">'\n    close_nowrap_span = '</span>'\n\n    def col_values(self, row: Row) -> list[str]:\n        o = self.open_nowrap_span\n        c = self.close_nowrap_span\n\n        return [\n            f'{o}`{row.field_type_str}`{c}',\n            f'{o}`{row.input_type_str}`{c}',\n            '\u2713' if row.strict else '',\n            f'{o}{row.input_source_str}{c}',\n            row.condition if row.condition else '',\n        ]\n\n    @staticmethod\n    def row_as_markdown(cols: list[str]) -> str:\n        return f'| {\" | \".join(cols)} |'\n\n    def as_markdown(self) -> str:\n        lines = [self.row_as_markdown(self.col_names), self.row_as_markdown(['-'] * len(self.col_names))]\n        for row in self.rows:\n            lines.append(self.row_as_markdown(self.col_values(row)))\n        return '\\n'.join(lines)\n\n    @staticmethod\n    def row_sort_key(row: Row) -> Any:\n        field_type = row.field_type_str or ' '\n        input_type = row.input_type_str or ' '\n        input_source = row.input_source_str\n\n        # Include the .isupper() to make it so that leading-lowercase items come first\n        return field_type[0].isupper(), field_type, input_type[0].isupper(), input_type, input_source\n\n    def sorted(self) -> ConversionTable:\n        return ConversionTable(sorted(self.rows, key=self.row_sort_key))\n\n    def filtered(self, predicate: typing.Callable[[Row], bool]) -> ConversionTable:\n        return ConversionTable([row for row in self.rows if predicate(row)])\n\n\ntable_rows: list[Row] = [\n    Row(\n        str,\n        str,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.StringSchema],\n    ),\n    Row(\n        str,\n        bytes,\n        python_input=True,\n        condition='Assumes UTF-8, error on unicode decoding error.',\n        valid_examples=[b'this is bytes'],\n        invalid_examples=[b'\\x81'],\n        core_schemas=[core_schema.StringSchema],\n    ),\n    Row(\n        str,\n        bytearray,\n        python_input=True,\n        condition='Assumes UTF-8, error on unicode decoding error.',\n        valid_examples=[bytearray(b'this is bytearray' * 3)],\n        invalid_examples=[bytearray(b'\\x81' * 5)],\n        core_schemas=[core_schema.StringSchema],\n    ),\n    Row(\n        bytes,\n        bytes,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.BytesSchema],\n    ),\n    Row(\n        bytes,\n        str,\n        strict=True,\n        json_input=True,\n        valid_examples=['foo'],\n        core_schemas=[core_schema.BytesSchema],\n    ),\n    Row(\n        bytes,\n        str,\n        python_input=True,\n        valid_examples=['foo'],\n        core_schemas=[core_schema.BytesSchema],\n    ),\n    Row(\n        bytes,\n        bytearray,\n        python_input=True,\n        valid_examples=[bytearray(b'this is bytearray' * 3)],\n        core_schemas=[core_schema.BytesSchema],\n    ),\n    Row(\n        int,\n        int,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        condition='`bool` is explicitly forbidden.',\n        invalid_examples=[2**64, True, False],\n        core_schemas=[core_schema.IntSchema],\n    ),\n    Row(\n        int,\n        int,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.IntSchema],\n    ),\n    Row(\n        int,\n        float,\n        python_input=True,\n        json_input=True,\n        condition='Must be exact int, e.g. `val % 1 == 0`, raises error for `nan`, `inf`.',\n        valid_examples=[2.0],\n        invalid_examples=[2.1, 2.2250738585072011e308, float('nan'), float('inf')],\n        core_schemas=[core_schema.IntSchema],\n    ),\n    Row(\n        int,\n        Decimal,\n        python_input=True,\n        condition='Must be exact int, e.g. `val % 1 == 0`.',\n        valid_examples=[Decimal(2.0)],\n        invalid_examples=[Decimal(2.1)],\n        core_schemas=[core_schema.IntSchema],\n    ),\n    Row(\n        int,\n        bool,\n        python_input=True,\n        json_input=True,\n        valid_examples=[True, False],\n        core_schemas=[core_schema.IntSchema],\n    ),\n    Row(\n        int,\n        str,\n        python_input=True,\n        json_input=True,\n        condition='Must be numeric only, e.g. `[0-9]+`.',\n        valid_examples=['123'],\n        invalid_examples=['test', '123x'],\n        core_schemas=[core_schema.IntSchema],\n    ),\n    Row(\n        int,\n        bytes,\n        python_input=True,\n        condition='Must be numeric only, e.g. `[0-9]+`.',\n        valid_examples=[b'123'],\n        invalid_examples=[b'test', b'123x'],\n        core_schemas=[core_schema.IntSchema],\n    ),\n    Row(\n        float,\n        float,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        condition='`bool` is explicitly forbidden.',\n        invalid_examples=[True, False],\n        core_schemas=[core_schema.FloatSchema],\n    ),\n    Row(\n        float,\n        int,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        valid_examples=[123],\n        core_schemas=[core_schema.FloatSchema],\n    ),\n    Row(\n        float,\n        str,\n        python_input=True,\n        json_input=True,\n        condition='Must match `[0-9]+(\\\\.[0-9]+)?`.',\n        valid_examples=['3.141'],\n        invalid_examples=['test', '3.141x'],\n        core_schemas=[core_schema.FloatSchema],\n    ),\n    Row(\n        float,\n        bytes,\n        python_input=True,\n        condition='Must match `[0-9]+(\\\\.[0-9]+)?`.',\n        valid_examples=[b'3.141'],\n        invalid_examples=[b'test', b'3.141x'],\n        core_schemas=[core_schema.FloatSchema],\n    ),\n    Row(\n        float,\n        Decimal,\n        python_input=True,\n        valid_examples=[Decimal(3.5)],\n        core_schemas=[core_schema.FloatSchema],\n    ),\n    Row(\n        float,\n        bool,\n        python_input=True,\n        json_input=True,\n        valid_examples=[True, False],\n        core_schemas=[core_schema.FloatSchema],\n    ),\n    Row(\n        bool,\n        bool,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        valid_examples=[True, False],\n        core_schemas=[core_schema.BoolSchema],\n    ),\n    Row(\n        bool,\n        int,\n        python_input=True,\n        json_input=True,\n        condition='Allowed values: `0, 1`.',\n        valid_examples=[0, 1],\n        invalid_examples=[2, 100],\n        core_schemas=[core_schema.BoolSchema],\n    ),\n    Row(\n        bool,\n        float,\n        python_input=True,\n        json_input=True,\n        condition='Allowed values: `0.0, 1.0`.',\n        valid_examples=[0.0, 1.0],\n        invalid_examples=[2.0, 100.0],\n        core_schemas=[core_schema.BoolSchema],\n    ),\n    Row(\n        bool,\n        Decimal,\n        python_input=True,\n        condition='Allowed values: `Decimal(0), Decimal(1)`.',\n        valid_examples=[Decimal(0), Decimal(1)],\n        invalid_examples=[Decimal(2), Decimal(100)],\n        core_schemas=[core_schema.BoolSchema],\n    ),\n    Row(\n        bool,\n        str,\n        python_input=True,\n        json_input=True,\n        condition=(\n            \"Allowed values: `'f'`, `'n'`, `'no'`, `'off'`, `'false'`, `'False'`, `'t'`, `'y'`, \"\n            \"`'on'`, `'yes'`, `'true'`, `'True'`.\"\n        ),\n        valid_examples=['f', 'n', 'no', 'off', 'false', 'False', 't', 'y', 'on', 'yes', 'true', 'True'],\n        invalid_examples=['test'],\n        core_schemas=[core_schema.BoolSchema],\n    ),\n    Row(\n        None,\n        None,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.NoneSchema],\n    ),\n    Row(\n        date,\n        date,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.DateSchema],\n    ),\n    Row(\n        date,\n        datetime,\n        python_input=True,\n        condition='Must be exact date, eg. no `H`, `M`, `S`, `f`.',\n        valid_examples=[datetime(2017, 5, 5)],\n        invalid_examples=[datetime(2017, 5, 5, 10)],\n        core_schemas=[core_schema.DateSchema],\n    ),\n    Row(\n        date,\n        str,\n        python_input=True,\n        json_input=True,\n        condition='Format: `YYYY-MM-DD`.',\n        valid_examples=['2017-05-05'],\n        invalid_examples=['2017-5-5', '2017/05/05'],\n        core_schemas=[core_schema.DateSchema],\n    ),\n    Row(\n        date,\n        bytes,\n        python_input=True,\n        condition='Format: `YYYY-MM-DD` (UTF-8).',\n        valid_examples=[b'2017-05-05'],\n        invalid_examples=[b'2017-5-5', b'2017/05/05'],\n        core_schemas=[core_schema.DateSchema],\n    ),\n    Row(\n        date,\n        int,\n        python_input=True,\n        json_input=True,\n        condition=(\n            'Interpreted as seconds or ms from epoch. '\n            'See [speedate](https://docs.rs/speedate/latest/speedate/). Must be exact date.'\n        ),\n        valid_examples=[1493942400000, 1493942400],\n        invalid_examples=[1493942401000],\n        core_schemas=[core_schema.DateSchema],\n    ),\n    Row(\n        date,\n        float,\n        python_input=True,\n        json_input=True,\n        condition=(\n            'Interpreted as seconds or ms from epoch. '\n            'See [speedate](https://docs.rs/speedate/latest/speedate/). Must be exact date.'\n        ),\n        valid_examples=[1493942400000.0, 1493942400.0],\n        invalid_examples=[1493942401000.0],\n        core_schemas=[core_schema.DateSchema],\n    ),\n    Row(\n        date,\n        Decimal,\n        python_input=True,\n        condition=(\n            'Interpreted as seconds or ms from epoch. '\n            'See [speedate](https://docs.rs/speedate/latest/speedate/). Must be exact date.'\n        ),\n        valid_examples=[Decimal(1493942400000), Decimal(1493942400)],\n        invalid_examples=[Decimal(1493942401000)],\n        core_schemas=[core_schema.DateSchema],\n    ),\n    Row(\n        datetime,\n        datetime,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.DatetimeSchema],\n    ),\n    Row(\n        datetime,\n        date,\n        python_input=True,\n        valid_examples=[date(2017, 5, 5)],\n        core_schemas=[core_schema.DatetimeSchema],\n    ),\n    Row(\n        datetime,\n        str,\n        python_input=True,\n        json_input=True,\n        condition='Format: `YYYY-MM-DDTHH:MM:SS.f` or `YYYY-MM-DD`. See [speedate](https://docs.rs/speedate/latest/speedate/).',\n        valid_examples=['2017-05-05 10:10:10', '2017-05-05T10:10:10.0002', '2017-05-05 10:10:10+00:00', '2017-05-05'],\n        invalid_examples=['2017-5-5T10:10:10'],\n        core_schemas=[core_schema.DatetimeSchema],\n    ),\n    Row(\n        datetime,\n        bytes,\n        python_input=True,\n        condition=(\n            'Format: `YYYY-MM-DDTHH:MM:SS.f` or `YYYY-MM-DD`. See [speedate](https://docs.rs/speedate/latest/speedate/), (UTF-8).'\n        ),\n        valid_examples=[b'2017-05-05 10:10:10', b'2017-05-05T10:10:10.0002', b'2017-05-05 10:10:10+00:00'],\n        invalid_examples=[b'2017-5-5T10:10:10'],\n        core_schemas=[core_schema.DatetimeSchema],\n    ),\n    Row(\n        datetime,\n        int,\n        python_input=True,\n        json_input=True,\n        condition='Interpreted as seconds or ms from epoch, see [speedate](https://docs.rs/speedate/latest/speedate/).',\n        valid_examples=[1493979010000, 1493979010],\n        core_schemas=[core_schema.DatetimeSchema],\n    ),\n    Row(\n        datetime,\n        float,\n        python_input=True,\n        json_input=True,\n        condition='Interpreted as seconds or ms from epoch, see [speedate](https://docs.rs/speedate/latest/speedate/).',\n        valid_examples=[1493979010000.0, 1493979010.0],\n        core_schemas=[core_schema.DatetimeSchema],\n    ),\n    Row(\n        datetime,\n        Decimal,\n        python_input=True,\n        condition='Interpreted as seconds or ms from epoch, see [speedate](https://docs.rs/speedate/latest/speedate/).',\n        valid_examples=[Decimal(1493979010000), Decimal(1493979010)],\n        core_schemas=[core_schema.DatetimeSchema],\n    ),\n    Row(\n        time,\n        time,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.TimeSchema],\n    ),\n    Row(\n        time,\n        str,\n        python_input=True,\n        json_input=True,\n        condition='Format: `HH:MM:SS.FFFFFF`. See [speedate](https://docs.rs/speedate/latest/speedate/).',\n        valid_examples=['10:10:10.0002'],\n        invalid_examples=['1:1:1'],\n        core_schemas=[core_schema.TimeSchema],\n    ),\n    Row(\n        time,\n        bytes,\n        python_input=True,\n        condition='Format: `HH:MM:SS.FFFFFF`. See [speedate](https://docs.rs/speedate/latest/speedate/).',\n        valid_examples=[b'10:10:10.0002'],\n        invalid_examples=[b'1:1:1'],\n        core_schemas=[core_schema.TimeSchema],\n    ),\n    Row(\n        time,\n        int,\n        python_input=True,\n        json_input=True,\n        condition='Interpreted as seconds, range `0 - 86399`.',\n        valid_examples=[3720],\n        invalid_examples=[-1, 86400],\n        core_schemas=[core_schema.TimeSchema],\n    ),\n    Row(\n        time,\n        float,\n        python_input=True,\n        json_input=True,\n        condition='Interpreted as seconds, range `0 - 86399.9*`.',\n        valid_examples=[3720.0002],\n        invalid_examples=[-1.0, 86400.0],\n        core_schemas=[core_schema.TimeSchema],\n    ),\n    Row(\n        time,\n        Decimal,\n        python_input=True,\n        condition='Interpreted as seconds, range `0 - 86399.9*`.',\n        valid_examples=[Decimal(3720.0002)],\n        invalid_examples=[Decimal(-1), Decimal(86400)],\n        core_schemas=[core_schema.TimeSchema],\n    ),\n    Row(\n        timedelta,\n        timedelta,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.TimedeltaSchema],\n    ),\n    Row(\n        timedelta,\n        str,\n        python_input=True,\n        json_input=True,\n        condition='Format: `ISO8601`. See [speedate](https://docs.rs/speedate/latest/speedate/).',\n        valid_examples=['1 days 10:10', '1 d 10:10'],\n        invalid_examples=['1 10:10'],\n        core_schemas=[core_schema.TimedeltaSchema],\n    ),\n    Row(\n        timedelta,\n        bytes,\n        python_input=True,\n        condition='Format: `ISO8601`. See [speedate](https://docs.rs/speedate/latest/speedate/), (UTF-8).',\n        valid_examples=[b'1 days 10:10', b'1 d 10:10'],\n        invalid_examples=[b'1 10:10'],\n        core_schemas=[core_schema.TimedeltaSchema],\n    ),\n    Row(\n        timedelta,\n        int,\n        python_input=True,\n        json_input=True,\n        condition='Interpreted as seconds.',\n        valid_examples=[123_000],\n        core_schemas=[core_schema.TimedeltaSchema],\n    ),\n    Row(\n        timedelta,\n        float,\n        python_input=True,\n        json_input=True,\n        condition='Interpreted as seconds.',\n        valid_examples=[123_000.0002],\n        core_schemas=[core_schema.TimedeltaSchema],\n    ),\n    Row(\n        timedelta,\n        Decimal,\n        python_input=True,\n        condition='Interpreted as seconds.',\n        valid_examples=[Decimal(123_000.0002)],\n        core_schemas=[core_schema.TimedeltaSchema],\n    ),\n    Row(\n        dict,\n        dict,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.DictSchema],\n    ),\n    Row(\n        dict,\n        'Object',\n        strict=True,\n        json_input=True,\n        valid_examples=['{\"v\": {\"1\": 1, \"2\": 2}}'],\n        core_schemas=[core_schema.DictSchema],\n    ),\n    Row(\n        dict,\n        Mapping,\n        python_input=True,\n        condition='Must implement the mapping interface and have an `items()` method.',\n        valid_examples=[],\n        core_schemas=[core_schema.DictSchema],\n    ),\n    Row(\n        TypedDict,\n        dict,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.TypedDictSchema],\n    ),\n    Row(\n        TypedDict,\n        'Object',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.TypedDictSchema],\n    ),\n    Row(\n        TypedDict,\n        Any,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.TypedDictSchema],\n    ),\n    Row(\n        TypedDict,\n        Mapping,\n        python_input=True,\n        condition='Must implement the mapping interface and have an `items()` method.',\n        valid_examples=[],\n        core_schemas=[core_schema.TypedDictSchema],\n    ),\n    Row(\n        list,\n        list,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.ListSchema],\n    ),\n    Row(\n        list,\n        'Array',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.ListSchema],\n    ),\n    Row(\n        list,\n        tuple,\n        python_input=True,\n        core_schemas=[core_schema.ListSchema],\n    ),\n    Row(\n        list,\n        set,\n        python_input=True,\n        core_schemas=[core_schema.ListSchema],\n    ),\n    Row(\n        list,\n        frozenset,\n        python_input=True,\n        core_schemas=[core_schema.ListSchema],\n    ),\n    Row(\n        list,\n        deque,\n        python_input=True,\n        core_schemas=[core_schema.ListSchema],\n    ),\n    Row(\n        list,\n        'dict_keys',\n        python_input=True,\n        core_schemas=[core_schema.ListSchema],\n    ),\n    Row(\n        list,\n        'dict_values',\n        python_input=True,\n        core_schemas=[core_schema.ListSchema],\n    ),\n    Row(\n        tuple,\n        tuple,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.TupleSchema],\n    ),\n    Row(\n        tuple,\n        'Array',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.TupleSchema],\n    ),\n    Row(\n        tuple,\n        list,\n        python_input=True,\n        core_schemas=[core_schema.TupleSchema],\n    ),\n    Row(\n        tuple,\n        set,\n        python_input=True,\n        core_schemas=[core_schema.TupleSchema],\n    ),\n    Row(\n        tuple,\n        frozenset,\n        python_input=True,\n        core_schemas=[core_schema.TupleSchema],\n    ),\n    Row(\n        tuple,\n        deque,\n        python_input=True,\n        core_schemas=[core_schema.TupleSchema],\n    ),\n    Row(\n        tuple,\n        'dict_keys',\n        python_input=True,\n        core_schemas=[core_schema.TupleSchema],\n    ),\n    Row(\n        tuple,\n        'dict_values',\n        python_input=True,\n        core_schemas=[core_schema.TupleSchema],\n    ),\n    Row(\n        set,\n        set,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.SetSchema],\n    ),\n    Row(\n        set,\n        'Array',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.SetSchema],\n    ),\n    Row(\n        set,\n        list,\n        python_input=True,\n        core_schemas=[core_schema.SetSchema],\n    ),\n    Row(\n        set,\n        tuple,\n        python_input=True,\n        core_schemas=[core_schema.SetSchema],\n    ),\n    Row(\n        set,\n        frozenset,\n        python_input=True,\n        core_schemas=[core_schema.SetSchema],\n    ),\n    Row(\n        set,\n        deque,\n        python_input=True,\n        core_schemas=[core_schema.SetSchema],\n    ),\n    Row(\n        set,\n        'dict_keys',\n        python_input=True,\n        core_schemas=[core_schema.SetSchema],\n    ),\n    Row(\n        set,\n        'dict_values',\n        python_input=True,\n        core_schemas=[core_schema.SetSchema],\n    ),\n    Row(\n        frozenset,\n        frozenset,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.FrozenSetSchema],\n    ),\n    Row(\n        frozenset,\n        'Array',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.FrozenSetSchema],\n    ),\n    Row(\n        frozenset,\n        list,\n        python_input=True,\n        core_schemas=[core_schema.FrozenSetSchema],\n    ),\n    Row(\n        frozenset,\n        tuple,\n        python_input=True,\n        core_schemas=[core_schema.FrozenSetSchema],\n    ),\n    Row(\n        frozenset,\n        set,\n        python_input=True,\n        core_schemas=[core_schema.FrozenSetSchema],\n    ),\n    Row(\n        frozenset,\n        deque,\n        python_input=True,\n        core_schemas=[core_schema.FrozenSetSchema],\n    ),\n    Row(\n        frozenset,\n        'dict_keys',\n        python_input=True,\n        core_schemas=[core_schema.FrozenSetSchema],\n    ),\n    Row(\n        frozenset,\n        'dict_values',\n        python_input=True,\n        core_schemas=[core_schema.FrozenSetSchema],\n    ),\n    Row(\n        InstanceOf,\n        Any,\n        strict=True,\n        python_input=True,\n        condition='`isinstance()` check must return `True`.',\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        InstanceOf,\n        '-',\n        json_input=True,\n        condition='Never valid.',\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        callable,\n        Any,\n        strict=True,\n        python_input=True,\n        condition='`callable()` check must return `True`.',\n        core_schemas=[core_schema.CallableSchema],\n    ),\n    Row(\n        callable,\n        '-',\n        json_input=True,\n        condition='Never valid.',\n        core_schemas=[core_schema.CallableSchema],\n    ),\n    Row(\n        deque,\n        deque,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.WrapValidatorFunctionSchema],\n    ),\n    Row(\n        deque,\n        'Array',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.WrapValidatorFunctionSchema],\n    ),\n    Row(\n        deque,\n        list,\n        python_input=True,\n        core_schemas=[core_schema.ChainSchema],\n    ),\n    Row(\n        deque,\n        tuple,\n        python_input=True,\n        core_schemas=[core_schema.ChainSchema],\n    ),\n    Row(\n        deque,\n        set,\n        python_input=True,\n        core_schemas=[core_schema.ChainSchema],\n    ),\n    Row(\n        deque,\n        frozenset,\n        python_input=True,\n        core_schemas=[core_schema.ChainSchema],\n    ),\n    Row(\n        Any,\n        Any,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.AnySchema],\n    ),\n    Row(\n        typing.NamedTuple,\n        typing.NamedTuple,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        typing.NamedTuple,\n        'Array',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        typing.NamedTuple,\n        collections.namedtuple,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        typing.NamedTuple,\n        tuple,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        typing.NamedTuple,\n        list,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        typing.NamedTuple,\n        dict,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        collections.namedtuple,\n        collections.namedtuple,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        collections.namedtuple,\n        'Array',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        collections.namedtuple,\n        typing.NamedTuple,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        collections.namedtuple,\n        tuple,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        collections.namedtuple,\n        list,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        collections.namedtuple,\n        dict,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CallSchema],\n    ),\n    Row(\n        Sequence,\n        list,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.ChainSchema],\n    ),\n    Row(\n        Sequence,\n        'Array',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.ChainSchema],\n    ),\n    Row(\n        Sequence,\n        tuple,\n        python_input=True,\n        core_schemas=[core_schema.ChainSchema],\n    ),\n    Row(\n        Sequence,\n        deque,\n        python_input=True,\n        core_schemas=[core_schema.ChainSchema],\n    ),\n    Row(\n        Iterable,\n        list,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.GeneratorSchema],\n    ),\n    Row(\n        Iterable,\n        'Array',\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.GeneratorSchema],\n    ),\n    Row(\n        Iterable,\n        tuple,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.GeneratorSchema],\n    ),\n    Row(\n        Iterable,\n        set,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.GeneratorSchema],\n    ),\n    Row(\n        Iterable,\n        frozenset,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.GeneratorSchema],\n    ),\n    Row(\n        Iterable,\n        deque,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.GeneratorSchema],\n    ),\n    Row(\n        Type,\n        Type,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsSubclassSchema],\n    ),\n    Row(\n        Pattern,\n        str,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        condition='Input must be a valid pattern.',\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        Pattern,\n        bytes,\n        strict=True,\n        python_input=True,\n        condition='Input must be a valid pattern.',\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Address,\n        IPv4Address,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        IPv4Address,\n        IPv4Interface,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        IPv4Address,\n        str,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Address,\n        str,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Address,\n        bytes,\n        python_input=True,\n        valid_examples=[b'\\x00\\x00\\x00\\x00'],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Address,\n        int,\n        python_input=True,\n        condition='integer representing the IP address, must be less than `2**32`',\n        valid_examples=[168_430_090],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Interface,\n        IPv4Interface,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        IPv4Interface,\n        str,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Interface,\n        IPv4Address,\n        python_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Interface,\n        str,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Interface,\n        bytes,\n        python_input=True,\n        valid_examples=[b'\\xff\\xff\\xff\\xff'],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Interface,\n        tuple,\n        python_input=True,\n        valid_examples=[('192.168.0.1', '24')],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Interface,\n        int,\n        python_input=True,\n        condition='integer representing the IP address, must be less than `2**32`',\n        valid_examples=[168_430_090],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Network,\n        IPv4Network,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        IPv4Network,\n        str,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Network,\n        IPv4Address,\n        python_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Network,\n        IPv4Interface,\n        python_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Network,\n        str,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Network,\n        bytes,\n        python_input=True,\n        valid_examples=[b'\\xff\\xff\\xff\\xff'],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv4Network,\n        int,\n        python_input=True,\n        condition='integer representing the IP network, must be less than `2**32`',\n        valid_examples=[168_430_090],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Address,\n        IPv6Address,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        IPv6Address,\n        IPv6Interface,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        IPv6Address,\n        str,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Address,\n        str,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Address,\n        bytes,\n        python_input=True,\n        valid_examples=[b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01'],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Address,\n        int,\n        python_input=True,\n        condition='integer representing the IP address, must be less than `2**128`',\n        valid_examples=[340_282_366_920_938_463_463_374_607_431_768_211_455],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Interface,\n        IPv6Interface,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        IPv6Interface,\n        str,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Interface,\n        IPv6Address,\n        python_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Interface,\n        str,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Interface,\n        bytes,\n        python_input=True,\n        valid_examples=[b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01'],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Interface,\n        tuple,\n        python_input=True,\n        valid_examples=[('2001:db00::1', '120')],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Interface,\n        int,\n        python_input=True,\n        condition='integer representing the IP address, must be less than `2**128`',\n        valid_examples=[340_282_366_920_938_463_463_374_607_431_768_211_455],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Network,\n        IPv6Network,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        IPv6Network,\n        str,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Network,\n        IPv6Address,\n        python_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Network,\n        IPv6Interface,\n        python_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Network,\n        str,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Network,\n        bytes,\n        python_input=True,\n        valid_examples=[b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01'],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IPv6Network,\n        int,\n        python_input=True,\n        condition='integer representing the IP address, must be less than `2**128`',\n        valid_examples=[340_282_366_920_938_463_463_374_607_431_768_211_455],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        Enum,\n        Enum,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        Enum,\n        Any,\n        strict=True,\n        json_input=True,\n        condition='Input value must be convertible to enum values.',\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        Enum,\n        Any,\n        python_input=True,\n        condition='Input value must be convertible to enum values.',\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IntEnum,\n        IntEnum,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        IntEnum,\n        Any,\n        strict=True,\n        json_input=True,\n        condition='Input value must be convertible to enum values.',\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        IntEnum,\n        Any,\n        python_input=True,\n        condition='Input value must be convertible to enum values.',\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        Decimal,\n        Decimal,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.CustomErrorSchema],\n    ),\n    Row(\n        Decimal,\n        int,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.CustomErrorSchema],\n    ),\n    Row(\n        Decimal,\n        str,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.CustomErrorSchema],\n    ),\n    Row(\n        Decimal,\n        float,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.CustomErrorSchema],\n    ),\n    Row(\n        Decimal,\n        int,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        Decimal,\n        str,\n        python_input=True,\n        json_input=True,\n        condition='Must match `[0-9]+(\\\\.[0-9]+)?`.',\n        valid_examples=['3.141'],\n        invalid_examples=['test', '3.141x'],\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        Decimal,\n        float,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        Path,\n        Path,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        Path,\n        str,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        Path,\n        str,\n        python_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        UUID,\n        UUID,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.IsInstanceSchema],\n    ),\n    Row(\n        UUID,\n        str,\n        strict=True,\n        json_input=True,\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        UUID,\n        str,\n        python_input=True,\n        valid_examples=['{12345678-1234-5678-1234-567812345678}'],\n        core_schemas=[core_schema.AfterValidatorFunctionSchema],\n    ),\n    Row(\n        ByteSize,\n        str,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        valid_examples=['1.2', '1.5 KB', '6.2EiB'],\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        ByteSize,\n        int,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        ByteSize,\n        float,\n        strict=True,\n        python_input=True,\n        json_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n    Row(\n        ByteSize,\n        Decimal,\n        strict=True,\n        python_input=True,\n        core_schemas=[core_schema.PlainValidatorFunctionSchema],\n    ),\n]\n\nconversion_table = ConversionTable(table_rows).sorted()\n", "docs/plugins/using_update.py": "from pathlib import Path\nfrom time import sleep\n\nimport requests\nimport tomli\n\nTHIS_DIR = Path(__file__).parent\n\nsession = requests.Session()\n\n\ndef update_lib(lib, *, retry=0):\n    repo = lib['repo']\n    url = f'https://api.github.com/repos/{repo}'\n    resp = session.get(url)\n    if resp.status_code == 403 and retry < 3:\n        print(f'retrying {repo} {retry}')\n        sleep(5)\n        return update_lib(lib, retry=retry + 1)\n\n    resp.raise_for_status()\n    data = resp.json()\n    stars = data['watchers_count']\n    print(f'{repo}: {stars}')\n    lib['stars'] = stars\n\n\nwith (THIS_DIR / 'using.toml').open('rb') as f:\n    table = tomli.load(f)\n\nlibs = table['libs']\nfor lib in libs:\n    update_lib(lib)\n\nlibs.sort(key=lambda lib: lib['stars'], reverse=True)\n\nwith (THIS_DIR / 'using.toml').open('w') as f:\n    for lib in libs:\n        f.write('[[libs]]\\nrepo = \"{repo}\"\\nstars = {stars}\\n'.format(**lib))\n", "docs/plugins/griffe_doclinks.py": "import ast\nimport re\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Tuple\n\nfrom griffe.dataclasses import Object as GriffeObject\nfrom griffe.extensions import VisitorExtension\nfrom pymdownx.slugs import slugify\n\nDOCS_PATH = Path(__file__).parent.parent\nslugifier = slugify(case='lower')\n\n\ndef find_heading(content: str, slug: str, file_path: Path) -> Tuple[str, int]:\n    for m in re.finditer('^#+ (.+)', content, flags=re.M):\n        heading = m.group(1)\n        h_slug = slugifier(heading, '-')\n        if h_slug == slug:\n            return heading, m.end()\n    raise ValueError(f'heading with slug {slug!r} not found in {file_path}')\n\n\ndef insert_at_top(path: str, api_link: str) -> str:\n    rel_file = path.rstrip('/') + '.md'\n    file_path = DOCS_PATH / rel_file\n    content = file_path.read_text()\n    second_heading = re.search('^#+ ', content, flags=re.M)\n    assert second_heading, 'unable to find second heading in file'\n    first_section = content[: second_heading.start()]\n\n    if f'[{api_link}]' not in first_section:\n        print(f'inserting API link \"{api_link}\" at the top of {file_path.relative_to(DOCS_PATH)}')\n        file_path.write_text('??? api \"API Documentation\"\\n' f'    [`{api_link}`][{api_link}]<br>\\n\\n' f'{content}')\n\n    heading = file_path.stem.replace('_', ' ').title()\n    return f'!!! abstract \"Usage Documentation\"\\n    [{heading}](../{rel_file})\\n'\n\n\ndef replace_links(m: re.Match, *, api_link: str) -> str:\n    path_group = m.group(1)\n    if '#' not in path_group:\n        # no heading id, put the content at the top of the page\n        return insert_at_top(path_group, api_link)\n\n    usage_path, slug = path_group.split('#', 1)\n    rel_file = usage_path.rstrip('/') + '.md'\n    file_path = DOCS_PATH / rel_file\n    content = file_path.read_text()\n    heading, heading_end = find_heading(content, slug, file_path)\n\n    next_heading = re.search('^#+ ', content[heading_end:], flags=re.M)\n    if next_heading:\n        next_section = content[heading_end : heading_end + next_heading.start()]\n    else:\n        next_section = content[heading_end:]\n\n    if f'[{api_link}]' not in next_section:\n        print(f'inserting API link \"{api_link}\" into {file_path.relative_to(DOCS_PATH)}')\n        file_path.write_text(\n            f'{content[:heading_end]}\\n\\n'\n            '??? api \"API Documentation\"\\n'\n            f'    [`{api_link}`][{api_link}]<br>'\n            f'{content[heading_end:]}'\n        )\n\n    return f'!!! abstract \"Usage Documentation\"\\n    [{heading}](../{rel_file}#{slug})\\n'\n\n\ndef update_docstring(obj: GriffeObject) -> str:\n    return re.sub(\n        r'usage[\\- ]docs: ?https://docs\\.pydantic\\.dev/.+?/(\\S+)',\n        partial(replace_links, api_link=obj.path),\n        obj.docstring.value,\n        flags=re.I,\n    )\n\n\ndef update_docstrings_recursively(obj: GriffeObject) -> None:\n    if obj.docstring:\n        obj.docstring.value = update_docstring(obj)\n    for member in obj.members.values():\n        if not member.is_alias:\n            update_docstrings_recursively(member)\n\n\nclass Extension(VisitorExtension):\n    def visit_module(self, node: ast.AST) -> None:\n        module = self.visitor.current.module\n        update_docstrings_recursively(module)\n", "docs/plugins/main.py": "from __future__ import annotations as _annotations\n\nimport json\nimport logging\nimport os\nimport re\nimport textwrap\nfrom pathlib import Path\nfrom textwrap import indent\n\nimport autoflake\nimport pyupgrade._main as pyupgrade_main  # type: ignore\nimport tomli\nimport yaml\nfrom jinja2 import Template  # type: ignore\nfrom mkdocs.config import Config\nfrom mkdocs.structure.files import Files\nfrom mkdocs.structure.pages import Page\n\nfrom .conversion_table import conversion_table\n\nlogger = logging.getLogger('mkdocs.plugin')\nTHIS_DIR = Path(__file__).parent\nDOCS_DIR = THIS_DIR.parent\nPROJECT_ROOT = DOCS_DIR.parent\n\n\ndef on_pre_build(config: Config) -> None:\n    \"\"\"\n    Before the build starts.\n    \"\"\"\n    add_changelog()\n    add_mkdocs_run_deps()\n\n\ndef on_files(files: Files, config: Config) -> Files:\n    \"\"\"\n    After the files are loaded, but before they are read.\n    \"\"\"\n    return files\n\n\ndef on_page_markdown(markdown: str, page: Page, config: Config, files: Files) -> str:\n    \"\"\"\n    Called on each file after it is read and before it is converted to HTML.\n    \"\"\"\n    markdown = upgrade_python(markdown)\n    markdown = insert_json_output(markdown)\n    markdown = remove_code_fence_attributes(markdown)\n    if md := render_index(markdown, page):\n        return md\n    if md := render_why(markdown, page):\n        return md\n    elif md := build_schema_mappings(markdown, page):\n        return md\n    elif md := build_conversion_table(markdown, page):\n        return md\n    elif md := devtools_example(markdown, page):\n        return md\n    elif md := populate_pydantic_people(markdown, page):\n        return md\n    else:\n        return markdown\n\n\ndef add_changelog() -> None:\n    history = (PROJECT_ROOT / 'HISTORY.md').read_text(encoding='utf-8')\n    history = re.sub(r'(\\s)@([\\w\\-]+)', r'\\1[@\\2](https://github.com/\\2)', history, flags=re.I)\n    history = re.sub(r'\\[GitHub release]\\(', r'[:simple-github: GitHub release](', history)\n    history = re.sub('@@', '@', history)\n    new_file = DOCS_DIR / 'changelog.md'\n\n    # avoid writing file unless the content has changed to avoid infinite build loop\n    if not new_file.is_file() or new_file.read_text(encoding='utf-8') != history:\n        new_file.write_text(history, encoding='utf-8')\n\n\ndef add_mkdocs_run_deps() -> None:\n    # set the pydantic, pydantic-core, pydantic-extra-types versions to configure for running examples in the browser\n    pyproject_toml = (PROJECT_ROOT / 'pyproject.toml').read_text()\n    pydantic_core_version = re.search(r'pydantic-core==(.+?)[\"\\']', pyproject_toml).group(1)\n\n    version_py = (PROJECT_ROOT / 'pydantic' / 'version.py').read_text()\n    pydantic_version = re.search(r'^VERSION ?= ([\"\\'])(.+)\\1', version_py, flags=re.M).group(2)\n\n    pdm_lock = (PROJECT_ROOT / 'pdm.lock').read_text()\n    pydantic_extra_types_version = re.search(r'name = \"pydantic-extra-types\"\\nversion = \"(.+?)\"', pdm_lock).group(1)\n\n    mkdocs_run_deps = json.dumps(\n        [\n            f'pydantic=={pydantic_version}',\n            f'pydantic-core=={pydantic_core_version}',\n            f'pydantic-extra-types=={pydantic_extra_types_version}',\n        ]\n    )\n    logger.info('Setting mkdocs_run_deps=%s', mkdocs_run_deps)\n\n    html = f\"\"\"\\\n    <script>\n    window.mkdocs_run_deps = {mkdocs_run_deps}\n    </script>\n\"\"\"\n    path = DOCS_DIR / 'theme/mkdocs_run_deps.html'\n    path.write_text(html)\n\n\nMIN_MINOR_VERSION = 7\nMAX_MINOR_VERSION = 11\n\n\ndef upgrade_python(markdown: str) -> str:\n    \"\"\"\n    Apply pyupgrade to all python code blocks, unless explicitly skipped, create a tab for each version.\n    \"\"\"\n\n    def add_tabs(match: re.Match[str]) -> str:\n        prefix = match.group(1)\n        if 'upgrade=\"skip\"' in prefix:\n            return match.group(0)\n\n        if m := re.search(r'requires=\"3.(\\d+)\"', prefix):\n            min_minor_version = int(m.group(1))\n        else:\n            min_minor_version = MIN_MINOR_VERSION\n\n        py_code = match.group(2)\n        numbers = match.group(3)\n        # import devtools\n        # devtools.debug(numbers)\n        output = []\n        last_code = py_code\n        for minor_version in range(min_minor_version, MAX_MINOR_VERSION + 1):\n            if minor_version == min_minor_version:\n                tab_code = py_code\n            else:\n                tab_code = _upgrade_code(py_code, minor_version)\n                if tab_code == last_code:\n                    continue\n                last_code = tab_code\n\n            content = indent(f'{prefix}\\n{tab_code}```{numbers}', ' ' * 4)\n            output.append(f'=== \"Python 3.{minor_version} and above\"\\n\\n{content}')\n\n        if len(output) == 1:\n            return match.group(0)\n        else:\n            return '\\n\\n'.join(output)\n\n    return re.sub(r'^(``` *py.*?)\\n(.+?)^```(\\s+(?:^\\d+\\. .+?\\n)+)', add_tabs, markdown, flags=re.M | re.S)\n\n\ndef _upgrade_code(code: str, min_version: int) -> str:\n    upgraded = pyupgrade_main._fix_plugins(\n        code,\n        settings=pyupgrade_main.Settings(\n            min_version=(3, min_version),\n            keep_percent_format=True,\n            keep_mock=False,\n            keep_runtime_typing=True,\n        ),\n    )\n    return autoflake.fix_code(upgraded, remove_all_unused_imports=True)\n\n\ndef insert_json_output(markdown: str) -> str:\n    \"\"\"\n    Find `output=\"json\"` code fence tags and replace with a separate JSON section\n    \"\"\"\n\n    def replace_json(m: re.Match[str]) -> str:\n        start, attrs, code = m.groups()\n\n        def replace_last_print(m2: re.Match[str]) -> str:\n            ind, json_text = m2.groups()\n            json_text = indent(json.dumps(json.loads(json_text), indent=2), ind)\n            # no trailing fence as that's not part of code\n            return f'\\n{ind}```\\n\\n{ind}JSON output:\\n\\n{ind}```json\\n{json_text}\\n'\n\n        code = re.sub(r'\\n( *)\"\"\"(.*?)\\1\"\"\"\\n$', replace_last_print, code, flags=re.S)\n        return f'{start}{attrs}{code}{start}\\n'\n\n    return re.sub(r'(^ *```)([^\\n]*?output=\"json\"[^\\n]*?\\n)(.+?)\\1', replace_json, markdown, flags=re.M | re.S)\n\n\ndef remove_code_fence_attributes(markdown: str) -> str:\n    \"\"\"\n    There's no way to add attributes to code fences that works with both pycharm and mkdocs, hence we use\n    `py key=\"value\"` to provide attributes to pytest-examples, then remove those attributes here.\n\n    https://youtrack.jetbrains.com/issue/IDEA-297873 & https://python-markdown.github.io/extensions/fenced_code_blocks/\n    \"\"\"\n\n    def remove_attrs(match: re.Match[str]) -> str:\n        suffix = re.sub(\n            r' (?:test|lint|upgrade|group|requires|output|rewrite_assert)=\".+?\"', '', match.group(2), flags=re.M\n        )\n        return f'{match.group(1)}{suffix}'\n\n    return re.sub(r'^( *``` *py)(.*)', remove_attrs, markdown, flags=re.M)\n\n\ndef get_orgs_data() -> list[dict[str, str]]:\n    with (THIS_DIR / 'orgs.toml').open('rb') as f:\n        orgs_data = tomli.load(f)\n    return orgs_data['orgs']\n\n\ntile_template = \"\"\"\n<div class=\"tile\">\n  <a href=\"why/#org-{key}\" title=\"{name}\">\n    <img src=\"logos/{key}_logo.png\" alt=\"{name}\" />\n  </a>\n</div>\"\"\"\n\n\ndef render_index(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'index.md':\n        return None\n\n    if version := os.getenv('PYDANTIC_VERSION'):\n        url = f'https://github.com/pydantic/pydantic/releases/tag/{version}'\n        version_str = f'Documentation for version: [{version}]({url})'\n    elif (version_ref := os.getenv('GITHUB_REF')) and version_ref.startswith('refs/tags/'):\n        version = re.sub('^refs/tags/', '', version_ref.lower())\n        url = f'https://github.com/pydantic/pydantic/releases/tag/{version}'\n        version_str = f'Documentation for version: [{version}]({url})'\n    elif sha := os.getenv('GITHUB_SHA'):\n        url = f'https://github.com/pydantic/pydantic/commit/{sha}'\n        sha = sha[:7]\n        version_str = f'Documentation for development version: [{sha}]({url})'\n    else:\n        version_str = 'Documentation for development version'\n    logger.info('Setting version prefix: %r', version_str)\n    markdown = re.sub(r'{{ *version *}}', version_str, markdown)\n\n    elements = [tile_template.format(**org) for org in get_orgs_data()]\n\n    orgs_grid = f'<div id=\"grid-container\"><div id=\"company-grid\" class=\"grid\">{\"\".join(elements)}</div></div>'\n    return re.sub(r'{{ *organisations *}}', orgs_grid, markdown)\n\n\ndef render_why(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'why.md':\n        return None\n\n    with (THIS_DIR / 'using.toml').open('rb') as f:\n        using = tomli.load(f)['libs']\n\n    libraries = '\\n'.join('* [`{repo}`](https://github.com/{repo}) {stars:,} stars'.format(**lib) for lib in using)\n    markdown = re.sub(r'{{ *libraries *}}', libraries, markdown)\n    default_description = '_(Based on the criteria described above)_'\n\n    elements = [\n        f'### {org[\"name\"]} {{#org-{org[\"key\"]}}}\\n\\n{org.get(\"description\") or default_description}'\n        for org in get_orgs_data()\n    ]\n    return re.sub(r'{{ *organisations *}}', '\\n\\n'.join(elements), markdown)\n\n\ndef _generate_table_row(col_values: list[str]) -> str:\n    return f'| {\" | \".join(col_values)} |\\n'\n\n\ndef _generate_table_heading(col_names: list[str]) -> str:\n    return _generate_table_row(col_names) + _generate_table_row(['-'] * len(col_names))\n\n\ndef build_schema_mappings(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'usage/schema.md':\n        return None\n\n    col_names = [\n        'Python type',\n        'JSON Schema Type',\n        'Additional JSON Schema',\n        'Defined in',\n        'Notes',\n    ]\n    table_text = _generate_table_heading(col_names)\n\n    with (THIS_DIR / 'schema_mappings.toml').open('rb') as f:\n        table = tomli.load(f)\n\n    for t in table.values():\n        py_type = t['py_type']\n        json_type = t['json_type']\n        additional = t['additional']\n        defined_in = t['defined_in']\n        notes = t['notes']\n        if additional and not isinstance(additional, str):\n            additional = json.dumps(additional)\n        cols = [f'`{py_type}`', f'`{json_type}`', f'`{additional}`' if additional else '', defined_in, notes]\n        table_text += _generate_table_row(cols)\n\n    return re.sub(r'{{ *schema_mappings_table *}}', table_text, markdown)\n\n\ndef build_conversion_table(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'concepts/conversion_table.md':\n        return None\n\n    filtered_table_predicates = {\n        'all': lambda r: True,\n        'json': lambda r: r.json_input,\n        'json_strict': lambda r: r.json_input and r.strict,\n        'python': lambda r: r.python_input,\n        'python_strict': lambda r: r.python_input and r.strict,\n    }\n\n    for table_id, predicate in filtered_table_predicates.items():\n        table_markdown = conversion_table.filtered(predicate).as_markdown()\n        table_markdown = textwrap.indent(table_markdown, '    ')\n        markdown = re.sub(rf'{{{{ *conversion_table_{table_id} *}}}}', table_markdown, markdown)\n\n    return markdown\n\n\ndef devtools_example(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'integrations/devtools.md':\n        return None\n\n    html = (THIS_DIR / 'devtools_output.html').read_text().strip('\\n')\n    full_html = f'<div class=\"highlight\">\\n<pre><code>{html}</code></pre>\\n</div>'\n    return re.sub(r'{{ *devtools_example *}}', full_html, markdown)\n\n\nexperts_template = Template(\n    \"\"\"\n<div class=\"user-list user-list-center\">\n    {% for user in people.experts %}\n    <div class=\"user\">\n        <a href=\"{{ user.url }}\" target=\"_blank\">\n            <div class=\"avatar-wrapper\">\n                <img src=\"{{ user.avatarUrl }}\"/>\n            </div>\n            <div class=\"title\">@{{ user.login }}</div>\n        </a>\n        <div class=\"count\">Questions replied: {{ user.count }}</div>\n    </div>\n    {% endfor %}\n</div>\n\"\"\"\n)\n\nmost_active_users_template = Template(\n    \"\"\"\n\n<div class=\"user-list user-list-center\">\n    {% for user in people.last_month_active %}\n    <div class=\"user\">\n        <a href=\"{{ user.url }}\" target=\"_blank\">\n            <div class=\"avatar-wrapper\">\n                <img src=\"{{ user.avatarUrl }}\"/>\n            </div>\n            <div class=\"title\">@{{ user.login }}</div>\n        </a>\n        <div class=\"count\">Questions replied: {{ user.count }}</div>\n    </div>\n    {% endfor %}\n</div>\n\"\"\"\n)\n\ntop_contributors_template = Template(\n    \"\"\"\n<div class=\"user-list user-list-center\">\n    {% for user in people.top_contributors %}\n    <div class=\"user\">\n        <a href=\"{{ user.url }}\" target=\"_blank\">\n            <div class=\"avatar-wrapper\">\n                <img src=\"{{ user.avatarUrl }}\"/>\n            </div>\n            <div class=\"title\">@{{ user.login }}</div>\n        </a>\n        <div class=\"count\">Contributions: {{ user.count }}</div>\n    </div>\n    {% endfor %}\n</div>\n\"\"\"\n)\n\ntop_reviewers_template = Template(\n    \"\"\"\n<div class=\"user-list user-list-center\">\n    {% for user in people.top_reviewers %}\n    <div class=\"user\">\n        <a href=\"{{ user.url }}\" target=\"_blank\">\n            <div class=\"avatar-wrapper\">\n                <img src=\"{{ user.avatarUrl }}\"/>\n            </div>\n            <div class=\"title\">@{{ user.login }}</div>\n        </a>\n        <div class=\"count\">Reviews: {{ user.count }}</div>\n    </div>\n    {% endfor %}\n</div>\n\"\"\"\n)\n\nmaintainers_template = Template(\n    \"\"\"\n<div class=\"user-list user-list-center\">\n    {% for user in people.maintainers %}\n    <div class=\"user\">\n        <a href=\"{{ user.url }}\" target=\"_blank\">\n            <div class=\"avatar-wrapper\">\n                <img src=\"{{ user.avatarUrl }}\"/>\n            </div>\n            <div class=\"title\">@{{ user.login }}</div>\n        </a>\n    </div>\n    {% endfor %}\n</div>\n\"\"\"\n)\n\n\ndef populate_pydantic_people(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'pydantic_people.md':\n        return None\n\n    # read people.yml file data\n    with (THIS_DIR / 'people.yml').open('rb') as f:\n        people = yaml.load(f, Loader=yaml.FullLoader)\n\n    # Render the templates\n    for name, template in [\n        ('experts', experts_template),\n        ('most_active_users', most_active_users_template),\n        ('top_contributors', top_contributors_template),\n        ('top_reviewers', top_reviewers_template),\n        ('maintainers', maintainers_template),\n    ]:\n        rendered = template.render(people=people)\n        markdown = re.sub(f'{{{{ {name} }}}}', rendered, markdown)\n\n    return markdown\n", "tests/test_validators.py": "import contextlib\nimport re\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom datetime import date, datetime\nfrom enum import Enum\nfrom functools import partial, partialmethod\nfrom itertools import product\nfrom os.path import normcase\nfrom typing import Any, Callable, Deque, Dict, FrozenSet, List, NamedTuple, Optional, Tuple, Union\nfrom unittest.mock import MagicMock\n\nimport pytest\nfrom dirty_equals import HasRepr, IsInstance\nfrom pydantic_core import core_schema\nfrom typing_extensions import Annotated, Literal, TypedDict\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    GetCoreSchemaHandler,\n    PlainSerializer,\n    PydanticDeprecatedSince20,\n    PydanticUserError,\n    TypeAdapter,\n    ValidationError,\n    ValidationInfo,\n    ValidatorFunctionWrapHandler,\n    errors,\n    field_validator,\n    model_validator,\n    root_validator,\n    validate_call,\n    validator,\n)\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\nfrom pydantic.functional_validators import AfterValidator, BeforeValidator, PlainValidator, WrapValidator\n\nV1_VALIDATOR_DEPRECATION_MATCH = r'Pydantic V1 style `@validator` validators are deprecated'\n\n\ndef test_annotated_validator_after() -> None:\n    MyInt = Annotated[int, AfterValidator(lambda x, _info: x if x != -1 else 0)]\n\n    class Model(BaseModel):\n        x: MyInt\n\n    assert Model(x=0).x == 0\n    assert Model(x=-1).x == 0\n    assert Model(x=-2).x == -2\n    assert Model(x=1).x == 1\n    assert Model(x='-1').x == 0\n\n\ndef test_annotated_validator_before() -> None:\n    FloatMaybeInf = Annotated[float, BeforeValidator(lambda x, _info: x if x != 'zero' else 0.0)]\n\n    class Model(BaseModel):\n        x: FloatMaybeInf\n\n    assert Model(x='zero').x == 0.0\n    assert Model(x=1.0).x == 1.0\n    assert Model(x='1.0').x == 1.0\n\n\ndef test_annotated_validator_builtin() -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/6752\"\"\"\n    TruncatedFloat = Annotated[float, BeforeValidator(int)]\n    DateTimeFromIsoFormat = Annotated[datetime, BeforeValidator(datetime.fromisoformat)]\n\n    class Model(BaseModel):\n        x: TruncatedFloat\n        y: DateTimeFromIsoFormat\n\n    m = Model(x=1.234, y='2011-11-04T00:05:23')\n    assert m.x == 1\n    assert m.y == datetime(2011, 11, 4, 0, 5, 23)\n\n\ndef test_annotated_validator_plain() -> None:\n    MyInt = Annotated[int, PlainValidator(lambda x, _info: x if x != -1 else 0)]\n\n    class Model(BaseModel):\n        x: MyInt\n\n    assert Model(x=0).x == 0\n    assert Model(x=-1).x == 0\n    assert Model(x=-2).x == -2\n\n\ndef test_annotated_validator_wrap() -> None:\n    def sixties_validator(val: Any, handler: ValidatorFunctionWrapHandler, info: ValidationInfo) -> date:\n        if val == 'epoch':\n            return date.fromtimestamp(0)\n        newval = handler(val)\n        if not date.fromisoformat('1960-01-01') <= newval < date.fromisoformat('1970-01-01'):\n            raise ValueError(f'{val} is not in the sixties!')\n        return newval\n\n    SixtiesDateTime = Annotated[date, WrapValidator(sixties_validator)]\n\n    class Model(BaseModel):\n        x: SixtiesDateTime\n\n    assert Model(x='epoch').x == date.fromtimestamp(0)\n    assert Model(x='1962-01-13').x == date(year=1962, month=1, day=13)\n    assert Model(x=datetime(year=1962, month=1, day=13)).x == date(year=1962, month=1, day=13)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=date(year=1970, month=4, day=17))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('1970-04-17 is not in the sixties!')))},\n            'input': date(1970, 4, 17),\n            'loc': ('x',),\n            'msg': 'Value error, 1970-04-17 is not in the sixties!',\n            'type': 'value_error',\n        }\n    ]\n\n\ndef test_annotated_validator_nested() -> None:\n    MyInt = Annotated[int, AfterValidator(lambda x: x if x != -1 else 0)]\n\n    def non_decreasing_list(data: List[int]) -> List[int]:\n        for prev, cur in zip(data, data[1:]):\n            assert cur >= prev\n        return data\n\n    class Model(BaseModel):\n        x: Annotated[List[MyInt], AfterValidator(non_decreasing_list)]\n\n    assert Model(x=[0, -1, 2]).x == [0, 0, 2]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=[0, -1, -2])\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(AssertionError('assert -2 >= 0')))},\n            'input': [0, -1, -2],\n            'loc': ('x',),\n            'msg': 'Assertion failed, assert -2 >= 0',\n            'type': 'assertion_error',\n        }\n    ]\n\n\ndef test_annotated_validator_runs_before_field_validators() -> None:\n    MyInt = Annotated[int, AfterValidator(lambda x: x if x != -1 else 0)]\n\n    class Model(BaseModel):\n        x: MyInt\n\n        @field_validator('x')\n        def val_x(cls, v: int) -> int:\n            assert v != -1\n            return v\n\n    assert Model(x=-1).x == 0\n\n\n@pytest.mark.parametrize(\n    'validator, func',\n    [\n        (PlainValidator, lambda x: x if x != -1 else 0),\n        (WrapValidator, lambda x, nxt: x if x != -1 else 0),\n        (BeforeValidator, lambda x: x if x != -1 else 0),\n        (AfterValidator, lambda x: x if x != -1 else 0),\n    ],\n)\ndef test_annotated_validator_typing_cache(validator, func):\n    FancyInt = Annotated[int, validator(func)]\n\n    class FancyIntModel(BaseModel):\n        x: Optional[FancyInt]\n\n    assert FancyIntModel(x=1234).x == 1234\n    assert FancyIntModel(x=-1).x == 0\n    assert FancyIntModel(x=0).x == 0\n\n\ndef test_simple():\n    class Model(BaseModel):\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def check_a(cls, v: Any):\n            if 'foobar' not in v:\n                raise ValueError('\"foobar\" not found in a')\n            return v\n\n    assert Model(a='this is foobar good').a == 'this is foobar good'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='snap')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('\"foobar\" not found in a')))},\n            'input': 'snap',\n            'loc': ('a',),\n            'msg': 'Value error, \"foobar\" not found in a',\n            'type': 'value_error',\n        }\n    ]\n\n\ndef test_int_validation():\n    class Model(BaseModel):\n        a: int\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='snap')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'snap',\n        }\n    ]\n    assert Model(a=3).a == 3\n    assert Model(a=True).a == 1\n    assert Model(a=False).a == 0\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=4.5)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_from_float',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, got a number with a fractional part',\n            'input': 4.5,\n        }\n    ]\n\n    # Doesn't raise ValidationError for number > (2 ^ 63) - 1\n    assert Model(a=(2**63) + 100).a == (2**63) + 100\n\n\n@pytest.mark.parametrize('value', [2.2250738585072011e308, float('nan'), float('inf')])\ndef test_int_overflow_validation(value):\n    class Model(BaseModel):\n        a: int\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=value)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'finite_number', 'loc': ('a',), 'msg': 'Input should be a finite number', 'input': value}\n    ]\n\n\ndef test_frozenset_validation():\n    class Model(BaseModel):\n        a: FrozenSet[int]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='snap')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_set_type', 'loc': ('a',), 'msg': 'Input should be a valid frozenset', 'input': 'snap'}\n    ]\n    assert Model(a={1, 2, 3}).a == frozenset({1, 2, 3})\n    assert Model(a=frozenset({1, 2, 3})).a == frozenset({1, 2, 3})\n    assert Model(a=[4, 5]).a == frozenset({4, 5})\n    assert Model(a=(6,)).a == frozenset({6})\n    assert Model(a={'1', '2', '3'}).a == frozenset({1, 2, 3})\n\n\ndef test_deque_validation():\n    class Model(BaseModel):\n        a: Deque[int]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='snap')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'list_type', 'loc': ('a',), 'msg': 'Input should be a valid list', 'input': 'snap'}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=['a'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=('a',))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        }\n    ]\n\n    assert Model(a={'1'}).a == deque([1])\n    assert Model(a=[4, 5]).a == deque([4, 5])\n    assert Model(a=(6,)).a == deque([6])\n\n\ndef test_validate_whole():\n    class Model(BaseModel):\n        a: List[int]\n\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a1(cls, v: List[Any]) -> List[Any]:\n            v.append('123')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def check_a2(cls, v: List[int]) -> List[Any]:\n            v.append(456)\n            return v\n\n    assert Model(a=[1, 2]).a == [1, 2, 123, 456]\n\n\ndef test_validate_pre_error():\n    calls = []\n\n    class Model(BaseModel):\n        a: List[int]\n\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a1(cls, v: Any):\n            calls.append(f'check_a1 {v}')\n            if 1 in v:\n                raise ValueError('a1 broken')\n            v[0] += 1\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def check_a2(cls, v: Any):\n            calls.append(f'check_a2 {v}')\n            if 10 in v:\n                raise ValueError('a2 broken')\n            return v\n\n    assert Model(a=[3, 8]).a == [4, 8]\n    assert calls == ['check_a1 [3, 8]', 'check_a2 [4, 8]']\n\n    calls = []\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=[1, 3])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('a1 broken')))},\n            'input': [1, 3],\n            'loc': ('a',),\n            'msg': 'Value error, a1 broken',\n            'type': 'value_error',\n        }\n    ]\n    assert calls == ['check_a1 [1, 3]']\n\n    calls = []\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=[5, 10])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('a2 broken')))},\n            'input': [6, 10],\n            'loc': ('a',),\n            'msg': 'Value error, a2 broken',\n            'type': 'value_error',\n        }\n    ]\n    assert calls == ['check_a1 [5, 10]', 'check_a2 [6, 10]']\n\n\n@pytest.fixture(scope='session', name='ValidateAssignmentModel')\ndef validate_assignment_model_fixture():\n    class ValidateAssignmentModel(BaseModel):\n        a: int = 4\n        b: str = ...\n        c: int = 0\n\n        @field_validator('b')\n        @classmethod\n        def b_length(cls, v, info):\n            values = info.data\n            if 'a' in values and len(v) < values['a']:\n                raise ValueError('b too short')\n            return v\n\n        @field_validator('c')\n        @classmethod\n        def double_c(cls, v: Any):\n            return v * 2\n\n        model_config = ConfigDict(validate_assignment=True, extra='allow')\n\n    return ValidateAssignmentModel\n\n\ndef test_validating_assignment_ok(ValidateAssignmentModel):\n    p = ValidateAssignmentModel(b='hello')\n    assert p.b == 'hello'\n\n\ndef test_validating_assignment_fail(ValidateAssignmentModel):\n    with pytest.raises(ValidationError):\n        ValidateAssignmentModel(a=10, b='hello')\n\n    p = ValidateAssignmentModel(b='hello')\n    with pytest.raises(ValidationError):\n        p.b = 'x'\n\n\ndef test_validating_assignment_value_change(ValidateAssignmentModel):\n    p = ValidateAssignmentModel(b='hello', c=2)\n    assert p.c == 4\n\n    p = ValidateAssignmentModel(b='hello')\n    assert p.c == 0\n    p.c = 3\n    assert p.c == 6\n    assert p.model_dump()['c'] == 6\n\n\ndef test_validating_assignment_extra(ValidateAssignmentModel):\n    p = ValidateAssignmentModel(b='hello', extra_field=1.23)\n    assert p.extra_field == 1.23\n\n    p = ValidateAssignmentModel(b='hello')\n    p.extra_field = 1.23\n    assert p.extra_field == 1.23\n    p.extra_field = 'bye'\n    assert p.extra_field == 'bye'\n    assert p.model_dump()['extra_field'] == 'bye'\n\n\ndef test_validating_assignment_dict(ValidateAssignmentModel):\n    with pytest.raises(ValidationError) as exc_info:\n        ValidateAssignmentModel(a='x', b='xx')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        }\n    ]\n\n\ndef test_validating_assignment_values_dict():\n    class ModelOne(BaseModel):\n        a: int\n\n    class ModelTwo(BaseModel):\n        m: ModelOne\n        b: int\n\n        @field_validator('b')\n        @classmethod\n        def validate_b(cls, b, info: ValidationInfo):\n            if 'm' in info.data:\n                return b + info.data['m'].a  # this fails if info.data['m'] is a dict\n            else:\n                return b\n\n        model_config = ConfigDict(validate_assignment=True)\n\n    model = ModelTwo(m=ModelOne(a=1), b=2)\n    assert model.b == 3\n    model.b = 3\n    assert model.b == 4\n\n\ndef test_validate_multiple():\n    class Model(BaseModel):\n        a: str\n        b: str\n\n        @field_validator('a', 'b')\n        @classmethod\n        def check_a_and_b(cls, v: Any, info: ValidationInfo) -> Any:\n            if len(v) < 4:\n                field = cls.model_fields[info.field_name]\n                raise AssertionError(f'{field.alias or info.field_name} is too short')\n            return v + 'x'\n\n    assert Model(a='1234', b='5678').model_dump() == {'a': '1234x', 'b': '5678x'}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='x', b='x')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(AssertionError('a is too short')))},\n            'input': 'x',\n            'loc': ('a',),\n            'msg': 'Assertion failed, a is too short',\n            'type': 'assertion_error',\n        },\n        {\n            'ctx': {'error': HasRepr(repr(AssertionError('b is too short')))},\n            'input': 'x',\n            'loc': ('b',),\n            'msg': 'Assertion failed, b is too short',\n            'type': 'assertion_error',\n        },\n    ]\n\n\ndef test_classmethod():\n    class Model(BaseModel):\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def check_a(cls, v: Any):\n            assert cls is Model\n            return v\n\n    m = Model(a='this is foobar good')\n    assert m.a == 'this is foobar good'\n    m.check_a('x')\n\n\ndef test_use_bare():\n    with pytest.raises(TypeError, match='`@validator` should be used with fields'):\n\n        class Model(BaseModel):\n            a: str\n\n            with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n                @validator\n                def checker(cls, v):\n                    return v\n\n\ndef test_use_bare_field_validator():\n    with pytest.raises(TypeError, match='`@field_validator` should be used with fields'):\n\n        class Model(BaseModel):\n            a: str\n\n            @field_validator\n            def checker(cls, v):\n                return v\n\n\ndef test_use_no_fields():\n    with pytest.raises(TypeError, match=re.escape(\"validator() missing 1 required positional argument: '__field'\")):\n\n        class Model(BaseModel):\n            a: str\n\n            @validator()\n            def checker(cls, v):\n                return v\n\n\ndef test_use_no_fields_field_validator():\n    with pytest.raises(TypeError, match=re.escape(\"field_validator() missing 1 required positional argument: 'field'\")):\n\n        class Model(BaseModel):\n            a: str\n\n            @field_validator()\n            def checker(cls, v):\n                return v\n\n\ndef test_validator_bad_fields_throws_configerror():\n    \"\"\"\n    Attempts to create a validator with fields set as a list of strings,\n    rather than just multiple string args. Expects ConfigError to be raised.\n    \"\"\"\n    with pytest.raises(TypeError, match='`@validator` fields should be passed as separate string args.'):\n\n        class Model(BaseModel):\n            a: str\n            b: str\n\n            with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n                @validator(['a', 'b'])\n                def check_fields(cls, v):\n                    return v\n\n\ndef test_field_validator_bad_fields_throws_configerror():\n    \"\"\"\n    Attempts to create a validator with fields set as a list of strings,\n    rather than just multiple string args. Expects ConfigError to be raised.\n    \"\"\"\n    with pytest.raises(TypeError, match='`@field_validator` fields should be passed as separate string args.'):\n\n        class Model(BaseModel):\n            a: str\n            b: str\n\n            @field_validator(['a', 'b'])\n            def check_fields(cls, v):\n                return v\n\n\ndef test_validate_always():\n    check_calls = 0\n\n    class Model(BaseModel):\n        a: str = None\n\n        with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n            @validator('a', pre=True, always=True)\n            @classmethod\n            def check_a(cls, v: Any):\n                nonlocal check_calls\n                check_calls += 1\n                return v or 'xxx'\n\n    assert Model().a == 'xxx'\n    assert check_calls == 1\n    assert Model(a='y').a == 'y'\n    assert check_calls == 2\n\n\ndef test_field_validator_validate_default():\n    check_calls = 0\n\n    class Model(BaseModel):\n        a: str = Field(None, validate_default=True)\n\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a(cls, v: Any):\n            nonlocal check_calls\n            check_calls += 1\n            return v or 'xxx'\n\n    assert Model().a == 'xxx'\n    assert check_calls == 1\n    assert Model(a='y').a == 'y'\n    assert check_calls == 2\n\n\ndef test_validate_always_on_inheritance():\n    check_calls = 0\n\n    class ParentModel(BaseModel):\n        a: str = None\n\n    class Model(ParentModel):\n        with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n            @validator('a', pre=True, always=True)\n            @classmethod\n            def check_a(cls, v: Any):\n                nonlocal check_calls\n                check_calls += 1\n                return v or 'xxx'\n\n    assert Model().a == 'xxx'\n    assert check_calls == 1\n    assert Model(a='y').a == 'y'\n    assert check_calls == 2\n\n\ndef test_field_validator_validate_default_on_inheritance():\n    check_calls = 0\n\n    class ParentModel(BaseModel):\n        a: str = Field(None, validate_default=True)\n\n    class Model(ParentModel):\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a(cls, v: Any):\n            nonlocal check_calls\n            check_calls += 1\n            return v or 'xxx'\n\n    assert Model().a == 'xxx'\n    assert check_calls == 1\n    assert Model(a='y').a == 'y'\n    assert check_calls == 2\n\n\ndef test_validate_not_always():\n    check_calls = 0\n\n    class Model(BaseModel):\n        a: Optional[str] = None\n\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a(cls, v: Any):\n            nonlocal check_calls\n            check_calls += 1\n            return v or 'xxx'\n\n    assert Model().a is None\n    assert check_calls == 0\n    assert Model(a='y').a == 'y'\n    assert check_calls == 1\n\n\n@pytest.mark.parametrize(\n    'decorator, pytest_warns',\n    [\n        (validator, pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH)),\n        (field_validator, contextlib.nullcontext()),\n    ],\n)\ndef test_wildcard_validators(decorator, pytest_warns):\n    calls: list[tuple[str, Any]] = []\n\n    with pytest_warns:\n\n        class Model(BaseModel):\n            a: str\n            b: int\n\n            @decorator('a')\n            def check_a(cls, v: Any) -> Any:\n                calls.append(('check_a', v))\n                return v\n\n            @decorator('*')\n            def check_all(cls, v: Any) -> Any:\n                calls.append(('check_all', v))\n                return v\n\n            @decorator('*', 'a')\n            def check_all_a(cls, v: Any) -> Any:\n                calls.append(('check_all_a', v))\n                return v\n\n    assert Model(a='abc', b='123').model_dump() == dict(a='abc', b=123)\n    assert calls == [\n        ('check_a', 'abc'),\n        ('check_all', 'abc'),\n        ('check_all_a', 'abc'),\n        ('check_all', 123),\n        ('check_all_a', 123),\n    ]\n\n\n@pytest.mark.parametrize(\n    'decorator, pytest_warns',\n    [\n        (validator, pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH)),\n        (field_validator, contextlib.nullcontext()),\n    ],\n)\ndef test_wildcard_validator_error(decorator, pytest_warns):\n    with pytest_warns:\n\n        class Model(BaseModel):\n            a: str\n            b: str\n\n            @decorator('*')\n            def check_all(cls, v: Any) -> Any:\n                if 'foobar' not in v:\n                    raise ValueError('\"foobar\" not found in a')\n                return v\n\n    assert Model(a='foobar a', b='foobar b').b == 'foobar b'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='snap')\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('\"foobar\" not found in a')))},\n            'input': 'snap',\n            'loc': ('a',),\n            'msg': 'Value error, \"foobar\" not found in a',\n            'type': 'value_error',\n        },\n        {'type': 'missing', 'loc': ('b',), 'msg': 'Field required', 'input': {'a': 'snap'}},\n    ]\n\n\ndef test_invalid_field():\n    msg = (\n        r'Decorators defined with incorrect fields:'\n        r' tests.test_validators.test_invalid_field.<locals>.Model:\\d+.check_b'\n        r\" \\(use check_fields=False if you're inheriting from the model and intended this\\)\"\n    )\n    with pytest.raises(errors.PydanticUserError, match=msg):\n\n        class Model(BaseModel):\n            a: str\n\n            @field_validator('b')\n            def check_b(cls, v: Any):\n                return v\n\n\ndef test_validate_child():\n    class Parent(BaseModel):\n        a: str\n\n    class Child(Parent):\n        @field_validator('a')\n        @classmethod\n        def check_a(cls, v: Any):\n            if 'foobar' not in v:\n                raise ValueError('\"foobar\" not found in a')\n            return v\n\n    assert Parent(a='this is not a child').a == 'this is not a child'\n    assert Child(a='this is foobar good').a == 'this is foobar good'\n    with pytest.raises(ValidationError):\n        Child(a='snap')\n\n\ndef test_validate_child_extra():\n    class Parent(BaseModel):\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def check_a_one(cls, v: Any):\n            if 'foobar' not in v:\n                raise ValueError('\"foobar\" not found in a')\n            return v\n\n    class Child(Parent):\n        @field_validator('a')\n        @classmethod\n        def check_a_two(cls, v: Any):\n            return v.upper()\n\n    assert Parent(a='this is foobar good').a == 'this is foobar good'\n    assert Child(a='this is foobar good').a == 'THIS IS FOOBAR GOOD'\n    with pytest.raises(ValidationError):\n        Child(a='snap')\n\n\ndef test_validate_all():\n    class MyModel(BaseModel):\n        x: int\n\n        @field_validator('*')\n        @classmethod\n        def validate_all(cls, v: Any):\n            return v * 2\n\n    assert MyModel(x=10).x == 20\n\n\ndef test_validate_child_all():\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Parent(BaseModel):\n            a: str\n\n        class Child(Parent):\n            @validator('*')\n            @classmethod\n            def check_a(cls, v: Any):\n                if 'foobar' not in v:\n                    raise ValueError('\"foobar\" not found in a')\n                return v\n\n        assert Parent(a='this is not a child').a == 'this is not a child'\n        assert Child(a='this is foobar good').a == 'this is foobar good'\n        with pytest.raises(ValidationError):\n            Child(a='snap')\n\n    class Parent(BaseModel):\n        a: str\n\n    class Child(Parent):\n        @field_validator('*')\n        @classmethod\n        def check_a(cls, v: Any):\n            if 'foobar' not in v:\n                raise ValueError('\"foobar\" not found in a')\n            return v\n\n    assert Parent(a='this is not a child').a == 'this is not a child'\n    assert Child(a='this is foobar good').a == 'this is foobar good'\n    with pytest.raises(ValidationError):\n        Child(a='snap')\n\n\ndef test_validate_parent():\n    class Parent(BaseModel):\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def check_a(cls, v: Any):\n            if 'foobar' not in v:\n                raise ValueError('\"foobar\" not found in a')\n            return v\n\n    class Child(Parent):\n        pass\n\n    assert Parent(a='this is foobar good').a == 'this is foobar good'\n    assert Child(a='this is foobar good').a == 'this is foobar good'\n    with pytest.raises(ValidationError):\n        Parent(a='snap')\n    with pytest.raises(ValidationError):\n        Child(a='snap')\n\n\ndef test_validate_parent_all():\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Parent(BaseModel):\n            a: str\n\n            @validator('*')\n            @classmethod\n            def check_a(cls, v: Any):\n                if 'foobar' not in v:\n                    raise ValueError('\"foobar\" not found in a')\n                return v\n\n        class Child(Parent):\n            pass\n\n        assert Parent(a='this is foobar good').a == 'this is foobar good'\n        assert Child(a='this is foobar good').a == 'this is foobar good'\n        with pytest.raises(ValidationError):\n            Parent(a='snap')\n        with pytest.raises(ValidationError):\n            Child(a='snap')\n\n    class Parent(BaseModel):\n        a: str\n\n        @field_validator('*')\n        @classmethod\n        def check_a(cls, v: Any):\n            if 'foobar' not in v:\n                raise ValueError('\"foobar\" not found in a')\n            return v\n\n    class Child(Parent):\n        pass\n\n    assert Parent(a='this is foobar good').a == 'this is foobar good'\n    assert Child(a='this is foobar good').a == 'this is foobar good'\n    with pytest.raises(ValidationError):\n        Parent(a='snap')\n    with pytest.raises(ValidationError):\n        Child(a='snap')\n\n\ndef test_inheritance_keep():\n    class Parent(BaseModel):\n        a: int\n\n        @field_validator('a')\n        @classmethod\n        def add_to_a(cls, v: Any):\n            return v + 1\n\n    class Child(Parent):\n        pass\n\n    assert Child(a=0).a == 1\n\n\ndef test_inheritance_replace():\n    \"\"\"We promise that if you add a validator\n    with the same _function_ name as an existing validator\n    it replaces the existing validator and is run instead of it.\n    \"\"\"\n\n    class Parent(BaseModel):\n        a: List[str]\n\n        @field_validator('a')\n        @classmethod\n        def parent_val_before(cls, v: List[str]):\n            v.append('parent before')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def val(cls, v: List[str]):\n            v.append('parent')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def parent_val_after(cls, v: List[str]):\n            v.append('parent after')\n            return v\n\n    class Child(Parent):\n        @field_validator('a')\n        @classmethod\n        def child_val_before(cls, v: List[str]):\n            v.append('child before')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def val(cls, v: List[str]):\n            v.append('child')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def child_val_after(cls, v: List[str]):\n            v.append('child after')\n            return v\n\n    assert Parent(a=[]).a == ['parent before', 'parent', 'parent after']\n    assert Child(a=[]).a == ['parent before', 'child', 'parent after', 'child before', 'child after']\n\n\ndef test_inheritance_replace_root_validator():\n    \"\"\"\n    We promise that if you add a validator\n    with the same _function_ name as an existing validator\n    it replaces the existing validator and is run instead of it.\n    \"\"\"\n\n    with pytest.warns(PydanticDeprecatedSince20):\n\n        class Parent(BaseModel):\n            a: List[str]\n\n            @root_validator(skip_on_failure=True)\n            def parent_val_before(cls, values: Dict[str, Any]):\n                values['a'].append('parent before')\n                return values\n\n            @root_validator(skip_on_failure=True)\n            def val(cls, values: Dict[str, Any]):\n                values['a'].append('parent')\n                return values\n\n            @root_validator(skip_on_failure=True)\n            def parent_val_after(cls, values: Dict[str, Any]):\n                values['a'].append('parent after')\n                return values\n\n        class Child(Parent):\n            @root_validator(skip_on_failure=True)\n            def child_val_before(cls, values: Dict[str, Any]):\n                values['a'].append('child before')\n                return values\n\n            @root_validator(skip_on_failure=True)\n            def val(cls, values: Dict[str, Any]):\n                values['a'].append('child')\n                return values\n\n            @root_validator(skip_on_failure=True)\n            def child_val_after(cls, values: Dict[str, Any]):\n                values['a'].append('child after')\n                return values\n\n    assert Parent(a=[]).a == ['parent before', 'parent', 'parent after']\n    assert Child(a=[]).a == ['parent before', 'child', 'parent after', 'child before', 'child after']\n\n\ndef test_validation_each_item():\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            foobar: Dict[int, int]\n\n            @validator('foobar', each_item=True)\n            @classmethod\n            def check_foobar(cls, v: Any):\n                return v + 1\n\n    assert Model(foobar={1: 1}).foobar == {1: 2}\n\n\ndef test_validation_each_item_invalid_type():\n    with pytest.raises(\n        TypeError, match=re.escape('@validator(..., each_item=True)` cannot be applied to fields with a schema of int')\n    ):\n        with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n            class Model(BaseModel):\n                foobar: int\n\n                @validator('foobar', each_item=True)\n                @classmethod\n                def check_foobar(cls, v: Any): ...\n\n\ndef test_validation_each_item_nullable():\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            foobar: Optional[List[int]]\n\n            @validator('foobar', each_item=True)\n            @classmethod\n            def check_foobar(cls, v: Any):\n                return v + 1\n\n    assert Model(foobar=[1]).foobar == [2]\n\n\ndef test_validation_each_item_one_sublevel():\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            foobar: List[Tuple[int, int]]\n\n            @validator('foobar', each_item=True)\n            @classmethod\n            def check_foobar(cls, v: Tuple[int, int]) -> Tuple[int, int]:\n                v1, v2 = v\n                assert v1 == v2\n                return v\n\n    assert Model(foobar=[(1, 1), (2, 2)]).foobar == [(1, 1), (2, 2)]\n\n\ndef test_key_validation():\n    class Model(BaseModel):\n        foobar: Dict[int, int]\n\n        @field_validator('foobar')\n        @classmethod\n        def check_foobar(cls, value):\n            return {k + 1: v + 1 for k, v in value.items()}\n\n    assert Model(foobar={1: 1}).foobar == {2: 2}\n\n\ndef test_validator_always_optional():\n    check_calls = 0\n\n    class Model(BaseModel):\n        a: Optional[str] = None\n\n        with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n            @validator('a', pre=True, always=True)\n            @classmethod\n            def check_a(cls, v: Any):\n                nonlocal check_calls\n                check_calls += 1\n                return v or 'default value'\n\n    assert Model(a='y').a == 'y'\n    assert check_calls == 1\n    assert Model().a == 'default value'\n    assert check_calls == 2\n\n\ndef test_field_validator_validate_default_optional():\n    check_calls = 0\n\n    class Model(BaseModel):\n        a: Optional[str] = Field(None, validate_default=True)\n\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a(cls, v: Any):\n            nonlocal check_calls\n            check_calls += 1\n            return v or 'default value'\n\n    assert Model(a='y').a == 'y'\n    assert check_calls == 1\n    assert Model().a == 'default value'\n    assert check_calls == 2\n\n\ndef test_validator_always_pre():\n    check_calls = 0\n\n    class Model(BaseModel):\n        a: str = None\n\n        with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n            @validator('a', pre=True, always=True)\n            @classmethod\n            def check_a(cls, v: Any):\n                nonlocal check_calls\n                check_calls += 1\n                return v or 'default value'\n\n    assert Model(a='y').a == 'y'\n    assert Model().a == 'default value'\n    assert check_calls == 2\n\n\ndef test_field_validator_validate_default_pre():\n    check_calls = 0\n\n    class Model(BaseModel):\n        a: str = Field(None, validate_default=True)\n\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a(cls, v: Any):\n            nonlocal check_calls\n            check_calls += 1\n            return v or 'default value'\n\n    assert Model(a='y').a == 'y'\n    assert Model().a == 'default value'\n    assert check_calls == 2\n\n\ndef test_validator_always_post():\n    class Model(BaseModel):\n        # NOTE: Unlike in v1, you can't replicate the behavior of only applying defined validators and not standard\n        # field validation. This is why I've set the default to '' instead of None.\n        # But, I think this is a good thing, and I don't think we should try to support this.\n        a: str = ''\n\n        with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n            @validator('a', always=True)\n            @classmethod\n            def check_a(cls, v: Any):\n                return v or 'default value'\n\n    assert Model(a='y').a == 'y'\n    assert Model().a == 'default value'\n\n\ndef test_field_validator_validate_default_post():\n    class Model(BaseModel):\n        a: str = Field('', validate_default=True)\n\n        @field_validator('a')\n        @classmethod\n        def check_a(cls, v: Any):\n            return v or 'default value'\n\n    assert Model(a='y').a == 'y'\n    assert Model().a == 'default value'\n\n\ndef test_validator_always_post_optional():\n    class Model(BaseModel):\n        a: Optional[str] = None\n\n        with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n            @validator('a', pre=True, always=True)\n            @classmethod\n            def check_a(cls, v: Any):\n                return 'default value' if v is None else v\n\n    assert Model(a='y').a == 'y'\n    assert Model().a == 'default value'\n\n\ndef test_field_validator_validate_default_post_optional():\n    class Model(BaseModel):\n        a: Optional[str] = Field(None, validate_default=True)\n\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a(cls, v: Any):\n            return v or 'default value'\n\n    assert Model(a='y').a == 'y'\n    assert Model().a == 'default value'\n\n\ndef test_datetime_validator():\n    check_calls = 0\n\n    class Model(BaseModel):\n        d: datetime = None\n\n        with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n            @validator('d', pre=True, always=True)\n            @classmethod\n            def check_d(cls, v: Any):\n                nonlocal check_calls\n                check_calls += 1\n                return v or datetime(2032, 1, 1)\n\n    assert Model(d='2023-01-01T00:00:00').d == datetime(2023, 1, 1)\n    assert check_calls == 1\n    assert Model().d == datetime(2032, 1, 1)\n    assert check_calls == 2\n    assert Model(d=datetime(2023, 1, 1)).d == datetime(2023, 1, 1)\n    assert check_calls == 3\n\n\ndef test_datetime_field_validator():\n    check_calls = 0\n\n    class Model(BaseModel):\n        d: datetime = Field(None, validate_default=True)\n\n        @field_validator('d', mode='before')\n        @classmethod\n        def check_d(cls, v: Any):\n            nonlocal check_calls\n            check_calls += 1\n            return v or datetime(2032, 1, 1)\n\n    assert Model(d='2023-01-01T00:00:00').d == datetime(2023, 1, 1)\n    assert check_calls == 1\n    assert Model().d == datetime(2032, 1, 1)\n    assert check_calls == 2\n    assert Model(d=datetime(2023, 1, 1)).d == datetime(2023, 1, 1)\n    assert check_calls == 3\n\n\ndef test_pre_called_once():\n    check_calls = 0\n\n    class Model(BaseModel):\n        a: Tuple[int, int, int]\n\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a(cls, v: Any):\n            nonlocal check_calls\n            check_calls += 1\n            return v\n\n    assert Model(a=['1', '2', '3']).a == (1, 2, 3)\n    assert check_calls == 1\n\n\ndef test_assert_raises_validation_error():\n    class Model(BaseModel):\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def check_a(cls, v: Any):\n            if v != 'a':\n                raise AssertionError('invalid a')\n            return v\n\n    Model(a='a')\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='snap')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(AssertionError('invalid a')))},\n            'input': 'snap',\n            'loc': ('a',),\n            'msg': 'Assertion failed, invalid a',\n            'type': 'assertion_error',\n        }\n    ]\n\n\ndef test_root_validator():\n    root_val_values: List[Dict[str, Any]] = []\n\n    class Model(BaseModel):\n        a: int = 1\n        b: str\n        c: str\n\n        @field_validator('b')\n        @classmethod\n        def repeat_b(cls, v: Any):\n            return v * 2\n\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            @root_validator(skip_on_failure=True)\n            def example_root_validator(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n                root_val_values.append(values)\n                if 'snap' in values.get('b', ''):\n                    raise ValueError('foobar')\n                return dict(values, b='changed')\n\n            @root_validator(skip_on_failure=True)\n            def example_root_validator2(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n                root_val_values.append(values)\n                if 'snap' in values.get('c', ''):\n                    raise ValueError('foobar2')\n                return dict(values, c='changed')\n\n    assert Model(a='123', b='bar', c='baz').model_dump() == {'a': 123, 'b': 'changed', 'c': 'changed'}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(b='snap dragon', c='snap dragon2')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('foobar')))},\n            'input': {'b': 'snap dragon', 'c': 'snap dragon2'},\n            'loc': (),\n            'msg': 'Value error, foobar',\n            'type': 'value_error',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='broken', b='bar', c='baz')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'broken',\n        }\n    ]\n\n    assert root_val_values == [\n        {'a': 123, 'b': 'barbar', 'c': 'baz'},\n        {'a': 123, 'b': 'changed', 'c': 'baz'},\n        {'a': 1, 'b': 'snap dragonsnap dragon', 'c': 'snap dragon2'},\n    ]\n\n\ndef test_root_validator_subclass():\n    \"\"\"\n    https://github.com/pydantic/pydantic/issues/5388\n    \"\"\"\n\n    class Parent(BaseModel):\n        x: int\n        expected: Any\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            @root_validator(skip_on_failure=True)\n            @classmethod\n            def root_val(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n                assert cls is values['expected']\n                return values\n\n    class Child1(Parent):\n        pass\n\n    class Child2(Parent):\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            @root_validator(skip_on_failure=True)\n            @classmethod\n            def root_val(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n                assert cls is Child2\n                values['x'] = values['x'] * 2\n                return values\n\n    class Child3(Parent):\n        @classmethod\n        def root_val(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n            assert cls is Child3\n            values['x'] = values['x'] * 3\n            return values\n\n    assert Parent(x=1, expected=Parent).x == 1\n    assert Child1(x=1, expected=Child1).x == 1\n    assert Child2(x=1, expected=Child2).x == 2\n    assert Child3(x=1, expected=Child3).x == 3\n\n\ndef test_root_validator_pre():\n    root_val_values: List[Dict[str, Any]] = []\n\n    class Model(BaseModel):\n        a: int = 1\n        b: str\n\n        @field_validator('b')\n        @classmethod\n        def repeat_b(cls, v: Any):\n            return v * 2\n\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            @root_validator(pre=True)\n            def root_validator(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n                root_val_values.append(values)\n                if 'snap' in values.get('b', ''):\n                    raise ValueError('foobar')\n                return {'a': 42, 'b': 'changed'}\n\n    assert Model(a='123', b='bar').model_dump() == {'a': 42, 'b': 'changedchanged'}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(b='snap dragon')\n\n    assert root_val_values == [{'a': '123', 'b': 'bar'}, {'b': 'snap dragon'}]\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('foobar')))},\n            'input': {'b': 'snap dragon'},\n            'loc': (),\n            'msg': 'Value error, foobar',\n            'type': 'value_error',\n        }\n    ]\n\n\ndef test_root_validator_types():\n    root_val_values = None\n\n    class Model(BaseModel):\n        a: int = 1\n        b: str\n\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            @root_validator(skip_on_failure=True)\n            def root_validator(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n                nonlocal root_val_values\n                root_val_values = cls, repr(values)\n                return values\n\n        model_config = ConfigDict(extra='allow')\n\n    assert Model(b='bar', c='wobble').model_dump() == {'a': 1, 'b': 'bar', 'c': 'wobble'}\n\n    assert root_val_values == (Model, \"{'a': 1, 'b': 'bar', 'c': 'wobble'}\")\n\n\ndef test_root_validator_returns_none_exception():\n    class Model(BaseModel):\n        a: int = 1\n\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            @root_validator(skip_on_failure=True)\n            def root_validator_repeated(cls, values):\n                return None\n\n    with pytest.raises(\n        TypeError,\n        match=r\"(:?__dict__ must be set to a dictionary, not a 'NoneType')|(:?setting dictionary to a non-dict)\",\n    ):\n        Model()\n\n\ndef test_model_validator_returns_ignore():\n    # This is weird, and I don't understand entirely why it's happening, but it kind of makes sense\n\n    class Model(BaseModel):\n        a: int = 1\n\n        @model_validator(mode='after')  # type: ignore\n        def model_validator_return_none(self) -> None:\n            return None\n\n    m = Model(a=2)\n    assert m.model_dump() == {'a': 2}\n\n\ndef reusable_validator(num: int) -> int:\n    return num * 2\n\n\ndef test_reuse_global_validators():\n    class Model(BaseModel):\n        x: int\n        y: int\n\n        double_x = field_validator('x')(reusable_validator)\n        double_y = field_validator('y')(reusable_validator)\n\n    assert dict(Model(x=1, y=1)) == {'x': 2, 'y': 2}\n\n\n@pytest.mark.parametrize('validator_classmethod,root_validator_classmethod', product(*[[True, False]] * 2))\ndef test_root_validator_classmethod(validator_classmethod, root_validator_classmethod):\n    root_val_values = []\n\n    class Model(BaseModel):\n        a: int = 1\n        b: str\n\n        def repeat_b(cls, v: Any):\n            return v * 2\n\n        if validator_classmethod:\n            repeat_b = classmethod(repeat_b)\n        repeat_b = field_validator('b')(repeat_b)\n\n        def example_root_validator(cls, values):\n            root_val_values.append(values)\n            if 'snap' in values.get('b', ''):\n                raise ValueError('foobar')\n            return dict(values, b='changed')\n\n        if root_validator_classmethod:\n            example_root_validator = classmethod(example_root_validator)\n\n        with pytest.warns(PydanticDeprecatedSince20):\n            example_root_validator = root_validator(skip_on_failure=True)(example_root_validator)\n\n    assert Model(a='123', b='bar').model_dump() == {'a': 123, 'b': 'changed'}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(b='snap dragon')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('foobar')))},\n            'input': {'b': 'snap dragon'},\n            'loc': (),\n            'msg': 'Value error, foobar',\n            'type': 'value_error',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='broken', b='bar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'broken',\n        }\n    ]\n\n    assert root_val_values == [{'a': 123, 'b': 'barbar'}, {'a': 1, 'b': 'snap dragonsnap dragon'}]\n\n\ndef test_assignment_validator_cls():\n    validator_calls = 0\n\n    class Model(BaseModel):\n        name: str\n\n        model_config = ConfigDict(validate_assignment=True)\n\n        @field_validator('name')\n        @classmethod\n        def check_foo(cls, value):\n            nonlocal validator_calls\n            validator_calls += 1\n            assert cls == Model\n            return value\n\n    m = Model(name='hello')\n    m.name = 'goodbye'\n    assert validator_calls == 2\n\n\ndef test_literal_validator():\n    class Model(BaseModel):\n        a: Literal['foo']\n\n    Model(a='foo')\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='nope')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': ('a',),\n            'msg': \"Input should be 'foo'\",\n            'input': 'nope',\n            'ctx': {'expected': \"'foo'\"},\n        }\n    ]\n\n\ndef test_literal_validator_str_enum():\n    class Bar(str, Enum):\n        FIZ = 'fiz'\n        FUZ = 'fuz'\n\n    class Foo(BaseModel):\n        bar: Bar\n        barfiz: Literal[Bar.FIZ]\n        fizfuz: Literal[Bar.FIZ, Bar.FUZ]\n\n    my_foo = Foo.model_validate({'bar': 'fiz', 'barfiz': 'fiz', 'fizfuz': 'fiz'})\n    assert my_foo.bar is Bar.FIZ\n    assert my_foo.barfiz is Bar.FIZ\n    assert my_foo.fizfuz is Bar.FIZ\n\n    my_foo = Foo.model_validate({'bar': 'fiz', 'barfiz': 'fiz', 'fizfuz': 'fuz'})\n    assert my_foo.bar is Bar.FIZ\n    assert my_foo.barfiz is Bar.FIZ\n    assert my_foo.fizfuz is Bar.FUZ\n\n\ndef test_nested_literal_validator():\n    L1 = Literal['foo']\n    L2 = Literal['bar']\n\n    class Model(BaseModel):\n        a: Literal[L1, L2]\n\n    Model(a='foo')\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='nope')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': ('a',),\n            'msg': \"Input should be 'foo' or 'bar'\",\n            'input': 'nope',\n            'ctx': {'expected': \"'foo' or 'bar'\"},\n        }\n    ]\n\n\ndef test_union_literal_with_constraints():\n    class Model(BaseModel, validate_assignment=True):\n        x: Union[Literal[42], Literal['pika']] = Field(frozen=True)\n\n    m = Model(x=42)\n    with pytest.raises(ValidationError) as exc_info:\n        m.x += 1\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 43, 'loc': ('x',), 'msg': 'Field is frozen', 'type': 'frozen_field'}\n    ]\n\n\ndef test_field_that_is_being_validated_is_excluded_from_validator_values():\n    check_values = MagicMock()\n\n    class Model(BaseModel):\n        foo: str\n        bar: str = Field(alias='pika')\n        baz: str\n\n        model_config = ConfigDict(validate_assignment=True)\n\n        @field_validator('foo')\n        @classmethod\n        def validate_foo(cls, v: Any, info: ValidationInfo) -> Any:\n            check_values({**info.data})\n            return v\n\n        @field_validator('bar')\n        @classmethod\n        def validate_bar(cls, v: Any, info: ValidationInfo) -> Any:\n            check_values({**info.data})\n            return v\n\n    model = Model(foo='foo_value', pika='bar_value', baz='baz_value')\n    check_values.reset_mock()\n\n    assert list(dict(model).items()) == [('foo', 'foo_value'), ('bar', 'bar_value'), ('baz', 'baz_value')]\n\n    model.foo = 'new_foo_value'\n    check_values.assert_called_once_with({'bar': 'bar_value', 'baz': 'baz_value'})\n    check_values.reset_mock()\n\n    model.bar = 'new_bar_value'\n    check_values.assert_called_once_with({'foo': 'new_foo_value', 'baz': 'baz_value'})\n\n    # ensure field order is the same\n    assert list(dict(model).items()) == [('foo', 'new_foo_value'), ('bar', 'new_bar_value'), ('baz', 'baz_value')]\n\n\ndef test_exceptions_in_field_validators_restore_original_field_value():\n    class Model(BaseModel):\n        foo: str\n\n        model_config = ConfigDict(validate_assignment=True)\n\n        @field_validator('foo')\n        @classmethod\n        def validate_foo(cls, v: Any):\n            if v == 'raise_exception':\n                raise RuntimeError('test error')\n            return v\n\n    model = Model(foo='foo')\n    with pytest.raises(RuntimeError, match='test error'):\n        model.foo = 'raise_exception'\n    assert model.foo == 'foo'\n\n\ndef test_overridden_root_validators():\n    validate_stub = MagicMock()\n\n    class A(BaseModel):\n        x: str\n\n        @model_validator(mode='before')\n        @classmethod\n        def pre_root(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n            validate_stub('A', 'pre')\n            return values\n\n        @model_validator(mode='after')\n        def post_root(self) -> 'A':\n            validate_stub('A', 'post')\n            return self\n\n    class B(A):\n        @model_validator(mode='before')\n        @classmethod\n        def pre_root(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n            validate_stub('B', 'pre')\n            return values\n\n        @model_validator(mode='after')\n        def post_root(self) -> 'B':\n            validate_stub('B', 'post')\n            return self\n\n    A(x='pika')\n    assert validate_stub.call_args_list == [[('A', 'pre'), {}], [('A', 'post'), {}]]\n\n    validate_stub.reset_mock()\n\n    B(x='pika')\n    assert validate_stub.call_args_list == [[('B', 'pre'), {}], [('B', 'post'), {}]]\n\n\ndef test_validating_assignment_pre_root_validator_fail():\n    class Model(BaseModel):\n        current_value: float = Field(..., alias='current')\n        max_value: float\n\n        model_config = ConfigDict(validate_assignment=True)\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            @root_validator(pre=True)\n            def values_are_not_string(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n                if any(isinstance(x, str) for x in values.values()):\n                    raise ValueError('values cannot be a string')\n                return values\n\n    m = Model(current=100, max_value=200)\n    with pytest.raises(ValidationError) as exc_info:\n        m.current_value = '100'\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('values cannot be a string')))},\n            'input': {'current_value': '100', 'max_value': 200.0},\n            'loc': (),\n            'msg': 'Value error, values cannot be a string',\n            'type': 'value_error',\n        }\n    ]\n\n\ndef test_validating_assignment_model_validator_before_fail():\n    class Model(BaseModel):\n        current_value: float = Field(..., alias='current')\n        max_value: float\n\n        model_config = ConfigDict(validate_assignment=True)\n\n        @model_validator(mode='before')\n        @classmethod\n        def values_are_not_string(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n            assert isinstance(values, dict)\n            if any(isinstance(x, str) for x in values.values()):\n                raise ValueError('values cannot be a string')\n            return values\n\n    m = Model(current=100, max_value=200)\n    with pytest.raises(ValidationError) as exc_info:\n        m.current_value = '100'\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('values cannot be a string')))},\n            'input': {'current_value': '100', 'max_value': 200.0},\n            'loc': (),\n            'msg': 'Value error, values cannot be a string',\n            'type': 'value_error',\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    'kwargs',\n    [\n        {'skip_on_failure': False},\n        {'skip_on_failure': False, 'pre': False},\n        {'pre': False},\n    ],\n)\ndef test_root_validator_skip_on_failure_invalid(kwargs: Dict[str, Any]):\n    with pytest.raises(TypeError, match='MUST specify `skip_on_failure=True`'):\n        with pytest.warns(\n            PydanticDeprecatedSince20, match='Pydantic V1 style `@root_validator` validators are deprecated.'\n        ):\n\n            class Model(BaseModel):\n                @root_validator(**kwargs)\n                def root_val(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n                    return values\n\n\n@pytest.mark.parametrize(\n    'kwargs',\n    [\n        {'skip_on_failure': True},\n        {'skip_on_failure': True, 'pre': False},\n        {'skip_on_failure': False, 'pre': True},\n        {'pre': True},\n    ],\n)\ndef test_root_validator_skip_on_failure_valid(kwargs: Dict[str, Any]):\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='Pydantic V1 style `@root_validator` validators are deprecated.'\n    ):\n\n        class Model(BaseModel):\n            @root_validator(**kwargs)\n            def root_val(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n                return values\n\n\ndef test_model_validator_many_values_change():\n    \"\"\"It should run root_validator on assignment and update ALL concerned fields\"\"\"\n\n    class Rectangle(BaseModel):\n        width: float\n        height: float\n        area: Optional[float] = None\n\n        model_config = ConfigDict(validate_assignment=True)\n\n        @model_validator(mode='after')\n        def set_area(self) -> 'Rectangle':\n            self.__dict__['area'] = self.width * self.height\n            return self\n\n    r = Rectangle(width=1, height=1)\n    assert r.area == 1\n    r.height = 5\n    assert r.area == 5\n\n\ndef _get_source_line(filename: str, lineno: int) -> str:\n    with open(filename) as f:\n        for _ in range(lineno - 1):\n            f.readline()\n        return f.readline()\n\n\ndef test_v1_validator_deprecated():\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH) as w:\n\n        class Point(BaseModel):\n            y: int\n            x: int\n\n            @validator('x')\n            @classmethod\n            def check_x(cls, x: int, values: Dict[str, Any]) -> int:\n                assert x * 2 == values['y']\n                return x\n\n    assert Point(x=1, y=2).model_dump() == {'x': 1, 'y': 2}\n\n    warnings = w.list\n    assert len(warnings) == 1\n    w = warnings[0]\n    # check that we got stacklevel correct\n    # if this fails you need to edit the stacklevel\n    # parameter to warnings.warn in _decorators.py\n    assert normcase(w.filename) == normcase(__file__)\n    source = _get_source_line(w.filename, w.lineno)\n    assert \"@validator('x')\" in source\n\n\ndef test_info_field_name_data_before():\n    \"\"\"\n    Test accessing info.field_name and info.data\n    We only test the `before` validator because they\n    all share the same implementation.\n    \"\"\"\n\n    class Model(BaseModel):\n        a: str\n        b: str\n\n        @field_validator('b', mode='before')\n        @classmethod\n        def check_a(cls, v: Any, info: ValidationInfo) -> Any:\n            assert v == b'but my barbaz is better'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 'your foobar is good'}\n            return 'just kidding!'\n\n    assert Model(a=b'your foobar is good', b=b'but my barbaz is better').b == 'just kidding!'\n\n\ndef test_decorator_proxy():\n    \"\"\"\n    Test that our validator decorator allows\n    calling the wrapped methods/functions.\n    \"\"\"\n\n    def val(v: int) -> int:\n        return v + 1\n\n    class Model(BaseModel):\n        x: int\n\n        @field_validator('x')\n        @staticmethod\n        def val1(v: int) -> int:\n            return v + 1\n\n        @field_validator('x')\n        @classmethod\n        def val2(cls, v: int) -> int:\n            return v + 1\n\n        val3 = field_validator('x')(val)\n\n    assert Model.val1(1) == 2\n    assert Model.val2(1) == 2\n    assert Model.val3(1) == 2\n\n\ndef test_root_validator_self():\n    with pytest.raises(TypeError, match=r'`@root_validator` cannot be applied to instance methods'):\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            class Model(BaseModel):\n                a: int = 1\n\n                @root_validator(skip_on_failure=True)\n                def root_validator(self, values: Any) -> Any:\n                    return values\n\n\ndef test_validator_self():\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n        with pytest.raises(TypeError, match=r'`@validator` cannot be applied to instance methods'):\n\n            class Model(BaseModel):\n                a: int = 1\n\n                @validator('a')\n                def check_a(self, values: Any) -> Any:\n                    return values\n\n\ndef test_field_validator_self():\n    with pytest.raises(TypeError, match=r'`@field_validator` cannot be applied to instance methods'):\n\n        class Model(BaseModel):\n            a: int = 1\n\n            @field_validator('a')\n            def check_a(self, values: Any) -> Any:\n                return values\n\n\ndef test_v1_validator_signature_kwargs_not_allowed() -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n        with pytest.raises(TypeError, match=r'Unsupported signature for V1 style validator'):\n\n            class Model(BaseModel):\n                a: int\n\n                @validator('a')\n                def check_a(cls, value: Any, foo: Any) -> Any: ...\n\n\ndef test_v1_validator_signature_kwargs1() -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            a: int\n            b: int\n\n            @validator('b')\n            def check_b(cls, value: Any, **kwargs: Any) -> Any:\n                assert kwargs == {'values': {'a': 1}}\n                assert value == 2\n                return value + 1\n\n    assert Model(a=1, b=2).model_dump() == {'a': 1, 'b': 3}\n\n\ndef test_v1_validator_signature_kwargs2() -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            a: int\n            b: int\n\n            @validator('b')\n            def check_b(cls, value: Any, values: Dict[str, Any], **kwargs: Any) -> Any:\n                assert kwargs == {}\n                assert values == {'a': 1}\n                assert value == 2\n                return value + 1\n\n    assert Model(a=1, b=2).model_dump() == {'a': 1, 'b': 3}\n\n\ndef test_v1_validator_signature_with_values() -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            a: int\n            b: int\n\n            @validator('b')\n            def check_b(cls, value: Any, values: Dict[str, Any]) -> Any:\n                assert values == {'a': 1}\n                assert value == 2\n                return value + 1\n\n    assert Model(a=1, b=2).model_dump() == {'a': 1, 'b': 3}\n\n\ndef test_v1_validator_signature_with_values_kw_only() -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            a: int\n            b: int\n\n            @validator('b')\n            def check_b(cls, value: Any, *, values: Dict[str, Any]) -> Any:\n                assert values == {'a': 1}\n                assert value == 2\n                return value + 1\n\n    assert Model(a=1, b=2).model_dump() == {'a': 1, 'b': 3}\n\n\ndef test_v1_validator_signature_with_field() -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n        with pytest.raises(TypeError, match=r'The `field` and `config` parameters are not available in Pydantic V2'):\n\n            class Model(BaseModel):\n                a: int\n                b: int\n\n                @validator('b')\n                def check_b(cls, value: Any, field: Any) -> Any: ...\n\n\ndef test_v1_validator_signature_with_config() -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n        with pytest.raises(TypeError, match=r'The `field` and `config` parameters are not available in Pydantic V2'):\n\n            class Model(BaseModel):\n                a: int\n                b: int\n\n                @validator('b')\n                def check_b(cls, value: Any, config: Any) -> Any: ...\n\n\ndef test_model_config_validate_default():\n    class Model(BaseModel):\n        x: int = -1\n\n        @field_validator('x')\n        @classmethod\n        def force_x_positive(cls, v):\n            assert v > 0\n            return v\n\n    assert Model().x == -1\n\n    class ValidatingModel(Model):\n        model_config = ConfigDict(validate_default=True)\n\n    with pytest.raises(ValidationError) as exc_info:\n        ValidatingModel()\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(AssertionError('assert -1 > 0')))},\n            'input': -1,\n            'loc': ('x',),\n            'msg': 'Assertion failed, assert -1 > 0',\n            'type': 'assertion_error',\n        }\n    ]\n\n\ndef partial_val_func1(\n    value: int,\n    allowed: int,\n) -> int:\n    assert value == allowed\n    return value\n\n\ndef partial_val_func2(\n    value: int,\n    *,\n    allowed: int,\n) -> int:\n    assert value == allowed\n    return value\n\n\ndef partial_values_val_func1(\n    value: int,\n    values: Dict[str, Any],\n    *,\n    allowed: int,\n) -> int:\n    assert isinstance(values, dict)\n    assert value == allowed\n    return value\n\n\ndef partial_values_val_func2(\n    value: int,\n    *,\n    values: Dict[str, Any],\n    allowed: int,\n) -> int:\n    assert isinstance(values, dict)\n    assert value == allowed\n    return value\n\n\ndef partial_info_val_func(\n    value: int,\n    info: ValidationInfo,\n    *,\n    allowed: int,\n) -> int:\n    assert isinstance(info.data, dict)\n    assert value == allowed\n    return value\n\n\ndef partial_cls_val_func1(\n    cls: Any,\n    value: int,\n    allowed: int,\n    expected_cls: Any,\n) -> int:\n    assert cls.__name__ == expected_cls\n    assert value == allowed\n    return value\n\n\ndef partial_cls_val_func2(\n    cls: Any,\n    value: int,\n    *,\n    allowed: int,\n    expected_cls: Any,\n) -> int:\n    assert cls.__name__ == expected_cls\n    assert value == allowed\n    return value\n\n\ndef partial_cls_values_val_func1(\n    cls: Any,\n    value: int,\n    values: Dict[str, Any],\n    *,\n    allowed: int,\n    expected_cls: Any,\n) -> int:\n    assert cls.__name__ == expected_cls\n    assert isinstance(values, dict)\n    assert value == allowed\n    return value\n\n\ndef partial_cls_values_val_func2(\n    cls: Any,\n    value: int,\n    *,\n    values: Dict[str, Any],\n    allowed: int,\n    expected_cls: Any,\n) -> int:\n    assert cls.__name__ == expected_cls\n    assert isinstance(values, dict)\n    assert value == allowed\n    return value\n\n\ndef partial_cls_info_val_func(\n    cls: Any,\n    value: int,\n    info: ValidationInfo,\n    *,\n    allowed: int,\n    expected_cls: Any,\n) -> int:\n    assert cls.__name__ == expected_cls\n    assert isinstance(info.data, dict)\n    assert value == allowed\n    return value\n\n\n@pytest.mark.parametrize(\n    'func',\n    [\n        partial_val_func1,\n        partial_val_func2,\n        partial_info_val_func,\n    ],\n)\ndef test_functools_partial_validator_v2(\n    func: Callable[..., Any],\n) -> None:\n    class Model(BaseModel):\n        x: int\n\n        val = field_validator('x')(partial(func, allowed=42))\n\n    Model(x=42)\n\n    with pytest.raises(ValidationError):\n        Model(x=123)\n\n\n@pytest.mark.parametrize(\n    'func',\n    [\n        partial_val_func1,\n        partial_val_func2,\n        partial_info_val_func,\n    ],\n)\ndef test_functools_partialmethod_validator_v2(\n    func: Callable[..., Any],\n) -> None:\n    class Model(BaseModel):\n        x: int\n\n        val = field_validator('x')(partialmethod(func, allowed=42))\n\n    Model(x=42)\n\n    with pytest.raises(ValidationError):\n        Model(x=123)\n\n\n@pytest.mark.parametrize(\n    'func',\n    [\n        partial_cls_val_func1,\n        partial_cls_val_func2,\n        partial_cls_info_val_func,\n    ],\n)\ndef test_functools_partialmethod_validator_v2_cls_method(\n    func: Callable[..., Any],\n) -> None:\n    class Model(BaseModel):\n        x: int\n\n        # note that you _have_ to wrap your function with classmethod\n        # it's partialmethod not us that requires it\n        # otherwise it creates a bound instance method\n        val = field_validator('x')(partialmethod(classmethod(func), allowed=42, expected_cls='Model'))\n\n    Model(x=42)\n\n    with pytest.raises(ValidationError):\n        Model(x=123)\n\n\n@pytest.mark.parametrize(\n    'func',\n    [\n        partial_val_func1,\n        partial_val_func2,\n        partial_values_val_func1,\n        partial_values_val_func2,\n    ],\n)\ndef test_functools_partial_validator_v1(\n    func: Callable[..., Any],\n) -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            x: int\n\n            val = validator('x')(partial(func, allowed=42))\n\n    Model(x=42)\n\n    with pytest.raises(ValidationError):\n        Model(x=123)\n\n\n@pytest.mark.parametrize(\n    'func',\n    [\n        partial_val_func1,\n        partial_val_func2,\n        partial_values_val_func1,\n        partial_values_val_func2,\n    ],\n)\ndef test_functools_partialmethod_validator_v1(\n    func: Callable[..., Any],\n) -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            x: int\n\n            val = validator('x')(partialmethod(func, allowed=42))\n\n        Model(x=42)\n\n        with pytest.raises(ValidationError):\n            Model(x=123)\n\n\n@pytest.mark.parametrize(\n    'func',\n    [\n        partial_cls_val_func1,\n        partial_cls_val_func2,\n        partial_cls_values_val_func1,\n        partial_cls_values_val_func2,\n    ],\n)\ndef test_functools_partialmethod_validator_v1_cls_method(\n    func: Callable[..., Any],\n) -> None:\n    with pytest.warns(PydanticDeprecatedSince20, match=V1_VALIDATOR_DEPRECATION_MATCH):\n\n        class Model(BaseModel):\n            x: int\n\n            # note that you _have_ to wrap your function with classmethod\n            # it's partialmethod not us that requires it\n            # otherwise it creates a bound instance method\n            val = validator('x')(partialmethod(classmethod(func), allowed=42, expected_cls='Model'))\n\n    Model(x=42)\n\n    with pytest.raises(ValidationError):\n        Model(x=123)\n\n\ndef test_validator_allow_reuse_inheritance():\n    class Parent(BaseModel):\n        x: int\n\n        @field_validator('x')\n        def val(cls, v: int) -> int:\n            return v + 1\n\n    class Child(Parent):\n        @field_validator('x')\n        def val(cls, v: int) -> int:\n            assert v == 1\n            v = super().val(v)\n            assert v == 2\n            return 4\n\n    assert Parent(x=1).model_dump() == {'x': 2}\n    assert Child(x=1).model_dump() == {'x': 4}\n\n\ndef test_validator_allow_reuse_same_field():\n    with pytest.warns(UserWarning, match='`val_x` overrides an existing Pydantic `@field_validator` decorator'):\n\n        class Model(BaseModel):\n            x: int\n\n            @field_validator('x')\n            def val_x(cls, v: int) -> int:\n                return v + 1\n\n            @field_validator('x')\n            def val_x(cls, v: int) -> int:\n                return v + 2\n\n        assert Model(x=1).model_dump() == {'x': 3}\n\n\ndef test_validator_allow_reuse_different_field_1():\n    with pytest.warns(UserWarning, match='`val` overrides an existing Pydantic `@field_validator` decorator'):\n\n        class Model(BaseModel):\n            x: int\n            y: int\n\n            @field_validator('x')\n            def val(cls, v: int) -> int:\n                return v + 1\n\n            @field_validator('y')\n            def val(cls, v: int) -> int:\n                return v + 2\n\n    assert Model(x=1, y=2).model_dump() == {'x': 1, 'y': 4}\n\n\ndef test_validator_allow_reuse_different_field_2():\n    with pytest.warns(UserWarning, match='`val_x` overrides an existing Pydantic `@field_validator` decorator'):\n\n        def val(cls: Any, v: int) -> int:\n            return v + 2\n\n        class Model(BaseModel):\n            x: int\n            y: int\n\n            @field_validator('x')\n            def val_x(cls, v: int) -> int:\n                return v + 1\n\n            val_x = field_validator('y')(val)\n\n    assert Model(x=1, y=2).model_dump() == {'x': 1, 'y': 4}\n\n\ndef test_validator_allow_reuse_different_field_3():\n    with pytest.warns(UserWarning, match='`val_x` overrides an existing Pydantic `@field_validator` decorator'):\n\n        def val1(v: int) -> int:\n            return v + 1\n\n        def val2(v: int) -> int:\n            return v + 2\n\n        class Model(BaseModel):\n            x: int\n            y: int\n\n            val_x = field_validator('x')(val1)\n            val_x = field_validator('y')(val2)\n\n    assert Model(x=1, y=2).model_dump() == {'x': 1, 'y': 4}\n\n\ndef test_validator_allow_reuse_different_field_4():\n    def val(v: int) -> int:\n        return v + 1\n\n    class Model(BaseModel):\n        x: int\n        y: int\n\n        val_x = field_validator('x')(val)\n        not_val_x = field_validator('y')(val)\n\n    assert Model(x=1, y=2).model_dump() == {'x': 2, 'y': 3}\n\n\n@pytest.mark.filterwarnings(\n    'ignore:Pydantic V1 style `@root_validator` validators are deprecated.*:pydantic.warnings.PydanticDeprecatedSince20'\n)\ndef test_root_validator_allow_reuse_same_field():\n    with pytest.warns(UserWarning, match='`root_val` overrides an existing Pydantic `@root_validator` decorator'):\n\n        class Model(BaseModel):\n            x: int\n\n            @root_validator(skip_on_failure=True)\n            def root_val(cls, v: Dict[str, Any]) -> Dict[str, Any]:\n                v['x'] += 1\n                return v\n\n            @root_validator(skip_on_failure=True)\n            def root_val(cls, v: Dict[str, Any]) -> Dict[str, Any]:\n                v['x'] += 2\n                return v\n\n        assert Model(x=1).model_dump() == {'x': 3}\n\n\ndef test_root_validator_allow_reuse_inheritance():\n    with pytest.warns(PydanticDeprecatedSince20):\n\n        class Parent(BaseModel):\n            x: int\n\n            @root_validator(skip_on_failure=True)\n            def root_val(cls, v: Dict[str, Any]) -> Dict[str, Any]:\n                v['x'] += 1\n                return v\n\n    with pytest.warns(PydanticDeprecatedSince20):\n\n        class Child(Parent):\n            @root_validator(skip_on_failure=True)\n            def root_val(cls, v: Dict[str, Any]) -> Dict[str, Any]:\n                assert v == {'x': 1}\n                v = super().root_val(v)\n                assert v == {'x': 2}\n                return {'x': 4}\n\n    assert Parent(x=1).model_dump() == {'x': 2}\n    assert Child(x=1).model_dump() == {'x': 4}\n\n\ndef test_bare_root_validator():\n    with pytest.raises(\n        PydanticUserError,\n        match=re.escape(\n            'If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.'\n            ' Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.'\n        ),\n    ):\n        with pytest.warns(\n            PydanticDeprecatedSince20, match='Pydantic V1 style `@root_validator` validators are deprecated.'\n        ):\n\n            class Model(BaseModel):\n                @root_validator\n                @classmethod\n                def validate_values(cls, values):\n                    return values\n\n\ndef test_validator_with_underscore_name() -> None:\n    \"\"\"\n    https://github.com/pydantic/pydantic/issues/5252\n    \"\"\"\n\n    def f(name: str) -> str:\n        return name.lower()\n\n    class Model(BaseModel):\n        name: str\n        _normalize_name = field_validator('name')(f)\n\n    assert Model(name='Adrian').name == 'adrian'\n\n\n@pytest.mark.parametrize(\n    'mode,config,input_str',\n    (\n        ('before', {}, \"type=value_error, input_value='123', input_type=str\"),\n        ('before', {'hide_input_in_errors': False}, \"type=value_error, input_value='123', input_type=str\"),\n        ('before', {'hide_input_in_errors': True}, 'type=value_error'),\n        ('after', {}, \"type=value_error, input_value='123', input_type=str\"),\n        ('after', {'hide_input_in_errors': False}, \"type=value_error, input_value='123', input_type=str\"),\n        ('after', {'hide_input_in_errors': True}, 'type=value_error'),\n        ('plain', {}, \"type=value_error, input_value='123', input_type=str\"),\n        ('plain', {'hide_input_in_errors': False}, \"type=value_error, input_value='123', input_type=str\"),\n        ('plain', {'hide_input_in_errors': True}, 'type=value_error'),\n    ),\n)\ndef test_validator_function_error_hide_input(mode, config, input_str):\n    class Model(BaseModel):\n        x: str\n\n        model_config = ConfigDict(**config)\n\n        @field_validator('x', mode=mode)\n        @classmethod\n        def check_a1(cls, v: str) -> str:\n            raise ValueError('foo')\n\n    with pytest.raises(ValidationError, match=re.escape(f'Value error, foo [{input_str}]')):\n        Model(x='123')\n\n\ndef foobar_validate(value: Any, info: core_schema.ValidationInfo):\n    data = info.data\n    if isinstance(data, dict):\n        data = data.copy()\n    return {'value': value, 'field_name': info.field_name, 'data': data}\n\n\nclass Foobar:\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        return core_schema.with_info_plain_validator_function(foobar_validate, field_name=handler.field_name)\n\n\ndef test_custom_type_field_name_model():\n    class MyModel(BaseModel):\n        foobar: Foobar\n\n    m = MyModel(foobar=1, tuple_nesting=(1, 2))\n    # insert_assert(m.foobar)\n    assert m.foobar == {'value': 1, 'field_name': 'foobar', 'data': {}}\n\n\ndef test_custom_type_field_name_model_nested():\n    class MyModel(BaseModel):\n        x: int\n        tuple_nested: Tuple[int, Foobar]\n\n    m = MyModel(x='123', tuple_nested=(1, 2))\n    # insert_assert(m.tuple_nested[1])\n    assert m.tuple_nested[1] == {'value': 2, 'field_name': 'tuple_nested', 'data': {'x': 123}}\n\n\ndef test_custom_type_field_name_typed_dict():\n    class MyDict(TypedDict):\n        x: int\n        foobar: Foobar\n\n    ta = TypeAdapter(MyDict)\n    m = ta.validate_python({'x': '123', 'foobar': 1})\n    # insert_assert(m['foobar'])\n    assert m['foobar'] == {'value': 1, 'field_name': 'foobar', 'data': {'x': 123}}\n\n\ndef test_custom_type_field_name_dataclass():\n    @dataclass\n    class MyDc:\n        x: int\n        foobar: Foobar\n\n    ta = TypeAdapter(MyDc)\n    m = ta.validate_python({'x': '123', 'foobar': 1})\n    # insert_assert(m.foobar)\n    assert m.foobar == {'value': 1, 'field_name': 'foobar', 'data': {'x': 123}}\n\n\ndef test_custom_type_field_name_named_tuple():\n    class MyNamedTuple(NamedTuple):\n        x: int\n        foobar: Foobar\n\n    ta = TypeAdapter(MyNamedTuple)\n    m = ta.validate_python({'x': '123', 'foobar': 1})\n    # insert_assert(m.foobar)\n    assert m.foobar == {'value': 1, 'field_name': 'foobar', 'data': None}\n\n\ndef test_custom_type_field_name_validate_call():\n    @validate_call\n    def foobar(x: int, y: Foobar):\n        return x, y\n\n    # insert_assert(foobar(1, 2))\n    assert foobar(1, 2) == (1, {'value': 2, 'field_name': 'y', 'data': None})\n\n\ndef test_after_validator_field_name():\n    class MyModel(BaseModel):\n        x: int\n        foobar: Annotated[int, AfterValidator(foobar_validate)]\n\n    m = MyModel(x='123', foobar='1')\n    # insert_assert(m.foobar)\n    assert m.foobar == {'value': 1, 'field_name': 'foobar', 'data': {'x': 123}}\n\n\ndef test_before_validator_field_name():\n    class MyModel(BaseModel):\n        x: int\n        foobar: Annotated[Dict[Any, Any], BeforeValidator(foobar_validate)]\n\n    m = MyModel(x='123', foobar='1')\n    # insert_assert(m.foobar)\n    assert m.foobar == {'value': '1', 'field_name': 'foobar', 'data': {'x': 123}}\n\n\ndef test_plain_validator_field_name():\n    class MyModel(BaseModel):\n        x: int\n        foobar: Annotated[int, PlainValidator(foobar_validate)]\n\n    m = MyModel(x='123', foobar='1')\n    # insert_assert(m.foobar)\n    assert m.foobar == {'value': '1', 'field_name': 'foobar', 'data': {'x': 123}}\n\n\ndef validate_wrap(value: Any, handler: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo):\n    data = info.data\n    if isinstance(data, dict):\n        data = data.copy()\n    return {'value': handler(value), 'field_name': info.field_name, 'data': data}\n\n\ndef test_wrap_validator_field_name():\n    class MyModel(BaseModel):\n        x: int\n        foobar: Annotated[int, WrapValidator(validate_wrap)]\n\n    m = MyModel(x='123', foobar='1')\n    # insert_assert(m.foobar)\n    assert m.foobar == {'value': 1, 'field_name': 'foobar', 'data': {'x': 123}}\n\n\ndef test_validate_default_raises_for_basemodel() -> None:\n    class Model(BaseModel):\n        value_0: str\n        value_a: Annotated[Optional[str], Field(None, validate_default=True)]\n        value_b: Annotated[Optional[str], Field(None, validate_default=True)]\n\n        @field_validator('value_a', mode='after')\n        def value_a_validator(cls, value):\n            raise AssertionError\n\n        @field_validator('value_b', mode='after')\n        def value_b_validator(cls, value):\n            raise AssertionError\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('value_0',), 'msg': 'Field required', 'input': {}},\n        {\n            'type': 'assertion_error',\n            'loc': ('value_a',),\n            'msg': 'Assertion failed, ',\n            'input': None,\n            'ctx': {'error': IsInstance(AssertionError)},\n        },\n        {\n            'type': 'assertion_error',\n            'loc': ('value_b',),\n            'msg': 'Assertion failed, ',\n            'input': None,\n            'ctx': {'error': IsInstance(AssertionError)},\n        },\n    ]\n\n\ndef test_validate_default_raises_for_dataclasses() -> None:\n    @pydantic_dataclass\n    class Model:\n        value_0: str\n        value_a: Annotated[Optional[str], Field(None, validate_default=True)]\n        value_b: Annotated[Optional[str], Field(None, validate_default=True)]\n\n        @field_validator('value_a', mode='after')\n        def value_a_validator(cls, value):\n            raise AssertionError\n\n        @field_validator('value_b', mode='after')\n        def value_b_validator(cls, value):\n            raise AssertionError\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('value_0',), 'msg': 'Field required', 'input': HasRepr('ArgsKwargs(())')},\n        {\n            'type': 'assertion_error',\n            'loc': ('value_a',),\n            'msg': 'Assertion failed, ',\n            'input': None,\n            'ctx': {'error': IsInstance(AssertionError)},\n        },\n        {\n            'type': 'assertion_error',\n            'loc': ('value_b',),\n            'msg': 'Assertion failed, ',\n            'input': None,\n            'ctx': {'error': IsInstance(AssertionError)},\n        },\n    ]\n\n\ndef test_plain_validator_plain_serializer() -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/8512\"\"\"\n    ser_type = str\n    serializer = PlainSerializer(lambda x: ser_type(int(x)), return_type=ser_type)\n    validator = PlainValidator(lambda x: bool(int(x)))\n\n    class Blah(BaseModel):\n        foo: Annotated[bool, validator, serializer]\n        bar: Annotated[bool, serializer, validator]\n\n    blah = Blah(foo='0', bar='1')\n    data = blah.model_dump()\n    assert isinstance(data['foo'], ser_type)\n    assert isinstance(data['bar'], ser_type)\n\n\ndef test_plain_validator_with_unsupported_type() -> None:\n    class UnsupportedClass:\n        pass\n\n    PreviouslySupportedType = Annotated[\n        UnsupportedClass,\n        PlainValidator(lambda _: UnsupportedClass()),\n    ]\n\n    type_adapter = TypeAdapter(PreviouslySupportedType)\n\n    model = type_adapter.validate_python('abcdefg')\n    assert isinstance(model, UnsupportedClass)\n    assert isinstance(type_adapter.dump_python(model), UnsupportedClass)\n\n\ndef test_validator_with_default_values() -> None:\n    def validate_x(v: int, unrelated_arg: int = 1, other_unrelated_arg: int = 2) -> int:\n        assert v != -1\n        return v\n\n    class Model(BaseModel):\n        x: int\n\n        val_x = field_validator('x')(validate_x)\n\n    with pytest.raises(ValidationError):\n        Model(x=-1)\n", "tests/test_construction.py": "import pickle\nfrom typing import Any, List, Optional\n\nimport pytest\nfrom pydantic_core import PydanticUndefined, ValidationError\n\nfrom pydantic import AliasChoices, AliasPath, BaseModel, ConfigDict, Field, PrivateAttr, PydanticDeprecatedSince20\n\n\nclass Model(BaseModel):\n    a: float\n    b: int = 10\n\n\ndef test_simple_construct():\n    m = Model.model_construct(a=3.14)\n    assert m.a == 3.14\n    assert m.b == 10\n    assert m.model_fields_set == {'a'}\n    assert m.model_dump() == {'a': 3.14, 'b': 10}\n\n\ndef test_construct_misuse():\n    m = Model.model_construct(b='foobar')\n    assert m.b == 'foobar'\n    with pytest.warns(UserWarning, match='Expected `int` but got `str`'):\n        assert m.model_dump() == {'b': 'foobar'}\n    with pytest.raises(AttributeError, match=\"'Model' object has no attribute 'a'\"):\n        print(m.a)\n\n\ndef test_construct_fields_set():\n    m = Model.model_construct(a=3.0, b=-1, _fields_set={'a'})\n    assert m.a == 3\n    assert m.b == -1\n    assert m.model_fields_set == {'a'}\n    assert m.model_dump() == {'a': 3, 'b': -1}\n\n\ndef test_construct_allow_extra():\n    \"\"\"model_construct() should allow extra fields only in the case of extra='allow'\"\"\"\n\n    class Foo(BaseModel, extra='allow'):\n        x: int\n\n    model = Foo.model_construct(x=1, y=2)\n    assert model.x == 1\n    assert model.y == 2\n\n\n@pytest.mark.parametrize('extra', ['ignore', 'forbid'])\ndef test_construct_ignore_extra(extra: str) -> None:\n    \"\"\"model_construct() should ignore extra fields only in the case of extra='ignore' or extra='forbid'\"\"\"\n\n    class Foo(BaseModel, extra=extra):\n        x: int\n\n    model = Foo.model_construct(x=1, y=2)\n    assert model.x == 1\n    assert model.__pydantic_extra__ is None\n    assert 'y' not in model.__dict__\n\n\ndef test_construct_keep_order():\n    class Foo(BaseModel):\n        a: int\n        b: int = 42\n        c: float\n\n    instance = Foo(a=1, b=321, c=3.14)\n    instance_construct = Foo.model_construct(**instance.model_dump())\n    assert instance == instance_construct\n    assert instance.model_dump() == instance_construct.model_dump()\n    assert instance.model_dump_json() == instance_construct.model_dump_json()\n\n\ndef test_construct_with_aliases():\n    class MyModel(BaseModel):\n        x: int = Field(alias='x_alias')\n\n    my_model = MyModel.model_construct(x_alias=1)\n    assert my_model.x == 1\n    assert my_model.model_fields_set == {'x'}\n    assert my_model.model_dump() == {'x': 1}\n\n\ndef test_construct_with_validation_aliases():\n    class MyModel(BaseModel):\n        x: int = Field(validation_alias='x_alias')\n\n    my_model = MyModel.model_construct(x_alias=1)\n    assert my_model.x == 1\n    assert my_model.model_fields_set == {'x'}\n    assert my_model.model_dump() == {'x': 1}\n\n\ndef test_large_any_str():\n    class Model(BaseModel):\n        a: bytes\n        b: str\n\n    content_bytes = b'x' * (2**16 + 1)\n    content_str = 'x' * (2**16 + 1)\n    m = Model(a=content_bytes, b=content_str)\n    assert m.a == content_bytes\n    assert m.b == content_str\n\n\ndef deprecated_copy(m: BaseModel, *, include=None, exclude=None, update=None, deep=False):\n    \"\"\"\n    This should only be used to make calls to the deprecated `copy` method with arguments\n    that have been removed from `model_copy`. Otherwise, use the `copy_method` fixture below\n    \"\"\"\n    with pytest.warns(\n        PydanticDeprecatedSince20,\n        match=(\n            'The `copy` method is deprecated; use `model_copy` instead. '\n            'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.'\n        ),\n    ):\n        return m.copy(include=include, exclude=exclude, update=update, deep=deep)\n\n\n@pytest.fixture(params=['copy', 'model_copy'])\ndef copy_method(request):\n    \"\"\"\n    Fixture to test both the old/deprecated `copy` and new `model_copy` methods.\n    \"\"\"\n    if request.param == 'copy':\n        return deprecated_copy\n    else:\n\n        def new_copy_method(m, *, update=None, deep=False):\n            return m.model_copy(update=update, deep=deep)\n\n        return new_copy_method\n\n\ndef test_simple_copy(copy_method):\n    m = Model(a=24)\n    m2 = copy_method(m)\n\n    assert m.a == m2.a == 24\n    assert m.b == m2.b == 10\n    assert m == m2\n    assert m.model_fields == m2.model_fields\n\n\n@pytest.fixture(scope='session', name='ModelTwo')\ndef model_two_fixture():\n    class ModelTwo(BaseModel):\n        _foo_ = PrivateAttr({'private'})\n\n        a: float\n        b: int = 10\n        c: str = 'foobar'\n        d: Model\n\n    return ModelTwo\n\n\ndef test_deep_copy(ModelTwo, copy_method):\n    m = ModelTwo(a=24, d=Model(a='12'))\n    m._foo_ = {'new value'}\n    m2 = copy_method(m, deep=True)\n\n    assert m.a == m2.a == 24\n    assert m.b == m2.b == 10\n    assert m.c == m2.c == 'foobar'\n    assert m.d is not m2.d\n    assert m == m2\n    assert m.model_fields == m2.model_fields\n    assert m._foo_ == m2._foo_\n    assert m._foo_ is not m2._foo_\n\n\ndef test_copy_exclude(ModelTwo):\n    m = ModelTwo(a=24, d=Model(a='12'))\n    m2 = deprecated_copy(m, exclude={'b'})\n\n    assert m.a == m2.a == 24\n    assert isinstance(m2.d, Model)\n    assert m2.d.a == 12\n\n    assert hasattr(m2, 'c')\n    assert not hasattr(m2, 'b')\n    assert set(m.model_dump().keys()) == {'a', 'b', 'c', 'd'}\n    assert set(m2.model_dump().keys()) == {'a', 'c', 'd'}\n\n    assert m != m2\n\n\ndef test_copy_include(ModelTwo):\n    m = ModelTwo(a=24, d=Model(a='12'))\n    m2 = deprecated_copy(m, include={'a'})\n\n    assert m.a == m2.a == 24\n    assert set(m.model_dump().keys()) == {'a', 'b', 'c', 'd'}\n    assert set(m2.model_dump().keys()) == {'a'}\n\n    assert m != m2\n\n\ndef test_copy_include_exclude(ModelTwo):\n    m = ModelTwo(a=24, d=Model(a='12'))\n    m2 = deprecated_copy(m, include={'a', 'b', 'c'}, exclude={'c'})\n\n    assert set(m.model_dump().keys()) == {'a', 'b', 'c', 'd'}\n    assert set(m2.model_dump().keys()) == {'a', 'b'}\n\n\ndef test_copy_advanced_exclude():\n    class SubSubModel(BaseModel):\n        a: str\n        b: str\n\n    class SubModel(BaseModel):\n        c: str\n        d: List[SubSubModel]\n\n    class Model(BaseModel):\n        e: str\n        f: SubModel\n\n    m = Model(e='e', f=SubModel(c='foo', d=[SubSubModel(a='a', b='b'), SubSubModel(a='c', b='e')]))\n    m2 = deprecated_copy(m, exclude={'f': {'c': ..., 'd': {-1: {'a'}}}})\n    assert hasattr(m.f, 'c')\n    assert not hasattr(m2.f, 'c')\n\n    assert m2.model_dump() == {'e': 'e', 'f': {'d': [{'a': 'a', 'b': 'b'}, {'b': 'e'}]}}\n    m2 = deprecated_copy(m, exclude={'e': ..., 'f': {'d'}})\n    assert m2.model_dump() == {'f': {'c': 'foo'}}\n\n\ndef test_copy_advanced_include():\n    class SubSubModel(BaseModel):\n        a: str\n        b: str\n\n    class SubModel(BaseModel):\n        c: str\n        d: List[SubSubModel]\n\n    class Model(BaseModel):\n        e: str\n        f: SubModel\n\n    m = Model(e='e', f=SubModel(c='foo', d=[SubSubModel(a='a', b='b'), SubSubModel(a='c', b='e')]))\n    m2 = deprecated_copy(m, include={'f': {'c'}})\n    assert hasattr(m.f, 'c')\n    assert hasattr(m2.f, 'c')\n    assert m2.model_dump() == {'f': {'c': 'foo'}}\n\n    m2 = deprecated_copy(m, include={'e': ..., 'f': {'d': {-1}}})\n    assert m2.model_dump() == {'e': 'e', 'f': {'d': [{'a': 'c', 'b': 'e'}]}}\n\n\ndef test_copy_advanced_include_exclude():\n    class SubSubModel(BaseModel):\n        a: str\n        b: str\n\n    class SubModel(BaseModel):\n        c: str\n        d: List[SubSubModel]\n\n    class Model(BaseModel):\n        e: str\n        f: SubModel\n\n    m = Model(e='e', f=SubModel(c='foo', d=[SubSubModel(a='a', b='b'), SubSubModel(a='c', b='e')]))\n    m2 = deprecated_copy(m, include={'e': ..., 'f': {'d'}}, exclude={'e': ..., 'f': {'d': {0}}})\n    assert m2.model_dump() == {'f': {'d': [{'a': 'c', 'b': 'e'}]}}\n\n\ndef test_copy_update(ModelTwo, copy_method):\n    m = ModelTwo(a=24, d=Model(a='12'))\n    m2 = copy_method(m, update={'a': 'different'})\n\n    assert m.a == 24\n    assert m2.a == 'different'\n    m_keys = m.model_dump().keys()\n    with pytest.warns(UserWarning, match='Expected `float` but got `str`'):\n        m2_keys = m2.model_dump().keys()\n    assert set(m_keys) == set(m2_keys) == {'a', 'b', 'c', 'd'}\n    assert m != m2\n\n\ndef test_copy_update_unset(copy_method):\n    class Foo(BaseModel):\n        foo: Optional[str] = None\n        bar: Optional[str] = None\n\n    assert (\n        copy_method(Foo(foo='hello'), update={'bar': 'world'}).model_dump_json(exclude_unset=True)\n        == '{\"foo\":\"hello\",\"bar\":\"world\"}'\n    )\n\n\nclass ExtraModel(BaseModel, extra='allow'):\n    pass\n\n\ndef test_copy_deep_extra(copy_method):\n    class Foo(BaseModel, extra='allow'):\n        pass\n\n    m = Foo(extra=[])\n    assert copy_method(m).extra is m.extra\n    assert copy_method(m, deep=True).extra == m.extra\n    assert copy_method(m, deep=True).extra is not m.extra\n\n\ndef test_copy_set_fields(ModelTwo, copy_method):\n    m = ModelTwo(a=24, d=Model(a='12'))\n    m2 = copy_method(m)\n\n    assert m.model_dump(exclude_unset=True) == {'a': 24.0, 'd': {'a': 12}}\n    assert m.model_dump(exclude_unset=True) == m2.model_dump(exclude_unset=True)\n\n\ndef test_simple_pickle():\n    m = Model(a='24')\n    b = pickle.dumps(m)\n    m2 = pickle.loads(b)\n    assert m.a == m2.a == 24\n    assert m.b == m2.b == 10\n    assert m == m2\n    assert m is not m2\n    assert tuple(m) == (('a', 24.0), ('b', 10))\n    assert tuple(m2) == (('a', 24.0), ('b', 10))\n    assert m.model_fields == m2.model_fields\n\n\ndef test_recursive_pickle(create_module):\n    @create_module\n    def module():\n        from pydantic import BaseModel, PrivateAttr\n\n        class PickleModel(BaseModel):\n            a: float\n            b: int = 10\n\n        class PickleModelTwo(BaseModel):\n            _foo_ = PrivateAttr({'private'})\n\n            a: float\n            b: int = 10\n            c: str = 'foobar'\n            d: PickleModel\n\n    m = module.PickleModelTwo(a=24, d=module.PickleModel(a='123.45'))\n    m2 = pickle.loads(pickle.dumps(m))\n    assert m == m2\n\n    assert m.d.a == 123.45\n    assert m2.d.a == 123.45\n    assert m.model_fields == m2.model_fields\n    assert m._foo_ == m2._foo_\n\n\ndef test_pickle_undefined(create_module):\n    @create_module\n    def module():\n        from pydantic import BaseModel, PrivateAttr\n\n        class PickleModel(BaseModel):\n            a: float\n            b: int = 10\n\n        class PickleModelTwo(BaseModel):\n            _foo_ = PrivateAttr({'private'})\n\n            a: float\n            b: int = 10\n            c: str = 'foobar'\n            d: PickleModel\n\n    m = module.PickleModelTwo(a=24, d=module.PickleModel(a='123.45'))\n    m2 = pickle.loads(pickle.dumps(m))\n    assert m2._foo_ == {'private'}\n\n    m._foo_ = PydanticUndefined\n    m3 = pickle.loads(pickle.dumps(m))\n    assert not hasattr(m3, '_foo_')\n\n\ndef test_copy_undefined(ModelTwo, copy_method):\n    m = ModelTwo(a=24, d=Model(a='123.45'))\n    m2 = copy_method(m)\n    assert m2._foo_ == {'private'}\n\n    m._foo_ = PydanticUndefined\n    m3 = copy_method(m)\n    assert not hasattr(m3, '_foo_')\n\n\ndef test_immutable_copy_with_frozen(copy_method):\n    class Model(BaseModel):\n        model_config = ConfigDict(frozen=True)\n        a: int\n        b: int\n\n    m = Model(a=40, b=10)\n    assert m == copy_method(m)\n    assert repr(m) == 'Model(a=40, b=10)'\n\n    m2 = copy_method(m, update={'b': 12})\n    assert repr(m2) == 'Model(a=40, b=12)'\n    with pytest.raises(ValidationError):\n        m2.b = 13\n\n\ndef test_pickle_fields_set():\n    m = Model(a=24)\n    assert m.model_dump(exclude_unset=True) == {'a': 24}\n    m2 = pickle.loads(pickle.dumps(m))\n    assert m2.model_dump(exclude_unset=True) == {'a': 24}\n\n\ndef test_pickle_preserves_extra():\n    m = ExtraModel(a=24)\n    assert m.model_extra == {'a': 24}\n    m2 = pickle.loads(pickle.dumps(m))\n    assert m2.model_extra == {'a': 24}\n\n\ndef test_copy_update_exclude():\n    class SubModel(BaseModel):\n        a: str\n        b: str\n\n    class Model(BaseModel):\n        c: str\n        d: SubModel\n\n    m = Model(c='ex', d=dict(a='ax', b='bx'))\n    assert m.model_dump() == {'c': 'ex', 'd': {'a': 'ax', 'b': 'bx'}}\n    assert deprecated_copy(m, exclude={'c'}).model_dump() == {'d': {'a': 'ax', 'b': 'bx'}}\n    with pytest.warns(UserWarning, match='Expected `str` but got `int`'):\n        assert deprecated_copy(m, exclude={'c'}, update={'c': 42}).model_dump() == {\n            'c': 42,\n            'd': {'a': 'ax', 'b': 'bx'},\n        }\n\n    with pytest.warns(\n        PydanticDeprecatedSince20,\n        match='The private method `_calculate_keys` will be removed and should no longer be used.',\n    ):\n        assert m._calculate_keys(exclude={'x': ...}, include=None, exclude_unset=False) == {'c', 'd'}\n        assert m._calculate_keys(exclude={'x': ...}, include=None, exclude_unset=False, update={'c': 42}) == {'d'}\n\n\ndef test_shallow_copy_modify(copy_method):\n    class X(BaseModel):\n        val: int\n        deep: Any\n\n    x = X(val=1, deep={'deep_thing': [1, 2]})\n\n    y = copy_method(x)\n    y.val = 2\n    y.deep['deep_thing'].append(3)\n\n    assert x.val == 1\n    assert y.val == 2\n    # deep['deep_thing'] gets modified\n    assert x.deep['deep_thing'] == [1, 2, 3]\n    assert y.deep['deep_thing'] == [1, 2, 3]\n\n\ndef test_construct_default_factory():\n    class Model(BaseModel):\n        foo: List[int] = Field(default_factory=list)\n        bar: str = 'Baz'\n\n    m = Model.model_construct()\n    assert m.foo == []\n    assert m.bar == 'Baz'\n\n\ndef test_copy_with_excluded_fields():\n    class User(BaseModel):\n        name: str\n        age: int\n        dob: str\n\n    user = User(name='test_user', age=23, dob='01/01/2000')\n    user_copy = deprecated_copy(user, exclude={'dob': ...})\n\n    assert 'dob' in user.model_fields_set\n    assert 'dob' not in user_copy.model_fields_set\n\n\ndef test_dunder_copy(ModelTwo):\n    m = ModelTwo(a=24, d=Model(a='12'))\n    m2 = m.__copy__()\n    assert m is not m2\n\n    assert m.a == m2.a == 24\n    assert isinstance(m2.d, Model)\n    assert m.d is m2.d\n    assert m.d.a == m2.d.a == 12\n\n    m.a = 12\n    assert m.a != m2.a\n\n\ndef test_dunder_deepcopy(ModelTwo):\n    m = ModelTwo(a=24, d=Model(a='12'))\n    m2 = m.__copy__()\n    assert m is not m2\n\n    assert m.a == m2.a == 24\n    assert isinstance(m2.d, Model)\n    assert m.d is m2.d\n    assert m.d.a == m2.d.a == 12\n\n    m.a = 12\n    assert m.a != m2.a\n\n\ndef test_model_copy(ModelTwo):\n    m = ModelTwo(a=24, d=Model(a='12'))\n    m2 = m.__copy__()\n    assert m is not m2\n\n    assert m.a == m2.a == 24\n    assert isinstance(m2.d, Model)\n    assert m.d is m2.d\n    assert m.d.a == m2.d.a == 12\n\n    m.a = 12\n    assert m.a != m2.a\n\n\ndef test_pydantic_extra():\n    class Model(BaseModel):\n        model_config = dict(extra='allow')\n        x: int\n\n    m = Model.model_construct(x=1, y=2)\n    assert m.__pydantic_extra__ == {'y': 2}\n\n\ndef test_retain_order_of_fields():\n    class MyModel(BaseModel):\n        a: str = 'a'\n        b: str\n\n    m = MyModel.model_construct(b='b')\n\n    assert m.model_dump_json() == '{\"a\":\"a\",\"b\":\"b\"}'\n\n\ndef test_initialize_with_private_attr():\n    class MyModel(BaseModel):\n        _a: str\n\n    m = MyModel.model_construct(_a='a')\n\n    assert m._a == 'a'\n    assert '_a' in m.__pydantic_private__\n\n\ndef test_model_construct_with_alias_choices() -> None:\n    class MyModel(BaseModel):\n        a: str = Field(validation_alias=AliasChoices('aaa', 'AAA'))\n\n    assert MyModel.model_construct(a='a_value').a == 'a_value'\n    assert MyModel.model_construct(aaa='a_value').a == 'a_value'\n    assert MyModel.model_construct(AAA='a_value').a == 'a_value'\n\n\ndef test_model_construct_with_alias_path() -> None:\n    class MyModel(BaseModel):\n        a: str = Field(validation_alias=AliasPath('aaa', 'AAA'))\n\n    assert MyModel.model_construct(a='a_value').a == 'a_value'\n    assert MyModel.model_construct(aaa={'AAA': 'a_value'}).a == 'a_value'\n\n\ndef test_model_construct_with_alias_choices_and_path() -> None:\n    class MyModel(BaseModel):\n        a: str = Field(validation_alias=AliasChoices('aaa', AliasPath('AAA', 'aaa')))\n\n    assert MyModel.model_construct(a='a_value').a == 'a_value'\n    assert MyModel.model_construct(aaa='a_value').a == 'a_value'\n    assert MyModel.model_construct(AAA={'aaa': 'a_value'}).a == 'a_value'\n", "tests/test_datetime.py": "import re\nfrom datetime import date, datetime, time, timedelta, timezone\n\nimport pytest\nfrom dirty_equals import HasRepr\nfrom typing_extensions import Annotated\n\nfrom pydantic import (\n    AwareDatetime,\n    BaseModel,\n    FutureDate,\n    FutureDatetime,\n    NaiveDatetime,\n    PastDate,\n    PastDatetime,\n    ValidationError,\n    condate,\n)\n\nfrom .conftest import Err\n\n\ndef create_tz(minutes):\n    return timezone(timedelta(minutes=minutes))\n\n\n@pytest.fixture(scope='module', params=[FutureDate, Annotated[date, FutureDate()]])\ndef future_date_type(request):\n    return request.param\n\n\n@pytest.fixture(scope='module', params=[PastDate, Annotated[date, PastDate()]])\ndef past_date_type(request):\n    return request.param\n\n\n@pytest.fixture(scope='module', params=[FutureDatetime, Annotated[datetime, FutureDatetime()]])\ndef future_datetime_type(request):\n    return request.param\n\n\n@pytest.fixture(scope='module', params=[PastDatetime, Annotated[datetime, PastDatetime()]])\ndef past_datetime_type(request):\n    return request.param\n\n\n@pytest.fixture(scope='module', params=[AwareDatetime, Annotated[datetime, AwareDatetime()]])\ndef aware_datetime_type(request):\n    return request.param\n\n\n@pytest.fixture(scope='module', params=[NaiveDatetime, Annotated[datetime, NaiveDatetime()]])\ndef naive_datetime_type(request):\n    return request.param\n\n\n@pytest.fixture(scope='module', name='DateModel')\ndef date_model_fixture():\n    class DateModel(BaseModel):\n        d: date\n\n    return DateModel\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        # Valid inputs\n        (1_493_942_400, date(2017, 5, 5)),\n        (1_493_942_400_000, date(2017, 5, 5)),\n        (0, date(1970, 1, 1)),\n        ('2012-04-23', date(2012, 4, 23)),\n        (b'2012-04-23', date(2012, 4, 23)),\n        (date(2012, 4, 9), date(2012, 4, 9)),\n        (datetime(2012, 4, 9, 0, 0), date(2012, 4, 9)),\n        # Invalid inputs\n        (datetime(2012, 4, 9, 12, 15), Err('Datetimes provided to dates should have zero time - e.g. be exact dates')),\n        ('x20120423', Err('Input should be a valid date or datetime, input is too short')),\n        ('2012-04-56', Err('Input should be a valid date or datetime, day value is outside expected range')),\n        (19_999_958_400, date(2603, 10, 11)),  # just before watershed\n        (20000044800, Err('type=date_from_datetime_inexact,')),  # just after watershed\n        (1_549_238_400, date(2019, 2, 4)),  # nowish in s\n        (1_549_238_400_000, date(2019, 2, 4)),  # nowish in ms\n        (1_549_238_400_000_000, Err('Input should be a valid date or datetime, dates after 9999')),  # nowish in \u03bcs\n        (1_549_238_400_000_000_000, Err('Input should be a valid date or datetime, dates after 9999')),  # nowish in ns\n        ('infinity', Err('Input should be a valid date or datetime, input is too short')),\n        (float('inf'), Err('Input should be a valid date or datetime, dates after 9999')),\n        (int('1' + '0' * 100), Err('Input should be a valid date or datetime, dates after 9999')),\n        (1e1000, Err('Input should be a valid date or datetime, dates after 9999')),\n        (float('-infinity'), Err('Input should be a valid date or datetime, dates before 1600')),\n        (float('nan'), Err('Input should be a valid date or datetime, NaN values not permitted')),\n    ],\n)\ndef test_date_parsing(DateModel, value, result):\n    if isinstance(result, Err):\n        with pytest.raises(ValidationError, match=result.message_escaped()):\n            DateModel(d=value)\n    else:\n        assert DateModel(d=value).d == result\n\n\n@pytest.fixture(scope='module', name='TimeModel')\ndef time_model_fixture():\n    class TimeModel(BaseModel):\n        d: time\n\n    return TimeModel\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        # Valid inputs\n        ('09:15:00', time(9, 15)),\n        ('10:10', time(10, 10)),\n        ('10:20:30.400', time(10, 20, 30, 400_000)),\n        (b'10:20:30.400', time(10, 20, 30, 400_000)),\n        (time(4, 8, 16), time(4, 8, 16)),\n        (3610, time(1, 0, 10, tzinfo=timezone.utc)),\n        (3600.5, time(1, 0, 0, 500000, tzinfo=timezone.utc)),\n        (86400 - 1, time(23, 59, 59, tzinfo=timezone.utc)),\n        # Invalid inputs\n        ('4:8:16', Err('Input should be in a valid time format, invalid character in hour [type=time_parsing,')),\n        (86400, Err('Input should be in a valid time format, numeric times may not exceed 86,399 seconds')),\n        ('xxx', Err('Input should be in a valid time format, input is too short [type=time_parsing,')),\n        ('091500', Err('Input should be in a valid time format, invalid time separator, expected `:`')),\n        (b'091500', Err('Input should be in a valid time format, invalid time separator, expected `:`')),\n        ('09:15:90', Err('Input should be in a valid time format, second value is outside expected range of 0-59')),\n        ('11:05:00Y', Err('Input should be in a valid time format, invalid timezone sign')),\n        # https://github.com/pydantic/speedate/issues/10\n        ('11:05:00-05:30', time(11, 5, 0, tzinfo=create_tz(-330))),\n        ('11:05:00-0530', time(11, 5, 0, tzinfo=create_tz(-330))),\n        ('11:05:00Z', time(11, 5, 0, tzinfo=timezone.utc)),\n        ('11:05:00+00:00', time(11, 5, 0, tzinfo=timezone.utc)),\n        ('11:05-06:00', time(11, 5, 0, tzinfo=create_tz(-360))),\n        ('11:05+06:00', time(11, 5, 0, tzinfo=create_tz(360))),\n        ('11:05:00-25:00', Err('Input should be in a valid time format, timezone offset must be less than 24 hours')),\n    ],\n)\ndef test_time_parsing(TimeModel, value, result):\n    if isinstance(result, Err):\n        with pytest.raises(ValidationError, match=result.message_escaped()):\n            TimeModel(d=value)\n    else:\n        assert TimeModel(d=value).d == result\n\n\n@pytest.fixture(scope='module', name='DatetimeModel')\ndef datetime_model_fixture():\n    class DatetimeModel(BaseModel):\n        dt: datetime\n\n    return DatetimeModel\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        # Valid inputs\n        # values in seconds\n        (1_494_012_444.883_309, datetime(2017, 5, 5, 19, 27, 24, 883_309, tzinfo=timezone.utc)),\n        (1_494_012_444, datetime(2017, 5, 5, 19, 27, 24, tzinfo=timezone.utc)),\n        # values in ms\n        (1_494_012_444_000, datetime(2017, 5, 5, 19, 27, 24, tzinfo=timezone.utc)),\n        ('2012-04-23T09:15:00', datetime(2012, 4, 23, 9, 15)),\n        ('2012-04-23T09:15:00Z', datetime(2012, 4, 23, 9, 15, 0, 0, timezone.utc)),\n        ('2012-04-23T10:20:30.400+02:30', datetime(2012, 4, 23, 10, 20, 30, 400_000, create_tz(150))),\n        ('2012-04-23T10:20:30.400+02:00', datetime(2012, 4, 23, 10, 20, 30, 400_000, create_tz(120))),\n        ('2012-04-23T10:20:30.400-02:00', datetime(2012, 4, 23, 10, 20, 30, 400_000, create_tz(-120))),\n        (b'2012-04-23T10:20:30.400-02:00', datetime(2012, 4, 23, 10, 20, 30, 400_000, create_tz(-120))),\n        (datetime(2017, 5, 5), datetime(2017, 5, 5)),\n        (0, datetime(1970, 1, 1, 0, 0, 0, tzinfo=timezone.utc)),\n        # Numeric inputs as strings\n        ('1494012444.883309', datetime(2017, 5, 5, 19, 27, 24, 883309, tzinfo=timezone.utc)),\n        ('1494012444', datetime(2017, 5, 5, 19, 27, 24, tzinfo=timezone.utc)),\n        (b'1494012444', datetime(2017, 5, 5, 19, 27, 24, tzinfo=timezone.utc)),\n        ('1494012444000.883309', datetime(2017, 5, 5, 19, 27, 24, 883301, tzinfo=timezone.utc)),\n        ('-1494012444000.883309', datetime(1922, 8, 29, 4, 32, 35, 999000, tzinfo=timezone.utc)),\n        (19_999_999_999, datetime(2603, 10, 11, 11, 33, 19, tzinfo=timezone.utc)),  # just before watershed\n        (20_000_000_001, datetime(1970, 8, 20, 11, 33, 20, 1000, tzinfo=timezone.utc)),  # just after watershed\n        (1_549_316_052, datetime(2019, 2, 4, 21, 34, 12, 0, tzinfo=timezone.utc)),  # nowish in s\n        (1_549_316_052_104, datetime(2019, 2, 4, 21, 34, 12, 104_000, tzinfo=timezone.utc)),  # nowish in ms\n        # Invalid inputs\n        (1_549_316_052_104_324, Err('Input should be a valid datetime, dates after 9999')),  # nowish in \u03bcs\n        (1_549_316_052_104_324_096, Err('Input should be a valid datetime, dates after 9999')),  # nowish in ns\n        (float('inf'), Err('Input should be a valid datetime, dates after 9999')),\n        (float('-inf'), Err('Input should be a valid datetime, dates before 1600')),\n        (1e50, Err('Input should be a valid datetime, dates after 9999')),\n        (float('nan'), Err('Input should be a valid datetime, NaN values not permitted')),\n    ],\n)\ndef test_datetime_parsing(DatetimeModel, value, result):\n    if isinstance(result, Err):\n        with pytest.raises(ValidationError, match=result.message_escaped()):\n            DatetimeModel(dt=value)\n    else:\n        assert DatetimeModel(dt=value).dt == result\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        # Invalid inputs\n        ('2012-4-9 4:8:16', Err('Input should be a valid datetime or date, invalid character in month')),\n        ('x20120423091500', Err('Input should be a valid datetime or date, invalid character in year')),\n        ('2012-04-56T09:15:90', Err('Input should be a valid datetime or date, day value is outside expected range')),\n        (\n            '2012-04-23T11:05:00-25:00',\n            Err('Input should be a valid datetime or date, unexpected extra characters at the end of the input'),\n        ),\n        ('infinity', Err('Input should be a valid datetime or date, input is too short')),\n    ],\n)\ndef test_datetime_parsing_from_str(DatetimeModel, value, result):\n    if isinstance(result, Err):\n        with pytest.raises(ValidationError, match=result.message_escaped()):\n            DatetimeModel(dt=value)\n    else:\n        assert DatetimeModel(dt=value).dt == result\n\n\ndef test_aware_datetime_validation_success(aware_datetime_type):\n    class Model(BaseModel):\n        foo: aware_datetime_type\n\n    value = datetime.now(tz=timezone.utc)\n\n    assert Model(foo=value).foo == value\n\n\ndef test_aware_datetime_validation_fails(aware_datetime_type):\n    class Model(BaseModel):\n        foo: aware_datetime_type\n\n    value = datetime.now()\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'timezone_aware',\n            'loc': ('foo',),\n            'msg': 'Input should have timezone info',\n            'input': value,\n        }\n    ]\n\n\ndef test_naive_datetime_validation_success(naive_datetime_type):\n    class Model(BaseModel):\n        foo: naive_datetime_type\n\n    value = datetime.now()\n\n    assert Model(foo=value).foo == value\n\n\ndef test_naive_datetime_validation_fails(naive_datetime_type):\n    class Model(BaseModel):\n        foo: naive_datetime_type\n\n    value = datetime.now(tz=timezone.utc)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'timezone_naive',\n            'loc': ('foo',),\n            'msg': 'Input should not have timezone info',\n            'input': value,\n        }\n    ]\n\n\n@pytest.fixture(scope='module', name='TimedeltaModel')\ndef timedelta_model_fixture():\n    class TimedeltaModel(BaseModel):\n        d: timedelta\n\n    return TimedeltaModel\n\n\n@pytest.mark.parametrize(\n    'delta',\n    [\n        timedelta(days=4, minutes=15, seconds=30, milliseconds=100),  # fractions of seconds\n        timedelta(hours=10, minutes=15, seconds=30),  # hours, minutes, seconds\n        timedelta(days=4, minutes=15, seconds=30),  # multiple days\n        timedelta(days=1, minutes=00, seconds=00),  # single day\n        timedelta(days=-4, minutes=15, seconds=30),  # negative durations\n        timedelta(minutes=15, seconds=30),  # minute & seconds\n        timedelta(seconds=30),  # seconds\n    ],\n)\ndef test_parse_python_format(TimedeltaModel, delta):\n    assert TimedeltaModel(d=delta).d == delta\n    # assert TimedeltaModel(d=str(delta)).d == delta\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        # seconds\n        (timedelta(seconds=30), timedelta(seconds=30)),\n        (30, timedelta(seconds=30)),\n        (30.1, timedelta(seconds=30, milliseconds=100)),\n        (9.9e-05, timedelta(microseconds=99)),\n        # minutes seconds\n        ('00:15:30', timedelta(minutes=15, seconds=30)),\n        ('00:05:30', timedelta(minutes=5, seconds=30)),\n        # hours minutes seconds\n        ('10:15:30', timedelta(hours=10, minutes=15, seconds=30)),\n        ('01:15:30', timedelta(hours=1, minutes=15, seconds=30)),\n        # ('100:200:300', timedelta(hours=100, minutes=200, seconds=300)),\n        # days\n        ('4d,00:15:30', timedelta(days=4, minutes=15, seconds=30)),\n        ('4d,10:15:30', timedelta(days=4, hours=10, minutes=15, seconds=30)),\n        # fractions of seconds\n        ('00:15:30.1', timedelta(minutes=15, seconds=30, milliseconds=100)),\n        ('00:15:30.01', timedelta(minutes=15, seconds=30, milliseconds=10)),\n        ('00:15:30.001', timedelta(minutes=15, seconds=30, milliseconds=1)),\n        ('00:15:30.0001', timedelta(minutes=15, seconds=30, microseconds=100)),\n        ('00:15:30.00001', timedelta(minutes=15, seconds=30, microseconds=10)),\n        ('00:15:30.000001', timedelta(minutes=15, seconds=30, microseconds=1)),\n        (b'00:15:30.000001', timedelta(minutes=15, seconds=30, microseconds=1)),\n        # negative\n        ('-4d,00:15:30', timedelta(days=-4, minutes=-15, seconds=-30)),\n        (-172800, timedelta(days=-2)),\n        ('-00:15:30', timedelta(minutes=-15, seconds=-30)),\n        ('-01:15:30', timedelta(hours=-1, minutes=-15, seconds=-30)),\n        (-30.1, timedelta(seconds=-30, milliseconds=-100)),\n        # iso_8601\n        ('30', Err('Input should be a valid timedelta, \"day\" identifier')),\n        ('P4Y', timedelta(days=1460)),\n        ('P4M', timedelta(days=120)),\n        ('P4W', timedelta(days=28)),\n        ('P4D', timedelta(days=4)),\n        ('P0.5D', timedelta(hours=12)),\n        ('PT5H', timedelta(hours=5)),\n        ('PT5M', timedelta(minutes=5)),\n        ('PT5S', timedelta(seconds=5)),\n        ('PT0.000005S', timedelta(microseconds=5)),\n        (b'PT0.000005S', timedelta(microseconds=5)),\n    ],\n)\ndef test_parse_durations(TimedeltaModel, value, result):\n    if isinstance(result, Err):\n        with pytest.raises(ValidationError, match=result.message_escaped()):\n            TimedeltaModel(d=value)\n    else:\n        assert TimedeltaModel(d=value).d == result\n\n\n@pytest.mark.parametrize(\n    'field, value, error_message',\n    [\n        ('dt', [], 'Input should be a valid datetime'),\n        ('dt', {}, 'Input should be a valid datetime'),\n        ('dt', object, 'Input should be a valid datetime'),\n        ('d', [], 'Input should be a valid date'),\n        ('d', {}, 'Input should be a valid date'),\n        ('d', object, 'Input should be a valid date'),\n        ('t', [], 'Input should be a valid time'),\n        ('t', {}, 'Input should be a valid time'),\n        ('t', object, 'Input should be a valid time'),\n        ('td', [], 'Input should be a valid timedelta'),\n        ('td', {}, 'Input should be a valid timedelta'),\n        ('td', object, 'Input should be a valid timedelta'),\n    ],\n)\ndef test_model_type_errors(field, value, error_message):\n    class Model(BaseModel):\n        dt: datetime = None\n        d: date = None\n        t: time = None\n        td: timedelta = None\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(**{field: value})\n    assert len(exc_info.value.errors(include_url=False)) == 1\n    error = exc_info.value.errors(include_url=False)[0]\n    assert error['msg'] == error_message\n\n\n@pytest.mark.parametrize('field', ['dt', 'd', 't', 'dt'])\ndef test_unicode_decode_error(field):\n    class Model(BaseModel):\n        dt: datetime = None\n        d: date = None\n        t: time = None\n        td: timedelta = None\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(**{field: b'\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81'})\n    assert exc_info.value.error_count() == 1\n    # errors vary\n\n\ndef test_nan():\n    class Model(BaseModel):\n        dt: datetime\n        d: date\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(dt=float('nan'), d=float('nan'))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'datetime_parsing',\n            'loc': ('dt',),\n            'msg': 'Input should be a valid datetime, NaN values not permitted',\n            'input': HasRepr('nan'),\n            'ctx': {'error': 'NaN values not permitted'},\n        },\n        {\n            'type': 'date_from_datetime_parsing',\n            'loc': ('d',),\n            'msg': 'Input should be a valid date or datetime, NaN values not permitted',\n            'input': HasRepr('nan'),\n            'ctx': {'error': 'NaN values not permitted'},\n        },\n    ]\n\n\n@pytest.mark.parametrize(\n    'constraint,msg,ok_value,error_value',\n    [\n        ('gt', 'greater than', date(2020, 1, 2), date(2019, 12, 31)),\n        ('gt', 'greater than', date(2020, 1, 2), date(2020, 1, 1)),\n        ('ge', 'greater than or equal to', date(2020, 1, 2), date(2019, 12, 31)),\n        ('ge', 'greater than or equal to', date(2020, 1, 1), date(2019, 12, 31)),\n        ('lt', 'less than', date(2019, 12, 31), date(2020, 1, 2)),\n        ('lt', 'less than', date(2019, 12, 31), date(2020, 1, 1)),\n        ('le', 'less than or equal to', date(2019, 12, 31), date(2020, 1, 2)),\n        ('le', 'less than or equal to', date(2020, 1, 1), date(2020, 1, 2)),\n    ],\n)\ndef test_date_constraints(constraint, msg, ok_value, error_value):\n    class Model(BaseModel):\n        a: condate(**{constraint: date(2020, 1, 1)})\n\n    assert Model(a=ok_value).model_dump() == {'a': ok_value}\n\n    with pytest.raises(ValidationError, match=re.escape(f'Input should be {msg} 2020-01-01')):\n        Model(a=error_value)\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    (\n        ('1996-01-22', date(1996, 1, 22)),\n        (date(1996, 1, 22), date(1996, 1, 22)),\n    ),\n)\ndef test_past_date_validation_success(value, result, past_date_type):\n    class Model(BaseModel):\n        foo: past_date_type\n\n    assert Model(foo=value).foo == result\n\n\n@pytest.mark.parametrize(\n    'value',\n    (\n        date.today(),\n        date.today() + timedelta(1),\n        '2064-06-01',\n    ),\n)\ndef test_past_date_validation_fails(value, past_date_type):\n    class Model(BaseModel):\n        foo: past_date_type\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'date_past',\n            'loc': ('foo',),\n            'msg': 'Date should be in the past',\n            'input': value,\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    (\n        (date.today() + timedelta(1), date.today() + timedelta(1)),\n        ('2064-06-01', date(2064, 6, 1)),\n    ),\n)\ndef test_future_date_validation_success(value, result, future_date_type):\n    class Model(BaseModel):\n        foo: future_date_type\n\n    assert Model(foo=value).foo == result\n\n\n@pytest.mark.parametrize(\n    'value',\n    (\n        date.today(),\n        date.today() - timedelta(1),\n        '1996-01-22',\n    ),\n)\ndef test_future_date_validation_fails(value, future_date_type):\n    class Model(BaseModel):\n        foo: future_date_type\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'date_future',\n            'loc': ('foo',),\n            'msg': 'Date should be in the future',\n            'input': value,\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    (\n        ('1996-01-22T10:20:30', datetime(1996, 1, 22, 10, 20, 30)),\n        (datetime(1996, 1, 22, 10, 20, 30), datetime(1996, 1, 22, 10, 20, 30)),\n    ),\n)\ndef test_past_datetime_validation_success(value, result, past_datetime_type):\n    class Model(BaseModel):\n        foo: past_datetime_type\n\n    assert Model(foo=value).foo == result\n\n\n@pytest.mark.parametrize(\n    'value',\n    (\n        datetime.now() + timedelta(1),\n        '2064-06-01T10:20:30',\n    ),\n)\ndef test_past_datetime_validation_fails(value, past_datetime_type):\n    class Model(BaseModel):\n        foo: past_datetime_type\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'datetime_past',\n            'loc': ('foo',),\n            'msg': 'Input should be in the past',\n            'input': value,\n        }\n    ]\n\n\ndef test_future_datetime_validation_success(future_datetime_type):\n    class Model(BaseModel):\n        foo: future_datetime_type\n\n    d = datetime.now() + timedelta(1)\n    assert Model(foo=d).foo == d\n    assert Model(foo='2064-06-01T10:20:30').foo == datetime(2064, 6, 1, 10, 20, 30)\n\n\n@pytest.mark.parametrize(\n    'value',\n    (\n        datetime.now(),\n        datetime.now() - timedelta(1),\n        '1996-01-22T10:20:30',\n    ),\n)\ndef test_future_datetime_validation_fails(value, future_datetime_type):\n    class Model(BaseModel):\n        foo: future_datetime_type\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'datetime_future',\n            'loc': ('foo',),\n            'msg': 'Input should be in the future',\n            'input': value,\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    'annotation',\n    (\n        PastDate,\n        PastDatetime,\n        FutureDate,\n        FutureDatetime,\n        NaiveDatetime,\n        AwareDatetime,\n    ),\n)\ndef test_invalid_annotated_type(annotation):\n    with pytest.raises(TypeError, match=f\"'{annotation.__name__}' cannot annotate 'str'.\"):\n\n        class Model(BaseModel):\n            foo: Annotated[str, annotation()]\n\n\ndef test_tzinfo_could_be_reused():\n    class Model(BaseModel):\n        value: datetime\n\n    m = Model(value='2015-10-21T15:28:00.000000+01:00')\n    assert m.model_dump_json() == '{\"value\":\"2015-10-21T15:28:00+01:00\"}'\n\n    target = datetime(1955, 11, 12, 14, 38, tzinfo=m.value.tzinfo)\n    assert target == datetime(1955, 11, 12, 14, 38, tzinfo=timezone(timedelta(hours=1)))\n\n    now = datetime.now(tz=m.value.tzinfo)\n    assert isinstance(now, datetime)\n\n\ndef test_datetime_from_date_str():\n    class Model(BaseModel):\n        value: datetime\n\n    m = Model(value='2015-10-21')\n    assert m.value == datetime(2015, 10, 21, 0, 0)\n", "tests/test_type_adapter.py": "import json\nimport sys\nfrom dataclasses import dataclass\nfrom datetime import date, datetime\nfrom typing import Any, Dict, ForwardRef, Generic, List, NamedTuple, Optional, Tuple, TypeVar, Union\n\nimport pytest\nfrom pydantic_core import ValidationError\nfrom typing_extensions import Annotated, TypeAlias, TypedDict\n\nfrom pydantic import BaseModel, Field, TypeAdapter, ValidationInfo, create_model, field_validator\nfrom pydantic._internal._typing_extra import annotated_type\nfrom pydantic.config import ConfigDict\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\nfrom pydantic.errors import PydanticUserError\nfrom pydantic.type_adapter import _type_has_config\n\nItemType = TypeVar('ItemType')\n\nNestedList = List[List[ItemType]]\n\nDEFER_ENABLE_MODE = ('model', 'type_adapter')\n\n\nclass PydanticModel(BaseModel):\n    x: int\n\n\nT = TypeVar('T')\n\n\nclass GenericPydanticModel(BaseModel, Generic[T]):\n    x: NestedList[T]\n\n\nclass SomeTypedDict(TypedDict):\n    x: int\n\n\nclass SomeNamedTuple(NamedTuple):\n    x: int\n\n\n@pytest.mark.parametrize(\n    'tp, val, expected',\n    [\n        (PydanticModel, PydanticModel(x=1), PydanticModel(x=1)),\n        (PydanticModel, {'x': 1}, PydanticModel(x=1)),\n        (SomeTypedDict, {'x': 1}, {'x': 1}),\n        (SomeNamedTuple, SomeNamedTuple(x=1), SomeNamedTuple(x=1)),\n        (List[str], ['1', '2'], ['1', '2']),\n        (Tuple[str], ('1',), ('1',)),\n        (Tuple[str, int], ('1', 1), ('1', 1)),\n        (Tuple[str, ...], ('1',), ('1',)),\n        (Dict[str, int], {'foo': 123}, {'foo': 123}),\n        (Union[int, str], 1, 1),\n        (Union[int, str], '2', '2'),\n        (GenericPydanticModel[int], {'x': [[1]]}, GenericPydanticModel[int](x=[[1]])),\n        (GenericPydanticModel[int], {'x': [['1']]}, GenericPydanticModel[int](x=[[1]])),\n        (NestedList[int], [[1]], [[1]]),\n        (NestedList[int], [['1']], [[1]]),\n    ],\n)\ndef test_types(tp: Any, val: Any, expected: Any):\n    v = TypeAdapter(tp).validate_python\n    assert expected == v(val)\n\n\nIntList = List[int]\nOuterDict = Dict[str, 'IntList']\n\n\n@pytest.mark.parametrize('defer_build', [False, True])\n@pytest.mark.parametrize('method', ['validate', 'serialize', 'json_schema', 'json_schemas'])\ndef test_global_namespace_variables(defer_build: bool, method: str, generate_schema_calls):\n    config = ConfigDict(defer_build=True, experimental_defer_build_mode=DEFER_ENABLE_MODE) if defer_build else None\n    ta = TypeAdapter(OuterDict, config=config)\n\n    assert generate_schema_calls.count == (0 if defer_build else 1), 'Should be built deferred'\n\n    if method == 'validate':\n        assert ta.validate_python({'foo': [1, '2']}) == {'foo': [1, 2]}\n    elif method == 'serialize':\n        assert ta.dump_python({'foo': [1, 2]}) == {'foo': [1, 2]}\n    elif method == 'json_schema':\n        assert ta.json_schema()['type'] == 'object'\n    else:\n        assert method == 'json_schemas'\n        schemas, _ = TypeAdapter.json_schemas([(OuterDict, 'validation', ta)])\n        assert schemas[(OuterDict, 'validation')]['type'] == 'object'\n\n\n@pytest.mark.parametrize('defer_build', [False, True])\n@pytest.mark.parametrize('method', ['validate', 'serialize', 'json_schema', 'json_schemas'])\ndef test_model_global_namespace_variables(defer_build: bool, method: str, generate_schema_calls):\n    class MyModel(BaseModel):\n        model_config = (\n            ConfigDict(defer_build=True, experimental_defer_build_mode=DEFER_ENABLE_MODE) if defer_build else None\n        )\n        x: OuterDict\n\n    ta = TypeAdapter(MyModel)\n\n    assert generate_schema_calls.count == (0 if defer_build else 1), 'Should be built deferred'\n\n    if method == 'validate':\n        assert ta.validate_python({'x': {'foo': [1, '2']}}) == MyModel(x={'foo': [1, 2]})\n    elif method == 'serialize':\n        assert ta.dump_python(MyModel(x={'foo': [1, 2]})) == {'x': {'foo': [1, 2]}}\n    elif method == 'json_schema':\n        assert ta.json_schema()['title'] == 'MyModel'\n    else:\n        assert method == 'json_schemas'\n        _, json_schema = TypeAdapter.json_schemas([(MyModel, 'validation', TypeAdapter(MyModel))])\n        assert 'MyModel' in json_schema['$defs']\n\n\n@pytest.mark.parametrize('defer_build', [False, True])\n@pytest.mark.parametrize('method', ['validate', 'serialize', 'json_schema', 'json_schemas'])\ndef test_local_namespace_variables(defer_build: bool, method: str, generate_schema_calls):\n    IntList = List[int]  # noqa: F841\n    OuterDict = Dict[str, 'IntList']\n\n    config = ConfigDict(defer_build=True, experimental_defer_build_mode=DEFER_ENABLE_MODE) if defer_build else None\n    ta = TypeAdapter(OuterDict, config=config)\n\n    assert generate_schema_calls.count == (0 if defer_build else 1), 'Should be built deferred'\n\n    if method == 'validate':\n        assert ta.validate_python({'foo': [1, '2']}) == {'foo': [1, 2]}\n    elif method == 'serialize':\n        assert ta.dump_python({'foo': [1, 2]}) == {'foo': [1, 2]}\n    elif method == 'json_schema':\n        assert ta.json_schema()['type'] == 'object'\n    else:\n        assert method == 'json_schemas'\n        schemas, _ = TypeAdapter.json_schemas([(OuterDict, 'validation', ta)])\n        assert schemas[(OuterDict, 'validation')]['type'] == 'object'\n\n\n@pytest.mark.parametrize('defer_build', [False, True])\n@pytest.mark.parametrize('method', ['validate', 'serialize', 'json_schema', 'json_schemas'])\ndef test_model_local_namespace_variables(defer_build: bool, method: str, generate_schema_calls):\n    IntList = List[int]  # noqa: F841\n\n    class MyModel(BaseModel):\n        model_config = (\n            ConfigDict(defer_build=True, experimental_defer_build_mode=DEFER_ENABLE_MODE) if defer_build else None\n        )\n        x: Dict[str, 'IntList']\n\n    ta = TypeAdapter(MyModel)\n\n    assert generate_schema_calls.count == (0 if defer_build else 1), 'Should be built deferred'\n\n    if method == 'validate':\n        assert ta.validate_python({'x': {'foo': [1, '2']}}) == MyModel(x={'foo': [1, 2]})\n    elif method == 'serialize':\n        assert ta.dump_python(MyModel(x={'foo': [1, 2]})) == {'x': {'foo': [1, 2]}}\n    elif method == 'json_schema':\n        assert ta.json_schema()['title'] == 'MyModel'\n    else:\n        assert method == 'json_schemas'\n        _, json_schema = TypeAdapter.json_schemas([(MyModel, 'validation', ta)])\n        assert 'MyModel' in json_schema['$defs']\n\n\n@pytest.mark.parametrize('defer_build', [False, True])\n@pytest.mark.parametrize('method', ['validate', 'serialize', 'json_schema', 'json_schemas'])\n@pytest.mark.skipif(sys.version_info < (3, 9), reason=\"ForwardRef doesn't accept module as a parameter in Python < 3.9\")\ndef test_top_level_fwd_ref(defer_build: bool, method: str, generate_schema_calls):\n    config = ConfigDict(defer_build=True, experimental_defer_build_mode=DEFER_ENABLE_MODE) if defer_build else None\n\n    FwdRef = ForwardRef('OuterDict', module=__name__)\n    ta = TypeAdapter(FwdRef, config=config)\n\n    assert generate_schema_calls.count == (0 if defer_build else 1), 'Should be built deferred'\n\n    if method == 'validate':\n        assert ta.validate_python({'foo': [1, '2']}) == {'foo': [1, 2]}\n    elif method == 'serialize':\n        assert ta.dump_python({'foo': [1, 2]}) == {'foo': [1, 2]}\n    elif method == 'json_schema':\n        assert ta.json_schema()['type'] == 'object'\n    else:\n        assert method == 'json_schemas'\n        schemas, _ = TypeAdapter.json_schemas([(FwdRef, 'validation', ta)])\n        assert schemas[(FwdRef, 'validation')]['type'] == 'object'\n\n\nMyUnion: TypeAlias = 'Union[str, int]'\n\n\ndef test_type_alias():\n    MyList = List[MyUnion]\n    v = TypeAdapter(MyList).validate_python\n    res = v([1, '2'])\n    assert res == [1, '2']\n\n\ndef test_validate_python_strict() -> None:\n    class Model(TypedDict):\n        x: int\n\n    class ModelStrict(Model):\n        __pydantic_config__ = ConfigDict(strict=True)  # type: ignore\n\n    lax_validator = TypeAdapter(Model)\n    strict_validator = TypeAdapter(ModelStrict)\n\n    assert lax_validator.validate_python({'x': '1'}, strict=None) == Model(x=1)\n    assert lax_validator.validate_python({'x': '1'}, strict=False) == Model(x=1)\n    with pytest.raises(ValidationError) as exc_info:\n        lax_validator.validate_python({'x': '1'}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        strict_validator.validate_python({'x': '1'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n    assert strict_validator.validate_python({'x': '1'}, strict=False) == Model(x=1)\n    with pytest.raises(ValidationError) as exc_info:\n        strict_validator.validate_python({'x': '1'}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n\n@pytest.mark.xfail(reason='Need to fix this in https://github.com/pydantic/pydantic/pull/5944')\ndef test_validate_json_strict() -> None:\n    class Model(TypedDict):\n        x: int\n\n    class ModelStrict(Model):\n        __pydantic_config__ = ConfigDict(strict=True)  # type: ignore\n\n    lax_validator = TypeAdapter(Model, config=ConfigDict(strict=False))\n    strict_validator = TypeAdapter(ModelStrict)\n\n    assert lax_validator.validate_json(json.dumps({'x': '1'}), strict=None) == Model(x=1)\n    assert lax_validator.validate_json(json.dumps({'x': '1'}), strict=False) == Model(x=1)\n    with pytest.raises(ValidationError) as exc_info:\n        lax_validator.validate_json(json.dumps({'x': '1'}), strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        strict_validator.validate_json(json.dumps({'x': '1'}), strict=None)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n    assert strict_validator.validate_json(json.dumps({'x': '1'}), strict=False) == Model(x=1)\n    with pytest.raises(ValidationError) as exc_info:\n        strict_validator.validate_json(json.dumps({'x': '1'}), strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n\ndef test_validate_python_context() -> None:\n    contexts: List[Any] = [None, None, {'foo': 'bar'}]\n\n    class Model(BaseModel):\n        x: int\n\n        @field_validator('x')\n        def val_x(cls, v: int, info: ValidationInfo) -> int:\n            assert info.context == contexts.pop(0)\n            return v\n\n    validator = TypeAdapter(Model)\n    validator.validate_python({'x': 1})\n    validator.validate_python({'x': 1}, context=None)\n    validator.validate_python({'x': 1}, context={'foo': 'bar'})\n    assert contexts == []\n\n\ndef test_validate_json_context() -> None:\n    contexts: List[Any] = [None, None, {'foo': 'bar'}]\n\n    class Model(BaseModel):\n        x: int\n\n        @field_validator('x')\n        def val_x(cls, v: int, info: ValidationInfo) -> int:\n            assert info.context == contexts.pop(0)\n            return v\n\n    validator = TypeAdapter(Model)\n    validator.validate_json(json.dumps({'x': 1}))\n    validator.validate_json(json.dumps({'x': 1}), context=None)\n    validator.validate_json(json.dumps({'x': 1}), context={'foo': 'bar'})\n    assert contexts == []\n\n\ndef test_validate_python_from_attributes() -> None:\n    class Model(BaseModel):\n        x: int\n\n    class ModelFromAttributesTrue(Model):\n        model_config = ConfigDict(from_attributes=True)\n\n    class ModelFromAttributesFalse(Model):\n        model_config = ConfigDict(from_attributes=False)\n\n    @dataclass\n    class UnrelatedClass:\n        x: int = 1\n\n    input = UnrelatedClass(1)\n\n    ta = TypeAdapter(Model)\n\n    for from_attributes in (False, None):\n        with pytest.raises(ValidationError) as exc_info:\n            ta.validate_python(UnrelatedClass(), from_attributes=from_attributes)\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'type': 'model_type',\n                'loc': (),\n                'msg': 'Input should be a valid dictionary or instance of Model',\n                'input': input,\n                'ctx': {'class_name': 'Model'},\n            }\n        ]\n\n    res = ta.validate_python(UnrelatedClass(), from_attributes=True)\n    assert res == Model(x=1)\n\n    ta = TypeAdapter(ModelFromAttributesTrue)\n\n    with pytest.raises(ValidationError) as exc_info:\n        ta.validate_python(UnrelatedClass(), from_attributes=False)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': (),\n            'msg': 'Input should be a valid dictionary or instance of ModelFromAttributesTrue',\n            'input': input,\n            'ctx': {'class_name': 'ModelFromAttributesTrue'},\n        }\n    ]\n\n    for from_attributes in (True, None):\n        res = ta.validate_python(UnrelatedClass(), from_attributes=from_attributes)\n        assert res == ModelFromAttributesTrue(x=1)\n\n    ta = TypeAdapter(ModelFromAttributesFalse)\n\n    for from_attributes in (False, None):\n        with pytest.raises(ValidationError) as exc_info:\n            ta.validate_python(UnrelatedClass(), from_attributes=from_attributes)\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'type': 'model_type',\n                'loc': (),\n                'msg': 'Input should be a valid dictionary or instance of ModelFromAttributesFalse',\n                'input': input,\n                'ctx': {'class_name': 'ModelFromAttributesFalse'},\n            }\n        ]\n\n    res = ta.validate_python(UnrelatedClass(), from_attributes=True)\n    assert res == ModelFromAttributesFalse(x=1)\n\n\n@pytest.mark.parametrize(\n    'field_type,input_value,expected,raises_match,strict',\n    [\n        (bool, 'true', True, None, False),\n        (bool, 'true', True, None, True),\n        (bool, 'false', False, None, False),\n        (bool, 'e', ValidationError, 'type=bool_parsing', False),\n        (int, '1', 1, None, False),\n        (int, '1', 1, None, True),\n        (int, 'xxx', ValidationError, 'type=int_parsing', True),\n        (float, '1.1', 1.1, None, False),\n        (float, '1.10', 1.1, None, False),\n        (float, '1.1', 1.1, None, True),\n        (float, '1.10', 1.1, None, True),\n        (date, '2017-01-01', date(2017, 1, 1), None, False),\n        (date, '2017-01-01', date(2017, 1, 1), None, True),\n        (date, '2017-01-01T12:13:14.567', ValidationError, 'type=date_from_datetime_inexact', False),\n        (date, '2017-01-01T12:13:14.567', ValidationError, 'type=date_parsing', True),\n        (date, '2017-01-01T00:00:00', date(2017, 1, 1), None, False),\n        (date, '2017-01-01T00:00:00', ValidationError, 'type=date_parsing', True),\n        (datetime, '2017-01-01T12:13:14.567', datetime(2017, 1, 1, 12, 13, 14, 567_000), None, False),\n        (datetime, '2017-01-01T12:13:14.567', datetime(2017, 1, 1, 12, 13, 14, 567_000), None, True),\n    ],\n    ids=repr,\n)\n@pytest.mark.parametrize('defer_build', [False, True])\ndef test_validate_strings(\n    field_type, input_value, expected, raises_match, strict, defer_build: bool, generate_schema_calls\n):\n    config = ConfigDict(defer_build=True, experimental_defer_build_mode=DEFER_ENABLE_MODE) if defer_build else None\n    ta = TypeAdapter(field_type, config=config)\n\n    assert generate_schema_calls.count == (0 if defer_build else 1), 'Should be built deferred'\n\n    if raises_match is not None:\n        with pytest.raises(expected, match=raises_match):\n            ta.validate_strings(input_value, strict=strict)\n    else:\n        assert ta.validate_strings(input_value, strict=strict) == expected\n\n    assert generate_schema_calls.count == 1, 'Should not build duplicates'\n\n\n@pytest.mark.parametrize('strict', [True, False])\ndef test_validate_strings_dict(strict):\n    assert TypeAdapter(Dict[int, date]).validate_strings({'1': '2017-01-01', '2': '2017-01-02'}, strict=strict) == {\n        1: date(2017, 1, 1),\n        2: date(2017, 1, 2),\n    }\n\n\ndef test_annotated_type_disallows_config() -> None:\n    class Model(BaseModel):\n        x: int\n\n    with pytest.raises(PydanticUserError, match='Cannot use `config`'):\n        TypeAdapter(Annotated[Model, ...], config=ConfigDict(strict=False))\n\n\ndef test_ta_config_with_annotated_type() -> None:\n    class TestValidator(BaseModel):\n        x: str\n\n        model_config = ConfigDict(str_to_lower=True)\n\n    assert TestValidator(x='ABC').x == 'abc'\n    assert TypeAdapter(TestValidator).validate_python({'x': 'ABC'}).x == 'abc'\n    assert TypeAdapter(Annotated[TestValidator, ...]).validate_python({'x': 'ABC'}).x == 'abc'\n\n    class TestSerializer(BaseModel):\n        some_bytes: bytes\n        model_config = ConfigDict(ser_json_bytes='base64')\n\n    result = TestSerializer(some_bytes=b'\\xaa')\n    assert result.model_dump(mode='json') == {'some_bytes': 'qg=='}\n    assert TypeAdapter(TestSerializer).dump_python(result, mode='json') == {'some_bytes': 'qg=='}\n\n    # cases where SchemaSerializer is constructed within TypeAdapter's __init__\n    assert TypeAdapter(Annotated[TestSerializer, ...]).dump_python(result, mode='json') == {'some_bytes': 'qg=='}\n    assert TypeAdapter(Annotated[List[TestSerializer], ...]).dump_python([result], mode='json') == [\n        {'some_bytes': 'qg=='}\n    ]\n\n\ndef test_eval_type_backport():\n    v = TypeAdapter('list[int | str]').validate_python\n    assert v([1, '2']) == [1, '2']\n    with pytest.raises(ValidationError) as exc_info:\n        v([{'not a str or int'}])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_type',\n            'loc': (0, 'int'),\n            'msg': 'Input should be a valid integer',\n            'input': {'not a str or int'},\n        },\n        {\n            'type': 'string_type',\n            'loc': (0, 'str'),\n            'msg': 'Input should be a valid string',\n            'input': {'not a str or int'},\n        },\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v('not a list')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'list_type', 'loc': (), 'msg': 'Input should be a valid list', 'input': 'not a list'}\n    ]\n\n\ndef defer_build_test_models(config: ConfigDict) -> List[Any]:\n    class Model(BaseModel):\n        model_config = config\n        x: int\n\n    class SubModel(Model):\n        y: Optional[int] = None\n\n    @pydantic_dataclass(config=config)\n    class DataClassModel:\n        x: int\n\n    @pydantic_dataclass\n    class SubDataClassModel(DataClassModel):\n        y: Optional[int] = None\n\n    class TypedDictModel(TypedDict):\n        __pydantic_config__ = config  # type: ignore\n        x: int\n\n    models = [\n        Model,\n        SubModel,\n        create_model('DynamicModel', __base__=Model),\n        create_model('DynamicSubModel', __base__=SubModel),\n        DataClassModel,\n        SubDataClassModel,\n        TypedDictModel,\n        Dict[str, int],\n    ]\n    return [\n        *models,\n        # FastAPI heavily uses Annotated so test that as well\n        *[Annotated[model, Field(title='abc')] for model in models],\n    ]\n\n\nCONFIGS = [\n    ConfigDict(defer_build=False, experimental_defer_build_mode=('model',)),\n    ConfigDict(defer_build=False, experimental_defer_build_mode=DEFER_ENABLE_MODE),\n    ConfigDict(defer_build=True, experimental_defer_build_mode=('model',)),\n    ConfigDict(defer_build=True, experimental_defer_build_mode=DEFER_ENABLE_MODE),\n]\nMODELS_CONFIGS: List[Tuple[Any, ConfigDict]] = [\n    (model, config) for config in CONFIGS for model in defer_build_test_models(config)\n]\n\n\n@pytest.mark.parametrize('model, config', MODELS_CONFIGS)\n@pytest.mark.parametrize('method', ['schema', 'validate', 'dump'])\ndef test_core_schema_respects_defer_build(model: Any, config: ConfigDict, method: str, generate_schema_calls) -> None:\n    type_ = annotated_type(model) or model\n    dumped = dict(x=1) if 'Dict[' in str(type_) else type_(x=1)\n    generate_schema_calls.reset()\n\n    type_adapter = TypeAdapter(model) if _type_has_config(model) else TypeAdapter(model, config=config)\n\n    if config['defer_build'] and 'type_adapter' in config['experimental_defer_build_mode']:\n        assert generate_schema_calls.count == 0, 'Should be built deferred'\n        assert type_adapter._core_schema is None, 'Should be initialized deferred'\n        assert type_adapter._validator is None, 'Should be initialized deferred'\n        assert type_adapter._serializer is None, 'Should be initialized deferred'\n    else:\n        built_inside_type_adapter = 'Dict' in str(model) or 'Annotated' in str(model)\n        assert generate_schema_calls.count == (1 if built_inside_type_adapter else 0), f'Should be built ({model})'\n        assert type_adapter._core_schema is not None, 'Should be initialized before usage'\n        assert type_adapter._validator is not None, 'Should be initialized before usage'\n        assert type_adapter._serializer is not None, 'Should be initialized before usage'\n\n    if method == 'schema':\n        json_schema = type_adapter.json_schema()  # Use it\n        assert \"'type': 'integer'\" in str(json_schema)  # Sanity check\n        # Do not check generate_schema_calls count here as the json_schema generation uses generate schema internally\n        # assert generate_schema_calls.count < 2, 'Should not build duplicates'\n    elif method == 'validate':\n        validated = type_adapter.validate_python({'x': 1})  # Use it\n        assert (validated['x'] if isinstance(validated, dict) else getattr(validated, 'x')) == 1  # Sanity check\n        assert generate_schema_calls.count < 2, 'Should not build duplicates'\n    else:\n        assert method == 'dump'\n        raw = type_adapter.dump_json(dumped)  # Use it\n        assert json.loads(raw.decode())['x'] == 1  # Sanity check\n        assert generate_schema_calls.count < 2, 'Should not build duplicates'\n\n    assert type_adapter._core_schema is not None, 'Should be initialized after the usage'\n    assert type_adapter._validator is not None, 'Should be initialized after the usage'\n    assert type_adapter._serializer is not None, 'Should be initialized after the usage'\n", "tests/test_tools.py": "from typing import Dict, List, Mapping, Union\n\nimport pytest\n\nfrom pydantic import BaseModel, PydanticDeprecatedSince20, ValidationError\nfrom pydantic.dataclasses import dataclass\nfrom pydantic.deprecated.tools import parse_obj_as, schema_json_of, schema_of\n\npytestmark = pytest.mark.filterwarnings('ignore::DeprecationWarning')\n\n\n@pytest.mark.parametrize('obj,type_,parsed', [('1', int, 1), (['1'], List[int], [1])])\ndef test_parse_obj(obj, type_, parsed):\n    assert parse_obj_as(type_, obj) == parsed\n\n\ndef test_parse_obj_as_model():\n    class Model(BaseModel):\n        x: int\n        y: bool\n        z: str\n\n    model_inputs = {'x': '1', 'y': 'true', 'z': 'abc'}\n    assert parse_obj_as(Model, model_inputs) == Model(**model_inputs)\n\n\ndef test_parse_obj_preserves_subclasses():\n    class ModelA(BaseModel):\n        a: Mapping[int, str]\n\n    class ModelB(ModelA):\n        b: int\n\n    model_b = ModelB(a={1: 'f'}, b=2)\n\n    parsed = parse_obj_as(List[ModelA], [model_b])\n    assert parsed == [model_b]\n\n\ndef test_parse_obj_fails():\n    with pytest.raises(ValidationError) as exc_info:\n        parse_obj_as(int, 'a')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': (),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_parsing_model_naming():\n    with pytest.raises(ValidationError) as exc_info:\n        parse_obj_as(int, 'a')\n    assert str(exc_info.value).split('\\n')[0] == '1 validation error for int'\n\n    with pytest.raises(ValidationError) as exc_info:\n        with pytest.warns(PydanticDeprecatedSince20, match='The type_name parameter is deprecated'):\n            parse_obj_as(int, 'a', type_name='ParsingModel')\n    assert str(exc_info.value).split('\\n')[0] == '1 validation error for int'\n\n\ndef test_parse_as_dataclass():\n    @dataclass\n    class PydanticDataclass:\n        x: int\n\n    inputs = {'x': '1'}\n    assert parse_obj_as(PydanticDataclass, inputs) == PydanticDataclass(1)\n\n\ndef test_parse_mapping_as():\n    inputs = {'1': '2'}\n    assert parse_obj_as(Dict[int, int], inputs) == {1: 2}\n\n\ndef test_schema():\n    assert schema_of(Union[int, str], title='IntOrStr') == {\n        'title': 'IntOrStr',\n        'anyOf': [{'type': 'integer'}, {'type': 'string'}],\n    }\n    assert schema_json_of(Union[int, str], title='IntOrStr', indent=2) == (\n        '{\\n'\n        '  \"anyOf\": [\\n'\n        '    {\\n'\n        '      \"type\": \"integer\"\\n'\n        '    },\\n'\n        '    {\\n'\n        '      \"type\": \"string\"\\n'\n        '    }\\n'\n        '  ],\\n'\n        '  \"title\": \"IntOrStr\"\\n'\n        '}'\n    )\n", "tests/test_deprecated_fields.py": "import importlib.metadata\n\nimport pytest\nfrom packaging.version import Version\nfrom typing_extensions import Annotated, Self, deprecated\n\nfrom pydantic import BaseModel, Field, computed_field, field_validator, model_validator\n\n\ndef test_deprecated_fields():\n    class Model(BaseModel):\n        a: Annotated[int, Field(deprecated='')]\n        b: Annotated[int, Field(deprecated='This is deprecated')]\n        c: Annotated[int, Field(deprecated=None)]\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'a': {'deprecated': True, 'title': 'A', 'type': 'integer'},\n            'b': {'deprecated': True, 'title': 'B', 'type': 'integer'},\n            'c': {'title': 'C', 'type': 'integer'},\n        },\n        'required': ['a', 'b', 'c'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n    instance = Model(a=1, b=1, c=1)\n\n    pytest.warns(DeprecationWarning, lambda: instance.a, match='^$')\n\n    with pytest.warns(DeprecationWarning, match='^This is deprecated$'):\n        b = instance.b\n\n    assert b == 1\n\n\n@pytest.mark.skipif(\n    Version(importlib.metadata.version('typing_extensions')) < Version('4.9'),\n    reason='`deprecated` type annotation requires typing_extensions>=4.9',\n)\ndef test_deprecated_fields_deprecated_class():\n    class Model(BaseModel):\n        a: Annotated[int, deprecated('')]\n        b: Annotated[int, deprecated('This is deprecated')] = 1\n        c: Annotated[int, Field(deprecated=deprecated('This is deprecated'))] = 1\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'a': {'deprecated': True, 'title': 'A', 'type': 'integer'},\n            'b': {'default': 1, 'deprecated': True, 'title': 'B', 'type': 'integer'},\n            'c': {'default': 1, 'deprecated': True, 'title': 'C', 'type': 'integer'},\n        },\n        'required': ['a'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n    instance = Model(a=1)\n\n    pytest.warns(DeprecationWarning, lambda: instance.a, match='^$')\n    pytest.warns(DeprecationWarning, lambda: instance.b, match='^This is deprecated$')\n    pytest.warns(DeprecationWarning, lambda: instance.c, match='^This is deprecated$')\n\n\ndef test_deprecated_fields_field_validator():\n    class Model(BaseModel):\n        x: int = Field(deprecated='x is deprecated')\n\n        @field_validator('x')\n        @classmethod\n        def validate_x(cls, v: int) -> int:\n            return v * 2\n\n    instance = Model(x=1)\n\n    with pytest.warns(DeprecationWarning):\n        assert instance.x == 2\n\n\ndef test_deprecated_fields_model_validator():\n    class Model(BaseModel):\n        x: int = Field(deprecated='x is deprecated')\n\n        @model_validator(mode='after')\n        def validate_x(self) -> Self:\n            self.x = self.x * 2\n            return self\n\n    with pytest.warns(DeprecationWarning):\n        instance = Model(x=1)\n        assert instance.x == 2\n\n\ndef test_deprecated_fields_validate_assignment():\n    class Model(BaseModel):\n        x: int = Field(deprecated='x is deprecated')\n\n        model_config = {'validate_assignment': True}\n\n    instance = Model(x=1)\n\n    with pytest.warns(DeprecationWarning):\n        assert instance.x == 1\n\n    instance.x = 2\n\n    with pytest.warns(DeprecationWarning):\n        assert instance.x == 2\n\n\ndef test_computed_field_deprecated():\n    class Model(BaseModel):\n        @computed_field\n        @property\n        @deprecated('This is deprecated')\n        def p1(self) -> int:\n            return 1\n\n        @computed_field(deprecated='This is deprecated')\n        @property\n        @deprecated('This is deprecated (this message is overridden)')\n        def p2(self) -> int:\n            return 1\n\n        @computed_field(deprecated='')\n        @property\n        def p3(self) -> int:\n            return 1\n\n        @computed_field(deprecated='This is deprecated')\n        @property\n        def p4(self) -> int:\n            return 1\n\n        @computed_field\n        @deprecated('This is deprecated')\n        def p5(self) -> int:\n            return 1\n\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {\n            'p1': {'deprecated': True, 'readOnly': True, 'title': 'P1', 'type': 'integer'},\n            'p2': {'deprecated': True, 'readOnly': True, 'title': 'P2', 'type': 'integer'},\n            'p3': {'deprecated': True, 'readOnly': True, 'title': 'P3', 'type': 'integer'},\n            'p4': {'deprecated': True, 'readOnly': True, 'title': 'P4', 'type': 'integer'},\n            'p5': {'deprecated': True, 'readOnly': True, 'title': 'P5', 'type': 'integer'},\n        },\n        'required': ['p1', 'p2', 'p3', 'p4', 'p5'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n    instance = Model()\n\n    pytest.warns(DeprecationWarning, lambda: instance.p1, match='^This is deprecated$')\n    pytest.warns(DeprecationWarning, lambda: instance.p2, match='^This is deprecated$')\n    pytest.warns(DeprecationWarning, lambda: instance.p4, match='^This is deprecated$')\n    pytest.warns(DeprecationWarning, lambda: instance.p5, match='^This is deprecated$')\n\n    with pytest.warns(DeprecationWarning, match='^$'):\n        p3 = instance.p3\n\n    assert p3 == 1\n\n\n@pytest.mark.skipif(\n    Version(importlib.metadata.version('typing_extensions')) < Version('4.9'),\n    reason='`deprecated` type annotation requires typing_extensions>=4.9',\n)\ndef test_computed_field_deprecated_deprecated_class():\n    class Model(BaseModel):\n        @computed_field(deprecated=deprecated('This is deprecated'))\n        @property\n        def p1(self) -> int:\n            return 1\n\n        @computed_field(deprecated=True)\n        @property\n        def p2(self) -> int:\n            return 2\n\n        @computed_field(deprecated='This is a deprecated string')\n        @property\n        def p3(self) -> int:\n            return 3\n\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {\n            'p1': {'deprecated': True, 'readOnly': True, 'title': 'P1', 'type': 'integer'},\n            'p2': {'deprecated': True, 'readOnly': True, 'title': 'P2', 'type': 'integer'},\n            'p3': {'deprecated': True, 'readOnly': True, 'title': 'P3', 'type': 'integer'},\n        },\n        'required': ['p1', 'p2', 'p3'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n    instance = Model()\n\n    with pytest.warns(DeprecationWarning, match='^This is deprecated$'):\n        p1 = instance.p1\n\n    with pytest.warns(DeprecationWarning, match='^deprecated$'):\n        p2 = instance.p2\n\n    with pytest.warns(DeprecationWarning, match='^This is a deprecated string$'):\n        p3 = instance.p3\n\n    assert p1 == 1\n    assert p2 == 2\n    assert p3 == 3\n\n\ndef test_deprecated_with_boolean() -> None:\n    class Model(BaseModel):\n        a: Annotated[int, Field(deprecated=True)]\n        b: Annotated[int, Field(deprecated=False)]\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'a': {'deprecated': True, 'title': 'A', 'type': 'integer'},\n            'b': {'title': 'B', 'type': 'integer'},\n        },\n        'required': ['a', 'b'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n    instance = Model(a=1, b=1)\n\n    pytest.warns(DeprecationWarning, lambda: instance.a, match='deprecated')\n", "tests/test_errors.py": "import re\n\nimport pytest\n\nfrom pydantic import BaseModel, PydanticUserError, ValidationError\nfrom pydantic.version import version_short\n\n\ndef test_user_error_url():\n    with pytest.raises(PydanticUserError) as exc_info:\n        BaseModel()\n\n    # insert_assert(str(exc_info.value))\n    assert str(exc_info.value) == (\n        'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly\\n\\n'\n        f'For further information visit https://errors.pydantic.dev/{version_short()}/u/base-model-instantiated'\n    )\n\n\n@pytest.mark.parametrize(\n    'hide_input,input_str',\n    ((False, 'type=greater_than, input_value=4, input_type=int'), (True, 'type=greater_than')),\n)\ndef test_raise_validation_error_hide_input(hide_input, input_str):\n    with pytest.raises(ValidationError, match=re.escape(f'Input should be greater than 5 [{input_str}]')):\n        raise ValidationError.from_exception_data(\n            'Foobar',\n            [{'type': 'greater_than', 'loc': ('a', 2), 'input': 4, 'ctx': {'gt': 5}}],\n            hide_input=hide_input,\n        )\n", "tests/test_types_self.py": "import dataclasses\nimport typing\nfrom typing import List, Optional, Union\n\nimport pytest\nimport typing_extensions\nfrom typing_extensions import NamedTuple, TypedDict\n\nfrom pydantic import BaseModel, Field, TypeAdapter, ValidationError\n\n\n@pytest.fixture(\n    name='Self',\n    params=[\n        pytest.param(typing, id='typing.Self'),\n        pytest.param(typing_extensions, id='t_e.Self'),\n    ],\n)\ndef fixture_self_all(request):\n    try:\n        return request.param.Self\n    except AttributeError:\n        pytest.skip(f'Self is not available from {request.param}')\n\n\ndef test_recursive_model(Self):\n    class SelfRef(BaseModel):\n        data: int\n        ref: typing.Optional[Self] = None\n\n    assert SelfRef(data=1, ref={'data': 2}).model_dump() == {'data': 1, 'ref': {'data': 2, 'ref': None}}\n\n\ndef test_recursive_model_invalid(Self):\n    class SelfRef(BaseModel):\n        data: int\n        ref: typing.Optional[Self] = None\n\n    with pytest.raises(\n        ValidationError,\n        match=r'ref\\.ref\\s+Input should be a valid dictionary or instance of SelfRef \\[type=model_type,',\n    ):\n        SelfRef(data=1, ref={'data': 2, 'ref': 3}).model_dump()\n\n\ndef test_recursive_model_with_subclass(Self):\n    \"\"\"Self refs should be valid and should reference the correct class in covariant direction\"\"\"\n\n    class SelfRef(BaseModel):\n        x: int\n        ref: Self | None = None\n\n    class SubSelfRef(SelfRef):\n        y: int\n\n    assert SubSelfRef(x=1, ref=SubSelfRef(x=3, y=4), y=2).model_dump() == {\n        'x': 1,\n        'ref': {'x': 3, 'ref': None, 'y': 4},  # SubSelfRef.ref: SubSelfRef\n        'y': 2,\n    }\n    assert SelfRef(x=1, ref=SubSelfRef(x=2, y=3)).model_dump() == {\n        'x': 1,\n        'ref': {'x': 2, 'ref': None},\n    }  # SelfRef.ref: SelfRef\n\n\ndef test_recursive_model_with_subclass_invalid(Self):\n    \"\"\"Self refs are invalid in contravariant direction\"\"\"\n\n    class SelfRef(BaseModel):\n        x: int\n        ref: Self | None = None\n\n    class SubSelfRef(SelfRef):\n        y: int\n\n    with pytest.raises(\n        ValidationError,\n        match=r'ref\\s+Input should be a valid dictionary or instance of SubSelfRef \\[type=model_type,',\n    ):\n        SubSelfRef(x=1, ref=SelfRef(x=2), y=3).model_dump()\n\n\ndef test_recursive_model_with_subclass_override(Self):\n    \"\"\"Self refs should be overridable\"\"\"\n\n    class SelfRef(BaseModel):\n        x: int\n        ref: Self | None = None\n\n    class SubSelfRef(SelfRef):\n        y: int\n        ref: Optional[Union[SelfRef, Self]] = None\n\n    assert SubSelfRef(x=1, ref=SubSelfRef(x=3, y=4), y=2).model_dump() == {\n        'x': 1,\n        'ref': {'x': 3, 'ref': None, 'y': 4},\n        'y': 2,\n    }\n    assert SubSelfRef(x=1, ref=SelfRef(x=3, y=4), y=2).model_dump() == {\n        'x': 1,\n        'ref': {'x': 3, 'ref': None},\n        'y': 2,\n    }\n\n\ndef test_self_type_with_field(Self):\n    with pytest.raises(TypeError, match=r'The following constraints cannot be applied.*\\'gt\\''):\n\n        class SelfRef(BaseModel):\n            x: int\n            refs: typing.List[Self] = Field(..., gt=0)\n\n\ndef test_self_type_json_schema(Self):\n    class SelfRef(BaseModel):\n        x: int\n        refs: Optional[List[Self]] = []\n\n    assert SelfRef.model_json_schema() == {\n        '$defs': {\n            'SelfRef': {\n                'properties': {\n                    'x': {'title': 'X', 'type': 'integer'},\n                    'refs': {\n                        'anyOf': [{'items': {'$ref': '#/$defs/SelfRef'}, 'type': 'array'}, {'type': 'null'}],\n                        'default': [],\n                        'title': 'Refs',\n                    },\n                },\n                'required': ['x'],\n                'title': 'SelfRef',\n                'type': 'object',\n            }\n        },\n        'allOf': [{'$ref': '#/$defs/SelfRef'}],\n    }\n\n\ndef test_self_type_in_named_tuple(Self):\n    class SelfRefNamedTuple(NamedTuple):\n        x: int\n        ref: Self | None\n\n    ta = TypeAdapter(SelfRefNamedTuple)\n    assert ta.validate_python({'x': 1, 'ref': {'x': 2, 'ref': None}}) == (1, (2, None))\n\n\ndef test_self_type_in_typed_dict(Self):\n    class SelfRefTypedDict(TypedDict):\n        x: int\n        ref: Self | None\n\n    ta = TypeAdapter(SelfRefTypedDict)\n    assert ta.validate_python({'x': 1, 'ref': {'x': 2, 'ref': None}}) == {'x': 1, 'ref': {'x': 2, 'ref': None}}\n\n\ndef test_self_type_in_dataclass(Self):\n    @dataclasses.dataclass(frozen=True)\n    class SelfRef:\n        x: int\n        ref: Self | None\n\n    class Model(BaseModel):\n        item: SelfRef\n\n    m = Model.model_validate({'item': {'x': 1, 'ref': {'x': 2, 'ref': None}}})\n    assert m.item.x == 1\n    assert m.item.ref.x == 2\n    with pytest.raises(dataclasses.FrozenInstanceError):\n        m.item.ref.x = 3\n", "tests/test_color.py": "from datetime import datetime\n\nimport pytest\nfrom pydantic_core import PydanticCustomError\n\nfrom pydantic import BaseModel, ValidationError\nfrom pydantic.color import Color\n\npytestmark = pytest.mark.filterwarnings(\n    'ignore:The `Color` class is deprecated, use `pydantic_extra_types` instead.*:DeprecationWarning'\n)\n\n\n@pytest.mark.parametrize(\n    'raw_color, as_tuple',\n    [\n        # named colors\n        ('aliceblue', (240, 248, 255)),\n        ('Antiquewhite', (250, 235, 215)),\n        ('#000000', (0, 0, 0)),\n        ('#DAB', (221, 170, 187)),\n        ('#dab', (221, 170, 187)),\n        ('#000', (0, 0, 0)),\n        ('0x797979', (121, 121, 121)),\n        ('0x777', (119, 119, 119)),\n        ('0x777777', (119, 119, 119)),\n        ('0x777777cc', (119, 119, 119, 0.8)),\n        ('777', (119, 119, 119)),\n        ('777c', (119, 119, 119, 0.8)),\n        (' 777', (119, 119, 119)),\n        ('777 ', (119, 119, 119)),\n        (' 777 ', (119, 119, 119)),\n        ((0, 0, 128), (0, 0, 128)),\n        ([0, 0, 128], (0, 0, 128)),\n        ((0, 0, 205, 1.0), (0, 0, 205)),\n        ((0, 0, 205, 0.5), (0, 0, 205, 0.5)),\n        ('rgb(0, 0, 205)', (0, 0, 205)),\n        ('rgb(0, 0, 205.2)', (0, 0, 205)),\n        ('rgb(0, 0.2, 205)', (0, 0, 205)),\n        ('rgba(0, 0, 128, 0.6)', (0, 0, 128, 0.6)),\n        ('rgba(0, 0, 128, .6)', (0, 0, 128, 0.6)),\n        ('rgba(0, 0, 128, 60%)', (0, 0, 128, 0.6)),\n        (' rgba(0, 0, 128,0.6) ', (0, 0, 128, 0.6)),\n        ('rgba(00,0,128,0.6  )', (0, 0, 128, 0.6)),\n        ('rgba(0, 0, 128, 0)', (0, 0, 128, 0)),\n        ('rgba(0, 0, 128, 1)', (0, 0, 128)),\n        ('rgb(0 0.2 205)', (0, 0, 205)),\n        ('rgb(0 0.2 205 / 0.6)', (0, 0, 205, 0.6)),\n        ('rgb(0 0.2 205 / 60%)', (0, 0, 205, 0.6)),\n        ('rgba(0 0 128)', (0, 0, 128)),\n        ('rgba(0 0 128 / 0.6)', (0, 0, 128, 0.6)),\n        ('rgba(0 0 128 / 60%)', (0, 0, 128, 0.6)),\n        ('hsl(270, 60%, 70%)', (178, 133, 224)),\n        ('hsl(180, 100%, 50%)', (0, 255, 255)),\n        ('hsl(630, 60%, 70%)', (178, 133, 224)),\n        ('hsl(270deg, 60%, 70%)', (178, 133, 224)),\n        ('hsl(.75turn, 60%, 70%)', (178, 133, 224)),\n        ('hsl(-.25turn, 60%, 70%)', (178, 133, 224)),\n        ('hsl(-0.25turn, 60%, 70%)', (178, 133, 224)),\n        ('hsl(4.71238rad, 60%, 70%)', (178, 133, 224)),\n        ('hsl(10.9955rad, 60%, 70%)', (178, 133, 224)),\n        ('hsl(270, 60%, 50%, .15)', (127, 51, 204, 0.15)),\n        ('hsl(270.00deg, 60%, 50%, 15%)', (127, 51, 204, 0.15)),\n        ('hsl(630 60% 70%)', (178, 133, 224)),\n        ('hsl(270 60% 50% / .15)', (127, 51, 204, 0.15)),\n        ('hsla(630, 60%, 70%)', (178, 133, 224)),\n        ('hsla(630 60% 70%)', (178, 133, 224)),\n        ('hsla(270 60% 50% / .15)', (127, 51, 204, 0.15)),\n    ],\n)\ndef test_color_success(raw_color, as_tuple):\n    c = Color(raw_color)\n    assert c.as_rgb_tuple() == as_tuple\n    assert c.original() == raw_color\n\n\n@pytest.mark.parametrize(\n    'color',\n    [\n        # named colors\n        'nosuchname',\n        'chucknorris',\n        # hex\n        '#0000000',\n        'x000',\n        # rgb/rgba tuples\n        (256, 256, 256),\n        (128, 128, 128, 0.5, 128),\n        (0, 0, 'x'),\n        (0, 0, 0, 1.5),\n        (0, 0, 0, 'x'),\n        (0, 0, 1280),\n        (0, 0, 1205, 0.1),\n        (0, 0, 1128, 0.5),\n        (0, 0, 1128, -0.5),\n        (0, 0, 1128, 1.5),\n        # rgb/rgba strings\n        'rgb(0, 0, 1205)',\n        'rgb(0, 0, 1128)',\n        'rgb(0, 0, 200 / 0.2)',\n        'rgb(72 122 18, 0.3)',\n        'rgba(0, 0, 11205, 0.1)',\n        'rgba(0, 0, 128, 11.5)',\n        'rgba(0, 0, 128 / 11.5)',\n        'rgba(72 122 18 0.3)',\n        # hsl/hsla strings\n        'hsl(180, 101%, 50%)',\n        'hsl(72 122 18 / 0.3)',\n        'hsl(630 60% 70%, 0.3)',\n        'hsla(72 122 18 / 0.3)',\n        # neither a tuple, not a string\n        datetime(2017, 10, 5, 19, 47, 7),\n        object,\n        range(10),\n    ],\n)\ndef test_color_fail(color):\n    with pytest.raises(PydanticCustomError) as exc_info:\n        Color(color)\n    assert exc_info.value.type == 'color_error'\n\n\ndef test_model_validation():\n    class Model(BaseModel):\n        color: Color\n\n    assert Model(color='red').color.as_hex() == '#f00'\n    assert Model(color=Color('red')).color.as_hex() == '#f00'\n    with pytest.raises(ValidationError) as exc_info:\n        Model(color='snot')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'color_error',\n            'loc': ('color',),\n            'msg': 'value is not a valid color: string not recognised as a valid color',\n            'input': 'snot',\n        }\n    ]\n\n\ndef test_as_rgb():\n    assert Color('bad').as_rgb() == 'rgb(187, 170, 221)'\n    assert Color((1, 2, 3, 0.123456)).as_rgb() == 'rgba(1, 2, 3, 0.12)'\n    assert Color((1, 2, 3, 0.1)).as_rgb() == 'rgba(1, 2, 3, 0.1)'\n\n\ndef test_as_rgb_tuple():\n    assert Color((1, 2, 3)).as_rgb_tuple(alpha=None) == (1, 2, 3)\n    assert Color((1, 2, 3, 1)).as_rgb_tuple(alpha=None) == (1, 2, 3)\n    assert Color((1, 2, 3, 0.3)).as_rgb_tuple(alpha=None) == (1, 2, 3, 0.3)\n    assert Color((1, 2, 3, 0.3)).as_rgb_tuple(alpha=None) == (1, 2, 3, 0.3)\n\n    assert Color((1, 2, 3)).as_rgb_tuple(alpha=False) == (1, 2, 3)\n    assert Color((1, 2, 3, 0.3)).as_rgb_tuple(alpha=False) == (1, 2, 3)\n\n    assert Color((1, 2, 3)).as_rgb_tuple(alpha=True) == (1, 2, 3, 1)\n    assert Color((1, 2, 3, 0.3)).as_rgb_tuple(alpha=True) == (1, 2, 3, 0.3)\n\n\ndef test_as_hsl():\n    assert Color('bad').as_hsl() == 'hsl(260, 43%, 77%)'\n    assert Color((1, 2, 3, 0.123456)).as_hsl() == 'hsl(210, 50%, 1%, 0.12)'\n    assert Color('hsl(260, 43%, 77%)').as_hsl() == 'hsl(260, 43%, 77%)'\n\n\ndef test_as_hsl_tuple():\n    c = Color('016997')\n    h, s, l_, a = c.as_hsl_tuple(alpha=True)\n    assert h == pytest.approx(0.551, rel=0.01)\n    assert s == pytest.approx(0.986, rel=0.01)\n    assert l_ == pytest.approx(0.298, rel=0.01)\n    assert a == 1\n\n    assert c.as_hsl_tuple(alpha=False) == c.as_hsl_tuple(alpha=None) == (h, s, l_)\n\n    c = Color((3, 40, 50, 0.5))\n    hsla = c.as_hsl_tuple(alpha=None)\n    assert len(hsla) == 4\n    assert hsla[3] == 0.5\n\n\ndef test_as_hex():\n    assert Color((1, 2, 3)).as_hex() == '#010203'\n    assert Color((119, 119, 119)).as_hex() == '#777'\n    assert Color((119, 0, 238)).as_hex() == '#70e'\n    assert Color('B0B').as_hex() == '#b0b'\n    assert Color((1, 2, 3, 0.123456)).as_hex() == '#0102031f'\n    assert Color((1, 2, 3, 0.1)).as_hex() == '#0102031a'\n\n\ndef test_as_named():\n    assert Color((0, 255, 255)).as_named() == 'cyan'\n    assert Color('#808000').as_named() == 'olive'\n    assert Color('hsl(180, 100%, 50%)').as_named() == 'cyan'\n\n    assert Color((240, 248, 255)).as_named() == 'aliceblue'\n    with pytest.raises(ValueError) as exc_info:\n        Color((1, 2, 3)).as_named()\n    assert exc_info.value.args[0] == 'no named color found, use fallback=True, as_hex() or as_rgb()'\n\n    assert Color((1, 2, 3)).as_named(fallback=True) == '#010203'\n    assert Color((1, 2, 3, 0.1)).as_named(fallback=True) == '#0102031a'\n\n\ndef test_str_repr():\n    assert str(Color('red')) == 'red'\n    assert repr(Color('red')) == \"Color('red', rgb=(255, 0, 0))\"\n    assert str(Color((1, 2, 3))) == '#010203'\n    assert repr(Color((1, 2, 3))) == \"Color('#010203', rgb=(1, 2, 3))\"\n\n\ndef test_eq():\n    assert Color('red') == Color('red')\n    assert Color('red') != Color('blue')\n    assert Color('red') != 'red'\n\n    assert Color('red') == Color((255, 0, 0))\n    assert Color('red') != Color((0, 0, 255))\n\n\ndef test_color_hashable():\n    assert hash(Color('red')) != hash(Color('blue'))\n    assert hash(Color('red')) == hash(Color((255, 0, 0)))\n    assert hash(Color('red')) != hash(Color((255, 0, 0, 0.5)))\n", "tests/test_aliases.py": "from contextlib import nullcontext as does_not_raise\nfrom inspect import signature\nfrom typing import Any, ContextManager, List, Optional\n\nimport pytest\nfrom dirty_equals import IsStr\nfrom pydantic_core import PydanticUndefined\n\nfrom pydantic import (\n    AliasChoices,\n    AliasGenerator,\n    AliasPath,\n    BaseModel,\n    ConfigDict,\n    Field,\n    ValidationError,\n    computed_field,\n)\n\n\ndef test_alias_generator():\n    def to_camel(string: str):\n        return ''.join(x.capitalize() for x in string.split('_'))\n\n    class MyModel(BaseModel):\n        model_config = ConfigDict(alias_generator=to_camel)\n        a: List[str] = None\n        foo_bar: str\n\n    data = {'A': ['foo', 'bar'], 'FooBar': 'foobar'}\n    v = MyModel(**data)\n    assert v.a == ['foo', 'bar']\n    assert v.foo_bar == 'foobar'\n    assert v.model_dump(by_alias=True) == data\n\n\ndef test_alias_generator_wrong_type_error():\n    def return_bytes(string):\n        return b'not a string'\n\n    with pytest.raises(TypeError) as e:\n\n        class MyModel(BaseModel):\n            model_config = ConfigDict(alias_generator=return_bytes)\n            bar: Any\n\n    assert str(e.value) == IsStr(regex=\"alias_generator <function .*> must return str, not <class 'bytes'>\")\n\n\ndef test_basic_alias():\n    class Model(BaseModel):\n        a: str = Field('foobar', alias='_a')\n\n    assert Model().a == 'foobar'\n    assert Model(_a='different').a == 'different'\n    assert repr(Model.model_fields['a']) == (\n        \"FieldInfo(annotation=str, required=False, default='foobar', alias='_a', alias_priority=2)\"\n    )\n\n\ndef test_field_info_repr_with_aliases():\n    class Model(BaseModel):\n        a: str = Field('foobar', alias='_a', validation_alias='a_val', serialization_alias='a_ser')\n\n    assert repr(Model.model_fields['a']) == (\n        \"FieldInfo(annotation=str, required=False, default='foobar', alias='_a', \"\n        \"alias_priority=2, validation_alias='a_val', serialization_alias='a_ser')\"\n    )\n\n\ndef test_alias_error():\n    class Model(BaseModel):\n        a: int = Field(123, alias='_a')\n\n    assert Model(_a='123').a == 123\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(_a='foo')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'foo',\n            'loc': ('_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_alias_error_loc_by_alias():\n    class Model(BaseModel):\n        model_config = dict(loc_by_alias=False)\n        a: int = Field(123, alias='_a')\n\n    assert Model(_a='123').a == 123\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(_a='foo')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'foo',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_annotation_config():\n    class Model(BaseModel):\n        b: float = Field(alias='foobar')\n        a: int = 10\n        _c: str\n\n    assert list(Model.model_fields.keys()) == ['b', 'a']\n    assert [f.alias for f in Model.model_fields.values()] == ['foobar', None]\n    assert Model(foobar='123').b == 123.0\n\n\ndef test_pop_by_field_name():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='forbid', populate_by_name=True)\n        last_updated_by: Optional[str] = Field(None, alias='lastUpdatedBy')\n\n    assert Model(lastUpdatedBy='foo').model_dump() == {'last_updated_by': 'foo'}\n    assert Model(last_updated_by='foo').model_dump() == {'last_updated_by': 'foo'}\n    with pytest.raises(ValidationError) as exc_info:\n        Model(lastUpdatedBy='foo', last_updated_by='bar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'bar',\n            'loc': ('last_updated_by',),\n            'msg': 'Extra inputs are not permitted',\n            'type': 'extra_forbidden',\n        }\n    ]\n\n\ndef test_alias_override_behavior():\n    class Parent(BaseModel):\n        # Use `gt` to demonstrate that using `Field` to override an alias does not preserve other attributes\n        x: int = Field(alias='x1', gt=0)\n\n    class Child(Parent):\n        x: int = Field(..., alias='x2')\n        y: int = Field(..., alias='y2')\n\n    assert Parent.model_fields['x'].alias == 'x1'\n    assert Child.model_fields['x'].alias == 'x2'\n    assert Child.model_fields['y'].alias == 'y2'\n\n    Parent(x1=1)\n    with pytest.raises(ValidationError) as exc_info:\n        Parent(x1=-1)\n    assert exc_info.value.errors(include_url=False) == [\n        {'ctx': {'gt': 0}, 'input': -1, 'loc': ('x1',), 'msg': 'Input should be greater than 0', 'type': 'greater_than'}\n    ]\n\n    Child(x2=1, y2=2)\n\n    # Check the gt=0 is not preserved from Parent\n    Child(x2=-1, y2=2)\n\n    # Check the alias from Parent cannot be used\n    with pytest.raises(ValidationError) as exc_info:\n        Child(x1=1, y2=2)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'x1': 1, 'y2': 2}, 'loc': ('x2',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n    # Check the type hint from Parent _is_ preserved\n    with pytest.raises(ValidationError) as exc_info:\n        Child(x2='a', y2=2)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('x2',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_alias_generator_parent():\n    class Parent(BaseModel):\n        model_config = ConfigDict(populate_by_name=True, alias_generator=lambda f_name: f_name + '1')\n        x: int\n\n    class Child(Parent):\n        model_config = ConfigDict(alias_generator=lambda f_name: f_name + '2')\n        y: int\n\n    assert Child.model_fields['y'].alias == 'y2'\n    assert Child.model_fields['x'].alias == 'x2'\n\n\nupper_alias_generator = [\n    pytest.param(\n        lambda x: x.upper(),\n        id='basic_callable',\n    ),\n    pytest.param(\n        AliasGenerator(lambda x: x.upper()),\n        id='alias_generator',\n    ),\n]\n\n\n@pytest.mark.parametrize('alias_generator', upper_alias_generator)\ndef test_alias_generator_on_parent(alias_generator):\n    class Parent(BaseModel):\n        model_config = ConfigDict(alias_generator=alias_generator)\n        x: bool = Field(..., alias='a_b_c')\n        y: str\n\n    class Child(Parent):\n        y: str\n        z: str\n\n    assert Parent.model_fields['x'].alias == 'a_b_c'\n    assert Parent.model_fields['y'].alias == 'Y'\n    assert Child.model_fields['x'].alias == 'a_b_c'\n    assert Child.model_fields['y'].alias == 'Y'\n    assert Child.model_fields['z'].alias == 'Z'\n\n\n@pytest.mark.parametrize('alias_generator', upper_alias_generator)\ndef test_alias_generator_on_child(alias_generator):\n    class Parent(BaseModel):\n        x: bool = Field(..., alias='abc')\n        y: str\n\n    class Child(Parent):\n        model_config = ConfigDict(alias_generator=alias_generator)\n\n        y: str\n        z: str\n\n    assert [f.alias for f in Parent.model_fields.values()] == ['abc', None]\n    assert [f.alias for f in Child.model_fields.values()] == ['abc', 'Y', 'Z']\n\n\n@pytest.mark.parametrize('alias_generator', upper_alias_generator)\ndef test_alias_generator_used_by_default(alias_generator):\n    class Model(BaseModel):\n        model_config = ConfigDict(alias_generator=alias_generator)\n\n        a: str\n        b: str = Field(..., alias='b_alias')\n        c: str = Field(..., validation_alias='c_val_alias')\n        d: str = Field(..., serialization_alias='d_ser_alias')\n        e: str = Field(..., alias='e_alias', validation_alias='e_val_alias')\n        f: str = Field(..., alias='f_alias', serialization_alias='f_ser_alias')\n        g: str = Field(..., alias='g_alias', validation_alias='g_val_alias', serialization_alias='g_ser_alias')\n\n    assert {\n        name: {k: getattr(f, k) for k in ('alias', 'validation_alias', 'serialization_alias')}\n        for name, f in Model.model_fields.items()\n    } == {\n        # Validation/serialization aliases should be:\n        # 1. The specific alias, if specified, or\n        # 2. The alias, if specified, or\n        # 3. The generated alias (i.e. the field name in upper case)\n        'a': {\n            'alias': 'A',\n            'validation_alias': 'A',\n            'serialization_alias': 'A',\n        },\n        'b': {\n            'alias': 'b_alias',\n            'validation_alias': 'b_alias',\n            'serialization_alias': 'b_alias',\n        },\n        'c': {\n            'alias': 'C',\n            'validation_alias': 'c_val_alias',\n            'serialization_alias': 'C',\n        },\n        'd': {\n            'alias': 'D',\n            'validation_alias': 'D',\n            'serialization_alias': 'd_ser_alias',\n        },\n        'e': {\n            'alias': 'e_alias',\n            'validation_alias': 'e_val_alias',\n            'serialization_alias': 'e_alias',\n        },\n        'f': {\n            'alias': 'f_alias',\n            'validation_alias': 'f_alias',\n            'serialization_alias': 'f_ser_alias',\n        },\n        'g': {\n            'alias': 'g_alias',\n            'validation_alias': 'g_val_alias',\n            'serialization_alias': 'g_ser_alias',\n        },\n    }\n\n\n@pytest.mark.parametrize('alias_generator', upper_alias_generator)\ndef test_low_priority_alias(alias_generator):\n    class Parent(BaseModel):\n        w: bool = Field(..., alias='w_', validation_alias='w_val_alias', serialization_alias='w_ser_alias')\n        x: bool = Field(\n            ..., alias='abc', alias_priority=1, validation_alias='x_val_alias', serialization_alias='x_ser_alias'\n        )\n        y: str\n\n    class Child(Parent):\n        model_config = ConfigDict(alias_generator=alias_generator)\n\n        y: str\n        z: str\n\n    assert [f.alias for f in Parent.model_fields.values()] == ['w_', 'abc', None]\n    assert [f.validation_alias for f in Parent.model_fields.values()] == ['w_val_alias', 'x_val_alias', None]\n    assert [f.serialization_alias for f in Parent.model_fields.values()] == ['w_ser_alias', 'x_ser_alias', None]\n    assert [f.alias for f in Child.model_fields.values()] == ['w_', 'X', 'Y', 'Z']\n    assert [f.validation_alias for f in Child.model_fields.values()] == ['w_val_alias', 'X', 'Y', 'Z']\n    assert [f.serialization_alias for f in Child.model_fields.values()] == ['w_ser_alias', 'X', 'Y', 'Z']\n\n\n@pytest.mark.parametrize(\n    'cls_params, field_params, validation_key, serialization_key',\n    [\n        pytest.param(\n            {},\n            {'alias': 'x1', 'validation_alias': 'x2'},\n            'x2',\n            'x1',\n            id='alias-validation_alias',\n        ),\n        pytest.param(\n            {'alias_generator': str.upper},\n            {'alias': 'x'},\n            'x',\n            'x',\n            id='alias_generator-alias',\n        ),\n        pytest.param(\n            {'alias_generator': str.upper},\n            {'alias': 'x1', 'validation_alias': 'x2'},\n            'x2',\n            'x1',\n            id='alias_generator-alias-validation_alias',\n        ),\n        pytest.param(\n            {'alias_generator': str.upper},\n            {'alias': 'x1', 'serialization_alias': 'x2'},\n            'x1',\n            'x2',\n            id='alias_generator-alias-serialization_alias',\n        ),\n        pytest.param(\n            {'alias_generator': str.upper},\n            {'alias': 'x1', 'validation_alias': 'x2', 'serialization_alias': 'x3'},\n            'x2',\n            'x3',\n            id='alias_generator-alias-validation_alias-serialization_alias',\n        ),\n    ],\n)\ndef test_aliases_priority(cls_params, field_params, validation_key, serialization_key):\n    class Model(BaseModel, **cls_params):\n        x: int = Field(**field_params)\n\n    model = Model(**{validation_key: 1})\n    assert model.x == 1\n    assert model.model_dump(by_alias=True).get(serialization_key, None) is not None\n\n\ndef test_empty_string_alias():\n    class Model(BaseModel):\n        empty_string_key: int = Field(alias='')\n\n    data = {'': 123}\n    m = Model(**data)\n    assert m.empty_string_key == 123\n    assert m.model_dump(by_alias=True) == data\n\n\n@pytest.mark.parametrize(\n    'use_construct, populate_by_name_config, arg_name, expectation',\n    [\n        [False, True, 'bar', does_not_raise()],\n        [False, True, 'bar_', does_not_raise()],\n        [False, False, 'bar', does_not_raise()],\n        [False, False, 'bar_', pytest.raises(ValueError)],\n        [True, True, 'bar', does_not_raise()],\n        [True, True, 'bar_', does_not_raise()],\n        [True, False, 'bar', does_not_raise()],\n        [True, False, 'bar_', does_not_raise()],\n    ],\n)\ndef test_populate_by_name_config(\n    use_construct: bool,\n    populate_by_name_config: bool,\n    arg_name: str,\n    expectation: ContextManager,\n):\n    expected_value: int = 7\n\n    class Foo(BaseModel):\n        model_config = ConfigDict(populate_by_name=populate_by_name_config)\n        bar_: int = Field(..., alias='bar')\n\n    with expectation:\n        if use_construct:\n            f = Foo.model_construct(**{arg_name: expected_value})\n        else:\n            f = Foo(**{arg_name: expected_value})\n\n        assert f.bar_ == expected_value\n\n\ndef test_validation_alias():\n    class Model(BaseModel):\n        x: str = Field(validation_alias='foo')\n\n    data = {'foo': 'bar'}\n    m = Model(**data)\n    assert m.x == 'bar'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x='bar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing',\n            'loc': ('foo',),\n            'msg': 'Field required',\n            'input': {'x': 'bar'},\n        }\n    ]\n\n\ndef test_validation_alias_with_alias():\n    class Model(BaseModel):\n        x: str = Field(alias='x_alias', validation_alias='foo')\n\n    data = {'foo': 'bar'}\n    m = Model(**data)\n    assert m.x == 'bar'\n    sig = signature(Model)\n    assert 'x_alias' in sig.parameters\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x='bar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing',\n            'loc': ('foo',),\n            'msg': 'Field required',\n            'input': {'x': 'bar'},\n        }\n    ]\n\n\ndef test_validation_alias_from_str_alias():\n    class Model(BaseModel):\n        x: str = Field(alias='foo')\n\n    data = {'foo': 'bar'}\n    m = Model(**data)\n    assert m.x == 'bar'\n    sig = signature(Model)\n    assert 'foo' in sig.parameters\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x='bar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing',\n            'loc': ('foo',),\n            'msg': 'Field required',\n            'input': {'x': 'bar'},\n        }\n    ]\n\n\ndef test_validation_alias_from_list_alias():\n    class Model(BaseModel):\n        x: str = Field(alias=['foo', 'bar'])\n\n    data = {'foo': {'bar': 'test'}}\n    m = Model(**data)\n    assert m.x == 'test'\n    sig = signature(Model)\n    assert 'x' in sig.parameters\n\n    class Model(BaseModel):\n        x: str = Field(alias=['foo', 1])\n\n    data = {'foo': ['bar0', 'bar1']}\n    m = Model(**data)\n    assert m.x == 'bar1'\n    sig = signature(Model)\n    assert 'x' in sig.parameters\n\n\ndef test_serialization_alias():\n    class Model(BaseModel):\n        x: str = Field(serialization_alias='foo')\n\n    m = Model(x='bar')\n    assert m.x == 'bar'\n    assert m.model_dump() == {'x': 'bar'}\n    assert m.model_dump(by_alias=True) == {'foo': 'bar'}\n\n\ndef test_serialization_alias_with_alias():\n    class Model(BaseModel):\n        x: str = Field(alias='x_alias', serialization_alias='foo')\n\n    data = {'x_alias': 'bar'}\n    m = Model(**data)\n    assert m.x == 'bar'\n    assert m.model_dump() == {'x': 'bar'}\n    assert m.model_dump(by_alias=True) == {'foo': 'bar'}\n    sig = signature(Model)\n    assert 'x_alias' in sig.parameters\n\n\ndef test_serialization_alias_from_alias():\n    class Model(BaseModel):\n        x: str = Field(alias='foo')\n\n    data = {'foo': 'bar'}\n    m = Model(**data)\n    assert m.x == 'bar'\n    assert m.model_dump() == {'x': 'bar'}\n    assert m.model_dump(by_alias=True) == {'foo': 'bar'}\n    sig = signature(Model)\n    assert 'foo' in sig.parameters\n\n\n@pytest.mark.parametrize(\n    'field,expected',\n    [\n        pytest.param(\n            Field(alias='x_alias', validation_alias='x_val_alias', serialization_alias='x_ser_alias'),\n            {\n                'properties': {'x_val_alias': {'title': 'X Val Alias', 'type': 'string'}},\n                'required': ['x_val_alias'],\n            },\n            id='single_alias',\n        ),\n        pytest.param(\n            Field(validation_alias=AliasChoices('y_alias', 'another_alias')),\n            {\n                'properties': {'y_alias': {'title': 'Y Alias', 'type': 'string'}},\n                'required': ['y_alias'],\n            },\n            id='multiple_aliases',\n        ),\n        pytest.param(\n            Field(validation_alias=AliasChoices(AliasPath('z_alias', 'even_another_alias'), 'and_another')),\n            {\n                'properties': {'and_another': {'title': 'And Another', 'type': 'string'}},\n                'required': ['and_another'],\n            },\n            id='multiple_aliases_with_path',\n        ),\n    ],\n)\ndef test_aliases_json_schema(field, expected):\n    class Model(BaseModel):\n        x: str = field\n\n    assert Model.model_json_schema() == {'title': 'Model', 'type': 'object', **expected}\n\n\n@pytest.mark.parametrize(\n    'value',\n    [\n        'a',\n        AliasPath('a', 'b', 1),\n        AliasChoices('a', 'b'),\n        AliasChoices('a', AliasPath('b', 1)),\n    ],\n)\ndef test_validation_alias_path(value):\n    class Model(BaseModel):\n        x: str = Field(validation_alias=value)\n\n    assert Model.model_fields['x'].validation_alias == value\n\n\ndef test_search_dict_for_alias_path():\n    ap = AliasPath('a', 1)\n    assert ap.search_dict_for_path({'a': ['hello', 'world']}) == 'world'\n    assert ap.search_dict_for_path({'a': 'hello'}) is PydanticUndefined\n\n\ndef test_validation_alias_invalid_value_type():\n    m = 'Invalid `validation_alias` type. it should be `str`, `AliasChoices`, or `AliasPath`'\n    with pytest.raises(TypeError, match=m):\n\n        class Model(BaseModel):\n            x: str = Field(validation_alias=123)\n\n\ndef test_validation_alias_parse_data():\n    class Model(BaseModel):\n        x: str = Field(validation_alias=AliasChoices('a', AliasPath('b', 1), 'c'))\n\n    assert Model.model_fields['x'].validation_alias == AliasChoices('a', AliasPath('b', 1), 'c')\n    assert Model.model_validate({'a': 'hello'}).x == 'hello'\n    assert Model.model_validate({'b': ['hello', 'world']}).x == 'world'\n    assert Model.model_validate({'c': 'test'}).x == 'test'\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'b': ['hello']})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing',\n            'loc': ('a',),\n            'msg': 'Field required',\n            'input': {'b': ['hello']},\n        }\n    ]\n\n\ndef test_alias_generator_class() -> None:\n    class Model(BaseModel):\n        a: str\n\n        model_config = ConfigDict(\n            alias_generator=AliasGenerator(\n                validation_alias=lambda field_name: f'validation_{field_name}',\n                serialization_alias=lambda field_name: f'serialization_{field_name}',\n            )\n        )\n\n    assert Model.model_fields['a'].validation_alias == 'validation_a'\n    assert Model.model_fields['a'].serialization_alias == 'serialization_a'\n    assert Model.model_fields['a'].alias is None\n\n\ndef test_alias_generator_with_alias() -> None:\n    class Model(BaseModel):\n        a: str\n\n        model_config = ConfigDict(alias_generator=AliasGenerator(alias=lambda field_name: f'{field_name}_alias'))\n\n    assert Model.model_fields['a'].validation_alias == 'a_alias'\n    assert Model.model_fields['a'].serialization_alias == 'a_alias'\n    assert Model.model_fields['a'].alias == 'a_alias'\n\n\ndef test_alias_generator_with_positional_arg() -> None:\n    class Model(BaseModel):\n        a: str\n\n        model_config = ConfigDict(alias_generator=AliasGenerator(lambda field_name: f'{field_name}_alias'))\n\n    assert Model.model_fields['a'].validation_alias == 'a_alias'\n    assert Model.model_fields['a'].serialization_alias == 'a_alias'\n    assert Model.model_fields['a'].alias == 'a_alias'\n\n\n@pytest.mark.parametrize('alias_generator', upper_alias_generator)\ndef test_alias_generator_with_computed_field(alias_generator) -> None:\n    class Rectangle(BaseModel):\n        model_config = ConfigDict(populate_by_name=True, alias_generator=alias_generator)\n\n        width: int\n        height: int\n\n        @computed_field\n        @property\n        def area(self) -> int:\n            return self.width * self.height\n\n    r = Rectangle(width=10, height=20)\n    assert r.model_dump(by_alias=True) == {'WIDTH': 10, 'HEIGHT': 20, 'AREA': 200}\n\n\ndef test_alias_generator_with_invalid_callables() -> None:\n    for alias_kind in ('validation_alias', 'serialization_alias', 'alias'):\n        with pytest.raises(\n            TypeError, match=f'Invalid `{alias_kind}` type. `{alias_kind}` generator must produce one of'\n        ):\n\n            class Foo(BaseModel):\n                a: str\n\n                model_config = ConfigDict(alias_generator=AliasGenerator(**{alias_kind: lambda x: 1}))\n\n\ndef test_all_alias_kinds_specified() -> None:\n    class Foo(BaseModel):\n        a: str\n\n        model_config = ConfigDict(\n            alias_generator=AliasGenerator(\n                alias=lambda field_name: f'{field_name}_alias',\n                validation_alias=lambda field_name: f'{field_name}_val_alias',\n                serialization_alias=lambda field_name: f'{field_name}_ser_alias',\n            )\n        )\n\n    assert Foo.model_fields['a'].alias == 'a_alias'\n    assert Foo.model_fields['a'].validation_alias == 'a_val_alias'\n    assert Foo.model_fields['a'].serialization_alias == 'a_ser_alias'\n\n    # the same behavior we'd expect if we defined alias, validation_alias\n    # and serialization_alias on the field itself\n    f = Foo(a_val_alias='a')\n    assert f.a == 'a'\n    assert f.model_dump(by_alias=True) == {'a_ser_alias': 'a'}\n    assert f.model_dump(by_alias=False) == {'a': 'a'}\n\n\ndef test_alias_generator_with_computed_field_for_serialization() -> None:\n    \"\"\"Tests that the alias generator is used for computed fields, with serialization_alias taking precedence over alias.\"\"\"\n\n    class Rectangle(BaseModel):\n        model_config = ConfigDict(\n            alias_generator=AliasGenerator(\n                validation_alias=lambda field_name: f'{field_name}_val_alias',\n                alias=lambda field_name: f'{field_name}_alias',\n                serialization_alias=lambda field_name: f'{field_name}_ser_alias',\n            )\n        )\n\n        width: int\n        height: int\n\n        @computed_field\n        def area(self) -> int:\n            return self.width * self.height\n\n    r = Rectangle(width_val_alias=10, height_val_alias=20)\n    assert r.model_dump(by_alias=True) == {'width_ser_alias': 10, 'height_ser_alias': 20, 'area_ser_alias': 200}\n\n\nempty_str_alias_generator = AliasGenerator(\n    validation_alias=lambda x: '', alias=lambda x: f'{x}_alias', serialization_alias=lambda x: ''\n)\n\n\ndef test_alias_gen_with_empty_string() -> None:\n    class Model(BaseModel):\n        a: str\n\n        model_config = ConfigDict(alias_generator=empty_str_alias_generator)\n\n    assert Model.model_fields['a'].validation_alias == ''\n    assert Model.model_fields['a'].serialization_alias == ''\n    assert Model.model_fields['a'].alias == 'a_alias'\n\n\ndef test_alias_gen_with_empty_string_and_computed_field() -> None:\n    class Model(BaseModel):\n        model_config = ConfigDict(alias_generator=empty_str_alias_generator)\n\n        a: str\n\n        @computed_field\n        def b(self) -> str:\n            return self.a\n\n    assert Model.model_fields['a'].validation_alias == ''\n    assert Model.model_fields['a'].serialization_alias == ''\n    assert Model.model_fields['a'].alias == 'a_alias'\n    assert Model.model_computed_fields['b'].alias == ''\n", "tests/test_validators_dataclass.py": "from dataclasses import asdict, is_dataclass\nfrom typing import Any, List\n\nimport pytest\nfrom dirty_equals import HasRepr\n\nfrom pydantic import ValidationError, field_validator, model_validator\nfrom pydantic.dataclasses import dataclass\n\n\ndef test_simple():\n    @dataclass\n    class MyDataclass:\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def change_a(cls, v):\n            return v + ' changed'\n\n    assert MyDataclass(a='this is foobar good').a == 'this is foobar good changed'\n\n\ndef test_validate_before():\n    @dataclass\n    class MyDataclass:\n        a: List[int]\n\n        @field_validator('a', mode='before')\n        @classmethod\n        def check_a1(cls, v: List[Any]) -> List[Any]:\n            v.append('123')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def check_a2(cls, v: List[int]) -> List[int]:\n            v.append(456)\n            return v\n\n    assert MyDataclass(a=[1, 2]).a == [1, 2, 123, 456]\n\n\ndef test_validate_multiple():\n    @dataclass\n    class MyDataclass:\n        a: str\n        b: str\n\n        @field_validator('a', 'b')\n        @classmethod\n        def check_a_and_b(cls, v, info):\n            if len(v) < 4:\n                raise ValueError(f'{info.field_name} is too short')\n            return v + 'x'\n\n    assert asdict(MyDataclass(a='1234', b='5678')) == {'a': '1234x', 'b': '5678x'}\n\n    with pytest.raises(ValidationError) as exc_info:\n        MyDataclass(a='x', b='x')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('a is too short')))},\n            'input': 'x',\n            'loc': ('a',),\n            'msg': 'Value error, a is too short',\n            'type': 'value_error',\n        },\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('b is too short')))},\n            'input': 'x',\n            'loc': ('b',),\n            'msg': 'Value error, b is too short',\n            'type': 'value_error',\n        },\n    ]\n\n\ndef test_type_error():\n    @dataclass\n    class MyDataclass:\n        a: str\n        b: str\n\n        @field_validator('a', 'b')\n        @classmethod\n        def check_a_and_b(cls, v, info):\n            if len(v) < 4:\n                raise TypeError(f'{info.field_name} is too short')\n            return v + 'x'\n\n    assert asdict(MyDataclass(a='1234', b='5678')) == {'a': '1234x', 'b': '5678x'}\n\n    with pytest.raises(TypeError, match='a is too short'):\n        MyDataclass(a='x', b='x')\n\n\ndef test_classmethod():\n    @dataclass\n    class MyDataclass:\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def check_a(cls, v):\n            assert cls is MyDataclass and is_dataclass(MyDataclass)\n            return v\n\n    m = MyDataclass(a='this is foobar good')\n    assert m.a == 'this is foobar good'\n    m.check_a('x')\n\n\ndef test_validate_parent():\n    @dataclass\n    class Parent:\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def change_a(cls, v):\n            return v + ' changed'\n\n    @dataclass\n    class Child(Parent):\n        pass\n\n    assert Parent(a='this is foobar good').a == 'this is foobar good changed'\n    assert Child(a='this is foobar good').a == 'this is foobar good changed'\n\n\ndef test_inheritance_replace():\n    @dataclass\n    class Parent:\n        a: int\n\n        @field_validator('a')\n        @classmethod\n        def add_to_a(cls, v):\n            return v + 1\n\n    @dataclass\n    class Child(Parent):\n        @field_validator('a')\n        @classmethod\n        def add_to_a(cls, v):\n            return v + 5\n\n    assert Child(a=0).a == 5\n\n\ndef test_model_validator():\n    root_val_values: list[Any] = []\n\n    @dataclass\n    class MyDataclass:\n        a: int\n        b: str\n\n        @field_validator('b')\n        @classmethod\n        def repeat_b(cls, v: str) -> str:\n            return v * 2\n\n        @model_validator(mode='after')\n        def root_validator(self) -> 'MyDataclass':\n            root_val_values.append(asdict(self))\n            if 'snap' in self.b:\n                raise ValueError('foobar')\n            self.b = 'changed'\n            return self\n\n    assert asdict(MyDataclass(a='123', b='bar')) == {'a': 123, 'b': 'changed'}\n\n    with pytest.raises(ValidationError) as exc_info:\n        MyDataclass(1, b='snap dragon')\n    assert root_val_values == [{'a': 123, 'b': 'barbar'}, {'a': 1, 'b': 'snap dragonsnap dragon'}]\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('foobar')))},\n            'input': HasRepr(\"ArgsKwargs((1,), {'b': 'snap dragon'})\"),\n            'loc': (),\n            'msg': 'Value error, foobar',\n            'type': 'value_error',\n        }\n    ]\n", "tests/test_edge_cases.py": "import functools\nimport importlib.util\nimport re\nimport sys\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Hashable\nfrom decimal import Decimal\nfrom enum import Enum, auto\nfrom typing import (\n    Any,\n    Dict,\n    ForwardRef,\n    FrozenSet,\n    Generic,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nimport pytest\nfrom dirty_equals import HasRepr, IsStr\nfrom pydantic_core import ErrorDetails, InitErrorDetails, PydanticSerializationError, core_schema\nfrom typing_extensions import Annotated, Literal, TypedDict, get_args\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    GetCoreSchemaHandler,\n    PydanticDeprecatedSince20,\n    PydanticInvalidForJsonSchema,\n    PydanticSchemaGenerationError,\n    RootModel,\n    TypeAdapter,\n    ValidationError,\n    constr,\n    errors,\n    field_validator,\n    model_validator,\n    root_validator,\n    validator,\n)\nfrom pydantic.fields import Field, computed_field\nfrom pydantic.functional_serializers import (\n    field_serializer,\n    model_serializer,\n)\n\n\ndef test_str_bytes():\n    class Model(BaseModel):\n        v: Union[str, bytes]\n\n    m = Model(v='s')\n    assert m.v == 's'\n    assert repr(m.model_fields['v']) == 'FieldInfo(annotation=Union[str, bytes], required=True)'\n\n    m = Model(v=b'b')\n    assert m.v == b'b'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=None)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('v', 'str'), 'msg': 'Input should be a valid string', 'input': None},\n        {'type': 'bytes_type', 'loc': ('v', 'bytes'), 'msg': 'Input should be a valid bytes', 'input': None},\n    ]\n\n\ndef test_str_bytes_none():\n    class Model(BaseModel):\n        v: Union[None, str, bytes] = ...\n\n    m = Model(v='s')\n    assert m.v == 's'\n\n    m = Model(v=b'b')\n    assert m.v == b'b'\n\n    m = Model(v=None)\n    assert m.v is None\n\n\ndef test_union_int_str():\n    class Model(BaseModel):\n        v: Union[int, str] = ...\n\n    m = Model(v=123)\n    assert m.v == 123\n\n    m = Model(v='123')\n    assert m.v == '123'\n\n    m = Model(v=b'foobar')\n    assert m.v == 'foobar'\n\n    m = Model(v=12.0)\n    assert m.v == 12\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=None)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('v', 'int'), 'msg': 'Input should be a valid integer', 'input': None},\n        {\n            'type': 'string_type',\n            'loc': ('v', 'str'),\n            'msg': 'Input should be a valid string',\n            'input': None,\n        },\n    ]\n\n\ndef test_union_int_any():\n    class Model(BaseModel):\n        v: Union[int, Any]\n\n    m = Model(v=123)\n    assert m.v == 123\n\n    m = Model(v='123')\n    assert m.v == '123'\n\n    m = Model(v='foobar')\n    assert m.v == 'foobar'\n\n    m = Model(v=None)\n    assert m.v is None\n\n\ndef test_typed_list():\n    class Model(BaseModel):\n        v: List[int] = ...\n\n    m = Model(v=[1, 2, '3'])\n    assert m.v == [1, 2, 3]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=[1, 'x', 'y'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'y',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'list_type', 'loc': ('v',), 'msg': 'Input should be a valid list', 'input': 1}\n    ]\n\n\ndef test_typed_set():\n    class Model(BaseModel):\n        v: Set[int] = ...\n\n    assert Model(v={1, 2, '3'}).v == {1, 2, 3}\n    assert Model(v=[1, 2, '3']).v == {1, 2, 3}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=[1, 'x'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        }\n    ]\n\n\ndef test_dict_dict():\n    class Model(BaseModel):\n        v: Dict[str, int] = ...\n\n    assert Model(v={'foo': 1}).model_dump() == {'v': {'foo': 1}}\n\n\ndef test_none_list():\n    class Model(BaseModel):\n        v: List[None] = [None]\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'v': {'title': 'V', 'default': [None], 'type': 'array', 'items': {'type': 'null'}}},\n    }\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        ({'a': 2, 'b': 4}, {'a': 2, 'b': 4}),\n        ({b'a': '2', 'b': 4}, {'a': 2, 'b': 4}),\n        # ([('a', 2), ('b', 4)], {'a': 2, 'b': 4}),\n    ],\n)\ndef test_typed_dict(value, result):\n    class Model(BaseModel):\n        v: Dict[str, int] = ...\n\n    assert Model(v=value).v == result\n\n\n@pytest.mark.parametrize(\n    'value,errors',\n    [\n        (1, [{'type': 'dict_type', 'loc': ('v',), 'msg': 'Input should be a valid dictionary', 'input': 1}]),\n        (\n            {'a': 'b'},\n            [\n                {\n                    'type': 'int_parsing',\n                    'loc': ('v', 'a'),\n                    'msg': 'Input should be a valid integer, unable to parse string as an integer',\n                    'input': 'b',\n                }\n            ],\n        ),\n        (\n            [1, 2, 3],\n            [{'type': 'dict_type', 'loc': ('v',), 'msg': 'Input should be a valid dictionary', 'input': [1, 2, 3]}],\n        ),\n    ],\n)\ndef test_typed_dict_error(value, errors):\n    class Model(BaseModel):\n        v: Dict[str, int] = ...\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    assert exc_info.value.errors(include_url=False) == errors\n\n\ndef test_dict_key_error():\n    class Model(BaseModel):\n        v: Dict[int, int] = ...\n\n    assert Model(v={1: 2, '3': '4'}).v == {1: 2, 3: 4}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v={'foo': 2, '3': '4'})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 'foo', '[key]'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'foo',\n        }\n    ]\n\n\ndef test_tuple():\n    class Model(BaseModel):\n        v: Tuple[int, float, bool]\n\n    m = Model(v=['1.0', '2.2', 'true'])\n    assert m.v == (1, 2.2, True)\n\n\ndef test_tuple_more():\n    class Model(BaseModel):\n        empty_tuple: Tuple[()]\n        simple_tuple: tuple = None\n        tuple_of_different_types: Tuple[int, float, str, bool] = None\n        tuple_of_single_tuples: Tuple[Tuple[int], ...] = ()\n\n    m = Model(\n        empty_tuple=[],\n        simple_tuple=[1, 2, 3, 4],\n        tuple_of_different_types=[4, 3.1, 'str', 1],\n        tuple_of_single_tuples=(('1',), (2,)),\n    )\n    assert m.model_dump() == {\n        'empty_tuple': (),\n        'simple_tuple': (1, 2, 3, 4),\n        'tuple_of_different_types': (4, 3.1, 'str', True),\n        'tuple_of_single_tuples': ((1,), (2,)),\n    }\n\n\n@pytest.mark.parametrize(\n    'dict_cls,frozenset_cls,list_cls,set_cls,tuple_cls,type_cls',\n    [\n        (Dict, FrozenSet, List, Set, Tuple, Type),\n        (dict, frozenset, list, set, tuple, type),\n    ],\n)\ndef test_pep585_generic_types(dict_cls, frozenset_cls, list_cls, set_cls, tuple_cls, type_cls):\n    class Type1:\n        pass\n\n    class Type2:\n        pass\n\n    class Model(BaseModel, arbitrary_types_allowed=True):\n        a: dict_cls\n        a1: 'dict_cls[str, int]'\n        b: frozenset_cls\n        b1: 'frozenset_cls[int]'\n        c: list_cls\n        c1: 'list_cls[int]'\n        d: set_cls\n        d1: 'set_cls[int]'\n        e: tuple_cls\n        e1: 'tuple_cls[int]'\n        e2: 'tuple_cls[int, ...]'\n        e3: 'tuple_cls[()]'\n        f: type_cls\n        f1: 'type_cls[Type1]'\n\n    default_model_kwargs = dict(\n        a={},\n        a1={'a': '1'},\n        b=[],\n        b1=('1',),\n        c=[],\n        c1=('1',),\n        d=[],\n        d1=['1'],\n        e=[],\n        e1=['1'],\n        e2=['1', '2'],\n        e3=[],\n        f=Type1,\n        f1=Type1,\n    )\n\n    m = Model(**default_model_kwargs)\n    assert m.a == {}\n    assert m.a1 == {'a': 1}\n    assert m.b == frozenset()\n    assert m.b1 == frozenset({1})\n    assert m.c == []\n    assert m.c1 == [1]\n    assert m.d == set()\n    assert m.d1 == {1}\n    assert m.e == ()\n    assert m.e1 == (1,)\n    assert m.e2 == (1, 2)\n    assert m.e3 == ()\n    assert m.f == Type1\n    assert m.f1 == Type1\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(**{**default_model_kwargs, 'e3': (1,)})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': ('e3',),\n            'msg': 'Tuple should have at most 0 items after validation, not 1',\n            'input': (1,),\n            'ctx': {'field_type': 'Tuple', 'max_length': 0, 'actual_length': 1},\n        }\n    ]\n\n    Model(**{**default_model_kwargs, 'f': Type2})\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(**{**default_model_kwargs, 'f1': Type2})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_subclass_of',\n            'loc': ('f1',),\n            'msg': 'Input should be a subclass of test_pep585_generic_types.<locals>.Type1',\n            'input': HasRepr(IsStr(regex=r\".+\\.Type2'>\")),\n            'ctx': {'class': 'test_pep585_generic_types.<locals>.Type1'},\n        }\n    ]\n\n\ndef test_tuple_length_error():\n    class Model(BaseModel):\n        v: Tuple[int, float, bool]\n        w: Tuple[()]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=[1, 2], w=[1])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('v', 2), 'msg': 'Field required', 'input': [1, 2]},\n        {\n            'type': 'too_long',\n            'loc': ('w',),\n            'msg': 'Tuple should have at most 0 items after validation, not 1',\n            'input': [1],\n            'ctx': {'field_type': 'Tuple', 'max_length': 0, 'actual_length': 1},\n        },\n    ]\n\n\ndef test_tuple_invalid():\n    class Model(BaseModel):\n        v: Tuple[int, float, bool]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v='xxx')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'tuple_type', 'loc': ('v',), 'msg': 'Input should be a valid tuple', 'input': 'xxx'}\n    ]\n\n\ndef test_tuple_value_error():\n    class Model(BaseModel):\n        v: Tuple[int, float, Decimal]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=['x', 'y', 'x'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        },\n        {\n            'type': 'float_parsing',\n            'loc': ('v', 1),\n            'msg': 'Input should be a valid number, unable to parse string as a number',\n            'input': 'y',\n        },\n        {\n            'type': 'decimal_parsing',\n            'loc': ('v', 2),\n            'msg': 'Input should be a valid decimal',\n            'input': 'x',\n        },\n    ]\n\n\ndef test_recursive_list():\n    class SubModel(BaseModel):\n        name: str = ...\n        count: int = None\n\n    class Model(BaseModel):\n        v: List[SubModel] = []\n\n    m = Model(v=[])\n    assert m.v == []\n\n    m = Model(v=[{'name': 'testing', 'count': 4}])\n    assert repr(m) == \"Model(v=[SubModel(name='testing', count=4)])\"\n    assert m.v[0].name == 'testing'\n    assert m.v[0].count == 4\n    assert m.model_dump() == {'v': [{'count': 4, 'name': 'testing'}]}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=['x'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': ('v', 0),\n            'msg': 'Input should be a valid dictionary or instance of SubModel',\n            'input': 'x',\n            'ctx': {'class_name': 'SubModel'},\n        }\n    ]\n\n\ndef test_recursive_list_error():\n    class SubModel(BaseModel):\n        name: str = ...\n        count: int = None\n\n    class Model(BaseModel):\n        v: List[SubModel] = []\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=[{}])\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('v', 0, 'name'), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\ndef test_list_unions():\n    class Model(BaseModel):\n        v: List[Union[int, str]] = ...\n\n    assert Model(v=[123, '456', 'foobar']).v == [123, '456', 'foobar']\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=[1, 2, None])\n\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': None, 'loc': ('v', 2, 'int'), 'msg': 'Input should be a valid integer', 'type': 'int_type'},\n        {'input': None, 'loc': ('v', 2, 'str'), 'msg': 'Input should be a valid string', 'type': 'string_type'},\n    ]\n\n\ndef test_recursive_lists():\n    class Model(BaseModel):\n        v: List[List[Union[int, float]]] = ...\n\n    assert Model(v=[[1, 2], [3, '4', '4.1']]).v == [[1, 2], [3, 4, 4.1]]\n    assert Model.model_fields['v'].annotation == List[List[Union[int, float]]]\n    assert Model.model_fields['v'].is_required()\n\n\nclass StrEnum(str, Enum):\n    a = 'a10'\n    b = 'b10'\n\n\ndef test_str_enum():\n    class Model(BaseModel):\n        v: StrEnum = ...\n\n    assert Model(v='a10').v is StrEnum.a\n\n    with pytest.raises(ValidationError):\n        Model(v='different')\n\n\ndef test_any_dict():\n    class Model(BaseModel):\n        v: Dict[int, Any] = ...\n\n    assert Model(v={1: 'foobar'}).model_dump() == {'v': {1: 'foobar'}}\n    assert Model(v={123: 456}).model_dump() == {'v': {123: 456}}\n    assert Model(v={2: [1, 2, 3]}).model_dump() == {'v': {2: [1, 2, 3]}}\n\n\ndef test_success_values_include():\n    class Model(BaseModel):\n        a: int = 1\n        b: int = 2\n        c: int = 3\n\n    m = Model()\n    assert m.model_dump() == {'a': 1, 'b': 2, 'c': 3}\n    assert m.model_dump(include={'a'}) == {'a': 1}\n    assert m.model_dump(exclude={'a'}) == {'b': 2, 'c': 3}\n    assert m.model_dump(include={'a', 'b'}, exclude={'a'}) == {'b': 2}\n\n\ndef test_include_exclude_unset():\n    class Model(BaseModel):\n        a: int\n        b: int\n        c: int = 3\n        d: int = 4\n        e: int = 5\n        f: int = 6\n\n    m = Model(a=1, b=2, e=5, f=7)\n    assert m.model_dump() == {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 7}\n    assert m.model_fields_set == {'a', 'b', 'e', 'f'}\n    assert m.model_dump(exclude_unset=True) == {'a': 1, 'b': 2, 'e': 5, 'f': 7}\n\n    assert m.model_dump(include={'a'}, exclude_unset=True) == {'a': 1}\n    assert m.model_dump(include={'c'}, exclude_unset=True) == {}\n\n    assert m.model_dump(exclude={'a'}, exclude_unset=True) == {'b': 2, 'e': 5, 'f': 7}\n    assert m.model_dump(exclude={'c'}, exclude_unset=True) == {'a': 1, 'b': 2, 'e': 5, 'f': 7}\n\n    assert m.model_dump(include={'a', 'b', 'c'}, exclude={'b'}, exclude_unset=True) == {'a': 1}\n    assert m.model_dump(include={'a', 'b', 'c'}, exclude={'a', 'c'}, exclude_unset=True) == {'b': 2}\n\n\ndef test_include_exclude_defaults():\n    class Model(BaseModel):\n        a: int\n        b: int\n        c: int = 3\n        d: int = 4\n        e: int = 5\n        f: int = 6\n\n    m = Model(a=1, b=2, e=5, f=7)\n    assert m.model_dump() == {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 7}\n    assert m.model_fields_set == {'a', 'b', 'e', 'f'}\n    assert m.model_dump(exclude_defaults=True) == {'a': 1, 'b': 2, 'f': 7}\n\n    assert m.model_dump(include={'a'}, exclude_defaults=True) == {'a': 1}\n    assert m.model_dump(include={'c'}, exclude_defaults=True) == {}\n\n    assert m.model_dump(exclude={'a'}, exclude_defaults=True) == {'b': 2, 'f': 7}\n    assert m.model_dump(exclude={'c'}, exclude_defaults=True) == {'a': 1, 'b': 2, 'f': 7}\n\n    assert m.model_dump(include={'a', 'b', 'c'}, exclude={'b'}, exclude_defaults=True) == {'a': 1}\n    assert m.model_dump(include={'a', 'b', 'c'}, exclude={'a', 'c'}, exclude_defaults=True) == {'b': 2}\n\n    assert m.model_dump(include={'a': 1}.keys()) == {'a': 1}\n    assert m.model_dump(exclude={'a': 1}.keys()) == {'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 7}\n\n    assert m.model_dump(include={'a': 1}.keys(), exclude_unset=True) == {'a': 1}\n    assert m.model_dump(exclude={'a': 1}.keys(), exclude_unset=True) == {'b': 2, 'e': 5, 'f': 7}\n\n    assert m.model_dump(include=['a']) == {'a': 1}\n    assert m.model_dump(exclude=['a']) == {'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 7}\n\n    assert m.model_dump(include=['a'], exclude_unset=True) == {'a': 1}\n    assert m.model_dump(exclude=['a'], exclude_unset=True) == {'b': 2, 'e': 5, 'f': 7}\n\n\ndef test_advanced_exclude():\n    class SubSubModel(BaseModel):\n        a: str\n        b: str\n\n    class SubModel(BaseModel):\n        c: str\n        d: List[SubSubModel]\n\n    class Model(BaseModel):\n        e: str\n        f: SubModel\n\n    m = Model(e='e', f=SubModel(c='foo', d=[SubSubModel(a='a', b='b'), SubSubModel(a='c', b='e')]))\n\n    assert m.model_dump(exclude={'f': {'c': ..., 'd': {-1: {'a'}}}}) == {\n        'e': 'e',\n        'f': {'d': [{'a': 'a', 'b': 'b'}, {'b': 'e'}]},\n    }\n    assert m.model_dump(exclude={'e': ..., 'f': {'d'}}) == {'f': {'c': 'foo'}}\n\n\ndef test_advanced_exclude_by_alias():\n    class SubSubModel(BaseModel):\n        a: str\n        aliased_b: str = Field(..., alias='b_alias')\n\n    class SubModel(BaseModel):\n        aliased_c: str = Field(..., alias='c_alias')\n        aliased_d: List[SubSubModel] = Field(..., alias='d_alias')\n\n    class Model(BaseModel):\n        aliased_e: str = Field(..., alias='e_alias')\n        aliased_f: SubModel = Field(..., alias='f_alias')\n\n    m = Model(\n        e_alias='e',\n        f_alias=SubModel(c_alias='foo', d_alias=[SubSubModel(a='a', b_alias='b'), SubSubModel(a='c', b_alias='e')]),\n    )\n\n    excludes = {'aliased_f': {'aliased_c': ..., 'aliased_d': {-1: {'a'}}}}\n    assert m.model_dump(exclude=excludes, by_alias=True) == {\n        'e_alias': 'e',\n        'f_alias': {'d_alias': [{'a': 'a', 'b_alias': 'b'}, {'b_alias': 'e'}]},\n    }\n\n    excludes = {'aliased_e': ..., 'aliased_f': {'aliased_d'}}\n    assert m.model_dump(exclude=excludes, by_alias=True) == {'f_alias': {'c_alias': 'foo'}}\n\n\ndef test_advanced_value_include():\n    class SubSubModel(BaseModel):\n        a: str\n        b: str\n\n    class SubModel(BaseModel):\n        c: str\n        d: List[SubSubModel]\n\n    class Model(BaseModel):\n        e: str\n        f: SubModel\n\n    m = Model(e='e', f=SubModel(c='foo', d=[SubSubModel(a='a', b='b'), SubSubModel(a='c', b='e')]))\n\n    assert m.model_dump(include={'f'}) == {'f': {'c': 'foo', 'd': [{'a': 'a', 'b': 'b'}, {'a': 'c', 'b': 'e'}]}}\n    assert m.model_dump(include={'e'}) == {'e': 'e'}\n    assert m.model_dump(include={'f': {'d': {0: ..., -1: {'b'}}}}) == {'f': {'d': [{'a': 'a', 'b': 'b'}, {'b': 'e'}]}}\n\n\ndef test_advanced_value_exclude_include():\n    class SubSubModel(BaseModel):\n        a: str\n        b: str\n\n    class SubModel(BaseModel):\n        c: str\n        d: List[SubSubModel]\n\n    class Model(BaseModel):\n        e: str\n        f: SubModel\n\n    m = Model(e='e', f=SubModel(c='foo', d=[SubSubModel(a='a', b='b'), SubSubModel(a='c', b='e')]))\n\n    assert m.model_dump(exclude={'f': {'c': ..., 'd': {-1: {'a'}}}}, include={'f'}) == {\n        'f': {'d': [{'a': 'a', 'b': 'b'}, {'b': 'e'}]}\n    }\n    assert m.model_dump(exclude={'e': ..., 'f': {'d'}}, include={'e', 'f'}) == {'f': {'c': 'foo'}}\n\n    assert m.model_dump(exclude={'f': {'d': {-1: {'a'}}}}, include={'f': {'d'}}) == {\n        'f': {'d': [{'a': 'a', 'b': 'b'}, {'b': 'e'}]}\n    }\n\n\n@pytest.mark.parametrize(\n    'exclude,expected',\n    [\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i'}}}}},\n            {'subs': [{'k': 1, 'subsubs': [{'j': 1}, {'j': 2}]}, {'k': 2, 'subsubs': [{'j': 3}]}]},\n            id='Normal nested __all__',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i'}}}, 0: {'subsubs': {'__all__': {'j'}}}}},\n            {'subs': [{'k': 1, 'subsubs': [{}, {}]}, {'k': 2, 'subsubs': [{'j': 3}]}]},\n            id='Merge sub dicts 1',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': ...}, 0: {'subsubs': {'__all__': {'j'}}}}},\n            {'subs': [{'k': 1, 'subsubs': [{'i': 1}, {'i': 2}]}, {'k': 2}]},\n            # {'subs': [{'k': 1                                 }, {'k': 2}]}\n            id='Merge sub sets 2',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'j'}}}, 0: {'subsubs': ...}}},\n            {'subs': [{'k': 1}, {'k': 2, 'subsubs': [{'i': 3}]}]},\n            id='Merge sub sets 3',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {0}}, 0: {'subsubs': {1}}}},\n            {'subs': [{'k': 1, 'subsubs': []}, {'k': 2, 'subsubs': []}]},\n            id='Merge sub sets 1',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {0: {'i'}}}, 0: {'subsubs': {1}}}},\n            {'subs': [{'k': 1, 'subsubs': [{'j': 1}]}, {'k': 2, 'subsubs': [{'j': 3}]}]},\n            id='Merge sub dict-set',\n        ),\n        pytest.param({'subs': {'__all__': {'subsubs'}, 0: {'k'}}}, {'subs': [{}, {'k': 2}]}, id='Different keys 1'),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': ...}, 0: {'k'}}}, {'subs': [{}, {'k': 2}]}, id='Different keys 2'\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs'}, 0: {'k': ...}}}, {'subs': [{}, {'k': 2}]}, id='Different keys 3'\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i'}, 0: {'j'}}}}},\n            {'subs': [{'k': 1, 'subsubs': [{}, {'j': 2}]}, {'k': 2, 'subsubs': [{}]}]},\n            id='Nested different keys 1',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i': ...}, 0: {'j'}}}}},\n            {'subs': [{'k': 1, 'subsubs': [{}, {'j': 2}]}, {'k': 2, 'subsubs': [{}]}]},\n            id='Nested different keys 2',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i'}, 0: {'j': ...}}}}},\n            {'subs': [{'k': 1, 'subsubs': [{}, {'j': 2}]}, {'k': 2, 'subsubs': [{}]}]},\n            id='Nested different keys 3',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs'}, 0: {'subsubs': {'__all__': {'j'}}}}},\n            {'subs': [{'k': 1, 'subsubs': [{'i': 1}, {'i': 2}]}, {'k': 2}]},\n            id='Ignore __all__ for index with defined exclude 1',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'j'}}}, 0: ...}},\n            {'subs': [{'k': 2, 'subsubs': [{'i': 3}]}]},\n            id='Ignore __all__ for index with defined exclude 2',\n        ),\n        pytest.param(\n            {'subs': {'__all__': ..., 0: {'subsubs'}}},\n            {'subs': [{'k': 1}]},\n            id='Ignore __all__ for index with defined exclude 3',\n        ),\n    ],\n)\ndef test_advanced_exclude_nested_lists(exclude, expected):\n    class SubSubModel(BaseModel):\n        i: int\n        j: int\n\n    class SubModel(BaseModel):\n        k: int\n        subsubs: List[SubSubModel]\n\n    class Model(BaseModel):\n        subs: List[SubModel]\n\n    m = Model(subs=[dict(k=1, subsubs=[dict(i=1, j=1), dict(i=2, j=2)]), dict(k=2, subsubs=[dict(i=3, j=3)])])\n\n    assert m.model_dump(exclude=exclude) == expected\n\n\n@pytest.mark.parametrize(\n    'include,expected',\n    [\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i'}}}}},\n            {'subs': [{'subsubs': [{'i': 1}, {'i': 2}]}, {'subsubs': [{'i': 3}]}]},\n            id='Normal nested __all__',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i'}}}, 0: {'subsubs': {'__all__': {'j'}}}}},\n            {'subs': [{'subsubs': [{'i': 1, 'j': 1}, {'i': 2, 'j': 2}]}, {'subsubs': [{'i': 3}]}]},\n            id='Merge sub dicts 1',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': ...}, 0: {'subsubs': {'__all__': {'j'}}}}},\n            {'subs': [{'subsubs': [{'j': 1}, {'j': 2}]}, {'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Merge sub dicts 2',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'j'}}}, 0: {'subsubs': ...}}},\n            {'subs': [{'subsubs': [{'i': 1, 'j': 1}, {'i': 2, 'j': 2}]}, {'subsubs': [{'j': 3}]}]},\n            id='Merge sub dicts 3',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {0}}, 0: {'subsubs': {1}}}},\n            {'subs': [{'subsubs': [{'i': 1, 'j': 1}, {'i': 2, 'j': 2}]}, {'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Merge sub sets',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {0: {'i'}}}, 0: {'subsubs': {1}}}},\n            {'subs': [{'subsubs': [{'i': 1}, {'i': 2, 'j': 2}]}, {'subsubs': [{'i': 3}]}]},\n            id='Merge sub dict-set',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs'}, 0: {'k'}}},\n            {'subs': [{'k': 1, 'subsubs': [{'i': 1, 'j': 1}, {'i': 2, 'j': 2}]}, {'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Nested different keys 1',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': ...}, 0: {'k'}}},\n            {'subs': [{'k': 1, 'subsubs': [{'i': 1, 'j': 1}, {'i': 2, 'j': 2}]}, {'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Nested different keys 2',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs'}, 0: {'k': ...}}},\n            {'subs': [{'k': 1, 'subsubs': [{'i': 1, 'j': 1}, {'i': 2, 'j': 2}]}, {'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Nested different keys 3',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i'}, 0: {'j'}}}}},\n            {'subs': [{'subsubs': [{'i': 1, 'j': 1}, {'i': 2}]}, {'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Nested different keys 1',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i': ...}, 0: {'j'}}}}},\n            {'subs': [{'subsubs': [{'i': 1, 'j': 1}, {'i': 2}]}, {'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Nested different keys 2',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'i'}, 0: {'j': ...}}}}},\n            {'subs': [{'subsubs': [{'i': 1, 'j': 1}, {'i': 2}]}, {'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Nested different keys 3',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs'}, 0: {'subsubs': {'__all__': {'j'}}}}},\n            {'subs': [{'subsubs': [{'j': 1}, {'j': 2}]}, {'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Ignore __all__ for index with defined include 1',\n        ),\n        pytest.param(\n            {'subs': {'__all__': {'subsubs': {'__all__': {'j'}}}, 0: ...}},\n            {'subs': [{'k': 1, 'subsubs': [{'i': 1, 'j': 1}, {'i': 2, 'j': 2}]}, {'subsubs': [{'j': 3}]}]},\n            id='Ignore __all__ for index with defined include 2',\n        ),\n        pytest.param(\n            {'subs': {'__all__': ..., 0: {'subsubs'}}},\n            {'subs': [{'subsubs': [{'i': 1, 'j': 1}, {'i': 2, 'j': 2}]}, {'k': 2, 'subsubs': [{'i': 3, 'j': 3}]}]},\n            id='Ignore __all__ for index with defined include 3',\n        ),\n    ],\n)\ndef test_advanced_include_nested_lists(include, expected):\n    class SubSubModel(BaseModel):\n        i: int\n        j: int\n\n    class SubModel(BaseModel):\n        k: int\n        subsubs: List[SubSubModel]\n\n    class Model(BaseModel):\n        subs: List[SubModel]\n\n    m = Model(subs=[dict(k=1, subsubs=[dict(i=1, j=1), dict(i=2, j=2)]), dict(k=2, subsubs=[dict(i=3, j=3)])])\n\n    assert m.model_dump(include=include) == expected\n\n\ndef test_field_set_ignore_extra():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='ignore')\n        a: int\n        b: int\n        c: int = 3\n\n    m = Model(a=1, b=2)\n    assert m.model_dump() == {'a': 1, 'b': 2, 'c': 3}\n    assert m.model_fields_set == {'a', 'b'}\n    assert m.model_dump(exclude_unset=True) == {'a': 1, 'b': 2}\n\n    m2 = Model(a=1, b=2, d=4)\n    assert m2.model_dump() == {'a': 1, 'b': 2, 'c': 3}\n    assert m2.model_fields_set == {'a', 'b'}\n    assert m2.model_dump(exclude_unset=True) == {'a': 1, 'b': 2}\n\n\ndef test_field_set_allow_extra():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        a: int\n        b: int\n        c: int = 3\n\n    m = Model(a=1, b=2)\n    assert m.model_dump() == {'a': 1, 'b': 2, 'c': 3}\n    assert m.model_fields_set == {'a', 'b'}\n    assert m.model_dump(exclude_unset=True) == {'a': 1, 'b': 2}\n\n    m2 = Model(a=1, b=2, d=4)\n    assert m2.model_dump() == {'a': 1, 'b': 2, 'c': 3, 'd': 4}\n    assert m2.model_fields_set == {'a', 'b', 'd'}\n    assert m2.model_dump(exclude_unset=True) == {'a': 1, 'b': 2, 'd': 4}\n\n\ndef test_field_set_field_name():\n    class Model(BaseModel):\n        a: int\n        field_set: int\n        b: int = 3\n\n    assert Model(a=1, field_set=2).model_dump() == {'a': 1, 'field_set': 2, 'b': 3}\n    assert Model(a=1, field_set=2).model_dump(exclude_unset=True) == {'a': 1, 'field_set': 2}\n    assert Model.model_construct(a=1, field_set=3).model_dump() == {'a': 1, 'field_set': 3, 'b': 3}\n\n\ndef test_values_order():\n    class Model(BaseModel):\n        a: int = 1\n        b: int = 2\n        c: int = 3\n\n    m = Model(c=30, b=20, a=10)\n    assert list(m) == [('a', 10), ('b', 20), ('c', 30)]\n\n\ndef test_inheritance():\n    class Foo(BaseModel):\n        a: float = ...\n\n    with pytest.raises(\n        TypeError,\n        match=(\n            \"Field 'a' defined on a base class was overridden by a non-annotated attribute. \"\n            'All field definitions, including overrides, require a type annotation.'\n        ),\n    ):\n\n        class Bar(Foo):\n            x: float = 12.3\n            a = 123.0\n\n    class Bar2(Foo):\n        x: float = 12.3\n        a: float = 123.0\n\n    assert Bar2().model_dump() == {'x': 12.3, 'a': 123.0}\n\n    class Bar3(Foo):\n        x: float = 12.3\n        a: float = Field(default=123.0)\n\n    assert Bar3().model_dump() == {'x': 12.3, 'a': 123.0}\n\n\ndef test_inheritance_subclass_default():\n    class MyStr(str):\n        pass\n\n    # Confirm hint supports a subclass default\n    class Simple(BaseModel):\n        x: str = MyStr('test')\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    # Confirm hint on a base can be overridden with a subclass default on a subclass\n    class Base(BaseModel):\n        x: str\n        y: str\n\n    class Sub(Base):\n        x: str = MyStr('test')\n        y: MyStr = MyStr('test')  # force subtype\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    assert Sub.model_fields['x'].annotation == str\n    assert Sub.model_fields['y'].annotation == MyStr\n\n\ndef test_invalid_type():\n    with pytest.raises(PydanticSchemaGenerationError) as exc_info:\n\n        class Model(BaseModel):\n            x: 43 = 123\n\n    assert 'Unable to generate pydantic-core schema for 43' in exc_info.value.args[0]\n\n\nclass CustomStr(str):\n    def foobar(self):\n        return 7\n\n\n@pytest.mark.parametrize(\n    'value,expected',\n    [\n        ('a string', 'a string'),\n        (b'some bytes', 'some bytes'),\n        (bytearray('foobar', encoding='utf8'), 'foobar'),\n        (StrEnum.a, 'a10'),\n        (CustomStr('whatever'), 'whatever'),\n    ],\n)\ndef test_valid_string_types(value, expected):\n    class Model(BaseModel):\n        v: str\n\n    assert Model(v=value).v == expected\n\n\n@pytest.mark.parametrize(\n    'value,errors',\n    [\n        (\n            {'foo': 'bar'},\n            [{'input': {'foo': 'bar'}, 'loc': ('v',), 'msg': 'Input should be a valid string', 'type': 'string_type'}],\n        ),\n        (\n            [1, 2, 3],\n            [{'input': [1, 2, 3], 'loc': ('v',), 'msg': 'Input should be a valid string', 'type': 'string_type'}],\n        ),\n    ],\n)\ndef test_invalid_string_types(value, errors):\n    class Model(BaseModel):\n        v: str\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    assert exc_info.value.errors(include_url=False) == errors\n\n\ndef test_inheritance_config():\n    class Parent(BaseModel):\n        a: str\n\n    class Child(Parent):\n        model_config = ConfigDict(str_to_lower=True)\n        b: str\n\n    m1 = Parent(a='A')\n    m2 = Child(a='A', b='B')\n    assert repr(m1) == \"Parent(a='A')\"\n    assert repr(m2) == \"Child(a='a', b='b')\"\n\n\ndef test_partial_inheritance_config():\n    class Parent(BaseModel):\n        a: int = Field(ge=0)\n\n    class Child(Parent):\n        b: int = Field(ge=0)\n\n    Child(a=0, b=0)\n    with pytest.raises(ValidationError) as exc_info:\n        Child(a=-1, b=0)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'ge': 0},\n            'input': -1,\n            'loc': ('a',),\n            'msg': 'Input should be greater than or equal to 0',\n            'type': 'greater_than_equal',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Child(a=0, b=-1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'ge': 0},\n            'input': -1,\n            'loc': ('b',),\n            'msg': 'Input should be greater than or equal to 0',\n            'type': 'greater_than_equal',\n        }\n    ]\n\n\ndef test_annotation_inheritance():\n    class A(BaseModel):\n        integer: int = 1\n\n    class B(A):\n        integer: int = 2\n\n    assert B.model_fields['integer'].annotation == int\n\n    class C(A):\n        integer: str = 'G'\n\n    assert C.__annotations__['integer'] == str\n    assert C.model_fields['integer'].annotation == str\n\n    with pytest.raises(\n        TypeError,\n        match=(\n            \"Field 'integer' defined on a base class was overridden by a non-annotated attribute. \"\n            'All field definitions, including overrides, require a type annotation.'\n        ),\n    ):\n\n        class D(A):\n            integer = 'G'\n\n\ndef test_string_none():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='ignore')\n        a: constr(min_length=20, max_length=1000) = ...\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=None)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': None, 'loc': ('a',), 'msg': 'Input should be a valid string', 'type': 'string_type'}\n    ]\n\n\n# def test_return_errors_ok():\n#     class Model(BaseModel):\n#         foo: int\n#         bar: List[int]\n#\n#     assert validate_model(Model, {'foo': '123', 'bar': (1, 2, 3)}) == (\n#         {'foo': 123, 'bar': [1, 2, 3]},\n#         {'foo', 'bar'},\n#         None,\n#     )\n#     d, f, e = validate_model(Model, {'foo': '123', 'bar': (1, 2, 3)}, False)\n#     assert d == {'foo': 123, 'bar': [1, 2, 3]}\n#     assert f == {'foo', 'bar'}\n#     assert e is None\n\n\n# def test_return_errors_error():\n#     class Model(BaseModel):\n#         foo: int\n#         bar: List[int]\n#\n#     d, f, e = validate_model(Model, {'foo': '123', 'bar': (1, 2, 'x')}, False)\n#     assert d == {'foo': 123}\n#     assert f == {'foo', 'bar'}\n#     assert e.errors() == [{'loc': ('bar', 2), 'msg': 'value is not a valid integer', 'type': 'type_error.integer'}]\n#\n#     d, f, e = validate_model(Model, {'bar': (1, 2, 3)}, False)\n#     assert d == {'bar': [1, 2, 3]}\n#     assert f == {'bar'}\n#     assert e.errors() == [{'loc': ('foo',), 'msg': 'field required', 'type': 'value_error.missing'}]\n\n\ndef test_optional_required():\n    class Model(BaseModel):\n        bar: Optional[int]\n\n    assert Model(bar=123).model_dump() == {'bar': 123}\n    assert Model(bar=None).model_dump() == {'bar': None}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('bar',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\ndef test_unable_to_infer():\n    with pytest.raises(\n        errors.PydanticUserError,\n        match=re.escape(\n            'A non-annotated attribute was detected: `x = None`. All model fields require a type annotation; '\n            'if `x` is not meant to be a field, you may be able to resolve this error by annotating it as a '\n            \"`ClassVar` or updating `model_config['ignored_types']`\"\n        ),\n    ):\n\n        class InvalidDefinitionModel(BaseModel):\n            x = None\n\n\ndef test_multiple_errors():\n    class Model(BaseModel):\n        a: Union[None, int, float, Decimal]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='foobar')\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a', 'int'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'foobar',\n        },\n        {\n            'type': 'float_parsing',\n            'loc': ('a', 'float'),\n            'msg': 'Input should be a valid number, unable to parse string as a number',\n            'input': 'foobar',\n        },\n        {\n            'type': 'decimal_parsing',\n            'loc': ('a', 'decimal'),\n            'msg': 'Input should be a valid decimal',\n            'input': 'foobar',\n        },\n    ]\n\n    assert Model(a=1.5).a == 1.5\n    assert Model(a=None).a is None\n\n\ndef test_validate_default():\n    class Model(BaseModel):\n        model_config = ConfigDict(validate_default=True)\n        a: int\n        b: int\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('a',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {}, 'loc': ('b',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n\n\ndef test_force_extra():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='ignore')\n        foo: int\n\n    assert Model.model_config['extra'] == 'ignore'\n\n\ndef test_submodel_different_type():\n    class Foo(BaseModel):\n        a: int\n\n    class Bar(BaseModel):\n        b: int\n\n    class Spam(BaseModel):\n        c: Foo\n\n    assert Spam(c={'a': '123'}).model_dump() == {'c': {'a': 123}}\n    with pytest.raises(ValidationError):\n        Spam(c={'b': '123'})\n\n    assert Spam(c=Foo(a='123')).model_dump() == {'c': {'a': 123}}\n    with pytest.raises(ValidationError):\n        Spam(c=Bar(b='123'))\n\n\ndef test_self():\n    class Model(BaseModel):\n        self: str\n\n    m = Model.model_validate(dict(self='some value'))\n    assert m.model_dump() == {'self': 'some value'}\n    assert m.self == 'some value'\n    assert m.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'self': {'title': 'Self', 'type': 'string'}},\n        'required': ['self'],\n    }\n\n\ndef test_no_name_conflict_in_constructor():\n    class Model(BaseModel):\n        self: int\n\n    m = Model(**{'__pydantic_self__': 4, 'self': 2})\n    assert m.self == 2\n\n\ndef test_self_recursive():\n    class SubModel(BaseModel):\n        self: int\n\n    class Model(BaseModel):\n        sm: SubModel\n\n    m = Model.model_validate({'sm': {'self': '123'}})\n    assert m.model_dump() == {'sm': {'self': 123}}\n\n\ndef test_custom_init():\n    class Model(BaseModel):\n        x: int\n\n        def __init__(self, x: int, y: int):\n            if isinstance(y, str):\n                y = len(y)\n            super().__init__(x=x + int(y))\n\n    assert Model(x=1, y=1).x == 2\n    assert Model.model_validate({'x': 1, 'y': 1}).x == 2\n    assert Model.model_validate_json('{\"x\": 1, \"y\": 2}').x == 3\n\n    # For documentation purposes: type hints on __init__ are not currently used for validation:\n    assert Model.model_validate({'x': 1, 'y': 'abc'}).x == 4\n\n\ndef test_nested_custom_init():\n    class NestedModel(BaseModel):\n        self: str\n        modified_number: int = 1\n\n        def __init__(someinit, **kwargs):\n            super().__init__(**kwargs)\n            someinit.modified_number += 1\n\n    class TopModel(BaseModel):\n        self: str\n        nest: NestedModel\n\n    m = TopModel.model_validate(dict(self='Top Model', nest=dict(self='Nested Model', modified_number=0)))\n    assert m.self == 'Top Model'\n    assert m.nest.self == 'Nested Model'\n    assert m.nest.modified_number == 1\n\n\ndef test_init_inspection():\n    calls = []\n\n    class Foobar(BaseModel):\n        x: int\n\n        def __init__(self, **data) -> None:\n            with pytest.raises(AttributeError):\n                calls.append(data)\n                assert self.x\n            super().__init__(**data)\n\n    Foobar(x=1)\n    Foobar.model_validate({'x': 2})\n    Foobar.model_validate_json('{\"x\": 3}')\n    assert calls == [{'x': 1}, {'x': 2}, {'x': 3}]\n\n\ndef test_type_on_annotation():\n    class FooBar:\n        pass\n\n    class Model(BaseModel):\n        a: Type[int]\n        b: Type[int] = int\n        c: Type[FooBar]\n        d: Type[FooBar] = FooBar\n        e: Sequence[Type[FooBar]] = [FooBar]\n        f: Union[Type[FooBar], Sequence[Type[FooBar]]] = FooBar\n        g: Union[Type[FooBar], Sequence[Type[FooBar]]] = [FooBar]\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    assert Model.model_fields.keys() == set('abcdefg')\n\n\ndef test_assign_type():\n    class Parent:\n        def echo(self):\n            return 'parent'\n\n    class Child(Parent):\n        def echo(self):\n            return 'child'\n\n    class Different:\n        def echo(self):\n            return 'different'\n\n    class Model(BaseModel):\n        v: Type[Parent] = Parent\n\n    assert Model(v=Parent).v().echo() == 'parent'\n    assert Model().v().echo() == 'parent'\n    assert Model(v=Child).v().echo() == 'child'\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=Different)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'test_assign_type.<locals>.Parent'},\n            'input': HasRepr(\"<class 'tests.test_edge_cases.test_assign_type.<locals>.Different'>\"),\n            'loc': ('v',),\n            'msg': 'Input should be a subclass of test_assign_type.<locals>.Parent',\n            'type': 'is_subclass_of',\n        }\n    ]\n\n\ndef test_optional_subfields():\n    class Model(BaseModel):\n        a: Optional[int]\n\n    assert Model.model_fields['a'].annotation == Optional[int]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='foobar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'foobar',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('a',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n    assert Model(a=None).a is None\n    assert Model(a=12).a == 12\n\n\ndef test_validated_optional_subfields():\n    class Model(BaseModel):\n        a: Optional[int]\n\n        @field_validator('a')\n        @classmethod\n        def check_a(cls, v):\n            return v\n\n    assert Model.model_fields['a'].annotation == Optional[int]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='foobar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'foobar',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('a',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n    assert Model(a=None).a is None\n    assert Model(a=12).a == 12\n\n\ndef test_optional_field_constraints():\n    class MyModel(BaseModel):\n        my_int: Optional[int] = Field(..., ge=3)\n\n    with pytest.raises(ValidationError) as exc_info:\n        MyModel(my_int=2)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'ge': 3},\n            'input': 2,\n            'loc': ('my_int',),\n            'msg': 'Input should be greater than or equal to 3',\n            'type': 'greater_than_equal',\n        }\n    ]\n\n\ndef test_field_str_shape():\n    class Model(BaseModel):\n        a: List[int]\n\n    assert repr(Model.model_fields['a']) == 'FieldInfo(annotation=List[int], required=True)'\n    assert str(Model.model_fields['a']) == 'annotation=List[int] required=True'\n\n\nT1 = TypeVar('T1')\nT2 = TypeVar('T2')\n\n\nclass DisplayGen(Generic[T1, T2]):\n    def __init__(self, t1: T1, t2: T2):\n        self.t1 = t1\n        self.t2 = t2\n\n\n@pytest.mark.parametrize(\n    'type_,expected',\n    [\n        (int, 'int'),\n        (Optional[int], 'Union[int, NoneType]'),\n        (Union[None, int, str], 'Union[NoneType, int, str]'),\n        (Union[int, str, bytes], 'Union[int, str, bytes]'),\n        (List[int], 'List[int]'),\n        (Tuple[int, str, bytes], 'Tuple[int, str, bytes]'),\n        (Union[List[int], Set[bytes]], 'Union[List[int], Set[bytes]]'),\n        (List[Tuple[int, int]], 'List[Tuple[int, int]]'),\n        (Dict[int, str], 'Dict[int, str]'),\n        (FrozenSet[int], 'FrozenSet[int]'),\n        (Tuple[int, ...], 'Tuple[int, ...]'),\n        (Optional[List[int]], 'Union[List[int], NoneType]'),\n        (dict, 'dict'),\n        pytest.param(\n            DisplayGen[bool, str],\n            'DisplayGen[bool, str]',\n            marks=pytest.mark.skipif(sys.version_info[:2] <= (3, 9), reason='difference in __name__ between versions'),\n        ),\n        pytest.param(\n            DisplayGen[bool, str],\n            'tests.test_edge_cases.DisplayGen[bool, str]',\n            marks=pytest.mark.skipif(sys.version_info[:2] > (3, 9), reason='difference in __name__ between versions'),\n        ),\n    ],\n)\ndef test_field_type_display(type_, expected):\n    class Model(BaseModel):\n        a: type_\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    assert re.search(rf'\\(annotation={re.escape(expected)},', str(Model.model_fields))\n\n\ndef test_any_none():\n    class MyModel(BaseModel):\n        foo: Any\n\n    m = MyModel(foo=None)\n    assert dict(m) == {'foo': None}\n\n\ndef test_type_var_any():\n    Foobar = TypeVar('Foobar')\n\n    class MyModel(BaseModel):\n        foo: Foobar\n\n    assert MyModel.model_json_schema() == {\n        'properties': {'foo': {'title': 'Foo'}},\n        'required': ['foo'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n    assert MyModel(foo=None).foo is None\n    assert MyModel(foo='x').foo == 'x'\n    assert MyModel(foo=123).foo == 123\n\n\ndef test_type_var_constraint():\n    Foobar = TypeVar('Foobar', int, str)\n\n    class MyModel(BaseModel):\n        foo: Foobar\n\n    assert MyModel.model_json_schema() == {\n        'title': 'MyModel',\n        'type': 'object',\n        'properties': {'foo': {'title': 'Foo', 'anyOf': [{'type': 'integer'}, {'type': 'string'}]}},\n        'required': ['foo'],\n    }\n    with pytest.raises(ValidationError) as exc_info:\n        MyModel(foo=None)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': None, 'loc': ('foo', 'int'), 'msg': 'Input should be a valid integer', 'type': 'int_type'},\n        {'input': None, 'loc': ('foo', 'str'), 'msg': 'Input should be a valid string', 'type': 'string_type'},\n    ]\n\n    with pytest.raises(ValidationError):\n        MyModel(foo=[1, 2, 3])\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': None, 'loc': ('foo', 'int'), 'msg': 'Input should be a valid integer', 'type': 'int_type'},\n        {'input': None, 'loc': ('foo', 'str'), 'msg': 'Input should be a valid string', 'type': 'string_type'},\n    ]\n\n    assert MyModel(foo='x').foo == 'x'\n    assert MyModel(foo=123).foo == 123\n\n\ndef test_type_var_bound():\n    Foobar = TypeVar('Foobar', bound=int)\n\n    class MyModel(BaseModel):\n        foo: Foobar\n\n    assert MyModel.model_json_schema() == {\n        'title': 'MyModel',\n        'type': 'object',\n        'properties': {'foo': {'title': 'Foo', 'type': 'integer'}},\n        'required': ['foo'],\n    }\n    with pytest.raises(ValidationError) as exc_info:\n        MyModel(foo=None)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': None, 'loc': ('foo',), 'msg': 'Input should be a valid integer', 'type': 'int_type'}\n    ]\n\n    with pytest.raises(ValidationError):\n        MyModel(foo='x')\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': None, 'loc': ('foo',), 'msg': 'Input should be a valid integer', 'type': 'int_type'}\n    ]\n    assert MyModel(foo=123).foo == 123\n\n\ndef test_dict_bare():\n    class MyModel(BaseModel):\n        foo: Dict\n\n    m = MyModel(foo={'x': 'a', 'y': None})\n    assert m.foo == {'x': 'a', 'y': None}\n\n\ndef test_list_bare():\n    class MyModel(BaseModel):\n        foo: List\n\n    m = MyModel(foo=[1, 2, None])\n    assert m.foo == [1, 2, None]\n\n\ndef test_dict_any():\n    class MyModel(BaseModel):\n        foo: Dict[str, Any]\n\n    m = MyModel(foo={'x': 'a', 'y': None})\n    assert m.foo == {'x': 'a', 'y': None}\n\n\ndef test_modify_fields():\n    class Foo(BaseModel):\n        foo: List[List[int]]\n\n        @field_validator('foo')\n        @classmethod\n        def check_something(cls, value):\n            return value\n\n    class Bar(Foo):\n        pass\n\n    assert repr(Foo.model_fields['foo']) == 'FieldInfo(annotation=List[List[int]], required=True)'\n    assert repr(Bar.model_fields['foo']) == 'FieldInfo(annotation=List[List[int]], required=True)'\n    assert Foo(foo=[[0, 1]]).foo == [[0, 1]]\n    assert Bar(foo=[[0, 1]]).foo == [[0, 1]]\n\n\ndef test_exclude_none():\n    class MyModel(BaseModel):\n        a: Optional[int] = None\n        b: int = 2\n\n    m = MyModel(a=5)\n    assert m.model_dump(exclude_none=True) == {'a': 5, 'b': 2}\n\n    m = MyModel(b=3)\n    assert m.model_dump(exclude_none=True) == {'b': 3}\n    assert m.model_dump_json(exclude_none=True) == '{\"b\":3}'\n\n\ndef test_exclude_none_recursive():\n    class ModelA(BaseModel):\n        a: Optional[int] = None\n        b: int = 1\n\n    class ModelB(BaseModel):\n        c: int\n        d: int = 2\n        e: ModelA\n        f: Optional[str] = None\n\n    m = ModelB(c=5, e={'a': 0})\n    assert m.model_dump() == {'c': 5, 'd': 2, 'e': {'a': 0, 'b': 1}, 'f': None}\n    assert m.model_dump(exclude_none=True) == {'c': 5, 'd': 2, 'e': {'a': 0, 'b': 1}}\n    assert dict(m) == {'c': 5, 'd': 2, 'e': ModelA(a=0), 'f': None}\n\n    m = ModelB(c=5, e={'b': 20}, f='test')\n    assert m.model_dump() == {'c': 5, 'd': 2, 'e': {'a': None, 'b': 20}, 'f': 'test'}\n    assert m.model_dump(exclude_none=True) == {'c': 5, 'd': 2, 'e': {'b': 20}, 'f': 'test'}\n    assert dict(m) == {'c': 5, 'd': 2, 'e': ModelA(b=20), 'f': 'test'}\n\n\ndef test_exclude_none_with_extra():\n    class MyModel(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        a: str = 'default'\n        b: Optional[str] = None\n\n    m = MyModel(a='a', c='c')\n\n    assert m.model_dump(exclude_none=True) == {'a': 'a', 'c': 'c'}\n    assert m.model_dump() == {'a': 'a', 'b': None, 'c': 'c'}\n\n    m = MyModel(a='a', b='b', c=None)\n\n    assert m.model_dump(exclude_none=True) == {'a': 'a', 'b': 'b'}\n    assert m.model_dump() == {'a': 'a', 'b': 'b', 'c': None}\n\n\ndef test_str_method_inheritance():\n    import pydantic\n\n    class Foo(pydantic.BaseModel):\n        x: int = 3\n        y: int = 4\n\n        def __str__(self):\n            return str(self.y + self.x)\n\n    class Bar(Foo):\n        z: bool = False\n\n    assert str(Foo()) == '7'\n    assert str(Bar()) == '7'\n\n\ndef test_repr_method_inheritance():\n    import pydantic\n\n    class Foo(pydantic.BaseModel):\n        x: int = 3\n        y: int = 4\n\n        def __repr__(self):\n            return repr(self.y + self.x)\n\n    class Bar(Foo):\n        z: bool = False\n\n    assert repr(Foo()) == '7'\n    assert repr(Bar()) == '7'\n\n\ndef test_optional_validator():\n    val_calls = []\n\n    class Model(BaseModel):\n        something: Optional[str]\n\n        @field_validator('something')\n        @classmethod\n        def check_something(cls, v):\n            val_calls.append(v)\n            return v\n\n    with pytest.raises(ValidationError) as exc_info:\n        assert Model().model_dump() == {'something': None}\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('something',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n    assert Model(something=None).model_dump() == {'something': None}\n    assert Model(something='hello').model_dump() == {'something': 'hello'}\n    assert val_calls == [None, 'hello']\n\n\ndef test_required_optional():\n    class Model(BaseModel):\n        nullable1: Optional[int] = ...\n        nullable2: Optional[int] = Field(...)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('nullable1',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {}, 'loc': ('nullable2',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        Model(nullable1=1)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'nullable1': 1}, 'loc': ('nullable2',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        Model(nullable2=2)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'nullable2': 2}, 'loc': ('nullable1',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n    assert Model(nullable1=None, nullable2=None).model_dump() == {'nullable1': None, 'nullable2': None}\n    assert Model(nullable1=1, nullable2=2).model_dump() == {'nullable1': 1, 'nullable2': 2}\n    with pytest.raises(ValidationError) as exc_info:\n        Model(nullable1='some text')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'some text',\n            'loc': ('nullable1',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n        {'input': {'nullable1': 'some text'}, 'loc': ('nullable2',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n\n\ndef test_required_any():\n    class Model(BaseModel):\n        optional1: Any\n        optional2: Any = None\n        optional3: Optional[Any] = None\n        nullable1: Any = ...\n        nullable2: Any = Field(...)\n        nullable3: Optional[Any]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('optional1',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {}, 'loc': ('nullable1',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {}, 'loc': ('nullable2',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {}, 'loc': ('nullable3',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        Model(nullable1='a')\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'nullable1': 'a'}, 'loc': ('optional1',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {'nullable1': 'a'}, 'loc': ('nullable2',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {'nullable1': 'a'}, 'loc': ('nullable3',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        Model(nullable2=False)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'nullable2': False}, 'loc': ('optional1',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {'nullable2': False}, 'loc': ('nullable1',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {'nullable2': False}, 'loc': ('nullable3',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        assert Model(nullable1=None, nullable2=None).model_dump() == {\n            'optional1': None,\n            'optional2': None,\n            'nullable1': None,\n            'nullable2': None,\n        }\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': {'nullable1': None, 'nullable2': None},\n            'loc': ('optional1',),\n            'msg': 'Field required',\n            'type': 'missing',\n        },\n        {\n            'input': {'nullable1': None, 'nullable2': None},\n            'loc': ('nullable3',),\n            'msg': 'Field required',\n            'type': 'missing',\n        },\n    ]\n    assert Model(optional1=None, nullable1=1, nullable2='two', nullable3=None).model_dump() == {\n        'optional1': None,\n        'optional2': None,\n        'optional3': None,\n        'nullable1': 1,\n        'nullable2': 'two',\n        'nullable3': None,\n    }\n    assert Model(optional1='op1', optional2=False, nullable1=1, nullable2='two', nullable3='three').model_dump() == {\n        'optional1': 'op1',\n        'optional2': False,\n        'optional3': None,\n        'nullable1': 1,\n        'nullable2': 'two',\n        'nullable3': 'three',\n    }\n\n\ndef test_custom_generic_validators():\n    T1 = TypeVar('T1')\n    T2 = TypeVar('T2')\n\n    class MyGen(Generic[T1, T2]):\n        def __init__(self, t1: T1, t2: T2):\n            self.t1 = t1\n            self.t2 = t2\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler):\n            schema = core_schema.is_instance_schema(cls)\n\n            args = get_args(source)\n            if not args:\n                return schema\n\n            t1_f = TypeAdapter(args[0]).validate_python\n            t2_f = TypeAdapter(args[1]).validate_python\n\n            def convert_to_init_error(e: ErrorDetails, loc: str) -> InitErrorDetails:\n                init_e = {'type': e['type'], 'loc': e['loc'] + (loc,), 'input': e['input']}\n                if 'ctx' in e:\n                    init_e['ctx'] = e['ctx']\n                return init_e\n\n            def validate(v, _info):\n                if not args:\n                    return v\n                try:\n                    v.t1 = t1_f(v.t1)\n                except ValidationError as exc:\n                    raise ValidationError.from_exception_data(\n                        exc.title, [convert_to_init_error(e, 't1') for e in exc.errors()]\n                    ) from exc\n                try:\n                    v.t2 = t2_f(v.t2)\n                except ValidationError as exc:\n                    raise ValidationError.from_exception_data(\n                        exc.title, [convert_to_init_error(e, 't2') for e in exc.errors()]\n                    ) from exc\n                return v\n\n            return core_schema.with_info_after_validator_function(validate, schema)\n\n    class Model(BaseModel):\n        a: str\n        gen: MyGen[str, bool]\n        gen2: MyGen\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='foo', gen='invalid', gen2='invalid')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'test_custom_generic_validators.<locals>.MyGen'},\n            'input': 'invalid',\n            'loc': ('gen',),\n            'msg': 'Input should be an instance of test_custom_generic_validators.<locals>.MyGen',\n            'type': 'is_instance_of',\n        },\n        {\n            'ctx': {'class': 'test_custom_generic_validators.<locals>.MyGen'},\n            'input': 'invalid',\n            'loc': ('gen2',),\n            'msg': 'Input should be an instance of test_custom_generic_validators.<locals>.MyGen',\n            'type': 'is_instance_of',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='foo', gen=MyGen(t1='bar', t2='baz'), gen2=MyGen(t1='bar', t2='baz'))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'baz',\n            'loc': ('gen', 't2'),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'type': 'bool_parsing',\n        }\n    ]\n\n    m = Model(a='foo', gen=MyGen(t1='bar', t2=True), gen2=MyGen(t1=1, t2=2))\n    assert m.a == 'foo'\n    assert m.gen.t1 == 'bar'\n    assert m.gen.t2 is True\n    assert m.gen2.t1 == 1\n    assert m.gen2.t2 == 2\n\n\ndef test_custom_generic_arbitrary_allowed():\n    T1 = TypeVar('T1')\n    T2 = TypeVar('T2')\n\n    class MyGen(Generic[T1, T2]):\n        def __init__(self, t1: T1, t2: T2):\n            self.t1 = t1\n            self.t2 = t2\n\n    class Model(BaseModel):\n        a: str\n        gen: MyGen[str, bool]\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='foo', gen='invalid')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'test_custom_generic_arbitrary_allowed.<locals>.MyGen'},\n            'input': 'invalid',\n            'loc': ('gen',),\n            'msg': 'Input should be an instance of ' 'test_custom_generic_arbitrary_allowed.<locals>.MyGen',\n            'type': 'is_instance_of',\n        }\n    ]\n\n    # No validation, no exception\n    m = Model(a='foo', gen=MyGen(t1='bar', t2='baz'))\n    assert m.a == 'foo'\n    assert m.gen.t1 == 'bar'\n    assert m.gen.t2 == 'baz'\n\n    m = Model(a='foo', gen=MyGen(t1='bar', t2=True))\n    assert m.a == 'foo'\n    assert m.gen.t1 == 'bar'\n    assert m.gen.t2 is True\n\n\ndef test_custom_generic_disallowed():\n    T1 = TypeVar('T1')\n    T2 = TypeVar('T2')\n\n    class MyGen(Generic[T1, T2]):\n        def __init__(self, t1: T1, t2: T2):\n            self.t1 = t1\n            self.t2 = t2\n\n    match = (\n        r'Unable to generate pydantic-core schema for (.*)MyGen\\[str, bool\\](.*). '\n        r'Set `arbitrary_types_allowed=True` in the model_config to ignore this error'\n    )\n    with pytest.raises(TypeError, match=match):\n\n        class Model(BaseModel):\n            a: str\n            gen: MyGen[str, bool]\n\n\ndef test_hashable_required():\n    class Model(BaseModel):\n        v: Hashable\n\n    Model(v=None)\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=[])\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': [], 'loc': ('v',), 'msg': 'Input should be hashable', 'type': 'is_hashable'}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('v',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\n@pytest.mark.parametrize('default', [1, None])\ndef test_hashable_optional(default):\n    class Model(BaseModel):\n        v: Hashable = default\n\n    Model(v=None)\n    Model()\n\n\ndef test_hashable_serialization():\n    class Model(BaseModel):\n        v: Hashable\n\n    class HashableButNotSerializable:\n        def __hash__(self):\n            return 0\n\n    assert Model(v=(1,)).model_dump_json() == '{\"v\":[1]}'\n    m = Model(v=HashableButNotSerializable())\n    with pytest.raises(\n        PydanticSerializationError, match='Unable to serialize unknown type:.*HashableButNotSerializable'\n    ):\n        m.model_dump_json()\n\n\ndef test_hashable_json_schema():\n    class Model(BaseModel):\n        v: Hashable\n\n    with pytest.raises(\n        PydanticInvalidForJsonSchema,\n        match=re.escape(\n            \"Cannot generate a JsonSchema for core_schema.IsInstanceSchema (<class 'collections.abc.Hashable'>)\"\n        ),\n    ):\n        Model.model_json_schema()\n\n\ndef test_default_factory_called_once():\n    \"\"\"It should never call `default_factory` more than once even when `validate_all` is set\"\"\"\n\n    v = 0\n\n    def factory() -> int:\n        nonlocal v\n        v += 1\n        return v\n\n    class MyModel(BaseModel):\n        model_config = ConfigDict(validate_default=True)\n        id: int = Field(default_factory=factory)\n\n    m1 = MyModel()\n    assert m1.id == 1\n\n    class MyBadModel(BaseModel):\n        model_config = ConfigDict(validate_default=True)\n        id: List[str] = Field(default_factory=factory)\n\n    with pytest.raises(ValidationError) as exc_info:\n        MyBadModel()\n    assert v == 2  # `factory` has been called to run validation\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 2, 'loc': ('id',), 'msg': 'Input should be a valid list', 'type': 'list_type'}\n    ]\n\n\ndef test_default_factory_validator_child():\n    class Parent(BaseModel):\n        foo: List[str] = Field(default_factory=list)\n\n        @field_validator('foo', mode='before')\n        @classmethod\n        def mutate_foo(cls, v):\n            return [f'{x}-1' for x in v]\n\n    assert Parent(foo=['a', 'b']).foo == ['a-1', 'b-1']\n\n    class Child(Parent):\n        pass\n\n    assert Child(foo=['a', 'b']).foo == ['a-1', 'b-1']\n\n\ndef test_resolve_annotations_module_missing(tmp_path):\n    # see https://github.com/pydantic/pydantic/issues/2363\n    file_path = tmp_path / 'module_to_load.py'\n    # language=Python\n    file_path.write_text(\n        \"\"\"\nfrom pydantic import BaseModel\nclass User(BaseModel):\n    id: int\n    name: str = 'Jane Doe'\n\"\"\"\n    )\n\n    spec = importlib.util.spec_from_file_location('my_test_module', file_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    assert module.User(id=12).model_dump() == {'id': 12, 'name': 'Jane Doe'}\n\n\ndef test_iter_coverage():\n    class MyModel(BaseModel):\n        x: int = 1\n        y: str = 'a'\n\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='The private method `_iter` will be removed and should no longer be used.'\n    ):\n        assert list(MyModel()._iter(by_alias=True)) == [('x', 1), ('y', 'a')]\n\n\ndef test_frozen_config_and_field():\n    class Foo(BaseModel):\n        model_config = ConfigDict(frozen=False, validate_assignment=True)\n        a: str = Field(...)\n\n    assert Foo.model_fields['a'].metadata == []\n\n    f = Foo(a='x')\n    f.a = 'y'\n    assert f.model_dump() == {'a': 'y'}\n\n    class Bar(BaseModel):\n        model_config = ConfigDict(validate_assignment=True)\n        a: str = Field(..., frozen=True)\n        c: Annotated[str, Field(frozen=True)]\n\n    assert Bar.model_fields['a'].frozen\n\n    b = Bar(a='x', c='z')\n    with pytest.raises(ValidationError) as exc_info:\n        b.a = 'y'\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 'y', 'loc': ('a',), 'msg': 'Field is frozen', 'type': 'frozen_field'}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        b.c = 'y'\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 'y', 'loc': ('c',), 'msg': 'Field is frozen', 'type': 'frozen_field'}\n    ]\n\n    assert b.model_dump() == {'a': 'x', 'c': 'z'}\n\n\ndef test_arbitrary_types_allowed_custom_eq():\n    class Foo:\n        def __eq__(self, other):\n            if other.__class__ is not Foo:\n                raise TypeError(f'Cannot interpret {other.__class__.__name__!r} as a valid type')\n            return True\n\n    class Model(BaseModel):\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n        x: Foo = Foo()\n\n    assert Model().x == Foo()\n\n\ndef test_bytes_subclass():\n    class MyModel(BaseModel):\n        my_bytes: bytes\n\n    class BytesSubclass(bytes):\n        def __new__(cls, data: bytes):\n            self = bytes.__new__(cls, data)\n            return self\n\n    m = MyModel(my_bytes=BytesSubclass(b'foobar'))\n    assert m.my_bytes.__class__ == BytesSubclass\n\n\ndef test_int_subclass():\n    class MyModel(BaseModel):\n        my_int: int\n\n    class IntSubclass(int):\n        def __new__(cls, data: int):\n            self = int.__new__(cls, data)\n            return self\n\n    m = MyModel(my_int=IntSubclass(123))\n    # This is expected behavior in `V2` because in pydantic-core we cast the value to a rust i64,\n    # so the sub-type information is lost.\"\n    # (more detail about how to handle this in: https://github.com/pydantic/pydantic/pull/5151#discussion_r1130691036)\n    assert m.my_int.__class__ != IntSubclass\n    assert isinstance(m.my_int, int)\n\n\ndef test_model_issubclass():\n    assert not issubclass(int, BaseModel)\n\n    class MyModel(BaseModel):\n        x: int\n\n    assert issubclass(MyModel, BaseModel)\n\n    class Custom:\n        __fields__ = True\n\n    assert not issubclass(Custom, BaseModel)\n\n\ndef test_long_int():\n    \"\"\"\n    see https://github.com/pydantic/pydantic/issues/1477 and in turn, https://github.com/python/cpython/issues/95778\n    \"\"\"\n\n    class Model(BaseModel):\n        x: int\n\n    assert Model(x='1' * 4_300).x == int('1' * 4_300)\n\n    too_long = '1' * 4_301\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=too_long)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing_size',\n            'loc': ('x',),\n            'msg': 'Unable to parse input string as an integer, exceeded maximum size',\n            'input': too_long,\n        }\n    ]\n\n    # this used to hang indefinitely\n    with pytest.raises(ValidationError):\n        Model(x='1' * (10**7))\n\n\ndef test_parent_field_with_default():\n    class Parent(BaseModel):\n        a: int = 1\n        b: int = Field(2)\n\n    class Child(Parent):\n        c: int = 3\n\n    c = Child()\n    assert c.a == 1\n    assert c.b == 2\n    assert c.c == 3\n\n\n@pytest.mark.skipif(sys.version_info < (3, 12), reason='error message different on older versions')\n@pytest.mark.parametrize(\n    'bases',\n    [\n        (BaseModel, ABC),\n        (ABC, BaseModel),\n        (BaseModel,),\n    ],\n)\ndef test_abstractmethod_missing_for_all_decorators(bases):\n    class AbstractSquare(*bases):\n        side: float\n\n        @field_validator('side')\n        @classmethod\n        @abstractmethod\n        def my_field_validator(cls, v):\n            raise NotImplementedError\n\n        @model_validator(mode='wrap')\n        @classmethod\n        @abstractmethod\n        def my_model_validator(cls, values, handler, info):\n            raise NotImplementedError\n\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            @root_validator(skip_on_failure=True)\n            @classmethod\n            @abstractmethod\n            def my_root_validator(cls, values):\n                raise NotImplementedError\n\n        with pytest.warns(PydanticDeprecatedSince20):\n\n            @validator('side')\n            @classmethod\n            @abstractmethod\n            def my_validator(cls, value, **kwargs):\n                raise NotImplementedError\n\n        @model_serializer(mode='wrap')\n        @abstractmethod\n        def my_model_serializer(self, handler, info):\n            raise NotImplementedError\n\n        @field_serializer('side')\n        @abstractmethod\n        def my_serializer(self, v, _info):\n            raise NotImplementedError\n\n        @computed_field\n        @property\n        @abstractmethod\n        def my_computed_field(self) -> Any:\n            raise NotImplementedError\n\n    class Square(AbstractSquare):\n        pass\n\n    with pytest.raises(\n        TypeError,\n        match=(\n            \"Can't instantiate abstract class Square without an implementation for abstract methods\"\n            \" 'my_computed_field',\"\n            \" 'my_field_validator',\"\n            \" 'my_model_serializer',\"\n            \" 'my_model_validator',\"\n            \" 'my_root_validator',\"\n            \" 'my_serializer',\"\n            \" 'my_validator'\"\n        ),\n    ):\n        Square(side=1.0)\n\n\ndef test_generic_wrapped_forwardref():\n    class Operation(BaseModel):\n        callbacks: 'list[PathItem]'\n\n    class PathItem(BaseModel):\n        pass\n\n    Operation.model_rebuild()\n\n    Operation.model_validate({'callbacks': [PathItem()]})\n    with pytest.raises(ValidationError) as exc_info:\n        Operation.model_validate({'callbacks': [1]})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': ('callbacks', 0),\n            'msg': 'Input should be a valid dictionary or instance of PathItem',\n            'input': 1,\n            'ctx': {'class_name': 'PathItem'},\n        }\n    ]\n\n\ndef test_plain_basemodel_field():\n    class Model(BaseModel):\n        x: BaseModel\n\n    class Model2(BaseModel):\n        pass\n\n    assert Model(x=Model2()).x == Model2()\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': ('x',),\n            'msg': 'Input should be a valid dictionary or instance of BaseModel',\n            'input': 1,\n            'ctx': {'class_name': 'BaseModel'},\n        }\n    ]\n\n\ndef test_invalid_forward_ref_model():\n    \"\"\"\n    This test is to document the fact that forward refs to a type with the same name as that of a field\n    can cause problems, and to demonstrate a way to work around this.\n    \"\"\"\n    # The problem:\n    if sys.version_info >= (3, 11):\n        # See PR #8243, this was a RecursionError raised by Python, but is now caught on the Pydantic side\n        error = errors.PydanticUserError\n    else:\n        error = TypeError\n    with pytest.raises(error):\n\n        class M(BaseModel):\n            B: ForwardRef('B') = Field(default=None)\n\n    # The solution:\n    class A(BaseModel):\n        B: ForwardRef('__types[\"B\"]') = Field()  # F821\n\n    assert A.model_fields['B'].annotation == ForwardRef('__types[\"B\"]')  # F821\n    A.model_rebuild(raise_errors=False)\n    assert A.model_fields['B'].annotation == ForwardRef('__types[\"B\"]')  # F821\n\n    class B(BaseModel):\n        pass\n\n    class C(BaseModel):\n        pass\n\n    assert not A.__pydantic_complete__\n    types = {'B': B}\n    A.model_rebuild(_types_namespace={'__types': types})\n    assert A.__pydantic_complete__\n\n    assert A(B=B()).B == B()\n    with pytest.raises(ValidationError) as exc_info:\n        A(B=C())\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': ('B',),\n            'msg': 'Input should be a valid dictionary or instance of B',\n            'input': C(),\n            'ctx': {'class_name': 'B'},\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    ('sequence_type', 'input_data', 'expected_error_type', 'expected_error_msg', 'expected_error_ctx'),\n    [\n        pytest.param(List[str], '1bc', 'list_type', 'Input should be a valid list', None, id='list[str]'),\n        pytest.param(\n            Sequence[str],\n            '1bc',\n            'sequence_str',\n            \"'str' instances are not allowed as a Sequence value\",\n            {'type_name': 'str'},\n            id='Sequence[str]',\n        ),\n        pytest.param(\n            Sequence[bytes],\n            b'1bc',\n            'sequence_str',\n            \"'bytes' instances are not allowed as a Sequence value\",\n            {'type_name': 'bytes'},\n            id='Sequence[bytes]',\n        ),\n    ],\n)\ndef test_sequences_str(sequence_type, input_data, expected_error_type, expected_error_msg, expected_error_ctx):\n    input_sequence = [input_data[:1], input_data[1:]]\n    expected_error = {\n        'type': expected_error_type,\n        'input': input_data,\n        'loc': ('str_sequence',),\n        'msg': expected_error_msg,\n    }\n    if expected_error_ctx is not None:\n        expected_error.update(ctx=expected_error_ctx)\n\n    class Model(BaseModel):\n        str_sequence: sequence_type\n\n    assert Model(str_sequence=input_sequence).str_sequence == input_sequence\n\n    with pytest.raises(ValidationError) as e:\n        Model(str_sequence=input_data)\n\n    assert e.value.errors(include_url=False) == [expected_error]\n\n\ndef test_multiple_enums():\n    \"\"\"See https://github.com/pydantic/pydantic/issues/6270\"\"\"\n\n    class MyEnum(Enum):\n        a = auto()\n\n    class MyModel(TypedDict):\n        a: Optional[MyEnum]\n        b: Optional[MyEnum]\n\n    TypeAdapter(MyModel)\n\n\n@pytest.mark.parametrize(\n    ('literal_type', 'other_type', 'data', 'json_value', 'data_reversed', 'json_value_reversed'),\n    [\n        (Literal[False], str, False, 'false', False, 'false'),\n        (Literal[True], str, True, 'true', True, 'true'),\n        (Literal[False], str, 'abc', '\"abc\"', 'abc', '\"abc\"'),\n        (Literal[False], int, False, 'false', False, 'false'),\n        (Literal[True], int, True, 'true', True, 'true'),\n        (Literal[False], int, 42, '42', 42, '42'),\n    ],\n)\ndef test_union_literal_with_other_type(literal_type, other_type, data, json_value, data_reversed, json_value_reversed):\n    class Model(BaseModel):\n        value: Union[literal_type, other_type]\n        value_types_reversed: Union[other_type, literal_type]\n\n    m = Model(value=data, value_types_reversed=data)\n    assert m.model_dump() == {'value': data, 'value_types_reversed': data_reversed}\n    assert m.model_dump_json() == f'{{\"value\":{json_value},\"value_types_reversed\":{json_value_reversed}}}'\n\n\ndef test_type_union():\n    class Model(BaseModel):\n        a: Type[Union[str, bytes]]\n        b: Type[Union[Any, str]]\n\n    m = Model(a=bytes, b=int)\n    assert m.model_dump() == {'a': bytes, 'b': int}\n    assert m.a == bytes\n\n\ndef test_model_repr_before_validation():\n    log = []\n\n    class MyModel(BaseModel):\n        x: int\n\n        def __init__(self, **kwargs):\n            log.append(f'before={self!r}')\n            super().__init__(**kwargs)\n            log.append(f'after={self!r}')\n\n    m = MyModel(x='10')\n    assert m.x == 10\n    # insert_assert(log)\n    assert log == ['before=MyModel()', 'after=MyModel(x=10)']\n\n\ndef test_custom_exception_handler():\n    from traceback import TracebackException\n\n    from pydantic import BaseModel\n\n    traceback_exceptions = []\n\n    class MyModel(BaseModel):\n        name: str\n\n    class CustomErrorCatcher:\n        def __enter__(self):\n            return None\n\n        def __exit__(self, _exception_type, exception, exception_traceback):\n            if exception is not None:\n                traceback_exceptions.append(\n                    TracebackException(\n                        exc_type=type(exception),\n                        exc_value=exception,\n                        exc_traceback=exception_traceback,\n                        capture_locals=True,\n                    )\n                )\n\n                return True\n            return False\n\n    with CustomErrorCatcher():\n        data = {'age': 'John Doe'}\n        MyModel(**data)\n\n    assert len(traceback_exceptions) == 1\n\n\ndef test_recursive_walk_fails_on_double_diamond_composition():\n    class A(BaseModel):\n        pass\n\n    class B(BaseModel):\n        a_1: A\n        a_2: A\n\n    class C(BaseModel):\n        b: B\n\n    class D(BaseModel):\n        c_1: C\n        c_2: C\n\n    class E(BaseModel):\n        c: C\n\n    # This is just to check that above model contraption doesn't fail\n    assert E(c=C(b=B(a_1=A(), a_2=A()))).model_dump() == {'c': {'b': {'a_1': {}, 'a_2': {}}}}\n\n\ndef test_recursive_root_models_in_discriminated_union():\n    class Model1(BaseModel):\n        kind: Literal['1'] = '1'\n        two: Optional['Model2']\n\n    class Model2(BaseModel):\n        kind: Literal['2'] = '2'\n        one: Optional[Model1]\n\n    class Root1(RootModel[Model1]):\n        @property\n        def kind(self):\n            # Ensures discriminated union validation works even with model instances\n            return self.root.kind\n\n    class Root2(RootModel[Model2]):\n        @property\n        def kind(self):\n            # Ensures discriminated union validation works even with model instances\n            return self.root.kind\n\n    class Outer(BaseModel):\n        a: Annotated[Union[Root1, Root2], Field(discriminator='kind')]\n        b: Annotated[Union[Root1, Root2], Field(discriminator='kind')]\n\n    validated = Outer.model_validate({'a': {'kind': '1', 'two': None}, 'b': {'kind': '2', 'one': None}})\n    assert validated == Outer(a=Root1(root=Model1(two=None)), b=Root2(root=Model2(one=None)))\n\n    # insert_assert(Outer.model_json_schema())\n    assert Outer.model_json_schema() == {\n        '$defs': {\n            'Model1': {\n                'properties': {\n                    'kind': {'const': '1', 'default': '1', 'enum': ['1'], 'title': 'Kind', 'type': 'string'},\n                    'two': {'anyOf': [{'$ref': '#/$defs/Model2'}, {'type': 'null'}]},\n                },\n                'required': ['two'],\n                'title': 'Model1',\n                'type': 'object',\n            },\n            'Model2': {\n                'properties': {\n                    'kind': {'const': '2', 'default': '2', 'enum': ['2'], 'title': 'Kind', 'type': 'string'},\n                    'one': {'anyOf': [{'$ref': '#/$defs/Model1'}, {'type': 'null'}]},\n                },\n                'required': ['one'],\n                'title': 'Model2',\n                'type': 'object',\n            },\n            'Root1': {'allOf': [{'$ref': '#/$defs/Model1'}], 'title': 'Root1'},\n            'Root2': {'allOf': [{'$ref': '#/$defs/Model2'}], 'title': 'Root2'},\n        },\n        'properties': {\n            'a': {\n                'discriminator': {'mapping': {'1': '#/$defs/Root1', '2': '#/$defs/Root2'}, 'propertyName': 'kind'},\n                'oneOf': [{'$ref': '#/$defs/Root1'}, {'$ref': '#/$defs/Root2'}],\n                'title': 'A',\n            },\n            'b': {\n                'discriminator': {'mapping': {'1': '#/$defs/Root1', '2': '#/$defs/Root2'}, 'propertyName': 'kind'},\n                'oneOf': [{'$ref': '#/$defs/Root1'}, {'$ref': '#/$defs/Root2'}],\n                'title': 'B',\n            },\n        },\n        'required': ['a', 'b'],\n        'title': 'Outer',\n        'type': 'object',\n    }\n\n\ndef test_eq_with_cached_property():\n    \"\"\"\n    Test BaseModel.__eq__ compatibility with functools.cached_property\n\n    See GH-7444: https://github.com/pydantic/pydantic/issues/7444\n    Previously, pydantic BaseModel.__eq__ compared the full __dict__ of\n    model instances. This is not compatible with e.g. functools.cached_property,\n    which caches the computed values in the instance's __dict__\n    \"\"\"\n\n    class Model(BaseModel):\n        attr: int\n\n        @functools.cached_property\n        def cached(self) -> int:\n            return 0\n\n    obj1 = Model(attr=1)\n    obj2 = Model(attr=1)\n    # ensure the instances are indeed equal before __dict__ mutations\n    assert obj1 == obj2\n    # This access to the cached_property has the side-effect of modifying obj1.__dict__\n    # See functools.cached_property documentation and source code\n    obj1.cached\n    # Ensure the objects still compare equals after caching a property\n    assert obj1 == obj2\n", "tests/test_serialize_as_any.py": "import json\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nimport pytest\nfrom typing_extensions import TypedDict\n\nfrom pydantic import BaseModel, ConfigDict, RootModel, SecretStr, SerializeAsAny, TypeAdapter\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\n\n\nclass User(BaseModel):\n    name: str\n\n\nclass UserLogin(User):\n    password: SecretStr\n\n\nuser = User(name='pydantic')\nuser_login = UserLogin(name='pydantic', password='password')\n\n\ndef test_serialize_as_any_annotation() -> None:\n    class OuterModel(BaseModel):\n        maybe_as_any: Optional[SerializeAsAny[User]] = None\n        as_any: SerializeAsAny[User]\n        without: User\n\n    # insert_assert(json.loads(OuterModel(as_any=user, without=user).model_dump_json()))\n    assert json.loads(OuterModel(maybe_as_any=user_login, as_any=user_login, without=user_login).model_dump_json()) == {\n        'maybe_as_any': {'name': 'pydantic', 'password': '**********'},\n        'as_any': {'name': 'pydantic', 'password': '**********'},\n        'without': {'name': 'pydantic'},\n    }\n\n\ndef test_serialize_as_any_runtime() -> None:\n    class OuterModel(BaseModel):\n        user: User\n\n    assert json.loads(OuterModel(user=user_login).model_dump_json(serialize_as_any=False)) == {\n        'user': {'name': 'pydantic'}\n    }\n    assert json.loads(OuterModel(user=user_login).model_dump_json(serialize_as_any=True)) == {\n        'user': {'name': 'pydantic', 'password': '**********'}\n    }\n\n\ndef test_serialize_as_any_runtime_recursive() -> None:\n    class User(BaseModel):\n        name: str\n        friends: List['User']\n\n    class UserLogin(User):\n        password: SecretStr\n\n    class OuterModel(BaseModel):\n        user: User\n\n    user = UserLogin(\n        name='pydantic', password='password', friends=[UserLogin(name='pydantic', password='password', friends=[])]\n    )\n\n    assert json.loads(OuterModel(user=user).model_dump_json(serialize_as_any=False)) == {\n        'user': {\n            'name': 'pydantic',\n            'friends': [{'name': 'pydantic', 'friends': []}],\n        },\n    }\n    assert json.loads(OuterModel(user=user).model_dump_json(serialize_as_any=True)) == {\n        'user': {\n            'name': 'pydantic',\n            'password': '**********',\n            'friends': [{'name': 'pydantic', 'password': '**********', 'friends': []}],\n        },\n    }\n\n\ndef test_serialize_as_any_with_rootmodel() -> None:\n    UserRoot = RootModel[User]\n    assert json.loads(UserRoot(root=user_login).model_dump_json(serialize_as_any=False)) == {'name': 'pydantic'}\n    assert json.loads(UserRoot(root=user_login).model_dump_json(serialize_as_any=True)) == {\n        'name': 'pydantic',\n        'password': '**********',\n    }\n\n\ndef test_serialize_as_any_type_adapter() -> None:\n    ta = TypeAdapter(User)\n    assert json.loads(ta.dump_json(user_login, serialize_as_any=False)) == {'name': 'pydantic'}\n    assert json.loads(ta.dump_json(user_login, serialize_as_any=True)) == {'name': 'pydantic', 'password': '**********'}\n\n\n@pytest.mark.parametrize('dataclass_constructor', [dataclass, pydantic_dataclass])\ndef test_serialize_as_any_with_dataclasses(dataclass_constructor) -> None:\n    @dataclass_constructor\n    class User:\n        name: str\n\n    @dataclass_constructor\n    class UserLogin(User):\n        password: str\n\n    user_login = UserLogin(name='pydantic', password='password')\n\n    ta = TypeAdapter(User)\n    assert json.loads(ta.dump_json(user_login, serialize_as_any=False, warnings=False)) == {'name': 'pydantic'}\n    assert json.loads(ta.dump_json(user_login, serialize_as_any=True, warnings=False)) == {\n        'name': 'pydantic',\n        'password': 'password',\n    }\n\n\ndef test_serialize_as_any_with_typed_dict() -> None:\n    class User(TypedDict):\n        name: str\n\n    class UserLogin(User):\n        password: str\n\n    user_login = UserLogin(name='pydantic', password='password')\n\n    ta = TypeAdapter(User)\n    assert json.loads(ta.dump_json(user_login, serialize_as_any=False, warnings=False)) == {'name': 'pydantic'}\n    assert json.loads(ta.dump_json(user_login, serialize_as_any=True, warnings=False)) == {\n        'name': 'pydantic',\n        'password': 'password',\n    }\n\n\ndef test_serialize_as_any_flag_on_unrelated_models() -> None:\n    class Parent(BaseModel):\n        x: int\n\n    class Other(BaseModel):\n        y: str\n\n        model_config = ConfigDict(extra='allow')\n\n    ta = TypeAdapter(Parent)\n    other = Other(x=1, y='hello')\n    assert ta.dump_python(other, serialize_as_any=False) == {}\n    assert ta.dump_python(other, serialize_as_any=True) == {'y': 'hello', 'x': 1}\n\n\ndef test_serialize_as_any_annotation_on_unrelated_models() -> None:\n    class Parent(BaseModel):\n        x: int\n\n    class Other(BaseModel):\n        y: str\n\n        model_config = ConfigDict(extra='allow')\n\n    ta = TypeAdapter(Parent)\n    other = Other(x=1, y='hello')\n    assert ta.dump_python(other) == {}\n\n    ta_any = TypeAdapter(SerializeAsAny[Parent])\n    assert ta_any.dump_python(other) == {'y': 'hello', 'x': 1}\n\n\ndef test_serialize_as_any_with_inner_models() -> None:\n    \"\"\"As with other serialization flags, serialize_as_any affects nested models as well.\"\"\"\n\n    class Inner(BaseModel):\n        x: int\n\n    class Outer(BaseModel):\n        inner: Inner\n\n    class InnerChild(Inner):\n        y: int\n\n    ta = TypeAdapter(Outer)\n    inner_child = InnerChild(x=1, y=2)\n    outer = Outer(inner=inner_child)\n\n    assert ta.dump_python(outer, serialize_as_any=False) == {'inner': {'x': 1}}\n    assert ta.dump_python(outer, serialize_as_any=True) == {'inner': {'x': 1, 'y': 2}}\n\n\ndef test_serialize_as_any_annotation_with_inner_models() -> None:\n    \"\"\"The SerializeAsAny annotation does not affect nested models.\"\"\"\n\n    class Inner(BaseModel):\n        x: int\n\n    class Outer(BaseModel):\n        inner: Inner\n\n    class InnerChild(Inner):\n        y: int\n\n    ta = TypeAdapter(SerializeAsAny[Outer])\n    inner_child = InnerChild(x=1, y=2)\n    outer = Outer(inner=inner_child)\n    assert ta.dump_python(outer) == {'inner': {'x': 1}}\n\n\ndef test_serialize_as_any_flag_with_incorrect_list_el_type() -> None:\n    # a warning is raised when using the `serialize_as_any` flag\n    ta = TypeAdapter(List[int])\n    with pytest.warns(UserWarning, match='Expected `int` but got `str`'):\n        assert ta.dump_python(['a', 'b', 'c'], serialize_as_any=False) == ['a', 'b', 'c']\n\n\ndef test_serialize_as_any_annotation_with_incorrect_list_el_type() -> None:\n    # notably, the warning is not raised when using the SerializeAsAny annotation\n    ta = TypeAdapter(SerializeAsAny[List[int]])\n    assert ta.dump_python(['a', 'b', 'c']) == ['a', 'b', 'c']\n", "tests/test_warnings.py": "from pydantic import PydanticDeprecatedSince20, PydanticDeprecationWarning\nfrom pydantic.version import version_short\n\n\ndef test_pydantic_deprecation_warning():\n    warning = PydanticDeprecationWarning('Warning message', 'Arbitrary argument', since=(2, 1), expected_removal=(4, 0))\n\n    assert str(warning) == 'Warning message. Deprecated in Pydantic V2.1 to be removed in V4.0.'\n    assert warning.args[0] == 'Warning message'\n    assert warning.args[1] == 'Arbitrary argument'\n\n\ndef test_pydantic_deprecation_warning_tailing_dot_in_message():\n    warning = PydanticDeprecationWarning('Warning message.', since=(2, 1), expected_removal=(4, 0))\n\n    assert str(warning) == 'Warning message. Deprecated in Pydantic V2.1 to be removed in V4.0.'\n    assert warning.args[0] == 'Warning message.'\n\n\ndef test_pydantic_deprecation_warning_calculated_expected_removal():\n    warning = PydanticDeprecationWarning('Warning message', since=(2, 1))\n\n    assert str(warning) == 'Warning message. Deprecated in Pydantic V2.1 to be removed in V3.0.'\n\n\ndef test_pydantic_deprecation_warning_2_0_migration_guide_link():\n    warning = PydanticDeprecationWarning('Warning message', since=(2, 0))\n\n    assert (\n        str(warning)\n        == f'Warning message. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/{version_short()}/migration/'\n    )\n\n\ndef test_pydantic_deprecated_since_2_0_warning():\n    warning = PydanticDeprecatedSince20('Warning message')\n\n    assert isinstance(warning, PydanticDeprecationWarning)\n    assert warning.message == 'Warning message'\n    assert warning.since == (2, 0)\n    assert warning.expected_removal == (3, 0)\n", "tests/test_config.py": "import json\nimport re\nimport sys\nfrom contextlib import nullcontext as does_not_raise\nfrom decimal import Decimal\nfrom inspect import signature\nfrom typing import Any, ContextManager, Dict, Iterable, NamedTuple, Optional, Tuple, Type, Union\n\nfrom dirty_equals import HasRepr, IsPartialDict\nfrom pydantic_core import SchemaError, SchemaSerializer, SchemaValidator\n\nfrom pydantic import (\n    BaseConfig,\n    BaseModel,\n    Field,\n    GenerateSchema,\n    PrivateAttr,\n    PydanticDeprecatedSince20,\n    PydanticSchemaGenerationError,\n    ValidationError,\n    create_model,\n    field_validator,\n    validate_call,\n    with_config,\n)\nfrom pydantic._internal._config import ConfigWrapper, config_defaults\nfrom pydantic._internal._mock_val_ser import MockValSer\nfrom pydantic._internal._typing_extra import get_type_hints\nfrom pydantic.config import ConfigDict, JsonValue\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\nfrom pydantic.errors import PydanticUserError\nfrom pydantic.fields import ComputedFieldInfo, FieldInfo\nfrom pydantic.type_adapter import TypeAdapter\nfrom pydantic.warnings import PydanticDeprecationWarning\n\nfrom .conftest import CallCounter\n\nif sys.version_info < (3, 9):\n    from typing_extensions import Annotated, Literal\nelse:\n    from typing import Annotated, Literal\n\nimport pytest\n\n\n@pytest.fixture(scope='session', name='BaseConfigModelWithStrictConfig')\ndef model_with_strict_config():\n    class ModelWithStrictConfig(BaseModel):\n        a: int\n        # strict=False overrides the Config\n        b: Annotated[int, Field(strict=False)]\n        # strict=None or not including it is equivalent\n        # lets this field be overridden by the Config\n        c: Annotated[int, Field(strict=None)]\n        d: Annotated[int, Field()]\n\n        model_config = ConfigDict(strict=True)\n\n    return ModelWithStrictConfig\n\n\ndef _equals(a: Union[str, Iterable[str]], b: Union[str, Iterable[str]]) -> bool:\n    \"\"\"\n    Compare strings with spaces removed\n    \"\"\"\n    if isinstance(a, str) and isinstance(b, str):\n        return a.replace(' ', '') == b.replace(' ', '')\n    elif isinstance(a, Iterable) and isinstance(b, Iterable):\n        return all(_equals(a_, b_) for a_, b_ in zip(a, b))\n    else:\n        raise TypeError(f'arguments must be both strings or both lists, not {type(a)}, {type(b)}')\n\n\ndef test_config_dict_missing_keys():\n    assert ConfigDict().get('missing_property') is None\n\n    with pytest.raises(KeyError, match=\"'missing_property'\"):\n        ConfigDict()['missing_property']\n\n\nclass TestsBaseConfig:\n    @pytest.mark.filterwarnings('ignore:.* is deprecated.*:DeprecationWarning')\n    def test_base_config_equality_defaults_of_config_dict_class(self):\n        for key, value in config_defaults.items():\n            assert getattr(BaseConfig, key) == value\n\n    def test_config_and_module_config_cannot_be_used_together(self):\n        with pytest.raises(PydanticUserError):\n\n            class MyModel(BaseModel):\n                model_config = ConfigDict(title='MyTitle')\n\n                class Config:\n                    title = 'MyTitleConfig'\n\n    @pytest.mark.filterwarnings('ignore:.* is deprecated.*:DeprecationWarning')\n    def test_base_config_properly_converted_to_dict(self):\n        class MyConfig(BaseConfig):\n            title = 'MyTitle'\n            frozen = True\n\n        class MyBaseModel(BaseModel):\n            class Config(MyConfig): ...\n\n        class MyModel(MyBaseModel): ...\n\n        MyModel.model_config['title'] = 'MyTitle'\n        MyModel.model_config['frozen'] = True\n        assert 'str_to_lower' not in MyModel.model_config\n\n    def test_base_config_custom_init_signature(self):\n        class MyModel(BaseModel):\n            id: int\n            name: str = 'John Doe'\n            f__: str = Field(..., alias='foo')\n\n            model_config = ConfigDict(extra='allow')\n\n            def __init__(self, id: int = 1, bar=2, *, baz: Any, **data):\n                super().__init__(id=id, **data)\n                self.bar = bar\n                self.baz = baz\n\n        sig = signature(MyModel)\n        assert _equals(\n            map(str, sig.parameters.values()),\n            ('id: int = 1', 'bar=2', 'baz: Any', \"name: str = 'John Doe'\", 'foo: str', '**data'),\n        )\n        assert _equals(str(sig), \"(id: int = 1, bar=2, *, baz: Any, name: str = 'John Doe', foo: str, **data) -> None\")\n\n    def test_base_config_custom_init_signature_with_no_var_kw(self):\n        class Model(BaseModel):\n            a: float\n            b: int = 2\n            c: int\n\n            def __init__(self, a: float, b: int):\n                super().__init__(a=a, b=b, c=1)\n\n            model_config = ConfigDict(extra='allow')\n\n        assert _equals(str(signature(Model)), '(a: float, b: int) -> None')\n\n    def test_base_config_use_field_name(self):\n        class Foo(BaseModel):\n            foo: str = Field(..., alias='this is invalid')\n\n            model_config = ConfigDict(populate_by_name=True)\n\n        assert _equals(str(signature(Foo)), '(*, foo: str) -> None')\n\n    def test_base_config_does_not_use_reserved_word(self):\n        class Foo(BaseModel):\n            from_: str = Field(..., alias='from')\n\n            model_config = ConfigDict(populate_by_name=True)\n\n        assert _equals(str(signature(Foo)), '(*, from_: str) -> None')\n\n    def test_base_config_extra_allow_no_conflict(self):\n        class Model(BaseModel):\n            spam: str\n\n            model_config = ConfigDict(extra='allow')\n\n        assert _equals(str(signature(Model)), '(*, spam: str, **extra_data: Any) -> None')\n\n    def test_base_config_extra_allow_conflict_twice(self):\n        class Model(BaseModel):\n            extra_data: str\n            extra_data_: str\n\n            model_config = ConfigDict(extra='allow')\n\n        assert _equals(str(signature(Model)), '(*, extra_data: str, extra_data_: str, **extra_data__: Any) -> None')\n\n    def test_base_config_extra_allow_conflict_custom_signature(self):\n        class Model(BaseModel):\n            extra_data: int\n\n            def __init__(self, extra_data: int = 1, **foobar: Any):\n                super().__init__(extra_data=extra_data, **foobar)\n\n            model_config = ConfigDict(extra='allow')\n\n        assert _equals(str(signature(Model)), '(extra_data: int = 1, **foobar: Any) -> None')\n\n    def test_base_config_private_attribute_intersection_with_extra_field(self):\n        class Model(BaseModel):\n            _foo = PrivateAttr('private_attribute')\n\n            model_config = ConfigDict(extra='allow')\n\n        assert set(Model.__private_attributes__) == {'_foo'}\n        m = Model(_foo='field')\n        assert m._foo == 'private_attribute'\n        assert m.__dict__ == {}\n        assert m.__pydantic_extra__ == {'_foo': 'field'}\n        assert m.model_dump() == {'_foo': 'field'}\n        m._foo = 'still_private'\n        assert m._foo == 'still_private'\n        assert m.__dict__ == {}\n        assert m.__pydantic_extra__ == {'_foo': 'field'}\n        assert m.model_dump() == {'_foo': 'field'}\n\n    def test_base_config_parse_model_with_strict_config_disabled(\n        self, BaseConfigModelWithStrictConfig: Type[BaseModel]\n    ) -> None:\n        class Model(BaseConfigModelWithStrictConfig):\n            model_config = ConfigDict(strict=False)\n\n        values = [\n            Model(a='1', b=2, c=3, d=4),\n            Model(a=1, b=2, c='3', d=4),\n            Model(a=1, b=2, c=3, d='4'),\n            Model(a=1, b='2', c=3, d=4),\n            Model(a=1, b=2, c=3, d=4),\n        ]\n        assert all(v.model_dump() == {'a': 1, 'b': 2, 'c': 3, 'd': 4} for v in values)\n\n    def test_finite_float_config(self):\n        class Model(BaseModel):\n            a: float\n\n            model_config = ConfigDict(allow_inf_nan=False)\n\n        assert Model(a=42).a == 42\n        with pytest.raises(ValidationError) as exc_info:\n            Model(a=float('nan'))\n        # insert_assert(exc_info.value.errors(include_url=False))\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'type': 'finite_number',\n                'loc': ('a',),\n                'msg': 'Input should be a finite number',\n                'input': HasRepr('nan'),\n            }\n        ]\n\n    @pytest.mark.parametrize(\n        'enabled,str_check,result_str_check',\n        [\n            (True, '  123  ', '123'),\n            (True, '  123\\t\\n', '123'),\n            (False, '  123  ', '  123  '),\n        ],\n    )\n    def test_str_strip_whitespace(self, enabled, str_check, result_str_check):\n        class Model(BaseModel):\n            str_check: str\n\n            model_config = ConfigDict(str_strip_whitespace=enabled)\n\n        m = Model(str_check=str_check)\n        assert m.str_check == result_str_check\n\n    @pytest.mark.parametrize(\n        'enabled,str_check,result_str_check',\n        [(True, 'ABCDefG', 'ABCDEFG'), (False, 'ABCDefG', 'ABCDefG')],\n    )\n    def test_str_to_upper(self, enabled, str_check, result_str_check):\n        class Model(BaseModel):\n            str_check: str\n\n            model_config = ConfigDict(str_to_upper=enabled)\n\n        m = Model(str_check=str_check)\n\n        assert m.str_check == result_str_check\n\n    @pytest.mark.parametrize(\n        'enabled,str_check,result_str_check',\n        [(True, 'ABCDefG', 'abcdefg'), (False, 'ABCDefG', 'ABCDefG')],\n    )\n    def test_str_to_lower(self, enabled, str_check, result_str_check):\n        class Model(BaseModel):\n            str_check: str\n\n            model_config = ConfigDict(str_to_lower=enabled)\n\n        m = Model(str_check=str_check)\n\n        assert m.str_check == result_str_check\n\n    def test_namedtuple_arbitrary_type(self):\n        class CustomClass:\n            pass\n\n        class Tup(NamedTuple):\n            c: CustomClass\n\n        class Model(BaseModel):\n            x: Tup\n\n            model_config = ConfigDict(arbitrary_types_allowed=True)\n\n        data = {'x': Tup(c=CustomClass())}\n        model = Model.model_validate(data)\n        assert isinstance(model.x.c, CustomClass)\n        with pytest.raises(PydanticSchemaGenerationError):\n\n            class ModelNoArbitraryTypes(BaseModel):\n                x: Tup\n\n    @pytest.mark.parametrize(\n        'use_construct, populate_by_name_config, arg_name, expectation',\n        [\n            [False, True, 'bar', does_not_raise()],\n            [False, True, 'bar_', does_not_raise()],\n            [False, False, 'bar', does_not_raise()],\n            [False, False, 'bar_', pytest.raises(ValueError)],\n            [True, True, 'bar', does_not_raise()],\n            [True, True, 'bar_', does_not_raise()],\n            [True, False, 'bar', does_not_raise()],\n            [True, False, 'bar_', does_not_raise()],\n        ],\n    )\n    def test_populate_by_name_config(\n        self,\n        use_construct: bool,\n        populate_by_name_config: bool,\n        arg_name: str,\n        expectation: ContextManager,\n    ):\n        expected_value: int = 7\n\n        class Foo(BaseModel):\n            bar_: int = Field(..., alias='bar')\n\n            model_config = dict(populate_by_name=populate_by_name_config)\n\n        with expectation:\n            if use_construct:\n                f = Foo.model_construct(**{arg_name: expected_value})\n            else:\n                f = Foo(**{arg_name: expected_value})\n            assert f.bar_ == expected_value\n\n    def test_immutable_copy_with_frozen(self):\n        class Model(BaseModel):\n            a: int\n            b: int\n\n            model_config = ConfigDict(frozen=True)\n\n        m = Model(a=40, b=10)\n        assert m == m.model_copy()\n\n    def test_config_class_is_deprecated(self):\n        with pytest.warns(PydanticDeprecatedSince20) as all_warnings:\n\n            class Config(BaseConfig):\n                pass\n\n        # typing-extensions swallows one of the warnings, so we need to support\n        # both ways for now.\n        assert len(all_warnings) in [1, 2]\n        expected_warnings = [\n            'Support for class-based `config` is deprecated, use ConfigDict instead',\n        ]\n        if len(all_warnings) == 2:\n            expected_warnings.insert(0, 'BaseConfig is deprecated. Use the `pydantic.ConfigDict` instead')\n        assert [w.message.message for w in all_warnings] == expected_warnings\n\n    def test_config_class_attributes_are_deprecated(self):\n        with pytest.warns(PydanticDeprecatedSince20) as all_warnings:\n            assert BaseConfig.validate_assignment is False\n            assert BaseConfig().validate_assignment is False\n\n            class Config(BaseConfig):\n                pass\n\n            assert Config.validate_assignment is False\n            assert Config().validate_assignment is False\n        assert len(all_warnings) == 7\n        expected_warnings = {\n            'Support for class-based `config` is deprecated, use ConfigDict instead',\n            'BaseConfig is deprecated. Use the `pydantic.ConfigDict` instead',\n            'Support for class-based `config` is deprecated, use ConfigDict instead',\n            'BaseConfig is deprecated. Use the `pydantic.ConfigDict` instead',\n            'Support for class-based `config` is deprecated, use ConfigDict instead',\n            'Support for class-based `config` is deprecated, use ConfigDict instead',\n            'Support for class-based `config` is deprecated, use ConfigDict instead',\n        }\n        assert set(w.message.message for w in all_warnings) <= expected_warnings\n\n    @pytest.mark.filterwarnings('ignore:.* is deprecated.*:DeprecationWarning')\n    def test_config_class_missing_attributes(self):\n        with pytest.raises(AttributeError, match=\"type object 'BaseConfig' has no attribute 'missing_attribute'\"):\n            BaseConfig.missing_attribute\n\n        with pytest.raises(AttributeError, match=\"'BaseConfig' object has no attribute 'missing_attribute'\"):\n            BaseConfig().missing_attribute\n\n        class Config(BaseConfig):\n            pass\n\n        with pytest.raises(AttributeError, match=\"type object 'Config' has no attribute 'missing_attribute'\"):\n            Config.missing_attribute\n\n        with pytest.raises(AttributeError, match=\"'Config' object has no attribute 'missing_attribute'\"):\n            Config().missing_attribute\n\n\ndef test_config_key_deprecation():\n    config_dict = {\n        'allow_mutation': None,\n        'error_msg_templates': None,\n        'fields': None,\n        'getter_dict': None,\n        'schema_extra': None,\n        'smart_union': None,\n        'underscore_attrs_are_private': None,\n        'allow_population_by_field_name': None,\n        'anystr_lower': None,\n        'anystr_strip_whitespace': None,\n        'anystr_upper': None,\n        'keep_untouched': None,\n        'max_anystr_length': None,\n        'min_anystr_length': None,\n        'orm_mode': None,\n        'validate_all': None,\n    }\n\n    warning_message = \"\"\"\nValid config keys have changed in V2:\n* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n* 'anystr_lower' has been renamed to 'str_to_lower'\n* 'anystr_strip_whitespace' has been renamed to 'str_strip_whitespace'\n* 'anystr_upper' has been renamed to 'str_to_upper'\n* 'keep_untouched' has been renamed to 'ignored_types'\n* 'max_anystr_length' has been renamed to 'str_max_length'\n* 'min_anystr_length' has been renamed to 'str_min_length'\n* 'orm_mode' has been renamed to 'from_attributes'\n* 'schema_extra' has been renamed to 'json_schema_extra'\n* 'validate_all' has been renamed to 'validate_default'\n* 'allow_mutation' has been removed\n* 'error_msg_templates' has been removed\n* 'fields' has been removed\n* 'getter_dict' has been removed\n* 'smart_union' has been removed\n* 'underscore_attrs_are_private' has been removed\n    \"\"\".strip()\n\n    with pytest.warns(UserWarning, match=re.escape(warning_message)):\n\n        class MyModel(BaseModel):\n            model_config = config_dict\n\n    with pytest.warns(UserWarning, match=re.escape(warning_message)):\n        create_model('MyCreatedModel', __config__=config_dict)\n\n    with pytest.warns(UserWarning, match=re.escape(warning_message)):\n\n        @pydantic_dataclass(config=config_dict)\n        class MyDataclass:\n            pass\n\n    with pytest.warns(UserWarning, match=re.escape(warning_message)):\n\n        @validate_call(config=config_dict)\n        def my_function():\n            pass\n\n\ndef test_invalid_extra():\n    extra_error = re.escape(\n        \"Input should be 'allow', 'forbid' or 'ignore'\"\n        \" [type=literal_error, input_value='invalid-value', input_type=str]\"\n    )\n    config_dict = {'extra': 'invalid-value'}\n\n    with pytest.raises(SchemaError, match=extra_error):\n\n        class MyModel(BaseModel):\n            model_config = config_dict\n\n    with pytest.raises(SchemaError, match=extra_error):\n        create_model('MyCreatedModel', __config__=config_dict)\n\n    with pytest.raises(SchemaError, match=extra_error):\n\n        @pydantic_dataclass(config=config_dict)\n        class MyDataclass:\n            pass\n\n\ndef test_invalid_config_keys():\n    @validate_call(config={'alias_generator': lambda x: x})\n    def my_function():\n        pass\n\n\ndef test_multiple_inheritance_config():\n    class Parent(BaseModel):\n        model_config = ConfigDict(frozen=True, extra='forbid')\n\n    class Mixin(BaseModel):\n        model_config = ConfigDict(use_enum_values=True)\n\n    class Child(Mixin, Parent):\n        model_config = ConfigDict(populate_by_name=True)\n\n    assert BaseModel.model_config.get('frozen') is None\n    assert BaseModel.model_config.get('populate_by_name') is None\n    assert BaseModel.model_config.get('extra') is None\n    assert BaseModel.model_config.get('use_enum_values') is None\n\n    assert Parent.model_config.get('frozen') is True\n    assert Parent.model_config.get('populate_by_name') is None\n    assert Parent.model_config.get('extra') == 'forbid'\n    assert Parent.model_config.get('use_enum_values') is None\n\n    assert Mixin.model_config.get('frozen') is None\n    assert Mixin.model_config.get('populate_by_name') is None\n    assert Mixin.model_config.get('extra') is None\n    assert Mixin.model_config.get('use_enum_values') is True\n\n    assert Child.model_config.get('frozen') is True\n    assert Child.model_config.get('populate_by_name') is True\n    assert Child.model_config.get('extra') == 'forbid'\n    assert Child.model_config.get('use_enum_values') is True\n\n\ndef test_config_wrapper_match():\n    localns = {\n        '_GenerateSchema': GenerateSchema,\n        'GenerateSchema': GenerateSchema,\n        'JsonValue': JsonValue,\n        'FieldInfo': FieldInfo,\n        'ComputedFieldInfo': ComputedFieldInfo,\n    }\n    config_dict_annotations = [(k, str(v)) for k, v in get_type_hints(ConfigDict, localns=localns).items()]\n    config_dict_annotations.sort()\n    # remove config\n    config_wrapper_annotations = [\n        (k, str(v)) for k, v in get_type_hints(ConfigWrapper, localns=localns).items() if k != 'config_dict'\n    ]\n    config_wrapper_annotations.sort()\n\n    assert (\n        config_dict_annotations == config_wrapper_annotations\n    ), 'ConfigDict and ConfigWrapper must have the same annotations (except ConfigWrapper.config_dict)'\n\n\n@pytest.mark.skipif(sys.version_info < (3, 11), reason='requires backport pre 3.11, fully tested in pydantic core')\ndef test_config_validation_error_cause():\n    class Foo(BaseModel):\n        foo: int\n\n        @field_validator('foo')\n        def check_foo(cls, v):\n            assert v > 5, 'Must be greater than 5'\n\n    # Should be disabled by default:\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(foo=4)\n    assert exc_info.value.__cause__ is None\n\n    Foo.model_config = ConfigDict(validation_error_cause=True)\n    Foo.model_rebuild(force=True)\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(foo=4)\n    # Confirm python error attached as a cause, and error location specified in a note:\n    assert exc_info.value.__cause__ is not None\n    assert isinstance(exc_info.value.__cause__, ExceptionGroup)\n    assert len(exc_info.value.__cause__.exceptions) == 1\n    src_exc = exc_info.value.__cause__.exceptions[0]\n    assert repr(src_exc) == \"AssertionError('Must be greater than 5\\\\nassert 4 > 5')\"\n    assert len(src_exc.__notes__) == 1\n    assert src_exc.__notes__[0] == '\\nPydantic: cause of loc: foo'\n\n\ndef test_config_defaults_match():\n    localns = {\n        '_GenerateSchema': GenerateSchema,\n        'GenerateSchema': GenerateSchema,\n        'FieldInfo': FieldInfo,\n        'ComputedFieldInfo': ComputedFieldInfo,\n    }\n    config_dict_keys = sorted(list(get_type_hints(ConfigDict, localns=localns).keys()))\n    config_defaults_keys = sorted(list(config_defaults.keys()))\n\n    assert config_dict_keys == config_defaults_keys, 'ConfigDict and config_defaults must have the same keys'\n\n\ndef test_config_is_not_inherited_in_model_fields():\n    from typing import List\n\n    from pydantic import BaseModel, ConfigDict\n\n    class Inner(BaseModel):\n        a: str\n\n    class Outer(BaseModel):\n        # this cause the inner model incorrectly dumpped:\n        model_config = ConfigDict(str_to_lower=True)\n\n        x: List[str]  # should be converted to lower\n        inner: Inner  # should not have fields converted to lower\n\n    m = Outer.model_validate(dict(x=['Abc'], inner=dict(a='Def')))\n\n    assert m.model_dump() == {'x': ['abc'], 'inner': {'a': 'Def'}}\n\n\n@pytest.mark.parametrize(\n    'config,input_str',\n    (\n        ({}, 'type=string_type, input_value=123, input_type=int'),\n        ({'hide_input_in_errors': False}, 'type=string_type, input_value=123, input_type=int'),\n        ({'hide_input_in_errors': True}, 'type=string_type'),\n    ),\n)\ndef test_hide_input_in_errors(config, input_str):\n    class Model(BaseModel):\n        x: str\n\n        model_config = ConfigDict(**config)\n\n    with pytest.raises(ValidationError, match=re.escape(f'Input should be a valid string [{input_str}]')):\n        Model(x=123)\n\n\nparametrize_inf_nan_capable_type = pytest.mark.parametrize('inf_nan_capable_type', [float, Decimal])\nparametrize_inf_nan_capable_value = pytest.mark.parametrize('inf_nan_value', ['Inf', 'NaN'])\n\n\n@parametrize_inf_nan_capable_value\n@parametrize_inf_nan_capable_type\ndef test_config_inf_nan_enabled(inf_nan_capable_type, inf_nan_value):\n    class Model(BaseModel):\n        model_config = ConfigDict(allow_inf_nan=True)\n        value: inf_nan_capable_type\n\n    assert Model(value=inf_nan_capable_type(inf_nan_value))\n\n\n@parametrize_inf_nan_capable_value\n@parametrize_inf_nan_capable_type\ndef test_config_inf_nan_disabled(inf_nan_capable_type, inf_nan_value):\n    class Model(BaseModel):\n        model_config = ConfigDict(allow_inf_nan=False)\n        value: inf_nan_capable_type\n\n    with pytest.raises(ValidationError) as e:\n        Model(value=inf_nan_capable_type(inf_nan_value))\n\n    assert e.value.errors(include_url=False)[0] == IsPartialDict(\n        {\n            'loc': ('value',),\n            'msg': 'Input should be a finite number',\n            'type': 'finite_number',\n        }\n    )\n\n\n@pytest.mark.parametrize(\n    'config,expected',\n    (\n        (ConfigDict(), 'ConfigWrapper()'),\n        (ConfigDict(title='test'), \"ConfigWrapper(title='test')\"),\n    ),\n)\ndef test_config_wrapper_repr(config, expected):\n    assert repr(ConfigWrapper(config=config)) == expected\n\n\ndef test_config_wrapper_get_item():\n    config_wrapper = ConfigWrapper(config=ConfigDict(title='test'))\n\n    assert config_wrapper.title == 'test'\n    with pytest.raises(AttributeError, match=\"Config has no attribute 'test'\"):\n        config_wrapper.test\n\n\ndef test_config_inheritance_with_annotations():\n    class Parent(BaseModel):\n        model_config: ConfigDict = {'extra': 'allow'}\n\n    class Child(Parent):\n        model_config: ConfigDict = {'str_to_lower': True}\n\n    assert Child.model_config == {'extra': 'allow', 'str_to_lower': True}\n\n\ndef test_json_encoders_model() -> None:\n    with pytest.warns(PydanticDeprecationWarning):\n\n        class Model(BaseModel):\n            model_config = ConfigDict(json_encoders={Decimal: lambda x: str(x * 2), int: lambda x: str(x * 3)})\n            value: Decimal\n            x: int\n\n    assert json.loads(Model(value=Decimal('1.1'), x=1).model_dump_json()) == {'value': '2.2', 'x': '3'}\n\n\n@pytest.mark.filterwarnings('ignore::pydantic.warnings.PydanticDeprecationWarning')\ndef test_json_encoders_type_adapter() -> None:\n    config = ConfigDict(json_encoders={Decimal: lambda x: str(x * 2), int: lambda x: str(x * 3)})\n\n    ta = TypeAdapter(int, config=config)\n    assert json.loads(ta.dump_json(1)) == '3'\n\n    ta = TypeAdapter(Decimal, config=config)\n    assert json.loads(ta.dump_json(Decimal('1.1'))) == '2.2'\n\n    ta = TypeAdapter(Union[Decimal, int], config=config)\n    assert json.loads(ta.dump_json(Decimal('1.1'))) == '2.2'\n    assert json.loads(ta.dump_json(1)) == '2'\n\n\n@pytest.mark.parametrize('defer_build_mode', [None, tuple(), ('model',), ('type_adapter',), ('model', 'type_adapter')])\ndef test_config_model_defer_build(\n    defer_build_mode: Optional[Tuple[Literal['model', 'type_adapter'], ...]], generate_schema_calls: CallCounter\n):\n    config = ConfigDict(defer_build=True)\n    if defer_build_mode is not None:\n        config['experimental_defer_build_mode'] = defer_build_mode\n\n    class MyModel(BaseModel):\n        model_config = config\n        x: int\n\n    if defer_build_mode is None or 'model' in defer_build_mode:\n        assert isinstance(MyModel.__pydantic_validator__, MockValSer)\n        assert isinstance(MyModel.__pydantic_serializer__, MockValSer)\n        assert generate_schema_calls.count == 0, 'Should respect experimental_defer_build_mode'\n    else:\n        assert isinstance(MyModel.__pydantic_validator__, SchemaValidator)\n        assert isinstance(MyModel.__pydantic_serializer__, SchemaSerializer)\n        assert generate_schema_calls.count == 1, 'Should respect experimental_defer_build_mode'\n\n    m = MyModel(x=1)\n    assert m.x == 1\n    assert m.model_dump()['x'] == 1\n    assert m.model_validate({'x': 2}).x == 2\n    assert m.model_json_schema()['type'] == 'object'\n\n    assert isinstance(MyModel.__pydantic_validator__, SchemaValidator)\n    assert isinstance(MyModel.__pydantic_serializer__, SchemaSerializer)\n    assert generate_schema_calls.count == 1, 'Should not build duplicated core schemas'\n\n\n@pytest.mark.parametrize('defer_build_mode', [None, tuple(), ('model',), ('type_adapter',), ('model', 'type_adapter')])\ndef test_config_model_type_adapter_defer_build(\n    defer_build_mode: Optional[Tuple[Literal['model', 'type_adapter'], ...]], generate_schema_calls: CallCounter\n):\n    config = ConfigDict(defer_build=True)\n    if defer_build_mode is not None:\n        config['experimental_defer_build_mode'] = defer_build_mode\n\n    class MyModel(BaseModel):\n        model_config = config\n        x: int\n\n    is_deferred = defer_build_mode is None or 'model' in defer_build_mode\n    assert generate_schema_calls.count == (0 if is_deferred else 1)\n    generate_schema_calls.reset()\n\n    ta = TypeAdapter(MyModel)\n\n    assert generate_schema_calls.count == 0, 'Should use model generated schema'\n\n    assert ta.validate_python({'x': 1}).x == 1\n    assert ta.validate_python({'x': 2}).x == 2\n    assert ta.dump_python(MyModel.model_construct(x=1))['x'] == 1\n    assert ta.json_schema()['type'] == 'object'\n\n    assert generate_schema_calls.count == (1 if is_deferred else 0), 'Should not build duplicate core schemas'\n\n\n@pytest.mark.parametrize('defer_build_mode', [None, tuple(), ('model',), ('type_adapter',), ('model', 'type_adapter')])\ndef test_config_plain_type_adapter_defer_build(\n    defer_build_mode: Optional[Tuple[Literal['model', 'type_adapter'], ...]], generate_schema_calls: CallCounter\n):\n    config = ConfigDict(defer_build=True)\n    if defer_build_mode is not None:\n        config['experimental_defer_build_mode'] = defer_build_mode\n    is_deferred = defer_build_mode is not None and 'type_adapter' in defer_build_mode\n\n    ta = TypeAdapter(Dict[str, int], config=config)\n\n    assert generate_schema_calls.count == (0 if is_deferred else 1)\n    generate_schema_calls.reset()\n\n    assert ta.validate_python({}) == {}\n    assert ta.validate_python({'x': 1}) == {'x': 1}\n    assert ta.dump_python({'x': 2}) == {'x': 2}\n    assert ta.json_schema()['type'] == 'object'\n\n    assert generate_schema_calls.count == (1 if is_deferred else 0), 'Should not build duplicate core schemas'\n\n\n@pytest.mark.parametrize('defer_build_mode', [None, ('model',), ('type_adapter',), ('model', 'type_adapter')])\ndef test_config_model_defer_build_nested(\n    defer_build_mode: Optional[Tuple[Literal['model', 'type_adapter'], ...]], generate_schema_calls: CallCounter\n):\n    config = ConfigDict(defer_build=True)\n    if defer_build_mode:\n        config['experimental_defer_build_mode'] = defer_build_mode\n\n    assert generate_schema_calls.count == 0\n\n    class MyNestedModel(BaseModel):\n        model_config = config\n        x: int\n\n    class MyModel(BaseModel):\n        y: MyNestedModel\n\n    assert isinstance(MyModel.__pydantic_validator__, SchemaValidator)\n    assert isinstance(MyModel.__pydantic_serializer__, SchemaSerializer)\n\n    expected_schema_count = 1 if defer_build_mode is None or 'model' in defer_build_mode else 2\n    assert generate_schema_calls.count == expected_schema_count, 'Should respect experimental_defer_build_mode'\n\n    if defer_build_mode is None or 'model' in defer_build_mode:\n        assert isinstance(MyNestedModel.__pydantic_validator__, MockValSer)\n        assert isinstance(MyNestedModel.__pydantic_serializer__, MockValSer)\n    else:\n        assert isinstance(MyNestedModel.__pydantic_validator__, SchemaValidator)\n        assert isinstance(MyNestedModel.__pydantic_serializer__, SchemaSerializer)\n\n    m = MyModel(y={'x': 1})\n    assert m.y.x == 1\n    assert m.model_dump() == {'y': {'x': 1}}\n    assert m.model_validate({'y': {'x': 1}}).y.x == 1\n    assert m.model_json_schema()['type'] == 'object'\n\n    if defer_build_mode is None or 'model' in defer_build_mode:\n        assert isinstance(MyNestedModel.__pydantic_validator__, MockValSer)\n        assert isinstance(MyNestedModel.__pydantic_serializer__, MockValSer)\n    else:\n        assert isinstance(MyNestedModel.__pydantic_validator__, SchemaValidator)\n        assert isinstance(MyNestedModel.__pydantic_serializer__, SchemaSerializer)\n\n    assert generate_schema_calls.count == expected_schema_count, 'Should not build duplicated core schemas'\n\n\ndef test_config_model_defer_build_ser_first():\n    class M1(BaseModel, defer_build=True):\n        a: str\n\n    class M2(BaseModel, defer_build=True):\n        b: M1\n\n    m = M2.model_validate({'b': {'a': 'foo'}})\n    assert m.b.model_dump() == {'a': 'foo'}\n\n\ndef test_defer_build_json_schema():\n    class M(BaseModel, defer_build=True):\n        a: int\n\n    assert M.model_json_schema() == {\n        'title': 'M',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'integer'}},\n        'required': ['a'],\n    }\n\n\ndef test_partial_creation_with_defer_build():\n    class M(BaseModel):\n        a: int\n        b: int\n\n    def create_partial(model, optionals):\n        override_fields = {}\n        model.model_rebuild()\n        for name, field in model.model_fields.items():\n            if field.is_required() and name in optionals:\n                assert field.annotation is not None\n                override_fields[name] = (Optional[field.annotation], FieldInfo.merge_field_infos(field, default=None))\n\n        return create_model(f'Partial{model.__name__}', __base__=model, **override_fields)\n\n    partial = create_partial(M, {'a'})\n\n    # Comment this away and the last assertion works\n    assert M.model_json_schema()['required'] == ['a', 'b']\n\n    # AssertionError: assert ['a', 'b'] == ['b']\n    assert partial.model_json_schema()['required'] == ['b']\n\n\ndef test_model_config_as_model_field_raises():\n    with pytest.raises(PydanticUserError) as exc_info:\n\n        class MyModel(BaseModel):\n            model_config: str\n\n    assert exc_info.value.code == 'model-config-invalid-field-name'\n\n\ndef test_dataclass_allowes_model_config_as_model_field():\n    config_title = 'from_config'\n    field_title = 'from_field'\n\n    @pydantic_dataclass(config={'title': config_title})\n    class MyDataclass:\n        model_config: dict\n\n    m = MyDataclass(model_config={'title': field_title})\n\n    assert m.model_config['title'] == field_title\n    assert getattr(m, '__pydantic_config__')['title'] == config_title\n\n\ndef test_with_config_disallowed_with_model():\n    msg = 'Cannot use `with_config` on Model as it is a Pydantic model'\n\n    with pytest.raises(PydanticUserError, match=msg):\n\n        @with_config({'coerce_numbers_to_str': True})\n        class Model(BaseModel):\n            pass\n", "tests/test_fastapi_json_schema.py": "\"\"\"\nThis file contains an initial proposal that can be scrapped and reworked if/when appropriate.\nEither way, this test file should probably be removed once the actual FastAPI implementation\nis complete and has integration tests with pydantic v2. However, we are including it here for now\nto get an early warning if this approach would require modification for compatibility with\nany future changes to the JSON schema generation logic, etc.\n\nSee the original PR for more details: https://github.com/pydantic/pydantic/pull/5094\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom dirty_equals import HasRepr, IsInstance, IsStr\n\nfrom pydantic import BaseModel, ConfigDict\nfrom pydantic._internal._core_metadata import CoreMetadataHandler\nfrom pydantic._internal._core_utils import CoreSchemaOrField\nfrom pydantic.errors import PydanticInvalidForJsonSchema\nfrom pydantic.json_schema import GenerateJsonSchema, JsonSchemaValue\n\n\nclass _ErrorKey(str):\n    pass\n\n\nclass FastAPIGenerateJsonSchema(GenerateJsonSchema):\n    \"\"\"\n    Idea: This class would be exported from FastAPI, and if users want to modify the way JSON schema is generated\n    in FastAPI, they should inherit from it and override it as appropriate.\n\n    In the JSON schema generation logic, FastAPI _could_ also attempt to work with classes that inherit directly from\n    GenerateJsonSchema by doing something like:\n\n        if UserGenerateJsonSchema.handle_invalid_for_json_schema is GenerateJsonSchema.handle_invalid_for_json_schema:\n            # The method has not been overridden; inherit from FastAPIGenerateJsonSchema\n            UserGenerateJsonSchema = type(\n                \"UserGenerateJsonSchema\", (FastAPIGenerateJsonSchema, UserGenerateJsonSchema), {}\n            )\n        else:\n            raise TypeError(f\"{UserGenerateJsonSchema.__name__} should inherit from FastAPIGenerateJsonSchema\")\n\n    I'm not sure which approach is better.\n    \"\"\"\n\n    def handle_invalid_for_json_schema(self, schema: CoreSchemaOrField, error_info: str) -> JsonSchemaValue:\n        # NOTE: I think it may be a good idea to rework this method to either not use CoreMetadataHandler,\n        #    and/or to make CoreMetadataHandler a public API.\n        if CoreMetadataHandler(schema).metadata.get('pydantic_js_modify_function') is not None:\n            # Since there is a json schema modify function, assume that this type is meant to be handled,\n            # and the modify function will set all properties as appropriate\n            return {}\n        else:\n            error = PydanticInvalidForJsonSchema(f'Cannot generate a JsonSchema for {error_info}')\n            return {_ErrorKey('error'): error}\n\n\n@dataclass\nclass ErrorDetails:\n    path: list[Any]\n    error: PydanticInvalidForJsonSchema\n\n\ndef collect_errors(schema: JsonSchemaValue) -> list[ErrorDetails]:\n    errors: list[ErrorDetails] = []\n\n    def _collect_errors(schema: JsonSchemaValue, path: list[Any]) -> None:\n        if isinstance(schema, dict):\n            for k, v in schema.items():\n                if isinstance(k, _ErrorKey):\n                    errors.append(ErrorDetails(path, schema[k]))\n                _collect_errors(v, list(path) + [k])\n        elif isinstance(schema, list):\n            for i, v in enumerate(schema):\n                _collect_errors(v, list(path) + [i])\n\n    _collect_errors(schema, [])\n    return errors\n\n\ndef test_inheritance_detection() -> None:\n    class GenerateJsonSchema2(GenerateJsonSchema):\n        pass\n\n    assert GenerateJsonSchema2.handle_invalid_for_json_schema is GenerateJsonSchema.handle_invalid_for_json_schema\n    # this is just a quick proof of the note above indicating that you can detect whether a specific method\n    # is overridden, for the purpose of allowing direct inheritance from GenerateJsonSchema.\n    assert (\n        FastAPIGenerateJsonSchema.handle_invalid_for_json_schema\n        is not GenerateJsonSchema.handle_invalid_for_json_schema\n    )\n\n\ndef test_collect_errors() -> None:\n    class Car:\n        def __init__(self, make: str, model: str, year: int):\n            self.make = make\n            self.model = model\n            self.year = year\n\n    class Model(BaseModel):\n        f1: int = 1\n        f2: Car\n\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    schema = Model.model_json_schema(schema_generator=FastAPIGenerateJsonSchema)\n    assert schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'f1': {'type': 'integer', 'default': 1, 'title': 'F1'},\n            'f2': {\n                'error': HasRepr(IsStr(regex=r'PydanticInvalidForJsonSchema\\(.*\\)')),\n                'title': 'F2',\n            },\n        },\n        'required': ['f2'],\n    }\n\n    collected_errors = collect_errors(schema)\n    assert collected_errors == [\n        ErrorDetails(\n            path=['properties', 'f2'],\n            error=IsInstance(PydanticInvalidForJsonSchema),\n        )\n    ]\n", "tests/test_pipeline.py": "\"\"\"Tests for the experimental transform module.\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nimport sys\nimport warnings\nfrom decimal import Decimal\nfrom typing import Any, Callable, Dict, FrozenSet, List, Set, Tuple, Union\n\nimport pytest\nimport pytz\nfrom annotated_types import Interval\nfrom typing_extensions import Annotated\n\nif sys.version_info >= (3, 9):\n    pass\n\nfrom pydantic import PydanticExperimentalWarning, TypeAdapter, ValidationError\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore', category=PydanticExperimentalWarning)\n    from pydantic.experimental.pipeline import _Pipeline, transform, validate_as  # type: ignore\n\n\n@pytest.mark.parametrize('potato_variation', ['potato', ' potato ', ' potato', 'potato ', ' POTATO ', ' PoTatO '])\ndef test_parse_str(potato_variation: str) -> None:\n    ta_lower = TypeAdapter[str](Annotated[str, validate_as(...).str_strip().str_lower()])\n    assert ta_lower.validate_python(potato_variation) == 'potato'\n\n\ndef test_parse_str_with_pattern() -> None:\n    ta_pattern = TypeAdapter[str](Annotated[str, validate_as(...).str_pattern(r'[a-z]+')])\n    assert ta_pattern.validate_python('potato') == 'potato'\n    with pytest.raises(ValueError):\n        ta_pattern.validate_python('POTATO')\n\n\n@pytest.mark.parametrize(\n    'type_, pipeline, valid_cases, invalid_cases',\n    [\n        (int, validate_as(...).ge(0), [0, 1, 100], [-1, -100]),\n        (float, validate_as(...).ge(0.0), [1.8, 0.0], [-1.0]),\n        (Decimal, validate_as(...).ge(Decimal(0.0)), [Decimal(1), Decimal(0.0)], [Decimal(-1.0)]),\n        (int, validate_as(...).le(5), [2, 4], [6, 100]),\n        (float, validate_as(...).le(1.0), [0.5, 0.0], [100.0]),\n        (Decimal, validate_as(...).le(Decimal(1.0)), [Decimal(1)], [Decimal(5.0)]),\n        (int, validate_as(...).gt(0), [1, 2, 100], [0, -1]),\n        (float, validate_as(...).gt(0.0), [0.1, 1.8], [0.0, -1.0]),\n        (Decimal, validate_as(...).gt(Decimal(0.0)), [Decimal(1)], [Decimal(0.0), Decimal(-1.0)]),\n        (int, validate_as(...).lt(5), [2, 4], [5, 6, 100]),\n        (float, validate_as(...).lt(1.0), [0.5, 0.0], [1.0, 100.0]),\n        (Decimal, validate_as(...).lt(Decimal(1.0)), [Decimal(0.5)], [Decimal(1.0), Decimal(5.0)]),\n    ],\n)\ndef test_ge_le_gt_lt(\n    type_: Any, pipeline: _Pipeline[Any, Any], valid_cases: list[Any], invalid_cases: list[Any]\n) -> None:\n    ta = TypeAdapter[Any](Annotated[type_, pipeline])\n    for x in valid_cases:\n        assert ta.validate_python(x) == x\n    for y in invalid_cases:\n        with pytest.raises(ValueError):\n            ta.validate_python(y)\n\n\n@pytest.mark.parametrize(\n    'type_, pipeline, valid_cases, invalid_cases',\n    [\n        (int, validate_as(int).multiple_of(5), [5, 20, 0], [18, 7]),\n        (float, validate_as(float).multiple_of(2.5), [2.5, 5.0, 7.5], [3.0, 1.1]),\n        (\n            Decimal,\n            validate_as(Decimal).multiple_of(Decimal('1.5')),\n            [Decimal('1.5'), Decimal('3.0'), Decimal('4.5')],\n            [Decimal('1.4'), Decimal('2.1')],\n        ),\n    ],\n)\ndef test_parse_multipleOf(type_: Any, pipeline: Any, valid_cases: list[Any], invalid_cases: list[Any]) -> None:\n    ta = TypeAdapter[Any](Annotated[type_, pipeline])\n    for x in valid_cases:\n        assert ta.validate_python(x) == x\n    for y in invalid_cases:\n        with pytest.raises(ValueError):\n            ta.validate_python(y)\n\n\n@pytest.mark.parametrize(\n    'type_, pipeline, valid_cases, invalid_cases',\n    [\n        (int, validate_as(int).constrain(Interval(ge=0, le=10)), [0, 5, 10], [11]),\n        (float, validate_as(float).constrain(Interval(gt=0.0, lt=10.0)), [0.1, 9.9], [10.0]),\n        (\n            Decimal,\n            validate_as(Decimal).constrain(Interval(ge=Decimal('1.0'), lt=Decimal('10.0'))),\n            [Decimal('1.0'), Decimal('5.5'), Decimal('9.9')],\n            [Decimal('0.0'), Decimal('10.0')],\n        ),\n        (int, validate_as(int).constrain(Interval(gt=1, lt=5)), [2, 4], [1, 5]),\n        (float, validate_as(float).constrain(Interval(ge=1.0, le=5.0)), [1.0, 3.0, 5.0], [0.9, 5.1]),\n    ],\n)\ndef test_interval_constraints(type_: Any, pipeline: Any, valid_cases: list[Any], invalid_cases: list[Any]) -> None:\n    ta = TypeAdapter[Any](Annotated[type_, pipeline])\n    for x in valid_cases:\n        assert ta.validate_python(x) == x\n    for y in invalid_cases:\n        with pytest.raises(ValueError):\n            ta.validate_python(y)\n\n\n@pytest.mark.parametrize(\n    'type_, pipeline, valid_cases, invalid_cases',\n    [\n        (\n            str,\n            validate_as(str).len(min_len=2, max_len=5),\n            ['ab', 'abc', 'abcd', 'abcde'],\n            ['a', 'abcdef'],\n        ),\n        (\n            List[int],\n            validate_as(List[int]).len(min_len=1, max_len=3),\n            [[1], [1, 2], [1, 2, 3]],\n            [[], [1, 2, 3, 4]],\n        ),\n        (Tuple[int, ...], validate_as(Tuple[int, ...]).len(min_len=1, max_len=2), [(1,), (1, 2)], [(), (1, 2, 3)]),\n        (\n            Set[int],\n            validate_as(Set[int]).len(min_len=2, max_len=4),\n            [{1, 2}, {1, 2, 3}, {1, 2, 3, 4}],\n            [{1}, {1, 2, 3, 4, 5}],\n        ),\n        (\n            FrozenSet[int],\n            validate_as(FrozenSet[int]).len(min_len=2, max_len=3),\n            [frozenset({1, 2}), frozenset({1, 2, 3})],\n            [frozenset({1}), frozenset({1, 2, 3, 4})],\n        ),\n        (\n            Dict[str, int],\n            validate_as(Dict[str, int]).len(min_len=1, max_len=2),\n            [{'a': 1}, {'a': 1, 'b': 2}],\n            [{}, {'a': 1, 'b': 2, 'c': 3}],\n        ),\n        (\n            str,\n            validate_as(str).len(min_len=2),  # max_len is None\n            ['ab', 'abc', 'abcd', 'abcde', 'abcdef'],\n            ['a'],\n        ),\n    ],\n)\ndef test_len_constraints(type_: Any, pipeline: Any, valid_cases: list[Any], invalid_cases: list[Any]) -> None:\n    ta = TypeAdapter[Any](Annotated[type_, pipeline])\n    for x in valid_cases:\n        assert ta.validate_python(x) == x\n    for y in invalid_cases:\n        with pytest.raises(ValueError):\n            ta.validate_python(y)\n\n\ndef test_parse_tz() -> None:\n    ta_tz = TypeAdapter[datetime.datetime](\n        Annotated[\n            datetime.datetime,\n            validate_as(datetime.datetime).datetime_tz_naive(),\n        ]\n    )\n    date = datetime.datetime(2032, 6, 4, 11, 15, 30, 400000)\n    assert ta_tz.validate_python(date) == date\n    date_a = datetime.datetime(2032, 6, 4, 11, 15, 30, 400000, tzinfo=pytz.UTC)\n    with pytest.raises(ValueError):\n        ta_tz.validate_python(date_a)\n\n    ta_tza = TypeAdapter[datetime.datetime](\n        Annotated[\n            datetime.datetime,\n            validate_as(datetime.datetime).datetime_tz_aware(),\n        ]\n    )\n    date_a = datetime.datetime(2032, 6, 4, 11, 15, 30, 400000, pytz.UTC)\n    assert ta_tza.validate_python(date_a) == date_a\n    with pytest.raises(ValueError):\n        ta_tza.validate_python(date)\n\n\n@pytest.mark.parametrize(\n    'method, method_arg, input_string, expected_output',\n    [\n        # transforms\n        ('lower', None, 'POTATO', 'potato'),\n        ('upper', None, 'potato', 'POTATO'),\n        ('title', None, 'potato potato', 'Potato Potato'),\n        ('strip', None, ' potato ', 'potato'),\n        # constraints\n        ('pattern', r'[a-z]+', 'potato', 'potato'),  # check lowercase\n        # predicates\n        ('contains', 'pot', 'potato', 'potato'),\n        ('starts_with', 'pot', 'potato', 'potato'),\n        ('ends_with', 'ato', 'potato', 'potato'),\n    ],\n)\ndef test_string_validator_valid(method: str, method_arg: str | None, input_string: str, expected_output: str):\n    # annotated metadata is equivalent to validate_as(str).str_method(method_arg)\n    # ex: validate_as(str).str_contains('pot')\n    annotated_metadata = getattr(validate_as(str), 'str_' + method)\n    annotated_metadata = annotated_metadata(method_arg) if method_arg else annotated_metadata()\n\n    ta = TypeAdapter[str](Annotated[str, annotated_metadata])\n    assert ta.validate_python(input_string) == expected_output\n\n\ndef test_string_validator_invalid() -> None:\n    ta_contains = TypeAdapter[str](Annotated[str, validate_as(str).str_contains('potato')])\n    with pytest.raises(ValidationError):\n        ta_contains.validate_python('tomato')\n\n    ta_starts_with = TypeAdapter[str](Annotated[str, validate_as(str).str_starts_with('potato')])\n    with pytest.raises(ValidationError):\n        ta_starts_with.validate_python('tomato')\n\n    ta_ends_with = TypeAdapter[str](Annotated[str, validate_as(str).str_ends_with('potato')])\n    with pytest.raises(ValidationError):\n        ta_ends_with.validate_python('tomato')\n\n\ndef test_parse_int() -> None:\n    ta_gt = TypeAdapter[int](Annotated[int, validate_as(int).gt(0)])\n    assert ta_gt.validate_python(1) == 1\n    assert ta_gt.validate_python('1') == 1\n    with pytest.raises(ValidationError):\n        ta_gt.validate_python(0)\n\n    ta_gt_strict = TypeAdapter[int](Annotated[int, validate_as(int, strict=True).gt(0)])\n    assert ta_gt_strict.validate_python(1) == 1\n    with pytest.raises(ValidationError):\n        ta_gt_strict.validate_python('1')\n    with pytest.raises(ValidationError):\n        ta_gt_strict.validate_python(0)\n\n\ndef test_parse_str_to_int() -> None:\n    ta = TypeAdapter[int](Annotated[int, validate_as(str).str_strip().validate_as(int)])\n    assert ta.validate_python('1') == 1\n    assert ta.validate_python(' 1 ') == 1\n    with pytest.raises(ValidationError):\n        ta.validate_python('a')\n\n\ndef test_predicates() -> None:\n    ta_int = TypeAdapter[int](Annotated[int, validate_as(int).predicate(lambda x: x % 2 == 0)])\n    assert ta_int.validate_python(2) == 2\n    with pytest.raises(ValidationError):\n        ta_int.validate_python(1)\n\n    ta_str = TypeAdapter[int](Annotated[str, validate_as(str).predicate(lambda x: x != 'potato')])\n    assert ta_str.validate_python('tomato') == 'tomato'\n    with pytest.raises(ValidationError):\n        ta_str.validate_python('potato')\n\n\n@pytest.mark.parametrize(\n    'model, expected_val_schema, expected_ser_schema',\n    [\n        (\n            Annotated[Union[int, str], validate_as(...) | validate_as(str)],\n            {'anyOf': [{'type': 'integer'}, {'type': 'string'}]},\n            {'anyOf': [{'type': 'integer'}, {'type': 'string'}]},\n        ),\n        (\n            Annotated[int, validate_as(...) | validate_as(str).validate_as(int)],\n            {'anyOf': [{'type': 'integer'}, {'type': 'string'}]},\n            {'type': 'integer'},\n        ),\n        (\n            Annotated[int, validate_as(...) | validate_as(str).validate_as(int)],\n            {'anyOf': [{'type': 'integer'}, {'type': 'string'}]},\n            {'type': 'integer'},\n        ),\n        (\n            Annotated[int, validate_as(...) | validate_as(str).transform(int).validate_as(int)],\n            {'anyOf': [{'type': 'integer'}, {'type': 'string'}]},\n            {'type': 'integer'},\n        ),\n        (\n            Annotated[int, validate_as(int).gt(0).lt(100)],\n            {'type': 'integer', 'exclusiveMinimum': 0, 'exclusiveMaximum': 100},\n            {'type': 'integer', 'exclusiveMinimum': 0, 'exclusiveMaximum': 100},\n        ),\n        (\n            Annotated[int, validate_as(int).gt(0) | validate_as(int).lt(100)],\n            {'anyOf': [{'type': 'integer', 'exclusiveMinimum': 0}, {'type': 'integer', 'exclusiveMaximum': 100}]},\n            {'anyOf': [{'type': 'integer', 'exclusiveMinimum': 0}, {'type': 'integer', 'exclusiveMaximum': 100}]},\n        ),\n        (\n            Annotated[List[int], validate_as(...).len(0, 100)],\n            {'type': 'array', 'items': {'type': 'integer'}, 'maxItems': 100},\n            {'type': 'array', 'items': {'type': 'integer'}, 'maxItems': 100},\n        ),\n        # note - we added this to confirm the fact that the transform doesn't impact the JSON schema,\n        # as it's applied as a function after validator\n        (\n            Annotated[int, validate_as(str).transform(int)],\n            {'type': 'string'},\n            {'type': 'string'},  # see this is still string\n        ),\n        # in juxtaposition to the case above, when we use validate_as (recommended),\n        # the JSON schema is updated appropriately\n        (\n            Annotated[int, validate_as(str).validate_as(int)],\n            {'type': 'string'},\n            {'type': 'integer'},  # aha, this is now an integer\n        ),\n    ],\n)\ndef test_json_schema(\n    model: type[Any], expected_val_schema: dict[str, Any], expected_ser_schema: dict[str, Any]\n) -> None:\n    ta = TypeAdapter(model)\n\n    schema = ta.json_schema(mode='validation')\n    assert schema == expected_val_schema\n\n    schema = ta.json_schema(mode='serialization')\n    assert schema == expected_ser_schema\n\n\ndef test_transform_first_step() -> None:\n    \"\"\"Check that when transform() is used as the first step in a pipeline it run after parsing.\"\"\"\n    ta = TypeAdapter[int](Annotated[int, transform(lambda x: x + 1)])\n    assert ta.validate_python('1') == 2\n\n\ndef test_not_eq() -> None:\n    ta = TypeAdapter[int](Annotated[str, validate_as(str).not_eq('potato')])\n    assert ta.validate_python('tomato') == 'tomato'\n    with pytest.raises(ValidationError):\n        ta.validate_python('potato')\n\n\ndef test_eq() -> None:\n    ta = TypeAdapter[int](Annotated[str, validate_as(str).eq('potato')])\n    assert ta.validate_python('potato') == 'potato'\n    with pytest.raises(ValidationError):\n        ta.validate_python('tomato')\n\n\ndef test_not_in() -> None:\n    ta = TypeAdapter[int](Annotated[str, validate_as(str).not_in(['potato', 'tomato'])])\n    assert ta.validate_python('carrot') == 'carrot'\n    with pytest.raises(ValidationError):\n        ta.validate_python('potato')\n\n\ndef test_in() -> None:\n    ta = TypeAdapter[int](Annotated[str, validate_as(str).in_(['potato', 'tomato'])])\n    assert ta.validate_python('potato') == 'potato'\n    with pytest.raises(ValidationError):\n        ta.validate_python('carrot')\n\n\ndef test_composition() -> None:\n    ta = TypeAdapter[int](Annotated[int, validate_as(int).gt(10) | validate_as(int).lt(5)])\n    assert ta.validate_python(1) == 1\n    assert ta.validate_python(20) == 20\n    with pytest.raises(ValidationError):\n        ta.validate_python(9)\n\n    ta = TypeAdapter[int](Annotated[int, validate_as(int).gt(10) & validate_as(int).le(20)])\n    assert ta.validate_python(15) == 15\n    with pytest.raises(ValidationError):\n        ta.validate_python(9)\n    with pytest.raises(ValidationError):\n        ta.validate_python(21)\n\n    # test that sticking a transform in the middle doesn't break the composition\n    calls: list[tuple[str, int]] = []\n\n    def tf(step: str) -> Callable[[int], int]:\n        def inner(x: int) -> int:\n            calls.append((step, x))\n            return x\n\n        return inner\n\n    ta = TypeAdapter[int](\n        Annotated[\n            int,\n            validate_as(int).transform(tf('1')).gt(10).transform(tf('2'))\n            | validate_as(int).transform(tf('3')).lt(5).transform(tf('4')),\n        ]\n    )\n    assert ta.validate_python(1) == 1\n    assert calls == [('1', 1), ('3', 1), ('4', 1)]\n    calls.clear()\n    assert ta.validate_python(20) == 20\n    assert calls == [('1', 20), ('2', 20)]\n    calls.clear()\n    with pytest.raises(ValidationError):\n        ta.validate_python(9)\n    assert calls == [('1', 9), ('3', 9)]\n    calls.clear()\n\n    ta = TypeAdapter[int](\n        Annotated[\n            int,\n            validate_as(int).transform(tf('1')).gt(10).transform(tf('2'))\n            & validate_as(int).transform(tf('3')).le(20).transform(tf('4')),\n        ]\n    )\n    assert ta.validate_python(15) == 15\n    assert calls == [('1', 15), ('2', 15), ('3', 15), ('4', 15)]\n    calls.clear()\n    with pytest.raises(ValidationError):\n        ta.validate_python(9)\n    assert calls == [('1', 9)]\n    calls.clear()\n    with pytest.raises(ValidationError):\n        ta.validate_python(21)\n    assert calls == [('1', 21), ('2', 21), ('3', 21)]\n    calls.clear()\n", "tests/test_v1.py": "from pydantic import VERSION\nfrom pydantic.v1 import VERSION as V1_VERSION\nfrom pydantic.v1 import BaseModel as V1BaseModel\nfrom pydantic.v1 import root_validator as v1_root_validator\n\n\ndef test_version():\n    assert V1_VERSION.startswith('1.')\n    assert V1_VERSION != VERSION\n\n\ndef test_root_validator():\n    class Model(V1BaseModel):\n        v: str\n\n        @v1_root_validator(pre=True)\n        @classmethod\n        def root_validator(cls, values):\n            values['v'] += '-v1'\n            return values\n\n    model = Model(v='value')\n    assert model.v == 'value-v1'\n", "tests/test_exports.py": "import importlib\nimport importlib.util\nimport json\nimport platform\nimport sys\nfrom pathlib import Path\nfrom types import ModuleType\n\nimport pytest\n\nimport pydantic\n\n\n@pytest.mark.filterwarnings('ignore::DeprecationWarning')\ndef test_init_export():\n    for name in dir(pydantic):\n        getattr(pydantic, name)\n\n\n@pytest.mark.filterwarnings('ignore::DeprecationWarning')\n@pytest.mark.parametrize(('attr_name', 'value'), list(pydantic._dynamic_imports.items()))\ndef test_public_api_dynamic_imports(attr_name, value):\n    package, module_name = value\n    if module_name == '__module__':\n        module = importlib.import_module(attr_name, package=package)\n        assert isinstance(module, ModuleType)\n    else:\n        imported_object = getattr(importlib.import_module(module_name, package=package), attr_name)\n        assert isinstance(imported_object, object)\n\n\n@pytest.mark.skipif(\n    platform.python_implementation() == 'PyPy' and platform.python_version_tuple() < ('3', '8'),\n    reason='Produces a weird error on pypy<3.8',\n)\n@pytest.mark.filterwarnings('ignore::DeprecationWarning')\n@pytest.mark.filterwarnings('ignore::pydantic.warnings.PydanticExperimentalWarning')\ndef test_public_internal():\n    \"\"\"\n    check we don't make anything from _internal public\n    \"\"\"\n    public_internal_attributes = []\n\n    def _test_file(file: Path, module_name: str):\n        if file.name != '__init__.py' and not file.name.startswith('_'):\n            module = sys.modules.get(module_name)\n            if module is None:\n                spec = importlib.util.spec_from_file_location(module_name, str(file))\n                module = importlib.util.module_from_spec(spec)\n                sys.modules[module_name] = module\n                try:\n                    spec.loader.exec_module(module)\n                except ImportError:\n                    return\n\n            for name, attr in vars(module).items():\n                if not name.startswith('_'):\n                    attr_module = getattr(attr, '__module__', '')\n                    if attr_module.startswith('pydantic._internal'):\n                        public_internal_attributes.append(f'{module.__name__}:{name} from {attr_module}')\n\n    pydantic_files = (Path(__file__).parent.parent / 'pydantic').glob('*.py')\n    experimental_files = (Path(__file__).parent.parent / 'pydantic' / 'experimental').glob('*.py')\n\n    for file in pydantic_files:\n        _test_file(file, f'pydantic.{file.stem}')\n    for file in experimental_files:\n        _test_file(file, f'pydantic.experimental.{file.stem}')\n\n    if public_internal_attributes:\n        pytest.fail('The following should not be publicly accessible:\\n  ' + '\\n  '.join(public_internal_attributes))\n\n\n# language=Python\nIMPORTED_PYDANTIC_CODE = \"\"\"\nimport sys\nimport pydantic\n\nmodules = list(sys.modules.keys())\n\nimport json\nprint(json.dumps(modules))\n\"\"\"\n\n\ndef test_import_pydantic(subprocess_run_code):\n    output = subprocess_run_code(IMPORTED_PYDANTIC_CODE)\n    imported_modules = json.loads(output)\n    # debug(imported_modules)\n    assert 'pydantic' in imported_modules\n    assert 'pydantic.deprecated' not in imported_modules\n\n\n# language=Python\nIMPORTED_BASEMODEL_CODE = \"\"\"\nimport sys\nfrom pydantic import BaseModel\n\nmodules = list(sys.modules.keys())\n\nimport json\nprint(json.dumps(modules))\n\"\"\"\n\n\ndef test_import_base_model(subprocess_run_code):\n    output = subprocess_run_code(IMPORTED_BASEMODEL_CODE)\n    imported_modules = json.loads(output)\n    # debug(sorted(imported_modules))\n    assert 'pydantic' in imported_modules\n    assert 'pydantic.fields' not in imported_modules\n    assert 'pydantic.types' not in imported_modules\n    assert 'annotated_types' not in imported_modules\n\n\ndef test_dataclass_import(subprocess_run_code):\n    @subprocess_run_code\n    def run_in_subprocess():\n        import pydantic\n\n        assert pydantic.dataclasses.__name__ == 'pydantic.dataclasses'\n\n        @pydantic.dataclasses.dataclass\n        class Foo:\n            a: int\n\n        try:\n            Foo('not an int')\n        except ValueError:\n            pass\n        else:\n            raise AssertionError('Should have raised a ValueError')\n\n\ndef test_dataclass_import2(subprocess_run_code):\n    @subprocess_run_code\n    def run_in_subprocess():\n        import pydantic.dataclasses\n\n        assert pydantic.dataclasses.__name__ == 'pydantic.dataclasses'\n\n        @pydantic.dataclasses.dataclass\n        class Foo:\n            a: int\n\n        try:\n            Foo('not an int')\n        except ValueError:\n            pass\n        else:\n            raise AssertionError('Should have raised a ValueError')\n", "tests/test_model_validator.py": "from __future__ import annotations\n\nfrom typing import Any, Dict, cast\n\nimport pytest\n\nfrom pydantic import BaseModel, ValidationInfo, ValidatorFunctionWrapHandler, model_validator\n\n\ndef test_model_validator_wrap() -> None:\n    class Model(BaseModel):\n        x: int\n        y: int\n\n        @model_validator(mode='wrap')\n        @classmethod\n        def val_model(cls, values: dict[str, Any] | Model, handler: ValidatorFunctionWrapHandler) -> Model:\n            if isinstance(values, dict):\n                assert values == {'x': 1, 'y': 2}\n                model = handler({'x': 2, 'y': 3})\n            else:\n                assert values.x == 1\n                assert values.y == 2\n                model = handler(Model.model_construct(x=2, y=3))\n            assert model.x == 2\n            assert model.y == 3\n            model.x = 20\n            model.y = 30\n            return model\n\n    assert Model(x=1, y=2).model_dump() == {'x': 20, 'y': 30}\n    assert Model.model_validate(Model.model_construct(x=1, y=2)).model_dump() == {'x': 20, 'y': 30}\n\n\n@pytest.mark.parametrize('classmethod_decorator', [classmethod, lambda x: x])\ndef test_model_validator_before(classmethod_decorator: Any) -> None:\n    class Model(BaseModel):\n        x: int\n        y: int\n\n        @model_validator(mode='before')\n        @classmethod_decorator\n        def val_model(cls, values: Any, info: ValidationInfo) -> dict[str, Any] | Model:\n            assert not info.context\n            if isinstance(values, dict):\n                values = cast(Dict[str, Any], values)\n                values['x'] += 1\n                values['y'] += 1\n            else:\n                assert isinstance(values, Model)\n                values.x += 1\n                values.y += 1\n            return values\n\n    m = Model(x=1, y=2)\n    assert m.model_dump() == {'x': 2, 'y': 3}\n    # model not changed because we don't revalidate m\n    assert Model.model_validate(m).model_dump() == {'x': 2, 'y': 3}\n\n\n@pytest.mark.parametrize('classmethod_decorator', [classmethod, lambda x: x])\ndef test_model_validator_before_revalidate_always(classmethod_decorator: Any) -> None:\n    class Model(BaseModel, revalidate_instances='always'):\n        x: int\n        y: int\n\n        @model_validator(mode='before')\n        @classmethod_decorator\n        def val_model(cls, values: Any, info: ValidationInfo) -> dict[str, Any] | Model:\n            assert not info.context\n            if isinstance(values, dict):\n                values = cast(Dict[str, Any], values)\n                values['x'] += 1\n                values['y'] += 1\n            else:\n                assert isinstance(values, Model)\n                values.x += 1\n                values.y += 1\n            return values\n\n    assert Model(x=1, y=2).model_dump() == {'x': 2, 'y': 3}\n    assert Model.model_validate(Model(x=1, y=2)).model_dump() == {'x': 3, 'y': 4}\n\n\ndef test_model_validator_after() -> None:\n    class Model(BaseModel):\n        x: int\n        y: int\n\n        @model_validator(mode='after')\n        def val_model(self, info: ValidationInfo) -> Model:\n            assert not info.context\n            self.x += 1\n            self.y += 1\n            return self\n\n    assert Model(x=1, y=2).model_dump() == {'x': 2, 'y': 3}\n    assert Model.model_validate(Model(x=1, y=2)).model_dump() == {'x': 3, 'y': 4}\n\n\ndef test_subclass() -> None:\n    class Human(BaseModel):\n        @model_validator(mode='before')\n        @classmethod\n        def run_model_validator(cls, values: dict[str, Any]) -> dict[str, Any]:\n            values['age'] *= 2\n            return values\n\n    class Person(Human):\n        age: int\n\n    assert Person(age=28).age == 56\n\n\ndef test_nested_models() -> None:\n    calls: list[str] = []\n\n    class Model(BaseModel):\n        inner: Model | None\n\n        @model_validator(mode='before')\n        @classmethod\n        def validate_model_before(cls, values: dict[str, Any]) -> dict[str, Any]:\n            calls.append('before')\n            return values\n\n        @model_validator(mode='after')\n        def validate_model_after(self) -> Model:\n            calls.append('after')\n            return self\n\n    Model.model_validate({'inner': None})\n    assert calls == ['before', 'after']\n    calls.clear()\n\n    Model.model_validate({'inner': {'inner': {'inner': None}}})\n    assert calls == ['before'] * 3 + ['after'] * 3\n    calls.clear()\n", "tests/test_abc.py": "import abc\nimport sys\n\nimport pytest\n\nfrom pydantic import BaseModel\n\n\ndef test_model_subclassing_abstract_base_classes():\n    class Model(BaseModel, abc.ABC):\n        some_field: str\n\n\n@pytest.mark.skipif(sys.version_info < (3, 12), reason='error value different on older versions')\ndef test_model_subclassing_abstract_base_classes_without_implementation_raises_exception():\n    class Model(BaseModel, abc.ABC):\n        some_field: str\n\n        @abc.abstractmethod\n        def my_abstract_method(self):\n            pass\n\n        @classmethod\n        @abc.abstractmethod\n        def my_abstract_classmethod(cls):\n            pass\n\n        @staticmethod\n        @abc.abstractmethod\n        def my_abstract_staticmethod():\n            pass\n\n        @property\n        @abc.abstractmethod\n        def my_abstract_property(self):\n            pass\n\n        @my_abstract_property.setter\n        @abc.abstractmethod\n        def my_abstract_property(self, val):\n            pass\n\n    with pytest.raises(TypeError) as excinfo:\n        Model(some_field='some_value')\n    assert str(excinfo.value) == (\n        \"Can't instantiate abstract class Model without an implementation for abstract methods \"\n        \"'my_abstract_classmethod', 'my_abstract_method', 'my_abstract_property', 'my_abstract_staticmethod'\"\n    )\n", "tests/test_pickle.py": "import dataclasses\nimport gc\nimport pickle\nfrom typing import Optional, Type\n\nimport cloudpickle\nimport pytest\n\nimport pydantic\nfrom pydantic import BaseModel, PositiveFloat, ValidationError\nfrom pydantic._internal._model_construction import _PydanticWeakRef\nfrom pydantic.config import ConfigDict\n\n\nclass IntWrapper:\n    def __init__(self, v: int):\n        self._v = v\n\n    def get(self) -> int:\n        return self._v\n\n    def __eq__(self, other: 'IntWrapper') -> bool:\n        return self.get() == other.get()\n\n\ndef test_pickle_pydantic_weakref():\n    obj1 = IntWrapper(1)\n    ref1 = _PydanticWeakRef(obj1)\n    assert ref1() is obj1\n\n    obj2 = IntWrapper(2)\n    ref2 = _PydanticWeakRef(obj2)\n    assert ref2() is obj2\n\n    ref3 = _PydanticWeakRef(IntWrapper(3))\n    gc.collect()  # PyPy does not use reference counting and always relies on GC.\n    assert ref3() is None\n\n    d = {\n        # Hold a hard reference to the underlying object for ref1 that will also\n        # be pickled.\n        'hard_ref': obj1,\n        # ref1's underlying object has a hard reference in the pickled object so it\n        # should maintain the reference after deserialization.\n        'has_hard_ref': ref1,\n        # ref2's underlying object has no hard reference in the pickled object so it\n        # should be `None` after deserialization.\n        'has_no_hard_ref': ref2,\n        # ref3's underlying object had already gone out of scope before pickling so it\n        # should be `None` after deserialization.\n        'ref_out_of_scope': ref3,\n    }\n\n    loaded = pickle.loads(pickle.dumps(d))\n    gc.collect()  # PyPy does not use reference counting and always relies on GC.\n\n    assert loaded['hard_ref'] == IntWrapper(1)\n    assert loaded['has_hard_ref']() is loaded['hard_ref']\n    assert loaded['has_no_hard_ref']() is None\n    assert loaded['ref_out_of_scope']() is None\n\n\nclass ImportableModel(BaseModel):\n    foo: str\n    bar: Optional[str] = None\n    val: PositiveFloat = 0.7\n\n\ndef model_factory() -> Type:\n    class NonImportableModel(BaseModel):\n        foo: str\n        bar: Optional[str] = None\n        val: PositiveFloat = 0.7\n\n    return NonImportableModel\n\n\n@pytest.mark.parametrize(\n    'model_type,use_cloudpickle',\n    [\n        # Importable model can be pickled with either pickle or cloudpickle.\n        (ImportableModel, False),\n        (ImportableModel, True),\n        # Locally-defined model can only be pickled with cloudpickle.\n        (model_factory(), True),\n    ],\n)\ndef test_pickle_model(model_type: Type, use_cloudpickle: bool):\n    if use_cloudpickle:\n        model_type = cloudpickle.loads(cloudpickle.dumps(model_type))\n    else:\n        model_type = pickle.loads(pickle.dumps(model_type))\n\n    m = model_type(foo='hi', val=1)\n    assert m.foo == 'hi'\n    assert m.bar is None\n    assert m.val == 1.0\n\n    if use_cloudpickle:\n        m = cloudpickle.loads(cloudpickle.dumps(m))\n    else:\n        m = pickle.loads(pickle.dumps(m))\n\n    assert m.foo == 'hi'\n    assert m.bar is None\n    assert m.val == 1.0\n\n    with pytest.raises(ValidationError):\n        model_type(foo='hi', val=-1.1)\n\n\nclass ImportableNestedModel(BaseModel):\n    inner: ImportableModel\n\n\ndef nested_model_factory() -> Type:\n    class NonImportableNestedModel(BaseModel):\n        inner: ImportableModel\n\n    return NonImportableNestedModel\n\n\n@pytest.mark.parametrize(\n    'model_type,use_cloudpickle',\n    [\n        # Importable model can be pickled with either pickle or cloudpickle.\n        (ImportableNestedModel, False),\n        (ImportableNestedModel, True),\n        # Locally-defined model can only be pickled with cloudpickle.\n        (nested_model_factory(), True),\n    ],\n)\ndef test_pickle_nested_model(model_type: Type, use_cloudpickle: bool):\n    if use_cloudpickle:\n        model_type = cloudpickle.loads(cloudpickle.dumps(model_type))\n    else:\n        model_type = pickle.loads(pickle.dumps(model_type))\n\n    m = model_type(inner=ImportableModel(foo='hi', val=1))\n    assert m.inner.foo == 'hi'\n    assert m.inner.bar is None\n    assert m.inner.val == 1.0\n\n    if use_cloudpickle:\n        m = cloudpickle.loads(cloudpickle.dumps(m))\n    else:\n        m = pickle.loads(pickle.dumps(m))\n\n    assert m.inner.foo == 'hi'\n    assert m.inner.bar is None\n    assert m.inner.val == 1.0\n\n\n@pydantic.dataclasses.dataclass\nclass ImportableDataclass:\n    a: int\n    b: float\n\n\ndef dataclass_factory() -> Type:\n    @pydantic.dataclasses.dataclass\n    class NonImportableDataclass:\n        a: int\n        b: float\n\n    return NonImportableDataclass\n\n\n@dataclasses.dataclass\nclass ImportableBuiltinDataclass:\n    a: int\n    b: float\n\n\ndef builtin_dataclass_factory() -> Type:\n    @dataclasses.dataclass\n    class NonImportableBuiltinDataclass:\n        a: int\n        b: float\n\n    return NonImportableBuiltinDataclass\n\n\nclass ImportableChildDataclass(ImportableDataclass):\n    pass\n\n\ndef child_dataclass_factory() -> Type:\n    class NonImportableChildDataclass(ImportableDataclass):\n        pass\n\n    return NonImportableChildDataclass\n\n\n@pytest.mark.parametrize(\n    'dataclass_type,use_cloudpickle',\n    [\n        # Importable Pydantic dataclass can be pickled with either pickle or cloudpickle.\n        (ImportableDataclass, False),\n        (ImportableDataclass, True),\n        (ImportableChildDataclass, False),\n        (ImportableChildDataclass, True),\n        # Locally-defined Pydantic dataclass can only be pickled with cloudpickle.\n        (dataclass_factory(), True),\n        (child_dataclass_factory(), True),\n        # Pydantic dataclass generated from builtin can only be pickled with cloudpickle.\n        (pydantic.dataclasses.dataclass(ImportableBuiltinDataclass), True),\n        # Pydantic dataclass generated from locally-defined builtin can only be pickled with cloudpickle.\n        (pydantic.dataclasses.dataclass(builtin_dataclass_factory()), True),\n    ],\n)\ndef test_pickle_dataclass(dataclass_type: Type, use_cloudpickle: bool):\n    if use_cloudpickle:\n        dataclass_type = cloudpickle.loads(cloudpickle.dumps(dataclass_type))\n    else:\n        dataclass_type = pickle.loads(pickle.dumps(dataclass_type))\n\n    d = dataclass_type('1', '2.5')\n    assert d.a == 1\n    assert d.b == 2.5\n\n    if use_cloudpickle:\n        d = cloudpickle.loads(cloudpickle.dumps(d))\n    else:\n        d = pickle.loads(pickle.dumps(d))\n\n    assert d.a == 1\n    assert d.b == 2.5\n\n    d = dataclass_type(b=10, a=20)\n    assert d.a == 20\n    assert d.b == 10\n\n    if use_cloudpickle:\n        d = cloudpickle.loads(cloudpickle.dumps(d))\n    else:\n        d = pickle.loads(pickle.dumps(d))\n\n    assert d.a == 20\n    assert d.b == 10\n\n\nclass ImportableNestedDataclassModel(BaseModel):\n    inner: ImportableBuiltinDataclass\n\n\ndef nested_dataclass_model_factory() -> Type:\n    class NonImportableNestedDataclassModel(BaseModel):\n        inner: ImportableBuiltinDataclass\n\n    return NonImportableNestedDataclassModel\n\n\n@pytest.mark.parametrize(\n    'model_type,use_cloudpickle',\n    [\n        # Importable model can be pickled with either pickle or cloudpickle.\n        (ImportableNestedDataclassModel, False),\n        (ImportableNestedDataclassModel, True),\n        # Locally-defined model can only be pickled with cloudpickle.\n        (nested_dataclass_model_factory(), True),\n    ],\n)\ndef test_pickle_dataclass_nested_in_model(model_type: Type, use_cloudpickle: bool):\n    if use_cloudpickle:\n        model_type = cloudpickle.loads(cloudpickle.dumps(model_type))\n    else:\n        model_type = pickle.loads(pickle.dumps(model_type))\n\n    m = model_type(inner=ImportableBuiltinDataclass(a=10, b=20))\n    assert m.inner.a == 10\n    assert m.inner.b == 20\n\n    if use_cloudpickle:\n        m = cloudpickle.loads(cloudpickle.dumps(m))\n    else:\n        m = pickle.loads(pickle.dumps(m))\n\n    assert m.inner.a == 10\n    assert m.inner.b == 20\n\n\nclass ImportableModelWithConfig(BaseModel):\n    model_config = ConfigDict(title='MyTitle')\n\n\ndef model_with_config_factory() -> Type:\n    class NonImportableModelWithConfig(BaseModel):\n        model_config = ConfigDict(title='MyTitle')\n\n    return NonImportableModelWithConfig\n\n\n@pytest.mark.parametrize(\n    'model_type,use_cloudpickle',\n    [\n        (ImportableModelWithConfig, False),\n        (ImportableModelWithConfig, True),\n        (model_with_config_factory(), True),\n    ],\n)\ndef test_pickle_model_with_config(model_type: Type, use_cloudpickle: bool):\n    if use_cloudpickle:\n        model_type = cloudpickle.loads(cloudpickle.dumps(model_type))\n    else:\n        model_type = pickle.loads(pickle.dumps(model_type))\n\n    assert model_type.model_config['title'] == 'MyTitle'\n", "tests/test_decorators.py": "import pytest\n\nfrom pydantic import PydanticUserError\nfrom pydantic._internal._decorators import inspect_annotated_serializer, inspect_validator\n\n\ndef _two_pos_required_args(a, b):\n    pass\n\n\ndef _two_pos_required_args_extra_optional(a, b, c=1, d=2, *, e=3):\n    pass\n\n\ndef _three_pos_required_args(a, b, c):\n    pass\n\n\ndef _one_pos_required_arg_one_optional(a, b=1):\n    pass\n\n\n@pytest.mark.parametrize(\n    [\n        'obj',\n        'mode',\n        'expected',\n    ],\n    [\n        (str, 'plain', False),\n        (float, 'plain', False),\n        (int, 'plain', False),\n        (lambda a: str(a), 'plain', False),\n        (lambda a='': str(a), 'plain', False),\n        (_two_pos_required_args, 'plain', True),\n        (_two_pos_required_args, 'wrap', False),\n        (_two_pos_required_args_extra_optional, 'plain', True),\n        (_two_pos_required_args_extra_optional, 'wrap', False),\n        (_three_pos_required_args, 'wrap', True),\n        (_one_pos_required_arg_one_optional, 'plain', False),\n    ],\n)\ndef test_inspect_validator(obj, mode, expected):\n    assert inspect_validator(obj, mode=mode) == expected\n\n\ndef test_inspect_validator_error_wrap():\n    def validator1(arg1):\n        pass\n\n    def validator4(arg1, arg2, arg3, arg4):\n        pass\n\n    with pytest.raises(PydanticUserError) as e:\n        inspect_validator(validator1, mode='wrap')\n\n    assert e.value.code == 'validator-signature'\n\n    with pytest.raises(PydanticUserError) as e:\n        inspect_validator(validator4, mode='wrap')\n\n    assert e.value.code == 'validator-signature'\n\n\n@pytest.mark.parametrize('mode', ['before', 'after', 'plain'])\ndef test_inspect_validator_error(mode):\n    def validator():\n        pass\n\n    def validator3(arg1, arg2, arg3):\n        pass\n\n    with pytest.raises(PydanticUserError) as e:\n        inspect_validator(validator, mode=mode)\n\n    assert e.value.code == 'validator-signature'\n\n    with pytest.raises(PydanticUserError) as e:\n        inspect_validator(validator3, mode=mode)\n\n    assert e.value.code == 'validator-signature'\n\n\n@pytest.mark.parametrize(\n    [\n        'obj',\n        'mode',\n        'expected',\n    ],\n    [\n        (str, 'plain', False),\n        (float, 'plain', False),\n        (int, 'plain', False),\n        (lambda a: str(a), 'plain', False),\n        (lambda a='': str(a), 'plain', False),\n        (_two_pos_required_args, 'plain', True),\n        (_two_pos_required_args, 'wrap', False),\n        (_two_pos_required_args_extra_optional, 'plain', True),\n        (_two_pos_required_args_extra_optional, 'wrap', False),\n        (_three_pos_required_args, 'wrap', True),\n        (_one_pos_required_arg_one_optional, 'plain', False),\n    ],\n)\ndef test_inspect_annotated_serializer(obj, mode, expected):\n    assert inspect_annotated_serializer(obj, mode=mode) == expected\n\n\n@pytest.mark.parametrize('mode', ['plain', 'wrap'])\ndef test_inspect_annotated_serializer_invalid_number_of_arguments(mode):\n    # TODO: add more erroneous cases\n    def serializer():\n        pass\n\n    with pytest.raises(PydanticUserError) as e:\n        inspect_annotated_serializer(serializer, mode=mode)\n\n    assert e.value.code == 'field-serializer-signature'\n", "tests/test_version.py": "from unittest.mock import patch\n\nimport pytest\nfrom packaging.version import parse as parse_version\n\nimport pydantic\nfrom pydantic.version import version_info, version_short\n\n\ndef test_version_info():\n    version_info_fields = [\n        'pydantic version',\n        'pydantic-core version',\n        'pydantic-core build',\n        'install path',\n        'python version',\n        'platform',\n        'related packages',\n        'commit',\n    ]\n\n    s = version_info()\n    assert all([f'{field}:' in s for field in version_info_fields])\n    assert s.count('\\n') == 7\n\n\ndef test_standard_version():\n    v = parse_version(pydantic.VERSION)\n    assert str(v) == pydantic.VERSION\n\n\ndef test_version_attribute_is_present():\n    assert hasattr(pydantic, '__version__')\n\n\ndef test_version_attribute_is_a_string():\n    assert isinstance(pydantic.__version__, str)\n\n\n@pytest.mark.parametrize('version,expected', (('2.1', '2.1'), ('2.1.0', '2.1')))\ndef test_version_short(version, expected):\n    with patch('pydantic.version.VERSION', version):\n        assert version_short() == expected\n", "tests/test_plugin_loader.py": "import importlib.metadata as importlib_metadata\nimport os\nfrom unittest.mock import patch\n\nimport pytest\n\nimport pydantic.plugin._loader as loader\n\n\nclass EntryPoint:\n    def __init__(self, name, value, group):\n        self.name = name\n        self.value = value\n        self.group = group\n\n    def load(self):\n        return self.value\n\n\nclass Dist:\n    entry_points = []\n\n    def __init__(self, entry_points):\n        self.entry_points = entry_points\n\n\n@pytest.fixture\ndef reset_plugins():\n    global loader\n    initial_plugins = loader._plugins\n    loader._plugins = None\n    yield\n    # teardown\n    loader._plugins = initial_plugins\n\n\n@pytest.fixture(autouse=True)\ndef mock():\n    mock_entry_1 = EntryPoint(name='test_plugin1', value='test_plugin:plugin1', group='pydantic')\n    mock_entry_2 = EntryPoint(name='test_plugin2', value='test_plugin:plugin2', group='pydantic')\n    mock_entry_3 = EntryPoint(name='test_plugin3', value='test_plugin:plugin3', group='pydantic')\n    mock_dist = Dist([mock_entry_1, mock_entry_2, mock_entry_3])\n\n    with patch.object(importlib_metadata, 'distributions', return_value=[mock_dist]):\n        yield\n\n\ndef test_loader(reset_plugins):\n    res = loader.get_plugins()\n    assert list(res) == ['test_plugin:plugin1', 'test_plugin:plugin2', 'test_plugin:plugin3']\n\n\ndef test_disable_all(reset_plugins):\n    os.environ['PYDANTIC_DISABLE_PLUGINS'] = '__all__'\n    res = loader.get_plugins()\n    assert res == ()\n\n\ndef test_disable_all_1(reset_plugins):\n    os.environ['PYDANTIC_DISABLE_PLUGINS'] = '1'\n    res = loader.get_plugins()\n    assert res == ()\n\n\ndef test_disable_true(reset_plugins):\n    os.environ['PYDANTIC_DISABLE_PLUGINS'] = 'true'\n    res = loader.get_plugins()\n    assert res == ()\n\n\ndef test_disable_one(reset_plugins):\n    os.environ['PYDANTIC_DISABLE_PLUGINS'] = 'test_plugin1'\n    res = loader.get_plugins()\n    assert len(list(res)) == 2\n    assert 'test_plugin:plugin1' not in list(res)\n\n\ndef test_disable_multiple(reset_plugins):\n    os.environ['PYDANTIC_DISABLE_PLUGINS'] = 'test_plugin1,test_plugin2'\n    res = loader.get_plugins()\n    assert len(list(res)) == 1\n    assert 'test_plugin:plugin1' not in list(res)\n    assert 'test_plugin:plugin2' not in list(res)\n", "tests/test_internal.py": "\"\"\"\nTests for internal things that are complex enough to warrant their own unit tests.\n\"\"\"\n\nfrom dataclasses import dataclass\n\nimport pytest\nfrom pydantic_core import CoreSchema, SchemaValidator\nfrom pydantic_core import core_schema as cs\n\nfrom pydantic._internal._core_utils import (\n    HAS_INVALID_SCHEMAS_METADATA_KEY,\n    Walk,\n    collect_invalid_schemas,\n    simplify_schema_references,\n    walk_core_schema,\n)\nfrom pydantic._internal._repr import Representation\n\n\ndef remove_metadata(schema: CoreSchema) -> CoreSchema:\n    def inner(s: CoreSchema, recurse: Walk) -> CoreSchema:\n        s = s.copy()\n        s.pop('metadata', None)\n        return recurse(s, inner)\n\n    return walk_core_schema(schema, inner)\n\n\n@pytest.mark.parametrize(\n    'input_schema,inlined',\n    [\n        # Test case 1: Simple schema with no references\n        (cs.list_schema(cs.int_schema()), cs.list_schema(cs.int_schema())),\n        # Test case 2: Schema with single-level nested references\n        (\n            cs.definitions_schema(\n                cs.list_schema(cs.definition_reference_schema('list_of_ints')),\n                definitions=[\n                    cs.list_schema(cs.definition_reference_schema('int'), ref='list_of_ints'),\n                    cs.int_schema(ref='int'),\n                ],\n            ),\n            cs.list_schema(cs.list_schema(cs.int_schema(ref='int'), ref='list_of_ints')),\n        ),\n        # Test case 3: Schema with multiple single-level nested references\n        (\n            cs.list_schema(\n                cs.definitions_schema(cs.definition_reference_schema('int'), definitions=[cs.int_schema(ref='int')])\n            ),\n            cs.list_schema(cs.int_schema(ref='int')),\n        ),\n        # Test case 4: A simple recursive schema\n        (\n            cs.list_schema(cs.definition_reference_schema(schema_ref='list'), ref='list'),\n            cs.definitions_schema(\n                cs.definition_reference_schema(schema_ref='list'),\n                definitions=[cs.list_schema(cs.definition_reference_schema(schema_ref='list'), ref='list')],\n            ),\n        ),\n        # Test case 5: Deeply nested schema with multiple references\n        (\n            cs.definitions_schema(\n                cs.list_schema(cs.definition_reference_schema('list_of_lists_of_ints')),\n                definitions=[\n                    cs.list_schema(cs.definition_reference_schema('list_of_ints'), ref='list_of_lists_of_ints'),\n                    cs.list_schema(cs.definition_reference_schema('int'), ref='list_of_ints'),\n                    cs.int_schema(ref='int'),\n                ],\n            ),\n            cs.list_schema(\n                cs.list_schema(\n                    cs.list_schema(cs.int_schema(ref='int'), ref='list_of_ints'), ref='list_of_lists_of_ints'\n                )\n            ),\n        ),\n        # Test case 6: More complex recursive schema\n        (\n            cs.definitions_schema(\n                cs.list_schema(cs.definition_reference_schema(schema_ref='list_of_ints_and_lists')),\n                definitions=[\n                    cs.list_schema(\n                        cs.definitions_schema(\n                            cs.definition_reference_schema(schema_ref='int_or_list'),\n                            definitions=[\n                                cs.int_schema(ref='int'),\n                                cs.tuple_variable_schema(\n                                    cs.definition_reference_schema(schema_ref='list_of_ints_and_lists'), ref='a tuple'\n                                ),\n                            ],\n                        ),\n                        ref='list_of_ints_and_lists',\n                    ),\n                    cs.int_schema(ref='int_or_list'),\n                ],\n            ),\n            cs.list_schema(cs.list_schema(cs.int_schema(ref='int_or_list'), ref='list_of_ints_and_lists')),\n        ),\n        # Test case 7: Schema with multiple definitions and nested references, some of which are unused\n        (\n            cs.definitions_schema(\n                cs.list_schema(cs.definition_reference_schema('list_of_ints')),\n                definitions=[\n                    cs.list_schema(\n                        cs.definitions_schema(\n                            cs.definition_reference_schema('int'), definitions=[cs.int_schema(ref='int')]\n                        ),\n                        ref='list_of_ints',\n                    )\n                ],\n            ),\n            cs.list_schema(cs.list_schema(cs.int_schema(ref='int'), ref='list_of_ints')),\n        ),\n        # Test case 8: Reference is used in multiple places\n        (\n            cs.definitions_schema(\n                cs.union_schema(\n                    [\n                        cs.definition_reference_schema('list_of_ints'),\n                        cs.tuple_variable_schema(cs.definition_reference_schema('int')),\n                    ]\n                ),\n                definitions=[\n                    cs.list_schema(cs.definition_reference_schema('int'), ref='list_of_ints'),\n                    cs.int_schema(ref='int'),\n                ],\n            ),\n            cs.definitions_schema(\n                cs.union_schema(\n                    [\n                        cs.list_schema(cs.definition_reference_schema('int'), ref='list_of_ints'),\n                        cs.tuple_variable_schema(cs.definition_reference_schema('int')),\n                    ]\n                ),\n                definitions=[cs.int_schema(ref='int')],\n            ),\n        ),\n        # Test case 9: https://github.com/pydantic/pydantic/issues/6270\n        (\n            cs.definitions_schema(\n                cs.definition_reference_schema('model'),\n                definitions=[\n                    cs.typed_dict_schema(\n                        {\n                            'a': cs.typed_dict_field(\n                                cs.nullable_schema(\n                                    cs.int_schema(ref='ref'),\n                                ),\n                            ),\n                            'b': cs.typed_dict_field(\n                                cs.nullable_schema(\n                                    cs.int_schema(ref='ref'),\n                                ),\n                            ),\n                        },\n                        ref='model',\n                    ),\n                ],\n            ),\n            cs.definitions_schema(\n                cs.typed_dict_schema(\n                    {\n                        'a': cs.typed_dict_field(\n                            cs.nullable_schema(cs.definition_reference_schema(schema_ref='ref')),\n                        ),\n                        'b': cs.typed_dict_field(\n                            cs.nullable_schema(cs.definition_reference_schema(schema_ref='ref')),\n                        ),\n                    },\n                    ref='model',\n                ),\n                definitions=[\n                    cs.int_schema(ref='ref'),\n                ],\n            ),\n        ),\n    ],\n)\ndef test_build_schema_defs(input_schema: cs.CoreSchema, inlined: cs.CoreSchema):\n    actual_inlined = remove_metadata(simplify_schema_references(input_schema))\n    assert actual_inlined == inlined\n    SchemaValidator(actual_inlined)  # check for validity\n\n\ndef test_representation_integrations():\n    devtools = pytest.importorskip('devtools')\n\n    @dataclass\n    class Obj(Representation):\n        int_attr: int = 42\n        str_attr: str = 'Marvin'\n\n    obj = Obj()\n\n    assert str(devtools.debug.format(obj)).split('\\n')[1:] == [\n        '    Obj(',\n        '        int_attr=42,',\n        \"        str_attr='Marvin',\",\n        '    ) (Obj)',\n    ]\n    assert list(obj.__rich_repr__()) == [('int_attr', 42), ('str_attr', 'Marvin')]\n\n\ndef test_schema_is_valid():\n    assert collect_invalid_schemas(cs.none_schema()) is False\n    assert (\n        collect_invalid_schemas(cs.nullable_schema(cs.int_schema(metadata={HAS_INVALID_SCHEMAS_METADATA_KEY: True})))\n        is True\n    )\n", "tests/test_type_hints.py": "\"\"\"\nTest pydantic model type hints (annotations) and that they can be\nqueried by :py:meth:`typing.get_type_hints`.\n\"\"\"\n\nimport inspect\nimport sys\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    Optional,\n    Set,\n    TypeVar,\n)\n\nimport pytest\nimport typing_extensions\n\nfrom pydantic import (\n    BaseModel,\n    RootModel,\n)\nfrom pydantic.dataclasses import dataclass\n\nDEPRECATED_MODEL_MEMBERS = {\n    'construct',\n    'copy',\n    'dict',\n    'from_orm',\n    'json',\n    'json_schema',\n    'parse_file',\n    'parse_obj',\n}\n\n# Disable deprecation warnings, as we enumerate members that may be\n# i.e. pydantic.warnings.PydanticDeprecatedSince20: The `__fields__` attribute is deprecated,\n#      use `model_fields` instead.\n# Additionally, only run these tests for 3.10+\npytestmark = [\n    pytest.mark.filterwarnings('ignore::DeprecationWarning'),\n    pytest.mark.skipif(sys.version_info < (3, 10), reason='requires python3.10 or higher to work properly'),\n]\n\n\n@pytest.fixture(name='ParentModel', scope='session')\ndef parent_sub_model_fixture():\n    class UltraSimpleModel(BaseModel):\n        a: float\n        b: int = 10\n\n    class ParentModel(BaseModel):\n        grape: bool\n        banana: UltraSimpleModel\n\n    return ParentModel\n\n\ndef inspect_type_hints(\n    obj_type, members: Optional[Set[str]] = None, exclude_members: Optional[Set[str]] = None, recursion_limit: int = 3\n):\n    \"\"\"\n    Test an object and its members to make sure type hints can be resolved.\n    :param obj_type: Type to check\n    :param members: Explicit set of members to check, None to check all\n    :param exclude_members: Set of member names to exclude\n    :param recursion_limit: Recursion limit (0 to disallow)\n    \"\"\"\n\n    try:\n        hints = typing_extensions.get_type_hints(obj_type)\n        assert isinstance(hints, dict), f'Type annotation(s) on {obj_type} are invalid'\n    except NameError as ex:\n        raise AssertionError(f'Type annotation(s) on {obj_type} are invalid: {str(ex)}') from ex\n\n    if recursion_limit <= 0:\n        return\n\n    if isinstance(obj_type, type):\n        # Check class members\n        for member_name, member_obj in inspect.getmembers(obj_type):\n            if member_name.startswith('_'):\n                # Ignore private members\n                continue\n            if (members and member_name not in members) or (exclude_members and member_name in exclude_members):\n                continue\n\n            if inspect.isclass(member_obj) or inspect.isfunction(member_obj):\n                # Inspect all child members (can't exclude specific ones)\n                inspect_type_hints(member_obj, recursion_limit=recursion_limit - 1)\n\n\n@pytest.mark.parametrize(\n    ('obj_type', 'members', 'exclude_members'),\n    [\n        (BaseModel, None, DEPRECATED_MODEL_MEMBERS),\n        (RootModel, None, DEPRECATED_MODEL_MEMBERS),\n    ],\n)\ndef test_obj_type_hints(obj_type, members: Optional[Set[str]], exclude_members: Optional[Set[str]]):\n    \"\"\"\n    Test an object and its members to make sure type hints can be resolved.\n    :param obj_type: Type to check\n    :param members: Explicit set of members to check, None to check all\n    :param exclude_members: Set of member names to exclude\n    \"\"\"\n    inspect_type_hints(obj_type, members, exclude_members)\n\n\ndef test_parent_sub_model(ParentModel):\n    inspect_type_hints(ParentModel, None, DEPRECATED_MODEL_MEMBERS)\n\n\ndef test_root_model_as_field():\n    class MyRootModel(RootModel[int]):\n        pass\n\n    class MyModel(BaseModel):\n        root_model: MyRootModel\n\n    inspect_type_hints(MyRootModel, None, DEPRECATED_MODEL_MEMBERS)\n    inspect_type_hints(MyModel, None, DEPRECATED_MODEL_MEMBERS)\n\n\ndef test_generics():\n    data_type = TypeVar('data_type')\n\n    class Result(BaseModel, Generic[data_type]):\n        data: data_type\n\n    inspect_type_hints(Result, None, DEPRECATED_MODEL_MEMBERS)\n    inspect_type_hints(Result[Dict[str, Any]], None, DEPRECATED_MODEL_MEMBERS)\n\n\ndef test_dataclasses():\n    @dataclass\n    class MyDataclass:\n        a: int\n        b: float\n\n    inspect_type_hints(MyDataclass)\n", "tests/test_create_model.py": "import platform\nimport re\nfrom typing import Generic, Optional, Tuple, TypeVar\n\nimport pytest\nfrom typing_extensions import Annotated\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    PrivateAttr,\n    PydanticDeprecatedSince20,\n    PydanticUserError,\n    ValidationError,\n    create_model,\n    errors,\n    field_validator,\n    validator,\n)\nfrom pydantic.fields import ModelPrivateAttr\n\n\ndef test_create_model():\n    model = create_model('FooModel', foo=(str, ...), bar=(int, 123))\n    assert issubclass(model, BaseModel)\n    assert model.model_config == BaseModel.model_config\n    assert model.__name__ == 'FooModel'\n    assert model.model_fields.keys() == {'foo', 'bar'}\n\n    assert not model.__pydantic_decorators__.validators\n    assert not model.__pydantic_decorators__.root_validators\n    assert not model.__pydantic_decorators__.field_validators\n    assert not model.__pydantic_decorators__.field_serializers\n\n    assert model.__module__ == 'tests.test_create_model'\n\n\ndef test_create_model_usage():\n    model = create_model('FooModel', foo=(str, ...), bar=(int, 123))\n    m = model(foo='hello')\n    assert m.foo == 'hello'\n    assert m.bar == 123\n    with pytest.raises(ValidationError):\n        model()\n    with pytest.raises(ValidationError):\n        model(foo='hello', bar='xxx')\n\n\ndef test_create_model_pickle(create_module):\n    \"\"\"\n    Pickle will work for dynamically created model only if it was defined globally with its class name\n    and module where it's defined was specified\n    \"\"\"\n\n    @create_module\n    def module():\n        import pickle\n\n        from pydantic import create_model\n\n        FooModel = create_model('FooModel', foo=(str, ...), bar=(int, 123), __module__=__name__)\n\n        m = FooModel(foo='hello')\n        d = pickle.dumps(m)\n        m2 = pickle.loads(d)\n        assert m2.foo == m.foo == 'hello'\n        assert m2.bar == m.bar == 123\n        assert m2 == m\n        assert m2 is not m\n\n\ndef test_create_model_multi_inheritance():\n    class Mixin:\n        pass\n\n    Generic_T = Generic[TypeVar('T')]\n    FooModel = create_model('FooModel', value=(int, ...), __base__=(BaseModel, Generic_T))\n\n    assert FooModel.__orig_bases__ == (BaseModel, Generic_T)\n\n\ndef test_create_model_must_not_reset_parent_namespace():\n    # It's important to use the annotation `'namespace'` as this is a particular string that is present\n    # in the parent namespace if you reset the parent namespace in the call to `create_model`.\n\n    AbcModel = create_model('AbcModel', abc=('namespace', None))\n    with pytest.raises(\n        PydanticUserError,\n        match=re.escape(\n            '`AbcModel` is not fully defined; you should define `namespace`, then call `AbcModel.model_rebuild()`.'\n        ),\n    ):\n        AbcModel(abc=1)\n\n    # Rebuild the model now that `namespace` is defined\n    namespace = int  # noqa F841\n    AbcModel.model_rebuild()\n\n    assert AbcModel(abc=1).abc == 1\n\n    with pytest.raises(ValidationError) as exc_info:\n        AbcModel(abc='a')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('abc',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        }\n    ]\n\n\ndef test_invalid_name():\n    with pytest.warns(RuntimeWarning):\n        model = create_model('FooModel', _foo=(str, ...))\n    assert len(model.model_fields) == 0\n\n\ndef test_field_wrong_tuple():\n    with pytest.raises(errors.PydanticUserError):\n        create_model('FooModel', foo=(1, 2, 3))\n\n\ndef test_config_and_base():\n    with pytest.raises(errors.PydanticUserError):\n        create_model('FooModel', __config__=BaseModel.model_config, __base__=BaseModel)\n\n\ndef test_inheritance():\n    class BarModel(BaseModel):\n        x: int = 1\n        y: int = 2\n\n    model = create_model('FooModel', foo=(str, ...), bar=(int, 123), __base__=BarModel)\n    assert model.model_fields.keys() == {'foo', 'bar', 'x', 'y'}\n    m = model(foo='a', x=4)\n    assert m.model_dump() == {'bar': 123, 'foo': 'a', 'x': 4, 'y': 2}\n\n    # bases as a tuple\n    model = create_model('FooModel', foo=(str, ...), bar=(int, 123), __base__=(BarModel,))\n    assert model.model_fields.keys() == {'foo', 'bar', 'x', 'y'}\n    m = model(foo='a', x=4)\n    assert m.model_dump() == {'bar': 123, 'foo': 'a', 'x': 4, 'y': 2}\n\n\ndef test_custom_config():\n    config = ConfigDict(frozen=True)\n    expected_config = BaseModel.model_config.copy()\n    expected_config['frozen'] = True\n\n    model = create_model('FooModel', foo=(int, ...), __config__=config)\n    m = model(**{'foo': '987'})\n    assert m.foo == 987\n    assert model.model_config == expected_config\n    with pytest.raises(ValidationError):\n        m.foo = 654\n\n\ndef test_custom_config_inherits():\n    class Config(ConfigDict):\n        custom_config: bool\n\n    config = Config(custom_config=True, validate_assignment=True)\n    expected_config = Config(BaseModel.model_config)\n    expected_config.update(config)\n\n    model = create_model('FooModel', foo=(int, ...), __config__=config)\n    m = model(**{'foo': '987'})\n    assert m.foo == 987\n    assert model.model_config == expected_config\n    with pytest.raises(ValidationError):\n        m.foo = ['123']\n\n\ndef test_custom_config_extras():\n    config = ConfigDict(extra='forbid')\n\n    model = create_model('FooModel', foo=(int, ...), __config__=config)\n    assert model(foo=654)\n    with pytest.raises(ValidationError):\n        model(bar=654)\n\n\ndef test_inheritance_validators():\n    class BarModel(BaseModel):\n        @field_validator('a', check_fields=False)\n        @classmethod\n        def check_a(cls, v):\n            if 'foobar' not in v:\n                raise ValueError('\"foobar\" not found in a')\n            return v\n\n    model = create_model('FooModel', a=(str, 'cake'), __base__=BarModel)\n    assert model().a == 'cake'\n    assert model(a='this is foobar good').a == 'this is foobar good'\n    with pytest.raises(ValidationError):\n        model(a='something else')\n\n\ndef test_inheritance_validators_always():\n    class BarModel(BaseModel):\n        @field_validator('a', check_fields=False)\n        @classmethod\n        def check_a(cls, v):\n            if 'foobar' not in v:\n                raise ValueError('\"foobar\" not found in a')\n            return v\n\n    model = create_model('FooModel', a=(str, Field('cake', validate_default=True)), __base__=BarModel)\n    with pytest.raises(ValidationError):\n        model()\n    assert model(a='this is foobar good').a == 'this is foobar good'\n    with pytest.raises(ValidationError):\n        model(a='something else')\n\n\ndef test_inheritance_validators_all():\n    with pytest.warns(PydanticDeprecatedSince20, match='Pydantic V1 style `@validator` validators are deprecated'):\n\n        class BarModel(BaseModel):\n            @validator('*')\n            @classmethod\n            def check_all(cls, v):\n                return v * 2\n\n    model = create_model('FooModel', a=(int, ...), b=(int, ...), __base__=BarModel)\n    assert model(a=2, b=6).model_dump() == {'a': 4, 'b': 12}\n\n\ndef test_funky_name():\n    model = create_model('FooModel', **{'this-is-funky': (int, ...)})\n    m = model(**{'this-is-funky': '123'})\n    assert m.model_dump() == {'this-is-funky': 123}\n    with pytest.raises(ValidationError) as exc_info:\n        model()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('this-is-funky',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\ndef test_repeat_base_usage():\n    class Model(BaseModel):\n        a: str\n\n    assert Model.model_fields.keys() == {'a'}\n\n    model = create_model('FooModel', b=(int, 1), __base__=Model)\n\n    assert Model.model_fields.keys() == {'a'}\n    assert model.model_fields.keys() == {'a', 'b'}\n\n    model2 = create_model('Foo2Model', c=(int, 1), __base__=Model)\n\n    assert Model.model_fields.keys() == {'a'}\n    assert model.model_fields.keys() == {'a', 'b'}\n    assert model2.model_fields.keys() == {'a', 'c'}\n\n    model3 = create_model('Foo2Model', d=(int, 1), __base__=model)\n\n    assert Model.model_fields.keys() == {'a'}\n    assert model.model_fields.keys() == {'a', 'b'}\n    assert model2.model_fields.keys() == {'a', 'c'}\n    assert model3.model_fields.keys() == {'a', 'b', 'd'}\n\n\ndef test_dynamic_and_static():\n    class A(BaseModel):\n        x: int\n        y: float\n        z: str\n\n    DynamicA = create_model('A', x=(int, ...), y=(float, ...), z=(str, ...))\n\n    for field_name in ('x', 'y', 'z'):\n        assert A.model_fields[field_name].default == DynamicA.model_fields[field_name].default\n\n\ndef test_create_model_field_and_model_title():\n    m = create_model('M', __config__=ConfigDict(title='abc'), a=(str, Field(title='field-title')))\n    assert m.model_json_schema() == {\n        'properties': {'a': {'title': 'field-title', 'type': 'string'}},\n        'required': ['a'],\n        'title': 'abc',\n        'type': 'object',\n    }\n\n\ndef test_create_model_field_description():\n    m = create_model('M', a=(str, Field(description='descr')), __doc__='Some doc')\n    assert m.model_json_schema() == {\n        'properties': {'a': {'description': 'descr', 'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'title': 'M',\n        'type': 'object',\n        'description': 'Some doc',\n    }\n\n\ndef test_create_model_with_doc():\n    model = create_model('FooModel', foo=(str, ...), bar=(int, 123), __doc__='The Foo model')\n    assert model.__name__ == 'FooModel'\n    assert model.__doc__ == 'The Foo model'\n\n\n@pytest.mark.parametrize('base', [ModelPrivateAttr, object])\n@pytest.mark.parametrize('use_annotation', [True, False])\ndef test_private_descriptors(base, use_annotation):\n    set_name_calls = []\n    get_calls = []\n    set_calls = []\n    delete_calls = []\n\n    class MyDescriptor(base):\n        def __init__(self, fn):\n            super().__init__()\n            self.fn = fn\n            self.name = ''\n\n        def __set_name__(self, owner, name):\n            set_name_calls.append((owner, name))\n            self.name = name\n\n        def __get__(self, obj, type=None):\n            get_calls.append((obj, type))\n            return self.fn(obj) if obj else self\n\n        def __set__(self, obj, value):\n            set_calls.append((obj, value))\n            self.fn = lambda obj: value\n\n        def __delete__(self, obj):\n            delete_calls.append(obj)\n\n            def fail(obj):\n                # I have purposely not used the exact formatting you'd get if the attribute wasn't defined,\n                # to make it clear this function is being called, while also having sensible behavior\n                raise AttributeError(f'{self.name!r} is not defined on {obj!r}')\n\n            self.fn = fail\n\n    class A(BaseModel):\n        x: int\n\n        if use_annotation:\n            _some_func: MyDescriptor = MyDescriptor(lambda self: self.x)\n        else:\n            _some_func = MyDescriptor(lambda self: self.x)\n\n        @property\n        def _double_x(self):\n            return self.x * 2\n\n    assert set(A.__private_attributes__) == {'_some_func'}\n    assert set_name_calls == [(A, '_some_func')]\n\n    a = A(x=2)\n\n    assert a._double_x == 4  # Ensure properties with leading underscores work fine and don't become private attributes\n\n    assert get_calls == []\n    assert a._some_func == 2\n    assert get_calls == [(a, A)]\n\n    assert set_calls == []\n    a._some_func = 3\n    assert set_calls == [(a, 3)]\n\n    assert a._some_func == 3\n    assert get_calls == [(a, A), (a, A)]\n\n    assert delete_calls == []\n    del a._some_func\n    assert delete_calls == [a]\n\n    with pytest.raises(AttributeError, match=r\"'_some_func' is not defined on A\\(x=2\\)\"):\n        a._some_func\n    assert get_calls == [(a, A), (a, A), (a, A)]\n\n\ndef test_private_attr_set_name():\n    class SetNameInt(int):\n        _owner_attr_name: Optional[str] = None\n\n        def __set_name__(self, owner, name):\n            self._owner_attr_name = f'{owner.__name__}.{name}'\n\n    _private_attr_default = SetNameInt(1)\n\n    class Model(BaseModel):\n        _private_attr_1: int = PrivateAttr(default=_private_attr_default)\n        _private_attr_2: SetNameInt = SetNameInt(2)\n\n    assert _private_attr_default._owner_attr_name == 'Model._private_attr_1'\n\n    m = Model()\n    assert m._private_attr_1 == 1\n    assert m._private_attr_1._owner_attr_name == 'Model._private_attr_1'\n    assert m._private_attr_2 == 2\n    assert m._private_attr_2._owner_attr_name == 'Model._private_attr_2'\n\n\ndef test_private_attr_default_descriptor_attribute_error():\n    class SetNameInt(int):\n        def __get__(self, obj, cls):\n            return self\n\n    _private_attr_default = SetNameInt(1)\n\n    class Model(BaseModel):\n        _private_attr: int = PrivateAttr(default=_private_attr_default)\n\n    assert Model.__private_attributes__['_private_attr'].__get__(None, Model) == _private_attr_default\n\n    with pytest.raises(AttributeError, match=\"'ModelPrivateAttr' object has no attribute 'some_attr'\"):\n        Model.__private_attributes__['_private_attr'].some_attr\n\n\ndef test_private_attr_set_name_do_not_crash_if_not_callable():\n    class SetNameInt(int):\n        __set_name__ = None\n\n    _private_attr_default = SetNameInt(2)\n\n    class Model(BaseModel):\n        _private_attr: int = PrivateAttr(default=_private_attr_default)\n\n    # Checks below are just to ensure that everything is the same as in `test_private_attr_set_name`\n    # The main check is that model class definition above doesn't crash\n    assert Model()._private_attr == 2\n\n\ndef test_del_model_attr():\n    class Model(BaseModel):\n        some_field: str\n\n    m = Model(some_field='value')\n    assert hasattr(m, 'some_field')\n\n    del m.some_field\n\n    assert not hasattr(m, 'some_field')\n\n\n@pytest.mark.skipif(\n    platform.python_implementation() == 'PyPy',\n    reason='In this single case `del` behaves weird on pypy',\n)\ndef test_del_model_attr_error():\n    class Model(BaseModel):\n        some_field: str\n\n    m = Model(some_field='value')\n    assert not hasattr(m, 'other_field')\n\n    with pytest.raises(AttributeError, match='other_field'):\n        del m.other_field\n\n\ndef test_del_model_attr_with_privat_attrs():\n    class Model(BaseModel):\n        _private_attr: int = PrivateAttr(default=1)\n        some_field: str\n\n    m = Model(some_field='value')\n    assert hasattr(m, 'some_field')\n\n    del m.some_field\n\n    assert not hasattr(m, 'some_field')\n\n\n@pytest.mark.skipif(\n    platform.python_implementation() == 'PyPy',\n    reason='In this single case `del` behaves weird on pypy',\n)\ndef test_del_model_attr_with_privat_attrs_error():\n    class Model(BaseModel):\n        _private_attr: int = PrivateAttr(default=1)\n        some_field: str\n\n    m = Model(some_field='value')\n    assert not hasattr(m, 'other_field')\n\n    with pytest.raises(AttributeError, match=\"'Model' object has no attribute 'other_field'\"):\n        del m.other_field\n\n\ndef test_del_model_attr_with_privat_attrs_twice_error():\n    class Model(BaseModel):\n        _private_attr: int = 1\n        some_field: str\n\n    m = Model(some_field='value')\n    assert hasattr(m, '_private_attr')\n\n    del m._private_attr\n\n    with pytest.raises(AttributeError, match=\"'Model' object has no attribute '_private_attr'\"):\n        del m._private_attr\n\n\ndef test_create_model_with_slots():\n    field_definitions = {'__slots__': (Optional[Tuple[str, ...]], None), 'foobar': (Optional[int], None)}\n    with pytest.warns(RuntimeWarning, match='__slots__ should not be passed to create_model'):\n        model = create_model('PartialPet', **field_definitions)\n\n    assert model.model_fields.keys() == {'foobar'}\n\n\ndef test_create_model_non_annotated():\n    with pytest.raises(\n        TypeError,\n        match='A non-annotated attribute was detected: `bar = 123`. All model fields require a type annotation',\n    ):\n        create_model('FooModel', foo=(str, ...), bar=123)\n\n\n@pytest.mark.parametrize(\n    'annotation_type,field_info',\n    [\n        (bool, Field(alias='foo_bool_alias', description='foo boolean')),\n        (str, Field(alias='foo_str_alis', description='foo string')),\n    ],\n)\ndef test_create_model_typing_annotated_field_info(annotation_type, field_info):\n    annotated_foo = Annotated[annotation_type, field_info]\n    model = create_model('FooModel', foo=annotated_foo, bar=(int, 123))\n\n    assert model.model_fields.keys() == {'foo', 'bar'}\n\n    foo = model.model_fields.get('foo')\n\n    assert foo is not None\n    assert foo.annotation == annotation_type\n    assert foo.alias == field_info.alias\n    assert foo.description == field_info.description\n\n\ndef test_create_model_expect_field_info_as_metadata_typing():\n    annotated_foo = Annotated[int, 10]\n\n    with pytest.raises(PydanticUserError, match=r'Field definitions should be a Annotated\\[<type>, <FieldInfo>\\]'):\n        create_model('FooModel', foo=annotated_foo)\n\n\ndef test_create_model_tuple():\n    model = create_model('FooModel', foo=(Tuple[int, int], (1, 2)))\n    assert model().foo == (1, 2)\n    assert model(foo=(3, 4)).foo == (3, 4)\n\n\ndef test_create_model_tuple_3():\n    with pytest.raises(PydanticUserError, match=r'^Field definitions should be a `\\(<type>, <default>\\)`\\.\\n'):\n        create_model('FooModel', foo=(Tuple[int, int], (1, 2), 'more'))\n\n\ndef test_create_model_protected_namespace_default():\n    with pytest.warns(UserWarning, match='Field \"model_prefixed_field\" has conflict with protected namespace \"model_\"'):\n        create_model('Model', model_prefixed_field=(str, ...))\n\n\ndef test_create_model_protected_namespace_real_conflict():\n    with pytest.raises(NameError, match='Field \"model_dump\" conflicts with member .* of protected namespace \"model_\"'):\n        create_model('Model', model_dump=(str, ...))\n\n\ndef test_create_model_custom_protected_namespace():\n    with pytest.warns(UserWarning, match='Field \"test_field\" has conflict with protected namespace \"test_\"'):\n        create_model(\n            'Model',\n            __config__=ConfigDict(protected_namespaces=('test_',)),\n            model_prefixed_field=(str, ...),\n            test_field=(str, ...),\n        )\n\n\ndef test_create_model_multiple_protected_namespace():\n    with pytest.warns(\n        UserWarning, match='Field \"also_protect_field\" has conflict with protected namespace \"also_protect_\"'\n    ):\n        create_model(\n            'Model',\n            __config__=ConfigDict(protected_namespaces=('protect_me_', 'also_protect_')),\n            also_protect_field=(str, ...),\n        )\n\n\ndef test_json_schema_with_inner_models_with_duplicate_names():\n    model_a = create_model(\n        'a',\n        inner=(str, ...),\n    )\n    model_b = create_model(\n        'a',\n        outer=(model_a, ...),\n    )\n    assert model_b.model_json_schema() == {\n        '$defs': {\n            'a': {\n                'properties': {'inner': {'title': 'Inner', 'type': 'string'}},\n                'required': ['inner'],\n                'title': 'a',\n                'type': 'object',\n            }\n        },\n        'properties': {'outer': {'$ref': '#/$defs/a'}},\n        'required': ['outer'],\n        'title': 'a',\n        'type': 'object',\n    }\n\n\ndef test_resolving_forward_refs_across_modules(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\\\nfrom __future__ import annotations\nfrom dataclasses import dataclass\nfrom pydantic import BaseModel\n\nclass X(BaseModel):\n    pass\n\n@dataclass\nclass Y:\n    x: X\n        \"\"\"\n    )\n    Z = create_model('Z', y=(module.Y, ...))\n    assert Z(y={'x': {}}).y is not None\n\n\ndef test_type_field_in_the_same_module():\n    class A:\n        pass\n\n    B = create_model('B', a_cls=(type, A))\n    b = B()\n    assert b.a_cls == A\n", "tests/test_migration.py": "import importlib\n\nimport pytest\n\nfrom pydantic._migration import DEPRECATED_MOVED_IN_V2, MOVED_IN_V2, REDIRECT_TO_V1, REMOVED_IN_V2, getattr_migration\nfrom pydantic.errors import PydanticImportError\n\n\ndef import_from(dotted_path: str):\n    if ':' in dotted_path:\n        module, obj_name = dotted_path.rsplit(':', 1)\n        module = importlib.import_module(module)\n        return getattr(module, obj_name)\n    else:\n        return importlib.import_module(dotted_path)\n\n\n@pytest.mark.filterwarnings('ignore::UserWarning')\n@pytest.mark.parametrize('module', MOVED_IN_V2.keys())\ndef test_moved_on_v2(module: str):\n    import_from(module)\n\n\n@pytest.mark.parametrize('module', DEPRECATED_MOVED_IN_V2.keys())\ndef test_moved_but_not_warn_on_v2(module: str):\n    import_from(module)\n\n\n@pytest.mark.filterwarnings('ignore::UserWarning')\n@pytest.mark.parametrize('module', REDIRECT_TO_V1.keys())\ndef test_redirect_to_v1(module: str):\n    import_from(module)\n\n\n@pytest.mark.parametrize('module', REMOVED_IN_V2)\ndef test_removed_on_v2(module: str):\n    with pytest.raises(PydanticImportError, match=f'`{module}` has been removed in V2.'):\n        import_from(module)\n        assert False, f'{module} should not be importable'\n\n\ndef test_base_settings_removed():\n    with pytest.raises(PydanticImportError, match='`BaseSettings` has been moved to the `pydantic-settings` package. '):\n        import_from('pydantic:BaseSettings')\n        assert False, 'pydantic:BaseSettings should not be importable'\n\n\ndef test_getattr_migration():\n    get_attr = getattr_migration(__name__)\n\n    assert callable(get_attr('test_getattr_migration')) is True\n\n    with pytest.raises(AttributeError, match=\"module 'tests.test_migration' has no attribute 'foo'\"):\n        get_attr('foo')\n", "tests/test_computed_fields.py": "import random\nimport sys\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Callable, ClassVar, Generic, List, Tuple, TypeVar\n\nimport pytest\nfrom pydantic_core import ValidationError, core_schema\nfrom typing_extensions import TypedDict\n\nfrom pydantic import (\n    BaseModel,\n    Field,\n    GetCoreSchemaHandler,\n    PrivateAttr,\n    TypeAdapter,\n    computed_field,\n    dataclasses,\n    field_serializer,\n    field_validator,\n)\nfrom pydantic.alias_generators import to_camel\nfrom pydantic.errors import PydanticUserError\n\ntry:\n    from functools import cached_property, lru_cache, singledispatchmethod\nexcept ImportError:\n    cached_property = None\n    lru_cache = None\n    singledispatchmethod = None\n\n\ndef test_computed_fields_get():\n    class Rectangle(BaseModel):\n        width: int\n        length: int\n\n        @computed_field\n        def area(self) -> int:\n            \"\"\"An awesome area\"\"\"\n            return self.width * self.length\n\n        @computed_field(title='Pikarea', description='Another area')\n        @property\n        def area2(self) -> int:\n            return self.width * self.length\n\n        @property\n        def double_width(self) -> int:\n            return self.width * 2\n\n    rect = Rectangle(width=10, length=5)\n    assert set(rect.model_fields) == {'width', 'length'}\n    assert set(rect.model_computed_fields) == {'area', 'area2'}\n    assert rect.__dict__ == {'width': 10, 'length': 5}\n\n    assert rect.model_computed_fields['area'].description == 'An awesome area'\n    assert rect.model_computed_fields['area2'].title == 'Pikarea'\n    assert rect.model_computed_fields['area2'].description == 'Another area'\n\n    assert rect.area == 50\n    assert rect.double_width == 20\n    assert rect.model_dump() == {'width': 10, 'length': 5, 'area': 50, 'area2': 50}\n    assert rect.model_dump_json() == '{\"width\":10,\"length\":5,\"area\":50,\"area2\":50}'\n\n    assert set(Rectangle.model_fields) == {'width', 'length'}\n    assert set(Rectangle.model_computed_fields) == {'area', 'area2'}\n\n    assert Rectangle.model_computed_fields['area'].description == 'An awesome area'\n    assert Rectangle.model_computed_fields['area2'].title == 'Pikarea'\n    assert Rectangle.model_computed_fields['area2'].description == 'Another area'\n\n\ndef test_computed_fields_json_schema():\n    class Rectangle(BaseModel):\n        width: int\n        length: int\n\n        @computed_field\n        def area(self) -> int:\n            \"\"\"An awesome area\"\"\"\n            return self.width * self.length\n\n        @computed_field(\n            title='Pikarea',\n            description='Another area',\n            examples=[100, 200],\n            json_schema_extra={'foo': 42},\n        )\n        @property\n        def area2(self) -> int:\n            return self.width * self.length\n\n        @property\n        def double_width(self) -> int:\n            return self.width * 2\n\n    assert Rectangle.model_json_schema(mode='serialization') == {\n        'title': 'Rectangle',\n        'type': 'object',\n        'properties': {\n            'width': {\n                'title': 'Width',\n                'type': 'integer',\n            },\n            'length': {\n                'title': 'Length',\n                'type': 'integer',\n            },\n            'area': {\n                'title': 'Area',\n                'description': 'An awesome area',\n                'type': 'integer',\n                'readOnly': True,\n            },\n            'area2': {\n                'title': 'Pikarea',\n                'description': 'Another area',\n                'examples': [100, 200],\n                'foo': 42,\n                'type': 'integer',\n                'readOnly': True,\n            },\n        },\n        'required': ['width', 'length', 'area', 'area2'],\n    }\n\n\ndef test_computed_fields_set():\n    class Square(BaseModel):\n        side: float\n\n        @computed_field\n        @property\n        def area(self) -> float:\n            return self.side**2\n\n        @computed_field\n        @property\n        def area_string(self) -> str:\n            return f'{self.area} square units'\n\n        @field_serializer('area_string')\n        def serialize_area_string(self, area_string):\n            return area_string.upper()\n\n        @area.setter\n        def area(self, new_area: int):\n            self.side = new_area**0.5\n\n    s = Square(side=10)\n    assert s.model_dump() == {'side': 10.0, 'area': 100.0, 'area_string': '100.0 SQUARE UNITS'}\n    s.area = 64\n    assert s.model_dump() == {'side': 8.0, 'area': 64.0, 'area_string': '64.0 SQUARE UNITS'}\n\n\ndef test_computed_fields_del():\n    class User(BaseModel):\n        first: str\n        last: str\n\n        @computed_field\n        def fullname(self) -> str:\n            return f'{self.first} {self.last}'\n\n        @fullname.setter\n        def fullname(self, new_fullname: str) -> None:\n            self.first, self.last = new_fullname.split()\n\n        @fullname.deleter\n        def fullname(self):\n            self.first = ''\n            self.last = ''\n\n    user = User(first='John', last='Smith')\n    assert user.model_dump() == {'first': 'John', 'last': 'Smith', 'fullname': 'John Smith'}\n    user.fullname = 'Pika Chu'\n    assert user.model_dump() == {'first': 'Pika', 'last': 'Chu', 'fullname': 'Pika Chu'}\n    del user.fullname\n    assert user.model_dump() == {'first': '', 'last': '', 'fullname': ' '}\n\n\n@pytest.mark.skipif(cached_property is None, reason='cached_property not available')\ndef test_cached_property():\n    class Model(BaseModel):\n        minimum: int = Field(alias='min')\n        maximum: int = Field(alias='max')\n\n        @computed_field(alias='the magic number')\n        @cached_property\n        def random_number(self) -> int:\n            \"\"\"An awesome area\"\"\"\n            return random.randint(self.minimum, self.maximum)\n\n        @cached_property\n        def cached_property_2(self) -> int:\n            return 42\n\n        @cached_property\n        def _cached_property_3(self) -> int:\n            return 43\n\n    rect = Model(min=10, max=10_000)\n    assert rect.__private_attributes__ == {}\n    assert rect.cached_property_2 == 42\n    assert rect._cached_property_3 == 43\n    first_n = rect.random_number\n    second_n = rect.random_number\n    assert first_n == second_n\n    assert rect.model_dump() == {'minimum': 10, 'maximum': 10_000, 'random_number': first_n}\n    assert rect.model_dump(by_alias=True) == {'min': 10, 'max': 10_000, 'the magic number': first_n}\n    assert rect.model_dump(by_alias=True, exclude={'random_number'}) == {'min': 10, 'max': 10000}\n\n\ndef test_properties_and_computed_fields():\n    class Model(BaseModel):\n        x: str\n        _private_float: float = PrivateAttr(0)\n\n        @property\n        def public_int(self) -> int:\n            return int(self._private_float)\n\n        @public_int.setter\n        def public_int(self, v: float) -> None:\n            self._private_float = v\n\n        @computed_field\n        @property\n        def public_str(self) -> str:\n            return f'public {self.public_int}'\n\n    m = Model(x='pika')\n    assert m.model_dump() == {'x': 'pika', 'public_str': 'public 0'}\n    m._private_float = 3.1\n    assert m.model_dump() == {'x': 'pika', 'public_str': 'public 3'}\n    m.public_int = 2\n    assert m._private_float == 2.0\n    assert m.model_dump() == {'x': 'pika', 'public_str': 'public 2'}\n\n\ndef test_computed_fields_repr():\n    class Model(BaseModel):\n        x: int\n\n        @computed_field(repr=False)\n        @property\n        def double(self) -> int:\n            return self.x * 2\n\n        @computed_field  # repr=True by default\n        @property\n        def triple(self) -> int:\n            return self.x * 3\n\n    assert repr(Model(x=2)) == 'Model(x=2, triple=6)'\n\n\n@pytest.mark.skipif(singledispatchmethod is None, reason='singledispatchmethod not available')\ndef test_functools():\n    class Model(BaseModel, frozen=True):\n        x: int\n\n        @lru_cache\n        def x_pow(self, p):\n            return self.x**p\n\n        @singledispatchmethod\n        def neg(self, arg):\n            raise NotImplementedError('Cannot negate a')\n\n        @neg.register\n        def _(self, arg: int):\n            return -arg\n\n        @neg.register\n        def _(self, arg: bool):\n            return not arg\n\n    m = Model(x=2)\n    assert m.x_pow(1) == 2\n    assert m.x_pow(2) == 4\n    assert m.neg(1) == -1\n    assert m.neg(True) is False\n\n\ndef test_include_exclude():\n    class Model(BaseModel):\n        x: int\n        y: int\n\n        @computed_field\n        def x_list(self) -> List[int]:\n            return [self.x, self.x + 1]\n\n        @computed_field\n        def y_list(self) -> List[int]:\n            return [self.y, self.y + 1, self.y + 2]\n\n    m = Model(x=1, y=2)\n    assert m.model_dump() == {'x': 1, 'y': 2, 'x_list': [1, 2], 'y_list': [2, 3, 4]}\n    assert m.model_dump(include={'x'}) == {'x': 1}\n    assert m.model_dump(include={'x': None, 'x_list': {0}}) == {'x': 1, 'x_list': [1]}\n    assert m.model_dump(exclude={'x': ..., 'y_list': {2}}) == {'y': 2, 'x_list': [1, 2], 'y_list': [2, 3]}\n\n\ndef test_exclude_none():\n    class Model(BaseModel):\n        x: int\n        y: int\n\n        @computed_field\n        def sum(self) -> int:\n            return self.x + self.y\n\n        @computed_field\n        def none(self) -> None:\n            return None\n\n    m = Model(x=1, y=2)\n    assert m.model_dump(exclude_none=False) == {'x': 1, 'y': 2, 'sum': 3, 'none': None}\n    assert m.model_dump(exclude_none=True) == {'x': 1, 'y': 2, 'sum': 3}\n    assert m.model_dump(mode='json', exclude_none=False) == {'x': 1, 'y': 2, 'sum': 3, 'none': None}\n    assert m.model_dump(mode='json', exclude_none=True) == {'x': 1, 'y': 2, 'sum': 3}\n\n\ndef test_expected_type():\n    class Model(BaseModel):\n        x: int\n        y: int\n\n        @computed_field\n        def x_list(self) -> List[int]:\n            return [self.x, self.x + 1]\n\n        @computed_field\n        def y_str(self) -> bytes:\n            s = f'y={self.y}'\n            return s.encode()\n\n    m = Model(x=1, y=2)\n    assert m.model_dump() == {'x': 1, 'y': 2, 'x_list': [1, 2], 'y_str': b'y=2'}\n    assert m.model_dump(mode='json') == {'x': 1, 'y': 2, 'x_list': [1, 2], 'y_str': 'y=2'}\n    assert m.model_dump_json() == '{\"x\":1,\"y\":2,\"x_list\":[1,2],\"y_str\":\"y=2\"}'\n\n\ndef test_expected_type_wrong():\n    class Model(BaseModel):\n        x: int\n\n        @computed_field\n        def x_list(self) -> List[int]:\n            return 'not a list'\n\n    m = Model(x=1)\n    with pytest.warns(UserWarning, match=r'Expected `list\\[int\\]` but got `str`'):\n        m.model_dump()\n    with pytest.warns(UserWarning, match=r'Expected `list\\[int\\]` but got `str`'):\n        m.model_dump(mode='json')\n    with pytest.warns(UserWarning, match=r'Expected `list\\[int\\]` but got `str`'):\n        m.model_dump_json()\n\n\ndef test_inheritance():\n    class Base(BaseModel):\n        x: int\n\n        @computed_field\n        def double(self) -> int:\n            return self.x * 2\n\n    class Child(Base):\n        y: int\n\n        @computed_field\n        def triple(self) -> int:\n            return self.y * 3\n\n    c = Child(x=2, y=3)\n    assert c.double == 4\n    assert c.triple == 9\n    assert c.model_dump() == {'x': 2, 'y': 3, 'double': 4, 'triple': 9}\n\n\ndef test_dataclass():\n    @dataclasses.dataclass\n    class MyDataClass:\n        x: int\n\n        @computed_field\n        def double(self) -> int:\n            return self.x * 2\n\n    m = MyDataClass(x=2)\n    assert m.double == 4\n    assert TypeAdapter(MyDataClass).dump_python(m) == {'x': 2, 'double': 4}\n\n\ndef test_free_function():\n    @property\n    def double_func(self) -> int:\n        return self.x * 2\n\n    class MyModel(BaseModel):\n        x: int\n        double = computed_field(double_func)\n\n    m = MyModel(x=2)\n    assert set(m.model_fields) == {'x'}\n    assert m.__private_attributes__ == {}\n    assert m.double == 4\n    assert repr(m) == 'MyModel(x=2, double=4)'\n    assert m.model_dump() == {'x': 2, 'double': 4}\n\n\ndef test_private_computed_field():\n    class MyModel(BaseModel):\n        x: int\n\n        @computed_field(repr=True)\n        def _double(self) -> int:\n            return self.x * 2\n\n    m = MyModel(x=2)\n    assert repr(m) == 'MyModel(x=2, _double=4)'\n    assert m.__private_attributes__ == {}\n    assert m._double == 4\n    assert m.model_dump() == {'x': 2, '_double': 4}\n\n\n@pytest.mark.skipif(sys.version_info < (3, 9), reason='@computed_field @classmethod @property only works in 3.9+')\ndef test_classmethod():\n    class MyModel(BaseModel):\n        x: int\n        y: ClassVar[int] = 4\n\n        @computed_field\n        @classmethod\n        @property\n        def two_y(cls) -> int:\n            return cls.y * 2\n\n    m = MyModel(x=1)\n    assert m.two_y == 8\n    assert m.model_dump() == {'x': 1, 'two_y': 8}\n\n\ndef test_frozen():\n    class Square(BaseModel, frozen=True):\n        side: float\n\n        @computed_field\n        @property\n        def area(self) -> float:\n            return self.side**2\n\n        @area.setter\n        def area(self, new_area: int):\n            self.side = new_area**0.5\n\n    m = Square(side=4)\n    assert m.area == 16.0\n    assert m.model_dump() == {'side': 4.0, 'area': 16.0}\n\n    with pytest.raises(ValidationError) as exc_info:\n        m.area = 4\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': ('area',), 'msg': 'Instance is frozen', 'input': 4}\n    ]\n\n\ndef test_validate_assignment():\n    class Square(BaseModel, validate_assignment=True):\n        side: float\n\n        @field_validator('side')\n        def small_side(cls, s):\n            if s < 2:\n                raise ValueError('must be >=2')\n            return float(round(s))\n\n        @computed_field\n        @property\n        def area(self) -> float:\n            return self.side**2\n\n        @area.setter\n        def area(self, new_area: int):\n            self.side = new_area**0.5\n\n    with pytest.raises(ValidationError, match=r'side\\s+Value error, must be >=2'):\n        Square(side=1)\n\n    m = Square(side=4.0)\n    assert m.area == 16.0\n    assert m.model_dump() == {'side': 4.0, 'area': 16.0}\n    m.area = 10.0\n    assert m.side == 3.0\n\n    with pytest.raises(ValidationError, match=r'side\\s+Value error, must be >=2'):\n        m.area = 3\n\n\ndef test_abstractmethod():\n    class AbstractSquare(BaseModel):\n        side: float\n\n        @computed_field\n        @property\n        @abstractmethod\n        def area(self) -> float:\n            raise NotImplementedError()\n\n    class Square(AbstractSquare):\n        @computed_field\n        @property\n        def area(self) -> float:\n            return self.side + 1\n\n    m = Square(side=4.0)\n    assert m.model_dump() == {'side': 4.0, 'area': 5.0}\n\n\n@pytest.mark.skipif(sys.version_info < (3, 12), reason='error message is different on older versions')\n@pytest.mark.parametrize(\n    'bases',\n    [\n        (BaseModel, ABC),\n        (ABC, BaseModel),\n        (BaseModel,),\n    ],\n)\ndef test_abstractmethod_missing(bases: Tuple[Any, ...]):\n    class AbstractSquare(*bases):\n        side: float\n\n        @computed_field\n        @property\n        @abstractmethod\n        def area(self) -> float:\n            raise NotImplementedError()\n\n    class Square(AbstractSquare):\n        pass\n\n    with pytest.raises(\n        TypeError, match=\"Can't instantiate abstract class Square without an implementation for abstract method 'area'\"\n    ):\n        Square(side=4.0)\n\n\nclass CustomType(str):\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(str)\n        schema['serialization'] = core_schema.plain_serializer_function_ser_schema(lambda x: '123')\n        return schema\n\n\ndef test_computed_fields_infer_return_type():\n    class Model(BaseModel):\n        @computed_field\n        def cfield(self) -> CustomType:\n            return CustomType('abc')\n\n    assert Model().model_dump() == {'cfield': '123'}\n    assert Model().model_dump_json() == '{\"cfield\":\"123\"}'\n\n\ndef test_computed_fields_missing_return_type():\n    with pytest.raises(PydanticUserError, match='Computed field is missing return type annotation'):\n\n        class _Model(BaseModel):\n            @computed_field\n            def cfield(self):\n                raise NotImplementedError\n\n    class Model(BaseModel):\n        @computed_field(return_type=CustomType)\n        def cfield(self):\n            return CustomType('abc')\n\n    assert Model().model_dump() == {'cfield': '123'}\n    assert Model().model_dump_json() == '{\"cfield\":\"123\"}'\n\n\ndef test_alias_generator():\n    class MyModel(BaseModel):\n        my_standard_field: int\n\n        @computed_field  # *will* be overridden by alias generator\n        @property\n        def my_computed_field(self) -> int:\n            return self.my_standard_field + 1\n\n        @computed_field(alias='my_alias_none')  # will *not* be overridden by alias generator\n        @property\n        def my_aliased_computed_field_none(self) -> int:\n            return self.my_standard_field + 2\n\n        @computed_field(alias='my_alias_1', alias_priority=1)  # *will* be overridden by alias generator\n        @property\n        def my_aliased_computed_field_1(self) -> int:\n            return self.my_standard_field + 3\n\n        @computed_field(alias='my_alias_2', alias_priority=2)  # will *not* be overridden by alias generator\n        @property\n        def my_aliased_computed_field_2(self) -> int:\n            return self.my_standard_field + 4\n\n    class MySubModel(MyModel):\n        model_config = dict(alias_generator=to_camel, populate_by_name=True)\n\n    model = MyModel(my_standard_field=1)\n    assert model.model_dump() == {\n        'my_standard_field': 1,\n        'my_computed_field': 2,\n        'my_aliased_computed_field_none': 3,\n        'my_aliased_computed_field_1': 4,\n        'my_aliased_computed_field_2': 5,\n    }\n    assert model.model_dump(by_alias=True) == {\n        'my_standard_field': 1,\n        'my_computed_field': 2,\n        'my_alias_none': 3,\n        'my_alias_1': 4,\n        'my_alias_2': 5,\n    }\n\n    submodel = MySubModel(my_standard_field=1)\n    assert submodel.model_dump() == {\n        'my_standard_field': 1,\n        'my_computed_field': 2,\n        'my_aliased_computed_field_none': 3,\n        'my_aliased_computed_field_1': 4,\n        'my_aliased_computed_field_2': 5,\n    }\n    assert submodel.model_dump(by_alias=True) == {\n        'myStandardField': 1,\n        'myComputedField': 2,\n        'my_alias_none': 3,\n        'myAliasedComputedField1': 4,\n        'my_alias_2': 5,\n    }\n\n\ndef make_base_model() -> Any:\n    class CompModel(BaseModel):\n        pass\n\n    class Model(BaseModel):\n        @computed_field\n        @property\n        def comp_1(self) -> CompModel:\n            return CompModel()\n\n        @computed_field\n        @property\n        def comp_2(self) -> CompModel:\n            return CompModel()\n\n    return Model\n\n\ndef make_dataclass() -> Any:\n    class CompModel(BaseModel):\n        pass\n\n    @dataclasses.dataclass\n    class Model:\n        @computed_field\n        @property\n        def comp_1(self) -> CompModel:\n            return CompModel()\n\n        @computed_field\n        @property\n        def comp_2(self) -> CompModel:\n            return CompModel()\n\n    return Model\n\n\ndef make_typed_dict() -> Any:\n    class CompModel(BaseModel):\n        pass\n\n    class Model(TypedDict):\n        @computed_field  # type: ignore\n        @property\n        def comp_1(self) -> CompModel:\n            return CompModel()\n\n        @computed_field  # type: ignore\n        @property\n        def comp_2(self) -> CompModel:\n            return CompModel()\n\n    return Model\n\n\n@pytest.mark.parametrize(\n    'model_factory',\n    [\n        make_base_model,\n        pytest.param(\n            make_typed_dict,\n            marks=pytest.mark.xfail(\n                reason='computed fields do not work with TypedDict yet. See https://github.com/pydantic/pydantic-core/issues/657'\n            ),\n        ),\n        make_dataclass,\n    ],\n)\ndef test_multiple_references_to_schema(model_factory: Callable[[], Any]) -> None:\n    \"\"\"\n    https://github.com/pydantic/pydantic/issues/5980\n    \"\"\"\n\n    model = model_factory()\n\n    ta = TypeAdapter(model)\n\n    assert ta.dump_python(model()) == {'comp_1': {}, 'comp_2': {}}\n\n    assert ta.json_schema() == {'type': 'object', 'properties': {}, 'title': 'Model'}\n\n    assert ta.json_schema(mode='serialization') == {\n        '$defs': {'CompModel': {'properties': {}, 'title': 'CompModel', 'type': 'object'}},\n        'properties': {\n            'comp_1': {'allOf': [{'$ref': '#/$defs/CompModel'}], 'readOnly': True},\n            'comp_2': {'allOf': [{'$ref': '#/$defs/CompModel'}], 'readOnly': True},\n        },\n        'required': ['comp_1', 'comp_2'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_generic_computed_field():\n    T = TypeVar('T')\n\n    class A(BaseModel, Generic[T]):\n        x: T\n\n        @computed_field\n        @property\n        def double_x(self) -> T:\n            return self.x * 2\n\n    assert A[int](x=1).model_dump() == {'x': 1, 'double_x': 2}\n    assert A[str](x='abc').model_dump() == {'x': 'abc', 'double_x': 'abcabc'}\n\n    assert A(x='xxxxxx').model_computed_fields['double_x'].return_type == T\n    assert A[int](x=123).model_computed_fields['double_x'].return_type == int\n    assert A[str](x='x').model_computed_fields['double_x'].return_type == str\n\n    class B(BaseModel, Generic[T]):\n        @computed_field\n        @property\n        def double_x(self) -> T:\n            return 'abc'  # this may not match the annotated return type, and will warn if not\n\n    with pytest.warns(UserWarning, match='Expected `int` but got `str` - serialized value may not be as expected'):\n        B[int]().model_dump()\n\n\ndef test_computed_field_override_raises():\n    class Model(BaseModel):\n        name: str = 'foo'\n\n    with pytest.raises(ValueError, match=\"you can't override a field with a computed field\"):\n\n        class SubModel(Model):\n            @computed_field\n            @property\n            def name(self) -> str:\n                return 'bar'\n\n\n@pytest.mark.skip(reason='waiting on next pydantic-core version, right now, causes a recursion error')\ndef test_computed_field_excluded_from_model_dump_recursive() -> None:\n    # see https://github.com/pydantic/pydantic/issues/9015 for a more contextualized example\n    class Model(BaseModel):\n        bar: int\n\n        @computed_field\n        @property\n        def id(self) -> str:\n            str_obj = self.model_dump_json(exclude={'id'})\n            # you could imagine hashing str_obj, etc. but for simplicity, just wrap it in a descriptive string\n            return f'id: {str_obj}'\n\n    m = Model(bar=42)\n    assert m.model_dump() == {'bar': 42, 'id': 'id: {\"bar\":42}'}\n", "tests/test_validate_call.py": "import asyncio\nimport inspect\nimport re\nfrom datetime import datetime, timezone\nfrom functools import partial\nfrom typing import Any, List, Tuple\n\nimport pytest\nfrom pydantic_core import ArgsKwargs\nfrom typing_extensions import Annotated, TypedDict\n\nfrom pydantic import Field, PydanticInvalidForJsonSchema, TypeAdapter, ValidationError, validate_call\nfrom pydantic.main import BaseModel\n\n\ndef test_args():\n    @validate_call\n    def foo(a: int, b: int):\n        return f'{a}, {b}'\n\n    assert foo(1, 2) == '1, 2'\n    assert foo(*[1, 2]) == '1, 2'\n    assert foo(*(1, 2)) == '1, 2'\n    assert foo(*[1], 2) == '1, 2'\n    assert foo(a=1, b=2) == '1, 2'\n    assert foo(1, b=2) == '1, 2'\n    assert foo(b=2, a=1) == '1, 2'\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo()\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing_argument', 'loc': ('a',), 'msg': 'Missing required argument', 'input': ArgsKwargs(())},\n        {'type': 'missing_argument', 'loc': ('b',), 'msg': 'Missing required argument', 'input': ArgsKwargs(())},\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(1, 'x')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (1,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        }\n    ]\n\n    with pytest.raises(ValidationError, match=r'2\\s+Unexpected positional argument'):\n        foo(1, 2, 3)\n\n    with pytest.raises(ValidationError, match=r'apple\\s+Unexpected keyword argument'):\n        foo(1, 2, apple=3)\n\n    with pytest.raises(ValidationError, match=r'a\\s+Got multiple values for argument'):\n        foo(1, 2, a=3)\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(1, 2, a=3, b=4)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'multiple_argument_values', 'loc': ('a',), 'msg': 'Got multiple values for argument', 'input': 3},\n        {'type': 'multiple_argument_values', 'loc': ('b',), 'msg': 'Got multiple values for argument', 'input': 4},\n    ]\n\n\ndef test_optional():\n    @validate_call\n    def foo_bar(a: int = None):\n        return f'a={a}'\n\n    assert foo_bar() == 'a=None'\n    assert foo_bar(1) == 'a=1'\n    with pytest.raises(ValidationError) as exc_info:\n        foo_bar(None)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': (0,), 'msg': 'Input should be a valid integer', 'input': None}\n    ]\n\n\ndef test_wrap():\n    @validate_call\n    def foo_bar(a: int, b: int):\n        \"\"\"This is the foo_bar method.\"\"\"\n        return f'{a}, {b}'\n\n    assert foo_bar.__doc__ == 'This is the foo_bar method.'\n    assert foo_bar.__name__ == 'foo_bar'\n    assert foo_bar.__module__ == 'tests.test_validate_call'\n    assert foo_bar.__qualname__ == 'test_wrap.<locals>.foo_bar'\n    assert callable(foo_bar.raw_function)\n    assert repr(inspect.signature(foo_bar)) == '<Signature (a: int, b: int)>'\n\n\ndef test_kwargs():\n    @validate_call\n    def foo(*, a: int, b: int):\n        return a + b\n\n    assert foo(a=1, b=3) == 4\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(a=1, b='x')\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'x',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(1, 'x')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing_keyword_only_argument',\n            'loc': ('a',),\n            'msg': 'Missing required keyword only argument',\n            'input': ArgsKwargs((1, 'x')),\n        },\n        {\n            'type': 'missing_keyword_only_argument',\n            'loc': ('b',),\n            'msg': 'Missing required keyword only argument',\n            'input': ArgsKwargs((1, 'x')),\n        },\n        {'type': 'unexpected_positional_argument', 'loc': (0,), 'msg': 'Unexpected positional argument', 'input': 1},\n        {'type': 'unexpected_positional_argument', 'loc': (1,), 'msg': 'Unexpected positional argument', 'input': 'x'},\n    ]\n\n\ndef test_untyped():\n    @validate_call\n    def foo(a, b, c='x', *, d='y'):\n        return ', '.join(str(arg) for arg in [a, b, c, d])\n\n    assert foo(1, 2) == '1, 2, x, y'\n    assert foo(1, {'x': 2}, c='3', d='4') == \"1, {'x': 2}, 3, 4\"\n\n\n@pytest.mark.parametrize('validated', (True, False))\ndef test_var_args_kwargs(validated):\n    def foo(a, b, *args, d=3, **kwargs):\n        return f'a={a!r}, b={b!r}, args={args!r}, d={d!r}, kwargs={kwargs!r}'\n\n    if validated:\n        foo = validate_call(foo)\n\n    assert foo(1, 2) == 'a=1, b=2, args=(), d=3, kwargs={}'\n    assert foo(1, 2, 3, d=4) == 'a=1, b=2, args=(3,), d=4, kwargs={}'\n    assert foo(*[1, 2, 3], d=4) == 'a=1, b=2, args=(3,), d=4, kwargs={}'\n    assert foo(1, 2, args=(10, 11)) == \"a=1, b=2, args=(), d=3, kwargs={'args': (10, 11)}\"\n    assert foo(1, 2, 3, args=(10, 11)) == \"a=1, b=2, args=(3,), d=3, kwargs={'args': (10, 11)}\"\n    assert foo(1, 2, 3, e=10) == \"a=1, b=2, args=(3,), d=3, kwargs={'e': 10}\"\n    assert foo(1, 2, kwargs=4) == \"a=1, b=2, args=(), d=3, kwargs={'kwargs': 4}\"\n    assert foo(1, 2, kwargs=4, e=5) == \"a=1, b=2, args=(), d=3, kwargs={'kwargs': 4, 'e': 5}\"\n\n\ndef test_field_can_provide_factory() -> None:\n    @validate_call\n    def foo(a: int, b: int = Field(default_factory=lambda: 99), *args: int) -> int:\n        \"\"\"mypy is happy with this\"\"\"\n        return a + b + sum(args)\n\n    assert foo(3) == 102\n    assert foo(1, 2, 3) == 6\n\n\ndef test_annotated_field_can_provide_factory() -> None:\n    @validate_call\n    def foo2(a: int, b: Annotated[int, Field(default_factory=lambda: 99)], *args: int) -> int:\n        \"\"\"mypy reports Incompatible default for argument \"b\" if we don't supply ANY as default\"\"\"\n        return a + b + sum(args)\n\n    assert foo2(1) == 100\n\n\ndef test_positional_only(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom pydantic import validate_call\n\n@validate_call\ndef foo(a, b, /, c=None):\n    return f'{a}, {b}, {c}'\n\"\"\"\n    )\n    assert module.foo(1, 2) == '1, 2, None'\n    assert module.foo(1, 2, 44) == '1, 2, 44'\n    assert module.foo(1, 2, c=44) == '1, 2, 44'\n    with pytest.raises(ValidationError) as exc_info:\n        module.foo(1, b=2)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing_positional_only_argument',\n            'loc': (1,),\n            'msg': 'Missing required positional only argument',\n            'input': ArgsKwargs((1,), {'b': 2}),\n        },\n        {'type': 'unexpected_keyword_argument', 'loc': ('b',), 'msg': 'Unexpected keyword argument', 'input': 2},\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        module.foo(a=1, b=2)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing_positional_only_argument',\n            'loc': (0,),\n            'msg': 'Missing required positional only argument',\n            'input': ArgsKwargs((), {'a': 1, 'b': 2}),\n        },\n        {\n            'type': 'missing_positional_only_argument',\n            'loc': (1,),\n            'msg': 'Missing required positional only argument',\n            'input': ArgsKwargs((), {'a': 1, 'b': 2}),\n        },\n        {'type': 'unexpected_keyword_argument', 'loc': ('a',), 'msg': 'Unexpected keyword argument', 'input': 1},\n        {'type': 'unexpected_keyword_argument', 'loc': ('b',), 'msg': 'Unexpected keyword argument', 'input': 2},\n    ]\n\n\ndef test_args_name():\n    @validate_call\n    def foo(args: int, kwargs: int):\n        return f'args={args!r}, kwargs={kwargs!r}'\n\n    assert foo(1, 2) == 'args=1, kwargs=2'\n\n    with pytest.raises(ValidationError, match=r'apple\\s+Unexpected keyword argument'):\n        foo(1, 2, apple=4)\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(1, 2, apple=4, banana=5)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'unexpected_keyword_argument', 'loc': ('apple',), 'msg': 'Unexpected keyword argument', 'input': 4},\n        {'type': 'unexpected_keyword_argument', 'loc': ('banana',), 'msg': 'Unexpected keyword argument', 'input': 5},\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(1, 2, 3)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'unexpected_positional_argument', 'loc': (2,), 'msg': 'Unexpected positional argument', 'input': 3}\n    ]\n\n\ndef test_v_args():\n    @validate_call\n    def foo1(v__args: int):\n        return v__args\n\n    assert foo1(123) == 123\n\n    @validate_call\n    def foo2(v__kwargs: int):\n        return v__kwargs\n\n    assert foo2(123) == 123\n\n    @validate_call\n    def foo3(v__positional_only: int):\n        return v__positional_only\n\n    assert foo3(123) == 123\n\n    @validate_call\n    def foo4(v__duplicate_kwargs: int):\n        return v__duplicate_kwargs\n\n    assert foo4(123) == 123\n\n\ndef test_async():\n    @validate_call\n    async def foo(a, b):\n        return f'a={a} b={b}'\n\n    async def run():\n        v = await foo(1, 2)\n        assert v == 'a=1 b=2'\n\n    asyncio.run(run())\n    with pytest.raises(ValidationError) as exc_info:\n        asyncio.run(foo('x'))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing_argument', 'loc': ('b',), 'msg': 'Missing required argument', 'input': ArgsKwargs(('x',))}\n    ]\n\n\ndef test_string_annotation():\n    @validate_call\n    def foo(a: 'List[int]', b: 'float'):\n        return f'a={a!r} b={b!r}'\n\n    assert foo([1, 2, 3], 22) == 'a=[1, 2, 3] b=22.0'\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(['x'])\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (0, 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        },\n        {'type': 'missing_argument', 'loc': ('b',), 'msg': 'Missing required argument', 'input': ArgsKwargs((['x'],))},\n    ]\n\n\ndef test_local_annotation():\n    ListInt = List[int]\n\n    @validate_call\n    def foo(a: ListInt):\n        return f'a={a!r}'\n\n    assert foo([1, 2, 3]) == 'a=[1, 2, 3]'\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(['x'])\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (0, 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        },\n    ]\n\n\ndef test_item_method():\n    class X:\n        def __init__(self, v):\n            self.v = v\n\n        @validate_call\n        def foo(self, a: int, b: int):\n            assert self.v == a\n            return f'{a}, {b}'\n\n    x = X(4)\n    assert x.foo(4, 2) == '4, 2'\n    assert x.foo(*[4, 2]) == '4, 2'\n\n    with pytest.raises(ValidationError) as exc_info:\n        x.foo()\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing_argument', 'loc': ('a',), 'msg': 'Missing required argument', 'input': ArgsKwargs((x,))},\n        {'type': 'missing_argument', 'loc': ('b',), 'msg': 'Missing required argument', 'input': ArgsKwargs((x,))},\n    ]\n\n\ndef test_class_method():\n    class X:\n        @classmethod\n        @validate_call\n        def foo(cls, a: int, b: int):\n            assert cls == X\n            return f'{a}, {b}'\n\n    x = X()\n    assert x.foo(4, 2) == '4, 2'\n    assert x.foo(*[4, 2]) == '4, 2'\n\n    with pytest.raises(ValidationError) as exc_info:\n        x.foo()\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing_argument', 'loc': ('a',), 'msg': 'Missing required argument', 'input': ArgsKwargs((X,))},\n        {'type': 'missing_argument', 'loc': ('b',), 'msg': 'Missing required argument', 'input': ArgsKwargs((X,))},\n    ]\n\n\ndef test_json_schema():\n    @validate_call\n    def foo(a: int, b: int = None):\n        return f'{a}, {b}'\n\n    assert foo(1, 2) == '1, 2'\n    assert foo(1, b=2) == '1, 2'\n    assert foo(1) == '1, None'\n    assert TypeAdapter(foo).json_schema() == {\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'default': None, 'title': 'B', 'type': 'integer'}},\n        'required': ['a'],\n        'additionalProperties': False,\n    }\n\n    @validate_call\n    def foo(a: int, /, b: int):\n        return f'{a}, {b}'\n\n    assert foo(1, 2) == '1, 2'\n    assert TypeAdapter(foo).json_schema() == {\n        'maxItems': 2,\n        'minItems': 2,\n        'prefixItems': [{'title': 'A', 'type': 'integer'}, {'title': 'B', 'type': 'integer'}],\n        'type': 'array',\n    }\n\n    @validate_call\n    def foo(a: int, /, *, b: int, c: int):\n        return f'{a}, {b}, {c}'\n\n    assert foo(1, b=2, c=3) == '1, 2, 3'\n    with pytest.raises(\n        PydanticInvalidForJsonSchema,\n        match=(\n            'Unable to generate JSON schema for arguments validator ' 'with positional-only and keyword-only arguments'\n        ),\n    ):\n        TypeAdapter(foo).json_schema()\n\n    @validate_call\n    def foo(*numbers: int) -> int:\n        return sum(numbers)\n\n    assert foo(1, 2, 3) == 6\n    assert TypeAdapter(foo).json_schema() == {'items': {'type': 'integer'}, 'prefixItems': [], 'type': 'array'}\n\n    @validate_call\n    def foo(**scores: int) -> str:\n        return ', '.join(f'{k}={v}' for k, v in sorted(scores.items()))\n\n    assert foo(a=1, b=2) == 'a=1, b=2'\n    assert TypeAdapter(foo).json_schema() == {\n        'additionalProperties': {'type': 'integer'},\n        'properties': {},\n        'type': 'object',\n    }\n\n    @validate_call\n    def foo(a: Annotated[int, Field(..., alias='A')]):\n        return a\n\n    assert foo(1) == 1\n    assert TypeAdapter(foo).json_schema() == {\n        'additionalProperties': False,\n        'properties': {'A': {'title': 'A', 'type': 'integer'}},\n        'required': ['A'],\n        'type': 'object',\n    }\n\n\ndef test_alias_generator():\n    @validate_call(config=dict(alias_generator=lambda x: x * 2))\n    def foo(a: int, b: int):\n        return f'{a}, {b}'\n\n    assert foo(1, 2) == '1, 2'\n    assert foo(aa=1, bb=2) == '1, 2'\n\n\ndef test_config_arbitrary_types_allowed():\n    class EggBox:\n        def __str__(self) -> str:\n            return 'EggBox()'\n\n    @validate_call(config=dict(arbitrary_types_allowed=True))\n    def foo(a: int, b: EggBox):\n        return f'{a}, {b}'\n\n    assert foo(1, EggBox()) == '1, EggBox()'\n    with pytest.raises(ValidationError) as exc_info:\n        assert foo(1, 2) == '1, 2'\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': (1,),\n            'msg': 'Input should be an instance of test_config_arbitrary_types_allowed.<locals>.EggBox',\n            'input': 2,\n            'ctx': {'class': 'test_config_arbitrary_types_allowed.<locals>.EggBox'},\n        }\n    ]\n\n\ndef test_config_strict():\n    @validate_call(config=dict(strict=True))\n    def foo(a: int, b: List[str]):\n        return f'{a}, {b[0]}'\n\n    assert foo(1, ['bar', 'foobar']) == '1, bar'\n    with pytest.raises(ValidationError) as exc_info:\n        foo('foo', ('bar', 'foobar'))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': (0,), 'msg': 'Input should be a valid integer', 'input': 'foo'},\n        {'type': 'list_type', 'loc': (1,), 'msg': 'Input should be a valid list', 'input': ('bar', 'foobar')},\n    ]\n\n\ndef test_annotated_use_of_alias():\n    @validate_call\n    def foo(a: Annotated[int, Field(alias='b')], c: Annotated[int, Field()], d: Annotated[int, Field(alias='')]):\n        return a + c + d\n\n    assert foo(**{'b': 10, 'c': 12, '': 1}) == 23\n\n    with pytest.raises(ValidationError) as exc_info:\n        assert foo(a=10, c=12, d=1) == 10\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing_argument',\n            'loc': ('b',),\n            'msg': 'Missing required argument',\n            'input': ArgsKwargs((), {'a': 10, 'c': 12, 'd': 1}),\n        },\n        {\n            'type': 'missing_argument',\n            'loc': ('',),\n            'msg': 'Missing required argument',\n            'input': ArgsKwargs((), {'a': 10, 'c': 12, 'd': 1}),\n        },\n        {'type': 'unexpected_keyword_argument', 'loc': ('a',), 'msg': 'Unexpected keyword argument', 'input': 10},\n        {'type': 'unexpected_keyword_argument', 'loc': ('d',), 'msg': 'Unexpected keyword argument', 'input': 1},\n    ]\n\n\ndef test_use_of_alias():\n    @validate_call\n    def foo(c: int = Field(default_factory=lambda: 20), a: int = Field(default_factory=lambda: 10, alias='b')):\n        return a + c\n\n    assert foo(b=10) == 30\n\n\ndef test_populate_by_name():\n    @validate_call(config=dict(populate_by_name=True))\n    def foo(a: Annotated[int, Field(alias='b')], c: Annotated[int, Field(alias='d')]):\n        return a + c\n\n    assert foo(b=10, d=1) == 11\n    assert foo(a=10, d=1) == 11\n    assert foo(b=10, c=1) == 11\n    assert foo(a=10, c=1) == 11\n\n\ndef test_validate_return():\n    @validate_call(config=dict(validate_return=True))\n    def foo(a: int, b: int) -> int:\n        return a + b\n\n    assert foo(1, 2) == 3\n\n\ndef test_validate_all():\n    @validate_call(config=dict(validate_default=True))\n    def foo(dt: datetime = Field(default_factory=lambda: 946684800)):\n        return dt\n\n    assert foo() == datetime(2000, 1, 1, tzinfo=timezone.utc)\n    assert foo(0) == datetime(1970, 1, 1, tzinfo=timezone.utc)\n\n\ndef test_validate_all_positional(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom datetime import datetime\n\nfrom pydantic import Field, validate_call\n\n@validate_call(config=dict(validate_default=True))\ndef foo(dt: datetime = Field(default_factory=lambda: 946684800), /):\n    return dt\n\"\"\"\n    )\n    assert module.foo() == datetime(2000, 1, 1, tzinfo=timezone.utc)\n    assert module.foo(0) == datetime(1970, 1, 1, tzinfo=timezone.utc)\n\n\ndef test_partial():\n    def my_wrapped_function(a: int, b: int, c: int):\n        return a + b + c\n\n    my_partial_function = partial(my_wrapped_function, c=3)\n    f = validate_call(my_partial_function)\n    assert f(1, 2) == 6\n\n\ndef test_validator_init():\n    class Foo:\n        @validate_call\n        def __init__(self, a: int, b: int):\n            self.v = a + b\n\n    assert Foo(1, 2).v == 3\n    assert Foo(1, '2').v == 3\n    with pytest.raises(ValidationError, match=\"type=int_parsing, input_value='x', input_type=str\"):\n        Foo(1, 'x')\n\n\ndef test_positional_and_keyword_with_same_name(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom pydantic import validate_call\n\n@validate_call\ndef f(a: int, /, **kwargs):\n    return a, kwargs\n\"\"\"\n    )\n    assert module.f(1, a=2) == (1, {'a': 2})\n\n\ndef test_model_as_arg() -> None:\n    class Model1(TypedDict):\n        x: int\n\n    class Model2(BaseModel):\n        y: int\n\n    @validate_call(validate_return=True)\n    def f1(m1: Model1, m2: Model2) -> Tuple[Model1, Model2]:\n        return (m1, m2.model_dump())  # type: ignore\n\n    res = f1({'x': '1'}, {'y': '2'})  # type: ignore\n    assert res == ({'x': 1}, Model2(y=2))\n\n\ndef test_do_not_call_repr_on_validate_call() -> None:\n    class Class:\n        @validate_call\n        def __init__(self, number: int) -> None: ...\n\n        def __repr__(self) -> str:\n            assert False\n\n    Class(50)\n\n\ndef test_methods_are_not_rebound():\n    class Thing:\n        def __init__(self, x: int):\n            self.x = x\n\n        def a(self, x: int):\n            return x + self.x\n\n        c = validate_call(a)\n\n    thing = Thing(1)\n    assert thing.a == thing.a\n    assert thing.c == thing.c\n    assert Thing.c == Thing.c\n\n    # Ensure validation is still happening\n    assert Thing.c(thing, '2') == 3\n    assert Thing(2).c('3') == 5\n\n\ndef test_basemodel_method():\n    class Foo(BaseModel):\n        @classmethod\n        @validate_call\n        def test(cls, x: int):\n            return cls, x\n\n    assert Foo.test('1') == (Foo, 1)\n\n    class Bar(BaseModel):\n        @validate_call\n        def test(self, x: int):\n            return self, x\n\n    bar = Bar()\n    assert bar.test('1') == (bar, 1)\n\n\ndef test_dynamic_method_decoration():\n    class Foo:\n        def bar(self, value: str) -> str:\n            return f'bar-{value}'\n\n    Foo.bar = validate_call(Foo.bar)\n    assert Foo.bar\n\n    foo = Foo()\n    assert foo.bar('test') == 'bar-test'\n\n\n@pytest.mark.parametrize('decorator', [staticmethod, classmethod])\ndef test_classmethod_order_error(decorator):\n    name = decorator.__name__\n    with pytest.raises(\n        TypeError,\n        match=re.escape(f'The `@{name}` decorator should be applied after `@validate_call` (put `@{name}` on top)'),\n    ):\n\n        class A:\n            @validate_call\n            @decorator\n            def method(self, x: int):\n                pass\n\n\ndef test_async_func() -> None:\n    @validate_call(validate_return=True)\n    async def foo(a: Any) -> int:\n        return a\n\n    res = asyncio.run(foo(1))\n    assert res == 1\n\n    with pytest.raises(ValidationError) as exc_info:\n        asyncio.run(foo('x'))\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        }\n    ]\n\n\ndef test_validate_call_with_slots() -> None:\n    class ClassWithSlots:\n        __slots__ = {}\n\n        @validate_call(validate_return=True)\n        def some_instance_method(self, x: str) -> str:\n            return x\n\n        @classmethod\n        @validate_call(validate_return=True)\n        def some_class_method(cls, x: str) -> str:\n            return x\n\n        @staticmethod\n        @validate_call(validate_return=True)\n        def some_static_method(x: str) -> str:\n            return x\n\n    c = ClassWithSlots()\n    assert c.some_instance_method(x='potato') == 'potato'\n    assert c.some_class_method(x='pepper') == 'pepper'\n    assert c.some_static_method(x='onion') == 'onion'\n\n    # verify that equality still holds for instance methods\n    assert c.some_instance_method == c.some_instance_method\n    assert c.some_class_method == c.some_class_method\n    assert c.some_static_method == c.some_static_method\n\n\ndef test_eval_type_backport():\n    @validate_call\n    def foo(bar: 'list[int | str]') -> 'list[int | str]':\n        return bar\n\n    assert foo([1, '2']) == [1, '2']\n    with pytest.raises(ValidationError) as exc_info:\n        foo('not a list')  # type: ignore\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'list_type',\n            'loc': (0,),\n            'msg': 'Input should be a valid list',\n            'input': 'not a list',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        foo([{'not a str or int'}])  # type: ignore\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_type',\n            'loc': (0, 0, 'int'),\n            'msg': 'Input should be a valid integer',\n            'input': {'not a str or int'},\n        },\n        {\n            'type': 'string_type',\n            'loc': (0, 0, 'str'),\n            'msg': 'Input should be a valid string',\n            'input': {'not a str or int'},\n        },\n    ]\n", "tests/test_types_namedtuple.py": "from collections import namedtuple\nfrom typing import Generic, NamedTuple, Optional, Tuple, TypeVar\n\nimport pytest\nfrom typing_extensions import NamedTuple as TypingExtensionsNamedTuple\n\nfrom pydantic import BaseModel, ConfigDict, PositiveInt, TypeAdapter, ValidationError\nfrom pydantic.errors import PydanticSchemaGenerationError\n\n\ndef test_namedtuple_simple():\n    Position = namedtuple('Pos', 'x y')\n\n    class Model(BaseModel):\n        pos: Position\n\n    model = Model(pos=('1', 2))\n    assert isinstance(model.pos, Position)\n    assert model.pos.x == '1'\n    assert model.pos == Position('1', 2)\n\n    model = Model(pos={'x': '1', 'y': 2})\n    assert model.pos == Position('1', 2)\n\n\ndef test_namedtuple():\n    class Event(NamedTuple):\n        a: int\n        b: int\n        c: int\n        d: str\n\n    class Model(BaseModel):\n        # pos: Position\n        event: Event\n\n    model = Model(event=(b'1', '2', 3, 'qwe'))\n    assert isinstance(model.event, Event)\n    assert model.event == Event(1, 2, 3, 'qwe')\n    assert repr(model) == \"Model(event=Event(a=1, b=2, c=3, d='qwe'))\"\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(pos=('1', 2), event=['qwe', '2', 3, 'qwe'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('event', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'qwe',\n        }\n    ]\n\n\ndef test_namedtuple_schema():\n    class Position1(NamedTuple):\n        x: int\n        y: int\n\n    Position2 = namedtuple('Position2', 'x y')\n\n    class Model(BaseModel):\n        pos1: Position1\n        pos2: Position2\n        pos3: Tuple[int, int]\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        '$defs': {\n            'Position1': {\n                'maxItems': 2,\n                'minItems': 2,\n                'prefixItems': [{'title': 'X', 'type': 'integer'}, {'title': 'Y', 'type': 'integer'}],\n                'type': 'array',\n            },\n            'Position2': {\n                'maxItems': 2,\n                'minItems': 2,\n                'prefixItems': [{'title': 'X'}, {'title': 'Y'}],\n                'type': 'array',\n            },\n        },\n        'properties': {\n            'pos1': {'$ref': '#/$defs/Position1'},\n            'pos2': {'$ref': '#/$defs/Position2'},\n            'pos3': {\n                'maxItems': 2,\n                'minItems': 2,\n                'prefixItems': [{'type': 'integer'}, {'type': 'integer'}],\n                'title': 'Pos3',\n                'type': 'array',\n            },\n        },\n        'required': ['pos1', 'pos2', 'pos3'],\n    }\n\n\ndef test_namedtuple_right_length():\n    class Point(NamedTuple):\n        x: int\n        y: int\n\n    class Model(BaseModel):\n        p: Point\n\n    assert isinstance(Model(p=(1, 2)), Model)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(p=(1, 2, 3))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'unexpected_positional_argument',\n            'loc': ('p', 2),\n            'msg': 'Unexpected positional argument',\n            'input': 3,\n        }\n    ]\n\n\ndef test_namedtuple_postponed_annotation():\n    \"\"\"\n    https://github.com/pydantic/pydantic/issues/2760\n    \"\"\"\n\n    class Tup(NamedTuple):\n        v: 'PositiveInt'\n\n    class Model(BaseModel):\n        t: Tup\n\n    # The effect of issue #2760 is that this call raises a `PydanticUserError` even though the type declared on `Tup.v`\n    # references a binding in this module's global scope.\n    with pytest.raises(ValidationError):\n        Model.model_validate({'t': [-1]})\n\n\ndef test_namedtuple_arbitrary_type():\n    class CustomClass:\n        pass\n\n    class Tup(NamedTuple):\n        c: CustomClass\n\n    class Model(BaseModel):\n        x: Tup\n\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    data = {'x': Tup(c=CustomClass())}\n    model = Model.model_validate(data)\n    assert isinstance(model.x.c, CustomClass)\n\n    with pytest.raises(PydanticSchemaGenerationError):\n\n        class ModelNoArbitraryTypes(BaseModel):\n            x: Tup\n\n\ndef test_recursive_namedtuple():\n    class MyNamedTuple(NamedTuple):\n        x: int\n        y: Optional['MyNamedTuple']\n\n    ta = TypeAdapter(MyNamedTuple)\n    assert ta.validate_python({'x': 1, 'y': {'x': 2, 'y': None}}) == (1, (2, None))\n\n    with pytest.raises(ValidationError) as exc_info:\n        ta.validate_python({'x': 1, 'y': {'x': 2, 'y': {'x': 'a', 'y': None}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('y', 'y', 'x'),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_recursive_generic_namedtuple():\n    # Need to use TypingExtensionsNamedTuple to make it work with Python <3.11\n    T = TypeVar('T')\n\n    class MyNamedTuple(TypingExtensionsNamedTuple, Generic[T]):\n        x: T\n        y: Optional['MyNamedTuple[T]']\n\n    ta = TypeAdapter(MyNamedTuple[int])\n    assert ta.validate_python({'x': 1, 'y': {'x': 2, 'y': None}}) == (1, (2, None))\n\n    with pytest.raises(ValidationError) as exc_info:\n        ta.validate_python({'x': 1, 'y': {'x': 2, 'y': {'x': 'a', 'y': None}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('y', 'y', 'x'),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_namedtuple_defaults():\n    class NT(NamedTuple):\n        x: int\n        y: int = 33\n\n    assert TypeAdapter(NT).validate_python([1]) == (1, 33)\n    assert TypeAdapter(NT).validate_python({'x': 22}) == (22, 33)\n\n\ndef test_eval_type_backport():\n    class MyNamedTuple(NamedTuple):\n        foo: 'list[int | str]'\n\n    class Model(BaseModel):\n        t: MyNamedTuple\n\n    assert Model(t=([1, '2'],)).model_dump() == {'t': ([1, '2'],)}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(t=('not a list',))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'list_type',\n            'loc': ('t', 0),\n            'msg': 'Input should be a valid list',\n            'input': 'not a list',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        Model(t=([{'not a str or int'}],))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_type',\n            'loc': ('t', 0, 0, 'int'),\n            'msg': 'Input should be a valid integer',\n            'input': {'not a str or int'},\n        },\n        {\n            'type': 'string_type',\n            'loc': ('t', 0, 0, 'str'),\n            'msg': 'Input should be a valid string',\n            'input': {'not a str or int'},\n        },\n    ]\n", "tests/test_networks_ipaddress.py": "import json\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom typing import Any, List\n\nimport pytest\n\nfrom pydantic import BaseModel, IPvAnyAddress, IPvAnyInterface, IPvAnyNetwork, ValidationError\nfrom pydantic.config import ConfigDict\n\n\n@pytest.mark.parametrize(\n    'value,cls',\n    [\n        ('0.0.0.0', IPv4Address),\n        ('1.1.1.1', IPv4Address),\n        ('10.10.10.10', IPv4Address),\n        ('192.168.0.1', IPv4Address),\n        ('255.255.255.255', IPv4Address),\n        ('::1:0:1', IPv6Address),\n        ('ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff', IPv6Address),\n        (b'\\x00\\x00\\x00\\x00', IPv4Address),\n        (b'\\x01\\x01\\x01\\x01', IPv4Address),\n        (b'\\n\\n\\n\\n', IPv4Address),\n        (b'\\xc0\\xa8\\x00\\x01', IPv4Address),\n        (b'\\xff\\xff\\xff\\xff', IPv4Address),\n        (b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01', IPv6Address),\n        (b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff', IPv6Address),\n        (0, IPv4Address),\n        (16_843_009, IPv4Address),\n        (168_430_090, IPv4Address),\n        (3_232_235_521, IPv4Address),\n        (4_294_967_295, IPv4Address),\n        (4_294_967_297, IPv6Address),\n        (340_282_366_920_938_463_463_374_607_431_768_211_455, IPv6Address),\n        (IPv4Address('192.168.0.1'), IPv4Address),\n        (IPv6Address('::1:0:1'), IPv6Address),\n    ],\n)\ndef test_ipaddress_success(value, cls):\n    class Model(BaseModel):\n        ip: IPvAnyAddress\n\n    assert Model(ip=value).ip == cls(value)\n\n\n@pytest.mark.parametrize(\n    'value',\n    [\n        '0.0.0.0',\n        '1.1.1.1',\n        '10.10.10.10',\n        '192.168.0.1',\n        '255.255.255.255',\n        b'\\x00\\x00\\x00\\x00',\n        b'\\x01\\x01\\x01\\x01',\n        b'\\n\\n\\n\\n',\n        b'\\xc0\\xa8\\x00\\x01',\n        b'\\xff\\xff\\xff\\xff',\n        0,\n        16_843_009,\n        168_430_090,\n        3_232_235_521,\n        4_294_967_295,\n        IPv4Address('0.0.0.0'),\n        IPv4Address('1.1.1.1'),\n        IPv4Address('10.10.10.10'),\n        IPv4Address('192.168.0.1'),\n        IPv4Address('255.255.255.255'),\n    ],\n)\ndef test_ipv4address_success(value):\n    class Model(BaseModel):\n        ipv4: IPv4Address\n\n    assert Model(ipv4=value).ipv4 == IPv4Address(value)\n\n\n@pytest.mark.parametrize(\n    'tp,value,errors',\n    [\n        (\n            IPv4Address,\n            IPv4Address('0.0.0.0'),\n            [\n                {\n                    'type': 'is_instance_of',\n                    'loc': ('v',),\n                    'msg': 'Input should be an instance of IPv4Address',\n                    'input': '0.0.0.0',\n                    'ctx': {'class': 'IPv4Address'},\n                }\n            ],\n        ),\n        (\n            IPv4Interface,\n            IPv4Interface('192.168.0.0/24'),\n            [\n                {\n                    'type': 'is_instance_of',\n                    'loc': ('v',),\n                    'msg': 'Input should be an instance of IPv4Interface',\n                    'input': '192.168.0.0/24',\n                    'ctx': {'class': 'IPv4Interface'},\n                }\n            ],\n        ),\n        (\n            IPv4Network,\n            IPv4Network('192.168.0.0/24'),\n            [\n                {\n                    'type': 'is_instance_of',\n                    'loc': ('v',),\n                    'msg': 'Input should be an instance of IPv4Network',\n                    'input': '192.168.0.0/24',\n                    'ctx': {'class': 'IPv4Network'},\n                }\n            ],\n        ),\n        (\n            IPv6Address,\n            IPv6Address('::1:0:1'),\n            [\n                {\n                    'type': 'is_instance_of',\n                    'loc': ('v',),\n                    'msg': 'Input should be an instance of IPv6Address',\n                    'input': '::1:0:1',\n                    'ctx': {'class': 'IPv6Address'},\n                }\n            ],\n        ),\n        (\n            IPv6Interface,\n            IPv6Interface('2001:db00::0/120'),\n            [\n                {\n                    'type': 'is_instance_of',\n                    'loc': ('v',),\n                    'msg': 'Input should be an instance of IPv6Interface',\n                    'input': '2001:db00::/120',\n                    'ctx': {'class': 'IPv6Interface'},\n                }\n            ],\n        ),\n        (\n            IPv6Network,\n            IPv6Network('2001:db00::0/120'),\n            [\n                {\n                    'type': 'is_instance_of',\n                    'loc': ('v',),\n                    'msg': 'Input should be an instance of IPv6Network',\n                    'input': '2001:db00::/120',\n                    'ctx': {'class': 'IPv6Network'},\n                }\n            ],\n        ),\n    ],\n)\ndef test_ip_strict(tp: Any, value: Any, errors: List[Any]) -> None:\n    class Model(BaseModel):\n        v: tp\n\n        model_config = ConfigDict(strict=True)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=str(value))\n    assert exc_info.value.errors(include_url=False) == errors\n\n    assert Model(v=value).v == value\n\n\n@pytest.mark.parametrize(\n    'value',\n    [\n        '::1:0:1',\n        'ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff',\n        b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01',\n        b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff',\n        4_294_967_297,\n        340_282_366_920_938_463_463_374_607_431_768_211_455,\n        IPv6Address('::1:0:1'),\n        IPv6Address('ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff'),\n    ],\n)\ndef test_ipv6address_success(value):\n    class Model(BaseModel):\n        ipv6: IPv6Address\n\n    assert Model(ipv6=value).ipv6 == IPv6Address(value)\n\n\n@pytest.mark.parametrize('value', ['hello,world', '192.168.0.1.1.1', -1, 2**128 + 1])\ndef test_ipaddress_fails(value):\n    class Model(BaseModel):\n        ip: IPvAnyAddress\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(ip=value)\n    assert exc_info.value.error_count() == 1\n    assert exc_info.value.errors(include_url=False)[0] == {\n        'type': 'ip_any_address',\n        'loc': ('ip',),\n        'msg': 'value is not a valid IPv4 or IPv6 address',\n        'input': value,\n    }\n\n\n@pytest.mark.parametrize('value', ['hello,world', '192.168.0.1.1.1', -1, 2**32 + 1, IPv6Address('::0:1:0')])\ndef test_ipv4address_fails(value):\n    class Model(BaseModel):\n        ipv4: IPv4Address\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(ipv4=value)\n    assert exc_info.value.error_count() == 1\n    assert exc_info.value.errors(include_url=False)[0] == {\n        'type': 'ip_v4_address',\n        'loc': ('ipv4',),\n        'msg': 'Input is not a valid IPv4 address',\n        'input': value,\n    }\n\n\n@pytest.mark.parametrize('value', ['hello,world', '192.168.0.1.1.1', -1, 2**128 + 1, IPv4Address('192.168.0.1')])\ndef test_ipv6address_fails(value):\n    class Model(BaseModel):\n        ipv6: IPv6Address\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(ipv6=value)\n    assert exc_info.value.error_count() == 1\n    # insert_assert(exc_info.value.errors(include_url=False)[0])\n    assert exc_info.value.errors(include_url=False)[0] == {\n        'type': 'ip_v6_address',\n        'loc': ('ipv6',),\n        'msg': 'Input is not a valid IPv6 address',\n        'input': value,\n    }\n\n\n@pytest.mark.parametrize(\n    'value,cls',\n    [\n        ('192.168.0.0/24', IPv4Network),\n        ('192.168.128.0/30', IPv4Network),\n        ('2001:db00::0/120', IPv6Network),\n        (2**32 - 1, IPv4Network),  # no mask equals to mask /32\n        (20_282_409_603_651_670_423_947_251_286_015, IPv6Network),  # /128\n        (b'\\xff\\xff\\xff\\xff', IPv4Network),  # /32\n        (b'\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff', IPv6Network),\n        (('192.168.0.0', 24), IPv4Network),\n        (('2001:db00::0', 120), IPv6Network),\n        (IPv4Network('192.168.0.0/24'), IPv4Network),\n    ],\n)\ndef test_ipnetwork_success(value, cls):\n    class Model(BaseModel):\n        ip: IPvAnyNetwork = None\n\n    assert Model(ip=value).ip == cls(value)\n\n\n@pytest.mark.parametrize(\n    'value,cls',\n    [\n        ('192.168.0.0/24', IPv4Network),\n        ('192.168.128.0/30', IPv4Network),\n        (2**32 - 1, IPv4Network),  # no mask equals to mask /32\n        (b'\\xff\\xff\\xff\\xff', IPv4Network),  # /32\n        (('192.168.0.0', 24), IPv4Network),\n        (IPv4Network('192.168.0.0/24'), IPv4Network),\n    ],\n)\ndef test_ip_v4_network_success(value, cls):\n    class Model(BaseModel):\n        ip: IPv4Network = None\n\n    assert Model(ip=value).ip == cls(value)\n\n\n@pytest.mark.parametrize(\n    'value,cls',\n    [\n        ('2001:db00::0/120', IPv6Network),\n        (20_282_409_603_651_670_423_947_251_286_015, IPv6Network),  # /128\n        (b'\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff', IPv6Network),\n        (('2001:db00::0', 120), IPv6Network),\n        (IPv6Network('2001:db00::0/120'), IPv6Network),\n    ],\n)\ndef test_ip_v6_network_success(value, cls):\n    class Model(BaseModel):\n        ip: IPv6Network = None\n\n    assert Model(ip=value).ip == cls(value)\n\n\n@pytest.mark.parametrize('value', ['hello,world', '192.168.0.1.1.1/24', -1, 2**128 + 1])\ndef test_ipnetwork_fails(value):\n    class Model(BaseModel):\n        ip: IPvAnyNetwork = None\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(ip=value)\n    assert exc_info.value.error_count() == 1\n    # insert_assert(exc_info.value.errors(include_url=False)[0])\n    assert exc_info.value.errors(include_url=False)[0] == {\n        'type': 'ip_any_network',\n        'loc': ('ip',),\n        'msg': 'value is not a valid IPv4 or IPv6 network',\n        'input': value,\n    }\n\n\n@pytest.mark.parametrize('value', ['hello,world', '192.168.0.1.1.1/24', -1, 2**128 + 1, '2001:db00::1/120'])\ndef test_ip_v4_network_fails(value):\n    class Model(BaseModel):\n        ip: IPv4Network = None\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(ip=value)\n    assert exc_info.value.error_count() == 1\n    # insert_assert(exc_info.value.errors(include_url=False)[0])\n    assert exc_info.value.errors(include_url=False)[0] == {\n        'type': 'ip_v4_network',\n        'loc': ('ip',),\n        'msg': 'Input is not a valid IPv4 network',\n        'input': value,\n    }\n\n\n@pytest.mark.parametrize('value', ['hello,world', '192.168.0.1.1.1/24', -1, 2**128 + 1, '192.168.0.1/24'])\ndef test_ip_v6_network_fails(value):\n    class Model(BaseModel):\n        ip: IPv6Network = None\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(ip=value)\n\n    assert exc_info.value.error_count() == 1\n    # insert_assert(exc_info.value.errors(include_url=False)[0])\n    assert exc_info.value.errors(include_url=False)[0] == {\n        'type': 'ip_v6_network',\n        'loc': ('ip',),\n        'msg': 'Input is not a valid IPv6 network',\n        'input': value,\n    }\n\n\ndef test_ipvany_serialization():\n    class Model(BaseModel):\n        address: IPvAnyAddress\n        network: IPvAnyNetwork\n        interface: IPvAnyInterface\n\n    m = Model(address='127.0.0.1', network='192.0.2.0/27', interface='127.0.0.1/32')\n    assert json.loads(m.model_dump_json()) == {\n        'address': '127.0.0.1',\n        'interface': '127.0.0.1/32',\n        'network': '192.0.2.0/27',\n    }\n\n\n@pytest.mark.parametrize(\n    'value,cls',\n    [\n        ('192.168.0.0/24', IPv4Interface),\n        ('192.168.0.1/24', IPv4Interface),\n        ('192.168.128.0/30', IPv4Interface),\n        ('192.168.128.1/30', IPv4Interface),\n        ('2001:db00::0/120', IPv6Interface),\n        ('2001:db00::1/120', IPv6Interface),\n        (2**32 - 1, IPv4Interface),  # no mask equals to mask /32\n        (2**32 - 1, IPv4Interface),  # so `strict` has no effect\n        (20_282_409_603_651_670_423_947_251_286_015, IPv6Interface),  # /128\n        (20_282_409_603_651_670_423_947_251_286_014, IPv6Interface),\n        (b'\\xff\\xff\\xff\\xff', IPv4Interface),  # /32\n        (b'\\xff\\xff\\xff\\xff', IPv4Interface),\n        (b'\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff', IPv6Interface),\n        (b'\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff', IPv6Interface),\n        (('192.168.0.0', 24), IPv4Interface),\n        (('192.168.0.1', 24), IPv4Interface),\n        (('2001:db00::0', 120), IPv6Interface),\n        (('2001:db00::1', 120), IPv6Interface),\n        (IPv4Interface('192.168.0.0/24'), IPv4Interface),\n        (IPv4Interface('192.168.0.1/24'), IPv4Interface),\n        (IPv6Interface('2001:db00::0/120'), IPv6Interface),\n        (IPv6Interface('2001:db00::1/120'), IPv6Interface),\n    ],\n)\ndef test_ipinterface_success(value, cls):\n    class Model(BaseModel):\n        ip: IPvAnyInterface = None\n\n    assert Model(ip=value).ip == cls(value)\n\n\n@pytest.mark.parametrize(\n    'value,cls',\n    [\n        ('192.168.0.0/24', IPv4Interface),\n        ('192.168.0.1/24', IPv4Interface),\n        ('192.168.128.0/30', IPv4Interface),\n        ('192.168.128.1/30', IPv4Interface),\n        (2**32 - 1, IPv4Interface),  # no mask equals to mask /32\n        (2**32 - 1, IPv4Interface),  # so `strict` has no effect\n        (b'\\xff\\xff\\xff\\xff', IPv4Interface),  # /32\n        (b'\\xff\\xff\\xff\\xff', IPv4Interface),\n        (('192.168.0.0', 24), IPv4Interface),\n        (('192.168.0.1', 24), IPv4Interface),\n        (IPv4Interface('192.168.0.0/24'), IPv4Interface),\n        (IPv4Interface('192.168.0.1/24'), IPv4Interface),\n    ],\n)\ndef test_ip_v4_interface_success(value, cls):\n    class Model(BaseModel):\n        ip: IPv4Interface\n\n    assert Model(ip=value).ip == cls(value)\n\n\n@pytest.mark.parametrize(\n    'value,cls',\n    [\n        ('2001:db00::0/120', IPv6Interface),\n        ('2001:db00::1/120', IPv6Interface),\n        (20_282_409_603_651_670_423_947_251_286_015, IPv6Interface),  # /128\n        (20_282_409_603_651_670_423_947_251_286_014, IPv6Interface),\n        (b'\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff', IPv6Interface),\n        (b'\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff', IPv6Interface),\n        (('2001:db00::0', 120), IPv6Interface),\n        (('2001:db00::1', 120), IPv6Interface),\n        (IPv6Interface('2001:db00::0/120'), IPv6Interface),\n        (IPv6Interface('2001:db00::1/120'), IPv6Interface),\n    ],\n)\ndef test_ip_v6_interface_success(value, cls):\n    class Model(BaseModel):\n        ip: IPv6Interface = None\n\n    assert Model(ip=value).ip == cls(value)\n\n\n@pytest.mark.parametrize('value', ['hello,world', '192.168.0.1.1.1/24', -1, 2**128 + 1])\ndef test_ipinterface_fails(value):\n    class Model(BaseModel):\n        ip: IPvAnyInterface = None\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(ip=value)\n\n    assert exc_info.value.error_count() == 1\n    # insert_assert(exc_info.value.errors(include_url=False)[0])\n    assert exc_info.value.errors(include_url=False)[0] == {\n        'type': 'ip_any_interface',\n        'loc': ('ip',),\n        'msg': 'value is not a valid IPv4 or IPv6 interface',\n        'input': value,\n    }\n\n\n@pytest.mark.parametrize('value', ['hello,world', '192.168.0.1.1.1/24', -1, 2**128 + 1])\ndef test_ip_v4_interface_fails(value):\n    class Model(BaseModel):\n        ip: IPv4Interface = None\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(ip=value)\n\n    assert exc_info.value.error_count() == 1\n    # insert_assert(exc_info.value.errors(include_url=False)[0])\n    assert exc_info.value.errors(include_url=False)[0] == {\n        'type': 'ip_v4_interface',\n        'loc': ('ip',),\n        'msg': 'Input is not a valid IPv4 interface',\n        'input': value,\n    }\n\n\n@pytest.mark.parametrize('value', ['hello,world', '192.168.0.1.1.1/24', -1, 2**128 + 1])\ndef test_ip_v6_interface_fails(value):\n    class Model(BaseModel):\n        ip: IPv6Interface = None\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(ip=value)\n\n    assert exc_info.value.error_count() == 1\n    # insert_assert(exc_info.value.errors(include_url=False)[0])\n    assert exc_info.value.errors(include_url=False)[0] == {\n        'type': 'ip_v6_interface',\n        'loc': ('ip',),\n        'msg': 'Input is not a valid IPv6 interface',\n        'input': value,\n    }\n", "tests/test_structural_pattern_matching.py": "import sys\n\nimport pytest\n\n\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='requires python 3.10 or higher')\ndef test_match_kwargs(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom pydantic import BaseModel\n\nclass Model(BaseModel):\n    a: str\n    b: str\n\ndef main(model):\n    match model:\n        case Model(a='a', b=b):\n            return b\n        case Model(a='a2'):\n            return 'b2'\n        case _:\n            return None\n\"\"\"\n    )\n    assert module.main(module.Model(a='a', b='b')) == 'b'\n    assert module.main(module.Model(a='a2', b='b')) == 'b2'\n    assert module.main(module.Model(a='x', b='b')) is None\n", "tests/test_annotated.py": "import datetime as dt\nimport sys\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Generic, Iterator, List, Optional, Set, TypeVar\n\nimport pytest\nimport pytz\nfrom annotated_types import BaseMetadata, GroupedMetadata, Gt, Lt, Predicate\nfrom pydantic_core import CoreSchema, PydanticUndefined, core_schema\nfrom typing_extensions import Annotated\n\nfrom pydantic import BaseModel, Field, GetCoreSchemaHandler, PydanticUserError, TypeAdapter, ValidationError\nfrom pydantic.errors import PydanticSchemaGenerationError\nfrom pydantic.fields import PrivateAttr\nfrom pydantic.functional_validators import AfterValidator\n\nNO_VALUE = object()\n\n\n@pytest.mark.parametrize(\n    'hint_fn,value,expected_repr',\n    [\n        (\n            lambda: Annotated[int, Gt(0)],\n            5,\n            'FieldInfo(annotation=int, required=False, default=5, metadata=[Gt(gt=0)])',\n        ),\n        (\n            lambda: Annotated[int, Field(gt=0)],\n            5,\n            'FieldInfo(annotation=int, required=False, default=5, metadata=[Gt(gt=0)])',\n        ),\n        (\n            lambda: int,\n            Field(5, gt=0),\n            'FieldInfo(annotation=int, required=False, default=5, metadata=[Gt(gt=0)])',\n        ),\n        (\n            lambda: int,\n            Field(default_factory=lambda: 5, gt=0),\n            'FieldInfo(annotation=int, required=False, default_factory=<lambda>, metadata=[Gt(gt=0)])',\n        ),\n        (\n            lambda: Annotated[int, Lt(2)],\n            Field(5, gt=0),\n            'FieldInfo(annotation=int, required=False, default=5, metadata=[Gt(gt=0), Lt(lt=2)])',\n        ),\n        (\n            lambda: Annotated[int, Gt(0)],\n            NO_VALUE,\n            'FieldInfo(annotation=int, required=True, metadata=[Gt(gt=0)])',\n        ),\n        (\n            lambda: Annotated[int, Gt(0)],\n            Field(),\n            'FieldInfo(annotation=int, required=True, metadata=[Gt(gt=0)])',\n        ),\n        (\n            lambda: int,\n            Field(gt=0),\n            'FieldInfo(annotation=int, required=True, metadata=[Gt(gt=0)])',\n        ),\n        (\n            lambda: Annotated[int, Gt(0)],\n            PydanticUndefined,\n            'FieldInfo(annotation=int, required=True, metadata=[Gt(gt=0)])',\n        ),\n        (\n            lambda: Annotated[int, Field(gt=0), Lt(2)],\n            5,\n            'FieldInfo(annotation=int, required=False, default=5, metadata=[Gt(gt=0), Lt(lt=2)])',\n        ),\n        (\n            lambda: Annotated[int, Field(alias='foobar')],\n            PydanticUndefined,\n            \"FieldInfo(annotation=int, required=True, alias='foobar', alias_priority=2)\",\n        ),\n    ],\n)\ndef test_annotated(hint_fn, value, expected_repr):\n    hint = hint_fn()\n\n    if value is NO_VALUE:\n\n        class M(BaseModel):\n            x: hint\n\n    else:\n\n        class M(BaseModel):\n            x: hint = value\n\n    assert repr(M.model_fields['x']) == expected_repr\n\n\n@pytest.mark.parametrize('metadata', [0, 'foo'])\ndef test_annotated_allows_unknown(metadata):\n    class M(BaseModel):\n        x: Annotated[int, metadata] = 5\n\n    field_info = M.model_fields['x']\n    assert len(field_info.metadata) == 1\n    assert metadata in field_info.metadata, 'Records the unknown metadata'\n    assert metadata in M.__annotations__['x'].__metadata__, 'Annotated type is recorded'\n\n\n@pytest.mark.parametrize(\n    ['hint_fn', 'value', 'empty_init_ctx'],\n    [\n        (\n            lambda: int,\n            PydanticUndefined,\n            pytest.raises(ValueError, match=r'Field required \\[type=missing,'),\n        ),\n        (\n            lambda: Annotated[int, Field()],\n            PydanticUndefined,\n            pytest.raises(ValueError, match=r'Field required \\[type=missing,'),\n        ),\n    ],\n)\ndef test_annotated_instance_exceptions(hint_fn, value, empty_init_ctx):\n    hint = hint_fn()\n\n    class M(BaseModel):\n        x: hint = value\n\n    with empty_init_ctx:\n        assert M().x == 5\n\n\ndef test_field_reuse():\n    field = Field(description='Long description')\n\n    class Model(BaseModel):\n        one: int = field\n\n    assert Model(one=1).model_dump() == {'one': 1}\n\n    class AnnotatedModel(BaseModel):\n        one: Annotated[int, field]\n\n    assert AnnotatedModel(one=1).model_dump() == {'one': 1}\n\n\ndef test_config_field_info():\n    class Foo(BaseModel):\n        a: Annotated[int, Field(description='descr', json_schema_extra={'foobar': 'hello'})]\n\n    assert Foo.model_json_schema(by_alias=True)['properties'] == {\n        'a': {'title': 'A', 'description': 'descr', 'foobar': 'hello', 'type': 'integer'},\n    }\n\n\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='repr different on older versions')\ndef test_annotated_alias() -> None:\n    # https://github.com/pydantic/pydantic/issues/2971\n\n    StrAlias = Annotated[str, Field(max_length=3)]\n    IntAlias = Annotated[int, Field(default_factory=lambda: 2)]\n\n    Nested = Annotated[List[StrAlias], Field(description='foo')]\n\n    class MyModel(BaseModel):\n        a: StrAlias = 'abc'\n        b: StrAlias\n        c: IntAlias\n        d: IntAlias\n        e: Nested\n\n    fields_repr = {k: repr(v) for k, v in MyModel.model_fields.items()}\n    assert fields_repr == {\n        'a': \"FieldInfo(annotation=str, required=False, default='abc', metadata=[MaxLen(max_length=3)])\",\n        'b': 'FieldInfo(annotation=str, required=True, metadata=[MaxLen(max_length=3)])',\n        'c': 'FieldInfo(annotation=int, required=False, default_factory=<lambda>)',\n        'd': 'FieldInfo(annotation=int, required=False, default_factory=<lambda>)',\n        'e': \"FieldInfo(annotation=List[Annotated[str, FieldInfo(annotation=NoneType, required=True, metadata=[MaxLen(max_length=3)])]], required=True, description='foo')\",\n    }\n    assert MyModel(b='def', e=['xyz']).model_dump() == dict(a='abc', b='def', c=2, d=2, e=['xyz'])\n\n\ndef test_modify_get_schema_annotated() -> None:\n    calls: List[str] = []\n\n    class CustomType:\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            calls.append('CustomType:before')\n            with pytest.raises(PydanticSchemaGenerationError):\n                handler(source)\n            schema = core_schema.no_info_plain_validator_function(lambda _: CustomType())\n            calls.append('CustomType:after')\n            return schema\n\n    class PydanticMetadata:\n        def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            calls.append('PydanticMetadata:before')\n            schema = handler(source)\n            calls.append('PydanticMetadata:after')\n            return schema\n\n    class GroupedMetadataMarker(GroupedMetadata):\n        def __iter__(self) -> Iterator[BaseMetadata]:\n            # no way to actually hook into schema building\n            # so just register when our iter is called\n            calls.append('GroupedMetadataMarker:iter')\n            yield from []\n\n    class _(BaseModel):\n        x: Annotated[CustomType, GroupedMetadataMarker(), PydanticMetadata()]\n\n    # insert_assert(calls)\n    assert calls == [\n        'GroupedMetadataMarker:iter',\n        'PydanticMetadata:before',\n        'CustomType:before',\n        'CustomType:after',\n        'PydanticMetadata:after',\n    ]\n\n    calls.clear()\n\n    class _(BaseModel):\n        x: Annotated[CustomType, PydanticMetadata(), GroupedMetadataMarker()]\n\n    # insert_assert(calls)\n    assert calls == [\n        'GroupedMetadataMarker:iter',\n        'PydanticMetadata:before',\n        'CustomType:before',\n        'CustomType:after',\n        'PydanticMetadata:after',\n    ]\n\n    calls.clear()\n\n\ndef test_annotated_alias_at_low_level() -> None:\n    with pytest.warns(\n        UserWarning,\n        match=r'`alias` specification on field \"low_level_alias_field\" must be set on outermost annotation to take effect.',\n    ):\n\n        class Model(BaseModel):\n            low_level_alias_field: Optional[Annotated[int, Field(alias='field_alias')]] = None\n\n    assert Model(field_alias=1).low_level_alias_field is None\n\n\ndef test_get_pydantic_core_schema_source_type() -> None:\n    types: Set[Any] = set()\n\n    class PydanticMarker:\n        def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            types.add(source)\n            return handler(source)\n\n    class _(BaseModel):\n        x: Annotated[Annotated[int, 'foo'], PydanticMarker()]\n\n    assert types == {int}\n    types.clear()\n\n    T = TypeVar('T')\n\n    class GenericModel(BaseModel, Generic[T]):\n        y: T\n\n    class _(BaseModel):\n        x: Annotated[GenericModel[int], PydanticMarker()]\n\n    assert types == {GenericModel[int]}\n    types.clear()\n\n\ndef test_merge_field_infos_type_adapter() -> None:\n    ta = TypeAdapter(\n        Annotated[\n            int, Field(gt=0), Field(lt=100), Field(gt=1), Field(description='abc'), Field(3), Field(description=None)\n        ]\n    )\n\n    default = ta.get_default_value()\n    assert default is not None\n    assert default.value == 3\n\n    # insert_assert(ta.validate_python(2))\n    assert ta.validate_python(2) == 2\n\n    with pytest.raises(ValidationError) as exc_info:\n        ta.validate_python(1)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'greater_than', 'loc': (), 'msg': 'Input should be greater than 1', 'input': 1, 'ctx': {'gt': 1}}\n    ]\n\n    # insert_assert(ta.json_schema())\n    assert ta.json_schema() == {\n        'default': 3,\n        'description': 'abc',\n        'exclusiveMaximum': 100,\n        'exclusiveMinimum': 1,\n        'type': 'integer',\n    }\n\n\ndef test_merge_field_infos_model() -> None:\n    class Model(BaseModel):\n        x: Annotated[\n            int, Field(gt=0), Field(lt=100), Field(gt=1), Field(description='abc'), Field(3), Field(description=None)\n        ] = Field(5)\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        'properties': {\n            'x': {'default': 5, 'exclusiveMaximum': 100, 'exclusiveMinimum': 1, 'title': 'X', 'type': 'integer'}\n        },\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_model_dump_doesnt_dump_annotated_dunder():\n    class Model(BaseModel):\n        one: int\n\n    AnnotatedModel = Annotated[Model, ...]\n\n    # In Pydantic v1, `AnnotatedModel.dict()` would have returned\n    # `{'one': 1, '__orig_class__': typing.Annotated[...]}`\n    assert AnnotatedModel(one=1).model_dump() == {'one': 1}\n\n\ndef test_merge_field_infos_ordering() -> None:\n    TheType = Annotated[int, AfterValidator(lambda x: x), Field(le=2), AfterValidator(lambda x: x * 2), Field(lt=4)]\n\n    class Model(BaseModel):\n        x: TheType\n\n    assert Model(x=1).x == 2\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=2)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'less_than', 'loc': ('x',), 'msg': 'Input should be less than 4', 'input': 2, 'ctx': {'lt': 4}}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=3)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'less_than_equal',\n            'loc': ('x',),\n            'msg': 'Input should be less than or equal to 2',\n            'input': 3,\n            'ctx': {'le': 2},\n        }\n    ]\n\n\ndef test_validate_float_inf_nan_python() -> None:\n    ta = TypeAdapter(Annotated[float, AfterValidator(lambda x: x * 3), Field(allow_inf_nan=False)])\n    assert ta.validate_python(2.0) == 6.0\n\n    ta = TypeAdapter(Annotated[float, AfterValidator(lambda _: float('nan')), Field(allow_inf_nan=False)])\n\n    with pytest.raises(ValidationError) as exc_info:\n        ta.validate_python(1.0)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    # TODO: input should be float('nan'), this seems like a subtle bug in pydantic-core\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'finite_number', 'loc': (), 'msg': 'Input should be a finite number', 'input': 1.0}\n    ]\n\n\ndef test_predicate_error_python() -> None:\n    ta = TypeAdapter(Annotated[int, Predicate(lambda x: x > 0)])\n\n    with pytest.raises(ValidationError) as exc_info:\n        ta.validate_python(-1)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'predicate_failed',\n            'loc': (),\n            'msg': 'Predicate test_predicate_error_python.<locals>.<lambda> failed',\n            'input': -1,\n        }\n    ]\n\n\ndef test_annotated_field_info_not_lost_from_forwardref():\n    from pydantic import BaseModel\n\n    class ForwardRefAnnotatedFieldModel(BaseModel):\n        foo: 'Annotated[Integer, Field(alias=\"bar\", default=1)]' = 2\n        foo2: 'Annotated[Integer, Field(alias=\"bar2\", default=1)]' = Field(default=2, alias='baz')\n\n    Integer = int\n\n    ForwardRefAnnotatedFieldModel.model_rebuild()\n\n    assert ForwardRefAnnotatedFieldModel(bar=3).foo == 3\n    assert ForwardRefAnnotatedFieldModel(baz=3).foo2 == 3\n\n    with pytest.raises(ValidationError) as exc_info:\n        ForwardRefAnnotatedFieldModel(bar='bar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'bar',\n            'loc': ('bar',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_annotated_private_field_with_default():\n    class AnnotatedPrivateFieldModel(BaseModel):\n        _foo: Annotated[int, PrivateAttr(default=1)]\n        _bar: Annotated[str, 'hello']\n\n    model = AnnotatedPrivateFieldModel()\n    assert model._foo == 1\n\n    assert model.__pydantic_private__ == {'_foo': 1}\n\n    with pytest.raises(AttributeError):\n        assert model._bar\n\n    model._bar = 'world'\n    assert model._bar == 'world'\n    assert model.__pydantic_private__ == {'_foo': 1, '_bar': 'world'}\n\n    with pytest.raises(AttributeError):\n        assert model.bar\n\n\ndef test_min_length_field_info_not_lost():\n    class AnnotatedFieldModel(BaseModel):\n        foo: 'Annotated[String, Field(min_length=3)]' = Field(description='hello')\n\n    String = str\n\n    AnnotatedFieldModel.model_rebuild()\n\n    assert AnnotatedFieldModel(foo='000').foo == '000'\n\n    with pytest.raises(ValidationError) as exc_info:\n        AnnotatedFieldModel(foo='00')\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'loc': ('foo',),\n            'input': '00',\n            'ctx': {'min_length': 3},\n            'msg': 'String should have at least 3 characters',\n            'type': 'string_too_short',\n        }\n    ]\n\n    # Ensure that the inner annotation does not override the outer, even for metadata:\n    class AnnotatedFieldModel2(BaseModel):\n        foo: 'Annotated[String, Field(min_length=3)]' = Field(description='hello', min_length=2)\n\n    AnnotatedFieldModel2(foo='00')\n\n    class AnnotatedFieldModel4(BaseModel):\n        foo: 'Annotated[String, Field(min_length=3)]' = Field(description='hello', min_length=4)\n\n    with pytest.raises(ValidationError) as exc_info:\n        AnnotatedFieldModel4(foo='00')\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'loc': ('foo',),\n            'input': '00',\n            'ctx': {'min_length': 4},\n            'msg': 'String should have at least 4 characters',\n            'type': 'string_too_short',\n        }\n    ]\n\n\ndef test_tzinfo_validator_example_pattern() -> None:\n    \"\"\"Test that tzinfo custom validator pattern works as explained in the examples/validators docs.\"\"\"\n\n    @dataclass(frozen=True)\n    class MyDatetimeValidator:\n        tz_constraint: Optional[str] = None\n\n        def tz_constraint_validator(\n            self,\n            value: dt.datetime,\n            handler: Callable,  # (1)!\n        ):\n            \"\"\"Validate tz_constraint and tz_info.\"\"\"\n            # handle naive datetimes\n            if self.tz_constraint is None:\n                assert value.tzinfo is None, 'tz_constraint is None, but provided value is tz-aware.'\n                return handler(value)\n\n            # validate tz_constraint and tz-aware tzinfo\n            if self.tz_constraint not in pytz.all_timezones:\n                raise PydanticUserError(\n                    f'Invalid tz_constraint: {self.tz_constraint}', code='unevaluable-type-annotation'\n                )\n            result = handler(value)  # (2)!\n            assert self.tz_constraint == str(\n                result.tzinfo\n            ), f'Invalid tzinfo: {str(result.tzinfo)}, expected: {self.tz_constraint}'\n\n            return result\n\n        def __get_pydantic_core_schema__(\n            self,\n            source_type: Any,\n            handler: GetCoreSchemaHandler,\n        ) -> CoreSchema:\n            return core_schema.no_info_wrap_validator_function(\n                self.tz_constraint_validator,\n                handler(source_type),\n            )\n\n    LA = 'America/Los_Angeles'\n\n    # passing naive test\n    ta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator()])\n    ta.validate_python(dt.datetime.now())\n\n    # failing naive test\n    ta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator()])\n    with pytest.raises(Exception):\n        ta.validate_python(dt.datetime.now(pytz.timezone(LA)))\n\n    # passing tz-aware test\n    ta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator(LA)])\n    ta.validate_python(dt.datetime.now(pytz.timezone(LA)))\n\n    # failing bad tz\n    ta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator('foo')])\n    with pytest.raises(Exception):\n        ta.validate_python(dt.datetime.now())\n\n    # failing tz-aware test\n    ta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator(LA)])\n    with pytest.raises(Exception):\n        ta.validate_python(dt.datetime.now())\n\n\ndef test_utcoffset_validator_example_pattern() -> None:\n    \"\"\"Test that utcoffset custom validator pattern works as explained in the examples/validators docs.\"\"\"\n\n    @dataclass(frozen=True)\n    class MyDatetimeValidator:\n        lower_bound: int\n        upper_bound: int\n\n        def validate_tz_bounds(self, value: dt.datetime, handler: Callable):\n            \"\"\"Validate and test bounds\"\"\"\n            assert value.utcoffset() is not None, 'UTC offset must exist'\n            assert self.lower_bound <= self.upper_bound, 'Invalid bounds'\n\n            result = handler(value)\n\n            hours_offset = value.utcoffset().total_seconds() / 3600\n            assert self.lower_bound <= hours_offset <= self.upper_bound, 'Value out of bounds'\n\n            return result\n\n        def __get_pydantic_core_schema__(\n            self,\n            source_type: Any,\n            handler: GetCoreSchemaHandler,\n        ) -> CoreSchema:\n            return core_schema.no_info_wrap_validator_function(\n                self.validate_tz_bounds,\n                handler(source_type),\n            )\n\n    LA = 'America/Los_Angeles'\n\n    # test valid bound passing\n    ta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator(-10, 10)])\n    ta.validate_python(dt.datetime.now(pytz.timezone(LA)))\n\n    # test valid bound failing - missing TZ\n    ta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator(-12, 12)])\n    with pytest.raises(Exception):\n        ta.validate_python(dt.datetime.now())\n\n    # test invalid bound\n    ta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator(0, 4)])\n    with pytest.raises(Exception):\n        ta.validate_python(dt.datetime.now(pytz.timezone(LA)))\n", "tests/test_docs.py": "from __future__ import annotations as _annotations\n\nimport os\nimport platform\nimport re\nimport subprocess\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom typing import Any\n\nimport pytest\nfrom pydantic_core import core_schema\nfrom pytest_examples import CodeExample, EvalExample, find_examples\n\nfrom pydantic.errors import PydanticErrorCodes\n\nINDEX_MAIN = None\nDOCS_ROOT = Path(__file__).parent.parent / 'docs'\nSOURCES_ROOT = Path(__file__).parent.parent / 'pydantic'\n\n\ndef skip_docs_tests():\n    if sys.platform not in {'linux', 'darwin'}:\n        return 'not in linux or macos'\n\n    if platform.python_implementation() != 'CPython':\n        return 'not cpython'\n\n    try:\n        import devtools  # noqa: F401\n    except ImportError:\n        return 'devtools not installed'\n\n    try:\n        import sqlalchemy  # noqa: F401\n    except ImportError:\n        return 'sqlalchemy not installed'\n\n    try:\n        import ansi2html  # noqa: F401\n    except ImportError:\n        return 'ansi2html not installed'\n\n\nclass GroupModuleGlobals:\n    def __init__(self) -> None:\n        self.name = None\n        self.module_dict: dict[str, str] = {}\n\n    def get(self, name: str | None):\n        if name is not None and name == self.name:\n            return self.module_dict\n\n    def set(self, name: str | None, module_dict: dict[str, str]):\n        self.name = name\n        if self.name is None:\n            self.module_dict = None\n        else:\n            self.module_dict = module_dict\n\n\ngroup_globals = GroupModuleGlobals()\n\n\nclass MockedDatetime(datetime):\n    @classmethod\n    def now(cls, *args, tz=None, **kwargs):\n        return datetime(2032, 1, 2, 3, 4, 5, 6, tzinfo=tz)\n\n\nskip_reason = skip_docs_tests()\nLINE_LENGTH = 80\n\n\ndef print_callback(print_statement: str) -> str:\n    return re.sub(r'(https://errors.pydantic.dev)/.+?/', r'\\1/2/', print_statement)\n\n\ndef run_example(example: CodeExample, eval_example: EvalExample, mocker: Any) -> None:  # noqa C901\n    eval_example.print_callback = print_callback\n\n    prefix_settings = example.prefix_settings()\n    test_settings = prefix_settings.get('test', '')\n    lint_settings = prefix_settings.get('lint', '')\n    if test_settings.startswith('skip') and lint_settings.startswith('skip'):\n        pytest.skip('both running code and lint skipped')\n\n    requires_settings = prefix_settings.get('requires')\n    if requires_settings:\n        major, minor = map(int, requires_settings.split('.'))\n        if sys.version_info < (major, minor):\n            pytest.skip(f'requires python {requires_settings}')\n\n    group_name = prefix_settings.get('group')\n\n    eval_example.set_config(ruff_ignore=['D', 'T', 'E721', 'Q001'], line_length=LINE_LENGTH)\n    if '# ignore-above' in example.source:\n        eval_example.set_config(ruff_ignore=eval_example.config.ruff_ignore + ['E402'], line_length=LINE_LENGTH)\n    if group_name:\n        eval_example.set_config(ruff_ignore=eval_example.config.ruff_ignore + ['F821'], line_length=LINE_LENGTH)\n\n    if not lint_settings.startswith('skip'):\n        if eval_example.update_examples:\n            eval_example.format(example)\n        else:\n            if example.in_py_file():\n                # Ignore isort as double newlines will cause it to fail, but we remove them in py files\n                eval_example.set_config(ruff_ignore=eval_example.config.ruff_ignore + ['I001'], line_length=LINE_LENGTH)\n            eval_example.lint(example)\n\n    if test_settings.startswith('skip'):\n        pytest.skip(test_settings[4:].lstrip(' -') or 'running code skipped')\n\n    group_name = prefix_settings.get('group')\n    d = group_globals.get(group_name)\n\n    mocker.patch('datetime.datetime', MockedDatetime)\n    mocker.patch('random.randint', return_value=3)\n\n    xfail = None\n    if test_settings.startswith('xfail'):\n        xfail = test_settings[5:].lstrip(' -')\n\n    rewrite_assertions = prefix_settings.get('rewrite_assert', 'true') == 'true'\n\n    try:\n        if test_settings == 'no-print-intercept':\n            d2 = eval_example.run(example, module_globals=d, rewrite_assertions=rewrite_assertions)\n        elif eval_example.update_examples:\n            d2 = eval_example.run_print_update(example, module_globals=d, rewrite_assertions=rewrite_assertions)\n        else:\n            d2 = eval_example.run_print_check(example, module_globals=d, rewrite_assertions=rewrite_assertions)\n    except BaseException as e:  # run_print_check raises a BaseException\n        if xfail:\n            pytest.xfail(f'{xfail}, {type(e).__name__}: {e}')\n        raise\n    else:\n        if xfail:\n            pytest.fail('expected xfail')\n        group_globals.set(group_name, d2)\n\n\n@pytest.mark.filterwarnings('ignore:(parse_obj_as|schema_json_of|schema_of) is deprecated.*:DeprecationWarning')\n@pytest.mark.skipif(bool(skip_reason), reason=skip_reason or 'not skipping')\n@pytest.mark.parametrize('example', find_examples(str(SOURCES_ROOT), skip=sys.platform == 'win32'), ids=str)\ndef test_docstrings_examples(example: CodeExample, eval_example: EvalExample, tmp_path: Path, mocker):\n    if str(example.path).startswith(str(SOURCES_ROOT / 'v1')):\n        pytest.skip('skip v1 examples')\n\n    run_example(example, eval_example, mocker)\n\n\n@pytest.fixture(scope='module', autouse=True)\ndef set_cwd():\n    # `test_docs_examples` needs to be run from this folder or relative paths will be wrong and some tests fail\n    execution_path = str(DOCS_ROOT.parent)\n\n    cwd = os.getcwd()\n    os.chdir(execution_path)\n    try:\n        yield\n    finally:\n        os.chdir(cwd)\n\n\n@pytest.mark.filterwarnings('ignore:(parse_obj_as|schema_json_of|schema_of) is deprecated.*:DeprecationWarning')\n@pytest.mark.filterwarnings('ignore::pydantic.warnings.PydanticExperimentalWarning')\n@pytest.mark.skipif(bool(skip_reason), reason=skip_reason or 'not skipping')\n@pytest.mark.parametrize('example', find_examples(str(DOCS_ROOT), skip=sys.platform == 'win32'), ids=str)\ndef test_docs_examples(example: CodeExample, eval_example: EvalExample, tmp_path: Path, mocker):\n    global INDEX_MAIN\n    if example.path.name == 'index.md':\n        if INDEX_MAIN is None:\n            INDEX_MAIN = example.source\n        else:\n            (tmp_path / 'index_main.py').write_text(INDEX_MAIN)\n            sys.path.append(str(tmp_path))\n\n    if example.path.name == 'devtools.md':\n        pytest.skip('tested below')\n\n    run_example(example, eval_example, mocker)\n\n\n@pytest.mark.skipif(bool(skip_reason), reason=skip_reason or 'not skipping')\n@pytest.mark.parametrize(\n    'example', find_examples(str(DOCS_ROOT / 'integrations/devtools.md'), skip=sys.platform == 'win32'), ids=str\n)\ndef test_docs_devtools_example(example: CodeExample, eval_example: EvalExample, tmp_path: Path):\n    from ansi2html import Ansi2HTMLConverter\n\n    eval_example.set_config(ruff_ignore=['D', 'T'], line_length=LINE_LENGTH)\n\n    if eval_example.update_examples:\n        eval_example.format(example)\n    else:\n        eval_example.lint(example)\n\n    with NamedTemporaryFile(mode='w', suffix='.py') as f:\n        f.write(example.source)\n        f.flush()\n        os.environ['PY_DEVTOOLS_HIGHLIGHT'] = 'true'\n        p = subprocess.run((sys.executable, f.name), stdout=subprocess.PIPE, check=True, encoding='utf8')\n\n    conv = Ansi2HTMLConverter()\n\n    # replace ugly file path with \"devtools_example.py\"\n    output = re.sub(r'/.+?\\.py', 'devtools_example.py', p.stdout)\n    output_html = conv.convert(output, full=False)\n    output_html = (\n        '<!-- DO NOT EDIT MANUALLY: '\n        'Generated by tests/test_docs.py::test_docs_devtools_example for use in docs -->\\n'\n        f'{output_html}'\n    )\n    output_file = DOCS_ROOT / 'plugins/devtools_output.html'\n\n    if eval_example.update_examples:\n        output_file.write_text(output_html)\n    elif not output_file.exists():\n        pytest.fail(f'output file {output_file} does not exist')\n    else:\n        assert output_html == output_file.read_text()\n\n\ndef test_error_codes():\n    error_text = (DOCS_ROOT / 'errors/usage_errors.md').read_text()\n\n    code_error_codes = PydanticErrorCodes.__args__\n\n    documented_error_codes = tuple(re.findall(r'^## .+ \\{#(.+?)}$', error_text, flags=re.MULTILINE))\n\n    assert code_error_codes == documented_error_codes, 'Error codes in code and docs do not match'\n\n\ndef test_validation_error_codes():\n    error_text = (DOCS_ROOT / 'errors/validation_errors.md').read_text()\n\n    expected_validation_error_codes = set(core_schema.ErrorType.__args__)\n    # Remove codes that are not currently accessible from pydantic:\n    expected_validation_error_codes.remove('timezone_offset')  # not currently exposed for configuration in pydantic\n\n    test_failures = []\n\n    documented_validation_error_codes = []\n    error_code_section = None\n    printed_error_code = None\n    for line in error_text.splitlines():\n        section_match = re.fullmatch(r'## `(.+)`', line)\n        if section_match:\n            if error_code_section is not None and printed_error_code != error_code_section:\n                test_failures.append(f'Error code {error_code_section!r} is not printed in its example')\n            error_code_section = section_match.group(1)\n            if error_code_section not in expected_validation_error_codes:\n                test_failures.append(f'Documented error code {error_code_section!r} is not a member of ErrorType')\n            documented_validation_error_codes.append(error_code_section)\n            printed_error_code = None\n            continue\n\n        printed_match = re.search(\"#> '(.+)'\", line)\n        if printed_match:\n            printed_error_code = printed_match.group(1)\n\n    assert test_failures == []\n\n    code_validation_error_codes = sorted(expected_validation_error_codes)\n    assert code_validation_error_codes == documented_validation_error_codes, 'Error codes in code and docs do not match'\n", "tests/test_model_signature.py": "import sys\nfrom inspect import Parameter, Signature, signature\nfrom typing import Any, Generic, Iterable, Optional, TypeVar, Union\n\nimport pytest\nfrom typing_extensions import Annotated\n\nfrom pydantic import BaseModel, ConfigDict, Field, create_model\nfrom pydantic._internal._typing_extra import is_annotated\n\n\ndef _equals(a: Union[str, Iterable[str]], b: Union[str, Iterable[str]]) -> bool:\n    \"\"\"\n    compare strings with spaces removed\n    \"\"\"\n    if isinstance(a, str) and isinstance(b, str):\n        return a.replace(' ', '') == b.replace(' ', '')\n    elif isinstance(a, Iterable) and isinstance(b, Iterable):\n        return all(_equals(a_, b_) for a_, b_ in zip(a, b))\n    else:\n        raise TypeError(f'arguments must be both strings or both lists, not {type(a)}, {type(b)}')\n\n\ndef test_model_signature():\n    class Model(BaseModel):\n        a: float = Field(..., title='A')\n        b: int = Field(10)\n\n    sig = signature(Model)\n    assert sig != signature(BaseModel)\n    assert _equals(map(str, sig.parameters.values()), ('a: float', 'b: int = 10'))\n    assert _equals(str(sig), '(*, a: float, b: int = 10) -> None')\n\n\ndef test_generic_model_signature():\n    T = TypeVar('T')\n\n    class Model(BaseModel, Generic[T]):\n        a: T\n\n    sig = signature(Model[int])\n    assert sig != signature(BaseModel)\n    assert _equals(map(str, sig.parameters.values()), ('a: int',))\n    assert _equals(str(sig), '(*, a: int) -> None')\n\n\ndef test_custom_init_signature():\n    class MyModel(BaseModel):\n        id: int\n        name: str = 'John Doe'\n        f__: str = Field(..., alias='foo')\n\n        model_config = ConfigDict(extra='allow')\n\n        def __init__(self, id: int = 1, bar=2, *, baz: Any, **data):\n            super().__init__(id=id, **data)\n            self.bar = bar\n            self.baz = baz\n\n    sig = signature(MyModel)\n    assert _equals(\n        map(str, sig.parameters.values()),\n        ('id: int = 1', 'bar=2', 'baz: Any', \"name: str = 'John Doe'\", 'foo: str', '**data'),\n    )\n\n    assert _equals(str(sig), \"(id: int = 1, bar=2, *, baz: Any, name: str = 'John Doe', foo: str, **data) -> None\")\n\n\ndef test_custom_init_signature_with_no_var_kw():\n    class Model(BaseModel):\n        a: float\n        b: int = 2\n        c: int\n\n        def __init__(self, a: float, b: int):\n            super().__init__(a=a, b=b, c=1)\n\n        model_config = ConfigDict(extra='allow')\n\n    assert _equals(str(signature(Model)), '(a: float, b: int) -> None')\n\n\ndef test_invalid_identifiers_signature():\n    model = create_model(\n        'Model',\n        **{'123 invalid identifier!': (int, Field(123, alias='valid_identifier')), '!': (int, Field(0, alias='yeah'))},\n    )\n    assert _equals(str(signature(model)), '(*, valid_identifier: int = 123, yeah: int = 0) -> None')\n    model = create_model('Model', **{'123 invalid identifier!': (int, 123), '!': (int, Field(0, alias='yeah'))})\n    assert _equals(str(signature(model)), '(*, yeah: int = 0, **extra_data: Any) -> None')\n\n\ndef test_use_field_name():\n    class Foo(BaseModel):\n        foo: str = Field(..., alias='this is invalid')\n\n        model_config = ConfigDict(populate_by_name=True)\n\n    assert _equals(str(signature(Foo)), '(*, foo: str) -> None')\n\n\ndef test_does_not_use_reserved_word():\n    class Foo(BaseModel):\n        from_: str = Field(..., alias='from')\n\n        model_config = ConfigDict(populate_by_name=True)\n\n    assert _equals(str(signature(Foo)), '(*, from_: str) -> None')\n\n\ndef test_extra_allow_no_conflict():\n    class Model(BaseModel):\n        spam: str\n\n        model_config = ConfigDict(extra='allow')\n\n    assert _equals(str(signature(Model)), '(*, spam: str, **extra_data: Any) -> None')\n\n\ndef test_extra_allow_conflict():\n    class Model(BaseModel):\n        extra_data: str\n\n        model_config = ConfigDict(extra='allow')\n\n    assert _equals(str(signature(Model)), '(*, extra_data: str, **extra_data_: Any) -> None')\n\n\ndef test_extra_allow_conflict_twice():\n    class Model(BaseModel):\n        extra_data: str\n        extra_data_: str\n\n        model_config = ConfigDict(extra='allow')\n\n    assert _equals(str(signature(Model)), '(*, extra_data: str, extra_data_: str, **extra_data__: Any) -> None')\n\n\ndef test_extra_allow_conflict_custom_signature():\n    class Model(BaseModel):\n        extra_data: int\n\n        def __init__(self, extra_data: int = 1, **foobar: Any):\n            super().__init__(extra_data=extra_data, **foobar)\n\n        model_config = ConfigDict(extra='allow')\n\n    assert _equals(str(signature(Model)), '(extra_data: int = 1, **foobar: Any) -> None')\n\n\ndef test_signature_is_class_only():\n    class Model(BaseModel):\n        foo: int = 123\n\n        def __call__(self, a: int) -> bool:\n            pass\n\n    assert _equals(str(signature(Model)), '(*, foo: int = 123) -> None')\n    assert _equals(str(signature(Model())), '(a: int) -> bool')\n    assert not hasattr(Model(), '__signature__')\n\n\ndef test_optional_field():\n    class Model(BaseModel):\n        foo: Optional[int] = None\n\n    assert signature(Model) == Signature(\n        [Parameter('foo', Parameter.KEYWORD_ONLY, default=None, annotation=Optional[int])], return_annotation=None\n    )\n\n\n@pytest.mark.skipif(sys.version_info < (3, 12), reason='repr different on older versions')\ndef test_annotated_field():\n    from annotated_types import Gt\n\n    class Model(BaseModel):\n        foo: Annotated[int, Gt(1)] = 1\n\n    sig = signature(Model)\n    assert str(sig) == '(*, foo: Annotated[int, Gt(gt=1)] = 1) -> None'\n    # check that the `Annotated` we created is a valid `Annotated`\n    assert is_annotated(sig.parameters['foo'].annotation)\n\n\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='repr different on older versions')\ndef test_annotated_optional_field():\n    from annotated_types import Gt\n\n    class Model(BaseModel):\n        foo: Annotated[Optional[int], Gt(1)] = None\n\n    assert str(signature(Model)) == '(*, foo: Annotated[Optional[int], Gt(gt=1)] = None) -> None'\n", "tests/test_plugins.py": "from __future__ import annotations\n\nimport contextlib\nfrom functools import partial\nfrom typing import Any, Generator, List\n\nfrom pydantic_core import ValidationError\n\nfrom pydantic import BaseModel, TypeAdapter, create_model, dataclasses, field_validator, validate_call\nfrom pydantic.plugin import (\n    PydanticPluginProtocol,\n    SchemaTypePath,\n    ValidateJsonHandlerProtocol,\n    ValidatePythonHandlerProtocol,\n    ValidateStringsHandlerProtocol,\n)\nfrom pydantic.plugin._loader import _plugins\n\n\n@contextlib.contextmanager\ndef install_plugin(plugin: PydanticPluginProtocol) -> Generator[None, None, None]:\n    _plugins[plugin.__class__.__qualname__] = plugin\n    try:\n        yield\n    finally:\n        _plugins.clear()\n\n\ndef test_on_validate_json_on_success() -> None:\n    class CustomOnValidateJson(ValidateJsonHandlerProtocol):\n        def on_enter(\n            self,\n            input: str | bytes | bytearray,\n            *,\n            strict: bool | None = None,\n            context: dict[str, Any] | None = None,\n            self_instance: Any | None = None,\n        ) -> None:\n            assert input == '{\"a\": 1}'\n            assert strict is None\n            assert context is None\n            assert self_instance is None\n\n        def on_success(self, result: Any) -> None:\n            assert isinstance(result, Model)\n\n    class CustomPlugin(PydanticPluginProtocol):\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert config == {'title': 'Model'}\n            assert plugin_settings == {'observe': 'all'}\n            assert schema_type.__name__ == 'Model'\n            assert schema_type_path == SchemaTypePath(\n                'tests.test_plugins', 'test_on_validate_json_on_success.<locals>.Model'\n            )\n            assert schema_kind == 'BaseModel'\n            return None, CustomOnValidateJson(), None\n\n    plugin = CustomPlugin()\n    with install_plugin(plugin):\n\n        class Model(BaseModel, plugin_settings={'observe': 'all'}):\n            a: int\n\n        assert Model.model_validate({'a': 1}) == Model(a=1)\n        assert Model.model_validate_json('{\"a\": 1}') == Model(a=1)\n\n        assert Model.__pydantic_validator__.title == 'Model'\n\n\ndef test_on_validate_json_on_error() -> None:\n    class CustomOnValidateJson:\n        def on_enter(\n            self,\n            input: str | bytes | bytearray,\n            *,\n            strict: bool | None = None,\n            context: dict[str, Any] | None = None,\n            self_instance: Any | None = None,\n        ) -> None:\n            assert input == '{\"a\": \"potato\"}'\n            assert strict is None\n            assert context is None\n            assert self_instance is None\n\n        def on_error(self, error: ValidationError) -> None:\n            assert error.title == 'Model'\n            assert error.errors(include_url=False) == [\n                {\n                    'input': 'potato',\n                    'loc': ('a',),\n                    'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n                    'type': 'int_parsing',\n                },\n            ]\n\n    class Plugin(PydanticPluginProtocol):\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert config == {'title': 'Model'}\n            assert plugin_settings == {'observe': 'all'}\n            return None, CustomOnValidateJson(), None\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n\n        class Model(BaseModel, plugin_settings={'observe': 'all'}):\n            a: int\n\n        assert Model.model_validate({'a': 1}) == Model(a=1)\n        with contextlib.suppress(ValidationError):\n            Model.model_validate_json('{\"a\": \"potato\"}')\n\n\ndef test_on_validate_python_on_success() -> None:\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        def on_enter(\n            self,\n            input: Any,\n            *,\n            strict: bool | None = None,\n            from_attributes: bool | None = None,\n            context: dict[str, Any] | None = None,\n            self_instance: Any | None = None,\n        ) -> None:\n            assert input == {'a': 1}\n            assert strict is None\n            assert context is None\n            assert self_instance is None\n\n        def on_success(self, result: Any) -> None:\n            assert isinstance(result, Model)\n\n    class Plugin:\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert config == {'title': 'Model'}\n            assert plugin_settings == {'observe': 'all'}\n            assert schema_type.__name__ == 'Model'\n            assert schema_kind == 'BaseModel'\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n\n        class Model(BaseModel, plugin_settings={'observe': 'all'}):\n            a: int\n\n        assert Model.model_validate({'a': 1}).model_dump() == {'a': 1}\n        assert Model.model_validate_json('{\"a\": 1}').model_dump() == {'a': 1}\n\n\ndef test_on_validate_python_on_error() -> None:\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        def on_enter(\n            self,\n            input: Any,\n            *,\n            strict: bool | None = None,\n            from_attributes: bool | None = None,\n            context: dict[str, Any] | None = None,\n            self_instance: Any | None = None,\n        ) -> None:\n            assert input == {'a': 'potato'}\n            assert strict is None\n            assert context is None\n            assert self_instance is None\n\n        def on_error(self, error: ValidationError) -> None:\n            assert error.title == 'Model'\n            assert error.errors(include_url=False) == [\n                {\n                    'input': 'potato',\n                    'loc': ('a',),\n                    'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n                    'type': 'int_parsing',\n                },\n            ]\n\n    class Plugin(PydanticPluginProtocol):\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert config == {'title': 'Model'}\n            assert plugin_settings == {'observe': 'all'}\n            assert schema_type.__name__ == 'Model'\n            assert schema_kind == 'BaseModel'\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n\n        class Model(BaseModel, plugin_settings={'observe': 'all'}):\n            a: int\n\n        with contextlib.suppress(ValidationError):\n            Model.model_validate({'a': 'potato'})\n        assert Model.model_validate_json('{\"a\": 1}').model_dump() == {'a': 1}\n\n\ndef test_stateful_plugin() -> None:\n    stack: list[Any] = []\n\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        def on_enter(\n            self,\n            input: Any,\n            *,\n            strict: bool | None = None,\n            from_attributes: bool | None = None,\n            context: dict[str, Any] | None = None,\n            self_instance: Any | None = None,\n        ) -> None:\n            stack.append(input)\n\n        def on_success(self, result: Any) -> None:\n            stack.pop()\n\n        def on_error(self, error: Exception) -> None:\n            stack.pop()\n\n        def on_exception(self, exception: Exception) -> None:\n            stack.pop()\n\n    class Plugin(PydanticPluginProtocol):\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin()\n\n    class MyException(Exception):\n        pass\n\n    with install_plugin(plugin):\n\n        class Model(BaseModel, plugin_settings={'observe': 'all'}):\n            a: int\n\n            @field_validator('a')\n            def validate_a(cls, v: int) -> int:\n                if v < 0:\n                    raise MyException\n                return v\n\n        with contextlib.suppress(ValidationError):\n            Model.model_validate({'a': 'potato'})\n        assert not stack\n        with contextlib.suppress(MyException):\n            Model.model_validate({'a': -1})\n        assert not stack\n        assert Model.model_validate({'a': 1}).a == 1\n        assert not stack\n\n\ndef test_all_handlers():\n    log = []\n\n    class Python(ValidatePythonHandlerProtocol):\n        def on_enter(self, input, **kwargs) -> None:\n            log.append(f'python enter input={input} kwargs={kwargs}')\n\n        def on_success(self, result: Any) -> None:\n            log.append(f'python success result={result}')\n\n        def on_error(self, error: ValidationError) -> None:\n            log.append(f'python error error={error}')\n\n    class Json(ValidateJsonHandlerProtocol):\n        def on_enter(self, input, **kwargs) -> None:\n            log.append(f'json enter input={input} kwargs={kwargs}')\n\n        def on_success(self, result: Any) -> None:\n            log.append(f'json success result={result}')\n\n        def on_error(self, error: ValidationError) -> None:\n            log.append(f'json error error={error}')\n\n    class Strings(ValidateStringsHandlerProtocol):\n        def on_enter(self, input, **kwargs) -> None:\n            log.append(f'strings enter input={input} kwargs={kwargs}')\n\n        def on_success(self, result: Any) -> None:\n            log.append(f'strings success result={result}')\n\n        def on_error(self, error: ValidationError) -> None:\n            log.append(f'strings error error={error}')\n\n    class Plugin(PydanticPluginProtocol):\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            return Python(), Json(), Strings()\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n\n        class Model(BaseModel):\n            a: int\n\n        assert Model(a=1).model_dump() == {'a': 1}\n        # insert_assert(log)\n        assert log == [\"python enter input={'a': 1} kwargs={'self_instance': Model()}\", 'python success result=a=1']\n        log.clear()\n        assert Model.model_validate_json('{\"a\": 2}', context={'c': 2}).model_dump() == {'a': 2}\n        # insert_assert(log)\n        assert log == [\n            \"json enter input={\\\"a\\\": 2} kwargs={'strict': None, 'context': {'c': 2}}\",\n            'json success result=a=2',\n        ]\n        log.clear()\n        assert Model.model_validate_strings({'a': '3'}, strict=True, context={'c': 3}).model_dump() == {'a': 3}\n        # insert_assert(log)\n        assert log == [\n            \"strings enter input={'a': '3'} kwargs={'strict': True, 'context': {'c': 3}}\",\n            'strings success result=a=3',\n        ]\n\n\ndef test_plugin_path_dataclass() -> None:\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        pass\n\n    class Plugin:\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert schema_type.__name__ == 'Bar'\n            assert schema_type_path == SchemaTypePath('tests.test_plugins', 'test_plugin_path_dataclass.<locals>.Bar')\n            assert schema_kind == 'dataclass'\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n\n        @dataclasses.dataclass\n        class Bar:\n            a: int\n\n\ndef test_plugin_path_type_adapter() -> None:\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        pass\n\n    class Plugin:\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert str(schema_type) == 'typing.List[str]'\n            assert schema_type_path == SchemaTypePath('tests.test_plugins', 'typing.List[str]')\n            assert schema_kind == 'TypeAdapter'\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n        TypeAdapter(List[str])\n\n\ndef test_plugin_path_type_adapter_with_module() -> None:\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        pass\n\n    class Plugin:\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert str(schema_type) == 'typing.List[str]'\n            assert schema_type_path == SchemaTypePath('provided_module_by_type_adapter', 'typing.List[str]')\n            assert schema_kind == 'TypeAdapter'\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n        TypeAdapter(List[str], module='provided_module_by_type_adapter')\n\n\ndef test_plugin_path_type_adapter_without_name_in_globals() -> None:\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        pass\n\n    class Plugin:\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert str(schema_type) == 'typing.List[str]'\n            assert schema_type_path == SchemaTypePath('', 'typing.List[str]')\n            assert schema_kind == 'TypeAdapter'\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n        code = \"\"\"\nfrom typing import List\n\nimport pydantic\npydantic.TypeAdapter(List[str])\n\"\"\"\n        exec(code, {'bar': 'baz'})\n\n\ndef test_plugin_path_validate_call() -> None:\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        pass\n\n    class Plugin1:\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert schema_type.__name__ == 'foo'\n            assert schema_type_path == SchemaTypePath(\n                'tests.test_plugins', 'test_plugin_path_validate_call.<locals>.foo'\n            )\n            assert schema_kind == 'validate_call'\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin1()\n    with install_plugin(plugin):\n\n        @validate_call()\n        def foo(a: int):\n            return a\n\n    class Plugin2:\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert schema_type.__name__ == 'my_wrapped_function'\n            assert schema_type_path == SchemaTypePath(\n                'tests.test_plugins', 'partial(test_plugin_path_validate_call.<locals>.my_wrapped_function)'\n            )\n            assert schema_kind == 'validate_call'\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin2()\n    with install_plugin(plugin):\n\n        def my_wrapped_function(a: int, b: int, c: int):\n            return a + b + c\n\n        my_partial_function = partial(my_wrapped_function, c=3)\n        validate_call(my_partial_function)\n\n\ndef test_plugin_path_create_model() -> None:\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        pass\n\n    class Plugin:\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            assert schema_type.__name__ == 'FooModel'\n            assert list(schema_type.model_fields.keys()) == ['foo', 'bar']\n            assert schema_type_path == SchemaTypePath('tests.test_plugins', 'FooModel')\n            assert schema_kind == 'create_model'\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n        create_model('FooModel', foo=(str, ...), bar=(int, 123))\n\n\ndef test_plugin_path_complex() -> None:\n    paths: list[tuple(str, str)] = []\n\n    class CustomOnValidatePython(ValidatePythonHandlerProtocol):\n        pass\n\n    class Plugin:\n        def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n            paths.append((schema_type.__name__, schema_type_path, schema_kind))\n            return CustomOnValidatePython(), None, None\n\n    plugin = Plugin()\n    with install_plugin(plugin):\n\n        def foo():\n            class Model1(BaseModel):\n                pass\n\n        def bar():\n            class Model2(BaseModel):\n                pass\n\n        foo()\n        bar()\n\n    assert paths == [\n        (\n            'Model1',\n            SchemaTypePath('tests.test_plugins', 'test_plugin_path_complex.<locals>.foo.<locals>.Model1'),\n            'BaseModel',\n        ),\n        (\n            'Model2',\n            SchemaTypePath('tests.test_plugins', 'test_plugin_path_complex.<locals>.bar.<locals>.Model2'),\n            'BaseModel',\n        ),\n    ]\n", "tests/test_types_typeddict.py": "\"\"\"\nTests for TypedDict\n\"\"\"\n\nimport sys\nimport typing\nfrom typing import Any, Dict, Generic, List, Optional, TypeVar\n\nimport pytest\nimport typing_extensions\nfrom annotated_types import Lt\nfrom pydantic_core import CoreSchema, core_schema\nfrom typing_extensions import Annotated, TypedDict\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    GenerateSchema,\n    GetCoreSchemaHandler,\n    PositiveInt,\n    PydanticUserError,\n    ValidationError,\n    with_config,\n)\nfrom pydantic._internal._decorators import get_attribute_from_bases\nfrom pydantic.functional_serializers import field_serializer, model_serializer\nfrom pydantic.functional_validators import field_validator, model_validator\nfrom pydantic.type_adapter import TypeAdapter\n\nfrom .conftest import Err\n\n\n@pytest.fixture(\n    name='TypedDictAll',\n    params=[\n        pytest.param(typing, id='typing.TypedDict'),\n        pytest.param(typing_extensions, id='t_e.TypedDict'),\n    ],\n)\ndef fixture_typed_dict_all(request):\n    try:\n        return request.param.TypedDict\n    except AttributeError:\n        pytest.skip(f'TypedDict is not available from {request.param}')\n\n\n@pytest.fixture(name='TypedDict')\ndef fixture_typed_dict(TypedDictAll):\n    class TestTypedDict(TypedDictAll):\n        foo: str\n\n    if sys.version_info < (3, 12) and TypedDictAll.__module__ == 'typing':\n        pytest.skip('typing.TypedDict does not support all pydantic features in Python < 3.12')\n\n    if hasattr(TestTypedDict, '__required_keys__'):\n        return TypedDictAll\n    else:\n        pytest.skip('TypedDict does not include __required_keys__')\n\n\n@pytest.fixture(\n    name='req_no_req',\n    params=[\n        pytest.param(typing, id='typing.Required'),\n        pytest.param(typing_extensions, id='t_e.Required'),\n    ],\n)\ndef fixture_req_no_req(request):\n    try:\n        return request.param.Required, request.param.NotRequired\n    except AttributeError:\n        pytest.skip(f'Required and NotRequired are not available from {request.param}')\n\n\ndef test_typeddict_all(TypedDictAll):\n    class MyDict(TypedDictAll):\n        foo: str\n\n    try:\n\n        class M(BaseModel):\n            d: MyDict\n\n    except PydanticUserError as e:\n        assert e.message == 'Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.'\n    else:\n        assert M(d=dict(foo='baz')).d == {'foo': 'baz'}\n\n\ndef test_typeddict_annotated_simple(TypedDict, req_no_req):\n    Required, NotRequired = req_no_req\n\n    class MyDict(TypedDict):\n        foo: str\n        bar: Annotated[int, Lt(10)]\n        spam: NotRequired[float]\n\n    class M(BaseModel):\n        d: MyDict\n\n    assert M(d=dict(foo='baz', bar='8')).d == {'foo': 'baz', 'bar': 8}\n    assert M(d=dict(foo='baz', bar='8', spam='44.4')).d == {'foo': 'baz', 'bar': 8, 'spam': 44.4}\n    with pytest.raises(ValidationError, match=r'd\\.bar\\s+Field required \\[type=missing,'):\n        M(d=dict(foo='baz'))\n\n    with pytest.raises(ValidationError, match=r'd\\.bar\\s+Input should be less than 10 \\[type=less_than,'):\n        M(d=dict(foo='baz', bar='11'))\n\n\ndef test_typeddict_total_false(TypedDict, req_no_req):\n    Required, NotRequired = req_no_req\n\n    class MyDict(TypedDict, total=False):\n        foo: Required[str]\n        bar: int\n\n    class M(BaseModel):\n        d: MyDict\n\n    assert M(d=dict(foo='baz', bar='8')).d == {'foo': 'baz', 'bar': 8}\n    assert M(d=dict(foo='baz')).d == {'foo': 'baz'}\n    with pytest.raises(ValidationError, match=r'd\\.foo\\s+Field required \\[type=missing,'):\n        M(d={})\n\n\ndef test_typeddict(TypedDict):\n    class TD(TypedDict):\n        a: int\n        b: int\n        c: int\n        d: str\n\n    class Model(BaseModel):\n        td: TD\n\n    m = Model(td={'a': '3', 'b': b'1', 'c': 4, 'd': 'qwe'})\n    assert m.td == {'a': 3, 'b': 1, 'c': 4, 'd': 'qwe'}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(td={'a': [1], 'b': 2, 'c': 3, 'd': 'qwe'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('td', 'a'), 'msg': 'Input should be a valid integer', 'input': [1]}\n    ]\n\n\ndef test_typeddict_non_total(TypedDict):\n    class FullMovie(TypedDict, total=True):\n        name: str\n        year: int\n\n    class Model(BaseModel):\n        movie: FullMovie\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(movie={'year': '2002'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('movie', 'name'), 'msg': 'Field required', 'input': {'year': '2002'}}\n    ]\n\n    class PartialMovie(TypedDict, total=False):\n        name: str\n        year: int\n\n    class Model(BaseModel):\n        movie: PartialMovie\n\n    m = Model(movie={'year': '2002'})\n    assert m.movie == {'year': 2002}\n\n\ndef test_partial_new_typeddict(TypedDict):\n    class OptionalUser(TypedDict, total=False):\n        name: str\n\n    class User(OptionalUser):\n        id: int\n\n    class Model(BaseModel):\n        user: User\n\n    assert Model(user={'id': 1, 'name': 'foobar'}).user == {'id': 1, 'name': 'foobar'}\n    assert Model(user={'id': 1}).user == {'id': 1}\n\n\ndef test_typeddict_extra_default(TypedDict):\n    class User(TypedDict):\n        name: str\n        age: int\n\n    ta = TypeAdapter(User)\n\n    assert ta.validate_python({'name': 'pika', 'age': 7, 'rank': 1}) == {'name': 'pika', 'age': 7}\n\n    class UserExtraAllow(User):\n        __pydantic_config__ = ConfigDict(extra='allow')\n\n    ta = TypeAdapter(UserExtraAllow)\n\n    assert ta.validate_python({'name': 'pika', 'age': 7, 'rank': 1}) == {'name': 'pika', 'age': 7, 'rank': 1}\n\n    class UserExtraForbid(User):\n        __pydantic_config__ = ConfigDict(extra='forbid')\n\n    ta = TypeAdapter(UserExtraForbid)\n\n    with pytest.raises(ValidationError) as exc_info:\n        ta.validate_python({'name': 'pika', 'age': 7, 'rank': 1})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('rank',), 'msg': 'Extra inputs are not permitted', 'input': 1}\n    ]\n\n\ndef test_typeddict_schema(TypedDict):\n    class Data(BaseModel):\n        a: int\n\n    class DataTD(TypedDict):\n        a: int\n\n    class CustomTD(TypedDict):\n        b: int\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source_type: Any, handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            schema = handler(source_type)\n            schema = handler.resolve_ref_schema(schema)\n            assert schema['type'] == 'typed-dict'\n            b = schema['fields']['b']['schema']\n            assert b['type'] == 'int'\n            b['gt'] = 0  # type: ignore\n            return schema\n\n    class Model(BaseModel):\n        data: Data\n        data_td: DataTD\n        custom_td: CustomTD\n\n    # insert_assert(Model.model_json_schema(mode='validation'))\n    assert Model.model_json_schema(mode='validation') == {\n        'type': 'object',\n        'properties': {\n            'data': {'$ref': '#/$defs/Data'},\n            'data_td': {'$ref': '#/$defs/DataTD'},\n            'custom_td': {'$ref': '#/$defs/CustomTD'},\n        },\n        'required': ['data', 'data_td', 'custom_td'],\n        'title': 'Model',\n        '$defs': {\n            'DataTD': {\n                'type': 'object',\n                'properties': {'a': {'type': 'integer', 'title': 'A'}},\n                'required': ['a'],\n                'title': 'DataTD',\n            },\n            'CustomTD': {\n                'type': 'object',\n                'properties': {'b': {'type': 'integer', 'exclusiveMinimum': 0, 'title': 'B'}},\n                'required': ['b'],\n                'title': 'CustomTD',\n            },\n            'Data': {\n                'type': 'object',\n                'properties': {'a': {'type': 'integer', 'title': 'A'}},\n                'required': ['a'],\n                'title': 'Data',\n            },\n        },\n    }\n\n    # insert_assert(Model.model_json_schema(mode='serialization'))\n    assert Model.model_json_schema(mode='serialization') == {\n        'type': 'object',\n        'properties': {\n            'data': {'$ref': '#/$defs/Data'},\n            'data_td': {'$ref': '#/$defs/DataTD'},\n            'custom_td': {'$ref': '#/$defs/CustomTD'},\n        },\n        'required': ['data', 'data_td', 'custom_td'],\n        'title': 'Model',\n        '$defs': {\n            'DataTD': {\n                'type': 'object',\n                'properties': {'a': {'type': 'integer', 'title': 'A'}},\n                'required': ['a'],\n                'title': 'DataTD',\n            },\n            'CustomTD': {\n                'type': 'object',\n                'properties': {'b': {'type': 'integer', 'exclusiveMinimum': 0, 'title': 'B'}},\n                'required': ['b'],\n                'title': 'CustomTD',\n            },\n            'Data': {\n                'type': 'object',\n                'properties': {'a': {'type': 'integer', 'title': 'A'}},\n                'required': ['a'],\n                'title': 'Data',\n            },\n        },\n    }\n\n\ndef test_typeddict_postponed_annotation(TypedDict):\n    class DataTD(TypedDict):\n        v: 'PositiveInt'\n\n    class Model(BaseModel):\n        t: DataTD\n\n    with pytest.raises(ValidationError):\n        Model.model_validate({'t': {'v': -1}})\n\n\ndef test_typeddict_required(TypedDict, req_no_req):\n    Required, _ = req_no_req\n\n    class DataTD(TypedDict, total=False):\n        a: int\n        b: Required[str]\n\n    class Model(BaseModel):\n        t: DataTD\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'t': {'$ref': '#/$defs/DataTD'}},\n        'required': ['t'],\n        '$defs': {\n            'DataTD': {\n                'title': 'DataTD',\n                'type': 'object',\n                'properties': {\n                    'a': {'title': 'A', 'type': 'integer'},\n                    'b': {'title': 'B', 'type': 'string'},\n                },\n                'required': ['b'],\n            }\n        },\n    }\n\n\ndef test_typeddict_from_attributes():\n    class UserCls:\n        def __init__(self, name: str, age: int):\n            self.name = name\n            self.age = age\n\n    class User(TypedDict):\n        name: str\n        age: int\n\n    class FromAttributesCls:\n        def __init__(self, u: User):\n            self.u = u\n\n    class Model(BaseModel):\n        u: Annotated[User, Field(strict=False)]\n\n    class FromAttributesModel(BaseModel, from_attributes=True):\n        u: Annotated[User, Field(strict=False)]\n\n    # You can validate the TypedDict from attributes from a type that has a field with an appropriate attribute\n    assert FromAttributesModel.model_validate(FromAttributesCls(u={'name': 'foo', 'age': 15}))\n\n    # The normal case: you can't populate a TypedDict from attributes with the relevant config setting disabled\n    with pytest.raises(ValidationError, match='Input should be a valid dictionary'):\n        Model(u=UserCls('foo', 15))\n\n    # Going further: even with from_attributes allowed, it won't attempt to populate a TypedDict from attributes\n    with pytest.raises(ValidationError, match='Input should be a valid dictionary'):\n        FromAttributesModel(u=UserCls('foo', 15))\n\n\ndef test_typeddict_not_required_schema(TypedDict, req_no_req):\n    Required, NotRequired = req_no_req\n\n    class DataTD(TypedDict, total=True):\n        a: NotRequired[int]\n        b: str\n\n    class Model(BaseModel):\n        t: DataTD\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'t': {'$ref': '#/$defs/DataTD'}},\n        'required': ['t'],\n        '$defs': {\n            'DataTD': {\n                'title': 'DataTD',\n                'type': 'object',\n                'properties': {\n                    'a': {'title': 'A', 'type': 'integer'},\n                    'b': {'title': 'B', 'type': 'string'},\n                },\n                'required': ['b'],\n            }\n        },\n    }\n\n\ndef test_typed_dict_inheritance_schema(TypedDict, req_no_req):\n    Required, NotRequired = req_no_req\n\n    class DataTDBase(TypedDict, total=True):\n        a: NotRequired[int]\n        b: str\n\n    class DataTD(DataTDBase, total=False):\n        c: Required[int]\n        d: str\n\n    class Model(BaseModel):\n        t: DataTD\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'t': {'$ref': '#/$defs/DataTD'}},\n        'required': ['t'],\n        '$defs': {\n            'DataTD': {\n                'title': 'DataTD',\n                'type': 'object',\n                'properties': {\n                    'a': {'title': 'A', 'type': 'integer'},\n                    'b': {'title': 'B', 'type': 'string'},\n                    'c': {'title': 'C', 'type': 'integer'},\n                    'd': {'title': 'D', 'type': 'string'},\n                },\n                'required': ['b', 'c'],\n            }\n        },\n    }\n\n\ndef test_typeddict_annotated_nonoptional_schema(TypedDict):\n    class DataTD(TypedDict):\n        a: Optional[int]\n        b: Annotated[Optional[int], Field(42)]\n        c: Annotated[Optional[int], Field(description='Test')]\n\n    class Model(BaseModel):\n        data_td: DataTD\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'data_td': {'$ref': '#/$defs/DataTD'}},\n        'required': ['data_td'],\n        '$defs': {\n            'DataTD': {\n                'type': 'object',\n                'title': 'DataTD',\n                'properties': {\n                    'a': {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'title': 'A'},\n                    'b': {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'default': 42, 'title': 'B'},\n                    'c': {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'description': 'Test', 'title': 'C'},\n                },\n                'required': ['a', 'c'],\n            },\n        },\n    }\n\n\n@pytest.mark.parametrize(\n    'input_value,expected',\n    [\n        ({'a': '1', 'b': 2, 'c': 3}, {'a': 1, 'b': 2, 'c': 3}),\n        ({'a': None, 'b': 2, 'c': 3}, {'a': None, 'b': 2, 'c': 3}),\n        ({'a': None, 'c': 3}, {'a': None, 'b': 42, 'c': 3}),\n        # ({}, None),\n        # ({'data_td': []}, None),\n        # ({'data_td': {'a': 1, 'b': 2, 'd': 4}}, None),\n    ],\n    ids=repr,\n)\ndef test_typeddict_annotated(TypedDict, input_value, expected):\n    class DataTD(TypedDict):\n        a: Optional[int]\n        b: Annotated[Optional[int], Field(42)]\n        c: Annotated[Optional[int], Field(description='Test', lt=4)]\n\n    class Model(BaseModel):\n        d: DataTD\n\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message_escaped()):\n            Model(d=input_value)\n    else:\n        assert Model(d=input_value).d == expected\n\n\ndef test_recursive_typeddict():\n    from typing import Optional\n\n    from typing_extensions import TypedDict\n\n    from pydantic import BaseModel\n\n    class RecursiveTypedDict(TypedDict):\n        foo: Optional['RecursiveTypedDict']\n\n    class RecursiveTypedDictModel(BaseModel):\n        rec: RecursiveTypedDict\n\n    assert RecursiveTypedDictModel(rec={'foo': {'foo': None}}).rec == {'foo': {'foo': None}}\n    with pytest.raises(ValidationError) as exc_info:\n        RecursiveTypedDictModel(rec={'foo': {'foo': {'foo': 1}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 1,\n            'loc': ('rec', 'foo', 'foo', 'foo'),\n            'msg': 'Input should be a valid dictionary',\n            'type': 'dict_type',\n        }\n    ]\n\n\nT = TypeVar('T')\n\n\ndef test_generic_typeddict_in_concrete_model():\n    T = TypeVar('T')\n\n    class GenericTypedDict(typing_extensions.TypedDict, Generic[T]):\n        x: T\n\n    class Model(BaseModel):\n        y: GenericTypedDict[int]\n\n    Model(y={'x': 1})\n    with pytest.raises(ValidationError) as exc_info:\n        Model(y={'x': 'a'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('y', 'x'),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_generic_typeddict_in_generic_model():\n    T = TypeVar('T')\n\n    class GenericTypedDict(typing_extensions.TypedDict, Generic[T]):\n        x: T\n\n    class Model(BaseModel, Generic[T]):\n        y: GenericTypedDict[T]\n\n    Model[int](y={'x': 1})\n    with pytest.raises(ValidationError) as exc_info:\n        Model[int](y={'x': 'a'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('y', 'x'),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_recursive_generic_typeddict_in_module(create_module):\n    @create_module\n    def module():\n        from typing import Generic, List, Optional, TypeVar\n\n        from typing_extensions import TypedDict\n\n        from pydantic import BaseModel\n\n        T = TypeVar('T')\n\n        class RecursiveGenTypedDictModel(BaseModel, Generic[T]):\n            rec: 'RecursiveGenTypedDict[T]'\n\n        class RecursiveGenTypedDict(TypedDict, Generic[T]):\n            foo: Optional['RecursiveGenTypedDict[T]']\n            ls: List[T]\n\n    int_data: module.RecursiveGenTypedDict[int] = {'foo': {'foo': None, 'ls': [1]}, 'ls': [1]}\n    assert module.RecursiveGenTypedDictModel[int](rec=int_data).rec == int_data\n\n    str_data: module.RecursiveGenTypedDict[str] = {'foo': {'foo': None, 'ls': ['a']}, 'ls': ['a']}\n    with pytest.raises(ValidationError) as exc_info:\n        module.RecursiveGenTypedDictModel[int](rec=str_data)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('rec', 'foo', 'ls', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n        {\n            'input': 'a',\n            'loc': ('rec', 'ls', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n    ]\n\n\ndef test_recursive_generic_typeddict_in_function_1():\n    T = TypeVar('T')\n\n    # First ordering: typed dict first\n    class RecursiveGenTypedDict(TypedDict, Generic[T]):\n        foo: Optional['RecursiveGenTypedDict[T]']\n        ls: List[T]\n\n    class RecursiveGenTypedDictModel(BaseModel, Generic[T]):\n        rec: 'RecursiveGenTypedDict[T]'\n\n    # Note: no model_rebuild() necessary here\n    # RecursiveGenTypedDictModel.model_rebuild()\n\n    int_data: RecursiveGenTypedDict[int] = {'foo': {'foo': None, 'ls': [1]}, 'ls': [1]}\n    assert RecursiveGenTypedDictModel[int](rec=int_data).rec == int_data\n\n    str_data: RecursiveGenTypedDict[str] = {'foo': {'foo': None, 'ls': ['a']}, 'ls': ['a']}\n    with pytest.raises(ValidationError) as exc_info:\n        RecursiveGenTypedDictModel[int](rec=str_data)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('rec', 'foo', 'ls', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n        {\n            'input': 'a',\n            'loc': ('rec', 'ls', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n    ]\n\n\ndef test_recursive_generic_typeddict_in_function_2():\n    T = TypeVar('T')\n\n    # Second ordering: model first\n    class RecursiveGenTypedDictModel(BaseModel, Generic[T]):\n        rec: 'RecursiveGenTypedDict[T]'\n\n    class RecursiveGenTypedDict(TypedDict, Generic[T]):\n        foo: Optional['RecursiveGenTypedDict[T]']\n        ls: List[T]\n\n    int_data: RecursiveGenTypedDict[int] = {'foo': {'foo': None, 'ls': [1]}, 'ls': [1]}\n    assert RecursiveGenTypedDictModel[int](rec=int_data).rec == int_data\n\n    str_data: RecursiveGenTypedDict[str] = {'foo': {'foo': None, 'ls': ['a']}, 'ls': ['a']}\n    with pytest.raises(ValidationError) as exc_info:\n        RecursiveGenTypedDictModel[int](rec=str_data)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('rec', 'foo', 'ls', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n        {\n            'input': 'a',\n            'loc': ('rec', 'ls', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n    ]\n\n\ndef test_recursive_generic_typeddict_in_function_3():\n    T = TypeVar('T')\n\n    class RecursiveGenTypedDictModel(BaseModel, Generic[T]):\n        rec: 'RecursiveGenTypedDict[T]'\n\n    IntModel = RecursiveGenTypedDictModel[int]\n\n    class RecursiveGenTypedDict(TypedDict, Generic[T]):\n        foo: Optional['RecursiveGenTypedDict[T]']\n        ls: List[T]\n\n    int_data: RecursiveGenTypedDict[int] = {'foo': {'foo': None, 'ls': [1]}, 'ls': [1]}\n    assert IntModel(rec=int_data).rec == int_data\n\n    str_data: RecursiveGenTypedDict[str] = {'foo': {'foo': None, 'ls': ['a']}, 'ls': ['a']}\n    with pytest.raises(ValidationError) as exc_info:\n        IntModel(rec=str_data)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('rec', 'foo', 'ls', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n        {\n            'input': 'a',\n            'loc': ('rec', 'ls', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n    ]\n\n\ndef test_typeddict_alias_generator(TypedDict):\n    def alias_generator(name: str) -> str:\n        return 'alias_' + name\n\n    class MyDict(TypedDict):\n        __pydantic_config__ = ConfigDict(alias_generator=alias_generator, extra='forbid')\n        foo: str\n\n    class Model(BaseModel):\n        d: MyDict\n\n    ta = TypeAdapter(MyDict)\n    model = ta.validate_python({'alias_foo': 'bar'})\n\n    assert model['foo'] == 'bar'\n\n    with pytest.raises(ValidationError) as exc_info:\n        ta.validate_python({'foo': 'bar'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('alias_foo',), 'msg': 'Field required', 'input': {'foo': 'bar'}},\n        {'input': 'bar', 'loc': ('foo',), 'msg': 'Extra inputs are not permitted', 'type': 'extra_forbidden'},\n    ]\n\n\ndef test_typeddict_inheritance(TypedDict: Any) -> None:\n    class Parent(TypedDict):\n        x: int\n\n    class Child(Parent):\n        y: float\n\n    ta = TypeAdapter(Child)\n    assert ta.validate_python({'x': '1', 'y': '1.0'}) == {'x': 1, 'y': 1.0}\n\n\ndef test_typeddict_field_validator(TypedDict: Any) -> None:\n    class Parent(TypedDict):\n        a: List[str]\n\n        @field_validator('a')\n        @classmethod\n        def parent_val_before(cls, v: List[str]):\n            v.append('parent before')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def val(cls, v: List[str]):\n            v.append('parent')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def parent_val_after(cls, v: List[str]):\n            v.append('parent after')\n            return v\n\n    class Child(Parent):\n        @field_validator('a')\n        @classmethod\n        def child_val_before(cls, v: List[str]):\n            v.append('child before')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def val(cls, v: List[str]):\n            v.append('child')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def child_val_after(cls, v: List[str]):\n            v.append('child after')\n            return v\n\n    parent_ta = TypeAdapter(Parent)\n    child_ta = TypeAdapter(Child)\n\n    assert parent_ta.validate_python({'a': []})['a'] == ['parent before', 'parent', 'parent after']\n    assert child_ta.validate_python({'a': []})['a'] == [\n        'parent before',\n        'child',\n        'parent after',\n        'child before',\n        'child after',\n    ]\n\n\ndef test_typeddict_model_validator(TypedDict) -> None:\n    class Model(TypedDict):\n        x: int\n        y: float\n\n        @model_validator(mode='before')\n        @classmethod\n        def val_model_before(cls, value: Dict[str, Any]) -> Dict[str, Any]:\n            return dict(x=value['x'] + 1, y=value['y'] + 2)\n\n        @model_validator(mode='after')\n        def val_model_after(self) -> 'Model':\n            return Model(x=self['x'] * 2, y=self['y'] * 3)\n\n    ta = TypeAdapter(Model)\n\n    assert ta.validate_python({'x': 1, 'y': 2.5}) == {'x': 4, 'y': 13.5}\n\n\ndef test_typeddict_field_serializer(TypedDict: Any) -> None:\n    class Parent(TypedDict):\n        a: List[str]\n\n        @field_serializer('a')\n        @classmethod\n        def ser(cls, v: List[str]):\n            v.append('parent')\n            return v\n\n    class Child(Parent):\n        @field_serializer('a')\n        @classmethod\n        def ser(cls, v: List[str]):\n            v.append('child')\n            return v\n\n    parent_ta = TypeAdapter(Parent)\n    child_ta = TypeAdapter(Child)\n\n    assert parent_ta.dump_python(Parent({'a': []}))['a'] == ['parent']\n    assert child_ta.dump_python(Child({'a': []}))['a'] == ['child']\n\n\ndef test_typeddict_model_serializer(TypedDict) -> None:\n    class Model(TypedDict):\n        x: int\n        y: float\n\n        @model_serializer(mode='plain')\n        def ser_model(self) -> Dict[str, Any]:\n            return {'x': self['x'] * 2, 'y': self['y'] * 3}\n\n    ta = TypeAdapter(Model)\n\n    assert ta.dump_python(Model({'x': 1, 'y': 2.5})) == {'x': 2, 'y': 7.5}\n\n\ndef test_model_config() -> None:\n    class Model(TypedDict):\n        x: str\n        __pydantic_config__ = ConfigDict(str_to_lower=True)  # type: ignore\n\n    ta = TypeAdapter(Model)\n\n    assert ta.validate_python({'x': 'ABC'}) == {'x': 'abc'}\n\n\ndef test_model_config_inherited() -> None:\n    class Base(TypedDict):\n        __pydantic_config__ = ConfigDict(str_to_lower=True)  # type: ignore\n\n    class Model(Base):\n        x: str\n\n    ta = TypeAdapter(Model)\n\n    assert ta.validate_python({'x': 'ABC'}) == {'x': 'abc'}\n\n\ndef test_schema_generator() -> None:\n    class LaxStrGenerator(GenerateSchema):\n        def str_schema(self) -> CoreSchema:\n            return core_schema.no_info_plain_validator_function(str)\n\n    class Model(TypedDict):\n        x: str\n        __pydantic_config__ = ConfigDict(schema_generator=LaxStrGenerator)  # type: ignore\n\n    ta = TypeAdapter(Model)\n\n    assert ta.validate_python(dict(x=1))['x'] == '1'\n\n\ndef test_grandparent_config():\n    class MyTypedDict(TypedDict):\n        __pydantic_config__ = ConfigDict(str_to_lower=True)\n        x: str\n\n    class MyMiddleTypedDict(MyTypedDict):\n        y: str\n\n    class MySubTypedDict(MyMiddleTypedDict):\n        z: str\n\n    validated_data = TypeAdapter(MySubTypedDict).validate_python({'x': 'ABC', 'y': 'DEF', 'z': 'GHI'})\n    assert validated_data == {'x': 'abc', 'y': 'def', 'z': 'ghi'}\n\n\ndef test_typeddict_mro():\n    class A(TypedDict):\n        x = 1\n\n    class B(A):\n        x = 2\n\n    class C(B):\n        pass\n\n    assert get_attribute_from_bases(C, 'x') == 2\n\n\ndef test_typeddict_with_config_decorator():\n    @with_config(ConfigDict(str_to_lower=True))\n    class Model(TypedDict):\n        x: str\n\n    ta = TypeAdapter(Model)\n\n    assert ta.validate_python({'x': 'ABC'}) == {'x': 'abc'}\n", "tests/conftest.py": "import importlib\nimport inspect\nimport os\nimport re\nimport secrets\nimport subprocess\nimport sys\nimport textwrap\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom types import FunctionType\nfrom typing import Any, Optional\n\nimport pytest\nfrom _pytest.assertion.rewrite import AssertionRewritingHook\n\nfrom pydantic import GenerateSchema\n\n\ndef pytest_addoption(parser: pytest.Parser):\n    parser.addoption('--test-mypy', action='store_true', help='run mypy tests')\n    parser.addoption('--update-mypy', action='store_true', help='update mypy tests')\n\n\ndef _extract_source_code_from_function(function):\n    if function.__code__.co_argcount:\n        raise RuntimeError(f'function {function.__qualname__} cannot have any arguments')\n\n    code_lines = ''\n    body_started = False\n    for line in textwrap.dedent(inspect.getsource(function)).split('\\n'):\n        if line.startswith('def '):\n            body_started = True\n            continue\n        elif body_started:\n            code_lines += f'{line}\\n'\n\n    return textwrap.dedent(code_lines)\n\n\ndef _create_module_file(code, tmp_path, name):\n    name = f'{name}_{secrets.token_hex(5)}'\n    path = tmp_path / f'{name}.py'\n    path.write_text(code)\n    return name, str(path)\n\n\n@pytest.fixture(scope='session', autouse=True)\ndef disable_error_urls():\n    # Don't add URLs during docs tests when printing\n    # Otherwise we'll get version numbers in the URLs that will update frequently\n    os.environ['PYDANTIC_ERRORS_INCLUDE_URL'] = 'false'\n\n\n@pytest.fixture\ndef create_module(tmp_path, request):\n    def run(source_code_or_function, rewrite_assertions=True, module_name_prefix=None):\n        \"\"\"\n        Create module object, execute it and return\n        Can be used as a decorator of the function from the source code of which the module will be constructed\n\n        :param source_code_or_function string or function with body as a source code for created module\n        :param rewrite_assertions: whether to rewrite assertions in module or not\n        :param module_name_prefix: string prefix to use in the name of the module, does not affect the name of the file.\n\n        \"\"\"\n        if isinstance(source_code_or_function, FunctionType):\n            source_code = _extract_source_code_from_function(source_code_or_function)\n        else:\n            source_code = source_code_or_function\n\n        module_name, filename = _create_module_file(source_code, tmp_path, request.node.name)\n        if module_name_prefix:\n            module_name = module_name_prefix + module_name\n\n        if rewrite_assertions:\n            loader = AssertionRewritingHook(config=request.config)\n            loader.mark_rewrite(module_name)\n        else:\n            loader = None\n\n        spec = importlib.util.spec_from_file_location(module_name, filename, loader=loader)\n        sys.modules[module_name] = module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return module\n\n    return run\n\n\n@pytest.fixture\ndef subprocess_run_code(tmp_path: Path):\n    def run_code(source_code_or_function) -> str:\n        if isinstance(source_code_or_function, FunctionType):\n            source_code = _extract_source_code_from_function(source_code_or_function)\n        else:\n            source_code = source_code_or_function\n\n        py_file = tmp_path / 'test.py'\n        py_file.write_text(source_code)\n\n        return subprocess.check_output([sys.executable, str(py_file)], cwd=tmp_path, encoding='utf8')\n\n    return run_code\n\n\n@dataclass\nclass Err:\n    message: str\n    errors: Optional[Any] = None\n\n    def __repr__(self):\n        if self.errors:\n            return f'Err({self.message!r}, errors={self.errors!r})'\n        else:\n            return f'Err({self.message!r})'\n\n    def message_escaped(self):\n        return re.escape(self.message)\n\n\n@dataclass\nclass CallCounter:\n    count: int = 0\n\n    def reset(self) -> None:\n        self.count = 0\n\n\n@pytest.fixture\ndef generate_schema_calls(monkeypatch) -> CallCounter:\n    orig_generate_schema = GenerateSchema.generate_schema\n    counter = CallCounter()\n    depth = 0  # generate_schema can be called recursively\n\n    def generate_schema_call_counter(*args: Any, **kwargs: Any) -> Any:\n        nonlocal depth\n        counter.count += 1 if depth == 0 else 0\n        depth += 1\n        try:\n            return orig_generate_schema(*args, **kwargs)\n        finally:\n            depth -= 1\n\n    monkeypatch.setattr(GenerateSchema, 'generate_schema', generate_schema_call_counter)\n    return counter\n", "tests/test_serialize.py": "\"\"\"\nNew tests for v2 of serialization logic.\n\"\"\"\n\nimport json\nimport re\nimport sys\nfrom enum import Enum\nfrom functools import partial, partialmethod\nfrom typing import Any, Callable, ClassVar, Dict, List, Optional, Pattern, Union\n\nimport pytest\nfrom pydantic_core import PydanticSerializationError, core_schema, to_jsonable_python\nfrom typing_extensions import Annotated, TypedDict\n\nfrom pydantic import (\n    BaseModel,\n    Field,\n    FieldSerializationInfo,\n    PydanticUserError,\n    SerializationInfo,\n    SerializerFunctionWrapHandler,\n    TypeAdapter,\n    computed_field,\n    errors,\n    field_serializer,\n    model_serializer,\n)\nfrom pydantic.config import ConfigDict\nfrom pydantic.functional_serializers import PlainSerializer, WrapSerializer\n\n\ndef test_serialize_extra_allow() -> None:\n    class Model(BaseModel):\n        x: int\n        model_config = ConfigDict(extra='allow')\n\n    m = Model(x=1, y=2)\n    assert m.y == 2\n    assert m.model_dump() == {'x': 1, 'y': 2}\n    assert json.loads(m.model_dump_json()) == {'x': 1, 'y': 2}\n\n\ndef test_serialize_extra_allow_subclass_1() -> None:\n    class Parent(BaseModel):\n        x: int\n\n    class Child(Parent):\n        model_config = ConfigDict(extra='allow')\n\n    class Model(BaseModel):\n        inner: Parent\n\n    m = Model(inner=Child(x=1, y=2))\n    assert m.inner.y == 2\n    assert m.model_dump() == {'inner': {'x': 1}}\n    assert json.loads(m.model_dump_json()) == {'inner': {'x': 1}}\n\n\ndef test_serialize_extra_allow_subclass_2() -> None:\n    class Parent(BaseModel):\n        x: int\n        model_config = ConfigDict(extra='allow')\n\n    class Child(Parent):\n        y: int\n\n    class Model(BaseModel):\n        inner: Parent\n\n    m = Model(inner=Child(x=1, y=2))\n    assert m.inner.y == 2\n    assert m.model_dump() == {'inner': {'x': 1}}\n    assert json.loads(m.model_dump_json()) == {'inner': {'x': 1}}\n\n    m = Model(inner=Parent(x=1, y=2))\n    assert m.inner.y == 2\n    assert m.model_dump() == {'inner': {'x': 1, 'y': 2}}\n    assert json.loads(m.model_dump_json()) == {'inner': {'x': 1, 'y': 2}}\n\n\ndef test_serializer_annotated_plain_always():\n    FancyInt = Annotated[int, PlainSerializer(lambda x: f'{x:,}', return_type=str)]\n\n    class MyModel(BaseModel):\n        x: FancyInt\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,234'}\n    assert MyModel(x=1234).model_dump(mode='json') == {'x': '1,234'}\n    assert MyModel(x=1234).model_dump_json() == '{\"x\":\"1,234\"}'\n\n\ndef test_serializer_annotated_plain_json():\n    FancyInt = Annotated[int, PlainSerializer(lambda x: f'{x:,}', return_type=str, when_used='json')]\n\n    class MyModel(BaseModel):\n        x: FancyInt\n\n    assert MyModel(x=1234).model_dump() == {'x': 1234}\n    assert MyModel(x=1234).model_dump(mode='json') == {'x': '1,234'}\n    assert MyModel(x=1234).model_dump_json() == '{\"x\":\"1,234\"}'\n\n\ndef test_serializer_annotated_wrap_always():\n    def ser_wrap(v: Any, nxt: SerializerFunctionWrapHandler) -> str:\n        return f'{nxt(v + 1):,}'\n\n    FancyInt = Annotated[int, WrapSerializer(ser_wrap, return_type=str)]\n\n    class MyModel(BaseModel):\n        x: FancyInt\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,235'}\n    assert MyModel(x=1234).model_dump(mode='json') == {'x': '1,235'}\n    assert MyModel(x=1234).model_dump_json() == '{\"x\":\"1,235\"}'\n\n\ndef test_serializer_annotated_wrap_json():\n    def ser_wrap(v: Any, nxt: SerializerFunctionWrapHandler) -> str:\n        return f'{nxt(v + 1):,}'\n\n    FancyInt = Annotated[int, WrapSerializer(ser_wrap, when_used='json')]\n\n    class MyModel(BaseModel):\n        x: FancyInt\n\n    assert MyModel(x=1234).model_dump() == {'x': 1234}\n    assert MyModel(x=1234).model_dump(mode='json') == {'x': '1,235'}\n    assert MyModel(x=1234).model_dump_json() == '{\"x\":\"1,235\"}'\n\n\n@pytest.mark.parametrize(\n    'serializer, func',\n    [\n        (PlainSerializer, lambda v: f'{v + 1:,}'),\n        (WrapSerializer, lambda v, nxt: f'{nxt(v + 1):,}'),\n    ],\n)\ndef test_serializer_annotated_typing_cache(serializer, func):\n    FancyInt = Annotated[int, serializer(func)]\n\n    class FancyIntModel(BaseModel):\n        x: Optional[FancyInt]\n\n    assert FancyIntModel(x=1234).model_dump() == {'x': '1,235'}\n\n\ndef test_serialize_decorator_always():\n    class MyModel(BaseModel):\n        x: Optional[int]\n\n        @field_serializer('x')\n        def customise_x_serialization(v, _info) -> str:\n            return f'{v:,}'\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,234'}\n    assert MyModel(x=1234).model_dump(mode='json') == {'x': '1,234'}\n    assert MyModel(x=1234).model_dump_json() == '{\"x\":\"1,234\"}'\n    m = MyModel(x=None)\n    # can't use v:, on None, hence error\n    error_msg = (\n        'Error calling function `customise_x_serialization`: '\n        'TypeError: unsupported format string passed to NoneType.__format__'\n    )\n    with pytest.raises(PydanticSerializationError, match=error_msg):\n        m.model_dump()\n    with pytest.raises(PydanticSerializationError, match=error_msg):\n        m.model_dump_json()\n\n\ndef test_serialize_decorator_json():\n    class MyModel(BaseModel):\n        x: int\n\n        @field_serializer('x', when_used='json')\n        def customise_x_serialization(v) -> str:\n            return f'{v:,}'\n\n    assert MyModel(x=1234).model_dump() == {'x': 1234}\n    assert MyModel(x=1234).model_dump(mode='json') == {'x': '1,234'}\n    assert MyModel(x=1234).model_dump_json() == '{\"x\":\"1,234\"}'\n\n\ndef test_serialize_decorator_unless_none():\n    class MyModel(BaseModel):\n        x: Optional[int]\n\n        @field_serializer('x', when_used='unless-none')\n        def customise_x_serialization(v):\n            return f'{v:,}'\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,234'}\n    assert MyModel(x=None).model_dump() == {'x': None}\n    assert MyModel(x=1234).model_dump(mode='json') == {'x': '1,234'}\n    assert MyModel(x=None).model_dump(mode='json') == {'x': None}\n    assert MyModel(x=1234).model_dump_json() == '{\"x\":\"1,234\"}'\n    assert MyModel(x=None).model_dump_json() == '{\"x\":null}'\n\n\ndef test_annotated_customisation():\n    def parse_int(s: str, _: Any) -> int:\n        return int(s.replace(',', ''))\n\n    class CommaFriendlyIntLogic:\n        @classmethod\n        def __get_pydantic_core_schema__(cls, _source, _handler):\n            # here we ignore the schema argument (which is just `{'type': 'int'}`) and return our own\n            return core_schema.with_info_before_validator_function(\n                parse_int,\n                core_schema.int_schema(),\n                serialization=core_schema.format_ser_schema(',', when_used='unless-none'),\n            )\n\n    CommaFriendlyInt = Annotated[int, CommaFriendlyIntLogic]\n\n    class MyModel(BaseModel):\n        x: CommaFriendlyInt\n\n    m = MyModel(x='1,000')\n    assert m.x == 1000\n    assert m.model_dump(mode='json') == {'x': '1,000'}\n    assert m.model_dump_json() == '{\"x\":\"1,000\"}'\n\n\ndef test_serialize_valid_signatures():\n    def ser_plain(v: Any, info: SerializationInfo) -> Any:\n        return f'{v:,}'\n\n    def ser_plain_no_info(v: Any, unrelated_arg: int = 1, other_unrelated_arg: int = 2) -> Any:\n        # Arguments with default values are not treated as info arg.\n        return f'{v:,}'\n\n    def ser_wrap(v: Any, nxt: SerializerFunctionWrapHandler, info: SerializationInfo) -> Any:\n        return f'{nxt(v):,}'\n\n    class MyModel(BaseModel):\n        f1: int\n        f2: int\n        f3: int\n        f4: int\n        f5: int\n\n        @field_serializer('f1')\n        def ser_f1(self, v: Any, info: FieldSerializationInfo) -> Any:\n            assert self.f1 == 1_000\n            assert v == 1_000\n            assert info.field_name == 'f1'\n            return f'{v:,}'\n\n        @field_serializer('f2', mode='wrap')\n        def ser_f2(self, v: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo) -> Any:\n            assert self.f2 == 2_000\n            assert v == 2_000\n            assert info.field_name == 'f2'\n            return f'{nxt(v):,}'\n\n        ser_f3 = field_serializer('f3')(ser_plain)\n        ser_f4 = field_serializer('f4')(ser_plain_no_info)\n        ser_f5 = field_serializer('f5', mode='wrap')(ser_wrap)\n\n    m = MyModel(**{f'f{x}': x * 1_000 for x in range(1, 9)})\n\n    assert m.model_dump() == {\n        'f1': '1,000',\n        'f2': '2,000',\n        'f3': '3,000',\n        'f4': '4,000',\n        'f5': '5,000',\n    }\n    assert m.model_dump_json() == '{\"f1\":\"1,000\",\"f2\":\"2,000\",\"f3\":\"3,000\",\"f4\":\"4,000\",\"f5\":\"5,000\"}'\n\n\ndef test_invalid_signature_no_params() -> None:\n    with pytest.raises(TypeError, match='Unrecognized field_serializer function signature'):\n\n        class _(BaseModel):\n            x: int\n\n            # caught by type checkers\n            @field_serializer('x')\n            def no_args() -> Any: ...\n\n\ndef test_invalid_signature_single_params() -> None:\n    with pytest.raises(TypeError, match='Unrecognized field_serializer function signature'):\n\n        class _(BaseModel):\n            x: int\n\n            # not caught by type checkers\n            @field_serializer('x')\n            def no_args(self) -> Any: ...\n\n\ndef test_invalid_signature_too_many_params_1() -> None:\n    with pytest.raises(TypeError, match='Unrecognized field_serializer function signature'):\n\n        class _(BaseModel):\n            x: int\n\n            # caught by type checkers\n            @field_serializer('x')\n            def no_args(self, value: Any, nxt: Any, info: Any, extra_param: Any) -> Any: ...\n\n\ndef test_invalid_signature_too_many_params_2() -> None:\n    with pytest.raises(TypeError, match='Unrecognized field_serializer function signature'):\n\n        class _(BaseModel):\n            x: int\n\n            # caught by type checkers\n            @field_serializer('x')\n            @staticmethod\n            def no_args(not_self: Any, value: Any, nxt: Any, info: Any) -> Any: ...\n\n\ndef test_invalid_signature_bad_plain_signature() -> None:\n    with pytest.raises(TypeError, match='Unrecognized field_serializer function signature for'):\n\n        class _(BaseModel):\n            x: int\n\n            # caught by type checkers\n            @field_serializer('x', mode='plain')\n            def no_args(self, value: Any, nxt: Any, info: Any) -> Any: ...\n\n\ndef test_serialize_ignore_info_plain():\n    class MyModel(BaseModel):\n        x: int\n\n        @field_serializer('x')\n        def ser_x(v: Any) -> str:\n            return f'{v:,}'\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,234'}\n\n\ndef test_serialize_ignore_info_wrap():\n    class MyModel(BaseModel):\n        x: int\n\n        @field_serializer('x', mode='wrap')\n        def ser_x(v: Any, handler: SerializerFunctionWrapHandler) -> str:\n            return f'{handler(v):,}'\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,234'}\n\n\ndef test_serialize_decorator_self_info():\n    class MyModel(BaseModel):\n        x: Optional[int]\n\n        @field_serializer('x')\n        def customise_x_serialization(self, v, info) -> str:\n            return f'{info.mode}:{v:,}'\n\n    assert MyModel(x=1234).model_dump() == {'x': 'python:1,234'}\n    assert MyModel(x=1234).model_dump(mode='foobar') == {'x': 'foobar:1,234'}\n\n\ndef test_serialize_decorator_self_no_info():\n    class MyModel(BaseModel):\n        x: Optional[int]\n\n        @field_serializer('x')\n        def customise_x_serialization(self, v) -> str:\n            return f'{v:,}'\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,234'}\n\n\ndef test_model_serializer_plain():\n    class MyModel(BaseModel):\n        a: int\n        b: bytes\n\n        @model_serializer\n        def _serialize(self):\n            if self.b == b'custom':\n                return f'MyModel(a={self.a!r}, b={self.b!r})'\n            else:\n                return self.__dict__\n\n    m = MyModel(a=1, b='boom')\n    assert m.model_dump() == {'a': 1, 'b': b'boom'}\n    assert m.model_dump(mode='json') == {'a': 1, 'b': 'boom'}\n    assert m.model_dump_json() == '{\"a\":1,\"b\":\"boom\"}'\n\n    assert m.model_dump(exclude={'a'}) == {'a': 1, 'b': b'boom'}  # exclude is ignored as we used self.__dict__\n    assert m.model_dump(mode='json', exclude={'a'}) == {'a': 1, 'b': 'boom'}\n    assert m.model_dump_json(exclude={'a'}) == '{\"a\":1,\"b\":\"boom\"}'\n\n    m = MyModel(a=1, b='custom')\n    assert m.model_dump() == \"MyModel(a=1, b=b'custom')\"\n    assert m.model_dump(mode='json') == \"MyModel(a=1, b=b'custom')\"\n    assert m.model_dump_json() == '\"MyModel(a=1, b=b\\'custom\\')\"'\n\n\ndef test_model_serializer_plain_info():\n    class MyModel(BaseModel):\n        a: int\n        b: bytes\n\n        @model_serializer\n        def _serialize(self, info):\n            if info.exclude:\n                return {k: v for k, v in self.__dict__.items() if k not in info.exclude}\n            else:\n                return self.__dict__\n\n    m = MyModel(a=1, b='boom')\n    assert m.model_dump() == {'a': 1, 'b': b'boom'}\n    assert m.model_dump(mode='json') == {'a': 1, 'b': 'boom'}\n    assert m.model_dump_json() == '{\"a\":1,\"b\":\"boom\"}'\n\n    assert m.model_dump(exclude={'a'}) == {'b': b'boom'}  # exclude is not ignored\n    assert m.model_dump(mode='json', exclude={'a'}) == {'b': 'boom'}\n    assert m.model_dump_json(exclude={'a'}) == '{\"b\":\"boom\"}'\n\n\ndef test_model_serializer_wrap():\n    class MyModel(BaseModel):\n        a: int\n        b: bytes\n        c: bytes = Field(exclude=True)\n\n        @model_serializer(mode='wrap')\n        def _serialize(self, handler):\n            d = handler(self)\n            d['extra'] = 42\n            return d\n\n    m = MyModel(a=1, b='boom', c='excluded')\n    assert m.model_dump() == {'a': 1, 'b': b'boom', 'extra': 42}\n    assert m.model_dump(mode='json') == {'a': 1, 'b': 'boom', 'extra': 42}\n    assert m.model_dump_json() == '{\"a\":1,\"b\":\"boom\",\"extra\":42}'\n\n    assert m.model_dump(exclude={'a'}) == {'b': b'boom', 'extra': 42}\n    assert m.model_dump(mode='json', exclude={'a'}) == {'b': 'boom', 'extra': 42}\n    assert m.model_dump_json(exclude={'a'}) == '{\"b\":\"boom\",\"extra\":42}'\n\n\ndef test_model_serializer_wrap_info():\n    class MyModel(BaseModel):\n        a: int\n        b: bytes\n        c: bytes = Field(exclude=True)\n\n        @model_serializer(mode='wrap')\n        def _serialize(self, handler, info):\n            d = handler(self)\n            d['info'] = f'mode={info.mode} exclude={info.exclude}'\n            return d\n\n    m = MyModel(a=1, b='boom', c='excluded')\n    assert m.model_dump() == {'a': 1, 'b': b'boom', 'info': 'mode=python exclude=None'}\n    assert m.model_dump(mode='json') == {'a': 1, 'b': 'boom', 'info': 'mode=json exclude=None'}\n    assert m.model_dump_json() == '{\"a\":1,\"b\":\"boom\",\"info\":\"mode=json exclude=None\"}'\n\n    assert m.model_dump(exclude={'a'}) == {'b': b'boom', 'info': \"mode=python exclude={'a'}\"}\n    assert m.model_dump(mode='json', exclude={'a'}) == {'b': 'boom', 'info': \"mode=json exclude={'a'}\"}\n    assert m.model_dump_json(exclude={'a'}) == '{\"b\":\"boom\",\"info\":\"mode=json exclude={\\'a\\'}\"}'\n\n\ndef test_model_serializer_plain_json_return_type():\n    class MyModel(BaseModel):\n        a: int\n\n        @model_serializer(when_used='json')\n        def _serialize(self) -> str:\n            if self.a == 666:\n                return self.a\n            else:\n                return f'MyModel(a={self.a!r})'\n\n    m = MyModel(a=1)\n    assert m.model_dump() == {'a': 1}\n    assert m.model_dump(mode='json') == 'MyModel(a=1)'\n    assert m.model_dump_json() == '\"MyModel(a=1)\"'\n\n    m = MyModel(a=666)\n    assert m.model_dump() == {'a': 666}\n    with pytest.warns(UserWarning, match='Expected `str` but got `int` - serialized value may not be as expected'):\n        assert m.model_dump(mode='json') == 666\n\n    with pytest.warns(UserWarning, match='Expected `str` but got `int` - serialized value may not be as expected'):\n        assert m.model_dump_json() == '666'\n\n\ndef test_model_serializer_wrong_args():\n    m = (\n        r'Unrecognized model_serializer function signature for '\n        r'<.+MyModel._serialize at 0x\\w+> with `mode=plain`:\\(self, x, y, z\\)'\n    )\n    with pytest.raises(TypeError, match=m):\n\n        class MyModel(BaseModel):\n            a: int\n\n            @model_serializer\n            def _serialize(self, x, y, z):\n                return self\n\n\ndef test_model_serializer_no_self():\n    with pytest.raises(TypeError, match='`@model_serializer` must be applied to instance methods'):\n\n        class MyModel(BaseModel):\n            a: int\n\n            @model_serializer\n            def _serialize(slf, x, y, z):\n                return slf\n\n\ndef test_model_serializer_classmethod():\n    with pytest.raises(TypeError, match='`@model_serializer` must be applied to instance methods'):\n\n        class MyModel(BaseModel):\n            a: int\n\n            @model_serializer\n            @classmethod\n            def _serialize(self, x, y, z):\n                return self\n\n\ndef test_field_multiple_serializer():\n    m = \"Multiple field serializer functions were defined for field 'x', this is not allowed.\"\n    with pytest.raises(TypeError, match=m):\n\n        class MyModel(BaseModel):\n            x: int\n            y: int\n\n            @field_serializer('x', 'y')\n            def serializer1(v) -> str:\n                return f'{v:,}'\n\n            @field_serializer('x')\n            def serializer2(v) -> str:\n                return v\n\n\ndef test_field_multiple_serializer_subclass():\n    class MyModel(BaseModel):\n        x: int\n\n        @field_serializer('x')\n        def serializer1(v) -> str:\n            return f'{v:,}'\n\n    class MySubModel(MyModel):\n        @field_serializer('x')\n        def serializer1(v) -> str:\n            return f'{v}'\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,234'}\n    assert MySubModel(x=1234).model_dump() == {'x': '1234'}\n\n\ndef test_serialize_all_fields():\n    class MyModel(BaseModel):\n        x: int\n\n        @field_serializer('*')\n        @classmethod\n        def serialize_all(cls, v: Any):\n            return v * 2\n\n    assert MyModel(x=10).model_dump() == {'x': 20}\n\n\ndef int_ser_func_without_info1(v: int, expected: int) -> str:\n    return f'{v:,}'\n\n\ndef int_ser_func_without_info2(v: int, *, expected: int) -> str:\n    return f'{v:,}'\n\n\ndef int_ser_func_with_info1(v: int, info: FieldSerializationInfo, expected: int) -> str:\n    return f'{v:,}'\n\n\ndef int_ser_func_with_info2(v: int, info: FieldSerializationInfo, *, expected: int) -> str:\n    return f'{v:,}'\n\n\ndef int_ser_instance_method_without_info1(self: Any, v: int, *, expected: int) -> str:\n    assert self.x == v\n    return f'{v:,}'\n\n\ndef int_ser_instance_method_without_info2(self: Any, v: int, expected: int) -> str:\n    assert self.x == v\n    return f'{v:,}'\n\n\ndef int_ser_instance_method_with_info1(self: Any, v: int, info: FieldSerializationInfo, expected: int) -> str:\n    assert self.x == v\n    return f'{v:,}'\n\n\ndef int_ser_instance_method_with_info2(self: Any, v: int, info: FieldSerializationInfo, *, expected: int) -> str:\n    assert self.x == v\n    return f'{v:,}'\n\n\n@pytest.mark.parametrize(\n    'func',\n    [\n        int_ser_func_with_info1,\n        int_ser_func_with_info2,\n        int_ser_func_without_info1,\n        int_ser_func_without_info2,\n        int_ser_instance_method_with_info1,\n        int_ser_instance_method_with_info2,\n        int_ser_instance_method_without_info1,\n        int_ser_instance_method_without_info2,\n    ],\n)\ndef test_serialize_partial(\n    func: Any,\n):\n    class MyModel(BaseModel):\n        x: int\n\n        ser = field_serializer('x', return_type=str)(partial(func, expected=1234))\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,234'}\n\n\n@pytest.mark.parametrize(\n    'func',\n    [\n        int_ser_func_with_info1,\n        int_ser_func_with_info2,\n        int_ser_func_without_info1,\n        int_ser_func_without_info2,\n        int_ser_instance_method_with_info1,\n        int_ser_instance_method_with_info2,\n        int_ser_instance_method_without_info1,\n        int_ser_instance_method_without_info2,\n    ],\n)\ndef test_serialize_partialmethod(\n    func: Any,\n):\n    class MyModel(BaseModel):\n        x: int\n\n        ser = field_serializer('x', return_type=str)(partialmethod(func, expected=1234))\n\n    assert MyModel(x=1234).model_dump() == {'x': '1,234'}\n\n\ndef test_serializer_allow_reuse_inheritance_override():\n    class Parent(BaseModel):\n        x: int\n\n        @field_serializer('x')\n        def ser_x(self, _v: int, _info: SerializationInfo) -> str:\n            return 'parent_encoder'\n\n    # overriding a serializer with a function / class var\n    # of the same name is allowed\n    # to mimic how inheritance works\n    # the serializer in the child class replaces the parent\n    # (without modifying the parent class itself)\n    class Child1(Parent):\n        @field_serializer('x')\n        def ser_x(self, _v: int, _info: SerializationInfo) -> str:\n            return 'child1_encoder' + ' ' + super().ser_x(_v, _info)\n\n    assert Parent(x=1).model_dump_json() == '{\"x\":\"parent_encoder\"}'\n    assert Child1(x=1).model_dump_json() == '{\"x\":\"child1_encoder parent_encoder\"}'\n\n    # defining an _different_ serializer on the other hand is not allowed\n    # because they would both \"exist\" thus causing confusion\n    # since it's not clear if both or just one will run\n    msg = 'Multiple field serializer functions were defined ' \"for field 'x', this is not allowed.\"\n    with pytest.raises(TypeError, match=msg):\n\n        class _(Parent):\n            @field_serializer('x')\n            def ser_x_other(self, _v: int) -> str:\n                return 'err'\n\n    # the same thing applies if defined on the same class\n    with pytest.raises(TypeError, match=msg):\n\n        class _(BaseModel):\n            x: int\n\n            @field_serializer('x')\n            def ser_x(self, _v: int) -> str:\n                return 'parent_encoder'\n\n            @field_serializer('x')\n            def other_func_name(self, _v: int) -> str:\n                return 'parent_encoder'\n\n\ndef test_serializer_allow_reuse_same_field():\n    with pytest.warns(UserWarning, match='`ser_x` overrides an existing Pydantic `@field_serializer` decorator'):\n\n        class Model(BaseModel):\n            x: int\n\n            @field_serializer('x')\n            def ser_x(self, _v: int) -> str:\n                return 'ser_1'\n\n            @field_serializer('x')\n            def ser_x(self, _v: int) -> str:\n                return 'ser_2'\n\n        assert Model(x=1).model_dump() == {'x': 'ser_2'}\n\n\ndef test_serializer_allow_reuse_different_field_1():\n    with pytest.warns(UserWarning, match='`ser` overrides an existing Pydantic `@field_serializer` decorator'):\n\n        class Model(BaseModel):\n            x: int\n            y: int\n\n            @field_serializer('x')\n            def ser(self, _v: int) -> str:\n                return 'x'\n\n            @field_serializer('y')\n            def ser(self, _v: int) -> str:\n                return 'y'\n\n    assert Model(x=1, y=2).model_dump() == {'x': 1, 'y': 'y'}\n\n\ndef test_serializer_allow_reuse_different_field_2():\n    with pytest.warns(UserWarning, match='`ser_x` overrides an existing Pydantic `@field_serializer` decorator'):\n\n        def ser(self: Any, _v: int, _info: Any) -> str:\n            return 'ser'\n\n        class Model(BaseModel):\n            x: int\n            y: int\n\n            @field_serializer('x')\n            def ser_x(self, _v: int) -> str:\n                return 'ser_x'\n\n            ser_x = field_serializer('y')(ser)\n\n    assert Model(x=1, y=2).model_dump() == {'x': 1, 'y': 'ser'}\n\n\ndef test_serializer_allow_reuse_different_field_3():\n    with pytest.warns(UserWarning, match='`ser_x` overrides an existing Pydantic `@field_serializer` decorator'):\n\n        def ser1(self: Any, _v: int, _info: Any) -> str:\n            return 'ser1'\n\n        def ser2(self: Any, _v: int, _info: Any) -> str:\n            return 'ser2'\n\n        class Model(BaseModel):\n            x: int\n            y: int\n\n            ser_x = field_serializer('x')(ser1)\n            ser_x = field_serializer('y')(ser2)\n\n    assert Model(x=1, y=2).model_dump() == {'x': 1, 'y': 'ser2'}\n\n\ndef test_serializer_allow_reuse_different_field_4():\n    def ser(self: Any, _v: int, _info: Any) -> str:\n        return f'{_v:,}'\n\n    class Model(BaseModel):\n        x: int\n        y: int\n\n        ser_x = field_serializer('x')(ser)\n        not_ser_x = field_serializer('y')(ser)\n\n    assert Model(x=1_000, y=2_000).model_dump() == {'x': '1,000', 'y': '2,000'}\n\n\ndef test_serialize_any_model():\n    class Model(BaseModel):\n        m: str\n\n        @field_serializer('m')\n        def ser_m(self, v: str, _info: SerializationInfo) -> str:\n            return f'custom:{v}'\n\n    class AnyModel(BaseModel):\n        x: Any\n\n    m = Model(m='test')\n    assert m.model_dump() == {'m': 'custom:test'}\n    assert to_jsonable_python(AnyModel(x=m)) == {'x': {'m': 'custom:test'}}\n    assert AnyModel(x=m).model_dump() == {'x': {'m': 'custom:test'}}\n\n\ndef test_invalid_field():\n    msg = (\n        r'Decorators defined with incorrect fields:'\n        r' tests.test_serialize.test_invalid_field.<locals>.Model:\\d+.customise_b_serialization'\n        r\" \\(use check_fields=False if you're inheriting from the model and intended this\\)\"\n    )\n    with pytest.raises(errors.PydanticUserError, match=msg):\n\n        class Model(BaseModel):\n            a: str\n\n            @field_serializer('b')\n            def customise_b_serialization(v):\n                return v\n\n\ndef test_serialize_with_extra():\n    class Inner(BaseModel):\n        a: str = 'a'\n\n    class Outer(BaseModel):\n        # this cause the inner model incorrectly dumpped:\n        model_config = ConfigDict(extra='allow')\n        inner: Inner = Field(default_factory=Inner)\n\n    m = Outer.model_validate({})\n\n    assert m.model_dump() == {'inner': {'a': 'a'}}\n\n\ndef test_model_serializer_nested_models() -> None:\n    class Model(BaseModel):\n        x: int\n        inner: Optional['Model']\n\n        @model_serializer(mode='wrap')\n        def ser_model(self, handler: Callable[['Model'], Dict[str, Any]]) -> Dict[str, Any]:\n            inner = handler(self)\n            inner['x'] += 1\n            return inner\n\n    assert Model(x=0, inner=None).model_dump() == {'x': 1, 'inner': None}\n\n    assert Model(x=2, inner=Model(x=1, inner=Model(x=0, inner=None))).model_dump() == {\n        'x': 3,\n        'inner': {'x': 2, 'inner': {'x': 1, 'inner': None}},\n    }\n\n\ndef test_pattern_serialize():\n    ta = TypeAdapter(Pattern[str])\n    pattern = re.compile('^regex$')\n    assert ta.dump_python(pattern) == pattern\n    assert ta.dump_python(pattern, mode='json') == '^regex$'\n    assert ta.dump_json(pattern) == b'\"^regex$\"'\n\n\ndef test_custom_return_schema():\n    class Model(BaseModel):\n        x: int\n\n        @field_serializer('x', return_type=str)\n        def ser_model(self, v) -> int:\n            return repr(v)\n\n    return_serializer = re.search(r'return_serializer: *\\w+', repr(Model.__pydantic_serializer__)).group(0)\n    assert return_serializer == 'return_serializer: Str'\n\n\ndef test_clear_return_schema():\n    class Model(BaseModel):\n        x: int\n\n        @field_serializer('x', return_type=Any)\n        def ser_model(self, v) -> int:\n            return repr(v)\n\n    return_serializer = re.search(r'return_serializer: *\\w+', repr(Model.__pydantic_serializer__)).group(0)\n    assert return_serializer == 'return_serializer: Any'\n\n\ndef test_type_adapter_dump_json():\n    class Model(TypedDict):\n        x: int\n        y: float\n\n        @model_serializer(mode='plain')\n        def ser_model(self) -> Dict[str, Any]:\n            return {'x': self['x'] * 2, 'y': self['y'] * 3}\n\n    ta = TypeAdapter(Model)\n\n    assert ta.dump_json(Model({'x': 1, 'y': 2.5})) == b'{\"x\":2,\"y\":7.5}'\n\n\ndef test_type_adapter_dump_with_context():\n    class Model(TypedDict):\n        x: int\n        y: float\n\n        @model_serializer(mode='wrap')\n        def _serialize(self, handler, info: SerializationInfo):\n            data = handler(self)\n            if info.context and info.context.get('mode') == 'x-only':\n                data.pop('y')\n            return data\n\n    ta = TypeAdapter(Model)\n\n    assert ta.dump_json(Model({'x': 1, 'y': 2.5}), context={'mode': 'x-only'}) == b'{\"x\":1}'\n\n\n@pytest.mark.parametrize('as_annotation', [True, False])\n@pytest.mark.parametrize('mode', ['plain', 'wrap'])\ndef test_forward_ref_for_serializers(as_annotation, mode):\n    if mode == 'plain':\n\n        def ser_model_func(v) -> 'SomeOtherModel':  # noqa F821\n            return OtherModel(y=v + 1)\n\n        def ser_model_method(self, v) -> 'SomeOtherModel':  # noqa F821\n            return ser_model_func(v)\n\n        annotation = PlainSerializer(ser_model_func)\n    else:\n\n        def ser_model_func(v, handler) -> 'SomeOtherModel':  # noqa F821\n            return OtherModel(y=v + 1)\n\n        def ser_model_method(self, v, handler) -> 'SomeOtherModel':  # noqa F821\n            return ser_model_func(v, handler)\n\n        annotation = WrapSerializer(ser_model_func)\n\n    class Model(BaseModel):\n        if as_annotation:\n            x: Annotated[int, annotation]\n        else:\n            x: int\n            ser_model = field_serializer('x', mode=mode)(ser_model_method)\n\n    class OtherModel(BaseModel):\n        y: int\n\n    Model.model_rebuild(_types_namespace={'SomeOtherModel': OtherModel})\n\n    assert Model(x=1).model_dump() == {'x': {'y': 2}}\n    assert Model.model_json_schema(mode='serialization') == {\n        '$defs': {\n            'OtherModel': {\n                'properties': {'y': {'title': 'Y', 'type': 'integer'}},\n                'required': ['y'],\n                'title': 'OtherModel',\n                'type': 'object',\n            }\n        },\n        'properties': {'x': {'allOf': [{'$ref': '#/$defs/OtherModel'}], 'title': 'X'}},\n        'required': ['x'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_forward_ref_for_computed_fields():\n    class Model(BaseModel):\n        x: int\n\n        @computed_field\n        @property\n        def two_x(self) -> 'IntAlias':  # noqa F821\n            return self.x * 2\n\n    Model.model_rebuild(_types_namespace={'IntAlias': int})\n\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {\n            'two_x': {'readOnly': True, 'title': 'Two X', 'type': 'integer'},\n            'x': {'title': 'X', 'type': 'integer'},\n        },\n        'required': ['x', 'two_x'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n    assert Model(x=1).model_dump() == {'two_x': 2, 'x': 1}\n\n\ndef test_computed_field_custom_serializer():\n    class Model(BaseModel):\n        x: int\n\n        @computed_field\n        @property\n        def two_x(self) -> int:\n            return self.x * 2\n\n        @field_serializer('two_x', when_used='json')\n        def ser_two_x(self, v):\n            return f'The double of x is {v}'\n\n    m = Model(x=1)\n\n    assert m.model_dump() == {'two_x': 2, 'x': 1}\n    assert json.loads(m.model_dump_json()) == {'two_x': 'The double of x is 2', 'x': 1}\n\n\ndef test_annotated_computed_field_custom_serializer():\n    class Model(BaseModel):\n        x: int\n\n        @computed_field\n        @property\n        def two_x(self) -> Annotated[int, PlainSerializer(lambda v: f'The double of x is {v}', return_type=str)]:\n            return self.x * 2\n\n        @computed_field\n        @property\n        def triple_x(self) -> Annotated[int, PlainSerializer(lambda v: f'The triple of x is {v}', return_type=str)]:\n            return self.two_x * 3\n\n        @computed_field\n        @property\n        def quadruple_x_plus_one(self) -> Annotated[int, PlainSerializer(lambda v: v + 1, return_type=int)]:\n            return self.two_x * 2\n\n    m = Model(x=1)\n    assert m.x == 1\n    assert m.two_x == 2\n    assert m.triple_x == 6\n    assert m.quadruple_x_plus_one == 4\n\n    # insert_assert(m.model_dump())\n    assert m.model_dump() == {\n        'x': 1,\n        'two_x': 'The double of x is 2',\n        'triple_x': 'The triple of x is 6',\n        'quadruple_x_plus_one': 5,\n    }\n\n    # insert_assert(json.loads(m.model_dump_json()))\n    assert json.loads(m.model_dump_json()) == {\n        'x': 1,\n        'two_x': 'The double of x is 2',\n        'triple_x': 'The triple of x is 6',\n        'quadruple_x_plus_one': 5,\n    }\n\n    # insert_assert(Model.model_json_schema(mode='serialization'))\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {\n            'x': {'title': 'X', 'type': 'integer'},\n            'two_x': {'readOnly': True, 'title': 'Two X', 'type': 'string'},\n            'triple_x': {'readOnly': True, 'title': 'Triple X', 'type': 'string'},\n            'quadruple_x_plus_one': {'readOnly': True, 'title': 'Quadruple X Plus One', 'type': 'integer'},\n        },\n        'required': ['x', 'two_x', 'triple_x', 'quadruple_x_plus_one'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_computed_field_custom_serializer_bad_signature():\n    error_msg = 'field_serializer on computed_field does not use info signature'\n\n    with pytest.raises(PydanticUserError, match=error_msg):\n\n        class Model(BaseModel):\n            x: int\n\n            @computed_field\n            @property\n            def two_x(self) -> int:\n                return self.x * 2\n\n            @field_serializer('two_x')\n            def ser_two_x_bad_signature(self, v, _info):\n                return f'The double of x is {v}'\n\n\n@pytest.mark.skipif(sys.version_info < (3, 9), reason='@computed_field @classmethod @property only works in 3.9+')\ndef test_forward_ref_for_classmethod_computed_fields():\n    class Model(BaseModel):\n        y: ClassVar[int] = 4\n\n        @computed_field\n        @classmethod\n        @property\n        def two_y(cls) -> 'IntAlias':  # noqa F821\n            return cls.y * 2\n\n    Model.model_rebuild(_types_namespace={'IntAlias': int})\n\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {\n            'two_y': {'readOnly': True, 'title': 'Two Y', 'type': 'integer'},\n        },\n        'required': ['two_y'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n    assert Model().model_dump() == {'two_y': 8}\n\n\ndef test_enum_as_dict_key() -> None:\n    # See https://github.com/pydantic/pydantic/issues/7639\n    class MyEnum(Enum):\n        A = 'a'\n        B = 'b'\n\n    class MyModel(BaseModel):\n        foo: Dict[MyEnum, str]\n        bar: MyEnum\n\n    assert MyModel(foo={MyEnum.A: 'hello'}, bar=MyEnum.B).model_dump_json() == '{\"foo\":{\"a\":\"hello\"},\"bar\":\"b\"}'\n\n\ndef test_subclass_support_unions() -> None:\n    class Pet(BaseModel):\n        name: str\n\n    class Dog(Pet):\n        breed: str\n\n    class Kid(BaseModel):\n        age: str\n\n    class Home(BaseModel):\n        little_guys: Union[List[Pet], List[Kid]]\n\n    class Shelter(BaseModel):\n        pets: List[Pet]\n\n    h1 = Home(little_guys=[Pet(name='spot'), Pet(name='buddy')])\n    assert h1.model_dump() == {'little_guys': [{'name': 'spot'}, {'name': 'buddy'}]}\n\n    h2 = Home(little_guys=[Dog(name='fluffy', breed='lab'), Dog(name='patches', breed='boxer')])\n    assert h2.model_dump() == {'little_guys': [{'name': 'fluffy'}, {'name': 'patches'}]}\n\n    # confirming same serialization + validation behavior as for a single list (not a union)\n    s = Shelter(pets=[Dog(name='fluffy', breed='lab'), Dog(name='patches', breed='boxer')])\n    assert s.model_dump() == {'pets': [{'name': 'fluffy'}, {'name': 'patches'}]}\n\n\ndef test_subclass_support_unions_with_forward_ref() -> None:\n    class Bar(BaseModel):\n        bar_id: int\n\n    class Baz(Bar):\n        baz_id: int\n\n    class Foo(BaseModel):\n        items: Union[List['Foo'], List[Bar]]\n\n    foo = Foo(items=[Baz(bar_id=1, baz_id=2), Baz(bar_id=3, baz_id=4)])\n    assert foo.model_dump() == {'items': [{'bar_id': 1}, {'bar_id': 3}]}\n\n    foo_recursive = Foo(items=[Foo(items=[Baz(bar_id=42, baz_id=99)])])\n    assert foo_recursive.model_dump() == {'items': [{'items': [{'bar_id': 42}]}]}\n\n\ndef test_serialize_python_context() -> None:\n    contexts: List[Any] = [None, None, {'foo': 'bar'}]\n\n    class Model(BaseModel):\n        x: int\n\n        @field_serializer('x')\n        def serialize_x(self, v: int, info: SerializationInfo) -> int:\n            assert info.context == contexts.pop(0)\n            return v\n\n    m = Model.model_construct(**{'x': 1})\n    m.model_dump()\n    m.model_dump(context=None)\n    m.model_dump(context={'foo': 'bar'})\n    assert contexts == []\n\n\ndef test_serialize_json_context() -> None:\n    contexts: List[Any] = [None, None, {'foo': 'bar'}]\n\n    class Model(BaseModel):\n        x: int\n\n        @field_serializer('x')\n        def serialize_x(self, v: int, info: SerializationInfo) -> int:\n            assert info.context == contexts.pop(0)\n            return v\n\n    m = Model.model_construct(**{'x': 1})\n    m.model_dump_json()\n    m.model_dump_json(context=None)\n    m.model_dump_json(context={'foo': 'bar'})\n    assert contexts == []\n\n\ndef test_plain_serializer_with_std_type() -> None:\n    \"\"\"Ensure that a plain serializer can be used with a standard type constructor, rather than having to use lambda x: std_type(x).\"\"\"\n\n    class MyModel(BaseModel):\n        x: Annotated[int, PlainSerializer(float)]\n\n    m = MyModel(x=1)\n    assert m.model_dump() == {'x': 1.0}\n    assert m.model_dump_json() == '{\"x\":1.0}'\n\n    assert m.model_json_schema(mode='serialization') == {\n        'properties': {'x': {'title': 'X', 'type': 'number'}},\n        'required': ['x'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n", "tests/test_typing.py": "import sys\nimport typing\nfrom collections import namedtuple\nfrom typing import Callable, ClassVar, ForwardRef, NamedTuple\n\nimport pytest\nfrom typing_extensions import Literal, get_origin\n\nfrom pydantic import BaseModel, Field  # noqa: F401\nfrom pydantic._internal._typing_extra import (\n    NoneType,\n    eval_type_lenient,\n    get_function_type_hints,\n    is_classvar,\n    is_literal_type,\n    is_namedtuple,\n    is_none_type,\n    origin_is_union,\n    parent_frame_namespace,\n)\n\ntry:\n    from typing import TypedDict as typing_TypedDict\nexcept ImportError:\n    typing_TypedDict = None\n\ntry:\n    from typing_extensions import TypedDict as typing_extensions_TypedDict\nexcept ImportError:\n    typing_extensions_TypedDict = None\n\nALL_TYPEDDICT_KINDS = (typing_TypedDict, typing_extensions_TypedDict)\n\n\ndef test_is_namedtuple():\n    class Employee(NamedTuple):\n        name: str\n        id: int = 3\n\n    assert is_namedtuple(namedtuple('Point', 'x y')) is True\n    assert is_namedtuple(Employee) is True\n    assert is_namedtuple(NamedTuple('Employee', [('name', str), ('id', int)])) is True\n\n    class Other(tuple):\n        name: str\n        id: int\n\n    assert is_namedtuple(Other) is False\n\n\ndef test_is_none_type():\n    assert is_none_type(Literal[None]) is True\n    assert is_none_type(None) is True\n    assert is_none_type(type(None)) is True\n    assert is_none_type(6) is False\n    assert is_none_type({}) is False\n    # WARNING: It's important to test `typing.Callable` not\n    # `collections.abc.Callable` (even with python >= 3.9) as they behave\n    # differently\n    assert is_none_type(Callable) is False\n\n\n@pytest.mark.parametrize(\n    'union',\n    [\n        typing.Union[int, str],\n        eval_type_lenient('int | str'),\n        *([int | str] if sys.version_info >= (3, 10) else []),\n    ],\n)\ndef test_is_union(union):\n    origin = get_origin(union)\n    assert origin_is_union(origin)\n\n\ndef test_is_literal_with_typing_extension_literal():\n    from typing_extensions import Literal\n\n    assert is_literal_type(Literal) is False\n    assert is_literal_type(Literal['foo']) is True\n\n\ndef test_is_literal_with_typing_literal():\n    from typing import Literal\n\n    assert is_literal_type(Literal) is False\n    assert is_literal_type(Literal['foo']) is True\n\n\n@pytest.mark.parametrize(\n    'ann_type,extepcted',\n    (\n        (None, False),\n        (ForwardRef('Other[int]'), False),\n        (ForwardRef('Other[ClassVar[int]]'), False),\n        (ForwardRef('ClassVar[int]'), True),\n        (ForwardRef('t.ClassVar[int]'), True),\n        (ForwardRef('typing.ClassVar[int]'), True),\n        (ClassVar[int], True),\n    ),\n)\ndef test_is_classvar(ann_type, extepcted):\n    assert is_classvar(ann_type) is extepcted\n\n\ndef test_parent_frame_namespace(mocker):\n    assert parent_frame_namespace() is not None\n\n    from dataclasses import dataclass\n\n    @dataclass\n    class MockedFrame:\n        f_back = None\n\n    mocker.patch('sys._getframe', return_value=MockedFrame())\n    assert parent_frame_namespace() is None\n\n\ndef test_get_function_type_hints_none_type():\n    def f(x: int, y: None) -> int:\n        return x\n\n    assert get_function_type_hints(f) == {'return': int, 'x': int, 'y': NoneType}\n\n\n@pytest.mark.skipif(sys.version_info[:2] > (3, 9), reason='testing using a feature not supported by older Python')\ndef test_eval_type_backport_not_installed():\n    sys.modules['eval_type_backport'] = None\n    try:\n        with pytest.raises(TypeError) as exc_info:\n\n            class _Model(BaseModel):\n                foo: 'int | str'\n\n        assert str(exc_info.value) == (\n            \"You have a type annotation 'int | str' which makes use of newer typing \"\n            'features than are supported in your version of Python. To handle this error, '\n            'you should either remove the use of new syntax or install the '\n            '`eval_type_backport` package.'\n        )\n\n    finally:\n        del sys.modules['eval_type_backport']\n", "tests/test_generics.py": "import gc\nimport itertools\nimport json\nimport platform\nimport re\nimport sys\nfrom collections import deque\nfrom enum import Enum, IntEnum\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Counter,\n    DefaultDict,\n    Deque,\n    Dict,\n    FrozenSet,\n    Generic,\n    Iterable,\n    List,\n    Mapping,\n    NamedTuple,\n    Optional,\n    OrderedDict,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nimport pytest\nfrom dirty_equals import HasRepr, IsStr\nfrom pydantic_core import CoreSchema, core_schema\nfrom typing_extensions import (\n    Annotated,\n    Literal,\n    NotRequired,\n    ParamSpec,\n    TypedDict,\n    TypeVarTuple,\n    Unpack,\n    get_args,\n)\nfrom typing_extensions import (\n    TypeVar as TypingExtensionsTypeVar,\n)\n\nfrom pydantic import (\n    BaseModel,\n    Field,\n    GetCoreSchemaHandler,\n    Json,\n    PositiveInt,\n    PydanticSchemaGenerationError,\n    PydanticUserError,\n    TypeAdapter,\n    ValidationError,\n    ValidationInfo,\n    computed_field,\n    field_validator,\n    model_validator,\n)\nfrom pydantic._internal._core_utils import collect_invalid_schemas\nfrom pydantic._internal._generics import (\n    _GENERIC_TYPES_CACHE,\n    _LIMITED_DICT_SIZE,\n    LimitedDict,\n    generic_recursion_self_type,\n    iter_contained_typevars,\n    recursively_defined_type_refs,\n    replace_types,\n)\nfrom pydantic.warnings import GenericBeforeBaseModelWarning\n\n\n@pytest.fixture()\ndef clean_cache():\n    # cleans up _GENERIC_TYPES_CACHE for checking item counts in the cache\n    _GENERIC_TYPES_CACHE.clear()\n    gc.collect(0)\n    gc.collect(1)\n    gc.collect(2)\n\n\ndef test_generic_name():\n    data_type = TypeVar('data_type')\n\n    class Result(BaseModel, Generic[data_type]):\n        data: data_type\n\n    if sys.version_info >= (3, 9):\n        assert Result[list[int]].__name__ == 'Result[list[int]]'\n    assert Result[List[int]].__name__ == 'Result[List[int]]'\n    assert Result[int].__name__ == 'Result[int]'\n\n\ndef test_double_parameterize_error():\n    data_type = TypeVar('data_type')\n\n    class Result(BaseModel, Generic[data_type]):\n        data: data_type\n\n    with pytest.raises(TypeError) as exc_info:\n        Result[int][int]\n\n    assert str(exc_info.value) == \"<class 'tests.test_generics.Result[int]'> is not a generic class\"\n\n\ndef test_value_validation():\n    T = TypeVar('T', bound=Dict[Any, Any])\n\n    class Response(BaseModel, Generic[T]):\n        data: T\n\n        @field_validator('data')\n        @classmethod\n        def validate_value_nonzero(cls, v: Any):\n            if any(x == 0 for x in v.values()):\n                raise ValueError('some value is zero')\n            return v\n\n        @model_validator(mode='after')\n        def validate_sum(self) -> 'Response[T]':\n            data = self.data\n            if sum(data.values()) > 5:\n                raise ValueError('sum too large')\n            return self\n\n    assert Response[Dict[int, int]](data={1: '4'}).model_dump() == {'data': {1: 4}}\n    with pytest.raises(ValidationError) as exc_info:\n        Response[Dict[int, int]](data={1: 'a'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('data', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Response[Dict[int, int]](data={1: 0})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('some value is zero')))},\n            'input': {1: 0},\n            'loc': ('data',),\n            'msg': 'Value error, some value is zero',\n            'type': 'value_error',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Response[Dict[int, int]](data={1: 3, 2: 6})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('sum too large')))},\n            'input': {'data': {1: 3, 2: 6}},\n            'loc': (),\n            'msg': 'Value error, sum too large',\n            'type': 'value_error',\n        }\n    ]\n\n\ndef test_methods_are_inherited():\n    class CustomModel(BaseModel):\n        def method(self):\n            return self.data\n\n    T = TypeVar('T')\n\n    class Model(CustomModel, Generic[T]):\n        data: T\n\n    instance = Model[int](data=1)\n\n    assert instance.method() == 1\n\n\ndef test_config_is_inherited():\n    class CustomGenericModel(BaseModel, frozen=True): ...\n\n    T = TypeVar('T')\n\n    class Model(CustomGenericModel, Generic[T]):\n        data: T\n\n    instance = Model[int](data=1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        instance.data = 2\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': ('data',), 'msg': 'Instance is frozen', 'input': 2}\n    ]\n\n\ndef test_default_argument():\n    T = TypeVar('T')\n\n    class Result(BaseModel, Generic[T]):\n        data: T\n        other: bool = True\n\n    result = Result[int](data=1)\n    assert result.other is True\n\n\ndef test_default_argument_for_typevar():\n    T = TypeVar('T')\n\n    class Result(BaseModel, Generic[T]):\n        data: T = 4\n\n    result = Result[int]()\n    assert result.data == 4\n\n    result = Result[float]()\n    assert result.data == 4\n\n    result = Result[int](data=1)\n    assert result.data == 1\n\n\ndef test_classvar():\n    T = TypeVar('T')\n\n    class Result(BaseModel, Generic[T]):\n        data: T\n        other: ClassVar[int] = 1\n\n    assert Result.other == 1\n    assert Result[int].other == 1\n    assert Result[int](data=1).other == 1\n    assert 'other' not in Result.model_fields\n\n\ndef test_non_annotated_field():\n    T = TypeVar('T')\n\n    with pytest.raises(PydanticUserError, match='A non-annotated attribute was detected: `other = True`'):\n\n        class Result(BaseModel, Generic[T]):\n            data: T\n            other = True\n\n\ndef test_non_generic_field():\n    T = TypeVar('T')\n\n    class Result(BaseModel, Generic[T]):\n        data: T\n        other: bool = True\n\n    assert 'other' in Result.model_fields\n    assert 'other' in Result[int].model_fields\n\n    result = Result[int](data=1)\n    assert result.other is True\n\n\ndef test_must_inherit_from_generic():\n    with pytest.raises(TypeError) as exc_info:\n\n        class Result(BaseModel):\n            pass\n\n        Result[int]\n\n    assert str(exc_info.value) == (\n        \"<class 'tests.test_generics.test_must_inherit_from_generic.<locals>.Result'> cannot be \"\n        'parametrized because it does not inherit from typing.Generic'\n    )\n\n\ndef test_parameters_placed_on_generic():\n    T = TypeVar('T')\n    with pytest.raises(TypeError, match='Type parameters should be placed on typing.Generic, not BaseModel'):\n\n        class Result(BaseModel[T]):\n            pass\n\n\ndef test_parameters_must_be_typevar():\n    with pytest.raises(TypeError, match='Type parameters should be placed on typing.Generic, not BaseModel'):\n\n        class Result(BaseModel[int]):\n            pass\n\n\ndef test_subclass_can_be_genericized():\n    T = TypeVar('T')\n\n    class Result(BaseModel, Generic[T]):\n        pass\n\n    Result[T]\n\n\ndef test_parameter_count():\n    T = TypeVar('T')\n    S = TypeVar('S')\n\n    class Model(BaseModel, Generic[T, S]):\n        x: T\n        y: S\n\n    with pytest.raises(TypeError) as exc_info:\n        Model[int, int, int]\n\n    # This error message, which comes from `typing`, changed 'parameters' to 'arguments' in 3.11\n    error_message = str(exc_info.value)\n    assert error_message.startswith('Too many parameters') or error_message.startswith('Too many arguments')\n    assert error_message.endswith(\n        \" for <class 'tests.test_generics.test_parameter_count.<locals>.Model'>; actual 3, expected 2\"\n    )\n\n\ndef test_cover_cache(clean_cache):\n    cache_size = len(_GENERIC_TYPES_CACHE)\n    T = TypeVar('T')\n\n    class Model(BaseModel, Generic[T]):\n        x: T\n\n    models = []  # keep references to models to get cache size\n\n    models.append(Model[int])  # adds both with-tuple and without-tuple version to cache\n    assert len(_GENERIC_TYPES_CACHE) == cache_size + 3\n    models.append(Model[int])  # uses the cache\n    assert len(_GENERIC_TYPES_CACHE) == cache_size + 3\n    del models\n\n\ndef test_cache_keys_are_hashable(clean_cache):\n    cache_size = len(_GENERIC_TYPES_CACHE)\n    T = TypeVar('T')\n    C = Callable[[str, Dict[str, Any]], Iterable[str]]\n\n    class MyGenericModel(BaseModel, Generic[T]):\n        t: T\n\n    # Callable's first params get converted to a list, which is not hashable.\n    # Make sure we can handle that special case\n    Simple = MyGenericModel[Callable[[int], str]]\n    models = []  # keep references to models to get cache size\n    models.append(Simple)\n\n    assert len(_GENERIC_TYPES_CACHE) == cache_size + 3\n    # Nested Callables\n    models.append(MyGenericModel[Callable[[C], Iterable[str]]])\n    assert len(_GENERIC_TYPES_CACHE) == cache_size + 6\n    models.append(MyGenericModel[Callable[[Simple], Iterable[int]]])\n    assert len(_GENERIC_TYPES_CACHE) == cache_size + 9\n    models.append(MyGenericModel[Callable[[MyGenericModel[C]], Iterable[int]]])\n    assert len(_GENERIC_TYPES_CACHE) == cache_size + 15\n\n    class Model(BaseModel):\n        x: MyGenericModel[Callable[[C], Iterable[str]]] = Field(...)\n\n    models.append(Model)\n    assert len(_GENERIC_TYPES_CACHE) == cache_size + 15\n    del models\n\n\n@pytest.mark.skipif(platform.python_implementation() == 'PyPy', reason='PyPy does not play nice with PyO3 gc')\ndef test_caches_get_cleaned_up(clean_cache):\n    initial_types_cache_size = len(_GENERIC_TYPES_CACHE)\n    T = TypeVar('T')\n\n    class MyGenericModel(BaseModel, Generic[T]):\n        x: T\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    n_types = 200\n    types = []\n    for i in range(n_types):\n\n        class MyType(int):\n            pass\n\n        types.append(MyGenericModel[MyType])  # retain a reference\n\n    assert len(_GENERIC_TYPES_CACHE) == initial_types_cache_size + 3 * n_types\n    types.clear()\n    gc.collect(0)\n    gc.collect(1)\n    gc.collect(2)\n    assert len(_GENERIC_TYPES_CACHE) < initial_types_cache_size + _LIMITED_DICT_SIZE\n\n\n@pytest.mark.skipif(platform.python_implementation() == 'PyPy', reason='PyPy does not play nice with PyO3 gc')\ndef test_caches_get_cleaned_up_with_aliased_parametrized_bases(clean_cache):\n    types_cache_size = len(_GENERIC_TYPES_CACHE)\n\n    def run() -> None:  # Run inside nested function to get classes in local vars cleaned also\n        T1 = TypeVar('T1')\n        T2 = TypeVar('T2')\n\n        class A(BaseModel, Generic[T1, T2]):\n            x: T1\n            y: T2\n\n        B = A[int, T2]\n        C = B[str]\n        assert len(_GENERIC_TYPES_CACHE) == types_cache_size + 5\n        del C\n        del B\n        gc.collect()\n\n    run()\n\n    gc.collect(0)\n    gc.collect(1)\n    gc.collect(2)\n    assert len(_GENERIC_TYPES_CACHE) < types_cache_size + _LIMITED_DICT_SIZE\n\n\n@pytest.mark.skipif(platform.python_implementation() == 'PyPy', reason='PyPy does not play nice with PyO3 gc')\n@pytest.mark.skipif(sys.version_info[:2] == (3, 9), reason='The test randomly fails on Python 3.9')\ndef test_circular_generic_refs_get_cleaned_up():\n    initial_cache_size = len(_GENERIC_TYPES_CACHE)\n\n    def fn():\n        T = TypeVar('T')\n        C = TypeVar('C')\n\n        class Inner(BaseModel, Generic[T, C]):\n            a: T\n            b: C\n\n        class Outer(BaseModel, Generic[C]):\n            c: Inner[int, C]\n\n        klass = Outer[str]\n        assert len(_GENERIC_TYPES_CACHE) > initial_cache_size\n        assert klass in _GENERIC_TYPES_CACHE.values()\n\n    fn()\n\n    gc.collect(0)\n    gc.collect(1)\n    gc.collect(2)\n\n    assert len(_GENERIC_TYPES_CACHE) == initial_cache_size\n\n\ndef test_generics_work_with_many_parametrized_base_models(clean_cache):\n    cache_size = len(_GENERIC_TYPES_CACHE)\n    count_create_models = 1000\n    T = TypeVar('T')\n    C = TypeVar('C')\n\n    class A(BaseModel, Generic[T, C]):\n        x: T\n        y: C\n\n    class B(A[int, C], BaseModel, Generic[C]):\n        pass\n\n    models = []\n    for i in range(count_create_models):\n\n        class M(BaseModel):\n            pass\n\n        M.__name__ = f'M{i}'\n        models.append(M)\n\n    generics = []\n    for m in models:\n        Working = B[m]\n        generics.append(Working)\n\n    target_size = cache_size + count_create_models * 3 + 2\n    assert len(_GENERIC_TYPES_CACHE) < target_size + _LIMITED_DICT_SIZE\n    del models\n    del generics\n\n\ndef test_generic_config():\n    data_type = TypeVar('data_type')\n\n    class Result(BaseModel, Generic[data_type], frozen=True):\n        data: data_type\n\n    result = Result[int](data=1)\n    assert result.data == 1\n    with pytest.raises(ValidationError):\n        result.data = 2\n\n\ndef test_enum_generic():\n    T = TypeVar('T')\n\n    class MyEnum(IntEnum):\n        x = 1\n        y = 2\n\n    class Model(BaseModel, Generic[T]):\n        enum: T\n\n    Model[MyEnum](enum=MyEnum.x)\n    Model[MyEnum](enum=2)\n\n\ndef test_generic():\n    data_type = TypeVar('data_type')\n    error_type = TypeVar('error_type')\n\n    class Result(BaseModel, Generic[data_type, error_type]):\n        data: Optional[List[data_type]] = None\n        error: Optional[error_type] = None\n        positive_number: int\n\n        @field_validator('error')\n        @classmethod\n        def validate_error(cls, v: Optional[error_type], info: ValidationInfo) -> Optional[error_type]:\n            values = info.data\n            if values.get('data', None) is None and v is None:\n                raise ValueError('Must provide data or error')\n            if values.get('data', None) is not None and v is not None:\n                raise ValueError('Must not provide both data and error')\n            return v\n\n        @field_validator('positive_number')\n        @classmethod\n        def validate_positive_number(cls, v: int) -> int:\n            if v < 0:\n                raise ValueError\n            return v\n\n    class Error(BaseModel):\n        message: str\n\n    class Data(BaseModel):\n        number: int\n        text: str\n\n    success1 = Result[Data, Error](data=[Data(number=1, text='a')], positive_number=1)\n    assert success1.model_dump() == {'data': [{'number': 1, 'text': 'a'}], 'error': None, 'positive_number': 1}\n    assert repr(success1) == (\n        'Result[test_generic.<locals>.Data,'\n        \" test_generic.<locals>.Error](data=[Data(number=1, text='a')], error=None, positive_number=1)\"\n    )\n\n    success2 = Result[Data, Error](error=Error(message='error'), positive_number=1)\n    assert success2.model_dump() == {'data': None, 'error': {'message': 'error'}, 'positive_number': 1}\n    assert repr(success2) == (\n        'Result[test_generic.<locals>.Data, test_generic.<locals>.Error]'\n        \"(data=None, error=Error(message='error'), positive_number=1)\"\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        Result[Data, Error](error=Error(message='error'), positive_number=-1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError()))},\n            'input': -1,\n            'loc': ('positive_number',),\n            'msg': 'Value error, ',\n            'type': 'value_error',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Result[Data, Error](data=[Data(number=1, text='a')], error=Error(message='error'), positive_number=1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('Must not provide both data and error')))},\n            'input': Error(message='error'),\n            'loc': ('error',),\n            'msg': 'Value error, Must not provide both data and error',\n            'type': 'value_error',\n        }\n    ]\n\n\ndef test_alongside_concrete_generics():\n    T = TypeVar('T')\n\n    class MyModel(BaseModel, Generic[T]):\n        item: T\n        metadata: Dict[str, Any]\n\n    model = MyModel[int](item=1, metadata={})\n    assert model.item == 1\n    assert model.metadata == {}\n\n\ndef test_complex_nesting():\n    T = TypeVar('T')\n\n    class MyModel(BaseModel, Generic[T]):\n        item: List[Dict[Union[int, T], str]]\n\n    item = [{1: 'a', 'a': 'a'}]\n    model = MyModel[str](item=item)\n    assert model.item == item\n\n\ndef test_required_value():\n    T = TypeVar('T')\n\n    class MyModel(BaseModel, Generic[T]):\n        a: int\n\n    with pytest.raises(ValidationError) as exc_info:\n        MyModel[int]()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('a',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\ndef test_optional_value():\n    T = TypeVar('T')\n\n    class MyModel(BaseModel, Generic[T]):\n        a: Optional[int] = 1\n\n    model = MyModel[int]()\n    assert model.model_dump() == {'a': 1}\n\n\ndef test_custom_schema():\n    T = TypeVar('T')\n\n    class MyModel(BaseModel, Generic[T]):\n        a: int = Field(1, description='Custom')\n\n    schema = MyModel[int].model_json_schema()\n    assert schema['properties']['a'].get('description') == 'Custom'\n\n\ndef test_child_schema():\n    T = TypeVar('T')\n\n    class Model(BaseModel, Generic[T]):\n        a: T\n\n    class Child(Model[T], Generic[T]):\n        pass\n\n    schema = Child[int].model_json_schema()\n    assert schema == {\n        'title': 'Child[int]',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'integer'}},\n        'required': ['a'],\n    }\n\n\ndef test_custom_generic_naming():\n    T = TypeVar('T')\n\n    class MyModel(BaseModel, Generic[T]):\n        value: Optional[T]\n\n        @classmethod\n        def model_parametrized_name(cls, params: Tuple[Type[Any], ...]) -> str:\n            param_names = [param.__name__ if hasattr(param, '__name__') else str(param) for param in params]\n            title = param_names[0].title()\n            return f'Optional{title}Wrapper'\n\n    assert repr(MyModel[int](value=1)) == 'OptionalIntWrapper(value=1)'\n    assert repr(MyModel[str](value=None)) == 'OptionalStrWrapper(value=None)'\n\n\ndef test_nested():\n    AT = TypeVar('AT')\n\n    class InnerT(BaseModel, Generic[AT]):\n        a: AT\n\n    inner_int = InnerT[int](a=8)\n    inner_str = InnerT[str](a='ate')\n    inner_dict_any = InnerT[Any](a={})\n    inner_int_any = InnerT[Any](a=7)\n\n    class OuterT_SameType(BaseModel, Generic[AT]):\n        i: InnerT[AT]\n\n    OuterT_SameType[int](i={'a': 8})\n    OuterT_SameType[int](i=inner_int)\n    OuterT_SameType[str](i=inner_str)\n    # TODO: The next line is failing, but passes in v1.\n    #   Should re-parse-from-dict if the pydantic_generic_origin is the same\n    # OuterT_SameType[str](i=inner_int_any)\n    OuterT_SameType[int](i=inner_int_any.model_dump())\n\n    with pytest.raises(ValidationError) as exc_info:\n        OuterT_SameType[int](i=inner_str.model_dump())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('i', 'a'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'ate',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        OuterT_SameType[int](i=inner_str)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': ('i',),\n            'msg': 'Input should be a valid dictionary or instance of InnerT[int]',\n            'input': InnerT[str](a='ate'),\n            'ctx': {'class_name': 'InnerT[int]'},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        OuterT_SameType[int](i=inner_dict_any.model_dump())\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('i', 'a'), 'msg': 'Input should be a valid integer', 'input': {}}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        OuterT_SameType[int](i=inner_dict_any)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': ('i',),\n            'msg': 'Input should be a valid dictionary or instance of InnerT[int]',\n            'input': InnerT[Any](a={}),\n            'ctx': {'class_name': 'InnerT[int]'},\n        }\n    ]\n\n\ndef test_partial_specification():\n    AT = TypeVar('AT')\n    BT = TypeVar('BT')\n\n    class Model(BaseModel, Generic[AT, BT]):\n        a: AT\n        b: BT\n\n    partial_model = Model[int, BT]\n    concrete_model = partial_model[str]\n    concrete_model(a=1, b='abc')\n    with pytest.raises(ValidationError) as exc_info:\n        concrete_model(a='abc', b=None)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'abc',\n        },\n        {'type': 'string_type', 'loc': ('b',), 'msg': 'Input should be a valid string', 'input': None},\n    ]\n\n\ndef test_partial_specification_with_inner_typevar():\n    AT = TypeVar('AT')\n    BT = TypeVar('BT')\n\n    class Model(BaseModel, Generic[AT, BT]):\n        a: List[AT]\n        b: List[BT]\n\n    partial_model = Model[int, BT]\n    assert partial_model.__pydantic_generic_metadata__['parameters']\n    concrete_model = partial_model[int]\n\n    assert not concrete_model.__pydantic_generic_metadata__['parameters']\n\n    # nested resolution of partial models should work as expected\n    nested_resolved = concrete_model(a=['123'], b=['456'])\n    assert nested_resolved.a == [123]\n    assert nested_resolved.b == [456]\n\n\n@pytest.mark.skipif(sys.version_info < (3, 12), reason='repr different on older versions')\ndef test_partial_specification_name():\n    AT = TypeVar('AT')\n    BT = TypeVar('BT')\n\n    class Model(BaseModel, Generic[AT, BT]):\n        a: AT\n        b: BT\n\n    partial_model = Model[int, BT]\n    assert partial_model.__name__ == 'Model[int, TypeVar]'\n    concrete_model = partial_model[str]\n    assert concrete_model.__name__ == 'Model[int, str]'\n\n\ndef test_partial_specification_instantiation():\n    AT = TypeVar('AT')\n    BT = TypeVar('BT')\n\n    class Model(BaseModel, Generic[AT, BT]):\n        a: AT\n        b: BT\n\n    partial_model = Model[int, BT]\n    partial_model(a=1, b=2)\n\n    partial_model(a=1, b='a')\n\n    with pytest.raises(ValidationError) as exc_info:\n        partial_model(a='a', b=2)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        }\n    ]\n\n\ndef test_partial_specification_instantiation_bounded():\n    AT = TypeVar('AT')\n    BT = TypeVar('BT', bound=int)\n\n    class Model(BaseModel, Generic[AT, BT]):\n        a: AT\n        b: BT\n\n    Model(a=1, b=1)\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=1, b='a')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        }\n    ]\n\n    partial_model = Model[int, BT]\n    partial_model(a=1, b=1)\n    with pytest.raises(ValidationError) as exc_info:\n        partial_model(a=1, b='a')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        }\n    ]\n\n\ndef test_typevar_parametrization():\n    AT = TypeVar('AT')\n    BT = TypeVar('BT')\n\n    class Model(BaseModel, Generic[AT, BT]):\n        a: AT\n        b: BT\n\n    CT = TypeVar('CT', bound=int)\n    DT = TypeVar('DT', bound=int)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model[CT, DT](a='a', b='b')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'b',\n        },\n    ]\n\n\ndef test_multiple_specification():\n    AT = TypeVar('AT')\n    BT = TypeVar('BT')\n\n    class Model(BaseModel, Generic[AT, BT]):\n        a: AT\n        b: BT\n\n    CT = TypeVar('CT')\n    partial_model = Model[CT, CT]\n    concrete_model = partial_model[str]\n\n    with pytest.raises(ValidationError) as exc_info:\n        concrete_model(a=None, b=None)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('a',), 'msg': 'Input should be a valid string', 'input': None},\n        {'type': 'string_type', 'loc': ('b',), 'msg': 'Input should be a valid string', 'input': None},\n    ]\n\n\ndef test_generic_subclass_of_concrete_generic():\n    T = TypeVar('T')\n    U = TypeVar('U')\n\n    class GenericBaseModel(BaseModel, Generic[T]):\n        data: T\n\n    class GenericSub(GenericBaseModel[int], Generic[U]):\n        extra: U\n\n    ConcreteSub = GenericSub[int]\n\n    with pytest.raises(ValidationError):\n        ConcreteSub(data=2, extra='wrong')\n\n    with pytest.raises(ValidationError):\n        ConcreteSub(data='wrong', extra=2)\n\n    ConcreteSub(data=2, extra=3)\n\n\ndef test_generic_model_pickle(create_module):\n    # Using create_module because pickle doesn't support\n    # objects with <locals> in their __qualname__  (e.g. defined in function)\n    @create_module\n    def module():\n        import pickle\n        from typing import Generic, TypeVar\n\n        from pydantic import BaseModel\n\n        t = TypeVar('t')\n\n        class Model(BaseModel):\n            a: float\n            b: int = 10\n\n        class MyGeneric(BaseModel, Generic[t]):\n            value: t\n\n        original = MyGeneric[Model](value=Model(a='24'))\n        dumped = pickle.dumps(original)\n        loaded = pickle.loads(dumped)\n        assert loaded.value.a == original.value.a == 24\n        assert loaded.value.b == original.value.b == 10\n        assert loaded == original\n\n\ndef test_generic_model_from_function_pickle_fail(create_module):\n    @create_module\n    def module():\n        import pickle\n        from typing import Generic, TypeVar\n\n        import pytest\n\n        from pydantic import BaseModel\n\n        t = TypeVar('t')\n\n        class Model(BaseModel):\n            a: float\n            b: int = 10\n\n        class MyGeneric(BaseModel, Generic[t]):\n            value: t\n\n        def get_generic(t):\n            return MyGeneric[t]\n\n        original = get_generic(Model)(value=Model(a='24'))\n        with pytest.raises(pickle.PicklingError):\n            pickle.dumps(original)\n\n\ndef test_generic_model_redefined_without_cache_fail(create_module, monkeypatch):\n    # match identity checker otherwise we never get to the redefinition check\n    monkeypatch.setattr('pydantic._internal._utils.all_identical', lambda left, right: False)\n\n    @create_module\n    def module():\n        from typing import Generic, TypeVar\n\n        from pydantic import BaseModel\n        from pydantic._internal._generics import _GENERIC_TYPES_CACHE\n\n        t = TypeVar('t')\n\n        class MyGeneric(BaseModel, Generic[t]):\n            value: t\n\n        class Model(BaseModel): ...\n\n        concrete = MyGeneric[Model]\n        _GENERIC_TYPES_CACHE.clear()\n        second_concrete = MyGeneric[Model]\n\n        class Model(BaseModel):  # same name, but type different, so it's not in cache\n            ...\n\n        third_concrete = MyGeneric[Model]\n        assert concrete is not second_concrete\n        assert concrete is not third_concrete\n        assert second_concrete is not third_concrete\n        assert globals()['MyGeneric[Model]'] is concrete\n        assert globals()['MyGeneric[Model]_'] is second_concrete\n        assert globals()['MyGeneric[Model]__'] is third_concrete\n\n\ndef test_generic_model_caching_detect_order_of_union_args_basic(create_module):\n    # Basic variant of https://github.com/pydantic/pydantic/issues/4474\n    @create_module\n    def module():\n        from typing import Generic, TypeVar, Union\n\n        from pydantic import BaseModel\n\n        t = TypeVar('t')\n\n        class Model(BaseModel, Generic[t]):\n            data: t\n\n        int_or_float_model = Model[Union[int, float]]\n        float_or_int_model = Model[Union[float, int]]\n\n        assert type(int_or_float_model(data='1').data) is int\n        assert type(float_or_int_model(data='1').data) is float\n\n\n@pytest.mark.skip(\n    reason=\"\"\"\nDepends on similar issue in CPython itself: https://github.com/python/cpython/issues/86483\nDocumented and skipped for possible fix later.\n\"\"\"\n)\ndef test_generic_model_caching_detect_order_of_union_args_nested(create_module):\n    # Nested variant of https://github.com/pydantic/pydantic/issues/4474\n    @create_module\n    def module():\n        from typing import Generic, List, TypeVar, Union\n\n        from pydantic import BaseModel\n\n        t = TypeVar('t')\n\n        class Model(BaseModel, Generic[t]):\n            data: t\n\n        int_or_float_model = Model[List[Union[int, float]]]\n        float_or_int_model = Model[List[Union[float, int]]]\n\n        assert type(int_or_float_model(data=['1']).data[0]) is int\n        assert type(float_or_int_model(data=['1']).data[0]) is float\n\n\ndef test_get_caller_frame_info(create_module):\n    @create_module\n    def module():\n        from pydantic._internal._generics import _get_caller_frame_info\n\n        def function():\n            assert _get_caller_frame_info() == (__name__, True)\n\n            another_function()\n\n        def another_function():\n            assert _get_caller_frame_info() == (__name__, False)\n            third_function()\n\n        def third_function():\n            assert _get_caller_frame_info() == (__name__, False)\n\n        function()\n\n\ndef test_get_caller_frame_info_called_from_module(create_module):\n    @create_module\n    def module():\n        from unittest.mock import patch\n\n        import pytest\n\n        from pydantic._internal._generics import _get_caller_frame_info\n\n        with pytest.raises(RuntimeError, match='This function must be used inside another function'):\n            with patch('sys._getframe', side_effect=ValueError('getframe_exc')):\n                _get_caller_frame_info()\n\n\ndef test_get_caller_frame_info_when_sys_getframe_undefined():\n    from pydantic._internal._generics import _get_caller_frame_info\n\n    getframe = sys._getframe\n    del sys._getframe\n    try:\n        assert _get_caller_frame_info() == (None, False)\n    finally:  # just to make sure we always setting original attribute back\n        sys._getframe = getframe\n\n\ndef test_iter_contained_typevars():\n    T = TypeVar('T')\n    T2 = TypeVar('T2')\n\n    class Model(BaseModel, Generic[T]):\n        a: T\n\n    assert list(iter_contained_typevars(Model[T])) == [T]\n    assert list(iter_contained_typevars(Optional[List[Union[str, Model[T]]]])) == [T]\n    assert list(iter_contained_typevars(Optional[List[Union[str, Model[int]]]])) == []\n    assert list(iter_contained_typevars(Optional[List[Union[str, Model[T], Callable[[T2, T], str]]]])) == [T, T2, T]\n\n\ndef test_nested_identity_parameterization():\n    T = TypeVar('T')\n    T2 = TypeVar('T2')\n\n    class Model(BaseModel, Generic[T]):\n        a: T\n\n    assert Model[T][T][T] is Model\n    assert Model[T] is Model\n    assert Model[T2] is not Model\n\n\ndef test_replace_types():\n    T = TypeVar('T')\n\n    class Model(BaseModel, Generic[T]):\n        a: T\n\n    assert replace_types(T, {T: int}) is int\n    assert replace_types(List[Union[str, list, T]], {T: int}) == List[Union[str, list, int]]\n    assert replace_types(Callable, {T: int}) == Callable\n    assert replace_types(Callable[[int, str, T], T], {T: int}) == Callable[[int, str, int], int]\n    assert replace_types(T, {}) is T\n    assert replace_types(Model[List[T]], {T: int}) == Model[List[int]]\n    assert replace_types(Model[List[T]], {T: int}) == Model[List[T]][int]\n    assert (\n        replace_types(Model[List[T]], {T: int}).model_fields['a'].annotation\n        == Model[List[T]][int].model_fields['a'].annotation\n    )\n    assert replace_types(T, {}) is T\n    assert replace_types(Type[T], {T: int}) == Type[int]\n    assert replace_types(Model[T], {T: T}) == Model[T]\n    assert replace_types(Json[T], {T: int}) == Json[int]\n\n    if sys.version_info >= (3, 9):\n        # Check generic aliases (subscripted builtin types) to make sure they\n        # resolve correctly (don't get translated to typing versions for\n        # example)\n        assert replace_types(list[Union[str, list, T]], {T: int}) == list[Union[str, list, int]]\n\n    if sys.version_info >= (3, 10):\n        # Check that types.UnionType gets handled properly\n        assert replace_types(str | list[T] | float, {T: int}) == str | list[int] | float\n\n\ndef test_replace_types_with_user_defined_generic_type_field():  # noqa: C901\n    \"\"\"Test that using user defined generic types as generic model fields are handled correctly.\"\"\"\n    T = TypeVar('T')\n    KT = TypeVar('KT')\n    VT = TypeVar('VT')\n\n    class CustomCounter(Counter[T]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(cls, handler(Counter[get_args(source_type)[0]]))\n\n    class CustomDefaultDict(DefaultDict[KT, VT]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            keys_type, values_type = get_args(source_type)\n            return core_schema.no_info_after_validator_function(\n                lambda x: cls(x.default_factory, x), handler(DefaultDict[keys_type, values_type])\n            )\n\n    class CustomDeque(Deque[T]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(cls, handler(Deque[get_args(source_type)[0]]))\n\n    class CustomDict(Dict[KT, VT]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            keys_type, values_type = get_args(source_type)\n            return core_schema.no_info_after_validator_function(cls, handler(Dict[keys_type, values_type]))\n\n    class CustomFrozenset(FrozenSet[T]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(cls, handler(FrozenSet[get_args(source_type)[0]]))\n\n    class CustomIterable(Iterable[T]):\n        def __init__(self, iterable):\n            self.iterable = iterable\n\n        def __iter__(self):\n            return self\n\n        def __next__(self):\n            return next(self.iterable)\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(cls, handler(Iterable[get_args(source_type)[0]]))\n\n    class CustomList(List[T]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(cls, handler(List[get_args(source_type)[0]]))\n\n    class CustomMapping(Mapping[KT, VT]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            keys_type, values_type = get_args(source_type)\n            return handler(Mapping[keys_type, values_type])\n\n    class CustomOrderedDict(OrderedDict[KT, VT]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            keys_type, values_type = get_args(source_type)\n            return core_schema.no_info_after_validator_function(cls, handler(OrderedDict[keys_type, values_type]))\n\n    class CustomSet(Set[T]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(cls, handler(Set[get_args(source_type)[0]]))\n\n    class CustomTuple(Tuple[T]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(cls, handler(Tuple[get_args(source_type)[0]]))\n\n    class CustomLongTuple(Tuple[T, VT]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(cls, handler(Tuple[get_args(source_type)]))\n\n    class Model(BaseModel, Generic[T, KT, VT]):\n        counter_field: CustomCounter[T]\n        default_dict_field: CustomDefaultDict[KT, VT]\n        deque_field: CustomDeque[T]\n        dict_field: CustomDict[KT, VT]\n        frozenset_field: CustomFrozenset[T]\n        iterable_field: CustomIterable[T]\n        list_field: CustomList[T]\n        mapping_field: CustomMapping[KT, VT]\n        ordered_dict_field: CustomOrderedDict[KT, VT]\n        set_field: CustomSet[T]\n        tuple_field: CustomTuple[T]\n        long_tuple_field: CustomLongTuple[T, VT]\n\n    assert replace_types(Model, {T: bool, KT: str, VT: int}) == Model[bool, str, int]\n    assert replace_types(Model[T, KT, VT], {T: bool, KT: str, VT: int}) == Model[bool, str, int]\n    assert replace_types(Model[T, VT, KT], {T: bool, KT: str, VT: int}) == Model[T, VT, KT][bool, int, str]\n\n    m = Model[bool, str, int](\n        counter_field=Counter([True, False]),\n        default_dict_field={'a': 1},\n        deque_field=[True, False],\n        dict_field={'a': 1},\n        frozenset_field=frozenset([True, False]),\n        iterable_field=[True, False],\n        list_field=[True, False],\n        mapping_field={'a': 2},\n        ordered_dict_field=OrderedDict([('a', 1)]),\n        set_field={True, False},\n        tuple_field=(True,),\n        long_tuple_field=(True, 42),\n    )\n\n    # The following assertions are just to document the current behavior, and should\n    # be updated if/when we do a better job of respecting the exact annotated type\n    assert type(m.counter_field) is CustomCounter\n    # assert type(m.default_dict_field) is CustomDefaultDict\n    assert type(m.deque_field) is CustomDeque\n    assert type(m.dict_field) is CustomDict\n    assert type(m.frozenset_field) is CustomFrozenset\n    assert type(m.iterable_field) is CustomIterable\n    assert type(m.list_field) is CustomList\n    assert type(m.mapping_field) is dict  # this is determined in CustomMapping.__get_pydantic_core_schema__\n    assert type(m.ordered_dict_field) is CustomOrderedDict\n    assert type(m.set_field) is CustomSet\n    assert type(m.tuple_field) is CustomTuple\n    assert type(m.long_tuple_field) is CustomLongTuple\n\n    assert m.model_dump() == {\n        'counter_field': {False: 1, True: 1},\n        'default_dict_field': {'a': 1},\n        'deque_field': deque([True, False]),\n        'dict_field': {'a': 1},\n        'frozenset_field': frozenset({False, True}),\n        'iterable_field': HasRepr(IsStr(regex=r'SerializationIterator\\(index=0, iterator=.*CustomIterable.*')),\n        'list_field': [True, False],\n        'mapping_field': {'a': 2},\n        'ordered_dict_field': {'a': 1},\n        'set_field': {False, True},\n        'tuple_field': (True,),\n        'long_tuple_field': (True, 42),\n    }\n\n\ndef test_custom_sequence_behavior():\n    T = TypeVar('T')\n\n    class CustomSequence(Sequence[T]):\n        pass\n\n    with pytest.raises(\n        PydanticSchemaGenerationError,\n        match=(\n            r'Unable to generate pydantic-core schema for .*'\n            ' Set `arbitrary_types_allowed=True` in the model_config to ignore this error'\n            ' or implement `__get_pydantic_core_schema__` on your type to fully support it'\n        ),\n    ):\n\n        class Model(BaseModel, Generic[T]):\n            x: CustomSequence[T]\n\n\ndef test_replace_types_identity_on_unchanged():\n    T = TypeVar('T')\n    U = TypeVar('U')\n\n    type_ = List[Union[str, Callable[[list], Optional[str]], U]]\n    assert replace_types(type_, {T: int}) is type_\n\n\ndef test_deep_generic():\n    T = TypeVar('T')\n    S = TypeVar('S')\n    R = TypeVar('R')\n\n    class OuterModel(BaseModel, Generic[T, S, R]):\n        a: Dict[R, Optional[List[T]]]\n        b: Optional[Union[S, R]]\n        c: R\n        d: float\n\n    class InnerModel(BaseModel, Generic[T, R]):\n        c: T\n        d: R\n\n    class NormalModel(BaseModel):\n        e: int\n        f: str\n\n    inner_model = InnerModel[int, str]\n    generic_model = OuterModel[inner_model, NormalModel, int]\n\n    inner_models = [inner_model(c=1, d='a')]\n    generic_model(a={1: inner_models, 2: None}, b=None, c=1, d=1.5)\n    generic_model(a={}, b=NormalModel(e=1, f='a'), c=1, d=1.5)\n    generic_model(a={}, b=1, c=1, d=1.5)\n\n    assert InnerModel.__pydantic_generic_metadata__['parameters']  # i.e., InnerModel is not concrete\n    assert not inner_model.__pydantic_generic_metadata__['parameters']  # i.e., inner_model is concrete\n\n\ndef test_deep_generic_with_inner_typevar():\n    T = TypeVar('T')\n\n    class OuterModel(BaseModel, Generic[T]):\n        a: List[T]\n\n    class InnerModel(OuterModel[T], Generic[T]):\n        pass\n\n    assert not InnerModel[int].__pydantic_generic_metadata__['parameters']  # i.e., InnerModel[int] is concrete\n    assert InnerModel.__pydantic_generic_metadata__['parameters']  # i.e., InnerModel is not concrete\n\n    with pytest.raises(ValidationError):\n        InnerModel[int](a=['wrong'])\n    assert InnerModel[int](a=['1']).a == [1]\n\n\ndef test_deep_generic_with_referenced_generic():\n    T = TypeVar('T')\n    R = TypeVar('R')\n\n    class ReferencedModel(BaseModel, Generic[R]):\n        a: R\n\n    class OuterModel(BaseModel, Generic[T]):\n        a: ReferencedModel[T]\n\n    class InnerModel(OuterModel[T], Generic[T]):\n        pass\n\n    assert not InnerModel[int].__pydantic_generic_metadata__['parameters']\n    assert InnerModel.__pydantic_generic_metadata__['parameters']\n\n    with pytest.raises(ValidationError):\n        InnerModel[int](a={'a': 'wrong'})\n    assert InnerModel[int](a={'a': 1}).a.a == 1\n\n\ndef test_deep_generic_with_referenced_inner_generic():\n    T = TypeVar('T')\n\n    class ReferencedModel(BaseModel, Generic[T]):\n        a: T\n\n    class OuterModel(BaseModel, Generic[T]):\n        a: Optional[List[Union[ReferencedModel[T], str]]]\n\n    class InnerModel(OuterModel[T], Generic[T]):\n        pass\n\n    assert not InnerModel[int].__pydantic_generic_metadata__['parameters']\n    assert InnerModel.__pydantic_generic_metadata__['parameters']\n\n    with pytest.raises(ValidationError):\n        InnerModel[int](a=['s', {'a': 'wrong'}])\n    assert InnerModel[int](a=['s', {'a': 1}]).a[1].a == 1\n\n    assert InnerModel[int].model_fields['a'].annotation == Optional[List[Union[ReferencedModel[int], str]]]\n\n\ndef test_deep_generic_with_multiple_typevars():\n    T = TypeVar('T')\n    U = TypeVar('U')\n\n    class OuterModel(BaseModel, Generic[T]):\n        data: List[T]\n\n    class InnerModel(OuterModel[T], Generic[U, T]):\n        extra: U\n\n    ConcreteInnerModel = InnerModel[int, float]\n\n    assert ConcreteInnerModel.model_fields['data'].annotation == List[float]\n    assert ConcreteInnerModel.model_fields['extra'].annotation == int\n\n    assert ConcreteInnerModel(data=['1'], extra='2').model_dump() == {'data': [1.0], 'extra': 2}\n\n\ndef test_deep_generic_with_multiple_inheritance():\n    K = TypeVar('K')\n    V = TypeVar('V')\n    T = TypeVar('T')\n\n    class OuterModelA(BaseModel, Generic[K, V]):\n        data: Dict[K, V]\n\n    class OuterModelB(BaseModel, Generic[T]):\n        stuff: List[T]\n\n    class InnerModel(OuterModelA[K, V], OuterModelB[T], Generic[K, V, T]):\n        extra: int\n\n    ConcreteInnerModel = InnerModel[int, float, str]\n\n    assert ConcreteInnerModel.model_fields['data'].annotation == Dict[int, float]\n    assert ConcreteInnerModel.model_fields['stuff'].annotation == List[str]\n    assert ConcreteInnerModel.model_fields['extra'].annotation == int\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConcreteInnerModel(data={1.1: '5'}, stuff=[123], extra=5)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 123, 'loc': ('stuff', 0), 'msg': 'Input should be a valid string', 'type': 'string_type'},\n        {\n            'input': 1.1,\n            'loc': ('data', '1.1', '[key]'),\n            'msg': 'Input should be a valid integer, got a number with a fractional part',\n            'type': 'int_from_float',\n        },\n    ]\n\n    assert ConcreteInnerModel(data={1: 5}, stuff=['123'], extra=5).model_dump() == {\n        'data': {1: 5},\n        'stuff': ['123'],\n        'extra': 5,\n    }\n\n\ndef test_generic_with_referenced_generic_type_1():\n    T = TypeVar('T')\n\n    class ModelWithType(BaseModel, Generic[T]):\n        # Type resolves to type origin of \"type\" which is non-subscriptible for\n        # python < 3.9 so we want to make sure it works for other versions\n        some_type: Type[T]\n\n    class ReferenceModel(BaseModel, Generic[T]):\n        abstract_base_with_type: ModelWithType[T]\n\n    ReferenceModel[int]\n\n\ndef test_generic_with_referenced_generic_type_bound():\n    T = TypeVar('T', bound=int)\n\n    class ModelWithType(BaseModel, Generic[T]):\n        # Type resolves to type origin of \"type\" which is non-subscriptible for\n        # python < 3.9 so we want to make sure it works for other versions\n        some_type: Type[T]\n\n    class ReferenceModel(BaseModel, Generic[T]):\n        abstract_base_with_type: ModelWithType[T]\n\n    class MyInt(int): ...\n\n    ReferenceModel[MyInt]\n\n\ndef test_generic_with_referenced_generic_union_type_bound():\n    T = TypeVar('T', bound=Union[str, int])\n\n    class ModelWithType(BaseModel, Generic[T]):\n        some_type: Type[T]\n\n    class MyInt(int): ...\n\n    class MyStr(str): ...\n\n    ModelWithType[MyInt]\n    ModelWithType[MyStr]\n\n\ndef test_generic_with_referenced_generic_type_constraints():\n    T = TypeVar('T', int, str)\n\n    class ModelWithType(BaseModel, Generic[T]):\n        # Type resolves to type origin of \"type\" which is non-subscriptible for\n        # python < 3.9 so we want to make sure it works for other versions\n        some_type: Type[T]\n\n    class ReferenceModel(BaseModel, Generic[T]):\n        abstract_base_with_type: ModelWithType[T]\n\n    ReferenceModel[int]\n\n\ndef test_generic_with_referenced_nested_typevar():\n    T = TypeVar('T')\n\n    class ModelWithType(BaseModel, Generic[T]):\n        # Type resolves to type origin of \"collections.abc.Sequence\" which is\n        # non-subscriptible for\n        # python < 3.9 so we want to make sure it works for other versions\n        some_type: Sequence[T]\n\n    class ReferenceModel(BaseModel, Generic[T]):\n        abstract_base_with_type: ModelWithType[T]\n\n    ReferenceModel[int]\n\n\ndef test_generic_with_callable():\n    T = TypeVar('T')\n\n    class Model(BaseModel, Generic[T]):\n        # Callable is a test for any type that accepts a list as an argument\n        some_callable: Callable[[Optional[int], T], None]\n\n    assert not Model[str].__pydantic_generic_metadata__['parameters']\n    assert Model.__pydantic_generic_metadata__['parameters']\n\n\ndef test_generic_with_partial_callable():\n    T = TypeVar('T')\n    U = TypeVar('U')\n\n    class Model(BaseModel, Generic[T, U]):\n        t: T\n        u: U\n        # Callable is a test for any type that accepts a list as an argument\n        some_callable: Callable[[Optional[int], str], None]\n\n    assert Model[str, U].__pydantic_generic_metadata__['parameters'] == (U,)\n    assert not Model[str, int].__pydantic_generic_metadata__['parameters']\n\n\ndef test_generic_recursive_models(create_module):\n    @create_module\n    def module():\n        from typing import Generic, TypeVar, Union\n\n        from pydantic import BaseModel\n\n        T = TypeVar('T')\n\n        class Model1(BaseModel, Generic[T]):\n            ref: 'Model2[T]'\n\n        class Model2(BaseModel, Generic[T]):\n            ref: Union[T, Model1[T]]\n\n        Model1.model_rebuild()\n\n    Model1 = module.Model1\n    Model2 = module.Model2\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model1[str].model_validate(dict(ref=dict(ref=dict(ref=dict(ref=123)))))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_type',\n            'loc': ('ref', 'ref', 'str'),\n            'msg': 'Input should be a valid string',\n            'input': {'ref': {'ref': 123}},\n        },\n        {\n            'type': 'string_type',\n            'loc': ('ref', 'ref', 'Model1[str]', 'ref', 'ref', 'str'),\n            'msg': 'Input should be a valid string',\n            'input': 123,\n        },\n        {\n            'type': 'model_type',\n            'loc': ('ref', 'ref', 'Model1[str]', 'ref', 'ref', 'Model1[str]'),\n            'msg': 'Input should be a valid dictionary or instance of Model1[str]',\n            'input': 123,\n            'ctx': {'class_name': 'Model1[str]'},\n        },\n    ]\n    result = Model1(ref=Model2(ref=Model1(ref=Model2(ref='123'))))\n    assert result.model_dump() == {'ref': {'ref': {'ref': {'ref': '123'}}}}\n\n    result = Model1[str].model_validate(dict(ref=dict(ref=dict(ref=dict(ref='123')))))\n    assert result.model_dump() == {'ref': {'ref': {'ref': {'ref': '123'}}}}\n\n\ndef test_generic_recursive_models_separate_parameters(create_module):\n    @create_module\n    def module():\n        from typing import Generic, TypeVar, Union\n\n        from pydantic import BaseModel\n\n        T = TypeVar('T')\n\n        class Model1(BaseModel, Generic[T]):\n            ref: 'Model2[T]'\n\n        S = TypeVar('S')\n\n        class Model2(BaseModel, Generic[S]):\n            ref: Union[S, Model1[S]]\n\n        Model1.model_rebuild()\n\n    Model1 = module.Model1\n    # Model2 = module.Model2\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model1[str].model_validate(dict(ref=dict(ref=dict(ref=dict(ref=123)))))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_type',\n            'loc': ('ref', 'ref', 'str'),\n            'msg': 'Input should be a valid string',\n            'input': {'ref': {'ref': 123}},\n        },\n        {\n            'type': 'string_type',\n            'loc': ('ref', 'ref', 'Model1[str]', 'ref', 'ref', 'str'),\n            'msg': 'Input should be a valid string',\n            'input': 123,\n        },\n        {\n            'type': 'model_type',\n            'loc': ('ref', 'ref', 'Model1[str]', 'ref', 'ref', 'Model1[str]'),\n            'msg': 'Input should be a valid dictionary or instance of Model1[str]',\n            'input': 123,\n            'ctx': {'class_name': 'Model1[str]'},\n        },\n    ]\n    # TODO: Unlike in the previous test, the following (commented) line currently produces this error:\n    #   >       result = Model1(ref=Model2(ref=Model1(ref=Model2(ref='123'))))\n    #   E       pydantic_core._pydantic_core.ValidationError: 1 validation error for Model2[~T]\n    #   E       ref\n    #   E         Input should be a valid dictionary [type=dict_type, input_value=Model2(ref='123'), input_type=Model2]\n    #  The root of this problem is that Model2[T] ends up being a proper subclass of Model2 since T != S.\n    #  I am sure we can solve this problem, just need to put a bit more effort in.\n    #  While I don't think we should block merging this functionality on getting the next line to pass,\n    #  I think we should come back and resolve this at some point.\n    # result = Model1(ref=Model2(ref=Model1(ref=Model2(ref='123'))))\n    # assert result.model_dump() == {'ref': {'ref': {'ref': {'ref': '123'}}}}\n\n    result = Model1[str].model_validate(dict(ref=dict(ref=dict(ref=dict(ref='123')))))\n    assert result.model_dump() == {'ref': {'ref': {'ref': {'ref': '123'}}}}\n\n\ndef test_generic_recursive_models_repeated_separate_parameters(create_module):\n    @create_module\n    def module():\n        from typing import Generic, TypeVar, Union\n\n        from pydantic import BaseModel\n\n        T = TypeVar('T')\n\n        class Model1(BaseModel, Generic[T]):\n            ref: 'Model2[T]'\n            ref2: Union['Model2[T]', None] = None\n\n        S = TypeVar('S')\n\n        class Model2(BaseModel, Generic[S]):\n            ref: Union[S, Model1[S]]\n            ref2: Union[S, Model1[S], None] = None\n\n        Model1.model_rebuild()\n\n    Model1 = module.Model1\n    # Model2 = module.Model2\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model1[str].model_validate(dict(ref=dict(ref=dict(ref=dict(ref=123)))))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_type',\n            'loc': ('ref', 'ref', 'str'),\n            'msg': 'Input should be a valid string',\n            'input': {'ref': {'ref': 123}},\n        },\n        {\n            'type': 'string_type',\n            'loc': ('ref', 'ref', 'Model1[str]', 'ref', 'ref', 'str'),\n            'msg': 'Input should be a valid string',\n            'input': 123,\n        },\n        {\n            'type': 'model_type',\n            'loc': ('ref', 'ref', 'Model1[str]', 'ref', 'ref', 'Model1[str]'),\n            'msg': 'Input should be a valid dictionary or instance of Model1[str]',\n            'input': 123,\n            'ctx': {'class_name': 'Model1[str]'},\n        },\n    ]\n\n    result = Model1[str].model_validate(dict(ref=dict(ref=dict(ref=dict(ref='123')))))\n    assert result.model_dump() == {\n        'ref': {'ref': {'ref': {'ref': '123', 'ref2': None}, 'ref2': None}, 'ref2': None},\n        'ref2': None,\n    }\n\n\ndef test_generic_recursive_models_triple(create_module):\n    @create_module\n    def module():\n        from typing import Generic, TypeVar, Union\n\n        from pydantic import BaseModel\n\n        T1 = TypeVar('T1')\n        T2 = TypeVar('T2')\n        T3 = TypeVar('T3')\n\n        class A1(BaseModel, Generic[T1]):\n            a1: 'A2[T1]'\n\n        class A2(BaseModel, Generic[T2]):\n            a2: 'A3[T2]'\n\n        class A3(BaseModel, Generic[T3]):\n            a3: Union['A1[T3]', T3]\n\n        A1.model_rebuild()\n\n    A1 = module.A1\n\n    with pytest.raises(ValidationError) as exc_info:\n        A1[str].model_validate({'a1': {'a2': {'a3': 1}}})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': ('a1', 'a2', 'a3', 'A1[str]'),\n            'msg': 'Input should be a valid dictionary or instance of A1[str]',\n            'input': 1,\n            'ctx': {'class_name': 'A1[str]'},\n        },\n        {'type': 'string_type', 'loc': ('a1', 'a2', 'a3', 'str'), 'msg': 'Input should be a valid string', 'input': 1},\n    ]\n\n    A1[int].model_validate({'a1': {'a2': {'a3': 1}}})\n\n\ndef test_generic_recursive_models_with_a_concrete_parameter(create_module):\n    @create_module\n    def module():\n        from typing import Generic, TypeVar, Union\n\n        from pydantic import BaseModel\n\n        V1 = TypeVar('V1')\n        V2 = TypeVar('V2')\n        V3 = TypeVar('V3')\n\n        class M1(BaseModel, Generic[V1, V2]):\n            a: V1\n            m: 'M2[V2]'\n\n        class M2(BaseModel, Generic[V3]):\n            m: Union[M1[int, V3], V3]\n\n        M1.model_rebuild()\n\n    M1 = module.M1\n\n    # assert M1.__pydantic_core_schema__ == {}\n    assert collect_invalid_schemas(M1.__pydantic_core_schema__) is False\n\n\ndef test_generic_recursive_models_complicated(create_module):\n    \"\"\"\n    Note: If we drop the use of LimitedDict and use WeakValueDictionary only, this test will fail if run by itself.\n        This is due to weird behavior with the WeakValueDictionary used for caching.\n        As part of the next batch of generics work, we should attempt to fix this if possible.\n        In the meantime, if this causes issues, or the test otherwise starts failing, please make it xfail\n        with strict=False\n    \"\"\"\n\n    @create_module\n    def module():\n        from typing import Generic, TypeVar, Union\n\n        from pydantic import BaseModel\n\n        T1 = TypeVar('T1')\n        T2 = TypeVar('T2')\n        T3 = TypeVar('T3')\n\n        class A1(BaseModel, Generic[T1]):\n            a1: 'A2[T1]'\n\n        class A2(BaseModel, Generic[T2]):\n            a2: 'A3[T2]'\n\n        class A3(BaseModel, Generic[T3]):\n            a3: Union[A1[T3], T3]\n\n        A1.model_rebuild()\n\n        S1 = TypeVar('S1')\n        S2 = TypeVar('S2')\n\n        class B1(BaseModel, Generic[S1]):\n            a1: 'B2[S1]'\n\n        class B2(BaseModel, Generic[S2]):\n            a2: 'B1[S2]'\n\n        B1.model_rebuild()\n\n        V1 = TypeVar('V1')\n        V2 = TypeVar('V2')\n        V3 = TypeVar('V3')\n\n        class M1(BaseModel, Generic[V1, V2]):\n            a: int\n            b: B1[V2]\n            m: 'M2[V1]'\n\n        class M2(BaseModel, Generic[V3]):\n            m: Union[M1[V3, int], V3]\n\n        M1.model_rebuild()\n\n    M1 = module.M1\n\n    assert collect_invalid_schemas(M1.__pydantic_core_schema__) is False\n\n\ndef test_generic_recursive_models_in_container(create_module):\n    @create_module\n    def module():\n        from typing import Generic, List, Optional, TypeVar\n\n        from pydantic import BaseModel\n\n        T = TypeVar('T')\n\n        class MyGenericModel(BaseModel, Generic[T]):\n            foobar: Optional[List['MyGenericModel[T]']]\n            spam: T\n\n    MyGenericModel = module.MyGenericModel\n    instance = MyGenericModel[int](foobar=[{'foobar': [], 'spam': 1}], spam=1)\n    assert type(instance.foobar[0]) == MyGenericModel[int]\n\n\ndef test_generic_enum():\n    T = TypeVar('T')\n\n    class SomeGenericModel(BaseModel, Generic[T]):\n        some_field: T\n\n    class SomeStringEnum(str, Enum):\n        A = 'A'\n        B = 'B'\n\n    class MyModel(BaseModel):\n        my_gen: SomeGenericModel[SomeStringEnum]\n\n    m = MyModel.model_validate({'my_gen': {'some_field': 'A'}})\n    assert m.my_gen.some_field is SomeStringEnum.A\n\n\ndef test_generic_literal():\n    FieldType = TypeVar('FieldType')\n    ValueType = TypeVar('ValueType')\n\n    class GModel(BaseModel, Generic[FieldType, ValueType]):\n        field: Dict[FieldType, ValueType]\n\n    Fields = Literal['foo', 'bar']\n    m = GModel[Fields, str](field={'foo': 'x'})\n    assert m.model_dump() == {'field': {'foo': 'x'}}\n\n\ndef test_generic_enums():\n    T = TypeVar('T')\n\n    class GModel(BaseModel, Generic[T]):\n        x: T\n\n    class EnumA(str, Enum):\n        a = 'a'\n\n    class EnumB(str, Enum):\n        b = 'b'\n\n    class Model(BaseModel):\n        g_a: GModel[EnumA]\n        g_b: GModel[EnumB]\n\n    assert set(Model.model_json_schema()['$defs']) == {'EnumA', 'EnumB', 'GModel_EnumA_', 'GModel_EnumB_'}\n\n\ndef test_generic_with_user_defined_generic_field():\n    T = TypeVar('T')\n\n    class GenericList(List[T]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(GenericList, handler(List[get_args(source_type)[0]]))\n\n    class Model(BaseModel, Generic[T]):\n        field: GenericList[T]\n\n    model = Model[int](field=[5])\n    assert model.field[0] == 5\n\n    with pytest.raises(ValidationError):\n        model = Model[int](field=['a'])\n\n\ndef test_generic_annotated():\n    T = TypeVar('T')\n\n    class SomeGenericModel(BaseModel, Generic[T]):\n        some_field: Annotated[T, Field(alias='the_alias')]\n\n    SomeGenericModel[str](the_alias='qwe')\n\n\ndef test_generic_subclass():\n    T = TypeVar('T')\n\n    class A(BaseModel, Generic[T]): ...\n\n    class B(A[T], Generic[T]): ...\n\n    class C(B[T], Generic[T]): ...\n\n    assert B[int].__name__ == 'B[int]'\n    assert issubclass(B[int], B)\n    assert issubclass(B[int], A)\n    assert not issubclass(B[int], C)\n\n\ndef test_generic_subclass_with_partial_application():\n    T = TypeVar('T')\n    S = TypeVar('S')\n\n    class A(BaseModel, Generic[T]): ...\n\n    class B(A[S], Generic[T, S]): ...\n\n    PartiallyAppliedB = B[str, T]\n    assert issubclass(PartiallyAppliedB[int], A)\n\n\ndef test_multilevel_generic_binding():\n    T = TypeVar('T')\n    S = TypeVar('S')\n\n    class A(BaseModel, Generic[T, S]): ...\n\n    class B(A[str, T], Generic[T]): ...\n\n    assert B[int].__name__ == 'B[int]'\n    assert issubclass(B[int], A)\n\n\ndef test_generic_subclass_with_extra_type():\n    T = TypeVar('T')\n    S = TypeVar('S')\n\n    class A(BaseModel, Generic[T]): ...\n\n    class B(A[S], Generic[T, S]): ...\n\n    assert B[int, str].__name__ == 'B[int, str]', B[int, str].__name__\n    assert issubclass(B[str, int], B)\n    assert issubclass(B[str, int], A)\n\n\ndef test_generic_subclass_with_extra_type_requires_all_params():\n    T = TypeVar('T')\n    S = TypeVar('S')\n\n    class A(BaseModel, Generic[T]): ...\n\n    with pytest.raises(\n        TypeError,\n        match=re.escape(\n            'All parameters must be present on typing.Generic; you should inherit from typing.Generic[~T, ~S]'\n        ),\n    ):\n\n        class B(A[T], Generic[S]): ...\n\n\ndef test_generic_subclass_with_extra_type_with_hint_message():\n    E = TypeVar('E', bound=BaseModel)\n    D = TypeVar('D')\n\n    with pytest.warns(\n        GenericBeforeBaseModelWarning,\n        match='Classes should inherit from `BaseModel` before generic classes',\n    ):\n\n        class BaseGenericClass(Generic[E, D], BaseModel):\n            uid: str\n            name: str\n\n    with pytest.raises(\n        TypeError,\n        match=re.escape(\n            'All parameters must be present on typing.Generic; you should inherit from typing.Generic[~E, ~D].'\n            ' Note: `typing.Generic` must go last:'\n            ' `class ChildGenericClass(BaseGenericClass, typing.Generic[~E, ~D]): ...`'\n        ),\n    ):\n        with pytest.warns(\n            GenericBeforeBaseModelWarning,\n            match='Classes should inherit from `BaseModel` before generic classes',\n        ):\n\n            class ChildGenericClass(BaseGenericClass[E, Dict[str, Any]]): ...\n\n\ndef test_multi_inheritance_generic_binding():\n    T = TypeVar('T')\n\n    class A(BaseModel, Generic[T]): ...\n\n    class B(A[int], Generic[T]): ...\n\n    class C(B[str], Generic[T]): ...\n\n    assert C[float].__name__ == 'C[float]'\n    assert issubclass(C[float], B)\n    assert issubclass(C[float], A)\n    assert not issubclass(B[float], C)\n\n\ndef test_parent_field_parametrization():\n    T = TypeVar('T')\n\n    class A(BaseModel, Generic[T]):\n        a: T\n\n    class B(A, Generic[T]):\n        b: T\n\n    with pytest.raises(ValidationError) as exc_info:\n        B[int](a='a', b=1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_multi_inheritance_generic_defaults():\n    T = TypeVar('T')\n\n    class A(BaseModel, Generic[T]):\n        a: T\n        x: str = 'a'\n\n    class B(A[int], Generic[T]):\n        b: Optional[T] = None\n        y: str = 'b'\n\n    class C(B[str], Generic[T]):\n        c: T\n        z: str = 'c'\n\n    assert C(a=1, c=...).model_dump() == {'a': 1, 'b': None, 'c': ..., 'x': 'a', 'y': 'b', 'z': 'c'}\n\n\ndef test_parse_generic_json():\n    T = TypeVar('T')\n\n    class MessageWrapper(BaseModel, Generic[T]):\n        message: Json[T]\n\n    class Payload(BaseModel):\n        payload_field: str\n\n    raw = json.dumps({'payload_field': 'payload'})\n    record = MessageWrapper[Payload](message=raw)\n    assert isinstance(record.message, Payload)\n\n    validation_schema = record.model_json_schema(mode='validation')\n    assert validation_schema == {\n        '$defs': {\n            'Payload': {\n                'properties': {'payload_field': {'title': 'Payload Field', 'type': 'string'}},\n                'required': ['payload_field'],\n                'title': 'Payload',\n                'type': 'object',\n            }\n        },\n        'properties': {\n            'message': {\n                'contentMediaType': 'application/json',\n                'contentSchema': {'$ref': '#/$defs/Payload'},\n                'title': 'Message',\n                'type': 'string',\n            }\n        },\n        'required': ['message'],\n        'title': 'MessageWrapper[test_parse_generic_json.<locals>.Payload]',\n        'type': 'object',\n    }\n\n    serialization_schema = record.model_json_schema(mode='serialization')\n    assert serialization_schema == {\n        '$defs': {\n            'Payload': {\n                'properties': {'payload_field': {'title': 'Payload Field', 'type': 'string'}},\n                'required': ['payload_field'],\n                'title': 'Payload',\n                'type': 'object',\n            }\n        },\n        'properties': {'message': {'allOf': [{'$ref': '#/$defs/Payload'}], 'title': 'Message'}},\n        'required': ['message'],\n        'title': 'MessageWrapper[test_parse_generic_json.<locals>.Payload]',\n        'type': 'object',\n    }\n\n\ndef memray_limit_memory(limit):\n    if '--memray' in sys.argv:\n        return pytest.mark.limit_memory(limit)\n    else:\n        return pytest.mark.skip(reason='memray not enabled')\n\n\n@memray_limit_memory('100 MB')\ndef test_generics_memory_use():\n    \"\"\"See:\n    - https://github.com/pydantic/pydantic/issues/3829\n    - https://github.com/pydantic/pydantic/pull/4083\n    - https://github.com/pydantic/pydantic/pull/5052\n    \"\"\"\n\n    T = TypeVar('T')\n    U = TypeVar('U')\n    V = TypeVar('V')\n\n    class MyModel(BaseModel, Generic[T, U, V]):\n        message: Json[T]\n        field: Dict[U, V]\n\n    class Outer(BaseModel, Generic[T]):\n        inner: T\n\n    types = [\n        int,\n        str,\n        float,\n        bool,\n        bytes,\n    ]\n\n    containers = [\n        List,\n        Tuple,\n        Set,\n        FrozenSet,\n    ]\n\n    all = [*types, *[container[tp] for container in containers for tp in types]]\n\n    total = list(itertools.product(all, all, all))\n\n    for t1, t2, t3 in total:\n\n        class Foo(MyModel[t1, t2, t3]):\n            pass\n\n        class _(Outer[Foo]):\n            pass\n\n\n@pytest.mark.xfail(reason='Generic models are not type aliases', raises=TypeError)\ndef test_generic_model_as_parameter_to_generic_type_alias() -> None:\n    T = TypeVar('T')\n\n    class GenericPydanticModel(BaseModel, Generic[T]):\n        x: T\n\n    GenericPydanticModelList = List[GenericPydanticModel[T]]\n    GenericPydanticModelList[int]\n\n\ndef test_double_typevar_substitution() -> None:\n    T = TypeVar('T')\n\n    class GenericPydanticModel(BaseModel, Generic[T]):\n        x: T = []\n\n    assert GenericPydanticModel[List[T]](x=[1, 2, 3]).model_dump() == {'x': [1, 2, 3]}\n\n\n@pytest.fixture(autouse=True)\ndef ensure_contextvar_gets_reset():\n    # Ensure that the generic recursion contextvar is empty at the start of every test\n    assert not recursively_defined_type_refs()\n\n\ndef test_generic_recursion_contextvar():\n    T = TypeVar('T')\n\n    class TestingException(Exception):\n        pass\n\n    class Model(BaseModel, Generic[T]):\n        pass\n\n    # Make sure that the contextvar-managed recursive types cache begins empty\n    assert not recursively_defined_type_refs()\n    try:\n        with generic_recursion_self_type(Model, (int,)):\n            # Make sure that something has been added to the contextvar-managed recursive types cache\n            assert recursively_defined_type_refs()\n            raise TestingException\n    except TestingException:\n        pass\n\n    # Make sure that an exception causes the contextvar-managed recursive types cache to be reset\n    assert not recursively_defined_type_refs()\n\n\ndef test_limited_dict():\n    d = LimitedDict(10)\n    d[1] = '1'\n    d[2] = '2'\n    assert list(d.items()) == [(1, '1'), (2, '2')]\n    for no in '34567890':\n        d[int(no)] = no\n    assert list(d.items()) == [\n        (1, '1'),\n        (2, '2'),\n        (3, '3'),\n        (4, '4'),\n        (5, '5'),\n        (6, '6'),\n        (7, '7'),\n        (8, '8'),\n        (9, '9'),\n        (0, '0'),\n    ]\n    d[11] = '11'\n\n    # reduce size to 9 after setting 11\n    assert len(d) == 9\n    assert list(d.items()) == [\n        (3, '3'),\n        (4, '4'),\n        (5, '5'),\n        (6, '6'),\n        (7, '7'),\n        (8, '8'),\n        (9, '9'),\n        (0, '0'),\n        (11, '11'),\n    ]\n    d[12] = '12'\n    assert len(d) == 10\n    d[13] = '13'\n    assert len(d) == 9\n\n\ndef test_construct_generic_model_with_validation():\n    T = TypeVar('T')\n\n    class Page(BaseModel, Generic[T]):\n        page: int = Field(ge=42)\n        items: Sequence[T]\n        unenforced: PositiveInt = Field(..., lt=10)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Page[int](page=41, items=[], unenforced=11)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'ge': 42},\n            'input': 41,\n            'loc': ('page',),\n            'msg': 'Input should be greater than or equal to 42',\n            'type': 'greater_than_equal',\n        },\n        {\n            'ctx': {'lt': 10},\n            'input': 11,\n            'loc': ('unenforced',),\n            'msg': 'Input should be less than 10',\n            'type': 'less_than',\n        },\n    ]\n\n\ndef test_construct_other_generic_model_with_validation():\n    # based on the test-case from https://github.com/samuelcolvin/pydantic/issues/2581\n    T = TypeVar('T')\n\n    class Page(BaseModel, Generic[T]):\n        page: int = Field(ge=42)\n        items: Sequence[T]\n\n    # Check we can perform this assignment, this is the actual test\n    concrete_model = Page[str]\n    print(concrete_model)\n    assert concrete_model.__name__ == 'Page[str]'\n\n    # Sanity check the resulting type works as expected\n    valid = concrete_model(page=42, items=[])\n    assert valid.page == 42\n\n    with pytest.raises(ValidationError) as exc_info:\n        concrete_model(page=41, items=[])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'ge': 42},\n            'input': 41,\n            'loc': ('page',),\n            'msg': 'Input should be greater than or equal to 42',\n            'type': 'greater_than_equal',\n        }\n    ]\n\n\ndef test_generic_enum_bound():\n    T = TypeVar('T', bound=Enum)\n\n    class MyEnum(Enum):\n        a = 1\n\n    class OtherEnum(Enum):\n        b = 2\n\n    class Model(BaseModel, Generic[T]):\n        x: T\n\n    m = Model(x=MyEnum.a)\n    assert m.x == MyEnum.a\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'Enum'},\n            'input': 1,\n            'loc': ('x',),\n            'msg': 'Input should be an instance of Enum',\n            'type': 'is_instance_of',\n        }\n    ]\n\n    m2 = Model[MyEnum](x=MyEnum.a)\n    assert m2.x == MyEnum.a\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model[MyEnum](x=OtherEnum.b)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'expected': '1'},\n            'input': OtherEnum.b,\n            'loc': ('x',),\n            'msg': 'Input should be 1',\n            'type': 'enum',\n        }\n    ]\n\n    # insert_assert(Model[MyEnum].model_json_schema())\n    assert Model[MyEnum].model_json_schema() == {\n        '$defs': {'MyEnum': {'const': 1, 'enum': [1], 'title': 'MyEnum', 'type': 'integer'}},\n        'properties': {'x': {'$ref': '#/$defs/MyEnum'}},\n        'required': ['x'],\n        'title': 'Model[test_generic_enum_bound.<locals>.MyEnum]',\n        'type': 'object',\n    }\n\n\ndef test_generic_intenum_bound():\n    T = TypeVar('T', bound=IntEnum)\n\n    class MyEnum(IntEnum):\n        a = 1\n\n    class OtherEnum(IntEnum):\n        b = 2\n\n    class Model(BaseModel, Generic[T]):\n        x: T\n\n    m = Model(x=MyEnum.a)\n    assert m.x == MyEnum.a\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'IntEnum'},\n            'input': 1,\n            'loc': ('x',),\n            'msg': 'Input should be an instance of IntEnum',\n            'type': 'is_instance_of',\n        }\n    ]\n\n    m2 = Model[MyEnum](x=MyEnum.a)\n    assert m2.x == MyEnum.a\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model[MyEnum](x=OtherEnum.b)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'expected': '1'},\n            'input': 2,\n            'loc': ('x',),\n            'msg': 'Input should be 1',\n            'type': 'enum',\n        }\n    ]\n\n    # insert_assert(Model[MyEnum].model_json_schema())\n    assert Model[MyEnum].model_json_schema() == {\n        '$defs': {'MyEnum': {'const': 1, 'enum': [1], 'title': 'MyEnum', 'type': 'integer'}},\n        'properties': {'x': {'$ref': '#/$defs/MyEnum'}},\n        'required': ['x'],\n        'title': 'Model[test_generic_intenum_bound.<locals>.MyEnum]',\n        'type': 'object',\n    }\n\n\n@pytest.mark.skipif(sys.version_info < (3, 11), reason='requires python 3.11 or higher')\n@pytest.mark.xfail(\n    reason='TODO: Variadic generic parametrization is not supported yet;'\n    ' Issue: https://github.com/pydantic/pydantic/issues/5804'\n)\ndef test_variadic_generic_init():\n    class ComponentModel(BaseModel):\n        pass\n\n    class Wrench(ComponentModel):\n        pass\n\n    class Screwdriver(ComponentModel):\n        pass\n\n    ComponentVar = TypeVar('ComponentVar', bound=ComponentModel)\n    NumberOfComponents = TypeVarTuple('NumberOfComponents')\n\n    class VariadicToolbox(BaseModel, Generic[ComponentVar, Unpack[NumberOfComponents]]):\n        main_component: ComponentVar\n        left_component_pocket: Optional[list[ComponentVar]] = Field(default_factory=list)\n        right_component_pocket: Optional[list[ComponentVar]] = Field(default_factory=list)\n\n        @computed_field\n        @property\n        def all_components(self) -> tuple[ComponentVar, Unpack[NumberOfComponents]]:\n            return (self.main_component, *self.left_component_pocket, *self.right_component_pocket)\n\n    sa, sb, w = Screwdriver(), Screwdriver(), Wrench()\n    my_toolbox = VariadicToolbox[Screwdriver, Screwdriver, Wrench](\n        main_component=sa, left_component_pocket=[w], right_component_pocket=[sb]\n    )\n\n    assert my_toolbox.all_components == [sa, w, sb]\n\n\n@pytest.mark.skipif(sys.version_info < (3, 11), reason='requires python 3.11 or higher')\n@pytest.mark.xfail(\n    reason='TODO: Variadic fields are not supported yet; Issue: https://github.com/pydantic/pydantic/issues/5804'\n)\ndef test_variadic_generic_with_variadic_fields():\n    class ComponentModel(BaseModel):\n        pass\n\n    class Wrench(ComponentModel):\n        pass\n\n    class Screwdriver(ComponentModel):\n        pass\n\n    ComponentVar = TypeVar('ComponentVar', bound=ComponentModel)\n    NumberOfComponents = TypeVarTuple('NumberOfComponents')\n\n    class VariadicToolbox(BaseModel, Generic[ComponentVar, Unpack[NumberOfComponents]]):\n        toolbelt_cm_size: Optional[tuple[Unpack[NumberOfComponents]]] = Field(default_factory=tuple)\n        manual_toolset: Optional[tuple[ComponentVar, Unpack[NumberOfComponents]]] = Field(default_factory=tuple)\n\n    MyToolboxClass = VariadicToolbox[Screwdriver, Screwdriver, Wrench]\n\n    sa, sb, w = Screwdriver(), Screwdriver(), Wrench()\n    MyToolboxClass(toolbelt_cm_size=(5, 10.5, 4), manual_toolset=(sa, sb, w))\n\n    with pytest.raises(TypeError):\n        # Should raise error because integer 5 does not meet the bound requirements of ComponentVar\n        MyToolboxClass(manual_toolset=(sa, sb, 5))\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 11),\n    reason=(\n        'Multiple inheritance with NamedTuple and the corresponding type annotations'\n        \" aren't supported before Python 3.11\"\n    ),\n)\ndef test_generic_namedtuple():\n    T = TypeVar('T')\n\n    class FlaggedValue(NamedTuple, Generic[T]):\n        value: T\n        flag: bool\n\n    class Model(BaseModel):\n        f_value: FlaggedValue[float]\n\n    assert Model(f_value=(1, True)).model_dump() == {'f_value': (1, True)}\n    with pytest.raises(ValidationError):\n        Model(f_value=(1, 'abc'))\n    with pytest.raises(ValidationError):\n        Model(f_value=('abc', True))\n\n\ndef test_generic_none():\n    T = TypeVar('T')\n\n    class Container(BaseModel, Generic[T]):\n        value: T\n\n    assert Container[type(None)](value=None).value is None\n    assert Container[None](value=None).value is None\n\n\n@pytest.mark.skipif(platform.python_implementation() == 'PyPy', reason='PyPy does not allow ParamSpec in generics')\ndef test_paramspec_is_usable():\n    # This used to cause a recursion error due to `P in P is True`\n    # This test doesn't actually test that ParamSpec works properly for validation or anything.\n\n    P = ParamSpec('P')\n\n    class MyGenericParamSpecClass(Generic[P]):\n        def __init__(self, func: Callable[P, None], *args: P.args, **kwargs: P.kwargs) -> None:\n            super().__init__()\n\n    class ParamSpecGenericModel(BaseModel, Generic[P]):\n        my_generic: MyGenericParamSpecClass[P]\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n\ndef test_parametrize_with_basemodel():\n    T = TypeVar('T')\n\n    class SimpleGenericModel(BaseModel, Generic[T]):\n        pass\n\n    class Concrete(SimpleGenericModel[BaseModel]):\n        pass\n\n\ndef test_no_generic_base():\n    T = TypeVar('T')\n\n    class A(BaseModel, Generic[T]):\n        a: T\n\n    class B(A[T]):\n        b: T\n\n    class C(B[int]):\n        pass\n\n    assert C(a='1', b='2').model_dump() == {'a': 1, 'b': 2}\n    with pytest.raises(ValidationError) as exc_info:\n        C(a='a', b='b')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'type': 'int_parsing',\n        },\n        {\n            'input': 'b',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'type': 'int_parsing',\n        },\n    ]\n\n\ndef test_reverse_order_generic_hashability():\n    T = TypeVar('T')\n\n    with pytest.warns(\n        GenericBeforeBaseModelWarning,\n        match='Classes should inherit from `BaseModel` before generic classes',\n    ):\n\n        class Model(Generic[T], BaseModel):\n            x: T\n            model_config = dict(frozen=True)\n\n    m1 = Model[int](x=1)\n    m2 = Model[int](x=1)\n    assert len({m1, m2}) == 1\n\n\ndef test_serialize_unsubstituted_typevars_bound() -> None:\n    class ErrorDetails(BaseModel):\n        foo: str\n\n    # This version of `TypeVar` does not support `default` on Python <3.12\n    ErrorDataT = TypeVar('ErrorDataT', bound=ErrorDetails)\n\n    class Error(BaseModel, Generic[ErrorDataT]):\n        message: str\n        details: ErrorDataT\n\n    class MyErrorDetails(ErrorDetails):\n        bar: str\n\n    sample_error = Error(\n        message='We just had an error',\n        details=MyErrorDetails(foo='var', bar='baz'),\n    )\n    assert sample_error.details.model_dump() == {\n        'foo': 'var',\n        'bar': 'baz',\n    }\n    assert sample_error.model_dump() == {\n        'message': 'We just had an error',\n        'details': {\n            'foo': 'var',\n            'bar': 'baz',\n        },\n    }\n\n    sample_error = Error[ErrorDetails](\n        message='We just had an error',\n        details=MyErrorDetails(foo='var', bar='baz'),\n    )\n    assert sample_error.details.model_dump() == {\n        'foo': 'var',\n        'bar': 'baz',\n    }\n    assert sample_error.model_dump() == {\n        'message': 'We just had an error',\n        'details': {\n            'foo': 'var',\n        },\n    }\n\n    sample_error = Error[MyErrorDetails](\n        message='We just had an error',\n        details=MyErrorDetails(foo='var', bar='baz'),\n    )\n    assert sample_error.details.model_dump() == {\n        'foo': 'var',\n        'bar': 'baz',\n    }\n    assert sample_error.model_dump() == {\n        'message': 'We just had an error',\n        'details': {\n            'foo': 'var',\n            'bar': 'baz',\n        },\n    }\n\n\ndef test_serialize_unsubstituted_typevars_bound_default_supported() -> None:\n    class ErrorDetails(BaseModel):\n        foo: str\n\n    # This version of `TypeVar` always support `default`\n    ErrorDataT = TypingExtensionsTypeVar('ErrorDataT', bound=ErrorDetails)\n\n    class Error(BaseModel, Generic[ErrorDataT]):\n        message: str\n        details: ErrorDataT\n\n    class MyErrorDetails(ErrorDetails):\n        bar: str\n\n    sample_error = Error(\n        message='We just had an error',\n        details=MyErrorDetails(foo='var', bar='baz'),\n    )\n    assert sample_error.details.model_dump() == {\n        'foo': 'var',\n        'bar': 'baz',\n    }\n    assert sample_error.model_dump() == {\n        'message': 'We just had an error',\n        'details': {\n            'foo': 'var',\n            'bar': 'baz',\n        },\n    }\n\n    sample_error = Error[ErrorDetails](\n        message='We just had an error',\n        details=MyErrorDetails(foo='var', bar='baz'),\n    )\n    assert sample_error.details.model_dump() == {\n        'foo': 'var',\n        'bar': 'baz',\n    }\n    assert sample_error.model_dump() == {\n        'message': 'We just had an error',\n        'details': {\n            'foo': 'var',\n        },\n    }\n\n    sample_error = Error[MyErrorDetails](\n        message='We just had an error',\n        details=MyErrorDetails(foo='var', bar='baz'),\n    )\n    assert sample_error.details.model_dump() == {\n        'foo': 'var',\n        'bar': 'baz',\n    }\n    assert sample_error.model_dump() == {\n        'message': 'We just had an error',\n        'details': {\n            'foo': 'var',\n            'bar': 'baz',\n        },\n    }\n\n\n@pytest.mark.parametrize(\n    'type_var',\n    [\n        TypingExtensionsTypeVar('ErrorDataT', default=BaseModel),\n        TypeVar('ErrorDataT', BaseModel, str),\n    ],\n    ids=['default', 'constraint'],\n)\ndef test_serialize_unsubstituted_typevars_variants(\n    type_var: Type[BaseModel],\n) -> None:\n    class ErrorDetails(BaseModel):\n        foo: str\n\n    class Error(BaseModel, Generic[type_var]):  # type: ignore\n        message: str\n        details: type_var\n\n    class MyErrorDetails(ErrorDetails):\n        bar: str\n\n    sample_error = Error(\n        message='We just had an error',\n        details=MyErrorDetails(foo='var', bar='baz'),\n    )\n    assert sample_error.details.model_dump() == {\n        'foo': 'var',\n        'bar': 'baz',\n    }\n    assert sample_error.model_dump() == {\n        'message': 'We just had an error',\n        'details': {},\n    }\n\n    sample_error = Error[ErrorDetails](\n        message='We just had an error',\n        details=MyErrorDetails(foo='var', bar='baz'),\n    )\n    assert sample_error.details.model_dump() == {\n        'foo': 'var',\n        'bar': 'baz',\n    }\n    assert sample_error.model_dump() == {\n        'message': 'We just had an error',\n        'details': {\n            'foo': 'var',\n        },\n    }\n\n    sample_error = Error[MyErrorDetails](\n        message='We just had an error',\n        details=MyErrorDetails(foo='var', bar='baz'),\n    )\n    assert sample_error.details.model_dump() == {\n        'foo': 'var',\n        'bar': 'baz',\n    }\n    assert sample_error.model_dump() == {\n        'message': 'We just had an error',\n        'details': {\n            'foo': 'var',\n            'bar': 'baz',\n        },\n    }\n\n\ndef test_mix_default_and_constraints() -> None:\n    T = TypingExtensionsTypeVar('T', str, int, default=str)\n\n    msg = 'Pydantic does not support mixing more than one of TypeVar bounds, constraints and defaults'\n    with pytest.raises(NotImplementedError, match=msg):\n\n        class _(BaseModel, Generic[T]):\n            x: T\n\n\ndef test_generic_with_not_required_in_typed_dict() -> None:\n    T = TypingExtensionsTypeVar('T')\n\n    class FooStr(TypedDict):\n        type: NotRequired[str]\n\n    class FooGeneric(TypedDict, Generic[T]):\n        type: NotRequired[T]\n\n    ta_foo_str = TypeAdapter(FooStr)\n    assert ta_foo_str.validate_python({'type': 'tomato'}) == {'type': 'tomato'}\n    assert ta_foo_str.validate_python({}) == {}\n    ta_foo_generic = TypeAdapter(FooGeneric[str])\n    assert ta_foo_generic.validate_python({'type': 'tomato'}) == {'type': 'tomato'}\n    assert ta_foo_generic.validate_python({}) == {}\n\n\ndef test_generic_with_allow_extra():\n    T = TypeVar('T')\n\n    # This used to raise an error related to accessing the __annotations__ attribute of the Generic class\n    class AllowExtraGeneric(BaseModel, Generic[T], extra='allow'):\n        data: T\n", "tests/test_networks.py": "import json\nfrom typing import Union\n\nimport pytest\nfrom pydantic_core import PydanticCustomError, Url\nfrom typing_extensions import Annotated\n\nfrom pydantic import (\n    AmqpDsn,\n    AnyUrl,\n    BaseModel,\n    ClickHouseDsn,\n    CockroachDsn,\n    FileUrl,\n    FtpUrl,\n    HttpUrl,\n    KafkaDsn,\n    MariaDBDsn,\n    MongoDsn,\n    MySQLDsn,\n    NameEmail,\n    NatsDsn,\n    PostgresDsn,\n    RedisDsn,\n    Strict,\n    UrlConstraints,\n    ValidationError,\n    WebsocketUrl,\n)\nfrom pydantic.networks import validate_email\n\ntry:\n    import email_validator\nexcept ImportError:\n    email_validator = None\n\n\n@pytest.mark.parametrize(\n    'value',\n    [\n        'http://example.org',\n        'http://test',\n        'http://localhost',\n        'https://example.org/whatever/next/',\n        'postgres://user:pass@localhost:5432/app',\n        'postgres://just-user@localhost:5432/app',\n        'postgresql+asyncpg://user:pass@localhost:5432/app',\n        'postgresql+pg8000://user:pass@localhost:5432/app',\n        'postgresql+psycopg://postgres:postgres@localhost:5432/hatch',\n        'postgresql+psycopg2://postgres:postgres@localhost:5432/hatch',\n        'postgresql+psycopg2cffi://user:pass@localhost:5432/app',\n        'postgresql+py-postgresql://user:pass@localhost:5432/app',\n        'postgresql+pygresql://user:pass@localhost:5432/app',\n        'mysql://user:pass@localhost:3306/app',\n        'mysql+mysqlconnector://user:pass@localhost:3306/app',\n        'mysql+aiomysql://user:pass@localhost:3306/app',\n        'mysql+asyncmy://user:pass@localhost:3306/app',\n        'mysql+mysqldb://user:pass@localhost:3306/app',\n        'mysql+pymysql://user:pass@localhost:3306/app?charset=utf8mb4',\n        'mysql+cymysql://user:pass@localhost:3306/app',\n        'mysql+pyodbc://user:pass@localhost:3306/app',\n        'mariadb://user:pass@localhost:3306/app',\n        'mariadb+mariadbconnector://user:pass@localhost:3306/app',\n        'mariadb+pymysql://user:pass@localhost:3306/app',\n        'foo-bar://example.org',\n        'foo.bar://example.org',\n        'foo0bar://example.org',\n        'https://example.org',\n        'http://localhost',\n        'http://localhost/',\n        'http://localhost:8000',\n        'http://localhost:8000/',\n        'https://foo_bar.example.com/',\n        'ftp://example.org',\n        'ftps://example.org',\n        'http://example.co.jp',\n        'http://www.example.com/a%C2%B1b',\n        'http://www.example.com/~username/',\n        'http://info.example.com?fred',\n        'http://info.example.com/?fred',\n        'http://xn--mgbh0fb.xn--kgbechtv/',\n        'http://example.com/blue/red%3Fand+green',\n        'http://www.example.com/?array%5Bkey%5D=value',\n        'http://xn--rsum-bpad.example.org/',\n        'http://123.45.67.8/',\n        'http://123.45.67.8:8329/',\n        'http://[2001:db8::ff00:42]:8329',\n        'http://[2001::1]:8329',\n        'http://[2001:db8::1]/',\n        'http://www.example.com:8000/foo',\n        'http://www.cwi.nl:80/%7Eguido/Python.html',\n        'https://www.python.org/\u043f\u0443\u0442\u044c',\n        'http://\u0430\u043d\u0434\u0440\u0435\u0439@example.com',\n        # AnyUrl('https://example.com', scheme='https', host='example.com'),\n        'https://exam_ple.com/',\n        'http://twitter.com/@handle/',\n        'http://11.11.11.11.example.com/action',\n        'http://abc.11.11.11.11.example.com/action',\n        'http://example#',\n        'http://example/#',\n        'http://example/#fragment',\n        'http://example/?#',\n        'http://example.org/path#',\n        'http://example.org/path#fragment',\n        'http://example.org/path?query#',\n        'http://example.org/path?query#fragment',\n        'file://localhost/foo/bar',\n    ],\n)\ndef test_any_url_success(value):\n    class Model(BaseModel):\n        v: AnyUrl\n\n    assert Model(v=value).v, value\n\n\n@pytest.mark.parametrize(\n    'value,err_type,err_msg',\n    [\n        ('http:///', 'url_parsing', 'Input should be a valid URL, empty host'),\n        ('http://??', 'url_parsing', 'Input should be a valid URL, empty host'),\n        (\n            'https://example.org more',\n            'url_parsing',\n            'Input should be a valid URL, invalid domain character',\n        ),\n        ('$https://example.org', 'url_parsing', 'Input should be a valid URL, relative URL without a base'),\n        ('../icons/logo.gif', 'url_parsing', 'Input should be a valid URL, relative URL without a base'),\n        ('abc', 'url_parsing', 'Input should be a valid URL, relative URL without a base'),\n        ('..', 'url_parsing', 'Input should be a valid URL, relative URL without a base'),\n        ('/', 'url_parsing', 'Input should be a valid URL, relative URL without a base'),\n        ('+http://example.com/', 'url_parsing', 'Input should be a valid URL, relative URL without a base'),\n        ('ht*tp://example.com/', 'url_parsing', 'Input should be a valid URL, relative URL without a base'),\n        (' ', 'url_parsing', 'Input should be a valid URL, relative URL without a base'),\n        ('', 'url_parsing', 'Input should be a valid URL, input is empty'),\n        (None, 'url_type', 'URL input should be a string or URL'),\n        (\n            'http://2001:db8::ff00:42:8329',\n            'url_parsing',\n            'Input should be a valid URL, invalid port number',\n        ),\n        ('http://[192.168.1.1]:8329', 'url_parsing', 'Input should be a valid URL, invalid IPv6 address'),\n        ('http://example.com:99999', 'url_parsing', 'Input should be a valid URL, invalid port number'),\n    ],\n)\ndef test_any_url_invalid(value, err_type, err_msg):\n    class Model(BaseModel):\n        v: AnyUrl\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    assert len(exc_info.value.errors(include_url=False)) == 1, exc_info.value.errors(include_url=False)\n    error = exc_info.value.errors(include_url=False)[0]\n    # debug(error)\n    assert {'type': error['type'], 'msg': error['msg']} == {'type': err_type, 'msg': err_msg}\n\n\ndef validate_url(s):\n    class Model(BaseModel):\n        v: AnyUrl\n\n    return Model(v=s).v\n\n\ndef test_any_url_parts():\n    url = validate_url('http://example.org')\n    assert str(url) == 'http://example.org/'\n    assert repr(url) == \"Url('http://example.org/')\"\n    assert url.scheme == 'http'\n    assert url.host == 'example.org'\n    assert url.port == 80\n\n\ndef test_url_repr():\n    url = validate_url('http://user:password@example.org:1234/the/path/?query=here#fragment=is;this=bit')\n    assert str(url) == 'http://user:password@example.org:1234/the/path/?query=here#fragment=is;this=bit'\n    assert repr(url) == \"Url('http://user:password@example.org:1234/the/path/?query=here#fragment=is;this=bit')\"\n    assert url.scheme == 'http'\n    assert url.username == 'user'\n    assert url.password == 'password'\n    assert url.host == 'example.org'\n    assert url.port == 1234\n    assert url.path == '/the/path/'\n    assert url.query == 'query=here'\n    assert url.fragment == 'fragment=is;this=bit'\n\n\ndef test_ipv4_port():\n    url = validate_url('ftp://123.45.67.8:8329/')\n    assert url.scheme == 'ftp'\n    assert url.host == '123.45.67.8'\n    assert url.port == 8329\n    assert url.username is None\n    assert url.password is None\n\n\ndef test_ipv4_no_port():\n    url = validate_url('ftp://123.45.67.8')\n    assert url.scheme == 'ftp'\n    assert url.host == '123.45.67.8'\n    assert url.port == 21\n    assert url.username is None\n    assert url.password is None\n\n\ndef test_ipv6_port():\n    url = validate_url('wss://[2001:db8::ff00:42]:8329')\n    assert url.scheme == 'wss'\n    assert url.host == '[2001:db8::ff00:42]'\n    assert url.port == 8329\n\n\ndef test_int_domain():\n    url = validate_url('https://\u00a3\u00a3\u00a3.org')\n    assert url.host == 'xn--9aaa.org'\n    assert str(url) == 'https://xn--9aaa.org/'\n\n\ndef test_co_uk():\n    url = validate_url('http://example.co.uk')\n    assert str(url) == 'http://example.co.uk/'\n    assert url.scheme == 'http'\n    assert url.host == 'example.co.uk'\n\n\ndef test_user_no_password():\n    url = validate_url('http://user:@example.org')\n    assert url.username == 'user'\n    assert url.password is None\n    assert url.host == 'example.org'\n\n\ndef test_user_info_no_user():\n    url = validate_url('http://:password@example.org')\n    assert url.username is None\n    assert url.password == 'password'\n    assert url.host == 'example.org'\n\n\ndef test_at_in_path():\n    url = validate_url('https://twitter.com/@handle')\n    assert url.scheme == 'https'\n    assert url.host == 'twitter.com'\n    assert url.username is None\n    assert url.password is None\n    assert url.path == '/@handle'\n\n\ndef test_fragment_without_query():\n    url = validate_url('https://docs.pydantic.dev/usage/types/#constrained-types')\n    assert url.scheme == 'https'\n    assert url.host == 'docs.pydantic.dev'\n    assert url.path == '/usage/types/'\n    assert url.query is None\n    assert url.fragment == 'constrained-types'\n\n\n@pytest.mark.parametrize(\n    'value,expected',\n    [\n        ('http://example.org', 'http://example.org/'),\n        ('http://example.org/foobar', 'http://example.org/foobar'),\n        ('http://example.org.', 'http://example.org./'),\n        ('http://example.org./foobar', 'http://example.org./foobar'),\n        ('HTTP://EXAMPLE.ORG', 'http://example.org/'),\n        ('https://example.org', 'https://example.org/'),\n        ('https://example.org?a=1&b=2', 'https://example.org/?a=1&b=2'),\n        ('https://example.org#a=3;b=3', 'https://example.org/#a=3;b=3'),\n        ('https://foo_bar.example.com/', 'https://foo_bar.example.com/'),\n        ('https://exam_ple.com/', 'https://exam_ple.com/'),\n        ('https://example.xn--p1ai', 'https://example.xn--p1ai/'),\n        ('https://example.xn--vermgensberatung-pwb', 'https://example.xn--vermgensberatung-pwb/'),\n        ('https://example.xn--zfr164b', 'https://example.xn--zfr164b/'),\n    ],\n)\ndef test_http_url_success(value, expected):\n    class Model(BaseModel):\n        v: HttpUrl\n\n    assert str(Model(v=value).v) == expected\n\n\ndef test_nullable_http_url():\n    class Model(BaseModel):\n        v: Union[HttpUrl, None]\n\n    assert Model(v=None).v is None\n    assert str(Model(v='http://example.org').v) == 'http://example.org/'\n\n\n@pytest.mark.parametrize(\n    'value,err_type,err_msg',\n    [\n        (\n            'ftp://example.com/',\n            'url_scheme',\n            \"URL scheme should be 'http' or 'https'\",\n        ),\n        (\n            'x' * 2084,\n            'url_too_long',\n            'URL should have at most 2083 characters',\n        ),\n    ],\n)\ndef test_http_url_invalid(value, err_type, err_msg):\n    class Model(BaseModel):\n        v: HttpUrl\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    assert len(exc_info.value.errors(include_url=False)) == 1, exc_info.value.errors(include_url=False)\n    error = exc_info.value.errors(include_url=False)[0]\n    assert {'type': error['type'], 'msg': error['msg']} == {'type': err_type, 'msg': err_msg}\n\n\n@pytest.mark.parametrize(\n    'input,output',\n    [\n        ('  https://www.example.com \\n', 'https://www.example.com/'),\n        (b'https://www.example.com', 'https://www.example.com/'),\n        # https://www.xudongz.com/blog/2017/idn-phishing/ accepted but converted\n        ('https://www.\u0430\u0440\u0440\u04cf\u0435.com/', 'https://www.xn--80ak6aa92e.com/'),\n        ('https://exampl\u00a3e.org', 'https://xn--example-gia.org/'),\n        ('https://example.\u73e0\u5b9d', 'https://example.xn--pbt977c/'),\n        ('https://example.verm\u00f6gensberatung', 'https://example.xn--vermgensberatung-pwb/'),\n        ('https://example.\u0440\u0444', 'https://example.xn--p1ai/'),\n        ('https://exampl\u00a3e.\u73e0\u5b9d', 'https://xn--example-gia.xn--pbt977c/'),\n    ],\n)\ndef test_coerce_url(input, output):\n    class Model(BaseModel):\n        v: HttpUrl\n\n    assert str(Model(v=input).v) == output\n\n\n@pytest.mark.parametrize(\n    'value,expected',\n    [\n        ('file:///foo/bar', 'file:///foo/bar'),\n        ('file://localhost/foo/bar', 'file:///foo/bar'),\n        ('file:////localhost/foo/bar', 'file:///localhost/foo/bar'),\n    ],\n)\ndef test_file_url_success(value, expected):\n    class Model(BaseModel):\n        v: FileUrl\n\n    assert str(Model(v=value).v) == expected\n\n\n@pytest.mark.parametrize(\n    'url,expected_port, expected_str',\n    [\n        ('https://www.example.com/', 443, 'https://www.example.com/'),\n        ('https://www.example.com:443/', 443, 'https://www.example.com/'),\n        ('https://www.example.com:8089/', 8089, 'https://www.example.com:8089/'),\n        ('http://www.example.com/', 80, 'http://www.example.com/'),\n        ('http://www.example.com:80/', 80, 'http://www.example.com/'),\n        ('http://www.example.com:8080/', 8080, 'http://www.example.com:8080/'),\n    ],\n)\ndef test_http_urls_default_port(url, expected_port, expected_str):\n    class Model(BaseModel):\n        v: HttpUrl\n\n    m = Model(v=url)\n    assert m.v.port == expected_port\n    assert str(m.v) == expected_str\n\n\n@pytest.mark.parametrize(\n    'value,expected',\n    [\n        ('ws://example.com', 'ws://example.com/'),\n        ('wss://example.com', 'wss://example.com/'),\n        ('wss://ws.example.com/', 'wss://ws.example.com/'),\n        ('ws://ws.example.com/', 'ws://ws.example.com/'),\n        ('ws://example.com:8080', 'ws://example.com:8080/'),\n        ('ws://example.com/path', 'ws://example.com/path'),\n        ('wss://example.com:4433', 'wss://example.com:4433/'),\n        ('wss://example.com/path', 'wss://example.com/path'),\n    ],\n)\ndef test_websocket_url_success(value, expected):\n    class Schema(BaseModel):\n        ws: WebsocketUrl\n\n    assert Schema(ws=value).ws.unicode_string() == expected\n\n\n@pytest.mark.parametrize(\n    'value,expected',\n    [\n        ('ws://example.com', 80),\n        ('wss://example.com', 443),\n        ('wss://ws.example.com/', 443),\n        ('ws://ws.example.com/', 80),\n        ('ws://example.com:8080', 8080),\n        ('ws://example.com:9999/path', 9999),\n        ('wss://example.com:4433', 4433),\n        ('wss://example.com/path', 443),\n    ],\n)\ndef test_websocket_url_port_success(value, expected):\n    class Schema(BaseModel):\n        ws: WebsocketUrl\n\n    assert Schema(ws=value).ws.port == expected\n\n\n@pytest.mark.parametrize(\n    'value,expected',\n    [\n        ('ws://example.com', '/'),\n        ('wss://example.com', '/'),\n        ('wss://ws.example.com/', '/'),\n        ('ws://ws.example.com/', '/'),\n        ('ws://example.com:8080', '/'),\n        ('ws://example.com:9999/path', '/path'),\n        ('wss://example.com:4433', '/'),\n        ('wss://example.com/path/to/ws', '/path/to/ws'),\n    ],\n)\ndef test_websocket_url_path_success(value, expected):\n    class Schema(BaseModel):\n        ws: WebsocketUrl\n\n    assert Schema(ws=value).ws.path == expected\n\n\n@pytest.mark.parametrize(\n    'value,expected',\n    [\n        ('ftp://example.com', 'ftp://example.com/'),\n        ('ftp://example.com/path/to/ftp', 'ftp://example.com/path/to/ftp'),\n        ('ftp://example.com:21', 'ftp://example.com/'),\n        ('ftp://example.com:21/path/to/ftp', 'ftp://example.com/path/to/ftp'),\n        ('ftp://example.com', 'ftp://example.com/'),\n        ('ftp://example.com/path/to/ftp', 'ftp://example.com/path/to/ftp'),\n        ('ftp://example.com:990', 'ftp://example.com:990/'),\n        ('ftp://example.com:990/path/to/ftp', 'ftp://example.com:990/path/to/ftp'),\n    ],\n)\ndef test_ftp_url_success(value, expected):\n    class Schema(BaseModel):\n        ftp: FtpUrl\n\n    assert Schema(ftp=value).ftp.unicode_string() == expected\n\n\n@pytest.mark.parametrize(\n    'value,expected',\n    [\n        ('ftp://example.com', 21),\n        ('ftp://example.com/path/to/ftp', 21),\n        ('ftp://example.com:21', 21),\n        ('ftp://exaMpl\u0424.com:221/path/to/ftp', 221),\n        ('ftp://example.com:144', 144),\n        ('ftp://example.com:990/path/to/ftp', 990),\n    ],\n)\ndef test_ftp_url_port_success(value, expected):\n    class Schema(BaseModel):\n        ftp: FtpUrl\n\n    assert Schema(ftp=value).ftp.port == expected\n\n\n@pytest.mark.parametrize(\n    'dsn',\n    [\n        'postgres://user:pass@localhost:5432/app',\n        'postgresql://user:pass@localhost:5432/app',\n        'postgresql+asyncpg://user:pass@localhost:5432/app',\n        'postgres://user:pass@host1.db.net,host2.db.net:6432/app',\n        'postgres://user:pass@%2Fvar%2Flib%2Fpostgresql/dbname',\n    ],\n)\ndef test_postgres_dsns(dsn):\n    class Model(BaseModel):\n        a: PostgresDsn\n\n    assert str(Model(a=dsn).a) == dsn\n\n\n@pytest.mark.parametrize(\n    'dsn',\n    [\n        'mysql://user:pass@localhost:3306/app',\n        'mysql+mysqlconnector://user:pass@localhost:3306/app',\n        'mysql+aiomysql://user:pass@localhost:3306/app',\n        'mysql+asyncmy://user:pass@localhost:3306/app',\n        'mysql+mysqldb://user:pass@localhost:3306/app',\n        'mysql+pymysql://user:pass@localhost:3306/app?charset=utf8mb4',\n        'mysql+cymysql://user:pass@localhost:3306/app',\n        'mysql+pyodbc://user:pass@localhost:3306/app',\n    ],\n)\ndef test_mysql_dsns(dsn):\n    class Model(BaseModel):\n        a: MySQLDsn\n\n    assert str(Model(a=dsn).a) == dsn\n\n\n@pytest.mark.parametrize(\n    'dsn',\n    [\n        'mariadb://user:pass@localhost:3306/app',\n        'mariadb+mariadbconnector://user:pass@localhost:3306/app',\n        'mariadb+pymysql://user:pass@localhost:3306/app',\n    ],\n)\ndef test_mariadb_dsns(dsn):\n    class Model(BaseModel):\n        a: MariaDBDsn\n\n    assert str(Model(a=dsn).a) == dsn\n\n\n@pytest.mark.parametrize(\n    'dsn',\n    [\n        'clickhouse+native://user:pass@localhost:9000/app',\n        'clickhouse+asynch://user:pass@localhost:9000/app',\n    ],\n)\ndef test_clickhouse_dsns(dsn):\n    class Model(BaseModel):\n        a: ClickHouseDsn\n\n    assert str(Model(a=dsn).a) == dsn\n\n\n@pytest.mark.parametrize(\n    'dsn,error_message',\n    (\n        (\n            'postgres://user:pass@host1.db.net:4321,/foo/bar:5432/app',\n            {\n                'type': 'url_parsing',\n                'loc': ('a',),\n                'msg': 'Input should be a valid URL, empty host',\n                'input': 'postgres://user:pass@host1.db.net:4321,/foo/bar:5432/app',\n            },\n        ),\n        (\n            'postgres://user:pass@host1.db.net,/app',\n            {\n                'type': 'url_parsing',\n                'loc': ('a',),\n                'msg': 'Input should be a valid URL, empty host',\n                'input': 'postgres://user:pass@host1.db.net,/app',\n            },\n        ),\n        (\n            'postgres://user:pass@/foo/bar:5432,host1.db.net:4321/app',\n            {\n                'type': 'url_parsing',\n                'loc': ('a',),\n                'msg': 'Input should be a valid URL, empty host',\n                'input': 'postgres://user:pass@/foo/bar:5432,host1.db.net:4321/app',\n            },\n        ),\n        (\n            'postgres://user@/foo/bar:5432/app',\n            {\n                'type': 'url_parsing',\n                'loc': ('a',),\n                'msg': 'Input should be a valid URL, empty host',\n                'input': 'postgres://user@/foo/bar:5432/app',\n            },\n        ),\n        (\n            'http://example.org',\n            {\n                'type': 'url_scheme',\n                'loc': ('a',),\n                'msg': (\n                    \"URL scheme should be 'postgres', 'postgresql', 'postgresql+asyncpg', 'postgresql+pg8000', \"\n                    \"'postgresql+psycopg', 'postgresql+psycopg2', 'postgresql+psycopg2cffi', \"\n                    \"'postgresql+py-postgresql' or 'postgresql+pygresql'\"\n                ),\n                'input': 'http://example.org',\n            },\n        ),\n    ),\n)\ndef test_postgres_dsns_validation_error(dsn, error_message):\n    class Model(BaseModel):\n        a: PostgresDsn\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=dsn)\n    error = exc_info.value.errors(include_url=False)[0]\n    error.pop('ctx', None)\n    assert error == error_message\n\n\ndef test_multihost_postgres_dsns():\n    class Model(BaseModel):\n        a: PostgresDsn\n\n    any_multihost_url = Model(a='postgres://user:pass@host1.db.net:4321,host2.db.net:6432/app').a\n    assert str(any_multihost_url) == 'postgres://user:pass@host1.db.net:4321,host2.db.net:6432/app'\n    assert any_multihost_url.scheme == 'postgres'\n    assert any_multihost_url.path == '/app'\n    # insert_assert(any_multihost_url.hosts())\n    assert any_multihost_url.hosts() == [\n        {'username': 'user', 'password': 'pass', 'host': 'host1.db.net', 'port': 4321},\n        {'username': None, 'password': None, 'host': 'host2.db.net', 'port': 6432},\n    ]\n\n    any_multihost_url = Model(a='postgres://user:pass@host.db.net:4321/app').a\n    assert any_multihost_url.scheme == 'postgres'\n    assert str(any_multihost_url) == 'postgres://user:pass@host.db.net:4321/app'\n    assert any_multihost_url.path == '/app'\n    # insert_assert(any_multihost_url.hosts())\n    assert any_multihost_url.hosts() == [{'username': 'user', 'password': 'pass', 'host': 'host.db.net', 'port': 4321}]\n\n\ndef test_cockroach_dsns():\n    class Model(BaseModel):\n        a: CockroachDsn\n\n    assert str(Model(a='cockroachdb://user:pass@localhost:5432/app').a) == 'cockroachdb://user:pass@localhost:5432/app'\n    assert (\n        str(Model(a='cockroachdb+psycopg2://user:pass@localhost:5432/app').a)\n        == 'cockroachdb+psycopg2://user:pass@localhost:5432/app'\n    )\n    assert (\n        str(Model(a='cockroachdb+asyncpg://user:pass@localhost:5432/app').a)\n        == 'cockroachdb+asyncpg://user:pass@localhost:5432/app'\n    )\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='http://example.org')\n    assert exc_info.value.errors(include_url=False)[0]['type'] == 'url_scheme'\n\n\ndef test_amqp_dsns():\n    class Model(BaseModel):\n        a: AmqpDsn\n\n    m = Model(a='amqp://user:pass@localhost:1234/app')\n    assert str(m.a) == 'amqp://user:pass@localhost:1234/app'\n    assert m.a.username == 'user'\n    assert m.a.password == 'pass'\n\n    m = Model(a='amqps://user:pass@localhost:5432//')\n    assert str(m.a) == 'amqps://user:pass@localhost:5432//'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='http://example.org')\n    assert exc_info.value.errors(include_url=False)[0]['type'] == 'url_scheme'\n\n    # Password is not required for AMQP protocol\n    m = Model(a='amqp://localhost:1234/app')\n    assert str(m.a) == 'amqp://localhost:1234/app'\n    assert m.a.username is None\n    assert m.a.password is None\n\n    # Only schema is required for AMQP protocol.\n    # https://www.rabbitmq.com/uri-spec.html\n    m = Model(a='amqps://')\n    assert m.a.scheme == 'amqps'\n    assert m.a.host is None\n    assert m.a.port is None\n    assert m.a.path is None\n\n\ndef test_redis_dsns():\n    class Model(BaseModel):\n        a: RedisDsn\n\n    m = Model(a='redis://user:pass@localhost:1234/app')\n    assert str(m.a) == 'redis://user:pass@localhost:1234/app'\n    assert m.a.username == 'user'\n    assert m.a.password == 'pass'\n\n    m = Model(a='rediss://user:pass@localhost:1234/app')\n    assert str(m.a) == 'rediss://user:pass@localhost:1234/app'\n\n    m = Model(a='rediss://:pass@localhost:1234')\n    assert str(m.a) == 'rediss://:pass@localhost:1234/0'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='http://example.org')\n    assert exc_info.value.errors(include_url=False)[0]['type'] == 'url_scheme'\n\n    # Password is not required for Redis protocol\n    m = Model(a='redis://localhost:1234/app')\n    assert str(m.a) == 'redis://localhost:1234/app'\n    assert m.a.username is None\n    assert m.a.password is None\n\n    # Only schema is required for Redis protocol. Otherwise it will be set to default\n    # https://www.iana.org/assignments/uri-schemes/prov/redis\n    m = Model(a='rediss://')\n    assert m.a.scheme == 'rediss'\n    assert m.a.host == 'localhost'\n    assert m.a.port == 6379\n    assert m.a.path == '/0'\n\n\ndef test_mongodb_dsns():\n    class Model(BaseModel):\n        a: MongoDsn\n\n    # TODO: Need to unit tests about \"Replica Set\", \"Sharded cluster\" and other deployment modes of MongoDB\n    m = Model(a='mongodb://user:pass@localhost:1234/app')\n    assert str(m.a) == 'mongodb://user:pass@localhost:1234/app'\n    # insert_assert(m.a.hosts())\n    assert m.a.hosts() == [{'username': 'user', 'password': 'pass', 'host': 'localhost', 'port': 1234}]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='http://example.org')\n    assert exc_info.value.errors(include_url=False)[0]['type'] == 'url_scheme'\n\n    # Password is not required for MongoDB protocol\n    m = Model(a='mongodb://localhost:1234/app')\n    assert str(m.a) == 'mongodb://localhost:1234/app'\n    # insert_assert(m.a.hosts())\n    assert m.a.hosts() == [{'username': None, 'password': None, 'host': 'localhost', 'port': 1234}]\n\n    # Only schema and host is required for MongoDB protocol\n    m = Model(a='mongodb://localhost')\n    assert m.a.scheme == 'mongodb'\n    # insert_assert(m.a.hosts())\n    assert m.a.hosts() == [{'username': None, 'password': None, 'host': 'localhost', 'port': 27017}]\n\n\n@pytest.mark.parametrize(\n    ('dsn', 'expected'),\n    [\n        ('mongodb://user:pass@localhost/app', 'mongodb://user:pass@localhost:27017/app'),\n        pytest.param(\n            'mongodb+srv://user:pass@localhost/app',\n            'mongodb+srv://user:pass@localhost/app',\n            marks=pytest.mark.xfail(\n                reason=(\n                    'This case is not supported. '\n                    'Check https://github.com/pydantic/pydantic/pull/7116 for more details.'\n                )\n            ),\n        ),\n    ],\n)\ndef test_mongodsn_default_ports(dsn: str, expected: str):\n    class Model(BaseModel):\n        dsn: MongoDsn\n\n    m = Model(dsn=dsn)\n    assert str(m.dsn) == expected\n\n\ndef test_kafka_dsns():\n    class Model(BaseModel):\n        a: KafkaDsn\n\n    m = Model(a='kafka://')\n    assert m.a.scheme == 'kafka'\n    assert m.a.host == 'localhost'\n    assert m.a.port == 9092\n    assert str(m.a) == 'kafka://localhost:9092'\n\n    m = Model(a='kafka://kafka1')\n    assert str(m.a) == 'kafka://kafka1:9092'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='http://example.org')\n    assert exc_info.value.errors(include_url=False)[0]['type'] == 'url_scheme'\n\n    m = Model(a='kafka://kafka3:9093')\n    assert m.a.username is None\n    assert m.a.password is None\n\n\n@pytest.mark.parametrize(\n    'dsn,result',\n    [\n        ('nats://user:pass@localhost:4222', 'nats://user:pass@localhost:4222'),\n        ('tls://user@localhost', 'tls://user@localhost:4222'),\n        ('ws://localhost:2355', 'ws://localhost:2355/'),\n        ('tls://', 'tls://localhost:4222'),\n        ('ws://:password@localhost:9999', 'ws://:password@localhost:9999/'),\n    ],\n)\ndef test_nats_dsns(dsn, result):\n    class Model(BaseModel):\n        dsn: NatsDsn\n\n    assert str(Model(dsn=dsn).dsn) == result\n\n\ndef test_custom_schemes():\n    class Model(BaseModel):\n        v: Annotated[Url, UrlConstraints(allowed_schemes=['ws', 'wss']), Strict()]\n\n    class Model2(BaseModel):\n        v: Annotated[Url, UrlConstraints(host_required=False, allowed_schemes=['foo'])]\n\n    assert str(Model(v='ws://example.org').v) == 'ws://example.org/'\n    assert str(Model2(v='foo:///foo/bar').v) == 'foo:///foo/bar'\n\n    with pytest.raises(ValidationError, match=r\"URL scheme should be 'ws' or 'wss' \\[type=url_scheme,\"):\n        Model(v='http://example.org')\n\n    with pytest.raises(ValidationError, match='leading or trailing control or space character are ignored in URLs'):\n        Model(v='ws://example.org  ')\n\n    with pytest.raises(ValidationError, match=r'syntax rules, expected // \\[type=url_syntax_violation,'):\n        Model(v='ws:///foo/bar')\n\n\n@pytest.mark.parametrize(\n    'options',\n    [\n        # Ensures the hash is generated correctly when a field is null\n        {'max_length': None},\n        {'allowed_schemes': None},\n        {'host_required': None},\n        {'default_host': None},\n        {'default_port': None},\n        {'default_path': None},\n    ],\n)\ndef test_url_constraints_hash_equal(options):\n    defaults = {\n        'max_length': 1,\n        'allowed_schemes': ['scheme'],\n        'host_required': False,\n        'default_host': 'host',\n        'default_port': 0,\n        'default_path': 'path',\n    }\n    options = {**defaults, **options}\n    assert hash(UrlConstraints(**options)) == hash(UrlConstraints(**options))\n\n\n@pytest.mark.parametrize(\n    'changes',\n    [\n        {'max_length': 2},\n        {'allowed_schemes': ['new-scheme']},\n        {'host_required': True},\n        {'default_host': 'new-host'},\n        {'default_port': 1},\n        {'default_path': 'new-path'},\n        {'max_length': None},\n        {'allowed_schemes': None},\n        {'host_required': None},\n        {'default_host': None},\n        {'default_port': None},\n        {'default_path': None},\n    ],\n)\ndef test_url_constraints_hash_inequal(changes):\n    options = {\n        'max_length': 1,\n        'allowed_schemes': ['scheme'],\n        'host_required': False,\n        'default_host': 'host',\n        'default_port': 0,\n        'default_path': 'path',\n    }\n    assert hash(UrlConstraints(**options)) != hash(UrlConstraints(**{**options, **changes}))\n\n\ndef test_json():\n    class Model(BaseModel):\n        v: HttpUrl\n\n    m = Model(v='http://foo@example.net')\n    assert m.model_dump_json() == '{\"v\":\"http://foo@example.net/\"}'\n\n\n@pytest.mark.skipif(not email_validator, reason='email_validator not installed')\n@pytest.mark.parametrize(\n    'value,name,email',\n    [\n        ('foobar@example.com', 'foobar', 'foobar@example.com'),\n        ('s@muelcolvin.com', 's', 's@muelcolvin.com'),\n        ('Samuel Colvin <s@muelcolvin.com>', 'Samuel Colvin', 's@muelcolvin.com'),\n        ('foobar <foobar@example.com>', 'foobar', 'foobar@example.com'),\n        (' foo.bar@example.com', 'foo.bar', 'foo.bar@example.com'),\n        ('foo.bar@example.com ', 'foo.bar', 'foo.bar@example.com'),\n        ('foo BAR <foobar@example.com >', 'foo BAR', 'foobar@example.com'),\n        ('FOO bar   <foobar@example.com> ', 'FOO bar', 'foobar@example.com'),\n        (' Whatever <foobar@example.com>', 'Whatever', 'foobar@example.com'),\n        ('Whatever < foobar@example.com>', 'Whatever', 'foobar@example.com'),\n        ('<FOOBAR@example.com> ', 'FOOBAR', 'FOOBAR@example.com'),\n        ('\u00f1o\u00f1\u00f3@example.com', '\u00f1o\u00f1\u00f3', '\u00f1o\u00f1\u00f3@example.com'),\n        ('\u6211\u8cb7@example.com', '\u6211\u8cb7', '\u6211\u8cb7@example.com'),\n        ('\u7532\u6590\u9ed2\u5ddd\u65e5\u672c@example.com', '\u7532\u6590\u9ed2\u5ddd\u65e5\u672c', '\u7532\u6590\u9ed2\u5ddd\u65e5\u672c@example.com'),\n        (\n            '\u0447\u0435\u0431\u0443\u0440\u0430\u0448\u043a\u0430\u044f\u0449\u0438\u043a-\u0441-\u0430\u043f\u0435\u043b\u044c\u0441\u0438\u043d\u0430\u043c\u0438.\u0440\u0444@example.com',\n            '\u0447\u0435\u0431\u0443\u0440\u0430\u0448\u043a\u0430\u044f\u0449\u0438\u043a-\u0441-\u0430\u043f\u0435\u043b\u044c\u0441\u0438\u043d\u0430\u043c\u0438.\u0440\u0444',\n            '\u0447\u0435\u0431\u0443\u0440\u0430\u0448\u043a\u0430\u044f\u0449\u0438\u043a-\u0441-\u0430\u043f\u0435\u043b\u044c\u0441\u0438\u043d\u0430\u043c\u0438.\u0440\u0444@example.com',\n        ),\n        ('\u0909\u0926\u093e\u0939\u0930\u0923.\u092a\u0930\u0940\u0915\u094d\u0937@domain.with.idn.tld', '\u0909\u0926\u093e\u0939\u0930\u0923.\u092a\u0930\u0940\u0915\u094d\u0937', '\u0909\u0926\u093e\u0939\u0930\u0923.\u092a\u0930\u0940\u0915\u094d\u0937@domain.with.idn.tld'),\n        ('foo.bar@example.com', 'foo.bar', 'foo.bar@example.com'),\n        ('foo.bar@exam-ple.com ', 'foo.bar', 'foo.bar@exam-ple.com'),\n        ('\u03b9\u03c9\u03ac\u03bd\u03bd\u03b7\u03c2@\u03b5\u03b5\u03c4\u03c4.gr', '\u03b9\u03c9\u03ac\u03bd\u03bd\u03b7\u03c2', '\u03b9\u03c9\u03ac\u03bd\u03bd\u03b7\u03c2@\u03b5\u03b5\u03c4\u03c4.gr'),\n        ('foobar@\u0430\u0440\u0440\u04cf\u0435.com', 'foobar', 'foobar@\u0430\u0440\u0440\u04cf\u0435.com'),\n        ('foobar@xn--80ak6aa92e.com', 'foobar', 'foobar@\u0430\u0440\u0440\u04cf\u0435.com'),\n        ('\u0430\u0440\u0440\u04cf\u0435@example.com', '\u0430\u0440\u0440\u04cf\u0435', '\u0430\u0440\u0440\u04cf\u0435@example.com'),\n        ('xn--80ak6aa92e@example.com', 'xn--80ak6aa92e', 'xn--80ak6aa92e@example.com'),\n        ('\uf96e\u58eb\u8c6a@\u81fa\u7db2\u4e2d\u5fc3.tw', '\u8449\u58eb\u8c6a', '\u8449\u58eb\u8c6a@\u81fa\u7db2\u4e2d\u5fc3.tw'),\n        ('\"first.last\" <first.last@example.com>', 'first.last', 'first.last@example.com'),\n        (\"Shaquille O'Neal <shaq@example.com>\", \"Shaquille O'Neal\", 'shaq@example.com'),\n    ],\n)\ndef test_address_valid(value, name, email):\n    assert validate_email(value) == (name, email)\n\n\n@pytest.mark.skipif(not email_validator, reason='email_validator not installed')\n@pytest.mark.parametrize(\n    'value,reason',\n    [\n        ('@example.com', 'There must be something before the @-sign.'),\n        ('f oo.bar@example.com', 'The email address contains invalid characters before the @-sign'),\n        ('foobar', 'An email address must have an @-sign.'),\n        ('foobar@localhost', 'The part after the @-sign is not valid. It should have a period.'),\n        ('foobar@127.0.0.1', 'The part after the @-sign is not valid. It is not within a valid top-level domain.'),\n        ('foo.bar@exam\\nple.com ', None),\n        ('foobar <foobar@example.com', None),\n        ('foobar@.example.com', None),\n        ('foobar@.com', None),\n        ('foo bar@example.com', None),\n        ('foo@bar@example.com', None),\n        ('\\n@example.com', None),\n        ('\\r@example.com', None),\n        ('\\f@example.com', None),\n        (' @example.com', None),\n        ('\\u0020@example.com', None),\n        ('\\u001f@example.com', None),\n        ('\"@example.com', None),\n        (',@example.com', None),\n        ('foobar <foobar<@example.com>', None),\n        ('foobar <foobar@example.com>>', None),\n        ('foobar <<foobar<@example.com>', None),\n        ('foobar <>', None),\n        ('first.last <first.last@example.com>', None),\n        pytest.param('foobar <' + 'a' * 4096 + '@example.com>', 'Length must not exceed 2048 characters', id='long'),\n    ],\n)\ndef test_address_invalid(value: str, reason: Union[str, None]):\n    with pytest.raises(PydanticCustomError, match=f'value is not a valid email address: {reason or \"\"}'):\n        validate_email(value)\n\n\n@pytest.mark.skipif(email_validator, reason='email_validator is installed')\ndef test_email_validator_not_installed():\n    with pytest.raises(ImportError):\n        validate_email('s@muelcolvin.com')\n\n\n@pytest.mark.skipif(not email_validator, reason='email_validator not installed')\ndef test_name_email():\n    class Model(BaseModel):\n        v: NameEmail\n\n    assert str(Model(v=NameEmail('foo bar', 'foobaR@example.com')).v) == 'foo bar <foobaR@example.com>'\n    assert str(Model(v='foo bar  <foobaR@example.com>').v) == 'foo bar <foobaR@example.com>'\n    assert str(Model(v='foobaR@example.com').v) == 'foobaR <foobaR@example.com>'\n    assert NameEmail('foo bar', 'foobaR@example.com') == NameEmail('foo bar', 'foobaR@example.com')\n    assert NameEmail('foo bar', 'foobaR@example.com') != NameEmail('foo bar', 'different@example.com')\n\n    assert Model.model_validate_json('{\"v\":\"foo bar <foobaR@example.com>\"}').v == NameEmail(\n        'foo bar', 'foobaR@example.com'\n    )\n    assert str(Model.model_validate_json('{\"v\":\"foobaR@example.com\"}').v) == 'foobaR <foobaR@example.com>'\n    assert (\n        Model(v=NameEmail('foo bar', 'foobaR@example.com')).model_dump_json() == '{\"v\":\"foo bar <foobaR@example.com>\"}'\n    )\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=1)\n    assert exc_info.value.errors() == [\n        {'input': 1, 'loc': ('v',), 'msg': 'Input is not a valid NameEmail', 'type': 'name_email_type'}\n    ]\n\n\n@pytest.mark.skipif(not email_validator, reason='email_validator not installed')\ndef test_name_email_serialization():\n    class Model(BaseModel):\n        email: NameEmail\n\n    m = Model.model_validate({'email': '\"name@mailbox.com\" <name@mailbox.com>'})\n    assert m.email.name == 'name@mailbox.com'\n    assert str(m.email) == '\"name@mailbox.com\" <name@mailbox.com>'\n\n    obj = json.loads(m.model_dump_json())\n    Model(email=obj['email'])\n", "tests/test_fields.py": "from typing import Union\n\nimport pytest\n\nimport pydantic.dataclasses\nfrom pydantic import BaseModel, ConfigDict, Field, PydanticUserError, RootModel, ValidationError, computed_field, fields\n\n\ndef test_field_info_annotation_keyword_argument():\n    \"\"\"This tests that `FieldInfo.from_field` raises an error if passed the `annotation` kwarg.\n\n    At the time of writing this test there is no way `FieldInfo.from_field` could receive the `annotation` kwarg from\n    anywhere inside Pydantic code. However, it is possible that this API is still being in use by applications and\n    third-party tools.\n    \"\"\"\n    with pytest.raises(TypeError) as e:\n        fields.FieldInfo.from_field(annotation=())\n\n    assert e.value.args == ('\"annotation\" is not permitted as a Field keyword argument',)\n\n\ndef test_field_info_annotated_attribute_name_clashing():\n    \"\"\"This tests that `FieldInfo.from_annotated_attribute` will raise a `PydanticUserError` if attribute names clashes\n    with a type.\n    \"\"\"\n\n    with pytest.raises(PydanticUserError):\n\n        class SubModel(BaseModel):\n            a: int = 1\n\n        class Model(BaseModel):\n            SubModel: SubModel = Field()\n\n\ndef test_init_var_field():\n    @pydantic.dataclasses.dataclass\n    class Foo:\n        bar: str\n        baz: str = Field(init_var=True)\n\n    class Model(BaseModel):\n        foo: Foo\n\n    model = Model(foo=Foo('bar', baz='baz'))\n    assert 'bar' in model.foo.__pydantic_fields__\n    assert 'baz' not in model.foo.__pydantic_fields__\n\n\ndef test_root_model_arbitrary_field_name_error():\n    with pytest.raises(\n        NameError, match=\"Unexpected field with name 'a_field'; only 'root' is allowed as a field of a `RootModel`\"\n    ):\n\n        class Model(RootModel[int]):\n            a_field: str\n\n\ndef test_root_model_arbitrary_private_field_works():\n    class Model(RootModel[int]):\n        _a_field: str = 'value 1'\n\n    m = Model(1)\n    assert m._a_field == 'value 1'\n\n    m._a_field = 'value 2'\n    assert m._a_field == 'value 2'\n\n\ndef test_root_model_field_override():\n    # Weird as this is, I think it's probably best to allow it to ensure it is possible to override\n    # the annotation in subclasses of RootModel subclasses. Basically, I think retaining the flexibility\n    # is worth the increased potential for weird/confusing \"accidental\" overrides.\n\n    # I'm mostly including this test now to document the behavior\n    class Model(RootModel[int]):\n        root: str\n\n    assert Model.model_validate('abc').root == 'abc'\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate(1)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 1, 'loc': (), 'msg': 'Input should be a valid string', 'type': 'string_type'}\n    ]\n\n    class SubModel(Model):\n        root: float\n\n    with pytest.raises(ValidationError) as exc_info:\n        SubModel.model_validate('abc')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'abc',\n            'loc': (),\n            'msg': 'Input should be a valid number, unable to parse string as a number',\n            'type': 'float_parsing',\n        }\n    ]\n\n    validated = SubModel.model_validate_json('1').root\n    assert validated == 1.0\n    assert isinstance(validated, float)\n\n\ndef test_frozen_field_repr():\n    class Model(BaseModel):\n        non_frozen_field: int = Field(frozen=False)\n        frozen_field: int = Field(frozen=True)\n\n    assert repr(Model.model_fields['non_frozen_field']) == 'FieldInfo(annotation=int, required=True)'\n    assert repr(Model.model_fields['frozen_field']) == 'FieldInfo(annotation=int, required=True, frozen=True)'\n\n\ndef test_model_field_default_info():\n    \"\"\"Test that __repr_args__ of FieldInfo includes the default value when it's set to None.\"\"\"\n\n    class Model(BaseModel):\n        a: Union[int, None] = Field(default=None)\n        b: Union[int, None] = None\n\n    assert str(Model.model_fields) == (\n        \"{'a': FieldInfo(annotation=Union[int, NoneType], required=False, default=None), \"\n        \"'b': FieldInfo(annotation=Union[int, NoneType], required=False, default=None)}\"\n    )\n\n\ndef test_computed_field_raises_correct_attribute_error():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n\n        @computed_field\n        def comp_field(self) -> str:\n            raise AttributeError('Computed field attribute error')\n\n        @property\n        def prop_field(self):\n            raise AttributeError('Property attribute error')\n\n    with pytest.raises(AttributeError, match='Computed field attribute error'):\n        Model().comp_field\n\n    with pytest.raises(AttributeError, match='Property attribute error'):\n        Model().prop_field\n\n    with pytest.raises(AttributeError, match=f\"'{Model.__name__}' object has no attribute 'invalid_field'\"):\n        Model().invalid_field\n\n\n@pytest.mark.parametrize('number', (1, 42, 443, 11.11, 0.553))\ndef test_coerce_numbers_to_str_field_option(number):\n    class Model(BaseModel):\n        field: str = Field(coerce_numbers_to_str=True, max_length=10)\n\n    assert Model(field=number).field == str(number)\n\n\n@pytest.mark.parametrize('number', (1, 42, 443, 11.11, 0.553))\ndef test_coerce_numbers_to_str_field_precedence(number):\n    class Model(BaseModel):\n        model_config = ConfigDict(coerce_numbers_to_str=True)\n\n        field: str = Field(coerce_numbers_to_str=False)\n\n    with pytest.raises(ValidationError):\n        Model(field=number)\n\n    class Model(BaseModel):\n        model_config = ConfigDict(coerce_numbers_to_str=False)\n\n        field: str = Field(coerce_numbers_to_str=True)\n\n    assert Model(field=number).field == str(number)\n", "tests/test_parse.py": "from typing import List, Tuple\n\nimport pytest\nfrom pydantic_core import CoreSchema\n\nfrom pydantic import BaseModel, GetJsonSchemaHandler, ValidationError, model_validator, parse_obj_as\nfrom pydantic.functional_serializers import model_serializer\nfrom pydantic.json_schema import JsonSchemaValue\n\n\nclass Model(BaseModel):\n    a: float\n    b: int = 10\n\n\ndef test_obj():\n    m = Model.model_validate(dict(a=10.2))\n    assert str(m) == 'a=10.2 b=10'\n\n\ndef test_model_validate_fails():\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate([1, 2, 3])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': (),\n            'msg': 'Input should be a valid dictionary or instance of Model',\n            'input': [1, 2, 3],\n            'ctx': {'class_name': 'Model'},\n        }\n    ]\n\n\ndef test_model_validate_submodel():\n    m = Model.model_validate(Model(a=10.2))\n    assert m.model_dump() == {'a': 10.2, 'b': 10}\n\n\ndef test_model_validate_wrong_model():\n    class Foo(BaseModel):\n        c: int = 123\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate(Foo())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': (),\n            'msg': 'Input should be a valid dictionary or instance of Model',\n            'input': Foo(),\n            'ctx': {'class_name': 'Model'},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate(Foo().model_dump())\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'c': 123}, 'loc': ('a',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\ndef test_root_model_error():\n    with pytest.raises(\n        TypeError, match=\"To define root models, use `pydantic.RootModel` rather than a field called '__root__'\"\n    ):\n\n        class MyModel(BaseModel):\n            __root__: str\n\n\ndef test_model_validate_root():\n    class MyModel(BaseModel):\n        root: str\n\n        # Note that the following three definitions require no changes across all __root__ models\n        # I couldn't see a nice way to create a decorator that reduces the boilerplate,\n        # but if we want to discourage this pattern, perhaps that's okay?\n        @model_validator(mode='before')\n        @classmethod\n        def populate_root(cls, values):\n            return {'root': values}\n\n        @model_serializer(mode='wrap')\n        def _serialize(self, handler, info):\n            data = handler(self)\n            if info.mode == 'json':\n                return data['root']\n            else:\n                return data\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            json_schema = handler(core_schema)\n            root = handler.resolve_ref_schema(json_schema)['properties']['root']\n            return root\n\n    # Validation\n    m = MyModel.model_validate('a')\n    assert m.root == 'a'\n\n    # Serialization\n    assert m.model_dump() == {'root': 'a'}\n    assert m.model_dump_json() == '\"a\"'\n\n    # JSON schema\n    assert m.model_json_schema() == {'title': 'Root', 'type': 'string'}\n\n\ndef test_parse_root_list():\n    class MyModel(BaseModel):\n        root: List[str]\n\n        @model_validator(mode='before')\n        @classmethod\n        def populate_root(cls, values):\n            return {'root': values}\n\n        @model_serializer(mode='wrap')\n        def _serialize(self, handler, info):\n            data = handler(self)\n            if info.mode == 'json':\n                return data['root']\n            else:\n                return data\n\n        @classmethod\n        def model_modify_json_schema(cls, json_schema):\n            return json_schema['properties']['root']\n\n    m = MyModel.model_validate(['a'])\n    assert m.model_dump() == {'root': ['a']}\n    assert m.model_dump_json() == '[\"a\"]'\n    assert m.root == ['a']\n\n\ndef test_parse_nested_root_list():\n    class NestedData(BaseModel):\n        id: str\n\n    class NestedModel(BaseModel):\n        root: List[NestedData]\n\n        @model_validator(mode='before')\n        @classmethod\n        def populate_root(cls, values):\n            return {'root': values}\n\n        @model_serializer(mode='wrap')\n        def _serialize(self, handler, info):\n            data = handler(self)\n            if info.mode == 'json':\n                return data['root']\n            else:\n                return data\n\n        @classmethod\n        def model_modify_json_schema(cls, json_schema):\n            return json_schema['properties']['root']\n\n    class MyModel(BaseModel):\n        nested: NestedModel\n\n    m = MyModel.model_validate({'nested': [{'id': 'foo'}]})\n    assert isinstance(m.nested, NestedModel)\n    assert isinstance(m.nested.root[0], NestedData)\n\n\n@pytest.mark.filterwarnings('ignore:`parse_obj_as` is deprecated.*:DeprecationWarning')\ndef test_parse_nested_root_tuple():\n    class NestedData(BaseModel):\n        id: str\n\n    class NestedModel(BaseModel):\n        root: Tuple[int, NestedData]\n\n        @model_validator(mode='before')\n        @classmethod\n        def populate_root(cls, values):\n            return {'root': values}\n\n        @model_serializer(mode='wrap')\n        def _serialize(self, handler, info):\n            data = handler(self)\n            if info.mode == 'json':\n                return data['root']\n            else:\n                return data\n\n        @classmethod\n        def model_modify_json_schema(cls, json_schema):\n            return json_schema['properties']['root']\n\n    class MyModel(BaseModel):\n        nested: List[NestedModel]\n\n    data = [0, {'id': 'foo'}]\n    m = MyModel.model_validate({'nested': [data]})\n    assert isinstance(m.nested[0], NestedModel)\n    assert isinstance(m.nested[0].root[1], NestedData)\n\n    nested = parse_obj_as(NestedModel, data)\n    assert isinstance(nested, NestedModel)\n\n\ndef test_parse_nested_custom_root():\n    class NestedModel(BaseModel):\n        root: List[str]\n\n        @model_validator(mode='before')\n        @classmethod\n        def populate_root(cls, values):\n            return {'root': values}\n\n        @model_serializer(mode='wrap')\n        def _serialize(self, handler, info):\n            data = handler(self)\n            if info.mode == 'json':\n                return data['root']\n            else:\n                return data\n\n        @classmethod\n        def model_modify_json_schema(cls, json_schema):\n            return json_schema['properties']['root']\n\n    class MyModel(BaseModel):\n        root: NestedModel\n\n        @model_validator(mode='before')\n        @classmethod\n        def populate_root(cls, values):\n            return {'root': values}\n\n        @model_serializer(mode='wrap')\n        def _serialize(self, handler, info):\n            data = handler(self)\n            if info.mode == 'json':\n                return data['root']\n            else:\n                return data\n\n        @classmethod\n        def model_modify_json_schema(cls, json_schema):\n            return json_schema['properties']['root']\n\n    nested = ['foo', 'bar']\n    m = MyModel.model_validate(nested)\n    assert isinstance(m, MyModel)\n    assert isinstance(m.root, NestedModel)\n    assert isinstance(m.root.root, List)\n    assert isinstance(m.root.root[0], str)\n\n\ndef test_json():\n    assert Model.model_validate_json('{\"a\": 12, \"b\": 8}') == Model(a=12, b=8)\n", "tests/test_titles.py": "import re\nimport typing\nfrom typing import Any, Callable, List\n\nimport pytest\nimport typing_extensions\n\nimport pydantic\nfrom pydantic import BaseModel, ConfigDict, Field, TypeAdapter, computed_field\nfrom pydantic.fields import FieldInfo\nfrom pydantic.json_schema import model_json_schema\n\nfrom .test_types_typeddict import fixture_typed_dict, fixture_typed_dict_all  # noqa\n\n\n@pytest.fixture(\n    name='Annotated',\n    params=[\n        pytest.param(typing, id='typing.Annotated'),\n        pytest.param(typing_extensions, id='t_e.Annotated'),\n    ],\n)\ndef fixture_annotated(request):\n    try:\n        return request.param.Annotated\n    except AttributeError:\n        pytest.skip(f'Annotated is not available from {request.param}')\n\n\ndef make_title(name: str, _):\n    def _capitalize(v: str):\n        return v[0].upper() + v[1:]\n\n    return re.sub(r'(?<=[a-z])([A-Z])', r' \\1', _capitalize(name))\n\n\nFIELD_TITLE_GENERATORS: List[Callable[[str, Any], str]] = [\n    lambda t, _: t.lower(),\n    lambda t, _: t * 2,\n    lambda t, _: 'My Title',\n    make_title,\n]\n\nMODEL_TITLE_GENERATORS: List[Callable[[Any], str]] = [\n    lambda m: m.__name__.upper(),\n    lambda m: m.__name__ * 2,\n    lambda m: 'My Model',\n]\n\n\n@pytest.mark.parametrize('model_title_generator', MODEL_TITLE_GENERATORS)\ndef test_model_model_title_generator(model_title_generator):\n    class Model(BaseModel):\n        model_config = ConfigDict(model_title_generator=model_title_generator)\n\n    assert Model.model_json_schema() == {\n        'properties': {},\n        'title': model_title_generator(Model),\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('model_title_generator', MODEL_TITLE_GENERATORS)\ndef test_model_title_generator_in_submodel(model_title_generator):\n    class SubModel(BaseModel):\n        model_config = ConfigDict(model_title_generator=model_title_generator)\n\n    class Model(BaseModel):\n        sub: SubModel\n\n    assert Model.model_json_schema() == {\n        '$defs': {'SubModel': {'properties': {}, 'title': model_title_generator(SubModel), 'type': 'object'}},\n        'properties': {'sub': {'$ref': '#/$defs/SubModel'}},\n        'required': ['sub'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('field_title_generator', FIELD_TITLE_GENERATORS)\ndef test_field_title_generator_in_model_fields(field_title_generator):\n    class Model(BaseModel):\n        field_a: str = Field(field_title_generator=field_title_generator)\n        field_b: int = Field(field_title_generator=field_title_generator)\n\n        @computed_field(field_title_generator=field_title_generator)\n        def field_c(self) -> str:\n            return self.field_a\n\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {\n            'field_a': {'title': field_title_generator('field_a', Model.model_fields['field_a']), 'type': 'string'},\n            'field_b': {'title': field_title_generator('field_b', Model.model_fields['field_b']), 'type': 'integer'},\n            'field_c': {\n                'readOnly': True,\n                'title': field_title_generator('field_c', Model.model_computed_fields['field_c']),\n                'type': 'string',\n            },\n        },\n        'required': ['field_a', 'field_b', 'field_c'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('field_title_generator', FIELD_TITLE_GENERATORS)\ndef test_model_config_field_title_generator(field_title_generator):\n    class Model(BaseModel):\n        model_config = ConfigDict(field_title_generator=field_title_generator)\n\n        field_a: str\n        field_b: int\n        field___c: bool\n\n        @computed_field\n        def field_d(self) -> str:\n            return self.field_a\n\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {\n            'field_a': {'title': field_title_generator('field_a', Model.model_fields['field_a']), 'type': 'string'},\n            'field_b': {'title': field_title_generator('field_b', Model.model_fields['field_b']), 'type': 'integer'},\n            'field___c': {\n                'title': field_title_generator('field___c', Model.model_fields['field___c']),\n                'type': 'boolean',\n            },\n            'field_d': {\n                'readOnly': True,\n                'title': field_title_generator('field_d', Model.model_computed_fields['field_d']),\n                'type': 'string',\n            },\n        },\n        'required': ['field_a', 'field_b', 'field___c', 'field_d'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('model_title_generator', MODEL_TITLE_GENERATORS)\ndef test_dataclass_model_title_generator(model_title_generator):\n    @pydantic.dataclasses.dataclass(config=ConfigDict(model_title_generator=model_title_generator))\n    class MyDataclass:\n        field_a: int\n\n    assert model_json_schema(MyDataclass) == {\n        'properties': {'field_a': {'title': 'Field A', 'type': 'integer'}},\n        'required': ['field_a'],\n        'title': model_title_generator(MyDataclass),\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('field_title_generator', FIELD_TITLE_GENERATORS)\ndef test_field_title_generator_in_dataclass_fields(field_title_generator):\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        field_a: str = Field(field_title_generator=field_title_generator)\n        field_b: int = Field(field_title_generator=field_title_generator)\n\n    assert model_json_schema(MyDataclass) == {\n        'properties': {\n            'field_a': {\n                'title': field_title_generator('field_a', MyDataclass.__pydantic_fields__['field_a']),\n                'type': 'string',\n            },\n            'field_b': {\n                'title': field_title_generator('field_b', MyDataclass.__pydantic_fields__['field_b']),\n                'type': 'integer',\n            },\n        },\n        'required': ['field_a', 'field_b'],\n        'title': 'MyDataclass',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('field_title_generator', FIELD_TITLE_GENERATORS)\ndef test_dataclass_config_field_title_generator(field_title_generator):\n    @pydantic.dataclasses.dataclass(config=ConfigDict(field_title_generator=field_title_generator))\n    class MyDataclass:\n        field_a: str\n        field_b: int\n        field___c: bool\n\n    assert model_json_schema(MyDataclass) == {\n        'properties': {\n            'field_a': {\n                'title': field_title_generator('field_a', MyDataclass.__pydantic_fields__['field_a']),\n                'type': 'string',\n            },\n            'field_b': {\n                'title': field_title_generator('field_b', MyDataclass.__pydantic_fields__['field_b']),\n                'type': 'integer',\n            },\n            'field___c': {\n                'title': field_title_generator('field___c', MyDataclass.__pydantic_fields__['field___c']),\n                'type': 'boolean',\n            },\n        },\n        'required': ['field_a', 'field_b', 'field___c'],\n        'title': 'MyDataclass',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('model_title_generator', MODEL_TITLE_GENERATORS)\ndef test_typeddict_model_title_generator(model_title_generator, TypedDict):\n    class MyTypedDict(TypedDict):\n        __pydantic_config__ = ConfigDict(model_title_generator=model_title_generator)\n        pass\n\n    assert TypeAdapter(MyTypedDict).json_schema() == {\n        'properties': {},\n        'title': model_title_generator(MyTypedDict),\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('field_title_generator', FIELD_TITLE_GENERATORS)\ndef test_field_title_generator_in_typeddict_fields(field_title_generator, TypedDict, Annotated):\n    class MyTypedDict(TypedDict):\n        field_a: Annotated[str, Field(field_title_generator=field_title_generator)]\n        field_b: Annotated[int, Field(field_title_generator=field_title_generator)]\n\n    assert TypeAdapter(MyTypedDict).json_schema() == {\n        'properties': {\n            'field_a': {\n                'title': field_title_generator(\n                    'field_a', FieldInfo.from_annotation(MyTypedDict.__annotations__['field_a'])\n                ),\n                'type': 'string',\n            },\n            'field_b': {\n                'title': field_title_generator(\n                    'field_b', FieldInfo.from_annotation(MyTypedDict.__annotations__['field_a'])\n                ),\n                'type': 'integer',\n            },\n        },\n        'required': ['field_a', 'field_b'],\n        'title': 'MyTypedDict',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('field_title_generator', FIELD_TITLE_GENERATORS)\ndef test_typeddict_config_field_title_generator(field_title_generator, TypedDict):\n    class MyTypedDict(TypedDict):\n        __pydantic_config__ = ConfigDict(field_title_generator=field_title_generator)\n        field_a: str\n        field_b: int\n        field___c: bool\n\n    assert TypeAdapter(MyTypedDict).json_schema() == {\n        'properties': {\n            'field_a': {\n                'title': field_title_generator(\n                    'field_a', FieldInfo.from_annotation(MyTypedDict.__annotations__['field_a'])\n                ),\n                'type': 'string',\n            },\n            'field_b': {\n                'title': field_title_generator(\n                    'field_b', FieldInfo.from_annotation(MyTypedDict.__annotations__['field_b'])\n                ),\n                'type': 'integer',\n            },\n            'field___c': {\n                'title': field_title_generator(\n                    'field___c', FieldInfo.from_annotation(MyTypedDict.__annotations__['field___c'])\n                ),\n                'type': 'boolean',\n            },\n        },\n        'required': ['field_a', 'field_b', 'field___c'],\n        'title': 'MyTypedDict',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    'field_level_title_generator,config_level_title_generator',\n    ((lambda f, _: f.lower(), lambda f, _: f.upper()), (lambda f, _: f, make_title)),\n)\ndef test_field_level_field_title_generator_precedence_over_config_level(\n    field_level_title_generator, config_level_title_generator, TypedDict, Annotated\n):\n    class MyModel(BaseModel):\n        model_config = ConfigDict(field_title_generator=field_level_title_generator)\n        field_a: str = Field(field_title_generator=field_level_title_generator)\n\n    assert MyModel.model_json_schema() == {\n        'properties': {\n            'field_a': {\n                'title': field_level_title_generator('field_a', MyModel.model_fields['field_a']),\n                'type': 'string',\n            }\n        },\n        'required': ['field_a'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n\n    @pydantic.dataclasses.dataclass(config=ConfigDict(field_title_generator=field_level_title_generator))\n    class MyDataclass:\n        field_a: str = Field(field_title_generator=field_level_title_generator)\n\n    assert model_json_schema(MyDataclass) == {\n        'properties': {\n            'field_a': {\n                'title': field_level_title_generator('field_a', MyDataclass.__pydantic_fields__['field_a']),\n                'type': 'string',\n            }\n        },\n        'required': ['field_a'],\n        'title': 'MyDataclass',\n        'type': 'object',\n    }\n\n    class MyTypedDict(TypedDict):\n        __pydantic_config__ = ConfigDict(field_title_generator=field_level_title_generator)\n        field_a: Annotated[str, Field(field_title_generator=field_level_title_generator)]\n\n    assert TypeAdapter(MyTypedDict).json_schema() == {\n        'properties': {\n            'field_a': {\n                'title': field_level_title_generator(\n                    'field_a', FieldInfo.from_annotation(MyTypedDict.__annotations__['field_a'])\n                ),\n                'type': 'string',\n            }\n        },\n        'required': ['field_a'],\n        'title': 'MyTypedDict',\n        'type': 'object',\n    }\n\n\ndef test_field_title_precedence_over_generators(TypedDict, Annotated):\n    class Model(BaseModel):\n        model_config = ConfigDict(field_title_generator=lambda f, _: f.upper())\n\n        field_a: str = Field(title='MyFieldA', field_title_generator=lambda f, _: f.upper())\n\n        @computed_field(title='MyFieldB', field_title_generator=lambda f, _: f.upper())\n        def field_b(self) -> str:\n            return self.field_a\n\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {\n            'field_a': {'title': 'MyFieldA', 'type': 'string'},\n            'field_b': {'readOnly': True, 'title': 'MyFieldB', 'type': 'string'},\n        },\n        'required': ['field_a', 'field_b'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n    @pydantic.dataclasses.dataclass(config=ConfigDict(field_title_generator=lambda f, _: f.upper()))\n    class MyDataclass:\n        field_a: str = Field(title='MyTitle', field_title_generator=lambda f, _: f.upper())\n\n    assert model_json_schema(MyDataclass) == {\n        'properties': {'field_a': {'title': 'MyTitle', 'type': 'string'}},\n        'required': ['field_a'],\n        'title': 'MyDataclass',\n        'type': 'object',\n    }\n\n    class MyTypedDict(TypedDict):\n        __pydantic_config__ = ConfigDict(field_title_generator=lambda f, _: f.upper())\n        field_a: Annotated[str, Field(title='MyTitle', field_title_generator=lambda f, _: f.upper())]\n\n    assert TypeAdapter(MyTypedDict).json_schema() == {\n        'properties': {'field_a': {'title': 'MyTitle', 'type': 'string'}},\n        'required': ['field_a'],\n        'title': 'MyTypedDict',\n        'type': 'object',\n    }\n\n\ndef test_class_title_precedence_over_generator():\n    class Model(BaseModel):\n        model_config = ConfigDict(title='MyTitle', model_title_generator=lambda m: m.__name__.upper())\n\n    assert Model.model_json_schema() == {\n        'properties': {},\n        'title': 'MyTitle',\n        'type': 'object',\n    }\n\n    @pydantic.dataclasses.dataclass(\n        config=ConfigDict(title='MyTitle', model_title_generator=lambda m: m.__name__.upper())\n    )\n    class MyDataclass:\n        pass\n\n    assert model_json_schema(MyDataclass) == {\n        'properties': {},\n        'title': 'MyTitle',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('invalid_return_value', (1, 2, 3, tuple(), list(), object()))\ndef test_model_title_generator_returns_invalid_type(invalid_return_value, TypedDict):\n    with pytest.raises(\n        TypeError, match=f'model_title_generator .* must return str, not {invalid_return_value.__class__}'\n    ):\n\n        class Model(BaseModel):\n            model_config = ConfigDict(model_title_generator=lambda m: invalid_return_value)\n\n    with pytest.raises(\n        TypeError, match=f'model_title_generator .* must return str, not {invalid_return_value.__class__}'\n    ):\n\n        @pydantic.dataclasses.dataclass(config=ConfigDict(model_title_generator=lambda m: invalid_return_value))\n        class MyDataclass:\n            pass\n\n    with pytest.raises(\n        TypeError, match=f'model_title_generator .* must return str, not {invalid_return_value.__class__}'\n    ):\n\n        class MyTypedDict(TypedDict):\n            __pydantic_config__ = ConfigDict(model_title_generator=lambda m: invalid_return_value)\n            pass\n\n        TypeAdapter(MyTypedDict)\n\n\n@pytest.mark.parametrize('invalid_return_value', (1, 2, 3, tuple(), list(), object()))\ndef test_config_field_title_generator_returns_invalid_type(invalid_return_value, TypedDict):\n    with pytest.raises(\n        TypeError, match=f'field_title_generator .* must return str, not {invalid_return_value.__class__}'\n    ):\n\n        class Model(BaseModel):\n            model_config = ConfigDict(field_title_generator=lambda f, _: invalid_return_value)\n\n            field_a: str\n\n    with pytest.raises(\n        TypeError, match=f'field_title_generator .* must return str, not {invalid_return_value.__class__}'\n    ):\n\n        @pydantic.dataclasses.dataclass(config=ConfigDict(field_title_generator=lambda f, _: invalid_return_value))\n        class MyDataclass:\n            field_a: str\n\n    with pytest.raises(\n        TypeError, match=f'field_title_generator .* must return str, not {invalid_return_value.__class__}'\n    ):\n\n        class MyTypedDict(TypedDict):\n            __pydantic_config__ = ConfigDict(field_title_generator=lambda f, _: invalid_return_value)\n            field_a: str\n\n        TypeAdapter(MyTypedDict)\n\n\n@pytest.mark.parametrize('invalid_return_value', (1, 2, 3, tuple(), list(), object()))\ndef test_field_title_generator_returns_invalid_type(invalid_return_value, TypedDict, Annotated):\n    with pytest.raises(\n        TypeError, match=f'field_title_generator .* must return str, not {invalid_return_value.__class__}'\n    ):\n\n        class Model(BaseModel):\n            field_a: Any = Field(field_title_generator=lambda f, _: invalid_return_value)\n\n        Model(field_a=invalid_return_value).model_json_schema()\n\n    with pytest.raises(\n        TypeError, match=f'field_title_generator .* must return str, not {invalid_return_value.__class__}'\n    ):\n\n        @pydantic.dataclasses.dataclass\n        class MyDataclass:\n            field_a: Any = Field(field_title_generator=lambda f, _: invalid_return_value)\n\n        model_json_schema(MyDataclass)\n\n    with pytest.raises(\n        TypeError, match=f'field_title_generator .* must return str, not {invalid_return_value.__class__}'\n    ):\n\n        class MyTypedDict(TypedDict):\n            field_a: Annotated[str, Field(field_title_generator=lambda f, _: invalid_return_value)]\n\n        TypeAdapter(MyTypedDict)\n", "tests/test_docs_extraction.py": "import textwrap\nfrom typing import Generic, TypeVar\n\nfrom typing_extensions import Annotated, TypedDict\n\nfrom pydantic import BaseModel, ConfigDict, Field, TypeAdapter, create_model\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\n\nT = TypeVar('T')\n\n\ndef dec_noop(obj):\n    return obj\n\n\ndef test_model_no_docs_extraction():\n    class MyModel(BaseModel):\n        a: int = 1\n        \"\"\"A docs\"\"\"\n\n        b: str = '1'\n\n        \"\"\"B docs\"\"\"\n\n    assert MyModel.model_fields['a'].description is None\n    assert MyModel.model_fields['b'].description is None\n\n\ndef test_model_docs_extraction():\n    # Using a couple dummy decorators to make sure the frame is pointing at\n    # the `class` line:\n    @dec_noop\n    @dec_noop\n    class MyModel(BaseModel):\n        a: int\n        \"\"\"A docs\"\"\"\n        b: int = 1\n\n        \"\"\"B docs\"\"\"\n        c: int = 1\n        # This isn't used as a description.\n\n        d: int\n\n        def dummy_method(self) -> None:\n            \"\"\"Docs for dummy that won't be used for d\"\"\"\n\n        e: Annotated[int, Field(description='Real description')]\n        \"\"\"Won't be used\"\"\"\n\n        f: int\n        \"\"\"F docs\"\"\"\n\n        \"\"\"Useless docs\"\"\"\n\n        g: int\n        \"\"\"G docs\"\"\"\n\n        model_config = ConfigDict(\n            use_attribute_docstrings=True,\n        )\n\n    assert MyModel.model_fields['a'].description == 'A docs'\n    assert MyModel.model_fields['b'].description == 'B docs'\n    assert MyModel.model_fields['c'].description is None\n    assert MyModel.model_fields['d'].description is None\n    assert MyModel.model_fields['e'].description == 'Real description'\n    assert MyModel.model_fields['g'].description == 'G docs'\n\n\ndef test_model_docs_duplicate_class():\n    \"\"\"Ensure source parsing is working correctly when using frames.\"\"\"\n\n    @dec_noop\n    class MyModel(BaseModel):\n        a: int\n        \"\"\"A docs\"\"\"\n\n        model_config = ConfigDict(\n            use_attribute_docstrings=True,\n        )\n\n    @dec_noop\n    class MyModel(BaseModel):\n        b: int\n        \"\"\"B docs\"\"\"\n\n        model_config = ConfigDict(\n            use_attribute_docstrings=True,\n        )\n\n    assert MyModel.model_fields['b'].description == 'B docs'\n\n    # With https://github.com/python/cpython/pull/106815/ introduced,\n    # inspect will fallback to the last found class in the source file.\n    # The following is to ensure using frames will still get the correct one\n    if True:\n\n        class MyModel(BaseModel):\n            a: int\n            \"\"\"A docs\"\"\"\n\n            model_config = ConfigDict(\n                use_attribute_docstrings=True,\n            )\n\n    else:\n\n        class MyModel(BaseModel):\n            b: int\n            \"\"\"B docs\"\"\"\n\n            model_config = ConfigDict(\n                use_attribute_docstrings=True,\n            )\n\n    assert MyModel.model_fields['a'].description == 'A docs'\n\n\ndef test_model_docs_dedented_string():\n    # fmt: off\n    class MyModel(BaseModel):\n        def bar(self):\n            \"\"\"\nAn inconveniently dedented string\n            \"\"\"\n\n        a: int\n        \"\"\"A docs\"\"\"\n\n        model_config = ConfigDict(\n            use_attribute_docstrings=True,\n        )\n    # fmt: on\n    assert MyModel.model_fields['a'].description == 'A docs'\n\n\ndef test_model_docs_inheritance():\n    class MyModel(BaseModel):\n        a: int\n        \"\"\"A docs\"\"\"\n\n        b: int\n        \"\"\"B docs\"\"\"\n\n        model_config = ConfigDict(\n            use_attribute_docstrings=True,\n        )\n\n    FirstModel = MyModel\n\n    class MyModel(FirstModel):\n        a: int\n        \"\"\"A overridden docs\"\"\"\n\n    assert FirstModel.model_fields['a'].description == 'A docs'\n    assert MyModel.model_fields['a'].description == 'A overridden docs'\n    assert MyModel.model_fields['b'].description == 'B docs'\n\n\ndef test_model_different_name():\n    # As we extract docstrings from cls in `ModelMetaclass.__new__`,\n    # we are not affected by `__name__` being altered in any way.\n\n    class MyModel(BaseModel):\n        a: int\n        \"\"\"A docs\"\"\"\n\n        model_config = ConfigDict(\n            use_attribute_docstrings=True,\n        )\n\n    MyModel.__name__ = 'OtherModel'\n    print(MyModel.__name__)\n\n    assert MyModel.model_fields['a'].description == 'A docs'\n\n\ndef test_model_generic():\n    class MyModel(BaseModel, Generic[T]):\n        a: T\n        \"\"\"A docs\"\"\"\n\n        model_config = ConfigDict(\n            use_attribute_docstrings=True,\n        )\n\n    assert MyModel.model_fields['a'].description == 'A docs'\n\n    class MyParameterizedModel(MyModel[int]):\n        a: int\n        \"\"\"A parameterized docs\"\"\"\n\n    assert MyParameterizedModel.model_fields['a'].description == 'A parameterized docs'\n    assert MyModel[int].model_fields['a'].description == 'A docs'\n\n\ndef test_dataclass_no_docs_extraction():\n    @pydantic_dataclass\n    class MyModel:\n        a: int = 1\n        \"\"\"A docs\"\"\"\n\n        b: str = '1'\n\n        \"\"\"B docs\"\"\"\n\n    assert MyModel.__pydantic_fields__['a'].description is None\n    assert MyModel.__pydantic_fields__['b'].description is None\n\n\ndef test_dataclass_docs_extraction():\n    @pydantic_dataclass(\n        config=ConfigDict(use_attribute_docstrings=True),\n    )\n    @dec_noop\n    class MyModel:\n        a: int\n        \"\"\"A docs\"\"\"\n        b: int = 1\n\n        \"\"\"B docs\"\"\"\n        c: int = 1\n        # This isn't used as a description.\n\n        d: int = 1\n\n        def dummy_method(self) -> None:\n            \"\"\"Docs for dummy_method that won't be used for d\"\"\"\n\n        e: int = Field(1, description='Real description')\n        \"\"\"Won't be used\"\"\"\n\n        f: int = 1\n        \"\"\"F docs\"\"\"\n\n        \"\"\"Useless docs\"\"\"\n\n        g: int = 1\n        \"\"\"G docs\"\"\"\n\n        h = 1\n        \"\"\"H docs\"\"\"\n\n        i: Annotated[int, Field(description='Real description')] = 1\n        \"\"\"Won't be used\"\"\"\n\n    assert MyModel.__pydantic_fields__['a'].description == 'A docs'\n    assert MyModel.__pydantic_fields__['b'].description == 'B docs'\n    assert MyModel.__pydantic_fields__['c'].description is None\n    assert MyModel.__pydantic_fields__['d'].description is None\n    assert MyModel.__pydantic_fields__['e'].description == 'Real description'\n    assert MyModel.__pydantic_fields__['g'].description == 'G docs'\n    assert MyModel.__pydantic_fields__['i'].description == 'Real description'\n\n\ndef test_typeddict():\n    class MyModel(TypedDict):\n        a: int\n        \"\"\"A docs\"\"\"\n\n    ta = TypeAdapter(MyModel)\n    assert ta.json_schema() == {\n        'properties': {'a': {'title': 'A', 'type': 'integer'}},\n        'required': ['a'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n\n    class MyModel(TypedDict):\n        a: int\n        \"\"\"A docs\"\"\"\n\n        __pydantic_config__ = ConfigDict(use_attribute_docstrings=True)\n\n    ta = TypeAdapter(MyModel)\n\n    assert ta.json_schema() == {\n        'properties': {'a': {'title': 'A', 'type': 'integer', 'description': 'A docs'}},\n        'required': ['a'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n\n\ndef test_typeddict_as_field():\n    class ModelTDAsField(TypedDict):\n        a: int\n        \"\"\"A docs\"\"\"\n\n        __pydantic_config__ = ConfigDict(use_attribute_docstrings=True)\n\n    class MyModel(BaseModel):\n        td: ModelTDAsField\n\n    a_property = MyModel.model_json_schema()['$defs']['ModelTDAsField']['properties']['a']\n    assert a_property['description'] == 'A docs'\n\n\ndef test_create_model_test():\n    # Duplicate class creation to ensure create_model\n    # doesn't fallback to using inspect, which could\n    # in turn use the wrong class:\n    class MyModel(BaseModel):\n        foo: str = '123'\n        \"\"\"Shouldn't be used\"\"\"\n\n        model_config = ConfigDict(\n            use_attribute_docstrings=True,\n        )\n\n    assert MyModel.model_fields['foo'].description == \"Shouldn't be used\"\n\n    MyModel = create_model(\n        'MyModel',\n        foo=(int, 123),\n        __config__=ConfigDict(use_attribute_docstrings=True),\n    )\n\n    assert MyModel.model_fields['foo'].description is None\n\n\ndef test_exec_cant_be_parsed():\n    source = textwrap.dedent(\n        '''\n        class MyModel(BaseModel):\n            a: int\n            \"\"\"A docs\"\"\"\n\n            model_config = ConfigDict(use_attribute_docstrings=True)\n        '''\n    )\n\n    locals_dict = {}\n\n    exec(source, globals(), locals_dict)\n    assert locals_dict['MyModel'].model_fields['a'].description is None\n", "tests/test_main.py": "import json\nimport platform\nimport re\nimport warnings\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom datetime import date, datetime\nfrom enum import Enum\nfrom functools import partial\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Final,\n    Generic,\n    List,\n    Mapping,\n    Optional,\n    Set,\n    Type,\n    TypeVar,\n    Union,\n    get_type_hints,\n)\nfrom uuid import UUID, uuid4\n\nimport pytest\nfrom pydantic_core import CoreSchema, core_schema\nfrom typing_extensions import Annotated, Literal\n\nfrom pydantic import (\n    AfterValidator,\n    BaseModel,\n    ConfigDict,\n    Field,\n    GenerateSchema,\n    GetCoreSchemaHandler,\n    PrivateAttr,\n    PydanticUndefinedAnnotation,\n    PydanticUserError,\n    SecretStr,\n    StringConstraints,\n    TypeAdapter,\n    ValidationError,\n    ValidationInfo,\n    constr,\n    field_validator,\n)\nfrom pydantic._internal._mock_val_ser import MockCoreSchema\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\n\n\ndef test_success():\n    # same as below but defined here so class definition occurs inside the test\n    class Model(BaseModel):\n        a: float\n        b: int = 10\n\n    m = Model(a=10.2)\n    assert m.a == 10.2\n    assert m.b == 10\n\n\n@pytest.fixture(name='UltraSimpleModel', scope='session')\ndef ultra_simple_model_fixture():\n    class UltraSimpleModel(BaseModel):\n        a: float\n        b: int = 10\n\n    return UltraSimpleModel\n\n\ndef test_ultra_simple_missing(UltraSimpleModel):\n    with pytest.raises(ValidationError) as exc_info:\n        UltraSimpleModel()\n    assert exc_info.value.errors(include_url=False) == [\n        {'loc': ('a',), 'msg': 'Field required', 'type': 'missing', 'input': {}}\n    ]\n    assert str(exc_info.value) == (\n        '1 validation error for UltraSimpleModel\\n'\n        'a\\n'\n        '  Field required [type=missing, input_value={}, input_type=dict]'\n    )\n\n\ndef test_ultra_simple_failed(UltraSimpleModel):\n    with pytest.raises(ValidationError) as exc_info:\n        UltraSimpleModel(a='x', b='x')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'float_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid number, unable to parse string as a number',\n            'input': 'x',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        },\n    ]\n\n\ndef test_ultra_simple_repr(UltraSimpleModel):\n    m = UltraSimpleModel(a=10.2)\n    assert str(m) == 'a=10.2 b=10'\n    assert repr(m) == 'UltraSimpleModel(a=10.2, b=10)'\n    assert repr(m.model_fields['a']) == 'FieldInfo(annotation=float, required=True)'\n    assert repr(m.model_fields['b']) == 'FieldInfo(annotation=int, required=False, default=10)'\n    assert dict(m) == {'a': 10.2, 'b': 10}\n    assert m.model_dump() == {'a': 10.2, 'b': 10}\n    assert m.model_dump_json() == '{\"a\":10.2,\"b\":10}'\n    assert str(m) == 'a=10.2 b=10'\n\n\ndef test_default_factory_field():\n    def myfunc():\n        return 1\n\n    class Model(BaseModel):\n        a: int = Field(default_factory=myfunc)\n\n    m = Model()\n    assert str(m) == 'a=1'\n    assert repr(m.model_fields['a']) == 'FieldInfo(annotation=int, required=False, default_factory=myfunc)'\n    assert dict(m) == {'a': 1}\n    assert m.model_dump_json() == '{\"a\":1}'\n\n\ndef test_comparing(UltraSimpleModel):\n    m = UltraSimpleModel(a=10.2, b='100')\n    assert m.model_dump() == {'a': 10.2, 'b': 100}\n    assert m != {'a': 10.2, 'b': 100}\n    assert m == UltraSimpleModel(a=10.2, b=100)\n\n\n@pytest.fixture(scope='session', name='NoneCheckModel')\ndef none_check_model_fix():\n    class NoneCheckModel(BaseModel):\n        existing_str_value: str = 'foo'\n        required_str_value: str = ...\n        required_str_none_value: Optional[str] = ...\n        existing_bytes_value: bytes = b'foo'\n        required_bytes_value: bytes = ...\n        required_bytes_none_value: Optional[bytes] = ...\n\n    return NoneCheckModel\n\n\ndef test_nullable_strings_success(NoneCheckModel):\n    m = NoneCheckModel(\n        required_str_value='v1', required_str_none_value=None, required_bytes_value='v2', required_bytes_none_value=None\n    )\n    assert m.required_str_value == 'v1'\n    assert m.required_str_none_value is None\n    assert m.required_bytes_value == b'v2'\n    assert m.required_bytes_none_value is None\n\n\ndef test_nullable_strings_fails(NoneCheckModel):\n    with pytest.raises(ValidationError) as exc_info:\n        NoneCheckModel(\n            required_str_value=None,\n            required_str_none_value=None,\n            required_bytes_value=None,\n            required_bytes_none_value=None,\n        )\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_type',\n            'loc': ('required_str_value',),\n            'msg': 'Input should be a valid string',\n            'input': None,\n        },\n        {\n            'type': 'bytes_type',\n            'loc': ('required_bytes_value',),\n            'msg': 'Input should be a valid bytes',\n            'input': None,\n        },\n    ]\n\n\n@pytest.fixture(name='ParentModel', scope='session')\ndef parent_sub_model_fixture():\n    class UltraSimpleModel(BaseModel):\n        a: float\n        b: int = 10\n\n    class ParentModel(BaseModel):\n        grape: bool\n        banana: UltraSimpleModel\n\n    return ParentModel\n\n\ndef test_parent_sub_model(ParentModel):\n    m = ParentModel(grape=1, banana={'a': 1})\n    assert m.grape is True\n    assert m.banana.a == 1.0\n    assert m.banana.b == 10\n    assert repr(m) == 'ParentModel(grape=True, banana=UltraSimpleModel(a=1.0, b=10))'\n\n\ndef test_parent_sub_model_fails(ParentModel):\n    with pytest.raises(ValidationError):\n        ParentModel(grape=1, banana=123)\n\n\ndef test_not_required():\n    class Model(BaseModel):\n        a: float = None\n\n    assert Model(a=12.2).a == 12.2\n    assert Model().a is None\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=None)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'float_type',\n            'loc': ('a',),\n            'msg': 'Input should be a valid number',\n            'input': None,\n        },\n    ]\n\n\ndef test_allow_extra():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        a: float\n\n    m = Model(a='10.2', b=12)\n    assert m.__dict__ == {'a': 10.2}\n    assert m.__pydantic_extra__ == {'b': 12}\n    assert m.a == 10.2\n    assert m.b == 12\n    assert m.model_extra == {'b': 12}\n    m.c = 42\n    assert 'c' not in m.__dict__\n    assert m.__pydantic_extra__ == {'b': 12, 'c': 42}\n    assert m.model_dump() == {'a': 10.2, 'b': 12, 'c': 42}\n\n\n@pytest.mark.parametrize('extra', ['ignore', 'forbid', 'allow'])\ndef test_allow_extra_from_attributes(extra: Literal['ignore', 'forbid', 'allow']):\n    class Model(BaseModel):\n        a: float\n\n        model_config = ConfigDict(extra=extra, from_attributes=True)\n\n    class TestClass:\n        a = 1.0\n        b = 12\n\n    m = Model.model_validate(TestClass())\n    assert m.a == 1.0\n    assert not hasattr(m, 'b')\n\n\ndef test_allow_extra_repr():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        a: float = ...\n\n    assert str(Model(a='10.2', b=12)) == 'a=10.2 b=12'\n\n\ndef test_forbidden_extra_success():\n    class ForbiddenExtra(BaseModel):\n        model_config = ConfigDict(extra='forbid')\n        foo: str = 'whatever'\n\n    m = ForbiddenExtra()\n    assert m.foo == 'whatever'\n\n\ndef test_forbidden_extra_fails():\n    class ForbiddenExtra(BaseModel):\n        model_config = ConfigDict(extra='forbid')\n        foo: str = 'whatever'\n\n    with pytest.raises(ValidationError) as exc_info:\n        ForbiddenExtra(foo='ok', bar='wrong', spam='xx')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'extra_forbidden',\n            'loc': ('bar',),\n            'msg': 'Extra inputs are not permitted',\n            'input': 'wrong',\n        },\n        {\n            'type': 'extra_forbidden',\n            'loc': ('spam',),\n            'msg': 'Extra inputs are not permitted',\n            'input': 'xx',\n        },\n    ]\n\n\ndef test_assign_extra_no_validate():\n    class Model(BaseModel):\n        model_config = ConfigDict(validate_assignment=True)\n        a: float\n\n    model = Model(a=0.2)\n    with pytest.raises(ValidationError, match=r\"b\\s+Object has no attribute 'b'\"):\n        model.b = 2\n\n\ndef test_assign_extra_validate():\n    class Model(BaseModel):\n        model_config = ConfigDict(validate_assignment=True)\n        a: float\n\n    model = Model(a=0.2)\n    with pytest.raises(ValidationError, match=r\"b\\s+Object has no attribute 'b'\"):\n        model.b = 2\n\n\ndef test_model_property_attribute_error():\n    class Model(BaseModel):\n        @property\n        def a_property(self):\n            raise AttributeError('Internal Error')\n\n    with pytest.raises(AttributeError, match='Internal Error'):\n        Model().a_property\n\n\ndef test_extra_allowed():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        a: float\n\n    model = Model(a=0.2, b=0.1)\n    assert model.b == 0.1\n\n    assert not hasattr(model, 'c')\n    model.c = 1\n    assert hasattr(model, 'c')\n    assert model.c == 1\n\n\ndef test_reassign_instance_method_with_extra_allow():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        name: str\n\n        def not_extra_func(self) -> str:\n            return f'hello {self.name}'\n\n    def not_extra_func_replacement(self_sub: Model) -> str:\n        return f'hi {self_sub.name}'\n\n    m = Model(name='james')\n    assert m.not_extra_func() == 'hello james'\n\n    m.not_extra_func = partial(not_extra_func_replacement, m)\n    assert m.not_extra_func() == 'hi james'\n    assert 'not_extra_func' in m.__dict__\n\n\ndef test_extra_ignored():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='ignore')\n        a: float\n\n    model = Model(a=0.2, b=0.1)\n    assert not hasattr(model, 'b')\n\n    with pytest.raises(ValueError, match='\"Model\" object has no field \"b\"'):\n        model.b = 1\n\n    assert model.model_extra is None\n\n\ndef test_field_order_is_preserved_with_extra():\n    \"\"\"This test covers https://github.com/pydantic/pydantic/issues/1234.\"\"\"\n\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n\n        a: int\n        b: str\n        c: float\n\n    model = Model(a=1, b='2', c=3.0, d=4)\n    assert repr(model) == \"Model(a=1, b='2', c=3.0, d=4)\"\n    assert str(model.model_dump()) == \"{'a': 1, 'b': '2', 'c': 3.0, 'd': 4}\"\n    assert str(model.model_dump_json()) == '{\"a\":1,\"b\":\"2\",\"c\":3.0,\"d\":4}'\n\n\ndef test_extra_broken_via_pydantic_extra_interference():\n    \"\"\"\n    At the time of writing this test there is `_model_construction.model_extra_getattr` being assigned to model's\n    `__getattr__`. The method then expects `BaseModel.__pydantic_extra__` isn't `None`. Both this actions happen when\n    `model_config.extra` is set to `True`. However, this behavior could be accidentally broken in a subclass of\n    `BaseModel`. In that case `AttributeError` should be thrown when `__getattr__` is being accessed essentially\n    disabling the `extra` functionality.\n    \"\"\"\n\n    class BrokenExtraBaseModel(BaseModel):\n        def model_post_init(self, __context: Any) -> None:\n            super().model_post_init(__context)\n            object.__setattr__(self, '__pydantic_extra__', None)\n\n    class Model(BrokenExtraBaseModel):\n        model_config = ConfigDict(extra='allow')\n\n    m = Model(extra_field='some extra value')\n\n    with pytest.raises(AttributeError) as e:\n        m.extra_field\n\n    assert e.value.args == (\"'Model' object has no attribute 'extra_field'\",)\n\n\ndef test_model_extra_is_none_when_extra_is_forbid():\n    class Foo(BaseModel):\n        model_config = ConfigDict(extra='forbid')\n\n    assert Foo().model_extra is None\n\n\ndef test_set_attr(UltraSimpleModel):\n    m = UltraSimpleModel(a=10.2)\n    assert m.model_dump() == {'a': 10.2, 'b': 10}\n\n    m.b = 20\n    assert m.model_dump() == {'a': 10.2, 'b': 20}\n\n\ndef test_set_attr_invalid():\n    class UltraSimpleModel(BaseModel):\n        a: float = ...\n        b: int = 10\n\n    m = UltraSimpleModel(a=10.2)\n    assert m.model_dump() == {'a': 10.2, 'b': 10}\n\n    with pytest.raises(ValueError) as exc_info:\n        m.c = 20\n    assert '\"UltraSimpleModel\" object has no field \"c\"' in exc_info.value.args[0]\n\n\ndef test_any():\n    class AnyModel(BaseModel):\n        a: Any = 10\n        b: object = 20\n\n    m = AnyModel()\n    assert m.a == 10\n    assert m.b == 20\n\n    m = AnyModel(a='foobar', b='barfoo')\n    assert m.a == 'foobar'\n    assert m.b == 'barfoo'\n\n\ndef test_population_by_field_name():\n    class Model(BaseModel):\n        model_config = ConfigDict(populate_by_name=True)\n        a: str = Field(alias='_a')\n\n    assert Model(a='different').a == 'different'\n    assert Model(a='different').model_dump() == {'a': 'different'}\n    assert Model(a='different').model_dump(by_alias=True) == {'_a': 'different'}\n\n\ndef test_field_order():\n    class Model(BaseModel):\n        c: float\n        b: int = 10\n        a: str\n        d: dict = {}\n\n    assert list(Model.model_fields.keys()) == ['c', 'b', 'a', 'd']\n\n\ndef test_required():\n    # same as below but defined here so class definition occurs inside the test\n    class Model(BaseModel):\n        a: float\n        b: int = 10\n\n    m = Model(a=10.2)\n    assert m.model_dump() == dict(a=10.2, b=10)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model()\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('a',), 'msg': 'Field required', 'input': {}}\n    ]\n\n\ndef test_mutability():\n    class TestModel(BaseModel):\n        a: int = 10\n\n        model_config = ConfigDict(extra='forbid', frozen=False)\n\n    m = TestModel()\n\n    assert m.a == 10\n    m.a = 11\n    assert m.a == 11\n\n\ndef test_frozen_model():\n    class FrozenModel(BaseModel):\n        model_config = ConfigDict(extra='forbid', frozen=True)\n\n        a: int = 10\n\n    m = FrozenModel()\n    assert m.a == 10\n\n    with pytest.raises(ValidationError) as exc_info:\n        m.a = 11\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': ('a',), 'msg': 'Instance is frozen', 'input': 11}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        del m.a\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': ('a',), 'msg': 'Instance is frozen', 'input': None}\n    ]\n\n    assert m.a == 10\n\n\ndef test_frozen_field():\n    class FrozenModel(BaseModel):\n        a: int = Field(10, frozen=True)\n\n    m = FrozenModel()\n    assert m.a == 10\n\n    with pytest.raises(ValidationError) as exc_info:\n        m.a = 11\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('a',), 'msg': 'Field is frozen', 'input': 11}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        del m.a\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('a',), 'msg': 'Field is frozen', 'input': None}\n    ]\n\n    assert m.a == 10\n\n\ndef test_not_frozen_are_not_hashable():\n    class TestModel(BaseModel):\n        a: int = 10\n\n    m = TestModel()\n    with pytest.raises(TypeError) as exc_info:\n        hash(m)\n    assert \"unhashable type: 'TestModel'\" in exc_info.value.args[0]\n\n\ndef test_with_declared_hash():\n    class Foo(BaseModel):\n        x: int\n\n        def __hash__(self):\n            return self.x**2\n\n    class Bar(Foo):\n        y: int\n\n        def __hash__(self):\n            return self.y**3\n\n    class Buz(Bar):\n        z: int\n\n    assert hash(Foo(x=2)) == 4\n    assert hash(Bar(x=2, y=3)) == 27\n    assert hash(Buz(x=2, y=3, z=4)) == 27\n\n\ndef test_frozen_with_hashable_fields_are_hashable():\n    class TestModel(BaseModel):\n        model_config = ConfigDict(frozen=True)\n        a: int = 10\n\n    m = TestModel()\n    assert m.__hash__ is not None\n    assert isinstance(hash(m), int)\n\n\ndef test_frozen_with_unhashable_fields_are_not_hashable():\n    class TestModel(BaseModel):\n        model_config = ConfigDict(frozen=True)\n        a: int = 10\n        y: List[int] = [1, 2, 3]\n\n    m = TestModel()\n    with pytest.raises(TypeError) as exc_info:\n        hash(m)\n    assert \"unhashable type: 'list'\" in exc_info.value.args[0]\n\n\ndef test_hash_function_empty_model():\n    class TestModel(BaseModel):\n        model_config = ConfigDict(frozen=True)\n\n    m = TestModel()\n    m2 = TestModel()\n    assert m == m2\n    assert hash(m) == hash(m2)\n\n\ndef test_hash_function_give_different_result_for_different_object():\n    class TestModel(BaseModel):\n        model_config = ConfigDict(frozen=True)\n\n        a: int = 10\n\n    m = TestModel()\n    m2 = TestModel()\n    m3 = TestModel(a=11)\n    assert hash(m) == hash(m2)\n    assert hash(m) != hash(m3)\n\n\ndef test_hash_function_works_when_instance_dict_modified():\n    class TestModel(BaseModel):\n        model_config = ConfigDict(frozen=True)\n\n        a: int\n        b: int\n\n    m = TestModel(a=1, b=2)\n    h = hash(m)\n\n    # Test edge cases where __dict__ is modified\n    # @functools.cached_property can add keys to __dict__, these should be ignored.\n    m.__dict__['c'] = 1\n    assert hash(m) == h\n\n    # Order of keys can be changed, e.g. with the deprecated copy method, which shouldn't matter.\n    m.__dict__ = {'b': 2, 'a': 1}\n    assert hash(m) == h\n\n    # Keys can be missing, e.g. when using the deprecated copy method.\n    # This could change the hash, and more importantly hashing shouldn't raise a KeyError\n    # We don't assert here, because a hash collision is possible: the hash is not guaranteed to change\n    # However, hashing must not raise an exception, which simply calling hash() checks for\n    del m.__dict__['a']\n    hash(m)\n\n\ndef test_default_hash_function_overrides_default_hash_function():\n    class A(BaseModel):\n        model_config = ConfigDict(frozen=True)\n\n        x: int\n\n    class B(A):\n        model_config = ConfigDict(frozen=True)\n\n        y: int\n\n    assert A.__hash__ != B.__hash__\n    assert hash(A(x=1)) != hash(B(x=1, y=2)) != hash(B(x=1, y=3))\n\n\ndef test_hash_method_is_inherited_for_frozen_models():\n    from functools import lru_cache\n\n    class MyBaseModel(BaseModel):\n        \"\"\"A base model with sensible configurations.\"\"\"\n\n        model_config = ConfigDict(frozen=True)\n\n        def __hash__(self):\n            return hash(id(self))\n\n    class MySubClass(MyBaseModel):\n        x: Dict[str, int]\n\n        @lru_cache(maxsize=None)\n        def cached_method(self):\n            return len(self.x)\n\n    my_instance = MySubClass(x={'a': 1, 'b': 2})\n    assert my_instance.cached_method() == 2\n\n    object.__setattr__(my_instance, 'x', {})  # can't change the \"normal\" way due to frozen\n    assert my_instance.cached_method() == 2\n\n\n@pytest.fixture(name='ValidateAssignmentModel', scope='session')\ndef validate_assignment_fixture():\n    class ValidateAssignmentModel(BaseModel):\n        model_config = ConfigDict(validate_assignment=True)\n        a: int = 2\n        b: constr(min_length=1)\n\n    return ValidateAssignmentModel\n\n\ndef test_validating_assignment_pass(ValidateAssignmentModel):\n    p = ValidateAssignmentModel(a=5, b='hello')\n    p.a = 2\n    assert p.a == 2\n    assert p.model_dump() == {'a': 2, 'b': 'hello'}\n    p.b = 'hi'\n    assert p.b == 'hi'\n    assert p.model_dump() == {'a': 2, 'b': 'hi'}\n\n\ndef test_validating_assignment_fail(ValidateAssignmentModel):\n    p = ValidateAssignmentModel(a=5, b='hello')\n\n    with pytest.raises(ValidationError) as exc_info:\n        p.a = 'b'\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'b',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        p.b = ''\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_too_short',\n            'loc': ('b',),\n            'msg': 'String should have at least 1 character',\n            'input': '',\n            'ctx': {'min_length': 1},\n        }\n    ]\n\n\nclass Foo(Enum):\n    foo = 'foo'\n    bar = 'bar'\n\n\n@pytest.mark.parametrize('value', [Foo.foo, Foo.foo.value, 'foo'])\ndef test_enum_values(value: Any) -> None:\n    class Model(BaseModel):\n        foo: Foo\n        model_config = ConfigDict(use_enum_values=True)\n\n    m = Model(foo=value)\n\n    foo = m.foo\n    assert type(foo) is str, type(foo)\n    assert foo == 'foo'\n\n    foo = m.model_dump()['foo']\n    assert type(foo) is str, type(foo)\n    assert foo == 'foo'\n\n\ndef test_literal_enum_values():\n    FooEnum = Enum('FooEnum', {'foo': 'foo_value', 'bar': 'bar_value'})\n\n    class Model(BaseModel):\n        baz: Literal[FooEnum.foo]\n        boo: str = 'hoo'\n        model_config = ConfigDict(use_enum_values=True)\n\n    m = Model(baz=FooEnum.foo)\n    assert m.model_dump() == {'baz': FooEnum.foo, 'boo': 'hoo'}\n    assert m.model_dump(mode='json') == {'baz': 'foo_value', 'boo': 'hoo'}\n    assert m.baz.value == 'foo_value'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(baz=FooEnum.bar)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': ('baz',),\n            'msg': \"Input should be <FooEnum.foo: 'foo_value'>\",\n            'input': FooEnum.bar,\n            'ctx': {'expected': \"<FooEnum.foo: 'foo_value'>\"},\n        }\n    ]\n\n\ndef test_strict_enum_values():\n    class MyEnum(Enum):\n        val = 'val'\n\n    class Model(BaseModel):\n        model_config = ConfigDict(use_enum_values=True)\n        x: MyEnum\n\n    assert Model.model_validate({'x': MyEnum.val}, strict=True).x == 'val'\n\n\ndef test_union_enum_values():\n    class MyEnum(Enum):\n        val = 'val'\n\n    class NormalModel(BaseModel):\n        x: Union[MyEnum, int]\n\n    class UseEnumValuesModel(BaseModel):\n        model_config = ConfigDict(use_enum_values=True)\n        x: Union[MyEnum, int]\n\n    assert NormalModel(x=MyEnum.val).x != 'val'\n    assert UseEnumValuesModel(x=MyEnum.val).x == 'val'\n\n\ndef test_enum_raw():\n    FooEnum = Enum('FooEnum', {'foo': 'foo', 'bar': 'bar'})\n\n    class Model(BaseModel):\n        foo: FooEnum = None\n\n    m = Model(foo='foo')\n    assert isinstance(m.foo, FooEnum)\n    assert m.foo != 'foo'\n    assert m.foo.value == 'foo'\n\n\ndef test_set_tuple_values():\n    class Model(BaseModel):\n        foo: set\n        bar: tuple\n\n    m = Model(foo=['a', 'b'], bar=['c', 'd'])\n    assert m.foo == {'a', 'b'}\n    assert m.bar == ('c', 'd')\n    assert m.model_dump() == {'foo': {'a', 'b'}, 'bar': ('c', 'd')}\n\n\ndef test_default_copy():\n    class User(BaseModel):\n        friends: List[int] = Field(default_factory=lambda: [])\n\n    u1 = User()\n    u2 = User()\n    assert u1.friends is not u2.friends\n\n\nclass ArbitraryType:\n    pass\n\n\ndef test_arbitrary_type_allowed_validation_success():\n    class ArbitraryTypeAllowedModel(BaseModel):\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n        t: ArbitraryType\n\n    arbitrary_type_instance = ArbitraryType()\n    m = ArbitraryTypeAllowedModel(t=arbitrary_type_instance)\n    assert m.t == arbitrary_type_instance\n\n\nclass OtherClass:\n    pass\n\n\ndef test_arbitrary_type_allowed_validation_fails():\n    class ArbitraryTypeAllowedModel(BaseModel):\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n        t: ArbitraryType\n\n    input_value = OtherClass()\n    with pytest.raises(ValidationError) as exc_info:\n        ArbitraryTypeAllowedModel(t=input_value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': ('t',),\n            'msg': 'Input should be an instance of ArbitraryType',\n            'input': input_value,\n            'ctx': {'class': 'ArbitraryType'},\n        }\n    ]\n\n\ndef test_arbitrary_types_not_allowed():\n    with pytest.raises(TypeError, match='Unable to generate pydantic-core schema for <class'):\n\n        class ArbitraryTypeNotAllowedModel(BaseModel):\n            t: ArbitraryType\n\n\n@pytest.fixture(scope='session', name='TypeTypeModel')\ndef type_type_model_fixture():\n    class TypeTypeModel(BaseModel):\n        t: Type[ArbitraryType]\n\n    return TypeTypeModel\n\n\ndef test_type_type_validation_success(TypeTypeModel):\n    arbitrary_type_class = ArbitraryType\n    m = TypeTypeModel(t=arbitrary_type_class)\n    assert m.t == arbitrary_type_class\n\n\ndef test_type_type_subclass_validation_success(TypeTypeModel):\n    class ArbitrarySubType(ArbitraryType):\n        pass\n\n    arbitrary_type_class = ArbitrarySubType\n    m = TypeTypeModel(t=arbitrary_type_class)\n    assert m.t == arbitrary_type_class\n\n\n@pytest.mark.parametrize('input_value', [OtherClass, 1])\ndef test_type_type_validation_fails(TypeTypeModel, input_value):\n    with pytest.raises(ValidationError) as exc_info:\n        TypeTypeModel(t=input_value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_subclass_of',\n            'loc': ('t',),\n            'msg': 'Input should be a subclass of ArbitraryType',\n            'input': input_value,\n            'ctx': {'class': 'ArbitraryType'},\n        }\n    ]\n\n\n@pytest.mark.parametrize('bare_type', [type, Type])\ndef test_bare_type_type_validation_success(bare_type):\n    class TypeTypeModel(BaseModel):\n        t: bare_type\n\n    arbitrary_type_class = ArbitraryType\n    m = TypeTypeModel(t=arbitrary_type_class)\n    assert m.t == arbitrary_type_class\n\n\n@pytest.mark.parametrize('bare_type', [type, Type])\ndef test_bare_type_type_validation_fails(bare_type):\n    class TypeTypeModel(BaseModel):\n        t: bare_type\n\n    arbitrary_type = ArbitraryType()\n    with pytest.raises(ValidationError) as exc_info:\n        TypeTypeModel(t=arbitrary_type)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_type',\n            'loc': ('t',),\n            'msg': 'Input should be a type',\n            'input': arbitrary_type,\n        }\n    ]\n\n\ndef test_annotation_field_name_shadows_attribute():\n    with pytest.raises(NameError):\n        # When defining a model that has an attribute with the name of a built-in attribute, an exception is raised\n        class BadModel(BaseModel):\n            model_json_schema: str  # This conflicts with the BaseModel's model_json_schema() class method\n\n\ndef test_value_field_name_shadows_attribute():\n    with pytest.raises(PydanticUserError, match=\"A non-annotated attribute was detected: `model_json_schema = 'abc'`\"):\n\n        class BadModel(BaseModel):\n            model_json_schema = (\n                'abc'  # This conflicts with the BaseModel's model_json_schema() class method, but has no annotation\n            )\n\n\ndef test_class_var():\n    class MyModel(BaseModel):\n        a: ClassVar\n        b: ClassVar[int] = 1\n        c: int = 2\n\n    assert list(MyModel.model_fields.keys()) == ['c']\n\n    class MyOtherModel(MyModel):\n        a = ''\n        b = 2\n\n    assert list(MyOtherModel.model_fields.keys()) == ['c']\n\n\ndef test_fields_set():\n    class MyModel(BaseModel):\n        a: int\n        b: int = 2\n\n    m = MyModel(a=5)\n    assert m.model_fields_set == {'a'}\n\n    m.b = 2\n    assert m.model_fields_set == {'a', 'b'}\n\n    m = MyModel(a=5, b=2)\n    assert m.model_fields_set == {'a', 'b'}\n\n\ndef test_exclude_unset_dict():\n    class MyModel(BaseModel):\n        a: int\n        b: int = 2\n\n    m = MyModel(a=5)\n    assert m.model_dump(exclude_unset=True) == {'a': 5}\n\n    m = MyModel(a=5, b=3)\n    assert m.model_dump(exclude_unset=True) == {'a': 5, 'b': 3}\n\n\ndef test_exclude_unset_recursive():\n    class ModelA(BaseModel):\n        a: int\n        b: int = 1\n\n    class ModelB(BaseModel):\n        c: int\n        d: int = 2\n        e: ModelA\n\n    m = ModelB(c=5, e={'a': 0})\n    assert m.model_dump() == {'c': 5, 'd': 2, 'e': {'a': 0, 'b': 1}}\n    assert m.model_dump(exclude_unset=True) == {'c': 5, 'e': {'a': 0}}\n    assert dict(m) == {'c': 5, 'd': 2, 'e': ModelA(a=0, b=1)}\n\n\ndef test_dict_exclude_unset_populated_by_alias():\n    class MyModel(BaseModel):\n        model_config = ConfigDict(populate_by_name=True)\n        a: str = Field('default', alias='alias_a')\n        b: str = Field('default', alias='alias_b')\n\n    m = MyModel(alias_a='a')\n\n    assert m.model_dump(exclude_unset=True) == {'a': 'a'}\n    assert m.model_dump(exclude_unset=True, by_alias=True) == {'alias_a': 'a'}\n\n\ndef test_dict_exclude_unset_populated_by_alias_with_extra():\n    class MyModel(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        a: str = Field('default', alias='alias_a')\n        b: str = Field('default', alias='alias_b')\n\n    m = MyModel(alias_a='a', c='c')\n\n    assert m.model_dump(exclude_unset=True) == {'a': 'a', 'c': 'c'}\n    assert m.model_dump(exclude_unset=True, by_alias=True) == {'alias_a': 'a', 'c': 'c'}\n\n\ndef test_exclude_defaults():\n    class Model(BaseModel):\n        mandatory: str\n        nullable_mandatory: Optional[str] = ...\n        facultative: str = 'x'\n        nullable_facultative: Optional[str] = None\n\n    m = Model(mandatory='a', nullable_mandatory=None)\n    assert m.model_dump(exclude_defaults=True) == {\n        'mandatory': 'a',\n        'nullable_mandatory': None,\n    }\n\n    m = Model(mandatory='a', nullable_mandatory=None, facultative='y', nullable_facultative=None)\n    assert m.model_dump(exclude_defaults=True) == {\n        'mandatory': 'a',\n        'nullable_mandatory': None,\n        'facultative': 'y',\n    }\n\n    m = Model(mandatory='a', nullable_mandatory=None, facultative='y', nullable_facultative='z')\n    assert m.model_dump(exclude_defaults=True) == {\n        'mandatory': 'a',\n        'nullable_mandatory': None,\n        'facultative': 'y',\n        'nullable_facultative': 'z',\n    }\n\n\ndef test_dir_fields():\n    class MyModel(BaseModel):\n        attribute_a: int\n        attribute_b: int = 2\n\n    m = MyModel(attribute_a=5)\n\n    assert 'model_dump' in dir(m)\n    assert 'model_dump_json' in dir(m)\n    assert 'attribute_a' in dir(m)\n    assert 'attribute_b' in dir(m)\n\n\ndef test_dict_with_extra_keys():\n    class MyModel(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        a: str = Field(None, alias='alias_a')\n\n    m = MyModel(extra_key='extra')\n    assert m.model_dump() == {'a': None, 'extra_key': 'extra'}\n    assert m.model_dump(by_alias=True) == {'alias_a': None, 'extra_key': 'extra'}\n\n\ndef test_ignored_types():\n    from pydantic import BaseModel\n\n    class _ClassPropertyDescriptor:\n        def __init__(self, getter):\n            self.getter = getter\n\n        def __get__(self, instance, owner):\n            return self.getter(owner)\n\n    classproperty = _ClassPropertyDescriptor\n\n    class Model(BaseModel):\n        model_config = ConfigDict(ignored_types=(classproperty,))\n\n        @classproperty\n        def class_name(cls) -> str:\n            return cls.__name__\n\n    assert Model.class_name == 'Model'\n    assert Model().class_name == 'Model'\n\n\ndef test_model_iteration():\n    class Foo(BaseModel):\n        a: int = 1\n        b: int = 2\n\n    class Bar(BaseModel):\n        c: int\n        d: Foo\n\n    m = Bar(c=3, d={})\n    assert m.model_dump() == {'c': 3, 'd': {'a': 1, 'b': 2}}\n    assert list(m) == [('c', 3), ('d', Foo())]\n    assert dict(m) == {'c': 3, 'd': Foo()}\n\n\ndef test_model_iteration_extra() -> None:\n    class Foo(BaseModel):\n        x: int = 1\n\n    class Bar(BaseModel):\n        a: int\n        b: Foo\n        model_config = ConfigDict(extra='allow')\n\n    m = Bar.model_validate({'a': 1, 'b': {}, 'c': 2, 'd': Foo()})\n    assert m.model_dump() == {'a': 1, 'b': {'x': 1}, 'c': 2, 'd': {'x': 1}}\n    assert list(m) == [('a', 1), ('b', Foo()), ('c', 2), ('d', Foo())]\n    assert dict(m) == {'a': 1, 'b': Foo(), 'c': 2, 'd': Foo()}\n\n\n@pytest.mark.parametrize(\n    'exclude,expected,raises_match',\n    [\n        pytest.param(\n            None,\n            {'c': 3, 'foos': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]},\n            None,\n            id='exclude nothing',\n        ),\n        pytest.param(\n            {'foos': {0: {'a'}, 1: {'a'}}},\n            {'c': 3, 'foos': [{'b': 2}, {'b': 4}]},\n            None,\n            id='excluding fields of indexed list items',\n        ),\n        pytest.param(\n            {'foos': {'a'}},\n            {'c': 3, 'foos': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]},\n            None,\n            id='Trying to exclude string keys on list field should be ignored (1)',\n        ),\n        pytest.param(\n            {'foos': {0: ..., 'a': ...}},\n            {'c': 3, 'foos': [{'a': 3, 'b': 4}]},\n            None,\n            id='Trying to exclude string keys on list field should be ignored (2)',\n        ),\n        pytest.param(\n            {'foos': {0: 1}},\n            TypeError,\n            '`exclude` argument must be a set or dict',\n            id='value as int should be an error',\n        ),\n        pytest.param(\n            {'foos': {'__all__': {1}}},\n            {'c': 3, 'foos': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]},\n            None,\n            id='excluding int in dict should have no effect',\n        ),\n        pytest.param(\n            {'foos': {'__all__': {'a'}}},\n            {'c': 3, 'foos': [{'b': 2}, {'b': 4}]},\n            None,\n            id='using \"__all__\" to exclude specific nested field',\n        ),\n        pytest.param(\n            {'foos': {0: {'b'}, '__all__': {'a'}}},\n            {'c': 3, 'foos': [{}, {'b': 4}]},\n            None,\n            id='using \"__all__\" to exclude specific nested field in combination with more specific exclude',\n        ),\n        pytest.param(\n            {'foos': {'__all__'}},\n            {'c': 3, 'foos': []},\n            None,\n            id='using \"__all__\" to exclude all list items',\n        ),\n        pytest.param(\n            {'foos': {1, '__all__'}},\n            {'c': 3, 'foos': []},\n            None,\n            id='using \"__all__\" and other items should get merged together, still excluding all list items',\n        ),\n        pytest.param(\n            {'foos': {-1: {'b'}}},\n            {'c': 3, 'foos': [{'a': 1, 'b': 2}, {'a': 3}]},\n            None,\n            id='negative indexes',\n        ),\n    ],\n)\ndef test_model_export_nested_list(exclude, expected, raises_match):\n    class Foo(BaseModel):\n        a: int = 1\n        b: int = 2\n\n    class Bar(BaseModel):\n        c: int\n        foos: List[Foo]\n\n    m = Bar(c=3, foos=[Foo(a=1, b=2), Foo(a=3, b=4)])\n\n    if raises_match is not None:\n        with pytest.raises(expected, match=raises_match):\n            m.model_dump(exclude=exclude)\n    else:\n        original_exclude = deepcopy(exclude)\n        assert m.model_dump(exclude=exclude) == expected\n        assert exclude == original_exclude\n\n\n@pytest.mark.parametrize(\n    'excludes,expected',\n    [\n        pytest.param(\n            {'bars': {0}},\n            {'a': 1, 'bars': [{'y': 2}, {'w': -1, 'z': 3}]},\n            id='excluding first item from list field using index',\n        ),\n        pytest.param({'bars': {'__all__'}}, {'a': 1, 'bars': []}, id='using \"__all__\" to exclude all list items'),\n        pytest.param(\n            {'bars': {'__all__': {'w'}}},\n            {'a': 1, 'bars': [{'x': 1}, {'y': 2}, {'z': 3}]},\n            id='exclude single dict key from all list items',\n        ),\n    ],\n)\ndef test_model_export_dict_exclusion(excludes, expected):\n    class Foo(BaseModel):\n        a: int = 1\n        bars: List[Dict[str, int]]\n\n    m = Foo(a=1, bars=[{'w': 0, 'x': 1}, {'y': 2}, {'w': -1, 'z': 3}])\n\n    original_excludes = deepcopy(excludes)\n    assert m.model_dump(exclude=excludes) == expected\n    assert excludes == original_excludes\n\n\ndef test_field_exclude():\n    class User(BaseModel):\n        _priv: int = PrivateAttr()\n        id: int\n        username: str\n        password: SecretStr = Field(exclude=True)\n        hobbies: List[str]\n\n    my_user = User(id=42, username='JohnDoe', password='hashedpassword', hobbies=['scuba diving'])\n\n    my_user._priv = 13\n    assert my_user.id == 42\n    assert my_user.password.get_secret_value() == 'hashedpassword'\n    assert my_user.model_dump() == {'id': 42, 'username': 'JohnDoe', 'hobbies': ['scuba diving']}\n\n\ndef test_revalidate_instances_never():\n    class User(BaseModel):\n        hobbies: List[str]\n\n    my_user = User(hobbies=['scuba diving'])\n\n    class Transaction(BaseModel):\n        user: User\n\n    t = Transaction(user=my_user)\n\n    assert t.user is my_user\n    assert t.user.hobbies is my_user.hobbies\n\n    class SubUser(User):\n        sins: List[str]\n\n    my_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\n\n    t = Transaction(user=my_sub_user)\n\n    assert t.user is my_sub_user\n    assert t.user.hobbies is my_sub_user.hobbies\n\n\ndef test_revalidate_instances_sub_instances():\n    class User(BaseModel, revalidate_instances='subclass-instances'):\n        hobbies: List[str]\n\n    my_user = User(hobbies=['scuba diving'])\n\n    class Transaction(BaseModel):\n        user: User\n\n    t = Transaction(user=my_user)\n\n    assert t.user is my_user\n    assert t.user.hobbies is my_user.hobbies\n\n    class SubUser(User):\n        sins: List[str]\n\n    my_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\n\n    t = Transaction(user=my_sub_user)\n\n    assert t.user is not my_sub_user\n    assert t.user.hobbies is not my_sub_user.hobbies\n    assert not hasattr(t.user, 'sins')\n\n\ndef test_revalidate_instances_always():\n    class User(BaseModel, revalidate_instances='always'):\n        hobbies: List[str]\n\n    my_user = User(hobbies=['scuba diving'])\n\n    class Transaction(BaseModel):\n        user: User\n\n    t = Transaction(user=my_user)\n\n    assert t.user is not my_user\n    assert t.user.hobbies is not my_user.hobbies\n\n    class SubUser(User):\n        sins: List[str]\n\n    my_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\n\n    t = Transaction(user=my_sub_user)\n\n    assert t.user is not my_sub_user\n    assert t.user.hobbies is not my_sub_user.hobbies\n    assert not hasattr(t.user, 'sins')\n\n\ndef test_revalidate_instances_always_list_of_model_instance():\n    class A(BaseModel):\n        model_config = ConfigDict(revalidate_instances='always')\n        name: str\n\n    class B(BaseModel):\n        list_a: List[A]\n\n    a = A(name='a')\n    b = B(list_a=[a])\n    assert b.list_a == [A(name='a')]\n    a.name = 'b'\n    assert b.list_a == [A(name='a')]\n\n\n@pytest.mark.skip(reason='not implemented')\n@pytest.mark.parametrize(\n    'kinds',\n    [\n        {'sub_fields', 'model_fields', 'model_config', 'sub_config', 'combined_config'},\n        {'sub_fields', 'model_fields', 'combined_config'},\n        {'sub_fields', 'model_fields'},\n        {'combined_config'},\n        {'model_config', 'sub_config'},\n        {'model_config', 'sub_fields'},\n        {'model_fields', 'sub_config'},\n    ],\n)\n@pytest.mark.parametrize(\n    'exclude,expected',\n    [\n        (None, {'a': 0, 'c': {'a': [3, 5], 'c': 'foobar'}, 'd': {'c': 'foobar'}}),\n        ({'c', 'd'}, {'a': 0}),\n        ({'a': ..., 'c': ..., 'd': {'a': ..., 'c': ...}}, {'d': {}}),\n    ],\n)\ndef test_model_export_exclusion_with_fields_and_config(kinds, exclude, expected):\n    \"\"\"Test that exporting models with fields using the export parameter works.\"\"\"\n\n    class ChildConfig:\n        pass\n\n    if 'sub_config' in kinds:\n        ChildConfig.fields = {'b': {'exclude': ...}, 'a': {'exclude': {1}}}\n\n    class ParentConfig:\n        pass\n\n    if 'combined_config' in kinds:\n        ParentConfig.fields = {\n            'b': {'exclude': ...},\n            'c': {'exclude': {'b': ..., 'a': {1}}},\n            'd': {'exclude': {'a': ..., 'b': ...}},\n        }\n\n    elif 'model_config' in kinds:\n        ParentConfig.fields = {'b': {'exclude': ...}, 'd': {'exclude': {'a'}}}\n\n    class Sub(BaseModel):\n        a: List[int] = Field([3, 4, 5], exclude={1} if 'sub_fields' in kinds else None)\n        b: int = Field(4, exclude=... if 'sub_fields' in kinds else None)\n        c: str = 'foobar'\n\n        Config = ChildConfig\n\n    class Model(BaseModel):\n        a: int = 0\n        b: int = Field(2, exclude=... if 'model_fields' in kinds else None)\n        c: Sub = Sub()\n        d: Sub = Field(Sub(), exclude={'a'} if 'model_fields' in kinds else None)\n\n        Config = ParentConfig\n\n    m = Model()\n    assert m.model_dump(exclude=exclude) == expected, 'Unexpected model export result'\n\n\n@pytest.mark.skip(reason='not implemented')\ndef test_model_export_exclusion_inheritance():\n    class Sub(BaseModel):\n        s1: str = 'v1'\n        s2: str = 'v2'\n        s3: str = 'v3'\n        s4: str = Field('v4', exclude=...)\n\n    class Parent(BaseModel):\n        model_config = ConfigDict(fields={'a': {'exclude': ...}, 's': {'exclude': {'s1'}}})\n        a: int\n        b: int = Field(..., exclude=...)\n        c: int\n        d: int\n        s: Sub = Sub()\n\n    class Child(Parent):\n        model_config = ConfigDict(fields={'c': {'exclude': ...}, 's': {'exclude': {'s2'}}})\n\n    actual = Child(a=0, b=1, c=2, d=3).model_dump()\n    expected = {'d': 3, 's': {'s3': 'v3'}}\n    assert actual == expected, 'Unexpected model export result'\n\n\n@pytest.mark.skip(reason='not implemented')\ndef test_model_export_with_true_instead_of_ellipsis():\n    class Sub(BaseModel):\n        s1: int = 1\n\n    class Model(BaseModel):\n        model_config = ConfigDict(fields={'c': {'exclude': True}})\n        a: int = 2\n        b: int = Field(3, exclude=True)\n        c: int = Field(4)\n        s: Sub = Sub()\n\n    m = Model()\n    assert m.model_dump(exclude={'s': True}) == {'a': 2}\n\n\n@pytest.mark.skip(reason='not implemented')\ndef test_model_export_inclusion():\n    class Sub(BaseModel):\n        s1: str = 'v1'\n        s2: str = 'v2'\n        s3: str = 'v3'\n        s4: str = 'v4'\n\n    class Model(BaseModel):\n        model_config = ConfigDict(\n            fields={'a': {'include': {'s2', 's1', 's3'}}, 'b': {'include': {'s1', 's2', 's3', 's4'}}}\n        )\n        a: Sub = Sub()\n        b: Sub = Field(Sub(), include={'s1'})\n        c: Sub = Field(Sub(), include={'s1', 's2'})\n\n    Model.model_fields['a'].field_info.include == {'s1': ..., 's2': ..., 's3': ...}\n    Model.model_fields['b'].field_info.include == {'s1': ...}\n    Model.model_fields['c'].field_info.include == {'s1': ..., 's2': ...}\n\n    actual = Model().model_dump(include={'a': {'s3', 's4'}, 'b': ..., 'c': ...})\n    # s1 included via field, s2 via config and s3 via .dict call:\n    expected = {'a': {'s3': 'v3'}, 'b': {'s1': 'v1'}, 'c': {'s1': 'v1', 's2': 'v2'}}\n\n    assert actual == expected, 'Unexpected model export result'\n\n\n@pytest.mark.skip(reason='not implemented')\ndef test_model_export_inclusion_inheritance():\n    class Sub(BaseModel):\n        s1: str = Field('v1', include=...)\n        s2: str = Field('v2', include=...)\n        s3: str = Field('v3', include=...)\n        s4: str = 'v4'\n\n    class Parent(BaseModel):\n        # b will be included since fields are set independently\n        model_config = ConfigDict(fields={'b': {'include': ...}})\n        a: int\n        b: int\n        c: int\n        s: Sub = Field(Sub(), include={'s1', 's2'})  # overrides includes set in Sub model\n\n    class Child(Parent):\n        # b is still included even if it doesn't occur here since fields\n        # are still considered separately.\n        # s however, is merged, resulting in only s1 being included.\n        model_config = ConfigDict(fields={'a': {'include': ...}, 's': {'include': {'s1'}}})\n\n    actual = Child(a=0, b=1, c=2).model_dump()\n    expected = {'a': 0, 'b': 1, 's': {'s1': 'v1'}}\n    assert actual == expected, 'Unexpected model export result'\n\n\ndef test_untyped_fields_warning():\n    with pytest.raises(\n        PydanticUserError,\n        match=re.escape(\n            'A non-annotated attribute was detected: `x = 1`. All model fields require a type annotation; '\n            'if `x` is not meant to be a field, you may be able to resolve this error by annotating it '\n            \"as a `ClassVar` or updating `model_config['ignored_types']`.\"\n        ),\n    ):\n\n        class WarningModel(BaseModel):\n            x = 1\n\n    # Prove that annotating with ClassVar prevents the warning\n    class NonWarningModel(BaseModel):\n        x: ClassVar = 1\n\n\ndef test_untyped_fields_error():\n    with pytest.raises(TypeError, match=\"Field 'a' requires a type annotation\"):\n\n        class Model(BaseModel):\n            a = Field('foobar')\n\n\ndef test_custom_init_subclass_params():\n    class DerivedModel(BaseModel):\n        def __init_subclass__(cls, something):\n            cls.something = something\n\n    # if this raises a TypeError, then there is a regression of issue 867:\n    # pydantic.main.MetaModel.__new__ should include **kwargs at the end of the\n    # method definition and pass them on to the super call at the end in order\n    # to allow the special method __init_subclass__ to be defined with custom\n    # parameters on extended BaseModel classes.\n    class NewModel(DerivedModel, something=2):\n        something: ClassVar = 1\n\n    assert NewModel.something == 2\n\n\ndef test_recursive_model():\n    class MyModel(BaseModel):\n        field: Optional['MyModel']\n\n    m = MyModel(field={'field': {'field': None}})\n    assert m.model_dump() == {'field': {'field': {'field': None}}}\n\n\ndef test_recursive_cycle_with_repeated_field():\n    class A(BaseModel):\n        b: 'B'\n\n    class B(BaseModel):\n        a1: Optional[A] = None\n        a2: Optional[A] = None\n\n    A.model_rebuild()\n\n    assert A.model_validate({'b': {'a1': {'b': {'a1': None}}}}) == A(b=B(a1=A(b=B(a1=None))))\n    with pytest.raises(ValidationError) as exc_info:\n        A.model_validate({'b': {'a1': {'a1': None}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'a1': None}, 'loc': ('b', 'a1', 'b'), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\ndef test_two_defaults():\n    with pytest.raises(TypeError, match='^cannot specify both default and default_factory$'):\n\n        class Model(BaseModel):\n            a: int = Field(default=3, default_factory=lambda: 3)\n\n\ndef test_default_factory():\n    class ValueModel(BaseModel):\n        uid: UUID = uuid4()\n\n    m1 = ValueModel()\n    m2 = ValueModel()\n    assert m1.uid == m2.uid\n\n    class DynamicValueModel(BaseModel):\n        uid: UUID = Field(default_factory=uuid4)\n\n    m1 = DynamicValueModel()\n    m2 = DynamicValueModel()\n    assert isinstance(m1.uid, UUID)\n    assert m1.uid != m2.uid\n\n    # With a callable: we still should be able to set callables as defaults\n    class FunctionModel(BaseModel):\n        a: int = 1\n        uid: Callable[[], UUID] = Field(uuid4)\n\n    m = FunctionModel()\n    assert m.uid is uuid4\n\n    # Returning a singleton from a default_factory is supported\n    class MySingleton:\n        pass\n\n    MY_SINGLETON = MySingleton()\n\n    class SingletonFieldModel(BaseModel):\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n        singleton: MySingleton = Field(default_factory=lambda: MY_SINGLETON)\n\n    assert SingletonFieldModel().singleton is SingletonFieldModel().singleton\n\n\ndef test_default_factory_called_once():\n    \"\"\"It should call only once the given factory by default\"\"\"\n\n    class Seq:\n        def __init__(self):\n            self.v = 0\n\n        def __call__(self):\n            self.v += 1\n            return self.v\n\n    class MyModel(BaseModel):\n        id: int = Field(default_factory=Seq())\n\n    m1 = MyModel()\n    assert m1.id == 1\n    m2 = MyModel()\n    assert m2.id == 2\n    assert m1.id == 1\n\n\ndef test_default_factory_called_once_2():\n    \"\"\"It should call only once the given factory by default\"\"\"\n\n    v = 0\n\n    def factory():\n        nonlocal v\n        v += 1\n        return v\n\n    class MyModel(BaseModel):\n        id: int = Field(default_factory=factory)\n\n    m1 = MyModel()\n    assert m1.id == 1\n    m2 = MyModel()\n    assert m2.id == 2\n\n\ndef test_default_factory_validate_children():\n    class Child(BaseModel):\n        x: int\n\n    class Parent(BaseModel):\n        children: List[Child] = Field(default_factory=list)\n\n    Parent(children=[{'x': 1}, {'x': 2}])\n    with pytest.raises(ValidationError) as exc_info:\n        Parent(children=[{'x': 1}, {'y': 2}])\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('children', 1, 'x'), 'msg': 'Field required', 'input': {'y': 2}}\n    ]\n\n\ndef test_default_factory_parse():\n    class Inner(BaseModel):\n        val: int = Field(0)\n\n    class Outer(BaseModel):\n        inner_1: Inner = Field(default_factory=Inner)\n        inner_2: Inner = Field(Inner())\n\n    default = Outer().model_dump()\n    parsed = Outer.model_validate(default)\n    assert parsed.model_dump() == {'inner_1': {'val': 0}, 'inner_2': {'val': 0}}\n    assert repr(parsed) == 'Outer(inner_1=Inner(val=0), inner_2=Inner(val=0))'\n\n\ndef test_reuse_same_field():\n    required_field = Field(...)\n\n    class Model1(BaseModel):\n        required: str = required_field\n\n    class Model2(BaseModel):\n        required: str = required_field\n\n    with pytest.raises(ValidationError):\n        Model1.model_validate({})\n    with pytest.raises(ValidationError):\n        Model2.model_validate({})\n\n\ndef test_base_config_type_hinting():\n    class M(BaseModel):\n        a: int\n\n    get_type_hints(type(M.model_config))\n\n\ndef test_frozen_field_with_validate_assignment():\n    \"\"\"assigning a frozen=True field should raise a TypeError\"\"\"\n\n    class Entry(BaseModel):\n        model_config = ConfigDict(validate_assignment=True)\n        id: float = Field(frozen=True)\n        val: float\n\n    r = Entry(id=1, val=100)\n    assert r.val == 100\n    r.val = 101\n    assert r.val == 101\n    assert r.id == 1\n    with pytest.raises(ValidationError) as exc_info:\n        r.id = 2\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 2, 'loc': ('id',), 'msg': 'Field is frozen', 'type': 'frozen_field'}\n    ]\n\n\ndef test_repr_field():\n    class Model(BaseModel):\n        a: int = Field()\n        b: float = Field(repr=True)\n        c: bool = Field(repr=False)\n\n    m = Model(a=1, b=2.5, c=True)\n    assert repr(m) == 'Model(a=1, b=2.5)'\n    assert repr(m.model_fields['a']) == 'FieldInfo(annotation=int, required=True)'\n    assert repr(m.model_fields['b']) == 'FieldInfo(annotation=float, required=True)'\n    assert repr(m.model_fields['c']) == 'FieldInfo(annotation=bool, required=True, repr=False)'\n\n\ndef test_inherited_model_field_copy():\n    \"\"\"It should copy models used as fields by default\"\"\"\n\n    class Image(BaseModel):\n        path: str\n\n        def __hash__(self):\n            return id(self)\n\n    class Item(BaseModel):\n        images: Set[Image]\n\n    image_1 = Image(path='my_image1.png')\n    image_2 = Image(path='my_image2.png')\n\n    item = Item(images={image_1, image_2})\n    assert image_1 in item.images\n\n    assert id(image_1) in {id(image) for image in item.images}\n    assert id(image_2) in {id(image) for image in item.images}\n\n\ndef test_mapping_subclass_as_input():\n    class CustomMap(dict):\n        pass\n\n    class Model(BaseModel):\n        x: Mapping[str, int]\n\n    d = CustomMap()\n    d['one'] = 1\n    d['two'] = 2\n\n    v = Model(x=d).x\n    # we don't promise that this will or will not be a CustomMap\n    # all we promise is that it _will_ be a mapping\n    assert isinstance(v, Mapping)\n    # but the current behavior is that it will be a dict, not a CustomMap\n    # so document that here\n    assert not isinstance(v, CustomMap)\n    assert v == {'one': 1, 'two': 2}\n\n\ndef test_typing_coercion_dict():\n    class Model(BaseModel):\n        x: Dict[str, int]\n\n    m = Model(x={'one': 1, 'two': 2})\n    assert repr(m) == \"Model(x={'one': 1, 'two': 2})\"\n\n\nKT = TypeVar('KT')\nVT = TypeVar('VT')\n\n\nclass MyDict(Dict[KT, VT]):\n    def __repr__(self):\n        return f'MyDict({super().__repr__()})'\n\n\ndef test_class_kwargs_config():\n    class Base(BaseModel, extra='forbid', alias_generator=str.upper):\n        a: int\n\n    assert Base.model_config['extra'] == 'forbid'\n    assert Base.model_config['alias_generator'] is str.upper\n    # assert Base.model_fields['a'].alias == 'A'\n\n    class Model(Base, extra='allow'):\n        b: int\n\n    assert Model.model_config['extra'] == 'allow'  # overwritten as intended\n    assert Model.model_config['alias_generator'] is str.upper  # inherited as intended\n    # assert Model.model_fields['b'].alias == 'B'  # alias_generator still works\n\n\ndef test_class_kwargs_config_and_attr_conflict():\n    class Model(BaseModel, extra='allow', alias_generator=str.upper):\n        model_config = ConfigDict(extra='forbid', title='Foobar')\n        b: int\n\n    assert Model.model_config['extra'] == 'allow'\n    assert Model.model_config['alias_generator'] is str.upper\n    assert Model.model_config['title'] == 'Foobar'\n\n\ndef test_class_kwargs_custom_config():\n    if platform.python_implementation() == 'PyPy':\n        msg = r\"__init_subclass__\\(\\) got an unexpected keyword argument 'some_config'\"\n    else:\n        msg = r'__init_subclass__\\(\\) takes no keyword arguments'\n    with pytest.raises(TypeError, match=msg):\n\n        class Model(BaseModel, some_config='new_value'):\n            a: int\n\n\ndef test_new_union_origin():\n    \"\"\"On 3.10+, origin of `int | str` is `types.UnionType`, not `typing.Union`\"\"\"\n\n    class Model(BaseModel):\n        x: 'int | str'\n\n    assert Model(x=3).x == 3\n    assert Model(x='3').x == '3'\n    assert Model(x='pika').x == 'pika'\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'x': {'title': 'X', 'anyOf': [{'type': 'integer'}, {'type': 'string'}]}},\n        'required': ['x'],\n    }\n\n\n@pytest.mark.parametrize(\n    'ann',\n    [Final, Final[int]],\n    ids=['no-arg', 'with-arg'],\n)\n@pytest.mark.parametrize(\n    'value',\n    [None, Field(...)],\n    ids=['none', 'field'],\n)\ndef test_frozen_field_decl_without_default_val(ann, value):\n    class Model(BaseModel):\n        a: ann\n\n        if value is not None:\n            a = value\n\n    assert 'a' not in Model.__class_vars__\n    assert 'a' in Model.model_fields\n\n    assert Model.model_fields['a'].frozen\n\n\n@pytest.mark.parametrize(\n    'ann',\n    [Final, Final[int]],\n    ids=['no-arg', 'with-arg'],\n)\ndef test_final_field_decl_with_default_val(ann):\n    class Model(BaseModel):\n        a: ann = 10\n\n    assert 'a' in Model.__class_vars__\n    assert 'a' not in Model.model_fields\n\n\ndef test_final_field_reassignment():\n    class Model(BaseModel):\n        model_config = ConfigDict(validate_assignment=True)\n\n        a: Final[int]\n\n    obj = Model(a=10)\n\n    with pytest.raises(ValidationError) as exc_info:\n        obj.a = 20\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 20, 'loc': ('a',), 'msg': 'Field is frozen', 'type': 'frozen_field'}\n    ]\n\n\ndef test_field_by_default_is_not_frozen():\n    class Model(BaseModel):\n        a: int\n\n    assert not Model.model_fields['a'].frozen\n\n\ndef test_annotated_final():\n    class Model(BaseModel):\n        a: Annotated[Final[int], Field(title='abc')]\n\n    assert Model.model_fields['a'].frozen\n    assert Model.model_fields['a'].title == 'abc'\n\n    class Model2(BaseModel):\n        a: Final[Annotated[int, Field(title='def')]]\n\n    assert Model2.model_fields['a'].frozen\n    assert Model2.model_fields['a'].title == 'def'\n\n\ndef test_post_init():\n    calls = []\n\n    class InnerModel(BaseModel):\n        a: int\n        b: int\n\n        def model_post_init(self, __context) -> None:\n            super().model_post_init(__context)  # this is included just to show it doesn't error\n            assert self.model_dump() == {'a': 3, 'b': 4}\n            calls.append('inner_model_post_init')\n\n    class Model(BaseModel):\n        c: int\n        d: int\n        sub: InnerModel\n\n        def model_post_init(self, __context) -> None:\n            assert self.model_dump() == {'c': 1, 'd': 2, 'sub': {'a': 3, 'b': 4}}\n            calls.append('model_post_init')\n\n    m = Model(c=1, d='2', sub={'a': 3, 'b': '4'})\n    assert calls == ['inner_model_post_init', 'model_post_init']\n    assert m.model_dump() == {'c': 1, 'd': 2, 'sub': {'a': 3, 'b': 4}}\n\n    class SubModel(Model):\n        def model_post_init(self, __context) -> None:\n            assert self.model_dump() == {'c': 1, 'd': 2, 'sub': {'a': 3, 'b': 4}}\n            super().model_post_init(__context)\n            calls.append('submodel_post_init')\n\n    calls.clear()\n    m = SubModel(c=1, d='2', sub={'a': 3, 'b': '4'})\n    assert calls == ['inner_model_post_init', 'model_post_init', 'submodel_post_init']\n    assert m.model_dump() == {'c': 1, 'd': 2, 'sub': {'a': 3, 'b': 4}}\n\n\n@pytest.mark.parametrize('include_private_attribute', [True, False])\ndef test_post_init_call_signatures(include_private_attribute):\n    calls = []\n\n    class Model(BaseModel):\n        a: int\n        b: int\n        if include_private_attribute:\n            _x: int = PrivateAttr(1)\n\n        def model_post_init(self, *args, **kwargs) -> None:\n            calls.append((args, kwargs))\n\n    Model(a=1, b=2)\n    assert calls == [((None,), {})]\n    Model.model_construct(a=3, b=4)\n    assert calls == [((None,), {}), ((None,), {})]\n\n\ndef test_post_init_not_called_without_override():\n    calls = []\n\n    def monkey_patched_model_post_init(cls, __context):\n        calls.append('BaseModel.model_post_init')\n\n    original_base_model_post_init = BaseModel.model_post_init\n    try:\n        BaseModel.model_post_init = monkey_patched_model_post_init\n\n        class WithoutOverrideModel(BaseModel):\n            pass\n\n        WithoutOverrideModel()\n        WithoutOverrideModel.model_construct()\n        assert calls == []\n\n        class WithOverrideModel(BaseModel):\n            def model_post_init(self, __context: Any) -> None:\n                calls.append('WithOverrideModel.model_post_init')\n\n        WithOverrideModel()\n        assert calls == ['WithOverrideModel.model_post_init']\n        WithOverrideModel.model_construct()\n        assert calls == ['WithOverrideModel.model_post_init', 'WithOverrideModel.model_post_init']\n\n    finally:\n        BaseModel.model_post_init = original_base_model_post_init\n\n\ndef test_model_post_init_subclass_private_attrs():\n    \"\"\"https://github.com/pydantic/pydantic/issues/7293\"\"\"\n    calls = []\n\n    class A(BaseModel):\n        a: int = 1\n\n        def model_post_init(self, __context: Any) -> None:\n            calls.append(f'{self.__class__.__name__}.model_post_init')\n\n    class B(A):\n        pass\n\n    class C(B):\n        _private: bool = True\n\n    C()\n\n    assert calls == ['C.model_post_init']\n\n\ndef test_model_post_init_supertype_private_attrs():\n    \"\"\"https://github.com/pydantic/pydantic/issues/9098\"\"\"\n\n    class Model(BaseModel):\n        _private: int = 12\n\n    class SubModel(Model):\n        def model_post_init(self, __context: Any) -> None:\n            if self._private == 12:\n                self._private = 13\n            super().model_post_init(__context)\n\n    m = SubModel()\n\n    assert m._private == 13\n\n\ndef test_model_post_init_subclass_setting_private_attrs():\n    \"\"\"https://github.com/pydantic/pydantic/issues/7091\"\"\"\n\n    class Model(BaseModel):\n        _priv1: int = PrivateAttr(91)\n        _priv2: int = PrivateAttr(92)\n\n        def model_post_init(self, __context) -> None:\n            self._priv1 = 100\n\n    class SubModel(Model):\n        _priv3: int = PrivateAttr(93)\n        _priv4: int = PrivateAttr(94)\n        _priv5: int = PrivateAttr()\n        _priv6: int = PrivateAttr()\n\n        def model_post_init(self, __context) -> None:\n            self._priv3 = 200\n            self._priv5 = 300\n            super().model_post_init(__context)\n\n    m = SubModel()\n\n    assert m._priv1 == 100\n    assert m._priv2 == 92\n    assert m._priv3 == 200\n    assert m._priv4 == 94\n    assert m._priv5 == 300\n    with pytest.raises(AttributeError):\n        assert m._priv6 == 94\n\n\ndef test_model_post_init_correct_mro():\n    \"\"\"https://github.com/pydantic/pydantic/issues/7293\"\"\"\n    calls = []\n\n    class A(BaseModel):\n        a: int = 1\n\n    class B(BaseModel):\n        b: int = 1\n\n        def model_post_init(self, __context: Any) -> None:\n            calls.append(f'{self.__class__.__name__}.model_post_init')\n\n    class C(A, B):\n        _private: bool = True\n\n    C()\n\n    assert calls == ['C.model_post_init']\n\n\ndef test_deeper_recursive_model():\n    class A(BaseModel):\n        b: 'B'\n\n    class B(BaseModel):\n        c: 'C'\n\n    class C(BaseModel):\n        a: Optional['A']\n\n    A.model_rebuild()\n    B.model_rebuild()\n    C.model_rebuild()\n\n    m = A(b=B(c=C(a=None)))\n    assert m.model_dump() == {'b': {'c': {'a': None}}}\n\n\ndef test_model_rebuild_localns():\n    class A(BaseModel):\n        x: int\n\n    class B(BaseModel):\n        a: 'Model'  # noqa: F821\n\n    B.model_rebuild(_types_namespace={'Model': A})\n\n    m = B(a={'x': 1})\n    assert m.model_dump() == {'a': {'x': 1}}\n    assert isinstance(m.a, A)\n\n    class C(BaseModel):\n        a: 'Model'  # noqa: F821\n\n    with pytest.raises(PydanticUndefinedAnnotation, match=\"name 'Model' is not defined\"):\n        C.model_rebuild(_types_namespace={'A': A})\n\n\ndef test_model_rebuild_zero_depth():\n    class Model(BaseModel):\n        x: 'X_Type'\n\n    X_Type = str\n\n    with pytest.raises(NameError, match='X_Type'):\n        Model.model_rebuild(_parent_namespace_depth=0)\n\n    Model.__pydantic_parent_namespace__.update({'X_Type': int})\n    Model.model_rebuild(_parent_namespace_depth=0)\n\n    m = Model(x=42)\n    assert m.model_dump() == {'x': 42}\n\n\n@pytest.fixture(scope='session', name='InnerEqualityModel')\ndef inner_equality_fixture():\n    class InnerEqualityModel(BaseModel):\n        iw: int\n        ix: int = 0\n        _iy: int = PrivateAttr()\n        _iz: int = PrivateAttr(0)\n\n    return InnerEqualityModel\n\n\n@pytest.fixture(scope='session', name='EqualityModel')\ndef equality_fixture(InnerEqualityModel):\n    class EqualityModel(BaseModel):\n        w: int\n        x: int = 0\n        _y: int = PrivateAttr()\n        _z: int = PrivateAttr(0)\n\n        model: InnerEqualityModel\n\n    return EqualityModel\n\n\ndef test_model_equality(EqualityModel, InnerEqualityModel):\n    m1 = EqualityModel(w=0, x=0, model=InnerEqualityModel(iw=0))\n    m2 = EqualityModel(w=0, x=0, model=InnerEqualityModel(iw=0))\n    assert m1 == m2\n\n\ndef test_model_equality_type(EqualityModel, InnerEqualityModel):\n    class Model1(BaseModel):\n        x: int\n\n    class Model2(BaseModel):\n        x: int\n\n    m1 = Model1(x=1)\n    m2 = Model2(x=1)\n\n    assert m1.model_dump() == m2.model_dump()\n    assert m1 != m2\n\n\ndef test_model_equality_dump(EqualityModel, InnerEqualityModel):\n    inner_model = InnerEqualityModel(iw=0)\n    assert inner_model != inner_model.model_dump()\n\n    model = EqualityModel(w=0, x=0, model=inner_model)\n    assert model != dict(model)\n    assert dict(model) != model.model_dump()  # Due to presence of inner model\n\n\ndef test_model_equality_fields_set(InnerEqualityModel):\n    m1 = InnerEqualityModel(iw=0)\n    m2 = InnerEqualityModel(iw=0, ix=0)\n    assert m1.model_fields_set != m2.model_fields_set\n    assert m1 == m2\n\n\ndef test_model_equality_private_attrs(InnerEqualityModel):\n    m = InnerEqualityModel(iw=0, ix=0)\n\n    m1 = m.model_copy()\n    m2 = m.model_copy()\n    m3 = m.model_copy()\n\n    m2._iy = 1\n    m3._iz = 1\n\n    models = [m1, m2, m3]\n    for i, first_model in enumerate(models):\n        for j, second_model in enumerate(models):\n            if i == j:\n                assert first_model == second_model\n            else:\n                assert first_model != second_model\n\n    m2_equal = m.model_copy()\n    m2_equal._iy = 1\n    assert m2 == m2_equal\n\n    m3_equal = m.model_copy()\n    m3_equal._iz = 1\n    assert m3 == m3_equal\n\n\ndef test_model_copy_extra():\n    class Model(BaseModel, extra='allow'):\n        x: int\n\n    m = Model(x=1, y=2)\n    assert m.model_dump() == {'x': 1, 'y': 2}\n    assert m.model_extra == {'y': 2}\n    m2 = m.model_copy()\n    assert m2.model_dump() == {'x': 1, 'y': 2}\n    assert m2.model_extra == {'y': 2}\n\n    m3 = m.model_copy(update={'x': 4, 'z': 3})\n    assert m3.model_dump() == {'x': 4, 'y': 2, 'z': 3}\n    assert m3.model_extra == {'y': 2, 'z': 3}\n\n    m4 = m.model_copy(update={'x': 4, 'z': 3})\n    assert m4.model_dump() == {'x': 4, 'y': 2, 'z': 3}\n    assert m4.model_extra == {'y': 2, 'z': 3}\n\n    m = Model(x=1, a=2)\n    m.__pydantic_extra__ = None\n    m5 = m.model_copy(update={'x': 4, 'b': 3})\n    assert m5.model_dump() == {'x': 4, 'b': 3}\n    assert m5.model_extra == {'b': 3}\n\n\ndef test_model_parametrized_name_not_generic():\n    class Model(BaseModel):\n        x: int\n\n    with pytest.raises(TypeError, match='Concrete names should only be generated for generic models.'):\n        Model.model_parametrized_name(())\n\n\ndef test_model_equality_generics():\n    T = TypeVar('T')\n\n    class GenericModel(BaseModel, Generic[T], frozen=True):\n        x: T\n\n    class ConcreteModel(BaseModel):\n        x: int\n\n    assert ConcreteModel(x=1) != GenericModel(x=1)\n    assert ConcreteModel(x=1) != GenericModel[Any](x=1)\n    assert ConcreteModel(x=1) != GenericModel[int](x=1)\n\n    assert GenericModel(x=1) != GenericModel(x=2)\n\n    S = TypeVar('S')\n    models = [\n        GenericModel(x=1),\n        GenericModel[S](x=1),\n        GenericModel[Any](x=1),\n        GenericModel[int](x=1),\n        GenericModel[float](x=1),\n    ]\n    for m1 in models:\n        for m2 in models:\n            # Test that it works with nesting as well\n            m3 = GenericModel[type(m1)](x=m1)\n            m4 = GenericModel[type(m2)](x=m2)\n            assert m1 == m2\n            assert m3 == m4\n            assert hash(m1) == hash(m2)\n            assert hash(m3) == hash(m4)\n\n\ndef test_model_validate_strict() -> None:\n    class LaxModel(BaseModel):\n        x: int\n\n        model_config = ConfigDict(strict=False)\n\n    class StrictModel(BaseModel):\n        x: int\n\n        model_config = ConfigDict(strict=True)\n\n    assert LaxModel.model_validate({'x': '1'}, strict=None) == LaxModel(x=1)\n    assert LaxModel.model_validate({'x': '1'}, strict=False) == LaxModel(x=1)\n    with pytest.raises(ValidationError) as exc_info:\n        LaxModel.model_validate({'x': '1'}, strict=True)\n    # there's no such thing on the model itself\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel.model_validate({'x': '1'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n    assert StrictModel.model_validate({'x': '1'}, strict=False) == StrictModel(x=1)\n    with pytest.raises(ValidationError) as exc_info:\n        LaxModel.model_validate({'x': '1'}, strict=True)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n\n@pytest.mark.xfail(\n    reason='strict=True in model_validate_json does not overwrite strict=False given in ConfigDict'\n    'See issue: https://github.com/pydantic/pydantic/issues/8930'\n)\ndef test_model_validate_list_strict() -> None:\n    # FIXME: This change must be implemented in pydantic-core. The argument strict=True\n    # in model_validate_json method is not overwriting the one set with ConfigDict(strict=False)\n    # for sequence like types. See: https://github.com/pydantic/pydantic/issues/8930\n\n    class LaxModel(BaseModel):\n        x: List[str]\n        model_config = ConfigDict(strict=False)\n\n    assert LaxModel.model_validate_json(json.dumps({'x': ('a', 'b', 'c')}), strict=None) == LaxModel(x=('a', 'b', 'c'))\n    assert LaxModel.model_validate_json(json.dumps({'x': ('a', 'b', 'c')}), strict=False) == LaxModel(x=('a', 'b', 'c'))\n    with pytest.raises(ValidationError) as exc_info:\n        LaxModel.model_validate_json(json.dumps({'x': ('a', 'b', 'c')}), strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'list_type', 'loc': ('x',), 'msg': 'Input should be a valid list', 'input': ('a', 'b', 'c')}\n    ]\n\n\ndef test_model_validate_json_strict() -> None:\n    class LaxModel(BaseModel):\n        x: int\n\n        model_config = ConfigDict(strict=False)\n\n    class StrictModel(BaseModel):\n        x: int\n\n        model_config = ConfigDict(strict=True)\n\n    assert LaxModel.model_validate_json(json.dumps({'x': '1'}), strict=None) == LaxModel(x=1)\n    assert LaxModel.model_validate_json(json.dumps({'x': '1'}), strict=False) == LaxModel(x=1)\n    with pytest.raises(ValidationError) as exc_info:\n        LaxModel.model_validate_json(json.dumps({'x': '1'}), strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel.model_validate_json(json.dumps({'x': '1'}), strict=None)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n    assert StrictModel.model_validate_json(json.dumps({'x': '1'}), strict=False) == StrictModel(x=1)\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel.model_validate_json(json.dumps({'x': '1'}), strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('x',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n\ndef test_validate_python_context() -> None:\n    contexts: List[Any] = [None, None, {'foo': 'bar'}]\n\n    class Model(BaseModel):\n        x: int\n\n        @field_validator('x')\n        def val_x(cls, v: int, info: ValidationInfo) -> int:\n            assert info.context == contexts.pop(0)\n            return v\n\n    Model.model_validate({'x': 1})\n    Model.model_validate({'x': 1}, context=None)\n    Model.model_validate({'x': 1}, context={'foo': 'bar'})\n    assert contexts == []\n\n\ndef test_validate_json_context() -> None:\n    contexts: List[Any] = [None, None, {'foo': 'bar'}]\n\n    class Model(BaseModel):\n        x: int\n\n        @field_validator('x')\n        def val_x(cls, v: int, info: ValidationInfo) -> int:\n            assert info.context == contexts.pop(0)\n            return v\n\n    Model.model_validate_json(json.dumps({'x': 1}))\n    Model.model_validate_json(json.dumps({'x': 1}), context=None)\n    Model.model_validate_json(json.dumps({'x': 1}), context={'foo': 'bar'})\n    assert contexts == []\n\n\ndef test_pydantic_init_subclass() -> None:\n    calls = []\n\n    class MyModel(BaseModel):\n        def __init_subclass__(cls, **kwargs):\n            super().__init_subclass__()  # can't pass kwargs to object.__init_subclass__, weirdly\n            calls.append((cls.__name__, '__init_subclass__', kwargs))\n\n        @classmethod\n        def __pydantic_init_subclass__(cls, **kwargs):\n            super().__pydantic_init_subclass__(**kwargs)\n            calls.append((cls.__name__, '__pydantic_init_subclass__', kwargs))\n\n    class MySubModel(MyModel, a=1):\n        pass\n\n    assert calls == [\n        ('MySubModel', '__init_subclass__', {'a': 1}),\n        ('MySubModel', '__pydantic_init_subclass__', {'a': 1}),\n    ]\n\n\ndef test_model_validate_with_context():\n    class InnerModel(BaseModel):\n        x: int\n\n        @field_validator('x')\n        def validate(cls, value, info):\n            return value * info.context.get('multiplier', 1)\n\n    class OuterModel(BaseModel):\n        inner: InnerModel\n\n    assert OuterModel.model_validate({'inner': {'x': 2}}, context={'multiplier': 1}).inner.x == 2\n    assert OuterModel.model_validate({'inner': {'x': 2}}, context={'multiplier': 2}).inner.x == 4\n    assert OuterModel.model_validate({'inner': {'x': 2}}, context={'multiplier': 3}).inner.x == 6\n\n\ndef test_extra_equality():\n    class MyModel(BaseModel, extra='allow'):\n        pass\n\n    assert MyModel(x=1) != MyModel()\n\n\ndef test_equality_delegation():\n    from unittest.mock import ANY\n\n    class MyModel(BaseModel):\n        foo: str\n\n    assert MyModel(foo='bar') == ANY\n\n\ndef test_recursion_loop_error():\n    class Model(BaseModel):\n        x: List['Model']\n\n    data = {'x': []}\n    data['x'].append(data)\n    with pytest.raises(ValidationError) as exc_info:\n        Model(**data)\n    assert repr(exc_info.value.errors(include_url=False)[0]) == (\n        \"{'type': 'recursion_loop', 'loc': ('x', 0, 'x', 0), 'msg': \"\n        \"'Recursion error - cyclic reference detected', 'input': {'x': [{...}]}}\"\n    )\n\n\ndef test_protected_namespace_default():\n    with pytest.warns(UserWarning, match='Field \"model_prefixed_field\" has conflict with protected namespace \"model_\"'):\n\n        class Model(BaseModel):\n            model_prefixed_field: str\n\n\ndef test_protected_namespace_real_conflict():\n    with pytest.raises(\n        NameError, match=r'Field \"model_validate\" conflicts with member .* of protected namespace \"model_\"\\.'\n    ):\n\n        class Model(BaseModel):\n            model_validate: str\n\n\ndef test_custom_protected_namespace():\n    with pytest.warns(UserWarning, match='Field \"test_field\" has conflict with protected namespace \"test_\"'):\n\n        class Model(BaseModel):\n            # this field won't raise error because we changed the default value for the\n            # `protected_namespaces` config.\n            model_prefixed_field: str\n            test_field: str\n\n            model_config = ConfigDict(protected_namespaces=('test_',))\n\n\ndef test_multiple_protected_namespace():\n    with pytest.warns(\n        UserWarning, match='Field \"also_protect_field\" has conflict with protected namespace \"also_protect_\"'\n    ):\n\n        class Model(BaseModel):\n            also_protect_field: str\n\n            model_config = ConfigDict(protected_namespaces=('protect_me_', 'also_protect_'))\n\n\ndef test_model_get_core_schema() -> None:\n    class Model(BaseModel):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            schema = handler(int)\n            schema.pop('metadata', None)  # we don't care about this in tests\n            assert schema == {'type': 'int'}\n            schema = handler.generate_schema(int)\n            schema.pop('metadata', None)  # we don't care about this in tests\n            assert schema == {'type': 'int'}\n            return handler(source_type)\n\n    Model()\n\n\ndef test_nested_types_ignored():\n    from pydantic import BaseModel\n\n    class NonNestedType:\n        pass\n\n    # Defining a nested type does not error\n    class GoodModel(BaseModel):\n        class NestedType:\n            pass\n\n        # You can still store such types on the class by annotating as a ClassVar\n        MyType: ClassVar[Type[Any]] = NonNestedType\n\n        # For documentation: you _can_ give multiple names to a nested type and it won't error:\n        # It might be better if it did, but this seems to be rare enough that I'm not concerned\n        x = NestedType\n\n    assert GoodModel.MyType is NonNestedType\n    assert GoodModel.x is GoodModel.NestedType\n\n    with pytest.raises(PydanticUserError, match='A non-annotated attribute was detected'):\n\n        class BadModel(BaseModel):\n            x = NonNestedType\n\n\ndef test_validate_python_from_attributes() -> None:\n    class Model(BaseModel):\n        x: int\n\n    class ModelFromAttributesTrue(Model):\n        model_config = ConfigDict(from_attributes=True)\n\n    class ModelFromAttributesFalse(Model):\n        model_config = ConfigDict(from_attributes=False)\n\n    @dataclass\n    class UnrelatedClass:\n        x: int = 1\n\n    input = UnrelatedClass(1)\n\n    for from_attributes in (False, None):\n        with pytest.raises(ValidationError) as exc_info:\n            Model.model_validate(UnrelatedClass(), from_attributes=from_attributes)\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'type': 'model_type',\n                'loc': (),\n                'msg': 'Input should be a valid dictionary or instance of Model',\n                'input': input,\n                'ctx': {'class_name': 'Model'},\n            }\n        ]\n\n    res = Model.model_validate(UnrelatedClass(), from_attributes=True)\n    assert res == Model(x=1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        ModelFromAttributesTrue.model_validate(UnrelatedClass(), from_attributes=False)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': (),\n            'msg': 'Input should be a valid dictionary or instance of ModelFromAttributesTrue',\n            'input': input,\n            'ctx': {'class_name': 'ModelFromAttributesTrue'},\n        }\n    ]\n\n    for from_attributes in (True, None):\n        res = ModelFromAttributesTrue.model_validate(UnrelatedClass(), from_attributes=from_attributes)\n        assert res == ModelFromAttributesTrue(x=1)\n\n    for from_attributes in (False, None):\n        with pytest.raises(ValidationError) as exc_info:\n            ModelFromAttributesFalse.model_validate(UnrelatedClass(), from_attributes=from_attributes)\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'type': 'model_type',\n                'loc': (),\n                'msg': 'Input should be a valid dictionary or instance of ModelFromAttributesFalse',\n                'input': input,\n                'ctx': {'class_name': 'ModelFromAttributesFalse'},\n            }\n        ]\n\n    res = ModelFromAttributesFalse.model_validate(UnrelatedClass(), from_attributes=True)\n    assert res == ModelFromAttributesFalse(x=1)\n\n\n@pytest.mark.parametrize(\n    'field_type,input_value,expected,raises_match,strict',\n    [\n        (bool, 'true', True, None, False),\n        (bool, 'true', True, None, True),\n        (bool, 'false', False, None, False),\n        (bool, 'e', ValidationError, 'type=bool_parsing', False),\n        (int, '1', 1, None, False),\n        (int, '1', 1, None, True),\n        (int, 'xxx', ValidationError, 'type=int_parsing', True),\n        (float, '1.1', 1.1, None, False),\n        (float, '1.10', 1.1, None, False),\n        (float, '1.1', 1.1, None, True),\n        (float, '1.10', 1.1, None, True),\n        (date, '2017-01-01', date(2017, 1, 1), None, False),\n        (date, '2017-01-01', date(2017, 1, 1), None, True),\n        (date, '2017-01-01T12:13:14.567', ValidationError, 'type=date_from_datetime_inexact', False),\n        (date, '2017-01-01T12:13:14.567', ValidationError, 'type=date_parsing', True),\n        (date, '2017-01-01T00:00:00', date(2017, 1, 1), None, False),\n        (date, '2017-01-01T00:00:00', ValidationError, 'type=date_parsing', True),\n        (datetime, '2017-01-01T12:13:14.567', datetime(2017, 1, 1, 12, 13, 14, 567_000), None, False),\n        (datetime, '2017-01-01T12:13:14.567', datetime(2017, 1, 1, 12, 13, 14, 567_000), None, True),\n    ],\n    ids=repr,\n)\ndef test_model_validate_strings(field_type, input_value, expected, raises_match, strict):\n    class Model(BaseModel):\n        x: field_type\n\n    if raises_match is not None:\n        with pytest.raises(expected, match=raises_match):\n            Model.model_validate_strings({'x': input_value}, strict=strict)\n    else:\n        assert Model.model_validate_strings({'x': input_value}, strict=strict).x == expected\n\n\n@pytest.mark.parametrize('strict', [True, False])\ndef test_model_validate_strings_dict(strict):\n    class Model(BaseModel):\n        x: Dict[int, date]\n\n    assert Model.model_validate_strings({'x': {'1': '2017-01-01', '2': '2017-01-02'}}, strict=strict).x == {\n        1: date(2017, 1, 1),\n        2: date(2017, 1, 2),\n    }\n\n\ndef test_model_signature_annotated() -> None:\n    class Model(BaseModel):\n        x: Annotated[int, 123]\n\n    # we used to accidentally convert `__metadata__` to a list\n    # which caused things like `typing.get_args()` to fail\n    assert Model.__signature__.parameters['x'].annotation.__metadata__ == (123,)\n\n\ndef test_get_core_schema_unpacks_refs_for_source_type() -> None:\n    # use a list to track since we end up calling `__get_pydantic_core_schema__` multiple times for models\n    # e.g. InnerModel.__get_pydantic_core_schema__ gets called:\n    # 1. When InnerModel is defined\n    # 2. When OuterModel is defined\n    # 3. When we use the TypeAdapter\n    received_schemas: dict[str, list[str]] = defaultdict(list)\n\n    @dataclass\n    class Marker:\n        name: str\n\n        def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            schema = handler(source_type)\n            received_schemas[self.name].append(schema['type'])\n            return schema\n\n    class InnerModel(BaseModel):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            schema = handler(source_type)\n            received_schemas['InnerModel'].append(schema['type'])\n            schema['metadata'] = schema.get('metadata', {})\n            schema['metadata']['foo'] = 'inner was here!'\n            return deepcopy(schema)\n\n    class OuterModel(BaseModel):\n        inner: Annotated[InnerModel, Marker('Marker(\"inner\")')]\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            schema = handler(source_type)\n            received_schemas['OuterModel'].append(schema['type'])\n            return schema\n\n    ta = TypeAdapter(Annotated[OuterModel, Marker('Marker(\"outer\")')])\n\n    # super hacky check but it works in all cases and avoids a complex and fragile iteration over CoreSchema\n    # the point here is to verify that `__get_pydantic_core_schema__`\n    assert 'inner was here' in str(ta.core_schema)\n\n    assert received_schemas == {\n        'InnerModel': ['model', 'model', 'model'],\n        'Marker(\"inner\")': ['definition-ref', 'definition-ref'],\n        'OuterModel': ['model', 'model'],\n        'Marker(\"outer\")': ['definition-ref'],\n    }\n\n\ndef test_get_core_schema_return_new_ref() -> None:\n    class InnerModel(BaseModel):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            schema = handler(source_type)\n            schema = deepcopy(schema)\n            schema['metadata'] = schema.get('metadata', {})\n            schema['metadata']['foo'] = 'inner was here!'\n            return deepcopy(schema)\n\n    class OuterModel(BaseModel):\n        inner: InnerModel\n        x: int = 1\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            schema = handler(source_type)\n\n            def set_x(m: 'OuterModel') -> 'OuterModel':\n                m.x += 1\n                return m\n\n            return core_schema.no_info_after_validator_function(set_x, schema, ref=schema.pop('ref'))\n\n    cs = OuterModel.__pydantic_core_schema__\n    # super hacky check but it works in all cases and avoids a complex and fragile iteration over CoreSchema\n    # the point here is to verify that `__get_pydantic_core_schema__`\n    assert 'inner was here' in str(cs)\n\n    assert OuterModel(inner=InnerModel()).x == 2\n\n\ndef test_resolve_def_schema_from_core_schema() -> None:\n    class Inner(BaseModel):\n        x: int\n\n    class Marker:\n        def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            schema = handler(source_type)\n            resolved = handler.resolve_ref_schema(schema)\n            assert resolved['type'] == 'model'\n            assert resolved['cls'] is Inner\n\n            def modify_inner(v: Inner) -> Inner:\n                v.x += 1\n                return v\n\n            return core_schema.no_info_after_validator_function(modify_inner, schema)\n\n    class Outer(BaseModel):\n        inner: Annotated[Inner, Marker()]\n\n    assert Outer.model_validate({'inner': {'x': 1}}).inner.x == 2\n\n\ndef test_extra_validator_scalar() -> None:\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n\n    class Child(Model):\n        __pydantic_extra__: Dict[str, int]\n\n    m = Child(a='1')\n    assert m.__pydantic_extra__ == {'a': 1}\n\n    # insert_assert(Child.model_json_schema())\n    assert Child.model_json_schema() == {\n        'additionalProperties': {'type': 'integer'},\n        'properties': {},\n        'title': 'Child',\n        'type': 'object',\n    }\n\n\ndef test_extra_validator_field() -> None:\n    class Model(BaseModel, extra='allow'):\n        # use Field(init=False) to ensure this is not treated as a field by dataclass_transform\n        __pydantic_extra__: Dict[str, int] = Field(init=False)\n\n    m = Model(a='1')\n    assert m.__pydantic_extra__ == {'a': 1}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='a')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n    # insert_assert(Child.model_json_schema())\n    assert Model.model_json_schema() == {\n        'additionalProperties': {'type': 'integer'},\n        'properties': {},\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_extra_validator_named() -> None:\n    class Foo(BaseModel):\n        x: int\n\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        __pydantic_extra__: 'dict[str, Foo]'\n\n    class Child(Model):\n        y: int\n\n    m = Child(a={'x': '1'}, y=2)\n    assert m.__pydantic_extra__ == {'a': Foo(x=1)}\n\n    # insert_assert(Child.model_json_schema())\n    assert Child.model_json_schema() == {\n        '$defs': {\n            'Foo': {\n                'properties': {'x': {'title': 'X', 'type': 'integer'}},\n                'required': ['x'],\n                'title': 'Foo',\n                'type': 'object',\n            }\n        },\n        'additionalProperties': {'$ref': '#/$defs/Foo'},\n        'properties': {'y': {'title': 'Y', 'type': 'integer'}},\n        'required': ['y'],\n        'title': 'Child',\n        'type': 'object',\n    }\n\n\ndef test_super_getattr_extra():\n    class Model(BaseModel):\n        model_config = {'extra': 'allow'}\n\n        def __getattr__(self, item):\n            if item == 'test':\n                return 'success'\n            return super().__getattr__(item)\n\n    m = Model(x=1)\n    assert m.x == 1\n    with pytest.raises(AttributeError):\n        m.y\n    assert m.test == 'success'\n\n\ndef test_super_getattr_private():\n    class Model(BaseModel):\n        _x: int = PrivateAttr()\n\n        def __getattr__(self, item):\n            if item == 'test':\n                return 'success'\n            else:\n                return super().__getattr__(item)\n\n    m = Model()\n    m._x = 1\n    assert m._x == 1\n    with pytest.raises(AttributeError):\n        m._y\n    assert m.test == 'success'\n\n\ndef test_super_delattr_extra():\n    test_calls = []\n\n    class Model(BaseModel):\n        model_config = {'extra': 'allow'}\n\n        def __delattr__(self, item):\n            if item == 'test':\n                test_calls.append('success')\n            else:\n                super().__delattr__(item)\n\n    m = Model(x=1)\n    assert m.x == 1\n    del m.x\n    with pytest.raises(AttributeError):\n        m._x\n    assert test_calls == []\n    del m.test\n    assert test_calls == ['success']\n\n\ndef test_super_delattr_private():\n    test_calls = []\n\n    class Model(BaseModel):\n        _x: int = PrivateAttr()\n\n        def __delattr__(self, item):\n            if item == 'test':\n                test_calls.append('success')\n            else:\n                super().__delattr__(item)\n\n    m = Model()\n    m._x = 1\n    assert m._x == 1\n    del m._x\n    with pytest.raises(AttributeError):\n        m._x\n    assert test_calls == []\n    del m.test\n    assert test_calls == ['success']\n\n\ndef test_arbitrary_types_not_a_type() -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/6477\"\"\"\n\n    class Foo:\n        pass\n\n    class Bar:\n        pass\n\n    with pytest.warns(UserWarning, match='is not a Python type'):\n        ta = TypeAdapter(Foo(), config=ConfigDict(arbitrary_types_allowed=True))\n\n    bar = Bar()\n    assert ta.validate_python(bar) is bar\n\n\n@pytest.mark.parametrize('is_dataclass', [False, True])\ndef test_deferred_core_schema(is_dataclass: bool) -> None:\n    if is_dataclass:\n\n        @pydantic_dataclass\n        class Foo:\n            x: 'Bar'\n    else:\n\n        class Foo(BaseModel):\n            x: 'Bar'\n\n    assert isinstance(Foo.__pydantic_core_schema__, MockCoreSchema)\n    with pytest.raises(PydanticUserError, match='`Foo` is not fully defined'):\n        Foo.__pydantic_core_schema__['type']\n\n    class Bar(BaseModel):\n        pass\n\n    assert Foo.__pydantic_core_schema__['type'] == ('dataclass' if is_dataclass else 'model')\n    assert isinstance(Foo.__pydantic_core_schema__, dict)\n\n\ndef test_help(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nimport pydoc\n\nfrom pydantic import BaseModel\n\nclass Model(BaseModel):\n    x: int\n\n\nhelp_result_string = pydoc.render_doc(Model)\n\"\"\"\n    )\n    assert 'class Model' in module.help_result_string\n\n\ndef test_cannot_use_leading_underscore_field_names():\n    with pytest.raises(\n        NameError, match=\"Fields must not use names with leading underscores; e.g., use 'x' instead of '_x'\"\n    ):\n\n        class Model1(BaseModel):\n            _x: int = Field(alias='x')\n\n    with pytest.raises(\n        NameError, match=\"Fields must not use names with leading underscores; e.g., use 'x__' instead of '__x__'\"\n    ):\n\n        class Model2(BaseModel):\n            __x__: int = Field()\n\n    with pytest.raises(\n        NameError, match=\"Fields must not use names with leading underscores; e.g., use 'my_field' instead of '___'\"\n    ):\n\n        class Model3(BaseModel):\n            ___: int = Field(default=1)\n\n\ndef test_schema_generator_customize_type() -> None:\n    class LaxStrGenerator(GenerateSchema):\n        def str_schema(self) -> CoreSchema:\n            return core_schema.no_info_plain_validator_function(str)\n\n    class Model(BaseModel):\n        x: str\n        model_config = ConfigDict(schema_generator=LaxStrGenerator)\n\n    assert Model(x=1).x == '1'\n\n\ndef test_schema_generator_customize_type_constraints() -> None:\n    class LaxStrGenerator(GenerateSchema):\n        def str_schema(self) -> CoreSchema:\n            return core_schema.no_info_plain_validator_function(str)\n\n    class Model(BaseModel):\n        x: Annotated[str, Field(pattern='^\\\\d+$')]\n        y: Annotated[float, Field(gt=0)]\n        z: Annotated[List[int], Field(min_length=1)]\n        model_config = ConfigDict(schema_generator=LaxStrGenerator)\n\n    # insert_assert(Model(x='123', y=1, z=[-1]).model_dump())\n    assert Model(x='123', y=1, z=[-1]).model_dump() == {'x': '123', 'y': 1.0, 'z': [-1]}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x='abc', y=-1, z=[])\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_pattern_mismatch',\n            'loc': ('x',),\n            'msg': \"String should match pattern '^\\\\d+$'\",\n            'input': 'abc',\n            'ctx': {'pattern': '^\\\\d+$'},\n        },\n        {\n            'type': 'greater_than',\n            'loc': ('y',),\n            'msg': 'Input should be greater than 0',\n            'input': -1,\n            'ctx': {'gt': 0.0},\n        },\n        {\n            'type': 'too_short',\n            'loc': ('z',),\n            'msg': 'List should have at least 1 item after validation, not 0',\n            'input': [],\n            'ctx': {'field_type': 'List', 'min_length': 1, 'actual_length': 0},\n        },\n    ]\n\n\ndef test_schema_generator_customize_type_constraints_order() -> None:\n    class Model(BaseModel):\n        # whitespace will be stripped first, then max length will be checked, should pass on ' 1 '\n        x: Annotated[str, AfterValidator(lambda x: x.strip()), StringConstraints(max_length=1)]\n        # max length will be checked first, then whitespace will be stripped, should fail on ' 1 '\n        y: Annotated[str, StringConstraints(max_length=1), AfterValidator(lambda x: x.strip())]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=' 1 ', y=' 1 ')\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_too_long',\n            'loc': ('y',),\n            'msg': 'String should have at most 1 character',\n            'input': ' 1 ',\n            'ctx': {'max_length': 1},\n        }\n    ]\n\n\ndef test_shadow_attribute() -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/7108\"\"\"\n\n    class Model(BaseModel):\n        foo: str\n\n        @classmethod\n        def __pydantic_init_subclass__(cls, **kwargs: Any):\n            super().__pydantic_init_subclass__(**kwargs)\n            for key in cls.model_fields.keys():\n                setattr(cls, key, getattr(cls, key, '') + ' edited!')\n\n    class One(Model):\n        foo: str = 'abc'\n\n    with pytest.warns(UserWarning, match=r'\"foo\" in \".*Two\" shadows an attribute in parent \".*One\"'):\n\n        class Two(One):\n            foo: str\n\n    with pytest.warns(UserWarning, match=r'\"foo\" in \".*Three\" shadows an attribute in parent \".*One\"'):\n\n        class Three(One):\n            foo: str = 'xyz'\n\n    # unlike dataclasses BaseModel does not preserve the value of defaults\n    # so when we access the attribute in `Model.__pydantic_init_subclass__` there is no default\n    # and hence we append `edited!` to an empty string\n    # we've talked about changing this but this is the current behavior as of this test\n    assert getattr(Model, 'foo', None) is None\n    assert getattr(One, 'foo', None) == ' edited!'\n    assert getattr(Two, 'foo', None) == ' edited! edited!'\n    assert getattr(Three, 'foo', None) == ' edited! edited!'\n\n\ndef test_shadow_attribute_warn_for_redefined_fields() -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/9107\"\"\"\n\n    # A simple class which defines a field\n    class Parent:\n        foo: bool = False\n\n    # When inheriting from the parent class, as long as the field is not defined at all, there should be no warning\n    # about shadowed fields.\n    with warnings.catch_warnings(record=True) as captured_warnings:\n        # Start capturing all warnings\n        warnings.simplefilter('always')\n\n        class ChildWithoutRedefinedField(BaseModel, Parent):\n            pass\n\n        # Check that no warnings were captured\n        assert len(captured_warnings) == 0\n\n    # But when inheriting from the parent class and a parent field is redefined, a warning should be raised about\n    # shadowed fields irrespective of whether it is defined with a type that is still compatible or narrower, or\n    # with a different default that is still compatible with the type definition.\n    with pytest.warns(\n        UserWarning,\n        match=r'\"foo\" in \".*ChildWithRedefinedField\" shadows an attribute in parent \".*Parent\"',\n    ):\n\n        class ChildWithRedefinedField(BaseModel, Parent):\n            foo: bool = True\n\n\ndef test_eval_type_backport():\n    class Model(BaseModel):\n        foo: 'list[int | str]'\n\n    assert Model(foo=[1, '2']).model_dump() == {'foo': [1, '2']}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo='not a list')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'list_type',\n            'loc': ('foo',),\n            'msg': 'Input should be a valid list',\n            'input': 'not a list',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=[{'not a str or int'}])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_type',\n            'loc': ('foo', 0, 'int'),\n            'msg': 'Input should be a valid integer',\n            'input': {'not a str or int'},\n        },\n        {\n            'type': 'string_type',\n            'loc': ('foo', 0, 'str'),\n            'msg': 'Input should be a valid string',\n            'input': {'not a str or int'},\n        },\n    ]\n\n\ndef test_inherited_class_vars(create_module):\n    @create_module\n    def module():\n        import typing\n\n        from pydantic import BaseModel\n\n        class Base(BaseModel):\n            CONST1: 'typing.ClassVar[str]' = 'a'\n            CONST2: 'ClassVar[str]' = 'b'\n\n    class Child(module.Base):\n        pass\n\n    assert Child.CONST1 == 'a'\n    assert Child.CONST2 == 'b'\n\n\ndef test_schema_valid_for_inner_generic() -> None:\n    T = TypeVar('T')\n\n    class Inner(BaseModel, Generic[T]):\n        x: T\n\n    class Outer(BaseModel):\n        inner: Inner[int]\n\n    assert Outer(inner={'x': 1}).inner.x == 1\n    # confirming that the typevars are substituted in the outer model schema\n    assert Outer.__pydantic_core_schema__['schema']['fields']['inner']['schema']['cls'] == Inner[int]\n    assert (\n        Outer.__pydantic_core_schema__['schema']['fields']['inner']['schema']['schema']['fields']['x']['schema']['type']\n        == 'int'\n    )\n\n\ndef test_validation_works_for_cyclical_forward_refs() -> None:\n    class X(BaseModel):\n        y: Union['Y', None]\n\n    class Y(BaseModel):\n        x: Union[X, None]\n\n    assert Y(x={'y': None}).x.y is None\n\n\ndef test_model_construct_with_model_post_init_and_model_copy() -> None:\n    class Model(BaseModel):\n        id: int\n\n        def model_post_init(self, context: Any) -> None:\n            super().model_post_init(context)\n\n    m = Model.model_construct(id=1)\n    copy = m.model_copy(deep=True)\n\n    assert m == copy\n    assert id(m) != id(copy)\n", "tests/test_forward_ref.py": "import dataclasses\nimport re\nimport sys\nimport typing\nfrom typing import Any, Optional, Tuple\n\nimport pytest\n\nfrom pydantic import BaseModel, PydanticUserError, ValidationError\n\n\ndef test_postponed_annotations(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\nclass Model(BaseModel):\n    a: int\n\"\"\"\n    )\n    m = module.Model(a='123')\n    assert m.model_dump() == {'a': 123}\n\n\ndef test_postponed_annotations_auto_model_rebuild(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\nclass Model(BaseModel):\n    a: Model\n\"\"\"\n    )\n    assert module.Model.model_fields['a'].annotation.__name__ == 'Model'\n\n\ndef test_forward_ref_auto_update_no_model(create_module):\n    @create_module\n    def module():\n        from typing import Optional\n\n        import pytest\n\n        from pydantic import BaseModel, PydanticUserError\n\n        class Foo(BaseModel):\n            a: Optional['Bar'] = None\n\n        with pytest.raises(PydanticUserError, match='`Foo` is not fully defined; you should define `Bar`,'):\n            Foo(a={'b': {'a': {}}})\n\n        class Bar(BaseModel):\n            b: 'Foo'\n\n    assert module.Bar.__pydantic_complete__ is True\n    assert repr(module.Bar.model_fields['b']) == 'FieldInfo(annotation=Foo, required=True)'\n\n    # Bar should be complete and ready to use\n    b = module.Bar(b={'a': {'b': {}}})\n    assert b.model_dump() == {'b': {'a': {'b': {'a': None}}}}\n\n    # model_fields is complete on Foo\n    assert repr(module.Foo.model_fields['a']) == (\n        'FieldInfo(annotation=Union[Bar, NoneType], required=False, default=None)'\n    )\n\n    assert module.Foo.__pydantic_complete__ is False\n    # Foo gets auto-rebuilt during the first attempt at validation\n    f = module.Foo(a={'b': {'a': {'b': {'a': None}}}})\n    assert module.Foo.__pydantic_complete__ is True\n    assert f.model_dump() == {'a': {'b': {'a': {'b': {'a': None}}}}}\n\n\ndef test_forward_ref_one_of_fields_not_defined(create_module):\n    @create_module\n    def module():\n        from pydantic import BaseModel\n\n        class Foo(BaseModel):\n            foo: 'Foo'\n            bar: 'Bar'\n\n    assert {k: repr(v) for k, v in module.Foo.model_fields.items()} == {\n        'foo': 'FieldInfo(annotation=Foo, required=True)',\n        'bar': \"FieldInfo(annotation=ForwardRef('Bar'), required=True)\",\n    }\n\n\ndef test_basic_forward_ref(create_module):\n    @create_module\n    def module():\n        from typing import ForwardRef, Optional\n\n        from pydantic import BaseModel\n\n        class Foo(BaseModel):\n            a: int\n\n        FooRef = ForwardRef('Foo')\n\n        class Bar(BaseModel):\n            b: Optional[FooRef] = None\n\n    assert module.Bar().model_dump() == {'b': None}\n    assert module.Bar(b={'a': '123'}).model_dump() == {'b': {'a': 123}}\n\n\ndef test_self_forward_ref_module(create_module):\n    @create_module\n    def module():\n        from typing import ForwardRef, Optional\n\n        from pydantic import BaseModel\n\n        FooRef = ForwardRef('Foo')\n\n        class Foo(BaseModel):\n            a: int = 123\n            b: Optional[FooRef] = None\n\n    assert module.Foo().model_dump() == {'a': 123, 'b': None}\n    assert module.Foo(b={'a': '321'}).model_dump() == {'a': 123, 'b': {'a': 321, 'b': None}}\n\n\ndef test_self_forward_ref_collection(create_module):\n    @create_module\n    def module():\n        from typing import Dict, List\n\n        from pydantic import BaseModel\n\n        class Foo(BaseModel):\n            a: int = 123\n            b: 'Foo' = None\n            c: 'List[Foo]' = []\n            d: 'Dict[str, Foo]' = {}\n\n    assert module.Foo().model_dump() == {'a': 123, 'b': None, 'c': [], 'd': {}}\n    assert module.Foo(b={'a': '321'}, c=[{'a': 234}], d={'bar': {'a': 345}}).model_dump() == {\n        'a': 123,\n        'b': {'a': 321, 'b': None, 'c': [], 'd': {}},\n        'c': [{'a': 234, 'b': None, 'c': [], 'd': {}}],\n        'd': {'bar': {'a': 345, 'b': None, 'c': [], 'd': {}}},\n    }\n\n    with pytest.raises(ValidationError) as exc_info:\n        module.Foo(b={'a': '321'}, c=[{'b': 234}], d={'bar': {'a': 345}})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': ('c', 0, 'b'),\n            'msg': 'Input should be a valid dictionary or instance of Foo',\n            'input': 234,\n            'ctx': {'class_name': 'Foo'},\n        }\n    ]\n\n    assert repr(module.Foo.model_fields['a']) == 'FieldInfo(annotation=int, required=False, default=123)'\n    assert repr(module.Foo.model_fields['b']) == 'FieldInfo(annotation=Foo, required=False, default=None)'\n    if sys.version_info < (3, 10):\n        return\n    assert repr(module.Foo.model_fields['c']) == ('FieldInfo(annotation=List[Foo], required=False, ' 'default=[])')\n    assert repr(module.Foo.model_fields['d']) == ('FieldInfo(annotation=Dict[str, Foo], required=False, default={})')\n\n\ndef test_self_forward_ref_local(create_module):\n    @create_module\n    def module():\n        from typing import ForwardRef\n\n        from pydantic import BaseModel\n\n        def main():\n            Foo = ForwardRef('Foo')\n\n            class Foo(BaseModel):\n                a: int = 123\n                b: Foo = None\n\n            return Foo\n\n    Foo = module.main()\n    assert Foo().model_dump() == {'a': 123, 'b': None}\n    assert Foo(b={'a': '321'}).model_dump() == {'a': 123, 'b': {'a': 321, 'b': None}}\n\n\ndef test_forward_ref_dataclass(create_module):\n    @create_module\n    def module():\n        from typing import Optional\n\n        from pydantic.dataclasses import dataclass\n\n        @dataclass\n        class MyDataclass:\n            a: int\n            b: Optional['MyDataclass'] = None\n\n    dc = module.MyDataclass(a=1, b={'a': 2, 'b': {'a': 3}})\n    assert dataclasses.asdict(dc) == {'a': 1, 'b': {'a': 2, 'b': {'a': 3, 'b': None}}}\n\n\ndef test_forward_ref_sub_types(create_module):\n    @create_module\n    def module():\n        from typing import ForwardRef, Union\n\n        from pydantic import BaseModel\n\n        class Leaf(BaseModel):\n            a: str\n\n        TreeType = Union[ForwardRef('Node'), Leaf]\n\n        class Node(BaseModel):\n            value: int\n            left: TreeType\n            right: TreeType\n\n    Node = module.Node\n    Leaf = module.Leaf\n    data = {'value': 3, 'left': {'a': 'foo'}, 'right': {'value': 5, 'left': {'a': 'bar'}, 'right': {'a': 'buzz'}}}\n\n    node = Node(**data)\n    assert isinstance(node.left, Leaf)\n    assert isinstance(node.right, Node)\n\n\ndef test_forward_ref_nested_sub_types(create_module):\n    @create_module\n    def module():\n        from typing import ForwardRef, Tuple, Union\n\n        from pydantic import BaseModel\n\n        class Leaf(BaseModel):\n            a: str\n\n        TreeType = Union[Union[Tuple[ForwardRef('Node'), str], int], Leaf]\n\n        class Node(BaseModel):\n            value: int\n            left: TreeType\n            right: TreeType\n\n    Node = module.Node\n    Leaf = module.Leaf\n    data = {\n        'value': 3,\n        'left': {'a': 'foo'},\n        'right': [{'value': 5, 'left': {'a': 'bar'}, 'right': {'a': 'buzz'}}, 'test'],\n    }\n\n    node = Node(**data)\n    assert isinstance(node.left, Leaf)\n    assert isinstance(node.right[0], Node)\n\n\ndef test_self_reference_json_schema(create_module):\n    @create_module\n    def module():\n        from typing import List\n\n        from pydantic import BaseModel\n\n        class Account(BaseModel):\n            name: str\n            subaccounts: List['Account'] = []\n\n    Account = module.Account\n    assert Account.model_json_schema() == {\n        'allOf': [{'$ref': '#/$defs/Account'}],\n        '$defs': {\n            'Account': {\n                'title': 'Account',\n                'type': 'object',\n                'properties': {\n                    'name': {'title': 'Name', 'type': 'string'},\n                    'subaccounts': {\n                        'title': 'Subaccounts',\n                        'default': [],\n                        'type': 'array',\n                        'items': {'$ref': '#/$defs/Account'},\n                    },\n                },\n                'required': ['name'],\n            }\n        },\n    }\n\n\ndef test_self_reference_json_schema_with_future_annotations(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\nclass Account(BaseModel):\n  name: str\n  subaccounts: list[Account] = []\n    \"\"\"\n    )\n    Account = module.Account\n    assert Account.model_json_schema() == {\n        'allOf': [{'$ref': '#/$defs/Account'}],\n        '$defs': {\n            'Account': {\n                'title': 'Account',\n                'type': 'object',\n                'properties': {\n                    'name': {'title': 'Name', 'type': 'string'},\n                    'subaccounts': {\n                        'title': 'Subaccounts',\n                        'default': [],\n                        'type': 'array',\n                        'items': {'$ref': '#/$defs/Account'},\n                    },\n                },\n                'required': ['name'],\n            }\n        },\n    }\n\n\ndef test_circular_reference_json_schema(create_module):\n    @create_module\n    def module():\n        from typing import List\n\n        from pydantic import BaseModel\n\n        class Owner(BaseModel):\n            account: 'Account'\n\n        class Account(BaseModel):\n            name: str\n            owner: 'Owner'\n            subaccounts: List['Account'] = []\n\n    Account = module.Account\n    assert Account.model_json_schema() == {\n        'allOf': [{'$ref': '#/$defs/Account'}],\n        '$defs': {\n            'Account': {\n                'title': 'Account',\n                'type': 'object',\n                'properties': {\n                    'name': {'title': 'Name', 'type': 'string'},\n                    'owner': {'$ref': '#/$defs/Owner'},\n                    'subaccounts': {\n                        'title': 'Subaccounts',\n                        'default': [],\n                        'type': 'array',\n                        'items': {'$ref': '#/$defs/Account'},\n                    },\n                },\n                'required': ['name', 'owner'],\n            },\n            'Owner': {\n                'title': 'Owner',\n                'type': 'object',\n                'properties': {'account': {'$ref': '#/$defs/Account'}},\n                'required': ['account'],\n            },\n        },\n    }\n\n\ndef test_circular_reference_json_schema_with_future_annotations(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\nclass Owner(BaseModel):\n  account: Account\n\nclass Account(BaseModel):\n  name: str\n  owner: Owner\n  subaccounts: list[Account] = []\n\n    \"\"\"\n    )\n    Account = module.Account\n    assert Account.model_json_schema() == {\n        'allOf': [{'$ref': '#/$defs/Account'}],\n        '$defs': {\n            'Account': {\n                'title': 'Account',\n                'type': 'object',\n                'properties': {\n                    'name': {'title': 'Name', 'type': 'string'},\n                    'owner': {'$ref': '#/$defs/Owner'},\n                    'subaccounts': {\n                        'title': 'Subaccounts',\n                        'default': [],\n                        'type': 'array',\n                        'items': {'$ref': '#/$defs/Account'},\n                    },\n                },\n                'required': ['name', 'owner'],\n            },\n            'Owner': {\n                'title': 'Owner',\n                'type': 'object',\n                'properties': {'account': {'$ref': '#/$defs/Account'}},\n                'required': ['account'],\n            },\n        },\n    }\n\n\ndef test_forward_ref_with_field(create_module):\n    @create_module\n    def module():\n        from typing import ForwardRef, List\n\n        import pytest\n\n        from pydantic import BaseModel, Field\n\n        Foo = ForwardRef('Foo')\n\n        with pytest.raises(TypeError, match=r'The following constraints cannot be applied.*\\'gt\\''):\n\n            class Foo(BaseModel):\n                c: List[Foo] = Field(..., gt=0)\n\n\ndef test_forward_ref_optional(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel, Field\n\n\nclass Spec(BaseModel):\n    spec_fields: list[str] = Field(..., alias=\"fields\")\n    filter: str | None = None\n    sort: str | None\n\n\nclass PSpec(Spec):\n    g: GSpec | None = None\n\n\nclass GSpec(Spec):\n    p: PSpec | None\n\n# PSpec.model_rebuild()\n\nclass Filter(BaseModel):\n    g: GSpec | None = None\n    p: PSpec | None\n    \"\"\"\n    )\n    Filter = module.Filter\n    assert isinstance(Filter(p={'sort': 'some_field:asc', 'fields': []}), Filter)\n\n\ndef test_forward_ref_with_create_model(create_module):\n    @create_module\n    def module():\n        import pydantic\n\n        Sub = pydantic.create_model('Sub', foo=(str, 'bar'), __module__=__name__)\n        assert Sub  # get rid of \"local variable 'Sub' is assigned to but never used\"\n        Main = pydantic.create_model('Main', sub=('Sub', ...), __module__=__name__)\n        instance = Main(sub={})\n        assert instance.sub.model_dump() == {'foo': 'bar'}\n\n\ndef test_resolve_forward_ref_dataclass(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel\nfrom typing_extensions import Literal\n\n@dataclass\nclass Base:\n    literal: Literal[1, 2]\n\nclass What(BaseModel):\n    base: Base\n        \"\"\"\n    )\n\n    m = module.What(base=module.Base(literal=1))\n    assert m.base.literal == 1\n\n\ndef test_nested_forward_ref():\n    class NestedTuple(BaseModel):\n        x: Tuple[int, Optional['NestedTuple']]\n\n    obj = NestedTuple.model_validate({'x': ('1', {'x': ('2', {'x': ('3', None)})})})\n    assert obj.model_dump() == {'x': (1, {'x': (2, {'x': (3, None)})})}\n\n\ndef test_discriminated_union_forward_ref(create_module):\n    @create_module\n    def module():\n        from typing import Union\n\n        from typing_extensions import Literal\n\n        from pydantic import BaseModel, Field\n\n        class Pet(BaseModel):\n            pet: Union['Cat', 'Dog'] = Field(discriminator='type')\n\n        class Cat(BaseModel):\n            type: Literal['cat']\n\n        class Dog(BaseModel):\n            type: Literal['dog']\n\n    assert module.Pet.__pydantic_complete__ is False\n\n    with pytest.raises(\n        ValidationError,\n        match=\"Input tag 'pika' found using 'type' does not match any of the expected tags: 'cat', 'dog'\",\n    ):\n        module.Pet.model_validate({'pet': {'type': 'pika'}})\n\n    # Ensure the rebuild has happened automatically despite validation failure\n    assert module.Pet.__pydantic_complete__ is True\n\n    # insert_assert(module.Pet.model_json_schema())\n    assert module.Pet.model_json_schema() == {\n        'title': 'Pet',\n        'required': ['pet'],\n        'type': 'object',\n        'properties': {\n            'pet': {\n                'title': 'Pet',\n                'discriminator': {'mapping': {'cat': '#/$defs/Cat', 'dog': '#/$defs/Dog'}, 'propertyName': 'type'},\n                'oneOf': [{'$ref': '#/$defs/Cat'}, {'$ref': '#/$defs/Dog'}],\n            }\n        },\n        '$defs': {\n            'Cat': {\n                'title': 'Cat',\n                'type': 'object',\n                'properties': {'type': {'const': 'cat', 'enum': ['cat'], 'title': 'Type', 'type': 'string'}},\n                'required': ['type'],\n            },\n            'Dog': {\n                'title': 'Dog',\n                'type': 'object',\n                'properties': {'type': {'const': 'dog', 'enum': ['dog'], 'title': 'Type', 'type': 'string'}},\n                'required': ['type'],\n            },\n        },\n    }\n\n\ndef test_class_var_as_string(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom typing import ClassVar\nfrom pydantic import BaseModel\n\nclass Model(BaseModel):\n    a: ClassVar[int]\n\"\"\"\n    )\n\n    assert module.Model.__class_vars__ == {'a'}\n\n\ndef test_json_encoder_str(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom pydantic import BaseModel, ConfigDict, field_serializer\n\n\nclass User(BaseModel):\n    x: str\n\n\nFooUser = User\n\n\nclass User(BaseModel):\n    y: str\n\n\nclass Model(BaseModel):\n    foo_user: FooUser\n    user: User\n\n    @field_serializer('user')\n    def serialize_user(self, v):\n        return f'User({v.y})'\n\n\"\"\"\n    )\n\n    m = module.Model(foo_user={'x': 'user1'}, user={'y': 'user2'})\n    # TODO: How can we replicate this custom-encoder functionality without affecting the serialization of `User`?\n    assert m.model_dump_json() == '{\"foo_user\":{\"x\":\"user1\"},\"user\":\"User(user2)\"}'\n\n\nskip_pep585 = pytest.mark.skipif(\n    sys.version_info < (3, 9), reason='PEP585 generics only supported for python 3.9 and above'\n)\n\n\n@skip_pep585\ndef test_pep585_self_referencing_generics(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\nclass SelfReferencing(BaseModel):\n    names: list[SelfReferencing]  # noqa: F821\n\"\"\"\n    )\n\n    SelfReferencing = module.SelfReferencing\n    if sys.version_info >= (3, 10):\n        assert (\n            repr(SelfReferencing.model_fields['names']) == 'FieldInfo(annotation=list[SelfReferencing], required=True)'\n        )\n\n    # test that object creation works\n    obj = SelfReferencing(names=[SelfReferencing(names=[])])\n    assert obj.names == [SelfReferencing(names=[])]\n\n\n@skip_pep585\ndef test_pep585_recursive_generics(create_module):\n    @create_module\n    def module():\n        from typing import ForwardRef\n\n        from pydantic import BaseModel\n\n        HeroRef = ForwardRef('Hero')\n\n        class Team(BaseModel):\n            name: str\n            heroes: list[HeroRef]\n\n        class Hero(BaseModel):\n            name: str\n            teams: list[Team]\n\n        Team.model_rebuild()\n\n    assert repr(module.Team.model_fields['heroes']) == 'FieldInfo(annotation=list[Hero], required=True)'\n    assert repr(module.Hero.model_fields['teams']) == 'FieldInfo(annotation=list[Team], required=True)'\n\n    h = module.Hero(name='Ivan', teams=[module.Team(name='TheBest', heroes=[])])\n    # insert_assert(h.model_dump())\n    assert h.model_dump() == {'name': 'Ivan', 'teams': [{'name': 'TheBest', 'heroes': []}]}\n\n\ndef test_class_var_forward_ref(create_module):\n    # see #3679\n    create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom typing import ClassVar\nfrom pydantic import BaseModel\n\nclass WithClassVar(BaseModel):\n    Instances: ClassVar[dict[str, WithClassVar]] = {}\n\"\"\"\n    )\n\n\ndef test_recursive_model(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass Foobar(BaseModel):\n    x: int\n    y: Optional[Foobar] = None\n\"\"\"\n    )\n    f = module.Foobar(x=1, y={'x': 2})\n    assert f.model_dump() == {'x': 1, 'y': {'x': 2, 'y': None}}\n    assert f.model_fields_set == {'x', 'y'}\n    assert f.y.model_fields_set == {'x'}\n\n\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='needs 3.10 or newer')\ndef test_recursive_models_union(create_module):\n    # This test should pass because PydanticRecursiveRef.__or__ is implemented,\n    # not because `eval_type_backport` magically makes `|` work,\n    # since it's installed for tests but otherwise optional.\n    sys.modules['eval_type_backport'] = None  # type: ignore\n    try:\n        module = create_module(\n            # language=Python\n            \"\"\"\nfrom __future__ import annotations\n\nfrom pydantic import BaseModel\nfrom typing import TypeVar, Generic\n\nT = TypeVar(\"T\")\n\nclass Foo(BaseModel):\n    bar: Bar[str] | None = None\n    bar2: int | Bar[float]\n\nclass Bar(BaseModel, Generic[T]):\n    foo: Foo\n    \"\"\"\n        )\n    finally:\n        del sys.modules['eval_type_backport']\n\n    assert module.Foo.model_fields['bar'].annotation == typing.Optional[module.Bar[str]]\n    assert module.Foo.model_fields['bar2'].annotation == typing.Union[int, module.Bar[float]]\n    assert module.Bar.model_fields['foo'].annotation == module.Foo\n\n\ndef test_recursive_models_union_backport(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\n\nfrom pydantic import BaseModel\nfrom typing import TypeVar, Generic\n\nT = TypeVar(\"T\")\n\nclass Foo(BaseModel):\n    bar: Bar[str] | None = None\n    # The `int | str` here differs from the previous test and requires the backport.\n    # At the same time, `PydanticRecursiveRef.__or__` means that the second `|` works normally,\n    # which actually triggered a bug in the backport that needed fixing.\n    bar2: int | str | Bar[float]\n\nclass Bar(BaseModel, Generic[T]):\n    foo: Foo\n\"\"\"\n    )\n\n    assert module.Foo.model_fields['bar'].annotation == typing.Optional[module.Bar[str]]\n    assert module.Foo.model_fields['bar2'].annotation == typing.Union[int, str, module.Bar[float]]\n    assert module.Bar.model_fields['foo'].annotation == module.Foo\n\n\ndef test_force_rebuild():\n    class Foobar(BaseModel):\n        b: int\n\n    assert Foobar.__pydantic_complete__ is True\n    assert Foobar.model_rebuild() is None\n    assert Foobar.model_rebuild(force=True) is True\n\n\ndef test_rebuild_subclass_of_built_model():\n    class Model(BaseModel):\n        x: int\n\n    class FutureReferencingModel(Model):\n        y: 'FutureModel'\n\n    class FutureModel(BaseModel):\n        pass\n\n    FutureReferencingModel.model_rebuild()\n\n    assert FutureReferencingModel(x=1, y=FutureModel()).model_dump() == {'x': 1, 'y': {}}\n\n\ndef test_nested_annotation(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\ndef nested():\n    class Foo(BaseModel):\n        a: int\n\n    class Bar(BaseModel):\n        b: Foo\n\n    return Bar\n\"\"\"\n    )\n\n    bar_model = module.nested()\n    assert bar_model.__pydantic_complete__ is True\n    assert bar_model(b={'a': 1}).model_dump() == {'b': {'a': 1}}\n\n\ndef test_nested_more_annotation(create_module):\n    @create_module\n    def module():\n        from pydantic import BaseModel\n\n        def nested():\n            class Foo(BaseModel):\n                a: int\n\n            def more_nested():\n                class Bar(BaseModel):\n                    b: 'Foo'\n\n                return Bar\n\n            return more_nested()\n\n    bar_model = module.nested()\n    # this does not work because Foo is in a parent scope\n    assert bar_model.__pydantic_complete__ is False\n\n\ndef test_nested_annotation_priority(create_module):\n    @create_module\n    def module():\n        from annotated_types import Gt\n        from typing_extensions import Annotated\n\n        from pydantic import BaseModel\n\n        Foobar = Annotated[int, Gt(0)]  # noqa: F841\n\n        def nested():\n            Foobar = Annotated[int, Gt(10)]\n\n            class Bar(BaseModel):\n                b: 'Foobar'\n\n            return Bar\n\n    bar_model = module.nested()\n    assert bar_model.model_fields['b'].metadata[0].gt == 10\n    assert bar_model(b=11).model_dump() == {'b': 11}\n    with pytest.raises(ValidationError, match=r'Input should be greater than 10 \\[type=greater_than,'):\n        bar_model(b=1)\n\n\ndef test_nested_model_rebuild(create_module):\n    @create_module\n    def module():\n        from pydantic import BaseModel\n\n        def nested():\n            class Bar(BaseModel):\n                b: 'Foo'\n\n            class Foo(BaseModel):\n                a: int\n\n            assert Bar.__pydantic_complete__ is False\n            Bar.model_rebuild()\n            return Bar\n\n    bar_model = module.nested()\n    assert bar_model.__pydantic_complete__ is True\n    assert bar_model(b={'a': 1}).model_dump() == {'b': {'a': 1}}\n\n\ndef pytest_raises_user_error_for_undefined_type(defining_class_name, missing_type_name):\n    \"\"\"\n    Returns a `pytest.raises` context manager that checks the error message when an undefined type is present.\n\n    usage: `with pytest_raises_user_error_for_undefined_type(class_name='Foobar', missing_class_name='UndefinedType'):`\n    \"\"\"\n\n    return pytest.raises(\n        PydanticUserError,\n        match=re.escape(\n            f'`{defining_class_name}` is not fully defined; you should define `{missing_type_name}`, then call'\n            f' `{defining_class_name}.model_rebuild()`.'\n        ),\n    )\n\n\n#   NOTE: the `undefined_types_warning` tests below are \"statically parameterized\" (i.e. have Duplicate Code).\n#   The initial attempt to refactor them into a single parameterized test was not straightforward due to the use of the\n#   `create_module` fixture and the requirement that `from __future__ import annotations` be the first line in a module.\n#\n#   Test Parameters:\n#     1. config setting: (1a) default behavior vs (1b) overriding with Config setting:\n#     2. type checking approach: (2a) `from __future__ import annotations` vs (2b) `ForwardRef`\n#\n#   The parameter tags \"1a\", \"1b\", \"2a\", and \"2b\" are used in the test names below, to indicate which combination\n#   of conditions the test is covering.\n\n\ndef test_undefined_types_warning_1a_raised_by_default_2a_future_annotations(create_module):\n    with pytest_raises_user_error_for_undefined_type(defining_class_name='Foobar', missing_type_name='UndefinedType'):\n        create_module(\n            # language=Python\n            \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\nclass Foobar(BaseModel):\n    a: UndefinedType\n\n# Trigger the error for an undefined type:\nFoobar(a=1)\n\"\"\"\n        )\n\n\ndef test_undefined_types_warning_1a_raised_by_default_2b_forward_ref(create_module):\n    with pytest_raises_user_error_for_undefined_type(defining_class_name='Foobar', missing_type_name='UndefinedType'):\n\n        @create_module\n        def module():\n            from typing import ForwardRef\n\n            from pydantic import BaseModel\n\n            UndefinedType = ForwardRef('UndefinedType')\n\n            class Foobar(BaseModel):\n                a: UndefinedType\n\n            # Trigger the error for an undefined type\n            Foobar(a=1)\n\n\ndef test_undefined_types_warning_1b_suppressed_via_config_2a_future_annotations(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\n# Because we don't instantiate the type, no error for an undefined type is raised\nclass Foobar(BaseModel):\n    a: UndefinedType\n\"\"\"\n    )\n    # Since we're testing the absence of an error, it's important to confirm pydantic was actually run.\n    # The presence of the `__pydantic_complete__` is a good indicator of this.\n    assert module.Foobar.__pydantic_complete__ is False\n\n\ndef test_undefined_types_warning_1b_suppressed_via_config_2b_forward_ref(create_module):\n    @create_module\n    def module():\n        from typing import ForwardRef\n\n        from pydantic import BaseModel\n\n        UndefinedType = ForwardRef('UndefinedType')\n\n        # Because we don't instantiate the type, no error for an undefined type is raised\n        class Foobar(BaseModel):\n            a: UndefinedType\n\n    # Since we're testing the absence of a warning, it's important to confirm pydantic was actually run.\n    # The presence of the `__pydantic_complete__` is a good indicator of this.\n    assert module.Foobar.__pydantic_complete__ is False\n\n\ndef test_undefined_types_warning_raised_by_usage(create_module):\n    with pytest_raises_user_error_for_undefined_type('Foobar', 'UndefinedType'):\n\n        @create_module\n        def module():\n            from typing import ForwardRef\n\n            from pydantic import BaseModel\n\n            UndefinedType = ForwardRef('UndefinedType')\n\n            class Foobar(BaseModel):\n                a: UndefinedType\n\n            Foobar(a=1)\n\n\ndef test_rebuild_recursive_schema():\n    from typing import ForwardRef, List\n\n    class Expressions_(BaseModel):\n        model_config = dict(undefined_types_warning=False)\n        items: List[\"types['Expression']\"]\n\n    class Expression_(BaseModel):\n        model_config = dict(undefined_types_warning=False)\n        Or: ForwardRef(\"types['allOfExpressions']\")\n        Not: ForwardRef(\"types['allOfExpression']\")\n\n    class allOfExpression_(BaseModel):\n        model_config = dict(undefined_types_warning=False)\n        Not: ForwardRef(\"types['Expression']\")\n\n    class allOfExpressions_(BaseModel):\n        model_config = dict(undefined_types_warning=False)\n        items: List[\"types['Expression']\"]\n\n    types_namespace = {\n        'types': {\n            'Expression': Expression_,\n            'Expressions': Expressions_,\n            'allOfExpression': allOfExpression_,\n            'allOfExpressions': allOfExpressions_,\n        }\n    }\n\n    models = [allOfExpressions_, Expressions_]\n    for m in models:\n        m.model_rebuild(_types_namespace=types_namespace)\n\n\ndef test_forward_ref_in_generic(create_module: Any) -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/6503\"\"\"\n\n    @create_module\n    def module():\n        import typing as tp\n\n        from pydantic import BaseModel\n\n        class Foo(BaseModel):\n            x: tp.Dict['tp.Type[Bar]', tp.Type['Bar']]\n\n        class Bar(BaseModel):\n            pass\n\n    Foo = module.Foo\n    Bar = module.Bar\n\n    assert Foo(x={Bar: Bar}).x[Bar] is Bar\n\n\ndef test_forward_ref_in_generic_separate_modules(create_module: Any) -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/6503\"\"\"\n\n    @create_module\n    def module_1():\n        import typing as tp\n\n        from pydantic import BaseModel\n\n        class Foo(BaseModel):\n            x: tp.Dict['tp.Type[Bar]', tp.Type['Bar']]\n\n    @create_module\n    def module_2():\n        from pydantic import BaseModel\n\n        class Bar(BaseModel):\n            pass\n\n    Foo = module_1.Foo\n    Bar = module_2.Bar\n    Foo.model_rebuild(_types_namespace={'tp': typing, 'Bar': Bar})\n    assert Foo(x={Bar: Bar}).x[Bar] is Bar\n", "tests/__init__.py": "", "tests/test_type_alias_type.py": "import datetime\nfrom dataclasses import dataclass\nfrom typing import Dict, Generic, List, Tuple, TypeVar, Union\n\nimport pytest\nfrom annotated_types import MaxLen\nfrom typing_extensions import Annotated, Literal, TypeAliasType\n\nfrom pydantic import BaseModel, Field, ValidationError\nfrom pydantic.type_adapter import TypeAdapter\n\nT = TypeVar('T')\n\nJsonType = TypeAliasType('JsonType', Union[List['JsonType'], Dict[str, 'JsonType'], str, int, float, bool, None])\nRecursiveGenericAlias = TypeAliasType(\n    'RecursiveGenericAlias', List[Union['RecursiveGenericAlias[T]', T]], type_params=(T,)\n)\nMyList = TypeAliasType('MyList', List[T], type_params=(T,))\n# try mixing with implicit type aliases\nShortMyList = Annotated[MyList[T], MaxLen(1)]\nShortRecursiveGenericAlias = Annotated[RecursiveGenericAlias[T], MaxLen(1)]\n\n\ndef test_type_alias() -> None:\n    t = TypeAdapter(MyList[int])\n\n    assert t.validate_python(['1', '2']) == [1, 2]\n\n    with pytest.raises(ValidationError) as exc_info:\n        t.validate_python(['a'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (0,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        }\n    ]\n\n    assert t.json_schema() == {'type': 'array', 'items': {'type': 'integer'}}\n\n\ndef test_recursive_type_alias() -> None:\n    t = TypeAdapter(JsonType)\n\n    assert t.validate_python({'a': [True, [{'b': None}]]}) == {'a': [True, [{'b': None}]]}\n\n    with pytest.raises(ValidationError) as exc_info:\n        t.validate_python({'a': datetime.date(year=1992, month=12, day=11)})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'list_type',\n            'loc': ('list[nullable[union[list[...],dict[str,...],str,int,float,bool]]]',),\n            'msg': 'Input should be a valid list',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n        {\n            'type': 'list_type',\n            'loc': ('dict[str,...]', 'a', 'list[nullable[union[list[...],dict[str,...],str,int,float,bool]]]'),\n            'msg': 'Input should be a valid list',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'dict_type',\n            'loc': ('dict[str,...]', 'a', 'dict[str,...]'),\n            'msg': 'Input should be a valid dictionary',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'string_type',\n            'loc': ('dict[str,...]', 'a', 'str'),\n            'msg': 'Input should be a valid string',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'int_type',\n            'loc': ('dict[str,...]', 'a', 'int'),\n            'msg': 'Input should be a valid integer',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'float_type',\n            'loc': ('dict[str,...]', 'a', 'float'),\n            'msg': 'Input should be a valid number',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'bool_type',\n            'loc': ('dict[str,...]', 'a', 'bool'),\n            'msg': 'Input should be a valid boolean',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'string_type',\n            'loc': ('str',),\n            'msg': 'Input should be a valid string',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n        {\n            'type': 'int_type',\n            'loc': ('int',),\n            'msg': 'Input should be a valid integer',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n        {\n            'type': 'float_type',\n            'loc': ('float',),\n            'msg': 'Input should be a valid number',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n        {\n            'type': 'bool_type',\n            'loc': ('bool',),\n            'msg': 'Input should be a valid boolean',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n    ]\n\n    assert t.json_schema() == {\n        'allOf': [{'$ref': '#/$defs/JsonType'}],\n        '$defs': {\n            'JsonType': {\n                'anyOf': [\n                    {'type': 'array', 'items': {'$ref': '#/$defs/JsonType'}},\n                    {'type': 'object', 'additionalProperties': {'$ref': '#/$defs/JsonType'}},\n                    {'type': 'string'},\n                    {'type': 'integer'},\n                    {'type': 'number'},\n                    {'type': 'boolean'},\n                    {'type': 'null'},\n                ]\n            }\n        },\n    }\n\n\ndef test_recursive_type_alias_name():\n    T = TypeVar('T')\n\n    @dataclass\n    class MyGeneric(Generic[T]):\n        field: T\n\n    MyRecursiveType = TypeAliasType('MyRecursiveType', Union[MyGeneric['MyRecursiveType'], int])\n    json_schema = TypeAdapter(MyRecursiveType).json_schema()\n    assert sorted(json_schema['$defs'].keys()) == ['MyGeneric_MyRecursiveType_', 'MyRecursiveType']\n\n\ndef test_type_alias_annotated() -> None:\n    t = TypeAdapter(ShortMyList[int])\n\n    assert t.validate_python(['1']) == [1]\n\n    with pytest.raises(ValidationError) as exc_info:\n        t.validate_python([1, 2])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'List should have at most 1 item after validation, not 2',\n            'input': [1, 2],\n            'ctx': {'field_type': 'List', 'max_length': 1, 'actual_length': 2},\n        }\n    ]\n\n    assert t.json_schema() == {'type': 'array', 'items': {'type': 'integer'}, 'maxItems': 1}\n\n\ndef test_type_alias_annotated_defs() -> None:\n    # force use of refs by referencing the schema in multiple places\n    t = TypeAdapter(Tuple[ShortMyList[int], ShortMyList[int]])\n\n    assert t.validate_python((['1'], ['2'])) == ([1], [2])\n\n    with pytest.raises(ValidationError) as exc_info:\n        t.validate_python(([1, 2], [1, 2]))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (0,),\n            'msg': 'List should have at most 1 item after validation, not 2',\n            'input': [1, 2],\n            'ctx': {'field_type': 'List', 'max_length': 1, 'actual_length': 2},\n        },\n        {\n            'type': 'too_long',\n            'loc': (1,),\n            'msg': 'List should have at most 1 item after validation, not 2',\n            'input': [1, 2],\n            'ctx': {'field_type': 'List', 'max_length': 1, 'actual_length': 2},\n        },\n    ]\n\n    assert t.json_schema() == {\n        'type': 'array',\n        'minItems': 2,\n        'prefixItems': [\n            {'$ref': '#/$defs/MyList_int__MaxLen_max_length_1_'},\n            {'$ref': '#/$defs/MyList_int__MaxLen_max_length_1_'},\n        ],\n        'maxItems': 2,\n        '$defs': {'MyList_int__MaxLen_max_length_1_': {'type': 'array', 'items': {'type': 'integer'}, 'maxItems': 1}},\n    }\n\n\ndef test_recursive_generic_type_alias() -> None:\n    t = TypeAdapter(RecursiveGenericAlias[int])\n\n    assert t.validate_python([[['1']]]) == [[[1]]]\n\n    with pytest.raises(ValidationError) as exc_info:\n        t.validate_python([[['a']]])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'list_type',\n            'loc': (0, 'list[union[...,int]]', 0, 'list[union[...,int]]', 0, 'list[union[...,int]]'),\n            'msg': 'Input should be a valid list',\n            'input': 'a',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': (0, 'list[union[...,int]]', 0, 'list[union[...,int]]', 0, 'int'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        },\n        {\n            'type': 'int_type',\n            'loc': (0, 'list[union[...,int]]', 0, 'int'),\n            'msg': 'Input should be a valid integer',\n            'input': ['a'],\n        },\n        {'type': 'int_type', 'loc': (0, 'int'), 'msg': 'Input should be a valid integer', 'input': [['a']]},\n    ]\n\n    assert t.json_schema() == {\n        'allOf': [{'$ref': '#/$defs/RecursiveGenericAlias_int_'}],\n        '$defs': {\n            'RecursiveGenericAlias_int_': {\n                'type': 'array',\n                'items': {'anyOf': [{'$ref': '#/$defs/RecursiveGenericAlias_int_'}, {'type': 'integer'}]},\n            }\n        },\n    }\n\n\ndef test_recursive_generic_type_alias_annotated() -> None:\n    t = TypeAdapter(ShortRecursiveGenericAlias[int])\n\n    assert t.validate_python([[]]) == [[]]\n\n    with pytest.raises(ValidationError) as exc_info:\n        t.validate_python([[], []])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'List should have at most 1 item after validation, not 2',\n            'input': [[], []],\n            'ctx': {'field_type': 'List', 'max_length': 1, 'actual_length': 2},\n        }\n    ]\n\n    # insert_assert(t.json_schema())\n    assert t.json_schema() == {\n        'type': 'array',\n        'items': {'anyOf': [{'$ref': '#/$defs/RecursiveGenericAlias_int_'}, {'type': 'integer'}]},\n        'maxItems': 1,\n        '$defs': {\n            'RecursiveGenericAlias_int_': {\n                'type': 'array',\n                'items': {'anyOf': [{'$ref': '#/$defs/RecursiveGenericAlias_int_'}, {'type': 'integer'}]},\n            }\n        },\n    }\n\n\ndef test_recursive_generic_type_alias_annotated_defs() -> None:\n    # force use of refs by referencing the schema in multiple places\n    t = TypeAdapter(Tuple[ShortRecursiveGenericAlias[int], ShortRecursiveGenericAlias[int]])\n\n    assert t.validate_python(([[]], [[]])) == ([[]], [[]])\n\n    with pytest.raises(ValidationError) as exc_info:\n        t.validate_python(([[], []], [[]]))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (0,),\n            'msg': 'List should have at most 1 item after validation, not 2',\n            'input': [[], []],\n            'ctx': {'field_type': 'List', 'max_length': 1, 'actual_length': 2},\n        }\n    ]\n\n    # insert_assert(t.json_schema())\n    assert t.json_schema() == {\n        'type': 'array',\n        'minItems': 2,\n        'prefixItems': [\n            {'$ref': '#/$defs/RecursiveGenericAlias_int__MaxLen_max_length_1_'},\n            {'$ref': '#/$defs/RecursiveGenericAlias_int__MaxLen_max_length_1_'},\n        ],\n        'maxItems': 2,\n        '$defs': {\n            'RecursiveGenericAlias_int_': {\n                'type': 'array',\n                'items': {'anyOf': [{'$ref': '#/$defs/RecursiveGenericAlias_int_'}, {'type': 'integer'}]},\n            },\n            'RecursiveGenericAlias_int__MaxLen_max_length_1_': {\n                'type': 'array',\n                'items': {'anyOf': [{'$ref': '#/$defs/RecursiveGenericAlias_int_'}, {'type': 'integer'}]},\n                'maxItems': 1,\n            },\n        },\n    }\n\n\n@pytest.mark.xfail(reason='description is currently dropped')\ndef test_field() -> None:\n    SomeAlias = TypeAliasType('SomeAlias', Annotated[int, Field(description='number')])\n\n    ta = TypeAdapter(Annotated[SomeAlias, Field(title='abc')])\n\n    # insert_assert(ta.json_schema())\n    assert ta.json_schema() == {\n        '$defs': {'SomeAlias': {'type': 'integer', 'description': 'number'}},\n        'allOf': [{'$ref': '#/$defs/SomeAlias'}],\n        'title': 'abc',\n    }\n\n\ndef test_nested_generic_type_alias_type() -> None:\n    class MyModel(BaseModel):\n        field_1: MyList[bool]\n        field_2: MyList[str]\n\n    model = MyModel(field_1=[True], field_2=['abc'])\n\n    assert model.model_json_schema() == {\n        '$defs': {\n            'MyList_bool_': {'items': {'type': 'boolean'}, 'type': 'array'},\n            'MyList_str_': {'items': {'type': 'string'}, 'type': 'array'},\n        },\n        'properties': {'field_1': {'$ref': '#/$defs/MyList_bool_'}, 'field_2': {'$ref': '#/$defs/MyList_str_'}},\n        'required': ['field_1', 'field_2'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n\n\ndef test_non_specified_generic_type_alias_type() -> None:\n    assert TypeAdapter(MyList).json_schema() == {'items': {}, 'type': 'array'}\n\n\ndef test_redefined_type_alias():\n    MyType = TypeAliasType('MyType', str)\n\n    class MyInnerModel(BaseModel):\n        x: MyType\n\n    MyType = TypeAliasType('MyType', int)\n\n    class MyOuterModel(BaseModel):\n        inner: MyInnerModel\n        y: MyType\n\n    data = {'inner': {'x': 'hello'}, 'y': 1}\n    assert MyOuterModel.model_validate(data).model_dump() == data\n\n\ndef test_type_alias_to_type_with_ref():\n    class Div(BaseModel):\n        type: Literal['Div'] = 'Div'\n        components: List['AnyComponent']\n\n    AnyComponent = TypeAliasType('AnyComponent', Div)\n\n    adapter = TypeAdapter(AnyComponent)\n    adapter.validate_python({'type': 'Div', 'components': [{'type': 'Div', 'components': []}]})\n    with pytest.raises(ValidationError) as exc_info:\n        adapter.validate_python({'type': 'Div', 'components': [{'type': 'NotDiv', 'components': []}]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'expected': \"'Div'\"},\n            'input': 'NotDiv',\n            'loc': ('components', 0, 'type'),\n            'msg': \"Input should be 'Div'\",\n            'type': 'literal_error',\n        }\n    ]\n", "tests/test_rich_repr.py": "from datetime import datetime\nfrom typing import List, Optional\n\nimport pytest\n\nfrom pydantic import BaseModel\nfrom pydantic.color import Color\n\n\n@pytest.fixture(scope='session', name='User')\ndef user_fixture():\n    class User(BaseModel):\n        id: int\n        name: str = 'John Doe'\n        signup_ts: Optional[datetime] = None\n        friends: List[int] = []\n\n    return User\n\n\ndef test_rich_repr(User):\n    user = User(id=22)\n    rich_repr = list(user.__rich_repr__())\n\n    assert rich_repr == [\n        ('id', 22),\n        ('name', 'John Doe'),\n        ('signup_ts', None),\n        ('friends', []),\n    ]\n\n\n@pytest.mark.filterwarnings('ignore::DeprecationWarning')\ndef test_rich_repr_color(User):\n    color = Color((10, 20, 30, 0.1))\n    rich_repr = list(color.__rich_repr__())\n\n    assert rich_repr == ['#0a141e1a', ('rgb', (10, 20, 30, 0.1))]\n", "tests/test_callable.py": "import sys\nfrom typing import Callable\n\nimport pytest\n\nfrom pydantic import BaseModel, ValidationError\n\ncollection_callable_types = [Callable, Callable[[int], int]]\nif sys.version_info >= (3, 9):\n    from collections.abc import Callable as CollectionsCallable\n\n    collection_callable_types += [CollectionsCallable, CollectionsCallable[[int], int]]\n\n\n@pytest.mark.parametrize('annotation', collection_callable_types)\ndef test_callable(annotation):\n    class Model(BaseModel):\n        callback: annotation\n\n    m = Model(callback=lambda x: x)\n    assert callable(m.callback)\n\n\n@pytest.mark.parametrize('annotation', collection_callable_types)\ndef test_non_callable(annotation):\n    class Model(BaseModel):\n        callback: annotation\n\n    with pytest.raises(ValidationError):\n        Model(callback=1)\n", "tests/test_strict.py": "import sys\nfrom typing import Any, Type\n\nif sys.version_info < (3, 9):\n    from typing_extensions import Annotated\nelse:\n    from typing import Annotated\n\nimport pytest\n\nfrom pydantic import BaseModel, ConfigDict, Field, ValidationError\n\n\n@pytest.fixture(scope='session', name='ModelWithStrictField')\ndef model_with_strict_field():\n    class ModelWithStrictField(BaseModel):\n        a: Annotated[int, Field(strict=True)]\n\n    return ModelWithStrictField\n\n\n@pytest.mark.parametrize(\n    'value',\n    [\n        '1',\n        True,\n        1.0,\n    ],\n)\ndef test_parse_strict_mode_on_field_invalid(value: Any, ModelWithStrictField: Type[BaseModel]) -> None:\n    with pytest.raises(ValidationError) as exc_info:\n        ModelWithStrictField(a=value)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('a',), 'msg': 'Input should be a valid integer', 'input': value}\n    ]\n\n\ndef test_parse_strict_mode_on_field_valid(ModelWithStrictField: Type[BaseModel]) -> None:\n    value = ModelWithStrictField(a=1)\n    assert value.model_dump() == {'a': 1}\n\n\n@pytest.fixture(scope='session', name='ModelWithStrictConfig')\ndef model_with_strict_config_false():\n    class ModelWithStrictConfig(BaseModel):\n        a: int\n        # strict=False overrides the Config\n        b: Annotated[int, Field(strict=False)]\n        # strict=None or not including it is equivalent\n        # lets this field be overridden by the Config\n        c: Annotated[int, Field(strict=None)]\n        d: Annotated[int, Field()]\n\n        model_config = ConfigDict(strict=True)\n\n    return ModelWithStrictConfig\n\n\ndef test_parse_model_with_strict_config_enabled(ModelWithStrictConfig: Type[BaseModel]) -> None:\n    with pytest.raises(ValidationError) as exc_info:\n        ModelWithStrictConfig(a='1', b=2, c=3, d=4)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('a',), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        ModelWithStrictConfig(a=1, b=2, c='3', d=4)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('c',), 'msg': 'Input should be a valid integer', 'input': '3'}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        ModelWithStrictConfig(a=1, b=2, c=3, d='4')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('d',), 'msg': 'Input should be a valid integer', 'input': '4'}\n    ]\n    values = [\n        ModelWithStrictConfig(a=1, b='2', c=3, d=4),\n        ModelWithStrictConfig(a=1, b=2, c=3, d=4),\n    ]\n    assert all(v.model_dump() == {'a': 1, 'b': 2, 'c': 3, 'd': 4} for v in values)\n\n\ndef test_parse_model_with_strict_config_disabled(ModelWithStrictConfig: Type[BaseModel]) -> None:\n    class Model(ModelWithStrictConfig):\n        model_config = ConfigDict(strict=False)\n\n    values = [\n        Model(a='1', b=2, c=3, d=4),\n        Model(a=1, b=2, c='3', d=4),\n        Model(a=1, b=2, c=3, d='4'),\n        Model(a=1, b='2', c=3, d=4),\n        Model(a=1, b=2, c=3, d=4),\n    ]\n    assert all(v.model_dump() == {'a': 1, 'b': 2, 'c': 3, 'd': 4} for v in values)\n", "tests/check_usage_docs.py": "\"\"\"\nCheck that all `Usage docs` tags in docstrings link to the latest version of pydantic.\n\"\"\"\n\nimport re\nimport sys\nfrom pathlib import Path\n\nROOT_DIR = Path(__file__).parent.parent\nPYDANTIC_DIR = ROOT_DIR / 'pydantic'\nversion_file = PYDANTIC_DIR / 'version.py'\n\n\nversion = re.search(rb\"VERSION = '(.*)'\", version_file.read_bytes()).group(1)\nversion_major_minor = b'.'.join(version.split(b'.')[:2])\nexpected_base = b'https://docs.pydantic.dev/' + version_major_minor + b'/'\n\npaths = sys.argv[1:]\nerror_count = 0\nfor path_str in paths:\n    path = ROOT_DIR / path_str\n    b = path.read_bytes()\n\n    changed = 0\n\n    def sub(m: re.Match) -> bytes:\n        global changed\n        if m.group(2) != expected_base:\n            changed += 1\n            return m.group(1) + expected_base\n        else:\n            return m.group(0)\n\n    b = re.sub(rb'(\"\"\" *usage.docs: *)(https://.+?/.+?/)', sub, b, flags=re.I)\n    if changed:\n        error_count += changed\n        path.write_bytes(b)\n        plural = 's' if changed > 1 else ''\n        print(f'{path_str:50} {changed} usage docs link{plural} updated')\n\nif error_count:\n    sys.exit(1)\n", "tests/test_types.py": "import collections\nimport itertools\nimport json\nimport math\nimport os\nimport re\nimport sys\nimport typing\nimport uuid\nfrom collections import OrderedDict, defaultdict, deque\nfrom dataclasses import dataclass\nfrom datetime import date, datetime, time, timedelta, timezone\nfrom decimal import Decimal\nfrom enum import Enum, IntEnum\nfrom numbers import Number\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Callable,\n    Counter,\n    DefaultDict,\n    Deque,\n    Dict,\n    FrozenSet,\n    Iterable,\n    List,\n    NewType,\n    Optional,\n    Pattern,\n    Sequence,\n    Set,\n    Tuple,\n    TypeVar,\n    Union,\n)\nfrom uuid import UUID\n\nimport annotated_types\nimport dirty_equals\nimport pytest\nfrom dirty_equals import HasRepr, IsFloatNan, IsOneOf, IsStr\nfrom pydantic_core import CoreSchema, PydanticCustomError, SchemaError, core_schema\nfrom typing_extensions import Annotated, Literal, NotRequired, TypedDict, get_args\n\nfrom pydantic import (\n    UUID1,\n    UUID3,\n    UUID4,\n    UUID5,\n    AfterValidator,\n    AllowInfNan,\n    AwareDatetime,\n    Base64Bytes,\n    Base64Str,\n    Base64UrlBytes,\n    Base64UrlStr,\n    BaseModel,\n    ByteSize,\n    ConfigDict,\n    DirectoryPath,\n    EmailStr,\n    Field,\n    FilePath,\n    FiniteFloat,\n    FutureDate,\n    FutureDatetime,\n    GetCoreSchemaHandler,\n    GetPydanticSchema,\n    ImportString,\n    InstanceOf,\n    Json,\n    JsonValue,\n    NaiveDatetime,\n    NameEmail,\n    NegativeFloat,\n    NegativeInt,\n    NewPath,\n    NonNegativeFloat,\n    NonNegativeInt,\n    NonPositiveFloat,\n    NonPositiveInt,\n    OnErrorOmit,\n    PastDate,\n    PastDatetime,\n    PlainSerializer,\n    PositiveFloat,\n    PositiveInt,\n    PydanticInvalidForJsonSchema,\n    PydanticSchemaGenerationError,\n    Secret,\n    SecretBytes,\n    SecretStr,\n    SerializeAsAny,\n    SkipValidation,\n    Strict,\n    StrictBool,\n    StrictBytes,\n    StrictFloat,\n    StrictInt,\n    StrictStr,\n    StringConstraints,\n    Tag,\n    TypeAdapter,\n    ValidationError,\n    conbytes,\n    condate,\n    condecimal,\n    confloat,\n    confrozenset,\n    conint,\n    conlist,\n    conset,\n    constr,\n    field_serializer,\n    field_validator,\n    validate_call,\n)\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\n\ntry:\n    import email_validator\nexcept ImportError:\n    email_validator = None\n\n\n# TODO add back tests for Iterator\n\n\n@pytest.fixture(scope='session', name='ConBytesModel')\ndef con_bytes_model_fixture():\n    class ConBytesModel(BaseModel):\n        v: conbytes(max_length=10) = b'foobar'\n\n    return ConBytesModel\n\n\ndef test_constrained_bytes_good(ConBytesModel):\n    m = ConBytesModel(v=b'short')\n    assert m.v == b'short'\n\n\ndef test_constrained_bytes_default(ConBytesModel):\n    m = ConBytesModel()\n    assert m.v == b'foobar'\n\n\ndef test_strict_raw_type():\n    class Model(BaseModel):\n        v: Annotated[str, Strict]\n\n    assert Model(v='foo').v == 'foo'\n    with pytest.raises(ValidationError, match=r'Input should be a valid string \\[type=string_type,'):\n        Model(v=b'fo')\n\n\n@pytest.mark.parametrize(\n    ('data', 'valid'),\n    [(b'this is too long', False), ('\u2ab6\u24f2\u2f7701'.encode(), False), (b'not long90', True), ('\u2ab6\u24f2\u2f770'.encode(), True)],\n)\ndef test_constrained_bytes_too_long(ConBytesModel, data: bytes, valid: bool):\n    if valid:\n        assert ConBytesModel(v=data).model_dump() == {'v': data}\n    else:\n        with pytest.raises(ValidationError) as exc_info:\n            ConBytesModel(v=data)\n        # insert_assert(exc_info.value.errors(include_url=False))\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'ctx': {'max_length': 10},\n                'input': data,\n                'loc': ('v',),\n                'msg': 'Data should have at most 10 bytes',\n                'type': 'bytes_too_long',\n            }\n        ]\n\n\ndef test_constrained_bytes_strict_true():\n    class Model(BaseModel):\n        v: conbytes(strict=True)\n\n    assert Model(v=b'foobar').v == b'foobar'\n    with pytest.raises(ValidationError):\n        Model(v=bytearray('foobar', 'utf-8'))\n\n    with pytest.raises(ValidationError):\n        Model(v='foostring')\n\n    with pytest.raises(ValidationError):\n        Model(v=42)\n\n    with pytest.raises(ValidationError):\n        Model(v=0.42)\n\n\ndef test_constrained_bytes_strict_false():\n    class Model(BaseModel):\n        v: conbytes(strict=False)\n\n    assert Model(v=b'foobar').v == b'foobar'\n    assert Model(v=bytearray('foobar', 'utf-8')).v == b'foobar'\n    assert Model(v='foostring').v == b'foostring'\n\n    with pytest.raises(ValidationError):\n        Model(v=42)\n\n    with pytest.raises(ValidationError):\n        Model(v=0.42)\n\n\ndef test_constrained_bytes_strict_default():\n    class Model(BaseModel):\n        v: conbytes()\n\n    assert Model(v=b'foobar').v == b'foobar'\n    assert Model(v=bytearray('foobar', 'utf-8')).v == b'foobar'\n    assert Model(v='foostring').v == b'foostring'\n\n    with pytest.raises(ValidationError):\n        Model(v=42)\n\n    with pytest.raises(ValidationError):\n        Model(v=0.42)\n\n\ndef test_constrained_list_good():\n    class ConListModelMax(BaseModel):\n        v: conlist(int) = []\n\n    m = ConListModelMax(v=[1, 2, 3])\n    assert m.v == [1, 2, 3]\n\n\ndef test_constrained_list_default():\n    class ConListModelMax(BaseModel):\n        v: conlist(int) = []\n\n    m = ConListModelMax()\n    assert m.v == []\n\n\ndef test_constrained_list_too_long():\n    class ConListModelMax(BaseModel):\n        v: conlist(int, max_length=10) = []\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConListModelMax(v=list(str(i) for i in range(11)))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': ('v',),\n            'msg': 'List should have at most 10 items after validation, not 11',\n            'input': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'],\n            'ctx': {'field_type': 'List', 'max_length': 10, 'actual_length': 11},\n        }\n    ]\n\n\ndef test_constrained_list_too_short():\n    class ConListModelMin(BaseModel):\n        v: conlist(int, min_length=1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConListModelMin(v=[])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('v',),\n            'msg': 'List should have at least 1 item after validation, not 0',\n            'input': [],\n            'ctx': {'field_type': 'List', 'min_length': 1, 'actual_length': 0},\n        }\n    ]\n\n\ndef test_constrained_list_optional():\n    class Model(BaseModel):\n        req: Optional[conlist(str, min_length=1)]\n        opt: Optional[conlist(str, min_length=1)] = None\n\n    assert Model(req=None).model_dump() == {'req': None, 'opt': None}\n    assert Model(req=None, opt=None).model_dump() == {'req': None, 'opt': None}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(req=[], opt=[])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('req',),\n            'msg': 'List should have at least 1 item after validation, not 0',\n            'input': [],\n            'ctx': {'field_type': 'List', 'min_length': 1, 'actual_length': 0},\n        },\n        {\n            'type': 'too_short',\n            'loc': ('opt',),\n            'msg': 'List should have at least 1 item after validation, not 0',\n            'input': [],\n            'ctx': {'field_type': 'List', 'min_length': 1, 'actual_length': 0},\n        },\n    ]\n\n    assert Model(req=['a'], opt=['a']).model_dump() == {'req': ['a'], 'opt': ['a']}\n\n\ndef test_constrained_list_constraints():\n    class ConListModelBoth(BaseModel):\n        v: conlist(int, min_length=7, max_length=11)\n\n    m = ConListModelBoth(v=list(range(7)))\n    assert m.v == list(range(7))\n\n    m = ConListModelBoth(v=list(range(11)))\n    assert m.v == list(range(11))\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConListModelBoth(v=list(range(6)))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('v',),\n            'msg': 'List should have at least 7 items after validation, not 6',\n            'input': [0, 1, 2, 3, 4, 5],\n            'ctx': {'field_type': 'List', 'min_length': 7, 'actual_length': 6},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConListModelBoth(v=list(range(12)))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': ('v',),\n            'msg': 'List should have at most 11 items after validation, not 12',\n            'input': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n            'ctx': {'field_type': 'List', 'max_length': 11, 'actual_length': 12},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConListModelBoth(v=1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'list_type', 'loc': ('v',), 'msg': 'Input should be a valid list', 'input': 1}\n    ]\n\n\ndef test_constrained_list_item_type_fails():\n    class ConListModel(BaseModel):\n        v: conlist(int) = []\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConListModel(v=['a', 'b', 'c'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'b',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'c',\n        },\n    ]\n\n\ndef test_conlist():\n    class Model(BaseModel):\n        foo: List[int] = Field(..., min_length=2, max_length=4)\n        bar: conlist(str, min_length=1, max_length=4) = None\n\n    assert Model(foo=[1, 2], bar=['spoon']).model_dump() == {'foo': [1, 2], 'bar': ['spoon']}\n\n    msg = r'List should have at least 2 items after validation, not 1 \\[type=too_short,'\n    with pytest.raises(ValidationError, match=msg):\n        Model(foo=[1])\n\n    msg = r'List should have at most 4 items after validation, not 5 \\[type=too_long,'\n    with pytest.raises(ValidationError, match=msg):\n        Model(foo=list(range(5)))\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=[1, 'x', 'y'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'y',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'list_type', 'loc': ('foo',), 'msg': 'Input should be a valid list', 'input': 1}\n    ]\n\n\ndef test_conlist_wrong_type_default():\n    \"\"\"It should not validate default value by default\"\"\"\n\n    class Model(BaseModel):\n        v: conlist(int) = 'a'\n\n    m = Model()\n    assert m.v == 'a'\n\n\ndef test_constrained_set_good():\n    class Model(BaseModel):\n        v: conset(int) = []\n\n    m = Model(v=[1, 2, 3])\n    assert m.v == {1, 2, 3}\n\n\ndef test_constrained_set_default():\n    class Model(BaseModel):\n        v: conset(int) = set()\n\n    m = Model()\n    assert m.v == set()\n\n\ndef test_constrained_set_default_invalid():\n    class Model(BaseModel):\n        v: conset(int) = 'not valid, not validated'\n\n    m = Model()\n    assert m.v == 'not valid, not validated'\n\n\ndef test_constrained_set_too_long():\n    class ConSetModelMax(BaseModel):\n        v: conset(int, max_length=10) = []\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConSetModelMax(v={str(i) for i in range(11)})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': ('v',),\n            'msg': 'Set should have at most 10 items after validation, not more',\n            'input': {'4', '3', '10', '9', '5', '6', '1', '8', '0', '7', '2'},\n            'ctx': {'field_type': 'Set', 'max_length': 10, 'actual_length': None},\n        }\n    ]\n\n\ndef test_constrained_set_too_short():\n    class ConSetModelMin(BaseModel):\n        v: conset(int, min_length=1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConSetModelMin(v=[])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('v',),\n            'msg': 'Set should have at least 1 item after validation, not 0',\n            'input': [],\n            'ctx': {'field_type': 'Set', 'min_length': 1, 'actual_length': 0},\n        }\n    ]\n\n\ndef test_constrained_set_optional():\n    class Model(BaseModel):\n        req: Optional[conset(str, min_length=1)]\n        opt: Optional[conset(str, min_length=1)] = None\n\n    assert Model(req=None).model_dump() == {'req': None, 'opt': None}\n    assert Model(req=None, opt=None).model_dump() == {'req': None, 'opt': None}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(req=set(), opt=set())\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('req',),\n            'msg': 'Set should have at least 1 item after validation, not 0',\n            'input': set(),\n            'ctx': {'field_type': 'Set', 'min_length': 1, 'actual_length': 0},\n        },\n        {\n            'type': 'too_short',\n            'loc': ('opt',),\n            'msg': 'Set should have at least 1 item after validation, not 0',\n            'input': set(),\n            'ctx': {'field_type': 'Set', 'min_length': 1, 'actual_length': 0},\n        },\n    ]\n\n    assert Model(req={'a'}, opt={'a'}).model_dump() == {'req': {'a'}, 'opt': {'a'}}\n\n\ndef test_constrained_set_constraints():\n    class ConSetModelBoth(BaseModel):\n        v: conset(int, min_length=7, max_length=11)\n\n    m = ConSetModelBoth(v=set(range(7)))\n    assert m.v == set(range(7))\n\n    m = ConSetModelBoth(v=set(range(11)))\n    assert m.v == set(range(11))\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConSetModelBoth(v=set(range(6)))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('v',),\n            'msg': 'Set should have at least 7 items after validation, not 6',\n            'input': {0, 1, 2, 3, 4, 5},\n            'ctx': {'field_type': 'Set', 'min_length': 7, 'actual_length': 6},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConSetModelBoth(v=set(range(12)))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': ('v',),\n            'msg': 'Set should have at most 11 items after validation, not more',\n            'input': {0, 8, 1, 9, 2, 10, 3, 7, 11, 4, 6, 5},\n            'ctx': {'field_type': 'Set', 'max_length': 11, 'actual_length': None},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConSetModelBoth(v=1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'set_type', 'loc': ('v',), 'msg': 'Input should be a valid set', 'input': 1}\n    ]\n\n\ndef test_constrained_set_item_type_fails():\n    class ConSetModel(BaseModel):\n        v: conset(int) = []\n\n    with pytest.raises(ValidationError) as exc_info:\n        ConSetModel(v=['a', 'b', 'c'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'b',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('v', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'c',\n        },\n    ]\n\n\ndef test_conset():\n    class Model(BaseModel):\n        foo: Set[int] = Field(..., min_length=2, max_length=4)\n        bar: conset(str, min_length=1, max_length=4) = None\n\n    assert Model(foo=[1, 2], bar=['spoon']).model_dump() == {'foo': {1, 2}, 'bar': {'spoon'}}\n\n    assert Model(foo=[1, 1, 1, 2, 2], bar=['spoon']).model_dump() == {'foo': {1, 2}, 'bar': {'spoon'}}\n\n    with pytest.raises(ValidationError, match='Set should have at least 2 items after validation, not 1'):\n        Model(foo=[1])\n\n    with pytest.raises(ValidationError, match='Set should have at most 4 items after validation, not more'):\n        Model(foo=list(range(5)))\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=[1, 'x', 'y'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'y',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'set_type', 'loc': ('foo',), 'msg': 'Input should be a valid set', 'input': 1}\n    ]\n\n\ndef test_conset_not_required():\n    class Model(BaseModel):\n        foo: Optional[Set[int]] = None\n\n    assert Model(foo=None).foo is None\n    assert Model().foo is None\n\n\ndef test_confrozenset():\n    class Model(BaseModel):\n        foo: FrozenSet[int] = Field(..., min_length=2, max_length=4)\n        bar: confrozenset(str, min_length=1, max_length=4) = None\n\n    m = Model(foo=[1, 2], bar=['spoon'])\n    assert m.model_dump() == {'foo': {1, 2}, 'bar': {'spoon'}}\n    assert isinstance(m.foo, frozenset)\n    assert isinstance(m.bar, frozenset)\n\n    assert Model(foo=[1, 1, 1, 2, 2], bar=['spoon']).model_dump() == {'foo': {1, 2}, 'bar': {'spoon'}}\n\n    with pytest.raises(ValidationError, match='Frozenset should have at least 2 items after validation, not 1'):\n        Model(foo=[1])\n\n    with pytest.raises(ValidationError, match='Frozenset should have at most 4 items after validation, not more'):\n        Model(foo=list(range(5)))\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=[1, 'x', 'y'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'y',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_set_type', 'loc': ('foo',), 'msg': 'Input should be a valid frozenset', 'input': 1}\n    ]\n\n\ndef test_confrozenset_not_required():\n    class Model(BaseModel):\n        foo: Optional[FrozenSet[int]] = None\n\n    assert Model(foo=None).foo is None\n    assert Model().foo is None\n\n\ndef test_constrained_frozenset_optional():\n    class Model(BaseModel):\n        req: Optional[confrozenset(str, min_length=1)]\n        opt: Optional[confrozenset(str, min_length=1)] = None\n\n    assert Model(req=None).model_dump() == {'req': None, 'opt': None}\n    assert Model(req=None, opt=None).model_dump() == {'req': None, 'opt': None}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(req=frozenset(), opt=frozenset())\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('req',),\n            'msg': 'Frozenset should have at least 1 item after validation, not 0',\n            'input': frozenset(),\n            'ctx': {'field_type': 'Frozenset', 'min_length': 1, 'actual_length': 0},\n        },\n        {\n            'type': 'too_short',\n            'loc': ('opt',),\n            'msg': 'Frozenset should have at least 1 item after validation, not 0',\n            'input': frozenset(),\n            'ctx': {'field_type': 'Frozenset', 'min_length': 1, 'actual_length': 0},\n        },\n    ]\n\n    assert Model(req={'a'}, opt={'a'}).model_dump() == {'req': {'a'}, 'opt': {'a'}}\n\n\n@pytest.fixture(scope='session', name='ConStringModel')\ndef constring_model_fixture():\n    class ConStringModel(BaseModel):\n        v: constr(max_length=10) = 'foobar'\n\n    return ConStringModel\n\n\ndef test_constrained_str_good(ConStringModel):\n    m = ConStringModel(v='short')\n    assert m.v == 'short'\n\n\ndef test_constrained_str_default(ConStringModel):\n    m = ConStringModel()\n    assert m.v == 'foobar'\n\n\n@pytest.mark.parametrize(\n    ('data', 'valid'),\n    [('this is too long', False), ('\u26c4' * 11, False), ('not long90', True), ('\u26c4' * 10, True)],\n)\ndef test_constrained_str_too_long(ConStringModel, data, valid):\n    if valid:\n        assert ConStringModel(v=data).model_dump() == {'v': data}\n    else:\n        with pytest.raises(ValidationError) as exc_info:\n            ConStringModel(v=data)\n        # insert_assert(exc_info.value.errors(include_url=False))\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'ctx': {'max_length': 10},\n                'input': data,\n                'loc': ('v',),\n                'msg': 'String should have at most 10 characters',\n                'type': 'string_too_long',\n            }\n        ]\n\n\n@pytest.mark.parametrize(\n    'to_upper, value, result',\n    [\n        (True, 'abcd', 'ABCD'),\n        (False, 'aBcD', 'aBcD'),\n    ],\n)\ndef test_constrained_str_upper(to_upper, value, result):\n    class Model(BaseModel):\n        v: constr(to_upper=to_upper)\n\n    m = Model(v=value)\n    assert m.v == result\n\n\n@pytest.mark.parametrize(\n    'to_lower, value, result',\n    [\n        (True, 'ABCD', 'abcd'),\n        (False, 'ABCD', 'ABCD'),\n    ],\n)\ndef test_constrained_str_lower(to_lower, value, result):\n    class Model(BaseModel):\n        v: constr(to_lower=to_lower)\n\n    m = Model(v=value)\n    assert m.v == result\n\n\ndef test_constrained_str_max_length_0():\n    class Model(BaseModel):\n        v: constr(max_length=0)\n\n    m = Model(v='')\n    assert m.v == ''\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v='qwe')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_too_long',\n            'loc': ('v',),\n            'msg': 'String should have at most 0 characters',\n            'input': 'qwe',\n            'ctx': {'max_length': 0},\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    'annotation',\n    [\n        ImportString[Callable[[Any], Any]],\n        Annotated[Callable[[Any], Any], ImportString],\n    ],\n)\ndef test_string_import_callable(annotation):\n    class PyObjectModel(BaseModel):\n        callable: annotation\n\n    m = PyObjectModel(callable='math.cos')\n    assert m.callable == math.cos\n\n    m = PyObjectModel(callable=math.cos)\n    assert m.callable == math.cos\n\n    with pytest.raises(ValidationError) as exc_info:\n        PyObjectModel(callable='foobar')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'import_error',\n            'loc': ('callable',),\n            'msg': \"Invalid python path: No module named 'foobar'\",\n            'input': 'foobar',\n            'ctx': {'error': \"No module named 'foobar'\"},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        PyObjectModel(callable='os.missing')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'import_error',\n            'loc': ('callable',),\n            'msg': \"Invalid python path: No module named 'os.missing'\",\n            'input': 'os.missing',\n            'ctx': {'error': \"No module named 'os.missing'\"},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        PyObjectModel(callable='os.path')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'callable_type', 'loc': ('callable',), 'msg': 'Input should be callable', 'input': os.path}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        PyObjectModel(callable=[1, 2, 3])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'callable_type', 'loc': ('callable',), 'msg': 'Input should be callable', 'input': [1, 2, 3]}\n    ]\n\n\n@pytest.mark.parametrize(\n    ('value', 'expected', 'mode'),\n    [\n        ('math:cos', 'math.cos', 'json'),\n        ('math:cos', math.cos, 'python'),\n        ('math.cos', 'math.cos', 'json'),\n        ('math.cos', math.cos, 'python'),\n        pytest.param(\n            'os.path', 'posixpath', 'json', marks=pytest.mark.skipif(sys.platform == 'win32', reason='different output')\n        ),\n        pytest.param(\n            'os.path', 'ntpath', 'json', marks=pytest.mark.skipif(sys.platform != 'win32', reason='different output')\n        ),\n        ('os.path', os.path, 'python'),\n        ([1, 2, 3], [1, 2, 3], 'json'),\n        ([1, 2, 3], [1, 2, 3], 'python'),\n        ('math', 'math', 'json'),\n        ('math', math, 'python'),\n        ('builtins.list', 'builtins.list', 'json'),\n        ('builtins.list', list, 'python'),\n        (list, 'builtins.list', 'json'),\n        (list, list, 'python'),\n        (f'{__name__}.pytest', 'pytest', 'json'),\n        (f'{__name__}.pytest', pytest, 'python'),\n    ],\n)\ndef test_string_import_any(value: Any, expected: Any, mode: Literal['json', 'python']):\n    class PyObjectModel(BaseModel):\n        thing: ImportString\n\n    assert PyObjectModel(thing=value).model_dump(mode=mode) == {'thing': expected}\n\n\n@pytest.mark.parametrize(\n    ('value', 'validate_default', 'expected'),\n    [\n        (math.cos, True, math.cos),\n        ('math:cos', True, math.cos),\n        (math.cos, False, math.cos),\n        ('math:cos', False, 'math:cos'),\n    ],\n)\ndef test_string_import_default_value(value: Any, validate_default: bool, expected: Any):\n    class PyObjectModel(BaseModel):\n        thing: ImportString = Field(default=value, validate_default=validate_default)\n\n    assert PyObjectModel().thing == expected\n\n\n@pytest.mark.parametrize('value', ['oss', 'os.os', f'{__name__}.x'])\ndef test_string_import_any_expected_failure(value: Any):\n    \"\"\"Ensure importString correctly fails to instantiate when it's supposed to\"\"\"\n\n    class PyObjectModel(BaseModel):\n        thing: ImportString\n\n    with pytest.raises(ValidationError, match='type=import_error'):\n        PyObjectModel(thing=value)\n\n\n@pytest.mark.parametrize(\n    'annotation',\n    [\n        ImportString[Annotated[float, annotated_types.Ge(3), annotated_types.Le(4)]],\n        Annotated[float, annotated_types.Ge(3), annotated_types.Le(4), ImportString],\n    ],\n)\ndef test_string_import_constraints(annotation):\n    class PyObjectModel(BaseModel):\n        thing: annotation\n\n    assert PyObjectModel(thing='math:pi').model_dump() == {'thing': pytest.approx(3.141592654)}\n    with pytest.raises(ValidationError, match='type=greater_than_equal'):\n        PyObjectModel(thing='math:e')\n\n\ndef test_string_import_examples():\n    import collections\n\n    adapter = TypeAdapter(ImportString)\n    assert adapter.validate_python('collections') is collections\n    assert adapter.validate_python('collections.abc') is collections.abc\n    assert adapter.validate_python('collections.abc.Mapping') is collections.abc.Mapping\n    assert adapter.validate_python('collections.abc:Mapping') is collections.abc.Mapping\n\n\n@pytest.mark.parametrize(\n    'import_string,errors',\n    [\n        (\n            'collections.abc.def',\n            [\n                {\n                    'ctx': {'error': \"No module named 'collections.abc.def'\"},\n                    'input': 'collections.abc.def',\n                    'loc': (),\n                    'msg': \"Invalid python path: No module named 'collections.abc.def'\",\n                    'type': 'import_error',\n                }\n            ],\n        ),\n        (\n            'collections.abc:def',\n            [\n                {\n                    'ctx': {'error': \"cannot import name 'def' from 'collections.abc'\"},\n                    'input': 'collections.abc:def',\n                    'loc': (),\n                    'msg': \"Invalid python path: cannot import name 'def' from 'collections.abc'\",\n                    'type': 'import_error',\n                }\n            ],\n        ),\n        (\n            'collections:abc:Mapping',\n            [\n                {\n                    'ctx': {'error': \"Import strings should have at most one ':'; received 'collections:abc:Mapping'\"},\n                    'input': 'collections:abc:Mapping',\n                    'loc': (),\n                    'msg': \"Invalid python path: Import strings should have at most one ':';\"\n                    \" received 'collections:abc:Mapping'\",\n                    'type': 'import_error',\n                }\n            ],\n        ),\n        (\n            '123_collections:Mapping',\n            [\n                {\n                    'ctx': {'error': \"No module named '123_collections'\"},\n                    'input': '123_collections:Mapping',\n                    'loc': (),\n                    'msg': \"Invalid python path: No module named '123_collections'\",\n                    'type': 'import_error',\n                }\n            ],\n        ),\n        (\n            ':Mapping',\n            [\n                {\n                    'ctx': {'error': \"Import strings should have a nonempty module name; received ':Mapping'\"},\n                    'input': ':Mapping',\n                    'loc': (),\n                    'msg': 'Invalid python path: Import strings should have a nonempty module '\n                    \"name; received ':Mapping'\",\n                    'type': 'import_error',\n                }\n            ],\n        ),\n    ],\n)\ndef test_string_import_errors(import_string, errors):\n    with pytest.raises(ValidationError) as exc_info:\n        TypeAdapter(ImportString).validate_python(import_string)\n    assert exc_info.value.errors() == errors\n\n\ndef test_decimal():\n    class Model(BaseModel):\n        v: Decimal\n\n    m = Model(v='1.234')\n    assert m.v == Decimal('1.234')\n    assert isinstance(m.v, Decimal)\n    assert m.model_dump() == {'v': Decimal('1.234')}\n\n\ndef test_decimal_allow_inf():\n    class MyModel(BaseModel):\n        value: Annotated[Decimal, AllowInfNan(True)]\n\n    m = MyModel(value='inf')\n    assert m.value == Decimal('inf')\n\n    m = MyModel(value=Decimal('inf'))\n    assert m.value == Decimal('inf')\n\n\ndef test_decimal_dont_allow_inf():\n    class MyModel(BaseModel):\n        value: Decimal\n\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        MyModel(value='inf')\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        MyModel(value=Decimal('inf'))\n\n\ndef test_decimal_strict():\n    class Model(BaseModel):\n        v: Decimal\n\n        model_config = ConfigDict(strict=True)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=1.23)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': ('v',),\n            'msg': 'Input should be an instance of Decimal',\n            'input': 1.23,\n            'ctx': {'class': 'Decimal'},\n        }\n    ]\n\n    v = Decimal(1.23)\n    assert Model(v=v).v == v\n    assert Model(v=v).model_dump() == {'v': v}\n\n    assert Model.model_validate_json('{\"v\": \"1.23\"}').v == Decimal('1.23')\n\n\ndef test_decimal_precision() -> None:\n    ta = TypeAdapter(Decimal)\n\n    num = f'{1234567890 * 100}.{1234567890 * 100}'\n\n    expected = Decimal(num)\n    assert ta.validate_python(num) == expected\n    assert ta.validate_json(f'\"{num}\"') == expected\n\n\ndef test_strict_date():\n    class Model(BaseModel):\n        v: Annotated[date, Field(strict=True)]\n\n    assert Model(v=date(2017, 5, 5)).v == date(2017, 5, 5)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=datetime(2017, 5, 5))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'date_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid date',\n            'input': datetime(2017, 5, 5),\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v='2017-05-05')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'date_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid date',\n            'input': '2017-05-05',\n        }\n    ]\n\n\ndef test_strict_datetime():\n    class Model(BaseModel):\n        v: Annotated[datetime, Field(strict=True)]\n\n    assert Model(v=datetime(2017, 5, 5, 10, 10, 10)).v == datetime(2017, 5, 5, 10, 10, 10)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=date(2017, 5, 5))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'datetime_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid datetime',\n            'input': date(2017, 5, 5),\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v='2017-05-05T10:10:10')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'datetime_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid datetime',\n            'input': '2017-05-05T10:10:10',\n        }\n    ]\n\n\ndef test_strict_time():\n    class Model(BaseModel):\n        v: Annotated[time, Field(strict=True)]\n\n    assert Model(v=time(10, 10, 10)).v == time(10, 10, 10)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v='10:10:10')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'time_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid time',\n            'input': '10:10:10',\n        }\n    ]\n\n\ndef test_strict_timedelta():\n    class Model(BaseModel):\n        v: Annotated[timedelta, Field(strict=True)]\n\n    assert Model(v=timedelta(days=1)).v == timedelta(days=1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v='1 days')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'time_delta_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid timedelta',\n            'input': '1 days',\n        }\n    ]\n\n\n@pytest.fixture(scope='session', name='CheckModel')\ndef check_model_fixture():\n    class CheckModel(BaseModel):\n        bool_check: bool = True\n        str_check: constr(strip_whitespace=True, max_length=10) = 's'\n        bytes_check: bytes = b's'\n        int_check: int = 1\n        float_check: float = 1.0\n        uuid_check: UUID = UUID('7bd00d58-6485-4ca6-b889-3da6d8df3ee4')\n        decimal_check: condecimal(allow_inf_nan=False) = Decimal('42.24')\n        date_check: date = date(2017, 5, 5)\n        datetime_check: datetime = datetime(2017, 5, 5, 10, 10, 10)\n        time_check: time = time(10, 10, 10)\n        timedelta_check: timedelta = timedelta(days=1)\n        list_check: List[str] = ['1', '2']\n        tuple_check: Tuple[str, ...] = ('1', '2')\n        set_check: Set[str] = {'1', '2'}\n        frozenset_check: FrozenSet[str] = frozenset(['1', '2'])\n\n    return CheckModel\n\n\nclass BoolCastable:\n    def __bool__(self) -> bool:\n        return True\n\n\n@pytest.mark.parametrize(\n    'field,value,result',\n    [\n        ('bool_check', True, True),\n        ('bool_check', 1, True),\n        ('bool_check', 1.0, True),\n        ('bool_check', Decimal(1), True),\n        ('bool_check', 'y', True),\n        ('bool_check', 'Y', True),\n        ('bool_check', 'yes', True),\n        ('bool_check', 'Yes', True),\n        ('bool_check', 'YES', True),\n        ('bool_check', 'true', True),\n        ('bool_check', 'True', True),\n        ('bool_check', 'TRUE', True),\n        ('bool_check', 'on', True),\n        ('bool_check', 'On', True),\n        ('bool_check', 'ON', True),\n        ('bool_check', '1', True),\n        ('bool_check', 't', True),\n        ('bool_check', 'T', True),\n        ('bool_check', b'TRUE', True),\n        ('bool_check', False, False),\n        ('bool_check', 0, False),\n        ('bool_check', 0.0, False),\n        ('bool_check', Decimal(0), False),\n        ('bool_check', 'n', False),\n        ('bool_check', 'N', False),\n        ('bool_check', 'no', False),\n        ('bool_check', 'No', False),\n        ('bool_check', 'NO', False),\n        ('bool_check', 'false', False),\n        ('bool_check', 'False', False),\n        ('bool_check', 'FALSE', False),\n        ('bool_check', 'off', False),\n        ('bool_check', 'Off', False),\n        ('bool_check', 'OFF', False),\n        ('bool_check', '0', False),\n        ('bool_check', 'f', False),\n        ('bool_check', 'F', False),\n        ('bool_check', b'FALSE', False),\n        ('bool_check', None, ValidationError),\n        ('bool_check', '', ValidationError),\n        ('bool_check', [], ValidationError),\n        ('bool_check', {}, ValidationError),\n        ('bool_check', [1, 2, 3, 4], ValidationError),\n        ('bool_check', {1: 2, 3: 4}, ValidationError),\n        ('bool_check', b'2', ValidationError),\n        ('bool_check', '2', ValidationError),\n        ('bool_check', 2, ValidationError),\n        ('bool_check', 2.0, ValidationError),\n        ('bool_check', Decimal(2), ValidationError),\n        ('bool_check', b'\\x81', ValidationError),\n        ('bool_check', BoolCastable(), ValidationError),\n        ('str_check', 's', 's'),\n        ('str_check', '  s  ', 's'),\n        ('str_check', ' leading', 'leading'),\n        ('str_check', 'trailing ', 'trailing'),\n        ('str_check', b's', 's'),\n        ('str_check', b'  s  ', 's'),\n        ('str_check', bytearray(b's' * 5), 'sssss'),\n        ('str_check', 1, ValidationError),\n        ('str_check', 'x' * 11, ValidationError),\n        ('str_check', b'x' * 11, ValidationError),\n        ('str_check', b'\\x81', ValidationError),\n        ('str_check', bytearray(b'\\x81' * 5), ValidationError),\n        ('bytes_check', 's', b's'),\n        ('bytes_check', '  s  ', b'  s  '),\n        ('bytes_check', b's', b's'),\n        ('bytes_check', 1, ValidationError),\n        ('bytes_check', bytearray('xx', encoding='utf8'), b'xx'),\n        ('bytes_check', True, ValidationError),\n        ('bytes_check', False, ValidationError),\n        ('bytes_check', {}, ValidationError),\n        ('bytes_check', 'x' * 11, b'x' * 11),\n        ('bytes_check', b'x' * 11, b'x' * 11),\n        ('int_check', 1, 1),\n        ('int_check', 1.0, 1),\n        ('int_check', 1.9, ValidationError),\n        ('int_check', Decimal(1), 1),\n        ('int_check', Decimal(1.9), ValidationError),\n        ('int_check', '1', 1),\n        ('int_check', '1.9', ValidationError),\n        ('int_check', b'1', 1),\n        ('int_check', 12, 12),\n        ('int_check', '12', 12),\n        ('int_check', b'12', 12),\n        ('float_check', 1, 1.0),\n        ('float_check', 1.0, 1.0),\n        ('float_check', Decimal(1.0), 1.0),\n        ('float_check', '1.0', 1.0),\n        ('float_check', '1', 1.0),\n        ('float_check', b'1.0', 1.0),\n        ('float_check', b'1', 1.0),\n        ('float_check', True, 1.0),\n        ('float_check', False, 0.0),\n        ('float_check', 't', ValidationError),\n        ('float_check', b't', ValidationError),\n        ('uuid_check', 'ebcdab58-6eb8-46fb-a190-d07a33e9eac8', UUID('ebcdab58-6eb8-46fb-a190-d07a33e9eac8')),\n        ('uuid_check', UUID('ebcdab58-6eb8-46fb-a190-d07a33e9eac8'), UUID('ebcdab58-6eb8-46fb-a190-d07a33e9eac8')),\n        ('uuid_check', b'ebcdab58-6eb8-46fb-a190-d07a33e9eac8', UUID('ebcdab58-6eb8-46fb-a190-d07a33e9eac8')),\n        ('uuid_check', b'\\x12\\x34\\x56\\x78' * 4, UUID('12345678-1234-5678-1234-567812345678')),\n        ('uuid_check', 'ebcdab58-6eb8-46fb-a190-', ValidationError),\n        ('uuid_check', 123, ValidationError),\n        ('decimal_check', 42.24, Decimal('42.24')),\n        ('decimal_check', '42.24', Decimal('42.24')),\n        ('decimal_check', b'42.24', ValidationError),\n        ('decimal_check', '  42.24  ', Decimal('42.24')),\n        ('decimal_check', Decimal('42.24'), Decimal('42.24')),\n        ('decimal_check', 'not a valid decimal', ValidationError),\n        ('decimal_check', 'NaN', ValidationError),\n        ('date_check', date(2017, 5, 5), date(2017, 5, 5)),\n        ('date_check', datetime(2017, 5, 5), date(2017, 5, 5)),\n        ('date_check', '2017-05-05', date(2017, 5, 5)),\n        ('date_check', b'2017-05-05', date(2017, 5, 5)),\n        ('date_check', 1493942400000, date(2017, 5, 5)),\n        ('date_check', 1493942400, date(2017, 5, 5)),\n        ('date_check', 1493942400000.0, date(2017, 5, 5)),\n        ('date_check', Decimal(1493942400000), date(2017, 5, 5)),\n        ('date_check', datetime(2017, 5, 5, 10), ValidationError),\n        ('date_check', '2017-5-5', ValidationError),\n        ('date_check', b'2017-5-5', ValidationError),\n        ('date_check', 1493942401000, ValidationError),\n        ('date_check', 1493942401000.0, ValidationError),\n        ('date_check', Decimal(1493942401000), ValidationError),\n        ('datetime_check', datetime(2017, 5, 5, 10, 10, 10), datetime(2017, 5, 5, 10, 10, 10)),\n        ('datetime_check', date(2017, 5, 5), datetime(2017, 5, 5, 0, 0, 0)),\n        ('datetime_check', '2017-05-05T10:10:10.0002', datetime(2017, 5, 5, 10, 10, 10, microsecond=200)),\n        ('datetime_check', '2017-05-05 10:10:10', datetime(2017, 5, 5, 10, 10, 10)),\n        ('datetime_check', '2017-05-05 10:10:10+00:00', datetime(2017, 5, 5, 10, 10, 10, tzinfo=timezone.utc)),\n        ('datetime_check', b'2017-05-05T10:10:10.0002', datetime(2017, 5, 5, 10, 10, 10, microsecond=200)),\n        ('datetime_check', 1493979010000, datetime(2017, 5, 5, 10, 10, 10, tzinfo=timezone.utc)),\n        ('datetime_check', 1493979010, datetime(2017, 5, 5, 10, 10, 10, tzinfo=timezone.utc)),\n        ('datetime_check', 1493979010000.0, datetime(2017, 5, 5, 10, 10, 10, tzinfo=timezone.utc)),\n        ('datetime_check', Decimal(1493979010), datetime(2017, 5, 5, 10, 10, 10, tzinfo=timezone.utc)),\n        ('datetime_check', '2017-5-5T10:10:10', ValidationError),\n        ('datetime_check', b'2017-5-5T10:10:10', ValidationError),\n        ('time_check', time(10, 10, 10), time(10, 10, 10)),\n        ('time_check', '10:10:10.0002', time(10, 10, 10, microsecond=200)),\n        ('time_check', b'10:10:10.0002', time(10, 10, 10, microsecond=200)),\n        ('time_check', 3720, time(1, 2, tzinfo=timezone.utc)),\n        ('time_check', 3720.0002, time(1, 2, microsecond=200, tzinfo=timezone.utc)),\n        ('time_check', Decimal(3720.0002), time(1, 2, microsecond=200, tzinfo=timezone.utc)),\n        ('time_check', '1:1:1', ValidationError),\n        ('time_check', b'1:1:1', ValidationError),\n        ('time_check', -1, ValidationError),\n        ('time_check', 86400, ValidationError),\n        ('time_check', 86400.0, ValidationError),\n        ('time_check', Decimal(86400), ValidationError),\n        ('timedelta_check', timedelta(days=1), timedelta(days=1)),\n        ('timedelta_check', '1 days 10:10', timedelta(days=1, seconds=36600)),\n        ('timedelta_check', '1 d 10:10', timedelta(days=1, seconds=36600)),\n        ('timedelta_check', b'1 days 10:10', timedelta(days=1, seconds=36600)),\n        ('timedelta_check', 123_000, timedelta(days=1, seconds=36600)),\n        ('timedelta_check', 123_000.0002, timedelta(days=1, seconds=36600, microseconds=200)),\n        ('timedelta_check', Decimal(123_000.0002), timedelta(days=1, seconds=36600, microseconds=200)),\n        ('timedelta_check', '1 10:10', ValidationError),\n        ('timedelta_check', b'1 10:10', ValidationError),\n        ('list_check', ['1', '2'], ['1', '2']),\n        ('list_check', ('1', '2'), ['1', '2']),\n        ('list_check', {'1': 1, '2': 2}.keys(), ['1', '2']),\n        ('list_check', {'1': '1', '2': '2'}.values(), ['1', '2']),\n        ('list_check', {'1', '2'}, dirty_equals.IsOneOf(['1', '2'], ['2', '1'])),\n        ('list_check', frozenset(['1', '2']), dirty_equals.IsOneOf(['1', '2'], ['2', '1'])),\n        ('list_check', {'1': 1, '2': 2}, ValidationError),\n        ('tuple_check', ('1', '2'), ('1', '2')),\n        ('tuple_check', ['1', '2'], ('1', '2')),\n        ('tuple_check', {'1': 1, '2': 2}.keys(), ('1', '2')),\n        ('tuple_check', {'1': '1', '2': '2'}.values(), ('1', '2')),\n        ('tuple_check', {'1', '2'}, dirty_equals.IsOneOf(('1', '2'), ('2', '1'))),\n        ('tuple_check', frozenset(['1', '2']), dirty_equals.IsOneOf(('1', '2'), ('2', '1'))),\n        ('tuple_check', {'1': 1, '2': 2}, ValidationError),\n        ('set_check', {'1', '2'}, {'1', '2'}),\n        ('set_check', ['1', '2', '1', '2'], {'1', '2'}),\n        ('set_check', ('1', '2', '1', '2'), {'1', '2'}),\n        ('set_check', frozenset(['1', '2']), {'1', '2'}),\n        ('set_check', {'1': 1, '2': 2}.keys(), {'1', '2'}),\n        ('set_check', {'1': '1', '2': '2'}.values(), {'1', '2'}),\n        ('set_check', {'1': 1, '2': 2}, ValidationError),\n        ('frozenset_check', frozenset(['1', '2']), frozenset(['1', '2'])),\n        ('frozenset_check', ['1', '2', '1', '2'], frozenset(['1', '2'])),\n        ('frozenset_check', ('1', '2', '1', '2'), frozenset(['1', '2'])),\n        ('frozenset_check', {'1', '2'}, frozenset(['1', '2'])),\n        ('frozenset_check', {'1': 1, '2': 2}.keys(), frozenset(['1', '2'])),\n        ('frozenset_check', {'1': '1', '2': '2'}.values(), frozenset(['1', '2'])),\n        ('frozenset_check', {'1': 1, '2': 2}, ValidationError),\n    ],\n)\ndef test_default_validators(field, value, result, CheckModel):\n    kwargs = {field: value}\n    if result == ValidationError:\n        with pytest.raises(ValidationError):\n            CheckModel(**kwargs)\n    else:\n        assert CheckModel(**kwargs).model_dump()[field] == result\n\n\n@pytest.fixture(scope='session', name='StrModel')\ndef str_model_fixture():\n    class StrModel(BaseModel):\n        str_check: Annotated[str, annotated_types.Len(5, 10)]\n\n    return StrModel\n\n\ndef test_string_too_long(StrModel):\n    with pytest.raises(ValidationError) as exc_info:\n        StrModel(str_check='x' * 150)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_too_long',\n            'loc': ('str_check',),\n            'msg': 'String should have at most 10 characters',\n            'input': 'x' * 150,\n            'ctx': {'max_length': 10},\n        }\n    ]\n\n\ndef test_string_too_short(StrModel):\n    with pytest.raises(ValidationError) as exc_info:\n        StrModel(str_check='x')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_too_short',\n            'loc': ('str_check',),\n            'msg': 'String should have at least 5 characters',\n            'input': 'x',\n            'ctx': {'min_length': 5},\n        }\n    ]\n\n\n@pytest.fixture(scope='session', name='DatetimeModel')\ndef datetime_model_fixture():\n    class DatetimeModel(BaseModel):\n        dt: datetime\n        date_: date\n        time_: time\n        duration: timedelta\n\n    return DatetimeModel\n\n\ndef test_datetime_successful(DatetimeModel):\n    m = DatetimeModel(dt='2017-10-05T19:47:07', date_=1493942400, time_='10:20:30.400', duration='00:15:30.0001')\n    assert m.dt == datetime(2017, 10, 5, 19, 47, 7)\n    assert m.date_ == date(2017, 5, 5)\n    assert m.time_ == time(10, 20, 30, 400_000)\n    assert m.duration == timedelta(minutes=15, seconds=30, microseconds=100)\n\n\ndef test_datetime_errors(DatetimeModel):\n    with pytest.raises(ValueError) as exc_info:\n        DatetimeModel(dt='2017-13-05T19:47:07', date_='XX1494012000', time_='25:20:30.400', duration='15:30.0001broken')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'datetime_from_date_parsing',\n            'loc': ('dt',),\n            'msg': 'Input should be a valid datetime or date, month value is outside expected range of 1-12',\n            'input': '2017-13-05T19:47:07',\n            'ctx': {'error': 'month value is outside expected range of 1-12'},\n        },\n        {\n            'type': 'date_from_datetime_parsing',\n            'loc': ('date_',),\n            'msg': 'Input should be a valid date or datetime, invalid character in year',\n            'input': 'XX1494012000',\n            'ctx': {'error': 'invalid character in year'},\n        },\n        {\n            'type': 'time_parsing',\n            'loc': ('time_',),\n            'msg': 'Input should be in a valid time format, hour value is outside expected range of 0-23',\n            'input': '25:20:30.400',\n            'ctx': {'error': 'hour value is outside expected range of 0-23'},\n        },\n        {\n            'type': 'time_delta_parsing',\n            'loc': ('duration',),\n            'msg': 'Input should be a valid timedelta, unexpected extra characters at the end of the input',\n            'input': '15:30.0001broken',\n            'ctx': {'error': 'unexpected extra characters at the end of the input'},\n        },\n    ]\n\n\n@pytest.fixture(scope='session')\ndef cooking_model():\n    class FruitEnum(str, Enum):\n        pear = 'pear'\n        banana = 'banana'\n\n    class ToolEnum(IntEnum):\n        spanner = 1\n        wrench = 2\n\n    class CookingModel(BaseModel):\n        fruit: FruitEnum = FruitEnum.pear\n        tool: ToolEnum = ToolEnum.spanner\n\n    return FruitEnum, ToolEnum, CookingModel\n\n\ndef test_enum_successful(cooking_model):\n    FruitEnum, ToolEnum, CookingModel = cooking_model\n    m = CookingModel(tool=2)\n    assert m.fruit == FruitEnum.pear\n    assert m.tool == ToolEnum.wrench\n    assert repr(m.tool) == '<ToolEnum.wrench: 2>'\n\n\ndef test_enum_fails(cooking_model):\n    FruitEnum, ToolEnum, CookingModel = cooking_model\n    with pytest.raises(ValueError) as exc_info:\n        CookingModel(tool=3)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'expected': '1 or 2'},\n            'input': 3,\n            'loc': ('tool',),\n            'msg': 'Input should be 1 or 2',\n            'type': 'enum',\n        }\n    ]\n\n\ndef test_enum_fails_error_msg():\n    class Number(IntEnum):\n        one = 1\n        two = 2\n        three = 3\n\n    class Model(BaseModel):\n        num: Number\n\n    with pytest.raises(ValueError) as exc_info:\n        Model(num=4)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'enum',\n            'loc': ('num',),\n            'msg': 'Input should be 1, 2 or 3',\n            'input': 4,\n            'ctx': {'expected': '1, 2 or 3'},\n        }\n    ]\n\n\ndef test_int_enum_successful_for_str_int(cooking_model):\n    FruitEnum, ToolEnum, CookingModel = cooking_model\n    m = CookingModel(tool='2')\n    assert m.tool == ToolEnum.wrench\n    assert repr(m.tool) == '<ToolEnum.wrench: 2>'\n\n\ndef test_plain_enum_validate():\n    class MyEnum(Enum):\n        a = 1\n\n    class Model(BaseModel):\n        x: MyEnum\n\n    m = Model(x=MyEnum.a)\n    assert m.x is MyEnum.a\n\n    assert TypeAdapter(MyEnum).validate_python(1) is MyEnum.a\n    with pytest.raises(ValidationError) as exc_info:\n        TypeAdapter(MyEnum).validate_python(1, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'test_plain_enum_validate.<locals>.MyEnum'},\n            'input': 1,\n            'loc': (),\n            'msg': IsStr(regex='Input should be an instance of test_plain_enum_validate.<locals>.MyEnum'),\n            'type': 'is_instance_of',\n        }\n    ]\n\n    assert TypeAdapter(MyEnum).validate_json('1') is MyEnum.a\n    TypeAdapter(MyEnum).validate_json('1', strict=True)\n    with pytest.raises(ValidationError) as exc_info:\n        TypeAdapter(MyEnum).validate_json('\"1\"', strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'ctx': {'expected': '1'}, 'input': '1', 'loc': (), 'msg': 'Input should be 1', 'type': 'enum'}\n    ]\n\n\ndef test_plain_enum_validate_json():\n    class MyEnum(Enum):\n        a = 1\n\n    class Model(BaseModel):\n        x: MyEnum\n\n    m = Model.model_validate_json('{\"x\":1}')\n    assert m.x is MyEnum.a\n\n\ndef test_enum_type():\n    class Model(BaseModel):\n        my_enum: Enum\n\n    class MyEnum(Enum):\n        a = 1\n\n    m = Model(my_enum=MyEnum.a)\n    assert m.my_enum == MyEnum.a\n    assert m.model_dump() == {'my_enum': MyEnum.a}\n    assert m.model_dump_json() == '{\"my_enum\":1}'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(my_enum=1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'Enum'},\n            'input': 1,\n            'loc': ('my_enum',),\n            'msg': 'Input should be an instance of Enum',\n            'type': 'is_instance_of',\n        }\n    ]\n\n\ndef test_enum_missing_default():\n    class MyEnum(Enum):\n        a = 1\n\n    ta = TypeAdapter(MyEnum)\n    missing_value = re.search(r'missing: (\\w+)', repr(ta.validator)).group(1)\n    assert missing_value == 'None'\n\n    assert ta.validate_python(1) is MyEnum.a\n    with pytest.raises(ValidationError):\n        ta.validate_python(2)\n\n\ndef test_enum_missing_custom():\n    class MyEnum(Enum):\n        a = 1\n\n        @classmethod\n        def _missing_(cls, value):\n            return MyEnum.a\n\n    ta = TypeAdapter(MyEnum)\n    missing_value = re.search(r'missing: (\\w+)', repr(ta.validator)).group(1)\n    assert missing_value == 'Some'\n\n    assert ta.validate_python(1) is MyEnum.a\n    assert ta.validate_python(2) is MyEnum.a\n\n\ndef test_int_enum_type():\n    class Model(BaseModel):\n        my_enum: IntEnum\n\n    class MyEnum(Enum):\n        a = 1\n\n    class MyIntEnum(IntEnum):\n        b = 2\n\n    m = Model(my_enum=MyIntEnum.b)\n    assert m.my_enum == MyIntEnum.b\n    assert m.model_dump() == {'my_enum': MyIntEnum.b}\n    assert m.model_dump_json() == '{\"my_enum\":2}'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(my_enum=MyEnum.a)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'IntEnum'},\n            'input': MyEnum.a,\n            'loc': ('my_enum',),\n            'msg': 'Input should be an instance of IntEnum',\n            'type': 'is_instance_of',\n        }\n    ]\n\n\n@pytest.mark.parametrize('enum_base,strict', [(Enum, False), (IntEnum, False), (IntEnum, True)])\ndef test_enum_from_json(enum_base, strict):\n    class MyEnum(enum_base):\n        a = 1\n        b = 3\n\n    class Model(BaseModel):\n        my_enum: MyEnum\n\n    m = Model.model_validate_json('{\"my_enum\":1}', strict=strict)\n    assert m.my_enum is MyEnum.a\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate_json('{\"my_enum\":2}', strict=strict)\n\n    MyEnum.__name__ if sys.version_info[:2] <= (3, 8) else MyEnum.__qualname__\n\n    if strict:\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'ctx': {'expected': '1 or 3'},\n                'input': 2,\n                'loc': ('my_enum',),\n                'msg': 'Input should be 1 or 3',\n                'type': 'enum',\n            }\n        ]\n    else:\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'ctx': {'expected': '1 or 3'},\n                'input': 2,\n                'loc': ('my_enum',),\n                'msg': 'Input should be 1 or 3',\n                'type': 'enum',\n            }\n        ]\n\n\ndef test_strict_enum() -> None:\n    class Demo(Enum):\n        A = 0\n        B = 1\n\n    class User(BaseModel):\n        model_config = ConfigDict(strict=True)\n\n        demo_strict: Demo\n        demo_not_strict: Demo = Field(strict=False)\n\n    user = User(demo_strict=Demo.A, demo_not_strict=1)\n\n    assert isinstance(user.demo_strict, Demo)\n    assert isinstance(user.demo_not_strict, Demo)\n    assert user.demo_strict.value == 0\n    assert user.demo_not_strict.value == 1\n\n    with pytest.raises(ValidationError, match='Input should be an instance of test_strict_enum.<locals>.Demo'):\n        User(demo_strict=0, demo_not_strict=1)\n\n\ndef test_enum_with_no_cases() -> None:\n    class MyEnum(Enum):\n        pass\n\n    class MyModel(BaseModel):\n        e: MyEnum\n\n    json_schema = MyModel.model_json_schema()\n    assert json_schema['properties']['e']['enum'] == []\n\n\n@pytest.mark.parametrize(\n    'kwargs,type_',\n    [\n        pytest.param(\n            {'pattern': '^foo$'},\n            int,\n            marks=pytest.mark.xfail(\n                reason='int cannot be used with pattern but we do not currently validate that at schema build time'\n            ),\n        ),\n        ({'gt': 0}, conlist(int, min_length=4)),\n        ({'gt': 0}, conset(int, min_length=4)),\n        ({'gt': 0}, confrozenset(int, min_length=4)),\n    ],\n)\ndef test_invalid_schema_constraints(kwargs, type_):\n    match = (\n        r'(:?Invalid Schema:\\n.*\\n  Extra inputs are not permitted)|(:?The following constraints cannot be applied to)'\n    )\n    with pytest.raises((SchemaError, TypeError), match=match):\n\n        class Foo(BaseModel):\n            a: type_ = Field('foo', title='A title', description='A description', **kwargs)\n\n\ndef test_invalid_decimal_constraint():\n    with pytest.raises(\n        TypeError, match=\"The following constraints cannot be applied to <class 'decimal.Decimal'>: 'max_length'\"\n    ):\n\n        class Foo(BaseModel):\n            a: Decimal = Field('foo', title='A title', description='A description', max_length=5)\n\n\n@pytest.mark.skipif(not email_validator, reason='email_validator not installed')\ndef test_string_success():\n    class MoreStringsModel(BaseModel):\n        str_strip_enabled: constr(strip_whitespace=True)\n        str_strip_disabled: constr(strip_whitespace=False)\n        str_regex: constr(pattern=r'^xxx\\d{3}$') = ...\n        str_min_length: constr(min_length=5) = ...\n        str_email: EmailStr = ...\n        name_email: NameEmail = ...\n        str_gt: Annotated[str, annotated_types.Gt('a')]\n\n    m = MoreStringsModel(\n        str_strip_enabled='   xxx123   ',\n        str_strip_disabled='   xxx123   ',\n        str_regex='xxx123',\n        str_min_length='12345',\n        str_email='foobar@example.com  ',\n        name_email='foo bar  <foobaR@example.com>',\n        str_gt='b',\n    )\n    assert m.str_strip_enabled == 'xxx123'\n    assert m.str_strip_disabled == '   xxx123   '\n    assert m.str_regex == 'xxx123'\n    assert m.str_email == 'foobar@example.com'\n    assert repr(m.name_email) == \"NameEmail(name='foo bar', email='foobaR@example.com')\"\n    assert str(m.name_email) == 'foo bar <foobaR@example.com>'\n    assert m.name_email.name == 'foo bar'\n    assert m.name_email.email == 'foobaR@example.com'\n    assert m.str_gt == 'b'\n\n\n@pytest.mark.skipif(not email_validator, reason='email_validator not installed')\ndef test_string_fails():\n    class MoreStringsModel(BaseModel):\n        str_regex: constr(pattern=r'^xxx\\d{3}$') = ...\n        str_min_length: constr(min_length=5) = ...\n        str_email: EmailStr = ...\n        name_email: NameEmail = ...\n\n    with pytest.raises(ValidationError) as exc_info:\n        MoreStringsModel(\n            str_regex='xxx123xxx',\n            str_min_length='1234',\n            str_email='foobar<@example.com',\n            name_email='foobar @example.com',\n        )\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_pattern_mismatch',\n            'loc': ('str_regex',),\n            'msg': \"String should match pattern '^xxx\\\\d{3}$'\",\n            'input': 'xxx123xxx',\n            'ctx': {'pattern': '^xxx\\\\d{3}$'},\n        },\n        {\n            'type': 'string_too_short',\n            'loc': ('str_min_length',),\n            'msg': 'String should have at least 5 characters',\n            'input': '1234',\n            'ctx': {'min_length': 5},\n        },\n        {\n            'type': 'value_error',\n            'loc': ('str_email',),\n            'msg': 'value is not a valid email address: An open angle bracket at the start of the email address has to be followed by a close angle bracket at the end.',\n            'input': 'foobar<@example.com',\n            'ctx': {\n                'reason': 'An open angle bracket at the start of the email address has to be followed by a close angle bracket at the end.'\n            },\n        },\n        {\n            'type': 'value_error',\n            'loc': ('name_email',),\n            'msg': 'value is not a valid email address: The email address contains invalid characters before the @-sign: SPACE.',\n            'input': 'foobar @example.com',\n            'ctx': {'reason': 'The email address contains invalid characters before the @-sign: SPACE.'},\n        },\n    ]\n\n\n@pytest.mark.skipif(email_validator, reason='email_validator is installed')\ndef test_email_validator_not_installed_email_str():\n    with pytest.raises(ImportError):\n\n        class Model(BaseModel):\n            str_email: EmailStr = ...\n\n\n@pytest.mark.skipif(email_validator, reason='email_validator is installed')\ndef test_email_validator_not_installed_name_email():\n    with pytest.raises(ImportError):\n\n        class Model(BaseModel):\n            str_email: NameEmail = ...\n\n\ndef test_dict():\n    class Model(BaseModel):\n        v: dict\n\n    assert Model(v={1: 10, 2: 20}).v == {1: 10, 2: 20}\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=[(1, 2), (3, 4)])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dict_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid dictionary',\n            'input': [(1, 2), (3, 4)],\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=[1, 2, 3])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'dict_type', 'loc': ('v',), 'msg': 'Input should be a valid dictionary', 'input': [1, 2, 3]}\n    ]\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    (\n        ([1, 2, '3'], [1, 2, '3']),\n        ((1, 2, '3'), [1, 2, '3']),\n        ((i**2 for i in range(5)), [0, 1, 4, 9, 16]),\n        (deque([1, 2, 3]), [1, 2, 3]),\n        ({1, '2'}, IsOneOf([1, '2'], ['2', 1])),\n    ),\n)\ndef test_list_success(value, result):\n    class Model(BaseModel):\n        v: list\n\n    assert Model(v=value).v == result\n\n\n@pytest.mark.parametrize('value', (123, '123'))\ndef test_list_fails(value):\n    class Model(BaseModel):\n        v: list\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'list_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid list',\n            'input': value,\n        }\n    ]\n\n\ndef test_ordered_dict():\n    class Model(BaseModel):\n        v: OrderedDict\n\n    assert Model(v=OrderedDict([(1, 10), (2, 20)])).v == OrderedDict([(1, 10), (2, 20)])\n    assert Model(v={1: 10, 2: 20}).v == OrderedDict([(1, 10), (2, 20)])\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=[1, 2, 3])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'dict_type', 'loc': ('v',), 'msg': 'Input should be a valid dictionary', 'input': [1, 2, 3]}\n    ]\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    (\n        ([1, 2, '3'], (1, 2, '3')),\n        ((1, 2, '3'), (1, 2, '3')),\n        ((i**2 for i in range(5)), (0, 1, 4, 9, 16)),\n        (deque([1, 2, 3]), (1, 2, 3)),\n        ({1, '2'}, IsOneOf((1, '2'), ('2', 1))),\n    ),\n)\ndef test_tuple_success(value, result):\n    class Model(BaseModel):\n        v: tuple\n\n    assert Model(v=value).v == result\n\n\n@pytest.mark.parametrize('value', (123, '123'))\ndef test_tuple_fails(value):\n    class Model(BaseModel):\n        v: tuple\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'tuple_type', 'loc': ('v',), 'msg': 'Input should be a valid tuple', 'input': value}\n    ]\n\n\n@pytest.mark.parametrize(\n    'value,cls,result',\n    (\n        ([1, 2, '3'], int, (1, 2, 3)),\n        ((1, 2, '3'), int, (1, 2, 3)),\n        ((i**2 for i in range(5)), int, (0, 1, 4, 9, 16)),\n        (('a', 'b', 'c'), str, ('a', 'b', 'c')),\n    ),\n)\ndef test_tuple_variable_len_success(value, cls, result):\n    class Model(BaseModel):\n        v: Tuple[cls, ...]\n\n    assert Model(v=value).v == result\n\n\n@pytest.mark.parametrize(\n    'value, cls, exc',\n    [\n        (\n            ('a', 'b', [1, 2], 'c'),\n            str,\n            [\n                {\n                    'type': 'string_type',\n                    'loc': ('v', 2),\n                    'msg': 'Input should be a valid string',\n                    'input': [1, 2],\n                }\n            ],\n        ),\n        (\n            ('a', 'b', [1, 2], 'c', [3, 4]),\n            str,\n            [\n                {\n                    'type': 'string_type',\n                    'loc': ('v', 2),\n                    'msg': 'Input should be a valid string',\n                    'input': [1, 2],\n                },\n                {\n                    'type': 'string_type',\n                    'loc': ('v', 4),\n                    'msg': 'Input should be a valid string',\n                    'input': [3, 4],\n                },\n            ],\n        ),\n    ],\n)\ndef test_tuple_variable_len_fails(value, cls, exc):\n    class Model(BaseModel):\n        v: Tuple[cls, ...]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    assert exc_info.value.errors(include_url=False) == exc\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    (\n        ({1, 2, 2, '3'}, {1, 2, '3'}),\n        ((1, 2, 2, '3'), {1, 2, '3'}),\n        ([1, 2, 2, '3'], {1, 2, '3'}),\n        ({i**2 for i in range(5)}, {0, 1, 4, 9, 16}),\n    ),\n)\ndef test_set_success(value, result):\n    class Model(BaseModel):\n        v: set\n\n    assert Model(v=value).v == result\n\n\n@pytest.mark.parametrize('value', (123, '123'))\ndef test_set_fails(value):\n    class Model(BaseModel):\n        v: set\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'set_type', 'loc': ('v',), 'msg': 'Input should be a valid set', 'input': value}\n    ]\n\n\ndef test_list_type_fails():\n    class Model(BaseModel):\n        v: List[int]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v='123')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'list_type', 'loc': ('v',), 'msg': 'Input should be a valid list', 'input': '123'}\n    ]\n\n\ndef test_set_type_fails():\n    class Model(BaseModel):\n        v: Set[int]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v='123')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'set_type', 'loc': ('v',), 'msg': 'Input should be a valid set', 'input': '123'}\n    ]\n\n\n@pytest.mark.parametrize(\n    'cls, value,result',\n    (\n        (int, [1, 2, 3], [1, 2, 3]),\n        (int, (1, 2, 3), (1, 2, 3)),\n        (int, range(5), [0, 1, 2, 3, 4]),\n        (int, deque((1, 2, 3)), deque((1, 2, 3))),\n        (Set[int], [{1, 2}, {3, 4}, {5, 6}], [{1, 2}, {3, 4}, {5, 6}]),\n        (Tuple[int, str], ((1, 'a'), (2, 'b'), (3, 'c')), ((1, 'a'), (2, 'b'), (3, 'c'))),\n    ),\n)\ndef test_sequence_success(cls, value, result):\n    class Model(BaseModel):\n        v: Sequence[cls]\n\n    assert Model(v=value).v == result\n\n\ndef int_iterable():\n    i = 0\n    while True:\n        i += 1\n        yield str(i)\n\n\ndef str_iterable():\n    while True:\n        yield from 'foobarbaz'\n\n\ndef test_infinite_iterable_int():\n    class Model(BaseModel):\n        it: Iterable[int]\n\n    m = Model(it=int_iterable())\n\n    assert repr(m.it) == 'ValidatorIterator(index=0, schema=Some(Int(IntValidator { strict: false })))'\n\n    output = []\n    for i in m.it:\n        output.append(i)\n        if i == 10:\n            break\n\n    assert output == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n    m = Model(it=[1, 2, 3])\n    assert list(m.it) == [1, 2, 3]\n\n    m = Model(it=str_iterable())\n    with pytest.raises(ValidationError) as exc_info:\n        next(m.it)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (0,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'f',\n        }\n    ]\n\n\n@pytest.mark.parametrize('type_annotation', (Iterable[Any], Iterable))\ndef test_iterable_any(type_annotation):\n    class Model(BaseModel):\n        it: type_annotation\n\n    m = Model(it=int_iterable())\n\n    output = []\n    for i in m.it:\n        output.append(i)\n        if int(i) == 10:\n            break\n\n    assert output == ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n\n    m = Model(it=[1, '2', b'three'])\n    assert list(m.it) == [1, '2', b'three']\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(it=3)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'iterable_type', 'loc': ('it',), 'msg': 'Input should be iterable', 'input': 3}\n    ]\n\n\ndef test_invalid_iterable():\n    class Model(BaseModel):\n        it: Iterable[int]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(it=3)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'iterable_type', 'loc': ('it',), 'msg': 'Input should be iterable', 'input': 3}\n    ]\n\n\n@pytest.mark.parametrize(\n    'config,input_str',\n    (\n        ({}, 'type=iterable_type, input_value=5, input_type=int'),\n        ({'hide_input_in_errors': False}, 'type=iterable_type, input_value=5, input_type=int'),\n        ({'hide_input_in_errors': True}, 'type=iterable_type'),\n    ),\n)\ndef test_iterable_error_hide_input(config, input_str):\n    class Model(BaseModel):\n        it: Iterable[int]\n\n        model_config = ConfigDict(**config)\n\n    with pytest.raises(ValidationError, match=re.escape(f'Input should be iterable [{input_str}]')):\n        Model(it=5)\n\n\ndef test_infinite_iterable_validate_first():\n    class Model(BaseModel):\n        it: Iterable[int]\n        b: int\n\n        @field_validator('it')\n        @classmethod\n        def infinite_first_int(cls, it):\n            return itertools.chain([next(it)], it)\n\n    m = Model(it=int_iterable(), b=3)\n\n    assert m.b == 3\n    assert m.it\n\n    for i in m.it:\n        assert i\n        if i == 10:\n            break\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(it=str_iterable(), b=3)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('it', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'f',\n        }\n    ]\n\n\ndef test_sequence_generator_fails():\n    class Model(BaseModel):\n        v: Sequence[int]\n\n    gen = (i for i in [1, 2, 3])\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=gen)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': ('v',),\n            'msg': 'Input should be an instance of Sequence',\n            'input': gen,\n            'ctx': {'class': 'Sequence'},\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    'cls,value,errors',\n    (\n        (\n            int,\n            [1, 'a', 3],\n            [\n                {\n                    'type': 'int_parsing',\n                    'loc': ('v', 1),\n                    'msg': 'Input should be a valid integer, unable to parse string as an integer',\n                    'input': 'a',\n                },\n            ],\n        ),\n        (\n            int,\n            (1, 2, 'a'),\n            [\n                {\n                    'type': 'int_parsing',\n                    'loc': ('v', 2),\n                    'msg': 'Input should be a valid integer, unable to parse string as an integer',\n                    'input': 'a',\n                },\n            ],\n        ),\n        (\n            float,\n            ('a', 2.2, 3.3),\n            [\n                {\n                    'type': 'float_parsing',\n                    'loc': ('v', 0),\n                    'msg': 'Input should be a valid number, unable to parse string as a number',\n                    'input': 'a',\n                },\n            ],\n        ),\n        (\n            float,\n            (1.1, 2.2, 'a'),\n            [\n                {\n                    'type': 'float_parsing',\n                    'loc': ('v', 2),\n                    'msg': 'Input should be a valid number, unable to parse string as a number',\n                    'input': 'a',\n                },\n            ],\n        ),\n        (\n            float,\n            {1.0, 2.0, 3.0},\n            [\n                {\n                    'type': 'is_instance_of',\n                    'loc': ('v',),\n                    'msg': 'Input should be an instance of Sequence',\n                    'input': {\n                        1.0,\n                        2.0,\n                        3.0,\n                    },\n                    'ctx': {\n                        'class': 'Sequence',\n                    },\n                },\n            ],\n        ),\n        (\n            Set[int],\n            [{1, 2}, {2, 3}, {'d'}],\n            [\n                {\n                    'type': 'int_parsing',\n                    'loc': ('v', 2, 0),\n                    'msg': 'Input should be a valid integer, unable to parse string as an integer',\n                    'input': 'd',\n                }\n            ],\n        ),\n        (\n            Tuple[int, str],\n            ((1, 'a'), ('a', 'a'), (3, 'c')),\n            [\n                {\n                    'type': 'int_parsing',\n                    'loc': ('v', 1, 0),\n                    'msg': 'Input should be a valid integer, unable to parse string as an integer',\n                    'input': 'a',\n                }\n            ],\n        ),\n        (\n            List[int],\n            [{'a': 1, 'b': 2}, [1, 2], [2, 3]],\n            [\n                {\n                    'type': 'list_type',\n                    'loc': ('v', 0),\n                    'msg': 'Input should be a valid list',\n                    'input': {'a': 1, 'b': 2},\n                }\n            ],\n        ),\n    ),\n    ids=repr,\n)\ndef test_sequence_fails(cls, value, errors):\n    class Model(BaseModel):\n        v: Sequence[cls]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    assert exc_info.value.errors(include_url=False) == errors\n\n\ndef test_sequence_strict():\n    assert TypeAdapter(Sequence[int]).validate_python((), strict=True) == ()\n\n\ndef test_list_strict() -> None:\n    class LaxModel(BaseModel):\n        v: List[int]\n\n        model_config = ConfigDict(strict=False)\n\n    class StrictModel(BaseModel):\n        v: List[int]\n\n        model_config = ConfigDict(strict=True)\n\n    assert LaxModel(v=(1, 2)).v == [1, 2]\n    assert LaxModel(v=('1', 2)).v == [1, 2]\n    # Tuple should be rejected\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel(v=(1, 2))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'list_type', 'loc': ('v',), 'msg': 'Input should be a valid list', 'input': (1, 2)}\n    ]\n    # Strict in each list item\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel(v=['1', 2])\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('v', 0), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n\ndef test_set_strict() -> None:\n    class LaxModel(BaseModel):\n        v: Set[int]\n\n        model_config = ConfigDict(strict=False)\n\n    class StrictModel(BaseModel):\n        v: Set[int]\n\n        model_config = ConfigDict(strict=True)\n\n    assert LaxModel(v=(1, 2)).v == {1, 2}\n    assert LaxModel(v=('1', 2)).v == {1, 2}\n    # Tuple should be rejected\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel(v=(1, 2))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'set_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid set',\n            'input': (1, 2),\n        }\n    ]\n    # Strict in each set item\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel(v={'1', 2})\n    err_info = exc_info.value.errors(include_url=False)\n    # Sets are not ordered\n    del err_info[0]['loc']\n    assert err_info == [{'type': 'int_type', 'msg': 'Input should be a valid integer', 'input': '1'}]\n\n\ndef test_frozenset_strict() -> None:\n    class LaxModel(BaseModel):\n        v: FrozenSet[int]\n\n        model_config = ConfigDict(strict=False)\n\n    class StrictModel(BaseModel):\n        v: FrozenSet[int]\n\n        model_config = ConfigDict(strict=True)\n\n    assert LaxModel(v=(1, 2)).v == frozenset((1, 2))\n    assert LaxModel(v=('1', 2)).v == frozenset((1, 2))\n    # Tuple should be rejected\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel(v=(1, 2))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'frozen_set_type',\n            'loc': ('v',),\n            'msg': 'Input should be a valid frozenset',\n            'input': (1, 2),\n        }\n    ]\n    # Strict in each set item\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel(v=frozenset(('1', 2)))\n    err_info = exc_info.value.errors(include_url=False)\n    # Sets are not ordered\n    del err_info[0]['loc']\n    assert err_info == [{'type': 'int_type', 'msg': 'Input should be a valid integer', 'input': '1'}]\n\n\ndef test_tuple_strict() -> None:\n    class LaxModel(BaseModel):\n        v: Tuple[int, int]\n\n        model_config = ConfigDict(strict=False)\n\n    class StrictModel(BaseModel):\n        v: Tuple[int, int]\n\n        model_config = ConfigDict(strict=True)\n\n    assert LaxModel(v=[1, 2]).v == (1, 2)\n    assert LaxModel(v=['1', 2]).v == (1, 2)\n    # List should be rejected\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel(v=[1, 2])\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'tuple_type', 'loc': ('v',), 'msg': 'Input should be a valid tuple', 'input': [1, 2]}\n    ]\n    # Strict in each list item\n    with pytest.raises(ValidationError) as exc_info:\n        StrictModel(v=('1', 2))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('v', 0), 'msg': 'Input should be a valid integer', 'input': '1'}\n    ]\n\n\ndef test_int_validation():\n    class Model(BaseModel):\n        a: PositiveInt = None\n        b: NegativeInt = None\n        c: NonNegativeInt = None\n        d: NonPositiveInt = None\n        e: conint(gt=4, lt=10) = None\n        f: conint(ge=0, le=10) = None\n        g: conint(multiple_of=5) = None\n\n    m = Model(a=5, b=-5, c=0, d=0, e=5, f=0, g=25)\n    assert m.model_dump() == {'a': 5, 'b': -5, 'c': 0, 'd': 0, 'e': 5, 'f': 0, 'g': 25}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=-5, b=5, c=-5, d=5, e=-5, f=11, g=42)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'greater_than',\n            'loc': ('a',),\n            'msg': 'Input should be greater than 0',\n            'input': -5,\n            'ctx': {'gt': 0},\n        },\n        {\n            'type': 'less_than',\n            'loc': ('b',),\n            'msg': 'Input should be less than 0',\n            'input': 5,\n            'ctx': {'lt': 0},\n        },\n        {\n            'type': 'greater_than_equal',\n            'loc': ('c',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -5,\n            'ctx': {'ge': 0},\n        },\n        {\n            'type': 'less_than_equal',\n            'loc': ('d',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 5,\n            'ctx': {'le': 0},\n        },\n        {\n            'type': 'greater_than',\n            'loc': ('e',),\n            'msg': 'Input should be greater than 4',\n            'input': -5,\n            'ctx': {'gt': 4},\n        },\n        {\n            'type': 'less_than_equal',\n            'loc': ('f',),\n            'msg': 'Input should be less than or equal to 10',\n            'input': 11,\n            'ctx': {'le': 10},\n        },\n        {\n            'type': 'multiple_of',\n            'loc': ('g',),\n            'msg': 'Input should be a multiple of 5',\n            'input': 42,\n            'ctx': {'multiple_of': 5},\n        },\n    ]\n\n\ndef test_float_validation():\n    class Model(BaseModel):\n        a: PositiveFloat = None\n        b: NegativeFloat = None\n        c: NonNegativeFloat = None\n        d: NonPositiveFloat = None\n        e: confloat(gt=4, lt=12.2) = None\n        f: confloat(ge=0, le=9.9) = None\n        g: confloat(multiple_of=0.5) = None\n        h: confloat(allow_inf_nan=False) = None\n\n    m = Model(a=5.1, b=-5.2, c=0, d=0, e=5.3, f=9.9, g=2.5, h=42)\n    assert m.model_dump() == {'a': 5.1, 'b': -5.2, 'c': 0, 'd': 0, 'e': 5.3, 'f': 9.9, 'g': 2.5, 'h': 42}\n\n    assert Model(a=float('inf')).a == float('inf')\n    assert Model(b=float('-inf')).b == float('-inf')\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=-5.1, b=5.2, c=-5.1, d=5.1, e=-5.3, f=9.91, g=4.2, h=float('nan'))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'greater_than',\n            'loc': ('a',),\n            'msg': 'Input should be greater than 0',\n            'input': -5.1,\n            'ctx': {\n                'gt': 0.0,\n            },\n        },\n        {\n            'type': 'less_than',\n            'loc': ('b',),\n            'msg': 'Input should be less than 0',\n            'input': 5.2,\n            'ctx': {\n                'lt': 0.0,\n            },\n        },\n        {\n            'type': 'greater_than_equal',\n            'loc': ('c',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -5.1,\n            'ctx': {\n                'ge': 0.0,\n            },\n        },\n        {\n            'type': 'less_than_equal',\n            'loc': ('d',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 5.1,\n            'ctx': {\n                'le': 0.0,\n            },\n        },\n        {\n            'type': 'greater_than',\n            'loc': ('e',),\n            'msg': 'Input should be greater than 4',\n            'input': -5.3,\n            'ctx': {\n                'gt': 4.0,\n            },\n        },\n        {\n            'type': 'less_than_equal',\n            'loc': ('f',),\n            'msg': 'Input should be less than or equal to 9.9',\n            'input': 9.91,\n            'ctx': {\n                'le': 9.9,\n            },\n        },\n        {\n            'type': 'multiple_of',\n            'loc': ('g',),\n            'msg': 'Input should be a multiple of 0.5',\n            'input': 4.2,\n            'ctx': {\n                'multiple_of': 0.5,\n            },\n        },\n        {\n            'type': 'finite_number',\n            'loc': ('h',),\n            'msg': 'Input should be a finite number',\n            'input': HasRepr('nan'),\n        },\n    ]\n\n\ndef test_infinite_float_validation():\n    class Model(BaseModel):\n        a: float = None\n\n    assert Model(a=float('inf')).a == float('inf')\n    assert Model(a=float('-inf')).a == float('-inf')\n    assert math.isnan(Model(a=float('nan')).a)\n\n\n@pytest.mark.parametrize(\n    ('ser_json_inf_nan', 'input', 'output', 'python_roundtrip'),\n    (\n        ('null', float('inf'), 'null', None),\n        ('null', float('-inf'), 'null', None),\n        ('null', float('nan'), 'null', None),\n        ('constants', float('inf'), 'Infinity', float('inf')),\n        ('constants', float('-inf'), '-Infinity', float('-inf')),\n        ('constants', float('nan'), 'NaN', IsFloatNan),\n    ),\n)\ndef test_infinite_float_json_serialization(ser_json_inf_nan, input, output, python_roundtrip):\n    class Model(BaseModel):\n        model_config = ConfigDict(ser_json_inf_nan=ser_json_inf_nan)\n        a: float\n\n    json_string = Model(a=input).model_dump_json()\n    assert json_string == f'{{\"a\":{output}}}'\n    assert json.loads(json_string) == {'a': python_roundtrip}\n\n\n@pytest.mark.parametrize('value', [float('inf'), float('-inf'), float('nan')])\ndef test_finite_float_validation_error(value):\n    class Model(BaseModel):\n        a: FiniteFloat\n\n    assert Model(a=42).a == 42\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'finite_number',\n            'loc': ('a',),\n            'msg': 'Input should be a finite number',\n            'input': HasRepr(repr(value)),\n        }\n    ]\n\n\ndef test_finite_float_config():\n    class Model(BaseModel):\n        a: float\n\n        model_config = ConfigDict(allow_inf_nan=False)\n\n    assert Model(a=42).a == 42\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=float('nan'))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'finite_number',\n            'loc': ('a',),\n            'msg': 'Input should be a finite number',\n            'input': HasRepr('nan'),\n        }\n    ]\n\n\ndef test_strict_bytes():\n    class Model(BaseModel):\n        v: StrictBytes\n\n    assert Model(v=b'foobar').v == b'foobar'\n    with pytest.raises(ValidationError, match='Input should be a valid bytes'):\n        Model(v=bytearray('foobar', 'utf-8'))\n\n    with pytest.raises(ValidationError, match='Input should be a valid bytes'):\n        Model(v='foostring')\n\n    with pytest.raises(ValidationError, match='Input should be a valid bytes'):\n        Model(v=42)\n\n    with pytest.raises(ValidationError, match='Input should be a valid bytes'):\n        Model(v=0.42)\n\n\ndef test_strict_bytes_max_length():\n    class Model(BaseModel):\n        u: StrictBytes = Field(..., max_length=5)\n\n    assert Model(u=b'foo').u == b'foo'\n\n    with pytest.raises(ValidationError, match=r'Input should be a valid bytes \\[type=bytes_type'):\n        Model(u=123)\n    with pytest.raises(ValidationError, match=r'Data should have at most 5 bytes \\[type=bytes_too_long,'):\n        Model(u=b'1234567')\n\n\ndef test_strict_str():\n    class FruitEnum(str, Enum):\n        \"\"\"A subclass of a string\"\"\"\n\n        pear = 'pear'\n        banana = 'banana'\n\n    class Model(BaseModel):\n        v: StrictStr\n\n    assert Model(v='foobar').v == 'foobar'\n\n    assert Model.model_validate({'v': FruitEnum.banana}) == Model.model_construct(v=FruitEnum.banana)\n\n    with pytest.raises(ValidationError, match='Input should be a valid string'):\n        Model(v=123)\n\n    with pytest.raises(ValidationError, match='Input should be a valid string'):\n        Model(v=b'foobar')\n\n\ndef test_strict_str_max_length():\n    class Model(BaseModel):\n        u: StrictStr = Field(..., max_length=5)\n\n    assert Model(u='foo').u == 'foo'\n\n    with pytest.raises(ValidationError, match='Input should be a valid string'):\n        Model(u=123)\n\n    with pytest.raises(ValidationError, match=r'String should have at most 5 characters \\[type=string_too_long,'):\n        Model(u='1234567')\n\n\ndef test_strict_bool():\n    class Model(BaseModel):\n        v: StrictBool\n\n    assert Model(v=True).v is True\n    assert Model(v=False).v is False\n\n    with pytest.raises(ValidationError):\n        Model(v=1)\n\n    with pytest.raises(ValidationError):\n        Model(v='1')\n\n    with pytest.raises(ValidationError):\n        Model(v=b'1')\n\n\ndef test_strict_int():\n    class Model(BaseModel):\n        v: StrictInt\n\n    assert Model(v=123456).v == 123456\n\n    with pytest.raises(ValidationError, match=r'Input should be a valid integer \\[type=int_type,'):\n        Model(v='123456')\n\n    with pytest.raises(ValidationError, match=r'Input should be a valid integer \\[type=int_type,'):\n        Model(v=3.14159)\n\n    with pytest.raises(ValidationError, match=r'Input should be a valid integer \\[type=int_type,'):\n        Model(v=True)\n\n\n@pytest.mark.parametrize(\n    ('input', 'expected_json'),\n    (\n        (9_223_372_036_854_775_807, b'9223372036854775807'),\n        (-9_223_372_036_854_775_807, b'-9223372036854775807'),\n        (1433352099889938534014333520998899385340, b'1433352099889938534014333520998899385340'),\n        (-1433352099889938534014333520998899385340, b'-1433352099889938534014333520998899385340'),\n    ),\n)\ndef test_big_int_json(input, expected_json):\n    v = TypeAdapter(int)\n    dumped = v.dump_json(input)\n    assert dumped == expected_json\n    assert v.validate_json(dumped) == input\n\n\ndef test_strict_float():\n    class Model(BaseModel):\n        v: StrictFloat\n\n    assert Model(v=3.14159).v == 3.14159\n    assert Model(v=123456).v == 123456\n\n    with pytest.raises(ValidationError, match=r'Input should be a valid number \\[type=float_type,'):\n        Model(v='3.14159')\n\n    with pytest.raises(ValidationError, match=r'Input should be a valid number \\[type=float_type,'):\n        Model(v=True)\n\n\ndef test_bool_unhashable_fails():\n    class Model(BaseModel):\n        v: bool\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v={})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'bool_type', 'loc': ('v',), 'msg': 'Input should be a valid boolean', 'input': {}}\n    ]\n\n\ndef test_uuid_error():\n    v = TypeAdapter(UUID)\n\n    valid = UUID('49fdfa1d856d4003a83e4b9236532ec6')\n\n    # sanity check\n    assert v.validate_python(valid) == valid\n    assert v.validate_python(valid.hex) == valid\n\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('ebcdab58-6eb8-46fb-a190-d07a3')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'loc': (),\n            'msg': 'Input should be a valid UUID, invalid group length in group 4: expected 12, found 5',\n            'input': 'ebcdab58-6eb8-46fb-a190-d07a3',\n            'ctx': {'error': 'invalid group length in group 4: expected 12, found 5'},\n            'type': 'uuid_parsing',\n        }\n    ]\n\n    not_a_valid_input_type = object()\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(not_a_valid_input_type)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': not_a_valid_input_type,\n            'loc': (),\n            'msg': 'UUID input should be a string, bytes or UUID object',\n            'type': 'uuid_type',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(valid.hex, strict=True)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': (),\n            'msg': 'Input should be an instance of UUID',\n            'input': '49fdfa1d856d4003a83e4b9236532ec6',\n            'ctx': {'class': 'UUID'},\n        }\n    ]\n\n    assert v.validate_json(json.dumps(valid.hex), strict=True) == valid\n\n\ndef test_uuid_json():\n    class Model(BaseModel):\n        v: UUID\n        v1: UUID1\n        v3: UUID3\n        v4: UUID4\n\n    m = Model(v=uuid.uuid4(), v1=uuid.uuid1(), v3=uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org'), v4=uuid.uuid4())\n    assert m.model_dump_json() == f'{{\"v\":\"{m.v}\",\"v1\":\"{m.v1}\",\"v3\":\"{m.v3}\",\"v4\":\"{m.v4}\"}}'\n\n\ndef test_uuid_validation():\n    class UUIDModel(BaseModel):\n        a: UUID1\n        b: UUID3\n        c: UUID4\n        d: UUID5\n        e: UUID\n\n    a = uuid.uuid1()\n    b = uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org')\n    c = uuid.uuid4()\n    d = uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org')\n    e = UUID('{00000000-7fff-4000-7fff-000000000000}')\n\n    m = UUIDModel(a=a, b=b, c=c, d=d, e=e)\n    assert m.model_dump() == {'a': a, 'b': b, 'c': c, 'd': d, 'e': e}\n\n    with pytest.raises(ValidationError) as exc_info:\n        UUIDModel(a=d, b=c, c=b, d=a, e=e)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'uuid_version',\n            'loc': ('a',),\n            'msg': 'UUID version 1 expected',\n            'input': d,\n            'ctx': {'expected_version': 1},\n        },\n        {\n            'type': 'uuid_version',\n            'loc': ('b',),\n            'msg': 'UUID version 3 expected',\n            'input': c,\n            'ctx': {'expected_version': 3},\n        },\n        {\n            'type': 'uuid_version',\n            'loc': ('c',),\n            'msg': 'UUID version 4 expected',\n            'input': b,\n            'ctx': {'expected_version': 4},\n        },\n        {\n            'type': 'uuid_version',\n            'loc': ('d',),\n            'msg': 'UUID version 5 expected',\n            'input': a,\n            'ctx': {'expected_version': 5},\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        UUIDModel(a=e, b=e, c=e, d=e, e=e)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'uuid_version',\n            'loc': ('a',),\n            'msg': 'UUID version 1 expected',\n            'input': e,\n            'ctx': {'expected_version': 1},\n        },\n        {\n            'type': 'uuid_version',\n            'loc': ('b',),\n            'msg': 'UUID version 3 expected',\n            'input': e,\n            'ctx': {'expected_version': 3},\n        },\n        {\n            'type': 'uuid_version',\n            'loc': ('c',),\n            'msg': 'UUID version 4 expected',\n            'input': e,\n            'ctx': {'expected_version': 4},\n        },\n        {\n            'type': 'uuid_version',\n            'loc': ('d',),\n            'msg': 'UUID version 5 expected',\n            'input': e,\n            'ctx': {'expected_version': 5},\n        },\n    ]\n\n\ndef test_uuid_strict() -> None:\n    class StrictByConfig(BaseModel):\n        a: UUID1\n        b: UUID3\n        c: UUID4\n        d: UUID5\n        e: uuid.UUID\n\n        model_config = ConfigDict(strict=True)\n\n    class StrictByField(BaseModel):\n        a: UUID1 = Field(..., strict=True)\n        b: UUID3 = Field(..., strict=True)\n        c: UUID4 = Field(..., strict=True)\n        d: UUID5 = Field(..., strict=True)\n        e: uuid.UUID = Field(..., strict=True)\n\n    a = uuid.UUID('7fb48116-ca6b-11ed-a439-3274d3adddac')  # uuid1\n    b = uuid.UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e')  # uuid3\n    c = uuid.UUID('260d1600-3680-4f4f-a968-f6fa622ffd8d')  # uuid4\n    d = uuid.UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')  # uuid5\n    e = uuid.UUID('7fb48116-ca6b-11ed-a439-3274d3adddac')  # any uuid\n\n    strict_errors = [\n        {\n            'type': 'is_instance_of',\n            'loc': ('a',),\n            'msg': 'Input should be an instance of UUID',\n            'input': '7fb48116-ca6b-11ed-a439-3274d3adddac',\n            'ctx': {'class': 'UUID'},\n        },\n        {\n            'type': 'is_instance_of',\n            'loc': ('b',),\n            'msg': 'Input should be an instance of UUID',\n            'input': '6fa459ea-ee8a-3ca4-894e-db77e160355e',\n            'ctx': {'class': 'UUID'},\n        },\n        {\n            'type': 'is_instance_of',\n            'loc': ('c',),\n            'msg': 'Input should be an instance of UUID',\n            'input': '260d1600-3680-4f4f-a968-f6fa622ffd8d',\n            'ctx': {'class': 'UUID'},\n        },\n        {\n            'type': 'is_instance_of',\n            'loc': ('d',),\n            'msg': 'Input should be an instance of UUID',\n            'input': '886313e1-3b8a-5372-9b90-0c9aee199e5d',\n            'ctx': {'class': 'UUID'},\n        },\n        {\n            'type': 'is_instance_of',\n            'loc': ('e',),\n            'msg': 'Input should be an instance of UUID',\n            'input': '7fb48116-ca6b-11ed-a439-3274d3adddac',\n            'ctx': {'class': 'UUID'},\n        },\n    ]\n\n    for model in [StrictByConfig, StrictByField]:\n        with pytest.raises(ValidationError) as exc_info:\n            model(a=str(a), b=str(b), c=str(c), d=str(d), e=str(e))\n        assert exc_info.value.errors(include_url=False) == strict_errors\n\n        m = model(a=a, b=b, c=c, d=d, e=e)\n        assert isinstance(m.a, type(a)) and m.a == a\n        assert isinstance(m.b, type(b)) and m.b == b\n        assert isinstance(m.c, type(c)) and m.c == c\n        assert isinstance(m.d, type(d)) and m.d == d\n        assert isinstance(m.e, type(e)) and m.e == e\n\n\n@pytest.mark.parametrize(\n    'enabled,str_check,result_str_check',\n    [\n        (True, '  123  ', '123'),\n        (True, '  123\\t\\n', '123'),\n        (False, '  123  ', '  123  '),\n    ],\n)\ndef test_str_strip_whitespace(enabled, str_check, result_str_check):\n    class Model(BaseModel):\n        str_check: str\n\n        model_config = ConfigDict(str_strip_whitespace=enabled)\n\n    m = Model(str_check=str_check)\n    assert m.str_check == result_str_check\n\n\n@pytest.mark.parametrize(\n    'enabled,str_check,result_str_check',\n    [(True, 'ABCDefG', 'ABCDEFG'), (False, 'ABCDefG', 'ABCDefG')],\n)\ndef test_str_to_upper(enabled, str_check, result_str_check):\n    class Model(BaseModel):\n        str_check: str\n\n        model_config = ConfigDict(str_to_upper=enabled)\n\n    m = Model(str_check=str_check)\n\n    assert m.str_check == result_str_check\n\n\n@pytest.mark.parametrize(\n    'enabled,str_check,result_str_check',\n    [(True, 'ABCDefG', 'abcdefg'), (False, 'ABCDefG', 'ABCDefG')],\n)\ndef test_str_to_lower(enabled, str_check, result_str_check):\n    class Model(BaseModel):\n        str_check: str\n\n        model_config = ConfigDict(str_to_lower=enabled)\n\n    m = Model(str_check=str_check)\n\n    assert m.str_check == result_str_check\n\n\npos_int_values = 'Inf', '+Inf', 'Infinity', '+Infinity'\nneg_int_values = '-Inf', '-Infinity'\nnan_values = 'NaN', '-NaN', '+NaN', 'sNaN', '-sNaN', '+sNaN'\nnon_finite_values = nan_values + pos_int_values + neg_int_values\n# dirty_equals.AnyThing() doesn't work with Decimal on PyPy, hence this hack\nANY_THING = object()\n\n\n@pytest.mark.parametrize(\n    'type_args,value,result',\n    [\n        (dict(gt=Decimal('42.24')), Decimal('43'), Decimal('43')),\n        (\n            dict(gt=Decimal('42.24')),\n            Decimal('42'),\n            [\n                {\n                    'type': 'greater_than',\n                    'loc': ('foo',),\n                    'msg': 'Input should be greater than 42.24',\n                    'input': Decimal('42'),\n                    'ctx': {'gt': Decimal('42.24')},\n                }\n            ],\n        ),\n        (dict(lt=Decimal('42.24')), Decimal('42'), Decimal('42')),\n        (\n            dict(lt=Decimal('42.24')),\n            Decimal('43'),\n            [\n                {\n                    'type': 'less_than',\n                    'loc': ('foo',),\n                    'msg': 'Input should be less than 42.24',\n                    'input': Decimal('43'),\n                    'ctx': {\n                        'lt': Decimal('42.24'),\n                    },\n                },\n            ],\n        ),\n        (dict(ge=Decimal('42.24')), Decimal('43'), Decimal('43')),\n        (dict(ge=Decimal('42.24')), Decimal('42.24'), Decimal('42.24')),\n        (\n            dict(ge=Decimal('42.24')),\n            Decimal('42'),\n            [\n                {\n                    'type': 'greater_than_equal',\n                    'loc': ('foo',),\n                    'msg': 'Input should be greater than or equal to 42.24',\n                    'input': Decimal('42'),\n                    'ctx': {\n                        'ge': Decimal('42.24'),\n                    },\n                }\n            ],\n        ),\n        (dict(le=Decimal('42.24')), Decimal('42'), Decimal('42')),\n        (dict(le=Decimal('42.24')), Decimal('42.24'), Decimal('42.24')),\n        (\n            dict(le=Decimal('42.24')),\n            Decimal('43'),\n            [\n                {\n                    'type': 'less_than_equal',\n                    'loc': ('foo',),\n                    'msg': 'Input should be less than or equal to 42.24',\n                    'input': Decimal('43'),\n                    'ctx': {\n                        'le': Decimal('42.24'),\n                    },\n                }\n            ],\n        ),\n        (dict(max_digits=2, decimal_places=2), Decimal('0.99'), Decimal('0.99')),\n        pytest.param(\n            dict(max_digits=2, decimal_places=1),\n            Decimal('0.99'),\n            [\n                {\n                    'type': 'decimal_max_places',\n                    'loc': ('foo',),\n                    'msg': 'Decimal input should have no more than 1 decimal place',\n                    'input': Decimal('0.99'),\n                    'ctx': {\n                        'decimal_places': 1,\n                    },\n                }\n            ],\n        ),\n        (\n            dict(max_digits=3, decimal_places=1),\n            Decimal('999'),\n            [\n                {\n                    'loc': ('foo',),\n                    'msg': 'Decimal input should have no more than 2 digits before the decimal point',\n                    'type': 'decimal_whole_digits',\n                    'input': Decimal('999'),\n                    'ctx': {'whole_digits': 2},\n                }\n            ],\n        ),\n        (dict(max_digits=4, decimal_places=1), Decimal('999'), Decimal('999')),\n        (dict(max_digits=20, decimal_places=2), Decimal('742403889818000000'), Decimal('742403889818000000')),\n        (dict(max_digits=20, decimal_places=2), Decimal('7.42403889818E+17'), Decimal('7.42403889818E+17')),\n        (dict(max_digits=6, decimal_places=2), Decimal('000000000001111.700000'), Decimal('000000000001111.700000')),\n        (\n            dict(max_digits=6, decimal_places=2),\n            Decimal('0000000000011111.700000'),\n            [\n                {\n                    'type': 'decimal_whole_digits',\n                    'loc': ('foo',),\n                    'msg': 'Decimal input should have no more than 4 digits before the decimal point',\n                    'input': Decimal('11111.700000'),\n                    'ctx': {'whole_digits': 4},\n                }\n            ],\n        ),\n        (\n            dict(max_digits=20, decimal_places=2),\n            Decimal('7424742403889818000000'),\n            [\n                {\n                    'type': 'decimal_max_digits',\n                    'loc': ('foo',),\n                    'msg': 'Decimal input should have no more than 20 digits in total',\n                    'input': Decimal('7424742403889818000000'),\n                    'ctx': {\n                        'max_digits': 20,\n                    },\n                },\n            ],\n        ),\n        (dict(max_digits=5, decimal_places=2), Decimal('7304E-1'), Decimal('7304E-1')),\n        (\n            dict(max_digits=5, decimal_places=2),\n            Decimal('7304E-3'),\n            [\n                {\n                    'type': 'decimal_max_places',\n                    'loc': ('foo',),\n                    'msg': 'Decimal input should have no more than 2 decimal places',\n                    'input': Decimal('7.304'),\n                    'ctx': {'decimal_places': 2},\n                }\n            ],\n        ),\n        (dict(max_digits=5, decimal_places=5), Decimal('70E-5'), Decimal('70E-5')),\n        (\n            dict(max_digits=4, decimal_places=4),\n            Decimal('70E-6'),\n            [\n                {\n                    'loc': ('foo',),\n                    'msg': 'Decimal input should have no more than 4 digits in total',\n                    'type': 'decimal_max_digits',\n                    'input': Decimal('0.00007'),\n                    'ctx': {'max_digits': 4},\n                }\n            ],\n        ),\n        *[\n            (\n                dict(decimal_places=2, max_digits=10, allow_inf_nan=False),\n                value,\n                [\n                    {\n                        'loc': ('foo',),\n                        'msg': 'Input should be a finite number',\n                        'type': 'finite_number',\n                        'input': value,\n                    }\n                ],\n            )\n            for value in non_finite_values\n        ],\n        *[\n            (\n                dict(decimal_places=2, max_digits=10, allow_inf_nan=False),\n                Decimal(value),\n                [\n                    {\n                        'loc': ('foo',),\n                        'msg': 'Input should be a finite number',\n                        'type': 'finite_number',\n                        'input': ANY_THING,\n                    }\n                ],\n            )\n            for value in non_finite_values\n        ],\n        (\n            dict(multiple_of=Decimal('5')),\n            Decimal('42'),\n            [\n                {\n                    'type': 'multiple_of',\n                    'loc': ('foo',),\n                    'msg': 'Input should be a multiple of 5',\n                    'input': Decimal('42'),\n                    'ctx': {'multiple_of': Decimal('5')},\n                }\n            ],\n        ),\n    ],\n)\n@pytest.mark.parametrize('mode', ['Field', 'condecimal'])\ndef test_decimal_validation(mode, type_args, value, result):\n    if mode == 'Field':\n\n        class Model(BaseModel):\n            foo: Decimal = Field(**type_args)\n\n    else:\n\n        class Model(BaseModel):\n            foo: condecimal(**type_args)\n\n    if not isinstance(result, Decimal):\n        with pytest.raises(ValidationError) as exc_info:\n            m = Model(foo=value)\n            print(f'unexpected result: {m!r}')\n        # debug(exc_info.value.errors(include_url=False))\n        # dirty_equals.AnyThing() doesn't work with Decimal on PyPy, hence this hack\n        errors = exc_info.value.errors(include_url=False)\n        if result[0].get('input') is ANY_THING:\n            for e in errors:\n                e['input'] = ANY_THING\n        assert errors == result\n        # assert exc_info.value.json().startswith('[')\n    else:\n        assert Model(foo=value).foo == result\n\n\n@pytest.fixture(scope='module', name='AllowInfModel')\ndef fix_allow_inf_model():\n    class Model(BaseModel):\n        v: condecimal(allow_inf_nan=True)\n\n    return Model\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        (Decimal('42'), 'unchanged'),\n        *[(v, 'is_nan') for v in nan_values],\n        *[(v, 'is_pos_inf') for v in pos_int_values],\n        *[(v, 'is_neg_inf') for v in neg_int_values],\n    ],\n)\ndef test_decimal_not_finite(value, result, AllowInfModel):\n    m = AllowInfModel(v=value)\n    if result == 'unchanged':\n        assert m.v == value\n    elif result == 'is_nan':\n        assert m.v.is_nan(), m.v\n    elif result == 'is_pos_inf':\n        assert m.v.is_infinite() and m.v > 0, m.v\n    else:\n        assert result == 'is_neg_inf'\n        assert m.v.is_infinite() and m.v < 0, m.v\n\n\ndef test_decimal_invalid():\n    with pytest.raises(SchemaError, match='allow_inf_nan=True cannot be used with max_digits or decimal_places'):\n\n        class Model(BaseModel):\n            v: condecimal(allow_inf_nan=True, max_digits=4)\n\n\n@pytest.mark.parametrize('value,result', (('/test/path', Path('/test/path')), (Path('/test/path'), Path('/test/path'))))\ndef test_path_validation_success(value, result):\n    class Model(BaseModel):\n        foo: Path\n\n    assert Model(foo=value).foo == result\n    assert Model.model_validate_json(json.dumps({'foo': str(value)})).foo == result\n\n\ndef test_path_validation_constrained():\n    ta = TypeAdapter(Annotated[Path, Field(min_length=9, max_length=20)])\n    with pytest.raises(ValidationError):\n        ta.validate_python('/short')\n    with pytest.raises(ValidationError):\n        ta.validate_python('/' + 'long' * 100)\n    assert ta.validate_python('/just/right/enough') == Path('/just/right/enough')\n\n\ndef test_path_like():\n    class Model(BaseModel):\n        foo: os.PathLike\n\n    assert Model(foo='/foo/bar').foo == Path('/foo/bar')\n    assert Model(foo=Path('/foo/bar')).foo == Path('/foo/bar')\n    assert Model.model_validate_json('{\"foo\": \"abc\"}').foo == Path('abc')\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        'type': 'object',\n        'properties': {'foo': {'type': 'string', 'format': 'path', 'title': 'Foo'}},\n        'required': ['foo'],\n        'title': 'Model',\n    }\n\n\ndef test_path_like_strict():\n    class Model(BaseModel):\n        model_config = dict(strict=True)\n\n        foo: os.PathLike\n\n    with pytest.raises(ValidationError, match='Input should be an instance of PathLike'):\n        Model(foo='/foo/bar')\n    assert Model(foo=Path('/foo/bar')).foo == Path('/foo/bar')\n    assert Model.model_validate_json('{\"foo\": \"abc\"}').foo == Path('abc')\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        'type': 'object',\n        'properties': {'foo': {'type': 'string', 'format': 'path', 'title': 'Foo'}},\n        'required': ['foo'],\n        'title': 'Model',\n    }\n\n\ndef test_path_strict_override():\n    class Model(BaseModel):\n        model_config = ConfigDict(strict=True)\n\n        x: Path = Field(strict=False)\n\n    m = Model(x='/foo/bar')\n    assert m.x == Path('/foo/bar')\n\n\ndef test_path_validation_fails():\n    class Model(BaseModel):\n        foo: Path\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=123)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'path_type', 'loc': ('foo',), 'msg': 'Input is not a valid path', 'input': 123}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=None)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'path_type', 'loc': ('foo',), 'msg': 'Input is not a valid path', 'input': None}\n    ]\n\n\ndef test_path_validation_strict():\n    class Model(BaseModel):\n        foo: Path\n\n        model_config = ConfigDict(strict=True)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo='/test/path')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': ('foo',),\n            'msg': 'Input should be an instance of Path',\n            'input': '/test/path',\n            'ctx': {'class': 'Path'},\n        }\n    ]\n\n    assert Model(foo=Path('/test/path')).foo == Path('/test/path')\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    (('tests/test_types.py', Path('tests/test_types.py')), (Path('tests/test_types.py'), Path('tests/test_types.py'))),\n)\ndef test_file_path_validation_success(value, result):\n    class Model(BaseModel):\n        foo: FilePath\n\n    assert Model(foo=value).foo == result\n\n\n@pytest.mark.parametrize('value', ['nonexistentfile', Path('nonexistentfile'), 'tests', Path('tests')])\ndef test_file_path_validation_fails(value):\n    class Model(BaseModel):\n        foo: FilePath\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'path_not_file',\n            'loc': ('foo',),\n            'msg': 'Path does not point to a file',\n            'input': value,\n        }\n    ]\n\n\n@pytest.mark.parametrize('value,result', (('tests', Path('tests')), (Path('tests'), Path('tests'))))\ndef test_directory_path_validation_success(value, result):\n    class Model(BaseModel):\n        foo: DirectoryPath\n\n    assert Model(foo=value).foo == result\n\n\n@pytest.mark.parametrize(\n    'value', ['nonexistentdirectory', Path('nonexistentdirectory'), 'tests/test_t.py', Path('tests/test_ypestypes.py')]\n)\ndef test_directory_path_validation_fails(value):\n    class Model(BaseModel):\n        foo: DirectoryPath\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'path_not_directory',\n            'loc': ('foo',),\n            'msg': 'Path does not point to a directory',\n            'input': value,\n        }\n    ]\n\n\n@pytest.mark.parametrize('value', ('tests/test_types.py', Path('tests/test_types.py')))\ndef test_new_path_validation_path_already_exists(value):\n    class Model(BaseModel):\n        foo: NewPath\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'path_exists',\n            'loc': ('foo',),\n            'msg': 'Path already exists',\n            'input': value,\n        }\n    ]\n\n\n@pytest.mark.parametrize('value', ('/nonexistentdir/foo.py', Path('/nonexistentdir/foo.py')))\ndef test_new_path_validation_parent_does_not_exist(value):\n    class Model(BaseModel):\n        foo: NewPath\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(foo=value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'parent_does_not_exist',\n            'loc': ('foo',),\n            'msg': 'Parent directory does not exist',\n            'input': value,\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    'value,result', (('tests/foo.py', Path('tests/foo.py')), (Path('tests/foo.py'), Path('tests/foo.py')))\n)\ndef test_new_path_validation_success(value, result):\n    class Model(BaseModel):\n        foo: NewPath\n\n    assert Model(foo=value).foo == result\n\n\ndef test_number_gt():\n    class Model(BaseModel):\n        a: conint(gt=-1) = 0\n\n    assert Model(a=0).model_dump() == {'a': 0}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=-1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'greater_than',\n            'loc': ('a',),\n            'msg': 'Input should be greater than -1',\n            'input': -1,\n            'ctx': {'gt': -1},\n        }\n    ]\n\n\ndef test_number_ge():\n    class Model(BaseModel):\n        a: conint(ge=0) = 0\n\n    assert Model(a=0).model_dump() == {'a': 0}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=-1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'greater_than_equal',\n            'loc': ('a',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -1,\n            'ctx': {'ge': 0},\n        }\n    ]\n\n\ndef test_number_lt():\n    class Model(BaseModel):\n        a: conint(lt=5) = 0\n\n    assert Model(a=4).model_dump() == {'a': 4}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=5)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'less_than',\n            'loc': ('a',),\n            'msg': 'Input should be less than 5',\n            'input': 5,\n            'ctx': {'lt': 5},\n        }\n    ]\n\n\ndef test_number_le():\n    class Model(BaseModel):\n        a: conint(le=5) = 0\n\n    assert Model(a=5).model_dump() == {'a': 5}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=6)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'less_than_equal',\n            'loc': ('a',),\n            'msg': 'Input should be less than or equal to 5',\n            'input': 6,\n            'ctx': {'le': 5},\n        }\n    ]\n\n\n@pytest.mark.parametrize('value', (10, 100, 20))\ndef test_number_multiple_of_int_valid(value):\n    class Model(BaseModel):\n        a: conint(multiple_of=5)\n\n    assert Model(a=value).model_dump() == {'a': value}\n\n\n@pytest.mark.parametrize('value', [1337, 23, 6, 14])\ndef test_number_multiple_of_int_invalid(value):\n    class Model(BaseModel):\n        a: conint(multiple_of=5)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'multiple_of',\n            'loc': ('a',),\n            'msg': 'Input should be a multiple of 5',\n            'input': value,\n            'ctx': {'multiple_of': 5},\n        }\n    ]\n\n\n@pytest.mark.parametrize('value', [0.2, 0.3, 0.4, 0.5, 1])\ndef test_number_multiple_of_float_valid(value):\n    class Model(BaseModel):\n        a: confloat(multiple_of=0.1)\n\n    assert Model(a=value).model_dump() == {'a': value}\n\n\n@pytest.mark.parametrize('value', [0.07, 1.27, 1.003])\ndef test_number_multiple_of_float_invalid(value):\n    class Model(BaseModel):\n        a: confloat(multiple_of=0.1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a=value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'multiple_of',\n            'loc': ('a',),\n            'msg': 'Input should be a multiple of 0.1',\n            'input': value,\n            'ctx': {'multiple_of': 0.1},\n        }\n    ]\n\n\ndef test_new_type_success():\n    a_type = NewType('a_type', int)\n    b_type = NewType('b_type', a_type)\n    c_type = NewType('c_type', List[int])\n\n    class Model(BaseModel):\n        a: a_type\n        b: b_type\n        c: c_type\n\n    m = Model(a=42, b=24, c=[1, 2, 3])\n    assert m.model_dump() == {'a': 42, 'b': 24, 'c': [1, 2, 3]}\n\n\ndef test_new_type_fails():\n    a_type = NewType('a_type', int)\n    b_type = NewType('b_type', a_type)\n    c_type = NewType('c_type', List[int])\n\n    class Model(BaseModel):\n        a: a_type\n        b: b_type\n        c: c_type\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='foo', b='bar', c=['foo'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'foo',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'bar',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('c', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'foo',\n        },\n    ]\n\n\ndef test_valid_simple_json():\n    class JsonModel(BaseModel):\n        json_obj: Json\n\n    obj = '{\"a\": 1, \"b\": [2, 3]}'\n    assert JsonModel(json_obj=obj).model_dump() == {'json_obj': {'a': 1, 'b': [2, 3]}}\n\n\ndef test_valid_simple_json_any():\n    class JsonModel(BaseModel):\n        json_obj: Json[Any]\n\n    obj = '{\"a\": 1, \"b\": [2, 3]}'\n    assert JsonModel(json_obj=obj).model_dump() == {'json_obj': {'a': 1, 'b': [2, 3]}}\n\n\n@pytest.mark.parametrize('gen_type', [lambda: Json, lambda: Json[Any]])\ndef test_invalid_simple_json(gen_type):\n    t = gen_type()\n\n    class JsonModel(BaseModel):\n        json_obj: t\n\n    obj = '{a: 1, b: [2, 3]}'\n    with pytest.raises(ValidationError) as exc_info:\n        JsonModel(json_obj=obj)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'json_invalid',\n            'loc': ('json_obj',),\n            'msg': 'Invalid JSON: key must be a string at line 1 column 2',\n            'input': '{a: 1, b: [2, 3]}',\n            'ctx': {'error': 'key must be a string at line 1 column 2'},\n        }\n    ]\n\n\ndef test_valid_simple_json_bytes():\n    class JsonModel(BaseModel):\n        json_obj: Json\n\n    obj = b'{\"a\": 1, \"b\": [2, 3]}'\n    assert JsonModel(json_obj=obj).model_dump() == {'json_obj': {'a': 1, 'b': [2, 3]}}\n\n\ndef test_valid_detailed_json():\n    class JsonDetailedModel(BaseModel):\n        json_obj: Json[List[int]]\n\n    obj = '[1, 2, 3]'\n    assert JsonDetailedModel(json_obj=obj).model_dump() == {'json_obj': [1, 2, 3]}\n\n    obj = b'[1, 2, 3]'\n    assert JsonDetailedModel(json_obj=obj).model_dump() == {'json_obj': [1, 2, 3]}\n\n    obj = '(1, 2, 3)'\n    with pytest.raises(ValidationError) as exc_info:\n        JsonDetailedModel(json_obj=obj)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'json_invalid',\n            'loc': ('json_obj',),\n            'msg': 'Invalid JSON: expected value at line 1 column 1',\n            'input': '(1, 2, 3)',\n            'ctx': {'error': 'expected value at line 1 column 1'},\n        }\n    ]\n\n\ndef test_valid_model_json():\n    class Model(BaseModel):\n        a: int\n        b: List[int]\n\n    class JsonDetailedModel(BaseModel):\n        json_obj: Json[Model]\n\n    obj = '{\"a\": 1, \"b\": [2, 3]}'\n    m = JsonDetailedModel(json_obj=obj)\n    assert isinstance(m.json_obj, Model)\n    assert m.json_obj.a == 1\n    assert m.model_dump() == {'json_obj': {'a': 1, 'b': [2, 3]}}\n\n\ndef test_invalid_model_json():\n    class Model(BaseModel):\n        a: int\n        b: List[int]\n\n    class JsonDetailedModel(BaseModel):\n        json_obj: Json[Model]\n\n    obj = '{\"a\": 1, \"c\": [2, 3]}'\n    with pytest.raises(ValidationError) as exc_info:\n        JsonDetailedModel(json_obj=obj)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('json_obj', 'b'), 'msg': 'Field required', 'input': {'a': 1, 'c': [2, 3]}}\n    ]\n\n\ndef test_invalid_detailed_json_type_error():\n    class JsonDetailedModel(BaseModel):\n        json_obj: Json[List[int]]\n\n    obj = '[\"a\", \"b\", \"c\"]'\n    with pytest.raises(ValidationError) as exc_info:\n        JsonDetailedModel(json_obj=obj)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('json_obj', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('json_obj', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'b',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('json_obj', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'c',\n        },\n    ]\n\n\ndef test_json_not_str():\n    class JsonDetailedModel(BaseModel):\n        json_obj: Json[List[int]]\n\n    obj = 12\n    with pytest.raises(ValidationError) as exc_info:\n        JsonDetailedModel(json_obj=obj)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'json_type',\n            'loc': ('json_obj',),\n            'msg': 'JSON input should be string, bytes or bytearray',\n            'input': 12,\n        }\n    ]\n\n\ndef test_json_before_validator():\n    call_count = 0\n\n    class JsonModel(BaseModel):\n        json_obj: Json[str]\n\n        @field_validator('json_obj', mode='before')\n        @classmethod\n        def check(cls, v):\n            assert v == '\"foobar\"'\n            nonlocal call_count\n            call_count += 1\n            return v\n\n    assert JsonModel(json_obj='\"foobar\"').model_dump() == {'json_obj': 'foobar'}\n    assert call_count == 1\n\n\ndef test_json_optional_simple():\n    class JsonOptionalModel(BaseModel):\n        json_obj: Optional[Json]\n\n    assert JsonOptionalModel(json_obj=None).model_dump() == {'json_obj': None}\n    assert JsonOptionalModel(json_obj='[\"x\", \"y\", \"z\"]').model_dump() == {'json_obj': ['x', 'y', 'z']}\n\n\ndef test_json_optional_complex():\n    class JsonOptionalModel(BaseModel):\n        json_obj: Optional[Json[List[int]]]\n\n    JsonOptionalModel(json_obj=None)\n\n    good = JsonOptionalModel(json_obj='[1, 2, 3]')\n    assert good.json_obj == [1, 2, 3]\n\n    with pytest.raises(ValidationError) as exc_info:\n        JsonOptionalModel(json_obj='[\"i should fail\"]')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('json_obj', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'i should fail',\n        }\n    ]\n\n\ndef test_json_required():\n    class JsonRequired(BaseModel):\n        json_obj: Json\n\n    assert JsonRequired(json_obj='[\"x\", \"y\", \"z\"]').model_dump() == {'json_obj': ['x', 'y', 'z']}\n    with pytest.raises(ValidationError, match=r'JSON input should be string, bytes or bytearray \\[type=json_type,'):\n        JsonRequired(json_obj=None)\n    with pytest.raises(ValidationError, match=r'Field required \\[type=missing,'):\n        JsonRequired()\n\n\n@pytest.mark.parametrize(\n    ('pattern_type', 'pattern_value', 'matching_value', 'non_matching_value'),\n    [\n        pytest.param(re.Pattern, r'^whatev.r\\d$', 'whatever1', ' whatever1', id='re.Pattern'),\n        pytest.param(Pattern, r'^whatev.r\\d$', 'whatever1', ' whatever1', id='Pattern'),\n        pytest.param(Pattern[str], r'^whatev.r\\d$', 'whatever1', ' whatever1', id='Pattern[str]'),\n        pytest.param(Pattern[bytes], rb'^whatev.r\\d$', b'whatever1', b' whatever1', id='Pattern[bytes]'),\n    ],\n)\ndef test_pattern(pattern_type, pattern_value, matching_value, non_matching_value):\n    class Foobar(BaseModel):\n        pattern: pattern_type\n\n    f = Foobar(pattern=pattern_value)\n    assert f.pattern.__class__.__name__ == 'Pattern'\n    # check it's really a proper pattern\n    assert f.pattern.match(matching_value)\n    assert not f.pattern.match(non_matching_value)\n\n    # Check that pre-compiled patterns are accepted unchanged\n    p = re.compile(pattern_value)\n    f2 = Foobar(pattern=p)\n    assert f2.pattern is p\n\n    assert Foobar.model_json_schema() == {\n        'type': 'object',\n        'title': 'Foobar',\n        'properties': {'pattern': {'type': 'string', 'format': 'regex', 'title': 'Pattern'}},\n        'required': ['pattern'],\n    }\n\n\n@pytest.mark.parametrize(\n    'use_field',\n    [pytest.param(True, id='Field'), pytest.param(False, id='constr')],\n)\ndef test_compiled_pattern_in_field(use_field):\n    \"\"\"\n    https://github.com/pydantic/pydantic/issues/9052\n    https://github.com/pydantic/pydantic/pull/9053\n    \"\"\"\n    pattern_value = r'^whatev.r\\d$'\n    field_pattern = re.compile(pattern_value)\n\n    if use_field:\n\n        class Foobar(BaseModel):\n            str_regex: str = Field(..., pattern=field_pattern)\n    else:\n\n        class Foobar(BaseModel):\n            str_regex: constr(pattern=field_pattern) = ...\n\n    field_general_metadata = Foobar.model_fields['str_regex'].metadata\n    assert len(field_general_metadata) == 1\n    field_metadata_pattern = field_general_metadata[0].pattern\n\n    assert field_metadata_pattern == field_pattern\n    assert isinstance(field_metadata_pattern, re.Pattern)\n\n    matching_value = 'whatever1'\n    f = Foobar(str_regex=matching_value)\n    assert f.str_regex == matching_value\n\n    with pytest.raises(\n        ValidationError,\n        match=re.escape(\"String should match pattern '\" + pattern_value + \"'\"),\n    ):\n        Foobar(str_regex=' whatever1')\n\n    assert Foobar.model_json_schema() == {\n        'type': 'object',\n        'title': 'Foobar',\n        'properties': {'str_regex': {'pattern': pattern_value, 'title': 'Str Regex', 'type': 'string'}},\n        'required': ['str_regex'],\n    }\n\n\ndef test_pattern_with_invalid_param():\n    with pytest.raises(\n        PydanticSchemaGenerationError,\n        match=re.escape('Unable to generate pydantic-core schema for typing.Pattern[int].'),\n    ):\n\n        class Foo(BaseModel):\n            pattern: Pattern[int]\n\n\n@pytest.mark.parametrize(\n    ('pattern_type', 'pattern_value', 'error_type', 'error_msg'),\n    [\n        pytest.param(\n            re.Pattern,\n            '[xx',\n            'pattern_regex',\n            'Input should be a valid regular expression',\n            id='re.Pattern-pattern_regex',\n        ),\n        pytest.param(\n            Pattern, '[xx', 'pattern_regex', 'Input should be a valid regular expression', id='re.Pattern-pattern_regex'\n        ),\n        pytest.param(\n            re.Pattern, (), 'pattern_type', 'Input should be a valid pattern', id='typing.Pattern-pattern_type'\n        ),\n        pytest.param(Pattern, (), 'pattern_type', 'Input should be a valid pattern', id='typing.Pattern-pattern_type'),\n        pytest.param(\n            Pattern[str],\n            re.compile(b''),\n            'pattern_str_type',\n            'Input should be a string pattern',\n            id='typing.Pattern[str]-pattern_str_type-non_str',\n        ),\n        pytest.param(\n            Pattern[str],\n            b'',\n            'pattern_str_type',\n            'Input should be a string pattern',\n            id='typing.Pattern[str]-pattern_str_type-bytes',\n        ),\n        pytest.param(\n            Pattern[str], (), 'pattern_type', 'Input should be a valid pattern', id='typing.Pattern[str]-pattern_type'\n        ),\n        pytest.param(\n            Pattern[bytes],\n            re.compile(''),\n            'pattern_bytes_type',\n            'Input should be a bytes pattern',\n            id='typing.Pattern[bytes]-pattern_bytes_type-non_bytes',\n        ),\n        pytest.param(\n            Pattern[bytes],\n            '',\n            'pattern_bytes_type',\n            'Input should be a bytes pattern',\n            id='typing.Pattern[bytes]-pattern_bytes_type-str',\n        ),\n        pytest.param(\n            Pattern[bytes],\n            (),\n            'pattern_type',\n            'Input should be a valid pattern',\n            id='typing.Pattern[bytes]-pattern_type',\n        ),\n    ],\n)\ndef test_pattern_error(pattern_type, pattern_value, error_type, error_msg):\n    class Foobar(BaseModel):\n        pattern: pattern_type\n\n    with pytest.raises(ValidationError) as exc_info:\n        Foobar(pattern=pattern_value)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': error_type, 'loc': ('pattern',), 'msg': error_msg, 'input': pattern_value}\n    ]\n\n\n@pytest.mark.parametrize('validate_json', [True, False])\ndef test_secretstr(validate_json):\n    class Foobar(BaseModel):\n        password: SecretStr\n        empty_password: SecretStr\n\n    if validate_json:\n        f = Foobar.model_validate_json('{\"password\": \"1234\", \"empty_password\": \"\"}')\n        with pytest.raises(ValidationError) as exc_info:\n            Foobar.model_validate_json('{\"password\": 1234, \"empty_password\": null}')\n    else:\n        f = Foobar(password='1234', empty_password='')\n        with pytest.raises(ValidationError) as exc_info:\n            Foobar(password=1234, empty_password=None)\n\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('password',), 'msg': 'Input should be a valid string', 'input': 1234},\n        {'type': 'string_type', 'loc': ('empty_password',), 'msg': 'Input should be a valid string', 'input': None},\n    ]\n\n    # Assert correct types.\n    assert f.password.__class__.__name__ == 'SecretStr'\n    assert f.empty_password.__class__.__name__ == 'SecretStr'\n\n    # Assert str and repr are correct.\n    assert str(f.password) == '**********'\n    assert str(f.empty_password) == ''\n    assert repr(f.password) == \"SecretStr('**********')\"\n    assert repr(f.empty_password) == \"SecretStr('')\"\n    assert len(f.password) == 4\n    assert len(f.empty_password) == 0\n\n    # Assert retrieval of secret value is correct\n    assert f.password.get_secret_value() == '1234'\n    assert f.empty_password.get_secret_value() == ''\n\n\ndef test_secretstr_subclass():\n    class DecryptableStr(SecretStr):\n        \"\"\"\n        Simulate a SecretStr with decryption capabilities.\n        \"\"\"\n\n        def decrypt_value(self) -> str:\n            return f'MOCK DECRYPTED {self.get_secret_value()}'\n\n    class Foobar(BaseModel):\n        password: DecryptableStr\n        empty_password: SecretStr\n\n    # Initialize the model.\n    f = Foobar(password='1234', empty_password='')\n\n    # Assert correct types.\n    assert f.password.__class__.__name__ == 'DecryptableStr'\n    assert f.empty_password.__class__.__name__ == 'SecretStr'\n\n    # Assert str and repr are correct.\n    assert str(f.password) == '**********'\n    assert str(f.empty_password) == ''\n    assert repr(f.password) == \"DecryptableStr('**********')\"\n    assert repr(f.empty_password) == \"SecretStr('')\"\n    assert len(f.password) == 4\n    assert len(f.empty_password) == 0\n\n    # Assert retrieval of secret value is correct\n    assert f.password.get_secret_value() == '1234'\n    assert f.empty_password.get_secret_value() == ''\n\n\ndef test_secretstr_equality():\n    assert SecretStr('abc') == SecretStr('abc')\n    assert SecretStr('123') != SecretStr('321')\n    assert SecretStr('123') != '123'\n    assert SecretStr('123') is not SecretStr('123')\n\n\ndef test_secretstr_idempotent():\n    class Foobar(BaseModel):\n        password: SecretStr\n\n    # Should not raise an exception\n    m = Foobar(password=SecretStr('1234'))\n    assert m.password.get_secret_value() == '1234'\n\n\nclass SecretDate(Secret[date]):\n    def _display(self) -> str:\n        return '****/**/**'\n\n\nclass SampleEnum(str, Enum):\n    foo = 'foo'\n    bar = 'bar'\n\n\nSecretEnum = Secret[SampleEnum]\n\n\n@pytest.mark.parametrize(\n    'value, result',\n    [\n        # Valid inputs\n        (1_493_942_400, date(2017, 5, 5)),\n        (1_493_942_400_000, date(2017, 5, 5)),\n        (0, date(1970, 1, 1)),\n        ('2012-04-23', date(2012, 4, 23)),\n        (b'2012-04-23', date(2012, 4, 23)),\n        (date(2012, 4, 9), date(2012, 4, 9)),\n        (datetime(2012, 4, 9, 0, 0), date(2012, 4, 9)),\n        (1_549_238_400, date(2019, 2, 4)),  # nowish in s\n        (1_549_238_400_000, date(2019, 2, 4)),  # nowish in ms\n        (19_999_958_400, date(2603, 10, 11)),  # just before watershed\n    ],\n)\ndef test_secretdate(value, result):\n    class Foobar(BaseModel):\n        value: SecretDate\n\n    f = Foobar(value=value)\n\n    # Assert correct type.\n    assert f.value.__class__.__name__ == 'SecretDate'\n\n    # Assert str and repr are correct.\n    assert str(f.value) == '****/**/**'\n    assert repr(f.value) == \"SecretDate('****/**/**')\"\n\n    # Assert retrieval of secret value is correct\n    assert f.value.get_secret_value() == result\n\n\ndef test_secretdate_json_serializable():\n    class _SecretDate(Secret[date]):\n        def _display(self) -> str:\n            return '****/**/**'\n\n    SecretDate = Annotated[\n        _SecretDate,\n        PlainSerializer(lambda v: v.get_secret_value().strftime('%Y-%m-%d'), when_used='json'),\n    ]\n\n    class Foobar(BaseModel):\n        value: SecretDate\n\n    f = Foobar(value='2017-01-01')\n\n    assert '2017-01-01' in f.model_dump_json()\n\n\ndef test_secretenum_json_serializable():\n    class SampleEnum(str, Enum):\n        foo = 'foo'\n        bar = 'bar'\n\n    SecretEnum = Annotated[\n        Secret[SampleEnum],\n        PlainSerializer(lambda v: v.get_secret_value(), when_used='json'),\n    ]\n\n    class Foobar(BaseModel):\n        value: SecretEnum\n\n    f = Foobar(value='foo')\n\n    assert f.model_dump_json() == '{\"value\":\"foo\"}'\n\n\n@pytest.mark.parametrize(\n    'SecretField, value, error_msg',\n    [\n        (SecretDate, 'not-a-date', r'Input should be a valid date'),\n        (SecretStr, 0, r'Input should be a valid string \\[type=string_type,'),\n        (SecretBytes, 0, r'Input should be a valid bytes \\[type=bytes_type,'),\n        (SecretEnum, 0, r'Input should be an instance of SampleEnum'),\n    ],\n)\ndef test_strict_secretfield_by_config(SecretField, value, error_msg):\n    class Foobar(BaseModel):\n        model_config = ConfigDict(strict=True)\n        value: SecretField\n\n    with pytest.raises(ValidationError, match=error_msg):\n        Foobar(value=value)\n\n\n@pytest.mark.parametrize(\n    'field, value, error_msg',\n    [\n        (date, 'not-a-date', r'Input should be a valid date'),\n        (str, 0, r'Input should be a valid string \\[type=string_type,'),\n        (bytes, 0, r'Input should be a valid bytes \\[type=bytes_type,'),\n        (SampleEnum, 0, r'Input should be an instance of SampleEnum'),\n    ],\n)\ndef test_strict_secretfield_annotated(field, value, error_msg):\n    SecretField = Annotated[field, Strict()]\n\n    class Foobar(BaseModel):\n        value: Secret[SecretField]\n\n    with pytest.raises(ValidationError, match=error_msg):\n        Foobar(value=value)\n\n\n@pytest.mark.parametrize(\n    'value',\n    [\n        datetime(2012, 4, 9, 12, 15),\n        'x20120423',\n        '2012-04-56',\n        20000044800,  # just after watershed\n        1_549_238_400_000_000,  # nowish in \u03bcs\n        1_549_238_400_000_000_000,  # nowish in ns\n        'infinity',\n        float('inf'),\n        int('1' + '0' * 100),\n        1e1000,\n        float('-infinity'),\n        float('nan'),\n    ],\n)\ndef test_secretdate_parsing(value):\n    class FooBar(BaseModel):\n        d: SecretDate\n\n    with pytest.raises(ValidationError):\n        FooBar(d=value)\n\n\ndef test_secretdate_equality():\n    assert SecretDate('2017-01-01') == SecretDate('2017-01-01')\n    assert SecretDate('2017-01-01') != SecretDate('2018-01-01')\n    assert SecretDate(date(2017, 1, 1)) != date(2017, 1, 1)\n    assert SecretDate('2017-01-01') is not SecretDate('2017-01-01')\n\n\ndef test_secretdate_idempotent():\n    class Foobar(BaseModel):\n        value: SecretDate\n\n    # Should not raise an exception\n    m = Foobar(value=SecretDate(date(2017, 1, 1)))\n    assert m.value.get_secret_value() == date(2017, 1, 1)\n\n\ndef test_secret_union_serializable() -> None:\n    class Base(BaseModel):\n        x: Union[Secret[int], Secret[str]]\n\n    model = Base(x=1)\n    assert model.model_dump() == {'x': Secret[int](1)}\n    assert model.model_dump_json() == '{\"x\":\"**********\"}'\n\n\n@pytest.mark.parametrize(\n    'pydantic_type',\n    [\n        Strict,\n        StrictBool,\n        conint,\n        PositiveInt,\n        NegativeInt,\n        NonPositiveInt,\n        NonNegativeInt,\n        StrictInt,\n        confloat,\n        PositiveFloat,\n        NegativeFloat,\n        NonPositiveFloat,\n        NonNegativeFloat,\n        StrictFloat,\n        FiniteFloat,\n        conbytes,\n        Secret,\n        SecretBytes,\n        constr,\n        StrictStr,\n        SecretStr,\n        ImportString,\n        conset,\n        confrozenset,\n        conlist,\n        condecimal,\n        UUID1,\n        UUID3,\n        UUID4,\n        UUID5,\n        FilePath,\n        DirectoryPath,\n        NewPath,\n        Json,\n        ByteSize,\n        condate,\n        PastDate,\n        FutureDate,\n        PastDatetime,\n        FutureDatetime,\n        AwareDatetime,\n        NaiveDatetime,\n    ],\n)\ndef test_is_hashable(pydantic_type):\n    assert type(hash(pydantic_type)) is int\n\n\ndef test_model_contain_hashable_type():\n    class MyModel(BaseModel):\n        v: Union[str, StrictStr]\n\n    assert MyModel(v='test').v == 'test'\n\n\ndef test_secretstr_error():\n    class Foobar(BaseModel):\n        password: SecretStr\n\n    with pytest.raises(ValidationError) as exc_info:\n        Foobar(password=[6, 23, 'abc'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_type',\n            'loc': ('password',),\n            'msg': 'Input should be a valid string',\n            'input': [6, 23, 'abc'],\n        }\n    ]\n\n\ndef test_secret_str_hashable():\n    assert type(hash(SecretStr('abs'))) is int\n\n\ndef test_secret_bytes_hashable():\n    assert type(hash(SecretBytes(b'abs'))) is int\n\n\ndef test_secret_str_min_max_length():\n    class Foobar(BaseModel):\n        password: SecretStr = Field(min_length=6, max_length=10)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Foobar(password='')\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('password',),\n            'msg': 'Value should have at least 6 items after validation, not 0',\n            'input': '',\n            'ctx': {'field_type': 'Value', 'min_length': 6, 'actual_length': 0},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Foobar(password='1' * 20)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': ('password',),\n            'msg': 'Value should have at most 10 items after validation, not 20',\n            'input': '11111111111111111111',\n            'ctx': {'field_type': 'Value', 'max_length': 10, 'actual_length': 20},\n        }\n    ]\n\n    value = '1' * 8\n    assert Foobar(password=value).password.get_secret_value() == value\n\n\ndef test_secretbytes_json():\n    class Foobar(BaseModel):\n        password: SecretBytes\n\n    assert Foobar(password='foo').model_dump_json() == '{\"password\":\"**********\"}'\n\n\ndef test_secretbytes():\n    class Foobar(BaseModel):\n        password: SecretBytes\n        empty_password: SecretBytes\n\n    # Initialize the model.\n    # Use bytes that can't be decoded with UTF8 (https://github.com/pydantic/pydantic/issues/7971)\n    password = b'\\x89PNG\\r\\n\\x1a\\n'\n    f = Foobar(password=password, empty_password=b'')\n\n    # Assert correct types.\n    assert f.password.__class__.__name__ == 'SecretBytes'\n    assert f.empty_password.__class__.__name__ == 'SecretBytes'\n\n    # Assert str and repr are correct.\n    assert str(f.password) == \"b'**********'\"\n    assert str(f.empty_password) == \"b''\"\n    assert repr(f.password) == \"SecretBytes(b'**********')\"\n    assert repr(f.empty_password) == \"SecretBytes(b'')\"\n\n    # Assert retrieval of secret value is correct\n    assert f.password.get_secret_value() == password\n    assert f.empty_password.get_secret_value() == b''\n\n    # Assert that SecretBytes is equal to SecretBytes if the secret is the same.\n    assert f == f.model_copy()\n    copied_with_changes = f.model_copy()\n    copied_with_changes.password = SecretBytes(b'4321')\n    assert f != copied_with_changes\n\n\ndef test_secretbytes_equality():\n    assert SecretBytes(b'abc') == SecretBytes(b'abc')\n    assert SecretBytes(b'123') != SecretBytes(b'321')\n    assert SecretBytes(b'123') != b'123'\n    assert SecretBytes(b'123') is not SecretBytes(b'123')\n\n\ndef test_secretbytes_idempotent():\n    class Foobar(BaseModel):\n        password: SecretBytes\n\n    # Should not raise an exception.\n    _ = Foobar(password=SecretBytes(b'1234'))\n\n\ndef test_secretbytes_error():\n    class Foobar(BaseModel):\n        password: SecretBytes\n\n    with pytest.raises(ValidationError) as exc_info:\n        Foobar(password=[6, 23, 'abc'])\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bytes_type',\n            'loc': ('password',),\n            'msg': 'Input should be a valid bytes',\n            'input': [6, 23, 'abc'],\n        }\n    ]\n\n\ndef test_secret_bytes_min_max_length():\n    class Foobar(BaseModel):\n        password: SecretBytes = Field(min_length=6, max_length=10)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Foobar(password=b'')\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('password',),\n            'msg': 'Value should have at least 6 items after validation, not 0',\n            'input': b'',\n            'ctx': {'field_type': 'Value', 'min_length': 6, 'actual_length': 0},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Foobar(password=b'1' * 20)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': ('password',),\n            'msg': 'Value should have at most 10 items after validation, not 20',\n            'input': b'11111111111111111111',\n            'ctx': {'field_type': 'Value', 'max_length': 10, 'actual_length': 20},\n        }\n    ]\n\n    value = b'1' * 8\n    assert Foobar(password=value).password.get_secret_value() == value\n\n\ndef test_generic_without_params():\n    class Model(BaseModel):\n        generic_list: List\n        generic_dict: Dict\n        generic_tuple: Tuple\n\n    m = Model(generic_list=[0, 'a'], generic_dict={0: 'a', 'a': 0}, generic_tuple=(1, 'q'))\n    assert m.model_dump() == {'generic_list': [0, 'a'], 'generic_dict': {0: 'a', 'a': 0}, 'generic_tuple': (1, 'q')}\n\n\ndef test_generic_without_params_error():\n    class Model(BaseModel):\n        generic_list: List\n        generic_dict: Dict\n        generic_tuple: Tuple\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(generic_list=0, generic_dict=0, generic_tuple=0)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'list_type',\n            'loc': ('generic_list',),\n            'msg': 'Input should be a valid list',\n            'input': 0,\n        },\n        {\n            'type': 'dict_type',\n            'loc': ('generic_dict',),\n            'msg': 'Input should be a valid dictionary',\n            'input': 0,\n        },\n        {'type': 'tuple_type', 'loc': ('generic_tuple',), 'msg': 'Input should be a valid tuple', 'input': 0},\n    ]\n\n\ndef test_literal_single():\n    class Model(BaseModel):\n        a: Literal['a']\n\n    Model(a='a')\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='b')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': ('a',),\n            'msg': \"Input should be 'a'\",\n            'input': 'b',\n            'ctx': {'expected': \"'a'\"},\n        }\n    ]\n\n\ndef test_literal_multiple():\n    class Model(BaseModel):\n        a_or_b: Literal['a', 'b']\n\n    Model(a_or_b='a')\n    Model(a_or_b='b')\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a_or_b='c')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': ('a_or_b',),\n            'msg': \"Input should be 'a' or 'b'\",\n            'input': 'c',\n            'ctx': {'expected': \"'a' or 'b'\"},\n        }\n    ]\n\n\ndef test_typing_mutable_set():\n    s1 = TypeAdapter(Set[int]).core_schema\n    s1.pop('metadata', None)\n    s2 = TypeAdapter(typing.MutableSet[int]).core_schema\n    s2.pop('metadata', None)\n    assert s1 == s2\n\n\ndef test_frozenset_field():\n    class FrozenSetModel(BaseModel):\n        set: FrozenSet[int]\n\n    test_set = frozenset({1, 2, 3})\n    object_under_test = FrozenSetModel(set=test_set)\n\n    assert object_under_test.set == test_set\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        ([1, 2, 3], frozenset([1, 2, 3])),\n        ({1, 2, 3}, frozenset([1, 2, 3])),\n        ((1, 2, 3), frozenset([1, 2, 3])),\n        (deque([1, 2, 3]), frozenset([1, 2, 3])),\n    ],\n)\ndef test_frozenset_field_conversion(value, result):\n    class FrozenSetModel(BaseModel):\n        set: FrozenSet[int]\n\n    object_under_test = FrozenSetModel(set=value)\n\n    assert object_under_test.set == result\n\n\ndef test_frozenset_field_not_convertible():\n    class FrozenSetModel(BaseModel):\n        set: FrozenSet[int]\n\n    with pytest.raises(ValidationError, match=r'frozenset'):\n        FrozenSetModel(set=42)\n\n\n@pytest.mark.parametrize(\n    'input_value,output,human_bin,human_dec,human_sep',\n    (\n        (1, 1, '1B', '1B', '1 B'),\n        ('1', 1, '1B', '1B', '1 B'),\n        ('1.0', 1, '1B', '1B', '1 B'),\n        ('1b', 1, '1B', '1B', '1 B'),\n        ('1.5 KB', int(1.5e3), '1.5KiB', '1.5KB', '1.5 KiB'),\n        ('1.5 K', int(1.5e3), '1.5KiB', '1.5KB', '1.5 KiB'),\n        ('1.5 MB', int(1.5e6), '1.4MiB', '1.5MB', '1.4 MiB'),\n        ('1.5 M', int(1.5e6), '1.4MiB', '1.5MB', '1.4 MiB'),\n        ('5.1kib', 5222, '5.1KiB', '5.2KB', '5.1 KiB'),\n        ('6.2EiB', 7148113328562451456, '6.2EiB', '7.1EB', '6.2 EiB'),\n        ('8bit', 1, '1B', '1B', '1 B'),\n        ('1kbit', 125, '125B', '125B', '125 B'),\n    ),\n)\ndef test_bytesize_conversions(input_value, output, human_bin, human_dec, human_sep):\n    class Model(BaseModel):\n        size: ByteSize\n\n    m = Model(size=input_value)\n\n    assert m.size == output\n\n    assert m.size.human_readable() == human_bin\n    assert m.size.human_readable(decimal=True) == human_dec\n    assert m.size.human_readable(separator=' ') == human_sep\n\n\ndef test_bytesize_to():\n    class Model(BaseModel):\n        size: ByteSize\n\n    m = Model(size='1GiB')\n\n    assert m.size.to('MiB') == pytest.approx(1024)\n    assert m.size.to('MB') == pytest.approx(1073.741824)\n    assert m.size.to('TiB') == pytest.approx(0.0009765625)\n    assert m.size.to('bit') == pytest.approx(8589934592)\n    assert m.size.to('kbit') == pytest.approx(8589934.592)\n\n\ndef test_bytesize_raises():\n    class Model(BaseModel):\n        size: ByteSize\n\n    with pytest.raises(ValidationError, match='parse value') as exc_info:\n        Model(size='d1MB')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'd1MB',\n            'loc': ('size',),\n            'msg': 'could not parse value and unit from byte string',\n            'type': 'byte_size',\n        }\n    ]\n\n    with pytest.raises(ValidationError, match='byte unit') as exc_info:\n        Model(size='1LiB')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'unit': 'LiB'},\n            'input': '1LiB',\n            'loc': ('size',),\n            'msg': 'could not interpret byte unit: LiB',\n            'type': 'byte_size_unit',\n        }\n    ]\n\n    # 1Gi is not a valid unit unlike 1G\n    with pytest.raises(ValidationError, match='byte unit'):\n        Model(size='1Gi')\n\n    m = Model(size='1MB')\n    with pytest.raises(PydanticCustomError, match='byte unit'):\n        m.size.to('bad_unit')\n\n    with pytest.raises(PydanticCustomError, match='byte unit'):\n        m.size.to('1ZiB')\n\n\ndef test_deque_success():\n    class Model(BaseModel):\n        v: deque\n\n    assert Model(v=[1, 2, 3]).v == deque([1, 2, 3])\n\n\n@pytest.mark.parametrize(\n    'cls,value,result',\n    (\n        (int, [1, 2, 3], deque([1, 2, 3])),\n        (int, (1, 2, 3), deque((1, 2, 3))),\n        (int, deque((1, 2, 3)), deque((1, 2, 3))),\n        (float, [1.0, 2.0, 3.0], deque([1.0, 2.0, 3.0])),\n        (Set[int], [{1, 2}, {3, 4}, {5, 6}], deque([{1, 2}, {3, 4}, {5, 6}])),\n        (Tuple[int, str], ((1, 'a'), (2, 'b'), (3, 'c')), deque(((1, 'a'), (2, 'b'), (3, 'c')))),\n        (str, [w for w in 'one two three'.split()], deque(['one', 'two', 'three'])),\n        (\n            int,\n            {1: 10, 2: 20, 3: 30}.keys(),\n            deque([1, 2, 3]),\n        ),\n        (\n            int,\n            {1: 10, 2: 20, 3: 30}.values(),\n            deque([10, 20, 30]),\n        ),\n        (\n            Tuple[int, int],\n            {1: 10, 2: 20, 3: 30}.items(),\n            deque([(1, 10), (2, 20), (3, 30)]),\n        ),\n        (\n            float,\n            {1, 2, 3},\n            deque([1, 2, 3]),\n        ),\n        (\n            float,\n            frozenset((1, 2, 3)),\n            deque([1, 2, 3]),\n        ),\n    ),\n)\ndef test_deque_generic_success(cls, value, result):\n    class Model(BaseModel):\n        v: Deque[cls]\n\n    assert Model(v=value).v == result\n\n\n@pytest.mark.parametrize(\n    'cls,value,result',\n    (\n        (int, deque((1, 2, 3)), deque((1, 2, 3))),\n        (str, deque(('1', '2', '3')), deque(('1', '2', '3'))),\n    ),\n)\ndef test_deque_generic_success_strict(cls, value: Any, result):\n    class Model(BaseModel):\n        v: Deque[cls]\n\n        model_config = ConfigDict(strict=True)\n\n    assert Model(v=value).v == result\n\n\n@pytest.mark.parametrize(\n    'cls,value,expected_error',\n    (\n        (\n            int,\n            [1, 'a', 3],\n            {\n                'type': 'int_parsing',\n                'loc': ('v', 1),\n                'msg': 'Input should be a valid integer, unable to parse string as an integer',\n                'input': 'a',\n            },\n        ),\n        (\n            int,\n            (1, 2, 'a'),\n            {\n                'type': 'int_parsing',\n                'loc': ('v', 2),\n                'msg': 'Input should be a valid integer, unable to parse string as an integer',\n                'input': 'a',\n            },\n        ),\n        (\n            Tuple[int, str],\n            ((1, 'a'), ('a', 'a'), (3, 'c')),\n            {\n                'type': 'int_parsing',\n                'loc': ('v', 1, 0),\n                'msg': 'Input should be a valid integer, unable to parse string as an integer',\n                'input': 'a',\n            },\n        ),\n        (\n            List[int],\n            [{'a': 1, 'b': 2}, [1, 2], [2, 3]],\n            {\n                'type': 'list_type',\n                'loc': ('v', 0),\n                'msg': 'Input should be a valid list',\n                'input': {\n                    'a': 1,\n                    'b': 2,\n                },\n            },\n        ),\n    ),\n)\ndef test_deque_fails(cls, value, expected_error):\n    class Model(BaseModel):\n        v: Deque[cls]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(v=value)\n    # debug(exc_info.value.errors(include_url=False))\n    assert len(exc_info.value.errors(include_url=False)) == 1\n    assert expected_error == exc_info.value.errors(include_url=False)[0]\n\n\ndef test_deque_model():\n    class Model2(BaseModel):\n        x: int\n\n    class Model(BaseModel):\n        v: Deque[Model2]\n\n    seq = [Model2(x=1), Model2(x=2)]\n    assert Model(v=seq).v == deque(seq)\n\n\ndef test_deque_json():\n    class Model(BaseModel):\n        v: Deque[int]\n\n    assert Model(v=deque((1, 2, 3))).model_dump_json() == '{\"v\":[1,2,3]}'\n\n\ndef test_deque_any_maxlen():\n    class DequeModel1(BaseModel):\n        field: deque\n\n    assert DequeModel1(field=deque()).field.maxlen is None\n    assert DequeModel1(field=deque(maxlen=8)).field.maxlen == 8\n\n    class DequeModel2(BaseModel):\n        field: deque = deque()\n\n    assert DequeModel2().field.maxlen is None\n    assert DequeModel2(field=deque()).field.maxlen is None\n    assert DequeModel2(field=deque(maxlen=8)).field.maxlen == 8\n\n    class DequeModel3(BaseModel):\n        field: deque = deque(maxlen=5)\n\n    assert DequeModel3().field.maxlen == 5\n    assert DequeModel3(field=deque()).field.maxlen is None\n    assert DequeModel3(field=deque(maxlen=8)).field.maxlen == 8\n\n\ndef test_deque_typed_maxlen():\n    class DequeModel1(BaseModel):\n        field: Deque[int]\n\n    assert DequeModel1(field=deque()).field.maxlen is None\n    assert DequeModel1(field=deque(maxlen=8)).field.maxlen == 8\n\n    class DequeModel2(BaseModel):\n        field: Deque[int] = deque()\n\n    assert DequeModel2().field.maxlen is None\n    assert DequeModel2(field=deque()).field.maxlen is None\n    assert DequeModel2(field=deque(maxlen=8)).field.maxlen == 8\n\n    class DequeModel3(BaseModel):\n        field: Deque[int] = deque(maxlen=5)\n\n    assert DequeModel3().field.maxlen == 5\n    assert DequeModel3(field=deque()).field.maxlen is None\n    assert DequeModel3(field=deque(maxlen=8)).field.maxlen == 8\n\n\ndef test_deque_set_maxlen():\n    class DequeModel1(BaseModel):\n        field: Annotated[Deque[int], Field(max_length=10)]\n\n    assert DequeModel1(field=deque()).field.maxlen == 10\n    assert DequeModel1(field=deque(maxlen=8)).field.maxlen == 8\n    assert DequeModel1(field=deque(maxlen=15)).field.maxlen == 10\n\n    class DequeModel2(BaseModel):\n        field: Annotated[Deque[int], Field(max_length=10)] = deque()\n\n    assert DequeModel2().field.maxlen is None\n    assert DequeModel2(field=deque()).field.maxlen == 10\n    assert DequeModel2(field=deque(maxlen=8)).field.maxlen == 8\n    assert DequeModel2(field=deque(maxlen=15)).field.maxlen == 10\n\n    class DequeModel3(DequeModel2):\n        model_config = ConfigDict(validate_default=True)\n\n    assert DequeModel3().field.maxlen == 10\n\n    class DequeModel4(BaseModel):\n        field: Annotated[Deque[int], Field(max_length=10)] = deque(maxlen=5)\n\n    assert DequeModel4().field.maxlen == 5\n\n    class DequeModel5(DequeModel4):\n        model_config = ConfigDict(validate_default=True)\n\n    assert DequeModel4().field.maxlen == 5\n\n\n@pytest.mark.parametrize('value_type', (None, type(None), None.__class__))\ndef test_none(value_type):\n    class Model(BaseModel):\n        my_none: value_type\n        my_none_list: List[value_type]\n        my_none_dict: Dict[str, value_type]\n        my_json_none: Json[value_type]\n\n    Model(\n        my_none=None,\n        my_none_list=[None] * 3,\n        my_none_dict={'a': None, 'b': None},\n        my_json_none='null',\n    )\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'my_none': {'type': 'null', 'title': 'My None'},\n            'my_none_list': {'type': 'array', 'items': {'type': 'null'}, 'title': 'My None List'},\n            'my_none_dict': {'type': 'object', 'additionalProperties': {'type': 'null'}, 'title': 'My None Dict'},\n            'my_json_none': {\n                'contentMediaType': 'application/json',\n                'contentSchema': {'type': 'null'},\n                'title': 'My Json None',\n                'type': 'string',\n            },\n        },\n        'required': ['my_none', 'my_none_list', 'my_none_dict', 'my_json_none'],\n    }\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(\n            my_none='qwe',\n            my_none_list=[1, None, 'qwe'],\n            my_none_dict={'a': 1, 'b': None},\n            my_json_none='\"a\"',\n        )\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'none_required', 'loc': ('my_none',), 'msg': 'Input should be None', 'input': 'qwe'},\n        {'type': 'none_required', 'loc': ('my_none_list', 0), 'msg': 'Input should be None', 'input': 1},\n        {\n            'type': 'none_required',\n            'loc': ('my_none_list', 2),\n            'msg': 'Input should be None',\n            'input': 'qwe',\n        },\n        {\n            'type': 'none_required',\n            'loc': ('my_none_dict', 'a'),\n            'msg': 'Input should be None',\n            'input': 1,\n        },\n        {'type': 'none_required', 'loc': ('my_json_none',), 'msg': 'Input should be None', 'input': 'a'},\n    ]\n\n\ndef test_none_literal():\n    class Model(BaseModel):\n        my_none: Literal[None]\n        my_none_list: List[Literal[None]]\n        my_none_dict: Dict[str, Literal[None]]\n        my_json_none: Json[Literal[None]]\n\n    Model(\n        my_none=None,\n        my_none_list=[None] * 3,\n        my_none_dict={'a': None, 'b': None},\n        my_json_none='null',\n    )\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'my_none': {'const': None, 'enum': [None], 'title': 'My None', 'type': 'null'},\n            'my_none_list': {\n                'items': {'const': None, 'enum': [None], 'type': 'null'},\n                'title': 'My None List',\n                'type': 'array',\n            },\n            'my_none_dict': {\n                'additionalProperties': {'const': None, 'enum': [None], 'type': 'null'},\n                'title': 'My None Dict',\n                'type': 'object',\n            },\n            'my_json_none': {\n                'contentMediaType': 'application/json',\n                'contentSchema': {'const': None, 'enum': [None], 'type': 'null'},\n                'title': 'My Json None',\n                'type': 'string',\n            },\n        },\n        'required': ['my_none', 'my_none_list', 'my_none_dict', 'my_json_none'],\n    }\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(\n            my_none='qwe',\n            my_none_list=[1, None, 'qwe'],\n            my_none_dict={'a': 1, 'b': None},\n            my_json_none='\"a\"',\n        )\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': ('my_none',),\n            'msg': 'Input should be None',\n            'input': 'qwe',\n            'ctx': {'expected': 'None'},\n        },\n        {\n            'type': 'literal_error',\n            'loc': ('my_none_list', 0),\n            'msg': 'Input should be None',\n            'input': 1,\n            'ctx': {'expected': 'None'},\n        },\n        {\n            'type': 'literal_error',\n            'loc': ('my_none_list', 2),\n            'msg': 'Input should be None',\n            'input': 'qwe',\n            'ctx': {'expected': 'None'},\n        },\n        {\n            'type': 'literal_error',\n            'loc': ('my_none_dict', 'a'),\n            'msg': 'Input should be None',\n            'input': 1,\n            'ctx': {'expected': 'None'},\n        },\n        {\n            'type': 'literal_error',\n            'loc': ('my_json_none',),\n            'msg': 'Input should be None',\n            'input': 'a',\n            'ctx': {'expected': 'None'},\n        },\n    ]\n\n\ndef test_default_union_types():\n    class DefaultModel(BaseModel):\n        v: Union[int, bool, str]\n\n    # do it this way since `1 == True`\n    assert repr(DefaultModel(v=True).v) == 'True'\n    assert repr(DefaultModel(v=1).v) == '1'\n    assert repr(DefaultModel(v='1').v) == \"'1'\"\n\n    assert DefaultModel.model_json_schema() == {\n        'title': 'DefaultModel',\n        'type': 'object',\n        'properties': {'v': {'title': 'V', 'anyOf': [{'type': t} for t in ('integer', 'boolean', 'string')]}},\n        'required': ['v'],\n    }\n\n\ndef test_default_union_types_left_to_right():\n    class DefaultModel(BaseModel):\n        v: Annotated[Union[int, bool, str], Field(union_mode='left_to_right')]\n\n    print(DefaultModel.__pydantic_core_schema__)\n\n    # int will coerce everything in left-to-right mode\n    assert repr(DefaultModel(v=True).v) == '1'\n    assert repr(DefaultModel(v=1).v) == '1'\n    assert repr(DefaultModel(v='1').v) == '1'\n\n    assert DefaultModel.model_json_schema() == {\n        'title': 'DefaultModel',\n        'type': 'object',\n        'properties': {'v': {'title': 'V', 'anyOf': [{'type': t} for t in ('integer', 'boolean', 'string')]}},\n        'required': ['v'],\n    }\n\n\ndef test_union_enum_int_left_to_right():\n    class BinaryEnum(IntEnum):\n        ZERO = 0\n        ONE = 1\n\n    # int will win over enum in this case\n    assert TypeAdapter(Union[BinaryEnum, int]).validate_python(0) is not BinaryEnum.ZERO\n\n    # in left to right mode, enum will validate successfully and take precedence\n    assert (\n        TypeAdapter(Annotated[Union[BinaryEnum, int], Field(union_mode='left_to_right')]).validate_python(0)\n        is BinaryEnum.ZERO\n    )\n\n\ndef test_union_uuid_str_left_to_right():\n    IdOrSlug = Union[UUID, str]\n\n    # in smart mode JSON and python are currently validated differently in this\n    # case, because in Python this is a str but in JSON a str is also a UUID\n    assert TypeAdapter(IdOrSlug).validate_json('\"f4fe10b4-e0c8-4232-ba26-4acd491c2414\"') == UUID(\n        'f4fe10b4-e0c8-4232-ba26-4acd491c2414'\n    )\n    assert (\n        TypeAdapter(IdOrSlug).validate_python('f4fe10b4-e0c8-4232-ba26-4acd491c2414')\n        == 'f4fe10b4-e0c8-4232-ba26-4acd491c2414'\n    )\n\n    IdOrSlugLTR = Annotated[Union[UUID, str], Field(union_mode='left_to_right')]\n\n    # in left to right mode both JSON and python are validated as UUID\n    assert TypeAdapter(IdOrSlugLTR).validate_json('\"f4fe10b4-e0c8-4232-ba26-4acd491c2414\"') == UUID(\n        'f4fe10b4-e0c8-4232-ba26-4acd491c2414'\n    )\n    assert TypeAdapter(IdOrSlugLTR).validate_python('f4fe10b4-e0c8-4232-ba26-4acd491c2414') == UUID(\n        'f4fe10b4-e0c8-4232-ba26-4acd491c2414'\n    )\n\n\ndef test_default_union_class():\n    class A(BaseModel):\n        x: str\n\n    class B(BaseModel):\n        x: str\n\n    class Model(BaseModel):\n        y: Union[A, B]\n\n    assert isinstance(Model(y=A(x='a')).y, A)\n    assert isinstance(Model(y=B(x='b')).y, B)\n\n\n@pytest.mark.parametrize('max_length', [10, None])\ndef test_union_subclass(max_length: Union[int, None]):\n    class MyStr(str): ...\n\n    class Model(BaseModel):\n        x: Union[int, Annotated[str, Field(max_length=max_length)]]\n\n    v = Model(x=MyStr('1')).x\n    assert type(v) is str\n    assert v == '1'\n\n\ndef test_union_compound_types():\n    class Model(BaseModel):\n        values: Union[Dict[str, str], List[str], Dict[str, List[str]]]\n\n    assert Model(values={'L': '1'}).model_dump() == {'values': {'L': '1'}}\n    assert Model(values=['L1']).model_dump() == {'values': ['L1']}\n    assert Model(values=('L1',)).model_dump() == {'values': ['L1']}\n    assert Model(values={'x': ['pika']}) != {'values': {'x': ['pika']}}\n    assert Model(values={'x': ('pika',)}).model_dump() == {'values': {'x': ['pika']}}\n    with pytest.raises(ValidationError) as e:\n        Model(values={'x': {'a': 'b'}})\n    # insert_assert(e.value.errors(include_url=False))\n    assert e.value.errors(include_url=False) == [\n        {\n            'type': 'string_type',\n            'loc': ('values', 'dict[str,str]', 'x'),\n            'msg': 'Input should be a valid string',\n            'input': {'a': 'b'},\n        },\n        {\n            'type': 'list_type',\n            'loc': ('values', 'list[str]'),\n            'msg': 'Input should be a valid list',\n            'input': {'x': {'a': 'b'}},\n        },\n        {\n            'type': 'list_type',\n            'loc': ('values', 'dict[str,list[str]]', 'x'),\n            'msg': 'Input should be a valid list',\n            'input': {'a': 'b'},\n        },\n    ]\n\n\ndef test_smart_union_compounded_types_edge_case():\n    class Model(BaseModel):\n        x: Union[List[str], List[int]]\n\n    assert Model(x=[1, 2]).x == [1, 2]\n    assert Model(x=['1', '2']).x == ['1', '2']\n    assert Model(x=[1, '2']).x == [1, 2]\n\n\ndef test_union_typeddict():\n    class Dict1(TypedDict):\n        foo: str\n\n    class Dict2(TypedDict):\n        bar: str\n\n    class M(BaseModel):\n        d: Union[Dict2, Dict1]\n\n    assert M(d=dict(foo='baz')).d == {'foo': 'baz'}\n\n\ndef test_custom_generic_containers():\n    T = TypeVar('T')\n\n    class GenericList(List[T]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(GenericList, handler(List[get_args(source_type)[0]]))\n\n    class Model(BaseModel):\n        field: GenericList[int]\n\n    model = Model(field=['1', '2'])\n    assert model.field == [1, 2]\n    assert isinstance(model.field, GenericList)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(field=['a'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('field', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    ('field_type', 'input_data', 'expected_value', 'serialized_data'),\n    [\n        pytest.param(Base64Bytes, b'Zm9vIGJhcg==\\n', b'foo bar', b'Zm9vIGJhcg==\\n', id='Base64Bytes-reversible'),\n        pytest.param(Base64Str, 'Zm9vIGJhcg==\\n', 'foo bar', 'Zm9vIGJhcg==\\n', id='Base64Str-reversible'),\n        pytest.param(Base64Bytes, b'Zm9vIGJhcg==', b'foo bar', b'Zm9vIGJhcg==\\n', id='Base64Bytes-bytes-input'),\n        pytest.param(Base64Bytes, 'Zm9vIGJhcg==', b'foo bar', b'Zm9vIGJhcg==\\n', id='Base64Bytes-str-input'),\n        pytest.param(\n            Base64Bytes, bytearray(b'Zm9vIGJhcg=='), b'foo bar', b'Zm9vIGJhcg==\\n', id='Base64Bytes-bytearray-input'\n        ),\n        pytest.param(Base64Str, b'Zm9vIGJhcg==', 'foo bar', 'Zm9vIGJhcg==\\n', id='Base64Str-bytes-input'),\n        pytest.param(Base64Str, 'Zm9vIGJhcg==', 'foo bar', 'Zm9vIGJhcg==\\n', id='Base64Str-str-input'),\n        pytest.param(\n            Base64Str, bytearray(b'Zm9vIGJhcg=='), 'foo bar', 'Zm9vIGJhcg==\\n', id='Base64Str-bytearray-input'\n        ),\n        pytest.param(\n            Base64Bytes,\n            b'BCq+6+1/Paun/Q==',\n            b'\\x04*\\xbe\\xeb\\xed\\x7f=\\xab\\xa7\\xfd',\n            b'BCq+6+1/Paun/Q==\\n',\n            id='Base64Bytes-bytes-alphabet-vanilla',\n        ),\n    ],\n)\ndef test_base64(field_type, input_data, expected_value, serialized_data):\n    class Model(BaseModel):\n        base64_value: field_type\n        base64_value_or_none: Optional[field_type] = None\n\n    m = Model(base64_value=input_data)\n    assert m.base64_value == expected_value\n\n    m = Model.model_construct(base64_value=expected_value)\n    assert m.base64_value == expected_value\n\n    assert m.model_dump() == {\n        'base64_value': serialized_data,\n        'base64_value_or_none': None,\n    }\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'base64_value': {\n                'format': 'base64',\n                'title': 'Base64 Value',\n                'type': 'string',\n            },\n            'base64_value_or_none': {\n                'anyOf': [{'type': 'string', 'format': 'base64'}, {'type': 'null'}],\n                'default': None,\n                'title': 'Base64 Value Or None',\n            },\n        },\n        'required': ['base64_value'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    ('field_type', 'input_data'),\n    [\n        pytest.param(Base64Bytes, b'Zm9vIGJhcg', id='Base64Bytes-invalid-base64-bytes'),\n        pytest.param(Base64Bytes, 'Zm9vIGJhcg', id='Base64Bytes-invalid-base64-str'),\n        pytest.param(Base64Str, b'Zm9vIGJhcg', id='Base64Str-invalid-base64-bytes'),\n        pytest.param(Base64Str, 'Zm9vIGJhcg', id='Base64Str-invalid-base64-str'),\n    ],\n)\ndef test_base64_invalid(field_type, input_data):\n    class Model(BaseModel):\n        base64_value: field_type\n\n    with pytest.raises(ValidationError) as e:\n        Model(base64_value=input_data)\n\n    assert e.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': 'Incorrect padding'},\n            'input': input_data,\n            'loc': ('base64_value',),\n            'msg': \"Base64 decoding error: 'Incorrect padding'\",\n            'type': 'base64_decode',\n        },\n    ]\n\n\n@pytest.mark.parametrize(\n    ('field_type', 'input_data', 'expected_value', 'serialized_data'),\n    [\n        pytest.param(Base64UrlBytes, b'Zm9vIGJhcg==\\n', b'foo bar', b'Zm9vIGJhcg==', id='Base64UrlBytes-reversible'),\n        pytest.param(Base64UrlStr, 'Zm9vIGJhcg==\\n', 'foo bar', 'Zm9vIGJhcg==', id='Base64UrlStr-reversible'),\n        pytest.param(Base64UrlBytes, b'Zm9vIGJhcg==', b'foo bar', b'Zm9vIGJhcg==', id='Base64UrlBytes-bytes-input'),\n        pytest.param(Base64UrlBytes, 'Zm9vIGJhcg==', b'foo bar', b'Zm9vIGJhcg==', id='Base64UrlBytes-str-input'),\n        pytest.param(\n            Base64UrlBytes, bytearray(b'Zm9vIGJhcg=='), b'foo bar', b'Zm9vIGJhcg==', id='Base64UrlBytes-bytearray-input'\n        ),\n        pytest.param(Base64UrlStr, b'Zm9vIGJhcg==', 'foo bar', 'Zm9vIGJhcg==', id='Base64UrlStr-bytes-input'),\n        pytest.param(Base64UrlStr, 'Zm9vIGJhcg==', 'foo bar', 'Zm9vIGJhcg==', id='Base64UrlStr-str-input'),\n        pytest.param(\n            Base64UrlStr, bytearray(b'Zm9vIGJhcg=='), 'foo bar', 'Zm9vIGJhcg==', id='Base64UrlStr-bytearray-input'\n        ),\n        pytest.param(\n            Base64UrlBytes,\n            b'BCq-6-1_Paun_Q==',\n            b'\\x04*\\xbe\\xeb\\xed\\x7f=\\xab\\xa7\\xfd',\n            b'BCq-6-1_Paun_Q==',\n            id='Base64UrlBytes-bytes-alphabet-url',\n        ),\n        pytest.param(\n            Base64UrlBytes,\n            b'BCq+6+1/Paun/Q==',\n            b'\\x04*\\xbe\\xeb\\xed\\x7f=\\xab\\xa7\\xfd',\n            b'BCq-6-1_Paun_Q==',\n            id='Base64UrlBytes-bytes-alphabet-vanilla',\n        ),\n    ],\n)\ndef test_base64url(field_type, input_data, expected_value, serialized_data):\n    class Model(BaseModel):\n        base64url_value: field_type\n        base64url_value_or_none: Optional[field_type] = None\n\n    m = Model(base64url_value=input_data)\n    assert m.base64url_value == expected_value\n\n    m = Model.model_construct(base64url_value=expected_value)\n    assert m.base64url_value == expected_value\n\n    assert m.model_dump() == {\n        'base64url_value': serialized_data,\n        'base64url_value_or_none': None,\n    }\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'base64url_value': {\n                'format': 'base64url',\n                'title': 'Base64Url Value',\n                'type': 'string',\n            },\n            'base64url_value_or_none': {\n                'anyOf': [{'type': 'string', 'format': 'base64url'}, {'type': 'null'}],\n                'default': None,\n                'title': 'Base64Url Value Or None',\n            },\n        },\n        'required': ['base64url_value'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    ('field_type', 'input_data'),\n    [\n        pytest.param(Base64UrlBytes, b'Zm9vIGJhcg', id='Base64UrlBytes-invalid-base64-bytes'),\n        pytest.param(Base64UrlBytes, 'Zm9vIGJhcg', id='Base64UrlBytes-invalid-base64-str'),\n        pytest.param(Base64UrlStr, b'Zm9vIGJhcg', id='Base64UrlStr-invalid-base64-bytes'),\n        pytest.param(Base64UrlStr, 'Zm9vIGJhcg', id='Base64UrlStr-invalid-base64-str'),\n    ],\n)\ndef test_base64url_invalid(field_type, input_data):\n    class Model(BaseModel):\n        base64url_value: field_type\n\n    with pytest.raises(ValidationError) as e:\n        Model(base64url_value=input_data)\n\n    assert e.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': 'Incorrect padding'},\n            'input': input_data,\n            'loc': ('base64url_value',),\n            'msg': \"Base64 decoding error: 'Incorrect padding'\",\n            'type': 'base64_decode',\n        },\n    ]\n\n\ndef test_sequence_subclass_without_core_schema() -> None:\n    class MyList(List[int]):\n        # The point of this is that subclasses can do arbitrary things\n        # This is the reason why we don't try to handle them automatically\n        # TBD if we introspect `__init__` / `__new__`\n        # (which is the main thing that would mess us up if modified in a subclass)\n        # and automatically handle cases where the subclass doesn't override it.\n        # There's still edge cases (again, arbitrary behavior...)\n        # and it's harder to explain, but could lead to a better user experience in some cases\n        # It will depend on how the complaints (which have and will happen in both directions)\n        # balance out\n        def __init__(self, *args: Any, required: int, **kwargs: Any) -> None:\n            super().__init__(*args, **kwargs)\n\n    with pytest.raises(\n        PydanticSchemaGenerationError, match='implement `__get_pydantic_core_schema__` on your type to fully support it'\n    ):\n\n        class _(BaseModel):\n            x: MyList\n\n\ndef test_typing_coercion_defaultdict():\n    class Model(BaseModel):\n        x: DefaultDict[int, str]\n\n    d = defaultdict(str)\n    d['1']\n    m = Model(x=d)\n    assert isinstance(m.x, defaultdict)\n    assert repr(m.x) == \"defaultdict(<class 'str'>, {1: ''})\"\n\n\ndef test_typing_coercion_counter():\n    class Model(BaseModel):\n        x: Counter[str]\n\n    m = Model(x={'a': 10})\n    assert isinstance(m.x, Counter)\n    assert repr(m.x) == \"Counter({'a': 10})\"\n\n\ndef test_typing_counter_value_validation():\n    class Model(BaseModel):\n        x: Counter[str]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x={'a': 'a'})\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('x', 'a'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'a',\n        }\n    ]\n\n\ndef test_mapping_subclass_without_core_schema() -> None:\n    class MyDict(Dict[int, int]):\n        # The point of this is that subclasses can do arbitrary things\n        # This is the reason why we don't try to handle them automatically\n        # TBD if we introspect `__init__` / `__new__`\n        # (which is the main thing that would mess us up if modified in a subclass)\n        # and automatically handle cases where the subclass doesn't override it.\n        # There's still edge cases (again, arbitrary behavior...)\n        # and it's harder to explain, but could lead to a better user experience in some cases\n        # It will depend on how the complaints (which have and will happen in both directions)\n        # balance out\n        def __init__(self, *args: Any, required: int, **kwargs: Any) -> None:\n            super().__init__(*args, **kwargs)\n\n    with pytest.raises(\n        PydanticSchemaGenerationError, match='implement `__get_pydantic_core_schema__` on your type to fully support it'\n    ):\n\n        class _(BaseModel):\n            x: MyDict\n\n\ndef test_defaultdict_unknown_default_factory() -> None:\n    \"\"\"\n    https://github.com/pydantic/pydantic/issues/4687\n    \"\"\"\n    with pytest.raises(\n        PydanticSchemaGenerationError,\n        match=r'Unable to infer a default factory for keys of type typing.DefaultDict\\[int, int\\]',\n    ):\n\n        class Model(BaseModel):\n            d: DefaultDict[int, DefaultDict[int, int]]\n\n\ndef test_defaultdict_infer_default_factory() -> None:\n    class Model(BaseModel):\n        a: DefaultDict[int, List[int]]\n        b: DefaultDict[int, int]\n        c: DefaultDict[int, set]\n\n    m = Model(a={}, b={}, c={})\n    assert m.a.default_factory is not None\n    assert m.a.default_factory() == []\n    assert m.b.default_factory is not None\n    assert m.b.default_factory() == 0\n    assert m.c.default_factory is not None\n    assert m.c.default_factory() == set()\n\n\ndef test_defaultdict_explicit_default_factory() -> None:\n    class MyList(List[int]):\n        pass\n\n    class Model(BaseModel):\n        a: DefaultDict[int, Annotated[List[int], Field(default_factory=lambda: MyList())]]\n\n    m = Model(a={})\n    assert m.a.default_factory is not None\n    assert isinstance(m.a.default_factory(), MyList)\n\n\ndef test_defaultdict_default_factory_preserved() -> None:\n    class Model(BaseModel):\n        a: DefaultDict[int, List[int]]\n\n    class MyList(List[int]):\n        pass\n\n    m = Model(a=defaultdict(lambda: MyList()))\n    assert m.a.default_factory is not None\n    assert isinstance(m.a.default_factory(), MyList)\n\n\ndef test_custom_default_dict() -> None:\n    KT = TypeVar('KT')\n    VT = TypeVar('VT')\n\n    class CustomDefaultDict(DefaultDict[KT, VT]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            keys_type, values_type = get_args(source_type)\n            return core_schema.no_info_after_validator_function(\n                lambda x: cls(x.default_factory, x), handler(DefaultDict[keys_type, values_type])\n            )\n\n    ta = TypeAdapter(CustomDefaultDict[str, int])\n\n    assert ta.validate_python({'a': 1}) == CustomDefaultDict(int, {'a': 1})\n\n\n@pytest.mark.parametrize('field_type', [typing.OrderedDict, collections.OrderedDict])\ndef test_ordered_dict_from_ordered_dict(field_type):\n    class Model(BaseModel):\n        od_field: field_type\n\n    od_value = collections.OrderedDict([('a', 1), ('b', 2)])\n\n    m = Model(od_field=od_value)\n\n    assert isinstance(m.od_field, collections.OrderedDict)\n    assert m.od_field == od_value\n    # we don't make any promises about preserving instances\n    # at the moment we always copy them for consistency and predictability\n    # so this is more so documenting the current behavior than a promise\n    # we make to users\n    assert m.od_field is not od_value\n\n    assert m.model_json_schema() == {\n        'properties': {'od_field': {'title': 'Od Field', 'type': 'object'}},\n        'required': ['od_field'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_ordered_dict_from_ordered_dict_typed():\n    class Model(BaseModel):\n        od_field: typing.OrderedDict[str, int]\n\n    od_value = collections.OrderedDict([('a', 1), ('b', 2)])\n\n    m = Model(od_field=od_value)\n\n    assert isinstance(m.od_field, collections.OrderedDict)\n    assert m.od_field == od_value\n\n    assert m.model_json_schema() == {\n        'properties': {\n            'od_field': {\n                'additionalProperties': {'type': 'integer'},\n                'title': 'Od Field',\n                'type': 'object',\n            }\n        },\n        'required': ['od_field'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('field_type', [typing.OrderedDict, collections.OrderedDict])\ndef test_ordered_dict_from_dict(field_type):\n    class Model(BaseModel):\n        od_field: field_type\n\n    od_value = {'a': 1, 'b': 2}\n\n    m = Model(od_field=od_value)\n\n    assert isinstance(m.od_field, collections.OrderedDict)\n    assert m.od_field == collections.OrderedDict(od_value)\n\n    assert m.model_json_schema() == {\n        'properties': {'od_field': {'title': 'Od Field', 'type': 'object'}},\n        'required': ['od_field'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_handle_3rd_party_custom_type_reusing_known_metadata() -> None:\n    class PdDecimal(Decimal):\n        def ___repr__(self) -> str:\n            return f'PdDecimal({super().__repr__()})'\n\n    class PdDecimalMarker:\n        def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_after_validator_function(PdDecimal, handler.generate_schema(Decimal))\n\n    class Model(BaseModel):\n        x: Annotated[PdDecimal, PdDecimalMarker(), annotated_types.Gt(0)]\n\n    assert isinstance(Model(x=1).x, PdDecimal)\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=-1)\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'greater_than',\n            'loc': ('x',),\n            'msg': 'Input should be greater than 0',\n            'input': -1,\n            'ctx': {'gt': 0},\n        }\n    ]\n\n\n@pytest.mark.parametrize('optional', [True, False])\ndef test_skip_validation(optional):\n    type_hint = SkipValidation[int]\n    if optional:\n        type_hint = Optional[type_hint]\n\n    @validate_call\n    def my_function(y: type_hint):\n        return repr(y)\n\n    assert my_function('2') == \"'2'\"\n\n\ndef test_skip_validation_model_reference():\n    class ModelA(BaseModel):\n        x: int\n\n    class ModelB(BaseModel):\n        y: SkipValidation[ModelA]\n\n    assert ModelB(y=123).y == 123\n\n\ndef test_skip_validation_serialization():\n    class A(BaseModel):\n        x: SkipValidation[int]\n\n        @field_serializer('x')\n        def double_x(self, v):\n            return v * 2\n\n    assert A(x=1).model_dump() == {'x': 2}\n    assert A(x='abc').model_dump() == {'x': 'abcabc'}  # no validation\n    assert A(x='abc').model_dump_json() == '{\"x\":\"abcabc\"}'\n\n\ndef test_skip_validation_json_schema():\n    class A(BaseModel):\n        x: SkipValidation[int]\n\n    assert A.model_json_schema() == {\n        'properties': {'x': {'title': 'X', 'type': 'integer'}},\n        'required': ['x'],\n        'title': 'A',\n        'type': 'object',\n    }\n\n\ndef test_transform_schema():\n    ValidateStrAsInt = Annotated[str, GetPydanticSchema(lambda _s, h: core_schema.int_schema())]\n\n    class Model(BaseModel):\n        x: Optional[ValidateStrAsInt]\n\n    assert Model(x=None).x is None\n    assert Model(x='1').x == 1\n\n\ndef test_transform_schema_for_first_party_class():\n    # Here, first party means you can define the `__get_pydantic_core_schema__` method on the class directly.\n    class LowercaseStr(str):\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            source_type: Any,\n            handler: GetCoreSchemaHandler,\n        ) -> CoreSchema:\n            schema = handler(str)\n            return core_schema.no_info_after_validator_function(lambda v: v.lower(), schema)\n\n    class Model(BaseModel):\n        lower: LowercaseStr = Field(min_length=1)\n\n    assert Model(lower='ABC').lower == 'abc'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(lower='')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('lower',),\n            'msg': 'Value should have at least 1 item after validation, not 0',\n            'input': '',\n            'ctx': {'field_type': 'Value', 'min_length': 1, 'actual_length': 0},\n        }\n    ]\n\n\ndef test_constraint_dataclass() -> None:\n    @dataclass(order=True)\n    # need to make it inherit from int so that\n    # because the PydanticKnownError requires it to be a number\n    # but it's not really relevant to this test\n    class MyDataclass(int):\n        x: int\n\n    ta = TypeAdapter(Annotated[MyDataclass, annotated_types.Gt(MyDataclass(0))])\n\n    assert ta.validate_python(MyDataclass(1)) == MyDataclass(1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        ta.validate_python(MyDataclass(0))\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'greater_than',\n            'loc': (),\n            'msg': 'Input should be greater than 0',\n            'input': MyDataclass(0),\n            'ctx': {'gt': MyDataclass(0)},\n        }\n    ]\n\n\ndef test_transform_schema_for_third_party_class():\n    # Here, third party means you can't define methods on the class directly, so have to use annotations.\n\n    class IntWrapper:\n        # This is pretending to be a third-party class. This example is specifically inspired by pandas.Timestamp,\n        # which can receive an item of type `datetime` as an input to its `__init__`.\n        # The important thing here is we are not defining any custom methods on this type directly.\n        def __init__(self, t: int) -> None:\n            self.t = t\n\n        def __eq__(self, value: object) -> bool:\n            if isinstance(value, IntWrapper):\n                return self.t == value.t\n            elif isinstance(value, int):\n                return self.t == value\n            return False\n\n        def __gt__(self, value: object) -> bool:\n            if isinstance(value, IntWrapper):\n                return self.t > value.t\n            elif isinstance(value, int):\n                return self.t > value\n            return NotImplemented\n\n    class _IntWrapperAnnotation:\n        # This is an auxiliary class that, when used as the first annotation for DatetimeWrapper,\n        # ensures pydantic can produce a valid schema.\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            source_type: Any,\n            handler: GetCoreSchemaHandler,\n        ) -> CoreSchema:\n            schema = handler.generate_schema(int)\n            return core_schema.no_info_after_validator_function(IntWrapper, schema)\n\n    # Giving a name to Annotated[IntWrapper, _IntWrapperAnnotation] makes it easier to use in code\n    # where I want a field of type `IntWrapper` that works as desired with pydantic.\n    PydanticDatetimeWrapper = Annotated[IntWrapper, _IntWrapperAnnotation]\n\n    class Model(BaseModel):\n        # The reason all of the above is necessary is specifically so that we get good behavior\n        x: Annotated[PydanticDatetimeWrapper, annotated_types.Gt(123)]\n\n    m = Model(x=1234)\n    assert isinstance(m.x, IntWrapper)\n    assert repr(m.x.t) == '1234'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=1)\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'greater_than',\n            'loc': ('x',),\n            'msg': 'Input should be greater than 123',\n            'input': 1,\n            'ctx': {'gt': 123},\n        }\n    ]\n\n\ndef test_iterable_arbitrary_type():\n    class CustomIterable(Iterable):\n        def __init__(self, iterable):\n            self.iterable = iterable\n\n        def __iter__(self):\n            return self\n\n        def __next__(self):\n            return next(self.iterable)\n\n    with pytest.raises(\n        PydanticSchemaGenerationError,\n        match='Unable to generate pydantic-core schema for .*CustomIterable.*. Set `arbitrary_types_allowed=True`',\n    ):\n\n        class Model(BaseModel):\n            x: CustomIterable\n\n\ndef test_typing_extension_literal_field():\n    from typing_extensions import Literal\n\n    class Model(BaseModel):\n        foo: Literal['foo']\n\n    assert Model(foo='foo').foo == 'foo'\n\n\ndef test_typing_literal_field():\n    from typing import Literal\n\n    class Model(BaseModel):\n        foo: Literal['foo']\n\n    assert Model(foo='foo').foo == 'foo'\n\n\ndef test_instance_of_annotation():\n    class Model(BaseModel):\n        # Note: the generic parameter gets ignored by runtime validation\n        x: InstanceOf[Sequence[int]]\n\n    class MyList(list):\n        pass\n\n    assert Model(x='abc').x == 'abc'\n    assert type(Model(x=MyList([1, 2, 3])).x) is MyList\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'Sequence'},\n            'input': 1,\n            'loc': ('x',),\n            'msg': 'Input should be an instance of Sequence',\n            'type': 'is_instance_of',\n        }\n    ]\n\n    assert Model.model_validate_json('{\"x\": [1,2,3]}').x == [1, 2, 3]\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate_json('{\"x\": \"abc\"}')\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 'abc', 'loc': ('x',), 'msg': 'Input should be a valid array', 'type': 'list_type'}\n    ]\n\n\ndef test_instanceof_invalid_core_schema():\n    class MyClass:\n        pass\n\n    class MyModel(BaseModel):\n        a: InstanceOf[MyClass]\n        b: Optional[InstanceOf[MyClass]]\n\n    MyModel(a=MyClass(), b=None)\n    MyModel(a=MyClass(), b=MyClass())\n    with pytest.raises(ValidationError) as exc_info:\n        MyModel(a=1, b=1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'test_instanceof_invalid_core_schema.<locals>.MyClass'},\n            'input': 1,\n            'loc': ('a',),\n            'msg': 'Input should be an instance of ' 'test_instanceof_invalid_core_schema.<locals>.MyClass',\n            'type': 'is_instance_of',\n        },\n        {\n            'ctx': {'class': 'test_instanceof_invalid_core_schema.<locals>.MyClass'},\n            'input': 1,\n            'loc': ('b',),\n            'msg': 'Input should be an instance of ' 'test_instanceof_invalid_core_schema.<locals>.MyClass',\n            'type': 'is_instance_of',\n        },\n    ]\n    with pytest.raises(\n        PydanticInvalidForJsonSchema, match='Cannot generate a JsonSchema for core_schema.IsInstanceSchema'\n    ):\n        MyModel.model_json_schema()\n\n\ndef test_instanceof_serialization():\n    class Inner(BaseModel):\n        pass\n\n    class SubInner(Inner):\n        x: int\n\n    class OuterStandard(BaseModel):\n        inner: InstanceOf[Inner]\n\n    assert OuterStandard(inner=SubInner(x=1)).model_dump() == {'inner': {}}\n\n    class OuterAsAny(BaseModel):\n        inner1: SerializeAsAny[InstanceOf[Inner]]\n        inner2: InstanceOf[SerializeAsAny[Inner]]\n\n    assert OuterAsAny(inner1=SubInner(x=2), inner2=SubInner(x=3)).model_dump() == {\n        'inner1': {'x': 2},\n        'inner2': {'x': 3},\n    }\n\n\ndef test_constraints_arbitrary_type() -> None:\n    class CustomType:\n        def __init__(self, v: Any) -> None:\n            self.v = v\n\n        def __eq__(self, o: object) -> bool:\n            return self.v == o\n\n        def __le__(self, o: object) -> bool:\n            return self.v <= o\n\n        def __lt__(self, o: object) -> bool:\n            return self.v < o\n\n        def __ge__(self, o: object) -> bool:\n            return self.v >= o\n\n        def __gt__(self, o: object) -> bool:\n            return self.v > o\n\n        def __mod__(self, o: Any) -> Any:\n            return self.v % o\n\n        def __len__(self) -> int:\n            return len(self.v)\n\n        def __repr__(self) -> str:\n            return f'CustomType({self.v})'\n\n    class Model(BaseModel):\n        gt: Annotated[CustomType, annotated_types.Gt(0)]\n        ge: Annotated[CustomType, annotated_types.Ge(0)]\n        lt: Annotated[CustomType, annotated_types.Lt(0)]\n        le: Annotated[CustomType, annotated_types.Le(0)]\n        multiple_of: Annotated[CustomType, annotated_types.MultipleOf(2)]\n        min_length: Annotated[CustomType, annotated_types.MinLen(1)]\n        max_length: Annotated[CustomType, annotated_types.MaxLen(1)]\n        predicate: Annotated[CustomType, annotated_types.Predicate(lambda x: x > 0)]\n\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    Model(\n        gt=CustomType(1),\n        ge=CustomType(0),\n        lt=CustomType(-1),\n        le=CustomType(0),\n        min_length=CustomType([1, 2]),\n        max_length=CustomType([1]),\n        multiple_of=CustomType(4),\n        predicate=CustomType(1),\n    )\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(\n            gt=CustomType(-1),\n            ge=CustomType(-1),\n            lt=CustomType(1),\n            le=CustomType(1),\n            min_length=CustomType([]),\n            max_length=CustomType([1, 2, 3]),\n            multiple_of=CustomType(3),\n            predicate=CustomType(-1),\n        )\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'greater_than',\n            'loc': ('gt',),\n            'msg': 'Input should be greater than 0',\n            'input': CustomType(-1),\n            'ctx': {'gt': 0},\n        },\n        {\n            'type': 'greater_than_equal',\n            'loc': ('ge',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': CustomType(-1),\n            'ctx': {'ge': 0},\n        },\n        {\n            'type': 'less_than',\n            'loc': ('lt',),\n            'msg': 'Input should be less than 0',\n            'input': CustomType(1),\n            'ctx': {'lt': 0},\n        },\n        {\n            'type': 'less_than_equal',\n            'loc': ('le',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': CustomType(1),\n            'ctx': {'le': 0},\n        },\n        {\n            'type': 'multiple_of',\n            'loc': ('multiple_of',),\n            'msg': 'Input should be a multiple of 2',\n            'input': CustomType(3),\n            'ctx': {'multiple_of': 2},\n        },\n        {\n            'type': 'too_short',\n            'loc': ('min_length',),\n            'msg': 'Value should have at least 1 item after validation, not 0',\n            'input': CustomType([]),\n            'ctx': {'field_type': 'Value', 'min_length': 1, 'actual_length': 0},\n        },\n        {\n            'type': 'too_long',\n            'loc': ('max_length',),\n            'msg': 'Value should have at most 1 item after validation, not 3',\n            'input': CustomType([1, 2, 3]),\n            'ctx': {'field_type': 'Value', 'max_length': 1, 'actual_length': 3},\n        },\n        {\n            'type': 'predicate_failed',\n            'loc': ('predicate',),\n            'msg': 'Predicate test_constraints_arbitrary_type.<locals>.Model.<lambda> failed',\n            'input': CustomType(-1),\n        },\n    ]\n\n\ndef test_annotated_default_value() -> None:\n    t = TypeAdapter(Annotated[List[int], Field(default=['1', '2'])])\n\n    r = t.get_default_value()\n    assert r is not None\n    assert r.value == ['1', '2']\n\n    # insert_assert(t.json_schema())\n    assert t.json_schema() == {'type': 'array', 'items': {'type': 'integer'}, 'default': ['1', '2']}\n\n\ndef test_annotated_default_value_validate_default() -> None:\n    t = TypeAdapter(Annotated[List[int], Field(default=['1', '2'])], config=ConfigDict(validate_default=True))\n\n    r = t.get_default_value()\n    assert r is not None\n    assert r.value == [1, 2]\n\n    # insert_assert(t.json_schema())\n    assert t.json_schema() == {'type': 'array', 'items': {'type': 'integer'}, 'default': ['1', '2']}\n\n\ndef test_annotated_default_value_functional_validator() -> None:\n    T = TypeVar('T')\n    WithAfterValidator = Annotated[T, AfterValidator(lambda x: [v * 2 for v in x])]\n    WithDefaultValue = Annotated[T, Field(default=['1', '2'])]\n\n    # the order of the args should not matter, we always put the default value on the outside\n    for tp in (WithDefaultValue[WithAfterValidator[List[int]]], WithAfterValidator[WithDefaultValue[List[int]]]):\n        t = TypeAdapter(tp, config=ConfigDict(validate_default=True))\n\n        r = t.get_default_value()\n        assert r is not None\n        assert r.value == [2, 4]\n\n        # insert_assert(t.json_schema())\n        assert t.json_schema() == {'type': 'array', 'items': {'type': 'integer'}, 'default': ['1', '2']}\n\n\n@pytest.mark.parametrize(\n    'pydantic_type,expected',\n    (\n        (Json, 'Json'),\n        (PastDate, 'PastDate'),\n        (FutureDate, 'FutureDate'),\n        (AwareDatetime, 'AwareDatetime'),\n        (NaiveDatetime, 'NaiveDatetime'),\n        (PastDatetime, 'PastDatetime'),\n        (FutureDatetime, 'FutureDatetime'),\n        (ImportString, 'ImportString'),\n    ),\n)\ndef test_types_repr(pydantic_type, expected):\n    assert repr(pydantic_type()) == expected\n\n\ndef test_enum_custom_schema() -> None:\n    class MyEnum(str, Enum):\n        foo = 'FOO'\n        bar = 'BAR'\n        baz = 'BAZ'\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            source_type: Any,\n            handler: GetCoreSchemaHandler,\n        ) -> CoreSchema:\n            # check that we can still call handler\n            handler(source_type)\n\n            # return a custom unrelated schema so we can test that\n            # it gets used\n            schema = core_schema.union_schema(\n                [\n                    core_schema.str_schema(),\n                    core_schema.is_instance_schema(cls),\n                ]\n            )\n            return core_schema.no_info_after_validator_function(\n                function=lambda x: MyEnum(x.upper()) if isinstance(x, str) else x,\n                schema=schema,\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    lambda x: x.value, return_schema=core_schema.int_schema()\n                ),\n            )\n\n    ta = TypeAdapter(MyEnum)\n\n    assert ta.validate_python('foo') == MyEnum.foo\n\n\ndef test_get_pydantic_core_schema_marker_unrelated_type() -> None:\n    \"\"\"Test using handler.generate_schema() to generate a schema that ignores\n    the current context of annotations and such\n    \"\"\"\n\n    @dataclass\n    class Marker:\n        num: int\n\n        def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            schema = handler.resolve_ref_schema(handler.generate_schema(source_type))\n            return core_schema.no_info_after_validator_function(lambda x: x * self.num, schema)\n\n    ta = TypeAdapter(Annotated[int, Marker(2), Marker(3)])\n\n    assert ta.validate_python('1') == 3\n\n\ndef test_string_constraints() -> None:\n    ta = TypeAdapter(\n        Annotated[str, StringConstraints(strip_whitespace=True, to_lower=True), AfterValidator(lambda x: x * 2)]\n    )\n    assert ta.validate_python(' ABC ') == 'abcabc'\n\n\ndef test_string_constraints_strict() -> None:\n    ta = TypeAdapter(Annotated[str, StringConstraints(strict=False)])\n    assert ta.validate_python(b'123') == '123'\n\n    ta = TypeAdapter(Annotated[str, StringConstraints(strict=True)])\n    with pytest.raises(ValidationError):\n        ta.validate_python(b'123')\n\n\ndef test_decimal_float_precision() -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/6807\"\"\"\n    ta = TypeAdapter(Decimal)\n    assert ta.validate_json('1.1') == Decimal('1.1')\n    assert ta.validate_python(1.1) == Decimal('1.1')\n    assert ta.validate_json('\"1.1\"') == Decimal('1.1')\n    assert ta.validate_python('1.1') == Decimal('1.1')\n    assert ta.validate_json('1') == Decimal('1')\n    assert ta.validate_python(1) == Decimal('1')\n\n\ndef test_coerce_numbers_to_str_disabled_in_strict_mode() -> None:\n    class Model(BaseModel):\n        model_config = ConfigDict(strict=True, coerce_numbers_to_str=True)\n        value: str\n\n    with pytest.raises(ValidationError, match='value'):\n        Model.model_validate({'value': 42})\n    with pytest.raises(ValidationError, match='value'):\n        Model.model_validate_json('{\"value\": 42}')\n\n\n@pytest.mark.parametrize('value_param', [True, False])\ndef test_coerce_numbers_to_str_raises_for_bool(value_param: bool) -> None:\n    class Model(BaseModel):\n        model_config = ConfigDict(coerce_numbers_to_str=True)\n        value: str\n\n    with pytest.raises(ValidationError, match='value'):\n        Model.model_validate({'value': value_param})\n    with pytest.raises(ValidationError, match='value'):\n        if value_param is True:\n            Model.model_validate_json('{\"value\": true}')\n        elif value_param is False:\n            Model.model_validate_json('{\"value\": false}')\n\n    @pydantic_dataclass(config=ConfigDict(coerce_numbers_to_str=True))\n    class Model:\n        value: str\n\n    with pytest.raises(ValidationError, match='value'):\n        Model(value=value_param)\n\n\n@pytest.mark.parametrize(\n    ('number', 'expected_str'),\n    [\n        pytest.param(42, '42', id='42'),\n        pytest.param(42.0, '42.0', id='42.0'),\n        pytest.param(Decimal('42.0'), '42.0', id=\"Decimal('42.0')\"),\n    ],\n)\ndef test_coerce_numbers_to_str(number: Number, expected_str: str) -> None:\n    class Model(BaseModel):\n        model_config = ConfigDict(coerce_numbers_to_str=True)\n        value: str\n\n    assert Model.model_validate({'value': number}).model_dump() == {'value': expected_str}\n\n\n@pytest.mark.parametrize(\n    ('number', 'expected_str'),\n    [\n        pytest.param('42', '42', id='42'),\n        pytest.param('42.0', '42', id='42.0'),\n        pytest.param('42.13', '42.13', id='42.13'),\n    ],\n)\ndef test_coerce_numbers_to_str_from_json(number: str, expected_str: str) -> None:\n    class Model(BaseModel):\n        model_config = ConfigDict(coerce_numbers_to_str=True)\n        value: str\n\n    assert Model.model_validate_json(f'{{\"value\": {number}}}').model_dump() == {'value': expected_str}\n\n\ndef test_union_tags_in_errors():\n    DoubledList = Annotated[List[int], AfterValidator(lambda x: x * 2)]\n    StringsMap = Dict[str, str]\n\n    adapter = TypeAdapter(Union[DoubledList, StringsMap])\n\n    with pytest.raises(ValidationError) as exc_info:\n        adapter.validate_python(['a'])\n\n    # yuck\n    assert '2 validation errors for union[function-after[<lambda>(), list[int]],dict[str,str]]' in str(exc_info)\n    # the loc's are bad here:\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('function-after[<lambda>(), list[int]]', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n        {\n            'input': ['a'],\n            'loc': ('dict[str,str]',),\n            'msg': 'Input should be a valid dictionary',\n            'type': 'dict_type',\n        },\n    ]\n\n    tag_adapter = TypeAdapter(\n        Union[Annotated[DoubledList, Tag('DoubledList')], Annotated[StringsMap, Tag('StringsMap')]]\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        tag_adapter.validate_python(['a'])\n\n    assert '2 validation errors for union[DoubledList,StringsMap]' in str(exc_info)  # nice\n    # the loc's are good here:\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('DoubledList', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n        {\n            'input': ['a'],\n            'loc': ('StringsMap',),\n            'msg': 'Input should be a valid dictionary',\n            'type': 'dict_type',\n        },\n    ]\n\n\ndef test_json_value():\n    adapter = TypeAdapter(JsonValue)\n    valid_json_data = {'a': {'b': {'c': 1, 'd': [2, None]}}}\n    # would pass validation as a dict[str, Any]\n    invalid_json_data = {'a': {'b': ...}}\n\n    assert adapter.validate_python(valid_json_data) == valid_json_data\n    assert adapter.validate_json(json.dumps(valid_json_data)) == valid_json_data\n\n    with pytest.raises(ValidationError) as exc_info:\n        adapter.validate_python(invalid_json_data)\n    assert exc_info.value.errors() == [\n        {\n            'input': Ellipsis,\n            'loc': ('dict', 'a', 'dict', 'b'),\n            'msg': 'input was not a valid JSON value',\n            'type': 'invalid-json-value',\n        }\n    ]\n\n\ndef test_json_value_with_subclassed_types():\n    class IntType(int):\n        pass\n\n    class FloatType(float):\n        pass\n\n    class StrType(str):\n        pass\n\n    class ListType(list):\n        pass\n\n    class DictType(dict):\n        pass\n\n    adapter = TypeAdapter(JsonValue)\n    valid_json_data = {'int': IntType(), 'float': FloatType(), 'str': StrType(), 'list': ListType(), 'dict': DictType()}\n    assert adapter.validate_python(valid_json_data) == valid_json_data\n\n\ndef test_json_value_roundtrip() -> None:\n    # see https://github.com/pydantic/pydantic/issues/8175\n    class MyModel(BaseModel):\n        val: Union[str, JsonValue]\n\n    round_trip_value = json.loads(MyModel(val=True).model_dump_json())['val']\n    assert round_trip_value is True, round_trip_value\n\n\ndef test_on_error_omit() -> None:\n    OmittableInt = OnErrorOmit[int]\n\n    class MyTypedDict(TypedDict):\n        a: NotRequired[OmittableInt]\n        b: NotRequired[OmittableInt]\n\n    class Model(BaseModel):\n        a_list: List[OmittableInt]\n        a_dict: Dict[OmittableInt, OmittableInt]\n        a_typed_dict: MyTypedDict\n\n    actual = Model(\n        a_list=[1, 2, 'a', 3],\n        a_dict={1: 1, 2: 2, 'a': 'a', 'b': 0, 3: 'c', 4: 4},\n        a_typed_dict=MyTypedDict(a=1, b='xyz'),  # type: ignore\n    )\n\n    expected = Model(a_list=[1, 2, 3], a_dict={1: 1, 2: 2, 4: 4}, a_typed_dict=MyTypedDict(a=1))\n\n    assert actual == expected\n\n\ndef test_on_error_omit_top_level() -> None:\n    ta = TypeAdapter(OnErrorOmit[int])\n\n    assert ta.validate_python(1) == 1\n    assert ta.validate_python('1') == 1\n\n    # we might want to just raise the OmitError or convert it to a ValidationError\n    # if it hits the top level, but this documents the current behavior at least\n    with pytest.raises(SchemaError, match='Uncaught Omit error'):\n        ta.validate_python('a')\n\n\ndef test_diff_enums_diff_configs() -> None:\n    class MyEnum(str, Enum):\n        A = 'a'\n\n    class MyModel(BaseModel, use_enum_values=True):\n        my_enum: MyEnum\n\n    class OtherModel(BaseModel):\n        my_enum: MyEnum\n\n    class Model(BaseModel):\n        my_model: MyModel\n        other_model: OtherModel\n\n    obj = Model.model_validate({'my_model': {'my_enum': 'a'}, 'other_model': {'my_enum': 'a'}})\n    assert not isinstance(obj.my_model.my_enum, MyEnum)\n    assert isinstance(obj.other_model.my_enum, MyEnum)\n\n\ndef test_can_serialize_deque_passed_to_sequence() -> None:\n    ta = TypeAdapter(Sequence[int])\n    my_dec = ta.validate_python(deque([1, 2, 3]))\n    assert my_dec == deque([1, 2, 3])\n\n    assert ta.dump_python(my_dec) == my_dec\n    assert ta.dump_json(my_dec) == b'[1,2,3]'\n\n\ndef test_strict_enum_with_use_enum_values() -> None:\n    class SomeEnum(int, Enum):\n        SOME_KEY = 1\n\n    class Foo(BaseModel):\n        model_config = ConfigDict(strict=False, use_enum_values=True)\n        foo: Annotated[SomeEnum, Strict(strict=True)]\n\n    f = Foo(foo=SomeEnum.SOME_KEY)\n    assert f.foo == 1\n\n    # validation error raised bc foo field uses strict mode\n    with pytest.raises(ValidationError):\n        Foo(foo='1')\n\n\ndef test_python_re_respects_flags() -> None:\n    class Model(BaseModel):\n        a: Annotated[str, StringConstraints(pattern=re.compile(r'[A-Z]+', re.IGNORECASE))]\n\n        model_config = ConfigDict(regex_engine='python-re')\n\n    # allows lowercase letters, even though the pattern is uppercase only due to the IGNORECASE flag\n    assert Model(a='abc').a == 'abc'\n\n\ndef test_constraints_on_str_like() -> None:\n    \"\"\"See https://github.com/pydantic/pydantic/issues/8577 for motivation.\"\"\"\n\n    class Foo(BaseModel):\n        baz: Annotated[EmailStr, StringConstraints(to_lower=True, strip_whitespace=True)]\n\n    assert Foo(baz=' uSeR@ExAmPlE.com  ').baz == 'user@example.com'\n", "tests/test_root_model.py": "import pickle\nfrom datetime import date, datetime\nfrom typing import Any, Dict, Generic, List, Optional, Union\n\nimport pytest\nfrom pydantic_core import CoreSchema\nfrom pydantic_core.core_schema import SerializerFunctionWrapHandler\nfrom typing_extensions import Annotated, Literal, TypeVar\n\nfrom pydantic import (\n    Base64Str,\n    BaseModel,\n    ConfigDict,\n    Field,\n    PrivateAttr,\n    PydanticDeprecatedSince20,\n    PydanticUserError,\n    RootModel,\n    ValidationError,\n    field_serializer,\n    model_validator,\n)\n\n\ndef parametrize_root_model():\n    class InnerModel(BaseModel):\n        int_field: int\n        str_field: str\n\n    return pytest.mark.parametrize(\n        ('root_type', 'root_value', 'dump_value'),\n        [\n            pytest.param(int, 42, 42, id='int'),\n            pytest.param(str, 'forty two', 'forty two', id='str'),\n            pytest.param(Dict[int, bool], {1: True, 2: False}, {1: True, 2: False}, id='dict[int, bool]'),\n            pytest.param(List[int], [4, 2, -1], [4, 2, -1], id='list[int]'),\n            pytest.param(\n                InnerModel,\n                InnerModel(int_field=42, str_field='forty two'),\n                {'int_field': 42, 'str_field': 'forty two'},\n                id='InnerModel',\n            ),\n        ],\n    )\n\n\ndef check_schema(schema: CoreSchema) -> None:\n    # we assume the shape of the core schema here, which is not a guarantee\n    # pydantic makes to its users but is useful to check here to make sure\n    # we are doing the right thing internally\n    assert schema['type'] == 'model'\n    assert schema['root_model'] is True\n    assert schema['custom_init'] is False\n\n\n@parametrize_root_model()\ndef test_root_model_specialized(root_type, root_value, dump_value):\n    Model = RootModel[root_type]\n\n    check_schema(Model.__pydantic_core_schema__)\n\n    m = Model(root_value)\n\n    assert m.model_dump() == dump_value\n    assert dict(m) == {'root': m.root}\n    assert m.__pydantic_fields_set__ == {'root'}\n\n\n@parametrize_root_model()\ndef test_root_model_inherited(root_type, root_value, dump_value):\n    class Model(RootModel[root_type]):\n        pass\n\n    check_schema(Model.__pydantic_core_schema__)\n\n    m = Model(root_value)\n\n    assert m.model_dump() == dump_value\n    assert dict(m) == {'root': m.root}\n    assert m.__pydantic_fields_set__ == {'root'}\n\n\ndef test_root_model_validation_error():\n    Model = RootModel[int]\n\n    with pytest.raises(ValidationError) as e:\n        Model('forty two')\n\n    assert e.value.errors(include_url=False) == [\n        {\n            'input': 'forty two',\n            'loc': (),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n    ]\n\n\ndef test_root_model_repr():\n    SpecializedRootModel = RootModel[int]\n\n    class SubRootModel(RootModel):\n        pass\n\n    class SpecializedSubRootModel(RootModel[int]):\n        pass\n\n    assert repr(SpecializedRootModel(1)) == 'RootModel[int](root=1)'\n    assert repr(SubRootModel(1)) == 'SubRootModel(root=1)'\n    assert repr(SpecializedSubRootModel(1)) == 'SpecializedSubRootModel(root=1)'\n\n\ndef test_root_model_recursive():\n    class A(RootModel[List['B']]):\n        def my_a_method(self):\n            pass\n\n    class B(RootModel[Dict[str, Optional[A]]]):\n        def my_b_method(self):\n            pass\n\n    assert repr(A.model_validate([{}])) == 'A(root=[B(root={})])'\n\n\ndef test_root_model_nested():\n    calls = []\n\n    class B(RootModel[int]):\n        def my_b_method(self):\n            calls.append(('my_b_method', self.root))\n\n    class A(RootModel[B]):\n        def my_a_method(self):\n            calls.append(('my_a_method', self.root.root))\n\n    m1 = A.model_validate(1)\n    m1.my_a_method()\n    m1.root.my_b_method()\n    assert calls == [('my_a_method', 1), ('my_b_method', 1)]\n\n    calls.clear()\n    m2 = A.model_validate_json('2')\n    m2.my_a_method()\n    m2.root.my_b_method()\n    assert calls == [('my_a_method', 2), ('my_b_method', 2)]\n\n\ndef test_root_model_as_field():\n    class MyRootModel(RootModel[int]):\n        pass\n\n    class MyModel(BaseModel):\n        root_model: MyRootModel\n\n    m = MyModel.model_validate({'root_model': 1})\n\n    assert isinstance(m.root_model, MyRootModel)\n\n\ndef test_v1_compatibility_serializer():\n    class MyInnerModel(BaseModel):\n        x: int\n\n    class MyRootModel(RootModel[MyInnerModel]):\n        # The following field_serializer can be added to achieve the same behavior as v1 had for .dict()\n        @field_serializer('root', mode='wrap')\n        def embed_in_dict(self, v: Any, handler: SerializerFunctionWrapHandler):\n            return {'__root__': handler(v)}\n\n    class MyOuterModel(BaseModel):\n        my_root: MyRootModel\n\n    m = MyOuterModel.model_validate({'my_root': {'x': 1}})\n\n    assert m.model_dump() == {'my_root': {'__root__': {'x': 1}}}\n    with pytest.warns(PydanticDeprecatedSince20):\n        assert m.dict() == {'my_root': {'__root__': {'x': 1}}}\n\n\ndef test_construct():\n    class Base64Root(RootModel[Base64Str]):\n        pass\n\n    v = Base64Root.model_construct('test')\n    assert v.model_dump() == 'dGVzdA==\\n'\n\n\ndef test_construct_nested():\n    class Base64RootProperty(BaseModel):\n        data: RootModel[Base64Str]\n\n    v = Base64RootProperty.model_construct(data=RootModel[Base64Str].model_construct('test'))\n    assert v.model_dump() == {'data': 'dGVzdA==\\n'}\n\n    # Note: model_construct requires the inputs to be valid; the root model value does not get \"validated\" into\n    # an actual root model instance:\n    v = Base64RootProperty.model_construct(data='test')\n    assert isinstance(v.data, str)  # should be RootModel[Base64Str], but model_construct skipped validation\n    with pytest.raises(AttributeError, match=\"'str' object has no attribute 'root'\"):\n        v.model_dump()\n\n\ndef test_assignment():\n    Model = RootModel[int]\n\n    m = Model(1)\n    assert m.model_fields_set == {'root'}\n    assert m.root == 1\n    m.root = 2\n    assert m.root == 2\n\n\ndef test_model_validator_before():\n    class Model(RootModel[int]):\n        @model_validator(mode='before')\n        @classmethod\n        def words(cls, v):\n            if v == 'one':\n                return 1\n            elif v == 'two':\n                return 2\n            else:\n                return v\n\n    assert Model('one').root == 1\n    assert Model('two').root == 2\n    assert Model('3').root == 3\n    with pytest.raises(ValidationError) as exc_info:\n        Model('three')\n    # insert_assert(exc_info.value.errors())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'three',\n        }\n    ]\n\n\ndef test_model_validator_after():\n    class Model(RootModel[int]):\n        @model_validator(mode='after')\n        def double(self) -> 'Model':\n            self.root *= 2\n            return self\n\n    assert Model('1').root == 2\n    assert Model('21').root == 42\n\n\ndef test_private_attr():\n    class Model(RootModel[int]):\n        _private_attr: str\n        _private_attr_default: str = PrivateAttr(default='abc')\n\n    m = Model(42)\n\n    assert m.root == 42\n    assert m._private_attr_default == 'abc'\n    with pytest.raises(AttributeError, match='_private_attr'):\n        m._private_attr\n\n    m._private_attr = 7\n    m._private_attr_default = 8\n    m._other_private_attr = 9\n    # TODO: Should this be an `AttributeError`?\n    with pytest.raises(ValueError, match='other_attr'):\n        m.other_attr = 10\n\n    assert m._private_attr == 7\n    assert m._private_attr_default == 8\n    assert m._other_private_attr == 9\n    assert m.model_dump() == 42\n\n\ndef test_validate_assignment_false():\n    Model = RootModel[int]\n\n    m = Model(42)\n    m.root = 'abc'\n    assert m.root == 'abc'\n\n\ndef test_validate_assignment_true():\n    class Model(RootModel[int]):\n        model_config = ConfigDict(validate_assignment=True)\n\n    m = Model(42)\n\n    with pytest.raises(ValidationError) as e:\n        m.root = 'abc'\n\n    assert e.value.errors(include_url=False) == [\n        {\n            'input': 'abc',\n            'loc': (),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\ndef test_root_model_literal():\n    assert RootModel[int](42).root == 42\n\n\ndef test_root_model_equality():\n    assert RootModel[int](42) == RootModel[int](42)\n    assert RootModel[int](42) != RootModel[int](7)\n    assert RootModel[int](42) != RootModel[float](42)\n    assert RootModel[int](42) == RootModel[int].model_construct(42)\n\n\ndef test_root_model_with_private_attrs_equality():\n    class Model(RootModel[int]):\n        _private_attr: str = PrivateAttr(default='abc')\n\n    m = Model(42)\n    assert m == Model(42)\n\n    m._private_attr = 'xyz'\n    assert m != Model(42)\n\n\ndef test_root_model_nested_equality():\n    class Model(BaseModel):\n        value: RootModel[int]\n\n    assert Model(value=42).value == RootModel[int](42)\n\n\ndef test_root_model_base_model_equality():\n    class R(RootModel[int]):\n        pass\n\n    class B(BaseModel):\n        root: int\n\n    assert R(42) != B(root=42)\n    assert B(root=42) != R(42)\n\n\n@pytest.mark.parametrize('extra_value', ['ignore', 'allow', 'forbid'])\ndef test_extra_error(extra_value):\n    with pytest.raises(PydanticUserError, match='extra'):\n\n        class Model(RootModel[int]):\n            model_config = ConfigDict(extra=extra_value)\n\n\ndef test_root_model_default_value():\n    class Model(RootModel):\n        root: int = 42\n\n    m = Model()\n    assert m.root == 42\n    assert m.model_dump() == 42\n    assert m.__pydantic_fields_set__ == set()\n\n\ndef test_root_model_default_factory():\n    class Model(RootModel):\n        root: int = Field(default_factory=lambda: 42)\n\n    m = Model()\n    assert m.root == 42\n    assert m.model_dump() == 42\n    assert m.__pydantic_fields_set__ == set()\n\n\ndef test_root_model_wrong_default_value_without_validate_default():\n    class Model(RootModel):\n        root: int = '42'\n\n    assert Model().root == '42'\n\n\ndef test_root_model_default_value_with_validate_default():\n    class Model(RootModel):\n        model_config = ConfigDict(validate_default=True)\n\n        root: int = '42'\n\n    m = Model()\n    assert m.root == 42\n    assert m.model_dump() == 42\n    assert m.__pydantic_fields_set__ == set()\n\n\ndef test_root_model_default_value_with_validate_default_on_field():\n    class Model(RootModel):\n        root: Annotated[int, Field(validate_default=True, default='42')]\n\n    m = Model()\n    assert m.root == 42\n    assert m.model_dump() == 42\n    assert m.__pydantic_fields_set__ == set()\n\n\ndef test_root_model_as_attr_with_validate_default():\n    class Model(BaseModel):\n        model_config = ConfigDict(validate_default=True)\n\n        rooted_value: RootModel[int] = 42\n\n    m = Model()\n    assert m.rooted_value == RootModel[int](42)\n    assert m.model_dump() == {'rooted_value': 42}\n    assert m.rooted_value.__pydantic_fields_set__ == {'root'}\n\n\ndef test_root_model_in_root_model_default():\n    class Nested(RootModel):\n        root: int = 42\n\n    class Model(RootModel):\n        root: Nested = Nested()\n\n    m = Model()\n    assert m.root.root == 42\n    assert m.__pydantic_fields_set__ == set()\n    assert m.root.__pydantic_fields_set__ == set()\n\n\ndef test_nested_root_model_naive_default():\n    class Nested(RootModel):\n        root: int = 42\n\n    class Model(BaseModel):\n        value: Nested\n\n    m = Model(value=Nested())\n    assert m.value.root == 42\n    assert m.value.__pydantic_fields_set__ == set()\n\n\ndef test_nested_root_model_proper_default():\n    class Nested(RootModel):\n        root: int = 42\n\n    class Model(BaseModel):\n        value: Nested = Field(default_factory=Nested)\n\n    m = Model()\n    assert m.value.root == 42\n    assert m.value.__pydantic_fields_set__ == set()\n\n\ndef test_root_model_json_schema_meta():\n    ParametrizedModel = RootModel[int]\n\n    class SubclassedModel(RootModel):\n        \"\"\"Subclassed Model docstring\"\"\"\n\n        root: int\n\n    parametrized_json_schema = ParametrizedModel.model_json_schema()\n    subclassed_json_schema = SubclassedModel.model_json_schema()\n\n    assert parametrized_json_schema.get('title') == 'RootModel[int]'\n    assert parametrized_json_schema.get('description') is None\n    assert subclassed_json_schema.get('title') == 'SubclassedModel'\n    assert subclassed_json_schema.get('description') == 'Subclassed Model docstring'\n\n\n@pytest.mark.parametrize('order', ['BR', 'RB'])\ndef test_root_model_dump_with_base_model(order):\n    class BModel(BaseModel):\n        value: str\n\n    class RModel(RootModel):\n        root: int\n\n    if order == 'BR':\n\n        class Model(RootModel):\n            root: List[Union[BModel, RModel]]\n\n    elif order == 'RB':\n\n        class Model(RootModel):\n            root: List[Union[RModel, BModel]]\n\n    m = Model([1, 2, {'value': 'abc'}])\n\n    assert m.root == [RModel(1), RModel(2), BModel.model_construct(value='abc')]\n    assert m.model_dump() == [1, 2, {'value': 'abc'}]\n    assert m.model_dump_json() == '[1,2,{\"value\":\"abc\"}]'\n\n\n@pytest.mark.parametrize(\n    'data',\n    [\n        pytest.param({'kind': 'IModel', 'int_value': 42}, id='IModel'),\n        pytest.param({'kind': 'SModel', 'str_value': 'abc'}, id='SModel'),\n    ],\n)\ndef test_mixed_discriminated_union(data):\n    class IModel(BaseModel):\n        kind: Literal['IModel']\n        int_value: int\n\n    class RModel(RootModel):\n        root: IModel\n\n    class SModel(BaseModel):\n        kind: Literal['SModel']\n        str_value: str\n\n    class Model(RootModel):\n        root: Union[SModel, RModel] = Field(discriminator='kind')\n\n    assert Model(data).model_dump() == data\n    assert Model(**data).model_dump() == data\n\n\ndef test_list_rootmodel():\n    class A(BaseModel):\n        type: Literal['a']\n        a: str\n\n    class B(BaseModel):\n        type: Literal['b']\n        b: str\n\n    class D(RootModel[Annotated[Union[A, B], Field(discriminator='type')]]):\n        pass\n\n    LD = RootModel[List[D]]\n\n    obj = LD.model_validate([{'type': 'a', 'a': 'a'}, {'type': 'b', 'b': 'b'}])\n    assert obj.model_dump() == [{'type': 'a', 'a': 'a'}, {'type': 'b', 'b': 'b'}]\n\n\ndef test_root_and_data_error():\n    class BModel(BaseModel):\n        value: int\n        other_value: str\n\n    Model = RootModel[BModel]\n\n    with pytest.raises(\n        ValueError,\n        match='\"RootModel.__init__\" accepts either a single positional argument or arbitrary keyword arguments',\n    ):\n        Model({'value': 42}, other_value='abc')\n\n\ndef test_pickle_root_model(create_module):\n    @create_module\n    def module():\n        from pydantic import RootModel\n\n        class MyRootModel(RootModel[str]):\n            pass\n\n    MyRootModel = module.MyRootModel\n    assert MyRootModel(root='abc') == pickle.loads(pickle.dumps(MyRootModel(root='abc')))\n\n\ndef test_json_schema_extra_on_model():\n    class Model(RootModel):\n        model_config = ConfigDict(json_schema_extra={'schema key': 'schema value'})\n        root: str\n\n    assert Model.model_json_schema() == {\n        'schema key': 'schema value',\n        'title': 'Model',\n        'type': 'string',\n    }\n\n\ndef test_json_schema_extra_on_field():\n    class Model(RootModel):\n        root: str = Field(json_schema_extra={'schema key': 'schema value'})\n\n    assert Model.model_json_schema() == {\n        'schema key': 'schema value',\n        'title': 'Model',\n        'type': 'string',\n    }\n\n\ndef test_json_schema_extra_on_model_and_on_field():\n    class Model(RootModel):\n        model_config = ConfigDict(json_schema_extra={'schema key on model': 'schema value on model'})\n        root: str = Field(json_schema_extra={'schema key on field': 'schema value on field'})\n\n    with pytest.raises(ValueError, match=r'json_schema_extra.*?must not be set simultaneously'):\n        Model.model_json_schema()\n\n\ndef test_help(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nimport pydoc\n\nfrom pydantic import RootModel\n\n\nhelp_result_string = pydoc.render_doc(RootModel)\n\"\"\"\n    )\n    assert 'class RootModel' in module.help_result_string\n\n\ndef test_copy_preserves_equality():\n    model = RootModel()\n\n    copied = model.__copy__()\n    assert model == copied\n\n    deepcopied = model.__deepcopy__()\n    assert model == deepcopied\n\n\n@pytest.mark.parametrize(\n    'root_type,input_value,expected,raises_match,strict',\n    [\n        (bool, 'true', True, None, False),\n        (bool, 'true', True, None, True),\n        (bool, 'false', False, None, False),\n        (bool, 'e', ValidationError, 'type=bool_parsing', False),\n        (int, '1', 1, None, False),\n        (int, '1', 1, None, True),\n        (int, 'xxx', ValidationError, 'type=int_parsing', True),\n        (float, '1.1', 1.1, None, False),\n        (float, '1.10', 1.1, None, False),\n        (float, '1.1', 1.1, None, True),\n        (float, '1.10', 1.1, None, True),\n        (date, '2017-01-01', date(2017, 1, 1), None, False),\n        (date, '2017-01-01', date(2017, 1, 1), None, True),\n        (date, '2017-01-01T12:13:14.567', ValidationError, 'type=date_from_datetime_inexact', False),\n        (date, '2017-01-01T12:13:14.567', ValidationError, 'type=date_parsing', True),\n        (date, '2017-01-01T00:00:00', date(2017, 1, 1), None, False),\n        (date, '2017-01-01T00:00:00', ValidationError, 'type=date_parsing', True),\n        (datetime, '2017-01-01T12:13:14.567', datetime(2017, 1, 1, 12, 13, 14, 567_000), None, False),\n        (datetime, '2017-01-01T12:13:14.567', datetime(2017, 1, 1, 12, 13, 14, 567_000), None, True),\n    ],\n    ids=repr,\n)\ndef test_model_validate_strings(root_type, input_value, expected, raises_match, strict):\n    Model = RootModel[root_type]\n\n    if raises_match is not None:\n        with pytest.raises(expected, match=raises_match):\n            Model.model_validate_strings(input_value, strict=strict)\n    else:\n        assert Model.model_validate_strings(input_value, strict=strict).root == expected\n\n\ndef test_model_construction_with_invalid_generic_specification() -> None:\n    T_ = TypeVar('T_', bound=BaseModel)\n\n    with pytest.raises(TypeError, match='You should parametrize RootModel directly'):\n\n        class GenericRootModel(RootModel, Generic[T_]):\n            root: Union[T_, int]\n\n\ndef test_model_with_field_description() -> None:\n    class AModel(RootModel):\n        root: int = Field(description='abc')\n\n    assert AModel.model_json_schema() == {'title': 'AModel', 'type': 'integer', 'description': 'abc'}\n\n\ndef test_model_with_both_docstring_and_field_description() -> None:\n    \"\"\"Check if the docstring is used as the description when both are present.\"\"\"\n\n    class AModel(RootModel):\n        \"\"\"More detailed description\"\"\"\n\n        root: int = Field(description='abc')\n\n    assert AModel.model_json_schema() == {\n        'title': 'AModel',\n        'type': 'integer',\n        'description': 'More detailed description',\n    }\n", "tests/test_dataclasses.py": "import dataclasses\nimport inspect\nimport pickle\nimport re\nimport sys\nimport traceback\nfrom collections.abc import Hashable\nfrom dataclasses import InitVar\nfrom datetime import date, datetime\nfrom pathlib import Path\nfrom typing import Any, Callable, ClassVar, Dict, FrozenSet, Generic, List, Optional, Set, TypeVar, Union\n\nimport pytest\nfrom dirty_equals import HasRepr\nfrom pydantic_core import ArgsKwargs, CoreSchema, SchemaValidator, core_schema\nfrom typing_extensions import Annotated, Literal\n\nimport pydantic\nfrom pydantic import (\n    BaseModel,\n    BeforeValidator,\n    ConfigDict,\n    GenerateSchema,\n    PydanticDeprecatedSince20,\n    PydanticUndefinedAnnotation,\n    PydanticUserError,\n    RootModel,\n    TypeAdapter,\n    ValidationError,\n    ValidationInfo,\n    computed_field,\n    field_serializer,\n    field_validator,\n    model_validator,\n    with_config,\n)\nfrom pydantic._internal._mock_val_ser import MockValSer\nfrom pydantic.dataclasses import is_pydantic_dataclass, rebuild_dataclass\nfrom pydantic.fields import Field, FieldInfo\nfrom pydantic.json_schema import model_json_schema\n\n\ndef test_cannot_create_dataclass_from_basemodel_subclass():\n    msg = 'Cannot create a Pydantic dataclass from SubModel as it is already a Pydantic model'\n\n    with pytest.raises(PydanticUserError, match=msg):\n\n        @pydantic.dataclasses.dataclass\n        class SubModel(BaseModel):\n            pass\n\n\ndef test_simple():\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        a: int\n        b: float\n\n    d = MyDataclass('1', '2.5')\n    assert d.a == 1\n    assert d.b == 2.5\n    d = MyDataclass(b=10, a=20)\n    assert d.a == 20\n    assert d.b == 10\n\n\ndef test_model_name():\n    @pydantic.dataclasses.dataclass\n    class MyDataClass:\n        model_name: str\n\n    d = MyDataClass('foo')\n    assert d.model_name == 'foo'\n    d = MyDataClass(model_name='foo')\n    assert d.model_name == 'foo'\n\n\ndef test_value_error():\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        a: int\n        b: int\n\n    with pytest.raises(ValidationError) as exc_info:\n        MyDataclass(1, 'wrong')\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (1,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]\n\n\ndef test_frozen():\n    @pydantic.dataclasses.dataclass(frozen=True)\n    class MyDataclass:\n        a: int\n\n    d = MyDataclass(1)\n    assert d.a == 1\n\n    with pytest.raises(AttributeError):\n        d.a = 7\n\n\ndef test_validate_assignment():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(validate_assignment=True))\n    class MyDataclass:\n        a: int\n\n    d = MyDataclass(1)\n    assert d.a == 1\n\n    d.a = '7'\n    assert d.a == 7\n\n\ndef test_validate_assignment_error():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(validate_assignment=True))\n    class MyDataclass:\n        a: int\n\n    d = MyDataclass(1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        d.a = 'xxx'\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xxx',\n        }\n    ]\n\n\ndef test_not_validate_assignment():\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        a: int\n\n    d = MyDataclass(1)\n    assert d.a == 1\n\n    d.a = '7'\n    assert d.a == '7'\n\n\ndef test_validate_assignment_value_change():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(validate_assignment=True), frozen=False)\n    class MyDataclass:\n        a: int\n\n        @field_validator('a')\n        @classmethod\n        def double_a(cls, v: int) -> int:\n            return v * 2\n\n    d = MyDataclass(2)\n    assert d.a == 4\n\n    d.a = 3\n    assert d.a == 6\n\n\n@pytest.mark.parametrize(\n    'config',\n    [\n        ConfigDict(validate_assignment=False),\n        ConfigDict(extra=None),\n        ConfigDict(extra='forbid'),\n        ConfigDict(extra='ignore'),\n        ConfigDict(validate_assignment=False, extra=None),\n        ConfigDict(validate_assignment=False, extra='forbid'),\n        ConfigDict(validate_assignment=False, extra='ignore'),\n        ConfigDict(validate_assignment=False, extra='allow'),\n        ConfigDict(validate_assignment=True, extra='allow'),\n    ],\n)\ndef test_validate_assignment_extra_unknown_field_assigned_allowed(config: ConfigDict):\n    @pydantic.dataclasses.dataclass(config=config)\n    class MyDataclass:\n        a: int\n\n    d = MyDataclass(1)\n    assert d.a == 1\n\n    d.extra_field = 123\n    assert d.extra_field == 123\n\n\n@pytest.mark.parametrize(\n    'config',\n    [\n        ConfigDict(validate_assignment=True),\n        ConfigDict(validate_assignment=True, extra=None),\n        ConfigDict(validate_assignment=True, extra='forbid'),\n        ConfigDict(validate_assignment=True, extra='ignore'),\n    ],\n)\ndef test_validate_assignment_extra_unknown_field_assigned_errors(config: ConfigDict):\n    @pydantic.dataclasses.dataclass(config=config)\n    class MyDataclass:\n        a: int\n\n    d = MyDataclass(1)\n    assert d.a == 1\n\n    with pytest.raises(ValidationError) as exc_info:\n        d.extra_field = 1.23\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('extra_field',),\n            'msg': \"Object has no attribute 'extra_field'\",\n            'input': 1.23,\n            'ctx': {'attribute': 'extra_field'},\n        }\n    ]\n\n\ndef test_post_init():\n    post_init_called = False\n\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        a: int\n\n        def __post_init__(self):\n            nonlocal post_init_called\n            post_init_called = True\n\n    d = MyDataclass('1')\n    assert d.a == 1\n    assert post_init_called\n\n\ndef test_post_init_validation():\n    @dataclasses.dataclass\n    class DC:\n        a: int\n\n        def __post_init__(self):\n            self.a *= 2\n\n    assert DC(a='2').a == '22'\n    PydanticDC = pydantic.dataclasses.dataclass(DC)\n    assert DC(a='2').a == '22'\n    assert PydanticDC(a='2').a == 4\n\n\ndef test_convert_vanilla_dc():\n    @dataclasses.dataclass\n    class DC:\n        a: int\n        b: str = dataclasses.field(init=False)\n\n        def __post_init__(self):\n            self.a *= 2\n            self.b = 'hello'\n\n    dc1 = DC(a='2')\n    assert dc1.a == '22'\n    assert dc1.b == 'hello'\n    PydanticDC = pydantic.dataclasses.dataclass(DC)\n    dc2 = DC(a='2')\n    assert dc2.a == '22'\n    assert dc2.b == 'hello'\n\n    py_dc = PydanticDC(a='2')\n    assert py_dc.a == 4\n    assert py_dc.b == 'hello'\n\n\ndef test_std_dataclass_with_parent():\n    @dataclasses.dataclass\n    class DCParent:\n        a: int\n\n    @dataclasses.dataclass\n    class DC(DCParent):\n        b: int\n\n        def __post_init__(self):\n            self.a *= 2\n\n    assert dataclasses.asdict(DC(a='2', b='1')) == {'a': '22', 'b': '1'}\n    PydanticDC = pydantic.dataclasses.dataclass(DC)\n    assert dataclasses.asdict(DC(a='2', b='1')) == {'a': '22', 'b': '1'}\n    assert dataclasses.asdict(PydanticDC(a='2', b='1')) == {'a': 4, 'b': 1}\n\n\ndef test_post_init_inheritance_chain():\n    parent_post_init_called = False\n    post_init_called = False\n\n    @pydantic.dataclasses.dataclass\n    class ParentDataclass:\n        a: int\n\n        def __post_init__(self):\n            nonlocal parent_post_init_called\n            parent_post_init_called = True\n\n    @pydantic.dataclasses.dataclass\n    class MyDataclass(ParentDataclass):\n        b: int\n\n        def __post_init__(self):\n            super().__post_init__()\n            nonlocal post_init_called\n            post_init_called = True\n\n    d = MyDataclass(a=1, b=2)\n    assert d.a == 1\n    assert d.b == 2\n    assert parent_post_init_called\n    assert post_init_called\n\n\ndef test_post_init_post_parse():\n    with pytest.warns(PydanticDeprecatedSince20, match='Support for `__post_init_post_parse__` has been dropped'):\n\n        @pydantic.dataclasses.dataclass\n        class MyDataclass:\n            a: int\n\n            def __post_init_post_parse__(self):\n                pass\n\n\ndef test_post_init_assignment():\n    from dataclasses import field\n\n    # Based on: https://docs.python.org/3/library/dataclasses.html#post-init-processing\n    @pydantic.dataclasses.dataclass\n    class C:\n        a: float\n        b: float\n        c: float = field(init=False)\n\n        def __post_init__(self):\n            self.c = self.a + self.b\n\n    c = C(0.1, 0.2)\n    assert c.a == 0.1\n    assert c.b == 0.2\n    assert c.c == 0.30000000000000004\n\n\ndef test_inheritance():\n    @pydantic.dataclasses.dataclass\n    class A:\n        a: str = None\n\n    a_ = A(a=b'a')\n    assert a_.a == 'a'\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        b: int = None\n\n    b = B(a='a', b=12)\n    assert b.a == 'a'\n    assert b.b == 12\n\n    with pytest.raises(ValidationError):\n        B(a='a', b='b')\n\n    a_ = A(a=b'a')\n    assert a_.a == 'a'\n\n\ndef test_validate_long_string_error():\n    @pydantic.dataclasses.dataclass(config=dict(str_max_length=3))\n    class MyDataclass:\n        a: str\n\n    with pytest.raises(ValidationError) as exc_info:\n        MyDataclass('xxxx')\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_too_long',\n            'loc': (0,),\n            'msg': 'String should have at most 3 characters',\n            'input': 'xxxx',\n            'ctx': {'max_length': 3},\n        }\n    ]\n\n\ndef test_validate_assignment_long_string_error():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(str_max_length=3, validate_assignment=True))\n    class MyDataclass:\n        a: str\n\n    d = MyDataclass('xxx')\n    with pytest.raises(ValidationError) as exc_info:\n        d.a = 'xxxx'\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_too_long',\n            'loc': ('a',),\n            'msg': 'String should have at most 3 characters',\n            'input': 'xxxx',\n            'ctx': {'max_length': 3},\n        }\n    ]\n\n\ndef test_no_validate_assignment_long_string_error():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(str_max_length=3, validate_assignment=False))\n    class MyDataclass:\n        a: str\n\n    d = MyDataclass('xxx')\n    d.a = 'xxxx'\n\n    assert d.a == 'xxxx'\n\n\ndef test_nested_dataclass():\n    @pydantic.dataclasses.dataclass\n    class Nested:\n        number: int\n\n    @pydantic.dataclasses.dataclass\n    class Outer:\n        n: Nested\n\n    navbar = Outer(n=Nested(number='1'))\n    assert isinstance(navbar.n, Nested)\n    assert navbar.n.number == 1\n\n    navbar = Outer(n={'number': '3'})\n    assert isinstance(navbar.n, Nested)\n    assert navbar.n.number == 3\n\n    with pytest.raises(ValidationError) as exc_info:\n        Outer(n='not nested')\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_type',\n            'loc': ('n',),\n            'msg': 'Input should be a dictionary or an instance of Nested',\n            'input': 'not nested',\n            'ctx': {'class_name': 'Nested'},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Outer(n={'number': 'x'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('n', 'number'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'x',\n        }\n    ]\n\n\ndef test_arbitrary_types_allowed():\n    class Button:\n        def __init__(self, href: str):\n            self.href = href\n\n    @pydantic.dataclasses.dataclass(config=dict(arbitrary_types_allowed=True))\n    class Navbar:\n        button: Button\n\n    btn = Button(href='a')\n    navbar = Navbar(button=btn)\n    assert navbar.button.href == 'a'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Navbar(button=('b',))\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': ('button',),\n            'msg': 'Input should be an instance of test_arbitrary_types_allowed.<locals>.Button',\n            'input': ('b',),\n            'ctx': {'class': 'test_arbitrary_types_allowed.<locals>.Button'},\n        }\n    ]\n\n\ndef test_nested_dataclass_model():\n    @pydantic.dataclasses.dataclass\n    class Nested:\n        number: int\n\n    class Outer(BaseModel):\n        n: Nested\n\n    navbar = Outer(n=Nested(number='1'))\n    assert navbar.n.number == 1\n\n\ndef test_fields():\n    @pydantic.dataclasses.dataclass\n    class User:\n        id: int\n        name: str = 'John Doe'\n        signup_ts: datetime = None\n\n    user = User(id=123)\n    fields = user.__pydantic_fields__\n\n    assert fields['id'].is_required() is True\n\n    assert fields['name'].is_required() is False\n    assert fields['name'].default == 'John Doe'\n\n    assert fields['signup_ts'].is_required() is False\n    assert fields['signup_ts'].default is None\n\n\n@pytest.mark.parametrize('field_constructor', [dataclasses.field, pydantic.dataclasses.Field])\ndef test_default_factory_field(field_constructor: Callable):\n    @pydantic.dataclasses.dataclass\n    class User:\n        id: int\n        other: Dict[str, str] = field_constructor(default_factory=lambda: {'John': 'Joey'})\n\n    user = User(id=123)\n\n    assert user.id == 123\n    assert user.other == {'John': 'Joey'}\n    fields = user.__pydantic_fields__\n\n    assert fields['id'].is_required() is True\n    assert repr(fields['id'].default) == 'PydanticUndefined'\n\n    assert fields['other'].is_required() is False\n    assert fields['other'].default_factory() == {'John': 'Joey'}\n\n\ndef test_default_factory_singleton_field():\n    class MySingleton:\n        pass\n\n    MY_SINGLETON = MySingleton()\n\n    @pydantic.dataclasses.dataclass(config=dict(arbitrary_types_allowed=True))\n    class Foo:\n        singleton: MySingleton = dataclasses.field(default_factory=lambda: MY_SINGLETON)\n\n    # Returning a singleton from a default_factory is supported\n    assert Foo().singleton is Foo().singleton\n\n\ndef test_schema():\n    @pydantic.dataclasses.dataclass\n    class User:\n        id: int\n        name: str = 'John Doe'\n        aliases: Dict[str, str] = dataclasses.field(default_factory=lambda: {'John': 'Joey'})\n        signup_ts: datetime = None\n        age: Optional[int] = dataclasses.field(\n            default=None, metadata=dict(title='The age of the user', description='do not lie!')\n        )\n        height: Optional[int] = pydantic.Field(None, title='The height in cm', ge=50, le=300)\n\n    User(id=123)\n    assert model_json_schema(User) == {\n        'properties': {\n            'age': {\n                'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n                'default': None,\n                'title': 'The age of the user',\n                'description': 'do not lie!',\n            },\n            'aliases': {\n                'additionalProperties': {'type': 'string'},\n                'title': 'Aliases',\n                'type': 'object',\n            },\n            'height': {\n                'anyOf': [{'maximum': 300, 'minimum': 50, 'type': 'integer'}, {'type': 'null'}],\n                'default': None,\n                'title': 'The height in cm',\n            },\n            'id': {'title': 'Id', 'type': 'integer'},\n            'name': {'default': 'John Doe', 'title': 'Name', 'type': 'string'},\n            'signup_ts': {'default': None, 'format': 'date-time', 'title': 'Signup Ts', 'type': 'string'},\n        },\n        'required': ['id'],\n        'title': 'User',\n        'type': 'object',\n    }\n\n\ndef test_nested_schema():\n    @pydantic.dataclasses.dataclass\n    class Nested:\n        number: int\n\n    @pydantic.dataclasses.dataclass\n    class Outer:\n        n: Nested\n\n    assert model_json_schema(Outer) == {\n        '$defs': {\n            'Nested': {\n                'properties': {'number': {'title': 'Number', 'type': 'integer'}},\n                'required': ['number'],\n                'title': 'Nested',\n                'type': 'object',\n            }\n        },\n        'properties': {'n': {'$ref': '#/$defs/Nested'}},\n        'required': ['n'],\n        'title': 'Outer',\n        'type': 'object',\n    }\n\n\ndef test_initvar():\n    @pydantic.dataclasses.dataclass\n    class TestInitVar:\n        x: int\n        y: dataclasses.InitVar\n\n    tiv = TestInitVar(1, 2)\n    assert tiv.x == 1\n    with pytest.raises(AttributeError):\n        tiv.y\n\n\ndef test_derived_field_from_initvar():\n    @pydantic.dataclasses.dataclass\n    class DerivedWithInitVar:\n        plusone: int = dataclasses.field(init=False)\n        number: dataclasses.InitVar[int]\n\n        def __post_init__(self, number):\n            self.plusone = number + 1\n\n    derived = DerivedWithInitVar('1')\n    assert derived.plusone == 2\n    with pytest.raises(ValidationError, match='Input should be a valid integer, unable to parse string as an integer'):\n        DerivedWithInitVar('Not A Number')\n\n\ndef test_initvars_post_init():\n    @pydantic.dataclasses.dataclass\n    class PathDataPostInit:\n        path: Path\n        base_path: dataclasses.InitVar[Optional[Path]] = None\n\n        def __post_init__(self, base_path):\n            if base_path is not None:\n                self.path = base_path / self.path\n\n    path_data = PathDataPostInit('world')\n    assert 'path' in path_data.__dict__\n    assert 'base_path' not in path_data.__dict__\n    assert path_data.path == Path('world')\n\n    p = PathDataPostInit('world', base_path='/hello')\n    assert p.path == Path('/hello/world')\n\n\ndef test_classvar():\n    @pydantic.dataclasses.dataclass\n    class TestClassVar:\n        klassvar: ClassVar = \"I'm a Class variable\"\n        x: int\n\n    tcv = TestClassVar(2)\n    assert tcv.klassvar == \"I'm a Class variable\"\n\n\ndef test_frozenset_field():\n    @pydantic.dataclasses.dataclass\n    class TestFrozenSet:\n        set: FrozenSet[int]\n\n    test_set = frozenset({1, 2, 3})\n    object_under_test = TestFrozenSet(set=test_set)\n\n    assert object_under_test.set == test_set\n\n\ndef test_inheritance_post_init():\n    post_init_called = False\n\n    @pydantic.dataclasses.dataclass\n    class Base:\n        a: int\n\n        def __post_init__(self):\n            nonlocal post_init_called\n            post_init_called = True\n\n    @pydantic.dataclasses.dataclass\n    class Child(Base):\n        b: int\n\n    Child(a=1, b=2)\n    assert post_init_called\n\n\ndef test_hashable_required():\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        v: Hashable\n\n    MyDataclass(v=None)\n    with pytest.raises(ValidationError) as exc_info:\n        MyDataclass(v=[])\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': [], 'loc': ('v',), 'msg': 'Input should be hashable', 'type': 'is_hashable'}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        # Should this raise a TypeError instead? https://github.com/pydantic/pydantic/issues/5487\n        MyDataclass()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': HasRepr('ArgsKwargs(())'), 'loc': ('v',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\n@pytest.mark.parametrize('default', [1, None])\ndef test_default_value(default):\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        v: int = default\n\n    assert dataclasses.asdict(MyDataclass()) == {'v': default}\n    assert dataclasses.asdict(MyDataclass(v=42)) == {'v': 42}\n\n\ndef test_default_value_ellipsis():\n    \"\"\"\n    https://github.com/pydantic/pydantic/issues/5488\n    \"\"\"\n\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        v: int = ...\n\n    assert dataclasses.asdict(MyDataclass(v=42)) == {'v': 42}\n    with pytest.raises(ValidationError, match='type=missing'):\n        MyDataclass()\n\n\ndef test_override_builtin_dataclass():\n    @dataclasses.dataclass\n    class File:\n        hash: str\n        name: Optional[str]\n        size: int\n        content: Optional[bytes] = None\n\n    ValidFile = pydantic.dataclasses.dataclass(File)\n\n    file = File(hash='xxx', name=b'whatever.txt', size='456')\n    valid_file = ValidFile(hash='xxx', name=b'whatever.txt', size='456')\n\n    assert file.name == b'whatever.txt'\n    assert file.size == '456'\n\n    assert valid_file.name == 'whatever.txt'\n    assert valid_file.size == 456\n\n    assert isinstance(valid_file, File)\n    assert isinstance(valid_file, ValidFile)\n\n    with pytest.raises(ValidationError) as e:\n        ValidFile(hash=[1], name='name', size=3)\n\n    assert e.value.errors(include_url=False) == [\n        {\n            'type': 'string_type',\n            'loc': ('hash',),\n            'msg': 'Input should be a valid string',\n            'input': [1],\n        },\n    ]\n\n\ndef test_override_builtin_dataclass_2():\n    @dataclasses.dataclass\n    class Meta:\n        modified_date: Optional[datetime]\n        seen_count: int\n\n    Meta(modified_date='not-validated', seen_count=0)\n\n    @pydantic.dataclasses.dataclass\n    @dataclasses.dataclass\n    class File(Meta):\n        filename: str\n\n    Meta(modified_date='still-not-validated', seen_count=0)\n\n    f = File(filename=b'thefilename', modified_date='2020-01-01T00:00', seen_count='7')\n    assert f.filename == 'thefilename'\n    assert f.modified_date == datetime(2020, 1, 1, 0, 0)\n    assert f.seen_count == 7\n\n\ndef test_override_builtin_dataclass_nested():\n    @dataclasses.dataclass\n    class Meta:\n        modified_date: Optional[datetime]\n        seen_count: int\n\n    @dataclasses.dataclass\n    class File:\n        filename: str\n        meta: Meta\n\n    FileChecked = pydantic.dataclasses.dataclass(File, config=ConfigDict(revalidate_instances='always'))\n    f = FileChecked(filename=b'thefilename', meta=Meta(modified_date='2020-01-01T00:00', seen_count='7'))\n    assert f.filename == 'thefilename'\n    assert f.meta.modified_date == datetime(2020, 1, 1, 0, 0)\n    assert f.meta.seen_count == 7\n\n    with pytest.raises(ValidationError) as e:\n        FileChecked(filename=b'thefilename', meta=Meta(modified_date='2020-01-01T00:00', seen_count=['7']))\n    # insert_assert(e.value.errors(include_url=False))\n    assert e.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('meta', 'seen_count'), 'msg': 'Input should be a valid integer', 'input': ['7']}\n    ]\n\n    class Foo(\n        BaseModel,\n    ):\n        file: File\n\n    foo = Foo.model_validate(\n        {\n            'file': {\n                'filename': b'thefilename',\n                'meta': {'modified_date': '2020-01-01T00:00', 'seen_count': '7'},\n            },\n        }\n    )\n    assert foo.file.filename == 'thefilename'\n    assert foo.file.meta.modified_date == datetime(2020, 1, 1, 0, 0)\n    assert foo.file.meta.seen_count == 7\n\n\ndef test_override_builtin_dataclass_nested_schema():\n    @dataclasses.dataclass\n    class Meta:\n        modified_date: Optional[datetime]\n        seen_count: int\n\n    @dataclasses.dataclass\n    class File:\n        filename: str\n        meta: Meta\n\n    FileChecked = pydantic.dataclasses.dataclass(File)\n    assert model_json_schema(FileChecked) == {\n        '$defs': {\n            'Meta': {\n                'properties': {\n                    'modified_date': {\n                        'anyOf': [{'format': 'date-time', 'type': 'string'}, {'type': 'null'}],\n                        'title': 'Modified Date',\n                    },\n                    'seen_count': {'title': 'Seen Count', 'type': 'integer'},\n                },\n                'required': ['modified_date', 'seen_count'],\n                'title': 'Meta',\n                'type': 'object',\n            }\n        },\n        'properties': {'filename': {'title': 'Filename', 'type': 'string'}, 'meta': {'$ref': '#/$defs/Meta'}},\n        'required': ['filename', 'meta'],\n        'title': 'File',\n        'type': 'object',\n    }\n\n\ndef test_inherit_builtin_dataclass():\n    @dataclasses.dataclass\n    class Z:\n        z: int\n\n    @dataclasses.dataclass\n    class Y(Z):\n        y: int\n\n    @pydantic.dataclasses.dataclass\n    class X(Y):\n        x: int\n\n    pika = X(x='2', y='4', z='3')\n    assert pika.x == 2\n    assert pika.y == 4\n    assert pika.z == 3\n\n\ndef test_forward_stdlib_dataclass_params():\n    @dataclasses.dataclass(frozen=True)\n    class Item:\n        name: str\n\n    class Example(BaseModel):\n        item: Item\n        other: str\n\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    e = Example(item=Item(name='pika'), other='bulbi')\n    e.other = 'bulbi2'\n    with pytest.raises(dataclasses.FrozenInstanceError):\n        e.item.name = 'pika2'\n\n\ndef test_pydantic_callable_field():\n    \"\"\"pydantic callable fields behaviour should be the same as stdlib dataclass\"\"\"\n\n    def foo(arg1, arg2):\n        return arg1, arg2\n\n    def bar(x: int, y: float, z: str) -> bool:\n        return str(x + y) == z\n\n    class PydanticModel(BaseModel):\n        required_callable: Callable\n        required_callable_2: Callable[[int, float, str], bool]\n\n        default_callable: Callable = foo\n        default_callable_2: Callable[[int, float, str], bool] = bar\n\n    @pydantic.dataclasses.dataclass\n    class PydanticDataclass:\n        required_callable: Callable\n        required_callable_2: Callable[[int, float, str], bool]\n\n        default_callable: Callable = foo\n        default_callable_2: Callable[[int, float, str], bool] = bar\n\n    @dataclasses.dataclass\n    class StdlibDataclass:\n        required_callable: Callable\n        required_callable_2: Callable[[int, float, str], bool]\n\n        default_callable: Callable = foo\n        default_callable_2: Callable[[int, float, str], bool] = bar\n\n    pyd_m = PydanticModel(required_callable=foo, required_callable_2=bar)\n    pyd_dc = PydanticDataclass(required_callable=foo, required_callable_2=bar)\n    std_dc = StdlibDataclass(required_callable=foo, required_callable_2=bar)\n\n    assert (\n        pyd_m.required_callable\n        is pyd_m.default_callable\n        is pyd_dc.required_callable\n        is pyd_dc.default_callable\n        is std_dc.required_callable\n        is std_dc.default_callable\n    )\n    assert (\n        pyd_m.required_callable_2\n        is pyd_m.default_callable_2\n        is pyd_dc.required_callable_2\n        is pyd_dc.default_callable_2\n        is std_dc.required_callable_2\n        is std_dc.default_callable_2\n    )\n\n\ndef test_pickle_overridden_builtin_dataclass(create_module: Any):\n    module = create_module(\n        # language=Python\n        \"\"\"\\\nimport dataclasses\nimport pydantic\n\n\n@pydantic.dataclasses.dataclass(config=pydantic.config.ConfigDict(validate_assignment=True))\nclass BuiltInDataclassForPickle:\n    value: int\n        \"\"\"\n    )\n    obj = module.BuiltInDataclassForPickle(value=5)\n\n    pickled_obj = pickle.dumps(obj)\n    restored_obj = pickle.loads(pickled_obj)\n\n    assert restored_obj.value == 5\n    assert restored_obj == obj\n\n    # ensure the restored dataclass is still a pydantic dataclass\n    with pytest.raises(ValidationError):\n        restored_obj.value = 'value of a wrong type'\n\n\ndef lazy_cases_for_dataclass_equality_checks():\n    \"\"\"\n    The reason for the convoluted structure of this function is to avoid\n    creating the classes while collecting tests, which may trigger breakpoints\n    etc. while working on one specific test.\n    \"\"\"\n    cases = []\n\n    def get_cases():\n        if cases:\n            return cases  # cases already \"built\"\n\n        @dataclasses.dataclass(frozen=True)\n        class StdLibFoo:\n            a: str\n            b: int\n\n        @pydantic.dataclasses.dataclass(frozen=True)\n        class PydanticFoo:\n            a: str\n            b: int\n\n        @dataclasses.dataclass(frozen=True)\n        class StdLibBar:\n            c: StdLibFoo\n\n        @pydantic.dataclasses.dataclass(frozen=True)\n        class PydanticBar:\n            c: PydanticFoo\n\n        @dataclasses.dataclass(frozen=True)\n        class StdLibBaz:\n            c: PydanticFoo\n\n        @pydantic.dataclasses.dataclass(frozen=True)\n        class PydanticBaz:\n            c: StdLibFoo\n\n        foo = StdLibFoo(a='Foo', b=1)\n        cases.append((foo, StdLibBar(c=foo)))\n\n        foo = PydanticFoo(a='Foo', b=1)\n        cases.append((foo, PydanticBar(c=foo)))\n\n        foo = PydanticFoo(a='Foo', b=1)\n        cases.append((foo, StdLibBaz(c=foo)))\n\n        foo = StdLibFoo(a='Foo', b=1)\n        cases.append((foo, PydanticBaz(c=foo)))\n\n        return cases\n\n    case_ids = ['stdlib_stdlib', 'pydantic_pydantic', 'pydantic_stdlib', 'stdlib_pydantic']\n\n    def case(i):\n        def get_foo_bar():\n            return get_cases()[i]\n\n        get_foo_bar.__name__ = case_ids[i]  # get nice names in pytest output\n        return get_foo_bar\n\n    return [case(i) for i in range(4)]\n\n\n@pytest.mark.parametrize('foo_bar_getter', lazy_cases_for_dataclass_equality_checks())\ndef test_dataclass_equality_for_field_values(foo_bar_getter):\n    # Related to issue #2162\n    foo, bar = foo_bar_getter()\n    assert dataclasses.asdict(foo) == dataclasses.asdict(bar.c)\n    assert dataclasses.astuple(foo) == dataclasses.astuple(bar.c)\n    assert foo == bar.c\n\n\ndef test_issue_2383():\n    @dataclasses.dataclass\n    class A:\n        s: str\n\n        def __hash__(self):\n            return 123\n\n    class B(pydantic.BaseModel):\n        a: A\n\n    a = A('')\n    b = B(a=a)\n\n    assert hash(a) == 123\n    assert hash(b.a) == 123\n\n\ndef test_issue_2398():\n    @dataclasses.dataclass(order=True)\n    class DC:\n        num: int = 42\n\n    class Model(pydantic.BaseModel):\n        dc: DC\n\n    real_dc = DC()\n    model = Model(dc=real_dc)\n\n    # This works as expected.\n    assert real_dc <= real_dc\n    assert model.dc <= model.dc\n    assert real_dc <= model.dc\n\n\ndef test_issue_2424():\n    @dataclasses.dataclass\n    class Base:\n        x: str\n\n    @dataclasses.dataclass\n    class Thing(Base):\n        y: str = dataclasses.field(default_factory=str)\n\n    assert Thing(x='hi').y == ''\n\n    @pydantic.dataclasses.dataclass\n    class ValidatedThing(Base):\n        y: str = dataclasses.field(default_factory=str)\n\n    assert Thing(x='hi').y == ''\n    assert ValidatedThing(x='hi').y == ''\n\n\ndef test_issue_2541():\n    @dataclasses.dataclass(frozen=True)\n    class Infos:\n        id: int\n\n    @dataclasses.dataclass(frozen=True)\n    class Item:\n        name: str\n        infos: Infos\n\n    class Example(BaseModel):\n        item: Item\n\n    e = Example.model_validate({'item': {'name': '123', 'infos': {'id': '1'}}})\n    assert e.item.name == '123'\n    assert e.item.infos.id == 1\n    with pytest.raises(dataclasses.FrozenInstanceError):\n        e.item.infos.id = 2\n\n\ndef test_complex_nested_vanilla_dataclass():\n    @dataclasses.dataclass\n    class Span:\n        first: int\n        last: int\n\n    @dataclasses.dataclass\n    class LabeledSpan(Span):\n        label: str\n\n    @dataclasses.dataclass\n    class BinaryRelation:\n        subject: LabeledSpan\n        object: LabeledSpan\n        label: str\n\n    @dataclasses.dataclass\n    class Sentence:\n        relations: BinaryRelation\n\n    class M(pydantic.BaseModel):\n        s: Sentence\n\n    assert M.model_json_schema() == {\n        '$defs': {\n            'BinaryRelation': {\n                'properties': {\n                    'label': {'title': 'Label', 'type': 'string'},\n                    'object': {'$ref': '#/$defs/LabeledSpan'},\n                    'subject': {'$ref': '#/$defs/LabeledSpan'},\n                },\n                'required': ['subject', 'object', 'label'],\n                'title': 'BinaryRelation',\n                'type': 'object',\n            },\n            'LabeledSpan': {\n                'properties': {\n                    'first': {'title': 'First', 'type': 'integer'},\n                    'label': {'title': 'Label', 'type': 'string'},\n                    'last': {'title': 'Last', 'type': 'integer'},\n                },\n                'required': ['first', 'last', 'label'],\n                'title': 'LabeledSpan',\n                'type': 'object',\n            },\n            'Sentence': {\n                'properties': {'relations': {'$ref': '#/$defs/BinaryRelation'}},\n                'required': ['relations'],\n                'title': 'Sentence',\n                'type': 'object',\n            },\n        },\n        'properties': {'s': {'$ref': '#/$defs/Sentence'}},\n        'required': ['s'],\n        'title': 'M',\n        'type': 'object',\n    }\n\n\ndef test_json_schema_with_computed_field():\n    @dataclasses.dataclass\n    class MyDataclass:\n        x: int\n\n        @computed_field\n        @property\n        def double_x(self) -> int:\n            return 2 * self.x\n\n    class Model(BaseModel):\n        dc: MyDataclass\n\n    assert Model.model_json_schema(mode='validation') == {\n        '$defs': {\n            'MyDataclass': {\n                'properties': {'x': {'title': 'X', 'type': 'integer'}},\n                'required': ['x'],\n                'title': 'MyDataclass',\n                'type': 'object',\n            }\n        },\n        'properties': {'dc': {'$ref': '#/$defs/MyDataclass'}},\n        'required': ['dc'],\n        'title': 'Model',\n        'type': 'object',\n    }\n    assert Model.model_json_schema(mode='serialization') == {\n        '$defs': {\n            'MyDataclass': {\n                'properties': {\n                    'double_x': {'readOnly': True, 'title': 'Double X', 'type': 'integer'},\n                    'x': {'title': 'X', 'type': 'integer'},\n                },\n                'required': ['x', 'double_x'],\n                'title': 'MyDataclass',\n                'type': 'object',\n            }\n        },\n        'properties': {'dc': {'$ref': '#/$defs/MyDataclass'}},\n        'required': ['dc'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_issue_2594():\n    @dataclasses.dataclass\n    class Empty:\n        pass\n\n    @pydantic.dataclasses.dataclass\n    class M:\n        e: Empty\n\n    assert isinstance(M(e={}).e, Empty)\n\n\ndef test_schema_description_unset():\n    @pydantic.dataclasses.dataclass\n    class A:\n        x: int\n\n    assert 'description' not in model_json_schema(A)\n\n    @pydantic.dataclasses.dataclass\n    @dataclasses.dataclass\n    class B:\n        x: int\n\n    assert 'description' not in model_json_schema(B)\n\n\ndef test_schema_description_set():\n    @pydantic.dataclasses.dataclass\n    class A:\n        \"\"\"my description\"\"\"\n\n        x: int\n\n    assert model_json_schema(A)['description'] == 'my description'\n\n    @pydantic.dataclasses.dataclass\n    @dataclasses.dataclass\n    class B:\n        \"\"\"my description\"\"\"\n\n        x: int\n\n    assert model_json_schema(A)['description'] == 'my description'\n\n\ndef test_issue_3011():\n    \"\"\"Validation of a subclass of a dataclass\"\"\"\n\n    @dataclasses.dataclass\n    class A:\n        thing_a: str\n\n    class B(A):\n        thing_b: str\n\n    @pydantic.dataclasses.dataclass\n    class C:\n        thing: A\n\n    b = B('Thing A')\n    c = C(thing=b)\n    assert c.thing.thing_a == 'Thing A'\n\n\ndef test_issue_3162():\n    @dataclasses.dataclass\n    class User:\n        id: int\n        name: str\n\n    class Users(BaseModel):\n        user: User\n        other_user: User\n\n    assert Users.model_json_schema() == {\n        '$defs': {\n            'User': {\n                'properties': {'id': {'title': 'Id', 'type': 'integer'}, 'name': {'title': 'Name', 'type': 'string'}},\n                'required': ['id', 'name'],\n                'title': 'User',\n                'type': 'object',\n            }\n        },\n        'properties': {'other_user': {'$ref': '#/$defs/User'}, 'user': {'$ref': '#/$defs/User'}},\n        'required': ['user', 'other_user'],\n        'title': 'Users',\n        'type': 'object',\n    }\n\n\ndef test_discriminated_union_basemodel_instance_value():\n    @pydantic.dataclasses.dataclass\n    class A:\n        l: Literal['a']  # noqa: E741\n\n    @pydantic.dataclasses.dataclass\n    class B:\n        l: Literal['b']  # noqa: E741\n\n    @pydantic.dataclasses.dataclass\n    class Top:\n        sub: Union[A, B] = dataclasses.field(metadata=dict(discriminator='l'))\n\n    t = Top(sub=A(l='a'))\n    assert isinstance(t, Top)\n    # insert_assert(model_json_schema(Top))\n    assert model_json_schema(Top) == {\n        'title': 'Top',\n        'type': 'object',\n        'properties': {\n            'sub': {\n                'title': 'Sub',\n                'discriminator': {'mapping': {'a': '#/$defs/A', 'b': '#/$defs/B'}, 'propertyName': 'l'},\n                'oneOf': [{'$ref': '#/$defs/A'}, {'$ref': '#/$defs/B'}],\n            }\n        },\n        'required': ['sub'],\n        '$defs': {\n            'A': {\n                'properties': {'l': {'const': 'a', 'enum': ['a'], 'title': 'L', 'type': 'string'}},\n                'required': ['l'],\n                'title': 'A',\n                'type': 'object',\n            },\n            'B': {\n                'properties': {'l': {'const': 'b', 'enum': ['b'], 'title': 'L', 'type': 'string'}},\n                'required': ['l'],\n                'title': 'B',\n                'type': 'object',\n            },\n        },\n    }\n\n\ndef test_post_init_after_validation():\n    @dataclasses.dataclass\n    class SetWrapper:\n        set: Set[int]\n\n        def __post_init__(self):\n            assert isinstance(\n                self.set, set\n            ), f\"self.set should be a set but it's {self.set!r} of type {type(self.set).__name__}\"\n\n    class Model(pydantic.BaseModel):\n        set_wrapper: SetWrapper\n\n    model = Model(set_wrapper=SetWrapper({1, 2, 3}))\n    json_text = model.model_dump_json()\n    assert Model.model_validate_json(json_text).model_dump() == model.model_dump()\n\n\ndef test_new_not_called():\n    \"\"\"\n    pydantic dataclasses do not preserve sunder attributes set in __new__\n    \"\"\"\n\n    class StandardClass:\n        \"\"\"Class which modifies instance creation.\"\"\"\n\n        a: str\n\n        def __new__(cls, *args, **kwargs):\n            instance = super().__new__(cls)\n\n            instance._special_property = 1\n\n            return instance\n\n    StandardLibDataclass = dataclasses.dataclass(StandardClass)\n    PydanticDataclass = pydantic.dataclasses.dataclass(StandardClass)\n\n    test_string = 'string'\n\n    std_instance = StandardLibDataclass(a=test_string)\n    assert std_instance._special_property == 1\n    assert std_instance.a == test_string\n\n    pyd_instance = PydanticDataclass(a=test_string)\n    assert not hasattr(pyd_instance, '_special_property')\n    assert pyd_instance.a == test_string\n\n\ndef test_ignore_extra():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='ignore'))\n    class Foo:\n        x: int\n\n    foo = Foo(**{'x': '1', 'y': '2'})\n    assert foo.__dict__ == {'x': 1}\n\n\ndef test_ignore_extra_subclass():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='ignore'))\n    class Foo:\n        x: int\n\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='ignore'))\n    class Bar(Foo):\n        y: int\n\n    bar = Bar(**{'x': '1', 'y': '2', 'z': '3'})\n    assert bar.__dict__ == {'x': 1, 'y': 2}\n\n\ndef test_allow_extra():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='allow'))\n    class Foo:\n        x: int\n\n    foo = Foo(**{'x': '1', 'y': '2'})\n    assert foo.__dict__ == {'x': 1, 'y': '2'}\n\n\ndef test_allow_extra_subclass():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='allow'))\n    class Foo:\n        x: int\n\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='allow'))\n    class Bar(Foo):\n        y: int\n\n    bar = Bar(**{'x': '1', 'y': '2', 'z': '3'})\n    assert bar.__dict__ == {'x': 1, 'y': 2, 'z': '3'}\n\n\ndef test_forbid_extra():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='forbid'))\n    class Foo:\n        x: int\n\n    msg = re.escape(\"Unexpected keyword argument [type=unexpected_keyword_argument, input_value='2', input_type=str]\")\n\n    with pytest.raises(ValidationError, match=msg):\n        Foo(**{'x': '1', 'y': '2'})\n\n\ndef test_self_reference_dataclass():\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        self_reference: Optional['MyDataclass'] = None\n\n    assert MyDataclass.__pydantic_fields__['self_reference'].annotation == Optional[MyDataclass]\n\n    instance = MyDataclass(self_reference=MyDataclass(self_reference=MyDataclass()))\n    assert TypeAdapter(MyDataclass).dump_python(instance) == {\n        'self_reference': {'self_reference': {'self_reference': None}}\n    }\n\n    with pytest.raises(ValidationError) as exc_info:\n        MyDataclass(self_reference=1)\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_type',\n            'loc': ('self_reference',),\n            'msg': 'Input should be a dictionary or an instance of MyDataclass',\n            'input': 1,\n            'ctx': {'class_name': 'MyDataclass'},\n        }\n    ]\n\n\ndef test_cyclic_reference_dataclass(create_module):\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='forbid'))\n    class D1:\n        d2: Optional['D2'] = None\n\n    @create_module\n    def module():\n        from typing import Optional\n\n        import pydantic\n\n        @pydantic.dataclasses.dataclass(config=pydantic.ConfigDict(extra='forbid'))\n        class D2:\n            d1: Optional['D1'] = None\n\n    # Ensure D2 is in the local namespace; note everything works even though it wasn't _defined_ in this namespace\n    D2 = module.D2\n\n    # Confirm D1 and D2 require rebuilding\n    assert isinstance(D1.__pydantic_validator__, MockValSer)\n    assert isinstance(D2.__pydantic_validator__, MockValSer)\n\n    # Note: the rebuilds of D1 and D2 happen automatically, and works since it grabs the locals here as the namespace,\n    # which contains D1 and D2\n    instance = D1(d2=D2(d1=D1(d2=D2(d1=D1()))))\n\n    # Confirm D1 and D2 have been rebuilt\n    assert isinstance(D1.__pydantic_validator__, SchemaValidator)\n    assert isinstance(D2.__pydantic_validator__, SchemaValidator)\n\n    assert TypeAdapter(D1).dump_python(instance) == {'d2': {'d1': {'d2': {'d1': {'d2': None}}}}}\n\n    with pytest.raises(ValidationError) as exc_info:\n        D2(d1=D2())\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_type',\n            'loc': ('d1',),\n            'msg': 'Input should be a dictionary or an instance of D1',\n            'input': D2(d1=None),\n            'ctx': {'class_name': 'D1'},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        TypeAdapter(D1).validate_python(dict(d2=dict(d1=dict(d2=dict(d2=dict())))))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': {},\n            'loc': ('d2', 'd1', 'd2', 'd2'),\n            'msg': 'Unexpected keyword argument',\n            'type': 'unexpected_keyword_argument',\n        }\n    ]\n\n\ndef test_cross_module_cyclic_reference_dataclass(create_module):\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='forbid'))\n    class D1:\n        d2: Optional['D2'] = None  # noqa F821\n\n    @create_module\n    def module():\n        from typing import Optional\n\n        import pydantic\n\n        @pydantic.dataclasses.dataclass(config=pydantic.ConfigDict(extra='forbid'))\n        class D2:\n            d1: Optional['D1'] = None\n\n    # Since D2 is not in locals, it will not be picked up by the auto-rebuild:\n    with pytest.raises(\n        PydanticUserError,\n        match=re.escape(\n            '`D1` is not fully defined; you should define `D2`, then call'\n            ' `pydantic.dataclasses.rebuild_dataclass(D1)`.'\n        ),\n    ):\n        D1()\n\n    # Explicitly rebuild D1, specifying the appropriate types namespace\n    rebuild_dataclass(D1, _types_namespace={'D2': module.D2, 'D1': D1})\n\n    # Confirm D2 still requires a rebuild (it will happen automatically)\n    assert isinstance(module.D2.__pydantic_validator__, MockValSer)\n\n    instance = D1(d2=module.D2(d1=D1(d2=module.D2(d1=D1()))))\n\n    # Confirm auto-rebuild of D2 has now happened\n    assert isinstance(module.D2.__pydantic_validator__, SchemaValidator)\n\n    assert TypeAdapter(D1).dump_python(instance) == {'d2': {'d1': {'d2': {'d1': {'d2': None}}}}}\n\n    with pytest.raises(ValidationError) as exc_info:\n        module.D2(d1=module.D2())\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_type',\n            'input': module.D2(d1=None),\n            'loc': ('d1',),\n            'msg': 'Input should be a dictionary or an instance of D1',\n            'ctx': {'class_name': 'D1'},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        TypeAdapter(D1).validate_python(dict(d2=dict(d1=dict(d2=dict(d2=dict())))))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': {},\n            'loc': ('d2', 'd1', 'd2', 'd2'),\n            'msg': 'Unexpected keyword argument',\n            'type': 'unexpected_keyword_argument',\n        }\n    ]\n\n\n@pytest.mark.parametrize(\n    'dataclass_decorator',\n    [\n        pydantic.dataclasses.dataclass,\n        dataclasses.dataclass,\n    ],\n    ids=['pydantic', 'stdlib'],\n)\ndef test_base_dataclasses_annotations_resolving(create_module, dataclass_decorator: Callable):\n    @create_module\n    def module():\n        import dataclasses\n        from typing import NewType\n\n        OddInt = NewType('OddInt', int)\n\n        @dataclasses.dataclass\n        class D1:\n            d1: 'OddInt'\n            s: str\n\n            __pydantic_config__ = {'str_to_lower': True}\n\n    @dataclass_decorator\n    class D2(module.D1):\n        d2: int\n\n    assert TypeAdapter(D2).validate_python({'d1': 1, 'd2': 2, 's': 'ABC'}) == D2(d1=1, d2=2, s='abc')\n\n\n@pytest.mark.parametrize(\n    'dataclass_decorator',\n    [\n        pydantic.dataclasses.dataclass,\n        dataclasses.dataclass,\n    ],\n    ids=['pydantic', 'stdlib'],\n)\ndef test_base_dataclasses_annotations_resolving_with_override(create_module, dataclass_decorator: Callable):\n    @create_module\n    def module1():\n        import dataclasses\n        from typing import NewType\n\n        IDType = NewType('IDType', int)\n\n        @dataclasses.dataclass\n        class D1:\n            db_id: 'IDType'\n\n            __pydantic_config__ = {'str_to_lower': True}\n\n    @create_module\n    def module2():\n        import dataclasses\n        from typing import NewType\n\n        IDType = NewType('IDType', str)\n\n        @dataclasses.dataclass\n        class D2:\n            db_id: 'IDType'\n            s: str\n\n            __pydantic_config__ = {'str_to_lower': False}\n\n    @dataclass_decorator\n    class D3(module1.D1, module2.D2): ...\n\n    assert TypeAdapter(D3).validate_python({'db_id': 42, 's': 'ABC'}) == D3(db_id=42, s='abc')\n\n\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='kw_only is not available in python < 3.10')\ndef test_kw_only():\n    @pydantic.dataclasses.dataclass(kw_only=True)\n    class A:\n        a: int | None = None\n        b: str\n\n    with pytest.raises(ValidationError):\n        A(1, '')\n\n    assert A(b='hi').b == 'hi'\n\n\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='kw_only is not available in python < 3.10')\ndef test_kw_only_subclass():\n    @pydantic.dataclasses.dataclass\n    class A:\n        x: int\n        y: int = pydantic.Field(default=0, kw_only=True)\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        z: int\n\n    assert B(1, 2) == B(x=1, y=0, z=2)\n    assert B(1, y=2, z=3) == B(x=1, y=2, z=3)\n\n\n@pytest.mark.parametrize('field_constructor', [pydantic.dataclasses.Field, dataclasses.field])\ndef test_repr_false(field_constructor: Callable):\n    @pydantic.dataclasses.dataclass\n    class A:\n        visible_field: str\n        hidden_field: str = field_constructor(repr=False)\n\n    instance = A(visible_field='this_should_be_included', hidden_field='this_should_not_be_included')\n    assert \"visible_field='this_should_be_included'\" in repr(instance)\n    assert \"hidden_field='this_should_not_be_included'\" not in repr(instance)\n\n\ndef dataclass_decorators(include_identity: bool = False, exclude_combined: bool = False):\n    decorators = [pydantic.dataclasses.dataclass, dataclasses.dataclass]\n    ids = ['pydantic', 'stdlib']\n\n    if not exclude_combined:\n\n        def combined_decorator(cls):\n            \"\"\"\n            Should be equivalent to:\n            @pydantic.dataclasses.dataclass\n            @dataclasses.dataclass\n            \"\"\"\n            return pydantic.dataclasses.dataclass(dataclasses.dataclass(cls))\n\n        decorators.append(combined_decorator)\n        ids.append('combined')\n\n    if include_identity:\n\n        def identity_decorator(cls):\n            return cls\n\n        decorators.append(identity_decorator)\n        ids.append('identity')\n\n    return {'argvalues': decorators, 'ids': ids}\n\n\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='kw_only is not available in python < 3.10')\n@pytest.mark.parametrize('decorator1', **dataclass_decorators(exclude_combined=True))\n@pytest.mark.parametrize('decorator2', **dataclass_decorators(exclude_combined=True))\ndef test_kw_only_inheritance(decorator1, decorator2):\n    # Exclude combined from the decorators since it doesn't know how to accept kw_only\n    @decorator1(kw_only=True)\n    class Parent:\n        x: int\n\n    @decorator2\n    class Child(Parent):\n        y: int\n\n    child = Child(1, x=2)\n    assert child.x == 2\n    assert child.y == 1\n\n\ndef test_extra_forbid_list_no_error():\n    @pydantic.dataclasses.dataclass(config=dict(extra='forbid'))\n    class Bar: ...\n\n    @pydantic.dataclasses.dataclass\n    class Foo:\n        a: List[Bar]\n\n    assert isinstance(Foo(a=[Bar()]).a[0], Bar)\n\n\ndef test_extra_forbid_list_error():\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra='forbid'))\n    class Bar: ...\n\n    with pytest.raises(ValidationError, match=r'a\\s+Unexpected keyword argument'):\n        Bar(a=1)\n\n\ndef test_field_validator():\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        a: int\n        b: float\n\n        @field_validator('b')\n        @classmethod\n        def double_b(cls, v):\n            return v * 2\n\n    d = MyDataclass('1', '2.5')\n    assert d.a == 1\n    assert d.b == 5.0\n\n\ndef test_model_validator_before():\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        a: int\n        b: float\n\n        @model_validator(mode='before')\n        @classmethod\n        def double_b(cls, v: ArgsKwargs):\n            v.kwargs['b'] *= 2\n            return v\n\n    d = MyDataclass('1', b='2')\n    assert d.a == 1\n    assert d.b == 22.0\n\n\ndef test_model_validator_after():\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        a: int\n        b: float\n\n        @model_validator(mode='after')\n        def double_b(self) -> 'MyDataclass':\n            self.b *= 2\n            return self\n\n    d = MyDataclass('1', b='2')\n    assert d.a == 1\n    assert d.b == 4\n\n\ndef test_parent_post_init():\n    \"\"\"\n    Test that the parent's __post_init__ gets called\n    and the order in which it gets called relative to validation.\n\n    In V1 we called it before validation, in V2 it gets called after.\n    \"\"\"\n\n    @dataclasses.dataclass\n    class A:\n        a: float\n\n        def __post_init__(self):\n            self.a *= 2\n\n    assert A(a=1.2).a == 2.4\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        @field_validator('a')\n        @classmethod\n        def validate_a(cls, value, _):\n            value += 3\n            return value\n\n    assert B(a=1).a == 8  # (1 + 3) * 2 = 8\n\n\ndef test_subclass_post_init_order():\n    @dataclasses.dataclass\n    class A:\n        a: float\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        def __post_init__(self):\n            self.a *= 2\n\n        @field_validator('a')\n        @classmethod\n        def validate_a(cls, value):\n            value += 3\n            return value\n\n    assert B(a=1).a == 8  # (1 + 3) * 2 = 8\n\n\ndef test_subclass_post_init_inheritance():\n    @dataclasses.dataclass\n    class A:\n        a: int\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        def __post_init__(self):\n            self.a *= 2\n\n        @field_validator('a')\n        @classmethod\n        def validate_a(cls, value):\n            value += 3\n            return value\n\n    @pydantic.dataclasses.dataclass\n    class C(B):\n        def __post_init__(self):\n            self.a *= 3\n\n    assert C(1).a == 12  # (1 + 3) * 3\n\n\ndef test_config_as_type_deprecated():\n    class Config:\n        validate_assignment = True\n\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='Support for class-based `config` is deprecated, use ConfigDict instead.'\n    ):\n\n        @pydantic.dataclasses.dataclass(config=Config)\n        class MyDataclass:\n            a: int\n\n    assert MyDataclass.__pydantic_config__ == ConfigDict(validate_assignment=True)\n\n\ndef test_validator_info_field_name_data_before():\n    \"\"\"\n    Test accessing info.field_name and info.data\n    We only test the `before` validator because they\n    all share the same implementation.\n    \"\"\"\n\n    @pydantic.dataclasses.dataclass\n    class Model:\n        a: str\n        b: str\n\n        @field_validator('b', mode='before')\n        @classmethod\n        def check_a(cls, v: Any, info: ValidationInfo) -> Any:\n            assert v == b'but my barbaz is better'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 'your foobar is good'}\n            return 'just kidding!'\n\n    assert Model(a=b'your foobar is good', b=b'but my barbaz is better').b == 'just kidding!'\n\n\n@pytest.mark.parametrize(\n    'decorator1, expected_parent, expected_child',\n    [\n        (\n            pydantic.dataclasses.dataclass,\n            ['parent before', 'parent', 'parent after'],\n            ['parent before', 'child', 'parent after', 'child before', 'child after'],\n        ),\n        (dataclasses.dataclass, [], ['parent before', 'child', 'parent after', 'child before', 'child after']),\n    ],\n    ids=['pydantic', 'stdlib'],\n)\ndef test_inheritance_replace(decorator1: Callable[[Any], Any], expected_parent: List[str], expected_child: List[str]):\n    \"\"\"We promise that if you add a validator\n    with the same _function_ name as an existing validator\n    it replaces the existing validator and is run instead of it.\n    \"\"\"\n\n    @decorator1\n    class Parent:\n        a: List[str]\n\n        @field_validator('a')\n        @classmethod\n        def parent_val_before(cls, v: List[str]):\n            v.append('parent before')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def val(cls, v: List[str]):\n            v.append('parent')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def parent_val_after(cls, v: List[str]):\n            v.append('parent after')\n            return v\n\n    @pydantic.dataclasses.dataclass\n    class Child(Parent):\n        @field_validator('a')\n        @classmethod\n        def child_val_before(cls, v: List[str]):\n            v.append('child before')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def val(cls, v: List[str]):\n            v.append('child')\n            return v\n\n        @field_validator('a')\n        @classmethod\n        def child_val_after(cls, v: List[str]):\n            v.append('child after')\n            return v\n\n    assert Parent(a=[]).a == expected_parent\n    assert Child(a=[]).a == expected_child\n\n\n@pytest.mark.parametrize(\n    'decorator1',\n    [\n        pydantic.dataclasses.dataclass,\n        dataclasses.dataclass,\n    ],\n    ids=['pydantic', 'stdlib'],\n)\n@pytest.mark.parametrize(\n    'default',\n    [1, dataclasses.field(default=1), Field(default=1)],\n    ids=['1', 'dataclasses.field(default=1)', 'pydantic.Field(default=1)'],\n)\ndef test_dataclasses_inheritance_default_value_is_not_deleted(\n    decorator1: Callable[[Any], Any], default: Literal[1]\n) -> None:\n    if decorator1 is dataclasses.dataclass and isinstance(default, FieldInfo):\n        pytest.skip(reason=\"stdlib dataclasses don't support Pydantic fields\")\n\n    @decorator1\n    class Parent:\n        a: int = default\n\n    assert Parent.a == 1\n    assert Parent().a == 1\n\n    @pydantic.dataclasses.dataclass\n    class Child(Parent):\n        pass\n\n    assert Child.a == 1\n    assert Child().a == 1\n\n\ndef test_dataclass_config_validate_default():\n    @pydantic.dataclasses.dataclass\n    class Model:\n        x: int = -1\n\n        @field_validator('x')\n        @classmethod\n        def force_x_positive(cls, v):\n            assert v > 0\n            return v\n\n    assert Model().x == -1\n\n    @pydantic.dataclasses.dataclass(config=ConfigDict(validate_default=True))\n    class ValidatingModel(Model):\n        pass\n\n    with pytest.raises(ValidationError) as exc_info:\n        ValidatingModel()\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(AssertionError('assert -1 > 0')))},\n            'input': -1,\n            'loc': ('x',),\n            'msg': 'Assertion failed, assert -1 > 0',\n            'type': 'assertion_error',\n        }\n    ]\n\n\n@pytest.mark.parametrize('dataclass_decorator', **dataclass_decorators())\ndef test_unparametrized_generic_dataclass(dataclass_decorator):\n    T = TypeVar('T')\n\n    @dataclass_decorator\n    class GenericDataclass(Generic[T]):\n        x: T\n\n    # In principle we could call GenericDataclass(...) below, but this won't do validation\n    # for standard dataclasses, so we just use TypeAdapter to get validation for each.\n    validator = pydantic.TypeAdapter(GenericDataclass)\n\n    assert validator.validate_python({'x': None}).x is None\n    assert validator.validate_python({'x': 1}).x == 1\n\n    with pytest.raises(ValidationError) as exc_info:\n        validator.validate_python({'y': None})\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'y': None}, 'loc': ('x',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\n@pytest.mark.parametrize('dataclass_decorator', **dataclass_decorators())\n@pytest.mark.parametrize(\n    'annotation,input_value,error,output_value',\n    [\n        (int, 1, False, 1),\n        (str, 'a', False, 'a'),\n        (\n            int,\n            'a',\n            True,\n            [\n                {\n                    'input': 'a',\n                    'loc': ('x',),\n                    'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n                    'type': 'int_parsing',\n                }\n            ],\n        ),\n    ],\n)\ndef test_parametrized_generic_dataclass(dataclass_decorator, annotation, input_value, error, output_value):\n    T = TypeVar('T')\n\n    @dataclass_decorator\n    class GenericDataclass(Generic[T]):\n        x: T\n\n    # Need to use TypeAdapter here because GenericDataclass[annotation] will be a GenericAlias, which delegates\n    # method calls to the (non-parametrized) origin class. This is essentially a limitation of typing._GenericAlias.\n    validator = pydantic.TypeAdapter(GenericDataclass[annotation])\n\n    if not error:\n        assert validator.validate_python({'x': input_value}).x == output_value\n    else:\n        with pytest.raises(ValidationError) as exc_info:\n            validator.validate_python({'x': input_value})\n        assert exc_info.value.errors(include_url=False) == output_value\n\n\ndef test_multiple_parametrized_generic_dataclasses():\n    T = TypeVar('T')\n\n    @pydantic.dataclasses.dataclass\n    class GenericDataclass(Generic[T]):\n        x: T\n\n    validator1 = pydantic.TypeAdapter(GenericDataclass[int])\n    validator2 = pydantic.TypeAdapter(GenericDataclass[str])\n\n    # verify that generic parameters are showing up in the type ref for generic dataclasses\n    # this can probably be removed if the schema changes in some way that makes this part of the test fail\n    assert '[int:' in validator1.core_schema['ref']\n    assert '[str:' in validator2.core_schema['ref']\n\n    assert validator1.validate_python({'x': 1}).x == 1\n    assert validator2.validate_python({'x': 'hello world'}).x == 'hello world'\n\n    with pytest.raises(ValidationError) as exc_info:\n        validator2.validate_python({'x': 1})\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 1, 'loc': ('x',), 'msg': 'Input should be a valid string', 'type': 'string_type'}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        validator1.validate_python({'x': 'hello world'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'hello world',\n            'loc': ('x',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\n@pytest.mark.parametrize('dataclass_decorator', **dataclass_decorators(include_identity=True))\ndef test_pydantic_dataclass_preserves_metadata(dataclass_decorator: Callable[[Any], Any]) -> None:\n    @dataclass_decorator\n    class FooStd:\n        \"\"\"Docstring\"\"\"\n\n    FooPydantic = pydantic.dataclasses.dataclass(FooStd)\n\n    assert FooPydantic.__module__ == FooStd.__module__\n    assert FooPydantic.__name__ == FooStd.__name__\n    assert FooPydantic.__qualname__ == FooStd.__qualname__\n\n\ndef test_recursive_dataclasses_gh_4509(create_module) -> None:\n    @create_module\n    def module():\n        import dataclasses\n        from typing import List\n\n        import pydantic\n\n        @dataclasses.dataclass\n        class Recipe:\n            author: 'Cook'\n\n        @dataclasses.dataclass\n        class Cook:\n            recipes: List[Recipe]\n\n        @pydantic.dataclasses.dataclass\n        class Foo(Cook):\n            pass\n\n    gordon = module.Cook([])\n\n    burger = module.Recipe(author=gordon)\n\n    me = module.Foo([burger])\n\n    assert me.recipes == [burger]\n\n\ndef test_dataclass_alias_generator():\n    def alias_generator(name: str) -> str:\n        return 'alias_' + name\n\n    @pydantic.dataclasses.dataclass(config=ConfigDict(alias_generator=alias_generator))\n    class User:\n        name: str\n        score: int = Field(alias='my_score')\n\n    user = User(**{'alias_name': 'test name', 'my_score': 2})\n    assert user.name == 'test name'\n    assert user.score == 2\n\n    with pytest.raises(ValidationError) as exc_info:\n        User(name='test name', score=2)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing',\n            'loc': ('alias_name',),\n            'msg': 'Field required',\n            'input': ArgsKwargs((), {'name': 'test name', 'score': 2}),\n        },\n        {\n            'type': 'missing',\n            'loc': ('my_score',),\n            'msg': 'Field required',\n            'input': ArgsKwargs((), {'name': 'test name', 'score': 2}),\n        },\n    ]\n\n\ndef test_init_vars_inheritance():\n    init_vars = []\n\n    @pydantic.dataclasses.dataclass\n    class Foo:\n        init: 'InitVar[int]'\n\n    @pydantic.dataclasses.dataclass\n    class Bar(Foo):\n        arg: int\n\n        def __post_init__(self, init: int) -> None:\n            init_vars.append(init)\n\n    bar = Bar(init=1, arg=2)\n    assert TypeAdapter(Bar).dump_python(bar) == {'arg': 2}\n    assert init_vars == [1]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Bar(init='a', arg=2)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'a',\n            'loc': ('init',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n\n@pytest.mark.skipif(not hasattr(pydantic.dataclasses, '_call_initvar'), reason='InitVar was not modified')\n@pytest.mark.parametrize('remove_monkeypatch', [True, False])\ndef test_init_vars_call_monkeypatch(remove_monkeypatch, monkeypatch):\n    # Parametrizing like this allows us to test that the behavior is the same with or without the monkeypatch\n\n    if remove_monkeypatch:\n        monkeypatch.delattr(InitVar, '__call__')\n\n    InitVar(int)  # this is what is produced by InitVar[int]; note monkeypatching __call__ doesn't break this\n\n    with pytest.raises(TypeError, match=\"'InitVar' object is not callable\") as exc:\n        InitVar[int]()\n\n    # Check that the custom __call__ was called precisely if the monkeypatch was not removed\n    stack_depth = len(traceback.extract_tb(exc.value.__traceback__))\n    assert stack_depth == 1 if remove_monkeypatch else 2\n\n\n@pytest.mark.parametrize('decorator1', **dataclass_decorators())\n@pytest.mark.parametrize('decorator2', **dataclass_decorators())\ndef test_decorators_in_model_field(decorator1, decorator2):\n    @decorator1\n    class Demo1:\n        int1: int\n\n        @field_validator('int1', mode='before')\n        def set_int_1(cls, v):\n            return v + 100\n\n        @field_serializer('int1')\n        def serialize_int_1(self, v):\n            return v + 10\n\n    @decorator2\n    class Demo2(Demo1):\n        int2: int\n\n        @field_validator('int2', mode='before')\n        def set_int_2(cls, v):\n            return v + 200\n\n        @field_serializer('int2')\n        def serialize_int_2(self, v):\n            return v + 20\n\n    class Model(BaseModel):\n        x: Demo2\n\n    m = Model.model_validate(dict(x=dict(int1=1, int2=2)))\n    assert m.x.int1 == 101\n    assert m.x.int2 == 202\n\n    assert m.model_dump() == {'x': {'int1': 111, 'int2': 222}}\n\n\n@pytest.mark.parametrize('decorator1', **dataclass_decorators())\n@pytest.mark.parametrize('decorator2', **dataclass_decorators())\ndef test_vanilla_dataclass_decorators_in_type_adapter(decorator1, decorator2):\n    @decorator1\n    class Demo1:\n        int1: int\n\n        @field_validator('int1', mode='before')\n        def set_int_1(cls, v):\n            return v + 100\n\n        @field_serializer('int1')\n        def serialize_int_1(self, v):\n            return v + 10\n\n    @decorator2\n    class Demo2(Demo1):\n        int2: int\n\n        @field_validator('int2', mode='before')\n        def set_int_2(cls, v):\n            return v + 200\n\n        @field_serializer('int2')\n        def serialize_int_2(self, v):\n            return v + 20\n\n    adapter = TypeAdapter(Demo2)\n\n    m = adapter.validate_python(dict(int1=1, int2=2))\n    assert m.int1 == 101\n    assert m.int2 == 202\n\n    assert adapter.dump_python(m) == {'int1': 111, 'int2': 222}\n\n\n@pytest.mark.parametrize(\n    'dataclass_decorator',\n    [\n        pydantic.dataclasses.dataclass,\n        dataclasses.dataclass,\n    ],\n    ids=['pydantic', 'stdlib'],\n)\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='slots are only supported for dataclasses in Python >= 3.10')\ndef test_dataclass_slots(dataclass_decorator):\n    @dataclass_decorator(slots=True)\n    class Model:\n        a: str\n        b: str\n\n    dc = TypeAdapter(Model).validate_python({'a': 'foo', 'b': 'bar'})\n    assert dc.a == 'foo'\n    assert dc.b == 'bar'\n\n\n@pytest.mark.parametrize(\n    'dataclass_decorator',\n    [\n        pydantic.dataclasses.dataclass,\n        dataclasses.dataclass,\n    ],\n    ids=['pydantic', 'stdlib'],\n)\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='slots are only supported for dataclasses in Python >= 3.10')\ndef test_dataclass_slots_mixed(dataclass_decorator):\n    @dataclass_decorator(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n\n    @dataclass_decorator\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n\n    dc = TypeAdapter(SubModel).validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert SubModel.z == 'z-classvar'\n    assert SubModel.z2 == 'z2-classvar'\n\n\ndef test_rebuild_dataclass():\n    @pydantic.dataclasses.dataclass\n    class MyDataClass:\n        x: str\n\n    assert rebuild_dataclass(MyDataClass) is None\n\n    @pydantic.dataclasses.dataclass()\n    class MyDataClass1:\n        d2: Optional['Foo'] = None  # noqa F821\n\n    with pytest.raises(PydanticUndefinedAnnotation, match=\"name 'Foo' is not defined\"):\n        rebuild_dataclass(MyDataClass1, _parent_namespace_depth=0)\n\n\n@pytest.mark.parametrize(\n    'dataclass_decorator',\n    [\n        pydantic.dataclasses.dataclass,\n        dataclasses.dataclass,\n    ],\n    ids=['pydantic', 'stdlib'],\n)\ndef test_model_config(dataclass_decorator: Any) -> None:\n    @dataclass_decorator\n    class Model:\n        x: str\n        __pydantic_config__ = ConfigDict(str_to_lower=True)\n\n    ta = TypeAdapter(Model)\n    assert ta.validate_python({'x': 'ABC'}).x == 'abc'\n\n\ndef test_model_config_override_in_decorator() -> None:\n    @pydantic.dataclasses.dataclass(config=ConfigDict(str_to_lower=False, str_strip_whitespace=True))\n    class Model:\n        x: str\n        __pydantic_config__ = ConfigDict(str_to_lower=True)\n\n    ta = TypeAdapter(Model)\n    assert ta.validate_python({'x': 'ABC '}).x == 'ABC'\n\n\ndef test_model_config_override_in_decorator_empty_config() -> None:\n    @pydantic.dataclasses.dataclass(config=ConfigDict())\n    class Model:\n        x: str\n        __pydantic_config__ = ConfigDict(str_to_lower=True)\n\n    ta = TypeAdapter(Model)\n    assert ta.validate_python({'x': 'ABC '}).x == 'ABC '\n\n\ndef test_dataclasses_with_config_decorator():\n    @dataclasses.dataclass\n    @with_config(ConfigDict(str_to_lower=True))\n    class Model1:\n        x: str\n\n    ta = TypeAdapter(Model1)\n    assert ta.validate_python({'x': 'ABC'}).x == 'abc'\n\n    @with_config(ConfigDict(str_to_lower=True))\n    @dataclasses.dataclass\n    class Model2:\n        x: str\n\n    ta = TypeAdapter(Model2)\n    assert ta.validate_python({'x': 'ABC'}).x == 'abc'\n\n\ndef test_pydantic_field_annotation():\n    @pydantic.dataclasses.dataclass\n    class Model:\n        x: Annotated[int, Field(gt=0)]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=-1)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'gt': 0},\n            'input': -1,\n            'loc': ('x',),\n            'msg': 'Input should be greater than 0',\n            'type': 'greater_than',\n        }\n    ]\n\n\ndef test_combined_field_annotations():\n    \"\"\"\n    This test is included to document the fact that `Field` and `field` can be used together.\n    That said, if you mix them like this, there is a good chance you'll run into surprising behavior/bugs.\n\n    (E.g., `x: Annotated[int, Field(gt=1, validate_default=True)] = field(default=0)` doesn't cause an error)\n\n    I would generally advise against doing this, and if we do change the behavior in the future to somehow merge\n    pydantic.FieldInfo and dataclasses.Field in a way that changes runtime behavior for existing code, I would probably\n    consider it a bugfix rather than a breaking change.\n    \"\"\"\n\n    @pydantic.dataclasses.dataclass\n    class Model:\n        x: Annotated[int, Field(gt=1)] = dataclasses.field(default=1)\n\n    assert Model().x == 1\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=0)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'gt': 1},\n            'input': 0,\n            'loc': ('x',),\n            'msg': 'Input should be greater than 1',\n            'type': 'greater_than',\n        }\n    ]\n\n\ndef test_dataclass_field_default_factory_with_init():\n    @pydantic.dataclasses.dataclass\n    class Model:\n        x: int = dataclasses.field(default_factory=lambda: 3, init=False)\n\n    m = Model()\n    assert 'x' in Model.__pydantic_fields__\n    assert m.x == 3\n    assert RootModel[Model](m).model_dump() == {'x': 3}\n\n\ndef test_dataclass_field_default_with_init():\n    @pydantic.dataclasses.dataclass\n    class Model:\n        x: int = dataclasses.field(default=3, init=False)\n\n    m = Model()\n    assert 'x' in Model.__pydantic_fields__\n    assert m.x == 3\n    assert RootModel[Model](m).model_dump() == {'x': 3}\n\n\ndef test_metadata():\n    @dataclasses.dataclass\n    class Test:\n        value: int = dataclasses.field(metadata={'info': 'Some int value', 'json_schema_extra': {'a': 'b'}})\n\n    PydanticTest = pydantic.dataclasses.dataclass(Test)\n\n    assert TypeAdapter(PydanticTest).json_schema() == {\n        'properties': {'value': {'a': 'b', 'title': 'Value', 'type': 'integer'}},\n        'required': ['value'],\n        'title': 'Test',\n        'type': 'object',\n    }\n\n\ndef test_signature():\n    @pydantic.dataclasses.dataclass\n    class Model:\n        x: int\n        y: str = 'y'\n        z: float = dataclasses.field(default=1.0)\n        a: float = dataclasses.field(default_factory=float)\n        b: float = Field(default=1.0)\n        c: float = Field(default_factory=float)\n        d: int = dataclasses.field(metadata={'alias': 'dd'}, default=1)\n\n    assert str(inspect.signature(Model)) == (\n        \"(x: int, y: str = 'y', z: float = 1.0, a: float = <factory>, b: float = 1.0, c: float = <factory>, dd: int = 1) -> None\"\n    )\n\n\ndef test_inherited_dataclass_signature():\n    @pydantic.dataclasses.dataclass\n    class A:\n        a: int\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        b: int\n\n    assert str(inspect.signature(A)) == '(a: int) -> None'\n    assert str(inspect.signature(B)) == '(a: int, b: int) -> None'\n\n\ndef test_dataclasses_with_slots_and_default():\n    @pydantic.dataclasses.dataclass(slots=True)\n    class A:\n        a: int = 0\n\n    assert A().a == 0\n\n    @pydantic.dataclasses.dataclass(slots=True)\n    class B:\n        b: int = Field(1)\n\n    assert B().b == 1\n\n\ndef test_schema_generator() -> None:\n    class LaxStrGenerator(GenerateSchema):\n        def str_schema(self) -> CoreSchema:\n            return core_schema.no_info_plain_validator_function(str)\n\n    @pydantic.dataclasses.dataclass\n    class Model:\n        x: str\n        __pydantic_config__ = ConfigDict(schema_generator=LaxStrGenerator)\n\n    assert Model(x=1).x == '1'\n\n\n@pytest.mark.parametrize('decorator1', **dataclass_decorators())\ndef test_annotated_before_validator_called_once(decorator1):\n    count = 0\n\n    def convert(value: int) -> str:\n        nonlocal count\n        count += 1\n        return str(value)\n\n    IntToStr = Annotated[str, BeforeValidator(convert)]\n\n    @decorator1\n    class A:\n        a: IntToStr\n\n    assert count == 0\n    TypeAdapter(A).validate_python({'a': 123})\n    assert count == 1\n\n\ndef test_is_pydantic_dataclass():\n    @pydantic.dataclasses.dataclass\n    class PydanticDataclass:\n        a: int\n\n    @dataclasses.dataclass\n    class StdLibDataclass:\n        b: int\n\n    assert is_pydantic_dataclass(PydanticDataclass) is True\n    assert is_pydantic_dataclass(StdLibDataclass) is False\n\n\ndef test_can_inherit_stdlib_dataclasses_with_defaults():\n    @dataclasses.dataclass\n    class Base:\n        a: None = None\n\n    class Model(BaseModel, Base):\n        pass\n\n    assert Model().a is None\n\n\ndef test_can_inherit_stdlib_dataclasses_default_factories_and_use_them():\n    \"\"\"This test documents that default factories are not supported\"\"\"\n\n    @dataclasses.dataclass\n    class Base:\n        a: str = dataclasses.field(default_factory=lambda: 'TEST')\n\n    class Model(BaseModel, Base):\n        pass\n\n    with pytest.raises(ValidationError):\n        assert Model().a == 'TEST'\n\n\ndef test_can_inherit_stdlib_dataclasses_default_factories_and_provide_a_value():\n    @dataclasses.dataclass\n    class Base:\n        a: str = dataclasses.field(default_factory=lambda: 'TEST')\n\n    class Model(BaseModel, Base):\n        pass\n\n    assert Model(a='NOT_THE_SAME').a == 'NOT_THE_SAME'\n\n\ndef test_can_inherit_stdlib_dataclasses_with_dataclass_fields():\n    @dataclasses.dataclass\n    class Base:\n        a: int = dataclasses.field(default=5)\n\n    class Model(BaseModel, Base):\n        pass\n\n    assert Model().a == 5\n\n\ndef test_alias_with_dashes():\n    \"\"\"Test for fix issue #7226.\"\"\"\n\n    @pydantic.dataclasses.dataclass\n    class Foo:\n        some_var: str = Field(alias='some-var')\n\n    obj = Foo(**{'some-var': 'some_value'})\n    assert obj.some_var == 'some_value'\n\n\ndef test_validate_strings():\n    @pydantic.dataclasses.dataclass\n    class Nested:\n        d: date\n\n    class Model(BaseModel):\n        n: Nested\n\n    assert Model.model_validate_strings({'n': {'d': '2017-01-01'}}).n.d == date(2017, 1, 1)\n\n\n@pytest.mark.parametrize('field_constructor', [dataclasses.field, pydantic.dataclasses.Field])\n@pytest.mark.parametrize('extra', ['ignore', 'forbid'])\ndef test_init_false_not_in_signature(extra, field_constructor):\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra=extra))\n    class MyDataclass:\n        a: int = field_constructor(init=False, default=-1)\n        b: int = pydantic.dataclasses.Field(default=2)\n\n    signature = inspect.signature(MyDataclass)\n    # `a` should not be in the __init__\n    assert 'a' not in signature.parameters.keys()\n    assert 'b' in signature.parameters.keys()\n\n\ninit_test_cases = [\n    ({'a': 2, 'b': -1}, 'ignore', {'a': 2, 'b': 1}),\n    ({'a': 2}, 'ignore', {'a': 2, 'b': 1}),\n    (\n        {'a': 2, 'b': -1},\n        'forbid',\n        [\n            {\n                'type': 'unexpected_keyword_argument',\n                'loc': ('b',),\n                'msg': 'Unexpected keyword argument',\n                'input': -1,\n            }\n        ],\n    ),\n    ({'a': 2}, 'forbid', {'a': 2, 'b': 1}),\n]\n\n\n@pytest.mark.parametrize('field_constructor', [dataclasses.field, pydantic.dataclasses.Field])\n@pytest.mark.parametrize(\n    'input_data,extra,expected',\n    init_test_cases,\n)\ndef test_init_false_with_post_init(input_data, extra, expected, field_constructor):\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra=extra))\n    class MyDataclass:\n        a: int\n        b: int = field_constructor(init=False)\n\n        def __post_init__(self):\n            self.b = 1\n\n    if isinstance(expected, list):\n        with pytest.raises(ValidationError) as exc_info:\n            MyDataclass(**input_data)\n\n        assert exc_info.value.errors(include_url=False) == expected\n    else:\n        assert dataclasses.asdict(MyDataclass(**input_data)) == expected\n\n\n@pytest.mark.parametrize('field_constructor', [dataclasses.field, pydantic.dataclasses.Field])\n@pytest.mark.parametrize(\n    'input_data,extra,expected',\n    init_test_cases,\n)\ndef test_init_false_with_default(input_data, extra, expected, field_constructor):\n    @pydantic.dataclasses.dataclass(config=ConfigDict(extra=extra))\n    class MyDataclass:\n        a: int\n        b: int = field_constructor(init=False, default=1)\n\n    if isinstance(expected, list):\n        with pytest.raises(ValidationError) as exc_info:\n            MyDataclass(**input_data)\n\n        assert exc_info.value.errors(include_url=False) == expected\n    else:\n        assert dataclasses.asdict(MyDataclass(**input_data)) == expected\n\n\ndef test_disallow_extra_allow_and_init_false() -> None:\n    with pytest.raises(PydanticUserError, match='This combination is not allowed.'):\n\n        @pydantic.dataclasses.dataclass(config=ConfigDict(extra='allow'))\n        class A:\n            a: int = Field(init=False, default=1)\n\n\ndef test_disallow_init_false_and_init_var_true() -> None:\n    with pytest.raises(PydanticUserError, match='mutually exclusive.'):\n\n        @pydantic.dataclasses.dataclass\n        class Foo:\n            bar: str = Field(..., init=False, init_var=True)\n\n\ndef test_annotations_valid_for_field_inheritance() -> None:\n    # testing https://github.com/pydantic/pydantic/issues/8670\n\n    @pydantic.dataclasses.dataclass()\n    class A:\n        a: int = pydantic.dataclasses.Field()\n\n    @pydantic.dataclasses.dataclass()\n    class B(A): ...\n\n    assert B.__pydantic_fields__['a'].annotation is int\n\n    assert B(a=1).a == 1\n\n\ndef test_annotations_valid_for_field_inheritance_with_existing_field() -> None:\n    # variation on testing https://github.com/pydantic/pydantic/issues/8670\n\n    @pydantic.dataclasses.dataclass()\n    class A:\n        a: int = pydantic.dataclasses.Field()\n\n    @pydantic.dataclasses.dataclass()\n    class B(A):\n        b: str = pydantic.dataclasses.Field()\n\n    assert B.__pydantic_fields__['a'].annotation is int\n    assert B.__pydantic_fields__['b'].annotation is str\n\n    b = B(a=1, b='b')\n    assert b.a == 1\n    assert b.b == 'b'\n\n\ndef test_annotation_with_double_override() -> None:\n    @pydantic.dataclasses.dataclass()\n    class A:\n        a: int\n        b: int\n        c: int = pydantic.dataclasses.Field()\n        d: int = pydantic.dataclasses.Field()\n\n    # note, the order of fields is different here, as to test that the annotation\n    # is correctly set on the field no matter the base's default / current class's default\n    @pydantic.dataclasses.dataclass()\n    class B(A):\n        a: str\n        c: str\n        b: str = pydantic.dataclasses.Field()\n        d: str = pydantic.dataclasses.Field()\n\n    @pydantic.dataclasses.dataclass()\n    class C(B): ...\n\n    for class_ in [B, C]:\n        instance = class_(a='a', b='b', c='c', d='d')\n        for field_name in ['a', 'b', 'c', 'd']:\n            assert class_.__pydantic_fields__[field_name].annotation is str\n            assert getattr(instance, field_name) == field_name\n\n\ndef test_schema_valid_for_inner_generic() -> None:\n    T = TypeVar('T')\n\n    @pydantic.dataclasses.dataclass()\n    class Inner(Generic[T]):\n        x: T\n\n    @pydantic.dataclasses.dataclass()\n    class Outer:\n        inner: Inner[int]\n\n    assert Outer(inner={'x': 1}).inner.x == 1\n    # note, this isn't Inner[Int] like it is for the BaseModel case, but the type of x is substituted, which is the important part\n    assert Outer.__pydantic_core_schema__['schema']['fields'][0]['schema']['cls'] == Inner\n    assert (\n        Outer.__pydantic_core_schema__['schema']['fields'][0]['schema']['schema']['fields'][0]['schema']['type']\n        == 'int'\n    )\n\n\ndef test_validation_works_for_cyclical_forward_refs() -> None:\n    @pydantic.dataclasses.dataclass()\n    class X:\n        y: Union['Y', None]\n\n    @pydantic.dataclasses.dataclass()\n    class Y:\n        x: Union[X, None]\n\n    assert Y(x={'y': None}).x.y is None\n", "tests/test_deprecated.py": "import platform\nimport re\nfrom datetime import date, timedelta\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom typing import Any, Dict, Iterable, List, Type\n\nimport pytest\nfrom pydantic_core import CoreSchema, core_schema\nfrom typing_extensions import Literal\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    GetCoreSchemaHandler,\n    GetJsonSchemaHandler,\n    PydanticDeprecatedSince20,\n    PydanticUserError,\n    ValidationError,\n    conlist,\n    root_validator,\n)\nfrom pydantic.config import Extra\nfrom pydantic.deprecated.decorator import validate_arguments\nfrom pydantic.deprecated.json import custom_pydantic_encoder, pydantic_encoder, timedelta_isoformat\nfrom pydantic.deprecated.parse import load_file, load_str_bytes\nfrom pydantic.deprecated.tools import parse_obj_as, schema_json_of, schema_of\nfrom pydantic.functional_serializers import model_serializer\nfrom pydantic.json_schema import JsonSchemaValue\nfrom pydantic.type_adapter import TypeAdapter\n\n\ndef deprecated_from_orm(model_type: Type[BaseModel], obj: Any) -> Any:\n    with pytest.warns(\n        PydanticDeprecatedSince20,\n        match=re.escape(\n            \"The `from_orm` method is deprecated; set `model_config['from_attributes']=True` \"\n            'and use `model_validate` instead.'\n        ),\n    ):\n        return model_type.from_orm(obj)\n\n\ndef test_from_attributes_root():\n    class PokemonCls:\n        def __init__(self, *, en_name: str, jp_name: str):\n            self.en_name = en_name\n            self.jp_name = jp_name\n\n    class Pokemon(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n        en_name: str\n        jp_name: str\n\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='Pydantic V1 style `@root_validator` validators are deprecated.'\n    ):\n\n        class PokemonList(BaseModel):\n            root: List[Pokemon]\n\n            @root_validator(pre=True)\n            @classmethod\n            def populate_root(cls, values):\n                return {'root': values}\n\n            @model_serializer(mode='wrap')\n            def _serialize(self, handler, info):\n                data = handler(self)\n                if info.mode == 'json':\n                    return data['root']\n                else:\n                    return data\n\n            @classmethod\n            def model_modify_json_schema(cls, json_schema):\n                return json_schema['properties']['root']\n\n            model_config = ConfigDict(from_attributes=True)\n\n    pika = PokemonCls(en_name='Pikachu', jp_name='\u30d4\u30ab\u30c1\u30e5\u30a6')\n    bulbi = PokemonCls(en_name='Bulbasaur', jp_name='\u30d5\u30b7\u30ae\u30c0\u30cd')\n\n    pokemons = deprecated_from_orm(PokemonList, [pika, bulbi])\n    assert pokemons.root == [\n        Pokemon(en_name='Pikachu', jp_name='\u30d4\u30ab\u30c1\u30e5\u30a6'),\n        Pokemon(en_name='Bulbasaur', jp_name='\u30d5\u30b7\u30ae\u30c0\u30cd'),\n    ]\n\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='Pydantic V1 style `@root_validator` validators are deprecated.'\n    ):\n\n        class PokemonDict(BaseModel):\n            root: Dict[str, Pokemon]\n            model_config = ConfigDict(from_attributes=True)\n\n            @root_validator(pre=True)\n            @classmethod\n            def populate_root(cls, values):\n                return {'root': values}\n\n            @model_serializer(mode='wrap')\n            def _serialize(self, handler, info):\n                data = handler(self)\n                if info.mode == 'json':\n                    return data['root']\n                else:\n                    return data\n\n            @classmethod\n            def model_modify_json_schema(cls, json_schema):\n                return json_schema['properties']['root']\n\n    pokemons = deprecated_from_orm(PokemonDict, {'pika': pika, 'bulbi': bulbi})\n    assert pokemons.root == {\n        'pika': Pokemon(en_name='Pikachu', jp_name='\u30d4\u30ab\u30c1\u30e5\u30a6'),\n        'bulbi': Pokemon(en_name='Bulbasaur', jp_name='\u30d5\u30b7\u30ae\u30c0\u30cd'),\n    }\n\n\ndef test_from_attributes():\n    class PetCls:\n        def __init__(self, *, name: str, species: str):\n            self.name = name\n            self.species = species\n\n    class PersonCls:\n        def __init__(self, *, name: str, age: float = None, pets: List[PetCls]):\n            self.name = name\n            self.age = age\n            self.pets = pets\n\n    class Pet(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n        name: str\n        species: str\n\n    class Person(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n        name: str\n        age: float = None\n        pets: List[Pet]\n\n    bones = PetCls(name='Bones', species='dog')\n    orion = PetCls(name='Orion', species='cat')\n    anna = PersonCls(name='Anna', age=20, pets=[bones, orion])\n\n    anna_model = deprecated_from_orm(Person, anna)\n\n    assert anna_model.model_dump() == {\n        'name': 'Anna',\n        'pets': [{'name': 'Bones', 'species': 'dog'}, {'name': 'Orion', 'species': 'cat'}],\n        'age': 20.0,\n    }\n\n\ndef test_not_from_attributes():\n    class Pet(BaseModel):\n        name: str\n        species: str\n\n    with pytest.raises(PydanticUserError):\n        deprecated_from_orm(Pet, None)\n\n\ndef test_object_with_getattr():\n    class FooGetAttr:\n        def __getattr__(self, key: str):\n            if key == 'foo':\n                return 'Foo'\n            else:\n                raise AttributeError\n\n    class Model(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n        foo: str\n        bar: int = 1\n\n    class ModelInvalid(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n        foo: str\n        bar: int\n\n    foo = FooGetAttr()\n    model = deprecated_from_orm(Model, foo)\n    assert model.foo == 'Foo'\n    assert model.bar == 1\n    assert model.model_dump(exclude_unset=True) == {'foo': 'Foo'}\n    with pytest.raises(ValidationError):\n        deprecated_from_orm(ModelInvalid, foo)\n\n\ndef test_properties():\n    class XyProperty:\n        x = 4\n\n        @property\n        def y(self):\n            return '5'\n\n    class Model(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n        x: int\n        y: int\n\n    model = deprecated_from_orm(Model, XyProperty())\n    assert model.x == 4\n    assert model.y == 5\n\n\n@pytest.mark.parametrize('extra', ['ignore', 'forbid', 'allow'])\ndef test_extra_allow_from_orm(extra: Literal['ignore', 'forbid', 'allow']):\n    class TestCls:\n        x = 1\n        y = 2\n\n    class Model(BaseModel):\n        model_config = ConfigDict(from_attributes=True, extra=extra)\n        x: int\n\n    model = deprecated_from_orm(Model, TestCls())\n    assert model.x == 1\n    assert not hasattr(model, 'y')\n\n\n@pytest.mark.filterwarnings('ignore:Pydantic V1 style `@root_validator` validators are deprecated.*:DeprecationWarning')\ndef test_root_validator():\n    validator_value = None\n\n    class TestCls:\n        x = 1\n        y = 2\n\n    class Model(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n        x: int\n        y: int\n        z: int\n\n        @root_validator(pre=True)\n        def change_input_data(cls, value):\n            nonlocal validator_value\n            validator_value = value\n            return {'x': value.x, 'y': value.y, 'z': value.x + value.y}\n\n    model = deprecated_from_orm(Model, TestCls())\n    assert model.model_dump() == {'x': 1, 'y': 2, 'z': 3}\n    # assert isinstance(validator_value, GetterDict)\n    assert isinstance(validator_value, TestCls)\n\n\ndef test_nested_orm():\n    class User(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n        first_name: str\n        last_name: str\n\n    class State(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n        user: User\n\n    # Pass an \"orm instance\"\n    deprecated_from_orm(State, SimpleNamespace(user=SimpleNamespace(first_name='John', last_name='Appleseed')))\n\n    # Pass dictionary data directly\n    State(**{'user': {'first_name': 'John', 'last_name': 'Appleseed'}})\n\n\ndef test_parse_raw_pass():\n    class Model(BaseModel):\n        x: int\n        y: int\n\n    with pytest.warns(PydanticDeprecatedSince20) as all_warnings:\n        model = Model.parse_raw('{\"x\": 1, \"y\": 2}')\n    assert model.model_dump() == {'x': 1, 'y': 2}\n    assert len(all_warnings) == 2\n    expected_warnings = [\n        'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead',\n        '`load_str_bytes` is deprecated',\n    ]\n    assert [w.message.message for w in all_warnings] == expected_warnings\n\n\n@pytest.mark.skipif(platform.python_implementation() == 'PyPy', reason='Different error str on PyPy')\ndef test_parse_raw_pass_fail():\n    class Model(BaseModel):\n        x: int\n        y: int\n\n    with pytest.warns(PydanticDeprecatedSince20) as all_warnings:\n        with pytest.raises(ValidationError, match='1 validation error for Model') as exc_info:\n            Model.parse_raw('invalid')\n    assert len(all_warnings) == 2\n    expected_warnings = [\n        'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead',\n        '`load_str_bytes` is deprecated',\n    ]\n    assert [w.message.message for w in all_warnings] == expected_warnings\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'value_error.jsondecode',\n            'loc': ('__root__',),\n            'msg': 'Expecting value: line 1 column 1 (char 0)',\n            'input': 'invalid',\n        }\n    ]\n\n\ndef test_fields():\n    class Model(BaseModel):\n        x: int\n        y: int = 2\n\n    m = Model(x=1)\n    assert len(Model.model_fields) == 2\n    assert len(m.model_fields) == 2\n    match = '^The `__fields__` attribute is deprecated, use `model_fields` instead.'\n    with pytest.warns(PydanticDeprecatedSince20, match=match):\n        assert len(Model.__fields__) == 2\n    with pytest.warns(PydanticDeprecatedSince20, match=match):\n        assert len(m.__fields__) == 2\n\n\ndef test_fields_set():\n    class Model(BaseModel):\n        x: int\n        y: int = 2\n\n    m = Model(x=1)\n    assert m.model_fields_set == {'x'}\n    match = '^The `__fields_set__` attribute is deprecated, use `model_fields_set` instead.'\n    with pytest.warns(PydanticDeprecatedSince20, match=match):\n        assert m.__fields_set__ == {'x'}\n\n\ndef test_fields_dir():\n    class Model(BaseModel):\n        x: int\n        y: int = 2\n\n    assert '__fields__' not in dir(Model)\n\n\n@pytest.mark.parametrize('attribute,value', [('allow', 'allow'), ('ignore', 'ignore'), ('forbid', 'forbid')])\ndef test_extra_used_as_enum(\n    attribute: str,\n    value: str,\n) -> None:\n    with pytest.warns(\n        PydanticDeprecatedSince20,\n        match=re.escape(\"`pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`)\"),\n    ):\n        assert getattr(Extra, attribute) == value\n\n\ndef test_field_min_items_deprecation():\n    m = '`min_items` is deprecated and will be removed. use `min_length` instead'\n    with pytest.warns(PydanticDeprecatedSince20, match=m):\n\n        class Model(BaseModel):\n            x: List[int] = Field(None, min_items=1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=[])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('x',),\n            'msg': 'List should have at least 1 item after validation, not 0',\n            'input': [],\n            'ctx': {'field_type': 'List', 'min_length': 1, 'actual_length': 0},\n        }\n    ]\n\n\ndef test_field_min_items_with_min_length():\n    m = '`min_items` is deprecated and will be removed. use `min_length` instead'\n    with pytest.warns(PydanticDeprecatedSince20, match=m):\n\n        class Model(BaseModel):\n            x: List[int] = Field(None, min_items=1, min_length=2)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=[1])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': ('x',),\n            'msg': 'List should have at least 2 items after validation, not 1',\n            'input': [1],\n            'ctx': {'field_type': 'List', 'min_length': 2, 'actual_length': 1},\n        }\n    ]\n\n\ndef test_field_max_items():\n    m = '`max_items` is deprecated and will be removed. use `max_length` instead'\n    with pytest.warns(PydanticDeprecatedSince20, match=m):\n\n        class Model(BaseModel):\n            x: List[int] = Field(None, max_items=1)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=[1, 2])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': ('x',),\n            'msg': 'List should have at most 1 item after validation, not 2',\n            'input': [1, 2],\n            'ctx': {'field_type': 'List', 'max_length': 1, 'actual_length': 2},\n        }\n    ]\n\n\ndef test_field_max_items_with_max_length():\n    m = '`max_items` is deprecated and will be removed. use `max_length` instead'\n    with pytest.warns(PydanticDeprecatedSince20, match=m):\n\n        class Model(BaseModel):\n            x: List[int] = Field(None, max_items=1, max_length=2)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x=[1, 2, 3])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': ('x',),\n            'msg': 'List should have at most 2 items after validation, not 3',\n            'input': [1, 2, 3],\n            'ctx': {'field_type': 'List', 'max_length': 2, 'actual_length': 3},\n        }\n    ]\n\n\ndef test_field_const():\n    with pytest.raises(PydanticUserError, match='`const` is removed. use `Literal` instead'):\n\n        class Model(BaseModel):\n            x: str = Field('test', const=True)\n\n\ndef test_field_include_deprecation():\n    with pytest.warns(PydanticDeprecatedSince20) as all_warnings:\n\n        class Model(BaseModel):\n            x: int = Field(include=True)\n\n    assert len(all_warnings) == 2\n    expected_warnings = [\n        \"Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'include')\",\n        '`include` is deprecated and does nothing. It will be removed, use `exclude` instead',\n    ]\n    assert [w.message.message for w in all_warnings] == expected_warnings\n\n\ndef test_unique_items_items():\n    with pytest.raises(PydanticUserError, match='`unique_items` is removed. use `Set` instead'):\n\n        class Model(BaseModel):\n            x: List[int] = Field(None, unique_items=True)\n\n\ndef test_unique_items_conlist():\n    with pytest.raises(PydanticUserError, match='`unique_items` is removed. use `Set` instead'):\n\n        class Model(BaseModel):\n            x: conlist(int, unique_items=True)\n\n\ndef test_allow_mutation():\n    m = '`allow_mutation` is deprecated and will be removed. use `frozen` instead'\n    with pytest.warns(PydanticDeprecatedSince20, match=m):\n\n        class Model(BaseModel):\n            model_config = ConfigDict(validate_assignment=True)\n            x: int = Field(allow_mutation=False)\n            y: int = Field(allow_mutation=True)\n\n    m = Model(x=1, y=2)\n\n    assert m.x == 1\n    with pytest.raises(ValidationError) as exc_info:\n        m.x = 2\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 2, 'loc': ('x',), 'msg': 'Field is frozen', 'type': 'frozen_field'}\n    ]\n\n    m.y = 3\n    assert m.y == 3\n\n\ndef test_field_regex():\n    with pytest.raises(PydanticUserError, match='`regex` is removed. use `pattern` instead'):\n\n        class Model(BaseModel):\n            x: str = Field('test', regex=r'^test$')\n\n\ndef test_modify_schema_error():\n    with pytest.raises(\n        PydanticUserError,\n        match='The `__modify_schema__` method is not supported in Pydantic v2. '\n        'Use `__get_pydantic_json_schema__` instead in class `Model`.',\n    ):\n\n        class Model(BaseModel):\n            @classmethod\n            def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n                pass\n\n\ndef test_modify_schema_on_nested_class_error() -> None:\n    class SomeLongName:\n        @classmethod\n        def __modify_schema__(cls, field_schema):\n            pass\n\n    with pytest.raises(\n        PydanticUserError,\n        match='The `__modify_schema__` method is not supported in Pydantic v2. '\n        'Use `__get_pydantic_json_schema__` instead in class `SomeLongName`.',\n    ):\n\n        class B(BaseModel):\n            model_config = ConfigDict(arbitrary_types_allowed=True)\n\n            a: SomeLongName\n\n\ndef test_v1_v2_custom_type_compatibility() -> None:\n    \"\"\"Create a custom type that works with V1 and V2\"\"\"\n\n    class MyType:\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.int_schema()\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            return {'anyOf': [{'type': 'string'}, {'type': 'number'}]}\n\n        @classmethod\n        def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n            raise NotImplementedError  # not actually called, we just want to make sure the method can exist\n\n        @classmethod\n        def __get_validators__(cls) -> Iterable[Any]:\n            raise NotImplementedError  # not actually called, we just want to make sure the method can exist\n            yield\n\n    ta = TypeAdapter(MyType)\n    assert ta.validate_python('123') == 123\n    assert ta.json_schema() == {'anyOf': [{'type': 'string'}, {'type': 'number'}]}\n\n\ndef test_v1_get_validators():\n    class CustomDate(date):\n        @classmethod\n        def __get_validators__(cls):\n            yield cls.validate1\n            yield cls.validate2\n\n        @classmethod\n        def validate1(cls, v, i):\n            print(v)\n\n            if v.year < 2000:\n                raise ValueError('Invalid year')\n            return v\n\n        @classmethod\n        def validate2(cls, v, i):\n            return date.today().replace(month=1, day=1)\n\n    with pytest.warns(\n        PydanticDeprecatedSince20,\n        match='^`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n    ):\n\n        class Model(BaseModel):\n            x: CustomDate\n\n    with pytest.raises(ValidationError, match='Value error, Invalid year'):\n        Model(x=date(1999, 1, 1))\n\n    m = Model(x=date.today())\n    assert m.x.day == 1\n\n\ndef test_v1_get_validators_invalid_validator():\n    class InvalidValidator:\n        @classmethod\n        def __get_validators__(cls):\n            yield cls.has_wrong_arguments\n\n        @classmethod\n        def has_wrong_arguments(cls):\n            pass\n\n    with pytest.warns(\n        PydanticDeprecatedSince20,\n        match='^`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n    ):\n\n        class InvalidValidatorModel(BaseModel):\n            x: InvalidValidator\n\n    with pytest.raises(TypeError, match='takes 1 positional argument but 3 were given'):\n        InvalidValidatorModel(x=1)\n\n\ndef test_field_extra_arguments():\n    m = re.escape(\n        'Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. '\n        \"(Extra keys: 'test', 'foo')\"\n    )\n    with pytest.warns(PydanticDeprecatedSince20, match=m):\n\n        class Model(BaseModel):\n            x: str = Field('test', test='test', foo='bar')\n\n    assert Model.model_json_schema(by_alias=True)['properties'] == {\n        'x': {'default': 'test', 'foo': 'bar', 'test': 'test', 'title': 'X', 'type': 'string'}\n    }\n\n\ndef test_field_extra_does_not_rewrite_json_schema_extra():\n    m = 'Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead'\n    with pytest.warns(PydanticDeprecatedSince20, match=m):\n\n        class Model(BaseModel):\n            x: str = Field('test', test='test', json_schema_extra={'test': 'json_schema_extra value'})\n\n    assert Model.model_json_schema(by_alias=True)['properties'] == {\n        'x': {'default': 'test', 'test': 'json_schema_extra value', 'title': 'X', 'type': 'string'}\n    }\n\n\nclass SimpleModel(BaseModel):\n    x: int\n\n\ndef test_dict():\n    m = SimpleModel(x=1)\n    with pytest.warns(PydanticDeprecatedSince20, match=r'^The `dict` method is deprecated; use `model_dump` instead\\.'):\n        assert m.dict() == {'x': 1}\n\n\ndef test_json():\n    m = SimpleModel(x=1)\n    with pytest.warns(\n        PydanticDeprecatedSince20, match=r'^The `json` method is deprecated; use `model_dump_json` instead\\.'\n    ):\n        assert m.json() == '{\"x\":1}'\n\n    with pytest.warns(PydanticDeprecatedSince20):\n        with pytest.raises(TypeError, match='The `encoder` argument is no longer supported'):\n            m.json(encoder=1)\n        with pytest.raises(TypeError, match='The `models_as_dict` argument is no longer supported'):\n            m.json(models_as_dict=True)\n        with pytest.raises(TypeError, match='`dumps_kwargs` keyword arguments are no longer supported.'):\n            m.json(foo=4)\n\n\ndef test_parse_obj():\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='^The `parse_obj` method is deprecated; use `model_validate` instead.'\n    ):\n        m = SimpleModel.parse_obj({'x': 1})\n\n    assert m.model_dump() == {'x': 1}\n\n\ndef test_parse_file(tmp_path):\n    path = tmp_path / 'test.json'\n    path.write_text('{\"x\": 12}')\n    with pytest.warns(PydanticDeprecatedSince20) as all_warnings:\n        assert SimpleModel.parse_file(str(path)).model_dump() == {'x': 12}\n    assert len(all_warnings) == 4\n    expected_warnings = [\n        'The `parse_file` method is deprecated; load the data from file, then if your data is JSON use `model_validate_json`, otherwise `model_validate` instead',\n        '`load_file` is deprecated',\n        '`load_str_bytes` is deprecated',\n        'The `parse_obj` method is deprecated; use `model_validate` instead',\n    ]\n    assert [w.message.message for w in all_warnings] == expected_warnings\n\n\ndef test_construct():\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='The `construct` method is deprecated; use `model_construct` instead.'\n    ):\n        m = SimpleModel.construct(x='not an int')\n\n    assert m.x == 'not an int'\n\n\ndef test_json_schema():\n    m = SimpleModel(x=1)\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='^The `schema` method is deprecated; use `model_json_schema` instead.'\n    ):\n        assert m.schema() == {\n            'title': 'SimpleModel',\n            'type': 'object',\n            'properties': {'x': {'title': 'X', 'type': 'integer'}},\n            'required': ['x'],\n        }\n\n\ndef test_validate():\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='^The `validate` method is deprecated; use `model_validate` instead.'\n    ):\n        m = SimpleModel.validate({'x': 1})\n\n    assert m.model_dump() == {'x': 1}\n\n\ndef test_update_forward_refs():\n    with pytest.warns(PydanticDeprecatedSince20, match='^The `update_forward_refs` method is deprecated;'):\n        SimpleModel.update_forward_refs()\n\n\ndef test_copy_and_set_values():\n    m = SimpleModel(x=1)\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='^The private method `_copy_and_set_values` will be removed and '\n    ):\n        m2 = m._copy_and_set_values(values={'x': 2}, fields_set={'x'}, deep=False)\n\n    assert m2.x == 2\n\n\ndef test_get_value():\n    m = SimpleModel(x=1)\n    with pytest.warns(PydanticDeprecatedSince20, match='^The private method `_get_value` will be removed and '):\n        v = m._get_value(\n            [1, 2, 3],\n            to_dict=False,\n            by_alias=False,\n            include=None,\n            exclude=None,\n            exclude_unset=False,\n            exclude_defaults=False,\n            exclude_none=False,\n        )\n    assert v == [1, 2, 3]\n\n\ndef test_deprecated_module(tmp_path: Path) -> None:\n    class Model(BaseModel):\n        x: int\n\n    with pytest.warns(PydanticDeprecatedSince20) as all_warnings:\n        assert hasattr(parse_obj_as, '__deprecated__')\n        parse_obj_as(Model, {'x': 1})\n        assert hasattr(schema_json_of, '__deprecated__')\n        schema_json_of(Model)\n        assert hasattr(schema_of, '__deprecated__')\n        schema_of(Model)\n        assert hasattr(load_str_bytes, '__deprecated__')\n        load_str_bytes('{\"x\": 1}')\n        assert hasattr(load_file, '__deprecated__')\n        file = tmp_path / 'main.py'\n        file.write_text('{\"x\": 1}')\n        load_file(file)\n        assert hasattr(pydantic_encoder, '__deprecated__')\n        pydantic_encoder(Model(x=1))\n        assert hasattr(custom_pydantic_encoder, '__deprecated__')\n        custom_pydantic_encoder({int: lambda x: str(x)}, Model(x=1))\n        assert hasattr(timedelta_isoformat, '__deprecated__')\n        timedelta_isoformat(timedelta(seconds=1))\n\n        def test(a: int, b: int):\n            pass\n\n        validate_arguments()(test)\n    assert len(all_warnings) == 12\n    expected_warnings = [\n        '`parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead',\n        '`schema_json_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead',\n        '`schema_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead',\n        '`schema_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead',\n        '`load_str_bytes` is deprecated',\n        '`load_file` is deprecated',\n        '`load_str_bytes` is deprecated',\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead',\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead',\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead',\n        '`timedelta_isoformat` is deprecated',\n        'The `validate_arguments` method is deprecated; use `validate_call` instead',\n    ]\n    assert [w.message.message for w in all_warnings] == expected_warnings\n\n\ndef test_deprecated_color():\n    from pydantic.color import Color\n\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='The `Color` class is deprecated, use `pydantic_extra_types` instead.'\n    ):\n        Color('red')\n\n\ndef test_deprecated_payment():\n    from pydantic import PaymentCardNumber\n\n    with pytest.warns(\n        PydanticDeprecatedSince20,\n        match='The `PaymentCardNumber` class is deprecated, use `pydantic_extra_types` instead.',\n    ):\n        PaymentCardNumber('4242424242424242')\n", "tests/test_json_schema.py": "import dataclasses\nimport importlib.metadata\nimport json\nimport math\nimport re\nimport sys\nimport typing\nfrom datetime import date, datetime, time, timedelta\nfrom decimal import Decimal\nfrom enum import Enum, IntEnum\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Callable,\n    Deque,\n    Dict,\n    FrozenSet,\n    Generic,\n    Iterable,\n    List,\n    NamedTuple,\n    NewType,\n    Optional,\n    Pattern,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\nfrom uuid import UUID\n\nimport pytest\nfrom dirty_equals import HasRepr\nfrom packaging.version import Version\nfrom pydantic_core import CoreSchema, SchemaValidator, core_schema, to_json\nfrom typing_extensions import Annotated, Literal, Self, TypedDict, deprecated\n\nimport pydantic\nfrom pydantic import (\n    AfterValidator,\n    BaseModel,\n    Field,\n    GetCoreSchemaHandler,\n    GetJsonSchemaHandler,\n    ImportString,\n    InstanceOf,\n    PlainSerializer,\n    PydanticDeprecatedSince20,\n    PydanticUserError,\n    RootModel,\n    ValidationError,\n    WithJsonSchema,\n    computed_field,\n    field_serializer,\n    field_validator,\n)\nfrom pydantic._internal._core_metadata import CoreMetadataHandler, build_metadata_dict\nfrom pydantic.color import Color\nfrom pydantic.config import ConfigDict\nfrom pydantic.dataclasses import dataclass\nfrom pydantic.errors import PydanticInvalidForJsonSchema\nfrom pydantic.json_schema import (\n    DEFAULT_REF_TEMPLATE,\n    Examples,\n    GenerateJsonSchema,\n    JsonSchemaValue,\n    PydanticJsonSchemaWarning,\n    SkipJsonSchema,\n    model_json_schema,\n    models_json_schema,\n)\nfrom pydantic.networks import AnyUrl, EmailStr, IPvAnyAddress, IPvAnyInterface, IPvAnyNetwork, MultiHostUrl, NameEmail\nfrom pydantic.type_adapter import TypeAdapter\nfrom pydantic.types import (\n    UUID1,\n    UUID3,\n    UUID4,\n    UUID5,\n    ByteSize,\n    DirectoryPath,\n    FilePath,\n    Json,\n    NegativeFloat,\n    NegativeInt,\n    NewPath,\n    NonNegativeFloat,\n    NonNegativeInt,\n    NonPositiveFloat,\n    NonPositiveInt,\n    PositiveFloat,\n    PositiveInt,\n    SecretBytes,\n    SecretStr,\n    StrictBool,\n    StrictStr,\n    conbytes,\n    condate,\n    condecimal,\n    confloat,\n    conint,\n    constr,\n)\n\ntry:\n    import email_validator\nexcept ImportError:\n    email_validator = None\n\nT = TypeVar('T')\n\n\ndef test_by_alias():\n    class ApplePie(BaseModel):\n        model_config = ConfigDict(title='Apple Pie')\n        a: float = Field(alias='Snap')\n        b: int = Field(10, alias='Crackle')\n\n    assert ApplePie.model_json_schema() == {\n        'type': 'object',\n        'title': 'Apple Pie',\n        'properties': {\n            'Snap': {'type': 'number', 'title': 'Snap'},\n            'Crackle': {'type': 'integer', 'title': 'Crackle', 'default': 10},\n        },\n        'required': ['Snap'],\n    }\n    assert list(ApplePie.model_json_schema(by_alias=True)['properties'].keys()) == ['Snap', 'Crackle']\n    assert list(ApplePie.model_json_schema(by_alias=False)['properties'].keys()) == ['a', 'b']\n\n\ndef test_ref_template():\n    class KeyLimePie(BaseModel):\n        x: str = None\n\n    class ApplePie(BaseModel):\n        model_config = ConfigDict(title='Apple Pie')\n        a: float = None\n        key_lime: Optional[KeyLimePie] = None\n\n    assert ApplePie.model_json_schema(ref_template='foobar/{model}.json') == {\n        'title': 'Apple Pie',\n        'type': 'object',\n        'properties': {\n            'a': {'default': None, 'title': 'A', 'type': 'number'},\n            'key_lime': {\n                'anyOf': [{'$ref': 'foobar/KeyLimePie.json'}, {'type': 'null'}],\n                'default': None,\n            },\n        },\n        '$defs': {\n            'KeyLimePie': {\n                'title': 'KeyLimePie',\n                'type': 'object',\n                'properties': {'x': {'default': None, 'title': 'X', 'type': 'string'}},\n            }\n        },\n    }\n    assert ApplePie.model_json_schema()['properties']['key_lime'] == {\n        'anyOf': [{'$ref': '#/$defs/KeyLimePie'}, {'type': 'null'}],\n        'default': None,\n    }\n    json_schema = json.dumps(ApplePie.model_json_schema(ref_template='foobar/{model}.json'))\n    assert 'foobar/KeyLimePie.json' in json_schema\n    assert '#/$defs/KeyLimePie' not in json_schema\n\n\ndef test_by_alias_generator():\n    class ApplePie(BaseModel):\n        model_config = ConfigDict(alias_generator=lambda x: x.upper())\n        a: float\n        b: int = 10\n\n    assert ApplePie.model_json_schema() == {\n        'title': 'ApplePie',\n        'type': 'object',\n        'properties': {'A': {'title': 'A', 'type': 'number'}, 'B': {'title': 'B', 'default': 10, 'type': 'integer'}},\n        'required': ['A'],\n    }\n    assert ApplePie.model_json_schema(by_alias=False)['properties'].keys() == {'a', 'b'}\n\n\ndef test_sub_model():\n    class Foo(BaseModel):\n        \"\"\"hello\"\"\"\n\n        b: float\n\n    class Bar(BaseModel):\n        a: int\n        b: Optional[Foo] = None\n\n    assert Bar.model_json_schema() == {\n        'type': 'object',\n        'title': 'Bar',\n        '$defs': {\n            'Foo': {\n                'type': 'object',\n                'title': 'Foo',\n                'description': 'hello',\n                'properties': {'b': {'type': 'number', 'title': 'B'}},\n                'required': ['b'],\n            }\n        },\n        'properties': {\n            'a': {'type': 'integer', 'title': 'A'},\n            'b': {'anyOf': [{'$ref': '#/$defs/Foo'}, {'type': 'null'}], 'default': None},\n        },\n        'required': ['a'],\n    }\n\n\ndef test_schema_class():\n    class Model(BaseModel):\n        foo: int = Field(4, title='Foo is Great')\n        bar: str = Field(..., description='this description of bar')\n\n    with pytest.raises(ValidationError):\n        Model()\n\n    m = Model(bar='123')\n    assert m.model_dump() == {'foo': 4, 'bar': '123'}\n\n    assert Model.model_json_schema() == {\n        'type': 'object',\n        'title': 'Model',\n        'properties': {\n            'foo': {'type': 'integer', 'title': 'Foo is Great', 'default': 4},\n            'bar': {'type': 'string', 'title': 'Bar', 'description': 'this description of bar'},\n        },\n        'required': ['bar'],\n    }\n\n\ndef test_schema_repr():\n    s = Field(4, title='Foo is Great')\n    assert str(s) == \"annotation=NoneType required=False default=4 title='Foo is Great'\"\n    assert repr(s) == \"FieldInfo(annotation=NoneType, required=False, default=4, title='Foo is Great')\"\n\n\ndef test_schema_class_by_alias():\n    class Model(BaseModel):\n        foo: int = Field(4, alias='foofoo')\n\n    assert list(Model.model_json_schema()['properties'].keys()) == ['foofoo']\n    assert list(Model.model_json_schema(by_alias=False)['properties'].keys()) == ['foo']\n\n\ndef test_choices():\n    FooEnum = Enum('FooEnum', {'foo': 'f', 'bar': 'b'})\n    BarEnum = IntEnum('BarEnum', {'foo': 1, 'bar': 2})\n\n    class SpamEnum(str, Enum):\n        foo = 'f'\n        bar = 'b'\n\n    class Model(BaseModel):\n        foo: FooEnum\n        bar: BarEnum\n        spam: SpamEnum = Field(None)\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'BarEnum': {'enum': [1, 2], 'title': 'BarEnum', 'type': 'integer'},\n            'FooEnum': {'enum': ['f', 'b'], 'title': 'FooEnum', 'type': 'string'},\n            'SpamEnum': {'enum': ['f', 'b'], 'title': 'SpamEnum', 'type': 'string'},\n        },\n        'properties': {\n            'foo': {'$ref': '#/$defs/FooEnum'},\n            'bar': {'$ref': '#/$defs/BarEnum'},\n            'spam': {'allOf': [{'$ref': '#/$defs/SpamEnum'}], 'default': None},\n        },\n        'required': ['foo', 'bar'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_enum_modify_schema():\n    class SpamEnum(str, Enum):\n        \"\"\"\n        Spam enum.\n        \"\"\"\n\n        foo = 'f'\n        bar = 'b'\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = handler(core_schema)\n            field_schema = handler.resolve_ref_schema(field_schema)\n            existing_comment = field_schema.get('$comment', '')\n            field_schema['$comment'] = existing_comment + 'comment'  # make sure this function is only called once\n            field_schema['tsEnumNames'] = [e.name for e in cls]\n            return field_schema\n\n    class Model(BaseModel):\n        spam: Optional[SpamEnum] = Field(None)\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'SpamEnum': {\n                '$comment': 'comment',\n                'description': 'Spam enum.',\n                'enum': ['f', 'b'],\n                'title': 'SpamEnum',\n                'tsEnumNames': ['foo', 'bar'],\n                'type': 'string',\n            }\n        },\n        'properties': {'spam': {'anyOf': [{'$ref': '#/$defs/SpamEnum'}, {'type': 'null'}], 'default': None}},\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_enum_schema_custom_field():\n    class FooBarEnum(str, Enum):\n        foo = 'foo'\n        bar = 'bar'\n\n    class Model(BaseModel):\n        pika: FooBarEnum = Field(alias='pikalias', title='Pikapika!', description='Pika is definitely the best!')\n        bulbi: FooBarEnum = Field('foo', alias='bulbialias', title='Bulbibulbi!', description='Bulbi is not...')\n        cara: FooBarEnum\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        'type': 'object',\n        'properties': {\n            'pikalias': {\n                'title': 'Pikapika!',\n                'description': 'Pika is definitely the best!',\n                'allOf': [{'$ref': '#/$defs/FooBarEnum'}],\n            },\n            'bulbialias': {\n                'allOf': [{'$ref': '#/$defs/FooBarEnum'}],\n                'default': 'foo',\n                'title': 'Bulbibulbi!',\n                'description': 'Bulbi is not...',\n            },\n            'cara': {'$ref': '#/$defs/FooBarEnum'},\n        },\n        'required': ['pikalias', 'cara'],\n        'title': 'Model',\n        '$defs': {'FooBarEnum': {'enum': ['foo', 'bar'], 'title': 'FooBarEnum', 'type': 'string'}},\n    }\n\n\ndef test_enum_and_model_have_same_behaviour():\n    class Names(str, Enum):\n        rick = 'Rick'\n        morty = 'Morty'\n        summer = 'Summer'\n\n    class Pika(BaseModel):\n        a: str\n\n    class Foo(BaseModel):\n        enum: Names\n        titled_enum: Names = Field(\n            ...,\n            title='Title of enum',\n            description='Description of enum',\n        )\n        model: Pika\n        titled_model: Pika = Field(\n            ...,\n            title='Title of model',\n            description='Description of model',\n        )\n\n    # insert_assert(Foo.model_json_schema())\n    assert Foo.model_json_schema() == {\n        'type': 'object',\n        'properties': {\n            'enum': {'$ref': '#/$defs/Names'},\n            'titled_enum': {\n                'title': 'Title of enum',\n                'description': 'Description of enum',\n                'allOf': [{'$ref': '#/$defs/Names'}],\n            },\n            'model': {'$ref': '#/$defs/Pika'},\n            'titled_model': {\n                'title': 'Title of model',\n                'description': 'Description of model',\n                'allOf': [{'$ref': '#/$defs/Pika'}],\n            },\n        },\n        'required': ['enum', 'titled_enum', 'model', 'titled_model'],\n        'title': 'Foo',\n        '$defs': {\n            'Names': {'enum': ['Rick', 'Morty', 'Summer'], 'title': 'Names', 'type': 'string'},\n            'Pika': {\n                'type': 'object',\n                'properties': {'a': {'type': 'string', 'title': 'A'}},\n                'required': ['a'],\n                'title': 'Pika',\n            },\n        },\n    }\n\n\ndef test_enum_includes_extra_without_other_params():\n    class Names(str, Enum):\n        rick = 'Rick'\n        morty = 'Morty'\n        summer = 'Summer'\n\n    class Foo(BaseModel):\n        enum: Names\n        extra_enum: Names = Field(..., json_schema_extra={'extra': 'Extra field'})\n\n    assert Foo.model_json_schema() == {\n        '$defs': {\n            'Names': {\n                'enum': ['Rick', 'Morty', 'Summer'],\n                'title': 'Names',\n                'type': 'string',\n            },\n        },\n        'properties': {\n            'enum': {'$ref': '#/$defs/Names'},\n            'extra_enum': {'allOf': [{'$ref': '#/$defs/Names'}], 'extra': 'Extra field'},\n        },\n        'required': ['enum', 'extra_enum'],\n        'title': 'Foo',\n        'type': 'object',\n    }\n\n\ndef test_invalid_json_schema_extra():\n    class MyModel(BaseModel):\n        model_config = ConfigDict(json_schema_extra=1)\n\n        name: str\n\n    with pytest.raises(\n        ValueError, match=re.escape(\"model_config['json_schema_extra']=1 should be a dict, callable, or None\")\n    ):\n        MyModel.model_json_schema()\n\n\ndef test_list_enum_schema_extras():\n    class FoodChoice(str, Enum):\n        spam = 'spam'\n        egg = 'egg'\n        chips = 'chips'\n\n    class Model(BaseModel):\n        foods: List[FoodChoice] = Field(examples=[['spam', 'egg']])\n\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'FoodChoice': {\n                'enum': ['spam', 'egg', 'chips'],\n                'title': 'FoodChoice',\n                'type': 'string',\n            }\n        },\n        'properties': {\n            'foods': {\n                'title': 'Foods',\n                'type': 'array',\n                'items': {'$ref': '#/$defs/FoodChoice'},\n                'examples': [['spam', 'egg']],\n            },\n        },\n        'required': ['foods'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_enum_schema_cleandoc():\n    class FooBar(str, Enum):\n        \"\"\"\n        This is docstring which needs to be cleaned up\n        \"\"\"\n\n        foo = 'foo'\n        bar = 'bar'\n\n    class Model(BaseModel):\n        enum: FooBar\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'enum': {'$ref': '#/$defs/FooBar'}},\n        'required': ['enum'],\n        '$defs': {\n            'FooBar': {\n                'title': 'FooBar',\n                'description': 'This is docstring which needs to be cleaned up',\n                'enum': ['foo', 'bar'],\n                'type': 'string',\n            }\n        },\n    }\n\n\ndef test_decimal_json_schema():\n    class Model(BaseModel):\n        a: bytes = b'foobar'\n        b: Decimal = Decimal('12.34')\n\n    model_json_schema_validation = Model.model_json_schema(mode='validation')\n    model_json_schema_serialization = Model.model_json_schema(mode='serialization')\n\n    assert model_json_schema_validation == {\n        'properties': {\n            'a': {'default': 'foobar', 'format': 'binary', 'title': 'A', 'type': 'string'},\n            'b': {'anyOf': [{'type': 'number'}, {'type': 'string'}], 'default': '12.34', 'title': 'B'},\n        },\n        'title': 'Model',\n        'type': 'object',\n    }\n    assert model_json_schema_serialization == {\n        'properties': {\n            'a': {'default': 'foobar', 'format': 'binary', 'title': 'A', 'type': 'string'},\n            'b': {'default': '12.34', 'title': 'B', 'type': 'string'},\n        },\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_list_sub_model():\n    class Foo(BaseModel):\n        a: float\n\n    class Bar(BaseModel):\n        b: List[Foo]\n\n    assert Bar.model_json_schema() == {\n        'title': 'Bar',\n        'type': 'object',\n        '$defs': {\n            'Foo': {\n                'title': 'Foo',\n                'type': 'object',\n                'properties': {'a': {'type': 'number', 'title': 'A'}},\n                'required': ['a'],\n            }\n        },\n        'properties': {'b': {'type': 'array', 'items': {'$ref': '#/$defs/Foo'}, 'title': 'B'}},\n        'required': ['b'],\n    }\n\n\ndef test_optional():\n    class Model(BaseModel):\n        a: Optional[str]\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'A'}},\n        'required': ['a'],\n    }\n\n\ndef test_optional_modify_schema():\n    class MyNone(Type[None]):\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source_type: Any, handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            return core_schema.nullable_schema(core_schema.none_schema())\n\n    class Model(BaseModel):\n        x: MyNone\n\n    assert Model.model_json_schema() == {\n        'properties': {'x': {'title': 'X', 'type': 'null'}},\n        'required': ['x'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_any():\n    class Model(BaseModel):\n        a: Any\n        b: object\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'a': {'title': 'A'},\n            'b': {'title': 'B'},\n        },\n        'required': ['a', 'b'],\n    }\n\n\ndef test_set():\n    class Model(BaseModel):\n        a: Set[int]\n        b: set\n        c: set = {1}\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'a': {'title': 'A', 'type': 'array', 'uniqueItems': True, 'items': {'type': 'integer'}},\n            'b': {'title': 'B', 'type': 'array', 'items': {}, 'uniqueItems': True},\n            'c': {'title': 'C', 'type': 'array', 'items': {}, 'default': [1], 'uniqueItems': True},\n        },\n        'required': ['a', 'b'],\n    }\n\n\n@pytest.mark.parametrize(\n    'field_type,extra_props',\n    [\n        pytest.param(tuple, {'items': {}}, id='tuple'),\n        pytest.param(Tuple, {'items': {}}, id='Tuple'),\n        pytest.param(\n            Tuple[str, int, Union[str, int, float], float],\n            {\n                'prefixItems': [\n                    {'type': 'string'},\n                    {'type': 'integer'},\n                    {'anyOf': [{'type': 'string'}, {'type': 'integer'}, {'type': 'number'}]},\n                    {'type': 'number'},\n                ],\n                'minItems': 4,\n                'maxItems': 4,\n            },\n            id='Tuple[str, int, Union[str, int, float], float]',\n        ),\n        pytest.param(Tuple[str], {'prefixItems': [{'type': 'string'}], 'minItems': 1, 'maxItems': 1}, id='Tuple[str]'),\n        pytest.param(Tuple[()], {'maxItems': 0, 'minItems': 0}, id='Tuple[()]'),\n        pytest.param(Tuple[str, ...], {'items': {'type': 'string'}, 'type': 'array'}, id='Tuple[str, ...]'),\n    ],\n)\ndef test_tuple(field_type, extra_props):\n    class Model(BaseModel):\n        a: field_type\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'array', **extra_props}},\n        'required': ['a'],\n    }\n\n    ta = TypeAdapter(field_type)\n\n    assert ta.json_schema() == {'type': 'array', **extra_props}\n\n\ndef test_deque():\n    class Model(BaseModel):\n        a: Deque[str]\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'array', 'items': {'type': 'string'}}},\n        'required': ['a'],\n    }\n\n\ndef test_bool():\n    class Model(BaseModel):\n        a: bool\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'boolean'}},\n        'required': ['a'],\n    }\n\n\ndef test_strict_bool():\n    class Model(BaseModel):\n        a: StrictBool\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'boolean'}},\n        'required': ['a'],\n    }\n\n\ndef test_dict():\n    class Model(BaseModel):\n        a: dict\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'object'}},\n        'required': ['a'],\n    }\n\n\ndef test_list():\n    class Model(BaseModel):\n        a: list\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'array', 'items': {}}},\n        'required': ['a'],\n    }\n\n\nclass Foo(BaseModel):\n    a: float\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        (\n            Union[int, str],\n            {\n                'properties': {'a': {'title': 'A', 'anyOf': [{'type': 'integer'}, {'type': 'string'}]}},\n                'required': ['a'],\n            },\n        ),\n        (\n            List[int],\n            {'properties': {'a': {'title': 'A', 'type': 'array', 'items': {'type': 'integer'}}}, 'required': ['a']},\n        ),\n        (\n            Dict[str, Foo],\n            {\n                '$defs': {\n                    'Foo': {\n                        'title': 'Foo',\n                        'type': 'object',\n                        'properties': {'a': {'title': 'A', 'type': 'number'}},\n                        'required': ['a'],\n                    }\n                },\n                'properties': {'a': {'title': 'A', 'type': 'object', 'additionalProperties': {'$ref': '#/$defs/Foo'}}},\n                'required': ['a'],\n            },\n        ),\n        (\n            Union[None, Foo],\n            {\n                '$defs': {\n                    'Foo': {\n                        'title': 'Foo',\n                        'type': 'object',\n                        'properties': {'a': {'title': 'A', 'type': 'number'}},\n                        'required': ['a'],\n                    }\n                },\n                'properties': {'a': {'anyOf': [{'$ref': '#/$defs/Foo'}, {'type': 'null'}]}},\n                'required': ['a'],\n                'title': 'Model',\n                'type': 'object',\n            },\n        ),\n        (\n            Union[int, int],\n            {'properties': {'a': {'title': 'A', 'type': 'integer'}}, 'required': ['a']},\n        ),\n        (Dict[str, Any], {'properties': {'a': {'title': 'A', 'type': 'object'}}, 'required': ['a']}),\n    ],\n)\ndef test_list_union_dict(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {'title': 'Model', 'type': 'object'}\n    base_schema.update(expected_schema)\n\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        (datetime, {'type': 'string', 'format': 'date-time'}),\n        (date, {'type': 'string', 'format': 'date'}),\n        (time, {'type': 'string', 'format': 'time'}),\n        (timedelta, {'type': 'string', 'format': 'duration'}),\n    ],\n)\ndef test_date_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    attribute_schema = {'title': 'A'}\n    attribute_schema.update(expected_schema)\n\n    base_schema = {'title': 'Model', 'type': 'object', 'properties': {'a': attribute_schema}, 'required': ['a']}\n\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        (condate(), {}),\n        (\n            condate(gt=date(2010, 1, 1), lt=date(2021, 2, 2)),\n            {'exclusiveMinimum': date(2010, 1, 1), 'exclusiveMaximum': date(2021, 2, 2)},\n        ),\n        (condate(ge=date(2010, 1, 1), le=date(2021, 2, 2)), {'minimum': date(2010, 1, 1), 'maximum': date(2021, 2, 2)}),\n    ],\n)\ndef test_date_constrained_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string', 'format': 'date', **expected_schema}},\n        'required': ['a'],\n    }\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        (Optional[str], {'properties': {'a': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'A'}}}),\n        (\n            Optional[bytes],\n            {'properties': {'a': {'title': 'A', 'anyOf': [{'type': 'string', 'format': 'binary'}, {'type': 'null'}]}}},\n        ),\n        (\n            Union[str, bytes],\n            {\n                'properties': {\n                    'a': {'title': 'A', 'anyOf': [{'type': 'string'}, {'type': 'string', 'format': 'binary'}]}\n                },\n            },\n        ),\n        (\n            Union[None, str, bytes],\n            {\n                'properties': {\n                    'a': {\n                        'title': 'A',\n                        'anyOf': [{'type': 'string'}, {'type': 'string', 'format': 'binary'}, {'type': 'null'}],\n                    }\n                }\n            },\n        ),\n    ],\n)\ndef test_str_basic_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {'title': 'Model', 'type': 'object', 'required': ['a']}\n    base_schema.update(expected_schema)\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        (Pattern, {'type': 'string', 'format': 'regex'}),\n        (Pattern[str], {'type': 'string', 'format': 'regex'}),\n        (Pattern[bytes], {'type': 'string', 'format': 'regex'}),\n    ],\n)\ndef test_pattern(field_type, expected_schema) -> None:\n    class Model(BaseModel):\n        a: field_type\n\n    expected_schema.update({'title': 'A'})\n    full_schema = {'title': 'Model', 'type': 'object', 'required': ['a'], 'properties': {'a': expected_schema}}\n    assert Model.model_json_schema() == full_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        (StrictStr, {'title': 'A', 'type': 'string'}),\n        # (ConstrainedStr, {'title': 'A', 'type': 'string'}),\n        (\n            constr(min_length=3, max_length=5, pattern='^text$'),\n            {'title': 'A', 'type': 'string', 'minLength': 3, 'maxLength': 5, 'pattern': '^text$'},\n        ),\n    ],\n)\ndef test_str_constrained_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    model_schema = Model.model_json_schema()\n    assert model_schema['properties']['a'] == expected_schema\n\n    base_schema = {'title': 'Model', 'type': 'object', 'properties': {'a': expected_schema}, 'required': ['a']}\n\n    assert model_schema == base_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        (AnyUrl, {'title': 'A', 'type': 'string', 'format': 'uri', 'minLength': 1}),\n        (\n            Annotated[AnyUrl, Field(max_length=2**16)],\n            {'title': 'A', 'type': 'string', 'format': 'uri', 'minLength': 1, 'maxLength': 2**16},\n        ),\n        (MultiHostUrl, {'title': 'A', 'type': 'string', 'format': 'multi-host-uri', 'minLength': 1}),\n    ],\n)\ndef test_special_str_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {'title': 'Model', 'type': 'object', 'properties': {'a': {}}, 'required': ['a']}\n    base_schema['properties']['a'] = expected_schema\n\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.skipif(not email_validator, reason='email_validator not installed')\n@pytest.mark.parametrize('field_type,expected_schema', [(EmailStr, 'email'), (NameEmail, 'name-email')])\ndef test_email_str_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n    }\n    base_schema['properties']['a']['format'] = expected_schema\n\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.parametrize('field_type,inner_type', [(SecretBytes, 'string'), (SecretStr, 'string')])\ndef test_secret_types(field_type, inner_type):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': inner_type, 'writeOnly': True, 'format': 'password'}},\n        'required': ['a'],\n    }\n\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        # (ConstrainedInt, {}),\n        (conint(gt=5, lt=10), {'exclusiveMinimum': 5, 'exclusiveMaximum': 10}),\n        (conint(ge=5, le=10), {'minimum': 5, 'maximum': 10}),\n        (conint(multiple_of=5), {'multipleOf': 5}),\n        (PositiveInt, {'exclusiveMinimum': 0}),\n        (NegativeInt, {'exclusiveMaximum': 0}),\n        (NonNegativeInt, {'minimum': 0}),\n        (NonPositiveInt, {'maximum': 0}),\n    ],\n)\ndef test_special_int_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'integer'}},\n        'required': ['a'],\n    }\n    base_schema['properties']['a'].update(expected_schema)\n\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        (confloat(gt=5, lt=10), {'exclusiveMinimum': 5, 'exclusiveMaximum': 10}),\n        (confloat(ge=5, le=10), {'minimum': 5, 'maximum': 10}),\n        (confloat(multiple_of=5), {'multipleOf': 5}),\n        (PositiveFloat, {'exclusiveMinimum': 0}),\n        (NegativeFloat, {'exclusiveMaximum': 0}),\n        (NonNegativeFloat, {'minimum': 0}),\n        (NonPositiveFloat, {'maximum': 0}),\n    ],\n)\ndef test_special_float_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'number'}},\n        'required': ['a'],\n    }\n    base_schema['properties']['a'].update(expected_schema)\n\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        (condecimal(gt=5, lt=10), {'exclusiveMinimum': 5, 'exclusiveMaximum': 10}),\n        (condecimal(ge=5, le=10), {'minimum': 5, 'maximum': 10}),\n        (condecimal(multiple_of=5), {'multipleOf': 5}),\n    ],\n)\ndef test_special_decimal_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'anyOf': [{'type': 'number'}, {'type': 'string'}], 'title': 'A'}},\n        'required': ['a'],\n    }\n    base_schema['properties']['a']['anyOf'][0].update(expected_schema)\n\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [(UUID, 'uuid'), (UUID1, 'uuid1'), (UUID3, 'uuid3'), (UUID4, 'uuid4'), (UUID5, 'uuid5')],\n)\ndef test_uuid_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string', 'format': 'uuid'}},\n        'required': ['a'],\n    }\n    base_schema['properties']['a']['format'] = expected_schema\n\n    assert Model.model_json_schema() == base_schema\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [(FilePath, 'file-path'), (DirectoryPath, 'directory-path'), (NewPath, 'path'), (Path, 'path')],\n)\ndef test_path_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string', 'format': ''}},\n        'required': ['a'],\n    }\n    base_schema['properties']['a']['format'] = expected_schema\n\n    assert Model.model_json_schema() == base_schema\n\n\ndef test_json_type():\n    class Model(BaseModel):\n        a: Json\n        b: Json[int]\n        c: Json[Any]\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'a': {'contentMediaType': 'application/json', 'contentSchema': {}, 'title': 'A', 'type': 'string'},\n            'b': {\n                'contentMediaType': 'application/json',\n                'contentSchema': {'type': 'integer'},\n                'title': 'B',\n                'type': 'string',\n            },\n            'c': {'contentMediaType': 'application/json', 'contentSchema': {}, 'title': 'C', 'type': 'string'},\n        },\n        'required': ['a', 'b', 'c'],\n        'title': 'Model',\n        'type': 'object',\n    }\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {'a': {'title': 'A'}, 'b': {'title': 'B', 'type': 'integer'}, 'c': {'title': 'C'}},\n        'required': ['a', 'b', 'c'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_ipv4address_type():\n    class Model(BaseModel):\n        ip_address: IPv4Address\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'ip_address': {'title': 'Ip Address', 'type': 'string', 'format': 'ipv4'}},\n        'required': ['ip_address'],\n    }\n\n\ndef test_ipv6address_type():\n    class Model(BaseModel):\n        ip_address: IPv6Address\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'ip_address': {'title': 'Ip Address', 'type': 'string', 'format': 'ipv6'}},\n        'required': ['ip_address'],\n    }\n\n\ndef test_ipvanyaddress_type():\n    class Model(BaseModel):\n        ip_address: IPvAnyAddress\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'ip_address': {'title': 'Ip Address', 'type': 'string', 'format': 'ipvanyaddress'}},\n        'required': ['ip_address'],\n    }\n\n\ndef test_ipv4interface_type():\n    class Model(BaseModel):\n        ip_interface: IPv4Interface\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'ip_interface': {'title': 'Ip Interface', 'type': 'string', 'format': 'ipv4interface'}},\n        'required': ['ip_interface'],\n    }\n\n\ndef test_ipv6interface_type():\n    class Model(BaseModel):\n        ip_interface: IPv6Interface\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'ip_interface': {'title': 'Ip Interface', 'type': 'string', 'format': 'ipv6interface'}},\n        'required': ['ip_interface'],\n    }\n\n\ndef test_ipvanyinterface_type():\n    class Model(BaseModel):\n        ip_interface: IPvAnyInterface\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'ip_interface': {'title': 'Ip Interface', 'type': 'string', 'format': 'ipvanyinterface'}},\n        'required': ['ip_interface'],\n    }\n\n\ndef test_ipv4network_type():\n    class Model(BaseModel):\n        ip_network: IPv4Network\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'ip_network': {'title': 'Ip Network', 'type': 'string', 'format': 'ipv4network'}},\n        'required': ['ip_network'],\n    }\n\n\ndef test_ipv6network_type():\n    class Model(BaseModel):\n        ip_network: IPv6Network\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'ip_network': {'title': 'Ip Network', 'type': 'string', 'format': 'ipv6network'}},\n        'required': ['ip_network'],\n    }\n\n\ndef test_ipvanynetwork_type():\n    class Model(BaseModel):\n        ip_network: IPvAnyNetwork\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'ip_network': {'title': 'Ip Network', 'type': 'string', 'format': 'ipvanynetwork'}},\n        'required': ['ip_network'],\n    }\n\n\n@pytest.mark.parametrize(\n    'type_,default_value',\n    (\n        (Callable, ...),\n        (Callable, lambda x: x),\n        (Callable[[int], int], ...),\n        (Callable[[int], int], lambda x: x),\n    ),\n)\n@pytest.mark.parametrize(\n    'base_json_schema,properties',\n    [\n        (\n            {'a': 'b'},\n            {\n                'callback': {'title': 'Callback', 'a': 'b'},\n                'foo': {'title': 'Foo', 'type': 'integer'},\n            },\n        ),\n        (\n            None,\n            {\n                'foo': {'title': 'Foo', 'type': 'integer'},\n            },\n        ),\n    ],\n)\ndef test_callable_type(type_, default_value, base_json_schema, properties):\n    class Model(BaseModel):\n        callback: type_ = default_value\n        foo: int\n\n    with pytest.raises(PydanticInvalidForJsonSchema):\n        Model.model_json_schema()\n\n    class ModelWithOverride(BaseModel):\n        callback: Annotated[type_, WithJsonSchema(base_json_schema)] = default_value\n        foo: int\n\n    if default_value is Ellipsis or base_json_schema is None:\n        model_schema = ModelWithOverride.model_json_schema()\n    else:\n        with pytest.warns(\n            PydanticJsonSchemaWarning,\n            match='Default value .* is not JSON serializable; excluding'\n            r' default from JSON schema \\[non-serializable-default]',\n        ):\n            model_schema = ModelWithOverride.model_json_schema()\n    assert model_schema['properties'] == properties\n\n\n@pytest.mark.parametrize(\n    'default_value,properties',\n    (\n        (Field(...), {'callback': {'title': 'Callback', 'type': 'integer'}}),\n        (1, {'callback': {'default': 1, 'title': 'Callback', 'type': 'integer'}}),\n    ),\n)\ndef test_callable_type_with_fallback(default_value, properties):\n    class Model(BaseModel):\n        callback: Union[int, Callable[[int], int]] = default_value\n\n    class MyGenerator(GenerateJsonSchema):\n        ignored_warning_kinds = ()\n\n    with pytest.warns(\n        PydanticJsonSchemaWarning,\n        match=re.escape('Cannot generate a JsonSchema for core_schema.CallableSchema [skipped-choice]'),\n    ):\n        model_schema = Model.model_json_schema(schema_generator=MyGenerator)\n    assert model_schema['properties'] == properties\n\n\ndef test_byte_size_type():\n    class Model(BaseModel):\n        a: ByteSize\n        b: ByteSize = Field('1MB', validate_default=True)\n\n    model_json_schema_validation = Model.model_json_schema(mode='validation')\n    model_json_schema_serialization = Model.model_json_schema(mode='serialization')\n\n    print(model_json_schema_serialization)\n\n    assert model_json_schema_validation == {\n        'properties': {\n            'a': {\n                'anyOf': [\n                    {'pattern': '^\\\\s*(\\\\d*\\\\.?\\\\d+)\\\\s*(\\\\w+)?', 'type': 'string'},\n                    {'minimum': 0, 'type': 'integer'},\n                ],\n                'title': 'A',\n            },\n            'b': {\n                'anyOf': [\n                    {'pattern': '^\\\\s*(\\\\d*\\\\.?\\\\d+)\\\\s*(\\\\w+)?', 'type': 'string'},\n                    {'minimum': 0, 'type': 'integer'},\n                ],\n                'default': '1MB',\n                'title': 'B',\n            },\n        },\n        'required': ['a'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n    assert model_json_schema_serialization == {\n        'properties': {\n            'a': {'minimum': 0, 'title': 'A', 'type': 'integer'},\n            'b': {'default': '1MB', 'minimum': 0, 'title': 'B', 'type': 'integer'},\n        },\n        'required': ['a'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    'type_,default_value,properties',\n    (\n        (\n            Dict[Any, Any],\n            {(lambda x: x): 1},\n            {'callback': {'title': 'Callback', 'type': 'object'}},\n        ),\n        (\n            Union[int, Callable[[int], int]],\n            lambda x: x,\n            {'callback': {'title': 'Callback', 'type': 'integer'}},\n        ),\n    ),\n)\ndef test_non_serializable_default(type_, default_value, properties):\n    class Model(BaseModel):\n        callback: type_ = default_value\n\n    with pytest.warns(\n        PydanticJsonSchemaWarning,\n        match=(\n            'Default value .* is not JSON serializable; excluding default from JSON schema '\n            r'\\[non-serializable-default\\]'\n        ),\n    ):\n        model_schema = Model.model_json_schema()\n    assert model_schema['properties'] == properties\n    assert model_schema.get('required') is None\n\n\ndef test_callable_fallback_with_non_serializable_default():\n    class Model(BaseModel):\n        callback: Union[int, Callable[[int], int]] = lambda x: x\n\n    class MyGenerator(GenerateJsonSchema):\n        ignored_warning_kinds = ()\n\n    inner_match = (\n        r'Default value .* is not JSON serializable; excluding default from JSON schema \\[non-serializable-default\\]'\n    )\n    outer_match = r'Cannot generate a JsonSchema for core_schema.CallableSchema \\[skipped-choice\\]'\n    with pytest.warns(PydanticJsonSchemaWarning, match=outer_match):\n        with pytest.warns(PydanticJsonSchemaWarning, match=inner_match):\n            model_schema = Model.model_json_schema(schema_generator=MyGenerator)\n    assert model_schema == {\n        'properties': {'callback': {'title': 'Callback', 'type': 'integer'}},\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_importstring_json_schema():\n    class Model(BaseModel):\n        a: ImportString\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n    }\n\n\ndef test_schema_overrides():\n    class Foo(BaseModel):\n        a: str\n\n    class Bar(BaseModel):\n        b: Foo = Foo(a='foo')\n\n    class Baz(BaseModel):\n        c: Optional[Bar]\n\n    class Model(BaseModel):\n        d: Baz\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        '$defs': {\n            'Foo': {\n                'title': 'Foo',\n                'type': 'object',\n                'properties': {'a': {'title': 'A', 'type': 'string'}},\n                'required': ['a'],\n            },\n            'Bar': {\n                'title': 'Bar',\n                'type': 'object',\n                'properties': {'b': {'allOf': [{'$ref': '#/$defs/Foo'}], 'default': {'a': 'foo'}}},\n            },\n            'Baz': {\n                'title': 'Baz',\n                'type': 'object',\n                'properties': {'c': {'anyOf': [{'$ref': '#/$defs/Bar'}, {'type': 'null'}]}},\n                'required': ['c'],\n            },\n        },\n        'properties': {'d': {'$ref': '#/$defs/Baz'}},\n        'required': ['d'],\n    }\n\n\ndef test_schema_overrides_w_union():\n    class Foo(BaseModel):\n        pass\n\n    class Bar(BaseModel):\n        pass\n\n    class Spam(BaseModel):\n        a: Union[Foo, Bar] = Field(..., description='xxx')\n\n    assert Spam.model_json_schema()['properties'] == {\n        'a': {\n            'title': 'A',\n            'description': 'xxx',\n            'anyOf': [{'$ref': '#/$defs/Foo'}, {'$ref': '#/$defs/Bar'}],\n        },\n    }\n\n\ndef test_schema_from_models():\n    class Foo(BaseModel):\n        a: str\n\n    class Bar(BaseModel):\n        b: Foo\n\n    class Baz(BaseModel):\n        c: Bar\n\n    class Model(BaseModel):\n        d: Baz\n\n    class Ingredient(BaseModel):\n        name: str\n\n    class Pizza(BaseModel):\n        name: str\n        ingredients: List[Ingredient]\n\n    json_schemas_map, model_schema = models_json_schema(\n        [(Model, 'validation'), (Pizza, 'validation')],\n        title='Multi-model schema',\n        description='Single JSON Schema with multiple definitions',\n    )\n    assert json_schemas_map == {\n        (Pizza, 'validation'): {'$ref': '#/$defs/Pizza'},\n        (Model, 'validation'): {'$ref': '#/$defs/Model'},\n    }\n    assert model_schema == {\n        'title': 'Multi-model schema',\n        'description': 'Single JSON Schema with multiple definitions',\n        '$defs': {\n            'Pizza': {\n                'title': 'Pizza',\n                'type': 'object',\n                'properties': {\n                    'name': {'title': 'Name', 'type': 'string'},\n                    'ingredients': {\n                        'title': 'Ingredients',\n                        'type': 'array',\n                        'items': {'$ref': '#/$defs/Ingredient'},\n                    },\n                },\n                'required': ['name', 'ingredients'],\n            },\n            'Ingredient': {\n                'title': 'Ingredient',\n                'type': 'object',\n                'properties': {'name': {'title': 'Name', 'type': 'string'}},\n                'required': ['name'],\n            },\n            'Model': {\n                'title': 'Model',\n                'type': 'object',\n                'properties': {'d': {'$ref': '#/$defs/Baz'}},\n                'required': ['d'],\n            },\n            'Baz': {\n                'title': 'Baz',\n                'type': 'object',\n                'properties': {'c': {'$ref': '#/$defs/Bar'}},\n                'required': ['c'],\n            },\n            'Bar': {\n                'title': 'Bar',\n                'type': 'object',\n                'properties': {'b': {'$ref': '#/$defs/Foo'}},\n                'required': ['b'],\n            },\n            'Foo': {\n                'title': 'Foo',\n                'type': 'object',\n                'properties': {'a': {'title': 'A', 'type': 'string'}},\n                'required': ['a'],\n            },\n        },\n    }\n\n\ndef test_schema_with_refs():\n    ref_template = '#/components/schemas/{model}'\n\n    class Foo(BaseModel):\n        a: str\n\n    class Bar(BaseModel):\n        b: Foo\n\n    class Baz(BaseModel):\n        c: Bar\n\n    keys_map, model_schema = models_json_schema([(Bar, 'validation'), (Baz, 'validation')], ref_template=ref_template)\n    assert keys_map == {\n        (Bar, 'validation'): {'$ref': '#/components/schemas/Bar'},\n        (Baz, 'validation'): {'$ref': '#/components/schemas/Baz'},\n    }\n    assert model_schema == {\n        '$defs': {\n            'Baz': {\n                'title': 'Baz',\n                'type': 'object',\n                'properties': {'c': {'$ref': '#/components/schemas/Bar'}},\n                'required': ['c'],\n            },\n            'Bar': {\n                'title': 'Bar',\n                'type': 'object',\n                'properties': {'b': {'$ref': '#/components/schemas/Foo'}},\n                'required': ['b'],\n            },\n            'Foo': {\n                'title': 'Foo',\n                'type': 'object',\n                'properties': {'a': {'title': 'A', 'type': 'string'}},\n                'required': ['a'],\n            },\n        }\n    }\n\n\ndef test_schema_with_custom_ref_template():\n    class Foo(BaseModel):\n        a: str\n\n    class Bar(BaseModel):\n        b: Foo\n\n    class Baz(BaseModel):\n        c: Bar\n\n    keys_map, model_schema = models_json_schema(\n        [(Bar, 'validation'), (Baz, 'validation')], ref_template='/schemas/{model}.json#/'\n    )\n    assert keys_map == {\n        (Bar, 'validation'): {'$ref': '/schemas/Bar.json#/'},\n        (Baz, 'validation'): {'$ref': '/schemas/Baz.json#/'},\n    }\n    assert model_schema == {\n        '$defs': {\n            'Baz': {\n                'title': 'Baz',\n                'type': 'object',\n                'properties': {'c': {'$ref': '/schemas/Bar.json#/'}},\n                'required': ['c'],\n            },\n            'Bar': {\n                'title': 'Bar',\n                'type': 'object',\n                'properties': {'b': {'$ref': '/schemas/Foo.json#/'}},\n                'required': ['b'],\n            },\n            'Foo': {\n                'title': 'Foo',\n                'type': 'object',\n                'properties': {'a': {'title': 'A', 'type': 'string'}},\n                'required': ['a'],\n            },\n        }\n    }\n\n\ndef test_schema_ref_template_key_error():\n    class Foo(BaseModel):\n        a: str\n\n    class Bar(BaseModel):\n        b: Foo\n\n    class Baz(BaseModel):\n        c: Bar\n\n    with pytest.raises(KeyError):\n        models_json_schema([(Bar, 'validation'), (Baz, 'validation')], ref_template='/schemas/{bad_name}.json#/')\n\n\ndef test_schema_no_definitions():\n    keys_map, model_schema = models_json_schema([], title='Schema without definitions')\n    assert keys_map == {}\n    assert model_schema == {'title': 'Schema without definitions'}\n\n\ndef test_list_default():\n    class UserModel(BaseModel):\n        friends: List[int] = [1]\n\n    assert UserModel.model_json_schema() == {\n        'title': 'UserModel',\n        'type': 'object',\n        'properties': {'friends': {'title': 'Friends', 'default': [1], 'type': 'array', 'items': {'type': 'integer'}}},\n    }\n\n\ndef test_enum_str_default():\n    class MyEnum(str, Enum):\n        FOO = 'foo'\n\n    class UserModel(BaseModel):\n        friends: MyEnum = MyEnum.FOO\n\n    default_value = UserModel.model_json_schema()['properties']['friends']['default']\n    assert type(default_value) is str\n    assert default_value == MyEnum.FOO.value\n\n\ndef test_enum_int_default():\n    class MyEnum(IntEnum):\n        FOO = 1\n\n    class UserModel(BaseModel):\n        friends: MyEnum = MyEnum.FOO\n\n    default_value = UserModel.model_json_schema()['properties']['friends']['default']\n    assert type(default_value) is int\n    assert default_value == MyEnum.FOO.value\n\n\ndef test_dict_default():\n    class UserModel(BaseModel):\n        friends: Dict[str, float] = {'a': 1.1, 'b': 2.2}\n\n    assert UserModel.model_json_schema() == {\n        'title': 'UserModel',\n        'type': 'object',\n        'properties': {\n            'friends': {\n                'title': 'Friends',\n                'default': {'a': 1.1, 'b': 2.2},\n                'type': 'object',\n                'additionalProperties': {'type': 'number'},\n            }\n        },\n    }\n\n\ndef test_model_default():\n    \"\"\"Make sure inner model types are encoded properly\"\"\"\n\n    class Inner(BaseModel):\n        a: Dict[Path, str] = {Path(): ''}\n\n    class Outer(BaseModel):\n        inner: Inner = Inner()\n\n    assert Outer.model_json_schema() == {\n        '$defs': {\n            'Inner': {\n                'properties': {\n                    'a': {\n                        'additionalProperties': {'type': 'string'},\n                        'default': {'.': ''},\n                        'title': 'A',\n                        'type': 'object',\n                    }\n                },\n                'title': 'Inner',\n                'type': 'object',\n            }\n        },\n        'properties': {'inner': {'allOf': [{'$ref': '#/$defs/Inner'}], 'default': {'a': {'.': ''}}}},\n        'title': 'Outer',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    'ser_json_timedelta,properties',\n    [\n        ('float', {'duration': {'default': 300.0, 'title': 'Duration', 'type': 'number'}}),\n        ('iso8601', {'duration': {'default': 'PT5M', 'format': 'duration', 'title': 'Duration', 'type': 'string'}}),\n    ],\n)\ndef test_model_default_timedelta(ser_json_timedelta: Literal['float', 'iso8601'], properties: typing.Dict[str, Any]):\n    class Model(BaseModel):\n        model_config = ConfigDict(ser_json_timedelta=ser_json_timedelta)\n\n        duration: timedelta = timedelta(minutes=5)\n\n    # insert_assert(Model.model_json_schema(mode='serialization'))\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': properties,\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    'ser_json_bytes,properties',\n    [\n        ('base64', {'data': {'default': 'Zm9vYmFy', 'format': 'base64url', 'title': 'Data', 'type': 'string'}}),\n        ('utf8', {'data': {'default': 'foobar', 'format': 'binary', 'title': 'Data', 'type': 'string'}}),\n    ],\n)\ndef test_model_default_bytes(ser_json_bytes: Literal['base64', 'utf8'], properties: typing.Dict[str, Any]):\n    class Model(BaseModel):\n        model_config = ConfigDict(ser_json_bytes=ser_json_bytes)\n\n        data: bytes = b'foobar'\n\n    # insert_assert(Model.model_json_schema(mode='serialization'))\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': properties,\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    'ser_json_timedelta,properties',\n    [\n        ('float', {'duration': {'default': 300.0, 'title': 'Duration', 'type': 'number'}}),\n        ('iso8601', {'duration': {'default': 'PT5M', 'format': 'duration', 'title': 'Duration', 'type': 'string'}}),\n    ],\n)\ndef test_dataclass_default_timedelta(\n    ser_json_timedelta: Literal['float', 'iso8601'], properties: typing.Dict[str, Any]\n):\n    @dataclass(config=ConfigDict(ser_json_timedelta=ser_json_timedelta))\n    class Dataclass:\n        duration: timedelta = timedelta(minutes=5)\n\n    # insert_assert(TypeAdapter(Dataclass).json_schema(mode='serialization'))\n    assert TypeAdapter(Dataclass).json_schema(mode='serialization') == {\n        'properties': properties,\n        'title': 'Dataclass',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    'ser_json_bytes,properties',\n    [\n        ('base64', {'data': {'default': 'Zm9vYmFy', 'format': 'base64url', 'title': 'Data', 'type': 'string'}}),\n        ('utf8', {'data': {'default': 'foobar', 'format': 'binary', 'title': 'Data', 'type': 'string'}}),\n    ],\n)\ndef test_dataclass_default_bytes(ser_json_bytes: Literal['base64', 'utf8'], properties: typing.Dict[str, Any]):\n    @dataclass(config=ConfigDict(ser_json_bytes=ser_json_bytes))\n    class Dataclass:\n        data: bytes = b'foobar'\n\n    # insert_assert(TypeAdapter(Dataclass).json_schema(mode='serialization'))\n    assert TypeAdapter(Dataclass).json_schema(mode='serialization') == {\n        'properties': properties,\n        'title': 'Dataclass',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    'ser_json_timedelta,properties',\n    [\n        ('float', {'duration': {'default': 300.0, 'title': 'Duration', 'type': 'number'}}),\n        ('iso8601', {'duration': {'default': 'PT5M', 'format': 'duration', 'title': 'Duration', 'type': 'string'}}),\n    ],\n)\ndef test_typeddict_default_timedelta(\n    ser_json_timedelta: Literal['float', 'iso8601'], properties: typing.Dict[str, Any]\n):\n    class MyTypedDict(TypedDict):\n        __pydantic_config__ = ConfigDict(ser_json_timedelta=ser_json_timedelta)\n\n        duration: Annotated[timedelta, Field(timedelta(minutes=5))]\n\n    # insert_assert(TypeAdapter(MyTypedDict).json_schema(mode='serialization'))\n    assert TypeAdapter(MyTypedDict).json_schema(mode='serialization') == {\n        'properties': properties,\n        'title': 'MyTypedDict',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(\n    'ser_json_bytes,properties',\n    [\n        ('base64', {'data': {'default': 'Zm9vYmFy', 'format': 'base64url', 'title': 'Data', 'type': 'string'}}),\n        ('utf8', {'data': {'default': 'foobar', 'format': 'binary', 'title': 'Data', 'type': 'string'}}),\n    ],\n)\ndef test_typeddict_default_bytes(ser_json_bytes: Literal['base64', 'utf8'], properties: typing.Dict[str, Any]):\n    class MyTypedDict(TypedDict):\n        __pydantic_config__ = ConfigDict(ser_json_bytes=ser_json_bytes)\n\n        data: Annotated[bytes, Field(b'foobar')]\n\n    # insert_assert(TypeAdapter(MyTypedDict).json_schema(mode='serialization'))\n    assert TypeAdapter(MyTypedDict).json_schema(mode='serialization') == {\n        'properties': properties,\n        'title': 'MyTypedDict',\n        'type': 'object',\n    }\n\n\ndef test_model_subclass_metadata():\n    class A(BaseModel):\n        \"\"\"A Model docstring\"\"\"\n\n    class B(A):\n        pass\n\n    assert A.model_json_schema() == {\n        'title': 'A',\n        'description': 'A Model docstring',\n        'type': 'object',\n        'properties': {},\n    }\n    assert B.model_json_schema() == {'title': 'B', 'type': 'object', 'properties': {}}\n\n\n@pytest.mark.parametrize(\n    'docstring,description',\n    [\n        ('foobar', 'foobar'),\n        ('\\n     foobar\\n    ', 'foobar'),\n        ('foobar\\n    ', 'foobar\\n    '),\n        ('foo\\n    bar\\n    ', 'foo\\nbar'),\n        ('\\n    foo\\n    bar\\n    ', 'foo\\nbar'),\n    ],\n)\ndef test_docstring(docstring, description):\n    class A(BaseModel):\n        x: int\n\n    A.__doc__ = docstring\n\n    assert A.model_json_schema()['description'] == description\n\n\n@pytest.mark.parametrize(\n    'kwargs,type_,expected_extra',\n    [\n        ({'max_length': 5}, str, {'type': 'string', 'maxLength': 5}),\n        ({}, constr(max_length=6), {'type': 'string', 'maxLength': 6}),\n        ({'min_length': 2}, str, {'type': 'string', 'minLength': 2}),\n        ({'max_length': 5}, bytes, {'type': 'string', 'maxLength': 5, 'format': 'binary'}),\n        ({'pattern': '^foo$'}, str, {'type': 'string', 'pattern': '^foo$'}),\n        ({'gt': 2}, int, {'type': 'integer', 'exclusiveMinimum': 2}),\n        ({'lt': 5}, int, {'type': 'integer', 'exclusiveMaximum': 5}),\n        ({'ge': 2}, int, {'type': 'integer', 'minimum': 2}),\n        ({'le': 5}, int, {'type': 'integer', 'maximum': 5}),\n        ({'multiple_of': 5}, int, {'type': 'integer', 'multipleOf': 5}),\n        ({'gt': 2}, float, {'type': 'number', 'exclusiveMinimum': 2}),\n        ({'lt': 5}, float, {'type': 'number', 'exclusiveMaximum': 5}),\n        ({'ge': 2}, float, {'type': 'number', 'minimum': 2}),\n        ({'le': 5}, float, {'type': 'number', 'maximum': 5}),\n        ({'gt': -math.inf}, float, {'type': 'number'}),\n        ({'lt': math.inf}, float, {'type': 'number'}),\n        ({'ge': -math.inf}, float, {'type': 'number'}),\n        ({'le': math.inf}, float, {'type': 'number'}),\n        ({'multiple_of': 5}, float, {'type': 'number', 'multipleOf': 5}),\n        ({'gt': 2}, Decimal, {'anyOf': [{'exclusiveMinimum': 2.0, 'type': 'number'}, {'type': 'string'}]}),\n        ({'lt': 5}, Decimal, {'anyOf': [{'type': 'number', 'exclusiveMaximum': 5}, {'type': 'string'}]}),\n        ({'ge': 2}, Decimal, {'anyOf': [{'type': 'number', 'minimum': 2}, {'type': 'string'}]}),\n        ({'le': 5}, Decimal, {'anyOf': [{'type': 'number', 'maximum': 5}, {'type': 'string'}]}),\n        ({'multiple_of': 5}, Decimal, {'anyOf': [{'type': 'number', 'multipleOf': 5}, {'type': 'string'}]}),\n    ],\n)\ndef test_constraints_schema_validation(kwargs, type_, expected_extra):\n    class Foo(BaseModel):\n        a: type_ = Field('foo', title='A title', description='A description', **kwargs)\n\n    expected_schema = {\n        'title': 'Foo',\n        'type': 'object',\n        'properties': {'a': {'title': 'A title', 'description': 'A description', 'default': 'foo'}},\n    }\n\n    expected_schema['properties']['a'].update(expected_extra)\n    assert Foo.model_json_schema(mode='validation') == expected_schema\n\n\n@pytest.mark.parametrize(\n    'kwargs,type_,expected_extra',\n    [\n        ({'max_length': 5}, str, {'type': 'string', 'maxLength': 5}),\n        ({}, constr(max_length=6), {'type': 'string', 'maxLength': 6}),\n        ({'min_length': 2}, str, {'type': 'string', 'minLength': 2}),\n        ({'max_length': 5}, bytes, {'type': 'string', 'maxLength': 5, 'format': 'binary'}),\n        ({'pattern': '^foo$'}, str, {'type': 'string', 'pattern': '^foo$'}),\n        ({'gt': 2}, int, {'type': 'integer', 'exclusiveMinimum': 2}),\n        ({'lt': 5}, int, {'type': 'integer', 'exclusiveMaximum': 5}),\n        ({'ge': 2}, int, {'type': 'integer', 'minimum': 2}),\n        ({'le': 5}, int, {'type': 'integer', 'maximum': 5}),\n        ({'multiple_of': 5}, int, {'type': 'integer', 'multipleOf': 5}),\n        ({'gt': 2}, float, {'type': 'number', 'exclusiveMinimum': 2}),\n        ({'lt': 5}, float, {'type': 'number', 'exclusiveMaximum': 5}),\n        ({'ge': 2}, float, {'type': 'number', 'minimum': 2}),\n        ({'le': 5}, float, {'type': 'number', 'maximum': 5}),\n        ({'gt': -math.inf}, float, {'type': 'number'}),\n        ({'lt': math.inf}, float, {'type': 'number'}),\n        ({'ge': -math.inf}, float, {'type': 'number'}),\n        ({'le': math.inf}, float, {'type': 'number'}),\n        ({'multiple_of': 5}, float, {'type': 'number', 'multipleOf': 5}),\n        ({'gt': 2}, Decimal, {'type': 'string'}),\n        ({'lt': 5}, Decimal, {'type': 'string'}),\n        ({'ge': 2}, Decimal, {'type': 'string'}),\n        ({'le': 5}, Decimal, {'type': 'string'}),\n        ({'multiple_of': 5}, Decimal, {'type': 'string'}),\n    ],\n)\ndef test_constraints_schema_serialization(kwargs, type_, expected_extra):\n    class Foo(BaseModel):\n        a: type_ = Field('foo', title='A title', description='A description', **kwargs)\n\n    expected_schema = {\n        'title': 'Foo',\n        'type': 'object',\n        'properties': {'a': {'title': 'A title', 'description': 'A description', 'default': 'foo'}},\n    }\n\n    expected_schema['properties']['a'].update(expected_extra)\n    assert Foo.model_json_schema(mode='serialization') == expected_schema\n\n\n@pytest.mark.parametrize(\n    'kwargs,type_,value',\n    [\n        ({'max_length': 5}, str, 'foo'),\n        ({'min_length': 2}, str, 'foo'),\n        ({'max_length': 5}, bytes, b'foo'),\n        ({'pattern': '^foo$'}, str, 'foo'),\n        ({'gt': 2}, int, 3),\n        ({'lt': 5}, int, 3),\n        ({'ge': 2}, int, 3),\n        ({'ge': 2}, int, 2),\n        ({'gt': 2}, int, '3'),\n        ({'le': 5}, int, 3),\n        ({'le': 5}, int, 5),\n        ({'gt': 2}, float, 3.0),\n        ({'gt': 2}, float, 2.1),\n        ({'lt': 5}, float, 3.0),\n        ({'lt': 5}, float, 4.9),\n        ({'ge': 2}, float, 3.0),\n        ({'ge': 2}, float, 2.0),\n        ({'le': 5}, float, 3.0),\n        ({'le': 5}, float, 5.0),\n        ({'gt': 2}, float, 3),\n        ({'gt': 2}, float, '3'),\n        ({'gt': 2}, Decimal, Decimal(3)),\n        ({'lt': 5}, Decimal, Decimal(3)),\n        ({'ge': 2}, Decimal, Decimal(3)),\n        ({'ge': 2}, Decimal, Decimal(2)),\n        ({'le': 5}, Decimal, Decimal(3)),\n        ({'le': 5}, Decimal, Decimal(5)),\n    ],\n)\ndef test_constraints_schema_validation_passes(kwargs, type_, value):\n    class Foo(BaseModel):\n        a: type_ = Field('foo', title='A title', description='A description', **kwargs)\n\n    assert Foo(a=value)\n\n\n@pytest.mark.parametrize(\n    'kwargs,type_,value',\n    [\n        ({'max_length': 5}, str, 'foobar'),\n        ({'min_length': 2}, str, 'f'),\n        ({'pattern': '^foo$'}, str, 'bar'),\n        ({'gt': 2}, int, 2),\n        ({'lt': 5}, int, 5),\n        ({'ge': 2}, int, 1),\n        ({'le': 5}, int, 6),\n        ({'gt': 2}, float, 2.0),\n        ({'lt': 5}, float, 5.0),\n        ({'ge': 2}, float, 1.9),\n        ({'le': 5}, float, 5.1),\n        ({'gt': 2}, Decimal, Decimal(2)),\n        ({'lt': 5}, Decimal, Decimal(5)),\n        ({'ge': 2}, Decimal, Decimal(1)),\n        ({'le': 5}, Decimal, Decimal(6)),\n    ],\n)\ndef test_constraints_schema_validation_raises(kwargs, type_, value):\n    class Foo(BaseModel):\n        a: type_ = Field('foo', title='A title', description='A description', **kwargs)\n\n    with pytest.raises(ValidationError):\n        Foo(a=value)\n\n\ndef test_schema_kwargs():\n    class Foo(BaseModel):\n        a: str = Field('foo', examples=['bar'])\n\n    assert Foo.model_json_schema() == {\n        'title': 'Foo',\n        'type': 'object',\n        'properties': {'a': {'type': 'string', 'title': 'A', 'default': 'foo', 'examples': ['bar']}},\n    }\n\n\ndef test_schema_dict_constr():\n    regex_str = r'^([a-zA-Z_][a-zA-Z0-9_]*)$'\n    ConStrType = constr(pattern=regex_str)\n    ConStrKeyDict = Dict[ConStrType, str]\n\n    class Foo(BaseModel):\n        a: ConStrKeyDict = {}\n\n    assert Foo.model_json_schema() == {\n        'title': 'Foo',\n        'type': 'object',\n        'properties': {\n            'a': {'type': 'object', 'title': 'A', 'default': {}, 'patternProperties': {regex_str: {'type': 'string'}}}\n        },\n    }\n\n\n@pytest.mark.parametrize(\n    'field_type,expected_schema',\n    [\n        # (ConstrainedBytes, {'title': 'A', 'type': 'string', 'format': 'binary'}),\n        (\n            conbytes(min_length=3, max_length=5),\n            {'title': 'A', 'type': 'string', 'format': 'binary', 'minLength': 3, 'maxLength': 5},\n        ),\n    ],\n)\ndef test_bytes_constrained_types(field_type, expected_schema):\n    class Model(BaseModel):\n        a: field_type\n\n    base_schema = {'title': 'Model', 'type': 'object', 'properties': {'a': {}}, 'required': ['a']}\n    base_schema['properties']['a'] = expected_schema\n\n    assert Model.model_json_schema() == base_schema\n\n\ndef test_optional_dict():\n    class Model(BaseModel):\n        something: Optional[Dict[str, Any]] = None\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'something': {'anyOf': [{'type': 'object'}, {'type': 'null'}], 'default': None, 'title': 'Something'}\n        },\n    }\n\n    assert Model().model_dump() == {'something': None}\n    assert Model(something={'foo': 'Bar'}).model_dump() == {'something': {'foo': 'Bar'}}\n\n\ndef test_optional_validator():\n    class Model(BaseModel):\n        something: Optional[str] = None\n\n        @field_validator('something')\n        def check_something(cls, v):\n            if v is not None and 'x' in v:\n                raise ValueError('should not contain x')\n            return v\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'something': {\n                'title': 'Something',\n                'anyOf': [{'type': 'string'}, {'type': 'null'}],\n                'default': None,\n            }\n        },\n    }\n\n    assert Model().model_dump() == {'something': None}\n    assert Model(something=None).model_dump() == {'something': None}\n    assert Model(something='hello').model_dump() == {'something': 'hello'}\n    with pytest.raises(ValidationError) as exc_info:\n        Model(something='hellox')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'error': HasRepr(repr(ValueError('should not contain x')))},\n            'input': 'hellox',\n            'loc': ('something',),\n            'msg': 'Value error, should not contain x',\n            'type': 'value_error',\n        }\n    ]\n\n\ndef test_field_with_validator():\n    class Model(BaseModel):\n        something: Optional[int] = None\n\n        @field_validator('something')\n        def check_field(cls, v, info):\n            return v\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'something': {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'default': None, 'title': 'Something'}\n        },\n    }\n\n\ndef test_unparameterized_schema_generation():\n    class FooList(BaseModel):\n        d: List\n\n    class BarList(BaseModel):\n        d: list\n\n    assert model_json_schema(FooList) == {\n        'title': 'FooList',\n        'type': 'object',\n        'properties': {'d': {'items': {}, 'title': 'D', 'type': 'array'}},\n        'required': ['d'],\n    }\n\n    foo_list_schema = model_json_schema(FooList)\n    bar_list_schema = model_json_schema(BarList)\n    bar_list_schema['title'] = 'FooList'  # to check for equality\n    assert foo_list_schema == bar_list_schema\n\n    class FooDict(BaseModel):\n        d: Dict\n\n    class BarDict(BaseModel):\n        d: dict\n\n    model_json_schema(Foo)\n    assert model_json_schema(FooDict) == {\n        'title': 'FooDict',\n        'type': 'object',\n        'properties': {'d': {'title': 'D', 'type': 'object'}},\n        'required': ['d'],\n    }\n\n    foo_dict_schema = model_json_schema(FooDict)\n    bar_dict_schema = model_json_schema(BarDict)\n    bar_dict_schema['title'] = 'FooDict'  # to check for equality\n    assert foo_dict_schema == bar_dict_schema\n\n\ndef test_known_model_optimization():\n    class Dep(BaseModel):\n        number: int\n\n    class Model(BaseModel):\n        dep: Dep\n        dep_l: List[Dep]\n\n    expected = {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'dep': {'$ref': '#/$defs/Dep'},\n            'dep_l': {'title': 'Dep L', 'type': 'array', 'items': {'$ref': '#/$defs/Dep'}},\n        },\n        'required': ['dep', 'dep_l'],\n        '$defs': {\n            'Dep': {\n                'title': 'Dep',\n                'type': 'object',\n                'properties': {'number': {'title': 'Number', 'type': 'integer'}},\n                'required': ['number'],\n            }\n        },\n    }\n\n    assert Model.model_json_schema() == expected\n\n\ndef test_new_type_schema():\n    a_type = NewType('a_type', int)\n    b_type = NewType('b_type', a_type)\n    c_type = NewType('c_type', str)\n\n    class Model(BaseModel):\n        a: a_type\n        b: b_type\n        c: c_type\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'a': {'title': 'A', 'type': 'integer'},\n            'b': {'title': 'B', 'type': 'integer'},\n            'c': {'title': 'C', 'type': 'string'},\n        },\n        'required': ['a', 'b', 'c'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_literal_schema():\n    class Model(BaseModel):\n        a: Literal[1]\n        b: Literal['a']\n        c: Literal['a', 1]\n        d: Literal['a', Literal['b'], 1, 2]\n        e: Literal[1.0]\n        f: Literal[['a', 1]]\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        'properties': {\n            'a': {'const': 1, 'enum': [1], 'title': 'A', 'type': 'integer'},\n            'b': {'const': 'a', 'enum': ['a'], 'title': 'B', 'type': 'string'},\n            'c': {'enum': ['a', 1], 'title': 'C'},\n            'd': {'enum': ['a', 'b', 1, 2], 'title': 'D'},\n            'e': {'const': 1.0, 'enum': [1.0], 'title': 'E', 'type': 'numeric'},\n            'f': {'const': ['a', 1], 'enum': [['a', 1]], 'title': 'F', 'type': 'array'},\n        },\n        'required': ['a', 'b', 'c', 'd', 'e', 'f'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_literal_enum():\n    class MyEnum(str, Enum):\n        FOO = 'foo'\n        BAR = 'bar'\n\n    class Model(BaseModel):\n        kind: Literal[MyEnum.FOO]\n        other: Literal[MyEnum.FOO, MyEnum.BAR]\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        'properties': {\n            'kind': {'const': 'foo', 'enum': ['foo'], 'title': 'Kind', 'type': 'string'},\n            'other': {'enum': ['foo', 'bar'], 'title': 'Other', 'type': 'string'},\n        },\n        'required': ['kind', 'other'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.skipif(sys.version_info[:2] == (3, 8), reason=\"ListEnum doesn't work in 3.8\")\ndef test_literal_types() -> None:\n    \"\"\"Test that we properly add `type` to json schema enums when there is a single type.\"\"\"\n\n    # for float and array we use an Enum because Literal can only accept str, int, bool or None\n    class FloatEnum(float, Enum):\n        a = 123.0\n        b = 123.1\n\n    class ListEnum(List[int], Enum):\n        a = [123]\n        b = [456]\n\n    class Model(BaseModel):\n        str_literal: Literal['foo', 'bar']\n        int_literal: Literal[123, 456]\n        float_literal: FloatEnum\n        bool_literal: Literal[True, False]\n        none_literal: Literal[None]  # ends up as a const since there's only 1\n        list_literal: ListEnum\n        mixed_literal: Literal[123, 'abc']\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'FloatEnum': {'enum': [123.0, 123.1], 'title': 'FloatEnum', 'type': 'numeric'},\n            'ListEnum': {'enum': [[123], [456]], 'title': 'ListEnum', 'type': 'array'},\n        },\n        'properties': {\n            'str_literal': {'enum': ['foo', 'bar'], 'title': 'Str Literal', 'type': 'string'},\n            'int_literal': {'enum': [123, 456], 'title': 'Int Literal', 'type': 'integer'},\n            'float_literal': {'$ref': '#/$defs/FloatEnum'},\n            'bool_literal': {'enum': [True, False], 'title': 'Bool Literal', 'type': 'boolean'},\n            'none_literal': {'const': None, 'enum': [None], 'title': 'None Literal', 'type': 'null'},\n            'list_literal': {'$ref': '#/$defs/ListEnum'},\n            'mixed_literal': {'enum': [123, 'abc'], 'title': 'Mixed Literal'},\n        },\n        'required': [\n            'str_literal',\n            'int_literal',\n            'float_literal',\n            'bool_literal',\n            'none_literal',\n            'list_literal',\n            'mixed_literal',\n        ],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_color_type():\n    class Model(BaseModel):\n        color: Color\n\n    model_schema = Model.model_json_schema()\n    assert model_schema == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'color': {'title': 'Color', 'type': 'string', 'format': 'color'}},\n        'required': ['color'],\n    }\n\n\ndef test_model_with_extra_forbidden():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='forbid')\n        a: str\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'additionalProperties': False,\n    }\n\n\ndef test_model_with_extra_allow():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='allow')\n        a: str\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'additionalProperties': True,\n    }\n\n\ndef test_model_with_extra_ignore():\n    class Model(BaseModel):\n        model_config = ConfigDict(extra='ignore')\n        a: str\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n    }\n\n\ndef test_dataclass_with_extra_allow():\n    @pydantic.dataclasses.dataclass\n    class Model:\n        __pydantic_config__ = ConfigDict(extra='allow')\n        a: str\n\n    assert TypeAdapter(Model).json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'additionalProperties': True,\n    }\n\n\ndef test_dataclass_with_extra_ignore():\n    @pydantic.dataclasses.dataclass\n    class Model:\n        __pydantic_config__ = ConfigDict(extra='ignore')\n        a: str\n\n    assert TypeAdapter(Model).json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n    }\n\n\ndef test_dataclass_with_extra_forbid():\n    @pydantic.dataclasses.dataclass\n    class Model:\n        __pydantic_config__ = ConfigDict(extra='ignore')\n        a: str\n\n    assert TypeAdapter(Model).json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n    }\n\n\ndef test_typeddict_with_extra_allow():\n    class Model(TypedDict):\n        __pydantic_config__ = ConfigDict(extra='allow')  # type: ignore\n        a: str\n\n    assert TypeAdapter(Model).json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'additionalProperties': True,\n    }\n\n\ndef test_typeddict_with_extra_behavior_allow():\n    class Model:\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.typed_dict_schema(\n                {'a': core_schema.typed_dict_field(core_schema.str_schema())},\n                extra_behavior='allow',\n            )\n\n    assert TypeAdapter(Model).json_schema() == {\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'additionalProperties': True,\n    }\n\n\ndef test_typeddict_with_extra_ignore():\n    class Model(TypedDict):\n        __pydantic_config__ = ConfigDict(extra='ignore')  # type: ignore\n        a: str\n\n    assert TypeAdapter(Model).json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n    }\n\n\ndef test_typeddict_with_extra_behavior_ignore():\n    class Model:\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.typed_dict_schema(\n                {'a': core_schema.typed_dict_field(core_schema.str_schema())},\n                extra_behavior='ignore',\n            )\n\n    assert TypeAdapter(Model).json_schema() == {\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n    }\n\n\ndef test_typeddict_with_extra_forbid():\n    @pydantic.dataclasses.dataclass\n    class Model:\n        __pydantic_config__ = ConfigDict(extra='forbid')\n        a: str\n\n    assert TypeAdapter(Model).json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'additionalProperties': False,\n    }\n\n\ndef test_typeddict_with_extra_behavior_forbid():\n    class Model:\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.typed_dict_schema(\n                {'a': core_schema.typed_dict_field(core_schema.str_schema())},\n                extra_behavior='forbid',\n            )\n\n    assert TypeAdapter(Model).json_schema() == {\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'additionalProperties': False,\n    }\n\n\ndef test_typeddict_with_title():\n    class Model(TypedDict):\n        __pydantic_config__ = ConfigDict(title='Test')  # type: ignore\n        a: str\n\n    assert TypeAdapter(Model).json_schema() == {\n        'title': 'Test',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n    }\n\n\ndef test_typeddict_with_json_schema_extra():\n    class Model(TypedDict):\n        __pydantic_config__ = ConfigDict(title='Test', json_schema_extra={'foobar': 'hello'})  # type: ignore\n        a: str\n\n    assert TypeAdapter(Model).json_schema() == {\n        'title': 'Test',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'foobar': 'hello',\n    }\n\n\ndef test_typeddict_with__callable_json_schema_extra():\n    def json_schema_extra(schema, model_class):\n        schema.pop('properties')\n        schema['type'] = 'override'\n        assert model_class is Model\n\n    class Model(TypedDict):\n        __pydantic_config__ = ConfigDict(title='Test', json_schema_extra=json_schema_extra)  # type: ignore\n        a: str\n\n    assert TypeAdapter(Model).json_schema() == {'title': 'Test', 'type': 'override', 'required': ['a']}\n\n\n@pytest.mark.parametrize(\n    'annotation,kwargs,field_schema',\n    [\n        (int, dict(gt=0), {'title': 'A', 'exclusiveMinimum': 0, 'type': 'integer'}),\n        (\n            Optional[int],\n            dict(gt=0),\n            {'title': 'A', 'anyOf': [{'exclusiveMinimum': 0, 'type': 'integer'}, {'type': 'null'}]},\n        ),\n        (\n            Tuple[Annotated[int, Field(gt=0)], ...],\n            {},\n            {'items': {'exclusiveMinimum': 0, 'type': 'integer'}, 'title': 'A', 'type': 'array'},\n        ),\n        (\n            Tuple[Annotated[int, Field(gt=0)], Annotated[int, Field(gt=0)], Annotated[int, Field(gt=0)]],\n            {},\n            {\n                'title': 'A',\n                'type': 'array',\n                'prefixItems': [\n                    {'exclusiveMinimum': 0, 'type': 'integer'},\n                    {'exclusiveMinimum': 0, 'type': 'integer'},\n                    {'exclusiveMinimum': 0, 'type': 'integer'},\n                ],\n                'minItems': 3,\n                'maxItems': 3,\n            },\n        ),\n        (\n            Union[Annotated[int, Field(gt=0)], Annotated[float, Field(gt=0)]],\n            {},\n            {\n                'title': 'A',\n                'anyOf': [{'exclusiveMinimum': 0, 'type': 'integer'}, {'exclusiveMinimum': 0, 'type': 'number'}],\n            },\n        ),\n        (\n            List[Annotated[int, Field(gt=0)]],\n            {},\n            {'title': 'A', 'type': 'array', 'items': {'exclusiveMinimum': 0, 'type': 'integer'}},\n        ),\n        (\n            Dict[str, Annotated[int, Field(gt=0)]],\n            {},\n            {\n                'title': 'A',\n                'type': 'object',\n                'additionalProperties': {'exclusiveMinimum': 0, 'type': 'integer'},\n            },\n        ),\n        (\n            Union[Annotated[str, Field(max_length=5)], Annotated[int, Field(gt=0)]],\n            {},\n            {'title': 'A', 'anyOf': [{'maxLength': 5, 'type': 'string'}, {'exclusiveMinimum': 0, 'type': 'integer'}]},\n        ),\n    ],\n)\ndef test_enforced_constraints(annotation, kwargs, field_schema):\n    class Model(BaseModel):\n        a: annotation = Field(..., **kwargs)\n\n    schema = Model.model_json_schema()\n    # debug(schema['properties']['a'])\n    assert schema['properties']['a'] == field_schema\n\n\ndef test_real_constraints():\n    class Model1(BaseModel):\n        model_config = ConfigDict(title='Test Model')\n        foo: int = Field(..., gt=123)\n\n    with pytest.raises(ValidationError, match='should be greater than 123'):\n        Model1(foo=123)\n\n    assert Model1(foo=124).model_dump() == {'foo': 124}\n\n    assert Model1.model_json_schema() == {\n        'title': 'Test Model',\n        'type': 'object',\n        'properties': {'foo': {'title': 'Foo', 'exclusiveMinimum': 123, 'type': 'integer'}},\n        'required': ['foo'],\n    }\n\n\ndef test_subfield_field_info():\n    class MyModel(BaseModel):\n        entries: Dict[str, List[int]]\n\n    assert MyModel.model_json_schema() == {\n        'title': 'MyModel',\n        'type': 'object',\n        'properties': {\n            'entries': {\n                'title': 'Entries',\n                'type': 'object',\n                'additionalProperties': {'type': 'array', 'items': {'type': 'integer'}},\n            }\n        },\n        'required': ['entries'],\n    }\n\n\ndef test_dataclass():\n    @dataclass\n    class Model:\n        a: bool\n\n    assert models_json_schema([(Model, 'validation')]) == (\n        {(Model, 'validation'): {'$ref': '#/$defs/Model'}},\n        {\n            '$defs': {\n                'Model': {\n                    'title': 'Model',\n                    'type': 'object',\n                    'properties': {'a': {'title': 'A', 'type': 'boolean'}},\n                    'required': ['a'],\n                }\n            }\n        },\n    )\n\n    assert model_json_schema(Model) == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'boolean'}},\n        'required': ['a'],\n    }\n\n\ndef test_schema_attributes():\n    class ExampleEnum(Enum):\n        \"\"\"This is a test description.\"\"\"\n\n        gt = 'GT'\n        lt = 'LT'\n        ge = 'GE'\n        le = 'LE'\n        max_length = 'ML'\n        multiple_of = 'MO'\n        regex = 'RE'\n\n    class Example(BaseModel):\n        example: ExampleEnum\n\n    # insert_assert(Example.model_json_schema())\n    assert Example.model_json_schema() == {\n        '$defs': {\n            'ExampleEnum': {\n                'description': 'This is a test description.',\n                'enum': ['GT', 'LT', 'GE', 'LE', 'ML', 'MO', 'RE'],\n                'title': 'ExampleEnum',\n                'type': 'string',\n            }\n        },\n        'properties': {'example': {'$ref': '#/$defs/ExampleEnum'}},\n        'required': ['example'],\n        'title': 'Example',\n        'type': 'object',\n    }\n\n\ndef test_tuple_with_extra_schema():\n    class MyTuple(Tuple[int, str]):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, _source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.tuple_schema(\n                [core_schema.int_schema(), core_schema.str_schema(), core_schema.int_schema()], variadic_item_index=2\n            )\n\n    class Model(BaseModel):\n        x: MyTuple\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'x': {\n                'items': {'type': 'integer'},\n                'minItems': 2,\n                'prefixItems': [{'type': 'integer'}, {'type': 'string'}],\n                'title': 'X',\n                'type': 'array',\n            }\n        },\n        'required': ['x'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_path_modify_schema():\n    class MyPath(Path):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, _source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            return handler(Path)\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            schema = handler(core_schema)\n            schema.update(foobar=123)\n            return schema\n\n    class Model(BaseModel):\n        path1: Path\n        path2: MyPath\n        path3: List[MyPath]\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'path1': {'title': 'Path1', 'type': 'string', 'format': 'path'},\n            'path2': {'title': 'Path2', 'type': 'string', 'format': 'path', 'foobar': 123},\n            'path3': {'title': 'Path3', 'type': 'array', 'items': {'type': 'string', 'format': 'path', 'foobar': 123}},\n        },\n        'required': ['path1', 'path2', 'path3'],\n    }\n\n\ndef test_frozen_set():\n    class Model(BaseModel):\n        a: FrozenSet[int] = frozenset({1, 2, 3})\n        b: FrozenSet = frozenset({1, 2, 3})\n        c: frozenset = frozenset({1, 2, 3})\n        d: frozenset = ...\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'a': {\n                'title': 'A',\n                'default': [1, 2, 3],\n                'type': 'array',\n                'items': {'type': 'integer'},\n                'uniqueItems': True,\n            },\n            'b': {'title': 'B', 'default': [1, 2, 3], 'type': 'array', 'items': {}, 'uniqueItems': True},\n            'c': {'title': 'C', 'default': [1, 2, 3], 'type': 'array', 'items': {}, 'uniqueItems': True},\n            'd': {'title': 'D', 'type': 'array', 'items': {}, 'uniqueItems': True},\n        },\n        'required': ['d'],\n    }\n\n\ndef test_iterable():\n    class Model(BaseModel):\n        a: Iterable[int]\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'array', 'items': {'type': 'integer'}}},\n        'required': ['a'],\n    }\n\n\ndef test_new_type():\n    new_type = NewType('NewStr', str)\n\n    class Model(BaseModel):\n        a: new_type\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n    }\n\n\ndef test_multiple_models_with_same_input_output(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom pydantic import BaseModel\n\n\nclass ModelOne(BaseModel):\n    class NestedModel(BaseModel):\n        a: float\n\n    nested: NestedModel\n\n\nclass ModelTwo(BaseModel):\n    class NestedModel(BaseModel):\n        b: float\n\n    nested: NestedModel\n\n\nclass NestedModel(BaseModel):\n    c: float\n        \"\"\"\n    )\n\n    # All validation\n    keys_map, schema = models_json_schema(\n        [(module.ModelOne, 'validation'), (module.ModelTwo, 'validation'), (module.NestedModel, 'validation')]\n    )\n    model_names = set(schema['$defs'].keys())\n    expected_model_names = {\n        'ModelOne',\n        'ModelTwo',\n        f'{module.__name__}__ModelOne__NestedModel',\n        f'{module.__name__}__ModelTwo__NestedModel',\n        f'{module.__name__}__NestedModel',\n    }\n    assert model_names == expected_model_names\n\n    # Validation + serialization\n    keys_map, schema = models_json_schema(\n        [\n            (module.ModelOne, 'validation'),\n            (module.ModelTwo, 'validation'),\n            (module.NestedModel, 'validation'),\n            (module.ModelOne, 'serialization'),\n            (module.ModelTwo, 'serialization'),\n            (module.NestedModel, 'serialization'),\n        ]\n    )\n    model_names = set(schema['$defs'].keys())\n    expected_model_names = {\n        'ModelOne',\n        'ModelTwo',\n        f'{module.__name__}__ModelOne__NestedModel',\n        f'{module.__name__}__ModelTwo__NestedModel',\n        f'{module.__name__}__NestedModel',\n    }\n    assert model_names == expected_model_names\n\n\ndef test_multiple_models_with_same_name_different_input_output(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom decimal import Decimal\n\nfrom pydantic import BaseModel\n\n\nclass ModelOne(BaseModel):\n    class NestedModel(BaseModel):\n        a: Decimal\n\n    nested: NestedModel\n\n\nclass ModelTwo(BaseModel):\n    class NestedModel(BaseModel):\n        b: Decimal\n\n    nested: NestedModel\n\n\nclass NestedModel(BaseModel):\n    c: Decimal\n        \"\"\"\n    )\n\n    # All validation\n    keys_map, schema = models_json_schema(\n        [(module.ModelOne, 'validation'), (module.ModelTwo, 'validation'), (module.NestedModel, 'validation')]\n    )\n    model_names = set(schema['$defs'].keys())\n    expected_model_names = {\n        'ModelOne',\n        'ModelTwo',\n        f'{module.__name__}__ModelOne__NestedModel',\n        f'{module.__name__}__ModelTwo__NestedModel',\n        f'{module.__name__}__NestedModel',\n    }\n    assert model_names == expected_model_names\n\n    # Validation + serialization\n    keys_map, schema = models_json_schema(\n        [\n            (module.ModelOne, 'validation'),\n            (module.ModelTwo, 'validation'),\n            (module.NestedModel, 'validation'),\n            (module.ModelOne, 'serialization'),\n            (module.ModelTwo, 'serialization'),\n            (module.NestedModel, 'serialization'),\n        ]\n    )\n    model_names = set(schema['$defs'].keys())\n    expected_model_names = {\n        'ModelOne-Input',\n        'ModelOne-Output',\n        'ModelTwo-Input',\n        'ModelTwo-Output',\n        f'{module.__name__}__ModelOne__NestedModel-Input',\n        f'{module.__name__}__ModelOne__NestedModel-Output',\n        f'{module.__name__}__ModelTwo__NestedModel-Input',\n        f'{module.__name__}__ModelTwo__NestedModel-Output',\n        f'{module.__name__}__NestedModel-Input',\n        f'{module.__name__}__NestedModel-Output',\n    }\n    assert model_names == expected_model_names\n\n\ndef test_multiple_enums_with_same_name(create_module):\n    module_1 = create_module(\n        # language=Python\n        \"\"\"\nfrom enum import Enum\n\nfrom pydantic import BaseModel\n\n\nclass MyEnum(str, Enum):\n    a = 'a'\n    b = 'b'\n    c = 'c'\n\n\nclass MyModel(BaseModel):\n    my_enum_1: MyEnum\n        \"\"\"\n    )\n\n    module_2 = create_module(\n        # language=Python\n        \"\"\"\nfrom enum import Enum\n\nfrom pydantic import BaseModel\n\n\nclass MyEnum(str, Enum):\n    d = 'd'\n    e = 'e'\n    f = 'f'\n\n\nclass MyModel(BaseModel):\n    my_enum_2: MyEnum\n        \"\"\"\n    )\n\n    class Model(BaseModel):\n        my_model_1: module_1.MyModel\n        my_model_2: module_2.MyModel\n\n    assert len(Model.model_json_schema()['$defs']) == 4\n    assert set(Model.model_json_schema()['$defs']) == {\n        f'{module_1.__name__}__MyEnum',\n        f'{module_1.__name__}__MyModel',\n        f'{module_2.__name__}__MyEnum',\n        f'{module_2.__name__}__MyModel',\n    }\n\n\ndef test_mode_name_causes_no_conflict():\n    class Organization(BaseModel):\n        pass\n\n    class OrganizationInput(BaseModel):\n        pass\n\n    class OrganizationOutput(BaseModel):\n        pass\n\n    class Model(BaseModel):\n        # Ensure the validation and serialization schemas are different:\n        x: Organization = Field(validation_alias='x_validation', serialization_alias='x_serialization')\n        y: OrganizationInput\n        z: OrganizationOutput\n\n    assert Model.model_json_schema(mode='validation') == {\n        '$defs': {\n            'Organization': {'properties': {}, 'title': 'Organization', 'type': 'object'},\n            'OrganizationInput': {'properties': {}, 'title': 'OrganizationInput', 'type': 'object'},\n            'OrganizationOutput': {'properties': {}, 'title': 'OrganizationOutput', 'type': 'object'},\n        },\n        'properties': {\n            'x_validation': {'$ref': '#/$defs/Organization'},\n            'y': {'$ref': '#/$defs/OrganizationInput'},\n            'z': {'$ref': '#/$defs/OrganizationOutput'},\n        },\n        'required': ['x_validation', 'y', 'z'],\n        'title': 'Model',\n        'type': 'object',\n    }\n    assert Model.model_json_schema(mode='serialization') == {\n        '$defs': {\n            'Organization': {'properties': {}, 'title': 'Organization', 'type': 'object'},\n            'OrganizationInput': {'properties': {}, 'title': 'OrganizationInput', 'type': 'object'},\n            'OrganizationOutput': {'properties': {}, 'title': 'OrganizationOutput', 'type': 'object'},\n        },\n        'properties': {\n            'x_serialization': {'$ref': '#/$defs/Organization'},\n            'y': {'$ref': '#/$defs/OrganizationInput'},\n            'z': {'$ref': '#/$defs/OrganizationOutput'},\n        },\n        'required': ['x_serialization', 'y', 'z'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_ref_conflict_resolution_without_mode_difference():\n    class OrganizationInput(BaseModel):\n        pass\n\n    class Organization(BaseModel):\n        x: int\n\n    schema_with_defs, defs = GenerateJsonSchema().generate_definitions(\n        [\n            (Organization, 'validation', Organization.__pydantic_core_schema__),\n            (Organization, 'serialization', Organization.__pydantic_core_schema__),\n            (OrganizationInput, 'validation', OrganizationInput.__pydantic_core_schema__),\n        ]\n    )\n    assert schema_with_defs == {\n        (Organization, 'serialization'): {'$ref': '#/$defs/Organization'},\n        (Organization, 'validation'): {'$ref': '#/$defs/Organization'},\n        (OrganizationInput, 'validation'): {'$ref': '#/$defs/OrganizationInput'},\n    }\n\n    assert defs == {\n        'OrganizationInput': {'properties': {}, 'title': 'OrganizationInput', 'type': 'object'},\n        'Organization': {\n            'properties': {'x': {'title': 'X', 'type': 'integer'}},\n            'required': ['x'],\n            'title': 'Organization',\n            'type': 'object',\n        },\n    }\n\n\ndef test_ref_conflict_resolution_with_mode_difference():\n    class OrganizationInput(BaseModel):\n        pass\n\n    class Organization(BaseModel):\n        x: int\n\n        @field_serializer('x')\n        def serialize_x(self, v: int) -> str:\n            return str(v)\n\n    schema_with_defs, defs = GenerateJsonSchema().generate_definitions(\n        [\n            (Organization, 'validation', Organization.__pydantic_core_schema__),\n            (Organization, 'serialization', Organization.__pydantic_core_schema__),\n            (OrganizationInput, 'validation', OrganizationInput.__pydantic_core_schema__),\n        ]\n    )\n    assert schema_with_defs == {\n        (Organization, 'serialization'): {'$ref': '#/$defs/Organization-Output'},\n        (Organization, 'validation'): {'$ref': '#/$defs/Organization-Input'},\n        (OrganizationInput, 'validation'): {'$ref': '#/$defs/OrganizationInput'},\n    }\n\n    assert defs == {\n        'OrganizationInput': {'properties': {}, 'title': 'OrganizationInput', 'type': 'object'},\n        'Organization-Input': {\n            'properties': {'x': {'title': 'X', 'type': 'integer'}},\n            'required': ['x'],\n            'title': 'Organization',\n            'type': 'object',\n        },\n        'Organization-Output': {\n            'properties': {'x': {'title': 'X', 'type': 'string'}},\n            'required': ['x'],\n            'title': 'Organization',\n            'type': 'object',\n        },\n    }\n\n\ndef test_conflicting_names():\n    class Organization__Input(BaseModel):\n        pass\n\n    class Organization(BaseModel):\n        x: int\n\n        @field_serializer('x')\n        def serialize_x(self, v: int) -> str:\n            return str(v)\n\n    schema_with_defs, defs = GenerateJsonSchema().generate_definitions(\n        [\n            (Organization, 'validation', Organization.__pydantic_core_schema__),\n            (Organization, 'serialization', Organization.__pydantic_core_schema__),\n            (Organization__Input, 'validation', Organization__Input.__pydantic_core_schema__),\n        ]\n    )\n    assert schema_with_defs == {\n        (Organization, 'serialization'): {'$ref': '#/$defs/Organization-Output'},\n        (Organization, 'validation'): {'$ref': '#/$defs/Organization-Input'},\n        (Organization__Input, 'validation'): {'$ref': '#/$defs/Organization__Input'},\n    }\n\n    assert defs == {\n        'Organization__Input': {'properties': {}, 'title': 'Organization__Input', 'type': 'object'},\n        'Organization-Input': {\n            'properties': {'x': {'title': 'X', 'type': 'integer'}},\n            'required': ['x'],\n            'title': 'Organization',\n            'type': 'object',\n        },\n        'Organization-Output': {\n            'properties': {'x': {'title': 'X', 'type': 'string'}},\n            'required': ['x'],\n            'title': 'Organization',\n            'type': 'object',\n        },\n    }\n\n\ndef test_schema_for_generic_field():\n    T = TypeVar('T')\n\n    class GenModel(Generic[T]):\n        def __init__(self, data: Any):\n            self.data = data\n\n        @classmethod\n        def __get_validators__(cls):\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, v: Any):\n            return v\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            source: Any,\n            handler: GetCoreSchemaHandler,\n        ) -> core_schema.PlainValidatorFunctionSchema:\n            source_args = getattr(source, '__args__', [Any])\n            param = source_args[0]\n            metadata = build_metadata_dict(js_functions=[lambda _c, h: h(handler.generate_schema(param))])\n            return core_schema.with_info_plain_validator_function(\n                GenModel,\n                metadata=metadata,\n            )\n\n    class Model(BaseModel):\n        data: GenModel[str]\n        data1: GenModel\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'data': {'type': 'string', 'title': 'Data'},\n            'data1': {\n                'title': 'Data1',\n            },\n        },\n        'required': ['data', 'data1'],\n    }\n\n    class GenModelModified(GenModel, Generic[T]):\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = handler(core_schema)\n            type = field_schema.pop('type', 'other')\n            field_schema.update(anyOf=[{'type': type}, {'type': 'array', 'items': {'type': type}}])\n            return field_schema\n\n    class ModelModified(BaseModel):\n        data: GenModelModified[str]\n        data1: GenModelModified\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    assert ModelModified.model_json_schema() == {\n        'title': 'ModelModified',\n        'type': 'object',\n        'properties': {\n            'data': {'title': 'Data', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]},\n            'data1': {'title': 'Data1', 'anyOf': [{'type': 'other'}, {'type': 'array', 'items': {'type': 'other'}}]},\n        },\n        'required': ['data', 'data1'],\n    }\n\n\ndef test_namedtuple_default():\n    class Coordinates(NamedTuple):\n        x: float\n        y: float\n\n    class LocationBase(BaseModel):\n        coords: Coordinates = Coordinates(34, 42)\n\n    assert LocationBase(coords=Coordinates(1, 2)).coords == Coordinates(1, 2)\n\n    assert LocationBase.model_json_schema() == {\n        '$defs': {\n            'Coordinates': {\n                'maxItems': 2,\n                'minItems': 2,\n                'prefixItems': [{'title': 'X', 'type': 'number'}, {'title': 'Y', 'type': 'number'}],\n                'type': 'array',\n            }\n        },\n        'properties': {'coords': {'allOf': [{'$ref': '#/$defs/Coordinates'}], 'default': [34, 42]}},\n        'title': 'LocationBase',\n        'type': 'object',\n    }\n\n\ndef test_namedtuple_modify_schema():\n    class Coordinates(NamedTuple):\n        x: float\n        y: float\n\n    class CustomCoordinates(Coordinates):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            schema = handler(source)\n            schema['arguments_schema']['metadata']['pydantic_js_prefer_positional_arguments'] = False\n            return schema\n\n    class Location(BaseModel):\n        coords: CustomCoordinates = CustomCoordinates(34, 42)\n\n    assert Location.model_json_schema() == {\n        '$defs': {\n            'CustomCoordinates': {\n                'additionalProperties': False,\n                'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}},\n                'required': ['x', 'y'],\n                'type': 'object',\n            }\n        },\n        'properties': {'coords': {'allOf': [{'$ref': '#/$defs/CustomCoordinates'}], 'default': [34, 42]}},\n        'title': 'Location',\n        'type': 'object',\n    }\n\n\ndef test_advanced_generic_schema():  # noqa: C901\n    T = TypeVar('T')\n    K = TypeVar('K')\n\n    class Gen(Generic[T]):\n        def __init__(self, data: Any):\n            self.data = data\n\n        @classmethod\n        def __get_validators__(cls):\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, v: Any):\n            return v\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            if hasattr(source, '__args__'):\n                arg = source.__args__[0]\n\n                def js_func(s, h):\n                    # ignore the schema we were given and get a new CoreSchema\n                    s = handler.generate_schema(Optional[arg])\n                    return h(s)\n\n                return core_schema.with_info_plain_validator_function(\n                    Gen,\n                    metadata={'pydantic_js_annotation_functions': [js_func]},\n                )\n            else:\n                return handler(source)\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            try:\n                field_schema = handler(core_schema)\n            except PydanticInvalidForJsonSchema:\n                field_schema = {}\n            the_type = field_schema.pop('anyOf', [{'type': 'string'}])[0]\n            field_schema.update(title='Gen title', anyOf=[the_type, {'type': 'array', 'items': the_type}])\n            return field_schema\n\n    class GenTwoParams(Generic[T, K]):\n        def __init__(self, x: str, y: Any):\n            self.x = x\n            self.y = y\n\n        @classmethod\n        def __get_validators__(cls):\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, v: Any):\n            return cls(*v)\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source: Any, handler: GetCoreSchemaHandler, **_kwargs: Any\n        ) -> core_schema.CoreSchema:\n            if hasattr(source, '__args__'):\n                # the js_function ignores the schema we were given and gets a new Tuple CoreSchema\n                metadata = build_metadata_dict(js_functions=[lambda _c, h: h(handler(Tuple[source.__args__]))])\n                return core_schema.with_info_plain_validator_function(\n                    GenTwoParams,\n                    metadata=metadata,\n                )\n            return handler(source)\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = handler(core_schema)\n            field_schema.pop('minItems')\n            field_schema.pop('maxItems')\n            field_schema.update(examples='examples')\n            return field_schema\n\n    class CustomType(Enum):\n        A = 'a'\n        B = 'b'\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> core_schema.CoreSchema:\n            json_schema = handler(core_schema)\n            json_schema.update(title='CustomType title', type='string')\n            return json_schema\n\n    class Model(BaseModel):\n        data0: Gen\n        data1: Gen[CustomType] = Field(title='Data1 title', description='Data 1 description')\n        data2: GenTwoParams[CustomType, UUID4] = Field(title='Data2 title', description='Data 2')\n        # check Tuple because changes in code touch that type\n        data3: Tuple\n        data4: Tuple[CustomType]\n        data5: Tuple[CustomType, str]\n\n        model_config = {'arbitrary_types_allowed': True}\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {'CustomType': {'enum': ['a', 'b'], 'title': 'CustomType title', 'type': 'string'}},\n        'properties': {\n            'data0': {\n                'anyOf': [{'type': 'string'}, {'items': {'type': 'string'}, 'type': 'array'}],\n                'title': 'Gen title',\n            },\n            'data1': {\n                'anyOf': [{'$ref': '#/$defs/CustomType'}, {'items': {'$ref': '#/$defs/CustomType'}, 'type': 'array'}],\n                'description': 'Data 1 description',\n                'title': 'Data1 title',\n            },\n            'data2': {\n                'description': 'Data 2',\n                'examples': 'examples',\n                'prefixItems': [{'$ref': '#/$defs/CustomType'}, {'format': 'uuid4', 'type': 'string'}],\n                'title': 'Data2 title',\n                'type': 'array',\n            },\n            'data3': {'items': {}, 'title': 'Data3', 'type': 'array'},\n            'data4': {\n                'maxItems': 1,\n                'minItems': 1,\n                'prefixItems': [{'$ref': '#/$defs/CustomType'}],\n                'title': 'Data4',\n                'type': 'array',\n            },\n            'data5': {\n                'maxItems': 2,\n                'minItems': 2,\n                'prefixItems': [{'$ref': '#/$defs/CustomType'}, {'type': 'string'}],\n                'title': 'Data5',\n                'type': 'array',\n            },\n        },\n        'required': ['data0', 'data1', 'data2', 'data3', 'data4', 'data5'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_nested_generic():\n    \"\"\"\n    Test a nested BaseModel that is also a Generic\n    \"\"\"\n\n    class Ref(BaseModel, Generic[T]):\n        uuid: str\n\n        def resolve(self) -> T: ...\n\n    class Model(BaseModel):\n        ref: Ref['Model']\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        '$defs': {\n            'Ref_Model_': {\n                'title': 'Ref[Model]',\n                'type': 'object',\n                'properties': {\n                    'uuid': {'title': 'Uuid', 'type': 'string'},\n                },\n                'required': ['uuid'],\n            },\n        },\n        'properties': {\n            'ref': {'$ref': '#/$defs/Ref_Model_'},\n        },\n        'required': ['ref'],\n    }\n\n\ndef test_nested_generic_model():\n    \"\"\"\n    Test a nested generic model\n    \"\"\"\n\n    class Box(BaseModel, Generic[T]):\n        uuid: str\n        data: T\n\n    class Model(BaseModel):\n        box_str: Box[str]\n        box_int: Box[int]\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        '$defs': {\n            'Box_str_': Box[str].model_json_schema(),\n            'Box_int_': Box[int].model_json_schema(),\n        },\n        'properties': {\n            'box_str': {'$ref': '#/$defs/Box_str_'},\n            'box_int': {'$ref': '#/$defs/Box_int_'},\n        },\n        'required': ['box_str', 'box_int'],\n    }\n\n\ndef test_complex_nested_generic():\n    \"\"\"\n    Handle a union of a generic.\n    \"\"\"\n\n    class Ref(BaseModel, Generic[T]):\n        uuid: str\n\n        def resolve(self) -> T: ...\n\n    class Model(BaseModel):\n        uuid: str\n        model: Union[Ref['Model'], 'Model']\n\n        def resolve(self) -> 'Model': ...\n\n    Model.model_rebuild()\n\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'Model': {\n                'title': 'Model',\n                'type': 'object',\n                'properties': {\n                    'uuid': {'title': 'Uuid', 'type': 'string'},\n                    'model': {\n                        'title': 'Model',\n                        'anyOf': [\n                            {'$ref': '#/$defs/Ref_Model_'},\n                            {'$ref': '#/$defs/Model'},\n                        ],\n                    },\n                },\n                'required': ['uuid', 'model'],\n            },\n            'Ref_Model_': {\n                'title': 'Ref[Model]',\n                'type': 'object',\n                'properties': {'uuid': {'title': 'Uuid', 'type': 'string'}},\n                'required': ['uuid'],\n            },\n        },\n        'allOf': [{'$ref': '#/$defs/Model'}],\n    }\n\n\ndef test_modify_schema_dict_keys() -> None:\n    class MyType:\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            return {'test': 'passed'}\n\n    class MyModel(BaseModel):\n        my_field: Dict[str, MyType]\n\n        model_config = dict(arbitrary_types_allowed=True)\n\n    assert MyModel.model_json_schema() == {\n        'properties': {\n            'my_field': {'additionalProperties': {'test': 'passed'}, 'title': 'My Field', 'type': 'object'}  # <----\n        },\n        'required': ['my_field'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n\n\ndef test_remove_anyof_redundancy() -> None:\n    class A:\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            return handler({'type': 'str'})\n\n    class B:\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            return handler({'type': 'str'})\n\n    class MyModel(BaseModel):\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n        # Union of two objects should give a JSON with an `anyOf` field, but in this case\n        # since the fields are the same, the `anyOf` is removed.\n        field: Union[A, B]\n\n    assert MyModel.model_json_schema() == {\n        'properties': {'field': {'title': 'Field', 'type': 'string'}},\n        'required': ['field'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n\n\ndef test_discriminated_union():\n    class Cat(BaseModel):\n        pet_type: Literal['cat']\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n\n    class Lizard(BaseModel):\n        pet_type: Literal['reptile', 'lizard']\n\n    class Model(BaseModel):\n        pet: Union[Cat, Dog, Lizard] = Field(..., discriminator='pet_type')\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'Cat': {\n                'properties': {'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'}},\n                'required': ['pet_type'],\n                'title': 'Cat',\n                'type': 'object',\n            },\n            'Dog': {\n                'properties': {'pet_type': {'const': 'dog', 'enum': ['dog'], 'title': 'Pet Type', 'type': 'string'}},\n                'required': ['pet_type'],\n                'title': 'Dog',\n                'type': 'object',\n            },\n            'Lizard': {\n                'properties': {'pet_type': {'enum': ['reptile', 'lizard'], 'title': 'Pet Type', 'type': 'string'}},\n                'required': ['pet_type'],\n                'title': 'Lizard',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'pet': {\n                'discriminator': {\n                    'mapping': {\n                        'cat': '#/$defs/Cat',\n                        'dog': '#/$defs/Dog',\n                        'lizard': '#/$defs/Lizard',\n                        'reptile': '#/$defs/Lizard',\n                    },\n                    'propertyName': 'pet_type',\n                },\n                'oneOf': [{'$ref': '#/$defs/Cat'}, {'$ref': '#/$defs/Dog'}, {'$ref': '#/$defs/Lizard'}],\n                'title': 'Pet',\n            }\n        },\n        'required': ['pet'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_discriminated_annotated_union():\n    class Cat(BaseModel):\n        pet_type: Literal['cat']\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n\n    class Lizard(BaseModel):\n        pet_type: Literal['reptile', 'lizard']\n\n    class Model(BaseModel):\n        pet: Annotated[Union[Cat, Dog, Lizard], Field(..., discriminator='pet_type')]\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'Cat': {\n                'properties': {'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'}},\n                'required': ['pet_type'],\n                'title': 'Cat',\n                'type': 'object',\n            },\n            'Dog': {\n                'properties': {'pet_type': {'const': 'dog', 'enum': ['dog'], 'title': 'Pet Type', 'type': 'string'}},\n                'required': ['pet_type'],\n                'title': 'Dog',\n                'type': 'object',\n            },\n            'Lizard': {\n                'properties': {'pet_type': {'enum': ['reptile', 'lizard'], 'title': 'Pet Type', 'type': 'string'}},\n                'required': ['pet_type'],\n                'title': 'Lizard',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'pet': {\n                'discriminator': {\n                    'mapping': {\n                        'cat': '#/$defs/Cat',\n                        'dog': '#/$defs/Dog',\n                        'lizard': '#/$defs/Lizard',\n                        'reptile': '#/$defs/Lizard',\n                    },\n                    'propertyName': 'pet_type',\n                },\n                'oneOf': [{'$ref': '#/$defs/Cat'}, {'$ref': '#/$defs/Dog'}, {'$ref': '#/$defs/Lizard'}],\n                'title': 'Pet',\n            }\n        },\n        'required': ['pet'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_nested_discriminated_union():\n    class BlackCatWithHeight(BaseModel):\n        color: Literal['black']\n        info: Literal['height']\n        height: float\n\n    class BlackCatWithWeight(BaseModel):\n        color: Literal['black']\n        info: Literal['weight']\n        weight: float\n\n    BlackCat = Annotated[Union[BlackCatWithHeight, BlackCatWithWeight], Field(discriminator='info')]\n\n    class WhiteCat(BaseModel):\n        color: Literal['white']\n        white_cat_info: str\n\n    class Cat(BaseModel):\n        pet: Annotated[Union[BlackCat, WhiteCat], Field(discriminator='color')]\n\n    # insert_assert(Cat.model_json_schema())\n    assert Cat.model_json_schema() == {\n        '$defs': {\n            'BlackCatWithHeight': {\n                'properties': {\n                    'color': {'const': 'black', 'enum': ['black'], 'title': 'Color', 'type': 'string'},\n                    'height': {'title': 'Height', 'type': 'number'},\n                    'info': {'const': 'height', 'enum': ['height'], 'title': 'Info', 'type': 'string'},\n                },\n                'required': ['color', 'info', 'height'],\n                'title': 'BlackCatWithHeight',\n                'type': 'object',\n            },\n            'BlackCatWithWeight': {\n                'properties': {\n                    'color': {'const': 'black', 'enum': ['black'], 'title': 'Color', 'type': 'string'},\n                    'info': {'const': 'weight', 'enum': ['weight'], 'title': 'Info', 'type': 'string'},\n                    'weight': {'title': 'Weight', 'type': 'number'},\n                },\n                'required': ['color', 'info', 'weight'],\n                'title': 'BlackCatWithWeight',\n                'type': 'object',\n            },\n            'WhiteCat': {\n                'properties': {\n                    'color': {'const': 'white', 'enum': ['white'], 'title': 'Color', 'type': 'string'},\n                    'white_cat_info': {'title': 'White Cat Info', 'type': 'string'},\n                },\n                'required': ['color', 'white_cat_info'],\n                'title': 'WhiteCat',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'pet': {\n                'discriminator': {\n                    'mapping': {\n                        'black': {\n                            'discriminator': {\n                                'mapping': {\n                                    'height': '#/$defs/BlackCatWithHeight',\n                                    'weight': '#/$defs/BlackCatWithWeight',\n                                },\n                                'propertyName': 'info',\n                            },\n                            'oneOf': [{'$ref': '#/$defs/BlackCatWithHeight'}, {'$ref': '#/$defs/BlackCatWithWeight'}],\n                        },\n                        'white': '#/$defs/WhiteCat',\n                    },\n                    'propertyName': 'color',\n                },\n                'oneOf': [\n                    {\n                        'discriminator': {\n                            'mapping': {'height': '#/$defs/BlackCatWithHeight', 'weight': '#/$defs/BlackCatWithWeight'},\n                            'propertyName': 'info',\n                        },\n                        'oneOf': [{'$ref': '#/$defs/BlackCatWithHeight'}, {'$ref': '#/$defs/BlackCatWithWeight'}],\n                    },\n                    {'$ref': '#/$defs/WhiteCat'},\n                ],\n                'title': 'Pet',\n            }\n        },\n        'required': ['pet'],\n        'title': 'Cat',\n        'type': 'object',\n    }\n\n\ndef test_deeper_nested_discriminated_annotated_union():\n    class BlackCatWithHeight(BaseModel):\n        pet_type: Literal['cat']\n        color: Literal['black']\n        info: Literal['height']\n        black_infos: str\n\n    class BlackCatWithWeight(BaseModel):\n        pet_type: Literal['cat']\n        color: Literal['black']\n        info: Literal['weight']\n        black_infos: str\n\n    BlackCat = Annotated[Union[BlackCatWithHeight, BlackCatWithWeight], Field(discriminator='info')]\n\n    class WhiteCat(BaseModel):\n        pet_type: Literal['cat']\n        color: Literal['white']\n        white_infos: str\n\n    Cat = Annotated[Union[BlackCat, WhiteCat], Field(discriminator='color')]\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n        dog_name: str\n\n    Pet = Annotated[Union[Cat, Dog], Field(discriminator='pet_type')]\n\n    class Model(BaseModel):\n        pet: Pet\n        number: int\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'BlackCatWithHeight': {\n                'properties': {\n                    'black_infos': {'title': 'Black Infos', 'type': 'string'},\n                    'color': {'const': 'black', 'enum': ['black'], 'title': 'Color', 'type': 'string'},\n                    'info': {'const': 'height', 'enum': ['height'], 'title': 'Info', 'type': 'string'},\n                    'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'},\n                },\n                'required': ['pet_type', 'color', 'info', 'black_infos'],\n                'title': 'BlackCatWithHeight',\n                'type': 'object',\n            },\n            'BlackCatWithWeight': {\n                'properties': {\n                    'black_infos': {'title': 'Black Infos', 'type': 'string'},\n                    'color': {'const': 'black', 'enum': ['black'], 'title': 'Color', 'type': 'string'},\n                    'info': {'const': 'weight', 'enum': ['weight'], 'title': 'Info', 'type': 'string'},\n                    'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'},\n                },\n                'required': ['pet_type', 'color', 'info', 'black_infos'],\n                'title': 'BlackCatWithWeight',\n                'type': 'object',\n            },\n            'Dog': {\n                'properties': {\n                    'dog_name': {'title': 'Dog Name', 'type': 'string'},\n                    'pet_type': {'const': 'dog', 'enum': ['dog'], 'title': 'Pet Type', 'type': 'string'},\n                },\n                'required': ['pet_type', 'dog_name'],\n                'title': 'Dog',\n                'type': 'object',\n            },\n            'WhiteCat': {\n                'properties': {\n                    'color': {'const': 'white', 'enum': ['white'], 'title': 'Color', 'type': 'string'},\n                    'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'},\n                    'white_infos': {'title': 'White Infos', 'type': 'string'},\n                },\n                'required': ['pet_type', 'color', 'white_infos'],\n                'title': 'WhiteCat',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'number': {'title': 'Number', 'type': 'integer'},\n            'pet': {\n                'discriminator': {\n                    'mapping': {\n                        'cat': {\n                            'discriminator': {\n                                'mapping': {\n                                    'black': {\n                                        'discriminator': {\n                                            'mapping': {\n                                                'height': '#/$defs/BlackCatWithHeight',\n                                                'weight': '#/$defs/BlackCatWithWeight',\n                                            },\n                                            'propertyName': 'info',\n                                        },\n                                        'oneOf': [\n                                            {'$ref': '#/$defs/BlackCatWithHeight'},\n                                            {'$ref': '#/$defs/BlackCatWithWeight'},\n                                        ],\n                                    },\n                                    'white': '#/$defs/WhiteCat',\n                                },\n                                'propertyName': 'color',\n                            },\n                            'oneOf': [\n                                {\n                                    'discriminator': {\n                                        'mapping': {\n                                            'height': '#/$defs/BlackCatWithHeight',\n                                            'weight': '#/$defs/BlackCatWithWeight',\n                                        },\n                                        'propertyName': 'info',\n                                    },\n                                    'oneOf': [\n                                        {'$ref': '#/$defs/BlackCatWithHeight'},\n                                        {'$ref': '#/$defs/BlackCatWithWeight'},\n                                    ],\n                                },\n                                {'$ref': '#/$defs/WhiteCat'},\n                            ],\n                        },\n                        'dog': '#/$defs/Dog',\n                    },\n                    'propertyName': 'pet_type',\n                },\n                'oneOf': [\n                    {\n                        'discriminator': {\n                            'mapping': {\n                                'black': {\n                                    'discriminator': {\n                                        'mapping': {\n                                            'height': '#/$defs/BlackCatWithHeight',\n                                            'weight': '#/$defs/BlackCatWithWeight',\n                                        },\n                                        'propertyName': 'info',\n                                    },\n                                    'oneOf': [\n                                        {'$ref': '#/$defs/BlackCatWithHeight'},\n                                        {'$ref': '#/$defs/BlackCatWithWeight'},\n                                    ],\n                                },\n                                'white': '#/$defs/WhiteCat',\n                            },\n                            'propertyName': 'color',\n                        },\n                        'oneOf': [\n                            {\n                                'discriminator': {\n                                    'mapping': {\n                                        'height': '#/$defs/BlackCatWithHeight',\n                                        'weight': '#/$defs/BlackCatWithWeight',\n                                    },\n                                    'propertyName': 'info',\n                                },\n                                'oneOf': [\n                                    {'$ref': '#/$defs/BlackCatWithHeight'},\n                                    {'$ref': '#/$defs/BlackCatWithWeight'},\n                                ],\n                            },\n                            {'$ref': '#/$defs/WhiteCat'},\n                        ],\n                    },\n                    {'$ref': '#/$defs/Dog'},\n                ],\n                'title': 'Pet',\n            },\n        },\n        'required': ['pet', 'number'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_discriminated_annotated_union_literal_enum():\n    class PetType(Enum):\n        cat = 'cat'\n        dog = 'dog'\n\n    class PetColor(str, Enum):\n        black = 'black'\n        white = 'white'\n\n    class PetInfo(Enum):\n        height = 0\n        weight = 1\n\n    class BlackCatWithHeight(BaseModel):\n        pet_type: Literal[PetType.cat]\n        color: Literal[PetColor.black]\n        info: Literal[PetInfo.height]\n        black_infos: str\n\n    class BlackCatWithWeight(BaseModel):\n        pet_type: Literal[PetType.cat]\n        color: Literal[PetColor.black]\n        info: Literal[PetInfo.weight]\n        black_infos: str\n\n    BlackCat = Annotated[Union[BlackCatWithHeight, BlackCatWithWeight], Field(discriminator='info')]\n\n    class WhiteCat(BaseModel):\n        pet_type: Literal[PetType.cat]\n        color: Literal[PetColor.white]\n        white_infos: str\n\n    Cat = Annotated[Union[BlackCat, WhiteCat], Field(discriminator='color')]\n\n    class Dog(BaseModel):\n        pet_type: Literal[PetType.dog]\n        dog_name: str\n\n    Pet = Annotated[Union[Cat, Dog], Field(discriminator='pet_type')]\n\n    class Model(BaseModel):\n        pet: Pet\n        number: int\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'BlackCatWithHeight': {\n                'properties': {\n                    'black_infos': {'title': 'Black Infos', 'type': 'string'},\n                    'color': {'const': 'black', 'enum': ['black'], 'title': 'Color', 'type': 'string'},\n                    'info': {'const': 0, 'enum': [0], 'title': 'Info', 'type': 'integer'},\n                    'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'},\n                },\n                'required': ['pet_type', 'color', 'info', 'black_infos'],\n                'title': 'BlackCatWithHeight',\n                'type': 'object',\n            },\n            'BlackCatWithWeight': {\n                'properties': {\n                    'black_infos': {'title': 'Black Infos', 'type': 'string'},\n                    'color': {'const': 'black', 'enum': ['black'], 'title': 'Color', 'type': 'string'},\n                    'info': {'const': 1, 'enum': [1], 'title': 'Info', 'type': 'integer'},\n                    'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'},\n                },\n                'required': ['pet_type', 'color', 'info', 'black_infos'],\n                'title': 'BlackCatWithWeight',\n                'type': 'object',\n            },\n            'Dog': {\n                'properties': {\n                    'dog_name': {'title': 'Dog Name', 'type': 'string'},\n                    'pet_type': {'const': 'dog', 'enum': ['dog'], 'title': 'Pet Type', 'type': 'string'},\n                },\n                'required': ['pet_type', 'dog_name'],\n                'title': 'Dog',\n                'type': 'object',\n            },\n            'WhiteCat': {\n                'properties': {\n                    'color': {'const': 'white', 'enum': ['white'], 'title': 'Color', 'type': 'string'},\n                    'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'},\n                    'white_infos': {'title': 'White Infos', 'type': 'string'},\n                },\n                'required': ['pet_type', 'color', 'white_infos'],\n                'title': 'WhiteCat',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'number': {'title': 'Number', 'type': 'integer'},\n            'pet': {\n                'discriminator': {\n                    'mapping': {\n                        'cat': {\n                            'discriminator': {\n                                'mapping': {\n                                    'black': {\n                                        'discriminator': {\n                                            'mapping': {\n                                                '0': '#/$defs/BlackCatWithHeight',\n                                                '1': '#/$defs/BlackCatWithWeight',\n                                            },\n                                            'propertyName': 'info',\n                                        },\n                                        'oneOf': [\n                                            {'$ref': '#/$defs/BlackCatWithHeight'},\n                                            {'$ref': '#/$defs/BlackCatWithWeight'},\n                                        ],\n                                    },\n                                    'white': '#/$defs/WhiteCat',\n                                },\n                                'propertyName': 'color',\n                            },\n                            'oneOf': [\n                                {\n                                    'discriminator': {\n                                        'mapping': {\n                                            '0': '#/$defs/BlackCatWithHeight',\n                                            '1': '#/$defs/BlackCatWithWeight',\n                                        },\n                                        'propertyName': 'info',\n                                    },\n                                    'oneOf': [\n                                        {'$ref': '#/$defs/BlackCatWithHeight'},\n                                        {'$ref': '#/$defs/BlackCatWithWeight'},\n                                    ],\n                                },\n                                {'$ref': '#/$defs/WhiteCat'},\n                            ],\n                        },\n                        'dog': '#/$defs/Dog',\n                    },\n                    'propertyName': 'pet_type',\n                },\n                'oneOf': [\n                    {\n                        'discriminator': {\n                            'mapping': {\n                                'black': {\n                                    'discriminator': {\n                                        'mapping': {\n                                            '0': '#/$defs/BlackCatWithHeight',\n                                            '1': '#/$defs/BlackCatWithWeight',\n                                        },\n                                        'propertyName': 'info',\n                                    },\n                                    'oneOf': [\n                                        {'$ref': '#/$defs/BlackCatWithHeight'},\n                                        {'$ref': '#/$defs/BlackCatWithWeight'},\n                                    ],\n                                },\n                                'white': '#/$defs/WhiteCat',\n                            },\n                            'propertyName': 'color',\n                        },\n                        'oneOf': [\n                            {\n                                'discriminator': {\n                                    'mapping': {'0': '#/$defs/BlackCatWithHeight', '1': '#/$defs/BlackCatWithWeight'},\n                                    'propertyName': 'info',\n                                },\n                                'oneOf': [\n                                    {'$ref': '#/$defs/BlackCatWithHeight'},\n                                    {'$ref': '#/$defs/BlackCatWithWeight'},\n                                ],\n                            },\n                            {'$ref': '#/$defs/WhiteCat'},\n                        ],\n                    },\n                    {'$ref': '#/$defs/Dog'},\n                ],\n                'title': 'Pet',\n            },\n        },\n        'required': ['pet', 'number'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_alias_same():\n    class Cat(BaseModel):\n        pet_type: Literal['cat'] = Field(alias='typeOfPet')\n        c: str\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog'] = Field(alias='typeOfPet')\n        d: str\n\n    class Model(BaseModel):\n        pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n        number: int\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'Cat': {\n                'properties': {\n                    'c': {'title': 'C', 'type': 'string'},\n                    'typeOfPet': {'const': 'cat', 'enum': ['cat'], 'title': 'Typeofpet', 'type': 'string'},\n                },\n                'required': ['typeOfPet', 'c'],\n                'title': 'Cat',\n                'type': 'object',\n            },\n            'Dog': {\n                'properties': {\n                    'd': {'title': 'D', 'type': 'string'},\n                    'typeOfPet': {'const': 'dog', 'enum': ['dog'], 'title': 'Typeofpet', 'type': 'string'},\n                },\n                'required': ['typeOfPet', 'd'],\n                'title': 'Dog',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'number': {'title': 'Number', 'type': 'integer'},\n            'pet': {\n                'oneOf': [{'$ref': '#/$defs/Cat'}, {'$ref': '#/$defs/Dog'}],\n                'title': 'Pet',\n                'discriminator': {'mapping': {'cat': '#/$defs/Cat', 'dog': '#/$defs/Dog'}, 'propertyName': 'typeOfPet'},\n            },\n        },\n        'required': ['pet', 'number'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_nested_python_dataclasses():\n    \"\"\"\n    Test schema generation for nested python dataclasses\n    \"\"\"\n\n    from dataclasses import dataclass as python_dataclass\n\n    @python_dataclass\n    class ChildModel:\n        name: str\n\n    @python_dataclass\n    class NestedModel:\n        \"\"\"\n        Custom description\n        \"\"\"\n\n        # Note: the Custom description will not be preserved as this is a vanilla dataclass\n        # This is the same behavior as in v1\n        child: List[ChildModel]\n\n    # insert_assert(model_json_schema(dataclass(NestedModel)))\n    assert model_json_schema(dataclass(NestedModel)) == {\n        '$defs': {\n            'ChildModel': {\n                'properties': {'name': {'title': 'Name', 'type': 'string'}},\n                'required': ['name'],\n                'title': 'ChildModel',\n                'type': 'object',\n            }\n        },\n        'properties': {'child': {'items': {'$ref': '#/$defs/ChildModel'}, 'title': 'Child', 'type': 'array'}},\n        'required': ['child'],\n        'title': 'NestedModel',\n        'type': 'object',\n    }\n\n\ndef test_discriminated_union_in_list():\n    class BlackCat(BaseModel):\n        pet_type: Literal['cat']\n        color: Literal['black']\n        black_name: str\n\n    class WhiteCat(BaseModel):\n        pet_type: Literal['cat']\n        color: Literal['white']\n        white_name: str\n\n    Cat = Annotated[Union[BlackCat, WhiteCat], Field(discriminator='color')]\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n        name: str\n\n    Pet = Annotated[Union[Cat, Dog], Field(discriminator='pet_type')]\n\n    class Model(BaseModel):\n        pets: Pet\n        n: int\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'BlackCat': {\n                'properties': {\n                    'black_name': {'title': 'Black Name', 'type': 'string'},\n                    'color': {'const': 'black', 'enum': ['black'], 'title': 'Color', 'type': 'string'},\n                    'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'},\n                },\n                'required': ['pet_type', 'color', 'black_name'],\n                'title': 'BlackCat',\n                'type': 'object',\n            },\n            'Dog': {\n                'properties': {\n                    'name': {'title': 'Name', 'type': 'string'},\n                    'pet_type': {'const': 'dog', 'enum': ['dog'], 'title': 'Pet Type', 'type': 'string'},\n                },\n                'required': ['pet_type', 'name'],\n                'title': 'Dog',\n                'type': 'object',\n            },\n            'WhiteCat': {\n                'properties': {\n                    'color': {'const': 'white', 'enum': ['white'], 'title': 'Color', 'type': 'string'},\n                    'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'},\n                    'white_name': {'title': 'White Name', 'type': 'string'},\n                },\n                'required': ['pet_type', 'color', 'white_name'],\n                'title': 'WhiteCat',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'n': {'title': 'N', 'type': 'integer'},\n            'pets': {\n                'discriminator': {\n                    'mapping': {\n                        'cat': {\n                            'discriminator': {\n                                'mapping': {'black': '#/$defs/BlackCat', 'white': '#/$defs/WhiteCat'},\n                                'propertyName': 'color',\n                            },\n                            'oneOf': [{'$ref': '#/$defs/BlackCat'}, {'$ref': '#/$defs/WhiteCat'}],\n                        },\n                        'dog': '#/$defs/Dog',\n                    },\n                    'propertyName': 'pet_type',\n                },\n                'oneOf': [\n                    {\n                        'discriminator': {\n                            'mapping': {'black': '#/$defs/BlackCat', 'white': '#/$defs/WhiteCat'},\n                            'propertyName': 'color',\n                        },\n                        'oneOf': [{'$ref': '#/$defs/BlackCat'}, {'$ref': '#/$defs/WhiteCat'}],\n                    },\n                    {'$ref': '#/$defs/Dog'},\n                ],\n                'title': 'Pets',\n            },\n        },\n        'required': ['pets', 'n'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_model_with_type_attributes():\n    class Foo:\n        a: float\n\n    class Bar(BaseModel):\n        b: int\n\n    class Baz(BaseModel):\n        a: Type[Foo]\n        b: Type[Bar]\n\n    assert Baz.model_json_schema() == {\n        'title': 'Baz',\n        'type': 'object',\n        'properties': {'a': {'title': 'A'}, 'b': {'title': 'B'}},\n        'required': ['a', 'b'],\n    }\n\n\n@pytest.mark.parametrize('secret_cls', [SecretStr, SecretBytes])\n@pytest.mark.parametrize(\n    'field_kw,schema_kw',\n    [\n        # [{}, {}],\n        [{'min_length': 6}, {'minLength': 6}],\n        [{'max_length': 10}, {'maxLength': 10}],\n        [{'min_length': 6, 'max_length': 10}, {'minLength': 6, 'maxLength': 10}],\n    ],\n    ids=['min-constraint', 'max-constraint', 'min-max-constraints'],\n)\ndef test_secrets_schema(secret_cls, field_kw, schema_kw):\n    class Foobar(BaseModel):\n        password: secret_cls = Field(**field_kw)\n\n    assert Foobar.model_json_schema() == {\n        'title': 'Foobar',\n        'type': 'object',\n        'properties': {\n            'password': {'title': 'Password', 'type': 'string', 'writeOnly': True, 'format': 'password', **schema_kw}\n        },\n        'required': ['password'],\n    }\n\n\ndef test_override_generate_json_schema():\n    class MyGenerateJsonSchema(GenerateJsonSchema):\n        def generate(self, schema, mode='validation'):\n            json_schema = super().generate(schema, mode=mode)\n            json_schema['$schema'] = self.schema_dialect\n            return json_schema\n\n    class MyBaseModel(BaseModel):\n        @classmethod\n        def model_json_schema(\n            cls,\n            by_alias: bool = True,\n            ref_template: str = DEFAULT_REF_TEMPLATE,\n            schema_generator: Type[GenerateJsonSchema] = MyGenerateJsonSchema,\n            mode='validation',\n        ) -> Dict[str, Any]:\n            return super().model_json_schema(by_alias, ref_template, schema_generator, mode)\n\n    class MyModel(MyBaseModel):\n        x: int\n\n    assert MyModel.model_json_schema() == {\n        '$schema': 'https://json-schema.org/draft/2020-12/schema',\n        'properties': {'x': {'title': 'X', 'type': 'integer'}},\n        'required': ['x'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n\n\ndef test_generate_json_schema_generate_twice():\n    generator = GenerateJsonSchema()\n\n    class Model(BaseModel):\n        title: str\n\n    generator.generate(Model.__pydantic_core_schema__)\n\n    with pytest.raises(\n        PydanticUserError,\n        match=re.escape(\n            'This JSON schema generator has already been used to generate a JSON schema. '\n            'You must create a new instance of GenerateJsonSchema to generate a new JSON schema.'\n        ),\n    ):\n        generator.generate(Model.__pydantic_core_schema__)\n\n    generator = GenerateJsonSchema()\n    generator.generate_definitions([(Model, 'validation', Model.__pydantic_core_schema__)])\n\n    with pytest.raises(\n        PydanticUserError,\n        match=re.escape(\n            'This JSON schema generator has already been used to generate a JSON schema. '\n            'You must create a new instance of GenerateJsonSchema to generate a new JSON schema.'\n        ),\n    ):\n        generator.generate_definitions([(Model, 'validation', Model.__pydantic_core_schema__)])\n\n\ndef test_nested_default_json_schema():\n    class InnerModel(BaseModel):\n        foo: str = 'bar'\n        baz: str = Field(default='foobar', alias='my_alias')\n\n    class OuterModel(BaseModel):\n        nested_field: InnerModel = InnerModel()\n\n    assert OuterModel.model_json_schema() == {\n        '$defs': {\n            'InnerModel': {\n                'properties': {\n                    'foo': {'default': 'bar', 'title': 'Foo', 'type': 'string'},\n                    'my_alias': {'default': 'foobar', 'title': 'My Alias', 'type': 'string'},\n                },\n                'title': 'InnerModel',\n                'type': 'object',\n            }\n        },\n        'properties': {\n            'nested_field': {'allOf': [{'$ref': '#/$defs/InnerModel'}], 'default': {'my_alias': 'foobar', 'foo': 'bar'}}\n        },\n        'title': 'OuterModel',\n        'type': 'object',\n    }\n\n\n@pytest.mark.xfail(\n    reason=(\n        'We are calling __get_pydantic_json_schema__ too many times.'\n        ' The second time we analyze a model we get the CoreSchema from __pydantic_core_schema__.'\n        ' But then we proceed to append to the metadata json schema functions.'\n    )\n)\ndef test_get_pydantic_core_schema_calls() -> None:\n    \"\"\"Verify when/how many times `__get_pydantic_core_schema__` gets called\"\"\"\n\n    calls: List[str] = []\n\n    class Model(BaseModel):\n        @classmethod\n        def __get_pydantic_json_schema__(cls, schema: CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            calls.append('Model::before')\n            json_schema = handler(schema)\n            calls.append('Model::after')\n            return json_schema\n\n    schema = Model.model_json_schema()\n    expected: JsonSchemaValue = {'type': 'object', 'properties': {}, 'title': 'Model'}\n\n    assert schema == expected\n    assert calls == ['Model::before', 'Model::after']\n\n    calls.clear()\n\n    class CustomAnnotation(NamedTuple):\n        name: str\n\n        def __get_pydantic_json_schema__(self, schema: CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            calls.append(f'CustomAnnotation({self.name})::before')\n            json_schema = handler(schema)\n            calls.append(f'CustomAnnotation({self.name})::after')\n            return json_schema\n\n    AnnotatedType = Annotated[str, CustomAnnotation('foo'), CustomAnnotation('bar')]\n\n    schema = TypeAdapter(AnnotatedType).json_schema()\n    expected: JsonSchemaValue = {'type': 'string'}\n\n    assert schema == expected\n    assert calls == [\n        'CustomAnnotation(bar)::before',\n        'CustomAnnotation(foo)::before',\n        'CustomAnnotation(foo)::after',\n        'CustomAnnotation(bar)::after',\n    ]\n\n    calls.clear()\n\n    class OuterModel(BaseModel):\n        x: Model\n\n        @classmethod\n        def __get_pydantic_json_schema__(cls, schema: CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            calls.append('OuterModel::before')\n            json_schema = handler(schema)\n            calls.append('OuterModel::after')\n            return json_schema\n\n    schema = OuterModel.model_json_schema()\n    expected: JsonSchemaValue = {\n        'type': 'object',\n        'properties': {'x': {'$ref': '#/$defs/Model'}},\n        'required': ['x'],\n        'title': 'OuterModel',\n        '$defs': {'Model': {'type': 'object', 'properties': {}, 'title': 'Model'}},\n    }\n\n    assert schema == expected\n    assert calls == [\n        'OuterModel::before',\n        'Model::before',\n        'Model::after',\n        'OuterModel::after',\n    ]\n\n    calls.clear()\n\n    AnnotatedModel = Annotated[Model, CustomAnnotation('foo')]\n\n    schema = TypeAdapter(AnnotatedModel).json_schema()\n    expected: JsonSchemaValue = {}\n\n    assert schema == expected\n    assert calls == [\n        'CustomAnnotation(foo)::before',\n        'Model::before',\n        'Model::after',\n        'CustomAnnotation(foo)::after',\n    ]\n\n    calls.clear()\n\n    class OuterModelWithAnnotatedField(BaseModel):\n        x: AnnotatedModel\n\n    schema = OuterModelWithAnnotatedField.model_json_schema()\n    expected: JsonSchemaValue = {\n        'type': 'object',\n        'properties': {'x': {'$ref': '#/$defs/Model'}},\n        'required': ['x'],\n        'title': 'OuterModel',\n        '$defs': {'Model': {'type': 'object', 'properties': {}, 'title': 'Model'}},\n    }\n\n    assert schema == expected\n    assert calls == [\n        'OuterModel::before',\n        'CustomAnnotation(foo)::before',\n        'Model::before',\n        'Model::after',\n        'CustomAnnotation(foo)::after',\n        'OuterModel::after',\n    ]\n\n    calls.clear()\n\n\ndef test_annotated_get_json_schema() -> None:\n    calls: List[int] = []\n\n    class CustomType(str):\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source_type: Any, handler: GetCoreSchemaHandler\n        ) -> core_schema.CoreSchema:\n            return handler(str)\n\n        @classmethod\n        def __get_pydantic_json_schema__(cls, schema: CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            calls.append(1)\n            json_schema = handler(schema)\n            return json_schema\n\n    TypeAdapter(Annotated[CustomType, 123]).json_schema()\n\n    assert sum(calls) == 1\n\n\ndef test_model_with_strict_mode():\n    class Model(BaseModel):\n        model_config = ConfigDict(strict=True)\n\n        a: str\n\n    assert Model.model_json_schema() == {\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_model_with_schema_extra():\n    class Model(BaseModel):\n        a: str\n\n        model_config = dict(json_schema_extra={'examples': [{'a': 'Foo'}]})\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {'a': {'title': 'A', 'type': 'string'}},\n        'required': ['a'],\n        'examples': [{'a': 'Foo'}],\n    }\n\n\ndef test_model_with_schema_extra_callable():\n    class Model(BaseModel):\n        name: str = None\n\n        @staticmethod\n        def json_schema_extra(schema, model_class):\n            schema.pop('properties')\n            schema['type'] = 'override'\n            assert model_class is Model\n\n        model_config = dict(json_schema_extra=json_schema_extra)\n\n    assert Model.model_json_schema() == {'title': 'Model', 'type': 'override'}\n\n\ndef test_model_with_schema_extra_callable_no_model_class():\n    class Model(BaseModel):\n        name: str = None\n\n        @classmethod\n        def json_schema_extra(cls, schema):\n            schema.pop('properties')\n            schema['type'] = 'override'\n\n        model_config = dict(json_schema_extra=json_schema_extra)\n\n    assert Model.model_json_schema() == {'title': 'Model', 'type': 'override'}\n\n\ndef test_model_with_schema_extra_callable_config_class():\n    with pytest.warns(PydanticDeprecatedSince20, match='use ConfigDict instead'):\n\n        class Model(BaseModel):\n            name: str = None\n\n            class Config:\n                @staticmethod\n                def json_schema_extra(schema, model_class):\n                    schema.pop('properties')\n                    schema['type'] = 'override'\n                    assert model_class is Model\n\n    assert Model.model_json_schema() == {'title': 'Model', 'type': 'override'}\n\n\ndef test_model_with_schema_extra_callable_no_model_class_config_class():\n    with pytest.warns(PydanticDeprecatedSince20):\n\n        class Model(BaseModel):\n            name: str = None\n\n            class Config:\n                @staticmethod\n                def json_schema_extra(schema):\n                    schema.pop('properties')\n                    schema['type'] = 'override'\n\n        assert Model.model_json_schema() == {'title': 'Model', 'type': 'override'}\n\n\ndef test_model_with_schema_extra_callable_classmethod():\n    with pytest.warns(PydanticDeprecatedSince20):\n\n        class Model(BaseModel):\n            name: str = None\n\n            class Config:\n                type = 'foo'\n\n                @classmethod\n                def json_schema_extra(cls, schema, model_class):\n                    schema.pop('properties')\n                    schema['type'] = cls.type\n                    assert model_class is Model\n\n        assert Model.model_json_schema() == {'title': 'Model', 'type': 'foo'}\n\n\ndef test_model_with_schema_extra_callable_instance_method():\n    with pytest.warns(PydanticDeprecatedSince20):\n\n        class Model(BaseModel):\n            name: str = None\n\n            class Config:\n                def json_schema_extra(schema, model_class):\n                    schema.pop('properties')\n                    schema['type'] = 'override'\n                    assert model_class is Model\n\n        assert Model.model_json_schema() == {'title': 'Model', 'type': 'override'}\n\n\ndef test_serialization_validation_interaction():\n    class Inner(BaseModel):\n        x: Json[int]\n\n    class Outer(BaseModel):\n        inner: Inner\n\n    _, v_schema = models_json_schema([(Outer, 'validation')])\n    assert v_schema == {\n        '$defs': {\n            'Inner': {\n                'properties': {\n                    'x': {\n                        'contentMediaType': 'application/json',\n                        'contentSchema': {'type': 'integer'},\n                        'title': 'X',\n                        'type': 'string',\n                    }\n                },\n                'required': ['x'],\n                'title': 'Inner',\n                'type': 'object',\n            },\n            'Outer': {\n                'properties': {'inner': {'$ref': '#/$defs/Inner'}},\n                'required': ['inner'],\n                'title': 'Outer',\n                'type': 'object',\n            },\n        }\n    }\n\n    _, s_schema = models_json_schema([(Outer, 'serialization')])\n    assert s_schema == {\n        '$defs': {\n            'Inner': {\n                'properties': {'x': {'title': 'X', 'type': 'integer'}},\n                'required': ['x'],\n                'title': 'Inner',\n                'type': 'object',\n            },\n            'Outer': {\n                'properties': {'inner': {'$ref': '#/$defs/Inner'}},\n                'required': ['inner'],\n                'title': 'Outer',\n                'type': 'object',\n            },\n        }\n    }\n\n    _, vs_schema = models_json_schema([(Outer, 'validation'), (Outer, 'serialization')])\n    assert vs_schema == {\n        '$defs': {\n            'Inner-Input': {\n                'properties': {\n                    'x': {\n                        'contentMediaType': 'application/json',\n                        'contentSchema': {'type': 'integer'},\n                        'title': 'X',\n                        'type': 'string',\n                    }\n                },\n                'required': ['x'],\n                'title': 'Inner',\n                'type': 'object',\n            },\n            'Inner-Output': {\n                'properties': {'x': {'title': 'X', 'type': 'integer'}},\n                'required': ['x'],\n                'title': 'Inner',\n                'type': 'object',\n            },\n            'Outer-Input': {\n                'properties': {'inner': {'$ref': '#/$defs/Inner-Input'}},\n                'required': ['inner'],\n                'title': 'Outer',\n                'type': 'object',\n            },\n            'Outer-Output': {\n                'properties': {'inner': {'$ref': '#/$defs/Inner-Output'}},\n                'required': ['inner'],\n                'title': 'Outer',\n                'type': 'object',\n            },\n        }\n    }\n\n\ndef test_extras_and_examples_are_json_encoded():\n    class Toy(BaseModel):\n        name: Annotated[str, Field(examples=['mouse', 'ball'])]\n\n    class Cat(BaseModel):\n        toys: Annotated[\n            List[Toy],\n            Field(examples=[[Toy(name='mouse'), Toy(name='ball')]], json_schema_extra={'special': Toy(name='bird')}),\n        ]\n\n    assert Cat.model_json_schema()['properties']['toys']['examples'] == [[{'name': 'mouse'}, {'name': 'ball'}]]\n    assert Cat.model_json_schema()['properties']['toys']['special'] == {'name': 'bird'}\n\n\ndef test_computed_field():\n    class Model(BaseModel):\n        x: int\n\n        @computed_field\n        @property\n        def double_x(self) -> int:\n            return 2 * self.x\n\n    assert Model.model_json_schema(mode='validation') == {\n        'properties': {'x': {'title': 'X', 'type': 'integer'}},\n        'required': ['x'],\n        'title': 'Model',\n        'type': 'object',\n    }\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {\n            'double_x': {'readOnly': True, 'title': 'Double X', 'type': 'integer'},\n            'x': {'title': 'X', 'type': 'integer'},\n        },\n        'required': ['x', 'double_x'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_serialization_schema_with_exclude():\n    class MyGenerateJsonSchema(GenerateJsonSchema):\n        def field_is_present(self, field) -> bool:\n            # Always include fields in the JSON schema, even if excluded from serialization\n            return True\n\n    class Model(BaseModel):\n        x: int\n        y: int = Field(exclude=True)\n\n    assert Model(x=1, y=1).model_dump() == {'x': 1}\n\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {'x': {'title': 'X', 'type': 'integer'}},\n        'required': ['x'],\n        'title': 'Model',\n        'type': 'object',\n    }\n    assert Model.model_json_schema(mode='serialization', schema_generator=MyGenerateJsonSchema) == {\n        'properties': {'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}},\n        'required': ['x', 'y'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize('mapping_type', [typing.Dict, typing.Mapping])\ndef test_mappings_str_int_json_schema(mapping_type: Any):\n    class Model(BaseModel):\n        str_int_map: mapping_type[str, int]\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'str_int_map': {\n                'title': 'Str Int Map',\n                'type': 'object',\n                'additionalProperties': {'type': 'integer'},\n            }\n        },\n        'required': ['str_int_map'],\n    }\n\n\n@pytest.mark.parametrize(('sequence_type'), [pytest.param(List), pytest.param(Sequence)])\ndef test_sequence_schema(sequence_type):\n    class Model(BaseModel):\n        field: sequence_type[int]\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'field': {'items': {'type': 'integer'}, 'title': 'Field', 'type': 'array'},\n        },\n        'required': ['field'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(('sequence_type',), [pytest.param(List), pytest.param(Sequence)])\ndef test_sequence_schema_with_max_length(sequence_type):\n    class Model(BaseModel):\n        field: sequence_type[int] = Field(max_length=5)\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'field': {'items': {'type': 'integer'}, 'maxItems': 5, 'title': 'Field', 'type': 'array'},\n        },\n        'required': ['field'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(('sequence_type',), [pytest.param(List), pytest.param(Sequence)])\ndef test_sequence_schema_with_min_length(sequence_type):\n    class Model(BaseModel):\n        field: sequence_type[int] = Field(min_length=1)\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'field': {'items': {'type': 'integer'}, 'minItems': 1, 'title': 'Field', 'type': 'array'},\n        },\n        'required': ['field'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.parametrize(('sequence_type',), [pytest.param(List), pytest.param(Sequence)])\ndef test_sequences_int_json_schema(sequence_type):\n    class Model(BaseModel):\n        int_seq: sequence_type[int]\n\n    assert Model.model_json_schema() == {\n        'title': 'Model',\n        'type': 'object',\n        'properties': {\n            'int_seq': {\n                'title': 'Int Seq',\n                'type': 'array',\n                'items': {'type': 'integer'},\n            },\n        },\n        'required': ['int_seq'],\n    }\n    assert Model.model_validate_json('{\"int_seq\": [1, 2, 3]}')\n\n\n@pytest.mark.parametrize(\n    'field_schema,model_schema',\n    [\n        (None, {'properties': {}, 'title': 'Model', 'type': 'object'}),\n        (\n            {'a': 'b'},\n            {'properties': {'x': {'a': 'b', 'title': 'X'}}, 'required': ['x'], 'title': 'Model', 'type': 'object'},\n        ),\n    ],\n)\n@pytest.mark.parametrize('instance_of', [True, False])\ndef test_arbitrary_type_json_schema(field_schema, model_schema, instance_of):\n    class ArbitraryClass:\n        pass\n\n    if instance_of:\n\n        class Model(BaseModel):\n            x: Annotated[InstanceOf[ArbitraryClass], WithJsonSchema(field_schema)]\n\n    else:\n\n        class Model(BaseModel):\n            model_config = dict(arbitrary_types_allowed=True)\n\n            x: Annotated[ArbitraryClass, WithJsonSchema(field_schema)]\n\n    assert Model.model_json_schema() == model_schema\n\n\n@pytest.mark.parametrize(\n    'metadata,json_schema',\n    [\n        (\n            WithJsonSchema({'type': 'float'}),\n            {\n                'properties': {'x': {'anyOf': [{'type': 'float'}, {'type': 'null'}], 'title': 'X'}},\n                'required': ['x'],\n                'title': 'Model',\n                'type': 'object',\n            },\n        ),\n        (\n            Examples({'Custom Example': [1, 2, 3]}),\n            {\n                'properties': {\n                    'x': {\n                        'anyOf': [{'examples': {'Custom Example': [1, 2, 3]}, 'type': 'integer'}, {'type': 'null'}],\n                        'title': 'X',\n                    }\n                },\n                'required': ['x'],\n                'title': 'Model',\n                'type': 'object',\n            },\n        ),\n    ],\n)\ndef test_hashable_types(metadata, json_schema):\n    class Model(BaseModel):\n        x: Union[Annotated[int, metadata], None]\n\n    assert Model.model_json_schema() == json_schema\n\n\ndef test_root_model():\n    class A(RootModel[int]):\n        \"\"\"A Model docstring\"\"\"\n\n    assert A.model_json_schema() == {'title': 'A', 'description': 'A Model docstring', 'type': 'integer'}\n\n    class B(RootModel[A]):\n        pass\n\n    assert B.model_json_schema() == {\n        '$defs': {'A': {'description': 'A Model docstring', 'title': 'A', 'type': 'integer'}},\n        'allOf': [{'$ref': '#/$defs/A'}],\n        'title': 'B',\n    }\n\n    class C(RootModel[A]):\n        \"\"\"C Model docstring\"\"\"\n\n    assert C.model_json_schema() == {\n        '$defs': {'A': {'description': 'A Model docstring', 'title': 'A', 'type': 'integer'}},\n        'allOf': [{'$ref': '#/$defs/A'}],\n        'title': 'C',\n        'description': 'C Model docstring',\n    }\n\n\ndef test_core_metadata_core_schema_metadata():\n    with pytest.raises(TypeError, match=re.escape(\"CoreSchema metadata should be a dict; got 'test'.\")):\n        CoreMetadataHandler({'metadata': 'test'})\n\n    core_metadata_handler = CoreMetadataHandler({})\n    core_metadata_handler._schema = {}\n    assert core_metadata_handler.metadata == {}\n    core_metadata_handler._schema = {'metadata': 'test'}\n    with pytest.raises(TypeError, match=re.escape(\"CoreSchema metadata should be a dict; got 'test'.\")):\n        core_metadata_handler.metadata\n\n\ndef test_build_metadata_dict_initial_metadata():\n    assert build_metadata_dict(initial_metadata={'foo': 'bar'}) == {\n        'foo': 'bar',\n        'pydantic_js_functions': [],\n        'pydantic_js_annotation_functions': [],\n    }\n\n    with pytest.raises(TypeError, match=re.escape(\"CoreSchema metadata should be a dict; got 'test'.\")):\n        build_metadata_dict(initial_metadata='test')\n\n\ndef test_type_adapter_json_schemas_title_description():\n    class Model(BaseModel):\n        a: str\n\n    _, json_schema = TypeAdapter.json_schemas([(Model, 'validation', TypeAdapter(Model))])\n    assert 'title' not in json_schema\n    assert 'description' not in json_schema\n\n    _, json_schema = TypeAdapter.json_schemas(\n        [(Model, 'validation', TypeAdapter(Model))],\n        title='test title',\n        description='test description',\n    )\n    assert json_schema['title'] == 'test title'\n    assert json_schema['description'] == 'test description'\n\n\ndef test_type_adapter_json_schemas_without_definitions():\n    _, json_schema = TypeAdapter.json_schemas(\n        [(int, 'validation', TypeAdapter(int))],\n        ref_template='#/components/schemas/{model}',\n    )\n\n    assert 'definitions' not in json_schema\n\n\ndef test_custom_chain_schema():\n    class MySequence:\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            list_schema = core_schema.list_schema()\n            return core_schema.chain_schema([list_schema])\n\n    class Model(BaseModel):\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n        a: MySequence\n\n    assert Model.model_json_schema() == {\n        'properties': {'a': {'items': {}, 'title': 'A', 'type': 'array'}},\n        'required': ['a'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_json_or_python_schema():\n    class MyJsonOrPython:\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            int_schema = core_schema.int_schema()\n            return core_schema.json_or_python_schema(json_schema=int_schema, python_schema=int_schema)\n\n    class Model(BaseModel):\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n        a: MyJsonOrPython\n\n    assert Model.model_json_schema() == {\n        'properties': {'a': {'title': 'A', 'type': 'integer'}},\n        'required': ['a'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_lax_or_strict_schema():\n    class MyLaxOrStrict:\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            int_schema = core_schema.int_schema()\n            return core_schema.lax_or_strict_schema(lax_schema=int_schema, strict_schema=int_schema, strict=True)\n\n    class Model(BaseModel):\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n        a: MyLaxOrStrict\n\n    assert Model.model_json_schema() == {\n        'properties': {'a': {'title': 'A', 'type': 'integer'}},\n        'required': ['a'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_override_enum_json_schema():\n    class CustomType(Enum):\n        A = 'a'\n        B = 'b'\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> core_schema.CoreSchema:\n            json_schema = handler(core_schema)\n            json_schema.update(title='CustomType title', type='string')\n            return json_schema\n\n    class Model(BaseModel):\n        x: CustomType\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {'CustomType': {'enum': ['a', 'b'], 'title': 'CustomType title', 'type': 'string'}},\n        'properties': {'x': {'$ref': '#/$defs/CustomType'}},\n        'required': ['x'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_json_schema_extras_on_ref() -> None:\n    @dataclass\n    class JsonSchemaExamples:\n        examples: Dict[str, Any]\n\n        def __get_pydantic_json_schema__(\n            self, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            json_schema = handler(core_schema)\n            assert json_schema.keys() == {'$ref'}\n            json_schema['examples'] = to_json(self.examples)\n            return json_schema\n\n    @dataclass\n    class JsonSchemaTitle:\n        title: str\n\n        def __get_pydantic_json_schema__(\n            self, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            json_schema = handler(core_schema)\n            assert json_schema.keys() == {'allOf', 'examples'}\n            json_schema['title'] = self.title\n            return json_schema\n\n    class Model(BaseModel):\n        name: str\n        age: int\n\n    ta = TypeAdapter(\n        Annotated[Model, JsonSchemaExamples({'foo': Model(name='John', age=28)}), JsonSchemaTitle('ModelTitle')]\n    )\n\n    # insert_assert(ta.json_schema())\n    assert ta.json_schema() == {\n        '$defs': {\n            'Model': {\n                'properties': {'age': {'title': 'Age', 'type': 'integer'}, 'name': {'title': 'Name', 'type': 'string'}},\n                'required': ['name', 'age'],\n                'title': 'Model',\n                'type': 'object',\n            }\n        },\n        'allOf': [{'$ref': '#/$defs/Model'}],\n        'examples': b'{\"foo\":{\"name\":\"John\",\"age\":28}}',\n        'title': 'ModelTitle',\n    }\n\n\ndef test_inclusion_of_defaults():\n    class Model(BaseModel):\n        x: int = 1\n        y: int = Field(default_factory=lambda: 2)\n\n    assert Model.model_json_schema() == {\n        'properties': {'x': {'default': 1, 'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}},\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_resolve_def_schema_from_core_schema() -> None:\n    class Inner(BaseModel):\n        x: int\n\n    class Marker:\n        def __get_pydantic_json_schema__(\n            self, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = handler(core_schema)\n            field_schema['title'] = 'Foo'\n            original_schema = handler.resolve_ref_schema(field_schema)\n            original_schema['title'] = 'Bar'\n            return field_schema\n\n    class Outer(BaseModel):\n        inner: Annotated[Inner, Marker()]\n\n    # insert_assert(Outer.model_json_schema())\n    assert Outer.model_json_schema() == {\n        '$defs': {\n            'Inner': {\n                'properties': {'x': {'title': 'X', 'type': 'integer'}},\n                'required': ['x'],\n                'title': 'Bar',\n                'type': 'object',\n            }\n        },\n        'properties': {'inner': {'allOf': [{'$ref': '#/$defs/Inner'}], 'title': 'Foo'}},\n        'required': ['inner'],\n        'title': 'Outer',\n        'type': 'object',\n    }\n\n\ndef test_examples_annotation() -> None:\n    ListWithExamples = Annotated[\n        List[float],\n        Examples({'Fibonacci': [1, 1, 2, 3, 5]}),\n    ]\n\n    ta = TypeAdapter(ListWithExamples)\n\n    # insert_assert(ta.json_schema())\n    assert ta.json_schema() == {\n        'examples': {'Fibonacci': [1, 1, 2, 3, 5]},\n        'items': {'type': 'number'},\n        'type': 'array',\n    }\n\n    ListWithMoreExamples = Annotated[\n        ListWithExamples,\n        Examples(\n            {\n                'Constants': [\n                    3.14,\n                    2.71,\n                ]\n            }\n        ),\n    ]\n\n    ta = TypeAdapter(ListWithMoreExamples)\n\n    # insert_assert(ta.json_schema())\n    assert ta.json_schema() == {\n        'examples': {'Constants': [3.14, 2.71], 'Fibonacci': [1, 1, 2, 3, 5]},\n        'items': {'type': 'number'},\n        'type': 'array',\n    }\n\n\ndef test_skip_json_schema_annotation() -> None:\n    class Model(BaseModel):\n        x: Union[int, SkipJsonSchema[None]] = None\n        y: Union[int, SkipJsonSchema[None]] = 1\n        z: Union[int, SkipJsonSchema[str]] = 'foo'\n\n    assert Model(y=None).y is None\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        'properties': {\n            'x': {'default': None, 'title': 'X', 'type': 'integer'},\n            'y': {'default': 1, 'title': 'Y', 'type': 'integer'},\n            'z': {'default': 'foo', 'title': 'Z', 'type': 'integer'},\n        },\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_skip_json_schema_exclude_default():\n    class Model(BaseModel):\n        x: Union[int, SkipJsonSchema[None]] = Field(default=None, json_schema_extra=lambda s: s.pop('default'))\n\n    assert Model().x is None\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        'properties': {\n            'x': {'title': 'X', 'type': 'integer'},\n        },\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\ndef test_typeddict_field_required_missing() -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/6192\"\"\"\n\n    class CustomType:\n        def __init__(self, data: Dict[str, int]) -> None:\n            self.data = data\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            data_schema = core_schema.typed_dict_schema(\n                {\n                    'subunits': core_schema.typed_dict_field(\n                        core_schema.int_schema(),\n                    ),\n                }\n            )\n            return core_schema.no_info_after_validator_function(cls, data_schema)\n\n    class Model(BaseModel):\n        t: CustomType\n\n    m = Model(t={'subunits': 123})\n    assert type(m.t) is CustomType\n    assert m.t.data == {'subunits': 123}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(t={'subunits': 'abc'})\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('t', 'subunits'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'abc',\n        }\n    ]\n\n\ndef test_json_schema_keys_sorting() -> None:\n    \"\"\"We sort all keys except those under a 'property' parent key\"\"\"\n\n    class Model(BaseModel):\n        b: int\n        a: str\n\n    class OuterModel(BaseModel):\n        inner: List[Model] = Field(default=[Model(b=1, a='fruit')])\n\n    # verify the schema contents\n    # this is just to get a nicer error message / diff if it fails\n    expected = {\n        '$defs': {\n            'Model': {\n                'properties': {'b': {'title': 'B', 'type': 'integer'}, 'a': {'title': 'A', 'type': 'string'}},\n                'required': ['b', 'a'],\n                'title': 'Model',\n                'type': 'object',\n            }\n        },\n        'properties': {\n            'inner': {\n                'default': [{'b': 1, 'a': 'fruit'}],\n                'items': {'$ref': '#/$defs/Model'},\n                'title': 'Inner',\n                'type': 'array',\n            }\n        },\n        'title': 'OuterModel',\n        'type': 'object',\n    }\n    actual = OuterModel.model_json_schema()\n    assert actual == expected\n\n    # verify order\n    # dumping to json just happens to be a simple way to verify the order\n    assert json.dumps(actual, indent=2) == json.dumps(expected, indent=2)\n\n\ndef test_custom_type_gets_unpacked_ref() -> None:\n    class Annotation:\n        def __get_pydantic_json_schema__(\n            self, schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            json_schema = handler(schema)\n            assert '$ref' in json_schema\n            json_schema['title'] = 'Set from annotation'\n            return json_schema\n\n    class Model(BaseModel):\n        x: int\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            json_schema = handler(schema)\n            assert json_schema['type'] == 'object' and '$ref' not in json_schema\n            return json_schema\n\n    ta = TypeAdapter(Annotated[Model, Annotation()])\n    # insert_assert(ta.json_schema())\n    assert ta.json_schema() == {\n        '$defs': {\n            'Model': {\n                'properties': {'x': {'title': 'X', 'type': 'integer'}},\n                'required': ['x'],\n                'title': 'Model',\n                'type': 'object',\n            }\n        },\n        'allOf': [{'$ref': '#/$defs/Model'}],\n        'title': 'Set from annotation',\n    }\n\n\n@pytest.mark.parametrize(\n    'annotation, expected',\n    [\n        (Annotated[int, Field(json_schema_extra={'title': 'abc'})], {'type': 'integer', 'title': 'abc'}),\n        (\n            Annotated[int, Field(title='abc'), Field(description='xyz')],\n            {'type': 'integer', 'title': 'abc', 'description': 'xyz'},\n        ),\n        (Annotated[int, Field(gt=0)], {'type': 'integer', 'exclusiveMinimum': 0}),\n        (\n            Annotated[int, Field(gt=0), Field(lt=100)],\n            {'type': 'integer', 'exclusiveMinimum': 0, 'exclusiveMaximum': 100},\n        ),\n        (Annotated[int, Field(examples={'number': 1})], {'type': 'integer', 'examples': {'number': 1}}),\n    ],\n    ids=repr,\n)\ndef test_field_json_schema_metadata(annotation: Type[Any], expected: JsonSchemaValue) -> None:\n    ta = TypeAdapter(annotation)\n    assert ta.json_schema() == expected\n\n\ndef test_multiple_models_with_same_qualname():\n    from pydantic import create_model\n\n    model_a1 = create_model(\n        'A',\n        inner_a1=(str, ...),\n    )\n    model_a2 = create_model(\n        'A',\n        inner_a2=(str, ...),\n    )\n\n    model_c = create_model(\n        'B',\n        outer_a1=(model_a1, ...),\n        outer_a2=(model_a2, ...),\n    )\n\n    # insert_assert(model_c.model_json_schema())\n    assert model_c.model_json_schema() == {\n        '$defs': {\n            'tests__test_json_schema__A__1': {\n                'properties': {'inner_a1': {'title': 'Inner A1', 'type': 'string'}},\n                'required': ['inner_a1'],\n                'title': 'A',\n                'type': 'object',\n            },\n            'tests__test_json_schema__A__2': {\n                'properties': {'inner_a2': {'title': 'Inner A2', 'type': 'string'}},\n                'required': ['inner_a2'],\n                'title': 'A',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'outer_a1': {'$ref': '#/$defs/tests__test_json_schema__A__1'},\n            'outer_a2': {'$ref': '#/$defs/tests__test_json_schema__A__2'},\n        },\n        'required': ['outer_a1', 'outer_a2'],\n        'title': 'B',\n        'type': 'object',\n    }\n\n\ndef test_generate_definitions_for_no_ref_schemas():\n    decimal_schema = TypeAdapter(Decimal).core_schema\n\n    class Model(BaseModel):\n        pass\n\n    result = GenerateJsonSchema().generate_definitions(\n        [\n            ('Decimal', 'validation', decimal_schema),\n            ('Decimal', 'serialization', decimal_schema),\n            ('Model', 'validation', Model.__pydantic_core_schema__),\n        ]\n    )\n    assert result == (\n        {\n            ('Decimal', 'serialization'): {'type': 'string'},\n            ('Decimal', 'validation'): {'anyOf': [{'type': 'number'}, {'type': 'string'}]},\n            ('Model', 'validation'): {'$ref': '#/$defs/Model'},\n        },\n        {'Model': {'properties': {}, 'title': 'Model', 'type': 'object'}},\n    )\n\n\ndef test_chain_schema():\n    # this is a contrived schema which requires a string input that can be coerced to an int:\n    s = core_schema.chain_schema([core_schema.str_schema(), core_schema.int_schema()])\n    assert SchemaValidator(s).validate_python('1') == 1  # proof it works this way\n\n    assert GenerateJsonSchema().generate(s, mode='validation') == {'type': 'string'}\n    assert GenerateJsonSchema().generate(s, mode='serialization') == {'type': 'integer'}\n\n\ndef test_deferred_json_schema():\n    class Foo(BaseModel):\n        x: 'Bar'\n\n    with pytest.raises(PydanticUserError, match='`Foo` is not fully defined'):\n        Foo.model_json_schema()\n\n    class Bar(BaseModel):\n        pass\n\n    Foo.model_rebuild()\n    assert Foo.model_json_schema() == {\n        '$defs': {'Bar': {'properties': {}, 'title': 'Bar', 'type': 'object'}},\n        'properties': {'x': {'$ref': '#/$defs/Bar'}},\n        'required': ['x'],\n        'title': 'Foo',\n        'type': 'object',\n    }\n\n\ndef test_dollar_ref_alias():\n    class MyModel(BaseModel):\n        my_field: str = Field(alias='$ref')\n\n    assert MyModel.model_json_schema() == {\n        'properties': {'$ref': {'title': '$Ref', 'type': 'string'}},\n        'required': ['$ref'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n\n\ndef test_multiple_parametrization_of_generic_model() -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/6708\"\"\"\n    T = TypeVar('T')\n\n    calls = 0\n\n    class Inner(BaseModel):\n        a: int\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            nonlocal calls\n            calls += 1\n            json_schema = handler(core_schema)\n            return json_schema\n\n    class Outer(BaseModel, Generic[T]):\n        b: Optional[T]\n\n    class ModelTest(BaseModel):\n        c: Outer[Inner]\n\n    for _ in range(sys.getrecursionlimit() + 1):\n\n        class ModelTest(BaseModel):\n            c: Outer[Inner]\n\n    ModelTest.model_json_schema()\n\n    # this is not necessarily a promise we make\n    # (in fact, we've had bugs in the past where this was not the case and we'd\n    # call the __get_pydantic_json_schema__ method multiple times)\n    # but it's much easier to test for than absence of a recursion limit\n    assert calls == 1\n\n\ndef test_callable_json_schema_extra():\n    def pop_default(s):\n        s.pop('default')\n\n    class Model(BaseModel):\n        a: int = Field(default=1, json_schema_extra=pop_default)\n        b: Annotated[int, Field(default=2), Field(json_schema_extra=pop_default)]\n        c: Annotated[int, Field(default=3)] = Field(json_schema_extra=pop_default)\n\n    assert Model().model_dump() == {'a': 1, 'b': 2, 'c': 3}\n    assert Model(a=11, b=12, c=13).model_dump() == {\n        'a': 11,\n        'b': 12,\n        'c': 13,\n    }\n\n    json_schema = Model.model_json_schema()\n    for key in 'abc':\n        assert json_schema['properties'][key] == {'title': key.upper(), 'type': 'integer'}  # default is not present\n\n\ndef test_callable_json_schema_extra_dataclass():\n    def pop_default(s):\n        s.pop('default')\n\n    @pydantic.dataclasses.dataclass\n    class MyDataclass:\n        # Note that a and b here have to come first since dataclasses requires annotation-only fields to come before\n        # fields with defaults (for similar reasons to why function arguments with defaults must come later)\n        # But otherwise, evnerything seems to work properly\n        a: Annotated[int, Field(json_schema_extra=pop_default), Field(default=1)]\n        b: Annotated[int, Field(default=2), Field(json_schema_extra=pop_default)]\n        c: int = Field(default=3, json_schema_extra=pop_default)\n        d: Annotated[int, Field(json_schema_extra=pop_default)] = 4\n        e: Annotated[int, Field(json_schema_extra=pop_default)] = Field(default=5)\n        f: Annotated[int, Field(default=6)] = Field(json_schema_extra=pop_default)\n\n    adapter = TypeAdapter(MyDataclass)\n    assert adapter.dump_python(MyDataclass()) == {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6}\n    assert adapter.dump_python(MyDataclass(a=11, b=12, c=13, d=14, e=15, f=16)) == {\n        'a': 11,\n        'b': 12,\n        'c': 13,\n        'd': 14,\n        'e': 15,\n        'f': 16,\n    }\n\n    json_schema = adapter.json_schema()\n    for key in 'abcdef':\n        assert json_schema['properties'][key] == {'title': key.upper(), 'type': 'integer'}  # default is not present\n\n\ndef test_model_rebuild_happens_even_with_parent_classes(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\nclass MyBaseModel(BaseModel):\n    pass\n\nclass B(MyBaseModel):\n    b: A\n\nclass A(MyBaseModel):\n    a: str\n    \"\"\"\n    )\n    assert module.B.model_json_schema() == {\n        '$defs': {\n            'A': {\n                'properties': {'a': {'title': 'A', 'type': 'string'}},\n                'required': ['a'],\n                'title': 'A',\n                'type': 'object',\n            }\n        },\n        'properties': {'b': {'$ref': '#/$defs/A'}},\n        'required': ['b'],\n        'title': 'B',\n        'type': 'object',\n    }\n\n\ndef test_enum_complex_value() -> None:\n    \"\"\"https://github.com/pydantic/pydantic/issues/7045\"\"\"\n\n    class MyEnum(Enum):\n        foo = (1, 2)\n        bar = (2, 3)\n\n    ta = TypeAdapter(MyEnum)\n\n    # insert_assert(ta.json_schema())\n    assert ta.json_schema() == {'enum': [[1, 2], [2, 3]], 'title': 'MyEnum', 'type': 'array'}\n\n\ndef test_json_schema_serialization_defaults_required():\n    class Model(BaseModel):\n        a: str = 'a'\n\n    class SerializationDefaultsRequiredModel(Model):\n        model_config = ConfigDict(json_schema_serialization_defaults_required=True)\n\n    model_schema = Model.model_json_schema(mode='serialization')\n    sdr_model_schema = SerializationDefaultsRequiredModel.model_json_schema(mode='serialization')\n\n    assert 'required' not in model_schema\n    assert sdr_model_schema['required'] == ['a']\n\n\ndef test_json_schema_mode_override():\n    class Model(BaseModel):\n        a: Json[int]  # requires a string to validate, but will dump an int\n\n    class ValidationModel(Model):\n        model_config = ConfigDict(json_schema_mode_override='validation', title='Model')\n\n    class SerializationModel(Model):\n        model_config = ConfigDict(json_schema_mode_override='serialization', title='Model')\n\n    # Ensure the ValidationModel and SerializationModel schemas do not depend on the value of the mode\n    assert ValidationModel.model_json_schema(mode='validation') == ValidationModel.model_json_schema(\n        mode='serialization'\n    )\n    assert SerializationModel.model_json_schema(mode='validation') == SerializationModel.model_json_schema(\n        mode='serialization'\n    )\n\n    # Ensure the two submodels models have different JSON schemas\n    assert ValidationModel.model_json_schema() != SerializationModel.model_json_schema()\n\n    # Ensure the submodels' JSON schemas match the expected mode even when the opposite value is specified:\n    assert ValidationModel.model_json_schema(mode='serialization') == Model.model_json_schema(mode='validation')\n    assert SerializationModel.model_json_schema(mode='validation') == Model.model_json_schema(mode='serialization')\n\n\ndef test_models_json_schema_generics() -> None:\n    class G(BaseModel, Generic[T]):\n        foo: T\n\n    class M(BaseModel):\n        foo: Literal['a', 'b']\n\n    GLiteral = G[Literal['a', 'b']]\n\n    assert models_json_schema(\n        [\n            (GLiteral, 'serialization'),\n            (GLiteral, 'validation'),\n            (M, 'validation'),\n        ]\n    ) == (\n        {\n            (GLiteral, 'serialization'): {'$ref': '#/$defs/G_Literal__a____b___'},\n            (GLiteral, 'validation'): {'$ref': '#/$defs/G_Literal__a____b___'},\n            (M, 'validation'): {'$ref': '#/$defs/M'},\n        },\n        {\n            '$defs': {\n                'G_Literal__a____b___': {\n                    'properties': {'foo': {'enum': ['a', 'b'], 'title': 'Foo', 'type': 'string'}},\n                    'required': ['foo'],\n                    'title': \"G[Literal['a', 'b']]\",\n                    'type': 'object',\n                },\n                'M': {\n                    'properties': {'foo': {'enum': ['a', 'b'], 'title': 'Foo', 'type': 'string'}},\n                    'required': ['foo'],\n                    'title': 'M',\n                    'type': 'object',\n                },\n            }\n        },\n    )\n\n\ndef test_recursive_non_generic_model() -> None:\n    class Foo(BaseModel):\n        maybe_bar: Union[None, 'Bar']\n\n    class Bar(BaseModel):\n        foo: Foo\n\n    # insert_assert(Bar(foo=Foo(maybe_bar=None)).model_dump())\n    assert Bar.model_validate({'foo': {'maybe_bar': None}}).model_dump() == {'foo': {'maybe_bar': None}}\n    # insert_assert(Bar.model_json_schema())\n    assert Bar.model_json_schema() == {\n        '$defs': {\n            'Bar': {\n                'properties': {'foo': {'$ref': '#/$defs/Foo'}},\n                'required': ['foo'],\n                'title': 'Bar',\n                'type': 'object',\n            },\n            'Foo': {\n                'properties': {'maybe_bar': {'anyOf': [{'$ref': '#/$defs/Bar'}, {'type': 'null'}]}},\n                'required': ['maybe_bar'],\n                'title': 'Foo',\n                'type': 'object',\n            },\n        },\n        'allOf': [{'$ref': '#/$defs/Bar'}],\n    }\n\n\ndef test_module_with_colon_in_name(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom pydantic import BaseModel\n\nclass Foo(BaseModel):\n    x: int\n        \"\"\",\n        module_name_prefix='C:\\\\',\n    )\n\n    foo_model = module.Foo\n    _, v_schema = models_json_schema([(foo_model, 'validation')])\n    assert v_schema == {\n        '$defs': {\n            'Foo': {\n                'properties': {'x': {'title': 'X', 'type': 'integer'}},\n                'required': ['x'],\n                'title': 'Foo',\n                'type': 'object',\n            }\n        }\n    }\n\n\ndef test_repeated_custom_type():\n    class Numeric(pydantic.BaseModel):\n        value: float\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: pydantic.GetCoreSchemaHandler) -> CoreSchema:\n            return core_schema.no_info_before_validator_function(cls.validate, handler(source_type))\n\n        @classmethod\n        def validate(cls, v: Any) -> Union[Dict[str, Any], Self]:\n            if isinstance(v, (str, float, int)):\n                return cls(value=v)\n            if isinstance(v, Numeric):\n                return v\n            if isinstance(v, dict):\n                return v\n            raise ValueError(f'Invalid value for {cls}: {v}')\n\n    def is_positive(value: Numeric):\n        assert value.value > 0.0, 'Must be positive'\n\n    class OuterModel(pydantic.BaseModel):\n        x: Numeric\n        y: Numeric\n        z: Annotated[Numeric, AfterValidator(is_positive)]\n\n    assert OuterModel(x=2, y=-1, z=1)\n\n    with pytest.raises(ValidationError):\n        OuterModel(x=2, y=-1, z=-1)\n\n\ndef test_description_not_included_for_basemodel() -> None:\n    class Model(BaseModel):\n        x: BaseModel\n\n    assert 'description' not in Model.model_json_schema()['$defs']['BaseModel']\n\n\ndef test_recursive_json_schema_build() -> None:\n    \"\"\"\n    Schema build for this case is a bit complicated due to the recursive nature of the models.\n    This was reported as broken in https://github.com/pydantic/pydantic/issues/8689, which was\n    originally caused by the change made in https://github.com/pydantic/pydantic/pull/8583, which has\n    since been reverted.\n    \"\"\"\n\n    class AllowedValues(str, Enum):\n        VAL1 = 'Val1'\n        VAL2 = 'Val2'\n\n    class ModelA(BaseModel):\n        modelA_1: AllowedValues = Field(..., max_length=60)\n\n    class ModelB(ModelA):\n        modelB_1: typing.List[ModelA]\n\n    class ModelC(BaseModel):\n        modelC_1: ModelB\n\n    class Model(BaseModel):\n        b: ModelB\n        c: ModelC\n\n    assert Model.model_json_schema()\n\n\ndef test_json_schema_annotated_with_field() -> None:\n    \"\"\"Ensure field specified with Annotated in create_model call is still marked as required.\"\"\"\n\n    from pydantic import create_model\n\n    Model = create_model(\n        'test_model',\n        bar=(Annotated[int, Field(description='Bar description')], ...),\n    )\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'bar': {'description': 'Bar description', 'title': 'Bar', 'type': 'integer'},\n        },\n        'required': ['bar'],\n        'title': 'test_model',\n        'type': 'object',\n    }\n\n\ndef test_required_fields_in_annotated_with_create_model() -> None:\n    \"\"\"Ensure multiple field specified with Annotated in create_model call is still marked as required.\"\"\"\n\n    from pydantic import create_model\n\n    Model = create_model(\n        'test_model',\n        foo=(int, ...),\n        bar=(Annotated[int, Field(description='Bar description')], ...),\n        baz=(Annotated[int, Field(..., description='Baz description')], ...),\n    )\n\n    assert Model.model_json_schema() == {\n        'properties': {\n            'foo': {'title': 'Foo', 'type': 'integer'},\n            'bar': {'description': 'Bar description', 'title': 'Bar', 'type': 'integer'},\n            'baz': {'description': 'Baz description', 'title': 'Baz', 'type': 'integer'},\n        },\n        'required': ['foo', 'bar', 'baz'],\n        'title': 'test_model',\n        'type': 'object',\n    }\n\n\ndef test_required_fields_in_annotated_with_basemodel() -> None:\n    \"\"\"\n    Ensure multiple field specified with Annotated in BaseModel is marked as required.\n    \"\"\"\n\n    class Model(BaseModel):\n        a: int = ...\n        b: Annotated[int, 'placeholder'] = ...\n        c: Annotated[int, Field()] = ...\n\n    assert Model.model_fields['a'].is_required()\n    assert Model.model_fields['b'].is_required()\n    assert Model.model_fields['c'].is_required()\n\n\n@pytest.mark.parametrize(\n    'field_type,default_value,expected_schema',\n    [\n        (\n            IPvAnyAddress,\n            IPv4Address('127.0.0.1'),\n            {\n                'properties': {\n                    'field': {'default': '127.0.0.1', 'format': 'ipvanyaddress', 'title': 'Field', 'type': 'string'}\n                },\n                'title': 'Model',\n                'type': 'object',\n            },\n        ),\n        (\n            IPvAnyAddress,\n            IPv6Address('::1'),\n            {\n                'properties': {\n                    'field': {'default': '::1', 'format': 'ipvanyaddress', 'title': 'Field', 'type': 'string'}\n                },\n                'title': 'Model',\n                'type': 'object',\n            },\n        ),\n    ],\n)\ndef test_default_value_encoding(field_type, default_value, expected_schema):\n    class Model(BaseModel):\n        field: field_type = default_value\n\n    schema = Model.model_json_schema()\n    assert schema == expected_schema\n\n\ndef _generate_deprecated_classes():\n    @deprecated('MyModel is deprecated')\n    class MyModel(BaseModel):\n        pass\n\n    @deprecated('MyPydanticDataclass is deprecated')\n    @pydantic.dataclasses.dataclass\n    class MyPydanticDataclass:\n        pass\n\n    @deprecated('MyBuiltinDataclass is deprecated')\n    @dataclasses.dataclass\n    class MyBuiltinDataclass:\n        pass\n\n    @deprecated('MyTypedDict is deprecated')\n    class MyTypedDict(TypedDict):\n        pass\n\n    return [\n        pytest.param(MyModel, id='BaseModel'),\n        pytest.param(MyPydanticDataclass, id='pydantic-dataclass'),\n        pytest.param(MyBuiltinDataclass, id='builtin-dataclass'),\n        pytest.param(MyTypedDict, id='TypedDict'),\n    ]\n\n\n@pytest.mark.skipif(\n    Version(importlib.metadata.version('typing_extensions')) < Version('4.9'),\n    reason='`deprecated` type annotation requires typing_extensions>=4.9',\n)\n@pytest.mark.parametrize('cls', _generate_deprecated_classes())\ndef test_deprecated_classes_json_schema(cls):\n    assert hasattr(cls, '__deprecated__')\n    assert TypeAdapter(cls).json_schema()['deprecated']\n\n\n@pytest.mark.skipif(\n    Version(importlib.metadata.version('typing_extensions')) < Version('4.9'),\n    reason='`deprecated` type annotation requires typing_extensions>=4.9',\n)\n@pytest.mark.parametrize('cls', _generate_deprecated_classes())\ndef test_deprecated_subclasses_json_schema(cls):\n    class Model(BaseModel):\n        subclass: cls\n\n    assert Model.model_json_schema() == {\n        '$defs': {cls.__name__: {'deprecated': True, 'properties': {}, 'title': f'{cls.__name__}', 'type': 'object'}},\n        'properties': {'subclass': {'$ref': f'#/$defs/{cls.__name__}'}},\n        'required': ['subclass'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.mark.skipif(\n    Version(importlib.metadata.version('typing_extensions')) < Version('4.9'),\n    reason='`deprecated` type annotation requires typing_extensions>=4.9',\n)\n@pytest.mark.parametrize('cls', _generate_deprecated_classes())\ndef test_deprecated_class_usage_warns(cls):\n    if issubclass(cls, dict):\n        pytest.skip('TypedDict does not generate a DeprecationWarning on usage')\n    with pytest.warns(DeprecationWarning, match=f'{cls.__name__} is deprecated'):\n        cls()\n\n\n@dataclasses.dataclass\nclass BuiltinDataclassParent:\n    name: str\n\n\n@pydantic.dataclasses.dataclass\nclass PydanticDataclassParent:\n    name: str\n\n\nclass TypedDictParent(TypedDict):\n    name: str\n\n\nclass ModelParent(BaseModel):\n    name: str\n\n\n@pytest.mark.parametrize(\n    'pydantic_type,expected_json_schema',\n    [\n        pytest.param(\n            BuiltinDataclassParent,\n            {\n                '$defs': {\n                    'BuiltinDataclassParent': {\n                        'properties': {'name': {'title': 'Name', 'type': 'string'}},\n                        'required': ['name'],\n                        'title': 'BuiltinDataclassParent',\n                        'type': 'object',\n                    }\n                },\n                'properties': {\n                    'parent': {'allOf': [{'$ref': '#/$defs/BuiltinDataclassParent'}], 'default': {'name': 'Jon Doe'}}\n                },\n                'title': 'child',\n                'type': 'object',\n            },\n            id='builtin-dataclass',\n        ),\n        pytest.param(\n            PydanticDataclassParent,\n            {\n                '$defs': {\n                    'PydanticDataclassParent': {\n                        'properties': {'name': {'title': 'Name', 'type': 'string'}},\n                        'required': ['name'],\n                        'title': 'PydanticDataclassParent',\n                        'type': 'object',\n                    }\n                },\n                'properties': {\n                    'parent': {'allOf': [{'$ref': '#/$defs/PydanticDataclassParent'}], 'default': {'name': 'Jon Doe'}}\n                },\n                'title': 'child',\n                'type': 'object',\n            },\n            id='pydantic-dataclass',\n        ),\n        pytest.param(\n            TypedDictParent,\n            {\n                '$defs': {\n                    'TypedDictParent': {\n                        'properties': {'name': {'title': 'Name', 'type': 'string'}},\n                        'required': ['name'],\n                        'title': 'TypedDictParent',\n                        'type': 'object',\n                    }\n                },\n                'properties': {\n                    'parent': {'allOf': [{'$ref': '#/$defs/TypedDictParent'}], 'default': {'name': 'Jon Doe'}}\n                },\n                'title': 'child',\n                'type': 'object',\n            },\n            id='typeddict',\n        ),\n        pytest.param(\n            ModelParent,\n            {\n                '$defs': {\n                    'ModelParent': {\n                        'properties': {'name': {'title': 'Name', 'type': 'string'}},\n                        'required': ['name'],\n                        'title': 'ModelParent',\n                        'type': 'object',\n                    }\n                },\n                'properties': {'parent': {'allOf': [{'$ref': '#/$defs/ModelParent'}], 'default': {'name': 'Jon Doe'}}},\n                'title': 'child',\n                'type': 'object',\n            },\n            id='model',\n        ),\n    ],\n)\ndef test_pydantic_types_as_default_values(pydantic_type, expected_json_schema):\n    class Child(BaseModel):\n        model_config = ConfigDict(title='child')\n        parent: pydantic_type = pydantic_type(name='Jon Doe')\n\n    assert Child.model_json_schema() == expected_json_schema\n\n\ndef test_str_schema_with_pattern() -> None:\n    assert TypeAdapter(Annotated[str, Field(pattern='abc')]).json_schema() == {'type': 'string', 'pattern': 'abc'}\n    assert TypeAdapter(Annotated[str, Field(pattern=re.compile('abc'))]).json_schema() == {\n        'type': 'string',\n        'pattern': 'abc',\n    }\n\n\ndef test_plain_serializer_applies_to_default() -> None:\n    class Model(BaseModel):\n        custom_str: Annotated[str, PlainSerializer(lambda x: f'serialized-{x}', return_type=str)] = 'foo'\n\n    assert Model.model_json_schema(mode='validation') == {\n        'properties': {'custom_str': {'default': 'foo', 'title': 'Custom Str', 'type': 'string'}},\n        'title': 'Model',\n        'type': 'object',\n    }\n    assert Model.model_json_schema(mode='serialization') == {\n        'properties': {'custom_str': {'default': 'serialized-foo', 'title': 'Custom Str', 'type': 'string'}},\n        'title': 'Model',\n        'type': 'object',\n    }\n", "tests/test_private_attributes.py": "import functools\nfrom typing import ClassVar, Generic, TypeVar\n\nimport pytest\nfrom pydantic_core import PydanticUndefined\n\nfrom pydantic import BaseModel, ConfigDict, PrivateAttr, computed_field\n\n\ndef test_private_attribute():\n    default = {'a': {}}\n\n    class Model(BaseModel):\n        _foo = PrivateAttr(default)\n\n    assert set(Model.__private_attributes__) == {'_foo'}\n\n    m = Model()\n    assert m._foo == default\n    assert m._foo is not default\n    assert m._foo['a'] is not default['a']\n\n    m._foo = None\n    assert m._foo is None\n\n    assert m.model_dump() == {}\n    assert m.__dict__ == {}\n\n\ndef test_private_attribute_double_leading_underscore():\n    default = {'a': {}}\n\n    class Model(BaseModel):\n        __foo = PrivateAttr(default)\n\n    assert set(Model.__private_attributes__) == {'_Model__foo'}\n\n    m = Model()\n\n    with pytest.raises(AttributeError, match='__foo'):\n        m.__foo\n    assert m._Model__foo == default\n    assert m._Model__foo is not default\n    assert m._Model__foo['a'] is not default['a']\n\n    m._Model__foo = None\n    assert m._Model__foo is None\n\n    assert m.model_dump() == {}\n    assert m.__dict__ == {}\n\n\ndef test_private_attribute_nested():\n    class SubModel(BaseModel):\n        _foo = PrivateAttr(42)\n        x: int\n\n    class Model(BaseModel):\n        y: int\n        sub: SubModel\n\n    m = Model(y=1, sub={'x': 2})\n    assert m.sub._foo == 42\n\n\ndef test_private_attribute_factory():\n    default = {'a': {}}\n\n    def factory():\n        return default\n\n    class Model(BaseModel):\n        _foo = PrivateAttr(default_factory=factory)\n\n    assert Model.__private_attributes__ == {'_foo': PrivateAttr(default_factory=factory)}\n\n    m = Model()\n    assert m._foo == default\n    assert m._foo is default\n    assert m._foo['a'] is default['a']\n\n    m._foo = None\n    assert m._foo is None\n\n    assert m.model_dump() == {}\n    assert m.__dict__ == {}\n\n\ndef test_private_attribute_annotation():\n    class Model(BaseModel):\n        \"\"\"The best model\"\"\"\n\n        _foo: str\n\n    assert Model.__private_attributes__ == {'_foo': PrivateAttr(PydanticUndefined)}\n    assert repr(Model.__doc__) == \"'The best model'\"\n\n    m = Model()\n    with pytest.raises(AttributeError):\n        m._foo\n\n    m._foo = '123'\n    assert m._foo == '123'\n\n    m._foo = None\n    assert m._foo is None\n\n    del m._foo\n\n    with pytest.raises(AttributeError):\n        m._foo\n\n    m._foo = '123'\n    assert m._foo == '123'\n\n    assert m.model_dump() == {}\n    assert m.__dict__ == {}\n\n\ndef test_underscore_attrs_are_private():\n    class Model(BaseModel):\n        _foo: str = 'abc'\n        _bar: ClassVar[str] = 'cba'\n\n    assert Model._bar == 'cba'\n    assert Model.__private_attributes__ == {'_foo': PrivateAttr('abc')}\n\n    m = Model()\n    assert m._foo == 'abc'\n    m._foo = None\n    assert m._foo is None\n\n    with pytest.raises(\n        AttributeError,\n        match=(\n            \"'_bar' is a ClassVar of `Model` and cannot be set on an instance. \"\n            'If you want to set a value on the class, use `Model._bar = value`.'\n        ),\n    ):\n        m._bar = 1\n\n\ndef test_private_attribute_intersection_with_extra_field():\n    class Model(BaseModel):\n        _foo = PrivateAttr('private_attribute')\n\n        model_config = ConfigDict(extra='allow')\n\n    assert set(Model.__private_attributes__) == {'_foo'}\n    m = Model(_foo='field')\n    assert m._foo == 'private_attribute'\n    assert m.__dict__ == {}\n    assert m.__pydantic_extra__ == {'_foo': 'field'}\n    assert m.model_dump() == {'_foo': 'field'}\n\n    m._foo = 'still_private'\n    assert m._foo == 'still_private'\n    assert m.__dict__ == {}\n    assert m.__pydantic_extra__ == {'_foo': 'field'}\n    assert m.model_dump() == {'_foo': 'field'}\n\n\ndef test_private_attribute_invalid_name():\n    with pytest.raises(\n        NameError,\n        match=\"Private attributes must not use valid field names; use sunder names, e.g. '_foo' instead of 'foo'.\",\n    ):\n\n        class Model(BaseModel):\n            foo = PrivateAttr()\n\n\ndef test_slots_are_ignored():\n    class Model(BaseModel):\n        __slots__ = (\n            'foo',\n            '_bar',\n        )\n\n        def __init__(self):\n            super().__init__()\n            for attr_ in self.__slots__:\n                object.__setattr__(self, attr_, 'spam')\n\n    assert Model.__private_attributes__ == {}\n    assert set(Model.__slots__) == {'foo', '_bar'}\n    m1 = Model()\n    m2 = Model()\n\n    for attr in Model.__slots__:\n        assert object.__getattribute__(m1, attr) == 'spam'\n\n    # In v2, you are always allowed to set instance attributes if the name starts with `_`.\n    m1._bar = 'not spam'\n    assert m1._bar == 'not spam'\n    assert m2._bar == 'spam'\n\n    with pytest.raises(ValueError, match='\"Model\" object has no field \"foo\"'):\n        m1.foo = 'not spam'\n\n\ndef test_default_and_default_factory_used_error():\n    with pytest.raises(TypeError, match='cannot specify both default and default_factory'):\n        PrivateAttr(default=123, default_factory=lambda: 321)\n\n\ndef test_config_override_init():\n    class MyModel(BaseModel):\n        x: str\n        _private_attr: int\n\n        def __init__(self, **data) -> None:\n            super().__init__(**data)\n            self._private_attr = 123\n\n    m = MyModel(x='hello')\n    assert m.model_dump() == {'x': 'hello'}\n    assert m._private_attr == 123\n\n\ndef test_generic_private_attribute():\n    T = TypeVar('T')\n\n    class Model(BaseModel, Generic[T]):\n        value: T\n        _private_value: T\n\n    m = Model[int](value=1, _private_attr=3)\n    m._private_value = 3\n    assert m.model_dump() == {'value': 1}\n\n\ndef test_private_attribute_multiple_inheritance():\n    # We need to test this since PrivateAttr uses __slots__ and that has some restrictions with regards to\n    # multiple inheritance\n    default = {'a': {}}\n\n    class GrandParentModel(BaseModel):\n        _foo = PrivateAttr(default)\n\n    class ParentAModel(GrandParentModel):\n        pass\n\n    class ParentBModel(GrandParentModel):\n        _bar = PrivateAttr(default)\n\n    class Model(ParentAModel, ParentBModel):\n        _baz = PrivateAttr(default)\n\n    assert GrandParentModel.__private_attributes__ == {\n        '_foo': PrivateAttr(default),\n    }\n    assert ParentBModel.__private_attributes__ == {\n        '_foo': PrivateAttr(default),\n        '_bar': PrivateAttr(default),\n    }\n    assert Model.__private_attributes__ == {\n        '_foo': PrivateAttr(default),\n        '_bar': PrivateAttr(default),\n        '_baz': PrivateAttr(default),\n    }\n\n    m = Model()\n    assert m._foo == default\n    assert m._foo is not default\n    assert m._foo['a'] is not default['a']\n\n    assert m._bar == default\n    assert m._bar is not default\n    assert m._bar['a'] is not default['a']\n\n    assert m._baz == default\n    assert m._baz is not default\n    assert m._baz['a'] is not default['a']\n\n    m._foo = None\n    assert m._foo is None\n\n    m._bar = None\n    assert m._bar is None\n\n    m._baz = None\n    assert m._baz is None\n\n    assert m.model_dump() == {}\n    assert m.__dict__ == {}\n\n\ndef test_private_attributes_not_dunder() -> None:\n    with pytest.raises(\n        NameError,\n        match='Private attributes must not use dunder names;' \" use a single underscore prefix instead of '__foo__'.\",\n    ):\n\n        class MyModel(BaseModel):\n            __foo__ = PrivateAttr({'private'})\n\n\ndef test_ignored_types_are_ignored() -> None:\n    class IgnoredType:\n        pass\n\n    class MyModel(BaseModel):\n        model_config = ConfigDict(ignored_types=(IgnoredType,))\n\n        _a = IgnoredType()\n        _b: int = IgnoredType()\n        _c: IgnoredType\n        _d: IgnoredType = IgnoredType()\n\n        # The following are included to document existing behavior, which is to make them into PrivateAttrs\n        # this can be updated if the current behavior is not the desired behavior\n        _e: int\n        _f: int = 1\n        _g = 1\n\n    assert sorted(MyModel.__private_attributes__.keys()) == ['_e', '_f', '_g']\n\n\n@pytest.mark.skipif(not hasattr(functools, 'cached_property'), reason='cached_property is not available')\ndef test_ignored_types_are_ignored_cached_property():\n    \"\"\"Demonstrate the members of functools are ignore here as with fields.\"\"\"\n\n    class MyModel(BaseModel):\n        _a: functools.cached_property\n        _b: int\n\n    assert set(MyModel.__private_attributes__) == {'_b'}\n\n\ndef test_none_as_private_attr():\n    from pydantic import BaseModel\n\n    class A(BaseModel):\n        _x: None\n\n    a = A()\n    a._x = None\n    assert a._x is None\n\n\ndef test_layout_compatible_multiple_private_parents():\n    import typing as t\n\n    import pydantic\n\n    class ModelMixin(pydantic.BaseModel):\n        _mixin_private: t.Optional[str] = pydantic.PrivateAttr(None)\n\n    class Model(pydantic.BaseModel):\n        public: str = 'default'\n        _private: t.Optional[str] = pydantic.PrivateAttr(None)\n\n    class NewModel(ModelMixin, Model):\n        pass\n\n    assert set(NewModel.__private_attributes__) == {'_mixin_private', '_private'}\n    m = NewModel()\n    m._mixin_private = 1\n    m._private = 2\n\n    assert m.__pydantic_private__ == {'_mixin_private': 1, '_private': 2}\n    assert m._mixin_private == 1\n    assert m._private == 2\n\n\ndef test_unannotated_private_attr():\n    from pydantic import BaseModel, PrivateAttr\n\n    class A(BaseModel):\n        _x = PrivateAttr()\n        _y = 52\n\n    a = A()\n    assert a._y == 52\n    assert a.__pydantic_private__ == {'_y': 52}\n    a._x = 1\n    assert a.__pydantic_private__ == {'_x': 1, '_y': 52}\n\n\ndef test_classvar_collision_prevention(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\n\nfrom pydantic import BaseModel\nimport typing as t\n\nclass BaseConfig(BaseModel):\n    _FIELD_UPDATE_STRATEGY: t.ClassVar[t.Dict[str, t.Any]] = {}\n\"\"\"\n    )\n\n    assert module.BaseConfig._FIELD_UPDATE_STRATEGY == {}\n\n\n@pytest.mark.skipif(not hasattr(functools, 'cached_property'), reason='cached_property is not available')\ndef test_private_properties_not_included_in_iter_cached_property() -> None:\n    class Model(BaseModel):\n        foo: int\n\n        @computed_field\n        @functools.cached_property\n        def _foo(self) -> int:\n            return -self.foo\n\n    m = Model(foo=1)\n    assert '_foo' not in list(k for k, _ in m)\n\n\ndef test_private_properties_not_included_in_iter_property() -> None:\n    class Model(BaseModel):\n        foo: int\n\n        @computed_field\n        @property\n        def _foo(self) -> int:\n            return -self.foo\n\n    m = Model(foo=1)\n    assert '_foo' not in list(k for k, _ in m)\n\n\ndef test_private_properties_not_included_in_repr_by_default_property() -> None:\n    class Model(BaseModel):\n        foo: int\n\n        @computed_field\n        @property\n        def _private_property(self) -> int:\n            return -self.foo\n\n    m = Model(foo=1)\n    m_repr = repr(m)\n    assert '_private_property' not in m_repr\n\n\n@pytest.mark.skipif(not hasattr(functools, 'cached_property'), reason='cached_property is not available')\ndef test_private_properties_not_included_in_repr_by_default_cached_property() -> None:\n    class Model(BaseModel):\n        foo: int\n\n        @computed_field\n        @functools.cached_property\n        def _private_cached_property(self) -> int:\n            return -self.foo\n\n    m = Model(foo=1)\n    m_repr = repr(m)\n    assert '_private_cached_property' not in m_repr\n", "tests/test_deprecated_validate_arguments.py": "import asyncio\nimport inspect\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\nfrom dirty_equals import IsInstance\nfrom typing_extensions import Annotated\n\nfrom pydantic import BaseModel, Field, PydanticDeprecatedSince20, ValidationError\nfrom pydantic.deprecated.decorator import ValidatedFunction\nfrom pydantic.deprecated.decorator import validate_arguments as validate_arguments_deprecated\nfrom pydantic.errors import PydanticUserError\n\n\ndef validate_arguments(*args, **kwargs):\n    with pytest.warns(\n        PydanticDeprecatedSince20, match='^The `validate_arguments` method is deprecated; use `validate_call`'\n    ):\n        return validate_arguments_deprecated(*args, **kwargs)\n\n\ndef test_args():\n    @validate_arguments\n    def foo(a: int, b: int):\n        return f'{a}, {b}'\n\n    assert foo(1, 2) == '1, 2'\n    assert foo(*[1, 2]) == '1, 2'\n    assert foo(*(1, 2)) == '1, 2'\n    assert foo(*[1], 2) == '1, 2'\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('a',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {}, 'loc': ('b',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(1, 'x')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'x',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n    with pytest.raises(TypeError, match='2 positional arguments expected but 3 given'):\n        foo(1, 2, 3)\n\n    with pytest.raises(TypeError, match=\"unexpected keyword argument: 'apple'\"):\n        foo(1, 2, apple=3)\n\n    with pytest.raises(TypeError, match=\"multiple values for argument: 'a'\"):\n        foo(1, 2, a=3)\n\n    with pytest.raises(TypeError, match=\"multiple values for arguments: 'a', 'b'\"):\n        foo(1, 2, a=3, b=4)\n\n\ndef test_wrap():\n    @validate_arguments\n    def foo_bar(a: int, b: int):\n        \"\"\"This is the foo_bar method.\"\"\"\n        return f'{a}, {b}'\n\n    assert foo_bar.__doc__ == 'This is the foo_bar method.'\n    assert foo_bar.__name__ == 'foo_bar'\n    assert foo_bar.__module__ == 'tests.test_deprecated_validate_arguments'\n    assert foo_bar.__qualname__ == 'test_wrap.<locals>.foo_bar'\n    assert isinstance(foo_bar.vd, ValidatedFunction)\n    assert callable(foo_bar.raw_function)\n    assert foo_bar.vd.arg_mapping == {0: 'a', 1: 'b'}\n    assert foo_bar.vd.positional_only_args == set()\n    assert issubclass(foo_bar.model, BaseModel)\n    assert foo_bar.model.model_fields.keys() == {'a', 'b', 'args', 'kwargs', 'v__duplicate_kwargs'}\n    assert foo_bar.model.__name__ == 'FooBar'\n    assert foo_bar.model.model_json_schema()['title'] == 'FooBar'\n    assert repr(inspect.signature(foo_bar)) == '<Signature (a: int, b: int)>'\n\n\ndef test_kwargs():\n    @validate_arguments\n    def foo(*, a: int, b: int):\n        return a + b\n\n    assert foo.model.model_fields.keys() == {'a', 'b', 'args', 'kwargs'}\n    assert foo(a=1, b=3) == 4\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(a=1, b='x')\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'x',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        }\n    ]\n\n    with pytest.raises(TypeError, match='0 positional arguments expected but 2 given'):\n        foo(1, 'x')\n\n\ndef test_untyped():\n    @validate_arguments\n    def foo(a, b, c='x', *, d='y'):\n        return ', '.join(str(arg) for arg in [a, b, c, d])\n\n    assert foo(1, 2) == '1, 2, x, y'\n    assert foo(1, {'x': 2}, c='3', d='4') == \"1, {'x': 2}, 3, 4\"\n\n\n@pytest.mark.parametrize('validated', (True, False))\ndef test_var_args_kwargs(validated):\n    def foo(a, b, *args, d=3, **kwargs):\n        return f'a={a!r}, b={b!r}, args={args!r}, d={d!r}, kwargs={kwargs!r}'\n\n    if validated:\n        foo = validate_arguments(foo)\n\n    assert foo(1, 2) == 'a=1, b=2, args=(), d=3, kwargs={}'\n    assert foo(1, 2, 3, d=4) == 'a=1, b=2, args=(3,), d=4, kwargs={}'\n    assert foo(*[1, 2, 3], d=4) == 'a=1, b=2, args=(3,), d=4, kwargs={}'\n    assert foo(1, 2, args=(10, 11)) == \"a=1, b=2, args=(), d=3, kwargs={'args': (10, 11)}\"\n    assert foo(1, 2, 3, args=(10, 11)) == \"a=1, b=2, args=(3,), d=3, kwargs={'args': (10, 11)}\"\n    assert foo(1, 2, 3, e=10) == \"a=1, b=2, args=(3,), d=3, kwargs={'e': 10}\"\n    assert foo(1, 2, kwargs=4) == \"a=1, b=2, args=(), d=3, kwargs={'kwargs': 4}\"\n    assert foo(1, 2, kwargs=4, e=5) == \"a=1, b=2, args=(), d=3, kwargs={'kwargs': 4, 'e': 5}\"\n\n\ndef test_field_can_provide_factory() -> None:\n    @validate_arguments\n    def foo(a: int, b: int = Field(default_factory=lambda: 99), *args: int) -> int:\n        \"\"\"mypy is happy with this\"\"\"\n        return a + b + sum(args)\n\n    assert foo(3) == 102\n    assert foo(1, 2, 3) == 6\n\n\ndef test_positional_only(create_module):\n    with pytest.warns(PydanticDeprecatedSince20):\n        module = create_module(\n            # language=Python\n            \"\"\"\nfrom pydantic.deprecated.decorator import validate_arguments\n\n@validate_arguments\ndef foo(a, b, /, c=None):\n    return f'{a}, {b}, {c}'\n\"\"\"\n        )\n    assert module.foo(1, 2) == '1, 2, None'\n    assert module.foo(1, 2, 44) == '1, 2, 44'\n    assert module.foo(1, 2, c=44) == '1, 2, 44'\n    with pytest.raises(TypeError, match=\"positional-only argument passed as keyword argument: 'b'\"):\n        module.foo(1, b=2)\n    with pytest.raises(TypeError, match=\"positional-only arguments passed as keyword arguments: 'a', 'b'\"):\n        module.foo(a=1, b=2)\n\n\ndef test_args_name():\n    @validate_arguments\n    def foo(args: int, kwargs: int):\n        return f'args={args!r}, kwargs={kwargs!r}'\n\n    assert foo.model.model_fields.keys() == {'args', 'kwargs', 'v__args', 'v__kwargs', 'v__duplicate_kwargs'}\n    assert foo(1, 2) == 'args=1, kwargs=2'\n\n    with pytest.raises(TypeError, match=\"unexpected keyword argument: 'apple'\"):\n        foo(1, 2, apple=4)\n\n    with pytest.raises(TypeError, match=\"unexpected keyword arguments: 'apple', 'banana'\"):\n        foo(1, 2, apple=4, banana=5)\n\n    with pytest.raises(TypeError, match='2 positional arguments expected but 3 given'):\n        foo(1, 2, 3)\n\n\ndef test_v_args():\n    with pytest.raises(\n        PydanticUserError,\n        match='\"v__args\", \"v__kwargs\", \"v__positional_only\" and \"v__duplicate_kwargs\" are not permitted',\n    ):\n\n        @validate_arguments\n        def foo1(v__args: int):\n            pass\n\n    with pytest.raises(\n        PydanticUserError,\n        match='\"v__args\", \"v__kwargs\", \"v__positional_only\" and \"v__duplicate_kwargs\" are not permitted',\n    ):\n\n        @validate_arguments\n        def foo2(v__kwargs: int):\n            pass\n\n    with pytest.raises(\n        PydanticUserError,\n        match='\"v__args\", \"v__kwargs\", \"v__positional_only\" and \"v__duplicate_kwargs\" are not permitted',\n    ):\n\n        @validate_arguments\n        def foo3(v__positional_only: int):\n            pass\n\n    with pytest.raises(\n        PydanticUserError,\n        match='\"v__args\", \"v__kwargs\", \"v__positional_only\" and \"v__duplicate_kwargs\" are not permitted',\n    ):\n\n        @validate_arguments\n        def foo4(v__duplicate_kwargs: int):\n            pass\n\n\ndef test_async():\n    @validate_arguments\n    async def foo(a, b):\n        return f'a={a} b={b}'\n\n    async def run():\n        v = await foo(1, 2)\n        assert v == 'a=1 b=2'\n\n    asyncio.run(run())\n    with pytest.raises(ValidationError) as exc_info:\n        asyncio.run(foo('x'))\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'a': 'x'}, 'loc': ('b',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n\ndef test_string_annotation():\n    @validate_arguments\n    def foo(a: 'List[int]', b: 'Path'):\n        return f'a={a!r} b={b!r}'\n\n    assert foo([1, 2, 3], '/')\n\n    with pytest.raises(ValidationError) as exc_info:\n        foo(['x'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 'x',\n            'loc': ('a', 0),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n        {'input': {'a': ['x']}, 'loc': ('b',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n\n\ndef test_item_method():\n    class X:\n        def __init__(self, v):\n            self.v = v\n\n        @validate_arguments\n        def foo(self, a: int, b: int):\n            assert self.v == a\n            return f'{a}, {b}'\n\n    x = X(4)\n    assert x.foo(4, 2) == '4, 2'\n    assert x.foo(*[4, 2]) == '4, 2'\n\n    with pytest.raises(ValidationError) as exc_info:\n        x.foo()\n\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'self': IsInstance(X)}, 'loc': ('a',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {'self': IsInstance(X)}, 'loc': ('b',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n\n\ndef test_class_method():\n    class X:\n        @classmethod\n        @validate_arguments\n        def foo(cls, a: int, b: int):\n            assert cls == X\n            return f'{a}, {b}'\n\n    x = X()\n    assert x.foo(4, 2) == '4, 2'\n    assert x.foo(*[4, 2]) == '4, 2'\n\n    with pytest.raises(ValidationError) as exc_info:\n        x.foo()\n\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'cls': X}, 'loc': ('a',), 'msg': 'Field required', 'type': 'missing'},\n        {'input': {'cls': X}, 'loc': ('b',), 'msg': 'Field required', 'type': 'missing'},\n    ]\n\n\ndef test_config_title():\n    @validate_arguments(config=dict(title='Testing'))\n    def foo(a: int, b: int):\n        return f'{a}, {b}'\n\n    assert foo(1, 2) == '1, 2'\n    assert foo(1, b=2) == '1, 2'\n    assert foo.model.model_json_schema()['title'] == 'Testing'\n\n\ndef test_config_title_cls():\n    class Config:\n        title = 'Testing'\n\n    @validate_arguments(config={'title': 'Testing'})\n    def foo(a: int, b: int):\n        return f'{a}, {b}'\n\n    assert foo(1, 2) == '1, 2'\n    assert foo(1, b=2) == '1, 2'\n    assert foo.model.model_json_schema()['title'] == 'Testing'\n\n\ndef test_config_fields():\n    with pytest.raises(PydanticUserError, match='Setting the \"alias_generator\" property on custom Config for @'):\n\n        @validate_arguments(config=dict(alias_generator=lambda x: x))\n        def foo(a: int, b: int):\n            return f'{a}, {b}'\n\n\ndef test_config_arbitrary_types_allowed():\n    class EggBox:\n        def __str__(self) -> str:\n            return 'EggBox()'\n\n    @validate_arguments(config=dict(arbitrary_types_allowed=True))\n    def foo(a: int, b: EggBox):\n        return f'{a}, {b}'\n\n    assert foo(1, EggBox()) == '1, EggBox()'\n    with pytest.raises(ValidationError) as exc_info:\n        assert foo(1, 2) == '1, 2'\n\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class': 'test_config_arbitrary_types_allowed.<locals>.EggBox'},\n            'input': 2,\n            'loc': ('b',),\n            'msg': 'Input should be an instance of ' 'test_config_arbitrary_types_allowed.<locals>.EggBox',\n            'type': 'is_instance_of',\n        }\n    ]\n\n\ndef test_validate(mocker):\n    stub = mocker.stub(name='on_something_stub')\n\n    @validate_arguments\n    def func(s: str, count: int, *, separator: bytes = b''):\n        stub(s, count, separator)\n\n    func.validate('qwe', 2)\n    with pytest.raises(ValidationError):\n        func.validate(['qwe'], 2)\n\n    stub.assert_not_called()\n\n\ndef test_use_of_alias():\n    @validate_arguments\n    def foo(c: int = Field(default_factory=lambda: 20), a: int = Field(default_factory=lambda: 10, alias='b')):\n        return a + c\n\n    assert foo(b=10) == 30\n\n\ndef test_populate_by_name():\n    @validate_arguments(config=dict(populate_by_name=True))\n    def foo(a: Annotated[int, Field(alias='b')], c: Annotated[int, Field(alias='d')]):\n        return a + c\n\n    assert foo(a=10, d=1) == 11\n    assert foo(b=10, c=1) == 11\n    assert foo(a=10, c=1) == 11\n", "tests/test_assert_in_validators.py": "\"\"\"\nPYTEST_DONT_REWRITE\n\"\"\"\n\nimport difflib\nimport pprint\n\nimport pytest\nfrom dirty_equals import HasRepr\n\nfrom pydantic import BaseModel, ValidationError, field_validator\n\n\ndef _pformat_lines(obj):\n    return pprint.pformat(obj).splitlines(keepends=True)\n\n\ndef _assert_eq(left, right):\n    if left != right:\n        pytest.fail('\\n' + '\\n'.join(difflib.ndiff(_pformat_lines(left), _pformat_lines(right))))\n\n\ndef test_assert_raises_validation_error():\n    class Model(BaseModel):\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def check_a(cls, v):\n            assert v == 'a', 'invalid a'\n            return v\n\n    assert Model(a='a').a == 'a'\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model(a='snap')\n\n    _assert_eq(\n        [\n            {\n                'ctx': {'error': HasRepr(repr(AssertionError('invalid a')))},\n                'input': 'snap',\n                'loc': ('a',),\n                'msg': 'Assertion failed, invalid a',\n                'type': 'assertion_error',\n            }\n        ],\n        exc_info.value.errors(include_url=False),\n    )\n", "tests/test_discriminated_union.py": "import re\nimport sys\nfrom enum import Enum, IntEnum\nfrom types import SimpleNamespace\nfrom typing import Any, Callable, Generic, List, Optional, Sequence, TypeVar, Union\n\nimport pytest\nfrom dirty_equals import HasRepr, IsStr\nfrom pydantic_core import SchemaValidator, core_schema\nfrom typing_extensions import Annotated, Literal, TypedDict\n\nfrom pydantic import BaseModel, ConfigDict, Discriminator, Field, TypeAdapter, ValidationError, field_validator\nfrom pydantic._internal._discriminated_union import apply_discriminator\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\nfrom pydantic.errors import PydanticUserError\nfrom pydantic.fields import FieldInfo\nfrom pydantic.json_schema import GenerateJsonSchema\nfrom pydantic.types import Tag\n\n\ndef test_discriminated_union_type():\n    with pytest.raises(\n        TypeError, match=\"'str' is not a valid discriminated union variant; should be a `BaseModel` or `dataclass`\"\n    ):\n\n        class Model(BaseModel):\n            x: str = Field(..., discriminator='qwe')\n\n\n@pytest.mark.parametrize('union', [True, False])\ndef test_discriminated_single_variant(union):\n    class InnerModel(BaseModel):\n        qwe: Literal['qwe']\n        y: int\n\n    class Model(BaseModel):\n        if union:\n            x: Union[InnerModel] = Field(..., discriminator='qwe')\n        else:\n            x: InnerModel = Field(..., discriminator='qwe')\n\n    assert Model(x={'qwe': 'qwe', 'y': 1}).x.qwe == 'qwe'\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x={'qwe': 'asd', 'y': 'a'})  # note: incorrect type of \"y\" is not reported due to discriminator failure\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'qwe'\", 'expected_tags': \"'qwe'\", 'tag': 'asd'},\n            'input': {'qwe': 'asd', 'y': 'a'},\n            'loc': ('x',),\n            'msg': \"Input tag 'asd' found using 'qwe' does not match any of the expected \" \"tags: 'qwe'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n\ndef test_discriminated_union_single_variant():\n    class InnerModel(BaseModel):\n        qwe: Literal['qwe']\n\n    class Model(BaseModel):\n        x: Union[InnerModel] = Field(..., discriminator='qwe')\n\n    assert Model(x={'qwe': 'qwe'}).x.qwe == 'qwe'\n\n\ndef test_discriminated_union_invalid_type():\n    with pytest.raises(\n        TypeError, match=\"'str' is not a valid discriminated union variant; should be a `BaseModel` or `dataclass`\"\n    ):\n\n        class Model(BaseModel):\n            x: Union[str, int] = Field(..., discriminator='qwe')\n\n\ndef test_discriminated_union_defined_discriminator():\n    class Cat(BaseModel):\n        c: str\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n        d: str\n\n    with pytest.raises(PydanticUserError, match=\"Model 'Cat' needs a discriminator field for key 'pet_type'\"):\n\n        class Model(BaseModel):\n            pet: Union[Cat, Dog] = Field(..., discriminator='pet_type')\n            number: int\n\n\ndef test_discriminated_union_literal_discriminator():\n    class Cat(BaseModel):\n        pet_type: int\n        c: str\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n        d: str\n\n    with pytest.raises(PydanticUserError, match=\"Model 'Cat' needs field 'pet_type' to be of type `Literal`\"):\n\n        class Model(BaseModel):\n            pet: Union[Cat, Dog] = Field(..., discriminator='pet_type')\n            number: int\n\n\ndef test_discriminated_union_root_same_discriminator():\n    class BlackCat(BaseModel):\n        pet_type: Literal['blackcat']\n\n    class WhiteCat(BaseModel):\n        pet_type: Literal['whitecat']\n\n    Cat = Union[BlackCat, WhiteCat]\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n\n    CatDog = TypeAdapter(Annotated[Union[Cat, Dog], Field(..., discriminator='pet_type')]).validate_python\n    CatDog({'pet_type': 'blackcat'})\n    CatDog({'pet_type': 'whitecat'})\n    CatDog({'pet_type': 'dog'})\n    with pytest.raises(ValidationError) as exc_info:\n        CatDog({'pet_type': 'llama'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'pet_type'\", 'expected_tags': \"'blackcat', 'whitecat', 'dog'\", 'tag': 'llama'},\n            'input': {'pet_type': 'llama'},\n            'loc': (),\n            'msg': \"Input tag 'llama' found using 'pet_type' does not match any of the \"\n            \"expected tags: 'blackcat', 'whitecat', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n\n@pytest.mark.parametrize('color_discriminator_kind', ['discriminator', 'field_str', 'field_discriminator'])\n@pytest.mark.parametrize('pet_discriminator_kind', ['discriminator', 'field_str', 'field_discriminator'])\ndef test_discriminated_union_validation(color_discriminator_kind, pet_discriminator_kind):\n    def _get_str_discriminator(discriminator: str, kind: str):\n        if kind == 'discriminator':\n            return Discriminator(discriminator)\n        elif kind == 'field_str':\n            return Field(discriminator=discriminator)\n        elif kind == 'field_discriminator':\n            return Field(discriminator=Discriminator(discriminator))\n        raise ValueError(f'Invalid kind: {kind}')\n\n    class BlackCat(BaseModel):\n        pet_type: Literal['cat']\n        color: Literal['black']\n        black_infos: str\n\n    class WhiteCat(BaseModel):\n        pet_type: Literal['cat']\n        color: Literal['white']\n        white_infos: str\n\n    color_discriminator = _get_str_discriminator('color', color_discriminator_kind)\n    Cat = Annotated[Union[BlackCat, WhiteCat], color_discriminator]\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n        d: str\n\n    class Lizard(BaseModel):\n        pet_type: Literal['reptile', 'lizard']\n        m: str\n\n    pet_discriminator = _get_str_discriminator('pet_type', pet_discriminator_kind)\n\n    class Model(BaseModel):\n        pet: Annotated[Union[Cat, Dog, Lizard], pet_discriminator]\n        number: int\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': {'pet_typ': 'cat'}, 'number': 'x'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'pet_type'\"},\n            'input': {'pet_typ': 'cat'},\n            'loc': ('pet',),\n            'msg': \"Unable to extract tag using discriminator 'pet_type'\",\n            'type': 'union_tag_not_found',\n        },\n        {\n            'input': 'x',\n            'loc': ('number',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': 'fish', 'number': 2})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_attributes_type',\n            'loc': ('pet',),\n            'msg': 'Input should be a valid dictionary or object to extract fields from',\n            'input': 'fish',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': {'pet_type': 'fish'}, 'number': 2})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'pet_type'\", 'expected_tags': \"'cat', 'dog', 'reptile', 'lizard'\", 'tag': 'fish'},\n            'input': {'pet_type': 'fish'},\n            'loc': ('pet',),\n            'msg': \"Input tag 'fish' found using 'pet_type' does not match any of the \"\n            \"expected tags: 'cat', 'dog', 'reptile', 'lizard'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': {'pet_type': 'lizard'}, 'number': 2})\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'pet_type': 'lizard'}, 'loc': ('pet', 'lizard', 'm'), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n    m = Model.model_validate({'pet': {'pet_type': 'lizard', 'm': 'pika'}, 'number': 2})\n    assert isinstance(m.pet, Lizard)\n    assert m.model_dump() == {'pet': {'pet_type': 'lizard', 'm': 'pika'}, 'number': 2}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': {'pet_type': 'cat', 'color': 'white'}, 'number': 2})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': {'color': 'white', 'pet_type': 'cat'},\n            'loc': ('pet', 'cat', 'white', 'white_infos'),\n            'msg': 'Field required',\n            'type': 'missing',\n        }\n    ]\n    m = Model.model_validate({'pet': {'pet_type': 'cat', 'color': 'white', 'white_infos': 'pika'}, 'number': 2})\n    assert isinstance(m.pet, WhiteCat)\n\n\ndef test_discriminated_annotated_union():\n    class BlackCat(BaseModel):\n        pet_type: Literal['cat']\n        color: Literal['black']\n        black_infos: str\n\n    class WhiteCat(BaseModel):\n        pet_type: Literal['cat']\n        color: Literal['white']\n        white_infos: str\n\n    Cat = Annotated[Union[BlackCat, WhiteCat], Field(discriminator='color')]\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n        dog_name: str\n\n    Pet = Annotated[Union[Cat, Dog], Field(discriminator='pet_type')]\n\n    class Model(BaseModel):\n        pet: Pet\n        number: int\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': {'pet_typ': 'cat'}, 'number': 'x'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'pet_type'\"},\n            'input': {'pet_typ': 'cat'},\n            'loc': ('pet',),\n            'msg': \"Unable to extract tag using discriminator 'pet_type'\",\n            'type': 'union_tag_not_found',\n        },\n        {\n            'input': 'x',\n            'loc': ('number',),\n            'msg': 'Input should be a valid integer, unable to parse string as an ' 'integer',\n            'type': 'int_parsing',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': {'pet_type': 'fish'}, 'number': 2})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'pet_type'\", 'expected_tags': \"'cat', 'dog'\", 'tag': 'fish'},\n            'input': {'pet_type': 'fish'},\n            'loc': ('pet',),\n            'msg': \"Input tag 'fish' found using 'pet_type' does not match any of the \" \"expected tags: 'cat', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': {'pet_type': 'dog'}, 'number': 2})\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'pet_type': 'dog'}, 'loc': ('pet', 'dog', 'dog_name'), 'msg': 'Field required', 'type': 'missing'}\n    ]\n    m = Model.model_validate({'pet': {'pet_type': 'dog', 'dog_name': 'milou'}, 'number': 2})\n    assert isinstance(m.pet, Dog)\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': {'pet_type': 'cat', 'color': 'red'}, 'number': 2})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'color'\", 'expected_tags': \"'black', 'white'\", 'tag': 'red'},\n            'input': {'color': 'red', 'pet_type': 'cat'},\n            'loc': ('pet', 'cat'),\n            'msg': \"Input tag 'red' found using 'color' does not match any of the \" \"expected tags: 'black', 'white'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'pet': {'pet_type': 'cat', 'color': 'white'}, 'number': 2})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': {'color': 'white', 'pet_type': 'cat'},\n            'loc': ('pet', 'cat', 'white', 'white_infos'),\n            'msg': 'Field required',\n            'type': 'missing',\n        }\n    ]\n    m = Model.model_validate({'pet': {'pet_type': 'cat', 'color': 'white', 'white_infos': 'pika'}, 'number': 2})\n    assert isinstance(m.pet, WhiteCat)\n\n\ndef test_discriminated_union_basemodel_instance_value():\n    class A(BaseModel):\n        foo: Literal['a']\n\n    class B(BaseModel):\n        foo: Literal['b']\n\n    class Top(BaseModel):\n        sub: Union[A, B] = Field(..., discriminator='foo')\n\n    t = Top(sub=A(foo='a'))\n    assert isinstance(t, Top)\n\n\ndef test_discriminated_union_basemodel_instance_value_with_alias():\n    class A(BaseModel):\n        literal: Literal['a'] = Field(alias='lit')\n\n    class B(BaseModel):\n        model_config = ConfigDict(populate_by_name=True)\n        literal: Literal['b'] = Field(alias='lit')\n\n    class Top(BaseModel):\n        sub: Union[A, B] = Field(..., discriminator='literal')\n\n    with pytest.raises(ValidationError) as exc_info:\n        Top(sub=A(literal='a'))\n\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'literal': 'a'}, 'loc': ('lit',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n    assert Top(sub=A(lit='a')).sub.literal == 'a'\n    assert Top(sub=B(lit='b')).sub.literal == 'b'\n    assert Top(sub=B(literal='b')).sub.literal == 'b'\n\n\ndef test_discriminated_union_int():\n    class A(BaseModel):\n        m: Literal[1]\n\n    class B(BaseModel):\n        m: Literal[2]\n\n    class Top(BaseModel):\n        sub: Union[A, B] = Field(..., discriminator='m')\n\n    assert isinstance(Top.model_validate({'sub': {'m': 2}}).sub, B)\n    with pytest.raises(ValidationError) as exc_info:\n        Top.model_validate({'sub': {'m': 3}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'m'\", 'expected_tags': '1, 2', 'tag': '3'},\n            'input': {'m': 3},\n            'loc': ('sub',),\n            'msg': \"Input tag '3' found using 'm' does not match any of the expected \" 'tags: 1, 2',\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n\nclass FooIntEnum(int, Enum):\n    pass\n\n\nclass FooStrEnum(str, Enum):\n    pass\n\n\nENUM_TEST_CASES = [\n    pytest.param(Enum, {'a': 1, 'b': 2}),\n    pytest.param(Enum, {'a': 'v_a', 'b': 'v_b'}),\n    (FooIntEnum, {'a': 1, 'b': 2}),\n    (IntEnum, {'a': 1, 'b': 2}),\n    (FooStrEnum, {'a': 'v_a', 'b': 'v_b'}),\n]\nif sys.version_info >= (3, 11):\n    from enum import StrEnum\n\n    ENUM_TEST_CASES.append((StrEnum, {'a': 'v_a', 'b': 'v_b'}))\n\n\n@pytest.mark.skipif(sys.version_info[:2] == (3, 8), reason='https://github.com/python/cpython/issues/103592')\n@pytest.mark.parametrize('base_class,choices', ENUM_TEST_CASES)\ndef test_discriminated_union_enum(base_class, choices):\n    EnumValue = base_class('EnumValue', choices)\n\n    class A(BaseModel):\n        m: Literal[EnumValue.a]\n\n    class B(BaseModel):\n        m: Literal[EnumValue.b]\n\n    class Top(BaseModel):\n        sub: Union[A, B] = Field(..., discriminator='m')\n\n    assert isinstance(Top.model_validate({'sub': {'m': EnumValue.b}}).sub, B)\n    if isinstance(EnumValue.b, (int, str)):\n        assert isinstance(Top.model_validate({'sub': {'m': EnumValue.b.value}}).sub, B)\n    with pytest.raises(ValidationError) as exc_info:\n        Top.model_validate({'sub': {'m': 3}})\n\n    expected_tags = f'{EnumValue.a!r}, {EnumValue.b!r}'\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'union_tag_invalid',\n            'loc': ('sub',),\n            'msg': f\"Input tag '3' found using 'm' does not match any of the expected tags: {expected_tags}\",\n            'input': {'m': 3},\n            'ctx': {'discriminator': \"'m'\", 'tag': '3', 'expected_tags': expected_tags},\n        }\n    ]\n\n\ndef test_alias_different():\n    class Cat(BaseModel):\n        pet_type: Literal['cat'] = Field(alias='U')\n        c: str\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog'] = Field(alias='T')\n        d: str\n\n    with pytest.raises(TypeError, match=re.escape(\"Aliases for discriminator 'pet_type' must be the same (got T, U)\")):\n\n        class Model(BaseModel):\n            pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n\n\ndef test_alias_same():\n    class Cat(BaseModel):\n        pet_type: Literal['cat'] = Field(alias='typeOfPet')\n        c: str\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog'] = Field(alias='typeOfPet')\n        d: str\n\n    class Model(BaseModel):\n        pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n\n    assert Model(**{'pet': {'typeOfPet': 'dog', 'd': 'milou'}}).pet.pet_type == 'dog'\n\n\ndef test_nested():\n    class Cat(BaseModel):\n        pet_type: Literal['cat']\n        name: str\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n        name: str\n\n    CommonPet = Annotated[Union[Cat, Dog], Field(discriminator='pet_type')]\n\n    class Lizard(BaseModel):\n        pet_type: Literal['reptile', 'lizard']\n        name: str\n\n    class Model(BaseModel):\n        pet: Union[CommonPet, Lizard] = Field(..., discriminator='pet_type')\n        n: int\n\n    assert isinstance(Model(**{'pet': {'pet_type': 'dog', 'name': 'Milou'}, 'n': 5}).pet, Dog)\n\n\ndef test_generic():\n    T = TypeVar('T')\n\n    class Success(BaseModel, Generic[T]):\n        type: Literal['Success'] = 'Success'\n        data: T\n\n    class Failure(BaseModel):\n        type: Literal['Failure'] = 'Failure'\n        error_message: str\n\n    class Container(BaseModel, Generic[T]):\n        result: Union[Success[T], Failure] = Field(discriminator='type')\n\n    with pytest.raises(ValidationError, match=\"Unable to extract tag using discriminator 'type'\"):\n        Container[str].model_validate({'result': {}})\n\n    with pytest.raises(\n        ValidationError,\n        match=re.escape(\n            \"Input tag 'Other' found using 'type' does not match any of the expected tags: 'Success', 'Failure'\"\n        ),\n    ):\n        Container[str].model_validate({'result': {'type': 'Other'}})\n\n    with pytest.raises(ValidationError, match=r'Container\\[str\\]\\nresult\\.Success\\.data') as exc_info:\n        Container[str].model_validate({'result': {'type': 'Success'}})\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'type': 'Success'}, 'loc': ('result', 'Success', 'data'), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n    # invalid types error\n    with pytest.raises(ValidationError) as exc_info:\n        Container[str].model_validate({'result': {'type': 'Success', 'data': 1}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': 1,\n            'loc': ('result', 'Success', 'data'),\n            'msg': 'Input should be a valid string',\n            'type': 'string_type',\n        }\n    ]\n\n    assert Container[str].model_validate({'result': {'type': 'Success', 'data': '1'}}).result.data == '1'\n\n\ndef test_optional_union():\n    class Cat(BaseModel):\n        pet_type: Literal['cat']\n        name: str\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n        name: str\n\n    class Pet(BaseModel):\n        pet: Optional[Union[Cat, Dog]] = Field(discriminator='pet_type')\n\n    assert Pet(pet={'pet_type': 'cat', 'name': 'Milo'}).model_dump() == {'pet': {'name': 'Milo', 'pet_type': 'cat'}}\n    assert Pet(pet={'pet_type': 'dog', 'name': 'Otis'}).model_dump() == {'pet': {'name': 'Otis', 'pet_type': 'dog'}}\n    assert Pet(pet=None).model_dump() == {'pet': None}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Pet()\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {}, 'loc': ('pet',), 'msg': 'Field required', 'type': 'missing'}\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Pet(pet={'name': 'Benji'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'pet_type'\"},\n            'input': {'name': 'Benji'},\n            'loc': ('pet',),\n            'msg': \"Unable to extract tag using discriminator 'pet_type'\",\n            'type': 'union_tag_not_found',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Pet(pet={'pet_type': 'lizard'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'pet_type'\", 'expected_tags': \"'cat', 'dog'\", 'tag': 'lizard'},\n            'input': {'pet_type': 'lizard'},\n            'loc': ('pet',),\n            'msg': \"Input tag 'lizard' found using 'pet_type' does not match any of the \" \"expected tags: 'cat', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n\ndef test_optional_union_with_defaults():\n    class Cat(BaseModel):\n        pet_type: Literal['cat'] = 'cat'\n        name: str\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog'] = 'dog'\n        name: str\n\n    class Pet(BaseModel):\n        pet: Optional[Union[Cat, Dog]] = Field(default=None, discriminator='pet_type')\n\n    assert Pet(pet={'pet_type': 'cat', 'name': 'Milo'}).model_dump() == {'pet': {'name': 'Milo', 'pet_type': 'cat'}}\n    assert Pet(pet={'pet_type': 'dog', 'name': 'Otis'}).model_dump() == {'pet': {'name': 'Otis', 'pet_type': 'dog'}}\n    assert Pet(pet=None).model_dump() == {'pet': None}\n    assert Pet().model_dump() == {'pet': None}\n\n    with pytest.raises(ValidationError) as exc_info:\n        Pet(pet={'name': 'Benji'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'pet_type'\"},\n            'input': {'name': 'Benji'},\n            'loc': ('pet',),\n            'msg': \"Unable to extract tag using discriminator 'pet_type'\",\n            'type': 'union_tag_not_found',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Pet(pet={'pet_type': 'lizard'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'pet_type'\", 'expected_tags': \"'cat', 'dog'\", 'tag': 'lizard'},\n            'input': {'pet_type': 'lizard'},\n            'loc': ('pet',),\n            'msg': \"Input tag 'lizard' found using 'pet_type' does not match any of the \" \"expected tags: 'cat', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n\ndef test_aliases_matching_is_not_sufficient() -> None:\n    class Case1(BaseModel):\n        kind_one: Literal['1'] = Field(alias='kind')\n\n    class Case2(BaseModel):\n        kind_two: Literal['2'] = Field(alias='kind')\n\n    with pytest.raises(PydanticUserError, match=\"Model 'Case1' needs a discriminator field for key 'kind'\"):\n\n        class TaggedParent(BaseModel):\n            tagged: Union[Case1, Case2] = Field(discriminator='kind')\n\n\ndef test_nested_optional_unions() -> None:\n    class Cat(BaseModel):\n        pet_type: Literal['cat'] = 'cat'\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog'] = 'dog'\n\n    class Lizard(BaseModel):\n        pet_type: Literal['lizard', 'reptile'] = 'lizard'\n\n    MaybeCatDog = Annotated[Optional[Union[Cat, Dog]], Field(discriminator='pet_type')]\n    MaybeDogLizard = Annotated[Union[Dog, Lizard, None], Field(discriminator='pet_type')]\n\n    class Pet(BaseModel):\n        pet: Union[MaybeCatDog, MaybeDogLizard] = Field(discriminator='pet_type')\n\n    Pet.model_validate({'pet': {'pet_type': 'dog'}})\n    Pet.model_validate({'pet': {'pet_type': 'cat'}})\n    Pet.model_validate({'pet': {'pet_type': 'lizard'}})\n    Pet.model_validate({'pet': {'pet_type': 'reptile'}})\n    Pet.model_validate({'pet': None})\n\n    with pytest.raises(ValidationError) as exc_info:\n        Pet.model_validate({'pet': {'pet_type': None}})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'union_tag_invalid',\n            'loc': ('pet',),\n            'msg': \"Input tag 'None' found using 'pet_type' does not match any of the expected tags: 'cat', 'dog', 'lizard', 'reptile'\",\n            'input': {'pet_type': None},\n            'ctx': {'discriminator': \"'pet_type'\", 'tag': 'None', 'expected_tags': \"'cat', 'dog', 'lizard', 'reptile'\"},\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Pet.model_validate({'pet': {'pet_type': 'fox'}})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'union_tag_invalid',\n            'loc': ('pet',),\n            'msg': \"Input tag 'fox' found using 'pet_type' does not match any of the expected tags: 'cat', 'dog', 'lizard', 'reptile'\",\n            'input': {'pet_type': 'fox'},\n            'ctx': {'discriminator': \"'pet_type'\", 'tag': 'fox', 'expected_tags': \"'cat', 'dog', 'lizard', 'reptile'\"},\n        }\n    ]\n\n\ndef test_nested_discriminated_union() -> None:\n    class Cat(BaseModel):\n        pet_type: Literal['cat', 'CAT']\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog', 'DOG']\n\n    class Lizard(BaseModel):\n        pet_type: Literal['lizard', 'LIZARD']\n\n    CatDog = Annotated[Union[Cat, Dog], Field(discriminator='pet_type')]\n    CatDogLizard = Annotated[Union[CatDog, Lizard], Field(discriminator='pet_type')]\n\n    class Pet(BaseModel):\n        pet: CatDogLizard\n\n    Pet.model_validate({'pet': {'pet_type': 'dog'}})\n    Pet.model_validate({'pet': {'pet_type': 'cat'}})\n    Pet.model_validate({'pet': {'pet_type': 'lizard'}})\n\n    with pytest.raises(ValidationError) as exc_info:\n        Pet.model_validate({'pet': {'pet_type': 'reptile'}})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'union_tag_invalid',\n            'loc': ('pet',),\n            'msg': \"Input tag 'reptile' found using 'pet_type' does not match any of the expected tags: 'cat', 'CAT', 'dog', 'DOG', 'lizard', 'LIZARD'\",\n            'input': {'pet_type': 'reptile'},\n            'ctx': {\n                'discriminator': \"'pet_type'\",\n                'tag': 'reptile',\n                'expected_tags': \"'cat', 'CAT', 'dog', 'DOG', 'lizard', 'LIZARD'\",\n            },\n        }\n    ]\n\n\ndef test_unions_of_optionals() -> None:\n    class Cat(BaseModel):\n        pet_type: Literal['cat'] = Field(alias='typeOfPet')\n        c: str\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog'] = Field(alias='typeOfPet')\n        d: str\n\n    class Lizard(BaseModel):\n        pet_type: Literal['lizard'] = Field(alias='typeOfPet')\n\n    MaybeCat = Annotated[Union[Cat, None], 'some annotation']\n    MaybeDogLizard = Annotated[Optional[Union[Dog, Lizard]], 'some other annotation']\n\n    class Model(BaseModel):\n        maybe_pet: Union[MaybeCat, MaybeDogLizard] = Field(discriminator='pet_type')\n\n    assert Model(**{'maybe_pet': None}).maybe_pet is None\n    assert Model(**{'maybe_pet': {'typeOfPet': 'dog', 'd': 'milou'}}).maybe_pet.pet_type == 'dog'\n    assert Model(**{'maybe_pet': {'typeOfPet': 'lizard'}}).maybe_pet.pet_type == 'lizard'\n\n\ndef test_union_discriminator_literals() -> None:\n    class Cat(BaseModel):\n        pet_type: Union[Literal['cat'], Literal['CAT']] = Field(alias='typeOfPet')\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog'] = Field(alias='typeOfPet')\n\n    class Model(BaseModel):\n        pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n\n    assert Model(**{'pet': {'typeOfPet': 'dog'}}).pet.pet_type == 'dog'\n    assert Model(**{'pet': {'typeOfPet': 'cat'}}).pet.pet_type == 'cat'\n    assert Model(**{'pet': {'typeOfPet': 'CAT'}}).pet.pet_type == 'CAT'\n    with pytest.raises(ValidationError) as exc_info:\n        Model(**{'pet': {'typeOfPet': 'Cat'}})\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'union_tag_invalid',\n            'loc': ('pet',),\n            'msg': \"Input tag 'Cat' found using 'pet_type' | 'typeOfPet' does not match any of the expected tags: 'cat', 'CAT', 'dog'\",\n            'input': {'typeOfPet': 'Cat'},\n            'ctx': {'discriminator': \"'pet_type' | 'typeOfPet'\", 'tag': 'Cat', 'expected_tags': \"'cat', 'CAT', 'dog'\"},\n        }\n    ]\n\n\ndef test_none_schema() -> None:\n    cat_fields = {'kind': core_schema.typed_dict_field(core_schema.literal_schema(['cat']))}\n    dog_fields = {'kind': core_schema.typed_dict_field(core_schema.literal_schema(['dog']))}\n    cat = core_schema.typed_dict_schema(cat_fields)\n    dog = core_schema.typed_dict_schema(dog_fields)\n    schema = core_schema.union_schema([cat, dog, core_schema.none_schema()])\n    schema = apply_discriminator(schema, 'kind')\n    validator = SchemaValidator(schema)\n    assert validator.validate_python({'kind': 'cat'})['kind'] == 'cat'\n    assert validator.validate_python({'kind': 'dog'})['kind'] == 'dog'\n    assert validator.validate_python(None) is None\n    with pytest.raises(ValidationError) as exc_info:\n        validator.validate_python({'kind': 'lizard'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'kind'\", 'expected_tags': \"'cat', 'dog'\", 'tag': 'lizard'},\n            'input': {'kind': 'lizard'},\n            'loc': (),\n            'msg': \"Input tag 'lizard' found using 'kind' does not match any of the \" \"expected tags: 'cat', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n\ndef test_nested_unwrapping() -> None:\n    cat_fields = {'kind': core_schema.typed_dict_field(core_schema.literal_schema(['cat']))}\n    dog_fields = {'kind': core_schema.typed_dict_field(core_schema.literal_schema(['dog']))}\n    cat = core_schema.typed_dict_schema(cat_fields)\n    dog = core_schema.typed_dict_schema(dog_fields)\n    schema = core_schema.union_schema([cat, dog])\n    for _ in range(3):\n        schema = core_schema.nullable_schema(schema)\n        schema = core_schema.nullable_schema(schema)\n        schema = core_schema.definitions_schema(schema, [])\n        schema = core_schema.definitions_schema(schema, [])\n\n    schema = apply_discriminator(schema, 'kind')\n\n    validator = SchemaValidator(schema)\n    assert validator.validate_python({'kind': 'cat'})['kind'] == 'cat'\n    assert validator.validate_python({'kind': 'dog'})['kind'] == 'dog'\n    assert validator.validate_python(None) is None\n    with pytest.raises(ValidationError) as exc_info:\n        validator.validate_python({'kind': 'lizard'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'kind'\", 'expected_tags': \"'cat', 'dog'\", 'tag': 'lizard'},\n            'input': {'kind': 'lizard'},\n            'loc': (),\n            'msg': \"Input tag 'lizard' found using 'kind' does not match any of the \" \"expected tags: 'cat', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n\ndef test_distinct_choices() -> None:\n    class Cat(BaseModel):\n        pet_type: Literal['cat', 'dog'] = Field(alias='typeOfPet')\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog'] = Field(alias='typeOfPet')\n\n    with pytest.raises(TypeError, match=\"Value 'dog' for discriminator 'pet_type' mapped to multiple choices\"):\n\n        class Model(BaseModel):\n            pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n\n\ndef test_invalid_discriminated_union_type() -> None:\n    class Cat(BaseModel):\n        pet_type: Literal['cat'] = Field(alias='typeOfPet')\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog'] = Field(alias='typeOfPet')\n\n    with pytest.raises(\n        TypeError, match=\"'str' is not a valid discriminated union variant; should be a `BaseModel` or `dataclass`\"\n    ):\n\n        class Model(BaseModel):\n            pet: Union[Cat, Dog, str] = Field(discriminator='pet_type')\n\n\ndef test_invalid_alias() -> None:\n    cat_fields = {\n        'kind': core_schema.typed_dict_field(core_schema.literal_schema(['cat']), validation_alias=['cat', 'CAT'])\n    }\n    dog_fields = {'kind': core_schema.typed_dict_field(core_schema.literal_schema(['dog']))}\n    cat = core_schema.typed_dict_schema(cat_fields)\n    dog = core_schema.typed_dict_schema(dog_fields)\n    schema = core_schema.union_schema([cat, dog])\n\n    with pytest.raises(TypeError, match=re.escape(\"Alias ['cat', 'CAT'] is not supported in a discriminated union\")):\n        apply_discriminator(schema, 'kind')\n\n\ndef test_invalid_discriminator_type() -> None:\n    cat_fields = {'kind': core_schema.typed_dict_field(core_schema.int_schema())}\n    dog_fields = {'kind': core_schema.typed_dict_field(core_schema.str_schema())}\n    cat = core_schema.typed_dict_schema(cat_fields)\n    dog = core_schema.typed_dict_schema(dog_fields)\n\n    with pytest.raises(TypeError, match=re.escape(\"TypedDict needs field 'kind' to be of type `Literal`\")):\n        apply_discriminator(core_schema.union_schema([cat, dog]), 'kind')\n\n\ndef test_missing_discriminator_field() -> None:\n    cat_fields = {'kind': core_schema.typed_dict_field(core_schema.int_schema())}\n    dog_fields = {}\n    cat = core_schema.typed_dict_schema(cat_fields)\n    dog = core_schema.typed_dict_schema(dog_fields)\n\n    with pytest.raises(TypeError, match=re.escape(\"TypedDict needs a discriminator field for key 'kind'\")):\n        apply_discriminator(core_schema.union_schema([dog, cat]), 'kind')\n\n\ndef test_wrap_function_schema() -> None:\n    cat_fields = {'kind': core_schema.typed_dict_field(core_schema.literal_schema(['cat']))}\n    dog_fields = {'kind': core_schema.typed_dict_field(core_schema.literal_schema(['dog']))}\n    cat = core_schema.with_info_wrap_validator_function(lambda x, y, z: None, core_schema.typed_dict_schema(cat_fields))\n    dog = core_schema.typed_dict_schema(dog_fields)\n    schema = core_schema.union_schema([cat, dog])\n\n    assert apply_discriminator(schema, 'kind') == {\n        'choices': {\n            'cat': {\n                'function': {\n                    'type': 'with-info',\n                    'function': HasRepr(IsStr(regex=r'<function [a-z_]*\\.<locals>\\.<lambda> at 0x[0-9a-fA-F]+>')),\n                },\n                'schema': {\n                    'fields': {\n                        'kind': {'schema': {'expected': ['cat'], 'type': 'literal'}, 'type': 'typed-dict-field'}\n                    },\n                    'type': 'typed-dict',\n                },\n                'type': 'function-wrap',\n            },\n            'dog': {\n                'fields': {'kind': {'schema': {'expected': ['dog'], 'type': 'literal'}, 'type': 'typed-dict-field'}},\n                'type': 'typed-dict',\n            },\n        },\n        'discriminator': 'kind',\n        'from_attributes': True,\n        'strict': False,\n        'type': 'tagged-union',\n    }\n\n\ndef test_plain_function_schema_is_invalid() -> None:\n    with pytest.raises(\n        TypeError,\n        match=\"'function-plain' is not a valid discriminated union variant; \" 'should be a `BaseModel` or `dataclass`',\n    ):\n        apply_discriminator(\n            core_schema.union_schema(\n                [core_schema.with_info_plain_validator_function(lambda x, y: None), core_schema.int_schema()]\n            ),\n            'kind',\n        )\n\n\ndef test_invalid_str_choice_discriminator_values() -> None:\n    cat = core_schema.typed_dict_schema({'kind': core_schema.typed_dict_field(core_schema.literal_schema(['cat']))})\n    dog = core_schema.str_schema()\n\n    schema = core_schema.union_schema(\n        [\n            cat,\n            # NOTE: Wrapping the union with a validator results in failure to more thoroughly decompose the tagged\n            # union. I think this would be difficult to avoid in the general case, and I would suggest that we not\n            # attempt to do more than this until presented with scenarios where it is helpful/necessary.\n            core_schema.with_info_wrap_validator_function(lambda x, y, z: x, dog),\n        ]\n    )\n\n    with pytest.raises(\n        TypeError, match=\"'str' is not a valid discriminated union variant; should be a `BaseModel` or `dataclass`\"\n    ):\n        apply_discriminator(schema, 'kind')\n\n\ndef test_lax_or_strict_definitions() -> None:\n    cat = core_schema.typed_dict_schema({'kind': core_schema.typed_dict_field(core_schema.literal_schema(['cat']))})\n    lax_dog = core_schema.typed_dict_schema({'kind': core_schema.typed_dict_field(core_schema.literal_schema(['DOG']))})\n    strict_dog = core_schema.definitions_schema(\n        core_schema.typed_dict_schema({'kind': core_schema.typed_dict_field(core_schema.literal_schema(['dog']))}),\n        [core_schema.int_schema(ref='my-int-definition')],\n    )\n    dog = core_schema.definitions_schema(\n        core_schema.lax_or_strict_schema(lax_schema=lax_dog, strict_schema=strict_dog),\n        [core_schema.str_schema(ref='my-str-definition')],\n    )\n    discriminated_schema = apply_discriminator(core_schema.union_schema([cat, dog]), 'kind')\n    # insert_assert(discriminated_schema)\n    assert discriminated_schema == {\n        'type': 'tagged-union',\n        'choices': {\n            'cat': {\n                'type': 'typed-dict',\n                'fields': {'kind': {'type': 'typed-dict-field', 'schema': {'type': 'literal', 'expected': ['cat']}}},\n            },\n            'DOG': {\n                'type': 'lax-or-strict',\n                'lax_schema': {\n                    'type': 'typed-dict',\n                    'fields': {\n                        'kind': {'type': 'typed-dict-field', 'schema': {'type': 'literal', 'expected': ['DOG']}}\n                    },\n                },\n                'strict_schema': {\n                    'type': 'definitions',\n                    'schema': {\n                        'type': 'typed-dict',\n                        'fields': {\n                            'kind': {'type': 'typed-dict-field', 'schema': {'type': 'literal', 'expected': ['dog']}}\n                        },\n                    },\n                    'definitions': [{'type': 'int', 'ref': 'my-int-definition'}],\n                },\n            },\n            'dog': {\n                'type': 'lax-or-strict',\n                'lax_schema': {\n                    'type': 'typed-dict',\n                    'fields': {\n                        'kind': {'type': 'typed-dict-field', 'schema': {'type': 'literal', 'expected': ['DOG']}}\n                    },\n                },\n                'strict_schema': {\n                    'type': 'definitions',\n                    'schema': {\n                        'type': 'typed-dict',\n                        'fields': {\n                            'kind': {'type': 'typed-dict-field', 'schema': {'type': 'literal', 'expected': ['dog']}}\n                        },\n                    },\n                    'definitions': [{'type': 'int', 'ref': 'my-int-definition'}],\n                },\n            },\n        },\n        'discriminator': 'kind',\n        'strict': False,\n        'from_attributes': True,\n    }\n\n\ndef test_wrapped_nullable_union() -> None:\n    cat = core_schema.typed_dict_schema({'kind': core_schema.typed_dict_field(core_schema.literal_schema(['cat']))})\n    dog = core_schema.typed_dict_schema({'kind': core_schema.typed_dict_field(core_schema.literal_schema(['dog']))})\n    ant = core_schema.typed_dict_schema({'kind': core_schema.typed_dict_field(core_schema.literal_schema(['ant']))})\n\n    schema = core_schema.union_schema(\n        [\n            ant,\n            # NOTE: Wrapping the union with a validator results in failure to more thoroughly decompose the tagged\n            # union. I think this would be difficult to avoid in the general case, and I would suggest that we not\n            # attempt to do more than this until presented with scenarios where it is helpful/necessary.\n            core_schema.with_info_wrap_validator_function(\n                lambda x, y, z: x, core_schema.nullable_schema(core_schema.union_schema([cat, dog]))\n            ),\n        ]\n    )\n    discriminated_schema = apply_discriminator(schema, 'kind')\n    validator = SchemaValidator(discriminated_schema)\n    assert validator.validate_python({'kind': 'ant'})['kind'] == 'ant'\n    assert validator.validate_python({'kind': 'cat'})['kind'] == 'cat'\n    assert validator.validate_python(None) is None\n    with pytest.raises(ValidationError) as exc_info:\n        validator.validate_python({'kind': 'armadillo'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'kind'\", 'expected_tags': \"'ant', 'cat', 'dog'\", 'tag': 'armadillo'},\n            'input': {'kind': 'armadillo'},\n            'loc': (),\n            'msg': \"Input tag 'armadillo' found using 'kind' does not match any of the \"\n            \"expected tags: 'ant', 'cat', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n    # insert_assert(discriminated_schema)\n    assert discriminated_schema == {\n        'type': 'nullable',\n        'schema': {\n            'type': 'tagged-union',\n            'choices': {\n                'ant': {\n                    'type': 'typed-dict',\n                    'fields': {\n                        'kind': {'type': 'typed-dict-field', 'schema': {'type': 'literal', 'expected': ['ant']}}\n                    },\n                },\n                'cat': {\n                    'type': 'function-wrap',\n                    'function': {\n                        'type': 'with-info',\n                        'function': HasRepr(IsStr(regex=r'<function [a-z_]*\\.<locals>\\.<lambda> at 0x[0-9a-fA-F]+>')),\n                    },\n                    'schema': {\n                        'type': 'nullable',\n                        'schema': {\n                            'type': 'union',\n                            'choices': [\n                                {\n                                    'type': 'typed-dict',\n                                    'fields': {\n                                        'kind': {\n                                            'type': 'typed-dict-field',\n                                            'schema': {'type': 'literal', 'expected': ['cat']},\n                                        }\n                                    },\n                                },\n                                {\n                                    'type': 'typed-dict',\n                                    'fields': {\n                                        'kind': {\n                                            'type': 'typed-dict-field',\n                                            'schema': {'type': 'literal', 'expected': ['dog']},\n                                        }\n                                    },\n                                },\n                            ],\n                        },\n                    },\n                },\n                'dog': {\n                    'type': 'function-wrap',\n                    'function': {\n                        'type': 'with-info',\n                        'function': HasRepr(IsStr(regex=r'<function [a-z_]*\\.<locals>\\.<lambda> at 0x[0-9a-fA-F]+>')),\n                    },\n                    'schema': {\n                        'type': 'nullable',\n                        'schema': {\n                            'type': 'union',\n                            'choices': [\n                                {\n                                    'type': 'typed-dict',\n                                    'fields': {\n                                        'kind': {\n                                            'type': 'typed-dict-field',\n                                            'schema': {'type': 'literal', 'expected': ['cat']},\n                                        }\n                                    },\n                                },\n                                {\n                                    'type': 'typed-dict',\n                                    'fields': {\n                                        'kind': {\n                                            'type': 'typed-dict-field',\n                                            'schema': {'type': 'literal', 'expected': ['dog']},\n                                        }\n                                    },\n                                },\n                            ],\n                        },\n                    },\n                },\n            },\n            'discriminator': 'kind',\n            'strict': False,\n            'from_attributes': True,\n        },\n    }\n\n\ndef test_union_in_submodel() -> None:\n    class UnionModel1(BaseModel):\n        type: Literal[1] = 1\n        other: Literal['UnionModel1'] = 'UnionModel1'\n\n    class UnionModel2(BaseModel):\n        type: Literal[2] = 2\n        other: Literal['UnionModel2'] = 'UnionModel2'\n\n    UnionModel = Annotated[Union[UnionModel1, UnionModel2], Field(discriminator='type')]\n\n    class SubModel1(BaseModel):\n        union_model: UnionModel\n\n    class SubModel2(BaseModel):\n        union_model: UnionModel\n\n    class TestModel(BaseModel):\n        submodel: Union[SubModel1, SubModel2]\n\n    m = TestModel.model_validate({'submodel': {'union_model': {'type': 1}}})\n    assert isinstance(m.submodel, SubModel1)\n    assert isinstance(m.submodel.union_model, UnionModel1)\n\n    m = TestModel.model_validate({'submodel': {'union_model': {'type': 2}}})\n    assert isinstance(m.submodel, SubModel1)\n    assert isinstance(m.submodel.union_model, UnionModel2)\n\n    with pytest.raises(ValidationError) as exc_info:\n        TestModel.model_validate({'submodel': {'union_model': {'type': 1, 'other': 'UnionModel2'}}})\n\n    # insert_assert(exc_info.value.errors(include_url=False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': ('submodel', 'SubModel1', 'union_model', 1, 'other'),\n            'msg': \"Input should be 'UnionModel1'\",\n            'input': 'UnionModel2',\n            'ctx': {'expected': \"'UnionModel1'\"},\n        },\n        {\n            'type': 'literal_error',\n            'loc': ('submodel', 'SubModel2', 'union_model', 1, 'other'),\n            'msg': \"Input should be 'UnionModel1'\",\n            'input': 'UnionModel2',\n            'ctx': {'expected': \"'UnionModel1'\"},\n        },\n    ]\n\n    # insert_assert(TestModel.model_json_schema())\n    assert TestModel.model_json_schema() == {\n        '$defs': {\n            'SubModel1': {\n                'properties': {\n                    'union_model': {\n                        'discriminator': {\n                            'mapping': {'1': '#/$defs/UnionModel1', '2': '#/$defs/UnionModel2'},\n                            'propertyName': 'type',\n                        },\n                        'oneOf': [{'$ref': '#/$defs/UnionModel1'}, {'$ref': '#/$defs/UnionModel2'}],\n                        'title': 'Union Model',\n                    }\n                },\n                'required': ['union_model'],\n                'title': 'SubModel1',\n                'type': 'object',\n            },\n            'SubModel2': {\n                'properties': {\n                    'union_model': {\n                        'discriminator': {\n                            'mapping': {'1': '#/$defs/UnionModel1', '2': '#/$defs/UnionModel2'},\n                            'propertyName': 'type',\n                        },\n                        'oneOf': [{'$ref': '#/$defs/UnionModel1'}, {'$ref': '#/$defs/UnionModel2'}],\n                        'title': 'Union Model',\n                    }\n                },\n                'required': ['union_model'],\n                'title': 'SubModel2',\n                'type': 'object',\n            },\n            'UnionModel1': {\n                'properties': {\n                    'type': {'const': 1, 'default': 1, 'enum': [1], 'title': 'Type', 'type': 'integer'},\n                    'other': {\n                        'const': 'UnionModel1',\n                        'default': 'UnionModel1',\n                        'enum': ['UnionModel1'],\n                        'title': 'Other',\n                        'type': 'string',\n                    },\n                },\n                'title': 'UnionModel1',\n                'type': 'object',\n            },\n            'UnionModel2': {\n                'properties': {\n                    'type': {'const': 2, 'default': 2, 'enum': [2], 'title': 'Type', 'type': 'integer'},\n                    'other': {\n                        'const': 'UnionModel2',\n                        'default': 'UnionModel2',\n                        'enum': ['UnionModel2'],\n                        'title': 'Other',\n                        'type': 'string',\n                    },\n                },\n                'title': 'UnionModel2',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'submodel': {'anyOf': [{'$ref': '#/$defs/SubModel1'}, {'$ref': '#/$defs/SubModel2'}], 'title': 'Submodel'}\n        },\n        'required': ['submodel'],\n        'title': 'TestModel',\n        'type': 'object',\n    }\n\n\ndef test_function_after_discriminator():\n    class CatModel(BaseModel):\n        name: Literal['kitty', 'cat']\n\n        @field_validator('name', mode='after')\n        def replace_name(cls, v):\n            return 'cat'\n\n    class DogModel(BaseModel):\n        name: Literal['puppy', 'dog']\n\n        # comment out the 2 field validators and model will work!\n        @field_validator('name', mode='after')\n        def replace_name(cls, v):\n            return 'dog'\n\n    AllowedAnimal = Annotated[Union[CatModel, DogModel], Field(discriminator='name')]\n\n    class Model(BaseModel):\n        x: AllowedAnimal\n\n    m = Model(x={'name': 'kitty'})\n    assert m.x.name == 'cat'\n\n    # Ensure a discriminated union is actually being used during validation\n    with pytest.raises(ValidationError) as exc_info:\n        Model(x={'name': 'invalid'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': \"'name'\", 'expected_tags': \"'kitty', 'cat', 'puppy', 'dog'\", 'tag': 'invalid'},\n            'input': {'name': 'invalid'},\n            'loc': ('x',),\n            'msg': \"Input tag 'invalid' found using 'name' does not match any of the \"\n            \"expected tags: 'kitty', 'cat', 'puppy', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n\ndef test_sequence_discriminated_union():\n    class Cat(BaseModel):\n        pet_type: Literal['cat']\n        meows: int\n\n    class Dog(BaseModel):\n        pet_type: Literal['dog']\n        barks: float\n\n    class Lizard(BaseModel):\n        pet_type: Literal['reptile', 'lizard']\n        scales: bool\n\n    Pet = Annotated[Union[Cat, Dog, Lizard], Field(discriminator='pet_type')]\n\n    class Model(BaseModel):\n        pet: Sequence[Pet]\n        n: int\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'Cat': {\n                'properties': {\n                    'pet_type': {'const': 'cat', 'enum': ['cat'], 'title': 'Pet Type', 'type': 'string'},\n                    'meows': {'title': 'Meows', 'type': 'integer'},\n                },\n                'required': ['pet_type', 'meows'],\n                'title': 'Cat',\n                'type': 'object',\n            },\n            'Dog': {\n                'properties': {\n                    'pet_type': {'const': 'dog', 'enum': ['dog'], 'title': 'Pet Type', 'type': 'string'},\n                    'barks': {'title': 'Barks', 'type': 'number'},\n                },\n                'required': ['pet_type', 'barks'],\n                'title': 'Dog',\n                'type': 'object',\n            },\n            'Lizard': {\n                'properties': {\n                    'pet_type': {'enum': ['reptile', 'lizard'], 'title': 'Pet Type', 'type': 'string'},\n                    'scales': {'title': 'Scales', 'type': 'boolean'},\n                },\n                'required': ['pet_type', 'scales'],\n                'title': 'Lizard',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'pet': {\n                'items': {\n                    'discriminator': {\n                        'mapping': {\n                            'cat': '#/$defs/Cat',\n                            'dog': '#/$defs/Dog',\n                            'lizard': '#/$defs/Lizard',\n                            'reptile': '#/$defs/Lizard',\n                        },\n                        'propertyName': 'pet_type',\n                    },\n                    'oneOf': [{'$ref': '#/$defs/Cat'}, {'$ref': '#/$defs/Dog'}, {'$ref': '#/$defs/Lizard'}],\n                },\n                'title': 'Pet',\n                'type': 'array',\n            },\n            'n': {'title': 'N', 'type': 'integer'},\n        },\n        'required': ['pet', 'n'],\n        'title': 'Model',\n        'type': 'object',\n    }\n\n\n@pytest.fixture(scope='session', name='animals')\ndef callable_discriminated_union_animals() -> SimpleNamespace:\n    class Cat(BaseModel):\n        pet_type: Literal['cat'] = 'cat'\n\n    class Dog(BaseModel):\n        pet_kind: Literal['dog'] = 'dog'\n\n    class Fish(BaseModel):\n        pet_kind: Literal['fish'] = 'fish'\n\n    class Lizard(BaseModel):\n        pet_variety: Literal['lizard'] = 'lizard'\n\n    animals = SimpleNamespace(cat=Cat, dog=Dog, fish=Fish, lizard=Lizard)\n    return animals\n\n\n@pytest.fixture(scope='session', name='get_pet_discriminator_value')\ndef shared_pet_discriminator_value() -> Callable[[Any], str]:\n    def get_discriminator_value(v):\n        if isinstance(v, dict):\n            return v.get('pet_type', v.get('pet_kind'))\n        return getattr(v, 'pet_type', getattr(v, 'pet_kind', None))\n\n    return get_discriminator_value\n\n\ndef test_callable_discriminated_union_with_type_adapter(\n    animals: SimpleNamespace, get_pet_discriminator_value: Callable[[Any], str]\n) -> None:\n    pet_adapter = TypeAdapter(\n        Annotated[\n            Union[Annotated[animals.cat, Tag('cat')], Annotated[animals.dog, Tag('dog')]],\n            Discriminator(get_pet_discriminator_value),\n        ]\n    )\n\n    assert pet_adapter.validate_python({'pet_type': 'cat'}).pet_type == 'cat'\n    assert pet_adapter.validate_python({'pet_kind': 'dog'}).pet_kind == 'dog'\n    assert pet_adapter.validate_python(animals.cat()).pet_type == 'cat'\n    assert pet_adapter.validate_python(animals.dog()).pet_kind == 'dog'\n    assert pet_adapter.validate_json('{\"pet_type\":\"cat\"}').pet_type == 'cat'\n    assert pet_adapter.validate_json('{\"pet_kind\":\"dog\"}').pet_kind == 'dog'\n\n    # Unexpected discriminator value for dict\n    with pytest.raises(ValidationError) as exc_info:\n        pet_adapter.validate_python({'pet_kind': 'fish'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': 'get_discriminator_value()', 'expected_tags': \"'cat', 'dog'\", 'tag': 'fish'},\n            'input': {'pet_kind': 'fish'},\n            'loc': (),\n            'msg': \"Input tag 'fish' found using get_discriminator_value() does not \"\n            \"match any of the expected tags: 'cat', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n    # Missing discriminator key for dict\n    with pytest.raises(ValidationError) as exc_info:\n        pet_adapter.validate_python({'pet_variety': 'lizard'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': 'get_discriminator_value()'},\n            'input': {'pet_variety': 'lizard'},\n            'loc': (),\n            'msg': 'Unable to extract tag using discriminator get_discriminator_value()',\n            'type': 'union_tag_not_found',\n        }\n    ]\n\n    # Unexpected discriminator value for instance\n    with pytest.raises(ValidationError) as exc_info:\n        pet_adapter.validate_python(animals.fish())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': 'get_discriminator_value()', 'expected_tags': \"'cat', 'dog'\", 'tag': 'fish'},\n            'input': animals.fish(pet_kind='fish'),\n            'loc': (),\n            'msg': \"Input tag 'fish' found using get_discriminator_value() does not \"\n            \"match any of the expected tags: 'cat', 'dog'\",\n            'type': 'union_tag_invalid',\n        }\n    ]\n\n    # Missing discriminator key for instance\n    with pytest.raises(ValidationError) as exc_info:\n        pet_adapter.validate_python(animals.lizard())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': 'get_discriminator_value()'},\n            'input': animals.lizard(pet_variety='lizard'),\n            'loc': (),\n            'msg': 'Unable to extract tag using discriminator get_discriminator_value()',\n            'type': 'union_tag_not_found',\n        }\n    ]\n\n\ndef test_various_syntax_options_for_callable_union(\n    animals: SimpleNamespace, get_pet_discriminator_value: Callable[[Any], str]\n) -> None:\n    class PetModelField(BaseModel):\n        pet: Union[Annotated[animals.cat, Tag('cat')], Annotated[animals.dog, Tag('dog')]] = Field(\n            discriminator=Discriminator(get_pet_discriminator_value)\n        )\n\n    class PetModelAnnotated(BaseModel):\n        pet: Annotated[\n            Union[Annotated[animals.cat, Tag('cat')], Annotated[animals.dog, Tag('dog')]],\n            Discriminator(get_pet_discriminator_value),\n        ]\n\n    class PetModelAnnotatedWithField(BaseModel):\n        pet: Annotated[\n            Union[Annotated[animals.cat, Tag('cat')], Annotated[animals.dog, Tag('dog')]],\n            Field(discriminator=Discriminator(get_pet_discriminator_value)),\n        ]\n\n    models = [PetModelField, PetModelAnnotated, PetModelAnnotatedWithField]\n\n    for model in models:\n        assert model.model_validate({'pet': {'pet_type': 'cat'}}).pet.pet_type == 'cat'\n        assert model.model_validate({'pet': {'pet_kind': 'dog'}}).pet.pet_kind == 'dog'\n        assert model(pet=animals.cat()).pet.pet_type == 'cat'\n        assert model(pet=animals.dog()).pet.pet_kind == 'dog'\n        assert model.model_validate_json('{\"pet\": {\"pet_type\":\"cat\"}}').pet.pet_type == 'cat'\n        assert model.model_validate_json('{\"pet\": {\"pet_kind\":\"dog\"}}').pet.pet_kind == 'dog'\n\n        # Unexpected discriminator value for dict\n        with pytest.raises(ValidationError) as exc_info:\n            model.model_validate({'pet': {'pet_kind': 'fish'}})\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'ctx': {'discriminator': 'get_discriminator_value()', 'expected_tags': \"'cat', 'dog'\", 'tag': 'fish'},\n                'input': {'pet_kind': 'fish'},\n                'loc': ('pet',),\n                'msg': \"Input tag 'fish' found using get_discriminator_value() does not \"\n                \"match any of the expected tags: 'cat', 'dog'\",\n                'type': 'union_tag_invalid',\n            }\n        ]\n\n        # Missing discriminator key for dict\n        with pytest.raises(ValidationError) as exc_info:\n            model.model_validate({'pet': {'pet_variety': 'lizard'}})\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'ctx': {'discriminator': 'get_discriminator_value()'},\n                'input': {'pet_variety': 'lizard'},\n                'loc': ('pet',),\n                'msg': 'Unable to extract tag using discriminator get_discriminator_value()',\n                'type': 'union_tag_not_found',\n            }\n        ]\n\n        # Unexpected discriminator value for instance\n        with pytest.raises(ValidationError) as exc_info:\n            model(pet=animals.fish())\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'ctx': {'discriminator': 'get_discriminator_value()', 'expected_tags': \"'cat', 'dog'\", 'tag': 'fish'},\n                'input': animals.fish(pet_kind='fish'),\n                'loc': ('pet',),\n                'msg': \"Input tag 'fish' found using get_discriminator_value() does not \"\n                \"match any of the expected tags: 'cat', 'dog'\",\n                'type': 'union_tag_invalid',\n            }\n        ]\n\n        # Missing discriminator key for instance\n        with pytest.raises(ValidationError) as exc_info:\n            model(pet=animals.lizard())\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'ctx': {'discriminator': 'get_discriminator_value()'},\n                'input': animals.lizard(pet_variety='lizard'),\n                'loc': ('pet',),\n                'msg': 'Unable to extract tag using discriminator get_discriminator_value()',\n                'type': 'union_tag_not_found',\n            }\n        ]\n\n\ndef test_callable_discriminated_union_recursive():\n    # Demonstrate that the errors are very verbose without a callable discriminator:\n    class Model(BaseModel):\n        x: Union[str, 'Model']\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'x': {'x': {'x': 1}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': {'x': {'x': 1}}, 'loc': ('x', 'str'), 'msg': 'Input should be a valid string', 'type': 'string_type'},\n        {\n            'input': {'x': 1},\n            'loc': ('x', 'Model', 'x', 'str'),\n            'msg': 'Input should be a valid string',\n            'type': 'string_type',\n        },\n        {\n            'input': 1,\n            'loc': ('x', 'Model', 'x', 'Model', 'x', 'str'),\n            'msg': 'Input should be a valid string',\n            'type': 'string_type',\n        },\n        {\n            'ctx': {'class_name': 'Model'},\n            'input': 1,\n            'loc': ('x', 'Model', 'x', 'Model', 'x', 'Model'),\n            'msg': 'Input should be a valid dictionary or instance of Model',\n            'type': 'model_type',\n        },\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        Model.model_validate({'x': {'x': {'x': {}}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': {'x': {'x': {}}},\n            'loc': ('x', 'str'),\n            'msg': 'Input should be a valid string',\n            'type': 'string_type',\n        },\n        {\n            'input': {'x': {}},\n            'loc': ('x', 'Model', 'x', 'str'),\n            'msg': 'Input should be a valid string',\n            'type': 'string_type',\n        },\n        {\n            'input': {},\n            'loc': ('x', 'Model', 'x', 'Model', 'x', 'str'),\n            'msg': 'Input should be a valid string',\n            'type': 'string_type',\n        },\n        {\n            'input': {},\n            'loc': ('x', 'Model', 'x', 'Model', 'x', 'Model', 'x'),\n            'msg': 'Field required',\n            'type': 'missing',\n        },\n    ]\n\n    # Demonstrate that the errors are less verbose _with_ a callable discriminator:\n    def model_x_discriminator(v):\n        if isinstance(v, str):\n            return 'str'\n        if isinstance(v, (dict, BaseModel)):\n            return 'model'\n\n    class DiscriminatedModel(BaseModel):\n        x: Annotated[\n            Union[Annotated[str, Tag('str')], Annotated['DiscriminatedModel', Tag('model')]],\n            Discriminator(\n                model_x_discriminator,\n                custom_error_type='invalid_union_member',\n                custom_error_message='Invalid union member',\n                custom_error_context={'discriminator': 'str_or_model'},\n            ),\n        ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        DiscriminatedModel.model_validate({'x': {'x': {'x': 1}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'discriminator': 'str_or_model'},\n            'input': 1,\n            'loc': ('x', 'model', 'x', 'model', 'x'),\n            'msg': 'Invalid union member',\n            'type': 'invalid_union_member',\n        }\n    ]\n\n    with pytest.raises(ValidationError) as exc_info:\n        DiscriminatedModel.model_validate({'x': {'x': {'x': {}}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'input': {},\n            'loc': ('x', 'model', 'x', 'model', 'x', 'model', 'x'),\n            'msg': 'Field required',\n            'type': 'missing',\n        }\n    ]\n    # Demonstrate that the data is still handled properly when valid:\n    data = {'x': {'x': {'x': 'a'}}}\n    m = DiscriminatedModel.model_validate(data)\n    assert m == DiscriminatedModel(x=DiscriminatedModel(x=DiscriminatedModel(x='a')))\n    assert m.model_dump() == data\n\n\ndef test_callable_discriminated_union_with_missing_tag() -> None:\n    def model_x_discriminator(v):\n        if isinstance(v, str):\n            return 'str'\n        if isinstance(v, (dict, BaseModel)):\n            return 'model'\n\n    try:\n\n        class DiscriminatedModel(BaseModel):\n            x: Annotated[\n                Union[str, 'DiscriminatedModel'],\n                Discriminator(model_x_discriminator),\n            ]\n    except PydanticUserError as exc_info:\n        assert exc_info.code == 'callable-discriminator-no-tag'\n\n    try:\n\n        class DiscriminatedModel(BaseModel):\n            x: Annotated[\n                Union[Annotated[str, Tag('str')], 'DiscriminatedModel'],\n                Discriminator(model_x_discriminator),\n            ]\n    except PydanticUserError as exc_info:\n        assert exc_info.code == 'callable-discriminator-no-tag'\n\n    try:\n\n        class DiscriminatedModel(BaseModel):\n            x: Annotated[\n                Union[str, Annotated['DiscriminatedModel', Tag('model')]],\n                Discriminator(model_x_discriminator),\n            ]\n    except PydanticUserError as exc_info:\n        assert exc_info.code == 'callable-discriminator-no-tag'\n\n\n@pytest.mark.xfail(reason='Issue not yet fixed, see: https://github.com/pydantic/pydantic/issues/8271.')\ndef test_presence_of_discriminator_when_generating_type_adaptor_json_schema_definitions() -> None:\n    class ItemType(str, Enum):\n        ITEM1 = 'item1'\n        ITEM2 = 'item2'\n\n    class CreateItem1(BaseModel):\n        item_type: Annotated[Literal[ItemType.ITEM1], Field(alias='type')]\n        id: int\n\n    class CreateItem2(BaseModel):\n        item_type: Annotated[Literal[ItemType.ITEM2], Field(alias='type')]\n        id: int\n\n    class CreateObjectDto(BaseModel):\n        id: int\n        items: List[\n            Annotated[\n                Union[\n                    CreateItem1,\n                    CreateItem2,\n                ],\n                Field(discriminator='item_type'),\n            ]\n        ]\n\n    adaptor = TypeAdapter(\n        Annotated[CreateObjectDto, FieldInfo(examples=[{'id': 1, 'items': [{'id': 3, 'type': 'ITEM1'}]}])]\n    )\n\n    schema_map, definitions = GenerateJsonSchema().generate_definitions([(adaptor, 'validation', adaptor.core_schema)])\n    assert definitions == {\n        'CreateItem1': {\n            'properties': {'id': {'title': 'Id', 'type': 'integer'}, 'type': {'const': 'item1', 'title': 'Type'}},\n            'required': ['type', 'id'],\n            'title': 'CreateItem1',\n            'type': 'object',\n        },\n        'CreateItem2': {\n            'properties': {'id': {'title': 'Id', 'type': 'integer'}, 'type': {'const': 'item2', 'title': 'Type'}},\n            'required': ['type', 'id'],\n            'title': 'CreateItem2',\n            'type': 'object',\n        },\n        'CreateObjectDto': {\n            'properties': {\n                'id': {'title': 'Id', 'type': 'integer'},\n                'items': {\n                    'items': {\n                        'discriminator': {\n                            'mapping': {'item1': '#/$defs/CreateItem1', 'item2': '#/$defs/CreateItem2'},\n                            'propertyName': 'type',\n                        },\n                        'oneOf': [{'$ref': '#/$defs/CreateItem1'}, {'$ref': '#/$defs/CreateItem2'}],\n                    },\n                    'title': 'Items',\n                    'type': 'array',\n                },\n            },\n            'required': ['id', 'items'],\n            'title': 'CreateObjectDto',\n            'type': 'object',\n        },\n    }\n\n\ndef test_nested_discriminator() -> None:\n    \"\"\"\n    The exact details of the JSON schema produced are not necessarily important; the test was added in response to a\n    regression that caused the inner union to lose its discriminator. Even if the schema changes, the important\n    thing is that the core schema (and therefore JSON schema) produced has an actual discriminated union in it.\n    For more context, see: https://github.com/pydantic/pydantic/issues/8688.\n    \"\"\"\n\n    class Step_A(BaseModel):\n        type: Literal['stepA']\n        count: int\n\n    class Step_B(BaseModel):\n        type: Literal['stepB']\n        value: float\n\n    class MyModel(BaseModel):\n        type: Literal['mixed']\n        sub_models: List['SubModel']\n        steps: Union[Step_A, Step_B] = Field(\n            default=None,\n            discriminator='type',\n        )\n\n    class SubModel(MyModel):\n        type: Literal['mixed']\n        blending: float\n\n    MyModel.model_rebuild()\n    # insert_assert(MyModel.model_json_schema())\n    assert MyModel.model_json_schema() == {\n        '$defs': {\n            'Step_A': {\n                'properties': {\n                    'count': {'title': 'Count', 'type': 'integer'},\n                    'type': {'const': 'stepA', 'enum': ['stepA'], 'title': 'Type', 'type': 'string'},\n                },\n                'required': ['type', 'count'],\n                'title': 'Step_A',\n                'type': 'object',\n            },\n            'Step_B': {\n                'properties': {\n                    'type': {'const': 'stepB', 'enum': ['stepB'], 'title': 'Type', 'type': 'string'},\n                    'value': {'title': 'Value', 'type': 'number'},\n                },\n                'required': ['type', 'value'],\n                'title': 'Step_B',\n                'type': 'object',\n            },\n            'SubModel': {\n                'properties': {\n                    'blending': {'title': 'Blending', 'type': 'number'},\n                    'steps': {\n                        'default': None,\n                        'discriminator': {\n                            'mapping': {'stepA': '#/$defs/Step_A', 'stepB': '#/$defs/Step_B'},\n                            'propertyName': 'type',\n                        },\n                        'oneOf': [{'$ref': '#/$defs/Step_A'}, {'$ref': '#/$defs/Step_B'}],\n                        'title': 'Steps',\n                    },\n                    'sub_models': {'items': {'$ref': '#/$defs/SubModel'}, 'title': 'Sub Models', 'type': 'array'},\n                    'type': {'const': 'mixed', 'enum': ['mixed'], 'title': 'Type', 'type': 'string'},\n                },\n                'required': ['type', 'sub_models', 'blending'],\n                'title': 'SubModel',\n                'type': 'object',\n            },\n        },\n        'properties': {\n            'steps': {\n                'default': None,\n                'discriminator': {\n                    'mapping': {'stepA': '#/$defs/Step_A', 'stepB': '#/$defs/Step_B'},\n                    'propertyName': 'type',\n                },\n                'oneOf': [{'$ref': '#/$defs/Step_A'}, {'$ref': '#/$defs/Step_B'}],\n                'title': 'Steps',\n            },\n            'sub_models': {'items': {'$ref': '#/$defs/SubModel'}, 'title': 'Sub Models', 'type': 'array'},\n            'type': {'const': 'mixed', 'enum': ['mixed'], 'title': 'Type', 'type': 'string'},\n        },\n        'required': ['type', 'sub_models'],\n        'title': 'MyModel',\n        'type': 'object',\n    }\n\n\ndef test_nested_schema_gen_uses_tagged_union_in_ref() -> None:\n    class NestedState(BaseModel):\n        state_type: Literal['nested']\n        substate: 'AnyState'\n\n    # If this type is left out, the model behaves normally again\n    class LoopState(BaseModel):\n        state_type: Literal['loop']\n        substate: 'AnyState'\n\n    class LeafState(BaseModel):\n        state_type: Literal['leaf']\n\n    AnyState = Annotated[Union[NestedState, LoopState, LeafState], Field(..., discriminator='state_type')]\n    adapter = TypeAdapter(AnyState)\n\n    assert adapter.core_schema['schema']['type'] == 'tagged-union'\n    for definition in adapter.core_schema['definitions']:\n        if definition['schema']['model_name'] in ['NestedState', 'LoopState']:\n            assert definition['schema']['fields']['substate']['schema']['schema']['type'] == 'tagged-union'\n\n\ndef test_recursive_discriminiated_union_with_typed_dict() -> None:\n    class Foo(TypedDict):\n        type: Literal['foo']\n        x: 'Foobar'\n\n    class Bar(TypedDict):\n        type: Literal['bar']\n\n    Foobar = Annotated[Union[Foo, Bar], Field(discriminator='type')]\n    ta = TypeAdapter(Foobar)\n\n    # len of errors should be 1 for each case, bc we're using a tagged union\n    with pytest.raises(ValidationError) as e:\n        ta.validate_python({'type': 'wrong'})\n    assert len(e.value.errors()) == 1\n\n    with pytest.raises(ValidationError) as e:\n        ta.validate_python({'type': 'foo', 'x': {'type': 'wrong'}})\n    assert len(e.value.errors()) == 1\n\n    core_schema = ta.core_schema\n    assert core_schema['schema']['type'] == 'tagged-union'\n    for definition in core_schema['definitions']:\n        if 'Foo' in definition['ref']:\n            assert definition['fields']['x']['schema']['type'] == 'tagged-union'\n\n\ndef test_recursive_discriminiated_union_with_base_model() -> None:\n    class Foo(BaseModel):\n        type: Literal['foo']\n        x: 'Foobar'\n\n    class Bar(BaseModel):\n        type: Literal['bar']\n\n    Foobar = Annotated[Union[Foo, Bar], Field(discriminator='type')]\n    ta = TypeAdapter(Foobar)\n\n    # len of errors should be 1 for each case, bc we're using a tagged union\n    with pytest.raises(ValidationError) as e:\n        ta.validate_python({'type': 'wrong'})\n    assert len(e.value.errors()) == 1\n\n    with pytest.raises(ValidationError) as e:\n        ta.validate_python({'type': 'foo', 'x': {'type': 'wrong'}})\n    assert len(e.value.errors()) == 1\n\n    core_schema = ta.core_schema\n    assert core_schema['schema']['type'] == 'tagged-union'\n    for definition in core_schema['definitions']:\n        if 'Foo' in definition['ref']:\n            assert definition['schema']['fields']['x']['schema']['type'] == 'tagged-union'\n\n\ndef test_recursive_discriminated_union_with_pydantic_dataclass() -> None:\n    @pydantic_dataclass\n    class Foo:\n        type: Literal['foo']\n        x: 'Foobar'\n\n    @pydantic_dataclass\n    class Bar:\n        type: Literal['bar']\n\n    Foobar = Annotated[Union[Foo, Bar], Field(discriminator='type')]\n    ta = TypeAdapter(Foobar)\n\n    # len of errors should be 1 for each case, bc we're using a tagged union\n    with pytest.raises(ValidationError) as e:\n        ta.validate_python({'type': 'wrong'})\n    assert len(e.value.errors()) == 1\n\n    with pytest.raises(ValidationError) as e:\n        ta.validate_python({'type': 'foo', 'x': {'type': 'wrong'}})\n    assert len(e.value.errors()) == 1\n\n    core_schema = ta.core_schema\n    assert core_schema['schema']['type'] == 'tagged-union'\n    for definition in core_schema['definitions']:\n        if 'Foo' in definition['ref']:\n            for field in definition['schema']['fields']:\n                assert field['schema']['type'] == 'tagged-union' if field['name'] == 'x' else True\n\n\ndef test_discriminated_union_with_nested_dataclass() -> None:\n    @pydantic_dataclass\n    class Cat:\n        type: Literal['cat'] = 'cat'\n\n    @pydantic_dataclass\n    class Dog:\n        type: Literal['dog'] = 'dog'\n\n    @pydantic_dataclass\n    class NestedDataClass:\n        animal: Annotated[Union[Cat, Dog], Discriminator('type')]\n\n    @pydantic_dataclass\n    class Root:\n        data_class: NestedDataClass\n\n    ta = TypeAdapter(Root)\n    assert ta.core_schema['schema']['fields'][0]['schema']['schema']['fields'][0]['schema']['type'] == 'tagged-union'\n\n\ndef test_discriminated_union_with_nested_typed_dicts() -> None:\n    class Cat(TypedDict):\n        type: Literal['cat']\n\n    class Dog(TypedDict):\n        type: Literal['dog']\n\n    class NestedTypedDict(TypedDict):\n        animal: Annotated[Union[Cat, Dog], Discriminator('type')]\n\n    class Root(TypedDict):\n        data_class: NestedTypedDict\n\n    ta = TypeAdapter(Root)\n    assert ta.core_schema['fields']['data_class']['schema']['fields']['animal']['schema']['type'] == 'tagged-union'\n\n\ndef test_discriminated_union_with_unsubstituted_type_var() -> None:\n    T = TypeVar('T')\n\n    class Dog(BaseModel, Generic[T]):\n        type_: Literal['dog']\n        friends: List['GenericPet']\n        id: T\n\n    class Cat(BaseModel, Generic[T]):\n        type_: Literal['cat']\n        friends: List['GenericPet']\n        id: T\n\n    GenericPet = Annotated[Union[Dog[T], Cat[T]], Field(..., discriminator='type_')]\n\n    ta = TypeAdapter(Dog[int])\n    int_dog = {\n        'type_': 'dog',\n        'friends': [{'type_': 'dog', 'friends': [], 'id': 2}, {'type_': 'cat', 'friends': [], 'id': 3}],\n        'id': 1,\n    }\n    assert ta.validate_python(int_dog).id == 1\n    assert ta.validate_python(int_dog).friends[0].id == 2\n    assert ta.validate_python(int_dog).friends[1].id == 3\n\n\n@pytest.mark.xfail(\n    reason='model_dump does not properly serialize the discriminator field to string if it is using an Enum. Issue: https://github.com/pydantic/pydantic/issues/9235'\n)\ndef test_discriminated_union_model_dump_with_nested_class():\n    class SomeEnum(str, Enum):\n        CAT = 'cat'\n        DOG = 'dog'\n\n    class Dog(BaseModel):\n        type: Literal[SomeEnum.DOG] = SomeEnum.DOG\n        name: str\n\n    class Cat(BaseModel):\n        type: Literal[SomeEnum.CAT] = SomeEnum.CAT\n        name: str\n\n    class Yard(BaseModel):\n        pet: Union[Dog, Cat] = Field(discriminator='type')\n\n    yard = Yard(pet=Dog(name='Rex'))\n    yard_dict = yard.model_dump(mode='json')\n    assert isinstance(yard_dict['pet']['type'], str)\n    assert not isinstance(yard_dict['pet']['type'], SomeEnum)\n    assert str(yard_dict['pet']['type']) == 'dog'\n", "tests/test_utils.py": "import collections.abc\nimport json\nimport os\nimport pickle\nimport sys\nfrom copy import copy, deepcopy\nfrom typing import Callable, Dict, Generic, List, NewType, Tuple, TypeVar, Union\n\nimport pytest\nfrom dirty_equals import IsList\nfrom pydantic_core import PydanticCustomError, PydanticUndefined, core_schema\nfrom typing_extensions import Annotated, Literal\n\nfrom pydantic import BaseModel\nfrom pydantic._internal import _repr\nfrom pydantic._internal._core_utils import _WalkCoreSchema, pretty_print_core_schema\nfrom pydantic._internal._typing_extra import all_literal_values, get_origin, is_new_type\nfrom pydantic._internal._utils import (\n    BUILTIN_COLLECTIONS,\n    ClassAttribute,\n    ValueItems,\n    all_identical,\n    deep_update,\n    lenient_issubclass,\n    smart_deepcopy,\n    unique_list,\n)\nfrom pydantic._internal._validators import import_string\nfrom pydantic.alias_generators import to_camel, to_pascal, to_snake\nfrom pydantic.color import Color\n\ntry:\n    import devtools\nexcept ImportError:\n    devtools = None\n\n\ndef test_import_module():\n    assert import_string('os.path') == os.path\n\n\ndef test_import_module_invalid():\n    with pytest.raises(PydanticCustomError, match=\"Invalid python path: No module named 'xx'\"):\n        import_string('xx')\n\n\ndef test_import_no_attr():\n    with pytest.raises(PydanticCustomError, match=\"cannot import name 'foobar' from 'os'\"):\n        import_string('os:foobar')\n\n\ndef foobar(a, b, c=4):\n    pass\n\n\nT = TypeVar('T')\n\n\nclass LoggedVar(Generic[T]):\n    def get(self) -> T: ...\n\n\n@pytest.mark.parametrize(\n    'value,expected',\n    [\n        (str, 'str'),\n        ('foobar', 'str'),\n        ('SomeForwardRefString', 'str'),  # included to document current behavior; could be changed\n        (List['SomeForwardRef'], \"List[ForwardRef('SomeForwardRef')]\"),  # noqa: F821\n        (Union[str, int], 'Union[str, int]'),\n        (list, 'list'),\n        (List, 'List'),\n        ([1, 2, 3], 'list'),\n        (List[Dict[str, int]], 'List[Dict[str, int]]'),\n        (Tuple[str, int, float], 'Tuple[str, int, float]'),\n        (Tuple[str, ...], 'Tuple[str, ...]'),\n        (Union[int, List[str], Tuple[str, int]], 'Union[int, List[str], Tuple[str, int]]'),\n        (foobar, 'foobar'),\n        (LoggedVar, 'LoggedVar'),\n        (LoggedVar(), 'LoggedVar'),\n    ],\n)\ndef test_display_as_type(value, expected):\n    assert _repr.display_as_type(value) == expected\n\n\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='requires python 3.10 or higher')\n@pytest.mark.parametrize(\n    'value_gen,expected',\n    [\n        (lambda: str, 'str'),\n        (lambda: 'SomeForwardRefString', 'str'),  # included to document current behavior; could be changed\n        (lambda: List['SomeForwardRef'], \"List[ForwardRef('SomeForwardRef')]\"),  # noqa: F821\n        (lambda: str | int, 'Union[str, int]'),\n        (lambda: list, 'list'),\n        (lambda: List, 'List'),\n        (lambda: list[int], 'list[int]'),\n        (lambda: List[int], 'List[int]'),\n        (lambda: list[dict[str, int]], 'list[dict[str, int]]'),\n        (lambda: list[Union[str, int]], 'list[Union[str, int]]'),\n        (lambda: list[str | int], 'list[Union[str, int]]'),\n        (lambda: LoggedVar[int], 'LoggedVar[int]'),\n        (lambda: LoggedVar[Dict[int, str]], 'LoggedVar[Dict[int, str]]'),\n    ],\n)\ndef test_display_as_type_310(value_gen, expected):\n    value = value_gen()\n    assert _repr.display_as_type(value) == expected\n\n\ndef test_lenient_issubclass():\n    class A(str):\n        pass\n\n    assert lenient_issubclass(A, str) is True\n\n\n@pytest.mark.skipif(sys.version_info < (3, 9), reason='generic aliases are not available in python < 3.9')\ndef test_lenient_issubclass_with_generic_aliases():\n    from collections.abc import Mapping\n\n    # should not raise an error here:\n    assert lenient_issubclass(list[str], Mapping) is False\n\n\ndef test_lenient_issubclass_is_lenient():\n    assert lenient_issubclass('a', 'a') is False\n\n\n@pytest.mark.parametrize(\n    'input_value,output',\n    [\n        ([], []),\n        ([1, 1, 1, 2, 1, 2, 3, 2, 3, 1, 4, 2, 3, 1], [1, 2, 3, 4]),\n        (['a', 'a', 'b', 'a', 'b', 'c', 'b', 'c', 'a'], ['a', 'b', 'c']),\n    ],\n)\ndef test_unique_list(input_value, output):\n    assert unique_list(input_value) == output\n    assert unique_list(unique_list(input_value)) == unique_list(input_value)\n\n\ndef test_value_items():\n    v = ['a', 'b', 'c']\n    vi = ValueItems(v, {0, -1})\n    assert vi.is_excluded(2)\n    assert [v_ for i, v_ in enumerate(v) if not vi.is_excluded(i)] == ['b']\n\n    assert vi.is_included(2)\n    assert [v_ for i, v_ in enumerate(v) if vi.is_included(i)] == ['a', 'c']\n\n    v2 = {'a': v, 'b': {'a': 1, 'b': (1, 2)}, 'c': 1}\n\n    vi = ValueItems(v2, {'a': {0, -1}, 'b': {'a': ..., 'b': -1}})\n\n    assert not vi.is_excluded('a')\n    assert vi.is_included('a')\n    assert not vi.is_excluded('c')\n    assert not vi.is_included('c')\n\n    assert str(vi) == \"{'a': {0, -1}, 'b': {'a': Ellipsis, 'b': -1}}\"\n    assert repr(vi) == \"ValueItems({'a': {0, -1}, 'b': {'a': Ellipsis, 'b': -1}})\"\n\n    excluded = {k_: v_ for k_, v_ in v2.items() if not vi.is_excluded(k_)}\n    assert excluded == {'a': v, 'b': {'a': 1, 'b': (1, 2)}, 'c': 1}\n\n    included = {k_: v_ for k_, v_ in v2.items() if vi.is_included(k_)}\n    assert included == {'a': v, 'b': {'a': 1, 'b': (1, 2)}}\n\n    sub_v = included['a']\n    sub_vi = ValueItems(sub_v, vi.for_element('a'))\n    assert repr(sub_vi) == 'ValueItems({0: Ellipsis, 2: Ellipsis})'\n\n    assert sub_vi.is_excluded(2)\n    assert [v_ for i, v_ in enumerate(sub_v) if not sub_vi.is_excluded(i)] == ['b']\n\n    assert sub_vi.is_included(2)\n    assert [v_ for i, v_ in enumerate(sub_v) if sub_vi.is_included(i)] == ['a', 'c']\n\n    vi = ValueItems([], {'__all__': {}})\n    assert vi._items == {}\n\n    with pytest.raises(TypeError, match='Unexpected type of exclude value for index \"a\" <class \\'NoneType\\'>'):\n        ValueItems(['a', 'b'], {'a': None})\n\n    m = (\n        'Excluding fields from a sequence of sub-models or dicts must be performed index-wise: '\n        'expected integer keys or keyword \"__all__\"'\n    )\n    with pytest.raises(TypeError, match=m):\n        ValueItems(['a', 'b'], {'a': {}})\n\n    vi = ValueItems([1, 2, 3, 4], {'__all__': True})\n    assert repr(vi) == 'ValueItems({0: Ellipsis, 1: Ellipsis, 2: Ellipsis, 3: Ellipsis})'\n\n    vi = ValueItems([1, 2], {'__all__': {1, 2}})\n    assert repr(vi) == 'ValueItems({0: {1: Ellipsis, 2: Ellipsis}, 1: {1: Ellipsis, 2: Ellipsis}})'\n\n\n@pytest.mark.parametrize(\n    'base,override,intersect,expected',\n    [\n        # Check in default (union) mode\n        (..., ..., False, ...),\n        (None, None, False, None),\n        ({}, {}, False, {}),\n        (..., None, False, ...),\n        (None, ..., False, ...),\n        (None, {}, False, {}),\n        ({}, None, False, {}),\n        (..., {}, False, {}),\n        ({}, ..., False, ...),\n        ({'a': None}, {'a': None}, False, {}),\n        ({'a'}, ..., False, ...),\n        ({'a'}, {}, False, {'a': ...}),\n        ({'a'}, {'b'}, False, {'a': ..., 'b': ...}),\n        ({'a': ...}, {'b': {'c'}}, False, {'a': ..., 'b': {'c': ...}}),\n        ({'a': ...}, {'a': {'c'}}, False, {'a': {'c': ...}}),\n        ({'a': {'c': ...}, 'b': {'d'}}, {'a': ...}, False, {'a': ..., 'b': {'d': ...}}),\n        # Check in intersection mode\n        (..., ..., True, ...),\n        (None, None, True, None),\n        ({}, {}, True, {}),\n        (..., None, True, ...),\n        (None, ..., True, ...),\n        (None, {}, True, {}),\n        ({}, None, True, {}),\n        (..., {}, True, {}),\n        ({}, ..., True, {}),\n        ({'a': None}, {'a': None}, True, {}),\n        ({'a'}, ..., True, {'a': ...}),\n        ({'a'}, {}, True, {}),\n        ({'a'}, {'b'}, True, {}),\n        ({'a': ...}, {'b': {'c'}}, True, {}),\n        ({'a': ...}, {'a': {'c'}}, True, {'a': {'c': ...}}),\n        ({'a': {'c': ...}, 'b': {'d'}}, {'a': ...}, True, {'a': {'c': ...}}),\n        # Check usage of `True` instead of `...`\n        (..., True, False, True),\n        (True, ..., False, ...),\n        (True, None, False, True),\n        ({'a': {'c': True}, 'b': {'d'}}, {'a': True}, False, {'a': True, 'b': {'d': ...}}),\n    ],\n)\ndef test_value_items_merge(base, override, intersect, expected):\n    actual = ValueItems.merge(base, override, intersect=intersect)\n    assert actual == expected\n\n\ndef test_value_items_error():\n    with pytest.raises(TypeError) as e:\n        ValueItems(1, (1, 2, 3))\n\n    assert str(e.value) == \"Unexpected type of exclude value <class 'tuple'>\"\n\n\ndef test_is_new_type():\n    new_type = NewType('new_type', str)\n    new_new_type = NewType('new_new_type', new_type)\n    assert is_new_type(new_type)\n    assert is_new_type(new_new_type)\n    assert not is_new_type(str)\n\n\ndef test_pretty():\n    class MyTestModel(BaseModel):\n        a: int = 1\n        b: List[int] = [1, 2, 3]\n\n    m = MyTestModel()\n    assert m.__repr_name__() == 'MyTestModel'\n    assert str(m) == 'a=1 b=[1, 2, 3]'\n    assert repr(m) == 'MyTestModel(a=1, b=[1, 2, 3])'\n    assert list(m.__pretty__(lambda x: f'fmt: {x!r}')) == [\n        'MyTestModel(',\n        1,\n        'a=',\n        'fmt: 1',\n        ',',\n        0,\n        'b=',\n        'fmt: [1, 2, 3]',\n        ',',\n        0,\n        -1,\n        ')',\n    ]\n\n\n@pytest.mark.filterwarnings('ignore::DeprecationWarning')\ndef test_pretty_color():\n    c = Color('red')\n    assert str(c) == 'red'\n    assert repr(c) == \"Color('red', rgb=(255, 0, 0))\"\n    assert list(c.__pretty__(lambda x: f'fmt: {x!r}')) == [\n        'Color(',\n        1,\n        \"fmt: 'red'\",\n        ',',\n        0,\n        'rgb=',\n        'fmt: (255, 0, 0)',\n        ',',\n        0,\n        -1,\n        ')',\n    ]\n\n\n@pytest.mark.skipif(not devtools, reason='devtools not installed')\ndef test_devtools_output():\n    class MyTestModel(BaseModel):\n        a: int = 1\n        b: List[int] = [1, 2, 3]\n\n    assert devtools.pformat(MyTestModel()) == 'MyTestModel(\\n    a=1,\\n    b=[1, 2, 3],\\n)'\n\n\n@pytest.mark.parametrize(\n    'mapping, updating_mapping, expected_mapping, msg',\n    [\n        (\n            {'key': {'inner_key': 0}},\n            {'other_key': 1},\n            {'key': {'inner_key': 0}, 'other_key': 1},\n            'extra keys are inserted',\n        ),\n        (\n            {'key': {'inner_key': 0}, 'other_key': 1},\n            {'key': [1, 2, 3]},\n            {'key': [1, 2, 3], 'other_key': 1},\n            'values that can not be merged are updated',\n        ),\n        (\n            {'key': {'inner_key': 0}},\n            {'key': {'other_key': 1}},\n            {'key': {'inner_key': 0, 'other_key': 1}},\n            'values that have corresponding keys are merged',\n        ),\n        (\n            {'key': {'inner_key': {'deep_key': 0}}},\n            {'key': {'inner_key': {'other_deep_key': 1}}},\n            {'key': {'inner_key': {'deep_key': 0, 'other_deep_key': 1}}},\n            'deeply nested values that have corresponding keys are merged',\n        ),\n    ],\n)\ndef test_deep_update(mapping, updating_mapping, expected_mapping, msg):\n    assert deep_update(mapping, updating_mapping) == expected_mapping, msg\n\n\ndef test_deep_update_is_not_mutating():\n    mapping = {'key': {'inner_key': {'deep_key': 1}}}\n    updated_mapping = deep_update(mapping, {'key': {'inner_key': {'other_deep_key': 1}}})\n    assert updated_mapping == {'key': {'inner_key': {'deep_key': 1, 'other_deep_key': 1}}}\n    assert mapping == {'key': {'inner_key': {'deep_key': 1}}}\n\n\ndef test_undefined_repr():\n    assert repr(PydanticUndefined) == 'PydanticUndefined'\n\n\ndef test_undefined_copy():\n    assert copy(PydanticUndefined) is PydanticUndefined\n    assert deepcopy(PydanticUndefined) is PydanticUndefined\n\n\ndef test_class_attribute():\n    class Foo:\n        attr = ClassAttribute('attr', 'foo')\n\n    assert Foo.attr == 'foo'\n\n    with pytest.raises(AttributeError, match=\"'attr' attribute of 'Foo' is class-only\"):\n        Foo().attr\n\n    f = Foo()\n    f.attr = 'not foo'\n    assert f.attr == 'not foo'\n\n\ndef test_all_literal_values():\n    L1 = Literal['1']\n    assert all_literal_values(L1) == ['1']\n\n    L2 = Literal['2']\n    L12 = Literal[L1, L2]\n    assert all_literal_values(L12) == IsList('1', '2', check_order=False)\n\n    L312 = Literal['3', Literal[L1, L2]]\n    assert all_literal_values(L312) == IsList('3', '1', '2', check_order=False)\n\n\n@pytest.mark.parametrize(\n    'obj',\n    (1, 1.0, '1', b'1', int, None, test_all_literal_values, len, test_all_literal_values.__code__, lambda: ..., ...),\n)\ndef test_smart_deepcopy_immutable_non_sequence(obj, mocker):\n    # make sure deepcopy is not used\n    # (other option will be to use obj.copy(), but this will produce error as none of given objects have this method)\n    mocker.patch('pydantic._internal._utils.deepcopy', side_effect=RuntimeError)\n    assert smart_deepcopy(obj) is deepcopy(obj) is obj\n\n\n@pytest.mark.parametrize('empty_collection', (collection() for collection in BUILTIN_COLLECTIONS))\ndef test_smart_deepcopy_empty_collection(empty_collection, mocker):\n    mocker.patch('pydantic._internal._utils.deepcopy', side_effect=RuntimeError)  # make sure deepcopy is not used\n    if not isinstance(empty_collection, (tuple, frozenset)):  # empty tuple or frozenset are always the same object\n        assert smart_deepcopy(empty_collection) is not empty_collection\n\n\n@pytest.mark.parametrize(\n    'collection', (c.fromkeys((1,)) if issubclass(c, dict) else c((1,)) for c in BUILTIN_COLLECTIONS)\n)\ndef test_smart_deepcopy_collection(collection, mocker):\n    expected_value = object()\n    mocker.patch('pydantic._internal._utils.deepcopy', return_value=expected_value)\n    assert smart_deepcopy(collection) is expected_value\n\n\n@pytest.mark.parametrize('error', [TypeError, ValueError, RuntimeError])\ndef test_smart_deepcopy_error(error, mocker):\n    class RaiseOnBooleanOperation(str):\n        def __bool__(self):\n            raise error('raised error')\n\n    obj = RaiseOnBooleanOperation()\n    expected_value = deepcopy(obj)\n    assert smart_deepcopy(obj) == expected_value\n\n\nT = TypeVar('T')\n\n\n@pytest.mark.parametrize(\n    'input_value,output_value',\n    [\n        (Annotated[int, 10] if Annotated else None, Annotated),\n        (Callable[[], T][int], collections.abc.Callable),\n        (Dict[str, int], dict),\n        (List[str], list),\n        (Union[int, str], Union),\n        (int, None),\n    ],\n)\ndef test_get_origin(input_value, output_value):\n    if input_value is None:\n        pytest.skip('Skipping undefined hint for this python version')\n    assert get_origin(input_value) is output_value\n\n\ndef test_all_identical():\n    a, b = object(), object()\n    c = [b]\n    assert all_identical([a, b], [a, b]) is True\n    assert all_identical([a, b], [a, b]) is True\n    assert all_identical([a, b, b], [a, b, b]) is True\n    assert all_identical([a, c, b], [a, c, b]) is True\n\n    assert all_identical([], [a]) is False, 'Expected iterables with different lengths to evaluate to `False`'\n    assert all_identical([a], []) is False, 'Expected iterables with different lengths to evaluate to `False`'\n    assert (\n        all_identical([a, [b], b], [a, [b], b]) is False\n    ), 'New list objects are different objects and should therefore not be identical.'\n\n\ndef test_undefined_pickle():\n    undefined2 = pickle.loads(pickle.dumps(PydanticUndefined))\n    assert undefined2 is PydanticUndefined\n\n\ndef test_on_lower_camel_zero_length():\n    assert to_camel('') == ''\n\n\ndef test_on_lower_camel_one_length():\n    assert to_camel('a') == 'a'\n\n\ndef test_on_lower_camel_many_length():\n    assert to_camel('i_like_turtles') == 'iLikeTurtles'\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        ('snake_to_camel', 'snakeToCamel'),\n        ('snake_2_camel', 'snake2Camel'),\n        ('snake2camel', 'snake2Camel'),\n        ('_snake_to_camel', '_snakeToCamel'),\n        ('snake_to_camel_', 'snakeToCamel_'),\n        ('__snake_to_camel__', '__snakeToCamel__'),\n        ('snake_2', 'snake2'),\n        ('_snake_2', '_snake2'),\n        ('snake_2_', 'snake2_'),\n    ],\n)\ndef test_snake2camel_start_lower(value: str, result: str) -> None:\n    assert to_camel(value) == result\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        ('snake_to_camel', 'SnakeToCamel'),\n        ('snake_2_camel', 'Snake2Camel'),\n        ('snake2camel', 'Snake2Camel'),\n        ('_snake_to_camel', '_SnakeToCamel'),\n        ('snake_to_camel_', 'SnakeToCamel_'),\n        ('__snake_to_camel__', '__SnakeToCamel__'),\n        ('snake_2', 'Snake2'),\n        ('_snake_2', '_Snake2'),\n        ('snake_2_', 'Snake2_'),\n    ],\n)\ndef test_snake2pascal(value: str, result: str) -> None:\n    assert to_pascal(value) == result\n\n\n@pytest.mark.parametrize(\n    'value,result',\n    [\n        ('camel_to_snake', 'camel_to_snake'),\n        ('camelToSnake', 'camel_to_snake'),\n        ('camel2Snake', 'camel_2_snake'),\n        ('_camelToSnake', '_camel_to_snake'),\n        ('camelToSnake_', 'camel_to_snake_'),\n        ('__camelToSnake__', '__camel_to_snake__'),\n        ('CamelToSnake', 'camel_to_snake'),\n        ('Camel2Snake', 'camel_2_snake'),\n        ('_CamelToSnake', '_camel_to_snake'),\n        ('CamelToSnake_', 'camel_to_snake_'),\n        ('CAMELToSnake', 'camel_to_snake'),\n        ('__CamelToSnake__', '__camel_to_snake__'),\n        ('Camel2', 'camel_2'),\n        ('Camel2_', 'camel_2_'),\n        ('_Camel2', '_camel_2'),\n        ('camel2', 'camel_2'),\n        ('camel2_', 'camel_2_'),\n        ('_camel2', '_camel_2'),\n        ('kebab-to-snake', 'kebab_to_snake'),\n        ('kebab-Snake', 'kebab_snake'),\n        ('Kebab-Snake', 'kebab_snake'),\n        ('PascalToSnake', 'pascal_to_snake'),\n        ('snake_to_snake', 'snake_to_snake'),\n    ],\n)\ndef test_to_snake(value: str, result: str) -> None:\n    assert to_snake(value) == result\n\n\ndef test_to_camel_from_camel() -> None:\n    assert to_camel('alreadyCamel') == 'alreadyCamel'\n\n\ndef test_handle_tuple_schema():\n    schema = core_schema.tuple_schema([core_schema.float_schema(), core_schema.int_schema()])\n\n    def walk(s, recurse):\n        # change extra_schema['type'] to 'str'\n        if s['type'] == 'float':\n            s['type'] = 'str'\n        return s\n\n    schema = _WalkCoreSchema().handle_tuple_schema(schema, walk)\n    assert schema == {\n        'items_schema': [{'type': 'str'}, {'type': 'int'}],\n        'type': 'tuple',\n    }\n\n\n@pytest.mark.parametrize(\n    'params,expected_extra_schema',\n    (\n        pytest.param({}, {}, id='Model fields without extra_validator'),\n        pytest.param(\n            {'extras_schema': core_schema.float_schema()},\n            {'extras_schema': {'type': 'str'}},\n            id='Model fields with extra_validator',\n        ),\n    ),\n)\ndef test_handle_model_fields_schema(params, expected_extra_schema):\n    schema = core_schema.model_fields_schema(\n        {\n            'foo': core_schema.model_field(core_schema.int_schema()),\n        },\n        **params,\n    )\n\n    def walk(s, recurse):\n        # change extra_schema['type'] to 'str'\n        if s['type'] == 'float':\n            s['type'] = 'str'\n        return s\n\n    schema = _WalkCoreSchema().handle_model_fields_schema(schema, walk)\n    assert schema == {\n        **expected_extra_schema,\n        'type': 'model-fields',\n        'fields': {'foo': {'type': 'model-field', 'schema': {'type': 'int'}}},\n    }\n\n\n@pytest.mark.parametrize(\n    'params,expected_extra_schema',\n    (\n        pytest.param({}, {}, id='Typeddict without extra_validator'),\n        pytest.param(\n            {'extras_schema': core_schema.float_schema()},\n            {'extras_schema': {'type': 'str'}},\n            id='Typeddict with extra_validator',\n        ),\n    ),\n)\ndef test_handle_typed_dict_schema(params, expected_extra_schema):\n    schema = core_schema.typed_dict_schema(\n        {\n            'foo': core_schema.model_field(core_schema.int_schema()),\n        },\n        **params,\n    )\n\n    def walk(s, recurse):\n        # change extra_validator['type'] to 'str'\n        if s['type'] == 'float':\n            s['type'] = 'str'\n        return s\n\n    schema = _WalkCoreSchema().handle_typed_dict_schema(schema, walk)\n    assert schema == {\n        **expected_extra_schema,\n        'type': 'typed-dict',\n        'fields': {'foo': {'type': 'model-field', 'schema': {'type': 'int'}}},\n    }\n\n\ndef test_handle_function_schema():\n    schema = core_schema.with_info_before_validator_function(\n        lambda v, _info: v, core_schema.float_schema(), field_name='field_name'\n    )\n\n    def walk(s, recurse):\n        # change type to str\n        if s['type'] == 'float':\n            s['type'] = 'str'\n        return s\n\n    schema = _WalkCoreSchema().handle_function_schema(schema, walk)\n    assert schema['type'] == 'function-before'\n    assert schema['schema'] == {'type': 'str'}\n\n    def walk1(s, recurse):\n        # this is here to make sure this function is not called\n        assert False\n\n    schema = _WalkCoreSchema().handle_function_schema(core_schema.int_schema(), walk1)\n    assert schema['type'] == 'int'\n\n\ndef test_handle_call_schema():\n    param_a = core_schema.arguments_parameter(name='a', schema=core_schema.str_schema(), mode='positional_only')\n    args_schema = core_schema.arguments_schema([param_a])\n\n    schema = core_schema.call_schema(\n        arguments=args_schema,\n        function=lambda a: int(a),\n        return_schema=core_schema.str_schema(),\n    )\n\n    def walk(s, recurse):\n        # change return schema\n        if 'return_schema' in schema:\n            schema['return_schema']['type'] = 'int'\n        return s\n\n    schema = _WalkCoreSchema().handle_call_schema(schema, walk)\n    assert schema['return_schema'] == {'type': 'int'}\n\n\nclass TestModel:\n    __slots__ = (\n        '__dict__',\n        '__pydantic_fields_set__',\n        '__pydantic_extra__',\n        '__pydantic_private__',\n    )\n\n\n@pytest.mark.parametrize(\n    'include_metadata, schema, expected',\n    [\n        # including metadata with a simple any schema\n        (\n            True,\n            core_schema.AnySchema(\n                type='any',\n                ref='meta_schema',\n                metadata={'schema_type': 'any', 'test_id': '42'},\n                serialization=core_schema.simple_ser_schema('bool'),\n            ),\n            {\n                'type': 'any',\n                'ref': 'meta_schema',\n                'metadata': {'schema_type': 'any', 'test_id': '42'},\n                'serialization': {'type': 'bool'},\n            },\n        ),\n        # excluding metadata with a model_fields_schema\n        (\n            False,\n            core_schema.model_fields_schema(\n                ref='meta_schema',\n                metadata={'schema_type': 'model', 'test_id': '43'},\n                computed_fields=[\n                    core_schema.computed_field(\n                        property_name='TestModel',\n                        return_schema=core_schema.model_fields_schema(\n                            fields={'a': core_schema.model_field(core_schema.str_schema())},\n                        ),\n                        alias='comp_field_1',\n                        metadata={'comp_field_key': 'comp_field_data'},\n                    )\n                ],\n                fields={'a': core_schema.model_field(core_schema.str_schema())},\n            ),\n            {\n                'type': 'model-fields',\n                'fields': {'a': {'type': 'model-field', 'schema': {'type': 'str'}}},\n                'computed_fields': [\n                    {\n                        'type': 'computed-field',\n                        'property_name': 'TestModel',\n                        'return_schema': {\n                            'type': 'model-fields',\n                            'fields': {'a': {'type': 'model-field', 'schema': {'type': 'str'}}},\n                        },\n                        'alias': 'comp_field_1',\n                        'metadata': {'comp_field_key': 'comp_field_data'},\n                    }\n                ],\n                'ref': 'meta_schema',\n            },\n        ),\n        # exclude metadata with a model_schema\n        (\n            False,\n            core_schema.model_schema(\n                ref='meta_schema',\n                metadata={'schema_type': 'model', 'test_id': '43'},\n                custom_init=False,\n                root_model=False,\n                cls=TestModel,\n                config=core_schema.CoreConfig(str_max_length=5),\n                schema=core_schema.model_fields_schema(\n                    fields={'a': core_schema.model_field(core_schema.str_schema())},\n                ),\n            ),\n            {\n                'type': 'model',\n                'schema': {'type': 'model-fields', 'fields': {'a': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n                'config': {'str_max_length': 5},\n                'ref': 'meta_schema',\n            },\n        ),\n    ],\n)\ndef test_pretty_print(include_metadata, schema, expected, capfd, monkeypatch):\n    \"\"\"Verify basic functionality of pretty_print_core_schema, which is used as a utility for debugging.\n\n    Given varied output, this test verifies that the content of the output is as expected,\n    Rather than doing robust formatting testing.\n    \"\"\"\n    # This can break the test by adding color to the output streams\n    monkeypatch.delenv('FORCE_COLOR', raising=False)\n\n    pretty_print_core_schema(schema=schema, include_metadata=include_metadata)\n    content = capfd.readouterr()\n    # Remove cls due to string formatting (for case 3 above)\n    cls_substring = \"'cls': <class 'tests.test_utils.TestModel'>,\"\n    new_content_out = content.out.replace(cls_substring, '')\n    content_as_json = json.loads(new_content_out.replace(\"'\", '\"'))\n    assert content_as_json == expected\n", "tests/test_json.py": "import json\nimport re\nimport sys\nfrom dataclasses import dataclass as vanilla_dataclass\nfrom datetime import date, datetime, time, timedelta, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom pathlib import Path\nfrom typing import Any, Generator, List, Optional, Pattern, Union\nfrom uuid import UUID\n\nimport pytest\nfrom pydantic_core import CoreSchema, SchemaSerializer, core_schema\nfrom typing_extensions import Annotated\n\nfrom pydantic import (\n    AfterValidator,\n    BaseModel,\n    ConfigDict,\n    GetCoreSchemaHandler,\n    GetJsonSchemaHandler,\n    NameEmail,\n    PlainSerializer,\n)\nfrom pydantic._internal._config import ConfigWrapper\nfrom pydantic._internal._generate_schema import GenerateSchema\nfrom pydantic.color import Color\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\nfrom pydantic.deprecated.json import pydantic_encoder, timedelta_isoformat\nfrom pydantic.functional_serializers import (\n    field_serializer,\n)\nfrom pydantic.json_schema import JsonSchemaValue\nfrom pydantic.type_adapter import TypeAdapter\nfrom pydantic.types import DirectoryPath, FilePath, SecretBytes, SecretStr, condecimal\n\ntry:\n    import email_validator\nexcept ImportError:\n    email_validator = None\n\n\npytestmark = pytest.mark.filterwarnings('ignore::DeprecationWarning')\n\n\nclass MyEnum(Enum):\n    foo = 'bar'\n    snap = 'crackle'\n\n\nclass MyModel(BaseModel):\n    a: str = 'b'\n    c: str = 'd'\n\n\n@pytest.mark.parametrize(\n    'ser_type,gen_value,json_output',\n    [\n        (UUID, lambda: UUID('ebcdab58-6eb8-46fb-a190-d07a33e9eac8'), b'\"ebcdab58-6eb8-46fb-a190-d07a33e9eac8\"'),\n        (IPv4Address, lambda: '192.168.0.1', b'\"192.168.0.1\"'),\n        (Color, lambda: Color('#000'), b'\"black\"'),\n        (Color, lambda: Color((1, 12, 123)), b'\"#010c7b\"'),\n        (SecretStr, lambda: SecretStr('abcd'), b'\"**********\"'),\n        (SecretStr, lambda: SecretStr(''), b'\"\"'),\n        (SecretBytes, lambda: SecretBytes(b'xyz'), b'\"**********\"'),\n        (SecretBytes, lambda: SecretBytes(b''), b'\"\"'),\n        (IPv6Address, lambda: IPv6Address('::1:0:1'), b'\"::1:0:1\"'),\n        (IPv4Interface, lambda: IPv4Interface('192.168.0.0/24'), b'\"192.168.0.0/24\"'),\n        (IPv6Interface, lambda: IPv6Interface('2001:db00::/120'), b'\"2001:db00::/120\"'),\n        (IPv4Network, lambda: IPv4Network('192.168.0.0/24'), b'\"192.168.0.0/24\"'),\n        (IPv6Network, lambda: IPv6Network('2001:db00::/120'), b'\"2001:db00::/120\"'),\n        (datetime, lambda: datetime(2032, 1, 1, 1, 1), b'\"2032-01-01T01:01:00\"'),\n        (datetime, lambda: datetime(2032, 1, 1, 1, 1, tzinfo=timezone.utc), b'\"2032-01-01T01:01:00Z\"'),\n        (datetime, lambda: datetime(2032, 1, 1), b'\"2032-01-01T00:00:00\"'),\n        (time, lambda: time(12, 34, 56), b'\"12:34:56\"'),\n        (timedelta, lambda: timedelta(days=12, seconds=34, microseconds=56), b'\"P12DT34.000056S\"'),\n        (timedelta, lambda: timedelta(seconds=-1), b'\"-PT1S\"'),\n        (set, lambda: {1, 2, 3}, b'[1,2,3]'),\n        (frozenset, lambda: frozenset([1, 2, 3]), b'[1,2,3]'),\n        (Generator[int, None, None], lambda: (v for v in range(4)), b'[0,1,2,3]'),\n        (bytes, lambda: b'this is bytes', b'\"this is bytes\"'),\n        (Decimal, lambda: Decimal('12.34'), b'\"12.34\"'),\n        (MyModel, lambda: MyModel(), b'{\"a\":\"b\",\"c\":\"d\"}'),\n        (MyEnum, lambda: MyEnum.foo, b'\"bar\"'),\n        (Pattern, lambda: re.compile('^regex$'), b'\"^regex$\"'),\n    ],\n)\ndef test_json_serialization(ser_type, gen_value, json_output):\n    ta: TypeAdapter[Any] = TypeAdapter(ser_type)\n    assert ta.dump_json(gen_value()) == json_output\n\n\n@pytest.mark.skipif(not email_validator, reason='email_validator not installed')\ndef test_json_serialization_email():\n    config_wrapper = ConfigWrapper({'arbitrary_types_allowed': False})\n    gen = GenerateSchema(config_wrapper, None)\n    schema = gen.generate_schema(NameEmail)\n    serializer = SchemaSerializer(schema)\n    assert serializer.to_json(NameEmail('foo bar', 'foobaR@example.com')) == b'\"foo bar <foobaR@example.com>\"'\n\n\n@pytest.mark.skipif(sys.platform.startswith('win'), reason='paths look different on windows')\ndef test_path_encoding(tmpdir):\n    class PathModel(BaseModel):\n        path: Path\n        file_path: FilePath\n        dir_path: DirectoryPath\n\n    tmpdir = Path(tmpdir)\n    file_path = tmpdir / 'bar'\n    file_path.touch()\n    dir_path = tmpdir / 'baz'\n    dir_path.mkdir()\n    model = PathModel(path=Path('/path/test/example/'), file_path=file_path, dir_path=dir_path)\n    expected = f'{{\"path\": \"/path/test/example\", \"file_path\": \"{file_path}\", \"dir_path\": \"{dir_path}\"}}'\n    assert json.dumps(model, default=pydantic_encoder) == expected\n\n\ndef test_model_encoding():\n    class ModelA(BaseModel):\n        x: int\n        y: str\n\n    class Model(BaseModel):\n        a: float\n        b: bytes\n        c: Decimal\n        d: ModelA\n\n    m = Model(a=10.2, b='foobar', c='10.2', d={'x': 123, 'y': '123'})\n    assert m.model_dump() == {'a': 10.2, 'b': b'foobar', 'c': Decimal('10.2'), 'd': {'x': 123, 'y': '123'}}\n    assert m.model_dump_json() == '{\"a\":10.2,\"b\":\"foobar\",\"c\":\"10.2\",\"d\":{\"x\":123,\"y\":\"123\"}}'\n    assert m.model_dump_json(exclude={'b'}) == '{\"a\":10.2,\"c\":\"10.2\",\"d\":{\"x\":123,\"y\":\"123\"}}'\n\n\ndef test_subclass_encoding():\n    class SubDate(datetime):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            def val(v: datetime) -> SubDate:\n                return SubDate.fromtimestamp(v.timestamp())\n\n            return core_schema.no_info_after_validator_function(val, handler(datetime))\n\n    class Model(BaseModel):\n        a: datetime\n        b: SubDate\n\n    m = Model(a=datetime(2032, 1, 1, 1, 1), b=SubDate(2020, 2, 29, 12, 30))\n    assert m.model_dump() == {'a': datetime(2032, 1, 1, 1, 1), 'b': SubDate(2020, 2, 29, 12, 30)}\n    assert m.model_dump_json() == '{\"a\":\"2032-01-01T01:01:00\",\"b\":\"2020-02-29T12:30:00\"}'\n\n\ndef test_subclass_custom_encoding():\n    class SubDt(datetime):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            def val(v: datetime) -> SubDt:\n                return SubDt.fromtimestamp(v.timestamp())\n\n            return core_schema.no_info_after_validator_function(val, handler(datetime))\n\n    class SubDelta(timedelta):\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n            def val(v: timedelta) -> SubDelta:\n                return cls(seconds=v.total_seconds())\n\n            return core_schema.no_info_after_validator_function(val, handler(timedelta))\n\n    class Model(BaseModel):\n        a: SubDt\n        b: SubDelta\n\n        @field_serializer('a', when_used='json')\n        def serialize_a(self, v: SubDt, _info):\n            return v.strftime('%a, %d %b %C %H:%M:%S')\n\n        model_config = ConfigDict(ser_json_timedelta='float')\n\n    m = Model(a=SubDt(2032, 1, 1, 1, 1), b=SubDelta(hours=100))\n    assert m.model_dump() == {'a': SubDt(2032, 1, 1, 1, 1), 'b': SubDelta(days=4, seconds=14400)}\n    assert m.model_dump(mode='json') == {'a': 'Thu, 01 Jan 20 01:01:00', 'b': 360000.0}\n    assert m.model_dump_json() == '{\"a\":\"Thu, 01 Jan 20 01:01:00\",\"b\":360000.0}'\n\n\ndef test_invalid_model():\n    class Foo:\n        pass\n\n    with pytest.raises(TypeError):\n        json.dumps(Foo, default=pydantic_encoder)\n\n\n@pytest.mark.parametrize(\n    'input,output',\n    [\n        (timedelta(days=12, seconds=34, microseconds=56), 'P12DT0H0M34.000056S'),\n        (timedelta(days=1001, hours=1, minutes=2, seconds=3, microseconds=654_321), 'P1001DT1H2M3.654321S'),\n        (timedelta(seconds=-1), '-P1DT23H59M59.000000S'),\n        (timedelta(), 'P0DT0H0M0.000000S'),\n    ],\n)\ndef test_iso_timedelta(input, output):\n    assert output == timedelta_isoformat(input)\n\n\ndef test_custom_encoder():\n    class Model(BaseModel):\n        x: timedelta\n        y: Decimal\n        z: date\n\n        @field_serializer('x')\n        def serialize_x(self, v: timedelta, _info):\n            return f'{v.total_seconds():0.3f}s'\n\n        @field_serializer('y')\n        def serialize_y(self, v: Decimal, _info):\n            return 'a decimal'\n\n    assert Model(x=123, y=5, z='2032-06-01').model_dump_json() == '{\"x\":\"123.000s\",\"y\":\"a decimal\",\"z\":\"2032-06-01\"}'\n\n\ndef test_iso_timedelta_simple():\n    class Model(BaseModel):\n        x: timedelta\n\n    m = Model(x=123)\n    json_data = m.model_dump_json()\n    assert json_data == '{\"x\":\"PT2M3S\"}'\n    assert Model.model_validate_json(json_data).x == timedelta(seconds=123)\n\n\ndef test_con_decimal_encode() -> None:\n    \"\"\"\n    Makes sure a decimal with decimal_places = 0, as well as one with places\n    can handle a encode/decode roundtrip.\n    \"\"\"\n\n    class Obj(BaseModel):\n        id: condecimal(gt=0, max_digits=22, decimal_places=0)\n        price: Decimal = Decimal('0.01')\n\n    json_data = '{\"id\":\"1\",\"price\":\"0.01\"}'\n    assert Obj(id=1).model_dump_json() == json_data\n    assert Obj.model_validate_json(json_data) == Obj(id=1)\n\n\ndef test_json_encoder_simple_inheritance():\n    class Parent(BaseModel):\n        dt: datetime = datetime.now()\n        timedt: timedelta = timedelta(hours=100)\n\n        @field_serializer('dt')\n        def serialize_dt(self, _v: datetime, _info):\n            return 'parent_encoder'\n\n    class Child(Parent):\n        @field_serializer('timedt')\n        def serialize_timedt(self, _v: timedelta, _info):\n            return 'child_encoder'\n\n    assert Child().model_dump_json() == '{\"dt\":\"parent_encoder\",\"timedt\":\"child_encoder\"}'\n\n\ndef test_encode_dataclass():\n    @vanilla_dataclass\n    class Foo:\n        bar: int\n        spam: str\n\n    f = Foo(bar=123, spam='apple pie')\n    assert '{\"bar\": 123, \"spam\": \"apple pie\"}' == json.dumps(f, default=pydantic_encoder)\n\n\ndef test_encode_pydantic_dataclass():\n    @pydantic_dataclass\n    class Foo:\n        bar: int\n        spam: str\n\n    f = Foo(bar=123, spam='apple pie')\n    assert json.dumps(f, default=pydantic_encoder) == '{\"bar\": 123, \"spam\": \"apple pie\"}'\n\n\ndef test_json_nested_encode_models():\n    class Phone(BaseModel):\n        manufacturer: str\n        number: int\n\n    class User(BaseModel):\n        name: str\n        SSN: int\n        birthday: datetime\n        phone: Phone\n        friend: Optional['User'] = None\n\n        @field_serializer('birthday')\n        def serialize_birthday(self, v: datetime, _info):\n            return v.timestamp()\n\n        @field_serializer('phone', when_used='unless-none')\n        def serialize_phone(self, v: Phone, _info):\n            return v.number\n\n        @field_serializer('friend', when_used='unless-none')\n        def serialize_user(self, v, _info):\n            return v.SSN\n\n    User.model_rebuild()\n\n    iphone = Phone(manufacturer='Apple', number=18002752273)\n    galaxy = Phone(manufacturer='Samsung', number=18007267864)\n\n    timon = User(name='Timon', SSN=123, birthday=datetime(1993, 6, 1, tzinfo=timezone.utc), phone=iphone)\n    pumbaa = User(name='Pumbaa', SSN=234, birthday=datetime(1993, 5, 15, tzinfo=timezone.utc), phone=galaxy)\n\n    timon.friend = pumbaa\n\n    assert iphone.model_dump_json() == '{\"manufacturer\":\"Apple\",\"number\":18002752273}'\n    assert (\n        pumbaa.model_dump_json()\n        == '{\"name\":\"Pumbaa\",\"SSN\":234,\"birthday\":737424000.0,\"phone\":18007267864,\"friend\":null}'\n    )\n    assert (\n        timon.model_dump_json() == '{\"name\":\"Timon\",\"SSN\":123,\"birthday\":738892800.0,\"phone\":18002752273,\"friend\":234}'\n    )\n\n\ndef test_custom_encode_fallback_basemodel():\n    class MyExoticType:\n        pass\n\n    class Foo(BaseModel):\n        x: MyExoticType\n\n        @field_serializer('x')\n        def serialize_x(self, _v: MyExoticType, _info):\n            return 'exo'\n\n        model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    class Bar(BaseModel):\n        foo: Foo\n\n    assert Bar(foo=Foo(x=MyExoticType())).model_dump_json() == '{\"foo\":{\"x\":\"exo\"}}'\n\n\ndef test_recursive(create_module):\n    module = create_module(\n        # language=Python\n        \"\"\"\nfrom __future__ import annotations\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass Model(BaseModel):\n    value: int\n    nested: Optional[Model] = None\n\"\"\"\n    )\n    M = module.Model\n\n    assert M(value=1, nested=M(value=2)).model_dump_json(exclude_none=True) == '{\"value\":1,\"nested\":{\"value\":2}}'\n\n\ndef test_resolve_ref_schema_recursive_model():\n    class Model(BaseModel):\n        mini_me: Union['Model', None]\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            json_schema = super().__get_pydantic_json_schema__(core_schema, handler)\n            json_schema = handler.resolve_ref_schema(json_schema)\n            json_schema['examples'] = {'foo': {'mini_me': None}}\n            return json_schema\n\n    # insert_assert(Model.model_json_schema())\n    assert Model.model_json_schema() == {\n        '$defs': {\n            'Model': {\n                'examples': {'foo': {'mini_me': None}},\n                'properties': {'mini_me': {'anyOf': [{'$ref': '#/$defs/Model'}, {'type': 'null'}]}},\n                'required': ['mini_me'],\n                'title': 'Model',\n                'type': 'object',\n            }\n        },\n        'allOf': [{'$ref': '#/$defs/Model'}],\n    }\n\n\ndef test_custom_json_encoder_config():\n    class Model(BaseModel):\n        x: timedelta\n        y: Decimal\n        z: date\n\n        model_config = ConfigDict(\n            json_encoders={timedelta: lambda v: f'{v.total_seconds():0.3f}s', Decimal: lambda v: 'a decimal'}\n        )\n\n    assert json.loads(Model(x=123, y=5, z='2032-06-01').model_dump_json()) == {\n        'x': '123.000s',\n        'y': 'a decimal',\n        'z': '2032-06-01',\n    }\n\n\ndef test_custom_iso_timedelta():\n    class Model(BaseModel):\n        x: timedelta\n        model_config = ConfigDict(json_encoders={timedelta: lambda _: 'P0DT0H2M3.000000S'})\n\n    m = Model(x=321)\n    assert json.loads(m.model_dump_json()) == {'x': 'P0DT0H2M3.000000S'}\n\n\ndef test_json_encoders_config_simple_inheritance():\n    \"\"\"json_encoders is not \"inheritable\", this is different than v1 but much simpler\"\"\"\n\n    class Parent(BaseModel):\n        dt: datetime = datetime.now()\n        timedt: timedelta = timedelta(hours=100)\n\n        model_config = ConfigDict(json_encoders={timedelta: lambda _: 'parent_encoder'})\n\n    class Child(Parent):\n        model_config = ConfigDict(json_encoders={datetime: lambda _: 'child_encoder'})\n\n    # insert_assert(Child().model_dump())\n    assert json.loads(Child().model_dump_json()) == {'dt': 'child_encoder', 'timedt': 'P4DT4H'}\n\n\ndef test_custom_iso_timedelta_annotated():\n    class Model(BaseModel):\n        # the json_encoders config applies to the type but the annotation overrides it\n        y: timedelta\n        x: Annotated[timedelta, AfterValidator(lambda x: x), PlainSerializer(lambda _: 'P0DT0H1M2.000000S')]\n        model_config = ConfigDict(json_encoders={timedelta: lambda _: 'P0DT0H2M3.000000S'})\n\n    m = Model(x=321, y=456)\n    assert json.loads(m.model_dump_json()) == {'x': 'P0DT0H1M2.000000S', 'y': 'P0DT0H2M3.000000S'}\n\n\ndef test_json_encoders_on_model() -> None:\n    \"\"\"Make sure that applying json_encoders to a BaseModel\n    does not edit its schema in place.\n    \"\"\"\n\n    class Model(BaseModel):\n        x: int\n\n    class Outer1(BaseModel):\n        m: Model\n        model_config = ConfigDict(json_encoders={Model: lambda x: 'encoded!'})\n\n    class Outer2(BaseModel):\n        m: Model\n\n    class Outermost(BaseModel):\n        inner: Union[Outer1, Outer2]\n\n    m = Outermost(inner=Outer1(m=Model(x=1)))\n    # insert_assert(m.model_dump())\n    assert json.loads(m.model_dump_json()) == {'inner': {'m': 'encoded!'}}\n\n    m = Outermost(inner=Outer2(m=Model(x=1)))\n    # insert_assert(m.model_dump())\n    assert json.loads(m.model_dump_json()) == {'inner': {'m': {'x': 1}}}\n\n\ndef test_json_encoders_not_used_for_python_dumps() -> None:\n    class Model(BaseModel):\n        x: int\n        model_config = ConfigDict(json_encoders={int: lambda x: 'encoded!'})\n\n    m = Model(x=1)\n    assert m.model_dump() == {'x': 1}\n    assert m.model_dump_json() == '{\"x\":\"encoded!\"}'\n\n\ndef test_json_encoders_types() -> None:\n    class MyEnum(Enum):\n        A = 'a'\n        B = 'b'\n\n    class A(BaseModel):\n        a: MyEnum\n        b: List[int]\n        c: Decimal\n        model_config = ConfigDict(\n            json_encoders={Enum: lambda val: val.name, List[int]: lambda val: 'list!', Decimal: lambda val: 'decimal!'}\n        )\n\n    m = A(a=MyEnum.A, b=[1, 2, 3], c=Decimal('0'))\n    assert m.model_dump_json() == '{\"a\":\"A\",\"b\":\"list!\",\"c\":\"decimal!\"}'\n    assert m.model_dump() == {'a': MyEnum.A, 'b': [1, 2, 3], 'c': Decimal('0')}\n", "tests/test_types_payment_card_number.py": "from collections import namedtuple\nfrom typing import Any\n\nimport pytest\nfrom pydantic_core import PydanticCustomError\n\nfrom pydantic import BaseModel, ValidationError\nfrom pydantic.types import PaymentCardBrand, PaymentCardNumber\n\npytestmark = pytest.mark.filterwarnings(\n    'ignore:' 'The `PaymentCardNumber` class is deprecated, use `pydantic_extra_types` instead.*' ':DeprecationWarning'\n)\n\n\nVALID_AMEX = '370000000000002'\nVALID_MC = '5100000000000003'\nVALID_VISA_13 = '4050000000001'\nVALID_VISA_16 = '4050000000000001'\nVALID_VISA_19 = '4050000000000000001'\nVALID_OTHER = '2000000000000000008'\nLUHN_INVALID = '4000000000000000'\nLEN_INVALID = '40000000000000006'\n\n\n# Mock PaymentCardNumber\nPCN = namedtuple('PaymentCardNumber', ['card_number', 'brand'])\nPCN.__len__ = lambda v: len(v.card_number)\n\n\n@pytest.fixture(scope='session', name='PaymentCard')\ndef payment_card_model_fixture():\n    class PaymentCard(BaseModel):\n        card_number: PaymentCardNumber\n\n    return PaymentCard\n\n\ndef test_validate_digits():\n    digits = '12345'\n    assert PaymentCardNumber.validate_digits(digits) is None\n    with pytest.raises(PydanticCustomError, match='Card number is not all digits'):\n        PaymentCardNumber.validate_digits('hello')\n\n\n@pytest.mark.parametrize(\n    'card_number, valid',\n    [\n        ('0', True),\n        ('00', True),\n        ('18', True),\n        ('0000000000000000', True),\n        ('4242424242424240', False),\n        ('4242424242424241', False),\n        ('4242424242424242', True),\n        ('4242424242424243', False),\n        ('4242424242424244', False),\n        ('4242424242424245', False),\n        ('4242424242424246', False),\n        ('4242424242424247', False),\n        ('4242424242424248', False),\n        ('4242424242424249', False),\n        ('42424242424242426', True),\n        ('424242424242424267', True),\n        ('4242424242424242675', True),\n        ('5164581347216566', True),\n        ('4345351087414150', True),\n        ('343728738009846', True),\n        ('5164581347216567', False),\n        ('4345351087414151', False),\n        ('343728738009847', False),\n        ('000000018', True),\n        ('99999999999999999999', True),\n        ('99999999999999999999999999999999999999999999999999999999999999999997', True),\n    ],\n)\ndef test_validate_luhn_check_digit(card_number: str, valid: bool):\n    if valid:\n        assert PaymentCardNumber.validate_luhn_check_digit(card_number) == card_number\n    else:\n        with pytest.raises(PydanticCustomError, match='Card number is not luhn valid'):\n            PaymentCardNumber.validate_luhn_check_digit(card_number)\n\n\n@pytest.mark.parametrize(\n    'card_number, brand, valid',\n    [\n        (VALID_VISA_13, PaymentCardBrand.visa, True),\n        (VALID_VISA_16, PaymentCardBrand.visa, True),\n        (VALID_VISA_19, PaymentCardBrand.visa, True),\n        (VALID_MC, PaymentCardBrand.mastercard, True),\n        (VALID_AMEX, PaymentCardBrand.amex, True),\n        (VALID_OTHER, PaymentCardBrand.other, True),\n        (LEN_INVALID, PaymentCardBrand.visa, False),\n    ],\n)\ndef test_length_for_brand(card_number: str, brand: PaymentCardBrand, valid: bool):\n    # pcn = PCN(card_number, brand)\n    if valid:\n        assert PaymentCardNumber.validate_brand(card_number) == brand\n    else:\n        with pytest.raises(PydanticCustomError) as exc_info:\n            PaymentCardNumber.validate_brand(card_number)\n        assert exc_info.value.type == 'payment_card_number_brand'\n\n\n@pytest.mark.parametrize(\n    'card_number, brand',\n    [\n        (VALID_AMEX, PaymentCardBrand.amex),\n        (VALID_MC, PaymentCardBrand.mastercard),\n        (VALID_VISA_16, PaymentCardBrand.visa),\n        (VALID_OTHER, PaymentCardBrand.other),\n    ],\n)\ndef test_get_brand(card_number: str, brand: PaymentCardBrand):\n    assert PaymentCardNumber.validate_brand(card_number) == brand\n\n\ndef test_valid(PaymentCard):\n    card = PaymentCard(card_number=VALID_VISA_16)\n    assert str(card.card_number) == VALID_VISA_16\n    assert card.card_number.masked == '405000******0001'\n\n\n@pytest.mark.parametrize(\n    'card_number, error_message',\n    [\n        (None, 'type=string_type'),\n        ('1' * 11, 'type=string_too_short,'),\n        ('1' * 20, 'type=string_too_long,'),\n        ('h' * 16, 'type=payment_card_number_digits'),\n        (LUHN_INVALID, 'type=payment_card_number_luhn,'),\n        (LEN_INVALID, 'type=payment_card_number_brand,'),\n    ],\n)\ndef test_error_types(card_number: Any, error_message: str, PaymentCard):\n    with pytest.raises(ValidationError, match=error_message):\n        PaymentCard(card_number=card_number)\n\n\ndef test_payment_card_brand():\n    b = PaymentCardBrand.visa\n    assert str(b) == 'Visa'\n    assert b is PaymentCardBrand.visa\n    assert b == PaymentCardBrand.visa\n    assert b in {PaymentCardBrand.visa, PaymentCardBrand.mastercard}\n\n    b = 'Visa'\n    assert b is not PaymentCardBrand.visa\n    assert b == PaymentCardBrand.visa\n    assert b in {PaymentCardBrand.visa, PaymentCardBrand.mastercard}\n\n    b = PaymentCardBrand.amex\n    assert b is not PaymentCardBrand.visa\n    assert b != PaymentCardBrand.visa\n    assert b not in {PaymentCardBrand.visa, PaymentCardBrand.mastercard}\n", "tests/mypy/test_mypy.py": "import dataclasses\nimport importlib\nimport os\nimport re\nimport sys\nfrom bisect import insort\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Tuple, Union\n\nimport pytest\n\ntry:\n    from mypy import api as mypy_api\n    from mypy.version import __version__ as mypy_version\n\n    from pydantic.version import parse_mypy_version\n\nexcept ImportError:\n    mypy_api = None\n    mypy_version = None\n\n    parse_mypy_version = lambda _: (0,)  # noqa: E731\n\n\nMYPY_VERSION_TUPLE = parse_mypy_version(mypy_version)\nPYDANTIC_ROOT = Path(__file__).parent.parent.parent\n\npytestmark = pytest.mark.skipif(\n    '--test-mypy' not in sys.argv\n    and os.environ.get('PYCHARM_HOSTED') != '1',  # never skip when running via the PyCharm runner\n    reason='Test only with \"--test-mypy\" flag',\n)\n\n# This ensures mypy can find the test files, no matter where tests are run from:\nos.chdir(Path(__file__).parent.parent.parent)\n\n\n@dataclasses.dataclass\nclass MypyCasesBuilder:\n    configs: Union[str, List[str]]\n    modules: Union[str, List[str]]\n    marks: Any = None\n\n    def build(self) -> List[Union[Tuple[str, str], Any]]:\n        \"\"\"\n        Produces the cartesian product of the configs and modules, optionally with marks.\n        \"\"\"\n        if isinstance(self.configs, str):\n            self.configs = [self.configs]\n        if isinstance(self.modules, str):\n            self.modules = [self.modules]\n        built_cases = []\n        for config in self.configs:\n            for module in self.modules:\n                built_cases.append((config, module))\n        if self.marks is not None:\n            built_cases = [pytest.param(config, module, marks=self.marks) for config, module in built_cases]\n        return built_cases\n\n\ncases = (\n    # No plugin\n    MypyCasesBuilder(\n        ['mypy-default.ini', 'pyproject-default.toml'],\n        ['fail1.py', 'fail2.py', 'fail3.py', 'fail4.py', 'pydantic_settings.py'],\n    ).build()\n    + MypyCasesBuilder(\n        ['mypy-default.ini', 'pyproject-default.toml'],\n        'success.py',\n        pytest.mark.skipif(MYPY_VERSION_TUPLE > (1, 0, 1), reason='Need to handle some more things for mypy >=1.1.1'),\n    ).build()\n    + MypyCasesBuilder(\n        ['mypy-default.ini', 'pyproject-default.toml'],\n        'root_models.py',\n        pytest.mark.skipif(\n            MYPY_VERSION_TUPLE < (1, 1, 1), reason='`dataclass_transform` only supported on mypy >= 1.1.1'\n        ),\n    ).build()\n    + MypyCasesBuilder(\n        'mypy-default.ini', ['plugin_success.py', 'plugin_success_baseConfig.py', 'metaclass_args.py']\n    ).build()\n    # Default plugin config\n    + MypyCasesBuilder(\n        ['mypy-plugin.ini', 'pyproject-plugin.toml'],\n        [\n            'plugin_success.py',\n            'plugin_fail.py',\n            'plugin_success_baseConfig.py',\n            'plugin_fail_baseConfig.py',\n            'pydantic_settings.py',\n        ],\n    ).build()\n    # Strict plugin config\n    + MypyCasesBuilder(\n        ['mypy-plugin-strict.ini', 'pyproject-plugin-strict.toml'],\n        [\n            'plugin_success.py',\n            'plugin_fail.py',\n            'fail_defaults.py',\n            'plugin_success_baseConfig.py',\n            'plugin_fail_baseConfig.py',\n        ],\n    ).build()\n    # One-off cases\n    + [\n        ('mypy-plugin.ini', 'custom_constructor.py'),\n        ('mypy-plugin.ini', 'config_conditional_extra.py'),\n        ('mypy-plugin.ini', 'covariant_typevar.py'),\n        ('mypy-plugin.ini', 'plugin_optional_inheritance.py'),\n        ('mypy-plugin.ini', 'generics.py'),\n        ('mypy-plugin.ini', 'root_models.py'),\n        ('mypy-plugin-strict.ini', 'plugin_default_factory.py'),\n        ('mypy-plugin-strict-no-any.ini', 'dataclass_no_any.py'),\n        ('mypy-plugin-very-strict.ini', 'metaclass_args.py'),\n        ('pyproject-default.toml', 'computed_fields.py'),\n        ('pyproject-default.toml', 'with_config_decorator.py'),\n        ('pyproject-plugin-no-strict-optional.toml', 'no_strict_optional.py'),\n        ('pyproject-plugin-strict-equality.toml', 'strict_equality.py'),\n    ]\n)\n\n\n@dataclasses.dataclass\nclass MypyTestTarget:\n    parsed_mypy_version: Tuple[int, ...]\n    output_path: Path\n\n\n@dataclasses.dataclass\nclass MypyTestConfig:\n    existing: Optional[MypyTestTarget]  # the oldest target with an output that is no older than the installed mypy\n    current: MypyTestTarget  # the target for the current installed mypy\n\n\ndef get_test_config(module_path: Path, config_path: Path) -> MypyTestConfig:\n    outputs_dir = PYDANTIC_ROOT / 'tests/mypy/outputs'\n    outputs_dir.mkdir(exist_ok=True)\n    existing_versions = [\n        x.name for x in outputs_dir.iterdir() if x.is_dir() and re.match(r'[0-9]+(?:\\.[0-9]+)*', x.name)\n    ]\n\n    def _convert_to_output_path(v: str) -> Path:\n        return outputs_dir / v / config_path.name.replace('.', '_') / module_path.name\n\n    existing = None\n\n    # Build sorted list of (parsed_version, version) pairs, including the current mypy version being used\n    parsed_version_pairs = sorted([(parse_mypy_version(v), v) for v in existing_versions])\n    if MYPY_VERSION_TUPLE not in [x[0] for x in parsed_version_pairs]:\n        insort(parsed_version_pairs, (MYPY_VERSION_TUPLE, mypy_version))\n\n    for parsed_version, version in parsed_version_pairs[::-1]:\n        if parsed_version > MYPY_VERSION_TUPLE:\n            continue\n        output_path = _convert_to_output_path(version)\n        if output_path.exists():\n            existing = MypyTestTarget(parsed_version, output_path)\n            break\n\n    current = MypyTestTarget(MYPY_VERSION_TUPLE, _convert_to_output_path(mypy_version))\n    return MypyTestConfig(existing, current)\n\n\n@pytest.mark.filterwarnings('ignore:ast.:DeprecationWarning')  # these are produced by mypy in python 3.12\n@pytest.mark.parametrize('config_filename,python_filename', cases)\ndef test_mypy_results(config_filename: str, python_filename: str, request: pytest.FixtureRequest) -> None:\n    input_path = PYDANTIC_ROOT / 'tests/mypy/modules' / python_filename\n    config_path = PYDANTIC_ROOT / 'tests/mypy/configs' / config_filename\n    test_config = get_test_config(input_path, config_path)\n\n    # Specifying a different cache dir for each configuration dramatically speeds up subsequent execution\n    # It also prevents cache-invalidation-related bugs in the tests\n    cache_dir = f'.mypy_cache/test-{os.path.splitext(config_filename)[0]}'\n    command = [\n        str(input_path),\n        '--config-file',\n        str(config_path),\n        '--cache-dir',\n        cache_dir,\n        '--show-error-codes',\n        '--show-traceback',\n    ]\n    print(f\"\\nExecuting: mypy {' '.join(command)}\")  # makes it easier to debug as necessary\n    mypy_out, mypy_err, mypy_returncode = mypy_api.run(command)\n\n    # Need to strip filenames due to differences in formatting by OS\n    mypy_out = '\\n'.join(['.py:'.join(line.split('.py:')[1:]) for line in mypy_out.split('\\n') if line]).strip()\n    mypy_out = re.sub(r'\\n\\s*\\n', r'\\n', mypy_out)\n    if mypy_out:\n        print('{0}\\n{1:^100}\\n{0}\\n{2}\\n{0}'.format('=' * 100, f'mypy {mypy_version} output', mypy_out))\n    assert mypy_err == ''\n\n    input_code = input_path.read_text()\n\n    existing_output_code: Optional[str] = None\n    if test_config.existing is not None:\n        existing_output_code = test_config.existing.output_path.read_text()\n        print(f'Comparing output with {test_config.existing.output_path}')\n    else:\n        print(f'Comparing output with {input_path} (expecting no mypy errors)')\n\n    merged_output = merge_python_and_mypy_output(input_code, mypy_out)\n\n    if merged_output == (existing_output_code or input_code):\n        # Test passed, no changes needed\n        pass\n    elif request.config.getoption('update_mypy'):\n        test_config.current.output_path.parent.mkdir(parents=True, exist_ok=True)\n        test_config.current.output_path.write_text(merged_output)\n    else:\n        print('**** Merged Output ****')\n        print(merged_output)\n        print('***********************')\n        assert existing_output_code is not None, 'No output file found, run `make test-mypy-update` to create it'\n        assert merged_output == existing_output_code\n        expected_returncode = get_expected_return_code(existing_output_code)\n        assert mypy_returncode == expected_returncode\n\n\ndef test_bad_toml_config() -> None:\n    full_config_filename = 'tests/mypy/configs/pyproject-plugin-bad-param.toml'\n    full_filename = 'tests/mypy/modules/success.py'\n\n    # Specifying a different cache dir for each configuration dramatically speeds up subsequent execution\n    # It also prevents cache-invalidation-related bugs in the tests\n    cache_dir = '.mypy_cache/test-pyproject-plugin-bad-param'\n    command = [full_filename, '--config-file', full_config_filename, '--cache-dir', cache_dir, '--show-error-codes']\n    if MYPY_VERSION_TUPLE >= (0, 990):\n        command.append('--disable-recursive-aliases')\n    print(f\"\\nExecuting: mypy {' '.join(command)}\")  # makes it easier to debug as necessary\n    with pytest.raises(ValueError) as e:\n        mypy_api.run(command)\n\n    assert str(e.value) == 'Configuration value must be a boolean for key: init_forbid_extra'\n\n\ndef get_expected_return_code(source_code: str) -> int:\n    if re.findall(r'^\\s*# MYPY:', source_code, flags=re.MULTILINE):\n        return 1\n    return 0\n\n\n@pytest.mark.parametrize('module', ['dataclass_no_any', 'plugin_success', 'plugin_success_baseConfig'])\n@pytest.mark.filterwarnings('ignore:.*is deprecated.*:DeprecationWarning')\n@pytest.mark.filterwarnings('ignore:.*are deprecated.*:DeprecationWarning')\ndef test_success_cases_run(module: str) -> None:\n    \"\"\"\n    Ensure the \"success\" files can actually be executed\n    \"\"\"\n    importlib.import_module(f'tests.mypy.modules.{module}')\n\n\ndef test_explicit_reexports():\n    from pydantic import __all__ as root_all\n    from pydantic.deprecated.tools import __all__ as tools\n    from pydantic.main import __all__ as main\n    from pydantic.networks import __all__ as networks\n    from pydantic.types import __all__ as types\n\n    for name, export_all in [('main', main), ('network', networks), ('tools', tools), ('types', types)]:\n        for export in export_all:\n            assert export in root_all, f'{export} is in {name}.__all__ but missing from re-export in __init__.py'\n\n\ndef test_explicit_reexports_exist():\n    import pydantic\n\n    for name in pydantic.__all__:\n        assert hasattr(pydantic, name), f'{name} is in pydantic.__all__ but missing from pydantic'\n\n\n@pytest.mark.parametrize(\n    'v_str,v_tuple',\n    [\n        ('0', (0,)),\n        ('0.930', (0, 930)),\n        ('0.940+dev.04cac4b5d911c4f9529e6ce86a27b44f28846f5d.dirty', (0, 940)),\n    ],\n)\ndef test_parse_mypy_version(v_str, v_tuple):\n    assert parse_mypy_version(v_str) == v_tuple\n\n\ndef merge_python_and_mypy_output(source_code: str, mypy_output: str) -> str:\n    merged_lines = [(line, False) for line in source_code.splitlines()]\n\n    for line in mypy_output.splitlines()[::-1]:\n        if not line:\n            continue\n        try:\n            line_number, message = re.split(r':(?:\\d+:)?', line, maxsplit=1)\n            merged_lines.insert(int(line_number), (f'# MYPY: {message.strip()}', True))\n        except ValueError:\n            # This could happen due to lack of a ':' in `line`, or the pre-':' contents not being a number\n            # Either way, put all such lines at the top of the file\n            merged_lines.insert(0, (f'# MYPY: {line.strip()}', True))\n\n    merged_lines = [line for line, is_mypy in merged_lines if is_mypy or not line.strip().startswith('# MYPY: ')]\n    return '\\n'.join(merged_lines) + '\\n'\n", "tests/mypy/__init__.py": "", "tests/mypy/outputs/1.0.1/pyproject-default_toml/computed_fields.py": "from pydantic import BaseModel, computed_field\n\n\nclass Square(BaseModel):\n    side: float\n\n    @computed_field\n# MYPY: error: Decorators on top of @property are not supported  [misc]\n    @property\n    def area(self) -> float:\n        return self.side**2\n\n    @area.setter\n    def area(self, area: float) -> None:\n        self.side = area**0.5\n\n\nsq = Square(side=10)\ny = 12.4 + sq.area\nz = 'x' + sq.area\n# MYPY: error: Unsupported operand types for + (\"str\" and \"float\")  [operator]\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n    pass\nelse:\n\n    class Square2(BaseModel):\n        side: float\n\n        @computed_field\n# MYPY: error: Decorators on top of @property are not supported  [misc]\n        @cached_property\n        def area(self) -> float:\n            return self.side**2\n\n    sq = Square(side=10)\n    y = 12.4 + sq.area\n    z = 'x' + sq.area\n# MYPY: error: Unsupported operand types for + (\"str\" and \"float\")  [operator]\n", "tests/mypy/outputs/1.0.1/pyproject-default_toml/fail4.py": "from typing import Any\n\nfrom pydantic import BaseModel, root_validator, validate_call\n\n\n@validate_call\ndef foo(a: int, *, c: str = 'x') -> str:\n# MYPY: note: \"foo\" defined here\n    return c * a\n\n\n# ok\nx: str = foo(1, c='hello')\n# fails\nfoo('x')\n# MYPY: error: Argument 1 to \"foo\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nfoo(1, c=1)\n# MYPY: error: Argument \"c\" to \"foo\" has incompatible type \"int\"; expected \"str\"  [arg-type]\nfoo(1, 2)\n# MYPY: error: Too many positional arguments for \"foo\"  [misc]\n# MYPY: error: Argument 2 to \"foo\" has incompatible type \"int\"; expected \"str\"  [arg-type]\nfoo(1, d=2)\n# MYPY: error: Unexpected keyword argument \"d\" for \"foo\"  [call-arg]\n# mypy assumes foo is just a function\ncallable(foo.raw_function)\n# MYPY: error: \"Callable[[int, DefaultNamedArg(str, 'c')], str]\" has no attribute \"raw_function\"  [attr-defined]\n\n\n@validate_call\ndef bar() -> str:\n    return 'x'\n\n\n# return type should be a string\ny: int = bar()\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n\n\n# Demonstrate type errors for root_validator signatures\nclass Model(BaseModel):\n    @root_validator()\n# MYPY: error: All overload variants of \"root_validator\" require at least one argument  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_1(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=True, skip_on_failure=True)\n# MYPY: error: No overload variant of \"root_validator\" matches argument types \"bool\", \"bool\"  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_2(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=False)\n# MYPY: error: No overload variant of \"root_validator\" matches argument type \"bool\"  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_3(cls, values: Any) -> Any:\n        return values\n\n\nModel.non_existent_attribute\n# MYPY: error: \"Type[Model]\" has no attribute \"non_existent_attribute\"  [attr-defined]\n", "tests/mypy/outputs/1.0.1/pyproject-default_toml/fail3.py": "\"\"\"\nTest mypy failure with invalid types.\n\"\"\"\nfrom typing import Generic, List, TypeVar\n\nfrom pydantic import BaseModel\n\nT = TypeVar('T')\n\n\nclass Model(BaseModel):\n    list_of_ints: List[int]\n\n\nclass WrapperModel(BaseModel, Generic[T]):\n    payload: T\n\n\nmodel_instance = Model(list_of_ints=[1])\nwrapper_instance = WrapperModel[Model](payload=model_instance)\nwrapper_instance.payload.list_of_ints.append('1')\n# MYPY: error: Argument 1 to \"append\" of \"list\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n", "tests/mypy/outputs/1.0.1/pyproject-default_toml/fail1.py": "\"\"\"\nTest mypy failure with missing attribute\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\nfrom pydantic.types import Json\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n    json_list_of_ints: Json[List[int]]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n\nprint(m.age + 'not integer')\n# MYPY: error: Unsupported operand types for + (\"int\" and \"str\")  [operator]\nm.json_list_of_ints[0] + 'not integer'\n# MYPY: error: Unsupported operand types for + (\"int\" and \"str\")  [operator]\n", "tests/mypy/outputs/1.0.1/pyproject-default_toml/pydantic_settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    foo: str\n\n\ns = Settings()\n\ns = Settings(foo='test', _case_sensitive=True, _env_prefix='test__', _env_file='test')\n\ns = Settings(foo='test', _case_sensitive=1, _env_prefix=2, _env_file=3)\n# MYPY: error: Argument \"_case_sensitive\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[bool]\"  [arg-type]\n# MYPY: error: Argument \"_env_prefix\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[str]\"  [arg-type]\n# MYPY: error: Argument \"_env_file\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[Union[Path, str, List[Union[Path, str]], Tuple[Union[Path, str], ...]]]\"  [arg-type]\n\n\nclass SettingsWithConfigDict(BaseSettings):\n    bar: str\n\n    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')\n\n\nscd = SettingsWithConfigDict()\n", "tests/mypy/outputs/1.0.1/pyproject-default_toml/success.py": "\"\"\"\nTest pydantic's compliance with mypy.\n\nDo a little skipping about with types to demonstrate its usage.\n\"\"\"\nimport os\nfrom datetime import date, datetime, timedelta, timezone\nfrom pathlib import Path, PurePath\nfrom typing import Any, ClassVar, Dict, ForwardRef, Generic, List, Optional, Type, TypeVar\nfrom uuid import UUID\n\nfrom typing_extensions import Annotated, TypedDict\n\nfrom pydantic import (\n    UUID1,\n    AwareDatetime,\n    BaseModel,\n    ConfigDict,\n    DirectoryPath,\n    FilePath,\n    FutureDate,\n    FutureDatetime,\n    ImportString,\n    Json,\n    NaiveDatetime,\n    NegativeFloat,\n    NegativeInt,\n    NonNegativeFloat,\n    NonNegativeInt,\n    NonPositiveFloat,\n    NonPositiveInt,\n    PastDate,\n    PastDatetime,\n    PositiveFloat,\n    PositiveInt,\n    StrictBool,\n    StrictBytes,\n    StrictFloat,\n    StrictInt,\n    StrictStr,\n    UrlConstraints,\n    WrapValidator,\n    create_model,\n    field_validator,\n    model_validator,\n    root_validator,\n    validate_call,\n)\nfrom pydantic.fields import Field, PrivateAttr\nfrom pydantic.json_schema import Examples\nfrom pydantic.networks import AnyUrl\n\n\nclass Flags(BaseModel):\n    strict_bool: StrictBool = False\n\n    def __str__(self) -> str:\n        return f'flag={self.strict_bool}'\n\n\nclass Model(BaseModel):\n    age: int\n    first_name: str = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n\n    @field_validator('age')\n    def check_age(cls, value: int) -> int:\n        assert value < 100, 'too old'\n        return value\n\n    @root_validator(skip_on_failure=True)\n    def root_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        return values\n\n    @root_validator(pre=True, allow_reuse=False)\n    def pre_root_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        return values\n\n\ndef dog_years(age: int) -> int:\n    return age * 7\n\n\ndef day_of_week(dt: datetime) -> int:\n    return dt.date().isoweekday()\n\n\nm = Model(age=21, list_of_ints=[1, 2, 3])\n\nassert m.age == 21, m.age\nm.age = 42\nassert m.age == 42, m.age\nassert m.first_name == 'John', m.first_name\nassert m.last_name is None, m.last_name\nassert m.list_of_ints == [1, 2, 3], m.list_of_ints\n\ndog_age = dog_years(m.age)\nassert dog_age == 294, dog_age\n\n\nModel(age=2, first_name='Woof', last_name='Woof', signup_ts=datetime(2017, 6, 7), list_of_ints=[1, 2, 3])\nm = Model.model_validate(\n    {\n        'age': 2,\n        'first_name': b'Woof',\n        'last_name': b'Woof',\n        'signup_ts': '2017-06-07 00:00',\n        'list_of_ints': [1, '2', b'3'],\n    }\n)\n\nassert m.first_name == 'Woof', m.first_name\nassert m.last_name == 'Woof', m.last_name\nassert m.signup_ts == datetime(2017, 6, 7), m.signup_ts\nassert day_of_week(m.signup_ts) == 3\n\n\ndata = {'age': 10, 'first_name': 'Alena', 'last_name': 'Sousova', 'list_of_ints': [410]}\nm_from_obj = Model.model_validate(data)\n\nassert isinstance(m_from_obj, Model)\nassert m_from_obj.age == 10\nassert m_from_obj.first_name == data['first_name']\nassert m_from_obj.last_name == data['last_name']\nassert m_from_obj.list_of_ints == data['list_of_ints']\n\nm_copy = m_from_obj.model_copy()\n\nassert isinstance(m_copy, Model)\nassert m_copy.age == m_from_obj.age\nassert m_copy.first_name == m_from_obj.first_name\nassert m_copy.last_name == m_from_obj.last_name\nassert m_copy.list_of_ints == m_from_obj.list_of_ints\n\n\nT = TypeVar('T')\n\n\nclass WrapperModel(BaseModel, Generic[T]):\n    payload: T\n\n\nint_instance = WrapperModel[int](payload=1)\nint_instance.payload += 1\nassert int_instance.payload == 2\n\nstr_instance = WrapperModel[str](payload='a')\nstr_instance.payload += 'a'\nassert str_instance.payload == 'aa'\n\nmodel_instance = WrapperModel[Model](payload=m)\nmodel_instance.payload.list_of_ints.append(4)\nassert model_instance.payload.list_of_ints == [1, 2, 3, 4]\n\n\nclass WithField(BaseModel):\n    age: int\n    first_name: str = Field('John', max_length=42)\n\n\n# simple decorator\n@validate_call\ndef foo(a: int, *, c: str = 'x') -> str:\n    return c * a\n\n\nfoo(1, c='thing')\nfoo(1)\n\n\n# nested decorator should not produce an error\n@validate_call(config={'arbitrary_types_allowed': True})\ndef bar(a: int, *, c: str = 'x') -> str:\n    return c * a\n\n\nbar(1, c='thing')\nbar(1)\n\n\nclass Foo(BaseModel):\n    a: int\n\n\nFooRef = ForwardRef('Foo')\n\n\nclass MyConf(BaseModel):\n    str_pyobject: ImportString[Type[date]] = Field(...)\n    callable_pyobject: ImportString[Type[date]] = Field(default=date)\n\n\nconf = MyConf(str_pyobject='datetime.date')\nvar1: date = conf.str_pyobject(2020, 12, 20)\nvar2: date = conf.callable_pyobject(2111, 1, 1)\n\n\nclass MyPrivateAttr(BaseModel):\n    _private_field: str = PrivateAttr()\n\n\nclass PydanticTypes(BaseModel):\n    model_config = ConfigDict(validate_default=True)\n\n    # Boolean\n    my_strict_bool: StrictBool = True\n    # Integer\n    my_positive_int: PositiveInt = 1\n    my_negative_int: NegativeInt = -1\n    my_non_positive_int: NonPositiveInt = -1\n    my_non_negative_int: NonNegativeInt = 1\n    my_strict_int: StrictInt = 1\n    # Float\n    my_positive_float: PositiveFloat = 1.1\n    my_negative_float: NegativeFloat = -1.1\n    my_non_positive_float: NonPositiveFloat = -1.1\n    my_non_negative_float: NonNegativeFloat = 1.1\n    my_strict_float: StrictFloat = 1.1\n    # Bytes\n    my_strict_bytes: StrictBytes = b'pika'\n    # String\n    my_strict_str: StrictStr = 'pika'\n    # ImportString\n    import_string_str: ImportString[Any] = 'datetime.date'  # type: ignore[misc]\n# MYPY: error: Unused \"type: ignore\" comment\n    import_string_callable: ImportString[Any] = date\n    # UUID\n    my_uuid1: UUID1 = UUID('a8098c1a-f86e-11da-bd1a-00112444be1e')\n    my_uuid1_str: UUID1 = 'a8098c1a-f86e-11da-bd1a-00112444be1e'\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"UUID\")  [assignment]\n    # Path\n    my_file_path: FilePath = Path(__file__)\n    my_file_path_str: FilePath = __file__\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"Path\")  [assignment]\n    my_dir_path: DirectoryPath = Path('.')\n    my_dir_path_str: DirectoryPath = '.'\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"Path\")  [assignment]\n    # Json\n    my_json: Json[Dict[str, str]] = '{\"hello\": \"world\"}'\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"Dict[str, str]\")  [assignment]\n    my_json_list: Json[List[str]] = '[\"hello\", \"world\"]'\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"List[str]\")  [assignment]\n    # Date\n    my_past_date: PastDate = date.today() - timedelta(1)\n    my_future_date: FutureDate = date.today() + timedelta(1)\n    # Datetime\n    my_past_datetime: PastDatetime = datetime.now() - timedelta(1)\n    my_future_datetime: FutureDatetime = datetime.now() + timedelta(1)\n    my_aware_datetime: AwareDatetime = datetime.now(tz=timezone.utc)\n    my_naive_datetime: NaiveDatetime = datetime.now()\n\n\nvalidated = PydanticTypes()\nvalidated.import_string_str(2021, 1, 1)\nvalidated.import_string_callable(2021, 1, 1)\nvalidated.my_uuid1.hex\nvalidated.my_file_path.absolute()\nvalidated.my_file_path_str.absolute()\nvalidated.my_dir_path.absolute()\nvalidated.my_dir_path_str.absolute()\nvalidated.my_json['hello'].capitalize()\nvalidated.my_json_list[0].capitalize()\n\n\nclass UrlModel(BaseModel):\n    x: Annotated[AnyUrl, UrlConstraints(allowed_schemes=['http'])] = Field(default=None)\n    y: Annotated[AnyUrl, UrlConstraints(allowed_schemes=['http'])] = Field(default=None)\n    z: Annotated[AnyUrl, UrlConstraints(allowed_schemes=['s3', 's3n', 's3a'])] = Field(default=None)\n\n\nurl_model = UrlModel(x='http://example.com')\nassert url_model.x.host == 'example.com'\n\n\nclass SomeDict(TypedDict):\n    val: int\n    name: str\n\n\nobj: SomeDict = {\n    'val': 12,\n    'name': 'John',\n}\n\n\nconfig = ConfigDict(title='Record', extra='ignore', str_max_length=1234)\n\n\nclass CustomPath(PurePath):\n    def __init__(self, *args: str):\n        self.path = os.path.join(*args)\n\n    def __fspath__(self) -> str:\n        return f'a/custom/{self.path}'\n\n\nDynamicModel = create_model('DynamicModel')\n\nexamples = Examples({})\n\n\ndef double(value: Any, handler: Any) -> int:\n    return handler(value) * 2\n\n\nclass WrapValidatorModel(BaseModel):\n    x: Annotated[int, WrapValidator(double)]\n\n\nclass Abstract(BaseModel):\n    class_id: ClassVar\n\n\nclass Concrete(Abstract):\n    class_id = 1\n\n\ndef two_dim_shape_validator(v: Dict[str, Any]) -> Dict[str, Any]:\n    assert 'volume' not in v, 'shape is 2d, cannot have volume'\n    return v\n\n\nclass Square(BaseModel):\n    width: float\n    height: float\n\n    free_validator = model_validator(mode='before')(two_dim_shape_validator)\n", "tests/mypy/outputs/1.0.1/pyproject-default_toml/fail2.py": "\"\"\"\nTest mypy failure with invalid types.\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n\nprint(m.foobar)\n# MYPY: error: \"Model\" has no attribute \"foobar\"  [attr-defined]\n", "tests/mypy/outputs/1.0.1/pyproject-plugin-strict_toml/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    class Config:\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.0.1/pyproject-plugin-strict_toml/plugin_success.py": "from dataclasses import InitVar\nfrom typing import Any, ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom typing_extensions import Self\n\nfrom pydantic import BaseModel, ConfigDict, Field, create_model, field_validator, model_validator, validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = ConfigDict(from_attributes=True)\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config={'validate_assignment': True})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n\n\n_TModel = TypeVar('_TModel')\n_TType = TypeVar('_TType')\n\n\nclass OrmMixin(Generic[_TModel, _TType]):\n    @classmethod\n    def from_orm(cls, model: _TModel) -> _TType:\n        raise NotImplementedError\n\n    @classmethod\n    def from_orm_optional(cls, model: Optional[_TModel]) -> Optional[_TType]:\n        if model is None:\n            return None\n        return cls.from_orm(model)\n\n\n@dataclass\nclass MyDataClass:\n    foo: InitVar[str]\n    bar: str\n\n\nMyDataClass(foo='foo', bar='bar')\n\n\ndef get_my_custom_validator(field_name: str) -> Any:\n    @validator(field_name, allow_reuse=True)\n    def my_custom_validator(cls: Any, v: int) -> int:\n        return v\n\n    return my_custom_validator\n\n\ndef foo() -> None:\n    class MyModel(BaseModel):\n        number: int\n        custom_validator = get_my_custom_validator('number')  # type: ignore[pydantic-field]\n\n        @model_validator(mode='before')\n        @classmethod\n        def validate_before(cls, values: Any) -> Any:\n            return values\n\n        @model_validator(mode='after')\n        def validate_after(self) -> Self:\n            return self\n\n    MyModel(number=2)\n\n\nclass InnerModel(BaseModel):\n    my_var: Union[str, None] = Field(default=None)\n\n\nclass OuterModel(InnerModel):\n    pass\n\n\nm = OuterModel()\nif m.my_var is None:\n    # In https://github.com/pydantic/pydantic/issues/7399, this was unreachable\n    print('not unreachable')\n", "tests/mypy/outputs/1.0.1/pyproject-plugin-strict_toml/fail_defaults.py": "from pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    # Required\n    undefined_default_no_args: int = Field()\n    undefined_default: int = Field(description='my desc')\n    positional_ellipsis_default: int = Field(...)\n    named_ellipsis_default: int = Field(default=...)\n\n    # Not required\n    positional_default: int = Field(1)\n    named_default: int = Field(default=2)\n    named_default_factory: int = Field(default_factory=lambda: 3)\n\n\nModel()\n# MYPY: error: Missing named argument \"undefined_default_no_args\" for \"Model\"  [call-arg]\n# MYPY: error: Missing named argument \"undefined_default\" for \"Model\"  [call-arg]\n# MYPY: error: Missing named argument \"positional_ellipsis_default\" for \"Model\"  [call-arg]\n# MYPY: error: Missing named argument \"named_ellipsis_default\" for \"Model\"  [call-arg]\n", "tests/mypy/outputs/1.0.1/pyproject-plugin-strict_toml/plugin_success_baseConfig.py": "from typing import ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom pydantic import BaseModel, Field, create_model, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = dict(from_attributes=True)\n\n    class NotConfig:\n        frozen = True\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n    class NotConfig:\n        frozen = True\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config=dict(validate_assignment=True))\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n", "tests/mypy/outputs/1.0.1/pyproject-plugin-strict_toml/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.0.1/pyproject-plugin_toml/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    class Config:\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.0.1/pyproject-plugin_toml/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.0.1/pyproject-plugin_toml/pydantic_settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    foo: str\n\n\ns = Settings()\n\ns = Settings(foo='test', _case_sensitive=True, _env_prefix='test__', _env_file='test')\n\ns = Settings(foo='test', _case_sensitive=1, _env_prefix=2, _env_file=3)\n# MYPY: error: Argument \"_case_sensitive\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[bool]\"  [arg-type]\n# MYPY: error: Argument \"_env_prefix\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[str]\"  [arg-type]\n# MYPY: error: Argument \"_env_file\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[Union[Path, str, List[Union[Path, str]], Tuple[Union[Path, str], ...]]]\"  [arg-type]\n\n\nclass SettingsWithConfigDict(BaseSettings):\n    bar: str\n\n    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')\n\n\nscd = SettingsWithConfigDict()\n", "tests/mypy/outputs/1.0.1/pyproject-plugin-strict-equality_toml/strict_equality.py": "from pydantic import BaseModel\n\n\nclass User(BaseModel):\n    username: str\n\n\nuser = User(username='test')\nprint(user == 'test')\n# MYPY: error: Non-overlapping equality check (left operand type: \"User\", right operand type: \"Literal['test']\")  [comparison-overlap]\nprint(user.username == int('1'))\n# MYPY: error: Non-overlapping equality check (left operand type: \"str\", right operand type: \"int\")  [comparison-overlap]\nprint(user.username == 'test')\n", "tests/mypy/outputs/1.0.1/mypy-default_ini/plugin_success.py": "from dataclasses import InitVar\nfrom typing import Any, ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom typing_extensions import Self\n\nfrom pydantic import BaseModel, ConfigDict, Field, create_model, field_validator, model_validator, validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = ConfigDict(from_attributes=True)\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config={'validate_assignment': True})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n# MYPY: error: Unexpected keyword argument \"name\" for \"AddProject\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"slug\" for \"AddProject\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"description\" for \"AddProject\"  [call-arg]\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n\n\n_TModel = TypeVar('_TModel')\n_TType = TypeVar('_TType')\n\n\nclass OrmMixin(Generic[_TModel, _TType]):\n    @classmethod\n    def from_orm(cls, model: _TModel) -> _TType:\n        raise NotImplementedError\n\n    @classmethod\n    def from_orm_optional(cls, model: Optional[_TModel]) -> Optional[_TType]:\n        if model is None:\n            return None\n        return cls.from_orm(model)\n\n\n@dataclass\nclass MyDataClass:\n    foo: InitVar[str]\n    bar: str\n\n\nMyDataClass(foo='foo', bar='bar')\n# MYPY: error: Unexpected keyword argument \"foo\" for \"MyDataClass\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"bar\" for \"MyDataClass\"  [call-arg]\n\n\ndef get_my_custom_validator(field_name: str) -> Any:\n    @validator(field_name, allow_reuse=True)\n    def my_custom_validator(cls: Any, v: int) -> int:\n        return v\n\n    return my_custom_validator\n\n\ndef foo() -> None:\n    class MyModel(BaseModel):\n        number: int\n        custom_validator = get_my_custom_validator('number')  # type: ignore[pydantic-field]\n# MYPY: error: Unused \"type: ignore\" comment\n\n        @model_validator(mode='before')\n        @classmethod\n        def validate_before(cls, values: Any) -> Any:\n            return values\n\n        @model_validator(mode='after')\n        def validate_after(self) -> Self:\n            return self\n\n    MyModel(number=2)\n\n\nclass InnerModel(BaseModel):\n    my_var: Union[str, None] = Field(default=None)\n\n\nclass OuterModel(InnerModel):\n    pass\n\n\nm = OuterModel()\nif m.my_var is None:\n    # In https://github.com/pydantic/pydantic/issues/7399, this was unreachable\n    print('not unreachable')\n", "tests/mypy/outputs/1.0.1/mypy-default_ini/fail4.py": "from typing import Any\n\nfrom pydantic import BaseModel, root_validator, validate_call\n\n\n@validate_call\ndef foo(a: int, *, c: str = 'x') -> str:\n# MYPY: note: \"foo\" defined here\n    return c * a\n\n\n# ok\nx: str = foo(1, c='hello')\n# fails\nfoo('x')\n# MYPY: error: Argument 1 to \"foo\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nfoo(1, c=1)\n# MYPY: error: Argument \"c\" to \"foo\" has incompatible type \"int\"; expected \"str\"  [arg-type]\nfoo(1, 2)\n# MYPY: error: Too many positional arguments for \"foo\"  [misc]\n# MYPY: error: Argument 2 to \"foo\" has incompatible type \"int\"; expected \"str\"  [arg-type]\nfoo(1, d=2)\n# MYPY: error: Unexpected keyword argument \"d\" for \"foo\"  [call-arg]\n# mypy assumes foo is just a function\ncallable(foo.raw_function)\n# MYPY: error: \"Callable[[int, DefaultNamedArg(str, 'c')], str]\" has no attribute \"raw_function\"  [attr-defined]\n\n\n@validate_call\ndef bar() -> str:\n    return 'x'\n\n\n# return type should be a string\ny: int = bar()\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n\n\n# Demonstrate type errors for root_validator signatures\nclass Model(BaseModel):\n    @root_validator()\n# MYPY: error: All overload variants of \"root_validator\" require at least one argument  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_1(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=True, skip_on_failure=True)\n# MYPY: error: No overload variant of \"root_validator\" matches argument types \"bool\", \"bool\"  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_2(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=False)\n# MYPY: error: No overload variant of \"root_validator\" matches argument type \"bool\"  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_3(cls, values: Any) -> Any:\n        return values\n\n\nModel.non_existent_attribute\n# MYPY: error: \"Type[Model]\" has no attribute \"non_existent_attribute\"  [attr-defined]\n", "tests/mypy/outputs/1.0.1/mypy-default_ini/fail3.py": "\"\"\"\nTest mypy failure with invalid types.\n\"\"\"\nfrom typing import Generic, List, TypeVar\n\nfrom pydantic import BaseModel\n\nT = TypeVar('T')\n\n\nclass Model(BaseModel):\n    list_of_ints: List[int]\n\n\nclass WrapperModel(BaseModel, Generic[T]):\n    payload: T\n\n\nmodel_instance = Model(list_of_ints=[1])\nwrapper_instance = WrapperModel[Model](payload=model_instance)\nwrapper_instance.payload.list_of_ints.append('1')\n# MYPY: error: Argument 1 to \"append\" of \"list\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n", "tests/mypy/outputs/1.0.1/mypy-default_ini/plugin_success_baseConfig.py": "from typing import ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom pydantic import BaseModel, Field, create_model, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = dict(from_attributes=True)\n\n    class NotConfig:\n        frozen = True\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n    class NotConfig:\n        frozen = True\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config=dict(validate_assignment=True))\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n# MYPY: error: Unexpected keyword argument \"name\" for \"AddProject\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"slug\" for \"AddProject\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"description\" for \"AddProject\"  [call-arg]\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n", "tests/mypy/outputs/1.0.1/mypy-default_ini/fail1.py": "\"\"\"\nTest mypy failure with missing attribute\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\nfrom pydantic.types import Json\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n    json_list_of_ints: Json[List[int]]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n\nprint(m.age + 'not integer')\n# MYPY: error: Unsupported operand types for + (\"int\" and \"str\")  [operator]\nm.json_list_of_ints[0] + 'not integer'\n# MYPY: error: Unsupported operand types for + (\"int\" and \"str\")  [operator]\n", "tests/mypy/outputs/1.0.1/mypy-default_ini/pydantic_settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    foo: str\n\n\ns = Settings()\n\ns = Settings(foo='test', _case_sensitive=True, _env_prefix='test__', _env_file='test')\n\ns = Settings(foo='test', _case_sensitive=1, _env_prefix=2, _env_file=3)\n# MYPY: error: Argument \"_case_sensitive\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[bool]\"  [arg-type]\n# MYPY: error: Argument \"_env_prefix\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[str]\"  [arg-type]\n# MYPY: error: Argument \"_env_file\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[Union[Path, str, List[Union[Path, str]], Tuple[Union[Path, str], ...]]]\"  [arg-type]\n\n\nclass SettingsWithConfigDict(BaseSettings):\n    bar: str\n\n    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')\n\n\nscd = SettingsWithConfigDict()\n", "tests/mypy/outputs/1.0.1/mypy-default_ini/success.py": "\"\"\"\nTest pydantic's compliance with mypy.\n\nDo a little skipping about with types to demonstrate its usage.\n\"\"\"\nimport os\nfrom datetime import date, datetime, timedelta, timezone\nfrom pathlib import Path, PurePath\nfrom typing import Any, ClassVar, Dict, ForwardRef, Generic, List, Optional, Type, TypeVar\nfrom uuid import UUID\n\nfrom typing_extensions import Annotated, TypedDict\n\nfrom pydantic import (\n    UUID1,\n    AwareDatetime,\n    BaseModel,\n    ConfigDict,\n    DirectoryPath,\n    FilePath,\n    FutureDate,\n    FutureDatetime,\n    ImportString,\n    Json,\n    NaiveDatetime,\n    NegativeFloat,\n    NegativeInt,\n    NonNegativeFloat,\n    NonNegativeInt,\n    NonPositiveFloat,\n    NonPositiveInt,\n    PastDate,\n    PastDatetime,\n    PositiveFloat,\n    PositiveInt,\n    StrictBool,\n    StrictBytes,\n    StrictFloat,\n    StrictInt,\n    StrictStr,\n    UrlConstraints,\n    WrapValidator,\n    create_model,\n    field_validator,\n    model_validator,\n    root_validator,\n    validate_call,\n)\nfrom pydantic.fields import Field, PrivateAttr\nfrom pydantic.json_schema import Examples\nfrom pydantic.networks import AnyUrl\n\n\nclass Flags(BaseModel):\n    strict_bool: StrictBool = False\n\n    def __str__(self) -> str:\n        return f'flag={self.strict_bool}'\n\n\nclass Model(BaseModel):\n    age: int\n    first_name: str = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n\n    @field_validator('age')\n    def check_age(cls, value: int) -> int:\n        assert value < 100, 'too old'\n        return value\n\n    @root_validator(skip_on_failure=True)\n    def root_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        return values\n\n    @root_validator(pre=True, allow_reuse=False)\n    def pre_root_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        return values\n\n\ndef dog_years(age: int) -> int:\n    return age * 7\n\n\ndef day_of_week(dt: datetime) -> int:\n    return dt.date().isoweekday()\n\n\nm = Model(age=21, list_of_ints=[1, 2, 3])\n\nassert m.age == 21, m.age\nm.age = 42\nassert m.age == 42, m.age\nassert m.first_name == 'John', m.first_name\nassert m.last_name is None, m.last_name\nassert m.list_of_ints == [1, 2, 3], m.list_of_ints\n\ndog_age = dog_years(m.age)\nassert dog_age == 294, dog_age\n\n\nModel(age=2, first_name='Woof', last_name='Woof', signup_ts=datetime(2017, 6, 7), list_of_ints=[1, 2, 3])\nm = Model.model_validate(\n    {\n        'age': 2,\n        'first_name': b'Woof',\n        'last_name': b'Woof',\n        'signup_ts': '2017-06-07 00:00',\n        'list_of_ints': [1, '2', b'3'],\n    }\n)\n\nassert m.first_name == 'Woof', m.first_name\nassert m.last_name == 'Woof', m.last_name\nassert m.signup_ts == datetime(2017, 6, 7), m.signup_ts\nassert day_of_week(m.signup_ts) == 3\n\n\ndata = {'age': 10, 'first_name': 'Alena', 'last_name': 'Sousova', 'list_of_ints': [410]}\nm_from_obj = Model.model_validate(data)\n\nassert isinstance(m_from_obj, Model)\nassert m_from_obj.age == 10\nassert m_from_obj.first_name == data['first_name']\nassert m_from_obj.last_name == data['last_name']\nassert m_from_obj.list_of_ints == data['list_of_ints']\n\nm_copy = m_from_obj.model_copy()\n\nassert isinstance(m_copy, Model)\nassert m_copy.age == m_from_obj.age\nassert m_copy.first_name == m_from_obj.first_name\nassert m_copy.last_name == m_from_obj.last_name\nassert m_copy.list_of_ints == m_from_obj.list_of_ints\n\n\nT = TypeVar('T')\n\n\nclass WrapperModel(BaseModel, Generic[T]):\n    payload: T\n\n\nint_instance = WrapperModel[int](payload=1)\nint_instance.payload += 1\nassert int_instance.payload == 2\n\nstr_instance = WrapperModel[str](payload='a')\nstr_instance.payload += 'a'\nassert str_instance.payload == 'aa'\n\nmodel_instance = WrapperModel[Model](payload=m)\nmodel_instance.payload.list_of_ints.append(4)\nassert model_instance.payload.list_of_ints == [1, 2, 3, 4]\n\n\nclass WithField(BaseModel):\n    age: int\n    first_name: str = Field('John', max_length=42)\n\n\n# simple decorator\n@validate_call\ndef foo(a: int, *, c: str = 'x') -> str:\n    return c * a\n\n\nfoo(1, c='thing')\nfoo(1)\n\n\n# nested decorator should not produce an error\n@validate_call(config={'arbitrary_types_allowed': True})\ndef bar(a: int, *, c: str = 'x') -> str:\n    return c * a\n\n\nbar(1, c='thing')\nbar(1)\n\n\nclass Foo(BaseModel):\n    a: int\n\n\nFooRef = ForwardRef('Foo')\n\n\nclass MyConf(BaseModel):\n    str_pyobject: ImportString[Type[date]] = Field(...)\n    callable_pyobject: ImportString[Type[date]] = Field(default=date)\n\n\nconf = MyConf(str_pyobject='datetime.date')\nvar1: date = conf.str_pyobject(2020, 12, 20)\nvar2: date = conf.callable_pyobject(2111, 1, 1)\n\n\nclass MyPrivateAttr(BaseModel):\n    _private_field: str = PrivateAttr()\n\n\nclass PydanticTypes(BaseModel):\n    model_config = ConfigDict(validate_default=True)\n\n    # Boolean\n    my_strict_bool: StrictBool = True\n    # Integer\n    my_positive_int: PositiveInt = 1\n    my_negative_int: NegativeInt = -1\n    my_non_positive_int: NonPositiveInt = -1\n    my_non_negative_int: NonNegativeInt = 1\n    my_strict_int: StrictInt = 1\n    # Float\n    my_positive_float: PositiveFloat = 1.1\n    my_negative_float: NegativeFloat = -1.1\n    my_non_positive_float: NonPositiveFloat = -1.1\n    my_non_negative_float: NonNegativeFloat = 1.1\n    my_strict_float: StrictFloat = 1.1\n    # Bytes\n    my_strict_bytes: StrictBytes = b'pika'\n    # String\n    my_strict_str: StrictStr = 'pika'\n    # ImportString\n    import_string_str: ImportString[Any] = 'datetime.date'  # type: ignore[misc]\n# MYPY: error: Unused \"type: ignore\" comment\n    import_string_callable: ImportString[Any] = date\n    # UUID\n    my_uuid1: UUID1 = UUID('a8098c1a-f86e-11da-bd1a-00112444be1e')\n    my_uuid1_str: UUID1 = 'a8098c1a-f86e-11da-bd1a-00112444be1e'\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"UUID\")  [assignment]\n    # Path\n    my_file_path: FilePath = Path(__file__)\n    my_file_path_str: FilePath = __file__\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"Path\")  [assignment]\n    my_dir_path: DirectoryPath = Path('.')\n    my_dir_path_str: DirectoryPath = '.'\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"Path\")  [assignment]\n    # Json\n    my_json: Json[Dict[str, str]] = '{\"hello\": \"world\"}'\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"Dict[str, str]\")  [assignment]\n    my_json_list: Json[List[str]] = '[\"hello\", \"world\"]'\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"List[str]\")  [assignment]\n    # Date\n    my_past_date: PastDate = date.today() - timedelta(1)\n    my_future_date: FutureDate = date.today() + timedelta(1)\n    # Datetime\n    my_past_datetime: PastDatetime = datetime.now() - timedelta(1)\n    my_future_datetime: FutureDatetime = datetime.now() + timedelta(1)\n    my_aware_datetime: AwareDatetime = datetime.now(tz=timezone.utc)\n    my_naive_datetime: NaiveDatetime = datetime.now()\n\n\nvalidated = PydanticTypes()\nvalidated.import_string_str(2021, 1, 1)\nvalidated.import_string_callable(2021, 1, 1)\nvalidated.my_uuid1.hex\nvalidated.my_file_path.absolute()\nvalidated.my_file_path_str.absolute()\nvalidated.my_dir_path.absolute()\nvalidated.my_dir_path_str.absolute()\nvalidated.my_json['hello'].capitalize()\nvalidated.my_json_list[0].capitalize()\n\n\nclass UrlModel(BaseModel):\n    x: Annotated[AnyUrl, UrlConstraints(allowed_schemes=['http'])] = Field(default=None)\n    y: Annotated[AnyUrl, UrlConstraints(allowed_schemes=['http'])] = Field(default=None)\n    z: Annotated[AnyUrl, UrlConstraints(allowed_schemes=['s3', 's3n', 's3a'])] = Field(default=None)\n\n\nurl_model = UrlModel(x='http://example.com')\nassert url_model.x.host == 'example.com'\n\n\nclass SomeDict(TypedDict):\n    val: int\n    name: str\n\n\nobj: SomeDict = {\n    'val': 12,\n    'name': 'John',\n}\n\n\nconfig = ConfigDict(title='Record', extra='ignore', str_max_length=1234)\n\n\nclass CustomPath(PurePath):\n    def __init__(self, *args: str):\n        self.path = os.path.join(*args)\n\n    def __fspath__(self) -> str:\n        return f'a/custom/{self.path}'\n\n\nDynamicModel = create_model('DynamicModel')\n\nexamples = Examples({})\n\n\ndef double(value: Any, handler: Any) -> int:\n    return handler(value) * 2\n\n\nclass WrapValidatorModel(BaseModel):\n    x: Annotated[int, WrapValidator(double)]\n\n\nclass Abstract(BaseModel):\n    class_id: ClassVar\n\n\nclass Concrete(Abstract):\n    class_id = 1\n\n\ndef two_dim_shape_validator(v: Dict[str, Any]) -> Dict[str, Any]:\n    assert 'volume' not in v, 'shape is 2d, cannot have volume'\n    return v\n\n\nclass Square(BaseModel):\n    width: float\n    height: float\n\n    free_validator = model_validator(mode='before')(two_dim_shape_validator)\n", "tests/mypy/outputs/1.0.1/mypy-default_ini/fail2.py": "\"\"\"\nTest mypy failure with invalid types.\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n\nprint(m.foobar)\n# MYPY: error: \"Model\" has no attribute \"foobar\"  [attr-defined]\n", "tests/mypy/outputs/1.0.1/mypy-plugin-very-strict_ini/metaclass_args.py": "from pydantic import BaseModel, Field\n\n\nclass ConfigClassUsed(BaseModel):\n    i: int = Field(2, alias='j')\n\n    class Config:\n        populate_by_name = True\n\n\nConfigClassUsed(i=None)\n# MYPY: error: Argument \"i\" to \"ConfigClassUsed\" has incompatible type \"None\"; expected \"int\"  [arg-type]\n\n\nclass MetaclassArgumentsNoDefault(BaseModel, populate_by_name=True):\n    i: int = Field(alias='j')\n\n\nMetaclassArgumentsNoDefault(i=None)\n# MYPY: error: Argument \"i\" to \"MetaclassArgumentsNoDefault\" has incompatible type \"None\"; expected \"int\"  [arg-type]\n\n\nclass MetaclassArgumentsWithDefault(BaseModel, populate_by_name=True):\n    i: int = Field(2, alias='j')\n\n\nMetaclassArgumentsWithDefault(i=None)\n# MYPY: error: Argument \"i\" to \"MetaclassArgumentsWithDefault\" has incompatible type \"None\"; expected \"int\"  [arg-type]\n\n\nclass NoArguments(BaseModel):\n    i: int = Field(2, alias='j')\n\n\nNoArguments(i=1)\n# MYPY: error: Unexpected keyword argument \"i\" for \"NoArguments\"  [call-arg]\nNoArguments(j=None)\n# MYPY: error: Argument \"j\" to \"NoArguments\" has incompatible type \"None\"; expected \"int\"  [arg-type]\n", "tests/mypy/outputs/1.0.1/mypy-plugin_ini/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    class Config:\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.0.1/mypy-plugin_ini/plugin_optional_inheritance.py": "from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    id: Optional[int]\n\n\nclass Bar(BaseModel):\n    foo: Optional[Foo]\n\n\nclass Baz(Bar):\n    name: str\n\n\nb = Bar(foo={'id': 1})\nassert b.foo.id == 1\n# MYPY: error: Item \"None\" of \"Optional[Foo]\" has no attribute \"id\"  [union-attr]\n\nz = Baz(foo={'id': 1}, name='test')\nassert z.foo.id == 1\n# MYPY: error: Item \"None\" of \"Optional[Foo]\" has no attribute \"id\"  [union-attr]\n", "tests/mypy/outputs/1.0.1/mypy-plugin_ini/custom_constructor.py": "from pydantic import BaseModel\n\n\nclass Person(BaseModel):\n    id: int\n    name: str\n    birth_year: int\n\n    def __init__(self, id: int) -> None:\n# MYPY: note: \"Person\" defined here\n        super().__init__(id=id, name='Patrick', birth_year=1991)\n\n\nPerson(1)\nPerson(id=1)\nPerson(name='Patrick')\n# MYPY: error: Unexpected keyword argument \"name\" for \"Person\"  [call-arg]\n", "tests/mypy/outputs/1.0.1/mypy-plugin_ini/root_models.py": "from typing import List\n\nfrom pydantic import RootModel\n\n\nclass Pets1(RootModel[List[str]]):\n    pass\n\n\npets_construct = Pets1.model_construct(['dog'])\n\nPets2 = RootModel[List[str]]\n\n\nclass Pets3(RootModel):\n# MYPY: error: Missing type parameters for generic type \"RootModel\"  [type-arg]\n    root: List[str]\n\n\npets1 = Pets1(['dog', 'cat'])\npets2 = Pets2(['dog', 'cat'])\npets3 = Pets3(['dog', 'cat'])\n\n\nclass Pets4(RootModel[List[str]]):\n    pets: List[str]\n# MYPY: error: Only `root` is allowed as a field of a `RootModel`  [pydantic-field]\n", "tests/mypy/outputs/1.0.1/mypy-plugin_ini/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.0.1/mypy-plugin_ini/pydantic_settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    foo: str\n\n\ns = Settings()\n\ns = Settings(foo='test', _case_sensitive=True, _env_prefix='test__', _env_file='test')\n\ns = Settings(foo='test', _case_sensitive=1, _env_prefix=2, _env_file=3)\n# MYPY: error: Argument \"_case_sensitive\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[bool]\"  [arg-type]\n# MYPY: error: Argument \"_env_prefix\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[str]\"  [arg-type]\n# MYPY: error: Argument \"_env_file\" to \"Settings\" has incompatible type \"int\"; expected \"Optional[Union[Path, str, List[Union[Path, str]], Tuple[Union[Path, str], ...]]]\"  [arg-type]\n\n\nclass SettingsWithConfigDict(BaseSettings):\n    bar: str\n\n    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')\n\n\nscd = SettingsWithConfigDict()\n", "tests/mypy/outputs/1.0.1/mypy-plugin-strict_ini/plugin_default_factory.py": "\"\"\"\nSee https://github.com/pydantic/pydantic/issues/4457\n\"\"\"\n\nfrom typing import Dict, List\n\nfrom pydantic import BaseModel, Field\n\n\ndef new_list() -> List[int]:\n    return []\n\n\nclass Model(BaseModel):\n    l1: List[str] = Field(default_factory=list)\n    l2: List[int] = Field(default_factory=new_list)\n    l3: List[str] = Field(default_factory=lambda: list())\n    l4: Dict[str, str] = Field(default_factory=dict)\n    l5: int = Field(default_factory=lambda: 123)\n    l6_error: List[str] = Field(default_factory=new_list)\n# MYPY: error: Incompatible types in assignment (expression has type \"List[int]\", variable has type \"List[str]\")  [assignment]\n    l7_error: int = Field(default_factory=list)\n# MYPY: error: Incompatible types in assignment (expression has type \"List[Any]\", variable has type \"int\")  [assignment]\n", "tests/mypy/outputs/1.0.1/mypy-plugin-strict_ini/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    class Config:\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.0.1/mypy-plugin-strict_ini/plugin_success.py": "from dataclasses import InitVar\nfrom typing import Any, ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom typing_extensions import Self\n\nfrom pydantic import BaseModel, ConfigDict, Field, create_model, field_validator, model_validator, validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = ConfigDict(from_attributes=True)\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config={'validate_assignment': True})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n\n\n_TModel = TypeVar('_TModel')\n_TType = TypeVar('_TType')\n\n\nclass OrmMixin(Generic[_TModel, _TType]):\n    @classmethod\n    def from_orm(cls, model: _TModel) -> _TType:\n        raise NotImplementedError\n\n    @classmethod\n    def from_orm_optional(cls, model: Optional[_TModel]) -> Optional[_TType]:\n        if model is None:\n            return None\n        return cls.from_orm(model)\n\n\n@dataclass\nclass MyDataClass:\n    foo: InitVar[str]\n    bar: str\n\n\nMyDataClass(foo='foo', bar='bar')\n\n\ndef get_my_custom_validator(field_name: str) -> Any:\n    @validator(field_name, allow_reuse=True)\n    def my_custom_validator(cls: Any, v: int) -> int:\n        return v\n\n    return my_custom_validator\n\n\ndef foo() -> None:\n    class MyModel(BaseModel):\n        number: int\n        custom_validator = get_my_custom_validator('number')  # type: ignore[pydantic-field]\n\n        @model_validator(mode='before')\n        @classmethod\n        def validate_before(cls, values: Any) -> Any:\n            return values\n\n        @model_validator(mode='after')\n        def validate_after(self) -> Self:\n            return self\n\n    MyModel(number=2)\n\n\nclass InnerModel(BaseModel):\n    my_var: Union[str, None] = Field(default=None)\n\n\nclass OuterModel(InnerModel):\n    pass\n\n\nm = OuterModel()\nif m.my_var is None:\n    # In https://github.com/pydantic/pydantic/issues/7399, this was unreachable\n    print('not unreachable')\n", "tests/mypy/outputs/1.0.1/mypy-plugin-strict_ini/fail_defaults.py": "from pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    # Required\n    undefined_default_no_args: int = Field()\n    undefined_default: int = Field(description='my desc')\n    positional_ellipsis_default: int = Field(...)\n    named_ellipsis_default: int = Field(default=...)\n\n    # Not required\n    positional_default: int = Field(1)\n    named_default: int = Field(default=2)\n    named_default_factory: int = Field(default_factory=lambda: 3)\n\n\nModel()\n# MYPY: error: Missing named argument \"undefined_default_no_args\" for \"Model\"  [call-arg]\n# MYPY: error: Missing named argument \"undefined_default\" for \"Model\"  [call-arg]\n# MYPY: error: Missing named argument \"positional_ellipsis_default\" for \"Model\"  [call-arg]\n# MYPY: error: Missing named argument \"named_ellipsis_default\" for \"Model\"  [call-arg]\n", "tests/mypy/outputs/1.0.1/mypy-plugin-strict_ini/plugin_success_baseConfig.py": "from typing import ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom pydantic import BaseModel, Field, create_model, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = dict(from_attributes=True)\n\n    class NotConfig:\n        frozen = True\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n    class NotConfig:\n        frozen = True\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config=dict(validate_assignment=True))\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n", "tests/mypy/outputs/1.0.1/mypy-plugin-strict_ini/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.1.1/pyproject-default_toml/root_models.py": "from typing import List\n\nfrom pydantic import RootModel\n\n\nclass Pets1(RootModel[List[str]]):\n    pass\n\n\npets_construct = Pets1.model_construct(['dog'])\n\nPets2 = RootModel[List[str]]\n\n\nclass Pets3(RootModel):\n# MYPY: error: Missing type parameters for generic type \"RootModel\"  [type-arg]\n    root: List[str]\n\n\npets1 = Pets1(['dog', 'cat'])\npets2 = Pets2(['dog', 'cat'])\npets3 = Pets3(['dog', 'cat'])\n\n\nclass Pets4(RootModel[List[str]]):\n    pets: List[str]\n", "tests/mypy/outputs/1.1.1/pyproject-default_toml/fail1.py": "\"\"\"\nTest mypy failure with missing attribute\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\nfrom pydantic.types import Json\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n    json_list_of_ints: Json[List[int]]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n# MYPY: error: Missing named argument \"json_list_of_ints\" for \"Model\"  [call-arg]\n# MYPY: error: List item 1 has incompatible type \"str\"; expected \"int\"  [list-item]\n# MYPY: error: List item 2 has incompatible type \"bytes\"; expected \"int\"  [list-item]\n\nprint(m.age + 'not integer')\n# MYPY: error: Unsupported operand types for + (\"int\" and \"str\")  [operator]\nm.json_list_of_ints[0] + 'not integer'\n# MYPY: error: Unsupported operand types for + (\"int\" and \"str\")  [operator]\n", "tests/mypy/outputs/1.1.1/pyproject-default_toml/pydantic_settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    foo: str\n\n\ns = Settings()\n# MYPY: error: Missing named argument \"foo\" for \"Settings\"  [call-arg]\n\ns = Settings(foo='test', _case_sensitive=True, _env_prefix='test__', _env_file='test')\n# MYPY: error: Unexpected keyword argument \"_case_sensitive\" for \"Settings\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"_env_prefix\" for \"Settings\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"_env_file\" for \"Settings\"  [call-arg]\n\ns = Settings(foo='test', _case_sensitive=1, _env_prefix=2, _env_file=3)\n# MYPY: error: Unexpected keyword argument \"_case_sensitive\" for \"Settings\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"_env_prefix\" for \"Settings\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"_env_file\" for \"Settings\"  [call-arg]\n\n\nclass SettingsWithConfigDict(BaseSettings):\n    bar: str\n\n    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')\n\n\nscd = SettingsWithConfigDict()\n# MYPY: error: Missing named argument \"bar\" for \"SettingsWithConfigDict\"  [call-arg]\n", "tests/mypy/outputs/1.1.1/pyproject-default_toml/fail2.py": "\"\"\"\nTest mypy failure with invalid types.\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n# MYPY: error: List item 1 has incompatible type \"str\"; expected \"int\"  [list-item]\n# MYPY: error: List item 2 has incompatible type \"bytes\"; expected \"int\"  [list-item]\n\nprint(m.foobar)\n# MYPY: error: \"Model\" has no attribute \"foobar\"  [attr-defined]\n", "tests/mypy/outputs/1.1.1/mypy-default_ini/plugin_success.py": "from dataclasses import InitVar\nfrom typing import Any, ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom typing_extensions import Self\n\nfrom pydantic import BaseModel, ConfigDict, Field, create_model, field_validator, model_validator, validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = ConfigDict(from_attributes=True)\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n# MYPY: error: Cannot inherit frozen dataclass from a non-frozen one  [misc]\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n# MYPY: error: Cannot inherit non-frozen dataclass from a frozen one  [misc]\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\n# MYPY: error: Property \"x\" defined in \"KwargsNoMutationModel\" is read-only  [misc]\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config={'validate_assignment': True})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n# MYPY: error: Cannot inherit frozen dataclass from a non-frozen one  [misc]\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n\n\n_TModel = TypeVar('_TModel')\n_TType = TypeVar('_TType')\n\n\nclass OrmMixin(Generic[_TModel, _TType]):\n    @classmethod\n    def from_orm(cls, model: _TModel) -> _TType:\n        raise NotImplementedError\n\n    @classmethod\n    def from_orm_optional(cls, model: Optional[_TModel]) -> Optional[_TType]:\n        if model is None:\n            return None\n        return cls.from_orm(model)\n\n\n@dataclass\nclass MyDataClass:\n    foo: InitVar[str]\n    bar: str\n\n\nMyDataClass(foo='foo', bar='bar')\n\n\ndef get_my_custom_validator(field_name: str) -> Any:\n    @validator(field_name, allow_reuse=True)\n    def my_custom_validator(cls: Any, v: int) -> int:\n        return v\n\n    return my_custom_validator\n\n\ndef foo() -> None:\n    class MyModel(BaseModel):\n        number: int\n        custom_validator = get_my_custom_validator('number')  # type: ignore[pydantic-field]\n# MYPY: error: Unused \"type: ignore\" comment\n\n        @model_validator(mode='before')\n        @classmethod\n        def validate_before(cls, values: Any) -> Any:\n            return values\n\n        @model_validator(mode='after')\n        def validate_after(self) -> Self:\n            return self\n\n    MyModel(number=2)\n\n\nclass InnerModel(BaseModel):\n    my_var: Union[str, None] = Field(default=None)\n\n\nclass OuterModel(InnerModel):\n    pass\n\n\nm = OuterModel()\nif m.my_var is None:\n    # In https://github.com/pydantic/pydantic/issues/7399, this was unreachable\n    print('not unreachable')\n", "tests/mypy/outputs/1.1.1/mypy-default_ini/root_models.py": "from typing import List\n\nfrom pydantic import RootModel\n\n\nclass Pets1(RootModel[List[str]]):\n    pass\n\n\npets_construct = Pets1.model_construct(['dog'])\n\nPets2 = RootModel[List[str]]\n\n\nclass Pets3(RootModel):\n# MYPY: error: Missing type parameters for generic type \"RootModel\"  [type-arg]\n    root: List[str]\n\n\npets1 = Pets1(['dog', 'cat'])\npets2 = Pets2(['dog', 'cat'])\npets3 = Pets3(['dog', 'cat'])\n\n\nclass Pets4(RootModel[List[str]]):\n    pets: List[str]\n", "tests/mypy/outputs/1.1.1/mypy-default_ini/plugin_success_baseConfig.py": "from typing import ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom pydantic import BaseModel, Field, create_model, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = dict(from_attributes=True)\n\n    class NotConfig:\n        frozen = True\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n    class NotConfig:\n        frozen = True\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n# MYPY: error: Cannot inherit frozen dataclass from a non-frozen one  [misc]\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n# MYPY: error: Cannot inherit non-frozen dataclass from a frozen one  [misc]\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\n# MYPY: error: Property \"x\" defined in \"KwargsNoMutationModel\" is read-only  [misc]\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config=dict(validate_assignment=True))\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n# MYPY: error: Cannot inherit frozen dataclass from a non-frozen one  [misc]\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n", "tests/mypy/outputs/1.1.1/mypy-default_ini/metaclass_args.py": "from pydantic import BaseModel, Field\n\n\nclass ConfigClassUsed(BaseModel):\n    i: int = Field(2, alias='j')\n\n    class Config:\n        populate_by_name = True\n\n\nConfigClassUsed(i=None)\n# MYPY: error: Unexpected keyword argument \"i\" for \"ConfigClassUsed\"  [call-arg]\n\n\nclass MetaclassArgumentsNoDefault(BaseModel, populate_by_name=True):\n    i: int = Field(alias='j')\n\n\nMetaclassArgumentsNoDefault(i=None)\n# MYPY: error: Unexpected keyword argument \"i\" for \"MetaclassArgumentsNoDefault\"  [call-arg]\n\n\nclass MetaclassArgumentsWithDefault(BaseModel, populate_by_name=True):\n    i: int = Field(2, alias='j')\n\n\nMetaclassArgumentsWithDefault(i=None)\n# MYPY: error: Unexpected keyword argument \"i\" for \"MetaclassArgumentsWithDefault\"  [call-arg]\n\n\nclass NoArguments(BaseModel):\n    i: int = Field(2, alias='j')\n\n\nNoArguments(i=1)\n# MYPY: error: Unexpected keyword argument \"i\" for \"NoArguments\"  [call-arg]\nNoArguments(j=None)\n# MYPY: error: Argument \"j\" to \"NoArguments\" has incompatible type \"None\"; expected \"int\"  [arg-type]\n", "tests/mypy/outputs/1.1.1/mypy-default_ini/fail1.py": "\"\"\"\nTest mypy failure with missing attribute\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\nfrom pydantic.types import Json\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n    json_list_of_ints: Json[List[int]]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n# MYPY: error: Missing named argument \"json_list_of_ints\" for \"Model\"  [call-arg]\n# MYPY: error: List item 1 has incompatible type \"str\"; expected \"int\"  [list-item]\n# MYPY: error: List item 2 has incompatible type \"bytes\"; expected \"int\"  [list-item]\n\nprint(m.age + 'not integer')\n# MYPY: error: Unsupported operand types for + (\"int\" and \"str\")  [operator]\nm.json_list_of_ints[0] + 'not integer'\n# MYPY: error: Unsupported operand types for + (\"int\" and \"str\")  [operator]\n", "tests/mypy/outputs/1.1.1/mypy-default_ini/pydantic_settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    foo: str\n\n\ns = Settings()\n# MYPY: error: Missing named argument \"foo\" for \"Settings\"  [call-arg]\n\ns = Settings(foo='test', _case_sensitive=True, _env_prefix='test__', _env_file='test')\n# MYPY: error: Unexpected keyword argument \"_case_sensitive\" for \"Settings\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"_env_prefix\" for \"Settings\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"_env_file\" for \"Settings\"  [call-arg]\n\ns = Settings(foo='test', _case_sensitive=1, _env_prefix=2, _env_file=3)\n# MYPY: error: Unexpected keyword argument \"_case_sensitive\" for \"Settings\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"_env_prefix\" for \"Settings\"  [call-arg]\n# MYPY: error: Unexpected keyword argument \"_env_file\" for \"Settings\"  [call-arg]\n\n\nclass SettingsWithConfigDict(BaseSettings):\n    bar: str\n\n    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')\n\n\nscd = SettingsWithConfigDict()\n# MYPY: error: Missing named argument \"bar\" for \"SettingsWithConfigDict\"  [call-arg]\n", "tests/mypy/outputs/1.1.1/mypy-default_ini/fail2.py": "\"\"\"\nTest mypy failure with invalid types.\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n# MYPY: error: List item 1 has incompatible type \"str\"; expected \"int\"  [list-item]\n# MYPY: error: List item 2 has incompatible type \"bytes\"; expected \"int\"  [list-item]\n\nprint(m.foobar)\n# MYPY: error: \"Model\" has no attribute \"foobar\"  [attr-defined]\n", "tests/mypy/outputs/1.4.1/pyproject-default_toml/fail4.py": "from typing import Any\n\nfrom pydantic import BaseModel, root_validator, validate_call\n\n\n@validate_call\ndef foo(a: int, *, c: str = 'x') -> str:\n# MYPY: note: \"foo\" defined here\n    return c * a\n\n\n# ok\nx: str = foo(1, c='hello')\n# fails\nfoo('x')\n# MYPY: error: Argument 1 to \"foo\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nfoo(1, c=1)\n# MYPY: error: Argument \"c\" to \"foo\" has incompatible type \"int\"; expected \"str\"  [arg-type]\nfoo(1, 2)\n# MYPY: error: Too many positional arguments for \"foo\"  [misc]\n# MYPY: error: Argument 2 to \"foo\" has incompatible type \"int\"; expected \"str\"  [arg-type]\nfoo(1, d=2)\n# MYPY: error: Unexpected keyword argument \"d\" for \"foo\"  [call-arg]\n# mypy assumes foo is just a function\ncallable(foo.raw_function)\n# MYPY: error: \"Callable[[int, DefaultNamedArg(str, 'c')], str]\" has no attribute \"raw_function\"  [attr-defined]\n\n\n@validate_call\ndef bar() -> str:\n    return 'x'\n\n\n# return type should be a string\ny: int = bar()\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n\n\n# Demonstrate type errors for root_validator signatures\nclass Model(BaseModel):\n    @root_validator()\n# MYPY: error: All overload variants of \"root_validator\" require at least one argument  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_1(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=True, skip_on_failure=True)\n# MYPY: error: No overload variant of \"root_validator\" matches argument types \"bool\", \"bool\"  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_2(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=False)\n# MYPY: error: No overload variant of \"root_validator\" matches argument type \"bool\"  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_3(cls, values: Any) -> Any:\n        return values\n\n\nModel.non_existent_attribute\n# MYPY: error: \"type[Model]\" has no attribute \"non_existent_attribute\"  [attr-defined]\n", "tests/mypy/outputs/1.4.1/pyproject-plugin-strict_toml/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    class Config:\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Callable[[], Any] | None\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.4.1/pyproject-plugin-strict_toml/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Callable[[], Any] | None\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.4.1/pyproject-plugin_toml/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    class Config:\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment  [unused-ignore]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Callable[[], Any] | None\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.4.1/pyproject-plugin_toml/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment  [unused-ignore]\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Callable[[], Any] | None\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.4.1/pyproject-plugin_toml/pydantic_settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    foo: str\n\n\ns = Settings()\n\ns = Settings(foo='test', _case_sensitive=True, _env_prefix='test__', _env_file='test')\n\ns = Settings(foo='test', _case_sensitive=1, _env_prefix=2, _env_file=3)\n# MYPY: error: Argument \"_case_sensitive\" to \"Settings\" has incompatible type \"int\"; expected \"bool | None\"  [arg-type]\n# MYPY: error: Argument \"_env_prefix\" to \"Settings\" has incompatible type \"int\"; expected \"str | None\"  [arg-type]\n# MYPY: error: Argument \"_env_file\" to \"Settings\" has incompatible type \"int\"; expected \"Path | str | list[Path | str] | tuple[Path | str, ...] | None\"  [arg-type]\n\n\nclass SettingsWithConfigDict(BaseSettings):\n    bar: str\n\n    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')\n\n\nscd = SettingsWithConfigDict()\n", "tests/mypy/outputs/1.4.1/mypy-default_ini/plugin_success.py": "from dataclasses import InitVar\nfrom typing import Any, ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom typing_extensions import Self\n\nfrom pydantic import BaseModel, ConfigDict, Field, create_model, field_validator, model_validator, validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = ConfigDict(from_attributes=True)\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n# MYPY: error: Cannot inherit non-frozen dataclass from a frozen one  [misc]\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\n# MYPY: error: Property \"x\" defined in \"KwargsNoMutationModel\" is read-only  [misc]\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config={'validate_assignment': True})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n\n\n_TModel = TypeVar('_TModel')\n_TType = TypeVar('_TType')\n\n\nclass OrmMixin(Generic[_TModel, _TType]):\n    @classmethod\n    def from_orm(cls, model: _TModel) -> _TType:\n        raise NotImplementedError\n\n    @classmethod\n    def from_orm_optional(cls, model: Optional[_TModel]) -> Optional[_TType]:\n        if model is None:\n            return None\n        return cls.from_orm(model)\n\n\n@dataclass\nclass MyDataClass:\n    foo: InitVar[str]\n    bar: str\n\n\nMyDataClass(foo='foo', bar='bar')\n\n\ndef get_my_custom_validator(field_name: str) -> Any:\n    @validator(field_name, allow_reuse=True)\n    def my_custom_validator(cls: Any, v: int) -> int:\n        return v\n\n    return my_custom_validator\n\n\ndef foo() -> None:\n    class MyModel(BaseModel):\n        number: int\n        custom_validator = get_my_custom_validator('number')  # type: ignore[pydantic-field]\n# MYPY: error: Unused \"type: ignore\" comment  [unused-ignore]\n\n        @model_validator(mode='before')\n        @classmethod\n        def validate_before(cls, values: Any) -> Any:\n            return values\n\n        @model_validator(mode='after')\n        def validate_after(self) -> Self:\n            return self\n\n    MyModel(number=2)\n\n\nclass InnerModel(BaseModel):\n    my_var: Union[str, None] = Field(default=None)\n\n\nclass OuterModel(InnerModel):\n    pass\n\n\nm = OuterModel()\nif m.my_var is None:\n    # In https://github.com/pydantic/pydantic/issues/7399, this was unreachable\n    print('not unreachable')\n", "tests/mypy/outputs/1.4.1/mypy-default_ini/fail4.py": "from typing import Any\n\nfrom pydantic import BaseModel, root_validator, validate_call\n\n\n@validate_call\ndef foo(a: int, *, c: str = 'x') -> str:\n# MYPY: note: \"foo\" defined here\n    return c * a\n\n\n# ok\nx: str = foo(1, c='hello')\n# fails\nfoo('x')\n# MYPY: error: Argument 1 to \"foo\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nfoo(1, c=1)\n# MYPY: error: Argument \"c\" to \"foo\" has incompatible type \"int\"; expected \"str\"  [arg-type]\nfoo(1, 2)\n# MYPY: error: Too many positional arguments for \"foo\"  [misc]\n# MYPY: error: Argument 2 to \"foo\" has incompatible type \"int\"; expected \"str\"  [arg-type]\nfoo(1, d=2)\n# MYPY: error: Unexpected keyword argument \"d\" for \"foo\"  [call-arg]\n# mypy assumes foo is just a function\ncallable(foo.raw_function)\n# MYPY: error: \"Callable[[int, DefaultNamedArg(str, 'c')], str]\" has no attribute \"raw_function\"  [attr-defined]\n\n\n@validate_call\ndef bar() -> str:\n    return 'x'\n\n\n# return type should be a string\ny: int = bar()\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n\n\n# Demonstrate type errors for root_validator signatures\nclass Model(BaseModel):\n    @root_validator()\n# MYPY: error: All overload variants of \"root_validator\" require at least one argument  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_1(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=True, skip_on_failure=True)\n# MYPY: error: No overload variant of \"root_validator\" matches argument types \"bool\", \"bool\"  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_2(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=False)\n# MYPY: error: No overload variant of \"root_validator\" matches argument type \"bool\"  [call-overload]\n# MYPY: note: Possible overload variants:\n# MYPY: note:     def root_validator(*, skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n# MYPY: note:     def root_validator(*, pre: Literal[False], skip_on_failure: Literal[True], allow_reuse: bool = ...) -> Callable[[_V1RootValidatorFunctionType], _V1RootValidatorFunctionType]\n    @classmethod\n    def validate_3(cls, values: Any) -> Any:\n        return values\n\n\nModel.non_existent_attribute\n# MYPY: error: \"type[Model]\" has no attribute \"non_existent_attribute\"  [attr-defined]\n", "tests/mypy/outputs/1.4.1/mypy-plugin_ini/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    class Config:\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment  [unused-ignore]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Callable[[], Any] | None\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.4.1/mypy-plugin_ini/plugin_optional_inheritance.py": "from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    id: Optional[int]\n\n\nclass Bar(BaseModel):\n    foo: Optional[Foo]\n\n\nclass Baz(Bar):\n    name: str\n\n\nb = Bar(foo={'id': 1})\nassert b.foo.id == 1\n# MYPY: error: Item \"None\" of \"Foo | None\" has no attribute \"id\"  [union-attr]\n\nz = Baz(foo={'id': 1}, name='test')\nassert z.foo.id == 1\n# MYPY: error: Item \"None\" of \"Foo | None\" has no attribute \"id\"  [union-attr]\n", "tests/mypy/outputs/1.4.1/mypy-plugin_ini/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment  [unused-ignore]\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Callable[[], Any] | None\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.4.1/mypy-plugin_ini/pydantic_settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    foo: str\n\n\ns = Settings()\n\ns = Settings(foo='test', _case_sensitive=True, _env_prefix='test__', _env_file='test')\n\ns = Settings(foo='test', _case_sensitive=1, _env_prefix=2, _env_file=3)\n# MYPY: error: Argument \"_case_sensitive\" to \"Settings\" has incompatible type \"int\"; expected \"bool | None\"  [arg-type]\n# MYPY: error: Argument \"_env_prefix\" to \"Settings\" has incompatible type \"int\"; expected \"str | None\"  [arg-type]\n# MYPY: error: Argument \"_env_file\" to \"Settings\" has incompatible type \"int\"; expected \"Path | str | list[Path | str] | tuple[Path | str, ...] | None\"  [arg-type]\n\n\nclass SettingsWithConfigDict(BaseSettings):\n    bar: str\n\n    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')\n\n\nscd = SettingsWithConfigDict()\n", "tests/mypy/outputs/1.4.1/mypy-plugin-strict_ini/plugin_default_factory.py": "\"\"\"\nSee https://github.com/pydantic/pydantic/issues/4457\n\"\"\"\n\nfrom typing import Dict, List\n\nfrom pydantic import BaseModel, Field\n\n\ndef new_list() -> List[int]:\n    return []\n\n\nclass Model(BaseModel):\n    l1: List[str] = Field(default_factory=list)\n    l2: List[int] = Field(default_factory=new_list)\n    l3: List[str] = Field(default_factory=lambda: list())\n    l4: Dict[str, str] = Field(default_factory=dict)\n    l5: int = Field(default_factory=lambda: 123)\n    l6_error: List[str] = Field(default_factory=new_list)\n# MYPY: error: Incompatible types in assignment (expression has type \"list[int]\", variable has type \"list[str]\")  [assignment]\n    l7_error: int = Field(default_factory=list)\n# MYPY: error: Incompatible types in assignment (expression has type \"list[Any]\", variable has type \"int\")  [assignment]\n", "tests/mypy/outputs/1.4.1/mypy-plugin-strict_ini/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    class Config:\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Callable[[], Any] | None\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.4.1/mypy-plugin-strict_ini/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Callable[[], Any] | None\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.2.0/pyproject-plugin-strict_toml/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    class Config:\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.2.0/pyproject-plugin-strict_toml/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.2.0/pyproject-plugin_toml/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    class Config:\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.2.0/pyproject-plugin_toml/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.2.0/mypy-default_ini/plugin_success.py": "from dataclasses import InitVar\nfrom typing import Any, ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom typing_extensions import Self\n\nfrom pydantic import BaseModel, ConfigDict, Field, create_model, field_validator, model_validator, validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = ConfigDict(from_attributes=True)\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n# MYPY: error: Cannot inherit non-frozen dataclass from a frozen one  [misc]\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\n# MYPY: error: Property \"x\" defined in \"KwargsNoMutationModel\" is read-only  [misc]\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config={'validate_assignment': True})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n\n\n_TModel = TypeVar('_TModel')\n_TType = TypeVar('_TType')\n\n\nclass OrmMixin(Generic[_TModel, _TType]):\n    @classmethod\n    def from_orm(cls, model: _TModel) -> _TType:\n        raise NotImplementedError\n\n    @classmethod\n    def from_orm_optional(cls, model: Optional[_TModel]) -> Optional[_TType]:\n        if model is None:\n            return None\n        return cls.from_orm(model)\n\n\n@dataclass\nclass MyDataClass:\n    foo: InitVar[str]\n    bar: str\n\n\nMyDataClass(foo='foo', bar='bar')\n\n\ndef get_my_custom_validator(field_name: str) -> Any:\n    @validator(field_name, allow_reuse=True)\n    def my_custom_validator(cls: Any, v: int) -> int:\n        return v\n\n    return my_custom_validator\n\n\ndef foo() -> None:\n    class MyModel(BaseModel):\n        number: int\n        custom_validator = get_my_custom_validator('number')  # type: ignore[pydantic-field]\n# MYPY: error: Unused \"type: ignore\" comment\n\n        @model_validator(mode='before')\n        @classmethod\n        def validate_before(cls, values: Any) -> Any:\n            return values\n\n        @model_validator(mode='after')\n        def validate_after(self) -> Self:\n            return self\n\n    MyModel(number=2)\n\n\nclass InnerModel(BaseModel):\n    my_var: Union[str, None] = Field(default=None)\n\n\nclass OuterModel(InnerModel):\n    pass\n\n\nm = OuterModel()\nif m.my_var is None:\n    # In https://github.com/pydantic/pydantic/issues/7399, this was unreachable\n    print('not unreachable')\n", "tests/mypy/outputs/1.2.0/mypy-default_ini/plugin_success_baseConfig.py": "from typing import ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom pydantic import BaseModel, Field, create_model, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = dict(from_attributes=True)\n\n    class NotConfig:\n        frozen = True\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n    class NotConfig:\n        frozen = True\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n# MYPY: error: Cannot inherit non-frozen dataclass from a frozen one  [misc]\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\n# MYPY: error: Property \"x\" defined in \"KwargsNoMutationModel\" is read-only  [misc]\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config=dict(validate_assignment=True))\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n", "tests/mypy/outputs/1.2.0/mypy-plugin_ini/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    class Config:\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.2.0/mypy-plugin_ini/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Missing named argument \"x\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n# MYPY: error: Unused \"type: ignore\" comment\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.2.0/mypy-plugin-strict_ini/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    class Config:\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/outputs/1.2.0/mypy-plugin-strict_ini/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"Model\"  [call-arg]\nmodel = Model(x=1)\n# MYPY: error: Missing named argument \"y\" for \"Model\"  [call-arg]\nmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"Model\" is read-only  [misc]\nModel.from_orm({})\n# MYPY: error: \"Model\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsModel\"  [call-arg]\nkwargs_model = KwargsModel(x=1)\n# MYPY: error: Missing named argument \"y\" for \"KwargsModel\"  [call-arg]\nkwargs_model.y = 'a'\n# MYPY: error: Property \"y\" defined in \"KwargsModel\" is read-only  [misc]\nKwargsModel.from_orm({})\n# MYPY: error: \"KwargsModel\" does not have from_attributes=True  [pydantic-orm]\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"ForbidExtraModel\"  [call-arg]\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsForbidExtraModel\"  [call-arg]\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n# MYPY: error: Invalid value for \"Config.extra\"  [pydantic-config]\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n# MYPY: note: Error code \"pydantic-config\" not covered by \"type: ignore\" comment\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n# MYPY: error: Invalid value for \"Config.from_attributes\"  [pydantic-config]\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n# MYPY: error: Incompatible types in assignment (expression has type \"ellipsis\", variable has type \"int\")  [assignment]\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    j = 1\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n\n\nDefaultTestingModel()\n# MYPY: error: Missing named argument \"a\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"b\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"c\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"d\" for \"DefaultTestingModel\"  [call-arg]\n# MYPY: error: Missing named argument \"f\" for \"DefaultTestingModel\"  [call-arg]\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n# MYPY: error: Name \"Undefined\" is not defined  [name-defined]\n\n\nUndefinedAnnotationModel()\n# MYPY: error: Missing named argument \"undefined\" for \"UndefinedAnnotationModel\"  [call-arg]\n\n\nModel.model_construct(x=1)\n# MYPY: error: Missing named argument \"y\" for \"model_construct\" of \"Model\"  [call-arg]\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"model_construct\" of \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\n# MYPY: error: Argument \"x\" to \"InheritingModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nModel(x='1', y='2')\n# MYPY: error: Argument \"x\" to \"Model\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n# MYPY: error: Argument \"data\" to \"Response\" has incompatible type \"int\"; expected \"Model\"  [arg-type]\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n# MYPY: error: Argument \"y\" to \"AliasModel\" has incompatible type \"int\"; expected \"str\"  [arg-type]\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n# MYPY: error: Argument \"z\" to \"DynamicAliasModel\" has incompatible type \"str\"; expected \"int\"  [arg-type]\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"DynamicAliasModel2\"  [call-arg]\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\n# MYPY: error: Unexpected keyword argument \"y\" for \"KwargsDynamicAliasModel\"  [call-arg]\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n# MYPY: error: Untyped fields disallowed  [pydantic-field]\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"AliasGeneratorModel2\"  [call-arg]\nAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"AliasGeneratorModel2\"  [call-arg]\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n    x: int = Field(..., alias='y')\n# MYPY: error: Required dynamic aliases disallowed  [pydantic-alias]\n\n\nKwargsAliasGeneratorModel2(x=1)\n# MYPY: error: Unexpected keyword argument \"x\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\nKwargsAliasGeneratorModel2(y=1, z=1)\n# MYPY: error: Unexpected keyword argument \"z\" for \"KwargsAliasGeneratorModel2\"  [call-arg]\n\n\nclass CoverageTester(Missing):  # noqa F821\n# MYPY: error: Name \"Missing\" is not defined  [name-defined]\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n# MYPY: error: Property \"y\" defined in \"FrozenModel\" is read-only  [misc]\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n    f: int = None\n# MYPY: error: Incompatible types in assignment (expression has type \"None\", variable has type \"int\")  [assignment]\n\n    # Default factory\n    g: str = Field(default_factory=set)\n# MYPY: error: Incompatible types in assignment (expression has type \"Set[Any]\", variable has type \"str\")  [assignment]\n    h: int = Field(default_factory=_default_factory)\n# MYPY: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n# MYPY: error: Argument \"default_factory\" to \"Field\" has incompatible type \"int\"; expected \"Optional[Callable[[], Any]]\"  [arg-type]\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n# MYPY: error: Field default and default_factory cannot be specified together  [pydantic-field]\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n# MYPY: error: Missing positional argument \"self\" in call to \"instance_method\" of \"ModelWithAnnotatedValidator\"  [call-arg]\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/modules/computed_fields.py": "from pydantic import BaseModel, computed_field\n\n\nclass Square(BaseModel):\n    side: float\n\n    @computed_field\n    @property\n    def area(self) -> float:\n        return self.side**2\n\n    @area.setter\n    def area(self, area: float) -> None:\n        self.side = area**0.5\n\n\nsq = Square(side=10)\ny = 12.4 + sq.area\nz = 'x' + sq.area\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n    pass\nelse:\n\n    class Square2(BaseModel):\n        side: float\n\n        @computed_field\n        @cached_property\n        def area(self) -> float:\n            return self.side**2\n\n    sq = Square(side=10)\n    y = 12.4 + sq.area\n    z = 'x' + sq.area\n", "tests/mypy/modules/plugin_default_factory.py": "\"\"\"\nSee https://github.com/pydantic/pydantic/issues/4457\n\"\"\"\n\nfrom typing import Dict, List\n\nfrom pydantic import BaseModel, Field\n\n\ndef new_list() -> List[int]:\n    return []\n\n\nclass Model(BaseModel):\n    l1: List[str] = Field(default_factory=list)\n    l2: List[int] = Field(default_factory=new_list)\n    l3: List[str] = Field(default_factory=lambda: list())\n    l4: Dict[str, str] = Field(default_factory=dict)\n    l5: int = Field(default_factory=lambda: 123)\n    l6_error: List[str] = Field(default_factory=new_list)\n    l7_error: int = Field(default_factory=list)\n", "tests/mypy/modules/generics.py": "from typing import Any, Dict, Generic, Optional, TypeVar\n\nfrom pydantic import BaseModel\n\nTbody = TypeVar('Tbody')\n\n\nclass Response(BaseModel, Generic[Tbody]):\n    url: str\n    body: Tbody\n\n\nclass JsonBody(BaseModel):\n    raw: str\n    data: Dict[str, Any]\n\n\nclass HtmlBody(BaseModel):\n    raw: str\n    doctype: str\n\n\nclass JsonResponse(Response[JsonBody]):\n    pass\n\n\nclass HtmlResponse(Response[HtmlBody]):\n    def custom_method(self) -> None:\n        doctype = self.body.doctype\n        print(f'self: {doctype}')\n\n\nexample = {'url': 'foo.com', 'body': {'raw': '..<html>..', 'doctype': 'html'}}\n\nresp = HtmlResponse.model_validate(example)\nresp.custom_method()\n\ndoctype = resp.body.doctype\n\n\nT = TypeVar('T', int, str)\n\n\nclass HistoryField(BaseModel, Generic[T]):\n    value: Optional[T]\n\n\nclass DomainType(HistoryField[int]):\n    pass\n\n\nthing = DomainType(value=None)\nvar: Optional[int] = thing.value\n", "tests/mypy/modules/plugin_fail_baseConfig.py": "from typing import Any, Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n        def config_method(self) -> None:\n            ...\n\n\nmodel = Model(x=1, y='y', z='z')\nmodel = Model(x=1)\nmodel.y = 'a'\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\nkwargs_model = KwargsModel(x=1)\nkwargs_model.y = 'a'\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    class Config:\n        extra = 'forbid'\n\n\nForbidExtraModel(x=1)\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n\n\nclass BadExtraModel(BaseModel):\n    class Config:\n        extra = 1  # type: ignore[pydantic-config]\n        extra = 1\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n    pass\n\n\nclass BadConfig1(BaseModel):\n    class Config:\n        from_attributes: Any = {}  # not sensible, but should still be handled gracefully\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n    pass\n\n\nclass BadConfig2(BaseModel):\n    class Config:\n        from_attributes = list  # not sensible, but should still be handled gracefully\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n    pass\n\n\nclass InheritingModel(Model):\n    class Config:\n        frozen = False\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n    j = 1\n\n\nDefaultTestingModel()\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n\n\nUndefinedAnnotationModel()\n\n\nModel.model_construct(x=1)\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    class Config:\n        populate_by_name = True\n\n\nDynamicAliasModel2(y='y', z=1)\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    class Config:\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    class Config:  # type: ignore[pydantic-alias]\n        alias_generator = lambda x: x + '_'  # noqa E731\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    class Config:\n        alias_generator = None\n        frozen = True\n        extra = Extra.forbid\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n\n\nclass InheritingModel2(FrozenModel):\n    class Config:\n        frozen = False\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n    f: int = None\n\n    # Default factory\n    g: str = Field(default_factory=set)\n    h: int = Field(default_factory=_default_factory)\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/modules/plugin_success.py": "from dataclasses import InitVar\nfrom typing import Any, ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom typing_extensions import Self\n\nfrom pydantic import BaseModel, ConfigDict, Field, create_model, field_validator, model_validator, validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = ConfigDict(from_attributes=True)\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config={'validate_assignment': True})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = ConfigDict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n\n\n_TModel = TypeVar('_TModel')\n_TType = TypeVar('_TType')\n\n\nclass OrmMixin(Generic[_TModel, _TType]):\n    @classmethod\n    def from_orm(cls, model: _TModel) -> _TType:\n        raise NotImplementedError\n\n    @classmethod\n    def from_orm_optional(cls, model: Optional[_TModel]) -> Optional[_TType]:\n        if model is None:\n            return None\n        return cls.from_orm(model)\n\n\n@dataclass\nclass MyDataClass:\n    foo: InitVar[str]\n    bar: str\n\n\nMyDataClass(foo='foo', bar='bar')\n\n\ndef get_my_custom_validator(field_name: str) -> Any:\n    @validator(field_name, allow_reuse=True)\n    def my_custom_validator(cls: Any, v: int) -> int:\n        return v\n\n    return my_custom_validator\n\n\ndef foo() -> None:\n    class MyModel(BaseModel):\n        number: int\n        custom_validator = get_my_custom_validator('number')  # type: ignore[pydantic-field]\n\n        @model_validator(mode='before')\n        @classmethod\n        def validate_before(cls, values: Any) -> Any:\n            return values\n\n        @model_validator(mode='after')\n        def validate_after(self) -> Self:\n            return self\n\n    MyModel(number=2)\n\n\nclass InnerModel(BaseModel):\n    my_var: Union[str, None] = Field(default=None)\n\n\nclass OuterModel(InnerModel):\n    pass\n\n\nm = OuterModel()\nif m.my_var is None:\n    # In https://github.com/pydantic/pydantic/issues/7399, this was unreachable\n    print('not unreachable')\n", "tests/mypy/modules/fail4.py": "from typing import Any\n\nfrom pydantic import BaseModel, root_validator, validate_call\n\n\n@validate_call\ndef foo(a: int, *, c: str = 'x') -> str:\n    return c * a\n\n\n# ok\nx: str = foo(1, c='hello')\n# fails\nfoo('x')\nfoo(1, c=1)\nfoo(1, 2)\nfoo(1, d=2)\n# mypy assumes foo is just a function\ncallable(foo.raw_function)\n\n\n@validate_call\ndef bar() -> str:\n    return 'x'\n\n\n# return type should be a string\ny: int = bar()\n\n\n# Demonstrate type errors for root_validator signatures\nclass Model(BaseModel):\n    @root_validator()\n    @classmethod\n    def validate_1(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=True, skip_on_failure=True)\n    @classmethod\n    def validate_2(cls, values: Any) -> Any:\n        return values\n\n    @root_validator(pre=False)\n    @classmethod\n    def validate_3(cls, values: Any) -> Any:\n        return values\n\n\nModel.non_existent_attribute\n", "tests/mypy/modules/with_config_decorator.py": "from typing import TypedDict\n\nfrom pydantic import ConfigDict, with_config\n\n\n@with_config(ConfigDict(str_to_lower=True))\nclass Model(TypedDict):\n    a: str\n\n\nmodel = Model(a='ABC')\n", "tests/mypy/modules/fail3.py": "\"\"\"\nTest mypy failure with invalid types.\n\"\"\"\nfrom typing import Generic, List, TypeVar\n\nfrom pydantic import BaseModel\n\nT = TypeVar('T')\n\n\nclass Model(BaseModel):\n    list_of_ints: List[int]\n\n\nclass WrapperModel(BaseModel, Generic[T]):\n    payload: T\n\n\nmodel_instance = Model(list_of_ints=[1])\nwrapper_instance = WrapperModel[Model](payload=model_instance)\nwrapper_instance.payload.list_of_ints.append('1')\n", "tests/mypy/modules/no_strict_optional.py": "from typing import Optional, Union\n\nfrom pydantic import BaseModel, ConfigDict\n\n\nclass MongoSettings(BaseModel):\n    MONGO_PASSWORD: Union[str, None]\n\n\nclass CustomBaseModel(BaseModel):\n    model_config = ConfigDict(\n        validate_assignment=True,\n        validate_default=True,\n        extra='forbid',\n        frozen=True,\n    )\n\n\nclass HealthStatus(CustomBaseModel):\n    status: str\n    description: Optional[str] = None\n\n\nhs = HealthStatus(status='healthy')\n", "tests/mypy/modules/plugin_optional_inheritance.py": "from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    id: Optional[int]\n\n\nclass Bar(BaseModel):\n    foo: Optional[Foo]\n\n\nclass Baz(Bar):\n    name: str\n\n\nb = Bar(foo={'id': 1})\nassert b.foo.id == 1\n\nz = Baz(foo={'id': 1}, name='test')\nassert z.foo.id == 1\n", "tests/mypy/modules/dataclass_no_any.py": "from pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass Foo:\n    foo: int\n\n\n@dataclass(config={'title': 'Bar Title'})\nclass Bar:\n    bar: str\n", "tests/mypy/modules/custom_constructor.py": "from pydantic import BaseModel\n\n\nclass Person(BaseModel):\n    id: int\n    name: str\n    birth_year: int\n\n    def __init__(self, id: int) -> None:\n        super().__init__(id=id, name='Patrick', birth_year=1991)\n\n\nPerson(1)\nPerson(id=1)\nPerson(name='Patrick')\n", "tests/mypy/modules/fail_defaults.py": "from pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    # Required\n    undefined_default_no_args: int = Field()\n    undefined_default: int = Field(description='my desc')\n    positional_ellipsis_default: int = Field(...)\n    named_ellipsis_default: int = Field(default=...)\n\n    # Not required\n    positional_default: int = Field(1)\n    named_default: int = Field(default=2)\n    named_default_factory: int = Field(default_factory=lambda: 3)\n\n\nModel()\n", "tests/mypy/modules/root_models.py": "from typing import List\n\nfrom pydantic import RootModel\n\n\nclass Pets1(RootModel[List[str]]):\n    pass\n\n\npets_construct = Pets1.model_construct(['dog'])\n\nPets2 = RootModel[List[str]]\n\n\nclass Pets3(RootModel):\n    root: List[str]\n\n\npets1 = Pets1(['dog', 'cat'])\npets2 = Pets2(['dog', 'cat'])\npets3 = Pets3(['dog', 'cat'])\n\n\nclass Pets4(RootModel[List[str]]):\n    pets: List[str]\n", "tests/mypy/modules/plugin_success_baseConfig.py": "from typing import ClassVar, Generic, List, Optional, TypeVar, Union\n\nfrom pydantic import BaseModel, Field, create_model, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    x: float\n    y: str\n\n    model_config = dict(from_attributes=True)\n\n    class NotConfig:\n        frozen = True\n\n\nclass SelfReferencingModel(BaseModel):\n    submodel: Optional['SelfReferencingModel']\n\n    @property\n    def prop(self) -> None:\n        ...\n\n\nSelfReferencingModel.model_rebuild()\n\nmodel = Model(x=1, y='y')\nModel(x=1, y='y', z='z')\nmodel.x = 2\nmodel.model_validate(model)\n\nself_referencing_model = SelfReferencingModel(submodel=SelfReferencingModel(submodel=None))\n\n\nclass KwargsModel(BaseModel, from_attributes=True):\n    x: float\n    y: str\n\n    class NotConfig:\n        frozen = True\n\n\nkwargs_model = KwargsModel(x=1, y='y')\nKwargsModel(x=1, y='y', z='z')\nkwargs_model.x = 2\nkwargs_model.model_validate(kwargs_model.__dict__)\n\n\nclass InheritingModel(Model):\n    z: int = 1\n\n\nInheritingModel.model_validate(model.__dict__)\n\n\nclass ForwardReferencingModel(Model):\n    future: 'FutureModel'\n\n\nclass FutureModel(Model):\n    pass\n\n\nForwardReferencingModel.model_rebuild()\nfuture_model = FutureModel(x=1, y='a')\nforward_model = ForwardReferencingModel(x=1, y='a', future=future_model)\n\n\nclass NoMutationModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass MutationModel(NoMutationModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nMutationModel(x=1).x = 2\nMutationModel.model_validate(model.__dict__)\n\n\nclass KwargsNoMutationModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsMutationModel(KwargsNoMutationModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsMutationModel(x=1).x = 2\nKwargsMutationModel.model_validate(model.__dict__)\n\n\nclass OverrideModel(Model):\n    x: int\n\n\nOverrideModel(x=1, y='b')\n\n\nclass Mixin:\n    def f(self) -> None:\n        pass\n\n\nclass MultiInheritanceModel(BaseModel, Mixin):\n    pass\n\n\nMultiInheritanceModel().f()\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n\n\nalias_model = AliasModel(y='hello')\nassert alias_model.x == 'hello'\n\n\nclass ClassVarModel(BaseModel):\n    x: int\n    y: ClassVar[int] = 1\n\n\nClassVarModel(x=1)\n\n\n@dataclass(config=dict(validate_assignment=True))\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\nclass TypeAliasAsAttribute(BaseModel):\n    __type_alias_attribute__ = Union[str, bytes]\n\n\nclass NestedModel(BaseModel):\n    class Model(BaseModel):\n        id: str\n\n    model: Model\n\n\n_ = NestedModel.Model\n\n\nDynamicModel = create_model('DynamicModel', __base__=Model)\n\ndynamic_model = DynamicModel(x=1, y='y')\ndynamic_model.x = 2\n\n\nclass FrozenModel(BaseModel):\n    x: int\n\n    model_config = dict(frozen=True)\n\n\nclass NotFrozenModel(FrozenModel):\n    a: int = 1\n\n    model_config = dict(frozen=False, from_attributes=True)\n\n\nNotFrozenModel(x=1).x = 2\nNotFrozenModel.model_validate(model.__dict__)\n\n\nclass KwargsFrozenModel(BaseModel, frozen=True):\n    x: int\n\n\nclass KwargsNotFrozenModel(FrozenModel, frozen=False, from_attributes=True):\n    a: int = 1\n\n\nKwargsNotFrozenModel(x=1).x = 2\nKwargsNotFrozenModel.model_validate(model.__dict__)\n\n\nclass ModelWithSelfField(BaseModel):\n    self: str\n\n\ndef f(name: str) -> str:\n    return name\n\n\nclass ModelWithAllowReuseValidator(BaseModel):\n    name: str\n    normalize_name = field_validator('name')(f)\n\n\nmodel_with_allow_reuse_validator = ModelWithAllowReuseValidator(name='xyz')\n\n\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(cls, name: str) -> str:\n        return name\n\n\ndef _default_factory_str() -> str:\n    return 'x'\n\n\ndef _default_factory_list() -> List[int]:\n    return [1, 2, 3]\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = Field()\n    c: int = Field(...)\n\n    # Default\n    d: int = Field(1)\n\n    # Default factory\n    g: List[int] = Field(default_factory=_default_factory_list)\n    h: str = Field(default_factory=_default_factory_str)\n    i: str = Field(default_factory=lambda: 'test')\n", "tests/mypy/modules/strict_equality.py": "from pydantic import BaseModel\n\n\nclass User(BaseModel):\n    username: str\n\n\nuser = User(username='test')\nprint(user == 'test')\nprint(user.username == int('1'))\nprint(user.username == 'test')\n", "tests/mypy/modules/config_conditional_extra.py": "from pydantic import BaseModel, ConfigDict\n\n\ndef condition() -> bool:\n    return True\n\n\nclass MyModel(BaseModel):\n    model_config = ConfigDict(extra='ignore' if condition() else 'forbid')\n", "tests/mypy/modules/plugin_fail.py": "from typing import Generic, List, Optional, Set, TypeVar, Union\n\nfrom pydantic import BaseModel, ConfigDict, Extra, Field, field_validator\nfrom pydantic.dataclasses import dataclass\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nmodel = Model(x=1, y='y', z='z')\nmodel = Model(x=1)\nmodel.y = 'a'\nModel.from_orm({})\n\n\nclass KwargsModel(BaseModel, alias_generator=None, frozen=True, extra=Extra.forbid):\n    x: int\n    y: str\n\n    def method(self) -> None:\n        pass\n\n\nkwargs_model = KwargsModel(x=1, y='y', z='z')\nkwargs_model = KwargsModel(x=1)\nkwargs_model.y = 'a'\nKwargsModel.from_orm({})\n\n\nclass ForbidExtraModel(BaseModel):\n    model_config = ConfigDict(extra=Extra.forbid)\n\n\nForbidExtraModel(x=1)\n\n\nclass KwargsForbidExtraModel(BaseModel, extra='forbid'):\n    pass\n\n\nKwargsForbidExtraModel(x=1)\n\n\nclass BadExtraModel(BaseModel):\n    model_config = ConfigDict(extra=1)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadExtraModel(BaseModel, extra=1):\n    pass\n\n\nclass BadConfig1(BaseModel):\n    model_config = ConfigDict(from_attributes={})  # type: ignore[typeddict-item]\n\n\nclass KwargsBadConfig1(BaseModel, from_attributes={}):\n    pass\n\n\nclass BadConfig2(BaseModel):\n    model_config = ConfigDict(from_attributes=list)  # type: ignore[typeddict-item]\n\n\nclass KwargsBadConfig2(BaseModel, from_attributes=list):\n    pass\n\n\nclass InheritingModel(Model):\n    model_config = ConfigDict(frozen=False)\n\n\nclass KwargsInheritingModel(KwargsModel, frozen=False):\n    pass\n\n\nclass DefaultTestingModel(BaseModel):\n    # Required\n    a: int\n    b: int = ...\n    c: int = Field(...)\n    d: Union[int, str]\n    e = ...\n\n    # Not required\n    f: Optional[int]\n    g: int = 1\n    h: int = Field(1)\n    i: int = Field(None)\n    j = 1\n\n\nDefaultTestingModel()\n\n\nclass UndefinedAnnotationModel(BaseModel):\n    undefined: Undefined  # noqa F821\n\n\nUndefinedAnnotationModel()\n\n\nModel.model_construct(x=1)\nModel.model_construct(_fields_set={'x'}, x=1, y='2')\nModel.model_construct(x='1', y='2')\n\n# Strict mode fails\ninheriting = InheritingModel(x='1', y='1')\nModel(x='1', y='2')\n\n\nclass Blah(BaseModel):\n    fields_set: Optional[Set[str]] = None\n\n\n# (comment to keep line numbers unchanged)\nT = TypeVar('T')\n\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    error: Optional[str]\n\n\nresponse = Response[Model](data=model, error=None)\nresponse = Response[Model](data=1, error=None)\n\n\nclass AliasModel(BaseModel):\n    x: str = Field(..., alias='y')\n    z: int\n\n\nAliasModel(y=1, z=2)\n\nx_alias = 'y'\n\n\nclass DynamicAliasModel(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nDynamicAliasModel(y='y', z='1')\n\n\nclass DynamicAliasModel2(BaseModel):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n    model_config = ConfigDict(populate_by_name=True)\n\n\nDynamicAliasModel2(y='y', z=1)\nDynamicAliasModel2(x='y', z=1)\n\n\nclass KwargsDynamicAliasModel(BaseModel, populate_by_name=True):\n    x: str = Field(..., alias=x_alias)\n    z: int\n\n\nKwargsDynamicAliasModel(y='y', z=1)\nKwargsDynamicAliasModel(x='y', z=1)\n\n\nclass AliasGeneratorModel(BaseModel):\n    x: int\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')\n\n\nAliasGeneratorModel(x=1)\nAliasGeneratorModel(x_=1)\nAliasGeneratorModel(z=1)\n\n\nclass AliasGeneratorModel2(BaseModel):\n    x: int = Field(..., alias='y')\n\n    model_config = ConfigDict(alias_generator=lambda x: x + '_')  # type: ignore[pydantic-alias]\n\n\nclass UntypedFieldModel(BaseModel):\n    x: int = 1\n    y = 2\n    z = 2  # type: ignore[pydantic-field]\n\n\nAliasGeneratorModel2(x=1)\nAliasGeneratorModel2(y=1, z=1)\n\n\nclass KwargsAliasGeneratorModel(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int\n\n\nKwargsAliasGeneratorModel(x=1)\nKwargsAliasGeneratorModel(x_=1)\nKwargsAliasGeneratorModel(z=1)\n\n\nclass KwargsAliasGeneratorModel2(BaseModel, alias_generator=lambda x: x + '_'):\n    x: int = Field(..., alias='y')\n\n\nKwargsAliasGeneratorModel2(x=1)\nKwargsAliasGeneratorModel2(y=1, z=1)\n\n\nclass CoverageTester(Missing):  # noqa F821\n    def from_orm(self) -> None:\n        pass\n\n\nCoverageTester().from_orm()\n\n\n@dataclass(config={})\nclass AddProject:\n    name: str\n    slug: Optional[str]\n    description: Optional[str]\n\n\np = AddProject(name='x', slug='y', description='z')\n\n\n# Same as Model, but with frozen = True\nclass FrozenModel(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(alias_generator=None, frozen=True, extra=Extra.forbid)\n\n\nfrozenmodel = FrozenModel(x=1, y='b')\nfrozenmodel.y = 'a'\n\n\nclass InheritingModel2(FrozenModel):\n    model_config = ConfigDict(frozen=False)\n\n\ninheriting2 = InheritingModel2(x=1, y='c')\ninheriting2.y = 'd'\n\n\ndef _default_factory() -> str:\n    return 'x'\n\n\ntest: List[str] = []\n\n\nclass FieldDefaultTestingModel(BaseModel):\n    # Default\n    e: int = Field(None)\n    f: int = None\n\n    # Default factory\n    g: str = Field(default_factory=set)\n    h: int = Field(default_factory=_default_factory)\n    i: List[int] = Field(default_factory=list)\n    l_: str = Field(default_factory=3)\n\n    # Default and default factory\n    m: int = Field(default=1, default_factory=list)\n\n\nclass ModelWithAnnotatedValidator(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def noop_validator_with_annotations(self, name: str) -> str:\n        # This is a mistake: the first argument to a validator is the class itself,\n        # like a classmethod.\n        self.instance_method()\n        return name\n\n    def instance_method(self) -> None:\n        ...\n", "tests/mypy/modules/metaclass_args.py": "from pydantic import BaseModel, Field\n\n\nclass ConfigClassUsed(BaseModel):\n    i: int = Field(2, alias='j')\n\n    class Config:\n        populate_by_name = True\n\n\nConfigClassUsed(i=None)\n\n\nclass MetaclassArgumentsNoDefault(BaseModel, populate_by_name=True):\n    i: int = Field(alias='j')\n\n\nMetaclassArgumentsNoDefault(i=None)\n\n\nclass MetaclassArgumentsWithDefault(BaseModel, populate_by_name=True):\n    i: int = Field(2, alias='j')\n\n\nMetaclassArgumentsWithDefault(i=None)\n\n\nclass NoArguments(BaseModel):\n    i: int = Field(2, alias='j')\n\n\nNoArguments(i=1)\nNoArguments(j=None)\n", "tests/mypy/modules/fail1.py": "\"\"\"\nTest mypy failure with missing attribute\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\nfrom pydantic.types import Json\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n    json_list_of_ints: Json[List[int]]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n\nprint(m.age + 'not integer')\nm.json_list_of_ints[0] + 'not integer'\n", "tests/mypy/modules/pydantic_settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    foo: str\n\n\ns = Settings()\n\ns = Settings(foo='test', _case_sensitive=True, _env_prefix='test__', _env_file='test')\n\ns = Settings(foo='test', _case_sensitive=1, _env_prefix=2, _env_file=3)\n\n\nclass SettingsWithConfigDict(BaseSettings):\n    bar: str\n\n    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')\n\n\nscd = SettingsWithConfigDict()\n", "tests/mypy/modules/success.py": "\"\"\"\nTest pydantic's compliance with mypy.\n\nDo a little skipping about with types to demonstrate its usage.\n\"\"\"\nimport os\nfrom datetime import date, datetime, timedelta, timezone\nfrom pathlib import Path, PurePath\nfrom typing import Any, ClassVar, Dict, ForwardRef, Generic, List, Optional, Type, TypeVar\nfrom uuid import UUID\n\nfrom typing_extensions import Annotated, TypedDict\n\nfrom pydantic import (\n    UUID1,\n    AwareDatetime,\n    BaseModel,\n    ConfigDict,\n    DirectoryPath,\n    FilePath,\n    FutureDate,\n    FutureDatetime,\n    ImportString,\n    Json,\n    NaiveDatetime,\n    NegativeFloat,\n    NegativeInt,\n    NonNegativeFloat,\n    NonNegativeInt,\n    NonPositiveFloat,\n    NonPositiveInt,\n    PastDate,\n    PastDatetime,\n    PositiveFloat,\n    PositiveInt,\n    StrictBool,\n    StrictBytes,\n    StrictFloat,\n    StrictInt,\n    StrictStr,\n    UrlConstraints,\n    WrapValidator,\n    create_model,\n    field_validator,\n    model_validator,\n    root_validator,\n    validate_call,\n)\nfrom pydantic.fields import Field, PrivateAttr\nfrom pydantic.json_schema import Examples\nfrom pydantic.networks import AnyUrl\n\n\nclass Flags(BaseModel):\n    strict_bool: StrictBool = False\n\n    def __str__(self) -> str:\n        return f'flag={self.strict_bool}'\n\n\nclass Model(BaseModel):\n    age: int\n    first_name: str = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n\n    @field_validator('age')\n    def check_age(cls, value: int) -> int:\n        assert value < 100, 'too old'\n        return value\n\n    @root_validator(skip_on_failure=True)\n    def root_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        return values\n\n    @root_validator(pre=True, allow_reuse=False)\n    def pre_root_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        return values\n\n\ndef dog_years(age: int) -> int:\n    return age * 7\n\n\ndef day_of_week(dt: datetime) -> int:\n    return dt.date().isoweekday()\n\n\nm = Model(age=21, list_of_ints=[1, 2, 3])\n\nassert m.age == 21, m.age\nm.age = 42\nassert m.age == 42, m.age\nassert m.first_name == 'John', m.first_name\nassert m.last_name is None, m.last_name\nassert m.list_of_ints == [1, 2, 3], m.list_of_ints\n\ndog_age = dog_years(m.age)\nassert dog_age == 294, dog_age\n\n\nModel(age=2, first_name='Woof', last_name='Woof', signup_ts=datetime(2017, 6, 7), list_of_ints=[1, 2, 3])\nm = Model.model_validate(\n    {\n        'age': 2,\n        'first_name': b'Woof',\n        'last_name': b'Woof',\n        'signup_ts': '2017-06-07 00:00',\n        'list_of_ints': [1, '2', b'3'],\n    }\n)\n\nassert m.first_name == 'Woof', m.first_name\nassert m.last_name == 'Woof', m.last_name\nassert m.signup_ts == datetime(2017, 6, 7), m.signup_ts\nassert day_of_week(m.signup_ts) == 3\n\n\ndata = {'age': 10, 'first_name': 'Alena', 'last_name': 'Sousova', 'list_of_ints': [410]}\nm_from_obj = Model.model_validate(data)\n\nassert isinstance(m_from_obj, Model)\nassert m_from_obj.age == 10\nassert m_from_obj.first_name == data['first_name']\nassert m_from_obj.last_name == data['last_name']\nassert m_from_obj.list_of_ints == data['list_of_ints']\n\nm_copy = m_from_obj.model_copy()\n\nassert isinstance(m_copy, Model)\nassert m_copy.age == m_from_obj.age\nassert m_copy.first_name == m_from_obj.first_name\nassert m_copy.last_name == m_from_obj.last_name\nassert m_copy.list_of_ints == m_from_obj.list_of_ints\n\n\nT = TypeVar('T')\n\n\nclass WrapperModel(BaseModel, Generic[T]):\n    payload: T\n\n\nint_instance = WrapperModel[int](payload=1)\nint_instance.payload += 1\nassert int_instance.payload == 2\n\nstr_instance = WrapperModel[str](payload='a')\nstr_instance.payload += 'a'\nassert str_instance.payload == 'aa'\n\nmodel_instance = WrapperModel[Model](payload=m)\nmodel_instance.payload.list_of_ints.append(4)\nassert model_instance.payload.list_of_ints == [1, 2, 3, 4]\n\n\nclass WithField(BaseModel):\n    age: int\n    first_name: str = Field('John', max_length=42)\n\n\n# simple decorator\n@validate_call\ndef foo(a: int, *, c: str = 'x') -> str:\n    return c * a\n\n\nfoo(1, c='thing')\nfoo(1)\n\n\n# nested decorator should not produce an error\n@validate_call(config={'arbitrary_types_allowed': True})\ndef bar(a: int, *, c: str = 'x') -> str:\n    return c * a\n\n\nbar(1, c='thing')\nbar(1)\n\n\nclass Foo(BaseModel):\n    a: int\n\n\nFooRef = ForwardRef('Foo')\n\n\nclass MyConf(BaseModel):\n    str_pyobject: ImportString[Type[date]] = Field(...)\n    callable_pyobject: ImportString[Type[date]] = Field(default=date)\n\n\nconf = MyConf(str_pyobject='datetime.date')\nvar1: date = conf.str_pyobject(2020, 12, 20)\nvar2: date = conf.callable_pyobject(2111, 1, 1)\n\n\nclass MyPrivateAttr(BaseModel):\n    _private_field: str = PrivateAttr()\n\n\nclass PydanticTypes(BaseModel):\n    model_config = ConfigDict(validate_default=True)\n\n    # Boolean\n    my_strict_bool: StrictBool = True\n    # Integer\n    my_positive_int: PositiveInt = 1\n    my_negative_int: NegativeInt = -1\n    my_non_positive_int: NonPositiveInt = -1\n    my_non_negative_int: NonNegativeInt = 1\n    my_strict_int: StrictInt = 1\n    # Float\n    my_positive_float: PositiveFloat = 1.1\n    my_negative_float: NegativeFloat = -1.1\n    my_non_positive_float: NonPositiveFloat = -1.1\n    my_non_negative_float: NonNegativeFloat = 1.1\n    my_strict_float: StrictFloat = 1.1\n    # Bytes\n    my_strict_bytes: StrictBytes = b'pika'\n    # String\n    my_strict_str: StrictStr = 'pika'\n    # ImportString\n    import_string_str: ImportString[Any] = 'datetime.date'  # type: ignore[misc]\n    import_string_callable: ImportString[Any] = date\n    # UUID\n    my_uuid1: UUID1 = UUID('a8098c1a-f86e-11da-bd1a-00112444be1e')\n    my_uuid1_str: UUID1 = 'a8098c1a-f86e-11da-bd1a-00112444be1e'\n    # Path\n    my_file_path: FilePath = Path(__file__)\n    my_file_path_str: FilePath = __file__\n    my_dir_path: DirectoryPath = Path('.')\n    my_dir_path_str: DirectoryPath = '.'\n    # Json\n    my_json: Json[Dict[str, str]] = '{\"hello\": \"world\"}'\n    my_json_list: Json[List[str]] = '[\"hello\", \"world\"]'\n    # Date\n    my_past_date: PastDate = date.today() - timedelta(1)\n    my_future_date: FutureDate = date.today() + timedelta(1)\n    # Datetime\n    my_past_datetime: PastDatetime = datetime.now() - timedelta(1)\n    my_future_datetime: FutureDatetime = datetime.now() + timedelta(1)\n    my_aware_datetime: AwareDatetime = datetime.now(tz=timezone.utc)\n    my_naive_datetime: NaiveDatetime = datetime.now()\n\n\nvalidated = PydanticTypes()\nvalidated.import_string_str(2021, 1, 1)\nvalidated.import_string_callable(2021, 1, 1)\nvalidated.my_uuid1.hex\nvalidated.my_file_path.absolute()\nvalidated.my_file_path_str.absolute()\nvalidated.my_dir_path.absolute()\nvalidated.my_dir_path_str.absolute()\nvalidated.my_json['hello'].capitalize()\nvalidated.my_json_list[0].capitalize()\n\n\nclass UrlModel(BaseModel):\n    x: Annotated[AnyUrl, UrlConstraints(allowed_schemes=['http'])] = Field(default=None)\n    y: Annotated[AnyUrl, UrlConstraints(allowed_schemes=['http'])] = Field(default=None)\n    z: Annotated[AnyUrl, UrlConstraints(allowed_schemes=['s3', 's3n', 's3a'])] = Field(default=None)\n\n\nurl_model = UrlModel(x='http://example.com')\nassert url_model.x.host == 'example.com'\n\n\nclass SomeDict(TypedDict):\n    val: int\n    name: str\n\n\nobj: SomeDict = {\n    'val': 12,\n    'name': 'John',\n}\n\n\nconfig = ConfigDict(title='Record', extra='ignore', str_max_length=1234)\n\n\nclass CustomPath(PurePath):\n    def __init__(self, *args: str):\n        self.path = os.path.join(*args)\n\n    def __fspath__(self) -> str:\n        return f'a/custom/{self.path}'\n\n\nDynamicModel = create_model('DynamicModel')\n\nexamples = Examples({})\n\n\ndef double(value: Any, handler: Any) -> int:\n    return handler(value) * 2\n\n\nclass WrapValidatorModel(BaseModel):\n    x: Annotated[int, WrapValidator(double)]\n\n\nclass Abstract(BaseModel):\n    class_id: ClassVar\n\n\nclass Concrete(Abstract):\n    class_id = 1\n\n\ndef two_dim_shape_validator(v: Dict[str, Any]) -> Dict[str, Any]:\n    assert 'volume' not in v, 'shape is 2d, cannot have volume'\n    return v\n\n\nclass Square(BaseModel):\n    width: float\n    height: float\n\n    free_validator = model_validator(mode='before')(two_dim_shape_validator)\n", "tests/mypy/modules/fail2.py": "\"\"\"\nTest mypy failure with invalid types.\n\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: List[int]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\n\nprint(m.foobar)\n", "tests/mypy/modules/covariant_typevar.py": "from typing import Generic, TypeVar\n\nfrom pydantic import BaseModel\n\nT = TypeVar(\"T\", covariant=True)\n\n\nclass Foo(BaseModel, Generic[T]):\n    value: T\n\n\nclass Bar(Foo[T]): ...\n", "tests/benchmarks/basemodel_eq_performance.py": "from __future__ import annotations\n\nimport dataclasses\nimport enum\nimport gc\nimport itertools\nimport operator\nimport sys\nimport textwrap\nimport timeit\nfrom importlib import metadata\nfrom typing import TYPE_CHECKING, Any, Callable, Generic, Iterable, Sized, TypeVar\n\n# Do not import additional dependencies at top-level\nif TYPE_CHECKING:\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from matplotlib import axes, figure\n\nimport pydantic\n\nPYTHON_VERSION = '.'.join(map(str, sys.version_info))\nPYDANTIC_VERSION = metadata.version('pydantic')\n\n\n# New implementation of pydantic.BaseModel.__eq__ to test\n\n\nclass OldImplementationModel(pydantic.BaseModel, frozen=True):\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, pydantic.BaseModel):\n            # When comparing instances of generic types for equality, as long as all field values are equal,\n            # only require their generic origin types to be equal, rather than exact type equality.\n            # This prevents headaches like MyGeneric(x=1) != MyGeneric[Any](x=1).\n            self_type = self.__pydantic_generic_metadata__['origin'] or self.__class__\n            other_type = other.__pydantic_generic_metadata__['origin'] or other.__class__\n\n            return (\n                self_type == other_type\n                and self.__dict__ == other.__dict__\n                and self.__pydantic_private__ == other.__pydantic_private__\n                and self.__pydantic_extra__ == other.__pydantic_extra__\n            )\n        else:\n            return NotImplemented  # delegate to the other item in the comparison\n\n\nclass DictComprehensionEqModel(pydantic.BaseModel, frozen=True):\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, pydantic.BaseModel):\n            # When comparing instances of generic types for equality, as long as all field values are equal,\n            # only require their generic origin types to be equal, rather than exact type equality.\n            # This prevents headaches like MyGeneric(x=1) != MyGeneric[Any](x=1).\n            self_type = self.__pydantic_generic_metadata__['origin'] or self.__class__\n            other_type = other.__pydantic_generic_metadata__['origin'] or other.__class__\n\n            field_names = type(self).model_fields.keys()\n\n            return (\n                self_type == other_type\n                and ({k: self.__dict__[k] for k in field_names} == {k: other.__dict__[k] for k in field_names})\n                and self.__pydantic_private__ == other.__pydantic_private__\n                and self.__pydantic_extra__ == other.__pydantic_extra__\n            )\n        else:\n            return NotImplemented  # delegate to the other item in the comparison\n\n\nclass ItemGetterEqModel(pydantic.BaseModel, frozen=True):\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, pydantic.BaseModel):\n            # When comparing instances of generic types for equality, as long as all field values are equal,\n            # only require their generic origin types to be equal, rather than exact type equality.\n            # This prevents headaches like MyGeneric(x=1) != MyGeneric[Any](x=1).\n            self_type = self.__pydantic_generic_metadata__['origin'] or self.__class__\n            other_type = other.__pydantic_generic_metadata__['origin'] or other.__class__\n\n            model_fields = type(self).model_fields.keys()\n            getter = operator.itemgetter(*model_fields) if model_fields else lambda _: None\n\n            return (\n                self_type == other_type\n                and getter(self.__dict__) == getter(other.__dict__)\n                and self.__pydantic_private__ == other.__pydantic_private__\n                and self.__pydantic_extra__ == other.__pydantic_extra__\n            )\n        else:\n            return NotImplemented  # delegate to the other item in the comparison\n\n\nclass ItemGetterEqModelFastPath(pydantic.BaseModel, frozen=True):\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, pydantic.BaseModel):\n            # When comparing instances of generic types for equality, as long as all field values are equal,\n            # only require their generic origin types to be equal, rather than exact type equality.\n            # This prevents headaches like MyGeneric(x=1) != MyGeneric[Any](x=1).\n            self_type = self.__pydantic_generic_metadata__['origin'] or self.__class__\n            other_type = other.__pydantic_generic_metadata__['origin'] or other.__class__\n\n            # Perform common checks first\n            if not (\n                self_type == other_type\n                and self.__pydantic_private__ == other.__pydantic_private__\n                and self.__pydantic_extra__ == other.__pydantic_extra__\n            ):\n                return False\n\n            # Fix GH-7444 by comparing only pydantic fields\n            # We provide a fast-path for performance: __dict__ comparison is *much* faster\n            # See tests/benchmarks/test_basemodel_eq_performances.py and GH-7825 for benchmarks\n            if self.__dict__ == other.__dict__:\n                # If the check above passes, then pydantic fields are equal, we can return early\n                return True\n            else:\n                # Else, we need to perform a more detailed, costlier comparison\n                model_fields = type(self).model_fields.keys()\n                getter = operator.itemgetter(*model_fields) if model_fields else lambda _: None\n                return getter(self.__dict__) == getter(other.__dict__)\n        else:\n            return NotImplemented  # delegate to the other item in the comparison\n\n\nK = TypeVar('K')\nV = TypeVar('V')\n\n\n# We need a sentinel value for missing fields when comparing models\n# Models are equals if-and-only-if they miss the same fields, and since None is a legitimate value\n# we can't default to None\n# We use the single-value enum trick to allow correct typing when using a sentinel\nclass _SentinelType(enum.Enum):\n    SENTINEL = enum.auto()\n\n\n_SENTINEL = _SentinelType.SENTINEL\n\n\n@dataclasses.dataclass\nclass _SafeGetItemProxy(Generic[K, V]):\n    \"\"\"Wrapper redirecting `__getitem__` to `get` and a sentinel value\n\n    This makes is safe to use in `operator.itemgetter` when some keys may be missing\n    \"\"\"\n\n    wrapped: dict[K, V]\n\n    def __getitem__(self, key: K, /) -> V | _SentinelType:\n        return self.wrapped.get(key, _SENTINEL)\n\n    def __contains__(self, key: K, /) -> bool:\n        return self.wrapped.__contains__(key)\n\n\nclass SafeItemGetterEqModelFastPath(pydantic.BaseModel, frozen=True):\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, pydantic.BaseModel):\n            # When comparing instances of generic types for equality, as long as all field values are equal,\n            # only require their generic origin types to be equal, rather than exact type equality.\n            # This prevents headaches like MyGeneric(x=1) != MyGeneric[Any](x=1).\n            self_type = self.__pydantic_generic_metadata__['origin'] or self.__class__\n            other_type = other.__pydantic_generic_metadata__['origin'] or other.__class__\n\n            # Perform common checks first\n            if not (\n                self_type == other_type\n                and self.__pydantic_private__ == other.__pydantic_private__\n                and self.__pydantic_extra__ == other.__pydantic_extra__\n            ):\n                return False\n\n            # Fix GH-7444 by comparing only pydantic fields\n            # We provide a fast-path for performance: __dict__ comparison is *much* faster\n            # See tests/benchmarks/test_basemodel_eq_performances.py and GH-7825 for benchmarks\n            if self.__dict__ == other.__dict__:\n                # If the check above passes, then pydantic fields are equal, we can return early\n                return True\n            else:\n                # Else, we need to perform a more detailed, costlier comparison\n                model_fields = type(self).model_fields.keys()\n                getter = operator.itemgetter(*model_fields) if model_fields else lambda _: None\n                return getter(_SafeGetItemProxy(self.__dict__)) == getter(_SafeGetItemProxy(other.__dict__))\n        else:\n            return NotImplemented  # delegate to the other item in the comparison\n\n\nclass ItemGetterEqModelFastPathFallback(pydantic.BaseModel, frozen=True):\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, pydantic.BaseModel):\n            # When comparing instances of generic types for equality, as long as all field values are equal,\n            # only require their generic origin types to be equal, rather than exact type equality.\n            # This prevents headaches like MyGeneric(x=1) != MyGeneric[Any](x=1).\n            self_type = self.__pydantic_generic_metadata__['origin'] or self.__class__\n            other_type = other.__pydantic_generic_metadata__['origin'] or other.__class__\n\n            # Perform common checks first\n            if not (\n                self_type == other_type\n                and self.__pydantic_private__ == other.__pydantic_private__\n                and self.__pydantic_extra__ == other.__pydantic_extra__\n            ):\n                return False\n\n            # Fix GH-7444 by comparing only pydantic fields\n            # We provide a fast-path for performance: __dict__ comparison is *much* faster\n            # See tests/benchmarks/test_basemodel_eq_performances.py and GH-7825 for benchmarks\n            if self.__dict__ == other.__dict__:\n                # If the check above passes, then pydantic fields are equal, we can return early\n                return True\n            else:\n                # Else, we need to perform a more detailed, costlier comparison\n                model_fields = type(self).model_fields.keys()\n                getter = operator.itemgetter(*model_fields) if model_fields else lambda _: None\n                try:\n                    return getter(self.__dict__) == getter(other.__dict__)\n                except KeyError:\n                    return getter(_SafeGetItemProxy(self.__dict__)) == getter(_SafeGetItemProxy(other.__dict__))\n        else:\n            return NotImplemented  # delegate to the other item in the comparison\n\n\nIMPLEMENTATIONS = {\n    # Commented out because it is too slow for benchmark to complete in reasonable time\n    # \"dict comprehension\": DictComprehensionEqModel,\n    'itemgetter': ItemGetterEqModel,\n    'itemgetter+fastpath': ItemGetterEqModelFastPath,\n    # Commented-out because it is too slow to run with run_benchmark_random_unequal\n    #'itemgetter+safety+fastpath': SafeItemGetterEqModelFastPath,\n    'itemgetter+fastpath+safe-fallback': ItemGetterEqModelFastPathFallback,\n}\n\n# Benchmark running & plotting code\n\n\ndef plot_all_benchmark(\n    bases: dict[str, type[pydantic.BaseModel]],\n    sizes: list[int],\n) -> figure.Figure:\n    import matplotlib.pyplot as plt\n\n    n_rows, n_cols = len(BENCHMARKS), 2\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 4))\n\n    for row, (name, benchmark) in enumerate(BENCHMARKS.items()):\n        for col, mimic_cached_property in enumerate([False, True]):\n            plot_benchmark(\n                f'{name}, {mimic_cached_property=}',\n                benchmark,\n                bases=bases,\n                sizes=sizes,\n                mimic_cached_property=mimic_cached_property,\n                ax=axes[row, col],\n            )\n    for ax in axes.ravel():\n        ax.legend()\n    fig.suptitle(f'python {PYTHON_VERSION}, pydantic {PYDANTIC_VERSION}')\n    return fig\n\n\ndef plot_benchmark(\n    title: str,\n    benchmark: Callable,\n    bases: dict[str, type[pydantic.BaseModel]],\n    sizes: list[int],\n    mimic_cached_property: bool,\n    ax: axes.Axes | None = None,\n):\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    ax = ax or plt.gca()\n    arr_sizes = np.asarray(sizes)\n\n    baseline = benchmark(\n        title=f'{title}, baseline',\n        base=OldImplementationModel,\n        sizes=sizes,\n        mimic_cached_property=mimic_cached_property,\n    )\n    ax.plot(sizes, baseline / baseline, label='baseline')\n    for name, base in bases.items():\n        times = benchmark(\n            title=f'{title}, {name}',\n            base=base,\n            sizes=sizes,\n            mimic_cached_property=mimic_cached_property,\n        )\n        mask_valid = ~np.isnan(times)\n        ax.plot(arr_sizes[mask_valid], times[mask_valid] / baseline[mask_valid], label=name)\n\n    ax.set_title(title)\n    ax.set_xlabel('Number of pydantic fields')\n    ax.set_ylabel('Average time relative to baseline')\n    return ax\n\n\nclass SizedIterable(Sized, Iterable):\n    pass\n\n\ndef run_benchmark_nodiff(\n    title: str,\n    base: type[pydantic.BaseModel],\n    sizes: SizedIterable,\n    mimic_cached_property: bool,\n    n_execution: int = 10_000,\n    n_repeat: int = 5,\n) -> np.ndarray:\n    setup = textwrap.dedent(\n        \"\"\"\n        import pydantic\n\n        Model = pydantic.create_model(\n            \"Model\",\n            __base__=Base,\n            **{f'x{i}': (int, i) for i in range(%(size)d)}\n        )\n        left = Model()\n        right = Model()\n        \"\"\"\n    )\n    if mimic_cached_property:\n        # Mimic functools.cached_property editing __dict__\n        # NOTE: we must edit both objects, otherwise the dict don't have the same size and\n        # dict.__eq__ has a very fast path. This makes our timing comparison incorrect\n        # However, the value must be different, otherwise *our* __dict__ == right.__dict__\n        # fast-path prevents our correct code from running\n        setup += textwrap.dedent(\n            \"\"\"\n            object.__setattr__(left, 'cache', None)\n            object.__setattr__(right, 'cache', -1)\n            \"\"\"\n        )\n    statement = 'left == right'\n    namespace = {'Base': base}\n    return run_benchmark(\n        title,\n        setup=setup,\n        statement=statement,\n        n_execution=n_execution,\n        n_repeat=n_repeat,\n        globals=namespace,\n        params={'size': sizes},\n    )\n\n\ndef run_benchmark_first_diff(\n    title: str,\n    base: type[pydantic.BaseModel],\n    sizes: SizedIterable,\n    mimic_cached_property: bool,\n    n_execution: int = 10_000,\n    n_repeat: int = 5,\n) -> np.ndarray:\n    setup = textwrap.dedent(\n        \"\"\"\n        import pydantic\n\n        Model = pydantic.create_model(\n            \"Model\",\n            __base__=Base,\n            **{f'x{i}': (int, i) for i in range(%(size)d)}\n        )\n        left = Model()\n        right = Model(f0=-1) if %(size)d > 0 else Model()\n        \"\"\"\n    )\n    if mimic_cached_property:\n        # Mimic functools.cached_property editing __dict__\n        # NOTE: we must edit both objects, otherwise the dict don't have the same size and\n        # dict.__eq__ has a very fast path. This makes our timing comparison incorrect\n        # However, the value must be different, otherwise *our* __dict__ == right.__dict__\n        # fast-path prevents our correct code from running\n        setup += textwrap.dedent(\n            \"\"\"\n            object.__setattr__(left, 'cache', None)\n            object.__setattr__(right, 'cache', -1)\n            \"\"\"\n        )\n    statement = 'left == right'\n    namespace = {'Base': base}\n    return run_benchmark(\n        title,\n        setup=setup,\n        statement=statement,\n        n_execution=n_execution,\n        n_repeat=n_repeat,\n        globals=namespace,\n        params={'size': sizes},\n    )\n\n\ndef run_benchmark_last_diff(\n    title: str,\n    base: type[pydantic.BaseModel],\n    sizes: SizedIterable,\n    mimic_cached_property: bool,\n    n_execution: int = 10_000,\n    n_repeat: int = 5,\n) -> np.ndarray:\n    setup = textwrap.dedent(\n        \"\"\"\n        import pydantic\n\n        Model = pydantic.create_model(\n            \"Model\",\n            __base__=Base,\n            # shift the range() so that there is a field named size\n            **{f'x{i}': (int, i) for i in range(1, %(size)d + 1)}\n        )\n        left = Model()\n        right = Model(f%(size)d=-1) if %(size)d > 0 else Model()\n        \"\"\"\n    )\n    if mimic_cached_property:\n        # Mimic functools.cached_property editing __dict__\n        # NOTE: we must edit both objects, otherwise the dict don't have the same size and\n        # dict.__eq__ has a very fast path. This makes our timing comparison incorrect\n        # However, the value must be different, otherwise *our* __dict__ == right.__dict__\n        # fast-path prevents our correct code from running\n        setup += textwrap.dedent(\n            \"\"\"\n            object.__setattr__(left, 'cache', None)\n            object.__setattr__(right, 'cache', -1)\n            \"\"\"\n        )\n    statement = 'left == right'\n    namespace = {'Base': base}\n    return run_benchmark(\n        title,\n        setup=setup,\n        statement=statement,\n        n_execution=n_execution,\n        n_repeat=n_repeat,\n        globals=namespace,\n        params={'size': sizes},\n    )\n\n\ndef run_benchmark_random_unequal(\n    title: str,\n    base: type[pydantic.BaseModel],\n    sizes: SizedIterable,\n    mimic_cached_property: bool,\n    n_samples: int = 100,\n    n_execution: int = 1_000,\n    n_repeat: int = 5,\n) -> np.ndarray:\n    import numpy as np\n\n    setup = textwrap.dedent(\n        \"\"\"\n        import pydantic\n\n        Model = pydantic.create_model(\n            \"Model\",\n            __base__=Base,\n            **{f'x{i}': (int, i) for i in range(%(size)d)}\n        )\n        left = Model()\n        right = Model(f%(field)d=-1)\n        \"\"\"\n    )\n    if mimic_cached_property:\n        # Mimic functools.cached_property editing __dict__\n        # NOTE: we must edit both objects, otherwise the dict don't have the same size and\n        # dict.__eq__ has a very fast path. This makes our timing comparison incorrect\n        # However, the value must be different, otherwise *our* __dict__ == right.__dict__\n        # fast-path prevents our correct code from running\n        setup += textwrap.dedent(\n            \"\"\"\n            object.__setattr__(left, 'cache', None)\n            object.__setattr__(right, 'cache', -1)\n            \"\"\"\n        )\n    statement = 'left == right'\n    namespace = {'Base': base}\n    arr_sizes = np.fromiter(sizes, dtype=int)\n    mask_valid_sizes = arr_sizes > 0\n    arr_valid_sizes = arr_sizes[mask_valid_sizes]  # we can't support 0 when sampling the field\n    rng = np.random.default_rng()\n    arr_fields = rng.integers(arr_valid_sizes, size=(n_samples, *arr_valid_sizes.shape))\n    # broadcast the sizes against their sample, so we can iterate on (size, field) tuple\n    # as parameters of the timing test\n    arr_size_broadcast, _ = np.meshgrid(arr_valid_sizes, arr_fields[:, 0])\n\n    results = run_benchmark(\n        title,\n        setup=setup,\n        statement=statement,\n        n_execution=n_execution,\n        n_repeat=n_repeat,\n        globals=namespace,\n        params={'size': arr_size_broadcast.ravel(), 'field': arr_fields.ravel()},\n    )\n    times = np.empty(arr_sizes.shape, dtype=float)\n    times[~mask_valid_sizes] = np.nan\n    times[mask_valid_sizes] = results.reshape((n_samples, *arr_valid_sizes.shape)).mean(axis=0)\n    return times\n\n\nBENCHMARKS = {\n    'All field equals': run_benchmark_nodiff,\n    'First field unequal': run_benchmark_first_diff,\n    'Last field unequal': run_benchmark_last_diff,\n    'Random unequal field': run_benchmark_random_unequal,\n}\n\n\ndef run_benchmark(\n    title: str,\n    setup: str,\n    statement: str,\n    n_execution: int = 10_000,\n    n_repeat: int = 5,\n    globals: dict[str, Any] | None = None,\n    progress_bar: bool = True,\n    params: dict[str, SizedIterable] | None = None,\n) -> np.ndarray:\n    import numpy as np\n    import tqdm.auto as tqdm\n\n    namespace = globals or {}\n    # fast-path\n    if not params:\n        length = 1\n        packed_params = [()]\n    else:\n        length = len(next(iter(params.values())))\n        # This iterator yields a tuple of (key, value) pairs\n        # First, make a list of N iterator over (key, value), where the provided values are iterated\n        param_pairs = [zip(itertools.repeat(name), value) for name, value in params.items()]\n        # Then pack our individual parameter iterator into one\n        packed_params = zip(*param_pairs)\n\n    times = [\n        # Take the min of the repeats as recommended by timeit doc\n        min(\n            timeit.Timer(\n                setup=setup % dict(local_params),\n                stmt=statement,\n                globals=namespace,\n            ).repeat(repeat=n_repeat, number=n_execution)\n        )\n        / n_execution\n        for local_params in tqdm.tqdm(packed_params, desc=title, total=length, disable=not progress_bar)\n    ]\n    gc.collect()\n\n    return np.asarray(times, dtype=float)\n\n\nif __name__ == '__main__':\n    # run with `pdm run tests/benchmarks/test_basemodel_eq_performance.py`\n    import argparse\n    import pathlib\n\n    try:\n        import matplotlib  # noqa: F401\n        import numpy  # noqa: F401\n        import tqdm  # noqa: F401\n    except ImportError as err:\n        raise ImportError(\n            'This benchmark additionally depends on numpy, matplotlib and tqdm. '\n            'Install those in your environment to run the benchmark.'\n        ) from err\n\n    parser = argparse.ArgumentParser(\n        description='Test the performance of various BaseModel.__eq__ implementations fixing GH-7444.'\n    )\n    parser.add_argument(\n        '-o',\n        '--output-path',\n        default=None,\n        type=pathlib.Path,\n        help=(\n            'Output directory or file in which to save the benchmark results. '\n            'If a directory is passed, a default filename is used.'\n        ),\n    )\n    parser.add_argument(\n        '--min-n-fields',\n        type=int,\n        default=0,\n        help=('Test the performance of BaseModel.__eq__ on models with at least this number of fields. Defaults to 0.'),\n    )\n    parser.add_argument(\n        '--max-n-fields',\n        type=int,\n        default=100,\n        help=('Test the performance of BaseModel.__eq__ on models with up to this number of fields. Defaults to 100.'),\n    )\n\n    args = parser.parse_args()\n\n    import matplotlib.pyplot as plt\n\n    sizes = list(range(args.min_n_fields, args.max_n_fields))\n    fig = plot_all_benchmark(IMPLEMENTATIONS, sizes=sizes)\n    plt.tight_layout()\n\n    if args.output_path is None:\n        plt.show()\n    else:\n        if args.output_path.suffix:\n            filepath = args.output_path\n        else:\n            filepath = args.output_path / f\"eq-benchmark_python-{PYTHON_VERSION.replace('.', '-')}.png\"\n        fig.savefig(\n            filepath,\n            dpi=200,\n            facecolor='white',\n            transparent=False,\n        )\n        print(f'wrote {filepath!s}', file=sys.stderr)\n", "tests/benchmarks/test_discriminated_unions.py": "from __future__ import annotations\n\nfrom typing import Literal, Union\n\nimport pytest\nfrom typing_extensions import Annotated\n\nfrom pydantic import BaseModel, Field, TypeAdapter\n\n\nclass NestedState(BaseModel):\n    state_type: Literal['nested']\n    substate: AnyState\n\n\nclass LoopState(BaseModel):\n    state_type: Literal['loop']\n    substate: AnyState\n\n\nclass LeafState(BaseModel):\n    state_type: Literal['leaf']\n\n\nAnyState = Annotated[Union[NestedState, LoopState, LeafState], Field(..., discriminator='state_type')]\n\n\n@pytest.mark.benchmark\ndef test_schema_build() -> None:\n    adapter = TypeAdapter(AnyState)\n    assert adapter.core_schema['schema']['type'] == 'tagged-union'\n\n\nany_state_adapter = TypeAdapter(AnyState)\n\n\ndef build_nested_state(n):\n    if n <= 0:\n        return {'state_type': 'leaf'}\n    else:\n        return {'state_type': 'loop', 'substate': {'state_type': 'nested', 'substate': build_nested_state(n - 1)}}\n\n\n@pytest.mark.benchmark\ndef test_efficiency_with_highly_nested_examples() -> None:\n    # can go much higher, but we keep it reasonably low here for a proof of concept\n    for i in range(1, 12):\n        very_nested_input = build_nested_state(i)\n        any_state_adapter.validate_python(very_nested_input)\n", "tests/benchmarks/test_north_star.py": "\"\"\"\nAn integration-style benchmark of a model with a class of what should\n(hopefully) be some of the most common field types used in pydantic validation.\n\nUsed to gauge overall pydantic performance.\n\"\"\"\n\nimport json\nfrom datetime import date, datetime, time\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import List, Union\nfrom uuid import UUID\n\nimport pytest\nfrom typing_extensions import Annotated, Literal\n\n\n@pytest.fixture(scope='module')\ndef pydantic_type_adapter():\n    from pydantic import BaseModel, Field, TypeAdapter\n    from pydantic.networks import AnyHttpUrl\n\n    class Blog(BaseModel):\n        type: Literal['blog']\n        title: str\n        post_count: int\n        readers: int\n        avg_post_rating: float\n        url: AnyHttpUrl\n\n    class SocialProfileBase(BaseModel):\n        type: Literal['profile']\n        network: Literal['facebook', 'twitter', 'linkedin']\n        username: str\n        join_date: date\n\n    class FacebookProfile(SocialProfileBase):\n        network: Literal['facebook']\n        friends: int\n\n    class TwitterProfile(SocialProfileBase):\n        network: Literal['twitter']\n        followers: int\n\n    class LinkedinProfile(SocialProfileBase):\n        network: Literal['linkedin']\n        connections: Annotated[int, Field(le=500)]\n\n    SocialProfile = Annotated[Union[FacebookProfile, TwitterProfile, LinkedinProfile], Field(discriminator='network')]\n\n    Website = Annotated[Union[Blog, SocialProfile], Field(discriminator='type')]\n\n    class Person(BaseModel):\n        id: UUID\n        name: str\n        height: Decimal\n        entry_created_date: date\n        entry_created_time: time\n        entry_updated_at: datetime\n        websites: List[Website] = Field(default_factory=list)\n\n    return TypeAdapter(List[Person])\n\n\n_NORTH_STAR_DATA_PATH = Path(__file__).parent / 'north_star_data.json'\n\n\n@pytest.fixture(scope='module')\ndef north_star_data_bytes():\n    return _north_star_data_bytes()\n\n\ndef _north_star_data_bytes() -> bytes:\n    from .generate_north_star_data import person_data\n\n    needs_generating = not _NORTH_STAR_DATA_PATH.exists()\n    if needs_generating:\n        data = json.dumps(person_data(length=1000)).encode()\n        _NORTH_STAR_DATA_PATH.write_bytes(data)\n    else:\n        data = _NORTH_STAR_DATA_PATH.read_bytes()\n\n    return data\n\n\ndef test_north_star_validate_json(pydantic_type_adapter, north_star_data_bytes, benchmark):\n    benchmark(pydantic_type_adapter.validate_json, north_star_data_bytes)\n\n\ndef test_north_star_validate_json_strict(pydantic_type_adapter, north_star_data_bytes, benchmark):\n    coerced_north_star_data = pydantic_type_adapter.dump_json(\n        pydantic_type_adapter.validate_json(north_star_data_bytes)\n    )\n    benchmark(pydantic_type_adapter.validate_json, coerced_north_star_data, strict=True)\n\n\ndef test_north_star_dump_json(pydantic_type_adapter, north_star_data_bytes, benchmark):\n    parsed = pydantic_type_adapter.validate_json(north_star_data_bytes)\n    benchmark(pydantic_type_adapter.dump_json, parsed)\n\n\ndef test_north_star_validate_python(pydantic_type_adapter, north_star_data_bytes, benchmark):\n    benchmark(pydantic_type_adapter.validate_python, json.loads(north_star_data_bytes))\n\n\ndef test_north_star_validate_python_strict(pydantic_type_adapter, north_star_data_bytes, benchmark):\n    coerced_north_star_data = pydantic_type_adapter.dump_python(\n        pydantic_type_adapter.validate_json(north_star_data_bytes)\n    )\n    benchmark(pydantic_type_adapter.validate_python, coerced_north_star_data, strict=True)\n\n\ndef test_north_star_dump_python(pydantic_type_adapter, north_star_data_bytes, benchmark):\n    parsed = pydantic_type_adapter.validate_python(json.loads(north_star_data_bytes))\n    benchmark(pydantic_type_adapter.dump_python, parsed)\n\n\ndef test_north_star_json_loads(north_star_data_bytes, benchmark):\n    benchmark(json.loads, north_star_data_bytes)\n\n\ndef test_north_star_json_dumps(north_star_data_bytes, benchmark):\n    parsed = json.loads(north_star_data_bytes)\n    benchmark(json.dumps, parsed)\n", "tests/benchmarks/test_fastapi_startup_simple.py": "\"\"\"https://github.com/pydantic/pydantic/issues/6768\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Tuple\nfrom uuid import UUID\n\nfrom annotated_types import Gt\nfrom typing_extensions import Annotated\n\nfrom pydantic import AnyUrl, BaseModel, EmailStr, TypeAdapter\nfrom pydantic.functional_validators import AfterValidator\nfrom pydantic.types import StringConstraints\n\n\ndef test_fastapi_startup_perf(benchmark: Callable[[Callable[[], Any]], None]):\n    def run() -> None:\n        class User(BaseModel):\n            id: int\n            username: str\n            email: EmailStr\n            full_name: str | None = None\n\n        class Address(BaseModel):\n            street: str\n            city: str\n            state: Annotated[str, AfterValidator(lambda x: x.upper())]\n            postal_code: Annotated[str, StringConstraints(min_length=5, max_length=5, pattern=r'[A-Z0-9]+')]\n\n        class Product(BaseModel):\n            id: int\n            name: str\n            price: Annotated[float, Gt(0)]\n            description: str | None = None\n\n        class BlogPost(BaseModel):\n            title: Annotated[str, StringConstraints(pattern=r'[A-Za-z0-9]+')]\n            content: str\n            author: User\n            published: bool = False\n\n        class Website(BaseModel):\n            name: str\n            url: AnyUrl\n            description: str | None = None\n\n        class Order(BaseModel):\n            order_id: str\n            customer: User\n            shipping_address: Address\n            products: list[Product]\n\n        class Comment(BaseModel):\n            text: str\n            author: User\n            post: BlogPost\n            created_at: datetime\n\n        class Event(BaseModel):\n            event_id: UUID\n            name: str\n            date: datetime\n            location: str\n\n        class Category(BaseModel):\n            name: str\n            description: str | None = None\n\n        ReviewGroup = List[Dict[Tuple[User, Product], Comment]]\n\n        data_models = [\n            User,\n            Address,\n            Product,\n            BlogPost,\n            Website,\n            Order,\n            Comment,\n            Event,\n            Category,\n            ReviewGroup,\n        ]\n\n        for _ in range(5):  # FastAPI creates a new TypeAdapter for each endpoint\n            for model in data_models:\n                TypeAdapter(model)\n\n    benchmark(run)\n\n\nif __name__ == '__main__':\n    # run with `pdm run tests/benchmarks/test_fastapi_startup_simple.py`\n    import cProfile\n    import sys\n    import time\n\n    print(f'Python version: {sys.version}')\n    if sys.argv[-1] == 'cProfile':\n        cProfile.run(\n            'test_fastapi_startup_perf(lambda f: f())',\n            sort='tottime',\n            filename=Path(__file__).name.strip('.py') + '.cprof',\n        )\n    else:\n        start = time.perf_counter()\n        test_fastapi_startup_perf(lambda f: f())\n        end = time.perf_counter()\n        print(f'Time taken: {end - start:.6f}s')\n", "tests/benchmarks/test_fastapi_startup_generics.py": "\"\"\"https://github.com/pydantic/pydantic/issues/6768\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Any, Generic, List, TypeVar\n\nfrom typing_extensions import Annotated\n\nfrom pydantic import BaseModel, TypeAdapter, create_model\nfrom pydantic.fields import FieldInfo\n\nTYPES_DEFAULTS = {int: 0, str: '', bool: False}  # some dummy basic types with defaults for some fields\nTYPES = [*TYPES_DEFAULTS.keys()]\n# these are set low to minimise test time, they're increased below in the cProfile call\nINNER_DATA_MODEL_COUNT = 5\nOUTER_DATA_MODEL_COUNT = 5\n\n\ndef create_data_models() -> list[Any]:\n    # Create varying inner models with different sizes and fields (not actually realistic)\n    models = []\n    for i in range(INNER_DATA_MODEL_COUNT):\n        fields = {}\n        for j in range(i):\n            type_ = TYPES[j % len(TYPES)]\n            type_default = TYPES_DEFAULTS[type_]\n            if j % 4 == 0:\n                type_ = List[type_]\n                type_default = []\n\n            default = ... if j % 2 == 0 else type_default\n            fields[f'f{j}'] = (type_, default)\n        models.append(create_model(f'M1{i}', **fields))\n\n    # Crate varying outer models where some fields use the inner models (not really realistic)\n    models_with_nested = []\n    for i in range(OUTER_DATA_MODEL_COUNT):\n        fields = {}\n        for j in range(i):\n            type_ = models[j % len(models)] if j % 2 == 0 else TYPES[j % len(TYPES)]\n            if j % 4 == 0:\n                type_ = List[type_]\n            fields[f'f{j}'] = (type_, ...)\n        models_with_nested.append(create_model(f'M2{i}', **fields))\n\n    return [*models, *models_with_nested]\n\n\ndef test_fastapi_startup_perf(benchmark: Any):\n    data_models = create_data_models()\n    # API models for reading / writing the different data models\n    T = TypeVar('T')\n\n    class GetModel(BaseModel, Generic[T]):\n        res: T\n\n    class GetModel2(GetModel[T], Generic[T]):\n        foo: str\n        bar: str\n\n    class GetManyModel(BaseModel, Generic[T]):\n        res: list[T]\n\n    class GetManyModel2(GetManyModel[T], Generic[T]):\n        foo: str\n        bar: str\n\n    class GetManyModel3(BaseModel, Generic[T]):\n        res: dict[str, T]\n\n    class GetManyModel4(BaseModel, Generic[T]):\n        res: dict[str, list[T]]\n\n    class PutModel(BaseModel, Generic[T]):\n        data: T\n\n    class PutModel2(PutModel[T], Generic[T]):\n        foo: str\n        bar: str\n\n    class PutManyModel(BaseModel, Generic[T]):\n        data: list[T]\n\n    class PutManyModel2(PutManyModel[T], Generic[T]):\n        foo: str\n        bar: str\n\n    api_models: list[Any] = [\n        GetModel,\n        GetModel2,\n        GetManyModel,\n        GetManyModel2,\n        GetManyModel3,\n        GetManyModel4,\n        PutModel,\n        PutModel2,\n        PutManyModel,\n        PutManyModel2,\n    ]\n\n    assert len(data_models) == INNER_DATA_MODEL_COUNT + OUTER_DATA_MODEL_COUNT\n\n    def bench():\n        concrete_api_models = []\n        adapters = []\n        for outer_api_model in api_models:\n            for data_model in data_models:\n                concrete_api_model = outer_api_model[\n                    data_model\n                ]  # Would be used eg as request or response body in FastAPI\n                concrete_api_models.append(concrete_api_model)\n\n                # Emulate FastAPI creating its TypeAdapters\n                adapt = TypeAdapter(Annotated[concrete_api_model, FieldInfo(description='foo')])\n                adapters.append(adapt)\n                adapt = TypeAdapter(Annotated[concrete_api_model, FieldInfo(description='bar')])\n                adapters.append(adapt)\n\n        assert len(concrete_api_models) == len(data_models) * len(api_models)\n        assert len(adapters) == len(concrete_api_models) * 2\n\n    benchmark(bench)\n\n\nif __name__ == '__main__':\n    # run with `pdm run tests/benchmarks/test_fastapi_startup.py`\n    import cProfile\n    import sys\n    import time\n\n    INNER_DATA_MODEL_COUNT = 50\n    OUTER_DATA_MODEL_COUNT = 50\n    print(f'Python version: {sys.version}')\n    if sys.argv[-1] == 'cProfile':\n        cProfile.run(\n            'test_fastapi_startup_perf(lambda f: f())',\n            sort='tottime',\n            filename=Path(__file__).name.strip('.py') + '.cprof',\n        )\n    else:\n        start = time.perf_counter()\n        test_fastapi_startup_perf(lambda f: f())\n        end = time.perf_counter()\n        print(f'Time taken: {end - start:.2f}s')\n", "tests/benchmarks/__init__.py": "", "tests/benchmarks/test_schema_build.py": "from typing import Literal, Union\n\nimport pytest\nfrom typing_extensions import Annotated\n\nfrom pydantic import BaseModel, Discriminator\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass(frozen=True, kw_only=True)\nclass Cat:\n    type: Literal['cat'] = 'cat'\n\n\n@dataclass(frozen=True, kw_only=True)\nclass Dog:\n    type: Literal['dog'] = 'dog'\n\n\n@dataclass(frozen=True, kw_only=True)\nclass NestedDataClass:\n    animal: Annotated[Union[Cat, Dog], Discriminator('type')]\n\n\nclass NestedModel(BaseModel):\n    animal: Annotated[Union[Cat, Dog], Discriminator('type')]\n\n\n@pytest.mark.benchmark\ndef test_construct_schema():\n    @dataclass(frozen=True, kw_only=True)\n    class Root:\n        data_class: NestedDataClass\n        model: NestedModel\n", "tests/benchmarks/generate_north_star_data.py": "from datetime import datetime\nfrom typing import Any, Callable, List, TypeVar, Union\n\nfrom faker import Faker\n\nf = Faker()\nFaker.seed(0)\n\n\nT = TypeVar('T')\n\n## Helper functions\n\n# by default faker uses upper bound of now for datetime, which\n# is not helpful for reproducing benchmark data\n_END_DATETIME = datetime(2023, 1, 1, 0, 0, 0, 0)\n\n\ndef one_of(*callables: Callable[[], Any]) -> Any:\n    return f.random.choice(callables)()\n\n\ndef list_of(callable: Callable[[], T], max_length: int) -> List[T]:\n    return [callable() for _ in range(f.random_int(max=max_length))]\n\n\ndef lax_int(*args: Any, **kwargs: Any) -> Union[int, float, str]:\n    return f.random.choice((int, float, str))(f.random_int(*args, **kwargs))\n\n\ndef lax_float(*args: Any, **kwargs: Any) -> Union[int, float, str]:\n    return f.random.choice((int, float, str))(f.pyfloat(*args, **kwargs))\n\n\ndef time_seconds() -> int:\n    dt = f.date_time(end_datetime=_END_DATETIME)\n    midnight = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n    return (dt - midnight).total_seconds()\n\n\ndef time_microseconds() -> float:\n    return float(time_seconds()) + (f.random_int(max=999999) * 1e-6)\n\n\ndef time_string() -> str:\n    return f.time()\n\n\ndef lax_time() -> Union[int, float, str]:\n    return one_of(time_seconds, time_microseconds, time_string)\n\n\ndef date_string() -> str:\n    return f.date(end_datetime=_END_DATETIME).format('%Y-%m-%d')\n\n\ndef datetime_timestamp() -> int:\n    dt = f.date_time(end_datetime=_END_DATETIME)\n    midnight = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n    return (dt - midnight).total_seconds()\n\n\ndef datetime_microseconds() -> float:\n    return float(datetime_timestamp()) + (f.random_int(max=999999) * 1e-6)\n\n\ndef datetime_str() -> str:\n    return f.date_time(end_datetime=_END_DATETIME).isoformat()\n\n\ndef lax_datetime() -> Union[int, float, str]:\n    return one_of(datetime_timestamp, datetime_microseconds, datetime_str)\n\n\n## Sample data generators\n\n\ndef blog() -> dict:\n    return {\n        'type': 'blog',\n        'title': f.text(max_nb_chars=40),\n        'post_count': lax_int(),\n        'readers': lax_int(),\n        'avg_post_rating': lax_float(min_value=0, max_value=5),\n        'url': f.url(),\n    }\n\n\ndef social_profile() -> dict:\n    return {\n        'type': 'profile',\n        'username': f.user_name(),\n        'join_date': date_string(),\n        **one_of(facebook_profile, twitter_profile, linkedin_profile),\n    }\n\n\ndef facebook_profile() -> dict:\n    return {'network': 'facebook', 'friends': lax_int()}\n\n\ndef twitter_profile() -> dict:\n    return {'network': 'twitter', 'followers': lax_int()}\n\n\ndef linkedin_profile() -> dict:\n    return {'network': 'linkedin', 'connections': min(f.random_int(), 500)}\n\n\ndef website() -> dict:\n    return one_of(blog, social_profile)\n\n\ndef person() -> dict:\n    return {\n        'id': f.uuid4(),\n        'name': f.name(),\n        'height': str(f.pydecimal(min_value=1, max_value=2, right_digits=2)),\n        'entry_created_date': date_string(),\n        'entry_created_time': lax_time(),\n        'entry_updated_at': lax_datetime(),\n        'websites': list_of(website, max_length=5),\n    }\n\n\ndef person_data(length: int) -> List[dict]:\n    return [person() for _ in range(length)]\n", "tests/plugin/example_plugin.py": "from pydantic import BaseModel\n\n\nclass MyModel(BaseModel):\n    x: int\n\n\nm = MyModel(x='10')\nif m.x != 10:\n    raise ValueError('m.x should be 10')\n\nlog = []\n\n\nclass ValidatePythonHandler:\n    def on_enter(self, *args, **kwargs) -> None:\n        log.append(f'on_enter args={args} kwargs={kwargs}')\n\n    def on_success(self, result) -> None:\n        log.append(f'on_success result={result}')\n\n    def on_error(self, error) -> None:\n        log.append(f'on_error error={error}')\n\n\nclass Plugin:\n    def new_schema_validator(self, schema, schema_type, schema_type_path, schema_kind, config, plugin_settings):\n        return ValidatePythonHandler(), None, None\n\n\nplugin = Plugin()\n", "tests/plugin/test_plugin.py": "import os\n\nimport pytest\n\npytestmark = pytest.mark.skipif(not os.getenv('TEST_PLUGIN'), reason='Test only with `TEST_PLUGIN` env var set.')\n\n\ndef test_plugin_usage():\n    from pydantic import BaseModel\n\n    class MyModel(BaseModel):\n        x: int\n        y: str\n\n    m = MyModel(x='10', y='hello')\n    assert m.x == 10\n    assert m.y == 'hello'\n\n    from example_plugin import log\n\n    assert log == [\n        \"on_enter args=({'x': '10', 'y': 'hello'},) kwargs={'self_instance': MyModel()}\",\n        \"on_success result=x=10 y='hello'\",\n    ]\n", "tests/pyright/pyright_example.py": "\"\"\"\nThis file is used to test pyright's ability to check pydantic code.\n\"\"\"\n\nfrom functools import cached_property\nfrom typing import List\n\nfrom pydantic import BaseModel, Field, computed_field\n\n\nclass MyModel(BaseModel):\n    x: str\n    y: List[int]\n\n\nm1 = MyModel(x='hello', y=[1, 2, 3])\n\nm2 = MyModel(x='hello')  # pyright: ignore\n\n\nclass Knight(BaseModel):\n    title: str = Field(default='Sir Lancelot')  # this is okay\n    age: int = Field(23)  # this works fine at runtime but will case an error for pyright\n\n\nk = Knight()  # pyright: ignore\n\n\nclass Square(BaseModel):\n    side: float\n\n    @computed_field\n    @property\n    def area(self) -> float:\n        return self.side**2\n\n    @area.setter\n    def area(self, area: float) -> None:\n        self.side = area**0.5\n\n\nsq = Square(side=10)\ny = 12.4 + sq.area\nz = 'x' + sq.area  # type: ignore\n\n\nclass Square2(BaseModel):\n    side: float\n\n    @computed_field\n    @cached_property\n    def area(self) -> float:\n        return self.side**2\n\n\nsq = Square(side=10)\ny = 12.4 + sq.area\nz = 'x' + sq.area  # type: ignore\n", "tests/pyright/pipeline_api.py": "import datetime\nfrom typing import Annotated\n\nfrom pydantic.experimental.pipeline import validate_as\n\n# this test works by adding type ignores and having pyright fail with\n# an unused type ignore error if the type checking isn't working\nAnnotated[str, validate_as(int)]  # type: ignore\nAnnotated[str, validate_as(str).transform(lambda x: int(x))]  # type: ignore\nAnnotated[float, validate_as(float).gt(0)]  # should be able to compare float to int\nAnnotated[datetime.datetime, validate_as(datetime.datetime).datetime_tz_naive()]\nAnnotated[datetime.datetime, validate_as(str).datetime_tz_naive()]  # type: ignore\nAnnotated[\n    datetime.datetime,\n    (\n        validate_as(str).transform(str.strip).validate_as(datetime.datetime).datetime_tz_naive()\n        | validate_as(int).transform(datetime.datetime.fromtimestamp).datetime_tz_aware()\n    ),\n]\n", "release/make_history.py": "from __future__ import annotations as _annotations\n\nimport argparse\nimport json\nimport re\nimport subprocess\nimport sys\nfrom datetime import date\nfrom pathlib import Path\n\nimport requests\n\n\ndef main():\n    root_dir = Path(__file__).parent.parent\n\n    parser = argparse.ArgumentParser()\n    # For easier iteration, can generate the release notes without saving\n    parser.add_argument('--preview', help='print preview of release notes to terminal without saving to HISTORY.md')\n    args = parser.parse_args()\n\n    if args.preview:\n        new_version = args.preview\n    else:\n        version_file = root_dir / 'pydantic' / 'version.py'\n        new_version = re.search(r\"VERSION = '(.*)'\", version_file.read_text()).group(1)\n\n    history_path = root_dir / 'HISTORY.md'\n    history_content = history_path.read_text()\n\n    # use ( to avoid matching beta versions\n    if f'## v{new_version} (' in history_content:\n        print(f'WARNING: v{new_version} already in history, stopping')\n        sys.exit(1)\n\n    date_today_str = f'{date.today():%Y-%m-%d}'\n    title = f'v{new_version} ({date_today_str})'\n    notes = get_notes(new_version)\n    new_chunk = (\n        f'## {title}\\n\\n'\n        f'[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v{new_version})\\n\\n'\n        f'{notes}\\n\\n'\n    )\n    if args.preview:\n        print(new_chunk)\n        return\n    history = new_chunk + history_content\n\n    history_path.write_text(history)\n    print(f'\\nSUCCESS: added \"{title}\" section to {history_path.relative_to(root_dir)}')\n\n    citation_path = root_dir / 'CITATION.cff'\n    citation_text = citation_path.read_text()\n    citation_text = re.sub(r'(?<=\\nversion: ).*', f'v{new_version}', citation_text)\n    citation_text = re.sub(r'(?<=date-released: ).*', date_today_str, citation_text)\n    citation_path.write_text(citation_text)\n    print(\n        f'SUCCESS: updated version=v{new_version} and date-released={date_today_str} in {citation_path.relative_to(root_dir)}'\n    )\n\n\ndef get_notes(new_version: str) -> str:\n    last_tag = get_last_tag()\n    auth_token = get_gh_auth_token()\n\n    data = {'target_committish': 'main', 'previous_tag_name': last_tag, 'tag_name': f'v{new_version}'}\n    response = requests.post(\n        'https://api.github.com/repos/pydantic/pydantic/releases/generate-notes',\n        headers={\n            'Accept': 'application/vnd.github+json',\n            'Authorization': f'Bearer {auth_token}',\n            'x-github-api-version': '2022-11-28',\n        },\n        data=json.dumps(data),\n    )\n    response.raise_for_status()\n\n    body = response.json()['body']\n    body = body.replace('<!-- Release notes generated using configuration in .github/release.yml at main -->\\n\\n', '')\n\n    # Add one level to all headers so they match HISTORY.md, and add trailing newline\n    body = re.sub(pattern='^(#+ .+?)$', repl=r'#\\1\\n', string=body, flags=re.MULTILINE)\n\n    # Ensure a blank line before headers\n    body = re.sub(pattern='([^\\n])(\\n#+ .+?\\n)', repl=r'\\1\\n\\2', string=body)\n\n    # Render PR links nicely\n    body = re.sub(\n        pattern='https://github.com/pydantic/pydantic/pull/(\\\\d+)',\n        repl=r'[#\\1](https://github.com/pydantic/pydantic/pull/\\1)',\n        string=body,\n    )\n\n    # Remove \"full changelog\" link\n    body = re.sub(\n        pattern=r'\\*\\*Full Changelog\\*\\*: https://.*$',\n        repl='',\n        string=body,\n    )\n\n    return body.strip()\n\n\ndef get_last_tag():\n    return run('git', 'describe', '--tags', '--abbrev=0')\n\n\ndef get_gh_auth_token():\n    return run('gh', 'auth', 'token')\n\n\ndef run(*args: str) -> str:\n    p = subprocess.run(args, stdout=subprocess.PIPE, check=True, encoding='utf-8')\n    return p.stdout.strip()\n\n\nif __name__ == '__main__':\n    main()\n"}