{"setup.py": "#!/usr/bin/env python\nimport codecs\nimport os.path\nimport re\n\nfrom setuptools import find_packages, setup\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read(*parts):\n    return codecs.open(os.path.join(here, *parts), 'r').read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(\n        r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\", version_file, re.M\n    )\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(\"Unable to find version string.\")\n\n\nrequires = [\n    'jmespath>=0.7.1,<2.0.0',\n    'python-dateutil>=2.1,<3.0.0',\n    # Prior to Python 3.10, Python doesn't require openssl 1.1.1\n    # but urllib3 2.0+ does. This means all botocore users will be\n    # broken by default on Amazon Linux 2 and AWS Lambda without this pin.\n    'urllib3>=1.25.4,<1.27 ; python_version < \"3.10\"',\n    'urllib3>=1.25.4,!=2.2.0,<3 ; python_version >= \"3.10\"',\n]\n\nextras_require = {\n    'crt': ['awscrt==0.20.11'],\n}\n\nsetup(\n    name='botocore',\n    version=find_version(\"botocore\", \"__init__.py\"),\n    description='Low-level, data-driven core of boto 3.',\n    long_description=open('README.rst').read(),\n    author='Amazon Web Services',\n    url='https://github.com/boto/botocore',\n    scripts=[],\n    packages=find_packages(exclude=['tests*']),\n    package_data={\n        'botocore': ['cacert.pem', 'data/*.json', 'data/*/*.json'],\n        'botocore.vendored.requests': ['*.pem'],\n    },\n    include_package_data=True,\n    install_requires=requires,\n    extras_require=extras_require,\n    license=\"Apache License 2.0\",\n    python_requires=\">= 3.8\",\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'Intended Audience :: System Administrators',\n        'Natural Language :: English',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3 :: Only',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n    ],\n)\n", "docs/source/conf.py": "# -*- coding: utf-8 -*-\n#\n# botocore documentation build configuration file, created by\n# sphinx-quickstart on Sun Dec  2 07:26:23 2012.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport datetime, sys, os\nfrom botocore.session import get_session\nfrom botocore.docs import generate_docs\nfrom botocore.docs.translator import BotoHTML5Translator\n\ngenerate_docs(os.path.dirname(os.path.abspath(__file__)), get_session())\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath('.'))\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = ['sphinx.ext.autodoc', 'sphinx_copybutton', 'sphinx_remove_toctrees']\n\n# Remove service docs from toctree to speed up writing phase.\nremove_from_toctrees = ['reference/services/*/**/*']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'botocore'\ncurrent_year = datetime.date.today().year\ncopyright = f'{current_year}, Amazon Web Services, Inc'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '1.34.1'\n# The full version, including alpha/beta/rc tags.\nrelease = '1.34.132'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"default\"\npygments_dark_style = \"monokai\"\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = 'furo'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\nhtml_theme_options = {\n    \"footer_icons\": [\n        {\n            \"name\": \"GitHub\",\n            \"url\": \"https://github.com/boto/botocore\",\n            \"html\": \"\"\"\n                <svg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" viewBox=\"0 0 16 16\">\n                    <path fill-rule=\"evenodd\" d=\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z\"></path>\n                </svg>\n            \"\"\",\n            \"class\": \"\",\n        },\n    ],\n    \"light_logo\": \"logos/aws_light_theme_logo.svg\",\n    \"dark_logo\": \"logos/aws_dark_theme_logo.svg\",\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# List of custom CSS files relative to _static directory.\nhtml_css_files = [\n    'css/custom.css',\n]\n\n# List of custom JS files relative to _static directory.\nhtml_js_files = [\n    'js/custom.js',\n]\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\nhtml_show_sourcelink = False\nhtml_sidebars = {\n    \"**\": [\n        \"sidebar/close-icon.html\",\n        \"sidebar/brand.html\",\n        \"sidebar/search.html\",\n        \"sidebar/scroll-start.html\",\n        \"sidebar/feedback.html\",\n        \"sidebar/navigation.html\",\n        \"sidebar/scroll-end.html\",\n    ]\n}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'botocoredoc'\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  ('index', 'botocore.tex', u'botocore Documentation',\n   u'Mitch Garnaat', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'botocore', u'botocore Documentation',\n     [u'Mitch Garnaat'], 3)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ------------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  ('index', 'botocore', u'botocore Documentation',\n   u'Mitch Garnaat', 'botocore', 'One line description of project.',\n   'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n\n\ndef setup(app):\n    # Register our custom HTML translator.\n    app.set_translator(\"html\", BotoHTML5Translator)\n", "tests/__init__.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport binascii\nimport contextlib\nimport datetime\nimport os\nimport platform\nimport random\nimport select\nimport shutil\nimport sys\nimport tempfile\nimport time\nimport unittest\nfrom contextlib import ContextDecorator\nfrom io import BytesIO\nfrom subprocess import PIPE, Popen\nfrom unittest import mock\n\nfrom dateutil.tz import tzlocal\n\nimport botocore.loaders\nimport botocore.session\nfrom botocore import credentials, utils\nfrom botocore.awsrequest import AWSResponse\nfrom botocore.compat import HAS_CRT, parse_qs, urlparse\nfrom botocore.stub import Stubber\n\n_LOADER = botocore.loaders.Loader()\n\n\ndef _all_services():\n    session = botocore.session.Session()\n    service_names = session.get_available_services()\n    return [session.get_service_model(name) for name in service_names]\n\n\n# Only compute our service models once\nALL_SERVICES = _all_services()\n\n\ndef skip_unless_has_memory_collection(cls):\n    \"\"\"Class decorator to skip tests that require memory collection.\n\n    Any test that uses memory collection (such as the resource leak tests)\n    can decorate their class with skip_unless_has_memory_collection to\n    indicate that if the platform does not support memory collection\n    the tests should be skipped.\n    \"\"\"\n    if platform.system() not in ['Darwin', 'Linux']:\n        return unittest.skip('Memory tests only supported on mac/linux.')(cls)\n    return cls\n\n\ndef skip_if_windows(reason):\n    \"\"\"Decorator to skip tests that should not be run on windows.\n    Example usage:\n        @skip_if_windows(\"Not valid\")\n        def test_some_non_windows_stuff(self):\n            self.assertEqual(...)\n    \"\"\"\n\n    def decorator(func):\n        return unittest.skipIf(\n            platform.system() not in ['Darwin', 'Linux'], reason\n        )(func)\n\n    return decorator\n\n\ndef requires_crt(reason=None):\n    if reason is None:\n        reason = \"Test requires awscrt to be installed\"\n\n    def decorator(func):\n        return unittest.skipIf(not HAS_CRT, reason)(func)\n\n    return decorator\n\n\ndef random_chars(num_chars):\n    \"\"\"Returns random hex characters.\n\n    Useful for creating resources with random names.\n\n    \"\"\"\n    return binascii.hexlify(os.urandom(int(num_chars / 2))).decode('ascii')\n\n\ndef create_session(**kwargs):\n    # Create a Session object.  By default,\n    # the _LOADER object is used as the loader\n    # so that we reused the same models across tests.\n    session = botocore.session.Session(**kwargs)\n    session.register_component('data_loader', _LOADER)\n    session.set_config_variable('credentials_file', 'noexist/foo/botocore')\n    return session\n\n\n@contextlib.contextmanager\ndef temporary_file(mode):\n    \"\"\"This is a cross platform temporary file creation.\n\n    tempfile.NamedTemporary file on windows creates a secure temp file\n    that can't be read by other processes and can't be opened a second time.\n\n    For tests, we generally *want* them to be read multiple times.\n    The test fixture writes the temp file contents, the test reads the\n    temp file.\n\n    \"\"\"\n    temporary_directory = tempfile.mkdtemp()\n    basename = 'tmpfile-{}-{}'.format(\n        int(time.time()), random.randint(1, 1000)\n    )\n    full_filename = os.path.join(temporary_directory, basename)\n    open(full_filename, 'w').close()\n    try:\n        with open(full_filename, mode) as f:\n            yield f\n    finally:\n        shutil.rmtree(temporary_directory)\n\n\nclass BaseEnvVar(unittest.TestCase):\n    def setUp(self):\n        # Automatically patches out os.environ for you\n        # and gives you a self.environ attribute that simulates\n        # the environment.  Also will automatically restore state\n        # for you in tearDown()\n        self.environ = {}\n        self.environ_patch = mock.patch('os.environ', self.environ)\n        self.environ_patch.start()\n\n    def tearDown(self):\n        self.environ_patch.stop()\n\n\nclass BaseSessionTest(BaseEnvVar):\n    \"\"\"Base class used to provide credentials.\n\n    This class can be used as a base class that want to use a real\n    session class but want to be completely isolated from the\n    external environment (including environment variables).\n\n    This class will also set credential vars so you can make fake\n    requests to services.\n\n    \"\"\"\n\n    def setUp(self, **environ):\n        super().setUp()\n        self.environ['AWS_ACCESS_KEY_ID'] = 'access_key'\n        self.environ['AWS_SECRET_ACCESS_KEY'] = 'secret_key'\n        self.environ['AWS_CONFIG_FILE'] = 'no-exist-foo'\n        self.environ.update(environ)\n        self.session = create_session()\n        self.session.config_filename = 'no-exist-foo'\n\n\n@skip_unless_has_memory_collection\nclass BaseClientDriverTest(unittest.TestCase):\n    INJECT_DUMMY_CREDS = False\n\n    def setUp(self):\n        self.driver = ClientDriver()\n        env = None\n        if self.INJECT_DUMMY_CREDS:\n            env = {'AWS_ACCESS_KEY_ID': 'foo', 'AWS_SECRET_ACCESS_KEY': 'bar'}\n        self.driver.start(env=env)\n\n    def cmd(self, *args):\n        self.driver.cmd(*args)\n\n    def send_cmd(self, *args):\n        self.driver.send_cmd(*args)\n\n    def record_memory(self):\n        self.driver.record_memory()\n\n    @property\n    def memory_samples(self):\n        return self.driver.memory_samples\n\n    def tearDown(self):\n        self.driver.stop()\n\n\nclass ClientDriver:\n    CLIENT_SERVER = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), 'cmd-runner'\n    )\n\n    def __init__(self):\n        self._popen = None\n        self.memory_samples = []\n\n    def _get_memory_with_ps(self, pid):\n        # It would be better to eventually switch to psutil,\n        # which should allow us to test on windows, but for now\n        # we'll just use ps and run on POSIX platforms.\n        command_list = ['ps', '-p', str(pid), '-o', 'rss']\n        p = Popen(command_list, stdout=PIPE)\n        stdout = p.communicate()[0]\n        if not p.returncode == 0:\n            raise RuntimeError(\"Could not retrieve memory\")\n        else:\n            # Get the RSS from output that looks like this:\n            # RSS\n            # 4496\n            return int(stdout.splitlines()[1].split()[0]) * 1024\n\n    def record_memory(self):\n        mem = self._get_memory_with_ps(self._popen.pid)\n        self.memory_samples.append(mem)\n\n    def start(self, env=None):\n        \"\"\"Start up the command runner process.\"\"\"\n        self._popen = Popen(\n            [sys.executable, self.CLIENT_SERVER],\n            stdout=PIPE,\n            stdin=PIPE,\n            env=env,\n        )\n\n    def stop(self):\n        \"\"\"Shutdown the command runner process.\"\"\"\n        self.cmd('exit')\n        self._popen.wait()\n\n    def send_cmd(self, *cmd):\n        \"\"\"Send a command and return immediately.\n\n        This is a lower level method than cmd().\n        This method will instruct the cmd-runner process\n        to execute a command, but this method will\n        immediately return.  You will need to use\n        ``is_cmd_finished()`` to check that the command\n        is finished.\n\n        This method is useful if you want to record attributes\n        about the process while an operation is occurring.  For\n        example, if you want to instruct the cmd-runner process\n        to upload a 1GB file to S3 and you'd like to record\n        the memory during the upload process, you can use\n        send_cmd() instead of cmd().\n\n        \"\"\"\n        cmd_str = ' '.join(cmd) + '\\n'\n        cmd_bytes = cmd_str.encode('utf-8')\n        self._popen.stdin.write(cmd_bytes)\n        self._popen.stdin.flush()\n\n    def is_cmd_finished(self):\n        rlist = [self._popen.stdout.fileno()]\n        result = select.select(rlist, [], [], 0.01)\n        if result[0]:\n            return True\n        return False\n\n    def cmd(self, *cmd):\n        \"\"\"Send a command and block until it finishes.\n\n        This method will send a command to the cmd-runner process\n        to run.  It will block until the cmd-runner process is\n        finished executing the command and sends back a status\n        response.\n\n        \"\"\"\n        self.send_cmd(*cmd)\n        result = self._popen.stdout.readline().strip()\n        if result != b'OK':\n            raise RuntimeError(f\"Error from command '{cmd}': {result}\")\n\n\n# This is added to this file because it's used in both\n# the functional and unit tests for cred refresh.\nclass IntegerRefresher(credentials.RefreshableCredentials):\n    \"\"\"Refreshable credentials to help with testing.\n\n    This class makes testing refreshable credentials easier.\n    It has the following functionality:\n\n        * A counter, self.refresh_counter, to indicate how many\n          times refresh was called.\n        * A way to specify how many seconds to make credentials\n          valid.\n        * Configurable advisory/mandatory refresh.\n        * An easy way to check consistency.  Each time creds are\n          refreshed, all the cred values are set to the next\n          incrementing integer.  Frozen credentials should always\n          have this value.\n    \"\"\"\n\n    _advisory_refresh_timeout = 2\n    _mandatory_refresh_timeout = 1\n    _credentials_expire = 3\n\n    def __init__(\n        self,\n        creds_last_for=_credentials_expire,\n        advisory_refresh=_advisory_refresh_timeout,\n        mandatory_refresh=_mandatory_refresh_timeout,\n        refresh_function=None,\n    ):\n        expires_in = self._current_datetime() + datetime.timedelta(\n            seconds=creds_last_for\n        )\n        if refresh_function is None:\n            refresh_function = self._do_refresh\n        super().__init__(\n            '0', '0', '0', expires_in, refresh_function, 'INTREFRESH'\n        )\n        self.creds_last_for = creds_last_for\n        self.refresh_counter = 0\n        self._advisory_refresh_timeout = advisory_refresh\n        self._mandatory_refresh_timeout = mandatory_refresh\n\n    def _do_refresh(self):\n        self.refresh_counter += 1\n        current = int(self._access_key)\n        next_id = str(current + 1)\n\n        return {\n            'access_key': next_id,\n            'secret_key': next_id,\n            'token': next_id,\n            'expiry_time': self._seconds_later(self.creds_last_for),\n        }\n\n    def _seconds_later(self, num_seconds):\n        # We need to guarantee at *least* num_seconds.\n        # Because this doesn't handle subsecond precision\n        # we'll round up to the next second.\n        num_seconds += 1\n        t = self._current_datetime() + datetime.timedelta(seconds=num_seconds)\n        return self._to_timestamp(t)\n\n    def _to_timestamp(self, datetime_obj):\n        obj = utils.parse_to_aware_datetime(datetime_obj)\n        return obj.strftime('%Y-%m-%dT%H:%M:%SZ')\n\n    def _current_timestamp(self):\n        return self._to_timestamp(self._current_datetime())\n\n    def _current_datetime(self):\n        return datetime.datetime.now(tzlocal())\n\n\ndef _urlparse(url):\n    if isinstance(url, bytes):\n        # Not really necessary, but it helps to reduce noise on Python 2.x\n        url = url.decode('utf8')\n    return urlparse(url)\n\n\ndef assert_url_equal(url1, url2):\n    parts1 = _urlparse(url1)\n    parts2 = _urlparse(url2)\n\n    # Because the query string ordering isn't relevant, we have to parse\n    # every single part manually and then handle the query string.\n    assert parts1.scheme == parts2.scheme\n    assert parts1.netloc == parts2.netloc\n    assert parts1.path == parts2.path\n    assert parts1.params == parts2.params\n    assert parts1.fragment == parts2.fragment\n    assert parts1.username == parts2.username\n    assert parts1.password == parts2.password\n    assert parts1.hostname == parts2.hostname\n    assert parts1.port == parts2.port\n    assert parse_qs(parts1.query) == parse_qs(parts2.query)\n\n\nclass HTTPStubberException(Exception):\n    pass\n\n\nclass RawResponse(BytesIO):\n    # TODO: There's a few objects similar to this in various tests, let's\n    # try and consolidate to this one in a future commit.\n    def stream(self, **kwargs):\n        contents = self.read()\n        while contents:\n            yield contents\n            contents = self.read()\n\n\nclass BaseHTTPStubber:\n    def __init__(self, obj_with_event_emitter, strict=True):\n        self.reset()\n        self._strict = strict\n        self._obj_with_event_emitter = obj_with_event_emitter\n\n    def reset(self):\n        self.requests = []\n        self.responses = []\n\n    def add_response(\n        self, url='https://example.com', status=200, headers=None, body=b''\n    ):\n        if headers is None:\n            headers = {}\n\n        raw = RawResponse(body)\n        response = AWSResponse(url, status, headers, raw)\n        self.responses.append(response)\n\n    @property\n    def _events(self):\n        raise NotImplementedError('_events')\n\n    def start(self):\n        self._events.register('before-send', self)\n\n    def stop(self):\n        self._events.unregister('before-send', self)\n\n    def __enter__(self):\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.stop()\n\n    def __call__(self, request, **kwargs):\n        self.requests.append(request)\n        if self.responses:\n            response = self.responses.pop(0)\n            if isinstance(response, Exception):\n                raise response\n            else:\n                return response\n        elif self._strict:\n            raise HTTPStubberException('Insufficient responses')\n        else:\n            return None\n\n\nclass ClientHTTPStubber(BaseHTTPStubber):\n    @property\n    def _events(self):\n        return self._obj_with_event_emitter.meta.events\n\n\nclass SessionHTTPStubber(BaseHTTPStubber):\n    @property\n    def _events(self):\n        return self._obj_with_event_emitter.get_component('event_emitter')\n\n\nclass ConsistencyWaiterException(Exception):\n    pass\n\n\nclass ConsistencyWaiter:\n    \"\"\"\n    A waiter class for some check to reach a consistent state.\n\n    :type min_successes: int\n    :param min_successes: The minimum number of successful check calls to\n    treat the check as stable. Default of 1 success.\n\n    :type max_attempts: int\n    :param min_successes: The maximum number of times to attempt calling\n    the check. Default of 20 attempts.\n\n    :type delay: int\n    :param delay: The number of seconds to delay the next API call after a\n    failed check call. Default of 5 seconds.\n    \"\"\"\n\n    def __init__(\n        self,\n        min_successes=1,\n        max_attempts=20,\n        delay=5,\n        delay_initial_poll=False,\n    ):\n        self.min_successes = min_successes\n        self.max_attempts = max_attempts\n        self.delay = delay\n        self.delay_initial_poll = delay_initial_poll\n\n    def wait(self, check, *args, **kwargs):\n        \"\"\"\n        Wait until the check succeeds the configured number of times\n\n        :type check: callable\n        :param check: A callable that returns True or False to indicate\n        if the check succeeded or failed.\n\n        :type args: list\n        :param args: Any ordered arguments to be passed to the check.\n\n        :type kwargs: dict\n        :param kwargs: Any keyword arguments to be passed to the check.\n        \"\"\"\n        attempts = 0\n        successes = 0\n        if self.delay_initial_poll:\n            time.sleep(self.delay)\n        while attempts < self.max_attempts:\n            attempts += 1\n            if check(*args, **kwargs):\n                successes += 1\n                if successes >= self.min_successes:\n                    return\n            else:\n                time.sleep(self.delay)\n        fail_msg = self._fail_message(attempts, successes)\n        raise ConsistencyWaiterException(fail_msg)\n\n    def _fail_message(self, attempts, successes):\n        format_args = (attempts, successes)\n        return 'Failed after %s attempts, only had %s successes' % format_args\n\n\nclass StubbedSession(botocore.session.Session):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._cached_clients = {}\n        self._client_stubs = {}\n\n    def create_client(self, service_name, *args, **kwargs):\n        if service_name not in self._cached_clients:\n            client = self._create_stubbed_client(service_name, *args, **kwargs)\n            self._cached_clients[service_name] = client\n        return self._cached_clients[service_name]\n\n    def _create_stubbed_client(self, service_name, *args, **kwargs):\n        client = super().create_client(service_name, *args, **kwargs)\n        stubber = Stubber(client)\n        self._client_stubs[service_name] = stubber\n        return client\n\n    def stub(self, service_name, *args, **kwargs):\n        if service_name not in self._client_stubs:\n            self.create_client(service_name, *args, **kwargs)\n        return self._client_stubs[service_name]\n\n    def activate_stubs(self):\n        for stub in self._client_stubs.values():\n            stub.activate()\n\n    def verify_stubs(self):\n        for stub in self._client_stubs.values():\n            stub.assert_no_pending_responses()\n\n\nclass FreezeTime(ContextDecorator):\n    \"\"\"\n    Context manager for mocking out datetime in arbitrary modules when creating\n    performing actions like signing which require point in time specificity.\n\n    :type module: module\n    :param module: reference to imported module to patch (e.g. botocore.auth.datetime)\n\n    :type date: datetime.datetime\n    :param date: datetime object specifying the output for utcnow()\n    \"\"\"\n\n    def __init__(self, module, date=None):\n        if date is None:\n            date = datetime.datetime.utcnow()\n        self.date = date\n        self.datetime_patcher = mock.patch.object(\n            module, 'datetime', mock.Mock(wraps=datetime.datetime)\n        )\n\n    def __enter__(self, *args, **kwargs):\n        mock = self.datetime_patcher.start()\n        mock.utcnow.return_value = self.date\n\n    def __exit__(self, *args, **kwargs):\n        self.datetime_patcher.stop()\n\n\ndef patch_load_service_model(\n    session, monkeypatch, service_model_json, ruleset_json\n):\n    def mock_load_service_model(service_name, type_name, api_version=None):\n        if type_name == 'service-2':\n            return service_model_json\n        if type_name == 'endpoint-rule-set-1':\n            return ruleset_json\n\n    loader = session.get_component('data_loader')\n    monkeypatch.setattr(loader, 'load_service_model', mock_load_service_model)\n", "tests/acceptance/features/environment.py": "import os\n\nimport botocore.session\n\nSESSION = botocore.session.get_session()\nKNOWN_SERVICES = SESSION.get_available_services()\n\n# For the services where the tag name doesn't match\n# the name we use to create_client(), we need to maintain\n# a map until we can get these changes pushed upstream.\nTAG_TO_ENDPOINT_PREFIX = {\n    'cognitoidentity': 'cognito-identity',\n    'cognitosync': 'cognito-sync',\n    'elasticloadbalancing': 'elb',\n    'elasticfilesystem': 'efs',\n}\nREGION = 'us-east-1'\nREGION_OVERRIDES = {\n    'devicefarm': 'us-west-2',\n    'efs': 'us-west-2',\n}\nSKIP_SERVICES = {\n    # efs/support require subscriptions and may not work on every machine.\n    'efs',\n    'support',\n    # sts and importexport are skipped because they do not\n    # work when using temporary credentials.\n    'sts',\n    'importexport',\n}\n\n\ndef before_feature(context, feature):\n    for tag in feature.tags:\n        if tag in TAG_TO_ENDPOINT_PREFIX:\n            service_name = TAG_TO_ENDPOINT_PREFIX[tag]\n            break\n        elif tag in KNOWN_SERVICES:\n            service_name = tag\n            break\n    else:\n        raise RuntimeError(\n            \"Unable to create a client for \" \"feature: %s\" % feature\n        )\n\n    if service_name in SKIP_SERVICES:\n        feature.mark_skipped()\n        return\n    region_name = _get_region_for_service(service_name)\n    context.client = SESSION.create_client(service_name, region_name)\n\n\ndef _get_region_for_service(service_name):\n    if os.environ.get('AWS_SMOKE_TEST_REGION', ''):\n        region_name = os.environ['AWS_SMOKE_TEST_REGION']\n    else:\n        region_name = REGION_OVERRIDES.get(service_name, REGION)\n    return region_name\n", "tests/acceptance/features/steps/base.py": "import json\n\nfrom behave import then, when\n\nfrom botocore import xform_name\nfrom botocore.exceptions import ClientError\n\n\ndef _params_from_table(table):\n    # Unfortunately the way we're using table is not quite how\n    # behave expects tables to be used:\n    # They expect:\n    #\n    #     | name      | department  |\n    #     | Barry     | foo         |\n    #     | Pudey     | bar         |\n    #     | Two-Lumps | bar         |\n    #\n    # Where the first row are headings that indicate the\n    # key name you can use to retrieve row values,\n    # e.g row['name'] -> Barry.\n    #\n    #\n    # We just use:\n    #      | LaunchConfigurationName | hello, world |\n    #      | ImageId                 | ami-12345678 |\n    #      | InstanceType            | m1.small     |\n    #\n    # So we have to grab the headings before iterating over\n    # the table rows.\n    params = {table.headings[0]: table.headings[1]}\n    for row in table:\n        params[row[0]] = row[1]\n    return params\n\n\n@when('I call the \"{}\" API')\ndef api_call_no_args(context, operation):\n    context.response = getattr(context.client, xform_name(operation))()\n\n\n@when('I call the \"{}\" API with')\ndef api_call_with_args(context, operation):\n    params = _params_from_table(context.table)\n    context.response = getattr(context.client, xform_name(operation))(**params)\n\n\n@when('I call the \"{}\" API with JSON')\ndef api_call_with_json(context, operation):\n    params = json.loads(context.text)\n    context.response = getattr(context.client, xform_name(operation))(**params)\n\n\n@when('I attempt to call the \"{}\" API with')\ndef api_call_with_error(context, operation):\n    params = _params_from_table(context.table)\n    try:\n        getattr(context.client, xform_name(operation))(**params)\n    except ClientError as e:\n        context.error_response = e\n\n\n@when('I attempt to call the \"{}\" API with JSON')\ndef api_call_with_json_and_error(context, operation):\n    params = json.loads(context.text)\n    try:\n        getattr(context.client, xform_name(operation))(**params)\n    except ClientError as e:\n        context.error_response = e\n\n\n@then('I expect the response error code to be \"{}\"')\ndef then_expected_error(context, code):\n    assert context.error_response.response['Error']['Code'] == code\n\n\n@then('the value at \"{}\" should be a list')\ndef then_expected_type_is_list(context, expression):\n    # In botocore, if there are no values with an element,\n    # it will not appear in the response dict, so it's actually\n    # ok if the element does not exist (and is not a list).\n    # If an exception happened the test will have already failed,\n    # which makes this step a noop.  We'll just verify\n    # the response is a dict to ensure it made it through\n    # our response parser properly.\n    if not isinstance(context.response, dict):\n        raise AssertionError(\"Response is not a dict: %s\" % context.response)\n\n\n@then('the response should contain a \"{}\"')\ndef then_should_contain_key(context, key):\n    # See then_expected_type_is_a_list for more background info.\n    # We really just care that the request succeeded for these\n    # smoke tests.\n    if not isinstance(context.response, dict):\n        raise AssertionError(\"Response is not a dict: %s\" % context.response)\n\n\n@then('I expect the response error to contain a message')\ndef then_error_has_message(context):\n    if 'Message' not in context.error_response.response['Error']:\n        raise AssertionError(\n            \"Message key missing from error response: %s\"\n            % context.error_response.response\n        )\n", "tests/integration/test_client_http.py": "import contextlib\nimport select\nimport socket\nimport socketserver\nimport threading\nfrom contextlib import contextmanager\nfrom http.server import BaseHTTPRequestHandler\n\nimport botocore.session\nfrom botocore.config import Config\nfrom botocore.exceptions import (\n    ClientError,\n    ConnectionClosedError,\n    ConnectTimeoutError,\n    EndpointConnectionError,\n    ProxyConnectionError,\n    ReadTimeoutError,\n)\nfrom botocore.vendored.requests import exceptions as requests_exceptions\nfrom tests import mock, unittest\n\n\nclass TestClientHTTPBehavior(unittest.TestCase):\n    def setUp(self):\n        self.port = unused_port()\n        self.localhost = 'http://localhost:%s/' % self.port\n        self.session = botocore.session.get_session()\n        # We need to set fake credentials to ensure credentials aren't searched\n        # for which might make additional API calls (assume role, etc).\n        self.session.set_credentials('fakeakid', 'fakesecret')\n\n    @unittest.skip('Test has suddenly become extremely flakey.')\n    def test_can_proxy_https_request_with_auth(self):\n        proxy_url = 'http://user:pass@localhost:%s/' % self.port\n        config = Config(proxies={'https': proxy_url}, region_name='us-west-1')\n        client = self.session.create_client('ec2', config=config)\n\n        class AuthProxyHandler(ProxyHandler):\n            event = threading.Event()\n\n            def validate_auth(self):\n                proxy_auth = self.headers.get('Proxy-Authorization')\n                return proxy_auth == 'Basic dXNlcjpwYXNz'\n\n        try:\n            with background(run_server, args=(AuthProxyHandler, self.port)):\n                AuthProxyHandler.event.wait(timeout=60)\n                client.describe_regions()\n        except BackgroundTaskFailed:\n            self.fail('Background task did not exit, proxy was not used.')\n\n    @unittest.skip('Proxy cannot connect to service when run in CodeBuild.')\n    def test_proxy_request_includes_host_header(self):\n        proxy_url = 'http://user:pass@localhost:%s/' % self.port\n        config = Config(\n            proxies={'https': proxy_url},\n            proxies_config={'proxy_use_forwarding_for_https': True},\n            region_name='us-west-1',\n        )\n        environ = {'BOTO_EXPERIMENTAL__ADD_PROXY_HOST_HEADER': \"True\"}\n        self.environ_patch = mock.patch('os.environ', environ)\n        self.environ_patch.start()\n        client = self.session.create_client('ec2', config=config)\n\n        class ConnectProxyHandler(ProxyHandler):\n            event = threading.Event()\n\n            def do_CONNECT(self):\n                remote_host, remote_port = self.path.split(':')\n\n                # Ensure we're sending the correct host header in CONNECT\n                if self.headers.get('host') != remote_host:\n                    self.send_response(400)\n                    self.end_headers()\n                    return\n\n                self.send_response(200)\n                self.end_headers()\n\n                remote_host, remote_port = self.path.split(':')\n                remote_socket = socket.socket(\n                    socket.AF_INET, socket.SOCK_STREAM\n                )\n                remote_socket.connect((remote_host, int(remote_port)))\n\n                self._tunnel(self.request, remote_socket)\n                remote_socket.close()\n\n        try:\n            with background(run_server, args=(ConnectProxyHandler, self.port)):\n                ConnectProxyHandler.event.wait(timeout=60)\n                client.describe_regions()\n        except BackgroundTaskFailed:\n            self.fail('Background task did not exit, proxy was not used.')\n        except ProxyConnectionError:\n            self.fail('Proxy CONNECT failed, unable to establish connection.')\n        except ClientError as e:\n            # Fake credentials won't resolve against service\n            # but we've successfully contacted through the proxy\n            assert e.response['Error']['Code'] == 'AuthFailure'\n        finally:\n            self.environ_patch.stop()\n\n    def _read_timeout_server(self):\n        config = Config(\n            read_timeout=0.1,\n            retries={'max_attempts': 0},\n            region_name='us-weast-2',\n        )\n        client = self.session.create_client(\n            'ec2', endpoint_url=self.localhost, config=config\n        )\n        client_call_ended_event = threading.Event()\n\n        class FakeEC2(SimpleHandler):\n            event = threading.Event()\n            msg = b'<response/>'\n\n            def get_length(self):\n                return len(self.msg)\n\n            def get_body(self):\n                client_call_ended_event.wait(timeout=60)\n                return self.msg\n\n        try:\n            with background(run_server, args=(FakeEC2, self.port)):\n                try:\n                    FakeEC2.event.wait(timeout=60)\n                    client.describe_regions()\n                finally:\n                    client_call_ended_event.set()\n        except BackgroundTaskFailed:\n            self.fail('Fake EC2 service was not called.')\n\n    def test_read_timeout_exception(self):\n        with self.assertRaises(ReadTimeoutError):\n            self._read_timeout_server()\n\n    def test_old_read_timeout_exception(self):\n        with self.assertRaises(requests_exceptions.ReadTimeout):\n            self._read_timeout_server()\n\n    @unittest.skip('The current implementation will fail to timeout on linux')\n    def test_connect_timeout_exception(self):\n        config = Config(\n            connect_timeout=0.2,\n            retries={'max_attempts': 0},\n            region_name='us-weast-2',\n        )\n        client = self.session.create_client(\n            'ec2', endpoint_url=self.localhost, config=config\n        )\n        server_bound_event = threading.Event()\n        client_call_ended_event = threading.Event()\n\n        def no_accept_server():\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            sock.bind(('', self.port))\n            server_bound_event.set()\n            client_call_ended_event.wait(timeout=60)\n            sock.close()\n\n        with background(no_accept_server):\n            server_bound_event.wait(timeout=60)\n            with self.assertRaises(ConnectTimeoutError):\n                client.describe_regions()\n            client_call_ended_event.set()\n\n    def test_invalid_host_gaierror(self):\n        config = Config(retries={'max_attempts': 0}, region_name='us-weast-1')\n        endpoint = 'https://ec2.us-weast-1.amazonaws.com/'\n        client = self.session.create_client(\n            'ec2', endpoint_url=endpoint, config=config\n        )\n        with self.assertRaises(EndpointConnectionError):\n            client.describe_regions()\n\n    def test_bad_status_line(self):\n        config = Config(retries={'max_attempts': 0}, region_name='us-weast-2')\n        client = self.session.create_client(\n            'ec2', endpoint_url=self.localhost, config=config\n        )\n\n        class BadStatusHandler(BaseHTTPRequestHandler):\n            event = threading.Event()\n\n            def do_POST(self):\n                self.wfile.write(b'garbage')\n\n        with background(run_server, args=(BadStatusHandler, self.port)):\n            with self.assertRaises(ConnectionClosedError):\n                BadStatusHandler.event.wait(timeout=60)\n                client.describe_regions()\n\n\ndef unused_port():\n    with contextlib.closing(socket.socket()) as sock:\n        sock.bind(('127.0.0.1', 0))\n        return sock.getsockname()[1]\n\n\nclass SimpleHandler(BaseHTTPRequestHandler):\n    status = 200\n\n    def get_length(self):\n        return 0\n\n    def get_body(self):\n        return b''\n\n    def do_GET(self):\n        length = str(self.get_length())\n        self.send_response(self.status)\n        self.send_header('Content-Length', length)\n        self.end_headers()\n        self.wfile.write(self.get_body())\n\n    do_POST = do_PUT = do_GET\n\n\nclass ProxyHandler(BaseHTTPRequestHandler):\n    tunnel_chunk_size = 1024\n    poll_limit = 10**4\n\n    def _tunnel(self, client, remote):\n        client.setblocking(0)\n        remote.setblocking(0)\n        sockets = [client, remote]\n        noop_count = 0\n        while True:\n            readable, writeable, _ = select.select(sockets, sockets, [], 1)\n            if client in readable and remote in writeable:\n                noop_count = 0\n                client_bytes = client.recv(self.tunnel_chunk_size)\n                if not client_bytes:\n                    break\n                remote.sendall(client_bytes)\n            if remote in readable and client in writeable:\n                noop_count = 0\n                remote_bytes = remote.recv(self.tunnel_chunk_size)\n                if not remote_bytes:\n                    break\n                client.sendall(remote_bytes)\n\n            if noop_count > self.poll_limit:\n                # We have a case where all communication has\n                # finished but we never saw an empty read.\n                # This will leave both sockets as writeable\n                # indefinitely. We'll force a break here if\n                # we've crossed our polling limit.\n                break\n\n            noop_count += 1\n\n    def do_CONNECT(self):\n        if not self.validate_auth():\n            self.send_response(401)\n            self.end_headers()\n            return\n\n        self.send_response(200)\n        self.end_headers()\n\n        remote_host, remote_port = self.path.split(':')\n        remote_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        remote_socket.connect((remote_host, int(remote_port)))\n\n        self._tunnel(self.request, remote_socket)\n        remote_socket.close()\n\n    def validate_auth(self):\n        return True\n\n\nclass BackgroundTaskFailed(Exception):\n    pass\n\n\n@contextmanager\ndef background(target, args=(), timeout=60):\n    thread = threading.Thread(target=target, args=args)\n    thread.daemon = True\n    thread.start()\n    try:\n        yield target\n    finally:\n        thread.join(timeout=timeout)\n        if thread.is_alive():\n            msg = 'Background task did not exit in a timely manner.'\n            raise BackgroundTaskFailed(msg)\n\n\ndef run_server(handler, port):\n    address = ('', port)\n    httpd = socketserver.TCPServer(address, handler, bind_and_activate=False)\n    httpd.allow_reuse_address = True\n    httpd.server_bind()\n    httpd.server_activate()\n    handler.event.set()\n    httpd.handle_request()\n    httpd.server_close()\n", "tests/integration/test_apigateway.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport time\n\nimport botocore.session\nfrom botocore import exceptions\nfrom tests import unittest\n\n\nclass TestApigateway(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('apigateway', 'us-east-1')\n\n        # Create a resource to use with this client.\n        self.api_name = 'mytestapi'\n        self.api_id = self.create_rest_api_or_skip()\n\n    def create_rest_api_or_skip(self):\n        try:\n            api_id = self.client.create_rest_api(name=self.api_name)['id']\n        except exceptions.ClientError as e:\n            if e.response['Error']['Code'] == 'TooManyRequestsException':\n                raise unittest.SkipTest(\n                    \"Hit API gateway throttle limit, skipping test.\"\n                )\n            raise\n        return api_id\n\n    def delete_api(self):\n        retries = 0\n        while retries < 10:\n            try:\n                self.client.delete_rest_api(restApiId=self.api_id)\n                break\n            except exceptions.ClientError as e:\n                if e.response['Error']['Code'] == 'TooManyRequestsException':\n                    retries += 1\n                    time.sleep(5)\n                else:\n                    raise\n\n    def tearDown(self):\n        self.delete_api()\n\n    def test_put_integration(self):\n        # The only resource on a brand new api is the path. So use that ID.\n        path_resource_id = self.client.get_resources(restApiId=self.api_id)[\n            'items'\n        ][0]['id']\n\n        # Create a method for the resource.\n        self.client.put_method(\n            restApiId=self.api_id,\n            resourceId=path_resource_id,\n            httpMethod='GET',\n            authorizationType='None',\n        )\n\n        # Put an integration on the method.\n        response = self.client.put_integration(\n            restApiId=self.api_id,\n            resourceId=path_resource_id,\n            httpMethod='GET',\n            type='HTTP',\n            integrationHttpMethod='GET',\n            uri='https://api.endpoint.com',\n        )\n        # Assert the response was successful by checking the integration type\n        self.assertEqual(response['type'], 'HTTP')\n", "tests/integration/test_cognito_identity.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore.session\nfrom tests import random_chars, unittest\n\n\nclass TestCognitoIdentity(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client(\n            'cognito-identity', 'us-east-1'\n        )\n\n    def test_can_create_and_delete_identity_pool(self):\n        pool_name = 'test%s' % random_chars(10)\n        response = self.client.create_identity_pool(\n            IdentityPoolName=pool_name, AllowUnauthenticatedIdentities=True\n        )\n        self.client.delete_identity_pool(\n            IdentityPoolId=response['IdentityPoolId']\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/integration/test_glacier.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport io\n\nimport botocore.session\nfrom botocore.exceptions import ClientError\nfrom tests import unittest\n\n\nclass TestGlacier(unittest.TestCase):\n    # We have to use a single vault for all the integration tests.\n    # This is because if we create a vault and upload then delete\n    # an archive, we cannot immediately clean up and delete the vault.\n    # The compromise is that we'll use a single vault and use\n    # get_or_create semantics for the integ tests.  This does mean you\n    # need to be careful when writing tests.  Assume that other code\n    # is also using this vault in parallel, so don't rely on things like\n    # number of archives in a vault.\n\n    VAULT_NAME = 'botocore-integ-test-vault'\n\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('glacier', 'us-west-2')\n        # There's no error if the vault already exists so we don't\n        # need to catch any exceptions here.\n        self.client.create_vault(vaultName=self.VAULT_NAME)\n\n    def test_can_list_vaults_without_account_id(self):\n        response = self.client.list_vaults()\n        self.assertIn('VaultList', response)\n\n    def test_can_handle_error_responses(self):\n        with self.assertRaises(ClientError):\n            self.client.list_vaults(accountId='asdf')\n\n    def test_can_upload_archive(self):\n        body = io.BytesIO(b\"bytes content\")\n        response = self.client.upload_archive(\n            vaultName=self.VAULT_NAME,\n            archiveDescription='test upload',\n            body=body,\n        )\n        self.assertEqual(response['ResponseMetadata']['HTTPStatusCode'], 201)\n        archive_id = response['archiveId']\n        response = self.client.delete_archive(\n            vaultName=self.VAULT_NAME, archiveId=archive_id\n        )\n        self.assertEqual(response['ResponseMetadata']['HTTPStatusCode'], 204)\n\n    def test_can_upload_archive_from_bytes(self):\n        response = self.client.upload_archive(\n            vaultName=self.VAULT_NAME,\n            archiveDescription='test upload',\n            body=b'bytes body',\n        )\n        self.assertEqual(response['ResponseMetadata']['HTTPStatusCode'], 201)\n        archive_id = response['archiveId']\n        response = self.client.delete_archive(\n            vaultName=self.VAULT_NAME, archiveId=archive_id\n        )\n        self.assertEqual(response['ResponseMetadata']['HTTPStatusCode'], 204)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/integration/test_loaders.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nimport botocore.session\nfrom tests import mock, unittest\n\n\n# Basic sanity checks for loader functionality.\n# We're not using BaseEnvVar here because we don't actually\n# want to patch out all of os.environ, we just want to ensure\n# AWS_DATA_PATH doesn't affect our test results.\nclass TestLoaderBasicFunctionality(unittest.TestCase):\n    def setUp(self):\n        self.environ = os.environ.copy()\n        self.patched = mock.patch('os.environ', self.environ)\n        self.patched.start()\n        self.environ.pop('AWS_DATA_PATH', None)\n\n        self.session = botocore.session.get_session()\n        self.loader = self.session.get_component('data_loader')\n\n    def tearDown(self):\n        self.patched.stop()\n\n    def test_search_path_has_at_least_one_entry(self):\n        self.assertTrue(len(self.loader.search_paths) > 0)\n\n    def test_can_list_available_services(self):\n        # We don't want an exact check, as this list changes over time.\n        # We just need a basic sanity check.\n        available_services = self.loader.list_available_services(\n            type_name='service-2'\n        )\n        self.assertIn('ec2', available_services)\n        self.assertIn('s3', available_services)\n\n    def test_can_determine_latest_version(self):\n        api_versions = self.loader.list_api_versions(\n            service_name='ec2', type_name='service-2'\n        )\n        self.assertEqual(\n            self.loader.determine_latest_version(\n                service_name='ec2', type_name='service-2'\n            ),\n            max(api_versions),\n        )\n\n    def test_can_load_service_model(self):\n        waiters = self.loader.load_service_model(\n            service_name='ec2', type_name='waiters-2'\n        )\n        self.assertIn('waiters', waiters)\n\n    def test_can_load_data(self):\n        api_version = self.loader.determine_latest_version(\n            service_name='ec2', type_name='service-2'\n        )\n        data = self.loader.load_data(\n            os.path.join('ec2', api_version, 'service-2')\n        )\n        self.assertIn('metadata', data)\n", "tests/integration/test_waiters.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nimport botocore.session\nfrom botocore.exceptions import WaiterError\nfrom tests import random_chars, unittest\n\n\n@pytest.mark.slow\nclass TestWaiterForDynamoDB(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('dynamodb', 'us-west-2')\n\n    def test_create_table_and_wait(self):\n        table_name = 'botocoretest-%s' % random_chars(10)\n        self.client.create_table(\n            TableName=table_name,\n            ProvisionedThroughput={\n                \"ReadCapacityUnits\": 5,\n                \"WriteCapacityUnits\": 5,\n            },\n            KeySchema=[{\"AttributeName\": \"foo\", \"KeyType\": \"HASH\"}],\n            AttributeDefinitions=[\n                {\"AttributeName\": \"foo\", \"AttributeType\": \"S\"}\n            ],\n        )\n        self.addCleanup(self.client.delete_table, TableName=table_name)\n        waiter = self.client.get_waiter('table_exists')\n        waiter.wait(TableName=table_name)\n        parsed = self.client.describe_table(TableName=table_name)\n        self.assertEqual(parsed['Table']['TableStatus'], 'ACTIVE')\n\n\nclass TestCanGetWaitersThroughClientInterface(unittest.TestCase):\n    def test_get_ses_waiter(self):\n        # We're checking this because ses is not the endpoint prefix\n        # for the service, it's email.  We want to make sure this does\n        # not affect the lookup process.\n        session = botocore.session.get_session()\n        client = session.create_client('ses', 'us-east-1')\n        # If we have at least one waiter in the list, we know that we have\n        # actually loaded the waiters and this test has passed.\n        self.assertTrue(len(client.waiter_names) > 0)\n\n\nclass TestMatchersWithErrors(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client(\n            'ec2', region_name='us-west-2'\n        )\n\n    def test_dont_search_on_error_responses(self):\n        \"\"\"Test that InstanceExists can handle a nonexistent instance.\"\"\"\n        waiter = self.client.get_waiter('instance_exists')\n        waiter.config.max_attempts = 1\n        with self.assertRaises(WaiterError):\n            waiter.wait(InstanceIds=['i-12345'])\n", "tests/integration/test_route53.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore.session\nfrom botocore.exceptions import ClientError\nfrom tests import unittest\n\n\nclass TestRoute53Pagination(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('route53', 'us-west-2')\n\n    def test_paginate_with_max_items(self):\n        # Route53 has a string type for MaxItems.  We need to ensure that this\n        # still works without any issues.\n        paginator = self.client.get_paginator('list_hosted_zones')\n        results = list(paginator.paginate(PaginationConfig={'MaxItems': '1'}))\n        self.assertTrue(len(results) >= 0)\n\n    def test_paginate_with_deprecated_paginator_and_limited_input_tokens(self):\n        paginator = self.client.get_paginator('list_resource_record_sets')\n\n        # We're making sure the paginator gets set without failing locally, so\n        # a ClientError is acceptable. In this case, the Hosted Zone specified\n        # does not exist.\n        with self.assertRaises(ClientError):\n            results = list(\n                paginator.paginate(\n                    PaginationConfig={\n                        'MaxItems': '1',\n                        'StartingToken': 'my.domain.name.',\n                    },\n                    HostedZoneId=\"foo\",\n                )\n            )\n            self.assertTrue(len(results) >= 0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/integration/test_rds.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport itertools\n\nimport botocore.session\nfrom tests import unittest\n\n\nclass TestRDSPagination(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('rds', 'us-west-2')\n\n    def test_can_paginate_reserved_instances(self):\n        # Using an operation that we know will paginate.\n        paginator = self.client.get_paginator(\n            'describe_reserved_db_instances_offerings'\n        )\n        generator = paginator.paginate()\n        results = list(itertools.islice(generator, 0, 3))\n        self.assertEqual(len(results), 3)\n        self.assertTrue(results[0]['Marker'] != results[1]['Marker'])\n\n    def test_can_paginate_orderable_db(self):\n        paginator = self.client.get_paginator(\n            'describe_orderable_db_instance_options'\n        )\n        generator = paginator.paginate(Engine='mysql')\n        results = list(itertools.islice(generator, 0, 2))\n        self.assertEqual(len(results), 2)\n        self.assertTrue(results[0].get('Marker') != results[1].get('Marker'))\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/integration/test_emr.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nimport botocore.session\nfrom botocore.exceptions import OperationNotPageableError\nfrom botocore.paginate import PageIterator\nfrom tests import unittest\n\n\n@pytest.fixture()\ndef botocore_session():\n    return botocore.session.get_session()\n\n\n@pytest.mark.parametrize(\n    \"region\",\n    [\n        'us-east-1',\n        'us-west-2',\n        'us-west-2',\n        'ap-northeast-1',\n        'ap-southeast-1',\n        'ap-southeast-2',\n        'sa-east-1',\n        'eu-west-1',\n        'eu-central-1',\n    ],\n)\ndef test_emr_endpoints_work_with_py26(botocore_session, region):\n    # Verify that we can talk to all currently supported EMR endpoints.\n    # Python2.6 has an SSL cert bug where it can't read the SAN of\n    # certain SSL certs.  We therefore need to always use the CN\n    # as the hostname.\n    client = botocore_session.create_client('emr', region_name=region)\n    response = client.list_clusters()\n    assert 'Clusters' in response\n\n\n# I consider these integration tests because they're\n# testing more than a single unit, we're ensuring everything\n# accessible from the session works as expected.\nclass TestEMRGetExtraResources(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('emr', 'us-west-2')\n\n    def test_can_access_pagination_configs(self):\n        # Using an operation that we know will paginate.\n        paginator = self.client.get_paginator('list_clusters')\n        page_iterator = paginator.paginate()\n        self.assertIsInstance(page_iterator, PageIterator)\n\n    def test_operation_cant_be_paginated(self):\n        with self.assertRaises(OperationNotPageableError):\n            self.client.get_paginator('add_instance_groups')\n\n    def test_can_get_waiters(self):\n        waiter = self.client.get_waiter('cluster_running')\n        self.assertTrue(hasattr(waiter, 'wait'))\n\n    def test_waiter_does_not_exist(self):\n        with self.assertRaises(ValueError):\n            self.client.get_waiter('does_not_exist')\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/integration/test_credentials.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport os\nimport shutil\nimport tempfile\nimport time\nfrom uuid import uuid4\n\nfrom botocore.exceptions import ClientError\nfrom botocore.session import Session\nfrom tests import BaseEnvVar, mock, random_chars, temporary_file\n\nS3_READ_POLICY_ARN = 'arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n\n\nclass TestCredentialPrecedence(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n\n        # Set the config file to something that doesn't exist so\n        # that we don't accidentally load a config.\n        os.environ['AWS_CONFIG_FILE'] = '~/.aws/config-missing'\n\n    def create_session(self, *args, **kwargs):\n        \"\"\"\n        Create a new session with the given arguments. Additionally,\n        this method will set the credentials file to the test credentials\n        used by the following test cases.\n        \"\"\"\n        kwargs['session_vars'] = {\n            'credentials_file': (\n                None,\n                None,\n                os.path.join(os.path.dirname(__file__), 'test-credentials'),\n                None,\n            )\n        }\n\n        return Session(*args, **kwargs)\n\n    def test_access_secret_vs_profile_env(self):\n        # If all three are given, then the access/secret keys should\n        # take precedence.\n        os.environ['AWS_ACCESS_KEY_ID'] = 'env'\n        os.environ['AWS_SECRET_ACCESS_KEY'] = 'env-secret'\n        os.environ['AWS_DEFAULT_PROFILE'] = 'test'\n\n        s = self.create_session()\n        credentials = s.get_credentials()\n\n        self.assertEqual(credentials.access_key, 'env')\n        self.assertEqual(credentials.secret_key, 'env-secret')\n\n    @mock.patch('botocore.credentials.Credentials')\n    def test_access_secret_vs_profile_code(self, credentials_cls):\n        # If all three are given, then the access/secret keys should\n        # take precedence.\n        s = self.create_session(profile='test')\n        s.create_client(\n            's3', aws_access_key_id='code', aws_secret_access_key='code-secret'\n        )\n\n        credentials_cls.assert_called_with(\n            access_key='code', secret_key='code-secret', token=mock.ANY\n        )\n\n    def test_profile_env_vs_code(self):\n        # If the profile is set both by the env var and by code,\n        # then the one set by code should take precedence.\n        os.environ['AWS_DEFAULT_PROFILE'] = 'test'\n        s = self.create_session(profile='default')\n        credentials = s.get_credentials()\n\n        self.assertEqual(credentials.access_key, 'default')\n        self.assertEqual(credentials.secret_key, 'default-secret')\n\n    @mock.patch('botocore.credentials.Credentials')\n    def test_access_secret_env_vs_code(self, credentials_cls):\n        # If the access/secret keys are set both as env vars and via\n        # code, then those set by code should take precedence.\n        os.environ['AWS_ACCESS_KEY_ID'] = 'env'\n        os.environ['AWS_SECRET_ACCESS_KEY'] = 'secret'\n        s = self.create_session()\n        s.create_client(\n            's3', aws_access_key_id='code', aws_secret_access_key='code-secret'\n        )\n\n        credentials_cls.assert_called_with(\n            access_key='code', secret_key='code-secret', token=mock.ANY\n        )\n\n    def test_access_secret_env_vs_profile_code(self):\n        # If access/secret keys are set in the environment, but then a\n        # specific profile is passed via code, then the access/secret\n        # keys defined in that profile should take precedence over\n        # the environment variables. Example:\n        #\n        # ``aws --profile dev s3 ls``\n        #\n        os.environ['AWS_ACCESS_KEY_ID'] = 'env'\n        os.environ['AWS_SECRET_ACCESS_KEY'] = 'env-secret'\n        s = self.create_session(profile='test')\n\n        credentials = s.get_credentials()\n\n        self.assertEqual(credentials.access_key, 'test')\n        self.assertEqual(credentials.secret_key, 'test-secret')\n\n    def test_honors_aws_shared_credentials_file_env_var(self):\n        with temporary_file('w') as f:\n            f.write(\n                '[default]\\n'\n                'aws_access_key_id=custom1\\n'\n                'aws_secret_access_key=custom2\\n'\n            )\n            f.flush()\n            os.environ['AWS_SHARED_CREDENTIALS_FILE'] = f.name\n            s = Session()\n            credentials = s.get_credentials()\n\n            self.assertEqual(credentials.access_key, 'custom1')\n            self.assertEqual(credentials.secret_key, 'custom2')\n\n\nclass TestAssumeRoleCredentials(BaseEnvVar):\n    def setUp(self):\n        self.env_original = os.environ.copy()\n        self.environ_copy = os.environ.copy()\n        super().setUp()\n        os.environ = self.environ_copy\n        # The tests rely on manipulating AWS_CONFIG_FILE,\n        # but we also need to make sure we don't accidentally\n        # pick up the ~/.aws/credentials file either.\n        os.environ['AWS_SHARED_CREDENTIALS_FILE'] = str(uuid4())\n        self.parent_session = Session()\n        self.iam = self.parent_session.create_client('iam')\n        self.sts = self.parent_session.create_client('sts')\n        self.tempdir = tempfile.mkdtemp()\n        self.config_file = os.path.join(self.tempdir, 'config')\n\n        # A role trust policy that allows the current account to call assume\n        # role on itself.\n        account_id = self.sts.get_caller_identity()['Account']\n        self.role_policy = {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\"AWS\": \"arn:aws:iam::%s:root\" % account_id},\n                    \"Action\": \"sts:AssumeRole\",\n                }\n            ],\n        }\n\n    def tearDown(self):\n        super().tearDown()\n        shutil.rmtree(self.tempdir)\n        os.environ = self.env_original.copy()\n\n    def random_name(self):\n        return 'botocoretest-' + random_chars(10)\n\n    def create_role(self, policy_document, policy_arn=None):\n        name = self.random_name()\n        response = self.iam.create_role(\n            RoleName=name, AssumeRolePolicyDocument=json.dumps(policy_document)\n        )\n        self.addCleanup(self.iam.delete_role, RoleName=name)\n        if policy_arn:\n            self.iam.attach_role_policy(RoleName=name, PolicyArn=policy_arn)\n            self.addCleanup(\n                self.iam.detach_role_policy,\n                RoleName=name,\n                PolicyArn=policy_arn,\n            )\n        return response['Role']\n\n    def create_user(self, policy_arns):\n        name = self.random_name()\n        user = self.iam.create_user(UserName=name)['User']\n        self.addCleanup(self.iam.delete_user, UserName=name)\n\n        for arn in policy_arns:\n            self.iam.attach_user_policy(UserName=name, PolicyArn=arn)\n            self.addCleanup(\n                self.iam.detach_user_policy, UserName=name, PolicyArn=arn\n            )\n\n        return user\n\n    def create_creds(self, user_name):\n        creds = self.iam.create_access_key(UserName=user_name)['AccessKey']\n        self.addCleanup(\n            self.iam.delete_access_key,\n            UserName=user_name,\n            AccessKeyId=creds['AccessKeyId'],\n        )\n        return creds\n\n    def wait_for_assume_role(\n        self,\n        role_arn,\n        access_key,\n        secret_key,\n        token=None,\n        attempts=30,\n        delay=10,\n        success_delay=1,\n        num_success=4,\n    ):\n        for _ in range(num_success):\n            creds = self._wait_for_assume_role(\n                role_arn, access_key, secret_key, token, attempts, delay\n            )\n            time.sleep(success_delay)\n        return creds\n\n    def _wait_for_assume_role(\n        self, role_arn, access_key, secret_key, token, attempts, delay\n    ):\n        # \"Why not use the policy simulator?\" you might ask. The answer is\n        # that the policy simulator will return success far before you can\n        # actually make the calls.\n        client = self.parent_session.create_client(\n            'sts',\n            aws_access_key_id=access_key,\n            aws_secret_access_key=secret_key,\n            aws_session_token=token,\n        )\n        attempts_remaining = attempts\n        role_session_name = random_chars(10)\n        while attempts_remaining > 0:\n            attempts_remaining -= 1\n            try:\n                result = client.assume_role(\n                    RoleArn=role_arn, RoleSessionName=role_session_name\n                )\n                return result['Credentials']\n            except ClientError as e:\n                code = e.response.get('Error', {}).get('Code')\n                if code in [\"InvalidClientTokenId\", \"AccessDenied\"]:\n                    time.sleep(delay)\n                else:\n                    raise\n\n        raise Exception(\"Unable to assume role %s\" % role_arn)\n\n    def create_assume_policy(self, role_arn):\n        policy_document = {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Resource\": role_arn,\n                    \"Action\": \"sts:AssumeRole\",\n                }\n            ],\n        }\n        name = self.random_name()\n        response = self.iam.create_policy(\n            PolicyName=name, PolicyDocument=json.dumps(policy_document)\n        )\n        self.addCleanup(\n            self.iam.delete_policy, PolicyArn=response['Policy']['Arn']\n        )\n        return response['Policy']['Arn']\n\n    def assert_s3_read_only_session(self, session):\n        # Calls to S3 should succeed\n        s3 = session.create_client('s3')\n        s3.list_buckets()\n\n        # Calls to other services should not\n        iam = session.create_client('iam')\n        try:\n            iam.list_groups()\n            self.fail(\"Expected call to list_groups to fail, but it passed.\")\n        except ClientError as e:\n            code = e.response.get('Error', {}).get('Code')\n            if code != 'AccessDenied':\n                raise\n\n    def test_recursive_assume_role(self):\n        # Create the final role, the one that will actually have access to s3\n        final_role = self.create_role(self.role_policy, S3_READ_POLICY_ARN)\n\n        # Create the role that can assume the final role\n        middle_policy_arn = self.create_assume_policy(final_role['Arn'])\n        middle_role = self.create_role(self.role_policy, middle_policy_arn)\n\n        # Create a user that can only assume the middle-man role, and then get\n        # static credentials for it.\n        user_policy_arn = self.create_assume_policy(middle_role['Arn'])\n        user = self.create_user([user_policy_arn])\n        user_creds = self.create_creds(user['UserName'])\n\n        # Setup the config file with the profiles we'll be using. For\n        # convenience static credentials are placed here instead of putting\n        # them in the credentials file.\n        config = (\n            '[default]\\n'\n            'aws_access_key_id = %s\\n'\n            'aws_secret_access_key = %s\\n'\n            '[profile middle]\\n'\n            'source_profile = default\\n'\n            'role_arn = %s\\n'\n            '[profile final]\\n'\n            'source_profile = middle\\n'\n            'role_arn = %s\\n'\n        )\n        config = config % (\n            user_creds['AccessKeyId'],\n            user_creds['SecretAccessKey'],\n            middle_role['Arn'],\n            final_role['Arn'],\n        )\n        with open(self.config_file, 'w') as f:\n            f.write(config)\n\n        # Wait for IAM permissions to propagate\n        middle_creds = self.wait_for_assume_role(\n            role_arn=middle_role['Arn'],\n            access_key=user_creds['AccessKeyId'],\n            secret_key=user_creds['SecretAccessKey'],\n        )\n        self.wait_for_assume_role(\n            role_arn=final_role['Arn'],\n            access_key=middle_creds['AccessKeyId'],\n            secret_key=middle_creds['SecretAccessKey'],\n            token=middle_creds['SessionToken'],\n        )\n\n        # Configure our credentials file to be THE credentials file\n        os.environ['AWS_CONFIG_FILE'] = self.config_file\n\n        self.assert_s3_read_only_session(Session(profile='final'))\n\n    def test_assume_role_with_credential_source(self):\n        # Create a role with read access to S3\n        role = self.create_role(self.role_policy, S3_READ_POLICY_ARN)\n\n        # Create a user that can assume the role and get static credentials\n        # for it.\n        user_policy_arn = self.create_assume_policy(role['Arn'])\n        user = self.create_user([user_policy_arn])\n        user_creds = self.create_creds(user['UserName'])\n\n        # Setup the config file with the profile we'll be using.\n        config = (\n            '[profile assume]\\n'\n            'role_arn = %s\\n'\n            'credential_source = Environment\\n'\n        )\n        config = config % role['Arn']\n        with open(self.config_file, 'w') as f:\n            f.write(config)\n\n        # Wait for IAM permissions to propagate\n        self.wait_for_assume_role(\n            role_arn=role['Arn'],\n            access_key=user_creds['AccessKeyId'],\n            secret_key=user_creds['SecretAccessKey'],\n        )\n\n        # Setup the environment so that our new config file is THE config\n        # file and add the expected credentials since we're using the\n        # environment as our credential source.\n        os.environ['AWS_CONFIG_FILE'] = self.config_file\n        os.environ['AWS_SECRET_ACCESS_KEY'] = user_creds['SecretAccessKey']\n        os.environ['AWS_ACCESS_KEY_ID'] = user_creds['AccessKeyId']\n\n        self.assert_s3_read_only_session(Session(profile='assume'))\n", "tests/integration/test_sts.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore.session\nfrom botocore.exceptions import ClientError\nfrom tests import unittest\n\n\nclass TestSTS(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        credentials = self.session.get_credentials()\n        if credentials.token is not None:\n            self.skipTest('STS tests require long-term credentials')\n\n    def test_regionalized_endpoints(self):\n        sts = self.session.create_client('sts', region_name='ap-southeast-1')\n        response = sts.get_session_token()\n        # Do not want to be revealing any temporary keys if the assertion fails\n        self.assertIn('Credentials', response.keys())\n\n        # Since we have to activate STS regionalization, we will test\n        # that you can send an STS request to a regionalized endpoint\n        # by making a call with the explicitly wrong region name\n        sts = self.session.create_client(\n            'sts',\n            region_name='ap-southeast-1',\n            endpoint_url='https://sts.us-west-2.amazonaws.com',\n        )\n        self.assertEqual(sts.meta.region_name, 'ap-southeast-1')\n        self.assertEqual(\n            sts.meta.endpoint_url, 'https://sts.us-west-2.amazonaws.com'\n        )\n        # Signing error will be thrown with the incorrect region name included.\n        with self.assertRaisesRegex(ClientError, 'ap-southeast-1'):\n            sts.get_session_token()\n", "tests/integration/test_ec2.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport itertools\n\nimport botocore.session\nfrom botocore.exceptions import ClientError\nfrom tests import unittest\n\n\nclass TestEC2(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client(\n            'ec2', region_name='us-west-2'\n        )\n\n    def test_can_make_request(self):\n        # Basic smoke test to ensure we can talk to ec2.\n        result = self.client.describe_availability_zones()\n        zones = list(\n            sorted(a['ZoneName'] for a in result['AvailabilityZones'])\n        )\n        self.assertTrue(\n            {'us-west-2a', 'us-west-2b', 'us-west-2c'}.issubset(zones)\n        )\n\n    def test_get_console_output_handles_error(self):\n        # Want to ensure the underlying ClientError is propogated\n        # on error.\n        with self.assertRaises(ClientError):\n            self.client.get_console_output(InstanceId='i-12345')\n\n\nclass TestEC2Pagination(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client(\n            'ec2', region_name='us-west-2'\n        )\n\n    def test_can_paginate(self):\n        # Using an operation that we know will paginate.\n        paginator = self.client.get_paginator(\n            'describe_reserved_instances_offerings'\n        )\n        pages = paginator.paginate()\n        results = list(itertools.islice(pages, 0, 3))\n        self.assertEqual(len(results), 3)\n        self.assertTrue(results[0]['NextToken'] != results[1]['NextToken'])\n\n    def test_can_paginate_with_page_size(self):\n        # Using an operation that we know will paginate.\n        paginator = self.client.get_paginator(\n            'describe_reserved_instances_offerings'\n        )\n        pages = paginator.paginate(PaginationConfig={'PageSize': 1})\n        results = list(itertools.islice(pages, 0, 3))\n        self.assertEqual(len(results), 3)\n        for parsed in results:\n            reserved_inst_offer = parsed['ReservedInstancesOfferings']\n            # There should be no more than  one reserved instance\n            # offering on each page.\n            self.assertLessEqual(len(reserved_inst_offer), 1)\n\n    def test_can_fall_back_to_old_starting_token(self):\n        # Using an operation that we know will paginate.\n        paginator = self.client.get_paginator(\n            'describe_reserved_instances_offerings'\n        )\n        pages = paginator.paginate(PaginationConfig={'NextToken': 'None___1'})\n\n        try:\n            results = list(itertools.islice(pages, 0, 3))\n            self.assertEqual(len(results), 3)\n            self.assertTrue(results[0]['NextToken'] != results[1]['NextToken'])\n        except ValueError:\n            self.fail(\"Old style paginator failed.\")\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/integration/test_s3.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\nimport shutil\nimport tempfile\nimport threading\nimport time\nfrom collections import defaultdict\nfrom contextlib import closing\nfrom io import BytesIO\nfrom tarfile import TarFile\n\nimport pytest\nimport urllib3\n\nimport botocore.auth\nimport botocore.credentials\nimport botocore.session\nfrom botocore.compat import OrderedDict, zip_longest\nfrom botocore.config import Config\nfrom botocore.exceptions import ClientError, ConnectionClosedError, WaiterError\nfrom tests import (\n    ClientHTTPStubber,\n    ConsistencyWaiter,\n    random_chars,\n    temporary_file,\n    unittest,\n)\n\n\ndef random_bucketname():\n    return 'botocoretest-' + random_chars(50)\n\n\nLOG = logging.getLogger('botocore.tests.integration')\n_SHARED_BUCKET = random_bucketname()\n_DEFAULT_REGION = 'us-west-2'\n\n\ndef http_get(url):\n    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED')\n    response = http.request('GET', url)\n    return response\n\n\ndef http_post(url, data, files):\n    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED')\n    merged_data = OrderedDict()\n    merged_data.update(data)\n    merged_data.update(files)\n    response = http.request(\n        'POST',\n        url,\n        fields=merged_data,\n    )\n    return response\n\n\ndef setup_module():\n    s3 = botocore.session.get_session().create_client('s3')\n    waiter = s3.get_waiter('bucket_exists')\n    params = {\n        'Bucket': _SHARED_BUCKET,\n        'CreateBucketConfiguration': {\n            'LocationConstraint': _DEFAULT_REGION,\n        },\n        'ObjectOwnership': 'ObjectWriter',\n    }\n    try:\n        s3.create_bucket(**params)\n    except Exception as e:\n        # A create_bucket can fail for a number of reasons.\n        # We're going to defer to the waiter below to make the\n        # final call as to whether or not the bucket exists.\n        LOG.debug(\"create_bucket() raised an exception: %s\", e, exc_info=True)\n    waiter.wait(Bucket=_SHARED_BUCKET)\n    s3.delete_public_access_block(Bucket=_SHARED_BUCKET)\n\n\ndef clear_out_bucket(bucket, region, delete_bucket=False):\n    s3 = botocore.session.get_session().create_client('s3', region_name=region)\n    # Ensure the bucket exists before attempting to wipe it out\n    exists_waiter = s3.get_waiter('bucket_exists')\n    exists_waiter.wait(Bucket=bucket)\n    page = s3.get_paginator('list_objects')\n    # Use pages paired with batch delete_objects().\n    for page in page.paginate(Bucket=bucket):\n        keys = [{'Key': obj['Key']} for obj in page.get('Contents', [])]\n        if keys:\n            s3.delete_objects(Bucket=bucket, Delete={'Objects': keys})\n    if delete_bucket:\n        for _ in range(5):\n            try:\n                s3.delete_bucket(Bucket=bucket)\n                break\n            except s3.exceptions.NoSuchBucket:\n                exists_waiter.wait(Bucket=bucket)\n            except Exception as e:\n                # We can sometimes get exceptions when trying to\n                # delete a bucket.  We'll let the waiter make\n                # the final call as to whether the bucket was able\n                # to be deleted.\n                LOG.debug(\n                    \"delete_bucket() raised an exception: %s\", e, exc_info=True\n                )\n                not_exists_waiter = s3.get_waiter('bucket_not_exists')\n                not_exists_waiter.wait(Bucket=bucket)\n            except WaiterError:\n                continue\n\n\ndef teardown_module():\n    clear_out_bucket(_SHARED_BUCKET, _DEFAULT_REGION, delete_bucket=True)\n\n\nclass BaseS3ClientTest(unittest.TestCase):\n    DEFAULT_DELAY = 5\n\n    def setUp(self):\n        self.bucket_name = _SHARED_BUCKET\n        self.region = _DEFAULT_REGION\n        clear_out_bucket(self.bucket_name, self.region)\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('s3', region_name=self.region)\n\n    def assert_status_code(self, response, status_code):\n        self.assertEqual(\n            response['ResponseMetadata']['HTTPStatusCode'], status_code\n        )\n\n    def create_bucket(self, region_name, bucket_name=None, client=None):\n        bucket_client = client or self.client\n        if bucket_name is None:\n            bucket_name = random_bucketname()\n        bucket_kwargs = {\n            'Bucket': bucket_name,\n            'ObjectOwnership': 'ObjectWriter',\n        }\n        if region_name != 'us-east-1':\n            bucket_kwargs['CreateBucketConfiguration'] = {\n                'LocationConstraint': region_name,\n            }\n        response = bucket_client.create_bucket(**bucket_kwargs)\n        self.assert_status_code(response, 200)\n        waiter = bucket_client.get_waiter('bucket_exists')\n        consistency_waiter = ConsistencyWaiter(\n            min_successes=3, delay=self.DEFAULT_DELAY, delay_initial_poll=True\n        )\n        consistency_waiter.wait(\n            lambda: waiter.wait(Bucket=bucket_name) is None\n        )\n        bucket_client.delete_public_access_block(Bucket=bucket_name)\n        self.addCleanup(clear_out_bucket, bucket_name, region_name, True)\n        return bucket_name\n\n    def create_object(self, key_name, body='foo', num_attempts=3):\n        for _ in range(num_attempts):\n            try:\n                self.client.put_object(\n                    Bucket=self.bucket_name, Key=key_name, Body=body\n                )\n                break\n            except self.client.exceptions.NoSuchBucket:\n                time.sleep(self.DEFAULT_DELAY)\n        self.wait_until_key_exists(self.bucket_name, key_name)\n\n    def make_tempdir(self):\n        tempdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, tempdir)\n        return tempdir\n\n    def wait_until_key_exists(\n        self, bucket_name, key_name, extra_params=None, min_successes=3\n    ):\n        self._wait_for_key(\n            bucket_name, key_name, extra_params, min_successes, exists=True\n        )\n\n    def wait_until_key_not_exists(\n        self, bucket_name, key_name, extra_params=None, min_successes=3\n    ):\n        self._wait_for_key(\n            bucket_name, key_name, extra_params, min_successes, exists=False\n        )\n\n    def _wait_for_key(\n        self,\n        bucket_name,\n        key_name,\n        extra_params=None,\n        min_successes=3,\n        exists=True,\n    ):\n        if exists:\n            waiter = self.client.get_waiter('object_exists')\n        else:\n            waiter = self.client.get_waiter('object_not_exists')\n        params = {'Bucket': bucket_name, 'Key': key_name}\n        if extra_params is not None:\n            params.update(extra_params)\n        for _ in range(min_successes):\n            waiter.wait(**params)\n\n    def _check_bucket_versioning(self, bucket, enabled=True):\n        client = self.session.create_client('s3', region_name=self.region)\n        response = client.get_bucket_versioning(Bucket=bucket)\n        status = response.get('Status')\n        return status == 'Enabled' if enabled else status != 'Enabled'\n\n    def wait_until_versioning_enabled(self, bucket, min_successes=3):\n        waiter = ConsistencyWaiter(\n            min_successes=min_successes,\n            delay=self.DEFAULT_DELAY,\n            delay_initial_poll=True,\n        )\n        waiter.wait(self._check_bucket_versioning, bucket)\n\n\nclass TestS3BaseWithBucket(BaseS3ClientTest):\n    def setUp(self):\n        super().setUp()\n        self.caught_exceptions = []\n\n    def create_multipart_upload(self, key_name):\n        parsed = self.client.create_multipart_upload(\n            Bucket=self.bucket_name, Key=key_name\n        )\n        upload_id = parsed['UploadId']\n        self.addCleanup(\n            self.client.abort_multipart_upload,\n            UploadId=upload_id,\n            Bucket=self.bucket_name,\n            Key=key_name,\n        )\n\n    def abort_multipart_upload(self, bucket_name, key, upload_id):\n        self.client.abort_multipart_upload(\n            UploadId=upload_id, Bucket=self.bucket_name, Key=key\n        )\n\n    def delete_object(self, key, bucket_name):\n        response = self.client.delete_object(Bucket=bucket_name, Key=key)\n        self.assert_status_code(response, 204)\n\n    def delete_bucket(self, bucket_name):\n        response = self.client.delete_bucket(Bucket=bucket_name)\n        self.assert_status_code(response, 204)\n\n    def create_object_catch_exceptions(self, key_name):\n        try:\n            self.create_object(key_name=key_name)\n        except Exception as e:\n            self.caught_exceptions.append(e)\n\n    def assert_num_uploads_found(\n        self, operation, num_uploads, max_items=None, num_attempts=5\n    ):\n        amount_seen = None\n        paginator = self.client.get_paginator(operation)\n        for _ in range(num_attempts):\n            pages = paginator.paginate(\n                Bucket=self.bucket_name,\n                PaginationConfig={'MaxItems': max_items},\n            )\n            iterators = pages.result_key_iters()\n            self.assertEqual(len(iterators), 2)\n            self.assertEqual(iterators[0].result_key.expression, 'Uploads')\n            # It sometimes takes a while for all the uploads to show up,\n            # especially if the upload was just created.  If we don't\n            # see the expected amount, we retry up to num_attempts time\n            # before failing.\n            amount_seen = len(list(iterators[0]))\n            if amount_seen == num_uploads:\n                # Test passed.\n                return\n            else:\n                # Sleep and try again.\n                time.sleep(2)\n        self.fail(\n            \"Expected to see {} uploads, instead saw: {}\".format(\n                num_uploads, amount_seen\n            )\n        )\n\n    def create_client(self):\n        # Even though the default signature_version is s3,\n        # we're being explicit in case this ever changes.\n        client_config = Config(signature_version='s3')\n        return self.session.create_client(\n            's3', self.region, config=client_config\n        )\n\n    def assert_can_put_object(self, body):\n        client = self.create_client()\n        response = client.put_object(\n            Bucket=self.bucket_name, Key='foo', Body=body\n        )\n        self.assert_status_code(response, 200)\n        self.addCleanup(\n            client.delete_object, Bucket=self.bucket_name, Key='foo'\n        )\n\n\nclass TestS3Buckets(TestS3BaseWithBucket):\n    def setUp(self):\n        super().setUp()\n\n    def test_can_make_request(self):\n        # Basic smoke test to ensure we can talk to s3.\n        result = self.client.list_buckets()\n        # Can't really assume anything about whether or not they have buckets,\n        # but we can assume something about the structure of the response.\n        self.assertEqual(\n            sorted(list(result.keys())),\n            ['Buckets', 'Owner', 'ResponseMetadata'],\n        )\n\n    def test_can_get_bucket_location(self):\n        result = self.client.get_bucket_location(Bucket=self.bucket_name)\n        self.assertIn('LocationConstraint', result)\n        self.assertEqual(result['LocationConstraint'], self.region)\n\n\nclass TestS3Objects(TestS3BaseWithBucket):\n    def increment_auth(self, request, **kwargs):\n        self.auth_paths.append(request.auth_path)\n\n    def test_can_delete_urlencoded_object(self):\n        key_name = 'a+b/foo'\n        self.create_object(key_name=key_name)\n        bucket_contents = self.client.list_objects(Bucket=self.bucket_name)[\n            'Contents'\n        ]\n        self.assertEqual(len(bucket_contents), 1)\n        self.assertEqual(bucket_contents[0]['Key'], 'a+b/foo')\n\n        subdir_contents = self.client.list_objects(\n            Bucket=self.bucket_name, Prefix='a+b'\n        )['Contents']\n        self.assertEqual(len(subdir_contents), 1)\n        self.assertEqual(subdir_contents[0]['Key'], 'a+b/foo')\n\n        response = self.client.delete_object(\n            Bucket=self.bucket_name, Key=key_name\n        )\n        self.assert_status_code(response, 204)\n\n    @pytest.mark.slow\n    def test_can_paginate(self):\n        for i in range(5):\n            key_name = 'key%s' % i\n            self.create_object(key_name)\n        # Eventual consistency.\n        time.sleep(3)\n        paginator = self.client.get_paginator('list_objects')\n        generator = paginator.paginate(MaxKeys=1, Bucket=self.bucket_name)\n        responses = list(generator)\n        self.assertEqual(len(responses), 5, responses)\n        key_names = [el['Contents'][0]['Key'] for el in responses]\n        self.assertEqual(key_names, ['key0', 'key1', 'key2', 'key3', 'key4'])\n\n    @pytest.mark.slow\n    def test_can_paginate_with_page_size(self):\n        for i in range(5):\n            key_name = 'key%s' % i\n            self.create_object(key_name)\n        # Eventual consistency.\n        time.sleep(3)\n        paginator = self.client.get_paginator('list_objects')\n        generator = paginator.paginate(\n            PaginationConfig={'PageSize': 1}, Bucket=self.bucket_name\n        )\n        responses = list(generator)\n        self.assertEqual(len(responses), 5, responses)\n        data = [r for r in responses]\n        key_names = [el['Contents'][0]['Key'] for el in data]\n        self.assertEqual(key_names, ['key0', 'key1', 'key2', 'key3', 'key4'])\n\n    @pytest.mark.slow\n    def test_result_key_iters(self):\n        for i in range(5):\n            key_name = f'key/{i}/{i}'\n            self.create_object(key_name)\n            key_name2 = 'key/%s' % i\n            self.create_object(key_name2)\n        time.sleep(3)\n        paginator = self.client.get_paginator('list_objects')\n        generator = paginator.paginate(\n            MaxKeys=2, Prefix='key/', Delimiter='/', Bucket=self.bucket_name\n        )\n        iterators = generator.result_key_iters()\n        response = defaultdict(list)\n        key_names = [i.result_key for i in iterators]\n        for vals in zip_longest(*iterators):\n            for k, val in zip(key_names, vals):\n                response.setdefault(k.expression, [])\n                response[k.expression].append(val)\n        self.assertIn('Contents', response)\n        self.assertIn('CommonPrefixes', response)\n\n    @pytest.mark.slow\n    def test_can_get_and_put_object(self):\n        self.create_object('foobarbaz', body='body contents')\n        time.sleep(3)\n\n        data = self.client.get_object(Bucket=self.bucket_name, Key='foobarbaz')\n        self.assertEqual(data['Body'].read().decode('utf-8'), 'body contents')\n\n    def test_can_put_large_string_body_on_new_bucket(self):\n        body = '*' * (5 * (1024**2))\n        self.assert_can_put_object(body)\n\n    def test_can_put_object_bytearray(self):\n        body_bytes = b'*' * 1024\n        body = bytearray(body_bytes)\n        self.assert_can_put_object(body)\n\n    def test_get_object_stream_wrapper(self):\n        self.create_object('foobarbaz', body='body contents')\n        response = self.client.get_object(\n            Bucket=self.bucket_name, Key='foobarbaz'\n        )\n        body = response['Body']\n        # Am able to set a socket timeout\n        body.set_socket_timeout(10)\n        self.assertEqual(body.read(amt=1).decode('utf-8'), 'b')\n        self.assertEqual(body.read().decode('utf-8'), 'ody contents')\n\n    def test_paginate_max_items(self):\n        self.create_multipart_upload('foo/key1')\n        self.create_multipart_upload('foo/key1')\n        self.create_multipart_upload('foo/key1')\n        self.create_multipart_upload('foo/key2')\n        self.create_multipart_upload('foobar/key1')\n        self.create_multipart_upload('foobar/key2')\n        self.create_multipart_upload('bar/key1')\n        self.create_multipart_upload('bar/key2')\n\n        # Verify when we have MaxItems=None, we get back all 8 uploads.\n        self.assert_num_uploads_found(\n            'list_multipart_uploads', max_items=None, num_uploads=8\n        )\n\n        # Verify when we have MaxItems=1, we get back 1 upload.\n        self.assert_num_uploads_found(\n            'list_multipart_uploads', max_items=1, num_uploads=1\n        )\n\n        paginator = self.client.get_paginator('list_multipart_uploads')\n        # Works similar with build_full_result()\n        pages = paginator.paginate(\n            PaginationConfig={'MaxItems': 1}, Bucket=self.bucket_name\n        )\n        full_result = pages.build_full_result()\n        self.assertEqual(len(full_result['Uploads']), 1)\n\n    def test_paginate_within_page_boundaries(self):\n        self.create_object('a')\n        self.create_object('b')\n        self.create_object('c')\n        self.create_object('d')\n        paginator = self.client.get_paginator('list_objects')\n        # First do it without a max keys so we're operating on a single page of\n        # results.\n        pages = paginator.paginate(\n            PaginationConfig={'MaxItems': 1}, Bucket=self.bucket_name\n        )\n        first = pages.build_full_result()\n        t1 = first['NextToken']\n\n        pages = paginator.paginate(\n            PaginationConfig={'MaxItems': 1, 'StartingToken': t1},\n            Bucket=self.bucket_name,\n        )\n        second = pages.build_full_result()\n        t2 = second['NextToken']\n\n        pages = paginator.paginate(\n            PaginationConfig={'MaxItems': 1, 'StartingToken': t2},\n            Bucket=self.bucket_name,\n        )\n        third = pages.build_full_result()\n        t3 = third['NextToken']\n\n        pages = paginator.paginate(\n            PaginationConfig={'MaxItems': 1, 'StartingToken': t3},\n            Bucket=self.bucket_name,\n        )\n        fourth = pages.build_full_result()\n\n        self.assertEqual(first['Contents'][-1]['Key'], 'a')\n        self.assertEqual(second['Contents'][-1]['Key'], 'b')\n        self.assertEqual(third['Contents'][-1]['Key'], 'c')\n        self.assertEqual(fourth['Contents'][-1]['Key'], 'd')\n\n    def test_unicode_key_put_list(self):\n        # Verify we can upload a key with a unicode char and list it as well.\n        key_name = '\\u2713'\n        self.create_object(key_name)\n        parsed = self.client.list_objects(Bucket=self.bucket_name)\n        self.assertEqual(len(parsed['Contents']), 1)\n        self.assertEqual(parsed['Contents'][0]['Key'], key_name)\n        parsed = self.client.get_object(Bucket=self.bucket_name, Key=key_name)\n        self.assertEqual(parsed['Body'].read().decode('utf-8'), 'foo')\n\n    def test_unicode_system_character(self):\n        # Verify we can use a unicode system character which would normally\n        # break the xml parser\n        key_name = 'foo\\x08'\n        self.create_object(key_name)\n        self.addCleanup(self.delete_object, key_name, self.bucket_name)\n        parsed = self.client.list_objects(Bucket=self.bucket_name)\n        self.assertEqual(len(parsed['Contents']), 1)\n        self.assertEqual(parsed['Contents'][0]['Key'], key_name)\n\n        parsed = self.client.list_objects(\n            Bucket=self.bucket_name, EncodingType='url'\n        )\n        self.assertEqual(len(parsed['Contents']), 1)\n        self.assertEqual(parsed['Contents'][0]['Key'], 'foo%08')\n\n    def test_unicode_system_character_with_list_v2(self):\n        # Verify we can use a unicode system character which would normally\n        # break the xml parser\n        key_name = 'foo\\x08'\n        self.create_object(key_name)\n        self.addCleanup(self.delete_object, key_name, self.bucket_name)\n        parsed = self.client.list_objects_v2(Bucket=self.bucket_name)\n        self.assertEqual(len(parsed['Contents']), 1)\n        self.assertEqual(parsed['Contents'][0]['Key'], key_name)\n\n        parsed = self.client.list_objects_v2(\n            Bucket=self.bucket_name, EncodingType='url'\n        )\n        self.assertEqual(len(parsed['Contents']), 1)\n        self.assertEqual(parsed['Contents'][0]['Key'], 'foo%08')\n\n    def test_unicode_system_character_with_list_object_versions(self):\n        # Verify we can use a unicode system character which would normally\n        # break the xml parser\n        key_name = 'foo\\x03'\n        self.create_object(key_name)\n        self.addCleanup(self.delete_object, key_name, self.bucket_name)\n        parsed = self.client.list_object_versions(Bucket=self.bucket_name)\n        self.assertEqual(len(parsed['Versions']), 1)\n        self.assertEqual(parsed['Versions'][0]['Key'], key_name)\n\n        parsed = self.client.list_object_versions(\n            Bucket=self.bucket_name, EncodingType='url'\n        )\n        self.assertEqual(len(parsed['Versions']), 1)\n        self.assertEqual(parsed['Versions'][0]['Key'], 'foo%03')\n\n    def test_thread_safe_auth(self):\n        self.auth_paths = []\n        emitter = self.session.get_component('event_emitter')\n        emitter.register_last('before-sign.s3', self.increment_auth)\n        # This test depends on auth_path, which is only added in virtual host\n        # style requests.\n        config = Config(s3={'addressing_style': 'virtual'})\n        self.client = self.session.create_client(\n            's3', self.region, config=config\n        )\n        self.create_object(key_name='foo1')\n        threads = []\n        for i in range(10):\n            t = threading.Thread(\n                target=self.create_object_catch_exceptions, args=('foo%s' % i,)\n            )\n            t.daemon = True\n            threads.append(t)\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n        self.assertEqual(\n            self.caught_exceptions,\n            [],\n            \"Unexpectedly caught exceptions: %s\" % self.caught_exceptions,\n        )\n        self.assertEqual(\n            len(set(self.auth_paths)),\n            10,\n            \"Expected 10 unique auth paths, instead received: %s\"\n            % (self.auth_paths),\n        )\n\n    def test_non_normalized_key_paths(self):\n        # The create_object method has assertEqual checks for 200 status.\n        self.create_object('key./././name')\n        bucket_contents = self.client.list_objects(Bucket=self.bucket_name)[\n            'Contents'\n        ]\n        self.assertEqual(len(bucket_contents), 1)\n        self.assertEqual(bucket_contents[0]['Key'], 'key./././name')\n\n\nclass TestS3Regions(BaseS3ClientTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client('s3', region_name=self.region)\n\n    def test_reset_stream_on_redirects(self):\n        # Create a bucket in a non classic region.\n        bucket_name = self.create_bucket(self.region)\n        # Then try to put a file like object to this location.\n        tempdir = self.make_tempdir()\n        filename = os.path.join(tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'foo' * 1024)\n        with open(filename, 'rb') as f:\n            self.client.put_object(Bucket=bucket_name, Key='foo', Body=f)\n\n        data = self.client.get_object(Bucket=bucket_name, Key='foo')\n        self.assertEqual(data['Body'].read(), b'foo' * 1024)\n\n\nclass TestS3Copy(TestS3BaseWithBucket):\n    def test_copy_with_quoted_char(self):\n        key_name = 'a+b/foo'\n        self.create_object(key_name=key_name)\n\n        key_name2 = key_name + 'bar'\n        self.client.copy_object(\n            Bucket=self.bucket_name,\n            Key=key_name2,\n            CopySource=f'{self.bucket_name}/{key_name}',\n        )\n\n        # Now verify we can retrieve the copied object.\n        data = self.client.get_object(Bucket=self.bucket_name, Key=key_name2)\n        self.assertEqual(data['Body'].read().decode('utf-8'), 'foo')\n\n    def test_copy_with_query_string(self):\n        key_name = 'a+b/foo?notVersionid=bar'\n        self.create_object(key_name=key_name)\n\n        key_name2 = key_name + 'bar'\n        self.client.copy_object(\n            Bucket=self.bucket_name,\n            Key=key_name2,\n            CopySource=f'{self.bucket_name}/{key_name}',\n        )\n\n        # Now verify we can retrieve the copied object.\n        data = self.client.get_object(Bucket=self.bucket_name, Key=key_name2)\n        self.assertEqual(data['Body'].read().decode('utf-8'), 'foo')\n\n    def test_can_copy_with_dict_form(self):\n        key_name = 'a+b/foo?versionId=abcd'\n        self.create_object(key_name=key_name)\n\n        key_name2 = key_name + 'bar'\n        self.client.copy_object(\n            Bucket=self.bucket_name,\n            Key=key_name2,\n            CopySource={'Bucket': self.bucket_name, 'Key': key_name},\n        )\n\n        # Now verify we can retrieve the copied object.\n        data = self.client.get_object(Bucket=self.bucket_name, Key=key_name2)\n        self.assertEqual(data['Body'].read().decode('utf-8'), 'foo')\n\n    def test_copy_with_s3_metadata(self):\n        key_name = 'foo.txt'\n        self.create_object(key_name=key_name)\n        copied_key = 'copied.txt'\n        parsed = self.client.copy_object(\n            Bucket=self.bucket_name,\n            Key=copied_key,\n            CopySource=f'{self.bucket_name}/{key_name}',\n            MetadataDirective='REPLACE',\n            Metadata={\"mykey\": \"myvalue\", \"mykey2\": \"myvalue2\"},\n        )\n        self.assert_status_code(parsed, 200)\n\n\nclass BaseS3PresignTest(BaseS3ClientTest):\n    def setup_bucket(self):\n        self.key = 'myobject'\n        self.create_object(key_name=self.key)\n\n\nclass TestS3PresignUsStandard(BaseS3PresignTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-east-1'\n        self.client_config = Config(region_name=self.region)\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n        self.bucket_name = self.create_bucket(self.region)\n        self.setup_bucket()\n\n    def test_presign_sigv2(self):\n        self.client_config.signature_version = 's3'\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n        presigned_url = self.client.generate_presigned_url(\n            'get_object', Params={'Bucket': self.bucket_name, 'Key': self.key}\n        )\n        self.assertTrue(\n            presigned_url.startswith(\n                'https://{}.s3.amazonaws.com/{}'.format(\n                    self.bucket_name, self.key\n                )\n            ),\n            \"Host was suppose to use DNS style, instead \"\n            \"got: %s\" % presigned_url,\n        )\n        # Try to retrieve the object using the presigned url.\n        self.assertEqual(http_get(presigned_url).data, b'foo')\n\n    def test_presign_with_existing_query_string_values(self):\n        content_disposition = 'attachment; filename=foo.txt;'\n        presigned_url = self.client.generate_presigned_url(\n            'get_object',\n            Params={\n                'Bucket': self.bucket_name,\n                'Key': self.key,\n                'ResponseContentDisposition': content_disposition,\n            },\n        )\n        response = http_get(presigned_url)\n        self.assertEqual(\n            response.headers['Content-Disposition'], content_disposition\n        )\n        self.assertEqual(response.data, b'foo')\n\n    def test_presign_sigv4(self):\n        self.client_config.signature_version = 's3v4'\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n        presigned_url = self.client.generate_presigned_url(\n            'get_object', Params={'Bucket': self.bucket_name, 'Key': self.key}\n        )\n        self.assertTrue(\n            presigned_url.startswith(\n                'https://{}.s3.amazonaws.com/{}'.format(\n                    self.bucket_name, self.key\n                )\n            ),\n            \"Host was suppose to be the us-east-1 endpoint, instead \"\n            \"got: %s\" % presigned_url,\n        )\n        # Try to retrieve the object using the presigned url.\n        self.assertEqual(http_get(presigned_url).data, b'foo')\n\n    def test_presign_post_sigv2(self):\n        self.client_config.signature_version = 's3'\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n\n        # Create some of the various supported conditions.\n        conditions = [\n            {\"acl\": \"public-read\"},\n        ]\n\n        # Create the fields that follow the policy.\n        fields = {\n            'acl': 'public-read',\n        }\n\n        # Retrieve the args for the presigned post.\n        post_args = self.client.generate_presigned_post(\n            self.bucket_name, self.key, Fields=fields, Conditions=conditions\n        )\n\n        # Make sure that the form can be posted successfully.\n        files = {'file': ('baz', 'some data')}\n\n        # Make sure the correct endpoint is being used\n        self.assertTrue(\n            post_args['url'].startswith(\n                'https://%s.s3.amazonaws.com' % self.bucket_name\n            ),\n            \"Host was suppose to use DNS style, instead \"\n            \"got: %s\" % post_args['url'],\n        )\n\n        # Try to retrieve the object using the presigned url.\n        r = http_post(post_args['url'], data=post_args['fields'], files=files)\n        self.assertEqual(r.status, 204)\n\n    def test_presign_post_sigv4(self):\n        self.client_config.signature_version = 's3v4'\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n\n        # Create some of the various supported conditions.\n        conditions = [\n            {\"acl\": 'public-read'},\n        ]\n\n        # Create the fields that follow the policy.\n        fields = {\n            'acl': 'public-read',\n        }\n\n        # Retrieve the args for the presigned post.\n        post_args = self.client.generate_presigned_post(\n            self.bucket_name, self.key, Fields=fields, Conditions=conditions\n        )\n\n        # Make sure that the form can be posted successfully.\n        files = {'file': ('baz', 'some data')}\n\n        # Make sure the correct endpoint is being used\n        self.assertTrue(\n            post_args['url'].startswith(\n                'https://%s.s3.amazonaws.com/' % self.bucket_name\n            ),\n            \"Host was suppose to use us-east-1 endpoint, instead \"\n            \"got: %s\" % post_args['url'],\n        )\n\n        r = http_post(post_args['url'], data=post_args['fields'], files=files)\n        self.assertEqual(r.status, 204)\n\n\nclass TestS3PresignNonUsStandard(BaseS3PresignTest):\n    def setUp(self):\n        super().setUp()\n        self.client_config = Config(region_name=self.region)\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n        self.setup_bucket()\n\n    def test_presign_sigv2(self):\n        self.client_config.signature_version = 's3'\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n\n        presigned_url = self.client.generate_presigned_url(\n            'get_object', Params={'Bucket': self.bucket_name, 'Key': self.key}\n        )\n        self.assertTrue(\n            presigned_url.startswith(\n                'https://{}.s3.amazonaws.com/{}'.format(\n                    self.bucket_name, self.key\n                )\n            ),\n            \"Host was suppose to use DNS style, instead \"\n            \"got: %s\" % presigned_url,\n        )\n        # Try to retrieve the object using the presigned url.\n        self.assertEqual(http_get(presigned_url).data, b'foo')\n\n    def test_presign_sigv4(self):\n        # For a newly created bucket, you can't use virtualhosted\n        # addressing and 's3v4' due to the backwards compat behavior\n        # using '.s3.amazonaws.com' for anything in the AWS partition.\n        # Instead you either have to use the older 's3' signature version\n        # of you have to use path style addressing.  The latter is being\n        # done here.\n        self.client_config.signature_version = 's3v4'\n        self.client_config.s3 = {'addressing_style': 'path'}\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n        presigned_url = self.client.generate_presigned_url(\n            'get_object', Params={'Bucket': self.bucket_name, 'Key': self.key}\n        )\n\n        self.assertTrue(\n            presigned_url.startswith(\n                'https://s3.us-west-2.amazonaws.com/{}/{}'.format(\n                    self.bucket_name, self.key\n                )\n            ),\n            \"Host was suppose to be the us-west-2 endpoint, instead \"\n            \"got: %s\" % presigned_url,\n        )\n        # Try to retrieve the object using the presigned url.\n        self.assertEqual(http_get(presigned_url).data, b'foo')\n\n    def test_presign_post_sigv2(self):\n        self.client_config.signature_version = 's3'\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n\n        # Create some of the various supported conditions.\n        conditions = [\n            {\"acl\": \"public-read\"},\n        ]\n\n        # Create the fields that follow the policy.\n        fields = {\n            'acl': 'public-read',\n        }\n\n        # Retrieve the args for the presigned post.\n        post_args = self.client.generate_presigned_post(\n            self.bucket_name, self.key, Fields=fields, Conditions=conditions\n        )\n\n        # Make sure that the form can be posted successfully.\n        files = {'file': ('baz', 'some data')}\n\n        # Make sure the correct endpoint is being used\n        self.assertTrue(\n            post_args['url'].startswith(\n                'https://%s.s3.amazonaws.com' % self.bucket_name\n            ),\n            \"Host was suppose to use DNS style, instead \"\n            \"got: %s\" % post_args['url'],\n        )\n\n        r = http_post(post_args['url'], data=post_args['fields'], files=files)\n        self.assertEqual(r.status, 204)\n\n    def test_presign_post_sigv4(self):\n        self.client_config.signature_version = 's3v4'\n        self.client = self.session.create_client(\n            's3', config=self.client_config\n        )\n\n        # Create some of the various supported conditions.\n        conditions = [\n            {\"acl\": \"public-read\"},\n        ]\n\n        # Create the fields that follow the policy.\n        fields = {\n            'acl': 'public-read',\n        }\n\n        # Retrieve the args for the presigned post.\n        post_args = self.client.generate_presigned_post(\n            self.bucket_name, self.key, Fields=fields, Conditions=conditions\n        )\n\n        # Make sure that the form can be posted successfully.\n        files = {'file': ('baz', 'some data')}\n\n        # Make sure the correct endpoint is being used\n        self.assertTrue(\n            post_args['url'].startswith(\n                'https://%s.s3.amazonaws.com/' % self.bucket_name\n            ),\n            \"Host was suppose to use DNS style, instead \"\n            \"got: %s\" % post_args['url'],\n        )\n\n        r = http_post(post_args['url'], data=post_args['fields'], files=files)\n        self.assertEqual(r.status, 204)\n\n\nclass TestCreateBucketInOtherRegion(TestS3BaseWithBucket):\n    def test_bucket_in_other_region(self):\n        # This verifies expect 100-continue behavior.  We previously\n        # had a bug where we did not support this behavior and trying to\n        # create a bucket and immediately PutObject with a file like object\n        # would actually cause errors.\n        client = self.session.create_client('s3', 'us-east-1')\n        with temporary_file('w') as f:\n            f.write('foobarbaz' * 1024 * 1024)\n            f.flush()\n            with open(f.name, 'rb') as body_file:\n                response = client.put_object(\n                    Bucket=self.bucket_name, Key='foo.txt', Body=body_file\n                )\n            self.assert_status_code(response, 200)\n\n    def test_bucket_in_other_region_using_http(self):\n        client = self.session.create_client('s3', 'us-east-1', use_ssl=False)\n        with temporary_file('w') as f:\n            f.write('foobarbaz' * 1024 * 1024)\n            f.flush()\n            with open(f.name, 'rb') as body_file:\n                response = client.put_object(\n                    Bucket=self.bucket_name, Key='foo.txt', Body=body_file\n                )\n            self.assert_status_code(response, 200)\n\n\nclass TestS3SigV4Client(BaseS3ClientTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client(\n            's3', self.region, config=Config(signature_version='s3v4')\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n    def test_can_get_bucket_location(self):\n        # Even though the bucket is in us-west-2, we should still be able to\n        # use the us-east-1 endpoint class to get the bucket location.\n        client = self.session.create_client('s3', 'us-east-1')\n        # Also keep in mind that while this test is useful, it doesn't test\n        # what happens once DNS propogates which is arguably more interesting,\n        # as DNS will point us to the eu-central-1 endpoint.\n        response = client.get_bucket_location(Bucket=self.bucket_name)\n        self.assertEqual(response['LocationConstraint'], 'us-west-2')\n\n    def test_request_retried_for_sigv4(self):\n        body = BytesIO(b\"Hello world!\")\n        exception = ConnectionClosedError(endpoint_url='')\n        self.http_stubber.responses.append(exception)\n        self.http_stubber.responses.append(None)\n        with self.http_stubber:\n            response = self.client.put_object(\n                Bucket=self.bucket_name, Key='foo.txt', Body=body\n            )\n            self.assert_status_code(response, 200)\n\n    @pytest.mark.slow\n    def test_paginate_list_objects_unicode(self):\n        key_names = [\n            'non-ascii-key-\\xe4\\xf6\\xfc-01.txt',\n            'non-ascii-key-\\xe4\\xf6\\xfc-02.txt',\n            'non-ascii-key-\\xe4\\xf6\\xfc-03.txt',\n            'non-ascii-key-\\xe4\\xf6\\xfc-04.txt',\n        ]\n        for key in key_names:\n            response = self.client.put_object(\n                Bucket=self.bucket_name, Key=key, Body=''\n            )\n            self.assert_status_code(response, 200)\n\n        list_objs_paginator = self.client.get_paginator('list_objects')\n        key_refs = []\n        for response in list_objs_paginator.paginate(\n            Bucket=self.bucket_name, PaginationConfig={'PageSize': 2}\n        ):\n            for content in response['Contents']:\n                key_refs.append(content['Key'])\n\n        self.assertEqual(key_names, key_refs)\n\n    @pytest.mark.slow\n    def test_paginate_list_objects_safe_chars(self):\n        key_names = [\n            '-._~safe-chars-key-01.txt',\n            '-._~safe-chars-key-02.txt',\n            '-._~safe-chars-key-03.txt',\n            '-._~safe-chars-key-04.txt',\n        ]\n        for key in key_names:\n            response = self.client.put_object(\n                Bucket=self.bucket_name, Key=key, Body=''\n            )\n            self.assert_status_code(response, 200)\n\n        list_objs_paginator = self.client.get_paginator('list_objects')\n        key_refs = []\n        for response in list_objs_paginator.paginate(\n            Bucket=self.bucket_name, PaginationConfig={'PageSize': 2}\n        ):\n            for content in response['Contents']:\n                key_refs.append(content['Key'])\n\n        self.assertEqual(key_names, key_refs)\n\n    def test_create_multipart_upload(self):\n        key = 'mymultipartupload'\n        response = self.client.create_multipart_upload(\n            Bucket=self.bucket_name, Key=key\n        )\n        self.assert_status_code(response, 200)\n        upload_id = response['UploadId']\n        self.addCleanup(\n            self.client.abort_multipart_upload,\n            Bucket=self.bucket_name,\n            Key=key,\n            UploadId=upload_id,\n        )\n\n        response = self.client.list_multipart_uploads(\n            Bucket=self.bucket_name, Prefix=key\n        )\n\n        # Make sure there is only one multipart upload.\n        self.assertEqual(len(response['Uploads']), 1)\n        # Make sure the upload id is as expected.\n        self.assertEqual(response['Uploads'][0]['UploadId'], upload_id)\n\n    def test_can_add_double_space_metadata(self):\n        # Ensure we get no sigv4 errors when we send\n        # metadata with consecutive spaces.\n        response = self.client.put_object(\n            Bucket=self.bucket_name,\n            Key='foo.txt',\n            Body=b'foobar',\n            Metadata={'foo': '  multi    spaces  '},\n        )\n        self.assert_status_code(response, 200)\n\n    def test_bad_request_on_invalid_credentials(self):\n        # A previous bug would cause this to hang.  We want\n        # to verify we get the 400 response.\n        # In order to test we need a key that actually\n        # exists so we use the properly configured self.client.\n        self.client.put_object(\n            Bucket=self.bucket_name, Key='foo.txt', Body=b'asdfasdf'\n        )\n        # Now we create a client with a bad session token\n        # which should give us a 400 response.\n        creds = self.session.get_credentials()\n        client = self.session.create_client(\n            's3',\n            self.region,\n            config=Config(signature_version='s3v4'),\n            aws_access_key_id=creds.access_key,\n            aws_secret_access_key=creds.secret_key,\n            aws_session_token='bad-token-causes-400',\n        )\n        with self.assertRaises(ClientError) as e:\n            client.head_object(\n                Bucket=self.bucket_name,\n                Key='foo.txt',\n            )\n        self.assertEqual(e.exception.response['Error']['Code'], '400')\n\n\nclass TestSSEKeyParamValidation(BaseS3ClientTest):\n    def test_make_request_with_sse(self):\n        key_bytes = os.urandom(32)\n        # Obviously a bad key here, but we just want to ensure we can use\n        # a str/unicode type as a key.\n        key_str = 'abcd' * 8\n\n        # Put two objects with an sse key, one with random bytes,\n        # one with str/unicode.  Then verify we can GetObject() both\n        # objects.\n        self.client.put_object(\n            Bucket=self.bucket_name,\n            Key='foo.txt',\n            Body=BytesIO(b'mycontents'),\n            SSECustomerAlgorithm='AES256',\n            SSECustomerKey=key_bytes,\n        )\n        self.addCleanup(\n            self.client.delete_object, Bucket=self.bucket_name, Key='foo.txt'\n        )\n        self.client.put_object(\n            Bucket=self.bucket_name,\n            Key='foo2.txt',\n            Body=BytesIO(b'mycontents2'),\n            SSECustomerAlgorithm='AES256',\n            SSECustomerKey=key_str,\n        )\n        self.addCleanup(\n            self.client.delete_object, Bucket=self.bucket_name, Key='foo2.txt'\n        )\n\n        self.assertEqual(\n            self.client.get_object(\n                Bucket=self.bucket_name,\n                Key='foo.txt',\n                SSECustomerAlgorithm='AES256',\n                SSECustomerKey=key_bytes,\n            )['Body'].read(),\n            b'mycontents',\n        )\n        self.assertEqual(\n            self.client.get_object(\n                Bucket=self.bucket_name,\n                Key='foo2.txt',\n                SSECustomerAlgorithm='AES256',\n                SSECustomerKey=key_str,\n            )['Body'].read(),\n            b'mycontents2',\n        )\n\n    def test_make_request_with_sse_copy_source(self):\n        encrypt_key = 'a' * 32\n        other_encrypt_key = 'b' * 32\n\n        # Upload the object using one encrypt key\n        self.client.put_object(\n            Bucket=self.bucket_name,\n            Key='foo.txt',\n            Body=BytesIO(b'mycontents'),\n            SSECustomerAlgorithm='AES256',\n            SSECustomerKey=encrypt_key,\n        )\n        self.addCleanup(\n            self.client.delete_object, Bucket=self.bucket_name, Key='foo.txt'\n        )\n\n        # Copy the object using the original encryption key as the copy source\n        # and encrypt with a new encryption key.\n        self.client.copy_object(\n            Bucket=self.bucket_name,\n            CopySource=self.bucket_name + '/foo.txt',\n            Key='bar.txt',\n            CopySourceSSECustomerAlgorithm='AES256',\n            CopySourceSSECustomerKey=encrypt_key,\n            SSECustomerAlgorithm='AES256',\n            SSECustomerKey=other_encrypt_key,\n        )\n        self.addCleanup(\n            self.client.delete_object, Bucket=self.bucket_name, Key='bar.txt'\n        )\n\n        # Download the object using the new encryption key.\n        # The content should not have changed.\n        self.assertEqual(\n            self.client.get_object(\n                Bucket=self.bucket_name,\n                Key='bar.txt',\n                SSECustomerAlgorithm='AES256',\n                SSECustomerKey=other_encrypt_key,\n            )['Body'].read(),\n            b'mycontents',\n        )\n\n\nclass TestS3UTF8Headers(BaseS3ClientTest):\n    def test_can_set_utf_8_headers(self):\n        bucket_name = _SHARED_BUCKET\n        body = BytesIO(b\"Hello world!\")\n        response = self.client.put_object(\n            Bucket=bucket_name,\n            Key=\"foo.txt\",\n            Body=body,\n            ContentDisposition=\"attachment; filename=5\u5c0f\u6642\u63a5\u529b\u8d77\u8dd1.jpg;\",\n        )\n        self.assert_status_code(response, 200)\n        self.addCleanup(\n            self.client.delete_object, Bucket=bucket_name, Key=\"foo.txt\"\n        )\n\n\nclass TestSupportedPutObjectBodyTypes(TestS3BaseWithBucket):\n    def test_can_put_unicode_content(self):\n        self.assert_can_put_object(body='\\u2713')\n\n    def test_can_put_non_ascii_bytes(self):\n        self.assert_can_put_object(body='\\u2713'.encode())\n\n    def test_can_put_arbitrary_binary_data(self):\n        body = os.urandom(5 * (1024**2))\n        self.assert_can_put_object(body)\n\n    def test_can_put_binary_file(self):\n        tempdir = self.make_tempdir()\n        filename = os.path.join(tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write('\\u2713'.encode())\n        with open(filename, 'rb') as binary_file:\n            self.assert_can_put_object(body=binary_file)\n\n    def test_can_put_extracted_file_from_tar(self):\n        tempdir = self.make_tempdir()\n        tarname = os.path.join(tempdir, 'mytar.tar')\n        filename = os.path.join(tempdir, 'foo')\n\n        # Set up a file to add the tarfile.\n        with open(filename, 'w') as f:\n            f.write('bar')\n\n        # Setup the tar file by adding the file to it.\n        # Note there is no context handler for TarFile in python 2.6\n        try:\n            tar = TarFile(tarname, 'w')\n            tar.add(filename, 'foo')\n        finally:\n            tar.close()\n\n        # See if an extracted file can be uploaded to s3.\n        try:\n            tar = TarFile(tarname, 'r')\n            with closing(tar.extractfile('foo')) as f:\n                self.assert_can_put_object(body=f)\n        finally:\n            tar.close()\n\n\nclass TestSupportedPutObjectBodyTypesSigv4(TestSupportedPutObjectBodyTypes):\n    def create_client(self):\n        client_config = Config(signature_version='s3v4')\n        return self.session.create_client(\n            's3', self.region, config=client_config\n        )\n\n\nclass TestAutoS3Addressing(BaseS3ClientTest):\n    def setUp(self):\n        super().setUp()\n        self.addressing_style = 'auto'\n        self.client = self.create_client()\n\n    def create_client(self, signature_version='s3'):\n        return self.session.create_client(\n            's3',\n            region_name=self.region,\n            config=Config(\n                s3={\n                    'addressing_style': self.addressing_style,\n                    'signature_version': signature_version,\n                }\n            ),\n        )\n\n    def test_can_list_buckets(self):\n        response = self.client.list_buckets()\n        self.assertIn('Buckets', response)\n\n    def test_can_make_bucket_and_put_object(self):\n        response = self.client.put_object(\n            Bucket=self.bucket_name, Key='foo', Body='contents'\n        )\n        self.assertEqual(response['ResponseMetadata']['HTTPStatusCode'], 200)\n\n    def test_can_make_bucket_and_put_object_with_sigv4(self):\n        self.region = 'eu-central-1'\n        self.client = self.create_client()\n        bucket_name = self.create_bucket(self.region)\n        response = self.client.put_object(\n            Bucket=bucket_name, Key='foo', Body='contents'\n        )\n        self.assertEqual(response['ResponseMetadata']['HTTPStatusCode'], 200)\n\n\nclass TestS3VirtualAddressing(TestAutoS3Addressing):\n    def setUp(self):\n        super().setUp()\n        self.addressing_style = 'virtual'\n        self.client = self.create_client()\n\n\nclass TestS3PathAddressing(TestAutoS3Addressing):\n    def setUp(self):\n        super().setUp()\n        self.addressing_style = 'path'\n        self.client = self.create_client()\n\n\nclass TestRegionRedirect(BaseS3ClientTest):\n    def setUp(self):\n        super().setUp()\n        self.bucket_region = self.region\n        self.client_region = 'eu-central-1'\n\n        self.client = self.session.create_client(\n            's3',\n            region_name=self.client_region,\n            config=Config(signature_version='s3v4'),\n        )\n\n        self.bucket_client = self.session.create_client(\n            's3',\n            region_name=self.bucket_region,\n            config=Config(signature_version='s3v4'),\n        )\n\n    def test_region_redirects(self):\n        try:\n            response = self.client.list_objects(Bucket=self.bucket_name)\n            self.assertEqual(\n                response['ResponseMetadata']['HTTPStatusCode'], 200\n            )\n        except ClientError as e:\n            error = e.response['Error'].get('Code', None)\n            if error == 'PermanentRedirect':\n                self.fail(\"S3 client failed to redirect to the proper region.\")\n\n    def test_region_redirect_sigv2_to_sigv4_raises_error(self):\n        self.bucket_region = 'eu-central-1'\n        sigv2_client = self.session.create_client(\n            's3',\n            region_name=self.client_region,\n            config=Config(signature_version='s3'),\n        )\n\n        eu_bucket = self.create_bucket(self.bucket_region)\n        msg = 'The authorization mechanism you have provided is not supported.'\n        with self.assertRaisesRegex(ClientError, msg):\n            sigv2_client.list_objects(Bucket=eu_bucket)\n\n    def test_region_redirects_multiple_requests(self):\n        try:\n            response = self.client.list_objects(Bucket=self.bucket_name)\n            self.assertEqual(\n                response['ResponseMetadata']['HTTPStatusCode'], 200\n            )\n            second_response = self.client.list_objects(Bucket=self.bucket_name)\n            self.assertEqual(\n                second_response['ResponseMetadata']['HTTPStatusCode'], 200\n            )\n        except ClientError as e:\n            error = e.response['Error'].get('Code', None)\n            if error == 'PermanentRedirect':\n                self.fail(\"S3 client failed to redirect to the proper region.\")\n\n    def test_redirects_head_bucket(self):\n        response = self.client.head_bucket(Bucket=self.bucket_name)\n        headers = response['ResponseMetadata']['HTTPHeaders']\n        region = headers.get('x-amz-bucket-region')\n        self.assertEqual(region, self.bucket_region)\n\n    def test_redirects_head_object(self):\n        key = 'foo'\n        self.bucket_client.put_object(\n            Bucket=self.bucket_name, Key=key, Body='bar'\n        )\n        self.wait_until_key_exists(self.bucket_name, key)\n        try:\n            response = self.client.head_object(\n                Bucket=self.bucket_name, Key=key\n            )\n            self.assertEqual(response.get('ContentLength'), len(key))\n        except ClientError as e:\n            self.fail(\"S3 Client failed to redirect Head Object: %s\" % e)\n\n\nclass TestBucketWithVersions(BaseS3ClientTest):\n    def extract_version_ids(self, versions):\n        version_ids = []\n        for marker in versions['DeleteMarkers']:\n            version_ids.append(marker['VersionId'])\n        for version in versions['Versions']:\n            version_ids.append(version['VersionId'])\n        return version_ids\n\n    def test_create_versioned_bucket(self):\n        # Verifies we can:\n        # 1. Create a bucket\n        # 2. Enable versioning\n        # 3. Put an Object\n        bucket = self.create_bucket(self.region)\n\n        self.client.put_bucket_versioning(\n            Bucket=bucket,\n            VersioningConfiguration={\"Status\": \"Enabled\"},\n        )\n        self.wait_until_versioning_enabled(bucket)\n\n        key = 'testkey'\n        body = b'bytes body'\n        response = self.client.put_object(Bucket=bucket, Key=key, Body=body)\n        self.addCleanup(\n            self.client.delete_object,\n            Bucket=bucket,\n            Key=key,\n            VersionId=response['VersionId'],\n        )\n        self.wait_until_key_exists(bucket, key)\n\n        response = self.client.get_object(Bucket=bucket, Key=key)\n        self.assertEqual(response['Body'].read(), body)\n\n        response = self.client.delete_object(Bucket=bucket, Key=key)\n        # This cleanup step removes the DeleteMarker that's created\n        # from the delete_object call above.\n        self.addCleanup(\n            self.client.delete_object,\n            Bucket=bucket,\n            Key=key,\n            VersionId=response['VersionId'],\n        )\n        # Object does not exist anymore.\n        with self.assertRaises(ClientError):\n            self.client.get_object(Bucket=bucket, Key=key)\n        versions = self.client.list_object_versions(Bucket=bucket)\n        version_ids = self.extract_version_ids(versions)\n        self.assertEqual(len(version_ids), 2)\n", "tests/integration/test_cloudformation.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore.session\nfrom botocore.exceptions import ClientError\nfrom tests import random_chars, unittest\n\n\nclass TestCloudformation(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('cloudformation', 'us-east-1')\n\n    def test_handles_errors_with_template_body(self):\n        # GetTemplate has a customization in handlers.py, so we're ensuring\n        # it handles the case when a stack does not exist.\n        with self.assertRaises(ClientError):\n            self.client.get_template(\n                StackName='does-not-exist-%s' % random_chars(10)\n            )\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/integration/test_client.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\nimport logging\nfrom io import StringIO\n\nimport botocore.session\nfrom botocore.client import ClientError\nfrom botocore.exceptions import EndpointConnectionError\nfrom tests import unittest\n\n\n# This is really a combination of testing the debug logging mechanism\n# as well as the response wire log, which theoretically could be\n# implemented in any number of modules, which makes it hard to pick\n# which integration test module this code should live in, so I picked\n# the client module.\nclass TestResponseLog(unittest.TestCase):\n    def test_debug_log_contains_headers_and_body(self):\n        # This test just verifies that the response headers/body\n        # are in the debug log.  It's an integration test so that\n        # we can refactor the code however we want, as long as we don't\n        # lose this feature.\n        session = botocore.session.get_session()\n        client = session.create_client('s3', region_name='us-west-2')\n        debug_log = StringIO()\n        session.set_stream_logger('', logging.DEBUG, debug_log)\n        client.list_buckets()\n        debug_log_contents = debug_log.getvalue()\n        self.assertIn('Response headers', debug_log_contents)\n        self.assertIn('Response body', debug_log_contents)\n\n\nclass TestAcceptedDateTimeFormats(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('emr', 'us-west-2')\n\n    def test_accepts_datetime_object(self):\n        response = self.client.list_clusters(\n            CreatedAfter=datetime.datetime.now()\n        )\n        self.assertIn('Clusters', response)\n\n    def test_accepts_epoch_format(self):\n        response = self.client.list_clusters(CreatedAfter=0)\n        self.assertIn('Clusters', response)\n\n    def test_accepts_iso_8601_unaware(self):\n        response = self.client.list_clusters(\n            CreatedAfter='2014-01-01T00:00:00'\n        )\n        self.assertIn('Clusters', response)\n\n    def test_accepts_iso_8601_utc(self):\n        response = self.client.list_clusters(\n            CreatedAfter='2014-01-01T00:00:00Z'\n        )\n        self.assertIn('Clusters', response)\n\n    def test_accepts_iso_8701_local(self):\n        response = self.client.list_clusters(\n            CreatedAfter='2014-01-01T00:00:00-08:00'\n        )\n        self.assertIn('Clusters', response)\n\n\nclass TestClientErrors(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n\n    def test_region_mentioned_in_invalid_region(self):\n        client = self.session.create_client(\n            'cloudformation', region_name='us-east-999'\n        )\n        with self.assertRaisesRegex(\n            EndpointConnectionError, 'Could not connect to the endpoint URL'\n        ):\n            client.list_stacks()\n\n    def test_client_modeled_exception(self):\n        client = self.session.create_client(\n            'dynamodb', region_name='us-west-2'\n        )\n        with self.assertRaises(client.exceptions.ResourceNotFoundException):\n            client.describe_table(TableName=\"NonexistentTable\")\n\n    def test_client_modeleded_exception_with_differing_code(self):\n        client = self.session.create_client('iam', region_name='us-west-2')\n        # The NoSuchEntityException should be raised on NoSuchEntity error\n        # code.\n        with self.assertRaises(client.exceptions.NoSuchEntityException):\n            client.get_role(RoleName=\"NonexistentIAMRole\")\n\n    def test_raises_general_client_error_for_non_modeled_exception(self):\n        client = self.session.create_client('ec2', region_name='us-west-2')\n        try:\n            client.describe_regions(DryRun=True)\n        except client.exceptions.ClientError as e:\n            self.assertIs(e.__class__, ClientError)\n\n    def test_can_catch_client_exceptions_across_two_different_clients(self):\n        client = self.session.create_client(\n            'dynamodb', region_name='us-west-2'\n        )\n        client2 = self.session.create_client(\n            'dynamodb', region_name='us-west-2'\n        )\n        with self.assertRaises(client2.exceptions.ResourceNotFoundException):\n            client.describe_table(TableName=\"NonexistentTable\")\n\n\nclass TestClientMeta(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n\n    def test_region_name_on_meta(self):\n        client = self.session.create_client('s3', 'us-west-2')\n        self.assertEqual(client.meta.region_name, 'us-west-2')\n\n    def test_endpoint_url_on_meta(self):\n        client = self.session.create_client(\n            's3', 'us-west-2', endpoint_url='https://foo'\n        )\n        self.assertEqual(client.meta.endpoint_url, 'https://foo')\n\n\nclass TestClientInjection(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n\n    def test_can_inject_client_methods(self):\n        def extra_client_method(self, name):\n            return name\n\n        def inject_client_method(class_attributes, **kwargs):\n            class_attributes['extra_client_method'] = extra_client_method\n\n        self.session.register('creating-client-class.s3', inject_client_method)\n\n        client = self.session.create_client('s3', 'us-west-2')\n\n        # We should now have access to the extra_client_method above.\n        self.assertEqual(client.extra_client_method('foo'), 'foo')\n\n\nclass TestMixedEndpointCasing(unittest.TestCase):\n    def setUp(self):\n        self.url = 'https://EC2.US-WEST-2.amazonaws.com/'\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client(\n            'ec2', 'us-west-2', endpoint_url=self.url\n        )\n\n    def test_sigv4_is_correct_when_mixed_endpoint_casing(self):\n        res = self.client.describe_regions()\n        status_code = res['ResponseMetadata']['HTTPStatusCode']\n        self.assertEqual(status_code, 200)\n", "tests/integration/test_smoke.py": "\"\"\"Smoke tests to verify basic communication to all AWS services.\n\nIf you want to control what services/regions are used you can\nalso provide two separate env vars:\n\n    * AWS_SMOKE_TEST_REGION - The region used to create clients.\n    * AWS_SMOKE_TEST_SERVICES - A CSV list of service names to test.\n\nOtherwise, the ``REGION`` variable specifies the default region\nto use and all the services in SMOKE_TESTS/ERROR_TESTS will be tested.\n\n\"\"\"\nimport logging\nimport os\nimport warnings\nfrom pprint import pformat\n\nimport pytest\n\nimport botocore.session\nfrom botocore import xform_name\nfrom botocore.client import ClientError\nfrom botocore.exceptions import ConnectionClosedError\nfrom tests import ClientHTTPStubber\n\n# Mapping of service -> api calls to try.\n# Each api call is a dict of OperationName->params.\n# Empty params means that the operation will be called with no params.  This is\n# used as a quick verification that we can successfully make calls to services.\nSMOKE_TESTS = {\n    'acm': {'ListCertificates': {}},\n    'apigateway': {'GetRestApis': {}},\n    'application-autoscaling': {\n        'DescribeScalableTargets': {'ServiceNamespace': 'ecs'}\n    },\n    'autoscaling': {\n        'DescribeAccountLimits': {},\n        'DescribeAdjustmentTypes': {},\n    },\n    'cloudformation': {'DescribeStacks': {}, 'ListStacks': {}},\n    'cloudfront': {'ListDistributions': {}, 'ListStreamingDistributions': {}},\n    'cloudhsmv2': {'DescribeBackups': {}},\n    'cloudsearch': {'DescribeDomains': {}, 'ListDomainNames': {}},\n    'cloudtrail': {'DescribeTrails': {}},\n    'cloudwatch': {'ListMetrics': {}},\n    'codecommit': {'ListRepositories': {}},\n    'codedeploy': {'ListApplications': {}},\n    'codepipeline': {'ListActionTypes': {}},\n    'cognito-identity': {'ListIdentityPools': {'MaxResults': 1}},\n    'cognito-sync': {'ListIdentityPoolUsage': {}},\n    'config': {'DescribeDeliveryChannels': {}},\n    'datapipeline': {'ListPipelines': {}},\n    'devicefarm': {'ListProjects': {}},\n    'directconnect': {'DescribeConnections': {}},\n    'ds': {'DescribeDirectories': {}},\n    'dynamodb': {'ListTables': {}},\n    'dynamodbstreams': {'ListStreams': {}},\n    'ec2': {'DescribeRegions': {}, 'DescribeInstances': {}},\n    'ecr': {'DescribeRepositories': {}},\n    'ecs': {'DescribeClusters': {}},\n    'elasticache': {'DescribeCacheClusters': {}},\n    'elasticbeanstalk': {'DescribeApplications': {}},\n    'elastictranscoder': {'ListPipelines': {}},\n    'elb': {'DescribeLoadBalancers': {}},\n    'emr': {'ListClusters': {}},\n    'es': {'ListDomainNames': {}},\n    'events': {'ListRules': {}},\n    'firehose': {'ListDeliveryStreams': {}},\n    'gamelift': {'ListBuilds': {}},\n    'glacier': {'ListVaults': {}},\n    'iam': {'ListUsers': {}},\n    # Does not work with session credentials so\n    # importexport tests are not run.\n    # 'importexport': {'ListJobs': {}},\n    'importexport': {},\n    'inspector': {'DescribeCrossAccountAccessRole': {}},\n    'iot': {'DescribeEndpoint': {}},\n    'kinesis': {'ListStreams': {}},\n    'kms': {'ListKeys': {}},\n    'lambda': {'ListFunctions': {}},\n    'logs': {'DescribeLogGroups': {}},\n    # 'opsworks': {'DescribeStacks': {}},\n    'rds': {'DescribeDBInstances': {}},\n    'redshift': {'DescribeClusters': {}},\n    'route53': {'ListHostedZones': {}},\n    'route53domains': {'ListDomains': {}},\n    's3': {'ListBuckets': {}},\n    'sdb': {'ListDomains': {}},\n    'ses': {'ListIdentities': {}},\n    'shield': {'GetSubscriptionState': {}},\n    'sns': {'ListTopics': {}},\n    'sqs': {'ListQueues': {}},\n    'ssm': {'ListDocuments': {}},\n    'storagegateway': {'ListGateways': {}},\n    # sts tests would normally go here, but\n    # there aren't any calls you can make when\n    # using session credentials so we don't run any\n    # sts tests.\n    'sts': {},\n    # 'sts': {'GetSessionToken': {}},\n    # Subscription needed for support API calls.\n    'support': {},\n    'swf': {'ListDomains': {'registrationStatus': 'REGISTERED'}},\n    'waf': {'ListWebACLs': {'Limit': 1}},\n    'workspaces': {'DescribeWorkspaces': {}},\n}\n\n\n# Same thing as the SMOKE_TESTS hash above, except these verify\n# that we get an error response back from the server because\n# we've sent invalid params.\nERROR_TESTS = {\n    'apigateway': {'GetRestApi': {'restApiId': 'fake-id'}},\n    'application-autoscaling': {\n        'DescribeScalableTargets': {\n            'ServiceNamespace': 'fake-service-namespace'\n        }\n    },\n    'autoscaling': {\n        'CreateLaunchConfiguration': {\n            'LaunchConfigurationName': 'foo',\n            'ImageId': 'ami-12345678',\n            'InstanceType': 'm1.small',\n        }\n    },\n    'cloudformation': {\n        'CreateStack': {\n            'StackName': 'fake',\n            'TemplateURL': 'http://s3.amazonaws.com/foo/bar',\n        }\n    },\n    'cloudfront': {'GetDistribution': {'Id': 'fake-id'}},\n    'cloudhsmv2': {'ListTags': {'ResourceId': 'fake-id'}},\n    'cloudsearch': {'DescribeIndexFields': {'DomainName': 'fakedomain'}},\n    'cloudtrail': {'DeleteTrail': {'Name': 'fake-trail'}},\n    'cloudwatch': {\n        'SetAlarmState': {\n            'AlarmName': 'abc',\n            'StateValue': 'mno',\n            'StateReason': 'xyz',\n        }\n    },\n    'logs': {'GetLogEvents': {'logGroupName': 'a', 'logStreamName': 'b'}},\n    'codecommit': {'ListBranches': {'repositoryName': 'fake-repo'}},\n    'codedeploy': {'GetDeployment': {'deploymentId': 'fake-id'}},\n    'codepipeline': {'GetPipeline': {'name': 'fake-pipeline'}},\n    'cognito-identity': {'DescribeIdentityPool': {'IdentityPoolId': 'fake'}},\n    'cognito-sync': {'DescribeIdentityPoolUsage': {'IdentityPoolId': 'fake'}},\n    'config': {\n        'GetResourceConfigHistory': {'resourceType': '', 'resourceId': 'fake'},\n    },\n    'datapipeline': {'GetPipelineDefinition': {'pipelineId': 'fake'}},\n    'devicefarm': {\n        'GetDevice': {'arn': 'arn:aws:devicefarm:REGION::device:f'}\n    },\n    'directconnect': {'DescribeConnections': {'connectionId': 'fake'}},\n    'ds': {'CreateDirectory': {'Name': 'n', 'Password': 'p', 'Size': '1'}},\n    'dynamodb': {'DescribeTable': {'TableName': 'fake'}},\n    'dynamodbstreams': {'DescribeStream': {'StreamArn': 'x' * 37}},\n    'ec2': {'DescribeInstances': {'InstanceIds': ['i-12345678']}},\n    'ecs': {'StopTask': {'task': 'fake'}},\n    'efs': {'DeleteFileSystem': {'FileSystemId': 'fake'}},\n    'elasticache': {'DescribeCacheClusters': {'CacheClusterId': 'fake'}},\n    'elasticbeanstalk': {\n        'DescribeEnvironmentResources': {'EnvironmentId': 'x'},\n    },\n    'elb': {'DescribeLoadBalancers': {'LoadBalancerNames': ['fake']}},\n    'elastictranscoder': {'ReadJob': {'Id': 'fake'}},\n    'emr': {'DescribeCluster': {'ClusterId': 'fake'}},\n    'es': {'DescribeElasticsearchDomain': {'DomainName': 'not-a-domain'}},\n    'gamelift': {'DescribeBuild': {'BuildId': 'fake-build-id'}},\n    'glacier': {'ListVaults': {'accountId': 'fake'}},\n    'iam': {'GetUser': {'UserName': 'fake'}},\n    'kinesis': {'DescribeStream': {'StreamName': 'fake'}},\n    'kms': {'GetKeyPolicy': {'KeyId': 'fake', 'PolicyName': 'fake'}},\n    'lambda': {'Invoke': {'FunctionName': 'fake'}},\n    # 'opsworks': {'DescribeLayers': {'StackId': 'fake'}},\n    'rds': {'DescribeDBInstances': {'DBInstanceIdentifier': 'fake'}},\n    'redshift': {'DescribeClusters': {'ClusterIdentifier': 'fake'}},\n    'route53': {'GetHostedZone': {'Id': 'fake'}},\n    'route53domains': {'GetDomainDetail': {'DomainName': 'fake'}},\n    's3': {'ListObjects': {'Bucket': 'thisbucketdoesnotexistasdf'}},\n    'ses': {'VerifyEmailIdentity': {'EmailAddress': 'fake'}},\n    'sdb': {'CreateDomain': {'DomainName': ''}},\n    'sns': {\n        'ConfirmSubscription': {'TopicArn': 'a', 'Token': 'b'},\n        'Publish': {'Message': 'hello', 'TopicArn': 'fake'},\n    },\n    'sqs': {'GetQueueUrl': {'QueueName': 'fake'}},\n    'ssm': {'GetDocument': {'Name': 'fake'}},\n    'storagegateway': {'ListVolumes': {'GatewayARN': 'x' * 50}},\n    'sts': {'GetFederationToken': {'Name': 'fake', 'Policy': 'fake'}},\n    'support': {\n        'CreateCase': {\n            'subject': 'x',\n            'communicationBody': 'x',\n            'categoryCode': 'x',\n            'serviceCode': 'x',\n            'severityCode': 'low',\n        }\n    },\n    'swf': {'DescribeDomain': {'name': 'fake'}},\n    'waf': {'GetWebACL': {'WebACLId': 'fake'}},\n    'workspaces': {'DescribeWorkspaces': {'DirectoryId': 'fake-directory-id'}},\n}\n\nREGION = 'us-east-1'\nREGION_OVERRIDES = {\n    'devicefarm': 'us-west-2',\n    'efs': 'us-west-2',\n    'inspector': 'us-west-2',\n}\nMAX_RETRIES = 8\nlogger = logging.getLogger(__name__)\n\n\ndef _get_client(session, service):\n    if os.environ.get('AWS_SMOKE_TEST_REGION', ''):\n        region_name = os.environ['AWS_SMOKE_TEST_REGION']\n    else:\n        region_name = REGION_OVERRIDES.get(service, REGION)\n    client = session.create_client(service, region_name=region_name)\n    client.meta.events.register_first('needs-retry.*.*', retry_handler)\n    return client\n\n\ndef retry_handler(response, attempts, **kwargs):\n    if response is not None:\n        _, parsed = response\n        code = parsed.get('Error', {}).get('Code')\n        # Catch ThrottleException, Throttling.\n        is_throttle_error = code is not None and 'throttl' in code.lower()\n        if is_throttle_error and attempts <= MAX_RETRIES:\n            # We want the exponential behavior with a fixed 10 second\n            # minimum, e.g. 11, 12, 14, 18, 26.  With a max retries of 8,\n            # this is about 7-8 minutes total we'll retry.\n            retry_delay = (2 ** (attempts - 1)) + 10\n            logger.debug(\"Using custom retry delay of: %s\", retry_delay)\n            return retry_delay\n\n\ndef _list_services(dict_entries):\n    # List all services in the provided dict_entry.\n    # If the AWS_SMOKE_TEST_SERVICES is provided,\n    # it's a comma separated list of services you can provide\n    # if you only want to run the smoke tests for certain services.\n    if 'AWS_SMOKE_TEST_SERVICES' not in os.environ:\n        return dict_entries.keys()\n    else:\n        wanted_services = os.environ.get('AWS_SMOKE_TEST_SERVICES', '').split(\n            ','\n        )\n        return [key for key in dict_entries if key in wanted_services]\n\n\n@pytest.fixture()\ndef botocore_session():\n    return botocore.session.get_session()\n\n\ndef _smoke_tests():\n    for service_name in _list_services(SMOKE_TESTS):\n        for operation_name in SMOKE_TESTS[service_name]:\n            kwargs = SMOKE_TESTS[service_name][operation_name]\n            yield service_name, operation_name, kwargs\n\n\ndef _error_tests():\n    for service_name in _list_services(ERROR_TESTS):\n        for operation_name in ERROR_TESTS[service_name]:\n            kwargs = ERROR_TESTS[service_name][operation_name]\n            yield service_name, operation_name, kwargs\n\n\n@pytest.mark.parametrize(\n    \"service_name, operation_name, kwargs\", _smoke_tests()\n)\ndef test_can_make_request_with_client(\n    botocore_session, service_name, operation_name, kwargs\n):\n    # Same as test_can_make_request, but with Client objects\n    # instead of service/operations.\n    client = _get_client(botocore_session, service_name)\n    method = getattr(client, xform_name(operation_name))\n    with warnings.catch_warnings(record=True) as caught_warnings:\n        response = method(**kwargs)\n        err_msg = f\"Warnings were emitted during smoke test: {caught_warnings}\"\n        assert len(caught_warnings) == 0, err_msg\n        assert 'Errors' not in response\n\n\n@pytest.mark.parametrize(\n    \"service_name, operation_name, kwargs\", _error_tests()\n)\ndef test_can_make_request_and_understand_errors_with_client(\n    botocore_session, service_name, operation_name, kwargs\n):\n    client = _get_client(botocore_session, service_name)\n    method = getattr(client, xform_name(operation_name))\n    with pytest.raises(ClientError):\n        method(**kwargs)\n\n\n@pytest.mark.parametrize(\n    \"service_name, operation_name, kwargs\", _smoke_tests()\n)\ndef test_client_can_retry_request_properly(\n    botocore_session, service_name, operation_name, kwargs\n):\n    client = _get_client(botocore_session, service_name)\n    operation = getattr(client, xform_name(operation_name))\n    exception = ConnectionClosedError(endpoint_url='https://mock.eror')\n    with ClientHTTPStubber(client, strict=False) as http_stubber:\n        http_stubber.responses.append(exception)\n        try:\n            operation(**kwargs)\n        except ClientError as e:\n            assert False, (\n                'Request was not retried properly, '\n                'received error:\\n%s' % pformat(e)\n            )\n        # Ensure we used the stubber as we're not using it in strict mode\n        assert len(http_stubber.responses) == 0, 'Stubber was not used!'\n", "tests/integration/__init__.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/integration/test_session.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore.session\nfrom tests import unittest\n\n\nclass TestCanChangeParsing(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n\n    def test_can_change_timestamp_with_clients(self):\n        factory = self.session.get_component('response_parser_factory')\n        factory.set_parser_defaults(timestamp_parser=lambda x: str(x))\n\n        # Now if we get a response with timestamps in the model, they\n        # will be returned as strings. We're testing service/operation\n        # objects, but we should also add a test for clients.\n        s3 = self.session.create_client('s3', 'us-west-2')\n        parsed = s3.list_buckets()\n        dates = [bucket['CreationDate'] for bucket in parsed['Buckets']]\n        self.assertTrue(\n            all(isinstance(date, str) for date in dates),\n            \"Expected all str types but instead got: %s\" % dates,\n        )\n\n    def test_maps_service_name_when_overriden(self):\n        ses = self.session.get_service_model('ses')\n        self.assertEqual(ses.endpoint_prefix, 'email')\n        # But we should map the service_name to be the same name\n        # used when calling get_service_model which is different\n        # than the endpoint_prefix.\n        self.assertEqual(ses.service_name, 'ses')\n\n    def test_maps_service_name_from_client(self):\n        # Same thing as test_maps_service_name_from_client,\n        # except through the client interface.\n        client = self.session.create_client('ses', region_name='us-east-1')\n        self.assertEqual(client.meta.service_model.service_name, 'ses')\n", "tests/integration/test_utils.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nimport botocore.session\nfrom botocore.utils import ArgumentGenerator\n\n\n@pytest.fixture(scope=\"module\")\ndef generator():\n    return ArgumentGenerator()\n\n\ndef _all_inputs():\n    session = botocore.session.get_session()\n    for service_name in session.get_available_services():\n        service_model = session.get_service_model(service_name)\n        for operation_name in service_model.operation_names:\n            operation_model = service_model.operation_model(operation_name)\n            input_shape = operation_model.input_shape\n            if input_shape is not None and input_shape.members:\n                yield input_shape, service_name, operation_name\n\n\n@pytest.mark.parametrize(\n    \"input_shape, service_name, operation_name\", _all_inputs()\n)\ndef test_can_generate_all_inputs(\n    generator, input_shape, service_name, operation_name\n):\n    generated = generator.generate_skeleton(input_shape)\n    # Do some basic sanity checks to make sure the generated shape\n    # looks right.  We're mostly just ensuring that the generate_skeleton\n    # doesn't throw an exception.\n    assert isinstance(generated, dict)\n\n    # The generated skeleton also shouldn't be empty (the test\n    # generator has already filtered out input_shapes of None).\n    assert len(generated) > 0\n", "tests/integration/test_elastictranscoder.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport botocore.session\nfrom tests import random_chars, unittest\n\nDEFAULT_ROLE_POLICY = \"\"\"\\\n{\"Statement\": [\n    {\n        \"Action\": \"sts:AssumeRole\",\n        \"Principal\": {\n            \"Service\": \"elastictranscoder.amazonaws.com\"\n        },\n        \"Effect\": \"Allow\",\n        \"Sid\": \"1\"\n    }\n]}\n\"\"\"\n\n\nclass TestElasticTranscoder(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client(\n            'elastictranscoder', 'us-east-1'\n        )\n        self.s3_client = self.session.create_client('s3', 'us-east-1')\n        self.iam_client = self.session.create_client('iam', 'us-east-1')\n\n    def create_bucket(self):\n        bucket_name = 'ets-bucket-1-%s' % random_chars(50)\n        self.s3_client.create_bucket(Bucket=bucket_name)\n        waiter = self.s3_client.get_waiter('bucket_exists')\n        waiter.wait(Bucket=bucket_name)\n        self.addCleanup(self.s3_client.delete_bucket, Bucket=bucket_name)\n        return bucket_name\n\n    def create_iam_role(self):\n        role_name = 'ets-role-name-1-%s' % random_chars(10)\n        parsed = self.iam_client.create_role(\n            RoleName=role_name, AssumeRolePolicyDocument=DEFAULT_ROLE_POLICY\n        )\n        arn = parsed['Role']['Arn']\n        self.addCleanup(self.iam_client.delete_role, RoleName=role_name)\n        return arn\n\n    def test_list_streams(self):\n        parsed = self.client.list_pipelines()\n        self.assertIn('Pipelines', parsed)\n\n    def test_list_presets(self):\n        parsed = self.client.list_presets(Ascending='true')\n        self.assertIn('Presets', parsed)\n\n    def test_create_pipeline(self):\n        # In order to create a pipeline, we need to create 2 s3 buckets\n        # and 1 iam role.\n        input_bucket = self.create_bucket()\n        output_bucket = self.create_bucket()\n        role = self.create_iam_role()\n        pipeline_name = 'botocore-test-create-%s' % random_chars(10)\n\n        parsed = self.client.create_pipeline(\n            InputBucket=input_bucket,\n            OutputBucket=output_bucket,\n            Role=role,\n            Name=pipeline_name,\n            Notifications={\n                'Progressing': '',\n                'Completed': '',\n                'Warning': '',\n                'Error': '',\n            },\n        )\n        pipeline_id = parsed['Pipeline']['Id']\n        self.addCleanup(self.client.delete_pipeline, Id=pipeline_id)\n        self.assertIn('Pipeline', parsed)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/functional/test_paginator_config.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport string\n\nimport jmespath\nimport pytest\nfrom jmespath.exceptions import JMESPathError\n\nimport botocore.session\n\nKNOWN_PAGE_KEYS = {\n    'input_token',\n    'py_input_token',\n    'output_token',\n    'result_key',\n    'limit_key',\n    'more_results',\n    'non_aggregate_keys',\n}\nMEMBER_NAME_CHARS = set(string.ascii_letters + string.digits)\n# The goal here should be to remove all of these by updating the paginators\n# to reference all the extra output keys. Nothing should ever be added to this\n# list, it represents all the current released paginators that fail this test.\nKNOWN_EXTRA_OUTPUT_KEYS = [\n    'apigateway.GetApiKeys.warnings',\n    'apigateway.GetUsage.usagePlanId',\n    'apigateway.GetUsage.startDate',\n    'apigateway.GetUsage.endDate',\n    'athena.GetQueryResults.ResultSet',\n    'cloudfront.ListCloudFrontOriginAccessIdentities.CloudFrontOriginAccessIdentityList',\n    'cloudfront.ListDistributions.DistributionList',\n    'cloudfront.ListInvalidations.InvalidationList',\n    'cloudfront.ListStreamingDistributions.StreamingDistributionList',\n    'cloudfront.ListKeyValueStores.KeyValueStoreList',\n    'codedeploy.ListDeploymentGroups.applicationName',\n    'dms.DescribeTableStatistics.ReplicationTaskArn',\n    'dms.DescribeReplicationTaskAssessmentResults.BucketName',\n    'ec2.DescribeSpotFleetInstances.SpotFleetRequestId',\n    'ec2.DescribeVpcEndpointServices.ServiceNames',\n    'efs.DescribeFileSystems.Marker',\n    'efs.DescribeMountTargets.Marker',\n    'efs.DescribeTags.Marker',\n    'elasticache.DescribeCacheParameters.CacheNodeTypeSpecificParameters',\n    'elasticache.DescribeEngineDefaultParameters.EngineDefaults',\n    'glacier.ListParts.PartSizeInBytes',\n    'glacier.ListParts.ArchiveDescription',\n    'glacier.ListParts.MultipartUploadId',\n    'glacier.ListParts.VaultARN',\n    'glacier.ListParts.CreationDate',\n    'kinesis.DescribeStream.StreamDescription',\n    'mturk.ListAssignmentsForHIT.NumResults',\n    'mturk.ListQualificationTypes.NumResults',\n    'mturk.ListHITs.NumResults',\n    'mturk.ListWorkerBlocks.NumResults',\n    'mturk.ListReviewableHITs.NumResults',\n    'mturk.ListHITsForQualificationType.NumResults',\n    'mturk.ListQualificationRequests.NumResults',\n    'mturk.ListWorkersWithQualificationType.NumResults',\n    'mturk.ListBonusPayments.NumResults',\n    'neptune.DescribeEngineDefaultParameters.EngineDefaults',\n    'rds.DescribeEngineDefaultClusterParameters.EngineDefaults',\n    'rds.DescribeEngineDefaultParameters.EngineDefaults',\n    'redshift.DescribeDefaultClusterParameters.DefaultClusterParameters',\n    'resource-groups.ListGroups.GroupIdentifiers',\n    'resource-groups.SearchResources.QueryErrors',\n    'resource-groups.ListGroupResources.QueryErrors',\n    'route53.ListHealthChecks.MaxItems',\n    'route53.ListHealthChecks.Marker',\n    'route53.ListHostedZones.MaxItems',\n    'route53.ListHostedZones.Marker',\n    'route53.ListResourceRecordSets.MaxItems',\n    's3.ListMultipartUploads.Delimiter',\n    's3.ListMultipartUploads.KeyMarker',\n    's3.ListMultipartUploads.Prefix',\n    's3.ListMultipartUploads.Bucket',\n    's3.ListMultipartUploads.MaxUploads',\n    's3.ListMultipartUploads.UploadIdMarker',\n    's3.ListMultipartUploads.EncodingType',\n    's3.ListObjectVersions.MaxKeys',\n    's3.ListObjectVersions.Delimiter',\n    's3.ListObjectVersions.VersionIdMarker',\n    's3.ListObjectVersions.KeyMarker',\n    's3.ListObjectVersions.Prefix',\n    's3.ListObjectVersions.Name',\n    's3.ListObjectVersions.EncodingType',\n    's3.ListObjects.MaxKeys',\n    's3.ListObjects.Delimiter',\n    's3.ListObjects.NextMarker',\n    's3.ListObjects.Prefix',\n    's3.ListObjects.Marker',\n    's3.ListObjects.Name',\n    's3.ListObjects.EncodingType',\n    's3.ListObjectsV2.StartAfter',\n    's3.ListObjectsV2.MaxKeys',\n    's3.ListObjectsV2.Delimiter',\n    's3.ListObjectsV2.ContinuationToken',\n    's3.ListObjectsV2.KeyCount',\n    's3.ListObjectsV2.Prefix',\n    's3.ListObjectsV2.Name',\n    's3.ListObjectsV2.EncodingType',\n    's3.ListParts.PartNumberMarker',\n    's3.ListParts.AbortDate',\n    's3.ListParts.MaxParts',\n    's3.ListParts.Bucket',\n    's3.ListParts.Key',\n    's3.ListParts.UploadId',\n    's3.ListParts.AbortRuleId',\n    's3.ListParts.RequestCharged',\n    'sms.GetReplicationRuns.replicationJob',\n    'sms.GetServers.lastModifiedOn',\n    'sms.GetServers.serverCatalogStatus',\n    'storagegateway.DescribeTapeRecoveryPoints.GatewayARN',\n    'storagegateway.DescribeVTLDevices.GatewayARN',\n    'storagegateway.ListVolumes.GatewayARN',\n    'workdocs.DescribeUsers.TotalNumberOfUsers',\n    'xray.BatchGetTraces.UnprocessedTraceIds',\n    'xray.GetServiceGraph.EndTime',\n    'xray.GetServiceGraph.ContainsOldGroupVersions',\n    'xray.GetServiceGraph.StartTime',\n    'xray.GetTraceSummaries.TracesProcessedCount',\n    'xray.GetTraceSummaries.ApproximateTime',\n]\n\n\ndef _pagination_configs():\n    session = botocore.session.get_session()\n    loader = session.get_component('data_loader')\n    services = loader.list_available_services('paginators-1')\n    for service_name in services:\n        service_model = session.get_service_model(service_name)\n        page_config = loader.load_service_model(\n            service_name, 'paginators-1', service_model.api_version\n        )\n        for op_name, single_config in page_config['pagination'].items():\n            yield (op_name, single_config, service_model)\n\n\n@pytest.mark.validates_models\n@pytest.mark.parametrize(\n    \"operation_name, page_config, service_model\", _pagination_configs()\n)\ndef test_lint_pagination_configs(operation_name, page_config, service_model):\n    _validate_known_pagination_keys(page_config)\n    _valiate_result_key_exists(page_config)\n    _validate_referenced_operation_exists(operation_name, service_model)\n    _validate_operation_has_output(operation_name, service_model)\n    _validate_input_keys_match(operation_name, page_config, service_model)\n    _validate_output_keys_match(operation_name, page_config, service_model)\n\n\ndef _validate_known_pagination_keys(page_config):\n    for key in page_config:\n        if key not in KNOWN_PAGE_KEYS:\n            raise AssertionError(\n                \"Unknown key '%s' in pagination config: %s\"\n                % (key, page_config)\n            )\n\n\ndef _valiate_result_key_exists(page_config):\n    if 'result_key' not in page_config:\n        raise AssertionError(\n            \"Required key 'result_key' is missing \"\n            \"from pagination config: %s\" % page_config\n        )\n\n\ndef _validate_referenced_operation_exists(operation_name, service_model):\n    if operation_name not in service_model.operation_names:\n        raise AssertionError(\n            \"Pagination config refers to operation that \"\n            \"does not exist: %s\" % operation_name\n        )\n\n\ndef _validate_operation_has_output(operation_name, service_model):\n    op_model = service_model.operation_model(operation_name)\n    output = op_model.output_shape\n    if output is None or not output.members:\n        raise AssertionError(\n            \"Pagination config refers to operation \"\n            \"that does not have any output: %s\" % operation_name\n        )\n\n\ndef _validate_input_keys_match(operation_name, page_config, service_model):\n    input_tokens = page_config['input_token']\n    if not isinstance(input_tokens, list):\n        input_tokens = [input_tokens]\n    valid_input_names = service_model.operation_model(\n        operation_name\n    ).input_shape.members\n    for token in input_tokens:\n        if token not in valid_input_names:\n            raise AssertionError(\n                \"input_token '%s' refers to a non existent \"\n                \"input member for operation: %s\" % (token, operation_name)\n            )\n    if 'limit_key' in page_config:\n        limit_key = page_config['limit_key']\n        if limit_key not in valid_input_names:\n            raise AssertionError(\n                \"limit_key '%s' refers to a non existent \"\n                \"input member for operation: %s, valid keys: \"\n                \"%s\"\n                % (\n                    limit_key,\n                    operation_name,\n                    ', '.join(list(valid_input_names)),\n                )\n            )\n\n\ndef _validate_output_keys_match(operation_name, page_config, service_model):\n    # NOTE: The original version of this function from translate.py had logic\n    # to ensure that the entire set of output_members was accounted for in the\n    # union of 'result_key', 'output_token', 'more_results', and\n    # 'non_aggregate_keys'.\n    # There's enough state drift (especially with non_aggregate_keys) that\n    # this is no longer a realistic thing to check.  Someone would have to\n    # backport the missing keys to all the paginators.\n    output_shape = service_model.operation_model(operation_name).output_shape\n    output_members = set(output_shape.members)\n    for key_name, output_key in _get_all_page_output_keys(page_config):\n        if _looks_like_jmespath(output_key):\n            _validate_jmespath_compiles(output_key)\n        else:\n            if output_key not in output_members:\n                raise AssertionError(\n                    \"Pagination key '%s' refers to an output \"\n                    \"member that does not exist: %s\" % (key_name, output_key)\n                )\n            output_members.remove(output_key)\n\n    for member in list(output_members):\n        key = \"{}.{}.{}\".format(\n            service_model.service_name, operation_name, member\n        )\n        if key in KNOWN_EXTRA_OUTPUT_KEYS:\n            output_members.remove(member)\n\n    if output_members:\n        raise AssertionError(\n            \"There are member names in the output shape of \"\n            \"%s that are not accounted for in the pagination \"\n            \"config for service %s: %s\"\n            % (\n                operation_name,\n                service_model.service_name,\n                ', '.join(output_members),\n            )\n        )\n\n\ndef _looks_like_jmespath(expression):\n    if all(ch in MEMBER_NAME_CHARS for ch in expression):\n        return False\n    return True\n\n\ndef _validate_jmespath_compiles(expression):\n    try:\n        jmespath.compile(expression)\n    except JMESPathError as e:\n        raise AssertionError(\n            \"Invalid JMESPath expression used \"\n            \"in pagination config: %s\\nerror: %s\" % (expression, e)\n        )\n\n\ndef _get_all_page_output_keys(page_config):\n    for key in _get_list_value(page_config, 'result_key'):\n        yield 'result_key', key\n    for key in _get_list_value(page_config, 'output_token'):\n        yield 'output_token', key\n    if 'more_results' in page_config:\n        yield 'more_results', page_config['more_results']\n    for key in page_config.get('non_aggregate_keys', []):\n        yield 'non_aggregate_keys', key\n\n\ndef _get_list_value(page_config, key):\n    # Some pagination config values can be a scalar value or a list of scalars.\n    # This function will always return a list of scalar values, converting as\n    # necessary.\n    value = page_config[key]\n    if not isinstance(value, list):\n        value = [value]\n    return value\n", "tests/functional/test_endpoint_rulesets.py": "# Copyright 2012-2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport json\nfrom functools import lru_cache\nfrom pathlib import Path\n\nimport pytest\n\nfrom botocore import xform_name\nfrom botocore.compat import HAS_CRT\nfrom botocore.config import Config\nfrom botocore.endpoint_provider import EndpointProvider\nfrom botocore.exceptions import (\n    BotoCoreError,\n    ClientError,\n    EndpointResolutionError,\n)\nfrom botocore.loaders import Loader\nfrom botocore.parsers import ResponseParserError\nfrom tests import ClientHTTPStubber\n\nENDPOINT_TESTDATA_DIR = Path(__file__).parent / 'endpoint-rules'\nLOADER = Loader()\n\n# For the purpose of the tests in this file, only services for which an\n# endpoint ruleset file exists matter. The existence of required endpoint\n# ruleset files is asserted for in tests/functional/test_model_completeness.py\nALL_SERVICES = [\n    service_name\n    for service_name in LOADER.list_available_services(\n        type_name='endpoint-rule-set-1'\n    )\n]\n\n\n@pytest.fixture(scope='module')\ndef partitions():\n    return LOADER.load_data('partitions')\n\n\n@lru_cache\ndef get_endpoint_tests_for_service(service_name):\n    file_path = ENDPOINT_TESTDATA_DIR / service_name / 'endpoint-tests-1.json'\n    if not file_path.is_file():\n        raise FileNotFoundError(\n            f'Cannot find endpoint tests file for \"{service_name}\" at '\n            'path {file_path}'\n        )\n    with file_path.open('r') as f:\n        return json.load(f)\n\n\n@pytest.mark.parametrize(\"service_name\", ALL_SERVICES)\ndef test_all_endpoint_tests_exist(service_name):\n    \"\"\"Tests the existence of endpoint-tests-1.json for each service that has\n    a ruleset and verifies that content is present.\"\"\"\n    data = get_endpoint_tests_for_service(service_name)\n    assert len(data['testCases']) > 0\n\n\ndef assert_all_signing_region_sets_have_length_one(rule):\n    \"\"\"Helper function for test_all_signing_region_sets_have_length_one()\"\"\"\n    if 'endpoint' in rule:\n        authSchemes = (\n            rule['endpoint'].get('properties', {}).get('authSchemes', [])\n        )\n        for authScheme in authSchemes:\n            if 'signingRegionSet' in authScheme:\n                assert len(authScheme['signingRegionSet']) == 1\n    for sub_rule in rule.get('rules', []):\n        assert_all_signing_region_sets_have_length_one(sub_rule)\n\n\n@pytest.mark.parametrize(\"service_name\", ALL_SERVICES)\ndef test_all_signing_region_sets_have_length_one(service_name):\n    \"\"\"Checks all endpoint rulesets for endpoints that contain an authSchemes\n    property with a `signingRegionSet` and asserts that it is a list of\n    length 1.\n\n    In theory, `signingRegionSet` could have >1 entries. As of writing this\n    test, no service uses >1 entry, the meaning of >1 entry is poorly defined,\n    and botocore cannot handle >1 entry. This test exists specifically to\n    fail if a ruleset ever uses >1 entry.\n\n    The test also fails for empty lists. While botocore would handle these\n    gracefully, the expected behavior for empty `signingRegionSet` lists is\n    not defined.\n    \"\"\"\n    ruleset = LOADER.load_service_model(service_name, 'endpoint-rule-set-1')\n    assert_all_signing_region_sets_have_length_one(ruleset)\n\n\ndef test_assert_all_signing_region_sets_have_length_one():\n    \"\"\"Negative test for to confirm that\n    assert_all_signing_region_sets_have_length_one() actually fails when two\n    sigingRegionSet entries are present.\"\"\"\n    with pytest.raises(AssertionError):\n        assert_all_signing_region_sets_have_length_one(\n            {\n                \"version\": \"1.0\",\n                \"parameters\": {},\n                \"rules\": [\n                    {\n                        \"conditions\": [],\n                        \"endpoint\": {\n                            \"url\": \"https://foo\",\n                            \"properties\": {\n                                \"authSchemes\": [\n                                    {\n                                        \"name\": \"sigv4a\",\n                                        \"disableDoubleEncoding\": True,\n                                        \"signingRegionSet\": [\"*\", \"abc\"],\n                                        \"signingName\": \"myservice\",\n                                    }\n                                ]\n                            },\n                            \"headers\": {},\n                        },\n                        \"type\": \"endpoint\",\n                    }\n                ],\n            }\n        )\n\n\ndef iter_all_test_cases():\n    for service_name in ALL_SERVICES:\n        test_data = get_endpoint_tests_for_service(service_name)\n        for test_case in test_data['testCases']:\n            yield service_name, test_case\n\n\ndef iter_provider_test_cases_that_produce(endpoints=False, errors=False):\n    for service_name, test in iter_all_test_cases():\n        input_params = test.get('params', {})\n        expected_object = test['expect']\n        if endpoints and 'endpoint' in expected_object:\n            yield service_name, input_params, expected_object['endpoint']\n        if errors and 'error' in expected_object:\n            yield service_name, input_params, expected_object['error']\n\n\ndef iter_e2e_test_cases_that_produce(endpoints=False, errors=False):\n    for service_name, test in iter_all_test_cases():\n        # Not all test cases contain operation inputs for end-to-end tests.\n        if 'operationInputs' not in test:\n            continue\n        # Each test case can contain a list of input sets for the same\n        # expected result.\n        for op_inputs in test['operationInputs']:\n            op_params = op_inputs.get('operationParams', {})\n            # Test cases that use invalid bucket names as inputs fail in\n            # botocore because botocore validated bucket names before running\n            # endpoint resolution.\n            if op_params.get('Bucket') in ['bucket name', 'example.com#']:\n                continue\n            op_name = op_inputs['operationName']\n            builtins = op_inputs.get('builtInParams', {})\n\n            expected_object = test['expect']\n            if endpoints and 'endpoint' in expected_object:\n                expected_endpoint = expected_object['endpoint']\n                expected_props = expected_endpoint.get('properties', {})\n                expected_authschemes = [\n                    auth_scheme['name']\n                    for auth_scheme in expected_props.get('authSchemes', [])\n                ]\n                yield pytest.param(\n                    service_name,\n                    op_name,\n                    op_params,\n                    builtins,\n                    expected_endpoint,\n                    marks=pytest.mark.skipif(\n                        'sigv4a' in expected_authschemes and not HAS_CRT,\n                        reason=\"Test case expects sigv4a which requires CRT\",\n                    ),\n                )\n            if errors and 'error' in expected_object:\n                yield pytest.param(\n                    service_name,\n                    op_name,\n                    op_params,\n                    builtins,\n                    expected_object['error'],\n                )\n\n\n@pytest.mark.parametrize(\n    'service_name, input_params, expected_endpoint',\n    iter_provider_test_cases_that_produce(endpoints=True),\n)\ndef test_endpoint_provider_test_cases_yielding_endpoints(\n    partitions, service_name, input_params, expected_endpoint\n):\n    ruleset = LOADER.load_service_model(service_name, 'endpoint-rule-set-1')\n    endpoint_provider = EndpointProvider(ruleset, partitions)\n    endpoint = endpoint_provider.resolve_endpoint(**input_params)\n    assert endpoint.url == expected_endpoint['url']\n    assert endpoint.properties == expected_endpoint.get('properties', {})\n    assert endpoint.headers == expected_endpoint.get('headers', {})\n\n\n@pytest.mark.parametrize(\n    'service_name, input_params, expected_error',\n    iter_provider_test_cases_that_produce(errors=True),\n)\ndef test_endpoint_provider_test_cases_yielding_errors(\n    partitions, service_name, input_params, expected_error\n):\n    ruleset = LOADER.load_service_model(service_name, 'endpoint-rule-set-1')\n    endpoint_provider = EndpointProvider(ruleset, partitions)\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        endpoint_provider.resolve_endpoint(**input_params)\n    assert str(exc_info.value) == expected_error\n\n\n@pytest.mark.parametrize(\n    'service_name, op_name, op_params, builtin_params, expected_endpoint',\n    iter_e2e_test_cases_that_produce(endpoints=True),\n)\ndef test_end_to_end_test_cases_yielding_endpoints(\n    patched_session,\n    service_name,\n    op_name,\n    op_params,\n    builtin_params,\n    expected_endpoint,\n):\n    def builtin_overwriter_handler(builtins, **kwargs):\n        # must edit builtins dict in place but need to erase all existing\n        # entries\n        for key in list(builtins.keys()):\n            del builtins[key]\n        for key, val in builtin_params.items():\n            builtins[key] = val\n\n    region = builtin_params.get('AWS::Region', 'us-east-1')\n    client = patched_session.create_client(\n        service_name,\n        region_name=region,\n        # endpoint ruleset test cases do not account for host prefixes from the\n        # operation model\n        config=Config(inject_host_prefix=False),\n    )\n    client.meta.events.register_last(\n        'before-endpoint-resolution', builtin_overwriter_handler\n    )\n    with ClientHTTPStubber(client, strict=True) as http_stubber:\n        http_stubber.add_response(status=418)\n        op_fn = getattr(client, xform_name(op_name))\n        try:\n            op_fn(**op_params)\n        except (ClientError, ResponseParserError):\n            pass\n        assert len(http_stubber.requests) > 0\n        actual_url = http_stubber.requests[0].url\n        assert actual_url.startswith(\n            expected_endpoint['url']\n        ), f\"{actual_url} does not start with {expected_endpoint['url']}\"\n\n\n@pytest.mark.parametrize(\n    'service_name, op_name, op_params, builtin_params, expected_error',\n    iter_e2e_test_cases_that_produce(errors=True),\n)\ndef test_end_to_end_test_cases_yielding_errors(\n    patched_session,\n    service_name,\n    op_name,\n    op_params,\n    builtin_params,\n    expected_error,\n):\n    def builtin_overwriter_handler(builtins, **kwargs):\n        # must edit builtins dict in place but need to erase all existing\n        # entries\n        for key in list(builtins.keys()):\n            del builtins[key]\n        for key, val in builtin_params.items():\n            builtins[key] = val\n\n    region = builtin_params.get('AWS::Region', 'us-east-1')\n    client = patched_session.create_client(service_name, region_name=region)\n    client.meta.events.register_last(\n        'before-endpoint-resolution', builtin_overwriter_handler\n    )\n    with ClientHTTPStubber(client, strict=True) as http_stubber:\n        http_stubber.add_response(status=418)\n        op_fn = getattr(client, xform_name(op_name))\n        with pytest.raises(BotoCoreError):\n            try:\n                op_fn(**op_params)\n            except (ClientError, ResponseParserError):\n                pass\n        assert len(http_stubber.requests) == 0\n", "tests/functional/test_machinelearning.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestMachineLearning(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client(\n            'machinelearning', self.region\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n    def test_predict(self):\n        self.http_stubber.add_response(body=b'{}')\n        with self.http_stubber:\n            custom_endpoint = 'https://myendpoint.amazonaws.com/'\n            self.client.predict(\n                MLModelId='ml-foo',\n                Record={'Foo': 'Bar'},\n                PredictEndpoint=custom_endpoint,\n            )\n            sent_request = self.http_stubber.requests[0]\n            self.assertEqual(sent_request.url, custom_endpoint)\n", "tests/functional/test_response_shadowing.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nfrom tests import ALL_SERVICES\n\n\ndef _all_service_error_shapes():\n    for service_model in ALL_SERVICES:\n        yield from service_model.error_shapes\n\n\ndef _all_operations():\n    for service_model in ALL_SERVICES:\n        for operation_name in service_model.operation_names:\n            yield service_model.operation_model(operation_name).output_shape\n\n\ndef _assert_not_shadowed(key, shape):\n    if not shape:\n        return\n\n    assert (\n        key not in shape.members\n    ), f'Found shape \"{shape.name}\" that shadows the botocore response key \"{key}\"'\n\n\n@pytest.mark.parametrize(\"operation_output_shape\", _all_operations())\ndef test_response_metadata_is_not_shadowed(operation_output_shape):\n    _assert_not_shadowed('ResponseMetadata', operation_output_shape)\n\n\n@pytest.mark.parametrize(\"error_shape\", _all_service_error_shapes())\ndef test_exceptions_do_not_shadow_response_metadata(error_shape):\n    _assert_not_shadowed('ResponseMetadata', error_shape)\n\n\n@pytest.mark.parametrize(\"error_shape\", _all_service_error_shapes())\ndef test_exceptions_do_not_shadow_error(error_shape):\n    _assert_not_shadowed('Error', error_shape)\n", "tests/functional/test_alias.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nimport botocore.session\nfrom botocore.exceptions import ParamValidationError\nfrom botocore.stub import Stubber\n\nALIAS_CASES = [\n    {\n        'service': 'ec2',\n        'operation': 'describe_flow_logs',\n        'original_name': 'Filter',\n        'new_name': 'Filters',\n        'parameter_value': [{'Name': 'traffic-type', 'Values': ['ACCEPT']}],\n    },\n    {\n        'service': 'cloudsearchdomain',\n        'operation': 'search',\n        'original_name': 'return',\n        'new_name': 'returnFields',\n        'parameter_value': '_all_fields',\n        'extra_args': {'query': 'foo'},\n    },\n    {\n        'service': 'logs',\n        'operation': 'create_export_task',\n        'original_name': 'from',\n        'new_name': 'fromTime',\n        'parameter_value': 0,\n        'extra_args': {\n            'logGroupName': 'name',\n            'to': 10,\n            'destination': 'mybucket',\n        },\n    },\n]\n\n\n@pytest.mark.parametrize(\"case\", ALIAS_CASES)\ndef test_can_use_alias(case):\n    session = botocore.session.get_session()\n    _can_use_parameter_in_client_call(session, case)\n\n\n@pytest.mark.parametrize(\"case\", ALIAS_CASES)\ndef test_can_use_original_name(case):\n    session = botocore.session.get_session()\n    _can_use_parameter_in_client_call(session, case, False)\n\n\ndef _can_use_parameter_in_client_call(session, case, use_alias=True):\n    client = session.create_client(\n        case['service'],\n        region_name='us-east-1',\n        aws_access_key_id='foo',\n        aws_secret_access_key='bar',\n    )\n\n    stubber = Stubber(client)\n    stubber.activate()\n    operation = case['operation']\n    params = case.get('extra_args', {})\n    params = params.copy()\n    param_name = case['original_name']\n    if use_alias:\n        param_name = case['new_name']\n    params[param_name] = case['parameter_value']\n    stubbed_response = case.get('stubbed_response', {})\n    stubber.add_response(operation, stubbed_response)\n    try:\n        getattr(client, operation)(**params)\n    except ParamValidationError as e:\n        raise AssertionError(\n            'Expecting %s to be valid parameter for %s.%s but received '\n            '%s.' % (case['new_name'], case['service'], case['operation'], e)\n        )\n", "tests/functional/test_six_threading.py": "\"\"\"\nRegression test for six issue #98 (https://github.com/benjaminp/six/issues/98)\n\"\"\"\nimport sys\nimport threading\nimport time\n\nfrom botocore.vendored import six\nfrom tests import mock\n\n_original_setattr = six.moves.__class__.__setattr__\n\n\ndef _wrapped_setattr(key, value):\n    # Monkey patch six.moves.__setattr__ to simulate\n    # a poorly-timed thread context switch\n    time.sleep(0.1)\n    return _original_setattr(six.moves, key, value)\n\n\ndef _reload_six():\n    # Issue #98 is caused by a race condition in six._LazyDescr.__get__\n    # which is only called once per moved module. Reload six so all the\n    # moved modules are reset.\n    import importlib\n\n    importlib.reload(six)\n\n\nclass _ExampleThread(threading.Thread):\n    def __init__(self):\n        super().__init__()\n        self.daemon = False\n        self.exc_info = None\n\n    def run(self):\n        try:\n            # Simulate use of six by\n            # botocore.configloader.raw_config_parse()\n            # Should raise AttributeError if six < 1.9.0\n            six.moves.configparser.RawConfigParser()\n        except Exception:\n            self.exc_info = sys.exc_info()\n\n\ndef test_six_thread_safety():\n    _reload_six()\n    with mock.patch(\n        'botocore.vendored.six.moves.__class__.__setattr__',\n        wraps=_wrapped_setattr,\n    ):\n        threads = []\n        for i in range(2):\n            t = _ExampleThread()\n            threads.append(t)\n            t.start()\n        while threads:\n            t = threads.pop()\n            t.join()\n            if t.exc_info:\n                six.reraise(*t.exc_info)\n", "tests/functional/test_regions.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nfrom botocore.client import ClientEndpointBridge\nfrom botocore.exceptions import NoRegionError\nfrom tests import BaseSessionTest, ClientHTTPStubber, mock\n\n# NOTE: sqs endpoint updated to be the CN in the SSL cert because\n# a bug in python2.6 prevents subjectAltNames from being parsed\n# and subsequently being used in cert validation.\n# Same thing is needed for rds.\nKNOWN_REGIONS = {\n    'ap-northeast-1': {\n        'apigateway': 'apigateway.ap-northeast-1.amazonaws.com',\n        'appstream': 'appstream.ap-northeast-1.amazonaws.com',\n        'autoscaling': 'autoscaling.ap-northeast-1.amazonaws.com',\n        'cloudformation': 'cloudformation.ap-northeast-1.amazonaws.com',\n        'cloudhsm': 'cloudhsm.ap-northeast-1.amazonaws.com',\n        'cloudsearch': 'cloudsearch.ap-northeast-1.amazonaws.com',\n        'cloudtrail': 'cloudtrail.ap-northeast-1.amazonaws.com',\n        'codedeploy': 'codedeploy.ap-northeast-1.amazonaws.com',\n        'cognito-identity': 'cognito-identity.ap-northeast-1.amazonaws.com',\n        'cognito-sync': 'cognito-sync.ap-northeast-1.amazonaws.com',\n        'config': 'config.ap-northeast-1.amazonaws.com',\n        'datapipeline': 'datapipeline.ap-northeast-1.amazonaws.com',\n        'directconnect': 'directconnect.ap-northeast-1.amazonaws.com',\n        'ds': 'ds.ap-northeast-1.amazonaws.com',\n        'dynamodb': 'dynamodb.ap-northeast-1.amazonaws.com',\n        'ec2': 'ec2.ap-northeast-1.amazonaws.com',\n        'ecs': 'ecs.ap-northeast-1.amazonaws.com',\n        'elasticache': 'elasticache.ap-northeast-1.amazonaws.com',\n        'elasticbeanstalk': 'elasticbeanstalk.ap-northeast-1.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.ap-northeast-1.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.ap-northeast-1.amazonaws.com',\n        'elastictranscoder': 'elastictranscoder.ap-northeast-1.amazonaws.com',\n        'glacier': 'glacier.ap-northeast-1.amazonaws.com',\n        'iot': 'iot.ap-northeast-1.amazonaws.com',\n        'kinesis': 'kinesis.ap-northeast-1.amazonaws.com',\n        'kms': 'kms.ap-northeast-1.amazonaws.com',\n        'lambda': 'lambda.ap-northeast-1.amazonaws.com',\n        'logs': 'logs.ap-northeast-1.amazonaws.com',\n        'monitoring': 'monitoring.ap-northeast-1.amazonaws.com',\n        'rds': 'rds.ap-northeast-1.amazonaws.com',\n        'redshift': 'redshift.ap-northeast-1.amazonaws.com',\n        's3': 's3.ap-northeast-1.amazonaws.com',\n        'sdb': 'sdb.ap-northeast-1.amazonaws.com',\n        'sns': 'sns.ap-northeast-1.amazonaws.com',\n        'sqs': 'sqs.ap-northeast-1.amazonaws.com',\n        'storagegateway': 'storagegateway.ap-northeast-1.amazonaws.com',\n        'streams.dynamodb': 'streams.dynamodb.ap-northeast-1.amazonaws.com',\n        'sts': 'sts.ap-northeast-1.amazonaws.com',\n        'swf': 'swf.ap-northeast-1.amazonaws.com',\n        'workspaces': 'workspaces.ap-northeast-1.amazonaws.com',\n    },\n    'ap-southeast-1': {\n        'autoscaling': 'autoscaling.ap-southeast-1.amazonaws.com',\n        'cloudformation': 'cloudformation.ap-southeast-1.amazonaws.com',\n        'cloudhsm': 'cloudhsm.ap-southeast-1.amazonaws.com',\n        'cloudsearch': 'cloudsearch.ap-southeast-1.amazonaws.com',\n        'cloudtrail': 'cloudtrail.ap-southeast-1.amazonaws.com',\n        'config': 'config.ap-southeast-1.amazonaws.com',\n        'directconnect': 'directconnect.ap-southeast-1.amazonaws.com',\n        'ds': 'ds.ap-southeast-1.amazonaws.com',\n        'dynamodb': 'dynamodb.ap-southeast-1.amazonaws.com',\n        'ec2': 'ec2.ap-southeast-1.amazonaws.com',\n        'elasticache': 'elasticache.ap-southeast-1.amazonaws.com',\n        'elasticbeanstalk': 'elasticbeanstalk.ap-southeast-1.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.ap-southeast-1.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.ap-southeast-1.amazonaws.com',\n        'elastictranscoder': 'elastictranscoder.ap-southeast-1.amazonaws.com',\n        'kinesis': 'kinesis.ap-southeast-1.amazonaws.com',\n        'kms': 'kms.ap-southeast-1.amazonaws.com',\n        'logs': 'logs.ap-southeast-1.amazonaws.com',\n        'monitoring': 'monitoring.ap-southeast-1.amazonaws.com',\n        'rds': 'rds.ap-southeast-1.amazonaws.com',\n        'redshift': 'redshift.ap-southeast-1.amazonaws.com',\n        's3': 's3.ap-southeast-1.amazonaws.com',\n        'sdb': 'sdb.ap-southeast-1.amazonaws.com',\n        'sns': 'sns.ap-southeast-1.amazonaws.com',\n        'sqs': 'sqs.ap-southeast-1.amazonaws.com',\n        'storagegateway': 'storagegateway.ap-southeast-1.amazonaws.com',\n        'streams.dynamodb': 'streams.dynamodb.ap-southeast-1.amazonaws.com',\n        'sts': 'sts.ap-southeast-1.amazonaws.com',\n        'swf': 'swf.ap-southeast-1.amazonaws.com',\n        'workspaces': 'workspaces.ap-southeast-1.amazonaws.com',\n    },\n    'ap-southeast-2': {\n        'autoscaling': 'autoscaling.ap-southeast-2.amazonaws.com',\n        'cloudformation': 'cloudformation.ap-southeast-2.amazonaws.com',\n        'cloudhsm': 'cloudhsm.ap-southeast-2.amazonaws.com',\n        'cloudsearch': 'cloudsearch.ap-southeast-2.amazonaws.com',\n        'cloudtrail': 'cloudtrail.ap-southeast-2.amazonaws.com',\n        'codedeploy': 'codedeploy.ap-southeast-2.amazonaws.com',\n        'config': 'config.ap-southeast-2.amazonaws.com',\n        'datapipeline': 'datapipeline.ap-southeast-2.amazonaws.com',\n        'directconnect': 'directconnect.ap-southeast-2.amazonaws.com',\n        'ds': 'ds.ap-southeast-2.amazonaws.com',\n        'dynamodb': 'dynamodb.ap-southeast-2.amazonaws.com',\n        'ec2': 'ec2.ap-southeast-2.amazonaws.com',\n        'ecs': 'ecs.ap-southeast-2.amazonaws.com',\n        'elasticache': 'elasticache.ap-southeast-2.amazonaws.com',\n        'elasticbeanstalk': 'elasticbeanstalk.ap-southeast-2.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.ap-southeast-2.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.ap-southeast-2.amazonaws.com',\n        'glacier': 'glacier.ap-southeast-2.amazonaws.com',\n        'kinesis': 'kinesis.ap-southeast-2.amazonaws.com',\n        'kms': 'kms.ap-southeast-2.amazonaws.com',\n        'logs': 'logs.ap-southeast-2.amazonaws.com',\n        'monitoring': 'monitoring.ap-southeast-2.amazonaws.com',\n        'rds': 'rds.ap-southeast-2.amazonaws.com',\n        'redshift': 'redshift.ap-southeast-2.amazonaws.com',\n        's3': 's3.ap-southeast-2.amazonaws.com',\n        'sdb': 'sdb.ap-southeast-2.amazonaws.com',\n        'sns': 'sns.ap-southeast-2.amazonaws.com',\n        'sqs': 'sqs.ap-southeast-2.amazonaws.com',\n        'storagegateway': 'storagegateway.ap-southeast-2.amazonaws.com',\n        'streams.dynamodb': 'streams.dynamodb.ap-southeast-2.amazonaws.com',\n        'sts': 'sts.ap-southeast-2.amazonaws.com',\n        'swf': 'swf.ap-southeast-2.amazonaws.com',\n        'workspaces': 'workspaces.ap-southeast-2.amazonaws.com',\n    },\n    'aws-us-gov-global': {'iam': 'iam.us-gov.amazonaws.com'},\n    'cn-north-1': {\n        'autoscaling': 'autoscaling.cn-north-1.amazonaws.com.cn',\n        'cloudformation': 'cloudformation.cn-north-1.amazonaws.com.cn',\n        'cloudtrail': 'cloudtrail.cn-north-1.amazonaws.com.cn',\n        'directconnect': 'directconnect.cn-north-1.amazonaws.com.cn',\n        'dynamodb': 'dynamodb.cn-north-1.amazonaws.com.cn',\n        'ec2': 'ec2.cn-north-1.amazonaws.com.cn',\n        'elasticache': 'elasticache.cn-north-1.amazonaws.com.cn',\n        'elasticbeanstalk': 'elasticbeanstalk.cn-north-1.amazonaws.com.cn',\n        'elasticloadbalancing': 'elasticloadbalancing.cn-north-1.amazonaws.com.cn',\n        'elasticmapreduce': 'elasticmapreduce.cn-north-1.amazonaws.com.cn',\n        'glacier': 'glacier.cn-north-1.amazonaws.com.cn',\n        'iam': 'iam.cn-north-1.amazonaws.com.cn',\n        'kinesis': 'kinesis.cn-north-1.amazonaws.com.cn',\n        'monitoring': 'monitoring.cn-north-1.amazonaws.com.cn',\n        'rds': 'rds.cn-north-1.amazonaws.com.cn',\n        's3': 's3.cn-north-1.amazonaws.com.cn',\n        'sns': 'sns.cn-north-1.amazonaws.com.cn',\n        'sqs': 'sqs.cn-north-1.amazonaws.com.cn',\n        'storagegateway': 'storagegateway.cn-north-1.amazonaws.com.cn',\n        'streams.dynamodb': 'streams.dynamodb.cn-north-1.amazonaws.com.cn',\n        'sts': 'sts.cn-north-1.amazonaws.com.cn',\n        'swf': 'swf.cn-north-1.amazonaws.com.cn',\n    },\n    'eu-central-1': {\n        'autoscaling': 'autoscaling.eu-central-1.amazonaws.com',\n        'cloudformation': 'cloudformation.eu-central-1.amazonaws.com',\n        'cloudhsm': 'cloudhsm.eu-central-1.amazonaws.com',\n        'cloudsearch': 'cloudsearch.eu-central-1.amazonaws.com',\n        'cloudtrail': 'cloudtrail.eu-central-1.amazonaws.com',\n        'codedeploy': 'codedeploy.eu-central-1.amazonaws.com',\n        'config': 'config.eu-central-1.amazonaws.com',\n        'directconnect': 'directconnect.eu-central-1.amazonaws.com',\n        'dynamodb': 'dynamodb.eu-central-1.amazonaws.com',\n        'ec2': 'ec2.eu-central-1.amazonaws.com',\n        'elasticache': 'elasticache.eu-central-1.amazonaws.com',\n        'elasticbeanstalk': 'elasticbeanstalk.eu-central-1.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.eu-central-1.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.eu-central-1.amazonaws.com',\n        'glacier': 'glacier.eu-central-1.amazonaws.com',\n        'kinesis': 'kinesis.eu-central-1.amazonaws.com',\n        'kms': 'kms.eu-central-1.amazonaws.com',\n        'logs': 'logs.eu-central-1.amazonaws.com',\n        'monitoring': 'monitoring.eu-central-1.amazonaws.com',\n        'rds': 'rds.eu-central-1.amazonaws.com',\n        'redshift': 'redshift.eu-central-1.amazonaws.com',\n        's3': 's3.eu-central-1.amazonaws.com',\n        'sns': 'sns.eu-central-1.amazonaws.com',\n        'sqs': 'sqs.eu-central-1.amazonaws.com',\n        'storagegateway': 'storagegateway.eu-central-1.amazonaws.com',\n        'streams.dynamodb': 'streams.dynamodb.eu-central-1.amazonaws.com',\n        'sts': 'sts.eu-central-1.amazonaws.com',\n        'swf': 'swf.eu-central-1.amazonaws.com',\n    },\n    'eu-west-1': {\n        'apigateway': 'apigateway.eu-west-1.amazonaws.com',\n        'autoscaling': 'autoscaling.eu-west-1.amazonaws.com',\n        'cloudformation': 'cloudformation.eu-west-1.amazonaws.com',\n        'cloudhsm': 'cloudhsm.eu-west-1.amazonaws.com',\n        'cloudsearch': 'cloudsearch.eu-west-1.amazonaws.com',\n        'cloudtrail': 'cloudtrail.eu-west-1.amazonaws.com',\n        'codedeploy': 'codedeploy.eu-west-1.amazonaws.com',\n        'cognito-identity': 'cognito-identity.eu-west-1.amazonaws.com',\n        'cognito-sync': 'cognito-sync.eu-west-1.amazonaws.com',\n        'config': 'config.eu-west-1.amazonaws.com',\n        'datapipeline': 'datapipeline.eu-west-1.amazonaws.com',\n        'directconnect': 'directconnect.eu-west-1.amazonaws.com',\n        'ds': 'ds.eu-west-1.amazonaws.com',\n        'dynamodb': 'dynamodb.eu-west-1.amazonaws.com',\n        'ec2': 'ec2.eu-west-1.amazonaws.com',\n        'ecs': 'ecs.eu-west-1.amazonaws.com',\n        'elasticache': 'elasticache.eu-west-1.amazonaws.com',\n        'elasticbeanstalk': 'elasticbeanstalk.eu-west-1.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.eu-west-1.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.eu-west-1.amazonaws.com',\n        'elastictranscoder': 'elastictranscoder.eu-west-1.amazonaws.com',\n        'email': 'email.eu-west-1.amazonaws.com',\n        'glacier': 'glacier.eu-west-1.amazonaws.com',\n        'iot': 'iot.eu-west-1.amazonaws.com',\n        'kinesis': 'kinesis.eu-west-1.amazonaws.com',\n        'kms': 'kms.eu-west-1.amazonaws.com',\n        'lambda': 'lambda.eu-west-1.amazonaws.com',\n        'logs': 'logs.eu-west-1.amazonaws.com',\n        'machinelearning': 'machinelearning.eu-west-1.amazonaws.com',\n        'monitoring': 'monitoring.eu-west-1.amazonaws.com',\n        'rds': 'rds.eu-west-1.amazonaws.com',\n        'redshift': 'redshift.eu-west-1.amazonaws.com',\n        's3': 's3.eu-west-1.amazonaws.com',\n        'sdb': 'sdb.eu-west-1.amazonaws.com',\n        'sns': 'sns.eu-west-1.amazonaws.com',\n        'sqs': 'sqs.eu-west-1.amazonaws.com',\n        'ssm': 'ssm.eu-west-1.amazonaws.com',\n        'storagegateway': 'storagegateway.eu-west-1.amazonaws.com',\n        'streams.dynamodb': 'streams.dynamodb.eu-west-1.amazonaws.com',\n        'sts': 'sts.eu-west-1.amazonaws.com',\n        'swf': 'swf.eu-west-1.amazonaws.com',\n        'workspaces': 'workspaces.eu-west-1.amazonaws.com',\n    },\n    'fips-us-gov-west-1': {'s3': 's3-fips.us-gov-west-1.amazonaws.com'},\n    's3-external-1': {'s3': 's3-external-1.amazonaws.com'},\n    'sa-east-1': {\n        'autoscaling': 'autoscaling.sa-east-1.amazonaws.com',\n        'cloudformation': 'cloudformation.sa-east-1.amazonaws.com',\n        'cloudsearch': 'cloudsearch.sa-east-1.amazonaws.com',\n        'cloudtrail': 'cloudtrail.sa-east-1.amazonaws.com',\n        'config': 'config.sa-east-1.amazonaws.com',\n        'directconnect': 'directconnect.sa-east-1.amazonaws.com',\n        'dynamodb': 'dynamodb.sa-east-1.amazonaws.com',\n        'ec2': 'ec2.sa-east-1.amazonaws.com',\n        'elasticache': 'elasticache.sa-east-1.amazonaws.com',\n        'elasticbeanstalk': 'elasticbeanstalk.sa-east-1.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.sa-east-1.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.sa-east-1.amazonaws.com',\n        'kms': 'kms.sa-east-1.amazonaws.com',\n        'monitoring': 'monitoring.sa-east-1.amazonaws.com',\n        'rds': 'rds.sa-east-1.amazonaws.com',\n        's3': 's3.sa-east-1.amazonaws.com',\n        'sdb': 'sdb.sa-east-1.amazonaws.com',\n        'sns': 'sns.sa-east-1.amazonaws.com',\n        'sqs': 'sqs.sa-east-1.amazonaws.com',\n        'storagegateway': 'storagegateway.sa-east-1.amazonaws.com',\n        'streams.dynamodb': 'streams.dynamodb.sa-east-1.amazonaws.com',\n        'sts': 'sts.sa-east-1.amazonaws.com',\n        'swf': 'swf.sa-east-1.amazonaws.com',\n    },\n    'us-east-1': {\n        'apigateway': 'apigateway.us-east-1.amazonaws.com',\n        'appstream': 'appstream.us-east-1.amazonaws.com',\n        'autoscaling': 'autoscaling.us-east-1.amazonaws.com',\n        'cloudformation': 'cloudformation.us-east-1.amazonaws.com',\n        'cloudfront': 'cloudfront.amazonaws.com',\n        'cloudhsm': 'cloudhsm.us-east-1.amazonaws.com',\n        'cloudsearch': 'cloudsearch.us-east-1.amazonaws.com',\n        'cloudtrail': 'cloudtrail.us-east-1.amazonaws.com',\n        'codecommit': 'codecommit.us-east-1.amazonaws.com',\n        'codedeploy': 'codedeploy.us-east-1.amazonaws.com',\n        'codepipeline': 'codepipeline.us-east-1.amazonaws.com',\n        'cognito-identity': 'cognito-identity.us-east-1.amazonaws.com',\n        'cognito-sync': 'cognito-sync.us-east-1.amazonaws.com',\n        'config': 'config.us-east-1.amazonaws.com',\n        'datapipeline': 'datapipeline.us-east-1.amazonaws.com',\n        'directconnect': 'directconnect.us-east-1.amazonaws.com',\n        'ds': 'ds.us-east-1.amazonaws.com',\n        'dynamodb': 'dynamodb.us-east-1.amazonaws.com',\n        'ec2': 'ec2.us-east-1.amazonaws.com',\n        'ecs': 'ecs.us-east-1.amazonaws.com',\n        'elasticache': 'elasticache.us-east-1.amazonaws.com',\n        'elasticbeanstalk': 'elasticbeanstalk.us-east-1.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.us-east-1.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.us-east-1.amazonaws.com',\n        'elastictranscoder': 'elastictranscoder.us-east-1.amazonaws.com',\n        'email': 'email.us-east-1.amazonaws.com',\n        'glacier': 'glacier.us-east-1.amazonaws.com',\n        'iam': 'iam.amazonaws.com',\n        'importexport': 'importexport.amazonaws.com',\n        'iot': 'iot.us-east-1.amazonaws.com',\n        'kinesis': 'kinesis.us-east-1.amazonaws.com',\n        'kms': 'kms.us-east-1.amazonaws.com',\n        'lambda': 'lambda.us-east-1.amazonaws.com',\n        'logs': 'logs.us-east-1.amazonaws.com',\n        'machinelearning': 'machinelearning.us-east-1.amazonaws.com',\n        'mobileanalytics': 'mobileanalytics.us-east-1.amazonaws.com',\n        'monitoring': 'monitoring.us-east-1.amazonaws.com',\n        'opsworks': 'opsworks.us-east-1.amazonaws.com',\n        'rds': 'rds.us-east-1.amazonaws.com',\n        'redshift': 'redshift.us-east-1.amazonaws.com',\n        'route53': 'route53.amazonaws.com',\n        'route53domains': 'route53domains.us-east-1.amazonaws.com',\n        's3': 's3.us-east-1.amazonaws.com',\n        'sdb': 'sdb.amazonaws.com',\n        'sns': 'sns.us-east-1.amazonaws.com',\n        'sqs': 'sqs.us-east-1.amazonaws.com',\n        'ssm': 'ssm.us-east-1.amazonaws.com',\n        'storagegateway': 'storagegateway.us-east-1.amazonaws.com',\n        'streams.dynamodb': 'streams.dynamodb.us-east-1.amazonaws.com',\n        'sts': 'sts.us-east-1.amazonaws.com',\n        'support': 'support.us-east-1.amazonaws.com',\n        'swf': 'swf.us-east-1.amazonaws.com',\n        'workspaces': 'workspaces.us-east-1.amazonaws.com',\n        'waf': 'waf.amazonaws.com',\n    },\n    'us-gov-west-1': {\n        'autoscaling': 'autoscaling.us-gov-west-1.amazonaws.com',\n        'cloudformation': 'cloudformation.us-gov-west-1.amazonaws.com',\n        'cloudhsm': 'cloudhsm.us-gov-west-1.amazonaws.com',\n        'cloudtrail': 'cloudtrail.us-gov-west-1.amazonaws.com',\n        'dynamodb': 'dynamodb.us-gov-west-1.amazonaws.com',\n        'ec2': 'ec2.us-gov-west-1.amazonaws.com',\n        'elasticache': 'elasticache.us-gov-west-1.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.us-gov-west-1.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.us-gov-west-1.amazonaws.com',\n        'glacier': 'glacier.us-gov-west-1.amazonaws.com',\n        'iam': 'iam.us-gov.amazonaws.com',\n        'kms': 'kms.us-gov-west-1.amazonaws.com',\n        'monitoring': 'monitoring.us-gov-west-1.amazonaws.com',\n        'rds': 'rds.us-gov-west-1.amazonaws.com',\n        'redshift': 'redshift.us-gov-west-1.amazonaws.com',\n        's3': 's3.us-gov-west-1.amazonaws.com',\n        'sns': 'sns.us-gov-west-1.amazonaws.com',\n        'sqs': 'sqs.us-gov-west-1.amazonaws.com',\n        'sts': 'sts.us-gov-west-1.amazonaws.com',\n        'swf': 'swf.us-gov-west-1.amazonaws.com',\n    },\n    'us-west-1': {\n        'autoscaling': 'autoscaling.us-west-1.amazonaws.com',\n        'cloudformation': 'cloudformation.us-west-1.amazonaws.com',\n        'cloudsearch': 'cloudsearch.us-west-1.amazonaws.com',\n        'cloudtrail': 'cloudtrail.us-west-1.amazonaws.com',\n        'config': 'config.us-west-1.amazonaws.com',\n        'directconnect': 'directconnect.us-west-1.amazonaws.com',\n        'dynamodb': 'dynamodb.us-west-1.amazonaws.com',\n        'ec2': 'ec2.us-west-1.amazonaws.com',\n        'ecs': 'ecs.us-west-1.amazonaws.com',\n        'elasticache': 'elasticache.us-west-1.amazonaws.com',\n        'elasticbeanstalk': 'elasticbeanstalk.us-west-1.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.us-west-1.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.us-west-1.amazonaws.com',\n        'elastictranscoder': 'elastictranscoder.us-west-1.amazonaws.com',\n        'glacier': 'glacier.us-west-1.amazonaws.com',\n        'kinesis': 'kinesis.us-west-1.amazonaws.com',\n        'kms': 'kms.us-west-1.amazonaws.com',\n        'logs': 'logs.us-west-1.amazonaws.com',\n        'monitoring': 'monitoring.us-west-1.amazonaws.com',\n        'rds': 'rds.us-west-1.amazonaws.com',\n        's3': 's3.us-west-1.amazonaws.com',\n        'sdb': 'sdb.us-west-1.amazonaws.com',\n        'sns': 'sns.us-west-1.amazonaws.com',\n        'sqs': 'sqs.us-west-1.amazonaws.com',\n        'storagegateway': 'storagegateway.us-west-1.amazonaws.com',\n        'streams.dynamodb': 'streams.dynamodb.us-west-1.amazonaws.com',\n        'sts': 'sts.us-west-1.amazonaws.com',\n        'swf': 'swf.us-west-1.amazonaws.com',\n    },\n    'us-west-2': {\n        'apigateway': 'apigateway.us-west-2.amazonaws.com',\n        'autoscaling': 'autoscaling.us-west-2.amazonaws.com',\n        'cloudformation': 'cloudformation.us-west-2.amazonaws.com',\n        'cloudhsm': 'cloudhsm.us-west-2.amazonaws.com',\n        'cloudsearch': 'cloudsearch.us-west-2.amazonaws.com',\n        'cloudtrail': 'cloudtrail.us-west-2.amazonaws.com',\n        'codedeploy': 'codedeploy.us-west-2.amazonaws.com',\n        'codepipeline': 'codepipeline.us-west-2.amazonaws.com',\n        'config': 'config.us-west-2.amazonaws.com',\n        'datapipeline': 'datapipeline.us-west-2.amazonaws.com',\n        'devicefarm': 'devicefarm.us-west-2.amazonaws.com',\n        'directconnect': 'directconnect.us-west-2.amazonaws.com',\n        'ds': 'ds.us-west-2.amazonaws.com',\n        'dynamodb': 'dynamodb.us-west-2.amazonaws.com',\n        'ec2': 'ec2.us-west-2.amazonaws.com',\n        'ecs': 'ecs.us-west-2.amazonaws.com',\n        'elasticache': 'elasticache.us-west-2.amazonaws.com',\n        'elasticbeanstalk': 'elasticbeanstalk.us-west-2.amazonaws.com',\n        'elasticfilesystem': 'elasticfilesystem.us-west-2.amazonaws.com',\n        'elasticloadbalancing': 'elasticloadbalancing.us-west-2.amazonaws.com',\n        'elasticmapreduce': 'elasticmapreduce.us-west-2.amazonaws.com',\n        'elastictranscoder': 'elastictranscoder.us-west-2.amazonaws.com',\n        'email': 'email.us-west-2.amazonaws.com',\n        'glacier': 'glacier.us-west-2.amazonaws.com',\n        'iot': 'iot.us-west-2.amazonaws.com',\n        'kinesis': 'kinesis.us-west-2.amazonaws.com',\n        'kms': 'kms.us-west-2.amazonaws.com',\n        'lambda': 'lambda.us-west-2.amazonaws.com',\n        'logs': 'logs.us-west-2.amazonaws.com',\n        'monitoring': 'monitoring.us-west-2.amazonaws.com',\n        'rds': 'rds.us-west-2.amazonaws.com',\n        'redshift': 'redshift.us-west-2.amazonaws.com',\n        's3': 's3.us-west-2.amazonaws.com',\n        'sdb': 'sdb.us-west-2.amazonaws.com',\n        'sns': 'sns.us-west-2.amazonaws.com',\n        'sqs': 'sqs.us-west-2.amazonaws.com',\n        'ssm': 'ssm.us-west-2.amazonaws.com',\n        'storagegateway': 'storagegateway.us-west-2.amazonaws.com',\n        'streams.dynamodb': 'streams.dynamodb.us-west-2.amazonaws.com',\n        'sts': 'sts.us-west-2.amazonaws.com',\n        'swf': 'swf.us-west-2.amazonaws.com',\n        'workspaces': 'workspaces.us-west-2.amazonaws.com',\n    },\n}\n\n\n# Lists the services in the aws partition that do not require a region\n# when resolving an endpoint because these services have partitionWide\n# endpoints.\nKNOWN_AWS_PARTITION_WIDE = {\n    'importexport': 'https://importexport.amazonaws.com',\n    'cloudfront': 'https://cloudfront.amazonaws.com',\n    'waf': 'https://waf.amazonaws.com',\n    'route53': 'https://route53.amazonaws.com',\n    's3': 'https://s3.amazonaws.com',\n    'sts': 'https://sts.amazonaws.com',\n    'iam': 'https://iam.amazonaws.com',\n}\n\n\ndef _known_endpoints_by_region():\n    for region_name, service_dict in KNOWN_REGIONS.items():\n        for service_name, endpoint in service_dict.items():\n            yield service_name, region_name, endpoint\n\n\n@pytest.mark.parametrize(\n    \"service_name, region_name, expected_endpoint\",\n    _known_endpoints_by_region(),\n)\ndef test_single_service_region_endpoint(\n    patched_session, service_name, region_name, expected_endpoint\n):\n    # Verify the actual values from the partition files.  While\n    # TestEndpointHeuristics verified the generic functionality given any\n    # endpoints file, this test actually verifies the partition data against a\n    # fixed list of known endpoints.  This list doesn't need to be kept 100% up\n    # to date, but serves as a basis for regressions as the endpoint data\n    # logic evolves.\n    resolver = patched_session._get_internal_component('endpoint_resolver')\n    bridge = ClientEndpointBridge(resolver, None, None)\n    result = bridge.resolve(service_name, region_name)\n    expected = 'https://%s' % expected_endpoint\n    assert result['endpoint_url'] == expected\n\n\n# Ensure that all S3 regions use s3v4 instead of v4\ndef test_all_s3_endpoints_have_s3v4(patched_session):\n    session = patched_session\n    partitions = session.get_available_partitions()\n    resolver = session._get_internal_component('endpoint_resolver')\n    for partition_name in partitions:\n        for endpoint in session.get_available_regions('s3', partition_name):\n            resolved = resolver.construct_endpoint('s3', endpoint)\n            assert 's3v4' in resolved['signatureVersions']\n            assert 'v4' not in resolved['signatureVersions']\n\n\n@pytest.mark.parametrize(\n    \"service_name, expected_endpoint\", KNOWN_AWS_PARTITION_WIDE.items()\n)\ndef test_single_service_partition_endpoint(\n    patched_session, service_name, expected_endpoint\n):\n    resolver = patched_session._get_internal_component('endpoint_resolver')\n    bridge = ClientEndpointBridge(resolver)\n    result = bridge.resolve(service_name)\n    assert result['endpoint_url'] == expected_endpoint\n\n\ndef test_non_partition_endpoint_requires_region(patched_session):\n    resolver = patched_session._get_internal_component('endpoint_resolver')\n    with pytest.raises(NoRegionError):\n        resolver.construct_endpoint('ec2')\n\n\nclass TestEndpointResolution(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.xml_response = (\n            b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n\\n'\n            b'<ListRolesResponse '\n            b'xmlns=\"https://iam.amazonaws.com/doc/2010-05-08/\">\\n'\n            b'<ListRolesResult>\\n</ListRolesResult>'\n            b'</ListRolesResponse>'\n        )\n\n    def create_stubbed_client(self, service_name, region_name, **kwargs):\n        client = self.session.create_client(\n            service_name, region_name, **kwargs\n        )\n        http_stubber = ClientHTTPStubber(client)\n        http_stubber.start()\n        return client, http_stubber\n\n    def test_regionalized_client_endpoint_resolution(self):\n        client, stubber = self.create_stubbed_client('s3', 'us-east-2')\n        stubber.add_response()\n        client.list_buckets()\n        self.assertEqual(\n            stubber.requests[0].url, 'https://s3.us-east-2.amazonaws.com/'\n        )\n\n    def test_regionalized_client_with_unknown_region(self):\n        client, stubber = self.create_stubbed_client('s3', 'not-real')\n        stubber.add_response()\n        client.list_buckets()\n        # Validate we don't fall back to partition endpoint for\n        # regionalized services.\n        self.assertEqual(\n            stubber.requests[0].url, 'https://s3.not-real.amazonaws.com/'\n        )\n\n    def test_unregionalized_client_endpoint_resolution(self):\n        client, stubber = self.create_stubbed_client('iam', 'us-west-2')\n        stubber.add_response(body=self.xml_response)\n        client.list_roles()\n        self.assertTrue(\n            stubber.requests[0].url.startswith('https://iam.amazonaws.com/')\n        )\n\n    def test_unregionalized_client_with_unknown_region(self):\n        client, stubber = self.create_stubbed_client('iam', 'not-real')\n        stubber.add_response(body=self.xml_response)\n        client.list_roles()\n        self.assertTrue(\n            stubber.requests[0].url.startswith('https://iam.amazonaws.com/')\n        )\n\n\n@pytest.mark.parametrize(\"is_builtin\", [True, False])\ndef test_endpoint_resolver_knows_its_datasource(patched_session, is_builtin):\n    # The information whether or not the endpoints.json file was loaded from\n    # the builtin data directory or not should be passed from Loader to\n    # EndpointResolver.\n    session = patched_session\n    loader = session.get_component('data_loader')\n    with mock.patch.object(loader, 'is_builtin_path', return_value=is_builtin):\n        resolver = session._get_internal_component('endpoint_resolver')\n        assert resolver.uses_builtin_data == is_builtin\n", "tests/functional/test_event_alias.py": "import pytest\n\nfrom botocore.session import Session\n\n# The list of services which were available when we switched over from using\n# endpoint prefix in event to using service id. These should all accept\n# either.\nSERVICES = {\n    \"acm\": {\"endpoint_prefix\": \"acm\", \"service_id\": \"acm\"},\n    \"acm-pca\": {\"endpoint_prefix\": \"acm-pca\", \"service_id\": \"acm-pca\"},\n    \"apigateway\": {\n        \"endpoint_prefix\": \"apigateway\",\n        \"service_id\": \"api-gateway\",\n    },\n    \"application-autoscaling\": {\"service_id\": \"application-auto-scaling\"},\n    \"appstream\": {\"endpoint_prefix\": \"appstream2\", \"service_id\": \"appstream\"},\n    \"appsync\": {\"endpoint_prefix\": \"appsync\", \"service_id\": \"appsync\"},\n    \"athena\": {\"endpoint_prefix\": \"athena\", \"service_id\": \"athena\"},\n    \"autoscaling\": {\n        \"endpoint_prefix\": \"autoscaling\",\n        \"service_id\": \"auto-scaling\",\n    },\n    \"autoscaling-plans\": {\"service_id\": \"auto-scaling-plans\"},\n    \"batch\": {\"endpoint_prefix\": \"batch\", \"service_id\": \"batch\"},\n    \"budgets\": {\"endpoint_prefix\": \"budgets\", \"service_id\": \"budgets\"},\n    \"ce\": {\"endpoint_prefix\": \"ce\", \"service_id\": \"cost-explorer\"},\n    \"cloud9\": {\"endpoint_prefix\": \"cloud9\", \"service_id\": \"cloud9\"},\n    \"clouddirectory\": {\n        \"endpoint_prefix\": \"clouddirectory\",\n        \"service_id\": \"clouddirectory\",\n    },\n    \"cloudformation\": {\n        \"endpoint_prefix\": \"cloudformation\",\n        \"service_id\": \"cloudformation\",\n    },\n    \"cloudfront\": {\n        \"endpoint_prefix\": \"cloudfront\",\n        \"service_id\": \"cloudfront\",\n    },\n    \"cloudhsm\": {\"endpoint_prefix\": \"cloudhsm\", \"service_id\": \"cloudhsm\"},\n    \"cloudhsmv2\": {\n        \"endpoint_prefix\": \"cloudhsmv2\",\n        \"service_id\": \"cloudhsm-v2\",\n    },\n    \"cloudsearch\": {\n        \"endpoint_prefix\": \"cloudsearch\",\n        \"service_id\": \"cloudsearch\",\n    },\n    \"cloudsearchdomain\": {\n        \"endpoint_prefix\": \"cloudsearchdomain\",\n        \"service_id\": \"cloudsearch-domain\",\n    },\n    \"cloudtrail\": {\n        \"endpoint_prefix\": \"cloudtrail\",\n        \"service_id\": \"cloudtrail\",\n    },\n    \"cloudwatch\": {\n        \"endpoint_prefix\": \"monitoring\",\n        \"service_id\": \"cloudwatch\",\n    },\n    \"codebuild\": {\"endpoint_prefix\": \"codebuild\", \"service_id\": \"codebuild\"},\n    \"codecommit\": {\n        \"endpoint_prefix\": \"codecommit\",\n        \"service_id\": \"codecommit\",\n    },\n    \"codedeploy\": {\n        \"endpoint_prefix\": \"codedeploy\",\n        \"service_id\": \"codedeploy\",\n    },\n    \"codepipeline\": {\n        \"endpoint_prefix\": \"codepipeline\",\n        \"service_id\": \"codepipeline\",\n    },\n    \"codestar\": {\"endpoint_prefix\": \"codestar\", \"service_id\": \"codestar\"},\n    \"cognito-identity\": {\n        \"endpoint_prefix\": \"cognito-identity\",\n        \"service_id\": \"cognito-identity\",\n    },\n    \"cognito-idp\": {\n        \"endpoint_prefix\": \"cognito-idp\",\n        \"service_id\": \"cognito-identity-provider\",\n    },\n    \"cognito-sync\": {\n        \"endpoint_prefix\": \"cognito-sync\",\n        \"service_id\": \"cognito-sync\",\n    },\n    \"comprehend\": {\n        \"endpoint_prefix\": \"comprehend\",\n        \"service_id\": \"comprehend\",\n    },\n    \"config\": {\"endpoint_prefix\": \"config\", \"service_id\": \"config-service\"},\n    \"connect\": {\"endpoint_prefix\": \"connect\", \"service_id\": \"connect\"},\n    \"cur\": {\n        \"endpoint_prefix\": \"cur\",\n        \"service_id\": \"cost-and-usage-report-service\",\n    },\n    \"datapipeline\": {\n        \"endpoint_prefix\": \"datapipeline\",\n        \"service_id\": \"data-pipeline\",\n    },\n    \"dax\": {\"endpoint_prefix\": \"dax\", \"service_id\": \"dax\"},\n    \"devicefarm\": {\n        \"endpoint_prefix\": \"devicefarm\",\n        \"service_id\": \"device-farm\",\n    },\n    \"directconnect\": {\n        \"endpoint_prefix\": \"directconnect\",\n        \"service_id\": \"direct-connect\",\n    },\n    \"discovery\": {\n        \"endpoint_prefix\": \"discovery\",\n        \"service_id\": \"application-discovery-service\",\n    },\n    \"dlm\": {\"endpoint_prefix\": \"dlm\", \"service_id\": \"dlm\"},\n    \"dms\": {\n        \"endpoint_prefix\": \"dms\",\n        \"service_id\": \"database-migration-service\",\n    },\n    \"ds\": {\"endpoint_prefix\": \"ds\", \"service_id\": \"directory-service\"},\n    \"dynamodb\": {\"endpoint_prefix\": \"dynamodb\", \"service_id\": \"dynamodb\"},\n    \"dynamodbstreams\": {\n        \"endpoint_prefix\": \"streams.dynamodb\",\n        \"service_id\": \"dynamodb-streams\",\n    },\n    \"ec2\": {\"endpoint_prefix\": \"ec2\", \"service_id\": \"ec2\"},\n    \"ecr\": {\"endpoint_prefix\": \"ecr\", \"service_id\": \"ecr\"},\n    \"ecs\": {\"endpoint_prefix\": \"ecs\", \"service_id\": \"ecs\"},\n    \"efs\": {\"endpoint_prefix\": \"elasticfilesystem\", \"service_id\": \"efs\"},\n    \"eks\": {\"endpoint_prefix\": \"eks\", \"service_id\": \"eks\"},\n    \"elasticache\": {\n        \"endpoint_prefix\": \"elasticache\",\n        \"service_id\": \"elasticache\",\n    },\n    \"elasticbeanstalk\": {\n        \"endpoint_prefix\": \"elasticbeanstalk\",\n        \"service_id\": \"elastic-beanstalk\",\n    },\n    \"elastictranscoder\": {\n        \"endpoint_prefix\": \"elastictranscoder\",\n        \"service_id\": \"elastic-transcoder\",\n    },\n    \"elb\": {\n        \"endpoint_prefix\": \"elasticloadbalancing\",\n        \"service_id\": \"elastic-load-balancing\",\n    },\n    \"elbv2\": {\"service_id\": \"elastic-load-balancing-v2\"},\n    \"emr\": {\"endpoint_prefix\": \"elasticmapreduce\", \"service_id\": \"emr\"},\n    \"es\": {\"endpoint_prefix\": \"es\", \"service_id\": \"elasticsearch-service\"},\n    \"events\": {\"endpoint_prefix\": \"events\", \"service_id\": \"cloudwatch-events\"},\n    \"firehose\": {\"endpoint_prefix\": \"firehose\", \"service_id\": \"firehose\"},\n    \"fms\": {\"endpoint_prefix\": \"fms\", \"service_id\": \"fms\"},\n    \"gamelift\": {\"endpoint_prefix\": \"gamelift\", \"service_id\": \"gamelift\"},\n    \"glacier\": {\"endpoint_prefix\": \"glacier\", \"service_id\": \"glacier\"},\n    \"glue\": {\"endpoint_prefix\": \"glue\", \"service_id\": \"glue\"},\n    \"greengrass\": {\n        \"endpoint_prefix\": \"greengrass\",\n        \"service_id\": \"greengrass\",\n    },\n    \"guardduty\": {\"endpoint_prefix\": \"guardduty\", \"service_id\": \"guardduty\"},\n    \"health\": {\"endpoint_prefix\": \"health\", \"service_id\": \"health\"},\n    \"iam\": {\"endpoint_prefix\": \"iam\", \"service_id\": \"iam\"},\n    \"importexport\": {\n        \"endpoint_prefix\": \"importexport\",\n        \"service_id\": \"importexport\",\n    },\n    \"inspector\": {\"endpoint_prefix\": \"inspector\", \"service_id\": \"inspector\"},\n    \"iot\": {\"endpoint_prefix\": \"iot\", \"service_id\": \"iot\"},\n    \"iot-data\": {\n        \"endpoint_prefix\": \"data.iot\",\n        \"service_id\": \"iot-data-plane\",\n    },\n    \"iot-jobs-data\": {\n        \"endpoint_prefix\": \"data.jobs.iot\",\n        \"service_id\": \"iot-jobs-data-plane\",\n    },\n    \"iot1click-devices\": {\n        \"endpoint_prefix\": \"devices.iot1click\",\n        \"service_id\": \"iot-1click-devices-service\",\n    },\n    \"iot1click-projects\": {\n        \"endpoint_prefix\": \"projects.iot1click\",\n        \"service_id\": \"iot-1click-projects\",\n    },\n    \"iotanalytics\": {\n        \"endpoint_prefix\": \"iotanalytics\",\n        \"service_id\": \"iotanalytics\",\n    },\n    \"kinesis\": {\"endpoint_prefix\": \"kinesis\", \"service_id\": \"kinesis\"},\n    \"kinesis-video-archived-media\": {\n        \"service_id\": \"kinesis-video-archived-media\"\n    },\n    \"kinesis-video-media\": {\"service_id\": \"kinesis-video-media\"},\n    \"kinesisanalytics\": {\n        \"endpoint_prefix\": \"kinesisanalytics\",\n        \"service_id\": \"kinesis-analytics\",\n    },\n    \"kinesisvideo\": {\n        \"endpoint_prefix\": \"kinesisvideo\",\n        \"service_id\": \"kinesis-video\",\n    },\n    \"kms\": {\"endpoint_prefix\": \"kms\", \"service_id\": \"kms\"},\n    \"lambda\": {\"endpoint_prefix\": \"lambda\", \"service_id\": \"lambda\"},\n    \"lex-models\": {\n        \"endpoint_prefix\": \"models.lex\",\n        \"service_id\": \"lex-model-building-service\",\n    },\n    \"lex-runtime\": {\n        \"endpoint_prefix\": \"runtime.lex\",\n        \"service_id\": \"lex-runtime-service\",\n    },\n    \"lightsail\": {\"endpoint_prefix\": \"lightsail\", \"service_id\": \"lightsail\"},\n    \"logs\": {\"endpoint_prefix\": \"logs\", \"service_id\": \"cloudwatch-logs\"},\n    \"machinelearning\": {\n        \"endpoint_prefix\": \"machinelearning\",\n        \"service_id\": \"machine-learning\",\n    },\n    \"marketplace-entitlement\": {\n        \"endpoint_prefix\": \"entitlement.marketplace\",\n        \"service_id\": \"marketplace-entitlement-service\",\n    },\n    \"marketplacecommerceanalytics\": {\n        \"endpoint_prefix\": \"marketplacecommerceanalytics\",\n        \"service_id\": \"marketplace-commerce-analytics\",\n    },\n    \"mediaconvert\": {\n        \"endpoint_prefix\": \"mediaconvert\",\n        \"service_id\": \"mediaconvert\",\n    },\n    \"medialive\": {\"endpoint_prefix\": \"medialive\", \"service_id\": \"medialive\"},\n    \"mediapackage\": {\n        \"endpoint_prefix\": \"mediapackage\",\n        \"service_id\": \"mediapackage\",\n    },\n    \"mediastore\": {\n        \"endpoint_prefix\": \"mediastore\",\n        \"service_id\": \"mediastore\",\n    },\n    \"mediastore-data\": {\n        \"endpoint_prefix\": \"data.mediastore\",\n        \"service_id\": \"mediastore-data\",\n    },\n    \"mediatailor\": {\n        \"endpoint_prefix\": \"api.mediatailor\",\n        \"service_id\": \"mediatailor\",\n    },\n    \"meteringmarketplace\": {\n        \"endpoint_prefix\": \"metering.marketplace\",\n        \"service_id\": \"marketplace-metering\",\n    },\n    \"mgh\": {\"endpoint_prefix\": \"mgh\", \"service_id\": \"migration-hub\"},\n    \"mobile\": {\"endpoint_prefix\": \"mobile\", \"service_id\": \"mobile\"},\n    \"mq\": {\"endpoint_prefix\": \"mq\", \"service_id\": \"mq\"},\n    \"mturk\": {\"endpoint_prefix\": \"mturk-requester\", \"service_id\": \"mturk\"},\n    \"neptune\": {\"service_id\": \"neptune\"},\n    \"opsworks\": {\"endpoint_prefix\": \"opsworks\", \"service_id\": \"opsworks\"},\n    \"opsworkscm\": {\n        \"endpoint_prefix\": \"opsworks-cm\",\n        \"service_id\": \"opsworkscm\",\n    },\n    \"organizations\": {\n        \"endpoint_prefix\": \"organizations\",\n        \"service_id\": \"organizations\",\n    },\n    \"pi\": {\"endpoint_prefix\": \"pi\", \"service_id\": \"pi\"},\n    \"pinpoint\": {\"endpoint_prefix\": \"pinpoint\", \"service_id\": \"pinpoint\"},\n    \"polly\": {\"endpoint_prefix\": \"polly\", \"service_id\": \"polly\"},\n    \"pricing\": {\"endpoint_prefix\": \"api.pricing\", \"service_id\": \"pricing\"},\n    \"rds\": {\"endpoint_prefix\": \"rds\", \"service_id\": \"rds\"},\n    \"redshift\": {\"endpoint_prefix\": \"redshift\", \"service_id\": \"redshift\"},\n    \"rekognition\": {\n        \"endpoint_prefix\": \"rekognition\",\n        \"service_id\": \"rekognition\",\n    },\n    \"resource-groups\": {\n        \"endpoint_prefix\": \"resource-groups\",\n        \"service_id\": \"resource-groups\",\n    },\n    \"resourcegroupstaggingapi\": {\n        \"endpoint_prefix\": \"tagging\",\n        \"service_id\": \"resource-groups-tagging-api\",\n    },\n    \"route53\": {\"endpoint_prefix\": \"route53\", \"service_id\": \"route-53\"},\n    \"route53domains\": {\n        \"endpoint_prefix\": \"route53domains\",\n        \"service_id\": \"route-53-domains\",\n    },\n    \"s3\": {\"endpoint_prefix\": \"s3\", \"service_id\": \"s3\"},\n    \"sagemaker\": {\n        \"endpoint_prefix\": \"api.sagemaker\",\n        \"service_id\": \"sagemaker\",\n    },\n    \"sagemaker-runtime\": {\n        \"endpoint_prefix\": \"runtime.sagemaker\",\n        \"service_id\": \"sagemaker-runtime\",\n    },\n    \"sdb\": {\"endpoint_prefix\": \"sdb\", \"service_id\": \"simpledb\"},\n    \"secretsmanager\": {\n        \"endpoint_prefix\": \"secretsmanager\",\n        \"service_id\": \"secrets-manager\",\n    },\n    \"serverlessrepo\": {\n        \"endpoint_prefix\": \"serverlessrepo\",\n        \"service_id\": \"serverlessapplicationrepository\",\n    },\n    \"servicecatalog\": {\n        \"endpoint_prefix\": \"servicecatalog\",\n        \"service_id\": \"service-catalog\",\n    },\n    \"servicediscovery\": {\n        \"endpoint_prefix\": \"servicediscovery\",\n        \"service_id\": \"servicediscovery\",\n    },\n    \"ses\": {\"endpoint_prefix\": \"email\", \"service_id\": \"ses\"},\n    \"shield\": {\"endpoint_prefix\": \"shield\", \"service_id\": \"shield\"},\n    \"sms\": {\"endpoint_prefix\": \"sms\", \"service_id\": \"sms\"},\n    \"snowball\": {\"endpoint_prefix\": \"snowball\", \"service_id\": \"snowball\"},\n    \"sns\": {\"endpoint_prefix\": \"sns\", \"service_id\": \"sns\"},\n    \"sqs\": {\"endpoint_prefix\": \"sqs\", \"service_id\": \"sqs\"},\n    \"ssm\": {\"endpoint_prefix\": \"ssm\", \"service_id\": \"ssm\"},\n    \"stepfunctions\": {\"endpoint_prefix\": \"states\", \"service_id\": \"sfn\"},\n    \"storagegateway\": {\n        \"endpoint_prefix\": \"storagegateway\",\n        \"service_id\": \"storage-gateway\",\n    },\n    \"sts\": {\"endpoint_prefix\": \"sts\", \"service_id\": \"sts\"},\n    \"support\": {\"endpoint_prefix\": \"support\", \"service_id\": \"support\"},\n    \"swf\": {\"endpoint_prefix\": \"swf\", \"service_id\": \"swf\"},\n    \"transcribe\": {\n        \"endpoint_prefix\": \"transcribe\",\n        \"service_id\": \"transcribe\",\n    },\n    \"translate\": {\"endpoint_prefix\": \"translate\", \"service_id\": \"translate\"},\n    \"waf\": {\"endpoint_prefix\": \"waf\", \"service_id\": \"waf\"},\n    \"waf-regional\": {\n        \"endpoint_prefix\": \"waf-regional\",\n        \"service_id\": \"waf-regional\",\n    },\n    \"workdocs\": {\"endpoint_prefix\": \"workdocs\", \"service_id\": \"workdocs\"},\n    \"workmail\": {\"endpoint_prefix\": \"workmail\", \"service_id\": \"workmail\"},\n    \"workspaces\": {\n        \"endpoint_prefix\": \"workspaces\",\n        \"service_id\": \"workspaces\",\n    },\n    \"xray\": {\"endpoint_prefix\": \"xray\", \"service_id\": \"xray\"},\n}\n\n\ndef _event_aliases():\n    for client_name in SERVICES.keys():\n        service_id = SERVICES[client_name]['service_id']\n        yield client_name, service_id\n\n\ndef _event_aliases_with_endpoint_prefix():\n    for client_name in SERVICES.keys():\n        endpoint_prefix = SERVICES[client_name].get('endpoint_prefix')\n        if endpoint_prefix is not None:\n            yield client_name, endpoint_prefix\n\n\n@pytest.mark.parametrize(\n    \"client_name, endpoint_prefix\", _event_aliases_with_endpoint_prefix()\n)\ndef test_event_alias_by_endpoint_prefix(client_name, endpoint_prefix):\n    _assert_handler_called(client_name, endpoint_prefix)\n\n\n@pytest.mark.parametrize(\"client_name, service_id\", _event_aliases())\ndef test_event_alias_by_service_id(client_name, service_id):\n    _assert_handler_called(client_name, service_id)\n\n\n@pytest.mark.parametrize(\"client_name, service_id\", _event_aliases())\ndef test_event_alias_by_client_name(client_name, service_id):\n    _assert_handler_called(client_name, client_name)\n\n\ndef _assert_handler_called(client_name, event_part):\n    hook_calls = []\n\n    def _hook(**kwargs):\n        hook_calls.append(kwargs['event_name'])\n\n    session = _get_session()\n    session.register('creating-client-class.%s' % event_part, _hook)\n    session.create_client(client_name)\n    assert len(hook_calls) == 1\n\n\ndef _get_session():\n    session = Session()\n    session.set_credentials('foo', 'bar')\n    session.set_config_variable('region', 'us-west-2')\n    session.config_filename = 'no-exist-foo'\n    return session\n", "tests/functional/test_client_class_names.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nimport botocore.session\n\nREGION = 'us-east-1'\n\nSERVICE_TO_CLASS_NAME = {\n    'autoscaling': 'AutoScaling',\n    'cloudformation': 'CloudFormation',\n    'cloudfront': 'CloudFront',\n    'cloudhsm': 'CloudHSM',\n    'cloudsearch': 'CloudSearch',\n    'cloudsearchdomain': 'CloudSearchDomain',\n    'cloudtrail': 'CloudTrail',\n    'cloudwatch': 'CloudWatch',\n    'codedeploy': 'CodeDeploy',\n    'cognito-identity': 'CognitoIdentity',\n    'cognito-sync': 'CognitoSync',\n    'config': 'ConfigService',\n    'datapipeline': 'DataPipeline',\n    'directconnect': 'DirectConnect',\n    'ds': 'DirectoryService',\n    'dynamodb': 'DynamoDB',\n    'ec2': 'EC2',\n    'ecs': 'ECS',\n    'efs': 'EFS',\n    'elasticache': 'ElastiCache',\n    'elasticbeanstalk': 'ElasticBeanstalk',\n    'elastictranscoder': 'ElasticTranscoder',\n    'elb': 'ElasticLoadBalancing',\n    'emr': 'EMR',\n    'glacier': 'Glacier',\n    'iam': 'IAM',\n    'importexport': 'ImportExport',\n    'kinesis': 'Kinesis',\n    'kms': 'KMS',\n    'lambda': 'Lambda',\n    'logs': 'CloudWatchLogs',\n    'machinelearning': 'MachineLearning',\n    'opsworks': 'OpsWorks',\n    'rds': 'RDS',\n    'redshift': 'Redshift',\n    'route53': 'Route53',\n    'route53domains': 'Route53Domains',\n    's3': 'S3',\n    'sdb': 'SimpleDB',\n    'ses': 'SES',\n    'sns': 'SNS',\n    'sqs': 'SQS',\n    'ssm': 'SSM',\n    'storagegateway': 'StorageGateway',\n    'sts': 'STS',\n    'support': 'Support',\n    'swf': 'SWF',\n    'workspaces': 'WorkSpaces',\n}\n\n\n@pytest.mark.parametrize(\"service_name\", SERVICE_TO_CLASS_NAME)\ndef test_client_has_correct_class_name(service_name):\n    session = botocore.session.get_session()\n    client = session.create_client(service_name, REGION)\n    assert client.__class__.__name__ == SERVICE_TO_CLASS_NAME[service_name]\n", "tests/functional/test_apigateway.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestApiGateway(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client('apigateway', self.region)\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n    def test_get_export(self):\n        params = {\n            'restApiId': 'foo',\n            'stageName': 'bar',\n            'exportType': 'swagger',\n            'accepts': 'application/yaml',\n        }\n\n        self.http_stubber.add_response(body=b'{}')\n        with self.http_stubber:\n            self.client.get_export(**params)\n            request = self.http_stubber.requests[0]\n            self.assertEqual(request.method, 'GET')\n            self.assertEqual(\n                request.headers.get('Accept'), b'application/yaml'\n            )\n", "tests/functional/test_mturk.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.stub import Stubber\nfrom tests import BaseSessionTest\n\n\nclass TestMturk(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client('mturk', self.region)\n        self.stubber = Stubber(self.client)\n        self.stubber.activate()\n\n    def tearDown(self):\n        super().tearDown()\n        self.stubber.deactivate()\n\n    def test_list_hits_aliased(self):\n        self.stubber.add_response('list_hits_for_qualification_type', {})\n        self.stubber.add_response('list_hits_for_qualification_type', {})\n\n        params = {'QualificationTypeId': 'foo'}\n\n        self.client.list_hi_ts_for_qualification_type(**params)\n        self.client.list_hits_for_qualification_type(**params)\n\n        self.stubber.assert_no_pending_responses()\n", "tests/functional/test_sqs.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport pytest\n\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass BaseSQSOperationTest(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = \"us-west-2\"\n        self.client = self.session.create_client(\"sqs\", self.region)\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n\nclass SQSQueryCompatibleTest(BaseSQSOperationTest):\n    def test_query_compatible_error_parsing(self):\n        \"\"\"When migrating SQS from the ``query`` protocol to ``json``,\n        we unintentionally moved from modeled errors to a general ``ClientError``.\n        This ensures we're not silently regressing that behavior.\n        \"\"\"\n\n        error_body = (\n            b'{\"__type\":\"com.amazonaws.sqs#QueueDoesNotExist\",'\n            b'\"message\":\"The specified queue does not exist.\"}'\n        )\n        error_headers = {\n            \"x-amzn-query-error\": \"AWS.SimpleQueueService.NonExistentQueue;Sender\",\n        }\n        with self.http_stubber as stub:\n            stub.add_response(\n                status=400, body=error_body, headers=error_headers\n            )\n            with pytest.raises(self.client.exceptions.QueueDoesNotExist):\n                self.client.delete_queue(\n                    QueueUrl=\"not-a-real-queue-botocore\",\n                )\n", "tests/functional/test_docdb.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestDocDBPresignUrlInjection(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client('docdb', 'us-west-2')\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n    def assert_presigned_url_injected_in_request(self, body):\n        self.assertIn('PreSignedUrl', body)\n        self.assertNotIn('SourceRegion', body)\n\n    def test_create_db_cluster(self):\n        params = {\n            'DBClusterIdentifier': 'my-cluster',\n            'Engine': 'docdb',\n            'SourceRegion': 'us-east-1',\n            'MasterUsername': 'master',\n            'MasterUserPassword': 'mypassword',\n        }\n        response_body = (\n            b'<CreateDBClusterResponse>'\n            b'<CreateDBClusterResult>'\n            b'</CreateDBClusterResult>'\n            b'</CreateDBClusterResponse>'\n        )\n        self.http_stubber.add_response(body=response_body)\n        with self.http_stubber:\n            self.client.create_db_cluster(**params)\n            sent_request = self.http_stubber.requests[0]\n            self.assert_presigned_url_injected_in_request(sent_request.body)\n\n    def test_copy_db_cluster_snapshot(self):\n        params = {\n            'SourceDBClusterSnapshotIdentifier': 'source-db',\n            'TargetDBClusterSnapshotIdentifier': 'target-db',\n            'SourceRegion': 'us-east-1',\n        }\n        response_body = (\n            b'<CopyDBClusterSnapshotResponse>'\n            b'<CopyDBClusterSnapshotResult>'\n            b'</CopyDBClusterSnapshotResult>'\n            b'</CopyDBClusterSnapshotResponse>'\n        )\n        self.http_stubber.add_response(body=response_body)\n        with self.http_stubber:\n            self.client.copy_db_cluster_snapshot(**params)\n            sent_request = self.http_stubber.requests[0]\n            self.assert_presigned_url_injected_in_request(sent_request.body)\n", "tests/functional/test_retry.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport contextlib\nimport datetime\nimport json\n\nimport botocore.endpoint\nfrom botocore.config import Config\nfrom botocore.exceptions import ClientError\nfrom tests import BaseSessionTest, ClientHTTPStubber, mock\n\nRETRY_MODES = ('legacy', 'standard', 'adaptive')\n\n\nclass BaseRetryTest(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.sleep_patch = mock.patch('time.sleep')\n        self.sleep_patch.start()\n\n    def tearDown(self):\n        super().tearDown()\n        self.sleep_patch.stop()\n\n    @contextlib.contextmanager\n    def assert_will_retry_n_times(\n        self, client, num_retries, status=500, body=b'{}'\n    ):\n        num_responses = num_retries + 1\n        if not isinstance(body, bytes):\n            body = json.dumps(body).encode()\n        with ClientHTTPStubber(client) as http_stubber:\n            for _ in range(num_responses):\n                http_stubber.add_response(status=status, body=body)\n            with self.assertRaisesRegex(\n                ClientError, 'reached max retries: %s' % num_retries\n            ):\n                yield\n            self.assertEqual(len(http_stubber.requests), num_responses)\n\n\nclass TestRetryHeader(BaseRetryTest):\n    def _retry_headers_test_cases(self):\n        responses = [\n            [\n                (500, {'Date': 'Sat, 01 Jun 2019 00:00:00 GMT'}),\n                (500, {'Date': 'Sat, 01 Jun 2019 00:00:01 GMT'}),\n                (200, {'Date': 'Sat, 01 Jun 2019 00:00:02 GMT'}),\n            ],\n            [\n                (500, {'Date': 'Sat, 01 Jun 2019 00:10:03 GMT'}),\n                (500, {'Date': 'Sat, 01 Jun 2019 00:10:09 GMT'}),\n                (200, {'Date': 'Sat, 01 Jun 2019 00:10:15 GMT'}),\n            ],\n        ]\n\n        # The first, third and seventh datetime values of each\n        # utcnow_side_effects list are side_effect values for when\n        # utcnow is called in SigV4 signing.\n        utcnow_side_effects = [\n            [\n                datetime.datetime(2019, 6, 1, 0, 0, 0, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 0, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 1, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 0, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 1, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 2, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 0, 0),\n            ],\n            [\n                datetime.datetime(2020, 6, 1, 0, 0, 0, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 5, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 6, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 0, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 11, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 12, 0),\n                datetime.datetime(2019, 6, 1, 0, 0, 0, 0),\n            ],\n        ]\n        expected_headers = [\n            [\n                b'attempt=1',\n                b'ttl=20190601T000011Z; attempt=2; max=3',\n                b'ttl=20190601T000012Z; attempt=3; max=3',\n            ],\n            [\n                b'attempt=1',\n                b'ttl=20190601T001014Z; attempt=2; max=3',\n                b'ttl=20190601T001020Z; attempt=3; max=3',\n            ],\n        ]\n        test_cases = list(\n            zip(responses, utcnow_side_effects, expected_headers)\n        )\n        return test_cases\n\n    def _test_amz_sdk_request_header_with_test_case(\n        self, responses, utcnow_side_effects, expected_headers, client_config\n    ):\n        datetime_patcher = mock.patch.object(\n            botocore.endpoint.datetime,\n            'datetime',\n            mock.Mock(wraps=datetime.datetime),\n        )\n        mocked_datetime = datetime_patcher.start()\n        mocked_datetime.utcnow.side_effect = utcnow_side_effects\n\n        client = self.session.create_client(\n            'dynamodb', self.region, config=client_config\n        )\n        with ClientHTTPStubber(client) as http_stubber:\n            for response in responses:\n                http_stubber.add_response(\n                    headers=response[1], status=response[0], body=b'{}'\n                )\n            client.list_tables()\n            amz_sdk_request_headers = [\n                request.headers['amz-sdk-request']\n                for request in http_stubber.requests\n            ]\n            self.assertListEqual(amz_sdk_request_headers, expected_headers)\n        datetime_patcher.stop()\n\n    def test_amz_sdk_request_header(self):\n        test_cases = self._retry_headers_test_cases()\n        for retry_mode in RETRY_MODES:\n            retries_config = {'mode': retry_mode, 'total_max_attempts': 3}\n            client_config = Config(read_timeout=10, retries=retries_config)\n            for test_case in test_cases:\n                self._test_amz_sdk_request_header_with_test_case(\n                    *test_case, client_config=client_config\n                )\n\n    def test_amz_sdk_invocation_id_header_persists(self):\n        for retry_mode in RETRY_MODES:\n            client_config = Config(retries={'mode': retry_mode})\n            client = self.session.create_client(\n                'dynamodb', self.region, config=client_config\n            )\n            num_retries = 2\n            with ClientHTTPStubber(client) as http_stubber:\n                for _ in range(num_retries):\n                    http_stubber.add_response(status=500)\n                http_stubber.add_response(status=200)\n                client.list_tables()\n                amz_sdk_invocation_id_headers = [\n                    request.headers['amz-sdk-invocation-id']\n                    for request in http_stubber.requests\n                ]\n                self.assertEqual(\n                    amz_sdk_invocation_id_headers[0],\n                    amz_sdk_invocation_id_headers[1],\n                )\n                self.assertEqual(\n                    amz_sdk_invocation_id_headers[1],\n                    amz_sdk_invocation_id_headers[2],\n                )\n\n    def test_amz_sdk_invocation_id_header_unique_per_invocation(self):\n        client = self.session.create_client('dynamodb', self.region)\n        num_of_invocations = 2\n        with ClientHTTPStubber(client) as http_stubber:\n            for _ in range(num_of_invocations):\n                http_stubber.add_response(status=500)\n                http_stubber.add_response(status=200)\n                client.list_tables()\n            amz_sdk_invocation_id_headers = [\n                request.headers['amz-sdk-invocation-id']\n                for request in http_stubber.requests\n            ]\n            self.assertEqual(\n                amz_sdk_invocation_id_headers[0],\n                amz_sdk_invocation_id_headers[1],\n            )\n            self.assertEqual(\n                amz_sdk_invocation_id_headers[2],\n                amz_sdk_invocation_id_headers[3],\n            )\n            self.assertNotEqual(\n                amz_sdk_invocation_id_headers[0],\n                amz_sdk_invocation_id_headers[2],\n            )\n\n\nclass TestLegacyRetry(BaseRetryTest):\n    def test_can_override_max_attempts(self):\n        client = self.session.create_client(\n            'dynamodb', self.region, config=Config(retries={'max_attempts': 1})\n        )\n        with self.assert_will_retry_n_times(client, 1):\n            client.list_tables()\n\n    def test_do_not_attempt_retries(self):\n        client = self.session.create_client(\n            'dynamodb', self.region, config=Config(retries={'max_attempts': 0})\n        )\n        with self.assert_will_retry_n_times(client, 0):\n            client.list_tables()\n\n    def test_setting_max_attempts_does_not_set_for_other_clients(self):\n        # Make one client with max attempts configured.\n        self.session.create_client(\n            'codecommit',\n            self.region,\n            config=Config(retries={'max_attempts': 1}),\n        )\n\n        # Make another client that has no custom retry configured.\n        client = self.session.create_client('codecommit', self.region)\n        # It should use the default max retries, which should be four retries\n        # for this service.\n        with self.assert_will_retry_n_times(client, 4):\n            client.list_repositories()\n\n    def test_service_specific_defaults_do_not_mutate_general_defaults(self):\n        # This tests for a bug where if you created a client for a service\n        # with specific retry configurations and then created a client for\n        # a service whose retry configurations fallback to the general\n        # defaults, the second client would actually use the defaults of\n        # the first client.\n\n        # Make a dynamodb client. It's a special case client that is\n        # configured to a make a maximum of 10 requests (9 retries).\n        client = self.session.create_client('dynamodb', self.region)\n        with self.assert_will_retry_n_times(client, 9):\n            client.list_tables()\n\n        # A codecommit client is not a special case for retries. It will at\n        # most make 5 requests (4 retries) for its default.\n        client = self.session.create_client('codecommit', self.region)\n        with self.assert_will_retry_n_times(client, 4):\n            client.list_repositories()\n\n    def test_set_max_attempts_on_session(self):\n        self.session.set_default_client_config(\n            Config(retries={'max_attempts': 1})\n        )\n        # Max attempts should be inherited from the session.\n        client = self.session.create_client('codecommit', self.region)\n        with self.assert_will_retry_n_times(client, 1):\n            client.list_repositories()\n\n    def test_can_clobber_max_attempts_on_session(self):\n        self.session.set_default_client_config(\n            Config(retries={'max_attempts': 1})\n        )\n        # Max attempts should override the session's configured max attempts.\n        client = self.session.create_client(\n            'codecommit',\n            self.region,\n            config=Config(retries={'max_attempts': 0}),\n        )\n        with self.assert_will_retry_n_times(client, 0):\n            client.list_repositories()\n\n\nclass TestRetriesV2(BaseRetryTest):\n    def create_client_with_retry_mode(\n        self, service, retry_mode, max_attempts=None\n    ):\n        retries = {'mode': retry_mode}\n        if max_attempts is not None:\n            retries['total_max_attempts'] = max_attempts\n        client = self.session.create_client(\n            service, self.region, config=Config(retries=retries)\n        )\n        return client\n\n    def test_standard_mode_has_default_3_retries(self):\n        client = self.create_client_with_retry_mode(\n            'dynamodb', retry_mode='standard'\n        )\n        with self.assert_will_retry_n_times(client, 2):\n            client.list_tables()\n\n    def test_standard_mode_can_configure_max_attempts(self):\n        client = self.create_client_with_retry_mode(\n            'dynamodb', retry_mode='standard', max_attempts=5\n        )\n        with self.assert_will_retry_n_times(client, 4):\n            client.list_tables()\n\n    def test_no_retry_needed_standard_mode(self):\n        client = self.create_client_with_retry_mode(\n            'dynamodb', retry_mode='standard'\n        )\n        with ClientHTTPStubber(client) as http_stubber:\n            http_stubber.add_response(status=200, body=b'{}')\n            client.list_tables()\n\n    def test_standard_mode_retry_throttling_error(self):\n        client = self.create_client_with_retry_mode(\n            'dynamodb', retry_mode='standard'\n        )\n        error_body = {\"__type\": \"ThrottlingException\", \"message\": \"Error\"}\n        with self.assert_will_retry_n_times(\n            client, 2, status=400, body=error_body\n        ):\n            client.list_tables()\n\n    def test_standard_mode_retry_transient_error(self):\n        client = self.create_client_with_retry_mode(\n            'dynamodb', retry_mode='standard'\n        )\n        with self.assert_will_retry_n_times(client, 2, status=502):\n            client.list_tables()\n\n    def test_adaptive_mode_still_retries_errors(self):\n        # Verify that adaptive mode is just adding on to standard mode.\n        client = self.create_client_with_retry_mode(\n            'dynamodb', retry_mode='adaptive'\n        )\n        with self.assert_will_retry_n_times(client, 2):\n            client.list_tables()\n\n    def test_adaptive_mode_retry_transient_error(self):\n        client = self.create_client_with_retry_mode(\n            'dynamodb', retry_mode='adaptive'\n        )\n        with self.assert_will_retry_n_times(client, 2, status=502):\n            client.list_tables()\n\n    def test_can_exhaust_default_retry_quota(self):\n        # Quota of 500 / 5 retry costs == 100 retry attempts\n        # 100 retry attempts / 2 retries per API call == 50 client calls\n        client = self.create_client_with_retry_mode(\n            'dynamodb', retry_mode='standard'\n        )\n        for i in range(50):\n            with self.assert_will_retry_n_times(client, 2, status=502):\n                client.list_tables()\n        # Now on the 51th attempt we should see quota errors, which we can\n        # verify by looking at the request metadata.\n        with ClientHTTPStubber(client) as http_stubber:\n            http_stubber.add_response(status=502, body=b'{}')\n            with self.assertRaises(ClientError) as e:\n                client.list_tables()\n        self.assertTrue(\n            e.exception.response['ResponseMetadata'].get('RetryQuotaReached')\n        )\n", "tests/functional/test_model_backcompat.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.session import Session\nfrom tests import ClientHTTPStubber\nfrom tests.functional import TEST_MODELS_DIR\n\n\ndef test_old_model_continues_to_work():\n    # This test ensures that botocore can load the service models as they exist\n    # today.  There's a directory in tests/functional/models that is a\n    # snapshot of a service model.  This test ensures that we can continue\n    # to stub an API call using this model.  That way if the models ever\n    # change we have a mechanism to ensure that the existing models continue\n    # to work with botocore.  The test should not change (with the exception\n    # of potential changes to the ClientHTTPStubber), and the files in\n    # tests/functional/models should not change!\n    session = Session()\n    loader = session.get_component('data_loader')\n    # We're adding our path to the existing search paths so we don't have to\n    # copy additional data files such as _retry.json to our TEST_MODELS_DIR.\n    # We only care about the service model and endpoints file not changing.\n    # This also prevents us from having to make any changes to this models dir\n    # if we end up adding a new data file that's needed to create clients.\n    # We're adding our TEST_MODELS_DIR as the first element in the list to\n    # ensure we load the endpoints.json file from TEST_MODELS_DIR.  For the\n    # service model we have an extra safety net where we can choose a custom\n    # client name.\n    loader.search_paths.insert(0, TEST_MODELS_DIR)\n\n    # The model dir we copied from botocore/data/acm was renamed to\n    # 'custom-acm' to ensure we're loading our version of the model and\n    # not the built in one.\n    client = session.create_client(\n        'custom-acm',\n        region_name='us-west-2',\n        aws_access_key_id='foo',\n        aws_secret_access_key='bar',\n    )\n    with ClientHTTPStubber(client) as stubber:\n        stubber.add_response(\n            url='https://acm.us-west-2.amazonaws.com/',\n            headers={\n                'x-amzn-RequestId': 'abcd',\n                'Date': 'Fri, 26 Oct 2018 01:46:30 GMT',\n                'Content-Length': '29',\n                'Content-Type': 'application/x-amz-json-1.1',\n            },\n            body=b'{\"CertificateSummaryList\":[]}',\n        )\n        response = client.list_certificates()\n        assert response == {\n            'CertificateSummaryList': [],\n            'ResponseMetadata': {\n                'HTTPHeaders': {\n                    'content-length': '29',\n                    'content-type': 'application/x-amz-json-1.1',\n                    'date': 'Fri, 26 Oct 2018 01:46:30 GMT',\n                    'x-amzn-requestid': 'abcd',\n                },\n                'HTTPStatusCode': 200,\n                'RequestId': 'abcd',\n                'RetryAttempts': 0,\n            },\n        }\n\n    # Also verify we can use the paginators as well.\n    assert client.can_paginate('list_certificates') is True\n    assert client.waiter_names == ['certificate_validated']\n", "tests/functional/test_endpoints.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nfrom botocore.session import get_session\nfrom botocore.utils import CLIENT_NAME_TO_HYPHENIZED_SERVICE_ID_OVERRIDES\n\nENDPOINT_PREFIX_OVERRIDE = {\n    # entry in endpoints.json -> actual endpoint prefix.\n    # The autoscaling-* services actually send requests to the\n    # autoscaling service, but they're exposed as separate clients\n    # in botocore.\n    'autoscaling-plans': 'autoscaling',\n    'application-autoscaling': 'autoscaling',\n    # For neptune, we send requests to the RDS endpoint.\n    'neptune': 'rds',\n    'docdb': 'rds',\n    # iotevents data endpoints.json and service-2.json don't line up.\n    'ioteventsdata': 'data.iotevents',\n    'iotsecuredtunneling': 'api.tunneling.iot',\n    'iotwireless': 'api.iotwireless',\n    'data.iot': 'data-ats.iot',\n}\n\nNOT_SUPPORTED_IN_SDK = [\n    'mobileanalytics',\n    'transcribestreaming',\n]\n\n\nSESSION = get_session()\nLOADER = SESSION.get_component('data_loader')\nAVAILABLE_SERVICES = LOADER.list_available_services('service-2')\n\n\ndef _known_endpoint_prefixes():\n    # The entries in endpoints.json are keyed off of the endpoint\n    # prefix.  We don't directly have that data, so we have to load\n    # every service model and look up its endpoint prefix in its\n    # ``metadata`` section.\n    return {\n        SESSION.get_service_model(service_name).endpoint_prefix\n        for service_name in AVAILABLE_SERVICES\n    }\n\n\ndef _computed_endpoint_prefixes():\n    # This verifies client names match up with data from the endpoints.json\n    # file.  We want to verify that every entry in the endpoints.json\n    # file corresponds to a client we can construct via\n    # session.create_client(...).\n    # So first we get a list of all the service names in the endpoints\n    # file.\n    endpoints = LOADER.load_data('endpoints')\n    # A service can be in multiple partitions so we're using\n    # a set here to remove dupes.\n    services_in_endpoints_file = set()\n    for partition in endpoints['partitions']:\n        for service in partition['services']:\n            # There are some services we don't support in the SDK\n            # so we don't need to add them to the list of services\n            # we need to check.\n            if service not in NOT_SUPPORTED_IN_SDK:\n                services_in_endpoints_file.add(service)\n\n    # Now we go through every known endpoint prefix in the endpoints.json\n    # file and ensure it maps to an endpoint prefix we've seen\n    # in a service model.\n    endpoint_prefixes = []\n    for endpoint_prefix in services_in_endpoints_file:\n        # Check for an override where we know that an entry\n        # in the endpoints.json actually maps to a different endpoint\n        # prefix.\n        endpoint_prefix = ENDPOINT_PREFIX_OVERRIDE.get(\n            endpoint_prefix, endpoint_prefix\n        )\n        endpoint_prefixes.append(endpoint_prefix)\n    return sorted(endpoint_prefixes)\n\n\nKNOWN_ENDPOINT_PREFIXES = _known_endpoint_prefixes()\nCOMPUTED_ENDPOINT_PREFIXES = _computed_endpoint_prefixes()\n\n\n@pytest.mark.parametrize(\"endpoint_prefix\", COMPUTED_ENDPOINT_PREFIXES)\ndef test_endpoint_matches_service(endpoint_prefix):\n    # We need to cross check all computed endpoints against our\n    # known values in endpoints.json, to ensure everything lines\n    # up correctly.\n    assert endpoint_prefix in KNOWN_ENDPOINT_PREFIXES\n\n\n@pytest.mark.parametrize(\"service_name\", AVAILABLE_SERVICES)\ndef test_client_name_matches_hyphenized_service_id(service_name):\n    \"\"\"Generates tests for each service to verify that the computed service\n    named based on the service id matches the service name used to\n    create a client (i.e the directory name in botocore/data)\n    unless there is an explicit exception.\n    \"\"\"\n    service_model = SESSION.get_service_model(service_name)\n    computed_name = service_model.service_id.replace(' ', '-').lower()\n\n    # Handle known exceptions where we have renamed the service directory\n    # for one reason or another.\n    actual_service_name = CLIENT_NAME_TO_HYPHENIZED_SERVICE_ID_OVERRIDES.get(\n        service_name, service_name\n    )\n\n    err_msg = (\n        f\"Actual service name `{actual_service_name}` does not match \"\n        f\"expected service name we computed: `{computed_name}`\"\n    )\n    assert computed_name == actual_service_name, err_msg\n", "tests/functional/test_service_names.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport re\n\nimport pytest\n\nfrom botocore.session import get_session\n\nBLOCKLIST = []\n\n# Service names are limited here to 50 characters here as that seems like a\n# reasonable limit in the general case. Services can be added to the\n# blacklist above to be given an exception.\nVALID_NAME_REGEX = re.compile(\n    (\n        '[a-z]'  # Starts with a letter\n        '[a-z0-9]*'  # Followed by any number of letters or digits\n        '(-[a-z0-9]+)*$'  # Dashes are allowed as long as they aren't\n        # consecutive or at the end\n    ),\n    re.M,\n)\nVALID_NAME_EXPLANATION = (\n    'Service names must be made up entirely of lowercase alphanumeric '\n    'characters and dashes. The name must start with a letter and may not end '\n    'with a dash'\n)\nMIN_NAME_LENGTH_EXPLANATION = (\n    'Service name must be greater than or equal to 2 characters in length.'\n)\nMAX_NAME_LENGTH_EXPLANATION = (\n    'Service name must be less than or equal to 50 characters in length.'\n)\nMIN_SERVICE_NAME_LENGTH = 2\nMAX_SERVICE_NAME_LENGTH = 50\n\n\ndef _service_names():\n    session = get_session()\n    loader = session.get_component('data_loader')\n    return loader.list_available_services('service-2')\n\n\n@pytest.mark.parametrize(\"service_name\", _service_names())\ndef test_service_names_are_valid_length(service_name):\n    if service_name not in BLOCKLIST:\n        service_name_length = len(service_name)\n        is_not_too_short = service_name_length >= MIN_SERVICE_NAME_LENGTH\n        is_not_too_long = service_name_length <= MAX_SERVICE_NAME_LENGTH\n\n        assert is_not_too_short, MIN_NAME_LENGTH_EXPLANATION\n        assert is_not_too_long, MAX_NAME_LENGTH_EXPLANATION\n\n\n@pytest.mark.parametrize(\"service_name\", _service_names())\ndef test_service_names_are_valid_pattern(service_name):\n    if service_name not in BLOCKLIST:\n        valid = VALID_NAME_REGEX.match(service_name) is not None\n        assert valid, VALID_NAME_EXPLANATION\n", "tests/functional/test_cloudsearchdomain.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestCloudsearchdomain(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client(\n            'cloudsearchdomain', self.region\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n    def test_search(self):\n        self.http_stubber.add_response(body=b'{}')\n        with self.http_stubber:\n            self.client.search(query='foo')\n            request = self.http_stubber.requests[0]\n            self.assertIn('q=foo', request.body)\n            self.assertEqual(request.method, 'POST')\n            content_type = b'application/x-www-form-urlencoded'\n            self.assertEqual(request.headers.get('Content-Type'), content_type)\n", "tests/functional/test_h2_required.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nfrom botocore.session import get_session\n\n_H2_REQUIRED = object()\n# Service names to list of known HTTP 2 operations\n_KNOWN_SERVICES = {\n    'qbusiness': ['Chat'],\n    'kinesis': ['SubscribeToShard'],\n    'lexv2-runtime': ['StartConversation'],\n}\n\n\ndef _all_test_cases():\n    session = get_session()\n    loader = session.get_component('data_loader')\n\n    services = loader.list_available_services('service-2')\n    h2_services = []\n    h2_operations = []\n\n    for service in services:\n        service_model = session.get_service_model(service)\n        h2_config = service_model.metadata.get('protocolSettings', {}).get(\n            'h2'\n        )\n        if h2_config == 'required':\n            h2_services.append(service)\n        elif h2_config == 'eventstream':\n            for operation in service_model.operation_names:\n                operation_model = service_model.operation_model(operation)\n                if operation_model.has_event_stream_output:\n                    h2_operations.append([service, operation])\n\n    return h2_services, h2_operations\n\n\nH2_SERVICES, H2_OPERATIONS = _all_test_cases()\n\n\n@pytest.mark.parametrize(\"h2_service\", H2_SERVICES)\ndef test_all_uses_of_h2_are_known(h2_service):\n    # Validates that a service that requires HTTP 2 for all operations is known\n    message = 'Found unknown HTTP 2 service: %s' % h2_service\n    assert _KNOWN_SERVICES.get(h2_service) is _H2_REQUIRED, message\n\n\n@pytest.mark.parametrize(\"h2_service, operation\", H2_OPERATIONS)\ndef test_all_h2_operations_are_known(h2_service, operation):\n    # Validates that an operation that requires HTTP 2 is known\n    known_operations = _KNOWN_SERVICES.get(h2_service, [])\n    message = 'Found unknown HTTP 2 operation: {}.{}'.format(\n        h2_service, operation\n    )\n    assert operation in known_operations, message\n", "tests/functional/test_loaders.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport shutil\n\nfrom botocore import loaders\nfrom tests import temporary_file, unittest\n\n\nclass TestLoaderAllowsDataPathOverride(unittest.TestCase):\n    def create_file(self, f, contents, name):\n        f.write(contents)\n        f.flush()\n        dirname = os.path.dirname(os.path.abspath(f.name))\n        override_name = os.path.join(dirname, name)\n        shutil.copy(f.name, override_name)\n        return override_name\n\n    def test_can_override_session(self):\n        with temporary_file('w') as f:\n            # We're going to override _retry.json in\n            # botocore/data by setting our own data directory.\n            override_name = self.create_file(\n                f, contents='{\"foo\": \"bar\"}', name='_retry.json'\n            )\n            new_data_path = os.path.dirname(override_name)\n            loader = loaders.create_loader(search_path_string=new_data_path)\n\n            new_content = loader.load_data('_retry')\n            # This should contain the content we just created.\n            self.assertEqual(new_content, {\"foo\": \"bar\"})\n", "tests/functional/test_cognito_idp.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nfrom tests import ClientHTTPStubber, create_session, mock\n\nOPERATION_PARAMS = {\n    'change_password': {\n        'PreviousPassword': 'myoldbadpassword',\n        'ProposedPassword': 'mynewgoodpassword',\n        'AccessToken': 'foobar',\n    },\n    'confirm_forgot_password': {\n        'ClientId': 'foo',\n        'Username': 'myusername',\n        'ConfirmationCode': 'thisismeforreal',\n        'Password': 'whydowesendpasswordsviaemail',\n    },\n    'confirm_sign_up': {\n        'ClientId': 'foo',\n        'Username': 'myusername',\n        'ConfirmationCode': 'ireallydowanttosignup',\n    },\n    'delete_user': {'AccessToken': 'foobar'},\n    'delete_user_attributes': {\n        'UserAttributeNames': ['myattribute'],\n        'AccessToken': 'foobar',\n    },\n    'forgot_password': {'ClientId': 'foo', 'Username': 'myusername'},\n    'get_user': {'AccessToken': 'foobar'},\n    'get_user_attribute_verification_code': {\n        'AttributeName': 'myattribute',\n        'AccessToken': 'foobar',\n    },\n    'resend_confirmation_code': {'ClientId': 'foo', 'Username': 'myusername'},\n    'set_user_settings': {\n        'AccessToken': 'randomtoken',\n        'MFAOptions': [\n            {'DeliveryMedium': 'SMS', 'AttributeName': 'someattributename'}\n        ],\n    },\n    'sign_up': {\n        'ClientId': 'foo',\n        'Username': 'bar',\n        'Password': 'mysupersecurepassword',\n    },\n    'update_user_attributes': {\n        'UserAttributes': [{'Name': 'someattributename', 'Value': 'newvalue'}],\n        'AccessToken': 'foobar',\n    },\n    'verify_user_attribute': {\n        'AttributeName': 'someattributename',\n        'Code': 'someverificationcode',\n        'AccessToken': 'foobar',\n    },\n}\n\n\n@pytest.mark.parametrize(\n    \"operation_name, parameters\", OPERATION_PARAMS.items()\n)\ndef test_unsigned_operations(operation_name, parameters):\n    environ = {\n        'AWS_ACCESS_KEY_ID': 'access_key',\n        'AWS_SECRET_ACCESS_KEY': 'secret_key',\n        'AWS_CONFIG_FILE': 'no-exist-foo',\n    }\n\n    with mock.patch('os.environ', environ):\n        session = create_session()\n        session.config_filename = 'no-exist-foo'\n        client = session.create_client('cognito-idp', 'us-west-2')\n        http_stubber = ClientHTTPStubber(client)\n\n        operation = getattr(client, operation_name)\n\n        http_stubber.add_response(body=b'{}')\n        with http_stubber:\n            operation(**parameters)\n            request = http_stubber.requests[0]\n\n        assert (\n            'authorization' not in request.headers\n        ), 'authorization header found in unsigned operation'\n", "tests/functional/test_model_completeness.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nimport pytest\n\nfrom botocore.exceptions import DataNotFoundError\nfrom botocore.loaders import Loader\n\nLOADER = Loader()\nAVAILABLE_SERVICES = LOADER.list_available_services(type_name='service-2')\nTEST_DATA_DIR = os.path.join(os.path.dirname(__file__), \"endpoint-rules\")\n\n\ndef _paginators_and_waiters_test_cases():\n    for service_name in AVAILABLE_SERVICES:\n        versions = LOADER.list_api_versions(service_name, 'service-2')\n        if len(versions) > 1:\n            for type_name in ['paginators-1', 'waiters-2']:\n                yield service_name, type_name, versions[-2], versions[-1]\n\n\n@pytest.mark.parametrize(\n    \"service_name, type_name, previous_version, latest_version\",\n    _paginators_and_waiters_test_cases(),\n)\ndef test_paginators_and_waiters_are_not_lost_in_new_version(\n    service_name, type_name, previous_version, latest_version\n):\n    # Make sure if a paginator and/or waiter exists in previous version,\n    # there will be a successor existing in latest version.\n    try:\n        LOADER.load_service_model(service_name, type_name, previous_version)\n    except DataNotFoundError:\n        pass\n    else:\n        try:\n            LOADER.load_service_model(service_name, type_name, latest_version)\n        except DataNotFoundError as e:\n            raise AssertionError(\n                f\"{type_name} must exist for {service_name}: {e}\"\n            )\n\n\ndef _endpoint_rule_set_cases():\n    for service_name in AVAILABLE_SERVICES:\n        versions = LOADER.list_api_versions(service_name, 'service-2')\n        for version in versions:\n            yield service_name, version\n\n\n# endpoint tests validations are included in\n# tests/functional/test_endpoint_rulesets.py\n@pytest.mark.parametrize(\n    \"service_name, version\",\n    _endpoint_rule_set_cases(),\n)\ndef test_all_endpoint_rule_sets_exist(service_name, version):\n    \"\"\"Tests the existence of endpoint-rule-set-1.json for each service\n    and verifies that content is present.\"\"\"\n    type_name = 'endpoint-rule-set-1'\n    data = LOADER.load_service_model(service_name, type_name, version)\n    assert len(data['rules']) >= 1\n\n\ndef test_partitions_exists():\n    \"\"\"Tests the existence of partitions.json and verifies that content is present.\"\"\"\n    data = LOADER.load_data('partitions')\n    assert len(data['partitions']) >= 4\n", "tests/functional/test_route53.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore.session\nfrom botocore.stub import Stubber\nfrom tests import BaseSessionTest, ClientHTTPStubber, unittest\n\n\nclass TestRoute53Pagination(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('route53', 'us-west-2')\n        self.stubber = Stubber(self.client)\n        # response has required fields\n        self.response = {\n            'HostedZones': [],\n            'Marker': '',\n            'IsTruncated': True,\n            'MaxItems': '1',\n        }\n        self.operation_name = 'list_hosted_zones'\n\n    def test_paginate_with_max_items_int(self):\n        # Route53 has a string type for MaxItems.  We need to ensure that this\n        # still works with integers as the cli auto converts the page size\n        # argument to an integer.\n        self.stubber.add_response(self.operation_name, self.response)\n        paginator = self.client.get_paginator('list_hosted_zones')\n        with self.stubber:\n            config = {'PageSize': 1}\n            results = list(paginator.paginate(PaginationConfig=config))\n            self.assertTrue(len(results) >= 0)\n\n    def test_paginate_with_max_items_str(self):\n        # Route53 has a string type for MaxItems.  We need to ensure that this\n        # still works with strings as that's the expected type for this key.\n        self.stubber.add_response(self.operation_name, self.response)\n        paginator = self.client.get_paginator('list_hosted_zones')\n        with self.stubber:\n            config = {'PageSize': '1'}\n            results = list(paginator.paginate(PaginationConfig=config))\n            self.assertTrue(len(results) >= 0)\n\n\nclass TestRoute53EndpointResolution(BaseSessionTest):\n    def create_stubbed_client(self, service_name, region_name, **kwargs):\n        client = self.session.create_client(\n            service_name, region_name, **kwargs\n        )\n        http_stubber = ClientHTTPStubber(client)\n        http_stubber.start()\n        http_stubber.add_response()\n        return client, http_stubber\n\n    def test_unregionalized_client_endpoint_resolution(self):\n        client, stubber = self.create_stubbed_client('route53', 'us-west-2')\n        client.list_geo_locations()\n        expected_url = 'https://route53.amazonaws.com/'\n        self.assertTrue(stubber.requests[0].url.startswith(expected_url))\n\n    def test_unregionalized_client_with_unknown_region(self):\n        client, stubber = self.create_stubbed_client('route53', 'not-real')\n        client.list_geo_locations()\n        expected_url = 'https://route53.amazonaws.com/'\n        self.assertTrue(stubber.requests[0].url.startswith(expected_url))\n", "tests/functional/test_service_alias.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nimport botocore.session\nfrom botocore.handlers import SERVICE_NAME_ALIASES\n\nCLIENT_KWARGS = {\n    \"region_name\": \"us-east-1\",\n    \"aws_access_key_id\": \"foo\",\n    \"aws_secret_access_key\": \"bar\",\n}\n\n\ndef _service_alias_test_cases():\n    session = botocore.session.get_session()\n    for alias, name in SERVICE_NAME_ALIASES.items():\n        yield session, name, alias\n\n\n@pytest.mark.parametrize(\n    \"session, service_name, service_alias\", _service_alias_test_cases()\n)\ndef test_can_use_service_alias(session, service_name, service_alias):\n    original_client = session.create_client(service_name, **CLIENT_KWARGS)\n    aliased_client = session.create_client(service_alias, **CLIENT_KWARGS)\n    original_model_name = original_client.meta.service_model.service_name\n    aliased_model_name = aliased_client.meta.service_model.service_name\n    assert original_model_name == aliased_model_name\n", "tests/functional/test_rds.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.stub import Stubber\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestRDSPresignUrlInjection(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client('rds', 'us-west-2')\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n    def assert_presigned_url_injected_in_request(self, body):\n        self.assertIn('PreSignedUrl', body)\n        self.assertNotIn('SourceRegion', body)\n\n    def test_copy_snapshot(self):\n        params = {\n            'SourceDBSnapshotIdentifier': 'source-db',\n            'TargetDBSnapshotIdentifier': 'target-db',\n            'SourceRegion': 'us-east-1',\n        }\n        response_body = (\n            b'<CopyDBSnapshotResponse>'\n            b'<CopyDBSnapshotResult></CopyDBSnapshotResult>'\n            b'</CopyDBSnapshotResponse>'\n        )\n        self.http_stubber.add_response(body=response_body)\n        with self.http_stubber:\n            self.client.copy_db_snapshot(**params)\n            sent_request = self.http_stubber.requests[0]\n            self.assert_presigned_url_injected_in_request(sent_request.body)\n\n    def test_create_db_instance_read_replica(self):\n        params = {\n            'SourceDBInstanceIdentifier': 'source-db',\n            'DBInstanceIdentifier': 'target-db',\n            'SourceRegion': 'us-east-1',\n        }\n        response_body = (\n            b'<CreateDBInstanceReadReplicaResponse>'\n            b'<CreateDBInstanceReadReplicaResult>'\n            b'</CreateDBInstanceReadReplicaResult>'\n            b'</CreateDBInstanceReadReplicaResponse>'\n        )\n        self.http_stubber.add_response(body=response_body)\n        with self.http_stubber:\n            self.client.create_db_instance_read_replica(**params)\n            sent_request = self.http_stubber.requests[0]\n            self.assert_presigned_url_injected_in_request(sent_request.body)\n\n    def test_start_db_instance_automated_backups_replication(self):\n        params = {\n            'SourceDBInstanceArn': 'arn:aws:rds:us-east-1:123456789012:db:source-db-instance',\n            'SourceRegion': 'us-east-1',\n        }\n        response_body = (\n            b'<StartDBInstanceAutomatedBackupsReplicationResponse>'\n            b'<StartDBInstanceAutomatedBackupsReplicationResult>'\n            b'</StartDBInstanceAutomatedBackupsReplicationResult>'\n            b'</StartDBInstanceAutomatedBackupsReplicationResponse>'\n        )\n        self.http_stubber.add_response(body=response_body)\n        with self.http_stubber:\n            self.client.start_db_instance_automated_backups_replication(\n                **params\n            )\n            sent_request = self.http_stubber.requests[0]\n            self.assert_presigned_url_injected_in_request(sent_request.body)\n\n\nclass TestRDS(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client('rds', 'us-west-2')\n        self.stubber = Stubber(self.client)\n        self.stubber.activate()\n\n    def test_generate_db_auth_token(self):\n        hostname = 'host.us-east-1.rds.amazonaws.com'\n        port = 3306\n        username = 'mySQLUser'\n        auth_token = self.client.generate_db_auth_token(\n            DBHostname=hostname, Port=port, DBUsername=username\n        )\n\n        endpoint_url = 'host.us-east-1.rds.amazonaws.com:3306'\n        self.assertIn(endpoint_url, auth_token)\n        self.assertIn('Action=connect', auth_token)\n\n        # Asserts that there is no scheme in the url\n        self.assertTrue(auth_token.startswith(hostname))\n", "tests/functional/test_lex.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom datetime import datetime\n\nfrom tests import BaseSessionTest, ClientHTTPStubber, mock\n\n\nclass TestLex(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client('lex-runtime', self.region)\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n    def test_unsigned_payload(self):\n        params = {\n            'botName': 'foo',\n            'botAlias': 'bar',\n            'userId': 'baz',\n            'contentType': 'application/octet-stream',\n            'inputStream': b'',\n        }\n\n        timestamp = datetime(2017, 3, 22, 0, 0)\n\n        with mock.patch('botocore.auth.datetime.datetime') as _datetime:\n            _datetime.utcnow.return_value = timestamp\n            self.http_stubber.add_response(body=b'{}')\n            with self.http_stubber:\n                self.client.post_content(**params)\n                request = self.http_stubber.requests[0]\n\n        # The payload gets added to the string to sign, and then part of the\n        # signature. The signature will be part of the authorization header.\n        # Since we don't have direct access to the payload signature,\n        # we compare the authorization instead.\n        authorization = request.headers.get('authorization')\n\n        expected_authorization = (\n            b'AWS4-HMAC-SHA256 '\n            b'Credential=access_key/20170322/us-west-2/lex/aws4_request, '\n            b'SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date,'\n            b' Signature='\n            b'7f93fde5c36163dce6ee116fcfebab13474ab903782fea04c00bb1dedc3fc4cc'\n        )\n        self.assertEqual(authorization, expected_authorization)\n\n        content_header = request.headers.get('x-amz-content-sha256')\n        self.assertEqual(content_header, b'UNSIGNED-PAYLOAD')\n\n\nclass TestLexV2(BaseSessionTest):\n    def test_start_conversation(self):\n        \"\"\"StartConversation operation removed due to h2 requirement\"\"\"\n        lexv2 = self.session.create_client('lexv2-runtime', 'us-west-2')\n        try:\n            lexv2.start_conversation\n        except AttributeError:\n            pass\n        else:\n            self.fail(\n                'start_conversation shouldn\\'t be available on the '\n                'lexv2-runtime client.'\n            )\n", "tests/functional/test_public_apis.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom collections import defaultdict\n\nimport pytest\n\nfrom botocore import xform_name\nfrom botocore.session import Session\nfrom tests import ClientHTTPStubber, mock\n\nREGIONS = defaultdict(lambda: 'us-east-1')\nPUBLIC_API_TESTS = {\n    \"cognito-identity\": {\n        \"GetId\": {\"IdentityPoolId\": \"region:1234\"},\n        \"GetOpenIdToken\": {\"IdentityId\": \"region:1234\"},\n        \"UnlinkIdentity\": {\n            \"IdentityId\": \"region:1234\",\n            \"Logins\": {},\n            \"LoginsToRemove\": [],\n        },\n        \"GetCredentialsForIdentity\": {\"IdentityId\": \"region:1234\"},\n    },\n    \"sts\": {\n        \"AssumeRoleWithSaml\": {\n            \"PrincipalArn\": \"a\" * 20,\n            \"RoleArn\": \"a\" * 20,\n            \"SAMLAssertion\": \"abcd\",\n        },\n        \"AssumeRoleWithWebIdentity\": {\n            \"RoleArn\": \"a\" * 20,\n            \"RoleSessionName\": \"foo\",\n            \"WebIdentityToken\": \"abcd\",\n        },\n    },\n}\n\n\nclass EarlyExit(Exception):\n    pass\n\n\ndef _public_apis():\n    session = Session()\n\n    # Mimic the scenario that user does not have aws credentials setup\n    session.get_credentials = mock.Mock(return_value=None)\n\n    for service_name in PUBLIC_API_TESTS:\n        client = session.create_client(service_name, REGIONS[service_name])\n        for operation_name in PUBLIC_API_TESTS[service_name]:\n            kwargs = PUBLIC_API_TESTS[service_name][operation_name]\n            method = getattr(client, xform_name(operation_name))\n            yield client, method, kwargs\n\n\n@pytest.mark.parametrize(\"client, operation, kwargs\", _public_apis())\ndef test_public_apis_will_not_be_signed(client, operation, kwargs):\n    with ClientHTTPStubber(client) as http_stubber:\n        http_stubber.responses.append(EarlyExit())\n        try:\n            operation(**kwargs)\n        except EarlyExit:\n            pass\n        request = http_stubber.requests[0]\n\n    sig_v2_disabled = 'SignatureVersion=2' not in request.url\n    assert sig_v2_disabled, \"SigV2 is incorrectly enabled\"\n    sig_v3_disabled = 'X-Amzn-Authorization' not in request.headers\n    assert sig_v3_disabled, \"SigV3 is incorrectly enabled\"\n    sig_v4_disabled = 'Authorization' not in request.headers\n    assert sig_v4_disabled, \"SigV4 is incorrectly enabled\"\n", "tests/functional/test_credentials.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport math\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport threading\nimport time\nimport uuid\nfrom datetime import datetime, timedelta, timezone\n\nimport pytest\nfrom dateutil.tz import tzlocal\n\nfrom botocore import UNSIGNED\nfrom botocore.config import Config\nfrom botocore.credentials import (\n    AssumeRoleProvider,\n    CanonicalNameCredentialSourcer,\n    ContainerProvider,\n    Credentials,\n    DeferredRefreshableCredentials,\n    EnvProvider,\n    InstanceMetadataProvider,\n    JSONFileCache,\n    ProfileProviderBuilder,\n    ReadOnlyCredentials,\n    create_credential_resolver,\n)\nfrom botocore.exceptions import (\n    CredentialRetrievalError,\n    InfiniteLoopConfigError,\n    InvalidConfigError,\n)\nfrom botocore.session import Session\nfrom botocore.stub import Stubber\nfrom botocore.tokens import SSOTokenProvider\nfrom botocore.utils import datetime2timestamp\nfrom tests import (\n    BaseEnvVar,\n    IntegerRefresher,\n    SessionHTTPStubber,\n    StubbedSession,\n    mock,\n    random_chars,\n    temporary_file,\n    unittest,\n)\n\nTIME_IN_ONE_HOUR = datetime.now(tz=timezone.utc) + timedelta(hours=1)\nTIME_IN_SIX_MONTHS = datetime.now(tz=timezone.utc) + timedelta(hours=4320)\n\n\nclass TestCredentialRefreshRaces(unittest.TestCase):\n    def assert_consistent_credentials_seen(self, creds, func):\n        collected = []\n        self._run_threads(20, func, collected)\n        for creds in collected:\n            # During testing, the refresher uses it's current\n            # refresh count as the values for the access, secret, and\n            # token value.  This means that at any given point in time,\n            # the credentials should be something like:\n            #\n            # ReadOnlyCredentials('1', '1', '1')\n            # ReadOnlyCredentials('2', '2', '2')\n            # ...\n            # ReadOnlyCredentials('30', '30', '30')\n            #\n            # This makes it really easy to verify we see a consistent\n            # set of credentials from the same time period.  We just\n            # check if all the credential values are the same.  If\n            # we ever see something like:\n            #\n            # ReadOnlyCredentials('1', '2', '1')\n            #\n            # We fail.  This is because we're using the access_key\n            # from the first refresh ('1'), the secret key from\n            # the second refresh ('2'), and the token from the\n            # first refresh ('1').\n            self.assertTrue(creds[0] == creds[1] == creds[2], creds)\n\n    def assert_non_none_retrieved_credentials(self, func):\n        collected = []\n        self._run_threads(50, func, collected)\n        for cred in collected:\n            self.assertIsNotNone(cred)\n\n    def _run_threads(self, num_threads, func, collected):\n        threads = []\n        for _ in range(num_threads):\n            threads.append(threading.Thread(target=func, args=(collected,)))\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n    def test_has_no_race_conditions(self):\n        creds = IntegerRefresher(\n            creds_last_for=2, advisory_refresh=1, mandatory_refresh=0\n        )\n\n        def _run_in_thread(collected):\n            for _ in range(4000):\n                frozen = creds.get_frozen_credentials()\n                collected.append(\n                    (frozen.access_key, frozen.secret_key, frozen.token)\n                )\n\n        start = time.time()\n        self.assert_consistent_credentials_seen(creds, _run_in_thread)\n        end = time.time()\n        # creds_last_for = 2 seconds (from above)\n        # So, for example, if execution time took 6.1 seconds, then\n        # we should see a maximum number of refreshes being (6 / 2.0) + 1 = 4\n        max_calls_allowed = math.ceil((end - start) / 2.0) + 1\n        self.assertTrue(\n            creds.refresh_counter <= max_calls_allowed,\n            \"Too many cred refreshes, max: %s, actual: %s, \"\n            \"time_delta: %.4f\"\n            % (max_calls_allowed, creds.refresh_counter, (end - start)),\n        )\n\n    def test_no_race_for_immediate_advisory_expiration(self):\n        creds = IntegerRefresher(\n            creds_last_for=1, advisory_refresh=1, mandatory_refresh=0\n        )\n\n        def _run_in_thread(collected):\n            for _ in range(100):\n                frozen = creds.get_frozen_credentials()\n                collected.append(\n                    (frozen.access_key, frozen.secret_key, frozen.token)\n                )\n\n        self.assert_consistent_credentials_seen(creds, _run_in_thread)\n\n    def test_no_race_for_initial_refresh_of_deferred_refreshable(self):\n        def get_credentials():\n            expiry_time = (\n                datetime.now(tzlocal()) + timedelta(hours=24)\n            ).isoformat()\n            return {\n                'access_key': 'my-access-key',\n                'secret_key': 'my-secret-key',\n                'token': 'my-token',\n                'expiry_time': expiry_time,\n            }\n\n        deferred_creds = DeferredRefreshableCredentials(\n            get_credentials, 'fixed'\n        )\n\n        def _run_in_thread(collected):\n            frozen = deferred_creds.get_frozen_credentials()\n            collected.append(frozen)\n\n        self.assert_non_none_retrieved_credentials(_run_in_thread)\n\n\nclass BaseAssumeRoleTest(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.tempdir = tempfile.mkdtemp()\n        self.config_file = os.path.join(self.tempdir, 'config')\n        self.environ['AWS_CONFIG_FILE'] = self.config_file\n        self.environ['AWS_SHARED_CREDENTIALS_FILE'] = str(uuid.uuid4())\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n        super().tearDown()\n\n    def some_future_time(self):\n        timeobj = datetime.now(tzlocal())\n        return timeobj + timedelta(hours=24)\n\n    def create_assume_role_response(self, credentials, expiration=None):\n        if expiration is None:\n            expiration = self.some_future_time()\n\n        response = {\n            'Credentials': {\n                'AccessKeyId': credentials.access_key,\n                'SecretAccessKey': credentials.secret_key,\n                'SessionToken': credentials.token,\n                'Expiration': expiration,\n            },\n            'AssumedRoleUser': {\n                'AssumedRoleId': 'myroleid',\n                'Arn': 'arn:aws:iam::1234567890:user/myuser',\n            },\n        }\n\n        return response\n\n    def create_random_credentials(self):\n        return Credentials(\n            'fake-%s' % random_chars(15),\n            'fake-%s' % random_chars(35),\n            'fake-%s' % random_chars(45),\n        )\n\n    def assert_creds_equal(self, c1, c2):\n        c1_frozen = c1\n        if not isinstance(c1_frozen, ReadOnlyCredentials):\n            c1_frozen = c1.get_frozen_credentials()\n        c2_frozen = c2\n        if not isinstance(c2_frozen, ReadOnlyCredentials):\n            c2_frozen = c2.get_frozen_credentials()\n        self.assertEqual(c1_frozen, c2_frozen)\n\n    def write_config(self, config):\n        with open(self.config_file, 'w') as f:\n            f.write(config)\n\n\nclass TestAssumeRole(BaseAssumeRoleTest):\n    def setUp(self):\n        super().setUp()\n        self.environ['AWS_ACCESS_KEY_ID'] = 'access_key'\n        self.environ['AWS_SECRET_ACCESS_KEY'] = 'secret_key'\n\n        self.metadata_provider = self.mock_provider(InstanceMetadataProvider)\n        self.env_provider = self.mock_provider(EnvProvider)\n        self.container_provider = self.mock_provider(ContainerProvider)\n        self.mock_client_creator = mock.Mock(spec=Session.create_client)\n        self.actual_client_region = None\n\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        credential_process = os.path.join(\n            current_dir, 'utils', 'credentialprocess.py'\n        )\n        self.credential_process = '{} {}'.format(\n            sys.executable, credential_process\n        )\n\n    def mock_provider(self, provider_cls):\n        mock_instance = mock.Mock(spec=provider_cls)\n        mock_instance.load.return_value = None\n        mock_instance.METHOD = provider_cls.METHOD\n        mock_instance.CANONICAL_NAME = provider_cls.CANONICAL_NAME\n        return mock_instance\n\n    def create_session(self, profile=None, sso_token_cache=None):\n        session = StubbedSession(profile=profile)\n        if not sso_token_cache:\n            sso_token_cache = JSONFileCache(self.tempdir)\n        # We have to set bogus credentials here or otherwise we'll trigger\n        # an early credential chain resolution.\n        sts = session.create_client(\n            'sts',\n            aws_access_key_id='spam',\n            aws_secret_access_key='eggs',\n        )\n        self.mock_client_creator.return_value = sts\n        assume_role_provider = AssumeRoleProvider(\n            load_config=lambda: session.full_config,\n            client_creator=self.mock_client_creator,\n            cache={},\n            profile_name=profile,\n            credential_sourcer=CanonicalNameCredentialSourcer(\n                [\n                    self.env_provider,\n                    self.container_provider,\n                    self.metadata_provider,\n                ]\n            ),\n            profile_provider_builder=ProfileProviderBuilder(\n                session,\n                sso_token_cache=sso_token_cache,\n            ),\n        )\n        stubber = session.stub('sts')\n        stubber.activate()\n\n        component_name = 'credential_provider'\n        resolver = session.get_component(component_name)\n        available_methods = [p.METHOD for p in resolver.providers]\n        replacements = {\n            'env': self.env_provider,\n            'iam-role': self.metadata_provider,\n            'container-role': self.container_provider,\n            'assume-role': assume_role_provider,\n        }\n        for name, provider in replacements.items():\n            try:\n                index = available_methods.index(name)\n            except ValueError:\n                # The provider isn't in the session\n                continue\n\n            resolver.providers[index] = provider\n\n        session.register_component('credential_provider', resolver)\n        return session, stubber\n\n    def test_assume_role(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n\\n'\n            '[profile B]\\n'\n            'aws_access_key_id = abc123\\n'\n            'aws_secret_access_key = def456\\n'\n        )\n        self.write_config(config)\n\n        expected_creds = self.create_random_credentials()\n        response = self.create_assume_role_response(expected_creds)\n        session, stubber = self.create_session(profile='A')\n        stubber.add_response('assume_role', response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n        stubber.assert_no_pending_responses()\n\n    def test_environment_credential_source(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'credential_source = Environment\\n'\n        )\n        self.write_config(config)\n\n        environment_creds = self.create_random_credentials()\n        self.env_provider.load.return_value = environment_creds\n\n        expected_creds = self.create_random_credentials()\n        response = self.create_assume_role_response(expected_creds)\n        session, stubber = self.create_session(profile='A')\n        stubber.add_response('assume_role', response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n\n        stubber.assert_no_pending_responses()\n        self.assertEqual(self.env_provider.load.call_count, 1)\n\n    def test_instance_metadata_credential_source(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'credential_source = Ec2InstanceMetadata\\n'\n        )\n        self.write_config(config)\n\n        metadata_creds = self.create_random_credentials()\n        self.metadata_provider.load.return_value = metadata_creds\n\n        expected_creds = self.create_random_credentials()\n        response = self.create_assume_role_response(expected_creds)\n        session, stubber = self.create_session(profile='A')\n        stubber.add_response('assume_role', response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n\n        stubber.assert_no_pending_responses()\n        self.assertEqual(self.metadata_provider.load.call_count, 1)\n\n    def test_container_credential_source(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'credential_source = EcsContainer\\n'\n        )\n        self.write_config(config)\n\n        container_creds = self.create_random_credentials()\n        self.container_provider.load.return_value = container_creds\n\n        expected_creds = self.create_random_credentials()\n        response = self.create_assume_role_response(expected_creds)\n        session, stubber = self.create_session(profile='A')\n        stubber.add_response('assume_role', response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n\n        stubber.assert_no_pending_responses()\n        self.assertEqual(self.container_provider.load.call_count, 1)\n\n    def test_invalid_credential_source(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'credential_source = CustomInvalidProvider\\n'\n        )\n        self.write_config(config)\n\n        with self.assertRaises(InvalidConfigError):\n            session, _ = self.create_session(profile='A')\n            session.get_credentials()\n\n    def test_misconfigured_source_profile(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n'\n            '[profile B]\\n'\n            'region = us-west-2\\n'\n        )\n        self.write_config(config)\n\n        with self.assertRaises(InvalidConfigError):\n            session, _ = self.create_session(profile='A')\n            session.get_credentials().get_frozen_credentials()\n\n    def test_recursive_assume_role(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n\\n'\n            '[profile B]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleB\\n'\n            'source_profile = C\\n\\n'\n            '[profile C]\\n'\n            'aws_access_key_id = abc123\\n'\n            'aws_secret_access_key = def456\\n'\n        )\n        self.write_config(config)\n\n        profile_b_creds = self.create_random_credentials()\n        profile_b_response = self.create_assume_role_response(profile_b_creds)\n        profile_a_creds = self.create_random_credentials()\n        profile_a_response = self.create_assume_role_response(profile_a_creds)\n\n        session, stubber = self.create_session(profile='A')\n        stubber.add_response('assume_role', profile_b_response)\n        stubber.add_response('assume_role', profile_a_response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, profile_a_creds)\n        stubber.assert_no_pending_responses()\n\n    def test_recursive_assume_role_stops_at_static_creds(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n\\n'\n            '[profile B]\\n'\n            'aws_access_key_id = abc123\\n'\n            'aws_secret_access_key = def456\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleB\\n'\n            'source_profile = C\\n\\n'\n            '[profile C]\\n'\n            'aws_access_key_id = abc123\\n'\n            'aws_secret_access_key = def456\\n'\n        )\n        self.write_config(config)\n\n        profile_a_creds = self.create_random_credentials()\n        profile_a_response = self.create_assume_role_response(profile_a_creds)\n        session, stubber = self.create_session(profile='A')\n        stubber.add_response('assume_role', profile_a_response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, profile_a_creds)\n        stubber.assert_no_pending_responses()\n\n    def test_infinitely_recursive_assume_role(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = A\\n'\n        )\n        self.write_config(config)\n\n        with self.assertRaises(InfiniteLoopConfigError):\n            session, _ = self.create_session(profile='A')\n            session.get_credentials()\n\n    def test_process_source_profile(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n'\n            '[profile B]\\n'\n            'credential_process = %s\\n' % self.credential_process\n        )\n        self.write_config(config)\n\n        expected_creds = self.create_random_credentials()\n        response = self.create_assume_role_response(expected_creds)\n        session, stubber = self.create_session(profile='A')\n        stubber.add_response('assume_role', response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n        stubber.assert_no_pending_responses()\n        # Assert that the client was created with the credentials from the\n        # credential process.\n        self.assertEqual(self.mock_client_creator.call_count, 1)\n        _, kwargs = self.mock_client_creator.call_args_list[0]\n        expected_kwargs = {\n            'aws_access_key_id': 'spam',\n            'aws_secret_access_key': 'eggs',\n            'aws_session_token': None,\n        }\n        self.assertEqual(kwargs, expected_kwargs)\n\n    def test_web_identity_source_profile(self):\n        token_path = os.path.join(self.tempdir, 'token')\n        with open(token_path, 'w') as token_file:\n            token_file.write('a.token')\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n'\n            '[profile B]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleB\\n'\n            'web_identity_token_file = %s\\n' % token_path\n        )\n        self.write_config(config)\n\n        session, stubber = self.create_session(profile='A')\n\n        identity_creds = self.create_random_credentials()\n        identity_response = self.create_assume_role_response(identity_creds)\n        stubber.add_response(\n            'assume_role_with_web_identity',\n            identity_response,\n        )\n\n        expected_creds = self.create_random_credentials()\n        assume_role_response = self.create_assume_role_response(expected_creds)\n        stubber.add_response('assume_role', assume_role_response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n        stubber.assert_no_pending_responses()\n        # Assert that the client was created with the credentials from the\n        # assume role with web identity call.\n        self.assertEqual(self.mock_client_creator.call_count, 1)\n        _, kwargs = self.mock_client_creator.call_args_list[0]\n        expected_kwargs = {\n            'aws_access_key_id': identity_creds.access_key,\n            'aws_secret_access_key': identity_creds.secret_key,\n            'aws_session_token': identity_creds.token,\n        }\n        self.assertEqual(kwargs, expected_kwargs)\n\n    def test_web_identity_source_profile_ignores_env_vars(self):\n        token_path = os.path.join(self.tempdir, 'token')\n        with open(token_path, 'w') as token_file:\n            token_file.write('a.token')\n        self.environ['AWS_ROLE_ARN'] = 'arn:aws:iam::123456789:role/RoleB'\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n'\n            '[profile B]\\n'\n            'web_identity_token_file = %s\\n' % token_path\n        )\n        self.write_config(config)\n\n        session, _ = self.create_session(profile='A')\n        # The config is split between the profile and the env, we\n        # should only be looking at the profile so this should raise\n        # a configuration error.\n        with self.assertRaises(InvalidConfigError):\n            session.get_credentials()\n\n    def test_sso_source_profile_legacy(self):\n        token_cache_key = 'f395038c92f1828cbb3991d2d6152d326b895606'\n        cached_token = {\n            'accessToken': 'a.token',\n            'expiresAt': self.some_future_time(),\n        }\n        temp_cache = JSONFileCache(self.tempdir)\n        temp_cache[token_cache_key] = cached_token\n\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n'\n            '[profile B]\\n'\n            'sso_region = us-east-1\\n'\n            'sso_start_url = https://test.url/start\\n'\n            'sso_role_name = SSORole\\n'\n            'sso_account_id = 1234567890\\n'\n        )\n        self.write_config(config)\n\n        session, sts_stubber = self.create_session(profile='A')\n        client_config = Config(\n            region_name='us-east-1',\n            signature_version=UNSIGNED,\n        )\n        sso_stubber = session.stub('sso', config=client_config)\n        sso_stubber.activate()\n        # The expiration needs to be in milliseconds\n        expiration = datetime2timestamp(self.some_future_time()) * 1000\n        sso_role_creds = self.create_random_credentials()\n        sso_role_response = {\n            'roleCredentials': {\n                'accessKeyId': sso_role_creds.access_key,\n                'secretAccessKey': sso_role_creds.secret_key,\n                'sessionToken': sso_role_creds.token,\n                'expiration': int(expiration),\n            }\n        }\n        sso_stubber.add_response('get_role_credentials', sso_role_response)\n\n        expected_creds = self.create_random_credentials()\n        assume_role_response = self.create_assume_role_response(expected_creds)\n        sts_stubber.add_response('assume_role', assume_role_response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n        sts_stubber.assert_no_pending_responses()\n        # Assert that the client was created with the credentials from the\n        # SSO get role credentials response\n        self.assertEqual(self.mock_client_creator.call_count, 1)\n        _, kwargs = self.mock_client_creator.call_args_list[0]\n        expected_kwargs = {\n            'aws_access_key_id': sso_role_creds.access_key,\n            'aws_secret_access_key': sso_role_creds.secret_key,\n            'aws_session_token': sso_role_creds.token,\n        }\n        self.assertEqual(kwargs, expected_kwargs)\n\n    def test_sso_source_profile(self):\n        token_cache_key = '32096c2e0eff33d844ee6d675407ace18289357d'\n        cached_token = {\n            'accessToken': 'C',\n            'expiresAt': TIME_IN_ONE_HOUR.strftime('%Y-%m-%dT%H:%M:%SZ'),\n        }\n        temp_cache = JSONFileCache(self.tempdir)\n        temp_cache[token_cache_key] = cached_token\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n'\n            '[profile B]\\n'\n            'sso_session = C\\n'\n            'sso_role_name = SSORole\\n'\n            'sso_account_id = 1234567890\\n'\n            '[sso-session C]\\n'\n            'sso_region = us-east-1\\n'\n            'sso_start_url = https://test.url/start\\n'\n        )\n        self.write_config(config)\n\n        session, sts_stubber = self.create_session(\n            profile='A', sso_token_cache=temp_cache\n        )\n        client_config = Config(\n            region_name='us-east-1',\n            signature_version=UNSIGNED,\n        )\n        sso_stubber = session.stub('sso', config=client_config)\n        sso_stubber.activate()\n        # The expiration needs to be in milliseconds\n        expiration = datetime2timestamp(self.some_future_time()) * 1000\n        sso_role_creds = self.create_random_credentials()\n        sso_role_response = {\n            'roleCredentials': {\n                'accessKeyId': sso_role_creds.access_key,\n                'secretAccessKey': sso_role_creds.secret_key,\n                'sessionToken': sso_role_creds.token,\n                'expiration': int(expiration),\n            }\n        }\n        sso_stubber.add_response('get_role_credentials', sso_role_response)\n\n        expected_creds = self.create_random_credentials()\n        assume_role_response = self.create_assume_role_response(expected_creds)\n        sts_stubber.add_response('assume_role', assume_role_response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n        sts_stubber.assert_no_pending_responses()\n        # Assert that the client was created with the credentials from the\n        # SSO get role credentials response\n        self.assertEqual(self.mock_client_creator.call_count, 1)\n        _, kwargs = self.mock_client_creator.call_args_list[0]\n        expected_kwargs = {\n            'aws_access_key_id': sso_role_creds.access_key,\n            'aws_secret_access_key': sso_role_creds.secret_key,\n            'aws_session_token': sso_role_creds.token,\n        }\n        self.assertEqual(kwargs, expected_kwargs)\n\n    def test_web_identity_credential_source_ignores_env_vars(self):\n        token_path = os.path.join(self.tempdir, 'token')\n        with open(token_path, 'w') as token_file:\n            token_file.write('a.token')\n        self.environ['AWS_ROLE_ARN'] = 'arn:aws:iam::123456789:role/RoleB'\n        self.environ['AWS_WEB_IDENTITY_TOKEN_FILE'] = token_path\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'credential_source = Environment\\n'\n        )\n        self.write_config(config)\n\n        session, _ = self.create_session(profile='A')\n        # We should not get credentials from web-identity configured in the\n        # environment when the Environment credential_source is set.\n        # There are no Environment credentials, so this should raise a\n        # retrieval error.\n        with self.assertRaises(CredentialRetrievalError):\n            session.get_credentials()\n\n    def test_self_referential_profile(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = A\\n'\n            'aws_access_key_id = abc123\\n'\n            'aws_secret_access_key = def456\\n'\n        )\n        self.write_config(config)\n\n        expected_creds = self.create_random_credentials()\n        response = self.create_assume_role_response(expected_creds)\n        session, stubber = self.create_session(profile='A')\n        stubber.add_response('assume_role', response)\n\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n        stubber.assert_no_pending_responses()\n\n    def create_stubbed_sts_client(self, session):\n        expected_creds = self.create_random_credentials()\n        _original_create_client = session.create_client\n\n        def create_client_sts_stub(service, *args, **kwargs):\n            client = _original_create_client(service, *args, **kwargs)\n            stub = Stubber(client)\n            response = self.create_assume_role_response(expected_creds)\n            self.actual_client_region = client.meta.region_name\n            stub.add_response('assume_role', response)\n            stub.activate()\n            return client\n\n        return create_client_sts_stub, expected_creds\n\n    def test_assume_role_uses_correct_region(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n\\n'\n            '[profile B]\\n'\n            'aws_access_key_id = abc123\\n'\n            'aws_secret_access_key = def456\\n'\n        )\n        self.write_config(config)\n        session = Session(profile='A')\n        # Verify that when we configure the session with a specific region\n        # that we use that region when creating the sts client.\n        session.set_config_variable('region', 'cn-north-1')\n\n        create_client, expected_creds = self.create_stubbed_sts_client(session)\n        session.create_client = create_client\n\n        resolver = create_credential_resolver(session)\n        provider = resolver.get_provider('assume-role')\n        creds = provider.load()\n        self.assert_creds_equal(creds, expected_creds)\n        self.assertEqual(self.actual_client_region, 'cn-north-1')\n\n\nclass TestAssumeRoleWithWebIdentity(BaseAssumeRoleTest):\n    def setUp(self):\n        super().setUp()\n        self.token_file = os.path.join(self.tempdir, 'token.jwt')\n        self.write_token('totally.a.token')\n\n    def write_token(self, token, path=None):\n        if path is None:\n            path = self.token_file\n        with open(path, 'w') as f:\n            f.write(token)\n\n    def assert_session_credentials(self, expected_params, **kwargs):\n        expected_creds = self.create_random_credentials()\n        response = self.create_assume_role_response(expected_creds)\n        session = StubbedSession(**kwargs)\n        stubber = session.stub('sts')\n        stubber.add_response(\n            'assume_role_with_web_identity', response, expected_params\n        )\n        stubber.activate()\n        actual_creds = session.get_credentials()\n        self.assert_creds_equal(actual_creds, expected_creds)\n        stubber.assert_no_pending_responses()\n\n    def test_assume_role(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'role_session_name = sname\\n'\n            'web_identity_token_file = %s\\n'\n        ) % self.token_file\n        self.write_config(config)\n        expected_params = {\n            'RoleArn': 'arn:aws:iam::123456789:role/RoleA',\n            'RoleSessionName': 'sname',\n            'WebIdentityToken': 'totally.a.token',\n        }\n        self.assert_session_credentials(expected_params, profile='A')\n\n    def test_assume_role_env_vars(self):\n        config = '[profile B]\\n' 'region = us-west-2\\n'\n        self.write_config(config)\n        self.environ['AWS_ROLE_ARN'] = 'arn:aws:iam::123456789:role/RoleB'\n        self.environ['AWS_WEB_IDENTITY_TOKEN_FILE'] = self.token_file\n        self.environ['AWS_ROLE_SESSION_NAME'] = 'bname'\n\n        expected_params = {\n            'RoleArn': 'arn:aws:iam::123456789:role/RoleB',\n            'RoleSessionName': 'bname',\n            'WebIdentityToken': 'totally.a.token',\n        }\n        self.assert_session_credentials(expected_params)\n\n    def test_assume_role_env_vars_do_not_take_precedence(self):\n        config = (\n            '[profile A]\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'role_session_name = aname\\n'\n            'web_identity_token_file = %s\\n'\n        ) % self.token_file\n        self.write_config(config)\n\n        different_token = os.path.join(self.tempdir, str(uuid.uuid4()))\n        self.write_token('totally.different.token', path=different_token)\n        self.environ['AWS_ROLE_ARN'] = 'arn:aws:iam::123456789:role/RoleC'\n        self.environ['AWS_WEB_IDENTITY_TOKEN_FILE'] = different_token\n        self.environ['AWS_ROLE_SESSION_NAME'] = 'cname'\n\n        expected_params = {\n            'RoleArn': 'arn:aws:iam::123456789:role/RoleA',\n            'RoleSessionName': 'aname',\n            'WebIdentityToken': 'totally.a.token',\n        }\n        self.assert_session_credentials(expected_params, profile='A')\n\n\nclass TestProcessProvider(unittest.TestCase):\n    def setUp(self):\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        credential_process = os.path.join(\n            current_dir, 'utils', 'credentialprocess.py'\n        )\n        self.credential_process = '{} {}'.format(\n            sys.executable, credential_process\n        )\n        self.environ = os.environ.copy()\n        self.environ_patch = mock.patch('os.environ', self.environ)\n        self.environ_patch.start()\n\n    def tearDown(self):\n        self.environ_patch.stop()\n\n    def test_credential_process(self):\n        config = '[profile processcreds]\\n' 'credential_process = %s\\n'\n        config = config % self.credential_process\n        with temporary_file('w') as f:\n            f.write(config)\n            f.flush()\n            self.environ['AWS_CONFIG_FILE'] = f.name\n\n            credentials = Session(profile='processcreds').get_credentials()\n            self.assertEqual(credentials.access_key, 'spam')\n            self.assertEqual(credentials.secret_key, 'eggs')\n\n    def test_credential_process_returns_error(self):\n        config = (\n            '[profile processcreds]\\n'\n            'credential_process = %s --raise-error\\n'\n        )\n        config = config % self.credential_process\n        with temporary_file('w') as f:\n            f.write(config)\n            f.flush()\n            self.environ['AWS_CONFIG_FILE'] = f.name\n\n            session = Session(profile='processcreds')\n\n            # This regex validates that there is no substring: b'\n            # The reason why we want to validate that is that we want to\n            # make sure that stderr is actually decoded so that in\n            # exceptional cases the error is properly formatted.\n            # As for how the regex works:\n            # `(?!b').` is a negative lookahead, meaning that it will only\n            # match if it is not followed by the pattern `b'`. Since it is\n            # followed by a `.` it will match any character not followed by\n            # that pattern. `((?!hede).)*` does that zero or more times. The\n            # final pattern adds `^` and `$` to anchor the beginning and end\n            # of the string so we can know the whole string is consumed.\n            # Finally `(?s)` at the beginning makes dots match newlines so\n            # we can handle a multi-line string.\n            reg = r\"(?s)^((?!b').)*$\"\n            with self.assertRaisesRegex(CredentialRetrievalError, reg):\n                session.get_credentials()\n\n\nclass TestSTSRegional(BaseAssumeRoleTest):\n    def add_assume_role_http_response(self, stubber):\n        stubber.add_response(body=self._get_assume_role_body('AssumeRole'))\n\n    def add_assume_role_with_web_identity_http_response(self, stubber):\n        stubber.add_response(\n            body=self._get_assume_role_body('AssumeRoleWithWebIdentity')\n        )\n\n    def _get_assume_role_body(self, method_name):\n        expiration = self.some_future_time()\n        body = (\n            '<{method_name}Response>'\n            '  <{method_name}Result>'\n            '    <AssumedRoleUser>'\n            '      <Arn>arn:aws:sts::0123456:user</Arn>'\n            '      <AssumedRoleId>AKID:mysession-1567020004</AssumedRoleId>'\n            '    </AssumedRoleUser>'\n            '    <Credentials>'\n            '      <AccessKeyId>AccessKey</AccessKeyId>'\n            '      <SecretAccessKey>SecretKey</SecretAccessKey>'\n            '      <SessionToken>SessionToken</SessionToken>'\n            '      <Expiration>{expiration}</Expiration>'\n            '    </Credentials>'\n            '  </{method_name}Result>'\n            '</{method_name}Response>'\n        ).format(method_name=method_name, expiration=expiration)\n        return body.encode('utf-8')\n\n    def make_stubbed_client_call_to_region(self, session, stubber, region):\n        ec2 = session.create_client('ec2', region_name=region)\n        stubber.add_response(body=b'<DescribeRegionsResponse/>')\n        ec2.describe_regions()\n\n    def test_assume_role_uses_same_region_as_client(self):\n        config = (\n            '[profile A]\\n'\n            'sts_regional_endpoints = regional\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'source_profile = B\\n\\n'\n            '[profile B]\\n'\n            'aws_access_key_id = abc123\\n'\n            'aws_secret_access_key = def456\\n'\n        )\n        self.write_config(config)\n\n        session = Session(profile='A')\n        with SessionHTTPStubber(session) as stubber:\n            self.add_assume_role_http_response(stubber)\n            # Make an arbitrary client and API call as we are really only\n            # looking to make sure the STS assume role call uses the correct\n            # endpoint.\n            self.make_stubbed_client_call_to_region(\n                session, stubber, 'us-west-2'\n            )\n            self.assertEqual(\n                stubber.requests[0].url, 'https://sts.us-west-2.amazonaws.com/'\n            )\n\n    def test_assume_role_web_identity_uses_same_region_as_client(self):\n        token_file = os.path.join(self.tempdir, 'token.jwt')\n        with open(token_file, 'w') as f:\n            f.write('some-token')\n        config = (\n            '[profile A]\\n'\n            'sts_regional_endpoints = regional\\n'\n            'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n            'web_identity_token_file = %s\\n'\n            'source_profile = B\\n\\n'\n            '[profile B]\\n'\n            'aws_access_key_id = abc123\\n'\n            'aws_secret_access_key = def456\\n' % token_file\n        )\n        self.write_config(config)\n        # Make an arbitrary client and API call as we are really only\n        # looking to make sure the STS assume role call uses the correct\n        # endpoint.\n        session = Session(profile='A')\n        with SessionHTTPStubber(session) as stubber:\n            self.add_assume_role_with_web_identity_http_response(stubber)\n            # Make an arbitrary client and API call as we are really only\n            # looking to make sure the STS assume role call uses the correct\n            # endpoint.\n            self.make_stubbed_client_call_to_region(\n                session, stubber, 'us-west-2'\n            )\n            self.assertEqual(\n                stubber.requests[0].url, 'https://sts.us-west-2.amazonaws.com/'\n            )\n\n\nclass MockCache:\n    \"\"\"Mock for JSONFileCache to avoid touching files on disk\"\"\"\n\n    def __init__(self, working_dir=None, dumps_func=None):\n        self.working_dir = working_dir\n        self.dumps_func = dumps_func\n\n    def __contains__(self, cache_key):\n        return True\n\n    def __getitem__(self, cache_key):\n        return {\n            \"startUrl\": \"https://test.awsapps.com/start\",\n            \"region\": \"us-east-1\",\n            \"accessToken\": \"access-token\",\n            \"expiresAt\": TIME_IN_ONE_HOUR.strftime('%Y-%m-%dT%H:%M:%SZ'),\n            \"expiresIn\": 3600,\n            \"clientId\": \"client-12345\",\n            \"clientSecret\": \"client-secret\",\n            \"registrationExpiresAt\": TIME_IN_SIX_MONTHS.strftime(\n                '%Y-%m-%dT%H:%M:%SZ'\n            ),\n            \"refreshToken\": \"refresh-here\",\n        }\n\n    def __delitem__(self, cache_key):\n        pass\n\n\nclass SSOSessionTest(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.tempdir = tempfile.mkdtemp()\n        self.config_file = os.path.join(self.tempdir, 'config')\n        self.environ['AWS_CONFIG_FILE'] = self.config_file\n        self.access_key_id = 'ASIA123456ABCDEFG'\n        self.secret_access_key = 'secret-key'\n        self.session_token = 'session-token'\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n        super().tearDown()\n\n    def write_config(self, config):\n        with open(self.config_file, 'w') as f:\n            f.write(config)\n\n    def test_token_chosen_from_provider(self):\n        profile = (\n            '[profile sso-test]\\n'\n            'region = us-east-1\\n'\n            'sso_session = sso-test-session\\n'\n            'sso_account_id = 12345678901234\\n'\n            'sso_role_name = ViewOnlyAccess\\n'\n            '\\n'\n            '[sso-session sso-test-session]\\n'\n            'sso_region = us-east-1\\n'\n            'sso_start_url = https://test.awsapps.com/start\\n'\n            'sso_registration_scopes = sso:account:access\\n'\n        )\n        self.write_config(profile)\n\n        session = Session(profile='sso-test')\n        with SessionHTTPStubber(session) as stubber:\n            self.add_credential_response(stubber)\n            stubber.add_response()\n            with mock.patch.object(\n                SSOTokenProvider, 'DEFAULT_CACHE_CLS', MockCache\n            ):\n                c = session.create_client('s3')\n                c.list_buckets()\n\n        self.assert_valid_sso_call(\n            stubber.requests[0],\n            (\n                'https://portal.sso.us-east-1.amazonaws.com/federation/credentials'\n                '?role_name=ViewOnlyAccess&account_id=12345678901234'\n            ),\n            b'access-token',\n        )\n        self.assert_credentials_used(\n            stubber.requests[1],\n            self.access_key_id.encode('utf-8'),\n            self.session_token.encode('utf-8'),\n        )\n\n    def test_mismatched_session_values(self):\n        profile = (\n            '[profile sso-test]\\n'\n            'region = us-east-1\\n'\n            'sso_session = sso-test-session\\n'\n            'sso_start_url = https://test2.awsapps.com/start\\n'\n            'sso_account_id = 12345678901234\\n'\n            'sso_role_name = ViewOnlyAccess\\n'\n            '\\n'\n            '[sso-session sso-test-session]\\n'\n            'sso_region = us-east-1\\n'\n            'sso_start_url = https://test.awsapps.com/start\\n'\n            'sso_registration_scopes = sso:account:access\\n'\n        )\n        self.write_config(profile)\n\n        session = Session(profile='sso-test')\n        with pytest.raises(InvalidConfigError):\n            c = session.create_client('s3')\n            c.list_buckets()\n\n    def test_missing_sso_session(self):\n        profile = (\n            '[profile sso-test]\\n'\n            'region = us-east-1\\n'\n            'sso_session = sso-test-session\\n'\n            'sso_start_url = https://test2.awsapps.com/start\\n'\n            'sso_account_id = 12345678901234\\n'\n            'sso_role_name = ViewOnlyAccess\\n'\n            '\\n'\n        )\n        self.write_config(profile)\n\n        session = Session(profile='sso-test')\n        with pytest.raises(InvalidConfigError):\n            c = session.create_client('s3')\n            c.list_buckets()\n\n    def assert_valid_sso_call(self, request, url, access_token):\n        assert request.url == url\n        assert 'x-amz-sso_bearer_token' in request.headers\n        assert request.headers['x-amz-sso_bearer_token'] == access_token\n\n    def assert_credentials_used(self, request, access_key, session_token):\n        assert access_key in request.headers.get('Authorization')\n        assert request.headers.get('X-Amz-Security-Token') == session_token\n\n    def add_credential_response(self, stubber):\n        response = {\n            'roleCredentials': {\n                'accessKeyId': self.access_key_id,\n                'secretAccessKey': self.secret_access_key,\n                'sessionToken': self.session_token,\n                'expiration': TIME_IN_ONE_HOUR.timestamp() * 1000,\n            }\n        }\n        stubber.add_response(body=json.dumps(response).encode('utf-8'))\n\n\nclass TestContextCredentials(unittest.TestCase):\n    ACCESS_KEY = \"access-key\"\n    SECRET_KEY = \"secret-key\"\n\n    def _add_fake_creds(self, request, **kwargs):\n        request.context.setdefault('signing', {})\n        request.context['signing']['request_credentials'] = Credentials(\n            self.ACCESS_KEY, self.SECRET_KEY\n        )\n\n    def test_credential_context_override(self):\n        session = StubbedSession()\n        with SessionHTTPStubber(session) as stubber:\n            s3 = session.create_client('s3')\n            s3.meta.events.register('before-sign', self._add_fake_creds)\n            stubber.add_response()\n            s3.list_buckets()\n            request = stubber.requests[0]\n            assert self.ACCESS_KEY in str(request.headers.get('Authorization'))\n", "tests/functional/test_dynamodb.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.compat import json\nfrom botocore.config import Config\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestDynamoDBEndpointDiscovery(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.config = Config(endpoint_discovery_enabled=True)\n        self.create_client()\n\n    def create_client(self):\n        self.client = self.session.create_client(\n            'dynamodb', self.region, config=self.config\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n    def test_dynamodb_endpoint_discovery_enabled(self):\n        discovered_endpoint = 'https://discovered.domain'\n        response = {\n            'Endpoints': [\n                {\n                    'Address': discovered_endpoint,\n                    'CachePeriodInMinutes': 1,\n                }\n            ]\n        }\n        response_body = json.dumps(response).encode()\n        with self.http_stubber as stubber:\n            stubber.add_response(status=200, body=response_body)\n            stubber.add_response(status=200, body=b'{}')\n            self.client.describe_table(TableName='sometable')\n            self.assertEqual(len(self.http_stubber.requests), 2)\n            discover_request = self.http_stubber.requests[1]\n            self.assertEqual(discover_request.url, discovered_endpoint)\n\n    def test_dynamodb_endpoint_discovery_disabled(self):\n        self.config = Config(endpoint_discovery_enabled=False)\n        self.create_client()\n        with self.http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            self.client.describe_table(TableName='sometable')\n            self.assertEqual(len(self.http_stubber.requests), 1)\n\n    def test_dynamodb_endpoint_discovery_no_config_default(self):\n        self.config = None\n        self.create_client()\n        with self.http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            self.client.describe_table(TableName='sometable')\n            self.assertEqual(len(self.http_stubber.requests), 1)\n", "tests/functional/test_history.py": "from botocore.history import BaseHistoryHandler, get_global_history_recorder\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass RecordingHandler(BaseHistoryHandler):\n    def __init__(self):\n        self.recorded_calls = []\n\n    def emit(self, event_type, payload, source):\n        self.recorded_calls.append((event_type, payload, source))\n\n\nclass TestRecordStatementsInjections(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client('s3', 'us-west-2')\n        self.http_stubber = ClientHTTPStubber(self.client)\n        self.s3_response_body = (\n            b'<ListAllMyBucketsResult '\n            b'    xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">'\n            b'  <Owner>'\n            b'    <ID>d41d8cd98f00b204e9800998ecf8427e</ID>'\n            b'    <DisplayName>foo</DisplayName>'\n            b'  </Owner>'\n            b'  <Buckets>'\n            b'    <Bucket>'\n            b'      <Name>bar</Name>'\n            b'      <CreationDate>1912-06-23T22:57:02.000Z</CreationDate>'\n            b'    </Bucket>'\n            b'  </Buckets>'\n            b'</ListAllMyBucketsResult>'\n        )\n        self.recording_handler = RecordingHandler()\n        history_recorder = get_global_history_recorder()\n        history_recorder.enable()\n        history_recorder.add_handler(self.recording_handler)\n\n    def _get_all_events_of_type(self, event_type):\n        recorded_calls = self.recording_handler.recorded_calls\n        matching = [call for call in recorded_calls if call[0] == event_type]\n        return matching\n\n    def test_does_record_api_call(self):\n        self.http_stubber.add_response(body=self.s3_response_body)\n        with self.http_stubber:\n            self.client.list_buckets()\n\n        api_call_events = self._get_all_events_of_type('API_CALL')\n        self.assertEqual(len(api_call_events), 1)\n        event = api_call_events[0]\n        event_type, payload, source = event\n        self.assertEqual(\n            payload,\n            {'operation': 'ListBuckets', 'params': {}, 'service': 's3'},\n        )\n        self.assertEqual(source, 'BOTOCORE')\n\n    def test_does_record_http_request(self):\n        self.http_stubber.add_response(body=self.s3_response_body)\n        with self.http_stubber:\n            self.client.list_buckets()\n\n        http_request_events = self._get_all_events_of_type('HTTP_REQUEST')\n        self.assertEqual(len(http_request_events), 1)\n        event = http_request_events[0]\n        event_type, payload, source = event\n\n        method = payload['method']\n        self.assertEqual(method, 'GET')\n\n        # The header values vary too much per request to verify them here.\n        # Instead just check the presense of each expected header.\n        headers = payload['headers']\n        for expected_header in [\n            'Authorization',\n            'User-Agent',\n            'X-Amz-Date',\n            'X-Amz-Content-SHA256',\n        ]:\n            self.assertIn(expected_header, headers)\n\n        body = payload['body']\n        self.assertIsNone(body)\n\n        streaming = payload['streaming']\n        self.assertEqual(streaming, False)\n\n        url = payload['url']\n        self.assertEqual(url, 'https://s3.us-west-2.amazonaws.com/')\n\n        self.assertEqual(source, 'BOTOCORE')\n\n    def test_does_record_http_response(self):\n        self.http_stubber.add_response(body=self.s3_response_body)\n        with self.http_stubber:\n            self.client.list_buckets()\n\n        http_response_events = self._get_all_events_of_type('HTTP_RESPONSE')\n        self.assertEqual(len(http_response_events), 1)\n        event = http_response_events[0]\n        event_type, payload, source = event\n\n        self.assertEqual(\n            payload,\n            {\n                'status_code': 200,\n                'headers': {},\n                'streaming': False,\n                'body': self.s3_response_body,\n                'context': {'operation_name': 'ListBuckets'},\n            },\n        )\n        self.assertEqual(source, 'BOTOCORE')\n\n    def test_does_record_parsed_response(self):\n        self.http_stubber.add_response(body=self.s3_response_body)\n        with self.http_stubber:\n            self.client.list_buckets()\n\n        parsed_response_events = self._get_all_events_of_type(\n            'PARSED_RESPONSE'\n        )\n        self.assertEqual(len(parsed_response_events), 1)\n        event = parsed_response_events[0]\n        event_type, payload, source = event\n\n        # Given that the request contains headers with a user agent string\n        # a date and a signature we need to disassemble the call and manually\n        # assert the interesting bits since mock can only assert if the args\n        # all match exactly.\n        owner = payload['Owner']\n        self.assertEqual(\n            owner,\n            {'DisplayName': 'foo', 'ID': 'd41d8cd98f00b204e9800998ecf8427e'},\n        )\n\n        buckets = payload['Buckets']\n        self.assertEqual(len(buckets), 1)\n        bucket = buckets[0]\n        self.assertEqual(bucket['Name'], 'bar')\n\n        metadata = payload['ResponseMetadata']\n        self.assertEqual(\n            metadata,\n            {'HTTPHeaders': {}, 'HTTPStatusCode': 200, 'RetryAttempts': 0},\n        )\n", "tests/functional/test_iot_data.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.exceptions import UnsupportedTLSVersionWarning\nfrom tests import BaseSessionTest, mock\n\n\nclass TestOpensslVersion(BaseSessionTest):\n    def test_incompatible_openssl_version(self):\n        with mock.patch('ssl.OPENSSL_VERSION_INFO', new=(0, 9, 8, 11, 15)):\n            with mock.patch('warnings.warn') as mock_warn:\n                self.session.create_client('iot-data', 'us-east-1')\n                call_args = mock_warn.call_args[0]\n                warning_message = call_args[0]\n                warning_type = call_args[1]\n                # We should say something specific about the service.\n                self.assertIn('iot-data', warning_message)\n                self.assertEqual(warning_type, UnsupportedTLSVersionWarning)\n\n    def test_compatible_openssl_version(self):\n        with mock.patch('ssl.OPENSSL_VERSION_INFO', new=(1, 0, 1, 1, 1)):\n            with mock.patch('warnings.warn') as mock_warn:\n                self.session.create_client('iot-data', 'us-east-1')\n                self.assertFalse(mock_warn.called)\n", "tests/functional/test_sts.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport re\nfrom datetime import datetime\n\nfrom botocore.config import Config\nfrom botocore.stub import Stubber\nfrom tests import (\n    BaseSessionTest,\n    ClientHTTPStubber,\n    assert_url_equal,\n    mock,\n    temporary_file,\n)\n\n_V4_SIGNING_REGION_REGEX = re.compile(\n    r'AWS4-HMAC-SHA256 ' r'Credential=\\w+/\\d+/(?P<signing_region>[a-z0-9-]+)/'\n)\n\n\nclass TestSTSPresignedUrl(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client('sts', 'us-west-2')\n        # Makes sure that no requests will go through\n        self.stubber = Stubber(self.client)\n        self.stubber.activate()\n\n    def test_presigned_url_contains_no_content_type(self):\n        timestamp = datetime(2017, 3, 22, 0, 0)\n        with mock.patch('botocore.auth.datetime.datetime') as _datetime:\n            _datetime.utcnow.return_value = timestamp\n            url = self.client.generate_presigned_url('get_caller_identity', {})\n\n        # There should be no 'content-type' in x-amz-signedheaders\n        expected_url = (\n            'https://sts.amazonaws.com/?Action=GetCallerIdentity&'\n            'Version=2011-06-15&X-Amz-Algorithm=AWS4-HMAC-SHA256&'\n            'X-Amz-Credential=access_key%2F20170322%2Fus-east-1%2Fsts%2F'\n            'aws4_request&X-Amz-Date=20170322T000000Z&X-Amz-Expires=3600&'\n            'X-Amz-SignedHeaders=host&X-Amz-Signature=767845d2ee858069a598d5f'\n            '8b497b75c7d57356885b1b3dba46dbbc0fc62bf5a'\n        )\n        assert_url_equal(url, expected_url)\n\n\nclass TestSTSEndpoints(BaseSessionTest):\n    def create_sts_client(\n        self, region, endpoint_url=None, use_ssl=True, config=None\n    ):\n        return self.session.create_client(\n            'sts',\n            region_name=region,\n            endpoint_url=endpoint_url,\n            use_ssl=use_ssl,\n            config=config,\n        )\n\n    def set_sts_regional_for_config_file(self, fileobj, config_val):\n        fileobj.write('[default]\\n' 'sts_regional_endpoints=%s\\n' % config_val)\n        fileobj.flush()\n        self.environ['AWS_CONFIG_FILE'] = fileobj.name\n\n    def assert_request_sent(\n        self, sts, expected_url, expected_signing_region=None\n    ):\n        body = (\n            b'<GetCallerIdentityResponse>'\n            b'  <GetCallerIdentityResult>'\n            b'     <Arn>arn:aws:iam::123456789012:user/myuser</Arn>'\n            b'     <UserId>UserID</UserId>'\n            b'     <Account>123456789012</Account>'\n            b'   </GetCallerIdentityResult>'\n            b'   <ResponseMetadata>'\n            b'     <RequestId>some-request</RequestId>'\n            b'   </ResponseMetadata>'\n            b'</GetCallerIdentityResponse>'\n        )\n        with ClientHTTPStubber(sts) as http_stubber:\n            http_stubber.add_response(body=body)\n            sts.get_caller_identity()\n            captured_request = http_stubber.requests[0]\n            self.assertEqual(captured_request.url, expected_url)\n            if expected_signing_region:\n                self.assertEqual(\n                    self._get_signing_region(captured_request),\n                    expected_signing_region,\n                )\n\n    def _get_signing_region(self, request):\n        authorization_val = request.headers['Authorization'].decode('utf-8')\n        match = _V4_SIGNING_REGION_REGEX.match(authorization_val)\n        return match.group('signing_region')\n\n    def test_legacy_region_with_legacy_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        sts = self.create_sts_client('us-west-2')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.amazonaws.com/',\n            expected_signing_region='us-east-1',\n        )\n\n    def test_legacy_region_with_regional_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'regional'\n        sts = self.create_sts_client('us-west-2')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.us-west-2.amazonaws.com/',\n            expected_signing_region='us-west-2',\n        )\n\n    def test_fips_endpoint_with_legacy_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        sts = self.create_sts_client('us-west-2-fips')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts-fips.us-west-2.amazonaws.com/',\n            expected_signing_region='us-west-2',\n        )\n\n    def test_fips_endpoint_with_regional_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'regional'\n        sts = self.create_sts_client('us-west-2-fips')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts-fips.us-west-2.amazonaws.com/',\n            expected_signing_region='us-west-2',\n        )\n\n    def test_dualstack_endpoint_with_legacy_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        dualstack_config = Config(use_dualstack_endpoint=True)\n        sts = self.create_sts_client('us-west-2', config=dualstack_config)\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.us-west-2.api.aws/',\n            expected_signing_region='us-west-2',\n        )\n\n    def test_dualstack_endpoint_with_regional_configured(self):\n        dualstack_config = Config(use_dualstack_endpoint=True)\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'regional'\n        sts = self.create_sts_client('us-west-2', config=dualstack_config)\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.us-west-2.api.aws/',\n            expected_signing_region='us-west-2',\n        )\n\n    def test_nonlegacy_region_with_legacy_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        sts = self.create_sts_client('ap-east-1')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.ap-east-1.amazonaws.com/',\n            expected_signing_region='ap-east-1',\n        )\n\n    def test_nonlegacy_region_with_regional_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'regional'\n        sts = self.create_sts_client('ap-east-1')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.ap-east-1.amazonaws.com/',\n            expected_signing_region='ap-east-1',\n        )\n\n    def test_nonaws_partition_region_with_legacy_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        sts = self.create_sts_client('cn-north-1')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.cn-north-1.amazonaws.com.cn/',\n            expected_signing_region='cn-north-1',\n        )\n\n    def test_nonaws_partition_region_with_regional_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'regional'\n        sts = self.create_sts_client('cn-north-1')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.cn-north-1.amazonaws.com.cn/',\n            expected_signing_region='cn-north-1',\n        )\n\n    def test_global_region_with_legacy_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        sts = self.create_sts_client('aws-global')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.amazonaws.com/',\n            expected_signing_region='us-east-1',\n        )\n\n    def test_global_region_with_regional_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'regional'\n        sts = self.create_sts_client('aws-global')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.amazonaws.com/',\n            expected_signing_region='us-east-1',\n        )\n\n    def test_defaults_to_global_endpoint_for_legacy_region(self):\n        sts = self.create_sts_client('us-west-2')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.amazonaws.com/',\n            expected_signing_region='us-east-1',\n        )\n\n    def test_defaults_to_regional_endpoint_for_nonlegacy_region(self):\n        sts = self.create_sts_client('ap-east-1')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.ap-east-1.amazonaws.com/',\n            expected_signing_region='ap-east-1',\n        )\n\n    def test_configure_sts_regional_from_config_file(self):\n        with temporary_file('w') as f:\n            self.set_sts_regional_for_config_file(f, 'regional')\n            sts = self.create_sts_client('us-west-2')\n            self.assert_request_sent(\n                sts,\n                expected_url='https://sts.us-west-2.amazonaws.com/',\n            )\n\n    def test_env_var_overrides_config_file(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        with temporary_file('w') as f:\n            self.set_sts_regional_for_config_file(f, 'regional')\n            sts = self.create_sts_client('us-west-2')\n            self.assert_request_sent(\n                sts, expected_url='https://sts.amazonaws.com/'\n            )\n\n    def test_user_provided_endpoint_with_legacy_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        sts = self.create_sts_client(\n            'us-west-2', endpoint_url='https://custom.com'\n        )\n        self.assert_request_sent(sts, expected_url='https://custom.com/')\n\n    def test_user_provided_endpoint_with_regional_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'regional'\n        sts = self.create_sts_client(\n            'us-west-2', endpoint_url='https://custom.com'\n        )\n        self.assert_request_sent(sts, expected_url='https://custom.com/')\n\n    def test_http_with_legacy_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        sts = self.create_sts_client('us-west-2', use_ssl=False)\n        self.assert_request_sent(sts, expected_url='http://sts.amazonaws.com/')\n\n    def test_client_for_unknown_region(self):\n        sts = self.create_sts_client('not-real')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.not-real.amazonaws.com/',\n            expected_signing_region='not-real',\n        )\n\n    def test_client_for_unknown_region_with_legacy_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'legacy'\n        sts = self.create_sts_client('not-real')\n        self.assert_request_sent(\n            sts, expected_url='https://sts.not-real.amazonaws.com/'\n        )\n\n    def test_client_for_unknown_region_with_regional_configured(self):\n        self.environ['AWS_STS_REGIONAL_ENDPOINTS'] = 'regional'\n        sts = self.create_sts_client('not-real')\n        self.assert_request_sent(\n            sts,\n            expected_url='https://sts.not-real.amazonaws.com/',\n            expected_signing_region='not-real',\n        )\n\n\ndef test_assume_role_with_saml_no_region_custom_endpoint(patched_session):\n    # When an endpoint_url and no region are given, AssumeRoleWithSAML should\n    # resolve to the endpoint_url and succeed, not fail in endpoint resolution:\n    # https://github.com/aws/aws-cli/issues/7455\n\n    client = patched_session.create_client(\n        'sts', region_name=None, endpoint_url=\"https://custom.endpoint.aws\"\n    )\n    assert client.meta.region_name is None\n\n    mock_response_body = b\"\"\"\\\n<AssumeRoleWithSAMLResponse xmlns=\"https://sts.amazonaws.com/doc/2011-06-15/\">\n    <AssumeRoleWithSAMLResult></AssumeRoleWithSAMLResult>\n</AssumeRoleWithSAMLResponse>\n\"\"\"\n    with ClientHTTPStubber(client) as http_stubber:\n        http_stubber.add_response(body=mock_response_body)\n        client.assume_role_with_saml(\n            RoleArn='arn:aws:iam::123456789:role/RoleA',\n            PrincipalArn='arn:aws:iam::123456789:role/RoleB',\n            SAMLAssertion='xxxx',\n        )\n    captured_request = http_stubber.requests[0]\n    assert captured_request.url == \"https://custom.endpoint.aws/\"\n", "tests/functional/test_stub.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore\nimport botocore.client\nimport botocore.config\nimport botocore.retryhandler\nimport botocore.session\nimport botocore.stub as stub\nimport botocore.translate\nfrom botocore.exceptions import (\n    ClientError,\n    ParamValidationError,\n    StubAssertionError,\n    StubResponseError,\n    UnStubbedResponseError,\n)\nfrom botocore.stub import Stubber\nfrom tests import unittest\n\n\nclass TestStubber(unittest.TestCase):\n    def setUp(self):\n        session = botocore.session.get_session()\n        config = botocore.config.Config(\n            signature_version=botocore.UNSIGNED,\n            s3={'addressing_style': 'path'},\n        )\n        self.client = session.create_client(\n            's3', region_name='us-east-1', config=config\n        )\n        self.stubber = Stubber(self.client)\n\n    def test_stubber_returns_response(self):\n        service_response = {'ResponseMetadata': {'foo': 'bar'}}\n        self.stubber.add_response('list_objects', service_response)\n        self.stubber.activate()\n        response = self.client.list_objects(Bucket='foo')\n        self.assertEqual(response, service_response)\n\n    def test_context_manager_returns_response(self):\n        service_response = {'ResponseMetadata': {'foo': 'bar'}}\n        self.stubber.add_response('list_objects', service_response)\n\n        with self.stubber:\n            response = self.client.list_objects(Bucket='foo')\n        self.assertEqual(response, service_response)\n\n    def test_activated_stubber_errors_with_no_registered_stubs(self):\n        self.stubber.activate()\n        # Params one per line for readability.\n        with self.assertRaisesRegex(\n            UnStubbedResponseError, \"Unexpected API Call\"\n        ):\n            self.client.list_objects(\n                Bucket='asdfasdfasdfasdf',\n                Delimiter='asdfasdfasdfasdf',\n                Prefix='asdfasdfasdfasdf',\n                EncodingType='url',\n            )\n\n    def test_stubber_errors_when_stubs_are_used_up(self):\n        self.stubber.add_response('list_objects', {})\n        self.stubber.activate()\n        self.client.list_objects(Bucket='foo')\n\n        with self.assertRaises(UnStubbedResponseError):\n            self.client.list_objects(Bucket='foo')\n\n    def test_client_error_response(self):\n        error_code = \"AccessDenied\"\n        error_message = \"Access Denied\"\n        self.stubber.add_client_error(\n            'list_objects', error_code, error_message\n        )\n        self.stubber.activate()\n\n        with self.assertRaises(ClientError):\n            self.client.list_objects(Bucket='foo')\n\n    def test_modeled_client_error_response(self):\n        error_code = \"InvalidObjectState\"\n        error_message = \"Object is in invalid state\"\n        modeled_fields = {\n            'StorageClass': 'foo',\n            'AccessTier': 'bar',\n        }\n        self.stubber.add_client_error(\n            'get_object',\n            error_code,\n            error_message,\n            modeled_fields=modeled_fields,\n        )\n        self.stubber.activate()\n\n        actual_exception = None\n        try:\n            self.client.get_object(Bucket='foo', Key='bar')\n        except self.client.exceptions.InvalidObjectState as e:\n            actual_exception = e\n        self.assertIsNotNone(actual_exception)\n        response = actual_exception.response\n        self.assertEqual(response['StorageClass'], 'foo')\n        self.assertEqual(response['AccessTier'], 'bar')\n\n    def test_modeled_client_error_response_validation_error(self):\n        error_code = \"InvalidObjectState\"\n        error_message = \"Object is in invalid state\"\n        modeled_fields = {\n            'BadField': 'fail please',\n        }\n        with self.assertRaises(ParamValidationError):\n            self.stubber.add_client_error(\n                'get_object',\n                error_code,\n                error_message,\n                modeled_fields=modeled_fields,\n            )\n\n    def test_modeled_client_unknown_code_validation_error(self):\n        error_code = \"NotARealError\"\n        error_message = \"Message\"\n        modeled_fields = {\n            'BadField': 'fail please',\n        }\n        with self.assertRaises(ParamValidationError):\n            self.stubber.add_client_error(\n                'get_object',\n                error_code,\n                error_message,\n                modeled_fields=modeled_fields,\n            )\n\n    def test_can_add_expected_params_to_client_error(self):\n        self.stubber.add_client_error(\n            'list_objects', 'Error', 'error', expected_params={'Bucket': 'foo'}\n        )\n        self.stubber.activate()\n        with self.assertRaises(ClientError):\n            self.client.list_objects(Bucket='foo')\n\n    def test_can_expected_param_fails_in_client_error(self):\n        self.stubber.add_client_error(\n            'list_objects', 'Error', 'error', expected_params={'Bucket': 'foo'}\n        )\n        self.stubber.activate()\n        # We expect an AssertionError instead of a ClientError\n        # because we're calling the operation with the wrong\n        # param value.\n        with self.assertRaises(AssertionError):\n            self.client.list_objects(Bucket='wrong-argument-value')\n\n    def test_expected_params_success(self):\n        service_response = {}\n        expected_params = {'Bucket': 'foo'}\n        self.stubber.add_response(\n            'list_objects', service_response, expected_params\n        )\n        self.stubber.activate()\n        # This should be called successfully with no errors being thrown\n        # for mismatching expected params.\n        response = self.client.list_objects(Bucket='foo')\n        self.assertEqual(response, service_response)\n\n    def test_expected_params_fail(self):\n        service_response = {}\n        expected_params = {'Bucket': 'bar'}\n        self.stubber.add_response(\n            'list_objects', service_response, expected_params\n        )\n        self.stubber.activate()\n        # This should call should raise an for mismatching expected params.\n        with self.assertRaisesRegex(StubResponseError, \"{'Bucket': 'bar'},\\n\"):\n            self.client.list_objects(Bucket='foo')\n\n    def test_expected_params_mixed_with_errors_responses(self):\n        # Add an error response\n        error_code = \"AccessDenied\"\n        error_message = \"Access Denied\"\n        self.stubber.add_client_error(\n            'list_objects', error_code, error_message\n        )\n\n        # Add a response with incorrect expected params\n        service_response = {}\n        expected_params = {'Bucket': 'bar'}\n        self.stubber.add_response(\n            'list_objects', service_response, expected_params\n        )\n\n        self.stubber.activate()\n\n        # The first call should throw and error as expected.\n        with self.assertRaises(ClientError):\n            self.client.list_objects(Bucket='foo')\n\n        # The second call should throw an error for unexpected parameters\n        with self.assertRaisesRegex(StubResponseError, 'Expected parameters'):\n            self.client.list_objects(Bucket='foo')\n\n    def test_can_continue_to_call_after_expected_params_fail(self):\n        service_response = {}\n        expected_params = {'Bucket': 'bar'}\n\n        self.stubber.add_response(\n            'list_objects', service_response, expected_params\n        )\n\n        self.stubber.activate()\n        # Throw an error for unexpected parameters\n        with self.assertRaises(StubResponseError):\n            self.client.list_objects(Bucket='foo')\n\n        # The stubber should still have the responses queued up\n        # even though the original parameters did not match the expected ones.\n        self.client.list_objects(Bucket='bar')\n        self.stubber.assert_no_pending_responses()\n\n    def test_still_relies_on_param_validation_with_expected_params(self):\n        service_response = {}\n        expected_params = {'Buck': 'bar'}\n\n        self.stubber.add_response(\n            'list_objects', service_response, expected_params\n        )\n\n        self.stubber.activate()\n        # Throw an error for invalid parameters\n        with self.assertRaises(ParamValidationError):\n            self.client.list_objects(Buck='bar')\n\n    def test_any_ignores_param_for_validation(self):\n        service_response = {}\n        expected_params = {'Bucket': stub.ANY}\n\n        self.stubber.add_response(\n            'list_objects', service_response, expected_params\n        )\n        self.stubber.add_response(\n            'list_objects', service_response, expected_params\n        )\n\n        try:\n            with self.stubber:\n                self.client.list_objects(Bucket='foo')\n                self.client.list_objects(Bucket='bar')\n        except StubAssertionError:\n            self.fail(\"stub.ANY failed to ignore parameter for validation.\")\n\n    def test_mixed_any_and_concrete_params(self):\n        service_response = {}\n        expected_params = {'Bucket': stub.ANY, 'Key': 'foo.txt'}\n\n        self.stubber.add_response(\n            'head_object', service_response, expected_params\n        )\n        self.stubber.add_response(\n            'head_object', service_response, expected_params\n        )\n\n        try:\n            with self.stubber:\n                self.client.head_object(Bucket='foo', Key='foo.txt')\n                self.client.head_object(Bucket='bar', Key='foo.txt')\n        except StubAssertionError:\n            self.fail(\"stub.ANY failed to ignore parameter for validation.\")\n\n    def test_nested_any_param(self):\n        service_response = {}\n        expected_params = {\n            'Bucket': 'foo',\n            'Key': 'bar.txt',\n            'Metadata': {\n                'MyMeta': stub.ANY,\n            },\n        }\n\n        self.stubber.add_response(\n            'put_object', service_response, expected_params\n        )\n        self.stubber.add_response(\n            'put_object', service_response, expected_params\n        )\n\n        try:\n            with self.stubber:\n                self.client.put_object(\n                    Bucket='foo',\n                    Key='bar.txt',\n                    Metadata={\n                        'MyMeta': 'Foo',\n                    },\n                )\n                self.client.put_object(\n                    Bucket='foo',\n                    Key='bar.txt',\n                    Metadata={\n                        'MyMeta': 'Bar',\n                    },\n                )\n        except StubAssertionError:\n            self.fail(\n                \"stub.ANY failed to ignore nested parameter for validation.\"\n            )\n\n    def test_ANY_repr(self):\n        self.assertEqual(repr(stub.ANY), '<ANY>')\n\n    def test_none_param(self):\n        service_response = {}\n        expected_params = {'Buck': None}\n\n        self.stubber.add_response(\n            'list_objects', service_response, expected_params\n        )\n\n        self.stubber.activate()\n        # Throw an error for invalid parameters\n        with self.assertRaises(StubAssertionError):\n            self.client.list_objects(Buck='bar')\n\n    def test_many_expected_params(self):\n        service_response = {}\n        expected_params = {\n            'Bucket': 'mybucket',\n            'Prefix': 'myprefix',\n            'Delimiter': '/',\n            'EncodingType': 'url',\n        }\n        self.stubber.add_response(\n            'list_objects', service_response, expected_params\n        )\n        try:\n            with self.stubber:\n                self.client.list_objects(**expected_params)\n        except StubAssertionError:\n            self.fail(\n                \"Stubber inappropriately raised error for same parameters.\"\n            )\n\n    def test_no_stub_for_presign_url(self):\n        try:\n            with self.stubber:\n                url = self.client.generate_presigned_url(\n                    ClientMethod='get_object',\n                    Params={'Bucket': 'mybucket', 'Key': 'mykey'},\n                )\n                self.assertEqual(\n                    url, 'https://s3.amazonaws.com/mybucket/mykey'\n                )\n        except StubResponseError:\n            self.fail(\n                'Stubbed responses should not be required for generating '\n                'presigned requests'\n            )\n\n    def test_can_stub_with_presign_url_mixed_in(self):\n        desired_response = {}\n        expected_params = {\n            'Bucket': 'mybucket',\n            'Prefix': 'myprefix',\n        }\n        self.stubber.add_response(\n            'list_objects', desired_response, expected_params\n        )\n        with self.stubber:\n            url = self.client.generate_presigned_url(\n                ClientMethod='get_object',\n                Params={'Bucket': 'myotherbucket', 'Key': 'myotherkey'},\n            )\n            self.assertEqual(\n                url, 'https://s3.amazonaws.com/myotherbucket/myotherkey'\n            )\n            actual_response = self.client.list_objects(**expected_params)\n            self.assertEqual(desired_response, actual_response)\n        self.stubber.assert_no_pending_responses()\n\n    def test_parse_get_bucket_location(self):\n        error_code = \"NoSuchBucket\"\n        error_message = \"The specified bucket does not exist\"\n        self.stubber.add_client_error(\n            'get_bucket_location', error_code, error_message\n        )\n        self.stubber.activate()\n\n        with self.assertRaises(ClientError):\n            self.client.get_bucket_location(Bucket='foo')\n\n    def test_parse_get_bucket_location_returns_response(self):\n        service_response = {\"LocationConstraint\": \"us-west-2\"}\n        self.stubber.add_response('get_bucket_location', service_response)\n        self.stubber.activate()\n        response = self.client.get_bucket_location(Bucket='foo')\n        self.assertEqual(response, service_response)\n", "tests/functional/test_ec2.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\n\nimport botocore.session\nfrom botocore.compat import parse_qs, urlparse\nfrom botocore.stub import ANY, Stubber\nfrom tests import BaseSessionTest, ClientHTTPStubber, mock, unittest\n\n\nclass TestIdempotencyToken(unittest.TestCase):\n    def setUp(self):\n        self.function_name = 'purchase_scheduled_instances'\n        self.region = 'us-west-2'\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('ec2', self.region)\n        self.stubber = Stubber(self.client)\n        self.service_response = {}\n        self.params_seen = []\n\n        # Record all the parameters that get seen\n        self.client.meta.events.register_first(\n            'before-call.*.*',\n            self.collect_params,\n            unique_id='TestIdempotencyToken',\n        )\n\n    def collect_params(self, model, params, *args, **kwargs):\n        self.params_seen.extend(params['body'].keys())\n\n    def test_provided_idempotency_token(self):\n        expected_params = {\n            'PurchaseRequests': [\n                {'PurchaseToken': 'foo', 'InstanceCount': 123}\n            ],\n            'ClientToken': ANY,\n        }\n        self.stubber.add_response(\n            self.function_name, self.service_response, expected_params\n        )\n\n        with self.stubber:\n            self.client.purchase_scheduled_instances(\n                PurchaseRequests=[\n                    {'PurchaseToken': 'foo', 'InstanceCount': 123}\n                ],\n                ClientToken='foobar',\n            )\n            self.assertIn('ClientToken', self.params_seen)\n\n    def test_insert_idempotency_token(self):\n        expected_params = {\n            'PurchaseRequests': [\n                {'PurchaseToken': 'foo', 'InstanceCount': 123}\n            ],\n        }\n\n        self.stubber.add_response(\n            self.function_name, self.service_response, expected_params\n        )\n\n        with self.stubber:\n            self.client.purchase_scheduled_instances(\n                PurchaseRequests=[\n                    {'PurchaseToken': 'foo', 'InstanceCount': 123}\n                ]\n            )\n            self.assertIn('ClientToken', self.params_seen)\n\n\nclass TestCopySnapshotCustomization(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('ec2', 'us-east-1')\n        self.http_stubber = ClientHTTPStubber(self.client)\n        self.snapshot_id = 'snap-0123abc'\n        self.copy_response = (\n            '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n            '<CopySnapshotResponse>\\n'\n            '<snapshotId>%s</snapshotId>\\n'\n            '</CopySnapshotResponse>\\n'\n        )\n        self.now = datetime.datetime(2011, 9, 9, 23, 36)\n        self.datetime_patch = mock.patch.object(\n            botocore.auth.datetime,\n            'datetime',\n            mock.Mock(wraps=datetime.datetime),\n        )\n        self.mocked_datetime = self.datetime_patch.start()\n        self.mocked_datetime.utcnow.return_value = self.now\n\n    def tearDown(self):\n        super().tearDown()\n        self.datetime_patch.stop()\n\n    def add_copy_snapshot_response(self, snapshot_id):\n        body = (self.copy_response % snapshot_id).encode('utf-8')\n        self.http_stubber.add_response(body=body)\n\n    def test_copy_snapshot_injects_presigned_url(self):\n        self.add_copy_snapshot_response(self.snapshot_id)\n        with self.http_stubber:\n            result = self.client.copy_snapshot(\n                SourceRegion='us-west-2',\n                SourceSnapshotId=self.snapshot_id,\n            )\n        self.assertEqual(result['SnapshotId'], self.snapshot_id)\n        self.assertEqual(len(self.http_stubber.requests), 1)\n        snapshot_request = self.http_stubber.requests[0]\n        body = parse_qs(snapshot_request.body)\n        self.assertIn('PresignedUrl', body)\n        presigned_url = urlparse(body['PresignedUrl'][0])\n        self.assertEqual(presigned_url.scheme, 'https')\n        self.assertEqual(presigned_url.netloc, 'ec2.us-west-2.amazonaws.com')\n        query_args = parse_qs(presigned_url.query)\n        self.assertEqual(query_args['Action'], ['CopySnapshot'])\n        self.assertEqual(query_args['Version'], ['2016-11-15'])\n        self.assertEqual(query_args['SourceRegion'], ['us-west-2'])\n        self.assertEqual(query_args['DestinationRegion'], ['us-east-1'])\n        self.assertEqual(query_args['SourceSnapshotId'], [self.snapshot_id])\n        self.assertEqual(query_args['X-Amz-Algorithm'], ['AWS4-HMAC-SHA256'])\n        expected_credential = 'access_key/20110909/us-west-2/ec2/aws4_request'\n        self.assertEqual(query_args['X-Amz-Credential'], [expected_credential])\n        self.assertEqual(query_args['X-Amz-Date'], ['20110909T233600Z'])\n        self.assertEqual(query_args['X-Amz-Expires'], ['3600'])\n        self.assertEqual(query_args['X-Amz-SignedHeaders'], ['host'])\n        expected_signature = (\n            'a94a6b52afdf3daa34c2e2a38a62b72c8dac129c9904c61aa1a5d86e38628537'\n        )\n        self.assertEqual(query_args['X-Amz-Signature'], [expected_signature])\n", "tests/functional/test_s3.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport datetime\nimport re\n\nimport pytest\n\nimport botocore.session\nfrom botocore import UNSIGNED\nfrom botocore.compat import get_md5, parse_qs, urlsplit\nfrom botocore.config import Config\nfrom botocore.exceptions import (\n    ClientError,\n    InvalidS3UsEast1RegionalEndpointConfigError,\n    ParamValidationError,\n    UnsupportedS3AccesspointConfigurationError,\n    UnsupportedS3ConfigurationError,\n)\nfrom botocore.parsers import ResponseParserError\nfrom tests import (\n    BaseSessionTest,\n    ClientHTTPStubber,\n    FreezeTime,\n    create_session,\n    mock,\n    requires_crt,\n    temporary_file,\n    unittest,\n)\n\nDATE = datetime.datetime(2021, 8, 27, 0, 0, 0)\n\n\nclass TestS3BucketValidation(unittest.TestCase):\n    def test_invalid_bucket_name_raises_error(self):\n        session = botocore.session.get_session()\n        s3 = session.create_client(\"s3\")\n        with self.assertRaises(ParamValidationError):\n            s3.put_object(\n                Bucket=\"adfgasdfadfs/bucket/name\", Key=\"foo\", Body=b\"asdf\"\n            )\n\n\nclass BaseS3OperationTest(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = \"us-west-2\"\n        self.client = self.session.create_client(\"s3\", self.region)\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n\nclass BaseS3ClientConfigurationTest(BaseSessionTest):\n    _V4_AUTH_REGEX = re.compile(\n        r\"AWS4-HMAC-SHA256 \"\n        r\"Credential=\\w+/\\d+/\"\n        r\"(?P<signing_region>[a-z0-9-]+)/\"\n        r\"(?P<signing_name>[a-z0-9-]+)/\"\n    )\n\n    _V4A_AUTH_REGEX = re.compile(\n        r\"AWS4-ECDSA-P256-SHA256 \"\n        r\"Credential=\\w+/\\d+/\"\n        r\"(?P<signing_name>[a-z0-9-]+)/\"\n    )\n\n    def setUp(self):\n        super().setUp()\n        self.region = \"us-west-2\"\n\n    def _get_auth_regex(self, auth_header):\n        if auth_header.startswith(\"AWS4-ECDSA\"):\n            return self._V4A_AUTH_REGEX\n        return self._V4_AUTH_REGEX\n\n    def assert_signing_region(self, request, expected_region):\n        auth_header = request.headers[\"Authorization\"].decode(\"utf-8\")\n        actual_region = None\n        auth_regex = self._get_auth_regex(auth_header)\n        match = auth_regex.match(auth_header)\n        if match and auth_regex is self._V4_AUTH_REGEX:\n            actual_region = match.group(\"signing_region\")\n            self.assertEqual(expected_region, actual_region)\n        else:\n            # SigV4a does not sign with a specific region\n            region_set = request.headers.get('X-Amz-Region-Set')\n            self.assertEqual(region_set, b'*')\n\n    def assert_signing_name(self, request, expected_name):\n        auth_header = request.headers[\"Authorization\"].decode(\"utf-8\")\n        actual_name = None\n        auth_regex = self._get_auth_regex(auth_header)\n        match = auth_regex.match(auth_header)\n        if match:\n            actual_name = match.group(\"signing_name\")\n        self.assertEqual(expected_name, actual_name)\n\n    def assert_signing_region_in_url(self, url, expected_region):\n        qs_components = parse_qs(urlsplit(url).query)\n        self.assertIn(expected_region, qs_components[\"X-Amz-Credential\"][0])\n\n    def assert_endpoint(self, request, expected_endpoint):\n        actual_endpoint = urlsplit(request.url).netloc\n        self.assertEqual(actual_endpoint, expected_endpoint)\n\n    def create_s3_client(self, **kwargs):\n        client_kwargs = {\"region_name\": self.region}\n        client_kwargs.update(kwargs)\n        return self.session.create_client(\"s3\", **client_kwargs)\n\n    def set_config_file(self, fileobj, contents):\n        fileobj.write(contents)\n        fileobj.flush()\n        self.environ[\"AWS_CONFIG_FILE\"] = fileobj.name\n\n\nclass TestS3ClientConfigResolution(BaseS3ClientConfigurationTest):\n    def test_no_s3_config(self):\n        client = self.create_s3_client()\n        self.assertIsNone(client.meta.config.s3)\n\n    def test_client_s3_dualstack_handles_uppercase_true(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    use_dualstack_endpoint = True\"\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3[\"use_dualstack_endpoint\"], True\n            )\n\n    def test_client_s3_dualstack_handles_lowercase_true(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    use_dualstack_endpoint = true\"\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3[\"use_dualstack_endpoint\"], True\n            )\n\n    def test_client_s3_accelerate_handles_uppercase_true(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    use_accelerate_endpoint = True\"\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3[\"use_accelerate_endpoint\"], True\n            )\n\n    def test_client_s3_accelerate_handles_lowercase_true(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    use_accelerate_endpoint = true\"\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3[\"use_accelerate_endpoint\"], True\n            )\n\n    def test_client_payload_signing_enabled_handles_uppercase_true(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    payload_signing_enabled = True\"\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3[\"payload_signing_enabled\"], True\n            )\n\n    def test_client_payload_signing_enabled_handles_lowercase_true(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    payload_signing_enabled = true\"\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3[\"payload_signing_enabled\"], True\n            )\n\n    def test_includes_unmodeled_s3_config_vars(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    unmodeled = unmodeled_val\"\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3[\"unmodeled\"], \"unmodeled_val\"\n            )\n\n    def test_mixed_modeled_and_unmodeled_config_vars(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f,\n                \"[default]\\n\"\n                \"s3 = \\n\"\n                \"    payload_signing_enabled = true\\n\"\n                \"    unmodeled = unmodeled_val\",\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3,\n                {\n                    \"payload_signing_enabled\": True,\n                    \"unmodeled\": \"unmodeled_val\",\n                },\n            )\n\n    def test_use_arn_region(self):\n        self.environ[\"AWS_S3_USE_ARN_REGION\"] = \"true\"\n        client = self.create_s3_client()\n        self.assertEqual(\n            client.meta.config.s3,\n            {\n                \"use_arn_region\": True,\n            },\n        )\n\n    def test_use_arn_region_config_var(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(f, \"[default]\\n\" \"s3_use_arn_region = true\")\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3,\n                {\n                    \"use_arn_region\": True,\n                },\n            )\n\n    def test_use_arn_region_nested_config_var(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    use_arn_region = true\"\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3,\n                {\n                    \"use_arn_region\": True,\n                },\n            )\n\n    def test_use_arn_region_is_case_insensitive(self):\n        self.environ[\"AWS_S3_USE_ARN_REGION\"] = \"True\"\n        client = self.create_s3_client()\n        self.assertEqual(\n            client.meta.config.s3,\n            {\n                \"use_arn_region\": True,\n            },\n        )\n\n    def test_use_arn_region_env_var_overrides_config_var(self):\n        self.environ[\"AWS_S3_USE_ARN_REGION\"] = \"false\"\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    use_arn_region = true\"\n            )\n            client = self.create_s3_client()\n        self.assertEqual(\n            client.meta.config.s3,\n            {\n                \"use_arn_region\": False,\n            },\n        )\n\n    def test_client_config_use_arn_region_overrides_env_var(self):\n        self.environ[\"AWS_S3_USE_ARN_REGION\"] = \"true\"\n        client = self.create_s3_client(\n            config=Config(s3={\"use_arn_region\": False})\n        )\n        self.assertEqual(\n            client.meta.config.s3,\n            {\n                \"use_arn_region\": False,\n            },\n        )\n\n    def test_client_config_use_arn_region_overrides_config_var(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3 = \\n\" \"    use_arn_region = true\"\n            )\n            client = self.create_s3_client(\n                config=Config(s3={\"use_arn_region\": False})\n            )\n        self.assertEqual(\n            client.meta.config.s3,\n            {\n                \"use_arn_region\": False,\n            },\n        )\n\n    def test_us_east_1_regional_env_var(self):\n        self.environ[\"AWS_S3_US_EAST_1_REGIONAL_ENDPOINT\"] = \"regional\"\n        client = self.create_s3_client()\n        self.assertEqual(\n            client.meta.config.s3,\n            {\n                \"us_east_1_regional_endpoint\": \"regional\",\n            },\n        )\n\n    def test_us_east_1_regional_config_var(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f, \"[default]\\n\" \"s3_us_east_1_regional_endpoint = regional\"\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3,\n                {\n                    \"us_east_1_regional_endpoint\": \"regional\",\n                },\n            )\n\n    def test_us_east_1_regional_nested_config_var(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f,\n                \"[default]\\n\"\n                \"s3 = \\n\"\n                \"    us_east_1_regional_endpoint = regional\",\n            )\n            client = self.create_s3_client()\n            self.assertEqual(\n                client.meta.config.s3,\n                {\n                    \"us_east_1_regional_endpoint\": \"regional\",\n                },\n            )\n\n    def test_us_east_1_regional_env_var_overrides_config_var(self):\n        self.environ[\"AWS_S3_US_EAST_1_REGIONAL_ENDPOINT\"] = \"regional\"\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f,\n                \"[default]\\n\"\n                \"s3 = \\n\"\n                \"    us_east_1_regional_endpoint = legacy\",\n            )\n            client = self.create_s3_client()\n        self.assertEqual(\n            client.meta.config.s3,\n            {\n                \"us_east_1_regional_endpoint\": \"regional\",\n            },\n        )\n\n    def test_client_config_us_east_1_regional_overrides_env_var(self):\n        self.environ[\"AWS_S3_US_EAST_1_REGIONAL_ENDPOINT\"] = \"regional\"\n        client = self.create_s3_client(\n            config=Config(s3={\"us_east_1_regional_endpoint\": \"legacy\"})\n        )\n        self.assertEqual(\n            client.meta.config.s3,\n            {\n                \"us_east_1_regional_endpoint\": \"legacy\",\n            },\n        )\n\n    def test_client_config_us_east_1_regional_overrides_config_var(self):\n        with temporary_file(\"w\") as f:\n            self.set_config_file(\n                f,\n                \"[default]\\n\"\n                \"s3 = \\n\"\n                \"    us_east_1_regional_endpoint = legacy\",\n            )\n            client = self.create_s3_client(\n                config=Config(s3={\"us_east_1_regional_endpoint\": \"regional\"})\n            )\n        self.assertEqual(\n            client.meta.config.s3,\n            {\n                \"us_east_1_regional_endpoint\": \"regional\",\n            },\n        )\n\n    def test_client_validates_us_east_1_regional(self):\n        with self.assertRaises(InvalidS3UsEast1RegionalEndpointConfigError):\n            self.create_s3_client(\n                config=Config(s3={\"us_east_1_regional_endpoint\": \"not-valid\"})\n            )\n\n    def test_client_region_defaults_to_us_east_1(self):\n        client = self.create_s3_client(region_name=None)\n        self.assertEqual(client.meta.region_name, \"us-east-1\")\n\n    def test_client_region_remains_us_east_1(self):\n        client = self.create_s3_client(region_name=\"us-east-1\")\n        self.assertEqual(client.meta.region_name, \"us-east-1\")\n\n    def test_client_region_remains_aws_global(self):\n        client = self.create_s3_client(region_name=\"aws-global\")\n        self.assertEqual(client.meta.region_name, \"aws-global\")\n\n    def test_client_region_defaults_to_aws_global_for_regional(self):\n        self.environ[\"AWS_S3_US_EAST_1_REGIONAL_ENDPOINT\"] = \"regional\"\n        client = self.create_s3_client(region_name=None)\n        self.assertEqual(client.meta.region_name, \"aws-global\")\n\n    def test_client_region_remains_us_east_1_for_regional(self):\n        self.environ[\"AWS_S3_US_EAST_1_REGIONAL_ENDPOINT\"] = \"regional\"\n        client = self.create_s3_client(region_name=\"us-east-1\")\n        self.assertEqual(client.meta.region_name, \"us-east-1\")\n\n    def test_client_region_remains_aws_global_for_regional(self):\n        self.environ[\"AWS_S3_US_EAST_1_REGIONAL_ENDPOINT\"] = \"regional\"\n        client = self.create_s3_client(region_name=\"aws-global\")\n        self.assertEqual(client.meta.region_name, \"aws-global\")\n\n\nclass TestS3Copy(BaseS3OperationTest):\n    def create_s3_client(self, **kwargs):\n        client_kwargs = {\"region_name\": self.region}\n        client_kwargs.update(kwargs)\n        return self.session.create_client(\"s3\", **client_kwargs)\n\n    def create_stubbed_s3_client(self, **kwargs):\n        client = self.create_s3_client(**kwargs)\n        http_stubber = ClientHTTPStubber(client)\n        http_stubber.start()\n        return client, http_stubber\n\n    def test_s3_copy_object_with_empty_response(self):\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-east-1\"\n        )\n\n        empty_body = b\"\"\n        complete_body = (\n            b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n\\n'\n            b\"<CopyObjectResult \"\n            b'xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">'\n            b\"<LastModified>2020-04-21T21:03:31.000Z</LastModified>\"\n            b\"<ETag>&quot;s0mEcH3cK5uM&quot;</ETag></CopyObjectResult>\"\n        )\n\n        self.http_stubber.add_response(status=200, body=empty_body)\n        self.http_stubber.add_response(status=200, body=complete_body)\n        response = self.client.copy_object(\n            Bucket=\"bucket\",\n            CopySource=\"other-bucket/test.txt\",\n            Key=\"test.txt\",\n        )\n\n        # Validate we retried and got second body\n        self.assertEqual(len(self.http_stubber.requests), 2)\n        self.assertEqual(response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200)\n        self.assertTrue(\"CopyObjectResult\" in response)\n\n    def test_s3_copy_object_with_incomplete_response(self):\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-east-1\"\n        )\n\n        incomplete_body = b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n\\n\\n'\n        self.http_stubber.add_response(status=200, body=incomplete_body)\n        with self.assertRaises(ResponseParserError):\n            self.client.copy_object(\n                Bucket=\"bucket\",\n                CopySource=\"other-bucket/test.txt\",\n                Key=\"test.txt\",\n            )\n\n\nclass TestAccesspointArn(BaseS3ClientConfigurationTest):\n    def setUp(self):\n        super().setUp()\n        self.client, self.http_stubber = self.create_stubbed_s3_client()\n\n    def create_stubbed_s3_client(self, **kwargs):\n        client = self.create_s3_client(**kwargs)\n        http_stubber = ClientHTTPStubber(client)\n        http_stubber.start()\n        return client, http_stubber\n\n    def assert_expected_copy_source_header(\n        self, http_stubber, expected_copy_source\n    ):\n        request = self.http_stubber.requests[0]\n        self.assertIn(\"x-amz-copy-source\", request.headers)\n        self.assertEqual(\n            request.headers[\"x-amz-copy-source\"], expected_copy_source\n        )\n\n    def add_copy_object_response(self, http_stubber):\n        http_stubber.add_response(\n            body=b\"<CopyObjectResult></CopyObjectResult>\"\n        )\n\n    def assert_endpoint(self, request, expected_endpoint):\n        actual_endpoint = urlsplit(request.url).netloc\n        self.assertEqual(actual_endpoint, expected_endpoint)\n\n    def assert_header_matches(self, request, header_key, expected_value):\n        self.assertEqual(request.headers.get(header_key), expected_value)\n\n    def test_missing_account_id_in_arn(self):\n        accesspoint_arn = \"arn:aws:s3:us-west-2::accesspoint:myendpoint\"\n        with self.assertRaises(botocore.exceptions.ParamValidationError):\n            self.client.list_objects(Bucket=accesspoint_arn)\n\n    def test_missing_accesspoint_name_in_arn(self):\n        accesspoint_arn = \"arn:aws:s3:us-west-2:123456789012:accesspoint\"\n        with self.assertRaises(botocore.exceptions.ParamValidationError):\n            self.client.list_objects(Bucket=accesspoint_arn)\n\n    def test_accesspoint_includes_asterisk(self):\n        accesspoint_arn = \"arn:aws:s3:us-west-2:123456789012:accesspoint:*\"\n        with self.assertRaises(botocore.exceptions.ParamValidationError):\n            self.client.list_objects(Bucket=accesspoint_arn)\n\n    def test_accesspoint_arn_contains_subresources(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint:object\"\n        )\n        with self.assertRaises(botocore.exceptions.ParamValidationError):\n            self.client.list_objects(Bucket=accesspoint_arn)\n\n    def test_accesspoint_arn_with_custom_endpoint(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, http_stubber = self.create_stubbed_s3_client(\n            endpoint_url=\"https://custom.com\"\n        )\n        http_stubber.add_response()\n        self.client.list_objects(Bucket=accesspoint_arn)\n        expected_endpoint = \"myendpoint-123456789012.custom.com\"\n        self.assert_endpoint(http_stubber.requests[0], expected_endpoint)\n\n    def test_accesspoint_arn_with_custom_endpoint_and_dualstack(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, http_stubber = self.create_stubbed_s3_client(\n            endpoint_url=\"https://custom.com\",\n            config=Config(s3={\"use_dualstack_endpoint\": True}),\n        )\n        http_stubber.add_response()\n        self.client.list_objects(Bucket=accesspoint_arn)\n        expected_endpoint = \"myendpoint-123456789012.custom.com\"\n        self.assert_endpoint(http_stubber.requests[0], expected_endpoint)\n\n    def test_accesspoint_arn_with_s3_accelerate(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            config=Config(s3={\"use_accelerate_endpoint\": True})\n        )\n        with self.assertRaises(\n            botocore.exceptions.UnsupportedS3AccesspointConfigurationError\n        ):\n            self.client.list_objects(Bucket=accesspoint_arn)\n\n    def test_accesspoint_arn_cross_partition(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            region_name=\"cn-north-1\"\n        )\n        with self.assertRaises(\n            botocore.exceptions.UnsupportedS3AccesspointConfigurationError\n        ):\n            self.client.list_objects(Bucket=accesspoint_arn)\n\n    def test_accesspoint_arn_cross_partition_use_client_region(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            region_name=\"cn-north-1\",\n            config=Config(s3={\"use_accelerate_endpoint\": True}),\n        )\n        with self.assertRaises(\n            botocore.exceptions.UnsupportedS3AccesspointConfigurationError\n        ):\n            self.client.list_objects(Bucket=accesspoint_arn)\n\n    def test_signs_with_arn_region(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-east-1\"\n        )\n        self.http_stubber.add_response()\n        self.client.list_objects(Bucket=accesspoint_arn)\n        self.assert_signing_region(self.http_stubber.requests[0], \"us-west-2\")\n\n    def test_signs_with_client_region_when_use_arn_region_false(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-east-1\",\n            config=Config(s3={\"use_arn_region\": False}),\n        )\n        self.http_stubber.add_response()\n        with self.assertRaises(UnsupportedS3AccesspointConfigurationError):\n            self.client.list_objects(Bucket=accesspoint_arn)\n\n    def test_presign_signs_with_arn_region(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            region_name=\"us-east-1\", config=Config(signature_version=\"s3v4\")\n        )\n        url = self.client.generate_presigned_url(\n            \"get_object\", {\"Bucket\": accesspoint_arn, \"Key\": \"mykey\"}\n        )\n        self.assert_signing_region_in_url(url, \"us-west-2\")\n\n    def test_presign_signs_with_client_region_when_use_arn_region_false(self):\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            region_name=\"us-east-1\",\n            config=Config(\n                signature_version=\"s3v4\", s3={\"use_arn_region\": False}\n            ),\n        )\n        with self.assertRaises(UnsupportedS3AccesspointConfigurationError):\n            self.client.generate_presigned_url(\n                \"get_object\", {\"Bucket\": accesspoint_arn, \"Key\": \"mykey\"}\n            )\n\n    def test_copy_source_str_with_accesspoint_arn(self):\n        copy_source = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint/\"\n            \"object/myprefix/myobject\"\n        )\n        self.client, self.http_stubber = self.create_stubbed_s3_client()\n        self.add_copy_object_response(self.http_stubber)\n        self.client.copy_object(\n            Bucket=\"mybucket\", Key=\"mykey\", CopySource=copy_source\n        )\n        self.assert_expected_copy_source_header(\n            self.http_stubber,\n            expected_copy_source=(\n                b\"arn%3Aaws%3As3%3Aus-west-2%3A123456789012%3Aaccesspoint%3A\"\n                b\"myendpoint/object/myprefix/myobject\"\n            ),\n        )\n\n    def test_copy_source_str_with_accesspoint_arn_and_version_id(self):\n        copy_source = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint/\"\n            \"object/myprefix/myobject?versionId=myversionid\"\n        )\n        self.client, self.http_stubber = self.create_stubbed_s3_client()\n        self.add_copy_object_response(self.http_stubber)\n        self.client.copy_object(\n            Bucket=\"mybucket\", Key=\"mykey\", CopySource=copy_source\n        )\n        self.assert_expected_copy_source_header(\n            self.http_stubber,\n            expected_copy_source=(\n                b\"arn%3Aaws%3As3%3Aus-west-2%3A123456789012%3Aaccesspoint%3A\"\n                b\"myendpoint/object/myprefix/myobject?versionId=myversionid\"\n            ),\n        )\n\n    def test_copy_source_dict_with_accesspoint_arn(self):\n        copy_source = {\n            \"Bucket\": \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\",\n            \"Key\": \"myprefix/myobject\",\n        }\n        self.client, self.http_stubber = self.create_stubbed_s3_client()\n        self.add_copy_object_response(self.http_stubber)\n        self.client.copy_object(\n            Bucket=\"mybucket\", Key=\"mykey\", CopySource=copy_source\n        )\n        self.assert_expected_copy_source_header(\n            self.http_stubber,\n            expected_copy_source=(\n                b\"arn%3Aaws%3As3%3Aus-west-2%3A123456789012%3Aaccesspoint%3A\"\n                b\"myendpoint/object/myprefix/myobject\"\n            ),\n        )\n\n    def test_copy_source_dict_with_accesspoint_arn_and_version_id(self):\n        copy_source = {\n            \"Bucket\": \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\",\n            \"Key\": \"myprefix/myobject\",\n            \"VersionId\": \"myversionid\",\n        }\n        self.client, self.http_stubber = self.create_stubbed_s3_client()\n        self.add_copy_object_response(self.http_stubber)\n        self.client.copy_object(\n            Bucket=\"mybucket\", Key=\"mykey\", CopySource=copy_source\n        )\n        self.assert_expected_copy_source_header(\n            self.http_stubber,\n            expected_copy_source=(\n                b\"arn%3Aaws%3As3%3Aus-west-2%3A123456789012%3Aaccesspoint%3A\"\n                b\"myendpoint/object/myprefix/myobject?versionId=myversionid\"\n            ),\n        )\n\n    def test_basic_outpost_arn(self):\n        outpost_arn = (\n            \"arn:aws:s3-outposts:us-west-2:123456789012:outpost:\"\n            \"op-01234567890123456:accesspoint:myaccesspoint\"\n        )\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-east-1\"\n        )\n        self.http_stubber.add_response()\n        self.client.list_objects(Bucket=outpost_arn)\n        request = self.http_stubber.requests[0]\n        self.assert_signing_name(request, \"s3-outposts\")\n        self.assert_signing_region(request, \"us-west-2\")\n        expected_endpoint = (\n            \"myaccesspoint-123456789012.op-01234567890123456.\"\n            \"s3-outposts.us-west-2.amazonaws.com\"\n        )\n        self.assert_endpoint(request, expected_endpoint)\n        sha_header = request.headers.get(\"x-amz-content-sha256\")\n        self.assertIsNotNone(sha_header)\n        self.assertNotEqual(sha_header, b\"UNSIGNED-PAYLOAD\")\n\n    def test_basic_outpost_arn_custom_endpoint(self):\n        outpost_arn = (\n            \"arn:aws:s3-outposts:us-west-2:123456789012:outpost:\"\n            \"op-01234567890123456:accesspoint:myaccesspoint\"\n        )\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            endpoint_url=\"https://custom.com\", region_name=\"us-east-1\"\n        )\n        self.http_stubber.add_response()\n        self.client.list_objects(Bucket=outpost_arn)\n        request = self.http_stubber.requests[0]\n        self.assert_signing_name(request, \"s3-outposts\")\n        self.assert_signing_region(request, \"us-west-2\")\n        expected_endpoint = (\n            \"myaccesspoint-123456789012.op-01234567890123456.custom.com\"\n        )\n        self.assert_endpoint(request, expected_endpoint)\n\n    def test_outpost_arn_presigned_url(self):\n        outpost_arn = (\n            'arn:aws:s3-outposts:us-west-2:123456789012:outpost/'\n            'op-01234567890123456/accesspoint/myaccesspoint'\n        )\n        expected_url = (\n            'myaccesspoint-123456789012.op-01234567890123456.'\n            's3-outposts.us-west-2.amazonaws.com'\n        )\n        expected_credentials = (\n            '20210827%2Fus-west-2%2Fs3-outposts%2Faws4_request'\n        )\n        expected_signature = (\n            'a944fbe2bfbae429f922746546d1c6f890649c88ba7826bd1d258ac13f327e09'\n        )\n        config = Config(signature_version='s3v4')\n        presigned_url = self._get_presigned_url(\n            outpost_arn, 'us-west-2', config=config\n        )\n        self._assert_presigned_url(\n            presigned_url,\n            expected_url,\n            expected_signature,\n            expected_credentials,\n        )\n\n    def test_outpost_arn_presigned_url_with_use_arn_region(self):\n        outpost_arn = (\n            'arn:aws:s3-outposts:us-west-2:123456789012:outpost/'\n            'op-01234567890123456/accesspoint/myaccesspoint'\n        )\n        expected_url = (\n            'myaccesspoint-123456789012.op-01234567890123456.'\n            's3-outposts.us-west-2.amazonaws.com'\n        )\n        expected_credentials = (\n            '20210827%2Fus-west-2%2Fs3-outposts%2Faws4_request'\n        )\n        expected_signature = (\n            'a944fbe2bfbae429f922746546d1c6f890649c88ba7826bd1d258ac13f327e09'\n        )\n        config = Config(\n            signature_version='s3v4',\n            s3={\n                'use_arn_region': True,\n            },\n        )\n        presigned_url = self._get_presigned_url(\n            outpost_arn, 'us-west-2', config=config\n        )\n        self._assert_presigned_url(\n            presigned_url,\n            expected_url,\n            expected_signature,\n            expected_credentials,\n        )\n\n    def test_outpost_arn_presigned_url_cross_region_arn(self):\n        outpost_arn = (\n            'arn:aws:s3-outposts:us-east-1:123456789012:outpost/'\n            'op-01234567890123456/accesspoint/myaccesspoint'\n        )\n        expected_url = (\n            'myaccesspoint-123456789012.op-01234567890123456.'\n            's3-outposts.us-east-1.amazonaws.com'\n        )\n        expected_credentials = (\n            '20210827%2Fus-east-1%2Fs3-outposts%2Faws4_request'\n        )\n        expected_signature = (\n            '7f93df0b81f80e590d95442d579bd6cf749a35ff4bbdc6373fa669b89c7fce4e'\n        )\n        config = Config(\n            signature_version='s3v4',\n            s3={\n                'use_arn_region': True,\n            },\n        )\n        presigned_url = self._get_presigned_url(\n            outpost_arn, 'us-west-2', config=config\n        )\n        self._assert_presigned_url(\n            presigned_url,\n            expected_url,\n            expected_signature,\n            expected_credentials,\n        )\n\n    def test_outpost_arn_with_s3_accelerate(self):\n        outpost_arn = (\n            \"arn:aws:s3-outposts:us-west-2:123456789012:outpost:\"\n            \"op-01234567890123456:accesspoint:myaccesspoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            config=Config(s3={\"use_accelerate_endpoint\": True})\n        )\n        with self.assertRaises(UnsupportedS3AccesspointConfigurationError):\n            self.client.list_objects(Bucket=outpost_arn)\n\n    def test_outpost_arn_with_s3_dualstack(self):\n        outpost_arn = (\n            \"arn:aws:s3-outposts:us-west-2:123456789012:outpost:\"\n            \"op-01234567890123456:accesspoint:myaccesspoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            config=Config(s3={\"use_dualstack_endpoint\": True})\n        )\n        with self.assertRaises(UnsupportedS3AccesspointConfigurationError):\n            self.client.list_objects(Bucket=outpost_arn)\n\n    def test_incorrect_outpost_format(self):\n        outpost_arn = \"arn:aws:s3-outposts:us-west-2:123456789012:outpost\"\n        with self.assertRaises(botocore.exceptions.ParamValidationError):\n            self.client.list_objects(Bucket=outpost_arn)\n\n    def test_incorrect_outpost_no_accesspoint(self):\n        outpost_arn = (\n            \"arn:aws:s3-outposts:us-west-2:123456789012:outpost:\"\n            \"op-01234567890123456\"\n        )\n        with self.assertRaises(botocore.exceptions.ParamValidationError):\n            self.client.list_objects(Bucket=outpost_arn)\n\n    def test_incorrect_outpost_resource_format(self):\n        outpost_arn = (\n            \"arn:aws:s3-outposts:us-west-2:123456789012:outpost:myaccesspoint\"\n        )\n        with self.assertRaises(botocore.exceptions.ParamValidationError):\n            self.client.list_objects(Bucket=outpost_arn)\n\n    def test_incorrect_outpost_sub_resources(self):\n        outpost_arn = (\n            \"arn:aws:s3-outposts:us-west-2:123456789012:outpost:\"\n            \"op-01234567890123456:accesspoint:mybucket:object:foo\"\n        )\n        with self.assertRaises(botocore.exceptions.ParamValidationError):\n            self.client.list_objects(Bucket=outpost_arn)\n\n    def test_incorrect_outpost_invalid_character(self):\n        outpost_arn = (\n            \"arn:aws:s3-outposts:us-west-2:123456789012:outpost:\"\n            \"op-0123456.890123456:accesspoint:myaccesspoint\"\n        )\n        with self.assertRaises(botocore.exceptions.ParamValidationError):\n            self.client.list_objects(Bucket=outpost_arn)\n\n    def test_s3_object_lambda_arn_with_s3_dualstack(self):\n        s3_object_lambda_arn = (\n            \"arn:aws:s3-object-lambda:us-west-2:123456789012:\"\n            \"accesspoint/myBanner\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            config=Config(s3={\"use_dualstack_endpoint\": True})\n        )\n        with self.assertRaises(UnsupportedS3AccesspointConfigurationError):\n            self.client.list_objects(Bucket=s3_object_lambda_arn)\n\n    def test_s3_object_lambda_fips_raise_for_cross_region(self):\n        s3_object_lambda_arn = (\n            \"arn:aws-us-gov:s3-object-lambda:us-gov-east-1:123456789012:\"\n            \"accesspoint/mybanner\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            region_name=\"fips-us-gov-west-1\",\n            config=Config(s3={\"use_arn_region\": False}),\n        )\n        expected_exception = UnsupportedS3AccesspointConfigurationError\n        with self.assertRaises(expected_exception):\n            self.client.list_objects(Bucket=s3_object_lambda_arn)\n\n    def test_s3_object_lambda_with_global_regions(self):\n        s3_object_lambda_arn = (\n            \"arn:aws:s3-object-lambda:us-east-1:123456789012:\"\n            \"accesspoint/mybanner\"\n        )\n        expected_exception = UnsupportedS3AccesspointConfigurationError\n        for region in (\"aws-global\", \"s3-external-1\"):\n            self.client, _ = self.create_stubbed_s3_client(\n                region_name=region, config=Config(s3={\"use_arn_region\": False})\n            )\n            with self.assertRaises(expected_exception):\n                self.client.list_objects(Bucket=s3_object_lambda_arn)\n\n    def test_s3_object_lambda_arn_with_us_east_1(self):\n        # test that us-east-1 region is not resolved\n        # into s3 global endpoint\n        s3_object_lambda_arn = (\n            \"arn:aws:s3-object-lambda:us-east-1:123456789012:\"\n            \"accesspoint/myBanner\"\n        )\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-east-1\",\n            config=Config(s3={\"use_arn_region\": False}),\n        )\n        self.http_stubber.add_response()\n        self.client.list_objects(Bucket=s3_object_lambda_arn)\n        request = self.http_stubber.requests[0]\n        self.assert_signing_name(request, \"s3-object-lambda\")\n        self.assert_signing_region(request, \"us-east-1\")\n        expected_endpoint = (\n            \"myBanner-123456789012.s3-object-lambda.us-east-1.amazonaws.com\"\n        )\n        self.assert_endpoint(request, expected_endpoint)\n\n    def test_basic_s3_object_lambda_arn(self):\n        s3_object_lambda_arn = (\n            \"arn:aws:s3-object-lambda:us-west-2:123456789012:\"\n            \"accesspoint/myBanner\"\n        )\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-east-1\"\n        )\n        self.http_stubber.add_response()\n        self.client.list_objects(Bucket=s3_object_lambda_arn)\n        request = self.http_stubber.requests[0]\n        self.assert_signing_name(request, \"s3-object-lambda\")\n        self.assert_signing_region(request, \"us-west-2\")\n        expected_endpoint = (\n            \"myBanner-123456789012.s3-object-lambda.us-west-2.amazonaws.com\"\n        )\n        self.assert_endpoint(request, expected_endpoint)\n        sha_header = request.headers.get(\"x-amz-content-sha256\")\n        self.assertIsNotNone(sha_header)\n        self.assertNotEqual(sha_header, b\"UNSIGNED-PAYLOAD\")\n\n    def test_outposts_raise_exception_if_fips_region(self):\n        outpost_arn = (\n            \"arn:aws:s3-outposts:us-gov-east-1:123456789012:outpost:\"\n            \"op-01234567890123456:accesspoint:myaccesspoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            region_name=\"fips-east-1\"\n        )\n        expected_exception = UnsupportedS3AccesspointConfigurationError\n        with self.assertRaises(expected_exception):\n            self.client.list_objects(Bucket=outpost_arn)\n\n    def test_accesspoint_fips_raise_for_cross_region(self):\n        s3_accesspoint_arn = (\n            \"arn:aws-us-gov:s3:us-gov-east-1:123456789012:\"\n            \"accesspoint:myendpoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            region_name=\"fips-us-gov-west-1\",\n            config=Config(s3={\"use_arn_region\": False}),\n        )\n        expected_exception = UnsupportedS3AccesspointConfigurationError\n        with self.assertRaises(expected_exception):\n            self.client.list_objects(Bucket=s3_accesspoint_arn)\n\n    def test_accesspoint_fips_raise_if_fips_in_arn(self):\n        s3_accesspoint_arn = (\n            \"arn:aws-us-gov:s3:fips-us-gov-west-1:123456789012:\"\n            \"accesspoint:myendpoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            region_name=\"fips-us-gov-west-1\",\n        )\n        expected_exception = UnsupportedS3AccesspointConfigurationError\n        with self.assertRaises(expected_exception):\n            self.client.list_objects(Bucket=s3_accesspoint_arn)\n\n    def test_accesspoint_with_global_regions(self):\n        s3_accesspoint_arn = (\n            \"arn:aws:s3:us-east-1:123456789012:accesspoint:myendpoint\"\n        )\n        self.client, _ = self.create_stubbed_s3_client(\n            region_name=\"aws-global\",\n            config=Config(s3={\"use_arn_region\": False}),\n        )\n        expected_exception = UnsupportedS3AccesspointConfigurationError\n        with self.assertRaises(expected_exception):\n            self.client.list_objects(Bucket=s3_accesspoint_arn)\n\n        # It shouldn't raise if use_arn_region is True\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"s3-external-1\",\n            config=Config(s3={\"use_arn_region\": True}),\n        )\n\n        self.http_stubber.add_response()\n        self.client.list_objects(Bucket=s3_accesspoint_arn)\n        request = self.http_stubber.requests[0]\n        expected_endpoint = (\n            \"myendpoint-123456789012.s3-accesspoint.\" \"us-east-1.amazonaws.com\"\n        )\n        self.assert_endpoint(request, expected_endpoint)\n\n        # It shouldn't raise if no use_arn_region is specified since\n        # use_arn_region defaults to True\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"s3-external-1\",\n        )\n\n        self.http_stubber.add_response()\n        self.client.list_objects(Bucket=s3_accesspoint_arn)\n        request = self.http_stubber.requests[0]\n        expected_endpoint = (\n            \"myendpoint-123456789012.s3-accesspoint.\" \"us-east-1.amazonaws.com\"\n        )\n        self.assert_endpoint(request, expected_endpoint)\n\n    @requires_crt()\n    def test_mrap_arn_with_client_regions(self):\n        mrap_arn = \"arn:aws:s3::123456789012:accesspoint:mfzwi23gnjvgw.mrap\"\n        region_tests = [\n            (\n                \"us-east-1\",\n                \"mfzwi23gnjvgw.mrap.accesspoint.s3-global.amazonaws.com\",\n            ),\n            (\n                \"us-west-2\",\n                \"mfzwi23gnjvgw.mrap.accesspoint.s3-global.amazonaws.com\",\n            ),\n            (\n                \"aws-global\",\n                \"mfzwi23gnjvgw.mrap.accesspoint.s3-global.amazonaws.com\",\n            ),\n        ]\n        for region, expected in region_tests:\n            self._assert_mrap_endpoint(mrap_arn, region, expected)\n\n    @requires_crt()\n    def test_mrap_arn_with_other_partition(self):\n        mrap_arn = \"arn:aws-cn:s3::123456789012:accesspoint:mfzwi23gnjvgw.mrap\"\n        expected = \"mfzwi23gnjvgw.mrap.accesspoint.s3-global.amazonaws.com.cn\"\n        self._assert_mrap_endpoint(mrap_arn, \"cn-north-1\", expected)\n\n    @requires_crt()\n    def test_mrap_arn_with_invalid_s3_configs(self):\n        mrap_arn = \"arn:aws:s3::123456789012:accesspoint:mfzwi23gnjvgw.mrap\"\n        config_tests = [\n            (\"us-west-2\", Config(s3={\"use_dualstack_endpoint\": True})),\n            (\"us-west-2\", Config(s3={\"use_accelerate_endpoint\": True})),\n        ]\n        for region, config in config_tests:\n            self._assert_mrap_config_failure(mrap_arn, region, config=config)\n\n    @requires_crt()\n    def test_mrap_arn_with_disable_config_enabled(self):\n        mrap_arn = \"arn:aws:s3::123456789012:accesspoint:mfzwi23gnjvgw.mrap\"\n        config = Config(s3={\"s3_disable_multiregion_access_points\": True})\n        for region in (\"us-west-2\", \"aws-global\"):\n            self._assert_mrap_config_failure(mrap_arn, region, config)\n\n    @requires_crt()\n    def test_mrap_arn_with_disable_config_enabled_custom_endpoint(self):\n        mrap_arn = \"arn:aws:s3::123456789012:accesspoint:myendpoint\"\n        config = Config(s3={\"s3_disable_multiregion_access_points\": True})\n        self._assert_mrap_config_failure(mrap_arn, \"us-west-2\", config)\n\n    @requires_crt()\n    def test_mrap_arn_with_disable_config_disabled(self):\n        mrap_arn = \"arn:aws:s3::123456789012:accesspoint:mfzwi23gnjvgw.mrap\"\n        config = Config(s3={\"s3_disable_multiregion_access_points\": False})\n        expected = \"mfzwi23gnjvgw.mrap.accesspoint.s3-global.amazonaws.com\"\n        self._assert_mrap_endpoint(\n            mrap_arn, \"us-west-2\", expected, config=config\n        )\n\n    @requires_crt()\n    def test_global_arn_without_mrap_suffix(self):\n        global_arn_tests = [\n            (\n                \"arn:aws:s3::123456789012:accesspoint:myendpoint\",\n                \"myendpoint.accesspoint.s3-global.amazonaws.com\",\n            ),\n            (\n                \"arn:aws:s3::123456789012:accesspoint:my.bucket\",\n                \"my.bucket.accesspoint.s3-global.amazonaws.com\",\n            ),\n        ]\n        for arn, expected in global_arn_tests:\n            self._assert_mrap_endpoint(arn, \"us-west-2\", expected)\n\n    @requires_crt()\n    def test_mrap_signing_algorithm_is_sigv4a(self):\n        s3_accesspoint_arn = (\n            \"arn:aws:s3::123456789012:accesspoint:mfzwi23gnjvgw.mrap\"\n        )\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-west-2\"\n        )\n        self.http_stubber.add_response()\n        self.client.list_objects(Bucket=s3_accesspoint_arn)\n        request = self.http_stubber.requests[0]\n        self._assert_sigv4a_used(request.headers)\n\n    @requires_crt()\n    def test_mrap_presigned_url(self):\n        mrap_arn = \"arn:aws:s3::123456789012:accesspoint:mfzwi23gnjvgw.mrap\"\n        config = Config(s3={\"s3_disable_multiregion_access_points\": False})\n        expected_url = \"mfzwi23gnjvgw.mrap.accesspoint.s3-global.amazonaws.com\"\n        self._assert_mrap_presigned_url(\n            mrap_arn, \"us-west-2\", expected_url, config=config\n        )\n\n    @requires_crt()\n    def test_mrap_presigned_url_disabled(self):\n        mrap_arn = \"arn:aws:s3::123456789012:accesspoint:mfzwi23gnjvgw.mrap\"\n        config = Config(s3={\"s3_disable_multiregion_access_points\": True})\n        self._assert_mrap_config_presigned_failure(\n            mrap_arn, \"us-west-2\", config\n        )\n\n    def _assert_mrap_config_failure(self, arn, region, config):\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=region, config=config\n        )\n        with self.assertRaises(\n            botocore.exceptions.UnsupportedS3AccesspointConfigurationError\n        ):\n            self.client.list_objects(Bucket=arn)\n\n    @FreezeTime(botocore.auth.datetime, date=DATE)\n    def _get_presigned_url(self, arn, region, config=None, endpoint_url=None):\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=region,\n            endpoint_url=endpoint_url,\n            config=config,\n            aws_access_key_id='ACCESS_KEY_ID',\n            aws_secret_access_key='SECRET_ACCESS_KEY',\n        )\n        presigned_url = self.client.generate_presigned_url(\n            'get_object', Params={'Bucket': arn, 'Key': 'obj'}, ExpiresIn=900\n        )\n        return presigned_url\n\n    def _assert_presigned_url(\n        self,\n        presigned_url,\n        expected_url,\n        expected_signature,\n        expected_credentials,\n    ):\n        url_parts = urlsplit(presigned_url)\n        assert url_parts.netloc == expected_url\n        query_strs = url_parts.query.split('&')\n        query_parts = dict(part.split('=') for part in query_strs)\n        assert expected_signature == query_parts['X-Amz-Signature']\n        assert expected_credentials in query_parts['X-Amz-Credential']\n\n    def _assert_mrap_presigned_url(\n        self, arn, region, expected, endpoint_url=None, config=None\n    ):\n        presigned_url = self._get_presigned_url(\n            arn, region, endpoint_url=endpoint_url, config=config\n        )\n        url_parts = urlsplit(presigned_url)\n        self.assertEqual(expected, url_parts.hostname)\n        # X-Amz-Region-Set header MUST be * (percent-encoded as %2A) for MRAPs\n        self.assertIn(\"X-Amz-Region-Set=%2A\", url_parts.query)\n\n    def _assert_mrap_config_presigned_failure(self, arn, region, config):\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=region, config=config\n        )\n        with self.assertRaises(\n            botocore.exceptions.UnsupportedS3AccesspointConfigurationError\n        ):\n            self.client.generate_presigned_url(\n                \"get_object\", Params={\"Bucket\": arn, \"Key\": \"test_object\"}\n            )\n\n    def _assert_mrap_endpoint(\n        self, arn, region, expected, endpoint_url=None, config=None\n    ):\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=region, endpoint_url=endpoint_url, config=config\n        )\n        self.http_stubber.add_response()\n        self.client.list_objects(Bucket=arn)\n        request = self.http_stubber.requests[0]\n        self.assert_endpoint(request, expected)\n        # MRAP requests MUST include a global signing region stored in the\n        # X-Amz-Region-Set header as *.\n        self.assert_header_matches(request, \"X-Amz-Region-Set\", b\"*\")\n\n    def _assert_sigv4a_used(self, headers):\n        self.assertIn(\n            b\"AWS4-ECDSA-P256-SHA256\", headers.get(\"Authorization\", \"\")\n        )\n\n\nclass TestOnlyAsciiCharsAllowed(BaseS3OperationTest):\n    def test_validates_non_ascii_chars_trigger_validation_error(self):\n        self.http_stubber.add_response()\n        with self.http_stubber:\n            with self.assertRaises(ParamValidationError):\n                self.client.put_object(\n                    Bucket=\"foo\",\n                    Key=\"bar\",\n                    Metadata={\"goodkey\": \"good\", \"non-ascii\": \"\\u2713\"},\n                )\n\n\nclass TestS3GetBucketLifecycle(BaseS3OperationTest):\n    def test_multiple_transitions_returns_one(self):\n        response_body = (\n            b'<?xml version=\"1.0\" ?>'\n            b'<LifecycleConfiguration xmlns=\"http://s3.amazonaws.'\n            b'com/doc/2006-03-01/\">'\n            b\"\t<Rule>\"\n            b\"\t\t<ID>transitionRule</ID>\"\n            b\"\t\t<Prefix>foo</Prefix>\"\n            b\"\t\t<Status>Enabled</Status>\"\n            b\"\t\t<Transition>\"\n            b\"\t\t\t<Days>40</Days>\"\n            b\"\t\t\t<StorageClass>STANDARD_IA</StorageClass>\"\n            b\"\t\t</Transition>\"\n            b\"\t\t<Transition>\"\n            b\"\t\t\t<Days>70</Days>\"\n            b\"\t\t\t<StorageClass>GLACIER</StorageClass>\"\n            b\"\t\t</Transition>\"\n            b\"\t</Rule>\"\n            b\"\t<Rule>\"\n            b\"\t\t<ID>noncurrentVersionRule</ID>\"\n            b\"\t\t<Prefix>bar</Prefix>\"\n            b\"\t\t<Status>Enabled</Status>\"\n            b\"\t\t<NoncurrentVersionTransition>\"\n            b\"\t\t\t<NoncurrentDays>40</NoncurrentDays>\"\n            b\"\t\t\t<StorageClass>STANDARD_IA</StorageClass>\"\n            b\"\t\t</NoncurrentVersionTransition>\"\n            b\"\t\t<NoncurrentVersionTransition>\"\n            b\"\t\t\t<NoncurrentDays>70</NoncurrentDays>\"\n            b\"\t\t\t<StorageClass>GLACIER</StorageClass>\"\n            b\"\t\t</NoncurrentVersionTransition>\"\n            b\"\t</Rule>\"\n            b\"</LifecycleConfiguration>\"\n        )\n        s3 = self.session.create_client(\"s3\")\n        with ClientHTTPStubber(s3) as http_stubber:\n            http_stubber.add_response(body=response_body)\n            response = s3.get_bucket_lifecycle(Bucket=\"mybucket\")\n        # Each Transition member should have at least one of the\n        # transitions provided.\n        self.assertEqual(\n            response[\"Rules\"][0][\"Transition\"],\n            {\"Days\": 40, \"StorageClass\": \"STANDARD_IA\"},\n        )\n        self.assertEqual(\n            response[\"Rules\"][1][\"NoncurrentVersionTransition\"],\n            {\"NoncurrentDays\": 40, \"StorageClass\": \"STANDARD_IA\"},\n        )\n\n\nclass TestS3PutObject(BaseS3OperationTest):\n    def test_500_error_with_non_xml_body(self):\n        # Note: This exact tesdict may not be applicable from\n        # an integration standpoint if the issue is fixed in the future.\n        #\n        # The issue is that:\n        # S3 returns a 200 response but the received response from urllib3 has\n        # a 500 status code and the headers are in the body of the\n        # the response. Botocore will try to parse out the error body as xml,\n        # but the body is invalid xml because it is full of headers.\n        # So instead of blowing up on an XML parsing error, we\n        # should at least use the 500 status code because that can be\n        # retried.\n        #\n        # We are unsure of what exactly causes the response to be mangled\n        # but we expect it to be how 100 continues are handled.\n        non_xml_content = (\n            b\"x-amz-id-2: foo\\r\\n\"\n            b\"x-amz-request-id: bar\\n\"\n            b\"Date: Tue, 06 Oct 2015 03:20:38 GMT\\r\\n\"\n            b'ETag: \"a6d856bc171fc6aa1b236680856094e2\"\\r\\n'\n            b\"Content-Length: 0\\r\\n\"\n            b\"Server: AmazonS3\\r\\n\"\n        )\n        s3 = self.session.create_client(\"s3\")\n        with ClientHTTPStubber(s3) as http_stubber:\n            http_stubber.add_response(status=500, body=non_xml_content)\n            http_stubber.add_response()\n            response = s3.put_object(\n                Bucket=\"mybucket\", Key=\"mykey\", Body=b\"foo\"\n            )\n            # The first response should have been retried even though the xml is\n            # invalid and eventually return the 200 response.\n            self.assertEqual(\n                response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200\n            )\n            self.assertEqual(len(http_stubber.requests), 2)\n\n\nclass TestWriteGetObjectResponse(BaseS3ClientConfigurationTest):\n    def create_stubbed_s3_client(self, **kwargs):\n        client = self.create_s3_client(**kwargs)\n        http_stubber = ClientHTTPStubber(client)\n        http_stubber.start()\n        return client, http_stubber\n\n    def test_endpoint_redirection(self):\n        regions = [\"us-west-2\", \"us-east-1\"]\n        for region in regions:\n            self.client, self.http_stubber = self.create_stubbed_s3_client(\n                region_name=region\n            )\n            self.http_stubber.add_response()\n            self.client.write_get_object_response(\n                RequestRoute=\"endpoint-io.a1c1d5c7\",\n                RequestToken=\"SecretToken\",\n            )\n            request = self.http_stubber.requests[0]\n            self.assert_signing_name(request, \"s3-object-lambda\")\n            self.assert_signing_region(request, region)\n            expected_endpoint = (\n                \"endpoint-io.a1c1d5c7.s3-object-lambda.\"\n                \"%s.amazonaws.com\" % region\n            )\n            self.assert_endpoint(request, expected_endpoint)\n\n    def test_endpoint_redirection_fails_with_custom_endpoint(self):\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-west-2\", endpoint_url=\"https://example.com\"\n        )\n        self.http_stubber.add_response()\n        self.client.write_get_object_response(\n            RequestRoute=\"endpoint-io.a1c1d5c7\",\n            RequestToken=\"SecretToken\",\n        )\n        request = self.http_stubber.requests[0]\n        self.assert_signing_name(request, \"s3-object-lambda\")\n        self.assert_signing_region(request, \"us-west-2\")\n        self.assert_endpoint(request, \"endpoint-io.a1c1d5c7.example.com\")\n\n    def test_endpoint_redirection_fails_with_accelerate_endpoint(self):\n        config = Config(s3={\"use_accelerate_endpoint\": True})\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-west-2\",\n            config=config,\n        )\n        self.http_stubber.add_response()\n        with self.assertRaises(UnsupportedS3ConfigurationError):\n            self.client.write_get_object_response(\n                RequestRoute=\"endpoint-io.a1c1d5c7\",\n                RequestToken=\"SecretToken\",\n            )\n\n    def test_invalid_request_route_raises(self):\n        self.client, self.http_stubber = self.create_stubbed_s3_client(\n            region_name=\"us-west-2\"\n        )\n        self.http_stubber.add_response()\n        with self.assertRaises(ParamValidationError):\n            self.client.write_get_object_response(\n                RequestRoute=\"my-route/\",\n                RequestToken=\"SecretToken\",\n            )\n\n\nclass TestS3SigV4(BaseS3OperationTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client(\n            \"s3\", self.region, config=Config(signature_version=\"s3v4\")\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n        self.http_stubber.add_response()\n\n    def get_sent_headers(self):\n        return self.http_stubber.requests[0].headers\n\n    def test_content_md5_set(self):\n        with self.http_stubber:\n            self.client.put_object(Bucket=\"foo\", Key=\"bar\", Body=\"baz\")\n        self.assertIn(\"content-md5\", self.get_sent_headers())\n\n    def test_content_md5_set_empty_body(self):\n        with self.http_stubber:\n            self.client.put_object(Bucket=\"foo\", Key=\"bar\", Body=\"\")\n        self.assertIn(\"content-md5\", self.get_sent_headers())\n\n    def test_content_md5_set_empty_file(self):\n        with self.http_stubber:\n            with temporary_file(\"rb\") as f:\n                assert f.read() == b\"\"\n                self.client.put_object(Bucket=\"foo\", Key=\"bar\", Body=f)\n        self.assertIn(\"content-md5\", self.get_sent_headers())\n\n    def test_content_sha256_set_if_config_value_is_true(self):\n        # By default, put_object() does not include an x-amz-content-sha256\n        # header because it also includes a `Content-MD5` header. The\n        # `payload_signing_enabled` config overrides this logic and forces the\n        # header.\n        config = Config(\n            signature_version=\"s3v4\", s3={\"payload_signing_enabled\": True}\n        )\n        self.client = self.session.create_client(\n            \"s3\", self.region, config=config\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n        self.http_stubber.add_response()\n        with self.http_stubber:\n            self.client.put_object(Bucket=\"foo\", Key=\"bar\", Body=\"baz\")\n        sent_headers = self.get_sent_headers()\n        sha_header = sent_headers.get(\"x-amz-content-sha256\")\n        self.assertNotEqual(sha_header, b\"UNSIGNED-PAYLOAD\")\n\n    def test_content_sha256_not_set_if_config_value_is_false(self):\n        config = Config(\n            signature_version=\"s3v4\", s3={\"payload_signing_enabled\": False}\n        )\n        self.client = self.session.create_client(\n            \"s3\", self.region, config=config\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n        self.http_stubber.add_response()\n        with self.http_stubber:\n            self.client.put_object(Bucket=\"foo\", Key=\"bar\", Body=\"baz\")\n        sent_headers = self.get_sent_headers()\n        sha_header = sent_headers.get(\"x-amz-content-sha256\")\n        self.assertEqual(sha_header, b\"UNSIGNED-PAYLOAD\")\n\n    def test_content_sha256_set_if_config_value_not_set_put_object(self):\n        # The default behavior matches payload_signing_enabled=False. For\n        # operations where the `Content-MD5` is present this means that\n        # `x-amz-content-sha256` is present but not set.\n        config = Config(signature_version=\"s3v4\")\n        self.client = self.session.create_client(\n            \"s3\", self.region, config=config\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n        self.http_stubber.add_response()\n        with self.http_stubber:\n            self.client.put_object(Bucket=\"foo\", Key=\"bar\", Body=\"baz\")\n        sent_headers = self.get_sent_headers()\n        sha_header = sent_headers.get(\"x-amz-content-sha256\")\n        self.assertEqual(sha_header, b\"UNSIGNED-PAYLOAD\")\n\n    def test_content_sha256_set_if_config_value_not_set_list_objects(self):\n        # The default behavior matches payload_signing_enabled=False. For\n        # operations where the `Content-MD5` is not present, this means that\n        # `x-amz-content-sha256` is present and set.\n        config = Config(signature_version=\"s3v4\")\n        self.client = self.session.create_client(\n            \"s3\", self.region, config=config\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n        self.http_stubber.add_response()\n        with self.http_stubber:\n            self.client.list_objects(Bucket=\"foo\")\n        sent_headers = self.get_sent_headers()\n        sha_header = sent_headers.get(\"x-amz-content-sha256\")\n        self.assertIsNotNone(sha_header)\n        self.assertNotEqual(sha_header, b\"UNSIGNED-PAYLOAD\")\n\n    def test_content_sha256_set_s3_on_outpost(self):\n        # S3 on Outpost bucket names should behave the same way.\n        config = Config(signature_version=\"s3v4\")\n        bucket = (\n            'test-accessp-e0000075431d83bebde8xz5w8ijx1qzlbp3i3kuse10--op-s3'\n        )\n        self.client = self.session.create_client(\n            \"s3\", self.region, config=config\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n        self.http_stubber.add_response()\n        with self.http_stubber:\n            self.client.list_objects(Bucket=bucket)\n        sent_headers = self.get_sent_headers()\n        sha_header = sent_headers.get(\"x-amz-content-sha256\")\n        self.assertNotEqual(sha_header, b\"UNSIGNED-PAYLOAD\")\n\n    def test_content_sha256_set_if_md5_is_unavailable(self):\n        with mock.patch(\"botocore.compat.MD5_AVAILABLE\", False):\n            with mock.patch(\"botocore.utils.MD5_AVAILABLE\", False):\n                with self.http_stubber:\n                    self.client.put_object(Bucket=\"foo\", Key=\"bar\", Body=\"baz\")\n        sent_headers = self.get_sent_headers()\n        unsigned = \"UNSIGNED-PAYLOAD\"\n        self.assertNotEqual(sent_headers[\"x-amz-content-sha256\"], unsigned)\n        self.assertNotIn(\"content-md5\", sent_headers)\n\n\nclass TestCanSendIntegerHeaders(BaseSessionTest):\n    def test_int_values_with_sigv4(self):\n        s3 = self.session.create_client(\n            \"s3\", config=Config(signature_version=\"s3v4\")\n        )\n        with ClientHTTPStubber(s3) as http_stubber:\n            http_stubber.add_response()\n            s3.upload_part(\n                Bucket=\"foo\",\n                Key=\"bar\",\n                Body=b\"foo\",\n                UploadId=\"bar\",\n                PartNumber=1,\n                ContentLength=3,\n            )\n            headers = http_stubber.requests[0].headers\n            # Verify that the request integer value of 3 has been converted to\n            # string '3'.  This also means we've made it pass the signer which\n            # expects string values in order to sign properly.\n            self.assertEqual(headers[\"Content-Length\"], b\"3\")\n\n\nclass TestRegionRedirect(BaseS3OperationTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client(\n            \"s3\",\n            \"us-west-2\",\n            config=Config(\n                signature_version=\"s3v4\",\n                s3={\"addressing_style\": \"path\"},\n            ),\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n        self.redirect_response = {\n            \"status\": 301,\n            \"headers\": {\"x-amz-bucket-region\": \"eu-central-1\"},\n            \"body\": (\n                b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n                b\"<Error>\"\n                b\"    <Code>PermanentRedirect</Code>\"\n                b\"    <Message>The bucket you are attempting to access must be\"\n                b\"        addressed using the specified endpoint. Please send \"\n                b\"        all future requests to this endpoint.\"\n                b\"    </Message>\"\n                b\"    <Bucket>foo</Bucket>\"\n                b\"    <Endpoint>foo.s3.eu-central-1.amazonaws.com</Endpoint>\"\n                b\"</Error>\"\n            ),\n        }\n        self.bad_signing_region_response = {\n            \"status\": 400,\n            \"headers\": {\"x-amz-bucket-region\": \"eu-central-1\"},\n            \"body\": (\n                b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n                b\"<Error>\"\n                b\"  <Code>AuthorizationHeaderMalformed</Code>\"\n                b\"  <Message>the region us-west-2 is wrong; \"\n                b\"expecting eu-central-1</Message>\"\n                b\"  <Region>eu-central-1</Region>\"\n                b\"  <RequestId>BD9AA1730D454E39</RequestId>\"\n                b\"  <HostId></HostId>\"\n                b\"</Error>\"\n            ),\n        }\n        self.success_response = {\n            \"status\": 200,\n            \"headers\": {},\n            \"body\": (\n                b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n                b\"<ListBucketResult>\"\n                b\"    <Name>foo</Name>\"\n                b\"    <Prefix></Prefix>\"\n                b\"    <Marker></Marker>\"\n                b\"    <MaxKeys>1000</MaxKeys>\"\n                b\"    <EncodingType>url</EncodingType>\"\n                b\"    <IsTruncated>false</IsTruncated>\"\n                b\"</ListBucketResult>\"\n            ),\n        }\n\n    def test_region_redirect(self):\n        self.http_stubber.add_response(**self.redirect_response)\n        self.http_stubber.add_response(**self.success_response)\n        with self.http_stubber:\n            response = self.client.list_objects(Bucket=\"foo\")\n        self.assertEqual(response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200)\n        self.assertEqual(len(self.http_stubber.requests), 2)\n\n        initial_url = (\n            \"https://s3.us-west-2.amazonaws.com/foo\" \"?encoding-type=url\"\n        )\n        self.assertEqual(self.http_stubber.requests[0].url, initial_url)\n\n        fixed_url = (\n            \"https://s3.eu-central-1.amazonaws.com/foo\" \"?encoding-type=url\"\n        )\n        self.assertEqual(self.http_stubber.requests[1].url, fixed_url)\n\n    def test_region_redirect_cache(self):\n        self.http_stubber.add_response(**self.redirect_response)\n        self.http_stubber.add_response(**self.success_response)\n        self.http_stubber.add_response(**self.success_response)\n\n        with self.http_stubber:\n            first_response = self.client.list_objects(Bucket=\"foo\")\n            second_response = self.client.list_objects(Bucket=\"foo\")\n\n        self.assertEqual(\n            first_response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200\n        )\n        self.assertEqual(\n            second_response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200\n        )\n\n        self.assertEqual(len(self.http_stubber.requests), 3)\n        initial_url = (\n            \"https://s3.us-west-2.amazonaws.com/foo\" \"?encoding-type=url\"\n        )\n        self.assertEqual(self.http_stubber.requests[0].url, initial_url)\n\n        fixed_url = (\n            \"https://s3.eu-central-1.amazonaws.com/foo\" \"?encoding-type=url\"\n        )\n        self.assertEqual(self.http_stubber.requests[1].url, fixed_url)\n        self.assertEqual(self.http_stubber.requests[2].url, fixed_url)\n\n    def test_resign_request_with_region_when_needed(self):\n        # Create a client with no explicit configuration so we can\n        # verify the default behavior.\n        client = self.session.create_client(\"s3\", \"us-west-2\")\n        with ClientHTTPStubber(client) as http_stubber:\n            http_stubber.add_response(**self.bad_signing_region_response)\n            http_stubber.add_response(**self.success_response)\n            first_response = client.list_objects(Bucket=\"foo\")\n            self.assertEqual(\n                first_response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200\n            )\n\n            self.assertEqual(len(http_stubber.requests), 2)\n            initial_url = (\n                \"https://foo.s3.us-west-2.amazonaws.com/\" \"?encoding-type=url\"\n            )\n            self.assertEqual(http_stubber.requests[0].url, initial_url)\n\n            fixed_url = (\n                \"https://foo.s3.eu-central-1.amazonaws.com/\"\n                \"?encoding-type=url\"\n            )\n            self.assertEqual(http_stubber.requests[1].url, fixed_url)\n\n    def test_resign_request_in_us_east_1(self):\n        region_headers = {\"x-amz-bucket-region\": \"eu-central-1\"}\n\n        # Verify that the default behavior in us-east-1 will redirect\n        client = self.session.create_client(\"s3\", \"us-east-1\")\n        with ClientHTTPStubber(client) as http_stubber:\n            http_stubber.add_response(status=400)\n            http_stubber.add_response(status=400, headers=region_headers)\n            http_stubber.add_response(headers=region_headers)\n            http_stubber.add_response()\n            response = client.head_object(Bucket=\"foo\", Key=\"bar\")\n            self.assertEqual(\n                response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200\n            )\n\n            self.assertEqual(len(http_stubber.requests), 4)\n            initial_url = \"https://foo.s3.amazonaws.com/bar\"\n            self.assertEqual(http_stubber.requests[0].url, initial_url)\n\n            fixed_url = \"https://foo.s3.eu-central-1.amazonaws.com/bar\"\n            self.assertEqual(http_stubber.requests[-1].url, fixed_url)\n\n    def test_resign_request_in_us_east_1_fails(self):\n        region_headers = {\"x-amz-bucket-region\": \"eu-central-1\"}\n\n        # Verify that the final 400 response is propagated\n        # back to the user.\n        client = self.session.create_client(\"s3\", \"us-east-1\")\n        with ClientHTTPStubber(client) as http_stubber:\n            http_stubber.add_response(status=400)\n            http_stubber.add_response(status=400, headers=region_headers)\n            http_stubber.add_response(headers=region_headers)\n            # The final request still fails with a 400.\n            http_stubber.add_response(status=400)\n            with self.assertRaises(ClientError):\n                client.head_object(Bucket=\"foo\", Key=\"bar\")\n            self.assertEqual(len(http_stubber.requests), 4)\n\n    def test_no_region_redirect_for_accesspoint(self):\n        self.http_stubber.add_response(**self.redirect_response)\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        with self.http_stubber:\n            try:\n                self.client.list_objects(Bucket=accesspoint_arn)\n            except self.client.exceptions.ClientError as e:\n                self.assertEqual(\n                    e.response[\"Error\"][\"Code\"], \"PermanentRedirect\"\n                )\n            else:\n                self.fail(\"PermanentRedirect error should have been raised\")\n\n\nclass TestFipsRegionRedirect(BaseS3OperationTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client(\n            \"s3\",\n            \"fips-us-west-2\",\n            config=Config(signature_version=\"s3v4\"),\n        )\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n        self.redirect_response = {\n            \"status\": 301,\n            \"headers\": {\"x-amz-bucket-region\": \"us-west-1\"},\n            \"body\": (\n                b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n                b\"<Error>\"\n                b\"    <Code>PermanentRedirect</Code>\"\n                b\"    <Message>The bucket you are attempting to access must be\"\n                b\"        addressed using the specified endpoint. Please send \"\n                b\"        all future requests to this endpoint.\"\n                b\"    </Message>\"\n                b\"    <Bucket>foo</Bucket>\"\n                b\"    <Endpoint>foo.s3-fips.us-west-1.amazonaws.com</Endpoint>\"\n                b\"</Error>\"\n            ),\n        }\n        self.success_response = {\n            \"status\": 200,\n            \"headers\": {},\n            \"body\": (\n                b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n                b\"<ListBucketResult>\"\n                b\"    <Name>foo</Name>\"\n                b\"    <Prefix></Prefix>\"\n                b\"    <Marker></Marker>\"\n                b\"    <MaxKeys>1000</MaxKeys>\"\n                b\"    <EncodingType>url</EncodingType>\"\n                b\"    <IsTruncated>false</IsTruncated>\"\n                b\"</ListBucketResult>\"\n            ),\n        }\n        self.bad_signing_region_response = {\n            \"status\": 400,\n            \"headers\": {\"x-amz-bucket-region\": \"us-west-1\"},\n            \"body\": (\n                b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n                b\"<Error>\"\n                b\"  <Code>AuthorizationHeaderMalformed</Code>\"\n                b\"  <Message>the region us-west-2 is wrong; \"\n                b\"expecting us-west-1</Message>\"\n                b\"  <Region>us-west-1</Region>\"\n                b\"  <RequestId>BD9AA1730D454E39</RequestId>\"\n                b\"  <HostId></HostId>\"\n                b\"</Error>\"\n            ),\n        }\n\n    def test_fips_region_redirect(self):\n        self.http_stubber.add_response(**self.redirect_response)\n        self.http_stubber.add_response(**self.success_response)\n        with self.http_stubber:\n            response = self.client.list_objects(Bucket=\"foo\")\n        self.assertEqual(response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200)\n        self.assertEqual(len(self.http_stubber.requests), 2)\n\n        initial_url = (\n            \"https://foo.s3-fips.us-west-2.amazonaws.com/?encoding-type=url\"\n        )\n        self.assertEqual(self.http_stubber.requests[0].url, initial_url)\n\n        fixed_url = (\n            \"https://foo.s3-fips.us-west-1.amazonaws.com/?encoding-type=url\"\n        )\n        self.assertEqual(self.http_stubber.requests[1].url, fixed_url)\n\n    def test_fips_region_redirect_cache(self):\n        self.http_stubber.add_response(**self.redirect_response)\n        self.http_stubber.add_response(**self.success_response)\n        self.http_stubber.add_response(**self.success_response)\n\n        with self.http_stubber:\n            first_response = self.client.list_objects(Bucket=\"foo\")\n            second_response = self.client.list_objects(Bucket=\"foo\")\n\n        self.assertEqual(\n            first_response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200\n        )\n        self.assertEqual(\n            second_response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200\n        )\n\n        self.assertEqual(len(self.http_stubber.requests), 3)\n        initial_url = (\n            \"https://foo.s3-fips.us-west-2.amazonaws.com/?encoding-type=url\"\n        )\n        self.assertEqual(self.http_stubber.requests[0].url, initial_url)\n\n        fixed_url = (\n            \"https://foo.s3-fips.us-west-1.amazonaws.com/?encoding-type=url\"\n        )\n        self.assertEqual(self.http_stubber.requests[1].url, fixed_url)\n        self.assertEqual(self.http_stubber.requests[2].url, fixed_url)\n\n    def test_fips_resign_request_with_region_when_needed(self):\n        # Create a client with no explicit configuration so we can\n        # verify the default behavior.\n        client = self.session.create_client(\"s3\", \"fips-us-west-2\")\n        with ClientHTTPStubber(client) as http_stubber:\n            http_stubber.add_response(**self.bad_signing_region_response)\n            http_stubber.add_response(**self.success_response)\n            first_response = client.list_objects(Bucket=\"foo\")\n            self.assertEqual(\n                first_response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200\n            )\n\n            self.assertEqual(len(http_stubber.requests), 2)\n            initial_url = (\n                \"https://foo.s3-fips.us-west-2.amazonaws.com/\"\n                \"?encoding-type=url\"\n            )\n            self.assertEqual(http_stubber.requests[0].url, initial_url)\n\n            fixed_url = (\n                \"https://foo.s3-fips.us-west-1.amazonaws.com/\"\n                \"?encoding-type=url\"\n            )\n            self.assertEqual(http_stubber.requests[1].url, fixed_url)\n\n    def test_fips_resign_request_in_us_east_1(self):\n        region_headers = {\"x-amz-bucket-region\": \"us-east-2\"}\n\n        # Verify that the default behavior in us-east-1 will redirect\n        client = self.session.create_client(\"s3\", \"fips-us-east-1\")\n        with ClientHTTPStubber(client) as http_stubber:\n            http_stubber.add_response(status=400)\n            http_stubber.add_response(status=400, headers=region_headers)\n            http_stubber.add_response(headers=region_headers)\n            http_stubber.add_response()\n            response = client.head_object(Bucket=\"foo\", Key=\"bar\")\n            self.assertEqual(\n                response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200\n            )\n\n            self.assertEqual(len(http_stubber.requests), 4)\n            initial_url = \"https://foo.s3-fips.us-east-1.amazonaws.com/bar\"\n            self.assertEqual(http_stubber.requests[0].url, initial_url)\n\n            fixed_url = \"https://foo.s3-fips.us-east-2.amazonaws.com/bar\"\n            self.assertEqual(http_stubber.requests[-1].url, fixed_url)\n\n    def test_fips_resign_request_in_us_east_1_fails(self):\n        region_headers = {\"x-amz-bucket-region\": \"us-east-2\"}\n\n        # Verify that the final 400 response is propagated\n        # back to the user.\n        client = self.session.create_client(\"s3\", \"fips-us-east-1\")\n        with ClientHTTPStubber(client) as http_stubber:\n            http_stubber.add_response(status=400)\n            http_stubber.add_response(status=400, headers=region_headers)\n            http_stubber.add_response(headers=region_headers)\n            # The final request still fails with a 400.\n            http_stubber.add_response(status=400)\n            with self.assertRaises(ClientError):\n                client.head_object(Bucket=\"foo\", Key=\"bar\")\n            self.assertEqual(len(http_stubber.requests), 4)\n\n    def test_fips_no_region_redirect_for_accesspoint(self):\n        self.http_stubber.add_response(**self.redirect_response)\n        accesspoint_arn = (\n            \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\n        )\n        with self.http_stubber:\n            try:\n                self.client.list_objects(Bucket=accesspoint_arn)\n            except self.client.exceptions.ClientError as e:\n                self.assertEqual(\n                    e.response[\"Error\"][\"Code\"], \"PermanentRedirect\"\n                )\n            else:\n                self.fail(\"PermanentRedirect error should have been raised\")\n\n\nclass TestGeneratePresigned(BaseS3OperationTest):\n    def assert_is_v2_presigned_url(self, url):\n        qs_components = parse_qs(urlsplit(url).query)\n        # Assert that it looks like a v2 presigned url by asserting it does\n        # not have a couple of the v4 qs components and assert that it has the\n        # v2 Signature component.\n        self.assertNotIn(\"X-Amz-Credential\", qs_components)\n        self.assertNotIn(\"X-Amz-Algorithm\", qs_components)\n        self.assertIn(\"Signature\", qs_components)\n\n    def test_generate_unauthed_url(self):\n        config = Config(signature_version=botocore.UNSIGNED)\n        client = self.session.create_client(\"s3\", self.region, config=config)\n        url = client.generate_presigned_url(\n            ClientMethod=\"get_object\", Params={\"Bucket\": \"foo\", \"Key\": \"bar\"}\n        )\n        self.assertEqual(url, \"https://foo.s3.amazonaws.com/bar\")\n\n    def test_generate_unauthed_post(self):\n        config = Config(signature_version=botocore.UNSIGNED)\n        client = self.session.create_client(\"s3\", self.region, config=config)\n        parts = client.generate_presigned_post(Bucket=\"foo\", Key=\"bar\")\n        expected = {\n            \"fields\": {\"key\": \"bar\"},\n            \"url\": \"https://foo.s3.amazonaws.com/\",\n        }\n        self.assertEqual(parts, expected)\n\n    def test_default_presign_uses_sigv2(self):\n        url = self.client.generate_presigned_url(ClientMethod=\"list_buckets\")\n        self.assertNotIn(\"Algorithm=AWS4-HMAC-SHA256\", url)\n\n    def test_sigv4_presign(self):\n        config = Config(signature_version=\"s3v4\")\n        client = self.session.create_client(\"s3\", self.region, config=config)\n        url = client.generate_presigned_url(ClientMethod=\"list_buckets\")\n        self.assertIn(\"Algorithm=AWS4-HMAC-SHA256\", url)\n\n    def test_sigv2_presign(self):\n        config = Config(signature_version=\"s3\")\n        client = self.session.create_client(\"s3\", self.region, config=config)\n        url = client.generate_presigned_url(ClientMethod=\"list_buckets\")\n        self.assertNotIn(\"Algorithm=AWS4-HMAC-SHA256\", url)\n\n    def test_uses_sigv4_for_unknown_region(self):\n        client = self.session.create_client(\"s3\", \"us-west-88\")\n        url = client.generate_presigned_url(ClientMethod=\"list_buckets\")\n        self.assertIn(\"Algorithm=AWS4-HMAC-SHA256\", url)\n\n    def test_default_presign_sigv4_in_sigv4_only_region(self):\n        client = self.session.create_client(\"s3\", \"us-east-2\")\n        url = client.generate_presigned_url(ClientMethod=\"list_buckets\")\n        self.assertIn(\"Algorithm=AWS4-HMAC-SHA256\", url)\n\n    def test_presign_unsigned(self):\n        config = Config(signature_version=botocore.UNSIGNED)\n        client = self.session.create_client(\"s3\", \"us-east-2\", config=config)\n        url = client.generate_presigned_url(ClientMethod=\"list_buckets\")\n        self.assertEqual(\"https://s3.amazonaws.com/\", url)\n\n    def test_presign_url_with_ssec(self):\n        config = Config(signature_version=\"s3\")\n        client = self.session.create_client(\"s3\", \"us-east-1\", config=config)\n        url = client.generate_presigned_url(\n            ClientMethod=\"get_object\",\n            Params={\n                \"Bucket\": \"mybucket\",\n                \"Key\": \"mykey\",\n                \"SSECustomerKey\": \"a\" * 32,\n                \"SSECustomerAlgorithm\": \"AES256\",\n            },\n        )\n        # The md5 of the sse-c key will be injected when parameters are\n        # built so it should show up in the presigned url as well.\n        self.assertIn(\"x-amz-server-side-encryption-customer-key-md5=\", url)\n\n    def test_presign_s3_accelerate(self):\n        config = Config(\n            signature_version=botocore.UNSIGNED,\n            s3={\"use_accelerate_endpoint\": True},\n        )\n        client = self.session.create_client(\"s3\", \"us-east-1\", config=config)\n        url = client.generate_presigned_url(\n            ClientMethod=\"get_object\",\n            Params={\"Bucket\": \"mybucket\", \"Key\": \"mykey\"},\n        )\n        # The url should be the accelerate endpoint\n        self.assertEqual(\n            \"https://mybucket.s3-accelerate.amazonaws.com/mykey\", url\n        )\n\n    def test_presign_s3_accelerate_fails_with_fips(self):\n        config = Config(\n            signature_version=botocore.UNSIGNED,\n            s3={\"use_accelerate_endpoint\": True},\n        )\n        client = self.session.create_client(\n            \"s3\", \"fips-us-east-1\", config=config\n        )\n        expected_exception = UnsupportedS3ConfigurationError\n        with self.assertRaisesRegex(\n            expected_exception, \"Accelerate cannot be used with FIPS\"\n        ):\n            client.generate_presigned_url(\n                ClientMethod=\"get_object\",\n                Params={\"Bucket\": \"mybucket\", \"Key\": \"mykey\"},\n            )\n\n    def test_presign_post_s3_accelerate(self):\n        config = Config(\n            signature_version=botocore.UNSIGNED,\n            s3={\"use_accelerate_endpoint\": True},\n        )\n        client = self.session.create_client(\"s3\", \"us-east-1\", config=config)\n        parts = client.generate_presigned_post(Bucket=\"mybucket\", Key=\"mykey\")\n        # The url should be the accelerate endpoint\n        expected = {\n            \"fields\": {\"key\": \"mykey\"},\n            \"url\": \"https://mybucket.s3-accelerate.amazonaws.com/\",\n        }\n        self.assertEqual(parts, expected)\n\n    def test_presign_uses_v2_for_aws_global(self):\n        client = self.session.create_client(\"s3\", \"aws-global\")\n        url = client.generate_presigned_url(\n            \"get_object\", {\"Bucket\": \"mybucket\", \"Key\": \"mykey\"}\n        )\n        self.assert_is_v2_presigned_url(url)\n\n    def test_presign_uses_v2_for_default_region_with_us_east_1_regional(self):\n        config = Config(s3={\"us_east_1_regional_endpoint\": \"regional\"})\n        client = self.session.create_client(\"s3\", config=config)\n        url = client.generate_presigned_url(\n            \"get_object\", {\"Bucket\": \"mybucket\", \"Key\": \"mykey\"}\n        )\n        self.assert_is_v2_presigned_url(url)\n\n    def test_presign_uses_v2_for_aws_global_with_us_east_1_regional(self):\n        config = Config(s3={\"us_east_1_regional_endpoint\": \"regional\"})\n        client = self.session.create_client(\"s3\", \"aws-global\", config=config)\n        url = client.generate_presigned_url(\n            \"get_object\", {\"Bucket\": \"mybucket\", \"Key\": \"mykey\"}\n        )\n        self.assert_is_v2_presigned_url(url)\n\n    def test_presign_uses_v2_for_us_east_1(self):\n        client = self.session.create_client(\"s3\", \"us-east-1\")\n        url = client.generate_presigned_url(\n            \"get_object\", {\"Bucket\": \"mybucket\", \"Key\": \"mykey\"}\n        )\n        self.assert_is_v2_presigned_url(url)\n\n    def test_presign_uses_v2_for_us_east_1_with_us_east_1_regional(self):\n        config = Config(s3={\"us_east_1_regional_endpoint\": \"regional\"})\n        client = self.session.create_client(\"s3\", \"us-east-1\", config=config)\n        url = client.generate_presigned_url(\n            \"get_object\", {\"Bucket\": \"mybucket\", \"Key\": \"mykey\"}\n        )\n        self.assert_is_v2_presigned_url(url)\n\n\nCHECKSUM_TEST_CASES = [\n    (\"put_bucket_tagging\", {\"Bucket\": \"foo\", \"Tagging\": {\"TagSet\": []}}),\n    (\n        \"put_bucket_lifecycle\",\n        {\"Bucket\": \"foo\", \"LifecycleConfiguration\": {\"Rules\": []}},\n    ),\n    (\n        \"put_bucket_lifecycle_configuration\",\n        {\"Bucket\": \"foo\", \"LifecycleConfiguration\": {\"Rules\": []}},\n    ),\n    (\n        \"put_bucket_cors\",\n        {\"Bucket\": \"foo\", \"CORSConfiguration\": {\"CORSRules\": []}},\n    ),\n    (\n        \"delete_objects\",\n        {\"Bucket\": \"foo\", \"Delete\": {\"Objects\": [{\"Key\": \"bar\"}]}},\n    ),\n    (\n        \"put_bucket_replication\",\n        {\n            \"Bucket\": \"foo\",\n            \"ReplicationConfiguration\": {\"Role\": \"\", \"Rules\": []},\n        },\n    ),\n    (\"put_bucket_acl\", {\"Bucket\": \"foo\", \"AccessControlPolicy\": {}}),\n    (\"put_bucket_logging\", {\"Bucket\": \"foo\", \"BucketLoggingStatus\": {}}),\n    (\n        \"put_bucket_notification\",\n        {\"Bucket\": \"foo\", \"NotificationConfiguration\": {}},\n    ),\n    (\"put_bucket_policy\", {\"Bucket\": \"foo\", \"Policy\": \"<bucket-policy>\"}),\n    (\n        \"put_bucket_request_payment\",\n        {\"Bucket\": \"foo\", \"RequestPaymentConfiguration\": {\"Payer\": \"\"}},\n    ),\n    (\n        \"put_bucket_versioning\",\n        {\"Bucket\": \"foo\", \"VersioningConfiguration\": {}},\n    ),\n    (\"put_bucket_website\", {\"Bucket\": \"foo\", \"WebsiteConfiguration\": {}}),\n    (\n        \"put_object_acl\",\n        {\"Bucket\": \"foo\", \"Key\": \"bar\", \"AccessControlPolicy\": {}},\n    ),\n    (\n        \"put_object_legal_hold\",\n        {\"Bucket\": \"foo\", \"Key\": \"bar\", \"LegalHold\": {\"Status\": \"ON\"}},\n    ),\n    (\n        \"put_object_retention\",\n        {\n            \"Bucket\": \"foo\",\n            \"Key\": \"bar\",\n            \"Retention\": {\"RetainUntilDate\": \"2020-11-05\"},\n        },\n    ),\n    (\n        \"put_object_lock_configuration\",\n        {\"Bucket\": \"foo\", \"ObjectLockConfiguration\": {}},\n    ),\n]\n\naccesspoint_arn = \"arn:aws:s3:us-west-2:123456789012:accesspoint:myendpoint\"\naccesspoint_arn_cn = (\n    \"arn:aws-cn:s3:cn-north-1:123456789012:accesspoint:myendpoint\"\n)\naccesspoint_arn_gov = (\n    \"arn:aws-us-gov:s3:us-gov-west-1:123456789012:accesspoint:myendpoint\"\n)\naccesspoint_cross_region_arn_gov = (\n    \"arn:aws-us-gov:s3:us-gov-east-1:123456789012:accesspoint:myendpoint\"\n)\n\n\n@pytest.mark.parametrize(\"operation, operation_kwargs\", CHECKSUM_TEST_CASES)\ndef test_checksums_included_in_expected_operations(\n    operation, operation_kwargs\n):\n    \"\"\"Validate expected calls include Content-MD5 header\"\"\"\n    client = _create_s3_client()\n    with ClientHTTPStubber(client) as stub:\n        stub.add_response()\n        call = getattr(client, operation)\n        call(**operation_kwargs)\n        assert \"Content-MD5\" in stub.requests[-1].headers\n\n\n@pytest.mark.parametrize(\n    \"content_encoding, expected_header\",\n    [(\"foo\", b\"foo,aws-chunked\"), (None, b\"aws-chunked\")],\n)\ndef test_checksum_content_encoding(content_encoding, expected_header):\n    op_kwargs = {\n        \"Bucket\": \"mybucket\",\n        \"Key\": \"mykey\",\n        \"Body\": b\"foo\",\n        \"ChecksumAlgorithm\": \"sha256\",\n    }\n    if content_encoding is not None:\n        op_kwargs[\"ContentEncoding\"] = content_encoding\n    s3 = _create_s3_client()\n    with ClientHTTPStubber(s3) as http_stubber:\n        http_stubber.add_response()\n        s3.put_object(**op_kwargs)\n        request_headers = http_stubber.requests[-1].headers\n        assert request_headers[\"Content-Encoding\"] == expected_header\n\n\ndef _s3_addressing_test_cases():\n    # The default behavior for sigv2. DNS compatible buckets\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://bucket.s3.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://bucket.s3.us-west-1.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        is_secure=False,\n        expected_url=\"http://bucket.s3.us-west-1.amazonaws.com/key\",\n    )\n\n    # Virtual host addressing is independent of signature version.\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.us-west-1.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        is_secure=False,\n        expected_url=\"http://bucket.s3.us-west-1.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"bucket-with-num-1\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        is_secure=False,\n        expected_url=\"http://bucket-with-num-1.s3.us-west-1.amazonaws.com/key\",\n    )\n\n    # Regions outside of the 'aws' partition.\n    # These should still default to virtual hosted addressing\n    # unless explicitly configured otherwise.\n    yield dict(\n        region=\"cn-north-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.cn-north-1.amazonaws.com.cn/key\",\n    )\n    # This isn't actually supported because cn-north-1 is sigv4 only,\n    # but we'll still double check that our internal logic is correct\n    # when building the expected url.\n    yield dict(\n        region=\"cn-north-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://bucket.s3.cn-north-1.amazonaws.com.cn/key\",\n    )\n    # If the request is unsigned, we should have the default\n    # fix_s3_host behavior which is to use virtual hosting where\n    # possible but fall back to path style when needed.\n    yield dict(\n        region=\"cn-north-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=UNSIGNED,\n        expected_url=\"https://bucket.s3.cn-north-1.amazonaws.com.cn/key\",\n    )\n    yield dict(\n        region=\"cn-north-1\",\n        bucket=\"bucket.dot\",\n        key=\"key\",\n        signature_version=UNSIGNED,\n        expected_url=\"https://s3.cn-north-1.amazonaws.com.cn/bucket.dot/key\",\n    )\n\n    # And of course you can explicitly specify which style to use.\n    virtual_hosting = {\"addressing_style\": \"virtual\"}\n    yield dict(\n        region=\"cn-north-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=UNSIGNED,\n        s3_config=virtual_hosting,\n        expected_url=\"https://bucket.s3.cn-north-1.amazonaws.com.cn/key\",\n    )\n\n    path_style = {\"addressing_style\": \"path\"}\n    yield dict(\n        region=\"cn-north-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=UNSIGNED,\n        s3_config=path_style,\n        expected_url=\"https://s3.cn-north-1.amazonaws.com.cn/bucket/key\",\n    )\n\n    # If you don't have a DNS compatible bucket, we use path style.\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket.dot\",\n        key=\"key\",\n        expected_url=\"https://s3.us-west-2.amazonaws.com/bucket.dot/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket.dot\",\n        key=\"key\",\n        expected_url=\"https://s3.amazonaws.com/bucket.dot/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"BucketName\",\n        key=\"key\",\n        expected_url=\"https://s3.amazonaws.com/BucketName/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"bucket_name\",\n        key=\"key\",\n        expected_url=\"https://s3.us-west-1.amazonaws.com/bucket_name/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"-bucket-name\",\n        key=\"key\",\n        expected_url=\"https://s3.us-west-1.amazonaws.com/-bucket-name/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"bucket-name-\",\n        key=\"key\",\n        expected_url=\"https://s3.us-west-1.amazonaws.com/bucket-name-/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"aa\",\n        key=\"key\",\n        expected_url=\"https://s3.us-west-1.amazonaws.com/aa/key\",\n    )\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"a\" * 64,\n        key=\"key\",\n        expected_url=(\n            \"https://s3.us-west-1.amazonaws.com/%s/key\" % (\"a\" * 64)\n        ),\n    )\n\n    # Custom endpoint url should always be used.\n    yield dict(\n        customer_provided_endpoint=\"https://my-custom-s3/\",\n        bucket=\"foo\",\n        key=\"bar\",\n        expected_url=\"https://my-custom-s3/foo/bar\",\n    )\n    yield dict(\n        customer_provided_endpoint=\"https://my-custom-s3/\",\n        bucket=\"bucket.dots\",\n        key=\"bar\",\n        expected_url=\"https://my-custom-s3/bucket.dots/bar\",\n    )\n    # Doesn't matter what region you specify, a custom endpoint url always\n    # wins.\n    yield dict(\n        customer_provided_endpoint=\"https://my-custom-s3/\",\n        region=\"us-west-2\",\n        bucket=\"foo\",\n        key=\"bar\",\n        expected_url=\"https://my-custom-s3/foo/bar\",\n    )\n\n    # Explicitly configuring \"virtual\" addressing_style.\n    virtual_hosting = {\"addressing_style\": \"virtual\"}\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=virtual_hosting,\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=virtual_hosting,\n        expected_url=\"https://bucket.s3.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"eu-central-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=virtual_hosting,\n        expected_url=\"https://bucket.s3.eu-central-1.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=virtual_hosting,\n        customer_provided_endpoint=\"https://foo.amazonaws.com\",\n        expected_url=\"https://bucket.foo.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"unknown\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=virtual_hosting,\n        expected_url=\"https://bucket.s3.unknown.amazonaws.com/key\",\n    )\n\n    # Test us-gov with virtual addressing.\n    yield dict(\n        region=\"us-gov-west-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=virtual_hosting,\n        expected_url=\"https://bucket.s3.us-gov-west-1.amazonaws.com/key\",\n    )\n\n    yield dict(\n        region=\"us-gov-west-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://bucket.s3.us-gov-west-1.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"fips-us-gov-west-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://bucket.s3-fips.us-gov-west-1.amazonaws.com/key\",\n    )\n\n    # Test path style addressing.\n    path_style = {\"addressing_style\": \"path\"}\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=path_style,\n        expected_url=\"https://s3.amazonaws.com/bucket/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=path_style,\n        customer_provided_endpoint=\"https://foo.amazonaws.com/\",\n        expected_url=\"https://foo.amazonaws.com/bucket/key\",\n    )\n    yield dict(\n        region=\"unknown\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=path_style,\n        expected_url=\"https://s3.unknown.amazonaws.com/bucket/key\",\n    )\n\n    # S3 accelerate\n    use_accelerate = {\"use_accelerate_endpoint\": True}\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_accelerate,\n        expected_url=\"https://bucket.s3-accelerate.amazonaws.com/key\",\n    )\n    yield dict(\n        # region is ignored with S3 accelerate.\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_accelerate,\n        expected_url=\"https://bucket.s3-accelerate.amazonaws.com/key\",\n    )\n    # Provided endpoints still get recognized as accelerate endpoints.\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        customer_provided_endpoint=\"https://s3-accelerate.amazonaws.com\",\n        expected_url=\"https://bucket.s3-accelerate.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        customer_provided_endpoint=\"http://s3-accelerate.amazonaws.com\",\n        expected_url=\"http://bucket.s3-accelerate.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_accelerate,\n        is_secure=False,\n        # Note we're using http://  because is_secure=False.\n        expected_url=\"http://bucket.s3-accelerate.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        # s3-accelerate must be the first part of the url.\n        customer_provided_endpoint=\"https://foo.s3-accelerate.amazonaws.com\",\n        expected_url=\"https://foo.s3-accelerate.amazonaws.com/bucket/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        # The endpoint must be an Amazon endpoint.\n        customer_provided_endpoint=\"https://s3-accelerate.notamazon.com\",\n        expected_url=\"https://s3-accelerate.notamazon.com/bucket/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        # Extra components must be whitelisted.\n        customer_provided_endpoint=\"https://s3-accelerate.foo.amazonaws.com\",\n        expected_url=\"https://s3-accelerate.foo.amazonaws.com/bucket/key\",\n    )\n    yield dict(\n        region=\"unknown\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_accelerate,\n        expected_url=\"https://bucket.s3-accelerate.amazonaws.com/key\",\n    )\n    # Use virtual even if path is specified for s3 accelerate because\n    # path style will not work with S3 accelerate.\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config={\n            \"use_accelerate_endpoint\": True,\n            \"addressing_style\": \"path\",\n        },\n        expected_url=\"https://bucket.s3-accelerate.amazonaws.com/key\",\n    )\n\n    # S3 dual stack endpoints.\n    use_dualstack = {\"use_dualstack_endpoint\": True}\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_dualstack,\n        signature_version=\"s3\",\n        # Still default to virtual hosted when possible on sigv2.\n        expected_url=\"https://bucket.s3.dualstack.us-east-1.amazonaws.com/key\",\n    )\n    yield dict(\n        region=None,\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_dualstack,\n        # Uses us-east-1 for no region set.\n        expected_url=\"https://bucket.s3.dualstack.us-east-1.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"aws-global\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_dualstack,\n        # The aws-global pseudo region does not support dualstack and should\n        # be resolved to us-east-1.\n        expected_url=(\n            \"https://bucket.s3.dualstack.us-east-1.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_dualstack,\n        signature_version=\"s3\",\n        # Still default to virtual hosted when possible on sigv2.\n        expected_url=\"https://bucket.s3.dualstack.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_dualstack,\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.dualstack.us-east-1.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_dualstack,\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.dualstack.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"unknown\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_dualstack,\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.dualstack.unknown.amazonaws.com/key\",\n    )\n    # Non DNS compatible buckets use path style for dual stack.\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket.dot\",\n        key=\"key\",\n        s3_config=use_dualstack,\n        # Still default to virtual hosted when possible.\n        expected_url=(\n            \"https://s3.dualstack.us-west-2.amazonaws.com/bucket.dot/key\"\n        ),\n    )\n    # Supports is_secure (use_ssl=False in create_client()).\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket.dot\",\n        key=\"key\",\n        is_secure=False,\n        s3_config=use_dualstack,\n        # Still default to virtual hosted when possible.\n        expected_url=(\n            \"http://s3.dualstack.us-west-2.amazonaws.com/bucket.dot/key\"\n        ),\n    )\n\n    # Is path style is requested, we should use it, even if the bucket is\n    # DNS compatible.\n    force_path_style = {\n        \"use_dualstack_endpoint\": True,\n        \"addressing_style\": \"path\",\n    }\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=force_path_style,\n        # Still default to virtual hosted when possible.\n        expected_url=\"https://s3.dualstack.us-west-2.amazonaws.com/bucket/key\",\n    )\n\n    # Accelerate + dual stack\n    use_accelerate_dualstack = {\n        \"use_accelerate_endpoint\": True,\n        \"use_dualstack_endpoint\": True,\n    }\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_accelerate_dualstack,\n        expected_url=(\n            \"https://bucket.s3-accelerate.dualstack.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        # Region is ignored with S3 accelerate.\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_accelerate_dualstack,\n        expected_url=(\n            \"https://bucket.s3-accelerate.dualstack.amazonaws.com/key\"\n        ),\n    )\n    # Only s3-accelerate overrides a customer endpoint.\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_dualstack,\n        customer_provided_endpoint=\"https://s3-accelerate.amazonaws.com\",\n        expected_url=(\"https://bucket.s3-accelerate.amazonaws.com/key\"),\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        # Dualstack is whitelisted.\n        customer_provided_endpoint=(\n            \"https://s3-accelerate.dualstack.amazonaws.com\"\n        ),\n        expected_url=(\n            \"https://bucket.s3-accelerate.dualstack.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        # Even whitelisted parts cannot be duplicated.\n        customer_provided_endpoint=(\n            \"https://s3-accelerate.dualstack.dualstack.amazonaws.com\"\n        ),\n        expected_url=(\n            \"https://s3-accelerate.dualstack.dualstack\"\n            \".amazonaws.com/bucket/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        # More than two extra parts is not allowed.\n        customer_provided_endpoint=(\n            \"https://s3-accelerate.dualstack.dualstack.dualstack\"\n            \".amazonaws.com\"\n        ),\n        expected_url=(\n            \"https://s3-accelerate.dualstack.dualstack.dualstack.amazonaws.com\"\n            \"/bucket/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        # Extra components must be whitelisted.\n        customer_provided_endpoint=\"https://s3-accelerate.foo.amazonaws.com\",\n        expected_url=\"https://s3-accelerate.foo.amazonaws.com/bucket/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_accelerate_dualstack,\n        is_secure=False,\n        # Note we're using http://  because is_secure=False.\n        expected_url=(\n            \"http://bucket.s3-accelerate.dualstack.amazonaws.com/key\"\n        ),\n    )\n    # Use virtual even if path is specified for s3 accelerate because\n    # path style will not work with S3 accelerate.\n    use_accelerate_dualstack[\"addressing_style\"] = \"path\"\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=use_accelerate_dualstack,\n        expected_url=(\n            \"https://bucket.s3-accelerate.dualstack.amazonaws.com/key\"\n        ),\n    )\n\n    # Access-point arn cases\n    yield dict(\n        region=\"us-west-2\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"use_arn_region\": True},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=accesspoint_arn,\n        key=\"myendpoint/key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/myendpoint/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=accesspoint_arn,\n        key=\"foo/myendpoint/key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/foo/myendpoint/key\"\n        ),\n    )\n    yield dict(\n        # Note: The access-point arn has us-west-2 and the client's region is\n        # us-east-1, for the defauldict the access-point arn region is used.\n        region=\"us-east-1\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"s3-external-1\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"use_arn_region\": True},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n\n    yield dict(\n        region=\"aws-global\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"use_arn_region\": True},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"unknown\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"use_arn_region\": True},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"cn-north-1\",\n        bucket=accesspoint_arn_cn,\n        key=\"key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"cn-north-1.amazonaws.com.cn/key\"\n        ),\n    )\n    yield dict(\n        region=\"cn-northwest-1\",\n        bucket=accesspoint_arn_cn,\n        key=\"key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"cn-north-1.amazonaws.com.cn/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-gov-west-1\",\n        bucket=accesspoint_arn_gov,\n        key=\"key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-gov-west-1.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"fips-us-gov-west-1\",\n        bucket=accesspoint_arn_gov,\n        key=\"key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint-fips.\"\n            \"us-gov-west-1.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"fips-us-gov-west-1\",\n        bucket=accesspoint_arn_gov,\n        key=\"key\",\n        s3_config={\"use_arn_region\": False},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint-fips.\"\n            \"us-gov-west-1.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"fips-us-gov-west-1\",\n        bucket=accesspoint_cross_region_arn_gov,\n        s3_config={\"use_arn_region\": True},\n        key=\"key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint-fips.\"\n            \"us-gov-east-1.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-gov-west-1\",\n        bucket=accesspoint_arn_gov,\n        key=\"key\",\n        s3_config={\"use_arn_region\": False},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint-fips.\"\n            \"us-gov-west-1.amazonaws.com/key\"\n        ),\n        use_fips_endpoint=True,\n    )\n    yield dict(\n        region=\"us-gov-west-1\",\n        bucket=accesspoint_cross_region_arn_gov,\n        key=\"key\",\n        s3_config={\"use_arn_region\": True},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint-fips.\"\n            \"us-gov-east-1.amazonaws.com/key\"\n        ),\n        use_fips_endpoint=True,\n    )\n\n    yield dict(\n        region=\"us-west-2\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        is_secure=False,\n        expected_url=(\n            \"http://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n    # Dual-stack with access-point arn\n    yield dict(\n        # Note: The access-point arn has us-west-2 and the client's region is\n        # us-east-1, for the defauldict the access-point arn region is used.\n        region=\"us-east-1\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\n            \"use_dualstack_endpoint\": True,\n        },\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.dualstack.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-gov-west-1\",\n        bucket=accesspoint_arn_gov,\n        key=\"key\",\n        s3_config={\n            \"use_dualstack_endpoint\": True,\n        },\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.dualstack.\"\n            \"us-gov-west-1.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"fips-us-gov-west-1\",\n        bucket=accesspoint_arn_gov,\n        key=\"key\",\n        s3_config={\n            \"use_arn_region\": True,\n            \"use_dualstack_endpoint\": True,\n        },\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint-fips.dualstack.\"\n            \"us-gov-west-1.amazonaws.com/key\"\n        ),\n    )\n    # None of the various s3 settings related to paths should affect what\n    # endpoint to use when an access-point is provided.\n    yield dict(\n        region=\"us-west-2\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"addressing_style\": \"auto\"},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"addressing_style\": \"virtual\"},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"addressing_style\": \"path\"},\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n\n    # Use us-east-1 regional endpoindicts: regional\n    us_east_1_regional_endpoint = {\"us_east_1_regional_endpoint\": \"regional\"}\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=us_east_1_regional_endpoint,\n        expected_url=(\"https://bucket.s3.us-east-1.amazonaws.com/key\"),\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=us_east_1_regional_endpoint,\n        expected_url=(\"https://bucket.s3.us-west-2.amazonaws.com/key\"),\n    )\n    yield dict(\n        region=None,\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=us_east_1_regional_endpoint,\n        expected_url=(\"https://bucket.s3.amazonaws.com/key\"),\n    )\n    yield dict(\n        region=\"unknown\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=us_east_1_regional_endpoint,\n        expected_url=(\"https://bucket.s3.unknown.amazonaws.com/key\"),\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config={\n            \"us_east_1_regional_endpoint\": \"regional\",\n            \"use_dualstack_endpoint\": True,\n        },\n        expected_url=(\n            \"https://bucket.s3.dualstack.us-east-1.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config={\n            \"us_east_1_regional_endpoint\": \"regional\",\n            \"use_accelerate_endpoint\": True,\n        },\n        expected_url=(\"https://bucket.s3-accelerate.amazonaws.com/key\"),\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config={\n            \"us_east_1_regional_endpoint\": \"regional\",\n            \"use_accelerate_endpoint\": True,\n            \"use_dualstack_endpoint\": True,\n        },\n        expected_url=(\n            \"https://bucket.s3-accelerate.dualstack.amazonaws.com/key\"\n        ),\n    )\n\n    # Use us-east-1 regional endpoindicts: legacy\n    us_east_1_regional_endpoint_legacy = {\n        \"us_east_1_regional_endpoint\": \"legacy\"\n    }\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=us_east_1_regional_endpoint_legacy,\n        expected_url=(\"https://bucket.s3.amazonaws.com/key\"),\n    )\n\n    yield dict(\n        region=None,\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=us_east_1_regional_endpoint_legacy,\n        expected_url=(\"https://bucket.s3.amazonaws.com/key\"),\n    )\n\n    yield dict(\n        region=\"unknown\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=us_east_1_regional_endpoint_legacy,\n        expected_url=(\"https://bucket.s3.unknown.amazonaws.com/key\"),\n    )\n\n    s3_object_lambda_arn_gov = (\n        \"arn:aws-us-gov:s3-object-lambda:us-gov-west-1:\"\n        \"123456789012:accesspoint:mybanner\"\n    )\n    yield dict(\n        region=\"fips-us-gov-west-1\",\n        bucket=s3_object_lambda_arn_gov,\n        key=\"key\",\n        expected_url=(\n            \"https://mybanner-123456789012.s3-object-lambda-fips.\"\n            \"us-gov-west-1.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-gov-west-1\",\n        bucket=s3_object_lambda_arn_gov,\n        key=\"key\",\n        expected_url=(\n            \"https://mybanner-123456789012.s3-object-lambda-fips.\"\n            \"us-gov-west-1.amazonaws.com/key\"\n        ),\n        use_fips_endpoint=True,\n    )\n    s3_object_lambda_cross_region_arn_gov = (\n        \"arn:aws-us-gov:s3-object-lambda:us-gov-east-1:\"\n        \"123456789012:accesspoint:mybanner\"\n    )\n    yield dict(\n        region=\"fips-us-gov-west-1\",\n        bucket=s3_object_lambda_cross_region_arn_gov,\n        key=\"key\",\n        s3_config={\"use_arn_region\": True},\n        expected_url=(\n            \"https://mybanner-123456789012.s3-object-lambda-fips.\"\n            \"us-gov-east-1.amazonaws.com/key\"\n        ),\n    )\n    yield dict(\n        region=\"us-gov-west-1\",\n        bucket=s3_object_lambda_cross_region_arn_gov,\n        key=\"key\",\n        s3_config={\"use_arn_region\": True},\n        expected_url=(\n            \"https://mybanner-123456789012.s3-object-lambda-fips.\"\n            \"us-gov-east-1.amazonaws.com/key\"\n        ),\n        use_fips_endpoint=True,\n    )\n\n    s3_object_lambda_arn = (\n        \"arn:aws:s3-object-lambda:us-east-1:\"\n        \"123456789012:accesspoint:mybanner\"\n    )\n    yield dict(\n        region=\"aws-global\",\n        bucket=s3_object_lambda_arn,\n        key=\"key\",\n        s3_config={\"use_arn_region\": True},\n        expected_url=(\n            \"https://mybanner-123456789012.s3-object-lambda.\"\n            \"us-east-1.amazonaws.com/key\"\n        ),\n    )\n\n\ndef _s3_addressing_invalid_test_cases():\n    # client region does not match access point ARN region and use_arn_region\n    # is False. If sent to service, this results in an \"invalid access point\"\n    # response. We expect it to be caught by the S3 endpoints ruleset.\n    yield dict(\n        region=\"us-east-1\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"use_arn_region\": False},\n        expected_exception_type=UnsupportedS3AccesspointConfigurationError,\n        expected_exception_regex=(\n            \"region from ARN `us-west-2` does not match client region \"\n            \"`us-east-1`\"\n        ),\n    )\n    yield dict(\n        region=\"cn-northwest-1\",\n        bucket=accesspoint_arn_cn,\n        key=\"key\",\n        s3_config={\"use_arn_region\": False},\n        expected_exception_type=UnsupportedS3AccesspointConfigurationError,\n        expected_exception_regex=(\n            \"region from ARN `cn-north-1` does not match client region \"\n            \"`cn-northwest-1`\"\n        ),\n    )\n    yield dict(\n        region=\"unknown\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"use_arn_region\": False},\n        expected_exception_type=UnsupportedS3AccesspointConfigurationError,\n        expected_exception_regex=None,\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        s3_config={\"use_arn_region\": False},\n        expected_exception_type=UnsupportedS3AccesspointConfigurationError,\n        expected_exception_regex=None,\n    )\n\n\n@pytest.mark.parametrize(\"test_case\", _s3_addressing_test_cases())\ndef test_correct_url_used_for_s3(test_case):\n    # Test that given various sets of config options and bucket names,\n    # we construct the expect endpoint url.\n    _verify_expected_endpoint_url(**test_case)\n\n\n@pytest.mark.parametrize(\"test_case\", _s3_addressing_invalid_test_cases())\ndef test_correct_exception_raise_for_s3(test_case):\n    # Test that invalid sets of config options and bucket names, result in\n    # appropriate exceptions.\n    _verify_expected_exception(**test_case)\n\n\ndef _verify_expected_endpoint_url(\n    region=None,\n    bucket=\"bucket\",\n    key=\"key\",\n    s3_config=None,\n    is_secure=True,\n    customer_provided_endpoint=None,\n    expected_url=None,\n    signature_version=None,\n    use_fips_endpoint=None,\n):\n    s3 = _create_s3_client(\n        region=region,\n        is_secure=is_secure,\n        endpoint_url=customer_provided_endpoint,\n        s3_config=s3_config,\n        signature_version=signature_version,\n        use_fips_endpoint=use_fips_endpoint,\n    )\n    with ClientHTTPStubber(s3) as http_stubber:\n        http_stubber.add_response()\n        s3.put_object(Bucket=bucket, Key=key, Body=b\"bar\")\n        assert http_stubber.requests[0].url == expected_url\n\n\ndef _verify_expected_exception(\n    expected_exception_type,\n    expected_exception_regex=None,\n    region=None,\n    bucket=\"bucket\",\n    key=\"key\",\n    s3_config=None,\n    is_secure=True,\n    customer_provided_endpoint=None,\n    signature_version=None,\n    use_fips_endpoint=None,\n):\n    s3 = _create_s3_client(\n        region=region,\n        is_secure=is_secure,\n        endpoint_url=customer_provided_endpoint,\n        s3_config=s3_config,\n        signature_version=signature_version,\n        use_fips_endpoint=use_fips_endpoint,\n    )\n    with ClientHTTPStubber(s3) as http_stubber:\n        http_stubber.add_response()\n        with pytest.raises(\n            expected_exception_type, match=expected_exception_regex\n        ):\n            s3.put_object(Bucket=bucket, Key=key, Body=b\"bar\")\n\n\ndef _create_s3_client(\n    region=\"us-west-2\",\n    is_secure=True,\n    endpoint_url=None,\n    s3_config=None,\n    signature_version=\"s3v4\",\n    use_fips_endpoint=None,\n):\n    environ = {}\n    with mock.patch(\"os.environ\", environ):\n        environ[\"AWS_ACCESS_KEY_ID\"] = \"access_key\"\n        environ[\"AWS_SECRET_ACCESS_KEY\"] = \"secret_key\"\n        environ[\"AWS_CONFIG_FILE\"] = \"no-exist-foo\"\n        environ[\"AWS_SHARED_CREDENTIALS_FILE\"] = \"no-exist-foo\"\n        session = create_session()\n        session.config_filename = \"no-exist-foo\"\n        config = Config(\n            signature_version=signature_version,\n            s3=s3_config,\n            use_fips_endpoint=use_fips_endpoint,\n        )\n        s3 = session.create_client(\n            \"s3\",\n            region_name=region,\n            use_ssl=is_secure,\n            config=config,\n            endpoint_url=endpoint_url,\n        )\n        return s3\n\n\ndef _addressing_for_presigned_url_test_cases():\n    # us-east-1, or the \"global\" endpoint. A signature version of\n    # None means the user doesn't have signature version configured.\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=None,\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        s3_config={\"addressing_style\": \"path\"},\n        expected_url=\"https://s3.amazonaws.com/bucket/key\",\n    )\n\n    # A region that supports both 's3' and 's3v4'.\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=None,\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        s3_config={\"addressing_style\": \"path\"},\n        expected_url=\"https://s3.us-west-2.amazonaws.com/bucket/key\",\n    )\n\n    # An 's3v4' only region.\n    yield dict(\n        region=\"us-east-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=None,\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        s3_config={\"addressing_style\": \"path\"},\n        expected_url=\"https://s3.us-east-2.amazonaws.com/bucket/key\",\n    )\n\n    # Dualstack endpoints\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=None,\n        s3_config={\"use_dualstack_endpoint\": True},\n        expected_url=\"https://bucket.s3.dualstack.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        s3_config={\"use_dualstack_endpoint\": True},\n        expected_url=\"https://bucket.s3.dualstack.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        s3_config={\"use_dualstack_endpoint\": True},\n        expected_url=\"https://bucket.s3.dualstack.us-west-2.amazonaws.com/key\",\n    )\n\n    # Accelerate\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=None,\n        s3_config={\"use_accelerate_endpoint\": True},\n        expected_url=\"https://bucket.s3-accelerate.amazonaws.com/key\",\n    )\n\n    # A region that we don't know about.\n    yield dict(\n        region=\"boto-west-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=None,\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n\n    # Customer provided URL results in us leaving the host untouched.\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=None,\n        customer_provided_endpoint=\"https://foo.com/\",\n        expected_url=\"https://foo.com/bucket/key\",\n    )\n\n    # Access-point\n    yield dict(\n        region=\"us-west-2\",\n        bucket=accesspoint_arn,\n        key=\"key\",\n        expected_url=(\n            \"https://myendpoint-123456789012.s3-accesspoint.\"\n            \"us-west-2.amazonaws.com/key\"\n        ),\n    )\n\n    # Use us-east-1 regional endpoint configuration cases\n    us_east_1_regional_endpoint = {\"us_east_1_regional_endpoint\": \"regional\"}\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=us_east_1_regional_endpoint,\n        signature_version=\"s3\",\n        expected_url=(\"https://bucket.s3.us-east-1.amazonaws.com/key\"),\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config=us_east_1_regional_endpoint,\n        signature_version=\"s3v4\",\n        expected_url=(\"https://bucket.s3.us-east-1.amazonaws.com/key\"),\n    )\n    # Bucket names that contain dots or are otherwise not virtual host style\n    # compatible should always resolve to a regional endpoint.\n    # https://github.com/boto/botocore/issues/2798\n    yield dict(\n        region=\"us-west-1\",\n        bucket=\"foo.bar.biz\",\n        key=\"key\",\n        signature_version=\"s3\",\n        expected_url=\"https://s3.us-west-1.amazonaws.com/foo.bar.biz/key\",\n    )\n    # Bucket names that contain dots and subcomponents that are less than\n    # 3 characters should still use virtual host style addressing if\n    # configured by the customer and they provide their own ``endpoint_url``\n    # that is insecure. https://github.com/boto/botocore/issues/2938\n    yield dict(\n        bucket=\"foo.b.biz\",\n        key=\"key\",\n        s3_config={\"addressing_style\": \"virtual\"},\n        customer_provided_endpoint=\"http://s3.us-west-2.amazonaws.com\",\n        expected_url=\"http://foo.b.biz.s3.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        bucket=\"foo.b.biz\",\n        key=\"key\",\n        s3_config={\"addressing_style\": \"virtual\"},\n        customer_provided_endpoint=\"https://s3.us-west-2.amazonaws.com\",\n        expected_url=\"https://s3.us-west-2.amazonaws.com/foo.b.biz/key\",\n    )\n\n    # virtual style addressing expicitly requested always uses\n    # regional endpoints except for us-east-1 and aws-global\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3\",\n        s3_config={\"addressing_style\": \"virtual\"},\n        expected_url=\"https://bucket.s3.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        signature_version=\"s3v4\",\n        s3_config={\"addressing_style\": \"virtual\"},\n        expected_url=\"https://bucket.s3.us-east-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-west-2\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config={\"addressing_style\": \"virtual\"},\n        expected_url=\"https://bucket.s3.us-west-2.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"us-east-1\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config={\"addressing_style\": \"virtual\"},\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n    yield dict(\n        region=\"aws-global\",\n        bucket=\"bucket\",\n        key=\"key\",\n        s3_config={\"addressing_style\": \"virtual\"},\n        expected_url=\"https://bucket.s3.amazonaws.com/key\",\n    )\n\n\n@pytest.mark.parametrize(\n    \"test_case\", _addressing_for_presigned_url_test_cases()\n)\ndef test_addressing_for_presigned_urls(test_case):\n    # Here we're just focusing on the addressing mode used for presigned URLs.\n    # We special case presigned URLs due to backward compatibility.\n    _verify_presigned_url_addressing(**test_case)\n\n\ndef _verify_presigned_url_addressing(\n    region=None,\n    bucket=\"bucket\",\n    key=\"key\",\n    s3_config=None,\n    is_secure=True,\n    customer_provided_endpoint=None,\n    expected_url=None,\n    signature_version=None,\n):\n    s3 = _create_s3_client(\n        region=region,\n        is_secure=is_secure,\n        endpoint_url=customer_provided_endpoint,\n        s3_config=s3_config,\n        signature_version=signature_version,\n    )\n    url = s3.generate_presigned_url(\n        \"get_object\", {\"Bucket\": bucket, \"Key\": key}\n    )\n    # We're not trying to verify the params for URL presigning,\n    # those are tested elsewhere.  We just care about the hostname/path.\n    parts = urlsplit(url)\n    actual = \"%s://%s%s\" % parts[:3]\n    assert actual == expected_url\n\n\nclass TestS3XMLPayloadEscape(BaseS3OperationTest):\n    def assert_correct_content_md5(self, request):\n        content_md5_bytes = get_md5(request.body).digest()\n        content_md5 = base64.b64encode(content_md5_bytes)\n        self.assertEqual(content_md5, request.headers[\"Content-MD5\"])\n\n    def test_escape_keys_in_xml_delete_objects(self):\n        self.http_stubber.add_response()\n        with self.http_stubber:\n            self.client.delete_objects(\n                Bucket=\"mybucket\",\n                Delete={\"Objects\": [{\"Key\": \"some\\r\\n\\rkey\"}]},\n            )\n        request = self.http_stubber.requests[0]\n        self.assertNotIn(b\"\\r\\n\\r\", request.body)\n        self.assertIn(b\"&#xD;&#xA;&#xD;\", request.body)\n        self.assert_correct_content_md5(request)\n\n    def test_escape_keys_in_xml_put_bucket_lifecycle_configuration(self):\n        self.http_stubber.add_response()\n        with self.http_stubber:\n            self.client.put_bucket_lifecycle_configuration(\n                Bucket=\"mybucket\",\n                LifecycleConfiguration={\n                    \"Rules\": [\n                        {\n                            \"Prefix\": \"my\\r\\n\\rprefix\",\n                            \"Status\": \"ENABLED\",\n                        }\n                    ]\n                },\n            )\n        request = self.http_stubber.requests[0]\n        self.assertNotIn(b\"my\\r\\n\\rprefix\", request.body)\n        self.assertIn(b\"my&#xD;&#xA;&#xD;prefix\", request.body)\n        self.assert_correct_content_md5(request)\n\n\nclass TestExpectContinueBehavior(BaseSessionTest):\n    def test_sets_100_continute_with_body(self):\n        op_kwargs = {\n            \"Bucket\": \"mybucket\",\n            \"Key\": \"mykey\",\n            \"Body\": b\"foo\",\n        }\n        s3 = _create_s3_client()\n        with ClientHTTPStubber(s3) as http_stubber:\n            http_stubber.add_response()\n            s3.put_object(**op_kwargs)\n            expect_header = http_stubber.requests[-1].headers.get(\"Expect\")\n            self.assertIsNotNone(expect_header)\n            self.assertEqual(expect_header, b\"100-continue\")\n\n    def test_does_not_set_100_continute_with_empty_body(self):\n        environ = {'BOTO_EXPERIMENTAL__NO_EMPTY_CONTINUE': \"True\"}\n        self.environ_patch = mock.patch('os.environ', environ)\n        self.environ_patch.start()\n        op_kwargs = {\"Bucket\": \"mybucket\", \"Key\": \"mykey\", \"Body\": \"\"}\n        s3 = _create_s3_client()\n        with ClientHTTPStubber(s3) as http_stubber:\n            http_stubber.add_response()\n            s3.put_object(**op_kwargs)\n            expect_header = http_stubber.requests[-1].headers.get(\"Expect\")\n            self.assertIsNone(expect_header)\n\n\nclass TestParameterInjection(BaseS3OperationTest):\n    BUCKET = \"foo\"\n    KEY = \"bar\"\n\n    def test_parameter_injection(self):\n        self.http_stubber.add_response()\n        self.client.meta.events.register(\n            'before-sign.s3', self._verify_bucket_and_key_in_context\n        )\n        with self.http_stubber:\n            self.client.put_object(\n                Bucket=self.BUCKET,\n                Key=self.KEY,\n            )\n\n    def _verify_bucket_and_key_in_context(self, request, **kwargs):\n        self.assertEqual(\n            request.context['input_params']['Bucket'], self.BUCKET\n        )\n        self.assertEqual(request.context['input_params']['Key'], self.KEY)\n", "tests/functional/test_importexport.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestImportexport(BaseSessionTest):\n    def create_client_and_stubber(self, service_name, region_name=None):\n        if region_name is None:\n            region_name = 'us-west-2'\n\n        client = self.session.create_client(service_name, region_name)\n        http_stubber = ClientHTTPStubber(client)\n\n        return client, http_stubber\n\n    def test_importexport_signature_version(self):\n        \"\"\"The importexport service has sigv2 registered as its \"signature_version\"\n        in the service model. While this was historically true, they migrated to\n        sigv4 with the introduction of endpoints.json.\n\n        This test ensures we always choose sigv4 regardless of what the model states.\n        \"\"\"\n        client, stubber = self.create_client_and_stubber('importexport')\n        importexport_response = (\n            b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n\\n'\n            b\"<CancelJobOutput>\"\n            b\"<CancelJobResult></CancelJobResult>\"\n            b\"</CancelJobOutput>\"\n        )\n\n        # Confirm we've ignored the model signatureVersion and chosen v4\n        assert client.meta.config.signature_version == \"v4\"\n        with stubber:\n            stubber.add_response(body=importexport_response)\n            client.cancel_job(JobId=\"12345\")\n\n        # Validate we actually signed with sigv4\n        auth_header = stubber.requests[0].headers.get('Authorization', '')\n        assert auth_header.startswith(b\"AWS4-HMAC-SHA256\")\n        assert b\"aws4_request\" in auth_header\n", "tests/functional/test_s3express.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\n\nimport pytest\nfrom dateutil.tz import tzutc\n\nimport botocore.session\nfrom botocore.auth import S3ExpressAuth\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.credentials import Credentials, RefreshableCredentials\nfrom botocore.utils import S3ExpressIdentityCache\nfrom tests import ClientHTTPStubber, mock\n\nACCESS_KEY = \"AKIDEXAMPLE\"\nSECRET_KEY = \"wJalrXUtnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY\"\nTOKEN = \"TOKEN\"\n\nEXPRESS_ACCESS_KEY = \"EXPRESS_AKIDEXAMPLE\"\nEXPRESS_SECRET_KEY = \"EXPRESS_53cr37\"\nEXPRESS_TOKEN = \"EXPRESS_TOKEN\"\n\nCREDENTIALS = Credentials(ACCESS_KEY, SECRET_KEY, TOKEN)\nENDPOINT = \"https://s3.us-west-2.amazonaws.com\"\n\nS3EXPRESS_BUCKET = \"mytestbucket--usw2-az5--x-s3\"\n\n\nDATE = datetime.datetime(2023, 11, 26, 0, 0, 0, tzinfo=tzutc())\n\n\nCREATE_SESSION_RESPONSE = (\n    b\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\"\n    b\"<CreateSessionResult xmlns=\\\"http://s3.amazonaws.com/doc/2006-03-01/\\\">\"\n    b\"<Credentials><SessionToken>EXPRESS_TOKEN</SessionToken>\"\n    b\"<SecretAccessKey>EXPRESS_53cr37</SecretAccessKey>\"\n    b\"<AccessKeyId>EXPRESS_AKIDEXAMPLE</AccessKeyId>\"\n    b\"<Expiration>2023-11-28T01:46:39Z</Expiration>\"\n    b\"</Credentials></CreateSessionResult>\"\n)\n\n\n@pytest.fixture\ndef default_s3_client():\n    session = botocore.session.Session()\n    return session.create_client(\n        's3',\n        'us-west-2',\n        aws_access_key_id=ACCESS_KEY,\n        aws_secret_access_key=SECRET_KEY,\n        aws_session_token=TOKEN,\n    )\n\n\n@pytest.fixture\ndef mock_datetime():\n    with mock.patch('datetime.datetime', spec=True) as mock_datetime:\n        yield mock_datetime\n\n\ndef _assert_expected_create_session_call(request, bucket):\n    assert bucket in request.url\n    assert request.url.endswith('?session')\n    assert request.method == \"GET\"\n\n\nclass TestS3ExpressAuth:\n    def _assert_has_expected_headers(self, request, header_list=None):\n        if header_list is None:\n            header_list = [\"Authorization\", \"X-Amz-S3Session-Token\"]\n\n        for header in header_list:\n            assert header in request.headers\n\n    def test_s3express_auth_requires_instance_cache(self):\n        assert hasattr(S3ExpressAuth, \"REQUIRES_IDENTITY_CACHE\")\n        assert S3ExpressAuth.REQUIRES_IDENTITY_CACHE is True\n\n    def test_s3_express_auth_headers(self):\n        request = AWSRequest(\n            method='GET',\n            url=ENDPOINT,\n        )\n        auth_instance = S3ExpressAuth(\n            CREDENTIALS, \"s3\", \"us-west-2\", identity_cache={}\n        )\n\n        # Confirm there is no existing auth info on request\n        assert 'Authorization' not in request.headers\n        auth_instance.add_auth(request)\n        self._assert_has_expected_headers(request)\n\n        # Confirm we're not including the unsupported X-Amz-Security-Token\n        # header for S3Express.\n        assert 'X-Amz-Security-Token' not in request.headers\n\n\nclass TestS3ExpressIdentityCache:\n    def test_default_s3_express_cache(self, default_s3_client, mock_datetime):\n        mock_datetime.now.return_value = DATE\n        mock_datetime.utcnow.return_value = DATE\n\n        identity_cache = S3ExpressIdentityCache(\n            default_s3_client,\n            RefreshableCredentials,\n        )\n        with ClientHTTPStubber(default_s3_client) as stubber:\n            stubber.add_response(body=CREATE_SESSION_RESPONSE)\n            credentials = identity_cache.get_credentials('my_bucket')\n\n            assert credentials.access_key == EXPRESS_ACCESS_KEY\n            assert credentials.secret_key == EXPRESS_SECRET_KEY\n            assert credentials.token == EXPRESS_TOKEN\n\n    def test_s3_express_cache_one_network_call(\n        self, default_s3_client, mock_datetime\n    ):\n        mock_datetime.now.return_value = DATE\n        mock_datetime.utcnow.return_value = DATE\n        bucket = 'my_bucket'\n\n        identity_cache = S3ExpressIdentityCache(\n            default_s3_client,\n            RefreshableCredentials,\n        )\n        with ClientHTTPStubber(default_s3_client) as stubber:\n            # Only set one response\n            stubber.add_response(body=CREATE_SESSION_RESPONSE)\n\n            first_creds = identity_cache.get_credentials(bucket)\n            second_creds = identity_cache.get_credentials(bucket)\n\n            # Confirm we got back the same credentials\n            assert first_creds == second_creds\n\n            # Confirm we didn't hit the API twice\n            assert len(stubber.requests) == 1\n            _assert_expected_create_session_call(stubber.requests[0], bucket)\n\n    def test_s3_express_cache_multiple_buckets(\n        self, default_s3_client, mock_datetime\n    ):\n        mock_datetime.now.return_value = DATE\n        mock_datetime.utcnow.return_value = DATE\n        bucket = 'my_bucket'\n        other_bucket = 'other_bucket'\n\n        identity_cache = S3ExpressIdentityCache(\n            default_s3_client,\n            RefreshableCredentials,\n        )\n        with ClientHTTPStubber(default_s3_client) as stubber:\n            stubber.add_response(body=CREATE_SESSION_RESPONSE)\n            stubber.add_response(body=CREATE_SESSION_RESPONSE)\n\n            identity_cache.get_credentials(bucket)\n            identity_cache.get_credentials(other_bucket)\n\n            # Confirm we hit the API for each bucket\n            assert len(stubber.requests) == 2\n            _assert_expected_create_session_call(stubber.requests[0], bucket)\n            _assert_expected_create_session_call(\n                stubber.requests[1], other_bucket\n            )\n\n\nclass TestS3ExpressRequests:\n    def _assert_standard_sigv4_signature(self, headers):\n        sigv4_auth_preamble = (\n            b'AWS4-HMAC-SHA256 Credential='\n            b'AKIDEXAMPLE/20231126/us-west-2/s3express/aws4_request'\n        )\n        assert 'Authorization' in headers\n        assert headers['Authorization'].startswith(sigv4_auth_preamble)\n\n    def _assert_s3express_credentials(self, headers):\n        s3express_auth_preamble = (\n            b'AWS4-HMAC-SHA256 Credential='\n            b'EXPRESS_AKIDEXAMPLE/20231126/us-west-2/s3express/aws4_request'\n        )\n        assert 'Authorization' in headers\n        assert 'x-amz-s3session-token' in headers\n        assert headers['Authorization'].startswith(s3express_auth_preamble)\n        assert headers['x-amz-s3session-token'] == b'EXPRESS_TOKEN'\n\n    def _assert_checksum_algorithm_added(self, algorithm, headers):\n        algorithm_header_name = f\"x-amz-checksum-{algorithm}\"\n        assert algorithm_header_name in headers\n\n    def _call_get_object(self, client):\n        return client.get_object(\n            Bucket=S3EXPRESS_BUCKET,\n            Key='my-test-object',\n        )\n\n    def test_create_bucket(self, default_s3_client, mock_datetime):\n        mock_datetime.utcnow.return_value = DATE\n\n        with ClientHTTPStubber(default_s3_client) as stubber:\n            stubber.add_response()\n\n            default_s3_client.create_bucket(\n                Bucket=S3EXPRESS_BUCKET,\n                CreateBucketConfiguration={\n                    'Location': {\n                        'Name': 'usw2-az5',\n                        'Type': 'AvailabilityZone',\n                    },\n                    'Bucket': {\n                        \"DataRedundancy\": \"SingleAvailabilityZone\",\n                        \"Type\": \"Directory\",\n                    },\n                },\n            )\n\n        # Confirm we don't call CreateSession for create_bucket\n        assert len(stubber.requests) == 1\n        self._assert_standard_sigv4_signature(stubber.requests[0].headers)\n\n    def test_get_object(self, default_s3_client, mock_datetime):\n        mock_datetime.utcnow.return_value = DATE\n        mock_datetime.now.return_value = DATE\n\n        with ClientHTTPStubber(default_s3_client) as stubber:\n            stubber.add_response(body=CREATE_SESSION_RESPONSE)\n            stubber.add_response()\n\n            self._call_get_object(default_s3_client)\n\n        # Confirm we called CreateSession for create_bucket\n        assert len(stubber.requests) == 2\n        _assert_expected_create_session_call(\n            stubber.requests[0], S3EXPRESS_BUCKET\n        )\n        self._assert_standard_sigv4_signature(stubber.requests[0].headers)\n\n        # Confirm actual PutObject request was signed with Session credentials\n        self._assert_s3express_credentials(stubber.requests[1].headers)\n\n    def test_cache_with_multiple_requests(\n        self, default_s3_client, mock_datetime\n    ):\n        mock_datetime.utcnow.return_value = DATE\n        mock_datetime.now.return_value = DATE\n\n        with ClientHTTPStubber(default_s3_client) as stubber:\n            stubber.add_response(body=CREATE_SESSION_RESPONSE)\n            stubber.add_response()\n            stubber.add_response()\n\n            self._call_get_object(default_s3_client)\n            self._call_get_object(default_s3_client)\n\n        # Confirm we called CreateSession for create_bucket once\n        assert len(stubber.requests) == 3\n        _assert_expected_create_session_call(\n            stubber.requests[0], S3EXPRESS_BUCKET\n        )\n        self._assert_standard_sigv4_signature(stubber.requests[0].headers)\n\n        # Confirm we signed both called with S3Express credentials\n        self._assert_s3express_credentials(stubber.requests[1].headers)\n        self._assert_s3express_credentials(stubber.requests[2].headers)\n\n    def test_delete_objects_injects_correct_checksum(\n        self, default_s3_client, mock_datetime\n    ):\n        mock_datetime.utcnow.return_value = DATE\n        mock_datetime.now.return_value = DATE\n\n        with ClientHTTPStubber(default_s3_client) as stubber:\n            stubber.add_response(body=CREATE_SESSION_RESPONSE)\n            stubber.add_response()\n\n            default_s3_client.delete_objects(\n                Bucket=S3EXPRESS_BUCKET,\n                Delete={\n                    \"Objects\": [\n                        {\n                            \"Key\": \"my-obj\",\n                            \"VersionId\": \"1\",\n                        }\n                    ]\n                },\n            )\n\n        # Confirm we called CreateSession for create_bucket\n        assert len(stubber.requests) == 2\n        _assert_expected_create_session_call(\n            stubber.requests[0], S3EXPRESS_BUCKET\n        )\n        self._assert_standard_sigv4_signature(stubber.requests[0].headers)\n\n        # Confirm we signed both called with S3Express credentials\n        self._assert_s3express_credentials(stubber.requests[1].headers)\n        self._assert_checksum_algorithm_added(\n            'crc32', stubber.requests[1].headers\n        )\n", "tests/functional/test_cloudformation.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests.functional.docs import BaseDocsFunctionalTest\n\n\nclass TestCloudFormationDocs(BaseDocsFunctionalTest):\n    def test_get_template_response_documented_as_dict(self):\n        content = self.get_docstring_for_method(\n            'cloudformation', 'get_template'\n        )\n        # String return type should be gone\n        self.assert_not_contains_line(\"(*string*) --\", content)\n        # Check for template body returning a dict\n        self.assert_contains_line(\"(*dict*) --\", content)\n        # Check the specifics of the returned dict\n        self.assert_contains_line('{}', content)\n", "tests/functional/test_sagemaker.py": "from botocore.stub import Stubber\nfrom tests import BaseSessionTest\n\n\nclass TestSagemaker(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client('sagemaker', self.region)\n        self.stubber = Stubber(self.client)\n        self.stubber.activate()\n        self.hook_calls = []\n\n    def _hook(self, **kwargs):\n        self.hook_calls.append(kwargs['event_name'])\n\n    def tearDown(self):\n        super().tearDown()\n        self.stubber.deactivate()\n\n    def test_event_with_old_prefix(self):\n        self.client.meta.events.register(\n            'provide-client-params.sagemaker.ListEndpoints', self._hook\n        )\n        self.stubber.add_response('list_endpoints', {'Endpoints': []})\n        self.client.list_endpoints()\n        self.assertEqual(\n            self.hook_calls, ['provide-client-params.sagemaker.ListEndpoints']\n        )\n\n    def test_event_with_new_prefix(self):\n        self.client.meta.events.register(\n            'provide-client-params.api.sagemaker.ListEndpoints', self._hook\n        )\n        self.stubber.add_response('list_endpoints', {'Endpoints': []})\n        self.client.list_endpoints()\n        self.assertEqual(\n            self.hook_calls, ['provide-client-params.sagemaker.ListEndpoints']\n        )\n", "tests/functional/test_client.py": "import unittest\n\nimport botocore\n\n\nclass TestCreateClients(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n\n    def test_client_can_clone_with_service_events(self):\n        # We should also be able to create a client object.\n        client = self.session.create_client('s3', region_name='us-west-2')\n        # We really just want to ensure create_client doesn't raise\n        # an exception, but we'll double check that the client looks right.\n        self.assertTrue(hasattr(client, 'list_buckets'))\n\n    def test_client_raises_exception_invalid_region(self):\n        with self.assertRaisesRegex(ValueError, ('invalid region name')):\n            self.session.create_client(\n                'cloudformation', region_name='invalid region name'\n            )\n", "tests/functional/conftest.py": "# Copyright 2012-2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport pytest\n\nfrom tests import create_session\n\n\n@pytest.fixture()\ndef patched_session(monkeypatch):\n    monkeypatch.setenv('AWS_ACCESS_KEY_ID', 'access_key')\n    monkeypatch.setenv('AWS_SECRET_ACCESS_KEY', 'secret_key')\n    monkeypatch.setenv('AWS_CONFIG_FILE', 'no-exist-foo')\n    monkeypatch.delenv('AWS_PROFILE', raising=False)\n    monkeypatch.delenv('AWS_DEFAULT_REGION', raising=False)\n    session = create_session()\n    return session\n", "tests/functional/test_discovery.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.compat import json\nfrom botocore.config import Config\nfrom botocore.discovery import EndpointDiscoveryRequired\nfrom botocore.exceptions import (\n    ClientError,\n    InvalidEndpointDiscoveryConfigurationError,\n)\nfrom tests import ClientHTTPStubber, temporary_file\nfrom tests.functional import FunctionalSessionTest\n\n\nclass TestEndpointDiscovery(FunctionalSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n\n    def create_client(\n        self,\n        service_name='dynamodb',\n        region=None,\n        config=None,\n        endpoint_url=None,\n    ):\n        if region is None:\n            region = self.region\n        client = self.session.create_client(\n            service_name, region, config=config, endpoint_url=endpoint_url\n        )\n        http_stubber = ClientHTTPStubber(client)\n\n        return client, http_stubber\n\n    def add_describe_endpoints_response(self, stubber, discovered_endpoint):\n        response = {\n            'Endpoints': [\n                {\n                    'Address': discovered_endpoint,\n                    'CachePeriodInMinutes': 1,\n                }\n            ]\n        }\n        response_body = json.dumps(response).encode()\n        stubber.add_response(status=200, body=response_body)\n        stubber.add_response(status=200, body=b'{}')\n\n    def set_endpoint_discovery_config_file(self, fileobj, config_val):\n        fileobj.write(\n            '[default]\\n' 'endpoint_discovery_enabled=%s\\n' % config_val\n        )\n        fileobj.flush()\n        self.environ['AWS_CONFIG_FILE'] = fileobj.name\n\n    def assert_endpoint_discovery_used(self, stubber, discovered_endpoint):\n        self.assertEqual(len(stubber.requests), 2)\n        discover_request = stubber.requests[1]\n        self.assertEqual(discover_request.url, discovered_endpoint)\n\n    def assert_discovery_skipped(self, stubber, operation):\n        self.assertEqual(len(stubber.requests), 1)\n        self.assertEqual(\n            stubber.requests[0].headers.get('X-Amz-Target'), operation\n        )\n\n    def assert_endpoint_used(self, actual_url, expected_url):\n        self.assertEqual(actual_url, expected_url)\n\n    def test_endpoint_discovery_enabled(self):\n        discovered_endpoint = 'https://discovered.domain'\n        config = Config(endpoint_discovery_enabled=True)\n        client, http_stubber = self.create_client(config=config)\n        with http_stubber as stubber:\n            self.add_describe_endpoints_response(stubber, discovered_endpoint)\n            client.describe_table(TableName='sometable')\n            self.assert_endpoint_discovery_used(stubber, discovered_endpoint)\n\n    def test_endpoint_discovery_with_invalid_endpoint(self):\n        response = {\n            'Error': {\n                'Code': 'InvalidEndpointException',\n                'Message': 'Test Error',\n            }\n        }\n        response_body = json.dumps(response).encode()\n\n        config = Config(endpoint_discovery_enabled=True)\n        client, http_stubber = self.create_client(config=config)\n        with http_stubber as stubber:\n            stubber.add_response(status=421, body=response_body)\n            with self.assertRaises(ClientError):\n                client.describe_table(TableName='sometable')\n\n    def test_endpoint_discovery_disabled(self):\n        config = Config(endpoint_discovery_enabled=False)\n        client, http_stubber = self.create_client(config=config)\n        with http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            client.describe_table(TableName='sometable')\n            self.assertEqual(len(stubber.requests), 1)\n\n    def test_endpoint_discovery_no_config_default(self):\n        client, http_stubber = self.create_client()\n        with http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            client.describe_table(TableName='sometable')\n            self.assertEqual(len(stubber.requests), 1)\n\n    def test_endpoint_discovery_default_required_endpoint(self):\n        discovered_endpoint = \"https://discovered.domain\"\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\"\n        )\n        with http_stubber as stubber:\n            self.add_describe_endpoints_response(stubber, discovered_endpoint)\n            client.test_discovery_required(Foo=\"bar\")\n            self.assert_endpoint_discovery_used(stubber, discovered_endpoint)\n\n    def test_endpoint_discovery_required_with_discovery_enabled(self):\n        discovered_endpoint = \"https://discovered.domain\"\n        config = Config(endpoint_discovery_enabled=True)\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\", config=config\n        )\n        with http_stubber as stubber:\n            self.add_describe_endpoints_response(stubber, discovered_endpoint)\n            client.test_discovery_required(Foo=\"bar\")\n            self.assert_endpoint_discovery_used(stubber, discovered_endpoint)\n\n    def test_endpoint_discovery_required_with_discovery_disabled(self):\n        discovered_endpoint = \"https://discovered.domain\"\n        config = Config(endpoint_discovery_enabled=False)\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\", config=config\n        )\n        self.add_describe_endpoints_response(http_stubber, discovered_endpoint)\n        with self.assertRaises(EndpointDiscoveryRequired):\n            client.test_discovery_required(Foo=\"bar\")\n\n    def test_endpoint_discovery_required_with_custom_endpoint(self):\n        endpoint = \"https://custom.domain/\"\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\", endpoint_url=endpoint\n        )\n        with http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            client.test_discovery_required(Foo=\"bar\")\n            self.assert_discovery_skipped(\n                stubber, b\"test-discovery-endpoint.TestDiscoveryRequired\"\n            )\n            self.assert_endpoint_used(stubber.requests[0].url, endpoint)\n\n    def test_endpoint_discovery_disabled_with_custom_endpoint(self):\n        endpoint = \"https://custom.domain/\"\n        config = Config(endpoint_discovery_enabled=False)\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\",\n            config=config,\n            endpoint_url=endpoint,\n        )\n        with http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            client.test_discovery_required(Foo=\"bar\")\n            self.assert_discovery_skipped(\n                stubber, b\"test-discovery-endpoint.TestDiscoveryRequired\"\n            )\n            self.assert_endpoint_used(stubber.requests[0].url, endpoint)\n\n    def test_endpoint_discovery_enabled_with_custom_endpoint(self):\n        endpoint = \"https://custom.domain/\"\n        config = Config(endpoint_discovery_enabled=True)\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\",\n            config=config,\n            endpoint_url=endpoint,\n        )\n        with http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            client.test_discovery_required(Foo=\"bar\")\n            self.assert_discovery_skipped(\n                stubber, b\"test-discovery-endpoint.TestDiscoveryRequired\"\n            )\n            self.assert_endpoint_used(stubber.requests[0].url, endpoint)\n\n    def test_endpoint_discovery_optional_with_custom_endpoint(self):\n        endpoint = \"https://custom.domain/\"\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\", endpoint_url=endpoint\n        )\n        with http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            client.test_discovery_optional(Foo=\"bar\")\n            self.assert_discovery_skipped(\n                stubber, b\"test-discovery-endpoint.TestDiscoveryOptional\"\n            )\n            self.assert_endpoint_used(stubber.requests[0].url, endpoint)\n\n    def test_endpoint_discovery_optional_disabled_with_custom_endpoint(self):\n        endpoint = \"https://custom.domain/\"\n        config = Config(endpoint_discovery_enabled=False)\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\",\n            config=config,\n            endpoint_url=endpoint,\n        )\n        with http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            client.test_discovery_optional(Foo=\"bar\")\n            self.assert_discovery_skipped(\n                stubber,\n                b\"test-discovery-endpoint.TestDiscoveryOptional\",\n            )\n            self.assert_endpoint_used(stubber.requests[0].url, endpoint)\n\n    def test_endpoint_discovery_default_optional_endpoint(self):\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\"\n        )\n        with http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            client.test_discovery_optional(Foo=\"bar\")\n            self.assertEqual(len(stubber.requests), 1)\n\n    def test_endpoint_discovery_enabled_optional_endpoint(self):\n        discovered_endpoint = 'https://discovered.domain'\n        config = Config(endpoint_discovery_enabled=True)\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\", config=config\n        )\n        with http_stubber as stubber:\n            self.add_describe_endpoints_response(stubber, discovered_endpoint)\n            client.test_discovery_optional(Foo=\"bar\")\n            self.assert_endpoint_discovery_used(stubber, discovered_endpoint)\n\n    def test_endpoint_discovery_manual_auto_on_required_endpoint(self):\n        discovered_endpoint = 'https://discovered.domain'\n        config = Config(endpoint_discovery_enabled=\" aUto  \\n\")\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\", config=config\n        )\n        with http_stubber as stubber:\n            self.add_describe_endpoints_response(stubber, discovered_endpoint)\n            client.test_discovery_required(Foo=\"bar\")\n            self.assert_endpoint_discovery_used(stubber, discovered_endpoint)\n\n    def test_endpoint_discovery_enabled_with_random_string(self):\n        config = Config(endpoint_discovery_enabled=\"bad value\")\n        with self.assertRaises(InvalidEndpointDiscoveryConfigurationError):\n            client, http_stubber = self.create_client(\n                service_name=\"test-discovery-endpoint\", config=config\n            )\n\n    def test_endpoint_discovery_required_with_env_var_default(self):\n        self.environ['AWS_ENDPOINT_DISCOVERY_ENABLED'] = 'auto'\n        discovered_endpoint = 'https://discovered.domain'\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\", config=None\n        )\n        with http_stubber as stubber:\n            self.add_describe_endpoints_response(stubber, discovered_endpoint)\n            client.test_discovery_required(Foo=\"bar\")\n            self.assert_endpoint_discovery_used(stubber, discovered_endpoint)\n\n    def test_endpoint_discovery_optional_with_env_var_default(self):\n        self.environ['AWS_ENDPOINT_DISCOVERY_ENABLED'] = 'auto'\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\", config=None\n        )\n        with http_stubber as stubber:\n            stubber.add_response(status=200, body=b'{}')\n            client.test_discovery_optional(Foo=\"bar\")\n            self.assert_discovery_skipped(\n                stubber, b\"test-discovery-endpoint.TestDiscoveryOptional\"\n            )\n\n    def test_endpoint_discovery_optional_with_env_var_enabled(self):\n        self.environ['AWS_ENDPOINT_DISCOVERY_ENABLED'] = \"True\"\n        discovered_endpoint = 'https://discovered.domain'\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\"\n        )\n        with http_stubber as stubber:\n            self.add_describe_endpoints_response(stubber, discovered_endpoint)\n            client.test_discovery_optional(Foo=\"bar\")\n            self.assert_endpoint_discovery_used(stubber, discovered_endpoint)\n\n    def test_endpoint_discovery_required_with_env_var_disabled(self):\n        self.environ['AWS_ENDPOINT_DISCOVERY_ENABLED'] = \"False\"\n        discovered_endpoint = 'https://discovered.domain'\n        client, http_stubber = self.create_client(\n            service_name=\"test-discovery-endpoint\"\n        )\n        self.add_describe_endpoints_response(http_stubber, discovered_endpoint)\n        with self.assertRaises(EndpointDiscoveryRequired):\n            client.test_discovery_required(Foo=\"bar\")\n\n    def test_endpoint_discovery_with_config_file_enabled(self):\n        with temporary_file('w') as f:\n            self.set_endpoint_discovery_config_file(f, \"True\")\n            discovered_endpoint = 'https://discovered.domain'\n            client, http_stubber = self.create_client(\n                service_name=\"test-discovery-endpoint\"\n            )\n            with http_stubber as stubber:\n                self.add_describe_endpoints_response(\n                    stubber, discovered_endpoint\n                )\n                client.test_discovery_required(Foo=\"bar\")\n                self.assert_endpoint_discovery_used(\n                    stubber, discovered_endpoint\n                )\n\n    def test_endpoint_discovery_with_config_file_enabled_lowercase(self):\n        with temporary_file('w') as f:\n            self.set_endpoint_discovery_config_file(f, \"true\")\n            discovered_endpoint = 'https://discovered.domain'\n            client, http_stubber = self.create_client(\n                service_name=\"test-discovery-endpoint\"\n            )\n            with http_stubber as stubber:\n                self.add_describe_endpoints_response(\n                    stubber, discovered_endpoint\n                )\n                client.test_discovery_required(Foo=\"bar\")\n                self.assert_endpoint_discovery_used(\n                    stubber, discovered_endpoint\n                )\n\n    def test_endpoint_discovery_with_config_file_disabled(self):\n        with temporary_file('w') as f:\n            self.set_endpoint_discovery_config_file(f, \"false\")\n            discovered_endpoint = 'https://discovered.domain'\n            client, http_stubber = self.create_client(\n                service_name=\"test-discovery-endpoint\"\n            )\n            self.add_describe_endpoints_response(\n                http_stubber, discovered_endpoint\n            )\n            with self.assertRaises(EndpointDiscoveryRequired):\n                client.test_discovery_required(Foo=\"bar\")\n\n    def test_endpoint_discovery_with_config_file_auto(self):\n        with temporary_file('w') as f:\n            self.set_endpoint_discovery_config_file(f, \"AUTO\")\n            discovered_endpoint = 'https://discovered.domain'\n            client, http_stubber = self.create_client(\n                service_name=\"test-discovery-endpoint\"\n            )\n            with http_stubber as stubber:\n                self.add_describe_endpoints_response(\n                    stubber, discovered_endpoint\n                )\n                client.test_discovery_required(Foo=\"bar\")\n                self.assert_endpoint_discovery_used(\n                    stubber, discovered_endpoint\n                )\n", "tests/functional/test_useragent.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nfrom itertools import product\n\nimport pytest\n\nfrom botocore import __version__ as botocore_version\nfrom botocore.config import Config\nfrom tests import ClientHTTPStubber\n\n\nclass UACapHTTPStubber(ClientHTTPStubber):\n    \"\"\"\n    Wrapper for ClientHTTPStubber that captures UA header from one request.\n    \"\"\"\n\n    def __init__(self, obj_with_event_emitter):\n        super().__init__(obj_with_event_emitter, strict=False)\n        self.add_response()  # expect exactly one request\n\n    @property\n    def captured_ua_string(self):\n        if len(self.requests) > 0:\n            return self.requests[0].headers['User-Agent'].decode()\n        return None\n\n\n@pytest.mark.parametrize(\n    'sess_name, sess_version, sess_extra, cfg_extra, cfg_appid',\n    # Produce every combination of User-Agent related config settings other\n    # than Config.user_agent which will always be set in this test.\n    product(\n        ('sess_name', None),\n        ('sess_version', None),\n        ('sess_extra', None),\n        ('cfg_extra', None),\n        ('cfg_appid', None),\n    ),\n)\ndef test_user_agent_from_config_replaces_default(\n    sess_name,\n    sess_version,\n    sess_extra,\n    cfg_extra,\n    cfg_appid,\n    patched_session,\n):\n    # Config.user_agent replaces all parts of the regular User-Agent header\n    # format except for itself and \"extras\" set in Session and Config. This\n    # behavior exists to maintain backwards compatibility for clients who\n    # expect an exact User-Agent header value.\n    expected_str = 'my user agent str'\n    if sess_name:\n        patched_session.user_agent_name = sess_name\n    if sess_version:\n        patched_session.user_agent_version = sess_version\n    if sess_extra:\n        patched_session.user_agent_extra = sess_extra\n        expected_str += f' {sess_extra}'\n    client_cfg = Config(\n        user_agent='my user agent str',\n        user_agent_extra=cfg_extra,\n        user_agent_appid=cfg_appid,\n    )\n    if cfg_extra:\n        expected_str += f' {cfg_extra}'\n    client_s3 = patched_session.create_client('s3', config=client_cfg)\n    with UACapHTTPStubber(client_s3) as stub_client:\n        client_s3.list_buckets()\n\n    assert stub_client.captured_ua_string == expected_str\n\n\n@pytest.mark.parametrize(\n    'sess_name, sess_version, cfg_appid',\n    # Produce every combination of User-Agent related config settings other\n    # than Config.user_agent which is never set in this test\n    # (``test_user_agent_from_config_replaces_default`` covers all cases where\n    # it is set) and Session.user_agent_extra and Config.user_agent_extra\n    # which both are always set in this test\n    product(\n        ('sess_name', None),\n        ('sess_version', None),\n        ('cfg_appid', None),\n    ),\n)\ndef test_user_agent_includes_extra(\n    sess_name,\n    sess_version,\n    cfg_appid,\n    patched_session,\n):\n    # Libraries and apps can use the ``Config.user_agent_extra`` and\n    # ``Session.user_agent_extra`` to append arbitrary data to the User-Agent\n    # header. Unless Config.user_agent is also set, these two fields should\n    # always appear at the end of the header value.\n    if sess_name:\n        patched_session.user_agent_name = sess_name\n    if sess_version:\n        patched_session.user_agent_version = sess_version\n    patched_session.user_agent_extra = \"sess_extra\"\n    client_cfg = Config(\n        user_agent=None,\n        user_agent_extra='cfg_extra',\n        user_agent_appid=cfg_appid,\n    )\n    client_s3 = patched_session.create_client('s3', config=client_cfg)\n    with UACapHTTPStubber(client_s3) as stub_client:\n        client_s3.list_buckets()\n\n    assert stub_client.captured_ua_string.endswith(' sess_extra cfg_extra')\n\n\n@pytest.mark.parametrize(\n    'sess_name, sess_version, sess_extra, cfg_extra',\n    # Produce every combination of User-Agent related config settings other\n    # than Config.user_agent which is never set in this test and\n    # Config.user_agent_appid which is always set in this test.\n    product(\n        ('sess_name', None),\n        ('sess_version', None),\n        ('sess_extra', None),\n        ('cfg_extra', None),\n    ),\n)\ndef test_user_agent_includes_appid(\n    sess_name,\n    sess_version,\n    sess_extra,\n    cfg_extra,\n    patched_session,\n):\n    # The User-Agent header string should always include the value set in\n    # ``Config.user_agent_appid``, unless ``Config.user_agent`` is also set\n    # (this latter case is covered in ``test_user_agent_from_config_replaces_default``).\n    if sess_name:\n        patched_session.user_agent_name = sess_name\n    if sess_version:\n        patched_session.user_agent_version = sess_version\n    if sess_extra:\n        patched_session.user_agent_extra = sess_extra\n    client_cfg = Config(\n        user_agent=None,\n        user_agent_appid='123456',\n        user_agent_extra=cfg_extra,\n    )\n    client_s3 = patched_session.create_client('s3', config=client_cfg)\n    with UACapHTTPStubber(client_s3) as stub_client:\n        client_s3.list_buckets()\n\n    uafields = stub_client.captured_ua_string.split(' ')\n    assert 'app/123456' in uafields\n\n\ndef test_user_agent_long_appid_yields_warning(patched_session, caplog):\n    # user_agent_appid config values longer than 50 characters should result\n    # in a warning\n    sixtychars = '000000000011111111112222222222333333333344444444445555555555'\n    assert len(sixtychars) > 50\n    client_cfg = Config(user_agent_appid=sixtychars)\n    client_s3 = patched_session.create_client('s3', config=client_cfg)\n    with UACapHTTPStubber(client_s3):\n        with caplog.at_level(logging.INFO):\n            client_s3.list_buckets()\n\n    assert (\n        'The configured value for user_agent_appid exceeds the maximum length'\n        in caplog.text\n    )\n\n\ndef test_user_agent_appid_gets_sanitized(patched_session, caplog):\n    # Parentheses are not valid characters in the user agent string\n    badchars = '1234('\n    client_cfg = Config(user_agent_appid=badchars)\n    client_s3 = patched_session.create_client('s3', config=client_cfg)\n\n    with UACapHTTPStubber(client_s3) as stub_client:\n        with caplog.at_level(logging.INFO):\n            client_s3.list_buckets()\n\n    # given string should be truncated to 50 characters\n    uafields = stub_client.captured_ua_string.split(' ')\n    assert 'app/1234-' in uafields\n\n\ndef test_boto3_user_agent(patched_session):\n    # emulate Boto3's behavior\n    botocore_info = f'Botocore/{patched_session.user_agent_version}'\n    if patched_session.user_agent_extra:\n        patched_session.user_agent_extra += ' ' + botocore_info\n    else:\n        patched_session.user_agent_extra = botocore_info\n    patched_session.user_agent_name = 'Boto3'\n    patched_session.user_agent_version = '9.9.9'  # Boto3 version\n\n    client_s3 = patched_session.create_client('s3')\n    with UACapHTTPStubber(client_s3) as stub_client:\n        client_s3.list_buckets()\n    # The user agent string should start with \"Boto3/9.9.9\" from the setting\n    # above, followed by Botocore's version info as metadata (\"md/...\").\n    assert stub_client.captured_ua_string.startswith(\n        f'Boto3/9.9.9 md/Botocore#{botocore_version} '\n    )\n    # The regular User-Agent header components for platform, language, ...\n    # should also be present:\n    assert ' ua/2.0 ' in stub_client.captured_ua_string\n    assert ' os/' in stub_client.captured_ua_string\n    assert ' lang/' in stub_client.captured_ua_string\n    assert ' cfg/' in stub_client.captured_ua_string\n\n\ndef test_awscli_v1_user_agent(patched_session):\n    # emulate behavior from awscli.clidriver._set_user_agent_for_session\n    patched_session.user_agent_name = 'aws-cli'\n    patched_session.user_agent_version = '1.1.1'\n    patched_session.user_agent_extra = f'botocore/{botocore_version}'\n\n    client_s3 = patched_session.create_client('s3')\n    with UACapHTTPStubber(client_s3) as stub_client:\n        client_s3.list_buckets()\n\n    # The user agent string should start with \"aws-cli/1.1.1\" from the setting\n    # above, followed by Botocore's version info as metadata (\"md/...\").\n    assert stub_client.captured_ua_string.startswith(\n        f'aws-cli/1.1.1 md/Botocore#{botocore_version} '\n    )\n    # The regular User-Agent header components for platform, language, ...\n    # should also be present:\n    assert ' ua/2.0 ' in stub_client.captured_ua_string\n    assert ' os/' in stub_client.captured_ua_string\n    assert ' lang/' in stub_client.captured_ua_string\n    assert ' cfg/' in stub_client.captured_ua_string\n\n\ndef test_awscli_v2_user_agent(patched_session):\n    # emulate behavior from awscli.clidriver._set_user_agent_for_session\n    patched_session.user_agent_name = 'aws-cli'\n    patched_session.user_agent_version = '2.2.2'\n    patched_session.user_agent_extra = 'sources/x86_64'\n    # awscli.clidriver.AWSCLIEntrypoint._run_driver\n    patched_session.user_agent_extra += ' prompt/off'\n    # from awscli.clidriver.ServiceOperation._add_customization_to_user_agent\n    patched_session.user_agent_extra += ' command/service-name.op-name'\n\n    client_s3 = patched_session.create_client('s3')\n    with UACapHTTPStubber(client_s3) as stub_client:\n        client_s3.list_buckets()\n    # The user agent string should start with \"aws-cli/1.1.1\" from the setting\n    # above, followed by Botocore's version info as metadata (\"md/...\").\n    assert stub_client.captured_ua_string.startswith(\n        f'aws-cli/2.2.2 md/Botocore#{botocore_version} '\n    )\n    assert stub_client.captured_ua_string.endswith(\n        ' sources/x86_64 prompt/off command/service-name.op-name'\n    )\n    # The regular User-Agent header components for platform, language, ...\n    # should also be present:\n    assert ' ua/2.0 ' in stub_client.captured_ua_string\n    assert ' os/' in stub_client.captured_ua_string\n    assert ' lang/' in stub_client.captured_ua_string\n    assert ' cfg/' in stub_client.captured_ua_string\n\n\ndef test_s3transfer_user_agent(patched_session):\n    # emulate behavior from s3transfer ClientFactory\n    cfg = Config(user_agent_extra='s3transfer/0.1.2 processpool')\n    client = patched_session.create_client('s3', config=cfg)\n    # s3transfer tests make assertions against the _modified_ `user_agent` field\n    # in ``client.meta.config.user_agent``. See for example\n    # ``tests.unit.test_processpool.TestClientFactory`` in s3transfer.\n    assert 'processpool' in client.meta.config.user_agent\n\n\ndef test_chalice_user_agent(patched_session):\n    # emulate behavior from chalice's cli.factory._add_chalice_user_agent\n    suffix = '{}/{}'.format(\n        patched_session.user_agent_name,\n        patched_session.user_agent_version,\n    )\n    patched_session.user_agent_name = 'aws-chalice'\n    patched_session.user_agent_version = '0.1.2'\n    patched_session.user_agent_extra = suffix\n    client_s3 = patched_session.create_client('s3')\n\n    with UACapHTTPStubber(client_s3) as stub_client:\n        client_s3.list_buckets()\n    assert stub_client.captured_ua_string.startswith(\n        f'aws-chalice/0.1.2 md/Botocore#{botocore_version} '\n    )\n", "tests/functional/test_context_params.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport pytest\n\nfrom botocore.config import Config\nfrom tests import ClientHTTPStubber, mock, patch_load_service_model\n\n# fake rulesets compatible with all fake service models below\nFAKE_RULESET_TEMPLATE = {\n    \"version\": \"1.0\",\n    \"parameters\": {},\n    \"rules\": [\n        {\n            \"conditions\": [],\n            \"type\": \"endpoint\",\n            \"endpoint\": {\n                \"url\": \"https://foo.bar\",\n                \"properties\": {},\n                \"headers\": {},\n            },\n        }\n    ],\n}\n\n# The region param is unrelated to context parameters and used as control in\n# all test cases to ascertain that ANY EndpointProvider paramaters get\n# populated.\nREGION_PARAM = {\n    \"builtIn\": \"AWS::Region\",\n    \"required\": False,\n    \"documentation\": \"\",\n    \"type\": \"String\",\n}\n\nFAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS = {\n    **FAKE_RULESET_TEMPLATE,\n    \"parameters\": {\n        \"Region\": REGION_PARAM,\n    },\n}\n\nFAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM = {\n    **FAKE_RULESET_TEMPLATE,\n    \"parameters\": {\n        \"Region\": REGION_PARAM,\n        \"FooClientContextParamName\": {\n            \"required\": False,\n            \"documentation\": \"\",\n            \"type\": \"String\",\n        },\n        \"BarClientContextParamName\": {\n            \"required\": False,\n            \"documentation\": \"\",\n            \"type\": \"String\",\n        },\n    },\n}\n\nFAKE_RULESET_WITH_STATIC_CONTEXT_PARAM = {\n    **FAKE_RULESET_TEMPLATE,\n    \"parameters\": {\n        \"Region\": REGION_PARAM,\n        \"FooStaticContextParamName\": {\n            \"required\": False,\n            \"documentation\": \"\",\n            \"type\": \"String\",\n        },\n    },\n}\n\nFAKE_RULESET_WITH_DYNAMIC_CONTEXT_PARAM = {\n    **FAKE_RULESET_TEMPLATE,\n    \"parameters\": {\n        \"Region\": REGION_PARAM,\n        \"FooDynamicContextParamName\": {\n            \"required\": False,\n            \"documentation\": \"\",\n            \"type\": \"String\",\n        },\n    },\n}\n\n# fake models for \"otherservice\"\n\nFAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS = {\n    \"version\": \"2.0\",\n    \"documentation\": \"\",\n    \"metadata\": {\n        \"apiVersion\": \"2020-02-02\",\n        \"endpointPrefix\": \"otherservice\",\n        \"protocol\": \"rest-xml\",\n        \"serviceFullName\": \"Other Service\",\n        \"serviceId\": \"Other Service\",\n        \"signatureVersion\": \"v4\",\n        \"signingName\": \"otherservice\",\n        \"uid\": \"otherservice-2020-02-02\",\n    },\n    \"operations\": {\n        \"MockOperation\": {\n            \"name\": \"MockOperation\",\n            \"http\": {\"method\": \"GET\", \"requestUri\": \"/\"},\n            \"input\": {\"shape\": \"MockOperationRequest\"},\n            \"documentation\": \"\",\n        },\n    },\n    \"shapes\": {\n        \"MockOpParam\": {\n            \"type\": \"string\",\n        },\n        \"MockOperationRequest\": {\n            \"type\": \"structure\",\n            \"required\": [\"MockOpParam\"],\n            \"members\": {\n                \"MockOpParam\": {\n                    \"shape\": \"MockOpParam\",\n                    \"documentation\": \"\",\n                    \"location\": \"uri\",\n                    \"locationName\": \"param\",\n                },\n            },\n        },\n    },\n}\n\nFAKE_MODEL_WITH_CLIENT_CONTEXT_PARAM = {\n    **FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n    \"clientContextParams\": {\n        \"FooClientContextParamName\": {\n            \"documentation\": \"My mock client context parameter\",\n            \"type\": \"string\",\n        },\n        \"BarClientContextParamName\": {\n            \"documentation\": \"My mock client context parameter\",\n            \"type\": \"string\",\n        },\n    },\n}\n\nFAKE_MODEL_WITH_STATIC_CONTEXT_PARAM = {\n    **FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n    \"operations\": {\n        \"MockOperation\": {\n            \"name\": \"MockOperation\",\n            \"http\": {\"method\": \"GET\", \"requestUri\": \"/\"},\n            \"input\": {\"shape\": \"MockOperationRequest\"},\n            \"documentation\": \"\",\n            \"staticContextParams\": {\n                \"FooStaticContextParamName\": {\n                    \"value\": \"foo-static-context-param-value\"\n                }\n            },\n        },\n    },\n}\n\nFAKE_MODEL_WITH_DYNAMIC_CONTEXT_PARAM = {\n    **FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n    \"shapes\": {\n        \"MockOpParam\": {\n            \"type\": \"string\",\n        },\n        \"MockOperationRequest\": {\n            \"type\": \"structure\",\n            \"required\": [\"MockOpParam\"],\n            \"members\": {\n                \"MockOpParam\": {\n                    \"shape\": \"MockOpParam\",\n                    \"documentation\": \"\",\n                    \"location\": \"uri\",\n                    \"locationName\": \"param\",\n                    \"contextParam\": {\"name\": \"FooDynamicContextParamName\"},\n                },\n            },\n        },\n    },\n    #\n}\n\n# fake models for s3 and s3control, the only services botocore currently\n# supports client context parameters for\n\nS3_METADATA = {\n    \"apiVersion\": \"2006-03-01\",\n    \"checksumFormat\": \"md5\",\n    \"endpointPrefix\": \"s3\",\n    \"globalEndpoint\": \"s3.amazonaws.com\",\n    \"protocol\": \"rest-xml\",\n    \"serviceAbbreviation\": \"Amazon S3\",\n    \"serviceFullName\": \"Amazon Simple Storage Service\",\n    \"serviceId\": \"S3\",\n    \"signatureVersion\": \"s3\",\n    \"uid\": \"s3-2006-03-01\",\n}\n\nFAKE_S3_MODEL_WITHOUT_ANY_CONTEXT_PARAMS = {\n    **FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n    \"metadata\": S3_METADATA,\n}\n\nFAKE_S3_MODEL_WITH_CLIENT_CONTEXT_PARAM = {\n    **FAKE_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n    \"metadata\": S3_METADATA,\n}\n\nS3CONTROL_METADATA = {\n    \"apiVersion\": \"2018-08-20\",\n    \"endpointPrefix\": \"s3-control\",\n    \"protocol\": \"rest-xml\",\n    \"serviceFullName\": \"AWS S3 Control\",\n    \"serviceId\": \"S3 Control\",\n    \"signatureVersion\": \"s3v4\",\n    \"signingName\": \"s3\",\n    \"uid\": \"s3control-2018-08-20\",\n}\n\nFAKE_S3CONTROL_MODEL_WITHOUT_ANY_CONTEXT_PARAMS = {\n    **FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n    \"metadata\": S3CONTROL_METADATA,\n}\n\nFAKE_S3CONTROL_MODEL_WITH_CLIENT_CONTEXT_PARAM = {\n    **FAKE_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n    \"metadata\": S3CONTROL_METADATA,\n}\nCLIENT_CONTEXT_PARAM_INPUT = {\n    \"foo_client_context_param_name\": \"foo_context_param_value\"\n}\nOTHER_CLIENT_CONTEXT_PARAM_INPUT = {\n    \"bar_client_context_param_name\": \"bar_value\"\n}\n\nCONFIG_WITH_S3 = Config(s3=CLIENT_CONTEXT_PARAM_INPUT)\nCONFIG_WITH_CLIENT_CONTEXT_PARAMS = Config(\n    client_context_params=CLIENT_CONTEXT_PARAM_INPUT\n)\nCONFIG_WITH_S3_AND_CLIENT_CONTEXT_PARAMS = Config(\n    s3=CLIENT_CONTEXT_PARAM_INPUT,\n    client_context_params=OTHER_CLIENT_CONTEXT_PARAM_INPUT,\n)\nCONFIG_WITH_CONFLICTING_S3_AND_CLIENT_CONTEXT_PARAMS = Config(\n    s3=CLIENT_CONTEXT_PARAM_INPUT,\n    client_context_params={\"foo_client_context_param_name\": \"bar_value\"},\n)\nNO_CTX_PARAM_EXPECTED_CALL_KWARGS = {\"Region\": \"us-east-1\"}\nCTX_PARAM_EXPECTED_CALL_KWARGS = {\n    **NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n    \"FooClientContextParamName\": \"foo_context_param_value\",\n}\nMULTIPLE_CONTEXT_PARAMS_EXPECTED_CALL_KWARGS = {\n    **CTX_PARAM_EXPECTED_CALL_KWARGS,\n    \"BarClientContextParamName\": \"bar_value\",\n}\n\n\n@pytest.mark.parametrize(\n    'service_name,service_model,ruleset,config,expected_call_kwargs',\n    [\n        # s3\n        (\n            's3',\n            FAKE_S3_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_S3,\n            CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            's3',\n            FAKE_S3_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            's3',\n            FAKE_S3_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            's3',\n            FAKE_S3_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n            FAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            's3',\n            FAKE_S3_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_CLIENT_CONTEXT_PARAMS,\n            CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        # use both s3 and client_context_params when they don't overlap\n        (\n            's3',\n            FAKE_S3_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_S3_AND_CLIENT_CONTEXT_PARAMS,\n            MULTIPLE_CONTEXT_PARAMS_EXPECTED_CALL_KWARGS,\n        ),\n        # use s3 over client_context_params when they overlap\n        (\n            's3',\n            FAKE_S3_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_CONFLICTING_S3_AND_CLIENT_CONTEXT_PARAMS,\n            CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        # s3control\n        (\n            's3control',\n            FAKE_S3CONTROL_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_S3,\n            CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            's3control',\n            FAKE_S3CONTROL_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            's3control',\n            FAKE_S3CONTROL_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            's3control',\n            FAKE_S3CONTROL_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n            FAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            's3control',\n            FAKE_S3CONTROL_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_CLIENT_CONTEXT_PARAMS,\n            CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        # use both s3 and client_context_params when they don't overlap\n        (\n            's3control',\n            FAKE_S3CONTROL_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_S3_AND_CLIENT_CONTEXT_PARAMS,\n            MULTIPLE_CONTEXT_PARAMS_EXPECTED_CALL_KWARGS,\n        ),\n        # use s3 over client_context_params when they overlap\n        (\n            's3control',\n            FAKE_S3CONTROL_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_CONFLICTING_S3_AND_CLIENT_CONTEXT_PARAMS,\n            CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        # otherservice\n        (\n            'otherservice',\n            FAKE_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            'otherservice',\n            FAKE_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            'otherservice',\n            FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            'otherservice',\n            FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n            FAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS,\n            CONFIG_WITH_S3,\n            NO_CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n        (\n            'otherservice',\n            FAKE_MODEL_WITH_CLIENT_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_CLIENT_CONTEXT_PARAM,\n            CONFIG_WITH_CLIENT_CONTEXT_PARAMS,\n            CTX_PARAM_EXPECTED_CALL_KWARGS,\n        ),\n    ],\n)\ndef test_client_context_param_sent_to_endpoint_resolver(\n    monkeypatch,\n    patched_session,\n    service_name,\n    service_model,\n    ruleset,\n    config,\n    expected_call_kwargs,\n):\n    # patch loader to return fake service model and fake endpoint ruleset\n    patch_load_service_model(\n        patched_session, monkeypatch, service_model, ruleset\n    )\n\n    # construct client using patched loader and a config object with an s3\n    # or client_context_param section that sets the foo_context_param to a value\n    client = patched_session.create_client(\n        service_name,\n        region_name='us-east-1',\n        config=config,\n    )\n\n    # Stub client to prevent a request from getting sent and ascertain that\n    # only a single request would get sent. Wrap the EndpointProvider's\n    # resolve_endpoint method for inspecting the arguments it gets called with.\n    with ClientHTTPStubber(client, strict=True) as http_stubber:\n        http_stubber.add_response(status=200)\n        with mock.patch.object(\n            client._ruleset_resolver._provider,\n            'resolve_endpoint',\n            wraps=client._ruleset_resolver._provider.resolve_endpoint,\n        ) as mock_resolve_endpoint:\n            client.mock_operation(MockOpParam='mock-op-param-value')\n\n        mock_resolve_endpoint.assert_called_once_with(**expected_call_kwargs)\n\n\n@pytest.mark.parametrize(\n    'service_name,service_model,ruleset,call_should_include_ctx_param',\n    [\n        (\n            'otherservice',\n            FAKE_MODEL_WITH_STATIC_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_STATIC_CONTEXT_PARAM,\n            True,\n        ),\n        (\n            'otherservice',\n            FAKE_MODEL_WITH_STATIC_CONTEXT_PARAM,\n            FAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS,\n            False,\n        ),\n        (\n            'otherservice',\n            FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n            FAKE_RULESET_WITH_STATIC_CONTEXT_PARAM,\n            False,\n        ),\n        (\n            'otherservice',\n            FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n            FAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS,\n            False,\n        ),\n    ],\n)\ndef test_static_context_param_sent_to_endpoint_resolver(\n    monkeypatch,\n    patched_session,\n    service_name,\n    service_model,\n    ruleset,\n    call_should_include_ctx_param,\n):\n    # patch loader to return fake service model and fake endpoint ruleset\n    patch_load_service_model(\n        patched_session, monkeypatch, service_model, ruleset\n    )\n\n    # construct client using patched loader, but no special config is required\n    # for static context param to take effect\n    client = patched_session.create_client(\n        service_name, region_name='us-east-1'\n    )\n\n    with ClientHTTPStubber(client, strict=True) as http_stubber:\n        http_stubber.add_response(status=200)\n        with mock.patch.object(\n            client._ruleset_resolver._provider,\n            'resolve_endpoint',\n            wraps=client._ruleset_resolver._provider.resolve_endpoint,\n        ) as mock_resolve_endpoint:\n            client.mock_operation(MockOpParam='mock-op-param-value')\n\n    if call_should_include_ctx_param:\n        mock_resolve_endpoint.assert_called_once_with(\n            Region='us-east-1',\n            FooStaticContextParamName='foo-static-context-param-value',\n        )\n    else:\n        mock_resolve_endpoint.assert_called_once_with(Region='us-east-1')\n\n\n@pytest.mark.parametrize(\n    'service_name,service_model,ruleset,call_should_include_ctx_param',\n    [\n        (\n            'otherservice',\n            FAKE_MODEL_WITH_DYNAMIC_CONTEXT_PARAM,\n            FAKE_RULESET_WITH_DYNAMIC_CONTEXT_PARAM,\n            True,\n        ),\n        (\n            'otherservice',\n            FAKE_MODEL_WITH_DYNAMIC_CONTEXT_PARAM,\n            FAKE_RULESET_WITHOUT_ANY_CONTEXT_PARAMS,\n            False,\n        ),\n        (\n            'otherservice',\n            FAKE_MODEL_WITHOUT_ANY_CONTEXT_PARAMS,\n            FAKE_RULESET_WITH_DYNAMIC_CONTEXT_PARAM,\n            False,\n        ),\n    ],\n)\ndef test_dynamic_context_param_sent_to_endpoint_resolver(\n    monkeypatch,\n    patched_session,\n    service_name,\n    service_model,\n    ruleset,\n    call_should_include_ctx_param,\n):\n    # patch loader to return fake service model and fake endpoint ruleset\n    patch_load_service_model(\n        patched_session, monkeypatch, service_model, ruleset\n    )\n\n    # construct client using patched loader, but no special config is required\n    # for static context param to take effect\n    client = patched_session.create_client(\n        service_name, region_name='us-east-1'\n    )\n\n    with ClientHTTPStubber(client, strict=True) as http_stubber:\n        http_stubber.add_response(status=200)\n        with mock.patch.object(\n            client._ruleset_resolver._provider,\n            'resolve_endpoint',\n            wraps=client._ruleset_resolver._provider.resolve_endpoint,\n        ) as mock_resolve_endpoint:\n            client.mock_operation(MockOpParam='mock-op-param-value')\n\n    if call_should_include_ctx_param:\n        mock_resolve_endpoint.assert_called_once_with(\n            Region='us-east-1',\n            FooDynamicContextParamName='mock-op-param-value',\n        )\n    else:\n        mock_resolve_endpoint.assert_called_once_with(Region='us-east-1')\n\n\ndef test_dynamic_context_param_from_event_handler_sent_to_endpoint_resolver(\n    monkeypatch,\n    patched_session,\n):\n    # patch loader to return fake service model and fake endpoint ruleset\n    patch_load_service_model(\n        patched_session,\n        monkeypatch,\n        FAKE_MODEL_WITH_DYNAMIC_CONTEXT_PARAM,\n        FAKE_RULESET_WITH_DYNAMIC_CONTEXT_PARAM,\n    )\n\n    # event handler for provide-client-params that modifies the value of the\n    # MockOpParam parameter\n    def change_param(params, **kwargs):\n        params['MockOpParam'] = 'mock-op-param-value-2'\n\n    client = patched_session.create_client(\n        'otherservice', region_name='us-east-1'\n    )\n    client.meta.events.register_last(\n        'provide-client-params.other-service.*', change_param\n    )\n\n    with ClientHTTPStubber(client, strict=True) as http_stubber:\n        http_stubber.add_response(status=200)\n        with mock.patch.object(\n            client._ruleset_resolver._provider,\n            'resolve_endpoint',\n            wraps=client._ruleset_resolver._provider.resolve_endpoint,\n        ) as mock_resolve_endpoint:\n            client.mock_operation(MockOpParam='mock-op-param-value-1')\n\n    mock_resolve_endpoint.assert_called_once_with(\n        Region='us-east-1',\n        FooDynamicContextParamName='mock-op-param-value-2',\n    )\n", "tests/functional/test_events.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests import BaseSessionTest, mock\n\n\nclass RecordingHandler:\n    def __init__(self):\n        self.recorded_events = []\n\n    def record(self, event_name, **kwargs):\n        self.recorded_events.append((event_name, kwargs))\n\n\nclass TestClientEvents(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client('ec2', self.region)\n\n    def test_emit_response_received(self):\n        recording_handler = RecordingHandler()\n        self.client.meta.events.register(\n            'response-received.ec2.DescribeRegions', recording_handler.record\n        )\n        with mock.patch(\n            'botocore.httpsession.URLLib3Session.send'\n        ) as mock_send:\n            response_body = (\n                b'<?xml version=\"1.0\" ?>'\n                b'<DescribeRegionsResponse xmlns=\"\">'\n                b'</DescribeRegionsResponse>'\n            )\n            mock_send.return_value = mock.Mock(\n                status_code=200, headers={}, content=response_body\n            )\n            self.client.describe_regions()\n        self.assertEqual(\n            recording_handler.recorded_events,\n            [\n                (\n                    'response-received.ec2.DescribeRegions',\n                    {\n                        'exception': None,\n                        'response_dict': {\n                            'body': response_body,\n                            'headers': {},\n                            'context': mock.ANY,\n                            'status_code': 200,\n                        },\n                        'parsed_response': {'ResponseMetadata': mock.ANY},\n                        'context': mock.ANY,\n                    },\n                )\n            ],\n        )\n\n    def test_emit_response_received_for_exception(self):\n        recording_handler = RecordingHandler()\n        self.client.meta.events.register(\n            'response-received.ec2.DescribeRegions', recording_handler.record\n        )\n        with mock.patch(\n            'botocore.httpsession.URLLib3Session.send'\n        ) as mock_send:\n            raised_exception = RuntimeError('Unexpected exception')\n            mock_send.side_effect = raised_exception\n            with self.assertRaises(RuntimeError):\n                self.client.describe_regions()\n        self.assertEqual(\n            recording_handler.recorded_events,\n            [\n                (\n                    'response-received.ec2.DescribeRegions',\n                    {\n                        'exception': raised_exception,\n                        'response_dict': None,\n                        'parsed_response': None,\n                        'context': mock.ANY,\n                    },\n                )\n            ],\n        )\n", "tests/functional/test_tagged_unions_unknown.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.session import Session\nfrom tests import unittest\n\n\nclass TestTaggedUnionsUnknown(unittest.TestCase):\n    def test_tagged_union_member_name_does_not_coincide_with_unknown_key(self):\n        # This test ensures that operation models do not use SDK_UNKNOWN_MEMBER\n        # as a member name. Thereby reserving SDK_UNKNOWN_MEMBER for the parser to\n        # set as a key on the reponse object. This is necessary when the client\n        # encounters a member that it is unaware of or not modeled.\n        session = Session()\n        for service_name in session.get_available_services():\n            service_model = session.get_service_model(service_name)\n            for shape_name in service_model.shape_names:\n                shape = service_model.shape_for(shape_name)\n                if hasattr(shape, 'is_tagged_union') and shape.is_tagged_union:\n                    self.assertNotIn('SDK_UNKNOWN_MEMBER', shape.members)\n", "tests/functional/test_modeled_exceptions.py": "# Copyright 2012-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestModeledExceptions(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = \"us-east-1\"\n\n    def _create_client(self, service):\n        client = self.session.create_client(service, self.region)\n        http_stubber = ClientHTTPStubber(client)\n        return client, http_stubber\n\n    def test_query_service(self):\n        body = (\n            b'<ErrorResponse xmlns=\"http://ses.amazonaws.com/doc/2010-12-01/\">'\n            b'<Error><Type>Sender</Type>'\n            b'<Name>foobar</Name>'\n            b'<Code>AlreadyExists</Code>'\n            b'<Message>Template already exists</Message>'\n            b'</Error></ErrorResponse>'\n        )\n        response = {\n            'Error': {\n                # NOTE: The name and type are also present here as we return\n                # the entire Error node as the 'Error' field for query\n                'Name': 'foobar',\n                'Type': 'Sender',\n                'Code': 'AlreadyExists',\n                'Message': 'Template already exists',\n            },\n            'ResponseMetadata': {\n                'HTTPStatusCode': 400,\n                'HTTPHeaders': {},\n                'RetryAttempts': 0,\n            },\n            # Modeled properties on the exception shape\n            'Name': 'foobar',\n        }\n        ses, http_stubber = self._create_client('ses')\n        exception_cls = ses.exceptions.AlreadyExistsException\n        with http_stubber as stubber:\n            stubber.add_response(status=400, headers={}, body=body)\n            with self.assertRaises(exception_cls) as assertion_context:\n                template = {\n                    'TemplateName': 'foobar',\n                    'SubjectPart': 'foo',\n                    'TextPart': 'bar',\n                }\n                ses.create_template(Template=template)\n            self.assertEqual(assertion_context.exception.response, response)\n\n    def test_rest_xml_service(self):\n        body = (\n            b'<?xml version=\"1.0\"?>\\n'\n            b'<ErrorResponse xmlns=\"http://cloudfront.amazonaws.com/doc/2019-03-26/\">'\n            b'<Error><Type>Sender</Type><Code>NoSuchDistribution</Code>'\n            b'<Message>The specified distribution does not exist.</Message>'\n            b'</Error>'\n            b'<RequestId>request-id</RequestId>'\n            b'</ErrorResponse>'\n        )\n        response = {\n            'Error': {\n                'Type': 'Sender',\n                'Code': 'NoSuchDistribution',\n                'Message': 'The specified distribution does not exist.',\n            },\n            'ResponseMetadata': {\n                'HTTPStatusCode': 404,\n                'HTTPHeaders': {},\n                'RequestId': 'request-id',\n                'RetryAttempts': 0,\n            },\n            # Modeled properties on the exception shape\n            'Message': 'The specified distribution does not exist.',\n        }\n        cloudfront, http_stubber = self._create_client('cloudfront')\n        exception_cls = cloudfront.exceptions.NoSuchDistribution\n        with http_stubber as stubber:\n            stubber.add_response(status=404, headers={}, body=body)\n            with self.assertRaises(exception_cls) as assertion_context:\n                cloudfront.get_distribution(Id='foobar')\n            self.assertEqual(assertion_context.exception.response, response)\n\n    def test_rest_json_service(self):\n        headers = {\n            'x-amzn-RequestId': 'request-id',\n            'x-amzn-ErrorType': 'FileSystemAlreadyExists:',\n        }\n        body = (\n            b'{\"ErrorCode\":\"FileSystemAlreadyExists\",'\n            b'\"FileSystemId\":\"fs-abcabc12\",'\n            b'\"Message\":\"File system already exists\"}'\n        )\n        response = {\n            'Error': {\n                'Code': 'FileSystemAlreadyExists',\n                'Message': 'File system already exists',\n            },\n            'ResponseMetadata': {\n                'HTTPStatusCode': 409,\n                'HTTPHeaders': {\n                    'x-amzn-requestid': 'request-id',\n                    'x-amzn-errortype': 'FileSystemAlreadyExists:',\n                },\n                'RequestId': 'request-id',\n                'RetryAttempts': 0,\n            },\n            # Modeled properties on the exception shape\n            'ErrorCode': 'FileSystemAlreadyExists',\n            'FileSystemId': 'fs-abcabc12',\n            'Message': 'File system already exists',\n        }\n        efs, http_stubber = self._create_client('efs')\n        exception_cls = efs.exceptions.FileSystemAlreadyExists\n        with http_stubber as stubber:\n            stubber.add_response(status=409, headers=headers, body=body)\n            with self.assertRaises(exception_cls) as assertion_context:\n                efs.create_file_system()\n            self.assertEqual(assertion_context.exception.response, response)\n\n    def test_json_service(self):\n        headers = {\n            'x-amzn-RequestId': 'request-id',\n            'x-amzn-id-2': 'id-2',\n        }\n        body = (\n            b'{\"__type\":\"ResourceNotFoundException\",'\n            b'\"message\":\"Stream not found\"}'\n        )\n        response = {\n            'Error': {\n                'Code': 'ResourceNotFoundException',\n                'Message': 'Stream not found',\n            },\n            'ResponseMetadata': {\n                'HTTPStatusCode': 400,\n                'HTTPHeaders': {\n                    'x-amzn-requestid': 'request-id',\n                    'x-amzn-id-2': 'id-2',\n                },\n                'RequestId': 'request-id',\n                'RetryAttempts': 0,\n            },\n            # Modeled properties on the exception shape\n            'message': 'Stream not found',\n        }\n        kinesis, http_stubber = self._create_client('kinesis')\n        exception_cls = kinesis.exceptions.ResourceNotFoundException\n        with http_stubber as stubber:\n            stubber.add_response(status=400, headers=headers, body=body)\n            with self.assertRaises(exception_cls) as assertion_context:\n                kinesis.describe_stream(StreamName='foobar')\n            self.assertEqual(assertion_context.exception.response, response)\n", "tests/functional/test_qbusiness.py": "# Copyright 2024 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nfrom botocore.session import get_session\n\n\ndef test_chat_removal():\n    \"\"\"Chat operation removed due to h2 requirement\"\"\"\n    session = get_session()\n    qbusiness = session.create_client('qbusiness', 'us-west-2')\n    with pytest.raises(AttributeError):\n        qbusiness.chat\n", "tests/functional/test_six_imports.py": "import ast\nimport os\n\nimport pytest\n\nimport botocore\n\nROOTDIR = os.path.dirname(botocore.__file__)\n\n\ndef _all_files():\n    for rootdir, dirnames, filenames in os.walk(ROOTDIR):\n        if 'vendored' in dirnames:\n            # We don't need to lint our vendored packages.\n            dirnames.remove('vendored')\n        for filename in filenames:\n            if not filename.endswith('.py'):\n                continue\n            yield os.path.join(rootdir, filename)\n\n\n@pytest.mark.parametrize(\"filename\", _all_files())\ndef test_no_bare_six_imports(filename):\n    with open(filename) as f:\n        contents = f.read()\n        parsed = ast.parse(contents, filename)\n        SixImportChecker(filename).visit(parsed)\n\n\nclass SixImportChecker(ast.NodeVisitor):\n    def __init__(self, filename):\n        self.filename = filename\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            if getattr(alias, 'name', '') == 'six':\n                line = self._get_line_content(self.filename, node.lineno)\n                raise AssertionError(\n                    \"A bare 'import six' was found in %s:\\n\"\n                    \"\\n%s: %s\\n\"\n                    \"Please use 'from botocore.compat import six' instead\"\n                    % (self.filename, node.lineno, line)\n                )\n\n    def visit_ImportFrom(self, node):\n        if node.module == 'six':\n            line = self._get_line_content(self.filename, node.lineno)\n            raise AssertionError(\n                \"A bare 'from six import ...' was found in %s:\\n\"\n                \"\\n%s:%s\\n\"\n                \"Please use 'from botocore.compat import six' instead\"\n                % (self.filename, node.lineno, line)\n            )\n\n    def _get_line_content(self, filename, lineno):\n        with open(filename) as f:\n            contents = f.readlines()\n            return contents[lineno - 1]\n", "tests/functional/test_config_provider.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom pathlib import Path\n\nimport pytest\n\nimport botocore.exceptions\nfrom botocore.config import Config\nfrom botocore.session import get_session\n\n_SDK_DEFAULT_CONFIGURATION_VALUES_ALLOWLIST = (\n    'retryMode',\n    'stsRegionalEndpoints',\n    's3UsEast1RegionalEndpoints',\n    'connectTimeoutInMillis',\n    'tlsNegotiationTimeoutInMillis',\n)\n\nsession = get_session()\nloader = session.get_component('data_loader')\nsdk_default_configuration = loader.load_data('sdk-default-configuration')\n\n\ndef assert_client_uses_standard_defaults(client):\n    assert client.meta.config.s3['us_east_1_regional_endpoint'] == 'regional'\n    assert client.meta.config.connect_timeout == 3.1\n    assert client.meta.endpoint_url == 'https://sts.us-west-2.amazonaws.com'\n    assert client.meta.config.retries['mode'] == 'standard'\n\n\n@pytest.mark.parametrize(\"mode\", sdk_default_configuration['base'])\ndef test_no_new_sdk_default_configuration_values(mode):\n    err_msg = (\n        f'New default configuration value {mode} introduced to '\n        f'sdk-default-configuration.json. Support for setting {mode} must be '\n        'considered and added to the DefaulConfigResolver. In addition, '\n        'must add value to _SDK_DEFAULT_CONFIGURATION_VALUES_ALLOWLIST.'\n    )\n    assert mode in _SDK_DEFAULT_CONFIGURATION_VALUES_ALLOWLIST, err_msg\n\n\ndef test_default_configurations_resolve_correctly():\n    session = get_session()\n    config = Config(defaults_mode='standard')\n    client = session.create_client(\n        'sts', config=config, region_name='us-west-2'\n    )\n    assert_client_uses_standard_defaults(client)\n\n\n@pytest.fixture\ndef loader():\n    test_models_dir = Path(__file__).parent / 'models'\n    loader = botocore.loaders.Loader()\n    loader.search_paths.insert(0, test_models_dir)\n    return loader\n\n\n@pytest.fixture\ndef session(loader):\n    session = botocore.session.Session()\n    session.register_component('data_loader', loader)\n    return session\n\n\ndef assert_client_uses_legacy_defaults(client):\n    assert client.meta.config.s3 is None\n    assert client.meta.config.connect_timeout == 60\n    assert client.meta.endpoint_url == 'https://sts.amazonaws.com'\n    assert client.meta.config.retries['mode'] == 'legacy'\n\n\ndef assert_client_uses_testing_defaults(client):\n    assert client.meta.config.s3['us_east_1_regional_endpoint'] == 'regional'\n    assert client.meta.config.connect_timeout == 9999\n    assert client.meta.endpoint_url == 'https://sts.amazonaws.com'\n    assert client.meta.config.retries['mode'] == 'standard'\n\n\nclass TestConfigurationDefaults:\n    def test_defaults_mode_resolved_from_config_store(self, session):\n        config_store = session.get_component('config_store')\n        config_store.set_config_variable('defaults_mode', 'standard')\n        client = session.create_client('sts', 'us-west-2')\n        assert_client_uses_testing_defaults(client)\n\n    def test_no_mutate_session_provider(self, session):\n        # Using the standard default mode should change the connect timeout\n        # on the client, but not the session\n        standard_client = session.create_client(\n            'sts', 'us-west-2', config=Config(defaults_mode='standard')\n        )\n        assert_client_uses_testing_defaults(standard_client)\n\n        # Using the legacy default mode should not change the connect timeout\n        # on the client or the session. By default the connect timeout for a client\n        # is 60 seconds, and unset on the session.\n        legacy_client = session.create_client('sts', 'us-west-2')\n        assert_client_uses_legacy_defaults(legacy_client)\n\n    def test_defaults_mode_resolved_from_client_config(self, session):\n        config = Config(defaults_mode='standard')\n        client = session.create_client('sts', 'us-west-2', config=config)\n        assert_client_uses_testing_defaults(client)\n\n    def test_defaults_mode_resolved_invalid_mode_exception(self, session):\n        with pytest.raises(botocore.exceptions.InvalidDefaultsMode):\n            config = Config(defaults_mode='invalid_default_mode')\n            session.create_client('sts', 'us-west-2', config=config)\n\n    def test_defaults_mode_resolved_legacy(self, session):\n        client = session.create_client('sts', 'us-west-2')\n        assert_client_uses_legacy_defaults(client)\n", "tests/functional/test_kinesis.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nfrom base64 import b64decode\nfrom uuid import uuid4\n\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestKinesisListStreams(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.stream_name = \"kinesis-test-stream\"\n        self.region = \"us-east-1\"\n        self.client = self.session.create_client(\"kinesis\", self.region)\n        self.http_stubber = ClientHTTPStubber(self.client)\n        self.http_stubber.add_response()\n\n    def assert_base64encoded_str_equals(self, encoded_str, expected_value):\n        \"\"\"Validate a value can be base64 decoded and equals expected value\"\"\"\n        try:\n            decoded_str = b64decode(encoded_str).decode(\"utf-8\")\n        except UnicodeDecodeError:\n            self.fail(\"Base64 encoded record is not a valid utf-8 string\")\n        self.assertEqual(decoded_str, expected_value)\n\n    def test_can_put_stream_blob(self):\n        unique_data = str(uuid4())\n        with self.http_stubber as stub:\n            self.client.put_record(\n                StreamName=self.stream_name,\n                PartitionKey=\"foo\",\n                Data=unique_data,\n            )\n            self.assertEqual(len(stub.requests), 1)\n            request = json.loads(stub.requests[0].body.decode(\"utf-8\"))\n            self.assertEqual(request[\"StreamName\"], self.stream_name)\n            self.assertEqual(request[\"PartitionKey\"], \"foo\")\n            self.assert_base64encoded_str_equals(request[\"Data\"], unique_data)\n\n    def test_can_put_records_single_blob(self):\n        unique_data = str(uuid4())\n        with self.http_stubber as stub:\n            self.client.put_records(\n                StreamName=self.stream_name,\n                Records=[{\"Data\": unique_data, \"PartitionKey\": \"foo\"}],\n            )\n            self.assertEqual(len(stub.requests), 1)\n            request = json.loads(stub.requests[0].body.decode(\"utf-8\"))\n            self.assertEqual(len(request[\"Records\"]), 1)\n            self.assertEqual(request[\"StreamName\"], self.stream_name)\n\n            record = request[\"Records\"][0]\n            self.assertEqual(record[\"PartitionKey\"], \"foo\")\n            self.assert_base64encoded_str_equals(record[\"Data\"], unique_data)\n\n    def test_can_put_records_multiple_blob(self):\n        with self.http_stubber as stub:\n            self.client.put_records(\n                StreamName=self.stream_name,\n                Records=[\n                    {\"Data\": \"foobar\", \"PartitionKey\": \"foo\"},\n                    {\"Data\": \"barfoo\", \"PartitionKey\": \"foo\"},\n                ],\n            )\n            self.assertEqual(len(stub.requests), 1)\n            request = json.loads(stub.requests[0].body.decode(\"utf-8\"))\n            self.assertEqual(len(request[\"Records\"]), 2)\n\n            record_foobar = request[\"Records\"][0]\n            record_barfoo = request[\"Records\"][1]\n            self.assert_base64encoded_str_equals(\n                record_foobar[\"Data\"], \"foobar\"\n            )\n            self.assert_base64encoded_str_equals(\n                record_barfoo[\"Data\"], \"barfoo\"\n            )\n", "tests/functional/test_neptune.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestNeptunePresignUrlInjection(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.client = self.session.create_client('neptune', 'us-west-2')\n        self.http_stubber = ClientHTTPStubber(self.client)\n\n    def assert_presigned_url_injected_in_request(self, body):\n        self.assertIn('PreSignedUrl', body)\n        self.assertNotIn('SourceRegion', body)\n\n    def test_create_db_cluster(self):\n        params = {\n            'DBClusterIdentifier': 'my-cluster',\n            'Engine': 'neptune',\n            'SourceRegion': 'us-east-1',\n        }\n        response_body = (\n            b'<CreateDBClusterResponse>'\n            b'<CreateDBClusterResult>'\n            b'</CreateDBClusterResult>'\n            b'</CreateDBClusterResponse>'\n        )\n        self.http_stubber.add_response(body=response_body)\n        with self.http_stubber:\n            self.client.create_db_cluster(**params)\n            sent_request = self.http_stubber.requests[0]\n            self.assert_presigned_url_injected_in_request(sent_request.body)\n\n    def test_copy_db_cluster_snapshot(self):\n        params = {\n            'SourceDBClusterSnapshotIdentifier': 'source-db',\n            'TargetDBClusterSnapshotIdentifier': 'target-db',\n            'SourceRegion': 'us-east-1',\n        }\n        response_body = (\n            b'<CopyDBClusterSnapshotResponse>'\n            b'<CopyDBClusterSnapshotResult>'\n            b'</CopyDBClusterSnapshotResult>'\n            b'</CopyDBClusterSnapshotResponse>'\n        )\n        self.http_stubber.add_response(body=response_body)\n        with self.http_stubber:\n            self.client.copy_db_cluster_snapshot(**params)\n            sent_request = self.http_stubber.requests[0]\n            self.assert_presigned_url_injected_in_request(sent_request.body)\n", "tests/functional/__init__.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport uuid\n\nimport botocore\nfrom tests import BaseEnvVar\n\n_ORIGINAL = os.environ.copy()\n# These are environment variables that allow users to control\n# the location of config files used by botocore.\n_CONFIG_FILE_ENV_VARS = [\n    'AWS_CONFIG_FILE',\n    'AWS_SHARED_CREDENTIALS_FILE',\n    'BOTO_CONFIG',\n]\n_CREDENTIAL_ENV_VARS = [\n    'AWS_ACCESS_KEY_ID',\n    'AWS_SECRET_ACCESS_KEY',\n    'AWS_SESSION_TOKEN',\n]\n\nTEST_MODELS_DIR = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)),\n    'models',\n)\n\n\ndef create_session(**kwargs):\n    # Create session and inject functional test models into loader\n    loader = _create_functional_test_loader()\n    session = botocore.session.Session(**kwargs)\n    session.register_component('data_loader', loader)\n    session.set_config_variable('credentials_file', 'noexist/foo/botocore')\n    return session\n\n\ndef _create_functional_test_loader():\n    loader = botocore.loaders.Loader()\n    loader.search_paths.insert(0, TEST_MODELS_DIR)\n    return loader\n\n\nclass FunctionalSessionTest(BaseEnvVar):\n    def setUp(self, **environ):\n        super().setUp()\n        self.environ['AWS_ACCESS_KEY_ID'] = 'access_key'\n        self.environ['AWS_SECRET_ACCESS_KEY'] = 'secret_key'\n        self.environ['AWS_CONFIG_FILE'] = 'no-exist-foo'\n        self.environ.update(environ)\n        self.session = create_session()\n        self.session.config_filename = 'no-exist-foo'\n\n\ndef setup_package():\n    # We're using a random uuid to ensure we're pointing\n    # AWS_CONFIG_FILE and other env vars at a filename that\n    # does not exist.\n    random_file = str(uuid.uuid4())\n    for varname in _CONFIG_FILE_ENV_VARS:\n        # The reason we're doing this is to ensure we don't automatically pick\n        # up any credentials a developer might have configured on their local\n        # machine.  Travis will not have any credentials available, so without\n        # this fixture setup, it's possible to have all the tests pass on your\n        # local machine (if you have credentials configured) but still fail on\n        # travis.\n        os.environ[varname] = random_file\n    for credvar in _CREDENTIAL_ENV_VARS:\n        os.environ.pop(credvar, None)\n\n\ndef teardown_package():\n    os.environ = _ORIGINAL\n", "tests/functional/test_compress.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport gzip\n\nimport pytest\n\nfrom botocore.compress import COMPRESSION_MAPPING\nfrom botocore.config import Config\nfrom tests import ALL_SERVICES, ClientHTTPStubber, patch_load_service_model\n\nFAKE_MODEL = {\n    \"version\": \"2.0\",\n    \"documentation\": \"\",\n    \"metadata\": {\n        \"apiVersion\": \"2020-02-02\",\n        \"endpointPrefix\": \"otherservice\",\n        \"protocol\": \"query\",\n        \"serviceFullName\": \"Other Service\",\n        \"serviceId\": \"Other Service\",\n        \"signatureVersion\": \"v4\",\n        \"signingName\": \"otherservice\",\n        \"uid\": \"otherservice-2020-02-02\",\n    },\n    \"operations\": {\n        \"MockOperation\": {\n            \"name\": \"MockOperation\",\n            \"http\": {\"method\": \"POST\", \"requestUri\": \"/\"},\n            \"input\": {\"shape\": \"MockOperationRequest\"},\n            \"documentation\": \"\",\n            \"requestcompression\": {\n                \"encodings\": [\"gzip\"],\n            },\n        },\n    },\n    \"shapes\": {\n        \"MockOpParamList\": {\n            \"type\": \"list\",\n            \"member\": {\"shape\": \"MockOpParam\"},\n        },\n        \"MockOpParam\": {\n            \"type\": \"structure\",\n            \"members\": {\"MockOpParam\": {\"shape\": \"MockOpParamValue\"}},\n        },\n        \"MockOpParamValue\": {\n            \"type\": \"string\",\n        },\n        \"MockOperationRequest\": {\n            \"type\": \"structure\",\n            \"required\": [\"MockOpParamList\"],\n            \"members\": {\n                \"MockOpParamList\": {\n                    \"shape\": \"MockOpParamList\",\n                    \"documentation\": \"\",\n                },\n            },\n        },\n    },\n}\n\nFAKE_RULESET = {\n    \"version\": \"1.0\",\n    \"parameters\": {},\n    \"rules\": [\n        {\n            \"conditions\": [],\n            \"type\": \"endpoint\",\n            \"endpoint\": {\n                \"url\": \"https://foo.bar\",\n                \"properties\": {},\n                \"headers\": {},\n            },\n        }\n    ],\n}\n\n\ndef _all_compression_operations():\n    for service_model in ALL_SERVICES:\n        for operation_name in service_model.operation_names:\n            operation_model = service_model.operation_model(operation_name)\n            if operation_model.request_compression is not None:\n                yield operation_model\n\n\n@pytest.mark.parametrize(\"operation_model\", _all_compression_operations())\ndef test_no_unknown_compression_encodings(operation_model):\n    for encoding in operation_model.request_compression[\"encodings\"]:\n        assert encoding in COMPRESSION_MAPPING.keys(), (\n            f\"Found unknown compression encoding '{encoding}' \"\n            f\"in operation {operation_model.name}.\"\n        )\n\n\ndef test_compression(patched_session, monkeypatch):\n    patch_load_service_model(\n        patched_session, monkeypatch, FAKE_MODEL, FAKE_RULESET\n    )\n    client = patched_session.create_client(\n        \"otherservice\",\n        region_name=\"us-west-2\",\n        config=Config(request_min_compression_size_bytes=100),\n    )\n    with ClientHTTPStubber(client, strict=True) as http_stubber:\n        http_stubber.add_response(status=200, body=b\"<response/>\")\n        params_list = [\n            {\"MockOpParam\": f\"MockOpParamValue{i}\"} for i in range(1, 21)\n        ]\n        client.mock_operation(MockOpParamList=params_list)\n        param_template = (\n            \"MockOpParamList.member.{i}.MockOpParam=MockOpParamValue{i}\"\n        )\n        serialized_params = \"&\".join(\n            param_template.format(i=i) for i in range(1, 21)\n        )\n        additional_params = \"Action=MockOperation&Version=2020-02-02\"\n        serialized_body = f\"{additional_params}&{serialized_params}\"\n        actual_body = gzip.decompress(http_stubber.requests[0].body)\n        assert serialized_body.encode('utf-8') == actual_body\n", "tests/functional/test_s3_control.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.awsrequest import AWSResponse\nfrom botocore.config import Config\nfrom tests import BaseSessionTest, mock\n\n\nclass S3ControlOperationTest(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client('s3control', self.region)\n        self.session_send_patch = mock.patch(\n            'botocore.endpoint.Endpoint._send'\n        )\n        self.http_session_send_mock = self.session_send_patch.start()\n        self.http_response = mock.Mock(spec=AWSResponse)\n        self.http_response.status_code = 200\n        self.http_response.headers = {}\n        self.http_response.content = ''\n        self.http_session_send_mock.return_value = self.http_response\n\n    def tearDown(self):\n        super(BaseSessionTest, self).tearDown()\n        self.session_send_patch.stop()\n\n    def test_does_add_account_id_to_host(self):\n        self.client.get_public_access_block(AccountId='123')\n        self.assertEqual(self.http_session_send_mock.call_count, 1)\n        request = self.http_session_send_mock.call_args_list[0][0][0]\n\n        self.assertTrue(\n            request.url.startswith(\n                'https://123.s3-control.us-west-2.amazonaws.com'\n            )\n        )\n\n    def test_does_not_remove_account_id_from_headers(self):\n        self.client.get_public_access_block(AccountId='123')\n        self.assertEqual(self.http_session_send_mock.call_count, 1)\n        request = self.http_session_send_mock.call_args_list[0][0][0]\n\n        self.assertIn('x-amz-account-id', request.headers)\n\n    def test_does_support_dualstack_endpoint(self):\n        # Re-create the client with the use_dualstack_endpoint configuration\n        # option set to True.\n        self.client = self.session.create_client(\n            's3control',\n            self.region,\n            config=Config(s3={'use_dualstack_endpoint': True}),\n        )\n        self.client.get_public_access_block(AccountId='123')\n\n        self.assertEqual(self.http_session_send_mock.call_count, 1)\n        request = self.http_session_send_mock.call_args_list[0][0][0]\n        self.assertTrue(\n            request.url.startswith(\n                'https://123.s3-control.dualstack.us-west-2.amazonaws.com'\n            )\n        )\n", "tests/functional/test_s3_control_redirects.py": "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport re\nfrom contextlib import contextmanager\n\nimport pytest\n\nfrom botocore import exceptions\nfrom botocore.compat import urlsplit\nfrom botocore.config import Config\nfrom botocore.exceptions import (\n    InvalidHostLabelError,\n    ParamValidationError,\n    UnsupportedS3ControlArnError,\n)\nfrom botocore.session import Session\nfrom tests import ClientHTTPStubber, unittest\n\nACCESSPOINT_ARN_TEST_CASES = [\n    # Outpost accesspoint arn test cases\n    {\n        'arn': (\n            'arn:aws:s3-outposts:us-west-2:123456789012:'\n            'outpost:op-01234567890123456:accesspoint:myaccesspoint'\n        ),\n        'region': 'us-west-2',\n        'config': {},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts.us-west-2.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-east-1:123456789012:outpost:op-01234567890123456:accesspoint:myaccesspoint',\n        'region': 'us-west-2',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts.us-east-1.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-east-1:123456789012:outpost:op-01234567890123456:accesspoint:myaccesspoint',\n        'region': 'us-west-2',\n        'config': {'s3': {'use_arn_region': False}},\n        'assertions': {\n            'exception': 'UnsupportedS3ControlConfigurationError',\n        },\n    },\n    {\n        'arn': 'arn:aws-cn:s3-outposts:cn-north-1:123456789012:outpost:op-01234567890123456:accesspoint:myaccesspoint',\n        'region': 'us-west-2',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'exception': 'UnsupportedS3ControlConfigurationError',\n        },\n    },\n    {\n        'arn': 'arn:aws-us-gov:s3-outposts:us-gov-east-1:123456789012:outpost:op-01234567890123456:accesspoint:myaccesspoint',\n        'region': 'us-gov-east-1',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts.us-gov-east-1.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws-us-gov:s3-outposts:us-gov-east-1:123456789012:outpost:op-01234567890123456:accesspoint:myaccesspoint',\n        'region': 'us-gov-east-1-fips',\n        'config': {'s3': {'use_arn_region': False}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts-fips.us-gov-east-1.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws-us-gov:s3-outposts:fips-us-gov-east-1:123456789012:outpost:op-01234567890123456:accesspoint:myaccesspoint',\n        'region': 'fips-us-gov-east-1',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'exception': 'UnsupportedS3ControlArnError',\n        },\n    },\n    {\n        'arn': 'arn:aws-us-gov:s3-outposts:us-gov-east-1:123456789012:outpost:op-01234567890123456:accesspoint:myaccesspoint',\n        'region': 'us-gov-east-1-fips',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts-fips.us-gov-east-1.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost:op-01234567890123456:accesspoint:myaccesspoint',\n        'config': {'s3': {'use_dualstack_endpoint': True}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts.us-west-2.api.aws',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost:op-01234567890123456:accesspoint:myaccesspoint',\n        'config': {'s3': {'use_accelerate_endpoint': True}},\n        'assertions': {\n            'exception': 'UnsupportedS3ControlConfigurationError',\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost',\n        'assertions': {\n            'exception': 'UnsupportedS3ControlArnError',\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost:op-01234567890123456',\n        'assertions': {\n            'exception': 'UnsupportedS3ControlArnError',\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost:myaccesspoint',\n        'assertions': {\n            'exception': 'UnsupportedS3ControlArnError',\n        },\n    },\n]\n\n\nBUCKET_ARN_TEST_CASES = [\n    # Outpost bucket arn test cases\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost:op-01234567890123456:bucket:mybucket',\n        'region': 'us-west-2',\n        'config': {},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts.us-west-2.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-east-1:123456789012:outpost:op-01234567890123456:bucket:mybucket',\n        'region': 'us-west-2',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts.us-east-1.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-east-1:123456789012:outpost:op-01234567890123456:bucket:mybucket',\n        'region': 'us-west-2',\n        'config': {'s3': {'use_arn_region': False}},\n        'assertions': {\n            'exception': 'UnsupportedS3ControlConfigurationError',\n        },\n    },\n    {\n        'arn': 'arn:aws-cn:s3-outposts:cn-north-1:123456789012:outpost:op-01234567890123456:bucket:mybucket',\n        'region': 'us-west-2',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'exception': 'UnsupportedS3ControlConfigurationError',\n        },\n    },\n    {\n        'arn': 'arn:aws-us-gov:s3-outposts:us-gov-east-1:123456789012:outpost:op-01234567890123456:bucket:mybucket',\n        'region': 'us-gov-east-1',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts.us-gov-east-1.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws-us-gov:s3-outposts:us-gov-east-1:123456789012:outpost:op-01234567890123456:bucket:mybucket',\n        'region': 'us-gov-east-1-fips',\n        'config': {'s3': {'use_arn_region': False}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts-fips.us-gov-east-1.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws-us-gov:s3-outposts:fips-us-gov-east-1:123456789012:outpost:op-01234567890123456:bucket:mybucket',\n        'region': 'fips-us-gov-east-1',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'exception': 'UnsupportedS3ControlArnError',\n        },\n    },\n    {\n        'arn': 'arn:aws-us-gov:s3-outposts:us-gov-east-1:123456789012:outpost:op-01234567890123456:bucket:mybucket',\n        'region': 'us-gov-east-1-fips',\n        'config': {'s3': {'use_arn_region': True}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts-fips.us-gov-east-1.amazonaws.com',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost:op-01234567890123456:bucket:mybucket',\n        'region': 'us-west-2',\n        'config': {'s3': {'use_dualstack_endpoint': True}},\n        'assertions': {\n            'signing_name': 's3-outposts',\n            'netloc': 's3-outposts.us-west-2.api.aws',\n            'headers': {\n                'x-amz-outpost-id': 'op-01234567890123456',\n                'x-amz-account-id': '123456789012',\n            },\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost',\n        'assertions': {\n            'exception': 'UnsupportedS3ControlArnError',\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost:op-01234567890123456',\n        'assertions': {\n            'exception': 'UnsupportedS3ControlArnError',\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost:bucket',\n        'assertions': {\n            'exception': 'UnsupportedS3ControlArnError',\n        },\n    },\n    {\n        'arn': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost:op-01234567890123456:bucket',\n        'assertions': {\n            'exception': 'UnsupportedS3ControlArnError',\n        },\n    },\n]\n\n\nV4_AUTH_REGEX = re.compile(\n    r'AWS4-HMAC-SHA256 Credential=\\w+/\\d+/'\n    r'(?P<region>[a-z0-9-]+)/(?P<name>[a-z0-9-]+)/'\n)\n\n\ndef _assert_signing_name(stubber, expected_name):\n    request = stubber.requests[0]\n    auth_header = request.headers['Authorization'].decode('utf-8')\n    actual_name = V4_AUTH_REGEX.match(auth_header).group('name')\n    assert expected_name == actual_name\n\n\ndef _assert_netloc(stubber, expected_netloc):\n    request = stubber.requests[0]\n    url_parts = urlsplit(request.url)\n    assert expected_netloc == url_parts.netloc\n\n\ndef _assert_header(stubber, key, value):\n    request = stubber.requests[0]\n    assert key in request.headers\n    actual_value = request.headers[key]\n    if isinstance(actual_value, bytes):\n        actual_value = actual_value.decode('utf-8')\n    assert value == actual_value\n\n\ndef _assert_headers(stubber, headers):\n    for key, value in headers.items():\n        _assert_header(stubber, key, value)\n\n\ndef _bootstrap_session():\n    session = Session()\n    session.set_credentials('access_key', 'secret_key')\n    return session\n\n\ndef _bootstrap_client(session, region, **kwargs):\n    client = session.create_client('s3control', region, **kwargs)\n    stubber = ClientHTTPStubber(client)\n    return client, stubber\n\n\ndef _bootstrap_test_case_client(session, test_case):\n    region = test_case.get('region', 'us-west-2')\n    config = test_case.get('config', {})\n    config = Config(**config)\n    return _bootstrap_client(session, region, config=config)\n\n\n@pytest.mark.parametrize(\"test_case\", ACCESSPOINT_ARN_TEST_CASES)\ndef test_accesspoint_arn_redirection(test_case):\n    session = _bootstrap_session()\n    client, stubber = _bootstrap_test_case_client(session, test_case)\n    with _assert_test_case(test_case, client, stubber):\n        client.get_access_point_policy(Name=test_case['arn'])\n\n\n@pytest.mark.parametrize(\"test_case\", BUCKET_ARN_TEST_CASES)\ndef test_bucket_arn_redirection(test_case):\n    session = _bootstrap_session()\n    client, stubber = _bootstrap_test_case_client(session, test_case)\n    with _assert_test_case(test_case, client, stubber):\n        client.get_bucket(Bucket=test_case['arn'])\n\n\n@contextmanager\ndef _assert_test_case(test_case, client, stubber):\n    stubber.add_response()\n    assertions = test_case['assertions']\n    exception_raised = None\n    try:\n        with stubber:\n            yield\n    except Exception as e:\n        if 'exception' not in assertions:\n            raise\n        exception_raised = e\n    if 'exception' in assertions:\n        exception_cls = getattr(exceptions, assertions['exception'])\n        if exception_raised is None:\n            raise RuntimeError(\n                'Expected exception \"%s\" was not raised' % exception_cls\n            )\n        error_msg = ('Expected exception \"%s\", got \"%s\"') % (\n            exception_cls,\n            type(exception_raised),\n        )\n        assert isinstance(exception_raised, exception_cls), error_msg\n    else:\n        assert len(stubber.requests) == 1\n        if 'signing_name' in assertions:\n            _assert_signing_name(stubber, assertions['signing_name'])\n        if 'headers' in assertions:\n            _assert_headers(stubber, assertions['headers'])\n        if 'netloc' in assertions:\n            _assert_netloc(stubber, assertions['netloc'])\n\n\nclass TestS3ControlRedirection(unittest.TestCase):\n    def setUp(self):\n        self.session = _bootstrap_session()\n        self.region = 'us-west-2'\n        self._bootstrap_client()\n\n    def _bootstrap_client(self, **kwargs):\n        client, stubber = _bootstrap_client(\n            self.session, self.region, **kwargs\n        )\n        self.client = client\n        self.stubber = stubber\n\n    def test_outpost_id_redirection_dualstack(self):\n        config = Config(s3={'use_dualstack_endpoint': True})\n        self._bootstrap_client(config=config)\n        self.stubber.add_response()\n        with self.stubber:\n            self.client.create_bucket(Bucket='foo', OutpostId='op-123')\n        _assert_netloc(self.stubber, 's3-outposts.us-west-2.api.aws')\n        _assert_header(self.stubber, 'x-amz-outpost-id', 'op-123')\n\n    def test_outpost_id_redirection_create_bucket(self):\n        self.stubber.add_response()\n        with self.stubber:\n            self.client.create_bucket(Bucket='foo', OutpostId='op-123')\n        _assert_netloc(self.stubber, 's3-outposts.us-west-2.amazonaws.com')\n        _assert_header(self.stubber, 'x-amz-outpost-id', 'op-123')\n\n    def test_outpost_id_redirection_list_regional_buckets(self):\n        self.stubber.add_response()\n        with self.stubber:\n            self.client.list_regional_buckets(\n                OutpostId='op-123',\n                AccountId='1234',\n            )\n        _assert_netloc(self.stubber, 's3-outposts.us-west-2.amazonaws.com')\n        _assert_header(self.stubber, 'x-amz-outpost-id', 'op-123')\n\n    def test_outpost_redirection_custom_endpoint(self):\n        self._bootstrap_client(endpoint_url='https://outpost.foo.com/')\n        self.stubber.add_response()\n        with self.stubber:\n            self.client.create_bucket(Bucket='foo', OutpostId='op-123')\n        _assert_netloc(self.stubber, 'outpost.foo.com')\n        _assert_header(self.stubber, 'x-amz-outpost-id', 'op-123')\n\n    def test_normal_ap_request_has_correct_endpoint(self):\n        self.stubber.add_response()\n        with self.stubber:\n            self.client.get_access_point_policy(Name='MyAp', AccountId='1234')\n        _assert_netloc(self.stubber, '1234.s3-control.us-west-2.amazonaws.com')\n\n    def test_normal_ap_request_custom_endpoint(self):\n        self._bootstrap_client(endpoint_url='https://example.com/')\n        self.stubber.add_response()\n        with self.stubber:\n            self.client.get_access_point_policy(Name='MyAp', AccountId='1234')\n        _assert_netloc(self.stubber, '1234.example.com')\n\n    def test_normal_bucket_request_has_correct_endpoint(self):\n        self.stubber.add_response()\n        with self.stubber:\n            self.client.create_bucket(Bucket='foo')\n        _assert_netloc(self.stubber, 's3-control.us-west-2.amazonaws.com')\n\n    def test_arn_account_id_mismatch(self):\n        arn = (\n            'arn:aws:s3-outposts:us-west-2:123456789012:outpost:'\n            'op-01234567890123456:accesspoint:myaccesspoint'\n        )\n        with self.assertRaises(UnsupportedS3ControlArnError):\n            self.client.get_access_point_policy(Name=arn, AccountId='1234')\n\n    def test_create_access_point_does_not_expand_name(self):\n        arn = (\n            'arn:aws:s3-outposts:us-west-2:123456789012:outpost:'\n            'op-01234567890123456:accesspoint:myaccesspoint'\n        )\n        # AccountId will not be backfilled by the arn expansion and thus\n        # fail parameter validation\n        with self.assertRaises(ParamValidationError):\n            self.client.create_access_point(Name=arn, Bucket='foo')\n\n    def test_arn_invalid_host_label(self):\n        config = Config(s3={'use_arn_region': True})\n        self._bootstrap_client(config=config)\n        arn = (\n            'arn:aws:s3-outposts:us-we$t-2:123456789012:outpost:'\n            'op-01234567890123456:accesspoint:myaccesspoint'\n        )\n        with self.assertRaises(InvalidHostLabelError):\n            self.client.get_access_point_policy(Name=arn)\n\n    def test_unknown_arn_format(self):\n        arn = 'arn:aws:foo:us-west-2:123456789012:bar:myresource'\n        with self.assertRaises(UnsupportedS3ControlArnError):\n            self.client.get_access_point_policy(Name=arn)\n", "tests/functional/test_session.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore.session\nfrom botocore.exceptions import ProfileNotFound\nfrom tests import mock, temporary_file, unittest\n\n\nclass TestSession(unittest.TestCase):\n    def setUp(self):\n        self.environ = {}\n        self.env_patch = mock.patch('os.environ', self.environ)\n        self.env_patch.start()\n        self.session = botocore.session.get_session()\n\n    def tearDown(self):\n        self.env_patch.stop()\n\n    def test_profile_precedence(self):\n        self.environ['AWS_PROFILE'] = 'from_env_var'\n        self.session.set_config_variable('profile', 'from_session_instance')\n        self.assertEqual(self.session.profile, 'from_session_instance')\n\n    def test_credentials_with_profile_precedence(self):\n        self.environ['AWS_PROFILE'] = 'from_env_var'\n        self.session.set_config_variable('profile', 'from_session_instance')\n        try:\n            self.session.get_credentials()\n        except ProfileNotFound as e:\n            self.assertNotIn('from_env_var', str(e))\n            self.assertIn('from_session_instance', str(e))\n\n    def test_session_profile_overrides_env_vars(self):\n        # If the \".profile\" attribute is set then the associated\n        # creds for that profile take precedence over the environment\n        # variables.\n        with temporary_file('w') as f:\n            # We test this by creating creds in two places,\n            # env vars and a fake shared creds file.  We ensure\n            # that if an explicit profile is set we pull creds\n            # from the shared creds file.\n            self.environ['AWS_ACCESS_KEY_ID'] = 'env_var_akid'\n            self.environ['AWS_SECRET_ACCESS_KEY'] = 'env_var_sak'\n            self.environ['AWS_SHARED_CREDENTIALS_FILE'] = f.name\n            f.write(\n                '[from_session_instance]\\n'\n                'aws_access_key_id=shared_creds_akid\\n'\n                'aws_secret_access_key=shared_creds_sak\\n'\n            )\n            f.flush()\n            self.session.set_config_variable(\n                'profile', 'from_session_instance'\n            )\n            creds = self.session.get_credentials()\n            self.assertEqual(creds.access_key, 'shared_creds_akid')\n            self.assertEqual(creds.secret_key, 'shared_creds_sak')\n\n    def test_profile_does_not_win_if_all_from_env_vars(self):\n        # Creds should be pulled from the env vars because\n        # if access_key/secret_key/profile are all specified on\n        # the same \"level\", then the explicit creds take\n        # precedence.\n        with temporary_file('w') as f:\n            self.environ['AWS_SHARED_CREDENTIALS_FILE'] = f.name\n            self.environ['AWS_PROFILE'] = 'myprofile'\n            # Even though we don't use the profile for credentials,\n            # if you have a profile configured in any way\n            # (env vars, set when creating a session, etc.) that profile\n            # must exist.  So we need to create an empty profile\n            # matching the value from AWS_PROFILE.\n            f.write('[myprofile]\\n')\n            f.flush()\n            self.environ['AWS_ACCESS_KEY_ID'] = 'env_var_akid'\n            self.environ['AWS_SECRET_ACCESS_KEY'] = 'env_var_sak'\n\n            creds = self.session.get_credentials()\n\n            self.assertEqual(creds.access_key, 'env_var_akid')\n            self.assertEqual(creds.secret_key, 'env_var_sak')\n\n    def test_provides_available_regions_for_same_endpoint_prefix(self):\n        regions = self.session.get_available_regions('s3')\n        self.assertTrue(regions)\n\n    def test_provides_available_regions_for_different_endpoint_prefix(self):\n        regions = self.session.get_available_regions('elb')\n        self.assertTrue(regions)\n\n    def test_does_not_provide_regions_for_mismatch_service_name(self):\n        # elb's endpoint prefix is elasticloadbalancing, but users should\n        # still be using the service name when getting regions\n        regions = self.session.get_available_regions('elasticloadbalancing')\n        self.assertEqual(regions, [])\n", "tests/functional/test_waiter_config.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport jmespath\nimport pytest\nfrom jsonschema import Draft4Validator\n\nimport botocore.session\nfrom botocore.exceptions import UnknownServiceError\nfrom botocore.utils import ArgumentGenerator\n\nWAITER_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"version\": {\"type\": \"number\"},\n        \"waiters\": {\n            \"type\": \"object\",\n            \"additionalProperties\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"type\": {\"type\": \"string\", \"enum\": [\"api\"]},\n                    \"operation\": {\"type\": \"string\"},\n                    \"description\": {\"type\": \"string\"},\n                    \"delay\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0,\n                    },\n                    \"maxAttempts\": {\"type\": \"integer\", \"minimum\": 1},\n                    \"acceptors\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"state\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\"success\", \"retry\", \"failure\"],\n                                },\n                                \"matcher\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                        \"path\",\n                                        \"pathAll\",\n                                        \"pathAny\",\n                                        \"status\",\n                                        \"error\",\n                                    ],\n                                },\n                                \"argument\": {\"type\": \"string\"},\n                                \"expected\": {\n                                    \"oneOf\": [\n                                        {\"type\": \"string\"},\n                                        {\"type\": \"number\"},\n                                        {\"type\": \"boolean\"},\n                                    ]\n                                },\n                            },\n                            \"required\": [\"state\", \"matcher\", \"expected\"],\n                            \"additionalProperties\": False,\n                        },\n                    },\n                },\n                \"required\": [\"operation\", \"delay\", \"maxAttempts\", \"acceptors\"],\n                \"additionalProperties\": False,\n            },\n        },\n    },\n    \"additionalProperties\": False,\n}\n\n\ndef _waiter_configs():\n    session = botocore.session.get_session()\n    validator = Draft4Validator(WAITER_SCHEMA)\n    for service_name in session.get_available_services():\n        client = session.create_client(service_name, 'us-east-1')\n        try:\n            # We use the loader directly here because we need the entire\n            # json document, not just the portions exposed (either\n            # internally or externally) by the WaiterModel class.\n            loader = session.get_component('data_loader')\n            waiter_model = loader.load_service_model(service_name, 'waiters-2')\n        except UnknownServiceError:\n            # The service doesn't have waiters\n            continue\n        yield validator, waiter_model, client\n\n\n@pytest.mark.parametrize(\"validator, waiter_model, client\", _waiter_configs())\ndef test_lint_waiter_configs(validator, waiter_model, client):\n    _validate_schema(validator, waiter_model)\n    for waiter_name in client.waiter_names:\n        _lint_single_waiter(client, waiter_name, client.meta.service_model)\n\n\ndef _lint_single_waiter(client, waiter_name, service_model):\n    try:\n        waiter = client.get_waiter(waiter_name)\n        # The 'acceptors' property is dynamic and will create\n        # the acceptor configs when first accessed.  This is still\n        # considered a failure to construct the waiter which is\n        # why it's in this try/except block.\n        # This catches things like:\n        # * jmespath expression compiles\n        # * matcher has a known value\n        acceptors = waiter.config.acceptors\n    except Exception as e:\n        raise AssertionError(f\"Could not create waiter '{waiter_name}': {e}\")\n    operation_name = waiter.config.operation\n    # Needs to reference an existing operation name.\n    if operation_name not in service_model.operation_names:\n        raise AssertionError(\n            \"Waiter config references unknown \"\n            \"operation: %s\" % operation_name\n        )\n    # Needs to have at least one acceptor.\n    if not waiter.config.acceptors:\n        raise AssertionError(\n            \"Waiter config must have at least \"\n            \"one acceptor state: %s\" % waiter.name\n        )\n    op_model = service_model.operation_model(operation_name)\n    for acceptor in acceptors:\n        _validate_acceptor(acceptor, op_model, waiter.name)\n\n    if not waiter.name.isalnum():\n        raise AssertionError(\n            \"Waiter name %s is not alphanumeric.\" % waiter_name\n        )\n\n\ndef _validate_schema(validator, waiter_json):\n    errors = list(e.message for e in validator.iter_errors(waiter_json))\n    if errors:\n        raise AssertionError('\\n'.join(errors))\n\n\ndef _validate_acceptor(acceptor, op_model, waiter_name):\n    if acceptor.matcher.startswith('path'):\n        expression = acceptor.argument\n        # The JMESPath expression should have the potential to match something\n        # in the response shape.\n        output_shape = op_model.output_shape\n        assert (\n            output_shape is not None\n        ), \"Waiter '{}' has JMESPath expression with no output shape: {}\".format(\n            waiter_name,\n            op_model,\n        )\n        # We want to check if the JMESPath expression makes sense.\n        # To do this, we'll generate sample output and evaluate the\n        # JMESPath expression against the output.  We'll then\n        # check a few things about this returned search result.\n        search_result = _search_jmespath_expression(expression, op_model)\n        if search_result is None:\n            raise AssertionError(\n                f\"JMESPath expression did not match anything for waiter \"\n                f\"'{waiter_name}': {expression}\"\n            )\n        if acceptor.matcher in ['pathAll', 'pathAny']:\n            assert isinstance(search_result, list), (\n                f\"Attempted to use '{acceptor.matcher}' matcher in waiter \"\n                f\"'{waiter_name}' with non list result in JMESPath expression: \"\n                f\"{expression}\"\n            )\n\n\ndef _search_jmespath_expression(expression, op_model):\n    arg_gen = ArgumentGenerator(use_member_names=True)\n    sample_output = arg_gen.generate_skeleton(op_model.output_shape)\n    search_result = jmespath.search(expression, sample_output)\n    return search_result\n", "tests/functional/test_client_metadata.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore.session\nfrom tests import unittest\n\n\nclass TestClientMeta(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n\n    def test_region_name_on_meta(self):\n        client = self.session.create_client('s3', 'us-west-2')\n        self.assertEqual(client.meta.region_name, 'us-west-2')\n\n    def test_endpoint_url_on_meta(self):\n        client = self.session.create_client(\n            's3', 'us-west-2', endpoint_url='https://foo'\n        )\n        self.assertEqual(client.meta.endpoint_url, 'https://foo')\n\n    def test_client_has_standard_partition_on_meta(self):\n        client = self.session.create_client('s3', 'us-west-2')\n        self.assertEqual(client.meta.partition, 'aws')\n\n    def test_client_has_china_partition_on_meta(self):\n        client = self.session.create_client('s3', 'cn-north-1')\n        self.assertEqual(client.meta.partition, 'aws-cn')\n\n    def test_client_has_gov_partition_on_meta(self):\n        client = self.session.create_client('s3', 'us-gov-west-1')\n        self.assertEqual(client.meta.partition, 'aws-us-gov')\n\n    def test_client_has_no_partition_on_meta_if_custom_region(self):\n        client = self.session.create_client('s3', 'myregion')\n        self.assertEqual(client.meta.partition, 'aws')\n", "tests/functional/test_paginate.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom datetime import datetime\nfrom math import ceil\n\nimport pytest\n\nfrom botocore.paginate import TokenDecoder, TokenEncoder\nfrom botocore.stub import StubAssertionError, Stubber\nfrom tests import BaseSessionTest, random_chars\n\n\nclass TestRDSPagination(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client('rds', self.region)\n        self.stubber = Stubber(self.client)\n\n    def test_can_specify_zero_marker(self):\n        service_response = {\n            'LogFileData': 'foo',\n            'Marker': '2',\n            'AdditionalDataPending': True,\n        }\n        expected_params = {\n            'DBInstanceIdentifier': 'foo',\n            'LogFileName': 'bar',\n            'NumberOfLines': 2,\n            'Marker': '0',\n        }\n        function_name = 'download_db_log_file_portion'\n\n        # The stubber will assert that the function is called with the expected\n        # parameters.\n        self.stubber.add_response(\n            function_name, service_response, expected_params\n        )\n        self.stubber.activate()\n\n        try:\n            paginator = self.client.get_paginator(function_name)\n            result = paginator.paginate(\n                DBInstanceIdentifier='foo',\n                LogFileName='bar',\n                NumberOfLines=2,\n                PaginationConfig={'StartingToken': '0', 'MaxItems': 3},\n            ).build_full_result()\n            self.assertEqual(result['LogFileData'], 'foo')\n            self.assertIn('NextToken', result)\n        except StubAssertionError as e:\n            self.fail(str(e))\n\n\nclass TestAutoscalingPagination(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client(\n            'autoscaling',\n            self.region,\n            aws_secret_access_key='foo',\n            aws_access_key_id='bar',\n            aws_session_token='baz',\n        )\n        self.stubber = Stubber(self.client)\n        self.stubber.activate()\n\n    def _setup_scaling_pagination(\n        self, page_size=200, max_items=100, total_items=600\n    ):\n        \"\"\"\n        Add to the stubber to test paginating describe_scaling_activities.\n\n        WARNING: This only handles cases where max_items cleanly divides\n        page_size.\n        \"\"\"\n        requests_per_page = page_size / max_items\n        if requests_per_page != ceil(requests_per_page):\n            raise NotImplementedError(\n                \"This only handles setup where max_items is less than \"\n                \"page_size and where max_items evenly divides page_size.\"\n            )\n        requests_per_page = int(requests_per_page)\n        num_pages = int(ceil(total_items / page_size))\n\n        previous_next_token = None\n        for i in range(num_pages):\n            page = self.create_describe_scaling_response(page_size=page_size)\n\n            # Don't create a next_token for the final page\n            if i + 1 == num_pages:\n                next_token = None\n            else:\n                next_token = random_chars(10)\n\n            expected_args = {}\n            if previous_next_token:\n                expected_args['StartingToken'] = previous_next_token\n\n            # The same page may be accessed multiple times because we are\n            # truncating it at max_items\n            for _ in range(requests_per_page - 1):\n                # The page is copied because the paginator will modify the\n                # response object, causing issues when using the stubber.\n                self.stubber.add_response(\n                    'describe_scaling_activities', page.copy()\n                )\n\n            if next_token is not None:\n                page['NextToken'] = next_token\n\n            # Copying the page here isn't necessary because it is about to\n            # be blown away anyway.\n            self.stubber.add_response('describe_scaling_activities', page)\n\n            previous_next_token = next_token\n\n    def create_describe_scaling_response(self, page_size=200):\n        \"\"\"Create a valid describe_scaling_activities response.\"\"\"\n        page = []\n        date = datetime.now()\n        for _ in range(page_size):\n            page.append(\n                {\n                    'AutoScalingGroupName': 'test',\n                    'ActivityId': random_chars(10),\n                    'Cause': 'test',\n                    'StartTime': date,\n                    'StatusCode': '200',\n                }\n            )\n        return {'Activities': page}\n\n    def test_repeated_build_full_results(self):\n        # This ensures that we can cleanly paginate using build_full_results.\n        max_items = 100\n        total_items = 600\n        self._setup_scaling_pagination(\n            max_items=max_items, total_items=total_items, page_size=200\n        )\n        paginator = self.client.get_paginator('describe_scaling_activities')\n        conf = {'MaxItems': max_items}\n\n        pagination_tokens = []\n\n        result = paginator.paginate(PaginationConfig=conf).build_full_result()\n        all_results = result['Activities']\n        while 'NextToken' in result:\n            starting_token = result['NextToken']\n            # We should never get a duplicate pagination token.\n            self.assertNotIn(starting_token, pagination_tokens)\n            pagination_tokens.append(starting_token)\n\n            conf['StartingToken'] = starting_token\n            pages = paginator.paginate(PaginationConfig=conf)\n            result = pages.build_full_result()\n            all_results.extend(result['Activities'])\n\n        self.assertEqual(len(all_results), total_items)\n\n\nclass TestCloudwatchLogsPagination(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client(\n            'logs',\n            self.region,\n            aws_secret_access_key='foo',\n            aws_access_key_id='bar',\n            aws_session_token='baz',\n        )\n        self.stubber = Stubber(self.client)\n        self.stubber.activate()\n\n    def test_token_with_triple_underscores(self):\n        response = {\n            'events': [\n                {\n                    'logStreamName': 'foobar',\n                    'timestamp': 1560195817,\n                    'message': 'a thing happened',\n                    'ingestionTime': 1560195817,\n                    'eventId': 'foo',\n                }\n            ],\n            'searchedLogStreams': [\n                {\n                    'logStreamName': 'foobar',\n                    'searchedCompletely': False,\n                }\n            ],\n        }\n        group_name = 'foo'\n        token = 'foo___bar'\n        expected_args = {\n            'logGroupName': group_name,\n            'nextToken': token,\n        }\n        self.stubber.add_response('filter_log_events', response, expected_args)\n        paginator = self.client.get_paginator('filter_log_events')\n        pages = paginator.paginate(\n            PaginationConfig={\n                'MaxItems': 1,\n                'StartingToken': token,\n            },\n            logGroupName=group_name,\n        )\n        result = pages.build_full_result()\n        self.assertEqual(len(result['events']), 1)\n\n\n@pytest.mark.parametrize(\n    \"token_dict\",\n    (\n        {'foo': 'bar'},\n        {'foo': b'bar'},\n        {'foo': {'bar': b'baz'}},\n        {'foo': ['bar', b'baz']},\n        {'foo': b'\\xff'},\n        {'foo': {'bar': b'baz', 'bin': [b'bam']}},\n    ),\n)\ndef test_token_encoding(token_dict):\n    encoded = TokenEncoder().encode(token_dict)\n    assert isinstance(encoded, str)\n    decoded = TokenDecoder().decode(encoded)\n    assert decoded == token_dict\n", "tests/functional/test_utils.py": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport shutil\nimport tempfile\n\nfrom urllib3.exceptions import LocationParseError\n\nfrom botocore.exceptions import (\n    ConnectionClosedError,\n    HTTPClientError,\n    InvalidIMDSEndpointError,\n)\nfrom botocore.utils import FileWebIdentityTokenLoader, InstanceMetadataFetcher\nfrom tests import mock, unittest\n\n\nclass TestFileWebIdentityTokenLoader(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n        self.tempdir = tempfile.mkdtemp()\n        self.token = 'totally.a.token'\n        self.token_file = os.path.join(self.tempdir, 'token.jwt')\n        self.write_token(self.token)\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n        super().tearDown()\n\n    def write_token(self, token, path=None):\n        if path is None:\n            path = self.token_file\n        with open(path, 'w') as f:\n            f.write(token)\n\n    def test_can_load_token(self):\n        loader = FileWebIdentityTokenLoader(self.token_file)\n        token = loader()\n        self.assertEqual(self.token, token)\n\n\nclass TestInstanceMetadataFetcher(unittest.TestCase):\n    def test_catch_retryable_http_errors(self):\n        with mock.patch(\n            'botocore.httpsession.URLLib3Session.send'\n        ) as send_mock:\n            fetcher = InstanceMetadataFetcher()\n            send_mock.side_effect = ConnectionClosedError(endpoint_url=\"foo\")\n            creds = fetcher.retrieve_iam_role_credentials()\n        self.assertEqual(send_mock.call_count, 2)\n        for call_instance in send_mock.call_args_list:\n            self.assertTrue(\n                call_instance[0][0].url.startswith(fetcher.get_base_url())\n            )\n        self.assertEqual(creds, {})\n\n    def test_catch_invalid_imds_error(self):\n        with mock.patch(\n            'botocore.httpsession.URLLib3Session.send'\n        ) as send_mock:\n            fetcher = InstanceMetadataFetcher()\n            e = LocationParseError(location=\"foo\")\n            send_mock.side_effect = HTTPClientError(error=e)\n            with self.assertRaises(InvalidIMDSEndpointError):\n                fetcher.retrieve_iam_role_credentials()\n", "tests/functional/test_eventbridge.py": "import json\n\nimport pytest\n\nfrom botocore.config import Config\nfrom botocore.exceptions import InvalidEndpointConfigurationError\nfrom tests import BaseSessionTest, ClientHTTPStubber, requires_crt\n\n\nclass TestClientEvents(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region = \"us-east-1\"\n\n    def create_eventbridge_client(self, region=None, **kwargs):\n        if region is None:\n            region = self.region\n        client = self.session.create_client(\"events\", region, **kwargs)\n        return client\n\n    def create_stubbed_eventbridge_client(\n        self, with_default_responses=False, **kwargs\n    ):\n        client = self.create_eventbridge_client(**kwargs)\n        http_stubber = ClientHTTPStubber(client)\n        http_stubber.start()\n        if with_default_responses:\n            http_stubber.add_response()\n            http_stubber.add_response()\n        return client, http_stubber\n\n    def _default_put_events_args(self):\n        return {\n            \"Entries\": [\n                {\n                    \"Source\": \"test\",\n                    \"Resources\": [\n                        \"resource\",\n                    ],\n                    \"DetailType\": \"my-detail\",\n                    \"Detail\": \"detail\",\n                    \"EventBusName\": \"my-bus\",\n                },\n            ]\n        }\n\n    def _assert_multi_region_endpoint(self, request, endpoint_id, suffix=None):\n        if suffix is None:\n            suffix = \"amazonaws.com\"\n        assert (\n            request.url == f\"https://{endpoint_id}.endpoint.events.{suffix}/\"\n        )\n\n    def _assert_sigv4a_headers(self, request):\n        assert request.headers[\"x-amz-region-set\"] == b\"*\"\n        assert request.headers[\"authorization\"].startswith(\n            b\"AWS4-ECDSA-P256-SHA256 Credential=\"\n        )\n\n    def _assert_params_in_body(self, request, params):\n        assert len(params) > 0\n        body = json.loads(request.body)\n        for key, value in params:\n            assert body[key] == value\n\n    def test_put_event_default_endpoint(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n        )\n        with stubber:\n            client.put_events(**self._default_put_events_args())\n        assert (\n            stubber.requests[0].url\n            == \"https://events.us-east-1.amazonaws.com/\"\n        )\n        assert b\"EndpointId\" not in stubber.requests[0].body\n\n    def test_put_event_default_endpoint_explicit_configs(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n            config=Config(\n                use_dualstack_endpoint=False,\n                use_fips_endpoint=False,\n            ),\n        )\n        with stubber:\n            client.put_events(**self._default_put_events_args())\n        assert (\n            stubber.requests[0].url\n            == \"https://events.us-east-1.amazonaws.com/\"\n        )\n        assert b\"EndpointId\" not in stubber.requests[0].body\n\n    @requires_crt()\n    def test_put_event_endpoint_id(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"abc123.456def\"\n\n        with stubber:\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n        self._assert_params_in_body(\n            stubber.requests[0],\n            [\n                (\"EndpointId\", endpoint_id),\n            ],\n        )\n        self._assert_multi_region_endpoint(stubber.requests[0], endpoint_id)\n        self._assert_sigv4a_headers(stubber.requests[0])\n\n    @requires_crt()\n    def test_put_event_endpoint_id_explicit_config(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n            config=Config(\n                use_dualstack_endpoint=False,\n                use_fips_endpoint=False,\n            ),\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"abc123.456def\"\n\n        with stubber:\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n        self._assert_params_in_body(\n            stubber.requests[0],\n            [\n                (\"EndpointId\", endpoint_id),\n            ],\n        )\n        self._assert_multi_region_endpoint(stubber.requests[0], endpoint_id)\n        self._assert_sigv4a_headers(stubber.requests[0])\n\n    @requires_crt()\n    def test_put_event_bad_endpoint_id(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"badactor.com?foo=bar\"\n\n        with pytest.raises(InvalidEndpointConfigurationError):\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n    @requires_crt()\n    def test_put_event_bad_endpoint_id_explicit_config(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n            config=Config(\n                use_dualstack_endpoint=False,\n                use_fips_endpoint=False,\n            ),\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"badactor.com?foo=bar\"\n\n        with pytest.raises(InvalidEndpointConfigurationError):\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n    @requires_crt()\n    def test_put_event_empty_endpoint_id(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"\"\n\n        with pytest.raises(InvalidEndpointConfigurationError):\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n    @requires_crt()\n    def test_put_event_empty_endpoint_id_explicit_config(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n            config=Config(\n                use_dualstack_endpoint=False,\n                use_fips_endpoint=False,\n            ),\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"\"\n\n        with pytest.raises(InvalidEndpointConfigurationError):\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n    def test_put_event_default_dualstack_endpoint(self):\n        config = Config(use_dualstack_endpoint=True, use_fips_endpoint=False)\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True, config=config\n        )\n        default_args = self._default_put_events_args()\n\n        with stubber:\n            client.put_events(**default_args)\n        assert stubber.requests[0].url == \"https://events.us-east-1.api.aws/\"\n\n    @requires_crt()\n    def test_put_events_endpoint_id_dualstack(self):\n        config = Config(use_dualstack_endpoint=True, use_fips_endpoint=False)\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True, config=config\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"abc123.456def\"\n\n        with stubber:\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n        self._assert_params_in_body(\n            stubber.requests[0],\n            [\n                (\"EndpointId\", endpoint_id),\n            ],\n        )\n        self._assert_multi_region_endpoint(\n            stubber.requests[0], endpoint_id, suffix=\"api.aws\"\n        )\n        self._assert_sigv4a_headers(stubber.requests[0])\n\n    def test_put_events_default_fips_endpoint(self):\n        config = Config(use_dualstack_endpoint=False, use_fips_endpoint=True)\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True, config=config\n        )\n        default_args = self._default_put_events_args()\n\n        with stubber:\n            client.put_events(**default_args)\n        assert (\n            stubber.requests[0].url\n            == \"https://events-fips.us-east-1.amazonaws.com/\"\n        )\n\n    @requires_crt()\n    def test_put_events_endpoint_id_fips(self):\n        config = Config(use_dualstack_endpoint=False, use_fips_endpoint=True)\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True, config=config\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"abc123.456def\"\n\n        with pytest.raises(InvalidEndpointConfigurationError):\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n    def test_put_events_default_dualstack_fips_endpoint(self):\n        config = Config(use_dualstack_endpoint=True, use_fips_endpoint=True)\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True, config=config\n        )\n        default_args = self._default_put_events_args()\n\n        with stubber:\n            client.put_events(**default_args)\n        assert (\n            stubber.requests[0].url == \"https://events-fips.us-east-1.api.aws/\"\n        )\n\n    @requires_crt()\n    def test_put_events_endpoint_id_dualstack_fips(self):\n        config = Config(use_dualstack_endpoint=True, use_fips_endpoint=True)\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True, config=config\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"abc123.456def\"\n\n        with pytest.raises(InvalidEndpointConfigurationError) as e:\n            client.put_events(EndpointId=endpoint_id, **default_args)\n        assert (\n            \"FIPS is not supported with EventBridge multi-region endpoints\"\n            in str(e.value)\n        )\n\n    def test_put_events_default_gov_endpoint(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n            region=\"us-iso-east-1\",\n        )\n        default_args = self._default_put_events_args()\n\n        with stubber:\n            client.put_events(**default_args)\n        assert (\n            stubber.requests[0].url\n            == \"https://events.us-iso-east-1.c2s.ic.gov/\"\n        )\n\n    @requires_crt()\n    def test_put_events_endpoint_id_gov(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True,\n            region=\"us-iso-east-1\",\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"abc123.456def\"\n\n        with stubber:\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n        self._assert_params_in_body(\n            stubber.requests[0],\n            [\n                (\"EndpointId\", endpoint_id),\n            ],\n        )\n        self._assert_multi_region_endpoint(\n            stubber.requests[0], endpoint_id, suffix=\"c2s.ic.gov\"\n        )\n        self._assert_sigv4a_headers(stubber.requests[0])\n\n    def test_put_events_default_custom_endpoint(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True, endpoint_url=\"https://example.org\"\n        )\n        default_args = self._default_put_events_args()\n\n        with stubber:\n            client.put_events(**default_args)\n        assert stubber.requests[0].url == \"https://example.org/\"\n\n    @requires_crt()\n    def test_put_events_endpoint_id_custom(self):\n        client, stubber = self.create_stubbed_eventbridge_client(\n            with_default_responses=True, endpoint_url=\"https://example.org\"\n        )\n        default_args = self._default_put_events_args()\n        endpoint_id = \"abc123.456def\"\n\n        with stubber:\n            client.put_events(EndpointId=endpoint_id, **default_args)\n\n        self._assert_params_in_body(\n            stubber.requests[0],\n            [\n                (\"EndpointId\", endpoint_id),\n            ],\n        )\n        assert stubber.requests[0].url == \"https://example.org/\"\n        self._assert_sigv4a_headers(stubber.requests[0])\n", "tests/functional/utils/credentialprocess.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"This is a dummy implementation of a credential provider process.\"\"\"\nimport argparse\nimport json\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--raise-error',\n        action='store_true',\n        help=(\n            'If set, this will cause the process to return a non-zero exit code '\n            'and print to stderr.'\n        ),\n    )\n    args = parser.parse_args()\n    if args.raise_error:\n        raise Exception('Failed to fetch credentials.')\n    print(\n        json.dumps(\n            {'AccessKeyId': 'spam', 'SecretAccessKey': 'eggs', 'Version': 1}\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "tests/functional/utils/__init__.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/functional/leak/test_resource_leaks.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests import BaseClientDriverTest\n\n\nclass TestDoesNotLeakMemory(BaseClientDriverTest):\n    # The user doesn't need to have credentials configured\n    # in order to run the functional tests for resource leaks.\n    # If we don't set this value and a user doesn't have creds\n    # configured, each create_client() call will have to go through\n    # the EC2 Instance Metadata provider's timeout, which can add\n    # a substantial amount of time to the total test run time.\n    INJECT_DUMMY_CREDS = True\n    # We're making up numbers here, but let's say arbitrarily\n    # that the memory can't increase by more than 10MB.\n    MAX_GROWTH_BYTES = 10 * 1024 * 1024\n\n    def test_create_single_client_memory_constant(self):\n        self.cmd('create_client', 's3')\n        self.cmd('free_clients')\n        self.record_memory()\n        for _ in range(100):\n            self.cmd('create_client', 's3')\n            self.cmd('free_clients')\n        self.record_memory()\n        start, end = self.memory_samples\n        self.assertTrue((end - start) < self.MAX_GROWTH_BYTES, (end - start))\n\n    def test_create_memory_clients_in_loop(self):\n        # We need to first create clients and free then before\n        # recording our memory samples.  This is because of two reasons:\n        # 1. Caching.  Some of the botocore internals will cache data, so\n        #    the first client created will consume more memory than subsequent\n        #    clients.  We're interested in growing memory, not total\n        #    memory usage (for now), so we we care about the memory in the\n        #    steady state case.\n        # 2. Python memory allocation.  Due to how python allocates memory\n        #    via it's small object allocator, arena's aren't freed until the\n        #    entire 256kb isn't in use.  If a single allocation in a single\n        #    pool in a single arena is still in use, the arena is not\n        #    freed.  This case is easy to hit, and pretty much any\n        #    fragmentation guarantees this case is hit.  The best we can\n        #    do is verify that memory that's released back to python's\n        #    allocator (but not to the OS) is at least reused in subsequent\n        #    requests to create botocore clients.\n        self.cmd('create_multiple_clients', '200', 's3')\n        self.cmd('free_clients')\n        self.record_memory()\n        # 500 clients in batches of 50.\n        for _ in range(10):\n            self.cmd('create_multiple_clients', '50', 's3')\n            self.cmd('free_clients')\n        self.record_memory()\n        start, end = self.memory_samples\n        self.assertTrue((end - start) < self.MAX_GROWTH_BYTES, (end - start))\n\n    def test_create_single_waiter_memory_constant(self):\n        self.cmd('create_waiter', 's3', 'bucket_exists')\n        self.cmd('free_waiters')\n        self.record_memory()\n        for _ in range(100):\n            self.cmd('create_waiter', 's3', 'bucket_exists')\n            self.cmd('free_waiters')\n        self.record_memory()\n        start, end = self.memory_samples\n        self.assertTrue((end - start) < self.MAX_GROWTH_BYTES, (end - start))\n\n    def test_create_memory_waiters_in_loop(self):\n        # See ``test_create_memory_clients_in_loop`` to understand why\n        # waiters are first initialized and then freed. Same reason applies.\n        self.cmd('create_multiple_waiters', '200', 's3', 'bucket_exists')\n        self.cmd('free_waiters')\n        self.record_memory()\n        # 500 waiters in batches of 50.\n        for _ in range(10):\n            self.cmd('create_multiple_waiters', '50', 's3', 'bucket_exists')\n            self.cmd('free_waiters')\n        self.record_memory()\n        start, end = self.memory_samples\n        self.assertTrue((end - start) < self.MAX_GROWTH_BYTES, (end - start))\n\n    def test_create_single_paginator_memory_constant(self):\n        self.cmd('create_paginator', 's3', 'list_objects')\n        self.cmd('free_paginators')\n        self.record_memory()\n        for _ in range(100):\n            self.cmd('create_paginator', 's3', 'list_objects')\n            self.cmd('free_paginators')\n        self.record_memory()\n        start, end = self.memory_samples\n        self.assertTrue((end - start) < self.MAX_GROWTH_BYTES, (end - start))\n\n    def test_create_memory_paginators_in_loop(self):\n        # See ``test_create_memory_clients_in_loop`` to understand why\n        # paginators are first initialized and then freed. Same reason applies.\n        self.cmd('create_multiple_paginators', '200', 's3', 'list_objects')\n        self.cmd('free_paginators')\n        self.record_memory()\n        # 500 waiters in batches of 50.\n        for _ in range(10):\n            self.cmd('create_multiple_paginators', '50', 's3', 'list_objects')\n            self.cmd('free_paginators')\n        self.record_memory()\n        start, end = self.memory_samples\n        self.assertTrue((end - start) < self.MAX_GROWTH_BYTES, (end - start))\n", "tests/functional/leak/__init__.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/functional/docs/test_alias.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests.functional.docs import BaseDocsFunctionalTest\nfrom tests.functional.test_alias import ALIAS_CASES\n\n\nclass TestAliasesDocumented(BaseDocsFunctionalTest):\n    def test_all_aliases_are_documented_correctly(self):\n        for case in ALIAS_CASES:\n            content = self.get_docstring_for_method(\n                case['service'], case['operation']\n            ).decode('utf-8')\n            new_name = case['new_name']\n            original_name = case['original_name']\n            param_name_template = ':param %s:'\n            param_type_template = ':type %s:'\n            param_example_template = '%s='\n\n            # Make sure the new parameters are in the documentation\n            # but the old names are not.\n            self.assertIn(param_name_template % new_name, content)\n            self.assertIn(param_type_template % new_name, content)\n            self.assertIn(param_example_template % new_name, content)\n\n            self.assertNotIn(param_name_template % original_name, content)\n            self.assertNotIn(param_type_template % original_name, content)\n            self.assertNotIn(param_example_template % original_name, content)\n", "tests/functional/docs/test_glacier.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests.functional.docs import BaseDocsFunctionalTest\n\n\nclass TestGlacierDocs(BaseDocsFunctionalTest):\n    def test_account_id(self):\n        self.assert_is_documented_as_autopopulated_param(\n            service_name='glacier',\n            method_name='abort_multipart_upload',\n            param_name='accountId',\n            doc_string='Note: this parameter is set to \"-\"',\n        )\n\n    def test_checksum(self):\n        self.assert_is_documented_as_autopopulated_param(\n            service_name='glacier',\n            method_name='upload_archive',\n            param_name='checksum',\n        )\n", "tests/functional/docs/test_lex.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests.functional.docs import BaseDocsFunctionalTest\n\n\nclass TestLexDocs(BaseDocsFunctionalTest):\n    TYPE_STRING = '{...}|[...]|123|123.4|\\'string\\'|True|None'\n\n    def test_jsonheader_docs(self):\n        docs = self.get_docstring_for_method('lex-runtime', 'post_content')\n        self.assert_contains_lines_in_order(\n            [\n                '**Request Syntax**',\n                'sessionAttributes=%s,' % self.TYPE_STRING,\n                ':type sessionAttributes: JSON serializable',\n                '**Response Syntax**',\n                '\\'slots\\': %s,' % self.TYPE_STRING,\n                '\\'sessionAttributes\\': %s' % self.TYPE_STRING,\n                '**slots** (JSON serializable)',\n                '**sessionAttributes** (JSON serializable)',\n            ],\n            docs,\n        )\n", "tests/functional/docs/test_shared_example_config.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nimport botocore.session\nfrom botocore.model import OperationNotFoundError\nfrom botocore.utils import parse_timestamp\n\n\ndef _shared_example_configs():\n    session = botocore.session.Session()\n    loader = session.get_component('data_loader')\n    services = loader.list_available_services('examples-1')\n    for service in services:\n        service_model = session.get_service_model(service)\n        example_config = loader.load_service_model(\n            service, 'examples-1', service_model.api_version\n        )\n        examples = example_config.get(\"examples\", {})\n        for operation, operation_examples in examples.items():\n            for example in operation_examples:\n                yield operation, example, service_model\n\n\n@pytest.mark.parametrize(\n    \"operation_name, example_config, service_model\", _shared_example_configs()\n)\ndef test_lint_shared_example_configs(\n    operation_name, example_config, service_model\n):\n    # The operation should actually exist\n    assert_operation_exists(service_model, operation_name)\n    operation_model = service_model.operation_model(operation_name)\n    assert_valid_values(\n        service_model.service_name, operation_model, example_config\n    )\n\n\ndef assert_valid_values(service_name, operation_model, example_config):\n    example_input = example_config.get('input')\n    input_shape = operation_model.input_shape\n    example_id = example_config['id']\n\n    if input_shape is None and example_input:\n        raise AssertionError(\n            \"Input found in example for %s from %s with id %s, but no input \"\n            \"shape is defined.\"\n            % (operation_model.name, service_name, example_id)\n        )\n\n    example_output = example_config.get('output')\n    output_shape = operation_model.output_shape\n\n    if output_shape is None and example_output:\n        raise AssertionError(\n            \"Output found in example for %s from %s with id %s, but no output \"\n            \"shape is defined.\"\n            % (operation_model.name, service_name, example_id)\n        )\n\n    try:\n        if example_input is not None and input_shape is not None:\n            _assert_valid_values(\n                input_shape, example_input, [input_shape.name]\n            )\n\n        if example_output is not None and output_shape is not None:\n            _assert_valid_values(\n                output_shape, example_output, [output_shape.name]\n            )\n    except AssertionError as e:\n        raise AssertionError(\n            \"Invalid value in example for {} from {} with id {}: {}\".format(\n                operation_model.name, service_name, example_id, e\n            )\n        )\n\n\ndef _assert_valid_values(shape, example_value, path):\n    if shape.type_name == 'timestamp':\n        _assert_valid_timestamp(example_value, path)\n    elif shape.type_name == 'structure':\n        _assert_valid_structure_values(shape, example_value, path)\n    elif shape.type_name == 'list':\n        _assert_valid_list_values(shape, example_value, path)\n    elif shape.type_name == 'map':\n        _assert_valid_map_values(shape, example_value, path)\n\n\ndef _assert_valid_structure_values(shape, example_dict, path):\n    invalid_members = [\n        k for k in example_dict.keys() if k not in shape.members\n    ]\n    if invalid_members:\n        dotted_path = '.'.join(path)\n        raise AssertionError(\n            \"Invalid members found for {}: {}\".format(\n                dotted_path, invalid_members\n            )\n        )\n\n    for member_name, example_value in example_dict.items():\n        member = shape.members[member_name]\n        _assert_valid_values(member, example_value, path + [member_name])\n\n\ndef _assert_valid_list_values(shape, example_values, path):\n    member = shape.member\n    for i, value in enumerate(example_values):\n        name = f\"{path[-1]}[{i}]\"\n        _assert_valid_values(member, value, path[:-1] + [name])\n\n\ndef _assert_valid_map_values(shape, example_value, path):\n    for key, value in example_value.items():\n        name = f'{path[-1]}[\"{key}\"]'\n        _assert_valid_values(shape.value, value, path[:-1] + [name])\n\n\ndef _assert_valid_timestamp(timestamp, path):\n    try:\n        parse_timestamp(timestamp).timetuple()\n    except Exception as e:\n        dotted_path = '.'.join(path)\n        raise AssertionError(\n            'Failed to parse timestamp {} for {}: {}'.format(\n                timestamp, dotted_path, e\n            )\n        )\n\n\ndef assert_operation_exists(service_model, operation_name):\n    try:\n        service_model.operation_model(operation_name)\n    except OperationNotFoundError:\n        raise AssertionError(\n            \"Examples found in {} for operation {} that does not exist.\".format(\n                service_model.service_name, operation_name\n            )\n        )\n", "tests/functional/docs/test_autoscaling.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests.functional.docs import BaseDocsFunctionalTest\n\n\nclass TestAutoscalingDocs(BaseDocsFunctionalTest):\n    def test_documents_encoding_of_user_data(self):\n        docs = self.get_parameter_documentation_from_service(\n            'autoscaling', 'create_launch_configuration', 'UserData'\n        )\n        self.assertIn('base64 encoded automatically', docs.decode('utf-8'))\n", "tests/functional/docs/test_ec2.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests.functional.docs import BaseDocsFunctionalTest\n\n\nclass TestEc2Docs(BaseDocsFunctionalTest):\n    def test_documents_encoding_of_user_data(self):\n        docs = self.get_parameter_documentation_from_service(\n            'ec2', 'run_instances', 'UserData'\n        )\n        self.assertIn('base64 encoded automatically', docs.decode('utf-8'))\n\n    def test_copy_snapshot_presigned_url_is_autopopulated(self):\n        self.assert_is_documented_as_autopopulated_param(\n            service_name='ec2',\n            method_name='copy_snapshot',\n            param_name='PresignedUrl',\n        )\n\n    def test_copy_snapshot_destination_region_is_autopopulated(self):\n        self.assert_is_documented_as_autopopulated_param(\n            service_name='ec2',\n            method_name='copy_snapshot',\n            param_name='DestinationRegion',\n        )\n\n    def test_idempotency_documented(self):\n        content = self.get_docstring_for_method(\n            'ec2', 'purchase_scheduled_instances'\n        )\n        # Client token should have had idempotentcy autopopulated doc appended\n        self.assert_contains_line(\n            'This field is autopopulated if not provided', content\n        )\n", "tests/functional/docs/test_s3.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore import xform_name\nfrom botocore.docs.client import ClientContextParamsDocumenter\nfrom botocore.docs.service import ServiceDocumenter\nfrom tests.functional.docs import BaseDocsFunctionalTest\n\n\nclass TestS3Docs(BaseDocsFunctionalTest):\n    def test_auto_populates_sse_customer_key_md5(self):\n        self.assert_is_documented_as_autopopulated_param(\n            service_name='s3',\n            method_name='put_object',\n            param_name='SSECustomerKeyMD5',\n        )\n\n    def test_auto_populates_copy_source_sse_customer_key_md5(self):\n        self.assert_is_documented_as_autopopulated_param(\n            service_name='s3',\n            method_name='copy_object',\n            param_name='CopySourceSSECustomerKeyMD5',\n        )\n\n    def test_hides_content_md5_when_impossible_to_provide(self):\n        modified_methods = [\n            'delete_objects',\n            'put_bucket_acl',\n            'put_bucket_cors',\n            'put_bucket_lifecycle',\n            'put_bucket_logging',\n            'put_bucket_policy',\n            'put_bucket_notification',\n            'put_bucket_tagging',\n            'put_bucket_replication',\n            'put_bucket_website',\n            'put_bucket_request_payment',\n            'put_object_acl',\n            'put_bucket_versioning',\n        ]\n        ServiceDocumenter(\n            's3', self._session, self.root_services_path\n        ).document_service()\n        for method_name in modified_methods:\n            contents = self.get_client_method_contents('s3', method_name)\n            method_contents = self.get_method_document_block(\n                method_name, contents\n            )\n            self.assertNotIn(\n                'ContentMD5=\\'string\\'', method_contents.decode('utf-8')\n            )\n\n    def test_generate_presigned_url_documented(self):\n        content = self.get_docstring_for_method('s3', 'generate_presigned_url')\n        self.assert_contains_line('generate_presigned_url', content)\n\n    def test_copy_source_documented_as_union_type(self):\n        content = self.get_docstring_for_method('s3', 'copy_object')\n        dict_form = (\n            \"{'Bucket': 'string', 'Key': 'string', 'VersionId': 'string'}\"\n        )\n        self.assert_contains_line(\n            \"CopySource='string' or %s\" % dict_form, content\n        )\n\n    def test_copy_source_param_docs_also_modified(self):\n        content = self.get_docstring_for_method('s3', 'copy_object')\n        param_docs = self.get_parameter_document_block('CopySource', content)\n        # We don't want to overspecify the test, so I've picked\n        # an arbitrary line from the customized docs.\n        self.assert_contains_line(\n            \"You can also provide this value as a dictionary\", param_docs\n        )\n\n    def test_s3_context_params_omitted(self):\n        omitted_params = ClientContextParamsDocumenter.OMITTED_CONTEXT_PARAMS\n        s3_omitted_params = omitted_params['s3']\n        content = ServiceDocumenter(\n            's3', self._session, self.root_services_path\n        ).document_service()\n        for param in s3_omitted_params:\n            param_name = f'``{xform_name(param)}``'\n            self.assert_not_contains_line(param_name, content)\n\n    def test_s3control_context_params_omitted(self):\n        omitted_params = ClientContextParamsDocumenter.OMITTED_CONTEXT_PARAMS\n        s3control_omitted_params = omitted_params['s3control']\n        content = ServiceDocumenter(\n            's3control', self._session, self.root_services_path\n        ).document_service()\n        for param in s3control_omitted_params:\n            param_name = f'``{xform_name(param)}``'\n            self.assert_not_contains_line(param_name, content)\n", "tests/functional/docs/test_streaming_body.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore import xform_name\nfrom botocore.docs.service import ServiceDocumenter\nfrom tests.functional.docs import BaseDocsFunctionalTest\n\n\nclass TestStreamingBodyDocumentation(BaseDocsFunctionalTest):\n    def test_all_streaming_body_are_properly_documented(self):\n        for service in self._session.get_available_services():\n            client = self._session.create_client(\n                service,\n                region_name='us-east-1',\n                aws_access_key_id='foo',\n                aws_secret_access_key='bar',\n            )\n            service_model = client.meta.service_model\n            for operation in service_model.operation_names:\n                operation_model = service_model.operation_model(operation)\n                if operation_model.has_streaming_output:\n                    self.assert_streaming_body_is_properly_documented(\n                        service, xform_name(operation)\n                    )\n\n    def assert_streaming_body_is_properly_documented(self, service, operation):\n        ServiceDocumenter(\n            service, self._session, self.root_services_path\n        ).document_service()\n        contents = self.get_client_method_contents(service, operation)\n        method_docs = self.get_method_document_block(operation, contents)\n        self.assert_contains_line('StreamingBody', method_docs)\n", "tests/functional/docs/__init__.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport re\nimport shutil\nimport tempfile\n\nfrom botocore.docs.service import ServiceDocumenter\nfrom botocore.session import get_session\nfrom tests import unittest\n\n\nclass BaseDocsFunctionalTest(unittest.TestCase):\n    def setUp(self):\n        self._session = get_session()\n        self.docs_root_dir = tempfile.mkdtemp()\n        self.root_services_path = os.path.join(\n            self.docs_root_dir, 'reference', 'services'\n        )\n\n    def tearDown(self):\n        shutil.rmtree(self.docs_root_dir)\n\n    def assert_contains_line(self, line, contents):\n        contents = contents.decode('utf-8')\n        self.assertIn(line, contents)\n\n    def assert_contains_lines_in_order(self, lines, contents):\n        contents = contents.decode('utf-8')\n        for line in lines:\n            self.assertIn(line, contents)\n            beginning = contents.find(line)\n            contents = contents[(beginning + len(line)) :]\n\n    def assert_not_contains_line(self, line, contents):\n        contents = contents.decode('utf-8')\n        self.assertNotIn(line, contents)\n\n    def assert_not_contains_lines(self, lines, contents):\n        contents = contents.decode('utf-8')\n        for line in lines:\n            self.assertNotIn(line, contents)\n\n    def get_client_method_contents(self, service_name, method_name):\n        service_file_path = os.path.join(\n            self.root_services_path,\n            service_name,\n            'client',\n            f'{method_name}.rst',\n        )\n        with open(service_file_path, 'rb') as f:\n            return f.read()\n\n    def get_title_section_for(self, service_name):\n        contents = (\n            ServiceDocumenter(\n                service_name, self._session, self.root_services_path\n            )\n            .document_service()\n            .decode('utf-8')\n        )\n        start_of_table_of_contents = 'Table of Contents'\n        start_index = contents.find(start_of_table_of_contents)\n        contents = contents[:start_index]\n        contents = contents.encode('utf-8')\n        return contents\n\n    def get_method_document_block(self, operation_name, contents):\n        contents = contents.decode('utf-8')\n        regex = rf'.. py:method:: ([a-zA-Z0-9]*\\.)*{operation_name}\\('\n        match = re.search(regex, contents)\n        self.assertIsNotNone(match, 'Method is not found in contents')\n        start_method_document = match.group()\n        start_index = match.start()\n        contents = contents[start_index:]\n        end_index = contents.find('.. py:method::', len(start_method_document))\n        contents = contents[:end_index]\n        return contents.encode('utf-8')\n\n    def get_parameter_document_block(self, param_name, contents):\n        contents = contents.decode('utf-8')\n        start_param_document = '  :type %s:' % param_name\n        start_index = contents.find(start_param_document)\n        self.assertNotEqual(start_index, -1, 'Param is not found in contents')\n        contents = contents[start_index:]\n        end_index = contents.find('  :type', len(start_param_document))\n        contents = contents[:end_index]\n        return contents.encode('utf-8')\n\n    def get_parameter_documentation_from_service(\n        self, service_name, method_name, param_name\n    ):\n        ServiceDocumenter(\n            service_name, self._session, self.root_services_path\n        ).document_service()\n        contents = self.get_client_method_contents(service_name, method_name)\n        method_contents = self.get_method_document_block(method_name, contents)\n        return self.get_parameter_document_block(param_name, method_contents)\n\n    def get_docstring_for_method(self, service_name, method_name):\n        ServiceDocumenter(\n            service_name, self._session, self.root_services_path\n        ).document_service()\n        contents = self.get_client_method_contents(service_name, method_name)\n        method_contents = self.get_method_document_block(method_name, contents)\n        return method_contents\n\n    def assert_is_documented_as_autopopulated_param(\n        self, service_name, method_name, param_name, doc_string=None\n    ):\n        ServiceDocumenter(\n            service_name, self._session, self.root_services_path\n        ).document_service()\n        contents = self.get_client_method_contents(service_name, method_name)\n        method_contents = self.get_method_document_block(method_name, contents)\n\n        # Ensure it is not in the example.\n        self.assert_not_contains_line(\n            '%s=\\'string\\'' % param_name, method_contents\n        )\n\n        # Ensure it is in the params.\n        param_contents = self.get_parameter_document_block(\n            param_name, method_contents\n        )\n\n        # Ensure it is not labeled as required.\n        self.assert_not_contains_line('REQUIRED', param_contents)\n\n        # Ensure the note about autopopulation was added.\n        if doc_string is None:\n            doc_string = 'Please note that this parameter is automatically'\n        self.assert_contains_line(doc_string, param_contents)\n", "tests/functional/docs/test_secretsmanager.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.service import ServiceDocumenter\nfrom tests.functional.docs import BaseDocsFunctionalTest\n\n\nclass TestSecretsManagerDocs(BaseDocsFunctionalTest):\n    def test_generate_presigned_url_is_not_documented(self):\n        documenter = ServiceDocumenter(\n            'secretsmanager', self._session, self.root_services_path\n        )\n        docs = documenter.document_service()\n        self.assert_not_contains_line('generate_presigned_url', docs)\n", "tests/functional/configured_endpoint_urls/test_configured_endpoint_url.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport json\nfrom pathlib import Path\nfrom unittest import mock\n\nimport pytest\n\nimport botocore.configprovider\nimport botocore.utils\nfrom botocore.compat import urlsplit\nfrom botocore.config import Config\nfrom tests import ClientHTTPStubber\n\nENDPOINT_TESTDATA_FILE = Path(__file__).parent / \"profile-tests.json\"\n\n\ndef dict_to_ini_section(ini_dict, section_header):\n    section_str = f'[{section_header}]\\n'\n    for key, value in ini_dict.items():\n        if isinstance(value, dict):\n            section_str += f\"{key} =\\n\"\n            for new_key, new_value in value.items():\n                section_str += f\"  {new_key}={new_value}\\n\"\n        else:\n            section_str += f\"{key}={value}\\n\"\n    return section_str + \"\\n\"\n\n\ndef create_cases():\n    with open(ENDPOINT_TESTDATA_FILE) as f:\n        test_suite = json.load(f)['testSuites'][0]\n\n    for test_case_data in test_suite['endpointUrlTests']:\n        yield pytest.param(\n            {\n                'service': test_case_data['service'],\n                'profile': test_case_data['profile'],\n                'expected_endpoint_url': test_case_data['output'][\n                    'endpointUrl'\n                ],\n                'client_args': get_create_client_args(\n                    test_suite['client_configs'].get(\n                        test_case_data['client_config'], {}\n                    )\n                ),\n                'config_file_contents': get_config_file_contents(\n                    test_case_data['profile'], test_suite\n                ),\n                'environment': test_suite['environments'].get(\n                    test_case_data['environment'], {}\n                ),\n            },\n            id=test_case_data['name'],\n        )\n\n\ndef get_create_client_args(test_case_client_config):\n    create_client_args = {}\n\n    if 'endpoint_url' in test_case_client_config:\n        create_client_args['endpoint_url'] = test_case_client_config[\n            'endpoint_url'\n        ]\n\n    if 'ignore_configured_endpoint_urls' in test_case_client_config:\n        create_client_args['config'] = Config(\n            ignore_configured_endpoint_urls=test_case_client_config[\n                'ignore_configured_endpoint_urls'\n            ]\n        )\n\n    return create_client_args\n\n\ndef get_config_file_contents(profile_name, test_suite):\n    profile = test_suite['profiles'][profile_name]\n\n    profile_str = dict_to_ini_section(\n        profile,\n        section_header=f\"profile {profile_name}\",\n    )\n\n    services_section_name = profile.get('services', None)\n\n    if services_section_name is None:\n        return profile_str\n\n    services_section = test_suite['services'][services_section_name]\n\n    service_section_str = dict_to_ini_section(\n        services_section,\n        section_header=f'services {services_section_name}',\n    )\n\n    return profile_str + service_section_str\n\n\n@pytest.fixture\ndef client_creator(tmp_path):\n    tmp_config_file_path = tmp_path / 'config'\n    environ = {'AWS_CONFIG_FILE': str(tmp_config_file_path)}\n\n    def _do_create_client(\n        service,\n        profile,\n        client_args=None,\n        config_file_contents=None,\n        environment=None,\n    ):\n        environ.update(environment)\n        with open(tmp_config_file_path, 'w') as f:\n            f.write(config_file_contents)\n            f.flush()\n\n        return botocore.session.Session(profile=profile).create_client(\n            service, **client_args\n        )\n\n    with mock.patch('os.environ', environ):\n        yield _do_create_client\n\n\ndef _normalize_endpoint(url):\n    split_endpoint = urlsplit(url)\n    actual_endpoint = f\"{split_endpoint.scheme}://{split_endpoint.netloc}\"\n    return actual_endpoint\n\n\ndef assert_client_endpoint_url(client, expected_endpoint_url):\n    assert client.meta.endpoint_url == expected_endpoint_url\n\n\ndef assert_endpoint_url_used_for_operation(\n    client, expected_endpoint_url, operation, params\n):\n    http_stubber = ClientHTTPStubber(client)\n    http_stubber.start()\n    http_stubber.add_response()\n\n    # Call an operation on the client\n    getattr(client, operation)(**params)\n\n    assert (\n        _normalize_endpoint(http_stubber.requests[0].url)\n        == expected_endpoint_url\n    )\n\n\ndef _known_service_names_and_ids():\n    my_session = botocore.session.get_session()\n    loader = my_session.get_component('data_loader')\n    available_services = loader.list_available_services('service-2')\n\n    result = []\n    for service_name in available_services:\n        model = my_session.get_service_model(service_name)\n        result.append((model.service_name, model.service_id))\n    return sorted(result)\n\n\nSERVICE_TO_OPERATION = {'s3': 'list_buckets', 'dynamodb': 'list_tables'}\n\n\n@pytest.mark.parametrize(\"test_case\", create_cases())\ndef test_resolve_configured_endpoint_url(test_case, client_creator):\n    client = client_creator(\n        service=test_case['service'],\n        profile=test_case['profile'],\n        client_args=test_case['client_args'],\n        config_file_contents=test_case['config_file_contents'],\n        environment=test_case['environment'],\n    )\n\n    assert_endpoint_url_used_for_operation(\n        client=client,\n        expected_endpoint_url=test_case['expected_endpoint_url'],\n        operation=SERVICE_TO_OPERATION[test_case['service']],\n        params={},\n    )\n\n\n@pytest.mark.parametrize(\n    'service_name,service_id', _known_service_names_and_ids()\n)\ndef test_expected_service_env_var_name_is_respected(\n    service_name, service_id, client_creator\n):\n    transformed_service_id = service_id.replace(' ', '_').upper()\n\n    client = client_creator(\n        service=service_name,\n        profile='default',\n        client_args={},\n        config_file_contents=(\n            '[profile default]\\n'\n            'aws_access_key_id=123\\n'\n            'aws_secret_access_key=456\\n'\n            'region=fake-region-10\\n'\n        ),\n        environment={\n            f'AWS_ENDPOINT_URL_{transformed_service_id}': 'https://endpoint-override'\n        },\n    )\n\n    assert_client_endpoint_url(\n        client=client, expected_endpoint_url='https://endpoint-override'\n    )\n\n\n@pytest.mark.parametrize(\n    'service_name,service_id', _known_service_names_and_ids()\n)\ndef test_expected_service_config_section_name_is_respected(\n    service_name, service_id, client_creator\n):\n    transformed_service_id = service_id.replace(' ', '_').lower()\n\n    client = client_creator(\n        service=service_name,\n        profile='default',\n        client_args={},\n        config_file_contents=(\n            f'[profile default]\\n'\n            f'services=my-services\\n'\n            f'aws_access_key_id=123\\n'\n            f'aws_secret_access_key=456\\n'\n            f'region=fake-region-10\\n\\n'\n            f'[services my-services]\\n'\n            f'{transformed_service_id} = \\n'\n            f'  endpoint_url = https://endpoint-override\\n\\n'\n        ),\n        environment={},\n    )\n\n    assert_client_endpoint_url(\n        client=client, expected_endpoint_url='https://endpoint-override'\n    )\n", "tests/functional/configured_endpoint_urls/__init__.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/functional/crt/__init__.py": "", "tests/functional/retries/test_quota.py": "import random\nimport threading\nimport time\n\nfrom botocore.retries import quota\nfrom tests import unittest\n\n\nclass TestRetryQuota(unittest.TestCase):\n    def setUp(self):\n        self.max_capacity = 50\n        self.retry_quota = quota.RetryQuota(self.max_capacity)\n        self.shutdown_threads = False\n        self.seen_capacities = []\n\n    def run_in_thread(self):\n        while not self.shutdown_threads:\n            capacity = random.randint(1, self.max_capacity)\n            self.retry_quota.acquire(capacity)\n            self.seen_capacities.append(self.retry_quota.available_capacity)\n            self.retry_quota.release(capacity)\n            self.seen_capacities.append(self.retry_quota.available_capacity)\n\n    def test_capacity_stays_within_range(self):\n        # The main thing we're after is that the available_capacity\n        # should always be 0 <= capacity <= max_capacity.\n        # So what we do is spawn a number of threads and them acquire\n        # random capacity.  They'll check that they never see an invalid\n        # capacity.\n        threads = []\n        for i in range(10):\n            threads.append(threading.Thread(target=self.run_in_thread))\n        for thread in threads:\n            thread.start()\n        # We'll let things run for a second.\n        time.sleep(1)\n        self.shutdown_threads = True\n        for thread in threads:\n            thread.join()\n        for seen_capacity in self.seen_capacities:\n            self.assertGreaterEqual(seen_capacity, 0)\n            self.assertLessEqual(seen_capacity, self.max_capacity)\n", "tests/functional/retries/__init__.py": "", "tests/functional/retries/test_bucket.py": "import random\nimport threading\nimport time\n\nfrom botocore.retries import bucket\nfrom tests import unittest\n\n\nclass InstrumentedTokenBucket(bucket.TokenBucket):\n    def _acquire(self, amount, block):\n        rval = super()._acquire(amount, block)\n        assert self._current_capacity >= 0\n        return rval\n\n\nclass TestTokenBucketThreading(unittest.TestCase):\n    def setUp(self):\n        self.shutdown_threads = False\n        self.caught_exceptions = []\n        self.acquisitions_by_thread = {}\n\n    def run_in_thread(self):\n        while not self.shutdown_threads:\n            capacity = random.randint(1, self.max_capacity)\n            self.retry_quota.acquire(capacity)\n            self.seen_capacities.append(self.retry_quota.available_capacity)\n            self.retry_quota.release(capacity)\n            self.seen_capacities.append(self.retry_quota.available_capacity)\n\n    def create_clock(self):\n        return bucket.Clock()\n\n    def test_can_change_max_rate_while_blocking(self):\n        # This isn't a stress test, we just want to verify we can change\n        # the rate at which we acquire a token.\n        min_rate = 0.1\n        max_rate = 1\n        token_bucket = bucket.TokenBucket(\n            min_rate=min_rate,\n            max_rate=max_rate,\n            clock=self.create_clock(),\n        )\n        # First we'll set the max_rate to 0.1 (min_rate).  This means that\n        # it will take 10 seconds to accumulate a single token.  We'll start\n        # a thread and have it acquire() a token.\n        # Then in the main thread we'll change the max_rate to something\n        # really quick (e.g 100).  We should immediately get a token back.\n        # This is going to be timing sensitive, but we can verify that\n        # as long as it doesn't take 10 seconds to get a token, we were\n        # able to update the rate as needed.\n        thread = threading.Thread(target=token_bucket.acquire)\n        token_bucket.max_rate = min_rate\n        start_time = time.time()\n        thread.start()\n        # This shouldn't block the main thread.\n        token_bucket.max_rate = 100\n        thread.join()\n        end_time = time.time()\n        self.assertLessEqual(end_time - start_time, 1.0 / min_rate)\n\n    def acquire_in_loop(self, token_bucket):\n        while not self.shutdown_threads:\n            try:\n                self.assertTrue(token_bucket.acquire())\n                thread_name = threading.current_thread().name\n                self.acquisitions_by_thread[thread_name] += 1\n            except Exception as e:\n                self.caught_exceptions.append(e)\n\n    def randomly_set_max_rate(self, token_bucket, min_val, max_val):\n        while not self.shutdown_threads:\n            new_rate = random.randint(min_val, max_val)\n            token_bucket.max_rate = new_rate\n            time.sleep(0.01)\n\n    def test_stress_test_token_bucket(self):\n        token_bucket = InstrumentedTokenBucket(\n            max_rate=10,\n            clock=self.create_clock(),\n        )\n        all_threads = []\n        for _ in range(2):\n            all_threads.append(\n                threading.Thread(\n                    target=self.randomly_set_max_rate,\n                    args=(token_bucket, 30, 200),\n                )\n            )\n        for _ in range(10):\n            t = threading.Thread(\n                target=self.acquire_in_loop, args=(token_bucket,)\n            )\n            self.acquisitions_by_thread[t.name] = 0\n            all_threads.append(t)\n        for thread in all_threads:\n            thread.start()\n        try:\n            # If you're working on this code you can bump this number way\n            # up to stress test it more locally.\n            time.sleep(3)\n        finally:\n            self.shutdown_threads = True\n            for thread in all_threads:\n                thread.join()\n        # Verify all threads complete sucessfully\n        self.assertEqual(self.caught_exceptions, [])\n", "tests/functional/csm/test_monitoring.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under thimport mock\nimport contextlib\nimport copy\nimport json\nimport logging\nimport os\nimport socket\nimport threading\n\nimport pytest\n\nimport botocore.config\nimport botocore.exceptions\nimport botocore.session\nfrom botocore import xform_name\nfrom tests import ClientHTTPStubber, mock, temporary_file\n\nlogger = logging.getLogger(__name__)\n\nCASES_FILE = os.path.join(os.path.dirname(__file__), 'cases.json')\nDATA_DIR = os.path.join(os.path.dirname(__file__), 'data/')\n\n\nclass RetryableException(botocore.exceptions.EndpointConnectionError):\n    fmt = '{message}'\n\n\nclass NonRetryableException(Exception):\n    pass\n\n\nEXPECTED_EXCEPTIONS_THROWN = (\n    botocore.exceptions.ClientError,\n    NonRetryableException,\n    RetryableException,\n)\n\n\ndef _load_test_cases():\n    with open(CASES_FILE) as f:\n        loaded_tests = json.loads(f.read())\n    test_cases = _get_cases_with_defaults(loaded_tests)\n    _replace_expected_anys(test_cases)\n    return test_cases\n\n\ndef _get_cases_with_defaults(loaded_tests):\n    cases = []\n    defaults = loaded_tests['defaults']\n    for case in loaded_tests['cases']:\n        base = copy.deepcopy(defaults)\n        base.update(case)\n        cases.append(base)\n    return cases\n\n\ndef _replace_expected_anys(test_cases):\n    for case in test_cases:\n        for expected_event in case['expectedMonitoringEvents']:\n            for entry, value in expected_event.items():\n                if value in ['ANY_STR', 'ANY_INT']:\n                    expected_event[entry] = mock.ANY\n\n\n@pytest.mark.parametrize(\"test_case\", _load_test_cases())\ndef test_client_monitoring(test_case):\n    _run_test_case(test_case)\n\n\n@contextlib.contextmanager\ndef _configured_session(case_configuration, listener_port):\n    environ = {\n        'AWS_ACCESS_KEY_ID': case_configuration['accessKey'],\n        'AWS_SECRET_ACCESS_KEY': 'secret-key',\n        'AWS_DEFAULT_REGION': case_configuration['region'],\n        'AWS_DATA_PATH': DATA_DIR,\n        'AWS_CSM_PORT': listener_port,\n    }\n    if 'sessionToken' in case_configuration:\n        environ['AWS_SESSION_TOKEN'] = case_configuration['sessionToken']\n    environ.update(case_configuration['environmentVariables'])\n    with temporary_file('w') as f:\n        _setup_shared_config(\n            f, case_configuration['sharedConfigFile'], environ\n        )\n        with mock.patch('os.environ', environ):\n            session = botocore.session.Session()\n            if 'maxRetries' in case_configuration:\n                _setup_max_retry_attempts(session, case_configuration)\n            yield session\n\n\ndef _setup_shared_config(fileobj, shared_config_options, environ):\n    fileobj.write('[default]\\n')\n    for key, value in shared_config_options.items():\n        fileobj.write(f'{key} = {value}\\n')\n    fileobj.flush()\n    environ['AWS_CONFIG_FILE'] = fileobj.name\n\n\ndef _setup_max_retry_attempts(session, case_configuration):\n    config = botocore.config.Config(\n        retries={'max_attempts': case_configuration['maxRetries']}\n    )\n    session.set_default_client_config(config)\n\n\ndef _run_test_case(case):\n    with MonitoringListener() as listener:\n        with _configured_session(\n            case['configuration'], listener.port\n        ) as session:\n            for api_call in case['apiCalls']:\n                _make_api_call(session, api_call)\n    assert listener.received_events == case['expectedMonitoringEvents']\n\n\ndef _make_api_call(session, api_call):\n    client = session.create_client(\n        api_call['serviceId'].lower().replace(' ', '')\n    )\n    operation_name = api_call['operationName']\n    client_method = getattr(client, xform_name(operation_name))\n    with _stubbed_http_layer(client, api_call['attemptResponses']):\n        try:\n            client_method(**api_call['params'])\n        except EXPECTED_EXCEPTIONS_THROWN:\n            pass\n\n\n@contextlib.contextmanager\ndef _stubbed_http_layer(client, attempt_responses):\n    with ClientHTTPStubber(client) as stubber:\n        _add_stubbed_responses(stubber, attempt_responses)\n        yield\n\n\ndef _add_stubbed_responses(stubber, attempt_responses):\n    for attempt_response in attempt_responses:\n        if 'sdkException' in attempt_response:\n            sdk_exception = attempt_response['sdkException']\n            _add_sdk_exception(\n                stubber, sdk_exception['message'], sdk_exception['isRetryable']\n            )\n        else:\n            _add_stubbed_response(stubber, attempt_response)\n\n\ndef _add_sdk_exception(stubber, message, is_retryable):\n    if is_retryable:\n        stubber.responses.append(RetryableException(message=message))\n    else:\n        stubber.responses.append(NonRetryableException(message))\n\n\ndef _add_stubbed_response(stubber, attempt_response):\n    headers = attempt_response['responseHeaders']\n    status_code = attempt_response['httpStatus']\n    if 'errorCode' in attempt_response:\n        error = {\n            '__type': attempt_response['errorCode'],\n            'message': attempt_response['errorMessage'],\n        }\n        content = json.dumps(error).encode('utf-8')\n    else:\n        content = b'{}'\n    stubber.add_response(status=status_code, headers=headers, body=content)\n\n\nclass MonitoringListener(threading.Thread):\n    _PACKET_SIZE = 1024 * 8\n\n    def __init__(self, port=0):\n        threading.Thread.__init__(self)\n        self._socket = None\n        self.port = port\n        self.received_events = []\n\n    def __enter__(self):\n        self._socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        self._socket.bind(('127.0.0.1', self.port))\n        # The socket may have been assigned to an unused port so we\n        # reset the port member after binding.\n        self.port = self._socket.getsockname()[1]\n        self.start()\n        return self\n\n    def __exit__(self, *args):\n        self._socket.sendto(b'', ('127.0.0.1', self.port))\n        self.join()\n        self._socket.close()\n\n    def run(self):\n        logger.debug('Started listener')\n        while True:\n            data = self._socket.recv(self._PACKET_SIZE)\n            logger.debug('Received: %s', data.decode('utf-8'))\n            if not data:\n                return\n            self.received_events.append(json.loads(data.decode('utf-8')))\n", "tests/functional/csm/__init__.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/unit/test_signers.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\nimport json\n\nimport pytest\nfrom dateutil.tz import tzutc\n\nimport botocore\nimport botocore.auth\nimport botocore.awsrequest\nimport botocore.session\nfrom botocore.config import Config\nfrom botocore.credentials import Credentials, ReadOnlyCredentials\nfrom botocore.exceptions import (\n    NoRegionError,\n    ParamValidationError,\n    UnknownClientMethodError,\n    UnknownSignatureVersionError,\n    UnsupportedSignatureVersionError,\n)\nfrom botocore.hooks import HierarchicalEmitter\nfrom botocore.model import ServiceId\nfrom botocore.signers import (\n    CloudFrontSigner,\n    RequestSigner,\n    S3PostPresigner,\n    generate_db_auth_token,\n)\nfrom tests import assert_url_equal, mock, unittest\n\n\n@pytest.fixture\ndef polly_client():\n    session = botocore.session.get_session()\n    session.set_credentials('key', 'secret')\n    return session.create_client('polly', region_name='us-west-2')\n\n\nclass BaseSignerTest(unittest.TestCase):\n    def setUp(self):\n        self.credentials = Credentials('key', 'secret')\n        self.emitter = mock.Mock()\n        self.emitter.emit_until_response.return_value = (None, None)\n        self.signer = RequestSigner(\n            ServiceId('service_name'),\n            'region_name',\n            'signing_name',\n            'v4',\n            self.credentials,\n            self.emitter,\n        )\n        self.fixed_credentials = self.credentials.get_frozen_credentials()\n        self.request = botocore.awsrequest.AWSRequest()\n\n\nclass TestSigner(BaseSignerTest):\n    def test_region_name(self):\n        self.assertEqual(self.signer.region_name, 'region_name')\n\n    def test_signature_version(self):\n        self.assertEqual(self.signer.signature_version, 'v4')\n\n    def test_signing_name(self):\n        self.assertEqual(self.signer.signing_name, 'signing_name')\n\n    def test_region_required_for_sigv4(self):\n        self.signer = RequestSigner(\n            ServiceId('service_name'),\n            None,\n            'signing_name',\n            'v4',\n            self.credentials,\n            self.emitter,\n        )\n\n        with self.assertRaises(NoRegionError):\n            self.signer.sign('operation_name', self.request)\n\n    def test_get_auth(self):\n        auth_cls = mock.Mock()\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4': auth_cls}):\n            auth = self.signer.get_auth('service_name', 'region_name')\n\n            self.assertEqual(auth, auth_cls.return_value)\n            auth_cls.assert_called_with(\n                credentials=self.fixed_credentials,\n                service_name='service_name',\n                region_name='region_name',\n            )\n\n    def test_get_auth_signature_override(self):\n        auth_cls = mock.Mock()\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'v4-custom': auth_cls}\n        ):\n            auth = self.signer.get_auth(\n                'service_name', 'region_name', signature_version='v4-custom'\n            )\n\n            self.assertEqual(auth, auth_cls.return_value)\n            auth_cls.assert_called_with(\n                credentials=self.fixed_credentials,\n                service_name='service_name',\n                region_name='region_name',\n            )\n\n    def test_get_auth_bad_override(self):\n        with self.assertRaises(UnknownSignatureVersionError):\n            self.signer.get_auth(\n                'service_name', 'region_name', signature_version='bad'\n            )\n\n    def test_emits_choose_signer(self):\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'v4': mock.Mock()}\n        ):\n            self.signer.sign('operation_name', self.request)\n\n        self.emitter.emit_until_response.assert_called_with(\n            'choose-signer.service_name.operation_name',\n            signing_name='signing_name',\n            region_name='region_name',\n            signature_version='v4',\n            context=mock.ANY,\n        )\n\n    def test_choose_signer_override(self):\n        auth = mock.Mock()\n        auth.REQUIRES_REGION = False\n        self.emitter.emit_until_response.return_value = (None, 'custom')\n\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'custom': auth}):\n            self.signer.sign('operation_name', self.request)\n\n        auth.assert_called_with(credentials=self.fixed_credentials)\n        auth.return_value.add_auth.assert_called_with(self.request)\n\n    def test_emits_before_sign(self):\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'v4': mock.Mock()}\n        ):\n            self.signer.sign('operation_name', self.request)\n\n        self.emitter.emit.assert_called_with(\n            'before-sign.service_name.operation_name',\n            request=self.request,\n            signing_name='signing_name',\n            region_name='region_name',\n            signature_version='v4',\n            request_signer=self.signer,\n            operation_name='operation_name',\n        )\n\n    def test_disable_signing(self):\n        # Returning botocore.UNSIGNED from choose-signer disables signing!\n        auth = mock.Mock()\n        self.emitter.emit_until_response.return_value = (\n            None,\n            botocore.UNSIGNED,\n        )\n\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4': auth}):\n            self.signer.sign('operation_name', self.request)\n\n        auth.assert_not_called()\n\n    def test_generate_url_emits_choose_signer(self):\n        request_dict = {\n            'headers': {},\n            'url': 'https://foo.com',\n            'body': b'',\n            'url_path': '/',\n            'method': 'GET',\n            'context': {},\n        }\n\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'v4': mock.Mock()}\n        ):\n            self.signer.generate_presigned_url(request_dict, 'operation_name')\n\n        self.emitter.emit_until_response.assert_called_with(\n            'choose-signer.service_name.operation_name',\n            signing_name='signing_name',\n            region_name='region_name',\n            signature_version='v4-query',\n            context=mock.ANY,\n        )\n\n    def test_choose_signer_passes_context(self):\n        self.request.context = {'foo': 'bar'}\n\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'v4': mock.Mock()}\n        ):\n            self.signer.sign('operation_name', self.request)\n\n        self.emitter.emit_until_response.assert_called_with(\n            'choose-signer.service_name.operation_name',\n            signing_name='signing_name',\n            region_name='region_name',\n            signature_version='v4',\n            context={'foo': 'bar'},\n        )\n\n    def test_generate_url_choose_signer_override(self):\n        request_dict = {\n            'headers': {},\n            'url': 'https://foo.com',\n            'body': b'',\n            'url_path': '/',\n            'method': 'GET',\n            'context': {},\n        }\n        auth = mock.Mock()\n        auth.REQUIRES_REGION = False\n        self.emitter.emit_until_response.return_value = (None, 'custom')\n\n        auth_types_map = {'custom': mock.Mock(), 'custom-query': auth}\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types_map):\n            self.signer.generate_presigned_url(request_dict, 'operation_name')\n\n        auth.assert_called_with(\n            credentials=self.fixed_credentials, expires=3600\n        )\n\n    def test_generate_url_unsigned(self):\n        request_dict = {\n            'headers': {},\n            'url': 'https://foo.com',\n            'body': b'',\n            'url_path': '/',\n            'method': 'GET',\n            'context': {},\n        }\n        self.emitter.emit_until_response.return_value = (\n            None,\n            botocore.UNSIGNED,\n        )\n\n        url = self.signer.generate_presigned_url(\n            request_dict, 'operation_name'\n        )\n\n        self.assertEqual(url, 'https://foo.com')\n\n    def test_generate_presigned_url(self):\n        auth = mock.Mock()\n        auth.REQUIRES_REGION = True\n\n        request_dict = {\n            'headers': {},\n            'url': 'https://foo.com',\n            'body': b'',\n            'url_path': '/',\n            'method': 'GET',\n            'context': {},\n        }\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4-query': auth}):\n            presigned_url = self.signer.generate_presigned_url(\n                request_dict, operation_name='operation_name'\n            )\n        auth.assert_called_with(\n            credentials=self.fixed_credentials,\n            region_name='region_name',\n            service_name='signing_name',\n            expires=3600,\n        )\n        self.assertEqual(presigned_url, 'https://foo.com')\n\n    def test_generate_presigned_url_with_region_override(self):\n        auth = mock.Mock()\n        auth.REQUIRES_REGION = True\n\n        request_dict = {\n            'headers': {},\n            'url': 'https://foo.com',\n            'body': b'',\n            'url_path': '/',\n            'method': 'GET',\n            'context': {},\n        }\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4-query': auth}):\n            presigned_url = self.signer.generate_presigned_url(\n                request_dict,\n                operation_name='operation_name',\n                region_name='us-west-2',\n            )\n        auth.assert_called_with(\n            credentials=self.fixed_credentials,\n            region_name='us-west-2',\n            service_name='signing_name',\n            expires=3600,\n        )\n        self.assertEqual(presigned_url, 'https://foo.com')\n\n    def test_generate_presigned_url_with_exipres_in(self):\n        auth = mock.Mock()\n        auth.REQUIRES_REGION = True\n\n        request_dict = {\n            'headers': {},\n            'url': 'https://foo.com',\n            'body': b'',\n            'url_path': '/',\n            'method': 'GET',\n            'context': {},\n        }\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4-query': auth}):\n            presigned_url = self.signer.generate_presigned_url(\n                request_dict, operation_name='operation_name', expires_in=900\n            )\n        auth.assert_called_with(\n            credentials=self.fixed_credentials,\n            region_name='region_name',\n            expires=900,\n            service_name='signing_name',\n        )\n        self.assertEqual(presigned_url, 'https://foo.com')\n\n    def test_presigned_url_throws_unsupported_signature_error(self):\n        request_dict = {\n            'headers': {},\n            'url': 'https://s3.amazonaws.com/mybucket/myobject',\n            'body': b'',\n            'url_path': '/',\n            'method': 'GET',\n            'context': {},\n        }\n        self.signer = RequestSigner(\n            ServiceId('service_name'),\n            'region_name',\n            'signing_name',\n            'foo',\n            self.credentials,\n            self.emitter,\n        )\n        with self.assertRaises(UnsupportedSignatureVersionError):\n            self.signer.generate_presigned_url(\n                request_dict, operation_name='foo'\n            )\n\n    def test_signer_with_refreshable_credentials_gets_credential_set(self):\n        class FakeCredentials(Credentials):\n            def get_frozen_credentials(self):\n                return ReadOnlyCredentials('foo', 'bar', 'baz')\n\n        self.credentials = FakeCredentials('a', 'b', 'c')\n\n        self.signer = RequestSigner(\n            ServiceId('service_name'),\n            'region_name',\n            'signing_name',\n            'v4',\n            self.credentials,\n            self.emitter,\n        )\n\n        auth_cls = mock.Mock()\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4': auth_cls}):\n            auth = self.signer.get_auth('service_name', 'region_name')\n            self.assertEqual(auth, auth_cls.return_value)\n            # Note we're called with 'foo', 'bar', 'baz', and *not*\n            # 'a', 'b', 'c'.\n            auth_cls.assert_called_with(\n                credentials=ReadOnlyCredentials('foo', 'bar', 'baz'),\n                service_name='service_name',\n                region_name='region_name',\n            )\n\n    def test_no_credentials_case_is_forwarded_to_signer(self):\n        # If no credentials are given to the RequestSigner, we should\n        # forward that fact on to the Auth class and let them handle\n        # the error (which they already do).\n        self.credentials = None\n        self.signer = RequestSigner(\n            ServiceId('service_name'),\n            'region_name',\n            'signing_name',\n            'v4',\n            self.credentials,\n            self.emitter,\n        )\n        auth_cls = mock.Mock()\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4': auth_cls}):\n            self.signer.get_auth_instance('service_name', 'region_name', 'v4')\n            auth_cls.assert_called_with(\n                service_name='service_name',\n                region_name='region_name',\n                credentials=None,\n            )\n\n    def test_sign_with_signing_type_standard(self):\n        auth = mock.Mock()\n        post_auth = mock.Mock()\n        query_auth = mock.Mock()\n        auth_types = {\n            'v4-presign-post': post_auth,\n            'v4-query': query_auth,\n            'v4': auth,\n        }\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.sign(\n                'operation_name', self.request, signing_type='standard'\n            )\n        self.assertFalse(post_auth.called)\n        self.assertFalse(query_auth.called)\n        auth.assert_called_with(\n            credentials=ReadOnlyCredentials('key', 'secret', None),\n            service_name='signing_name',\n            region_name='region_name',\n        )\n\n    def test_sign_with_signing_type_presign_url(self):\n        auth = mock.Mock()\n        post_auth = mock.Mock()\n        query_auth = mock.Mock()\n        auth_types = {\n            'v4-presign-post': post_auth,\n            'v4-query': query_auth,\n            'v4': auth,\n        }\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.sign(\n                'operation_name', self.request, signing_type='presign-url'\n            )\n        self.assertFalse(post_auth.called)\n        self.assertFalse(auth.called)\n        query_auth.assert_called_with(\n            credentials=ReadOnlyCredentials('key', 'secret', None),\n            service_name='signing_name',\n            region_name='region_name',\n        )\n\n    def test_sign_with_signing_type_presign_post(self):\n        auth = mock.Mock()\n        post_auth = mock.Mock()\n        query_auth = mock.Mock()\n        auth_types = {\n            'v4-presign-post': post_auth,\n            'v4-query': query_auth,\n            'v4': auth,\n        }\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.sign(\n                'operation_name', self.request, signing_type='presign-post'\n            )\n        self.assertFalse(auth.called)\n        self.assertFalse(query_auth.called)\n        post_auth.assert_called_with(\n            credentials=ReadOnlyCredentials('key', 'secret', None),\n            service_name='signing_name',\n            region_name='region_name',\n        )\n\n    def test_sign_with_region_name(self):\n        auth = mock.Mock()\n        auth_types = {'v4': auth}\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.sign('operation_name', self.request, region_name='foo')\n        auth.assert_called_with(\n            credentials=ReadOnlyCredentials('key', 'secret', None),\n            service_name='signing_name',\n            region_name='foo',\n        )\n\n    def test_sign_override_region_from_context(self):\n        auth = mock.Mock()\n        auth_types = {'v4': auth}\n        self.request.context = {'signing': {'region': 'my-override-region'}}\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.sign('operation_name', self.request)\n        auth.assert_called_with(\n            credentials=ReadOnlyCredentials('key', 'secret', None),\n            service_name='signing_name',\n            region_name='my-override-region',\n        )\n\n    def test_sign_with_region_name_overrides_context(self):\n        auth = mock.Mock()\n        auth_types = {'v4': auth}\n        self.request.context = {'signing': {'region': 'context-override'}}\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.sign(\n                'operation_name', self.request, region_name='param-override'\n            )\n        auth.assert_called_with(\n            credentials=ReadOnlyCredentials('key', 'secret', None),\n            service_name='signing_name',\n            region_name='param-override',\n        )\n\n    def test_sign_override_signing_name_from_context(self):\n        auth = mock.Mock()\n        auth_types = {'v4': auth}\n        self.request.context = {'signing': {'signing_name': 'override_name'}}\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.sign('operation_name', self.request)\n        auth.assert_called_with(\n            credentials=ReadOnlyCredentials('key', 'secret', None),\n            service_name='override_name',\n            region_name='region_name',\n        )\n\n    def test_sign_with_expires_in(self):\n        auth = mock.Mock()\n        auth_types = {'v4': auth}\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.sign('operation_name', self.request, expires_in=2)\n        auth.assert_called_with(\n            credentials=ReadOnlyCredentials('key', 'secret', None),\n            service_name='signing_name',\n            region_name='region_name',\n            expires=2,\n        )\n\n    def test_sign_with_custom_signing_name(self):\n        auth = mock.Mock()\n        auth_types = {'v4': auth}\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.sign(\n                'operation_name', self.request, signing_name='foo'\n            )\n        auth.assert_called_with(\n            credentials=ReadOnlyCredentials('key', 'secret', None),\n            service_name='foo',\n            region_name='region_name',\n        )\n\n    def test_presign_with_custom_signing_name(self):\n        auth = mock.Mock()\n        auth.REQUIRES_REGION = True\n\n        request_dict = {\n            'headers': {},\n            'url': 'https://foo.com',\n            'body': b'',\n            'url_path': '/',\n            'method': 'GET',\n            'context': {},\n        }\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4-query': auth}):\n            presigned_url = self.signer.generate_presigned_url(\n                request_dict,\n                operation_name='operation_name',\n                signing_name='foo',\n            )\n        auth.assert_called_with(\n            credentials=self.fixed_credentials,\n            region_name='region_name',\n            expires=3600,\n            service_name='foo',\n        )\n        self.assertEqual(presigned_url, 'https://foo.com')\n\n    def test_unknown_signer_raises_unknown_on_standard(self):\n        auth = mock.Mock()\n        auth_types = {'v4': auth}\n        self.emitter.emit_until_response.return_value = (None, 'custom')\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            with self.assertRaises(UnknownSignatureVersionError):\n                self.signer.sign(\n                    'operation_name', self.request, signing_type='standard'\n                )\n\n    def test_unknown_signer_raises_unsupported_when_not_standard(self):\n        auth = mock.Mock()\n        auth_types = {'v4': auth}\n        self.emitter.emit_until_response.return_value = (None, 'custom')\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            with self.assertRaises(UnsupportedSignatureVersionError):\n                self.signer.sign(\n                    'operation_name', self.request, signing_type='presign-url'\n                )\n\n            with self.assertRaises(UnsupportedSignatureVersionError):\n                self.signer.sign(\n                    'operation_name', self.request, signing_type='presign-post'\n                )\n\n\nclass TestCloudfrontSigner(BaseSignerTest):\n    def setUp(self):\n        super().setUp()\n        self.signer = CloudFrontSigner(\"MY_KEY_ID\", lambda message: b'signed')\n        # It helps but the long string diff will still be slightly different on\n        # Python 2.6/2.7/3.x. We won't soly rely on that anyway, so it's fine.\n        self.maxDiff = None\n\n    def test_build_canned_policy(self):\n        policy = self.signer.build_policy('foo', datetime.datetime(2016, 1, 1))\n        expected = (\n            '{\"Statement\":[{\"Resource\":\"foo\",'\n            '\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":1451606400}}}]}'\n        )\n        self.assertEqual(json.loads(policy), json.loads(expected))\n        self.assertEqual(policy, expected)  # This is to ensure the right order\n\n    def test_build_custom_policy(self):\n        policy = self.signer.build_policy(\n            'foo',\n            datetime.datetime(2016, 1, 1),\n            date_greater_than=datetime.datetime(2015, 12, 1),\n            ip_address='12.34.56.78/9',\n        )\n        expected = {\n            \"Statement\": [\n                {\n                    \"Resource\": \"foo\",\n                    \"Condition\": {\n                        \"DateGreaterThan\": {\"AWS:EpochTime\": 1448928000},\n                        \"DateLessThan\": {\"AWS:EpochTime\": 1451606400},\n                        \"IpAddress\": {\"AWS:SourceIp\": \"12.34.56.78/9\"},\n                    },\n                }\n            ]\n        }\n        self.assertEqual(json.loads(policy), expected)\n\n    def test_generate_presign_url_with_expire_time(self):\n        signed_url = self.signer.generate_presigned_url(\n            'http://test.com/foo.txt',\n            date_less_than=datetime.datetime(2016, 1, 1),\n        )\n        expected = (\n            'http://test.com/foo.txt?Expires=1451606400&Signature=c2lnbmVk'\n            '&Key-Pair-Id=MY_KEY_ID'\n        )\n        assert_url_equal(signed_url, expected)\n\n    def test_generate_presign_url_with_custom_policy(self):\n        policy = self.signer.build_policy(\n            'foo',\n            datetime.datetime(2016, 1, 1),\n            date_greater_than=datetime.datetime(2015, 12, 1),\n            ip_address='12.34.56.78/9',\n        )\n        signed_url = self.signer.generate_presigned_url(\n            'http://test.com/index.html?foo=bar', policy=policy\n        )\n        expected = (\n            'http://test.com/index.html?foo=bar'\n            '&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiZm9vIiwiQ29uZ'\n            'Gl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIj'\n            'oxNDUxNjA2NDAwfSwiSXBBZGRyZXNzIjp7IkFXUzpTb3VyY2VJcCI'\n            '6IjEyLjM0LjU2Ljc4LzkifSwiRGF0ZUdyZWF0ZXJUaGFuIjp7IkFX'\n            'UzpFcG9jaFRpbWUiOjE0NDg5MjgwMDB9fX1dfQ__'\n            '&Signature=c2lnbmVk&Key-Pair-Id=MY_KEY_ID'\n        )\n        assert_url_equal(signed_url, expected)\n\n\nclass TestS3PostPresigner(BaseSignerTest):\n    def setUp(self):\n        super().setUp()\n        self.request_signer = RequestSigner(\n            ServiceId('service_name'),\n            'region_name',\n            'signing_name',\n            's3v4',\n            self.credentials,\n            self.emitter,\n        )\n        self.signer = S3PostPresigner(self.request_signer)\n        self.request_dict = {\n            'headers': {},\n            'url': 'https://s3.amazonaws.com/mybucket',\n            'body': b'',\n            'url_path': '/',\n            'method': 'POST',\n            'context': {},\n        }\n        self.auth = mock.Mock()\n        self.auth.REQUIRES_REGION = True\n        self.add_auth = mock.Mock()\n        self.auth.return_value.add_auth = self.add_auth\n        self.fixed_credentials = self.credentials.get_frozen_credentials()\n\n        self.datetime_patch = mock.patch('botocore.signers.datetime')\n        self.datetime_mock = self.datetime_patch.start()\n        self.fixed_date = datetime.datetime(2014, 3, 10, 17, 2, 55, 0)\n        self.fixed_delta = datetime.timedelta(seconds=3600)\n        self.datetime_mock.datetime.utcnow.return_value = self.fixed_date\n        self.datetime_mock.timedelta.return_value = self.fixed_delta\n\n    def tearDown(self):\n        super().tearDown()\n        self.datetime_patch.stop()\n\n    def test_generate_presigned_post(self):\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'s3v4-presign-post': self.auth}\n        ):\n            post_form_args = self.signer.generate_presigned_post(\n                self.request_dict\n            )\n        self.auth.assert_called_with(\n            credentials=self.fixed_credentials,\n            region_name='region_name',\n            service_name='signing_name',\n        )\n        self.assertEqual(self.add_auth.call_count, 1)\n        ref_request = self.add_auth.call_args[0][0]\n        ref_policy = ref_request.context['s3-presign-post-policy']\n        self.assertEqual(ref_policy['expiration'], '2014-03-10T18:02:55Z')\n        self.assertEqual(ref_policy['conditions'], [])\n\n        self.assertEqual(\n            post_form_args['url'], 'https://s3.amazonaws.com/mybucket'\n        )\n        self.assertEqual(post_form_args['fields'], {})\n\n    def test_generate_presigned_post_emits_choose_signer(self):\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'s3v4-presign-post': self.auth}\n        ):\n            self.signer.generate_presigned_post(self.request_dict)\n        self.emitter.emit_until_response.assert_called_with(\n            'choose-signer.service_name.PutObject',\n            signing_name='signing_name',\n            region_name='region_name',\n            signature_version='s3v4-presign-post',\n            context=mock.ANY,\n        )\n\n    def test_generate_presigned_post_choose_signer_override(self):\n        auth = mock.Mock()\n        self.emitter.emit_until_response.return_value = (None, 'custom')\n        auth_types = {\n            's3v4-presign-post': self.auth,\n            'custom-presign-post': auth,\n        }\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.generate_presigned_post(self.request_dict)\n        auth.assert_called_with(\n            credentials=self.fixed_credentials,\n            region_name='region_name',\n            service_name='signing_name',\n        )\n\n    def test_generate_presigne_post_choose_signer_override_known(self):\n        auth = mock.Mock()\n        self.emitter.emit_until_response.return_value = (\n            None,\n            's3v4-presign-post',\n        )\n        auth_types = {\n            's3v4-presign-post': self.auth,\n            'custom-presign-post': auth,\n        }\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            self.signer.generate_presigned_post(self.request_dict)\n        self.auth.assert_called_with(\n            credentials=self.fixed_credentials,\n            region_name='region_name',\n            service_name='signing_name',\n        )\n\n    def test_generate_presigned_post_bad_signer_raises_error(self):\n        auth = mock.Mock()\n        self.emitter.emit_until_response.return_value = (None, 's3-query')\n        auth_types = {'s3v4-presign-post': self.auth, 's3-query': auth}\n        with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, auth_types):\n            with self.assertRaises(UnsupportedSignatureVersionError):\n                self.signer.generate_presigned_post(self.request_dict)\n\n    def test_generate_unsigned_post(self):\n        self.emitter.emit_until_response.return_value = (\n            None,\n            botocore.UNSIGNED,\n        )\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'s3v4-presign-post': self.auth}\n        ):\n            post_form_args = self.signer.generate_presigned_post(\n                self.request_dict\n            )\n        expected = {'fields': {}, 'url': 'https://s3.amazonaws.com/mybucket'}\n        self.assertEqual(post_form_args, expected)\n\n    def test_generate_presigned_post_with_conditions(self):\n        conditions = [{'bucket': 'mybucket'}, ['starts-with', '$key', 'bar']]\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'s3v4-presign-post': self.auth}\n        ):\n            self.signer.generate_presigned_post(\n                self.request_dict, conditions=conditions\n            )\n        self.auth.assert_called_with(\n            credentials=self.fixed_credentials,\n            region_name='region_name',\n            service_name='signing_name',\n        )\n        self.assertEqual(self.add_auth.call_count, 1)\n        ref_request = self.add_auth.call_args[0][0]\n        ref_policy = ref_request.context['s3-presign-post-policy']\n        self.assertEqual(ref_policy['conditions'], conditions)\n\n    def test_generate_presigned_post_with_region_override(self):\n        with mock.patch.dict(\n            botocore.auth.AUTH_TYPE_MAPS, {'s3v4-presign-post': self.auth}\n        ):\n            self.signer.generate_presigned_post(\n                self.request_dict, region_name='foo'\n            )\n        self.auth.assert_called_with(\n            credentials=self.fixed_credentials,\n            region_name='foo',\n            service_name='signing_name',\n        )\n\n    def test_presigned_post_throws_unsupported_signature_error(self):\n        request_dict = {\n            'headers': {},\n            'url': 'https://s3.amazonaws.com/mybucket/myobject',\n            'body': b'',\n            'url_path': '/',\n            'method': 'POST',\n            'context': {},\n        }\n        self.request_signer = RequestSigner(\n            ServiceId('service_name'),\n            'region_name',\n            'signing_name',\n            'foo',\n            self.credentials,\n            self.emitter,\n        )\n        self.signer = S3PostPresigner(self.request_signer)\n        with self.assertRaises(UnsupportedSignatureVersionError):\n            self.signer.generate_presigned_post(request_dict)\n\n\nclass TestGenerateUrl(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('s3', region_name='us-east-1')\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.client_kwargs = {'Bucket': self.bucket, 'Key': self.key}\n        self.generate_url_patch = mock.patch(\n            'botocore.signers.RequestSigner.generate_presigned_url'\n        )\n        self.generate_url_mock = self.generate_url_patch.start()\n\n    def tearDown(self):\n        self.generate_url_patch.stop()\n\n    def test_generate_presigned_url(self):\n        self.client.generate_presigned_url(\n            'get_object', Params={'Bucket': self.bucket, 'Key': self.key}\n        )\n\n        ref_request_dict = {\n            'body': b'',\n            'url': 'https://mybucket.s3.amazonaws.com/mykey',\n            'headers': {},\n            'auth_path': '/mybucket/mykey',\n            'query_string': {},\n            'url_path': '/mykey',\n            'method': 'GET',\n            # mock.ANY is used because client parameter related events\n            # inject values into the context. So using the context's exact\n            # value for these tests will be a maintenance burden if\n            # anymore customizations are added that inject into the context.\n            'context': mock.ANY,\n        }\n        self.generate_url_mock.assert_called_with(\n            request_dict=ref_request_dict,\n            expires_in=3600,\n            operation_name='GetObject',\n        )\n\n    def test_generate_presigned_url_with_query_string(self):\n        disposition = 'attachment; filename=\"download.jpg\"'\n        self.client.generate_presigned_url(\n            'get_object',\n            Params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'ResponseContentDisposition': disposition,\n            },\n        )\n        ref_request_dict = {\n            'body': b'',\n            'url': (\n                'https://mybucket.s3.amazonaws.com/mykey'\n                '?response-content-disposition='\n                'attachment%3B%20filename%3D%22download.jpg%22'\n            ),\n            'auth_path': '/mybucket/mykey',\n            'headers': {},\n            'query_string': {'response-content-disposition': disposition},\n            'url_path': '/mykey',\n            'method': 'GET',\n            'context': mock.ANY,\n        }\n        self.generate_url_mock.assert_called_with(\n            request_dict=ref_request_dict,\n            expires_in=3600,\n            operation_name='GetObject',\n        )\n\n    def test_generate_presigned_url_unknown_method_name(self):\n        with self.assertRaises(UnknownClientMethodError):\n            self.client.generate_presigned_url('getobject')\n\n    def test_generate_presigned_url_missing_required_params(self):\n        with self.assertRaises(ParamValidationError):\n            self.client.generate_presigned_url('get_object')\n\n    def test_generate_presigned_url_expires(self):\n        self.client.generate_presigned_url(\n            'get_object',\n            Params={'Bucket': self.bucket, 'Key': self.key},\n            ExpiresIn=20,\n        )\n        ref_request_dict = {\n            'body': b'',\n            'url': 'https://mybucket.s3.amazonaws.com/mykey',\n            'auth_path': '/mybucket/mykey',\n            'headers': {},\n            'query_string': {},\n            'url_path': '/mykey',\n            'method': 'GET',\n            'context': mock.ANY,\n        }\n        self.generate_url_mock.assert_called_with(\n            request_dict=ref_request_dict,\n            expires_in=20,\n            operation_name='GetObject',\n        )\n\n    def test_generate_presigned_url_override_http_method(self):\n        self.client.generate_presigned_url(\n            'get_object',\n            Params={'Bucket': self.bucket, 'Key': self.key},\n            HttpMethod='PUT',\n        )\n        ref_request_dict = {\n            'body': b'',\n            'url': 'https://mybucket.s3.amazonaws.com/mykey',\n            'auth_path': '/mybucket/mykey',\n            'headers': {},\n            'query_string': {},\n            'url_path': '/mykey',\n            'method': 'PUT',\n            'context': mock.ANY,\n        }\n        self.generate_url_mock.assert_called_with(\n            request_dict=ref_request_dict,\n            expires_in=3600,\n            operation_name='GetObject',\n        )\n\n    def test_generate_presigned_url_emits_param_events(self):\n        emitter = mock.Mock(HierarchicalEmitter)\n        emitter.emit.return_value = []\n        self.client.meta.events = emitter\n        self.client.generate_presigned_url(\n            'get_object', Params={'Bucket': self.bucket, 'Key': self.key}\n        )\n        events_emitted = [\n            emit_call[0][0] for emit_call in emitter.emit.call_args_list\n        ]\n        self.assertEqual(\n            events_emitted,\n            [\n                'provide-client-params.s3.GetObject',\n                'before-parameter-build.s3.GetObject',\n            ],\n        )\n\n    def test_generate_presign_url_emits_is_presign_in_context(self):\n        emitter = mock.Mock(HierarchicalEmitter)\n        emitter.emit.return_value = []\n        self.client.meta.events = emitter\n        self.client.generate_presigned_url(\n            'get_object', Params={'Bucket': self.bucket, 'Key': self.key}\n        )\n        kwargs_emitted = [\n            emit_call[1] for emit_call in emitter.emit.call_args_list\n        ]\n        for kwargs in kwargs_emitted:\n            self.assertTrue(\n                kwargs.get('context', {}).get('is_presign_request'),\n                'The context did not have is_presign_request set to True for '\n                'the following kwargs emitted: %s' % kwargs,\n            )\n\n    def test_context_param_from_event_handler_sent_to_endpoint_resolver(self):\n        def change_bucket_param(params, **kwargs):\n            params['Bucket'] = 'mybucket-bar'\n\n        self.client.meta.events.register_last(\n            'provide-client-params.s3.*', change_bucket_param\n        )\n\n        self.client.generate_presigned_url(\n            'get_object', Params={'Bucket': 'mybucket-foo', 'Key': self.key}\n        )\n\n        ref_request_dict = {\n            'body': b'',\n            # If the bucket name set in the provide-client-params event handler\n            # was correctly passed to the endpoint provider as a dynamic context\n            # parameter, it will appear in the URL and the auth_path:\n            'url': 'https://mybucket-bar.s3.amazonaws.com/mykey',\n            'headers': {},\n            'auth_path': '/mybucket-bar/mykey',\n            'query_string': {},\n            'url_path': '/mykey',\n            'method': 'GET',\n            'context': mock.ANY,\n        }\n        self.generate_url_mock.assert_called_with(\n            request_dict=ref_request_dict,\n            expires_in=3600,\n            operation_name='GetObject',\n        )\n\n\nclass TestGeneratePresignedPost(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client('s3', region_name='us-east-1')\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.presign_post_patch = mock.patch(\n            'botocore.signers.S3PostPresigner.generate_presigned_post'\n        )\n        self.presign_post_mock = self.presign_post_patch.start()\n\n    def tearDown(self):\n        self.presign_post_patch.stop()\n\n    def test_generate_presigned_post(self):\n        self.client.generate_presigned_post(self.bucket, self.key)\n\n        _, post_kwargs = self.presign_post_mock.call_args\n        request_dict = post_kwargs['request_dict']\n        fields = post_kwargs['fields']\n        conditions = post_kwargs['conditions']\n        self.assertEqual(\n            request_dict['url'], 'https://mybucket.s3.amazonaws.com/'\n        )\n        self.assertEqual(post_kwargs['expires_in'], 3600)\n        self.assertEqual(\n            conditions, [{'bucket': 'mybucket'}, {'key': 'mykey'}]\n        )\n        self.assertEqual(fields, {'key': 'mykey'})\n\n    def test_generate_presigned_post_with_filename(self):\n        self.key = 'myprefix/${filename}'\n        self.client.generate_presigned_post(self.bucket, self.key)\n\n        _, post_kwargs = self.presign_post_mock.call_args\n        request_dict = post_kwargs['request_dict']\n        fields = post_kwargs['fields']\n        conditions = post_kwargs['conditions']\n        self.assertEqual(\n            request_dict['url'], 'https://mybucket.s3.amazonaws.com/'\n        )\n        self.assertEqual(post_kwargs['expires_in'], 3600)\n        self.assertEqual(\n            conditions,\n            [{'bucket': 'mybucket'}, ['starts-with', '$key', 'myprefix/']],\n        )\n        self.assertEqual(fields, {'key': 'myprefix/${filename}'})\n\n    def test_generate_presigned_post_expires(self):\n        self.client.generate_presigned_post(\n            self.bucket, self.key, ExpiresIn=50\n        )\n        _, post_kwargs = self.presign_post_mock.call_args\n        request_dict = post_kwargs['request_dict']\n        fields = post_kwargs['fields']\n        conditions = post_kwargs['conditions']\n        self.assertEqual(\n            request_dict['url'], 'https://mybucket.s3.amazonaws.com/'\n        )\n        self.assertEqual(post_kwargs['expires_in'], 50)\n        self.assertEqual(\n            conditions, [{'bucket': 'mybucket'}, {'key': 'mykey'}]\n        )\n        self.assertEqual(fields, {'key': 'mykey'})\n\n    def test_generate_presigned_post_with_prefilled(self):\n        conditions = [{'acl': 'public-read'}]\n        fields = {'acl': 'public-read'}\n\n        self.client.generate_presigned_post(\n            self.bucket, self.key, Fields=fields, Conditions=conditions\n        )\n\n        self.assertEqual(fields, {'acl': 'public-read'})\n\n        _, post_kwargs = self.presign_post_mock.call_args\n        request_dict = post_kwargs['request_dict']\n        fields = post_kwargs['fields']\n        conditions = post_kwargs['conditions']\n        self.assertEqual(\n            request_dict['url'], 'https://mybucket.s3.amazonaws.com/'\n        )\n        self.assertEqual(\n            conditions,\n            [{'acl': 'public-read'}, {'bucket': 'mybucket'}, {'key': 'mykey'}],\n        )\n        self.assertEqual(fields['acl'], 'public-read')\n        self.assertEqual(fields, {'key': 'mykey', 'acl': 'public-read'})\n\n    def test_generate_presigned_post_non_s3_client(self):\n        self.client = self.session.create_client('ec2', 'us-west-2')\n        with self.assertRaises(AttributeError):\n            self.client.generate_presigned_post()\n\n\nclass TestGenerateDBAuthToken(BaseSignerTest):\n    maxDiff = None\n\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.client = self.session.create_client(\n            'rds',\n            region_name='us-east-1',\n            aws_access_key_id='akid',\n            aws_secret_access_key='skid',\n            config=Config(signature_version='v4'),\n        )\n\n    def test_generate_db_auth_token(self):\n        hostname = 'prod-instance.us-east-1.rds.amazonaws.com'\n        port = 3306\n        username = 'someusername'\n        clock = datetime.datetime(2016, 11, 7, 17, 39, 33, tzinfo=tzutc())\n\n        with mock.patch('datetime.datetime') as dt:\n            dt.utcnow.return_value = clock\n            result = generate_db_auth_token(\n                self.client, hostname, port, username\n            )\n\n        expected_result = (\n            'prod-instance.us-east-1.rds.amazonaws.com:3306/?Action=connect'\n            '&DBUser=someusername&X-Amz-Algorithm=AWS4-HMAC-SHA256'\n            '&X-Amz-Date=20161107T173933Z&X-Amz-SignedHeaders=host'\n            '&X-Amz-Expires=900&X-Amz-Credential=akid%2F20161107%2F'\n            'us-east-1%2Frds-db%2Faws4_request&X-Amz-Signature'\n            '=d1138cdbc0ca63eec012ec0fc6c2267e03642168f5884a7795320d4c18374c61'\n        )\n\n        # A scheme needs to be appended to the beginning or urlsplit may fail\n        # on certain systems.\n        assert_url_equal('https://' + result, 'https://' + expected_result)\n\n    def test_custom_region(self):\n        hostname = 'host.us-east-1.rds.amazonaws.com'\n        port = 3306\n        username = 'mySQLUser'\n        region = 'us-west-2'\n        result = generate_db_auth_token(\n            self.client, hostname, port, username, Region=region\n        )\n\n        self.assertIn(region, result)\n        # The hostname won't be changed even if a different region is specified\n        self.assertIn(hostname, result)\n\n\n@pytest.mark.parametrize(\n    'request_method',\n    ['GET', 'HEAD', 'OPTIONS', 'POST', 'PUT', 'DELETE', None],\n)\ndef test_generate_presigned_url_content_type_removal_for_polly(\n    polly_client,\n    request_method,\n):\n    url = polly_client.generate_presigned_url(\n        'synthesize_speech',\n        Params={\n            'OutputFormat': 'mp3',\n            'Text': 'Hello world!',\n            'VoiceId': 'Joanna',\n        },\n        HttpMethod=request_method,\n    )\n    assert 'content-type' not in url.lower()\n", "tests/unit/test_regions.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nfrom botocore import regions\nfrom botocore.exceptions import EndpointVariantError, NoRegionError\nfrom tests import unittest\n\n\nclass TestEndpointResolver(unittest.TestCase):\n    def _template(self):\n        return {\n            'partitions': [\n                {\n                    'partition': 'aws',\n                    'dnsSuffix': 'amazonaws.com',\n                    'regionRegex': r'^(us|eu)\\-\\w+$',\n                    'defaults': {\n                        'hostname': '{service}.{region}.{dnsSuffix}',\n                        'variants': [\n                            {\n                                'hostname': '{service}-fips.{region}.{dnsSuffix}',\n                                'tags': ['fips'],\n                            },\n                            {\n                                'dnsSuffix': 'api.aws',\n                                'hostname': '{service}.{region}.{dnsSuffix}',\n                                'tags': ['dualstack'],\n                            },\n                            {\n                                'dnsSuffix': 'api.aws',\n                                'hostname': '{service}-fips.{region}.{dnsSuffix}',\n                                'tags': ['dualstack', 'fips'],\n                            },\n                        ],\n                    },\n                    'regions': {\n                        'us-foo': {'regionName': 'a'},\n                        'us-bar': {'regionName': 'b'},\n                        'eu-baz': {'regionName': 'd'},\n                    },\n                    'services': {\n                        'ec2': {\n                            'defaults': {\n                                'protocols': ['http', 'https'],\n                                'variants': [\n                                    {\n                                        'dnsSuffix': 'api.aws',\n                                        'hostname': 'api.ec2.{region}.{dnsSuffix}',\n                                        'tags': ['dualstack'],\n                                    }\n                                ],\n                            },\n                            'endpoints': {\n                                'us-foo': {\n                                    'hostname': 'ec2.us-foo.amazonaws.com',\n                                    'variants': [\n                                        {\n                                            'dnsSuffix': 'api.aws',\n                                            'hostname': 'ec2.foo.{dnsSuffix}',\n                                            'tags': ['dualstack'],\n                                        },\n                                        {\n                                            'hostname': 'ec2-fips.foo.amazonaws.com',\n                                            'tags': ['fips'],\n                                        },\n                                        {\n                                            'hostname': 'ec2-fips.foo.api.aws',\n                                            'tags': ['fips', 'dualstack'],\n                                        },\n                                    ],\n                                },\n                                'us-bar': {},\n                                'us-dep': {\n                                    'deprecated': True,\n                                },\n                                'us-fizz': {\n                                    'credentialScope': {'region': 'us-fizz'},\n                                    'hostname': 'ec2.us-fizz.amazonaws.com',\n                                    'variants': [\n                                        {\n                                            'hostname': 'ec2.fizz.api.aws',\n                                            'tags': ['dualstack'],\n                                        }\n                                    ],\n                                },\n                                'eu-baz': {},\n                                'd': {},\n                            },\n                        },\n                        's3': {\n                            'defaults': {\n                                'sslCommonName': '{service}.{region}.{dnsSuffix}',\n                                'variants': [\n                                    {\n                                        'hostname': 's3.dualstack.{region}.{dnsSuffix}',\n                                        'tags': ['dualstack'],\n                                    },\n                                    {\n                                        'hostname': 's3-fips.{region}.{dnsSuffix}',\n                                        'tags': ['fips'],\n                                    },\n                                    {\n                                        'hostname': 's3-fips.dualstack.{region}.{dnsSuffix}',\n                                        'tags': ['dualstack', 'fips'],\n                                    },\n                                ],\n                            },\n                            'endpoints': {\n                                'us-foo': {\n                                    'sslCommonName': '{region}.{service}.{dnsSuffix}'\n                                },\n                                'us-bar': {},\n                                'us-fizz': {\n                                    'hostname': 's3.api.us-fizz.amazonaws.com',\n                                    'variants': [\n                                        {'tags': ['dualstack']},\n                                        {'tags': ['fips']},\n                                    ],\n                                },\n                                'eu-baz': {'hostname': 'foo'},\n                            },\n                        },\n                        'not-regionalized': {\n                            'isRegionalized': False,\n                            'partitionEndpoint': 'aws',\n                            'endpoints': {\n                                'aws': {'hostname': 'not-regionalized'},\n                                'us-foo': {},\n                                'eu-baz': {},\n                            },\n                        },\n                        'non-partition': {\n                            'partitionEndpoint': 'aws',\n                            'endpoints': {\n                                'aws': {'hostname': 'host'},\n                                'us-foo': {},\n                            },\n                        },\n                        'merge': {\n                            'defaults': {\n                                'signatureVersions': ['v2'],\n                                'protocols': ['http'],\n                            },\n                            'endpoints': {\n                                'us-foo': {'signatureVersions': ['v4']},\n                                'us-bar': {'protocols': ['https']},\n                            },\n                        },\n                    },\n                },\n                {\n                    'partition': 'foo',\n                    'dnsSuffix': 'foo.com',\n                    'regionRegex': r'^(foo)\\-\\w+$',\n                    'defaults': {\n                        'hostname': '{service}.{region}.{dnsSuffix}',\n                        'protocols': ['http'],\n                        'foo': 'bar',\n                    },\n                    'regions': {\n                        'foo-1': {'regionName': '1'},\n                        'foo-2': {'regionName': '2'},\n                        'foo-3': {'regionName': '3'},\n                    },\n                    'services': {\n                        'ec2': {\n                            'endpoints': {\n                                'foo-1': {'foo': 'baz'},\n                                'foo-2': {},\n                                'foo-3': {},\n                            }\n                        }\n                    },\n                },\n                {\n                    'partition': 'aws-iso',\n                    'dnsSuffix': 'amazonaws.com',\n                    'defaults': {\n                        'hostname': '{service}.{region}.{dnsSuffix}',\n                        'protocols': ['http'],\n                    },\n                    'regions': {\n                        'foo-1': {'regionName': '1'},\n                        'foo-2': {'regionName': '2'},\n                        'foo-3': {'regionName': '3'},\n                    },\n                    'services': {\n                        'ec2': {\n                            'endpoints': {\n                                'foo-1': {'foo': 'baz'},\n                                'foo-2': {},\n                                'foo-3': {},\n                            }\n                        }\n                    },\n                },\n            ]\n        }\n\n    def test_ensures_region_is_not_none(self):\n        with self.assertRaises(NoRegionError):\n            resolver = regions.EndpointResolver(self._template())\n            resolver.construct_endpoint('foo', None)\n\n    def test_ensures_required_keys_present(self):\n        with self.assertRaises(ValueError):\n            regions.EndpointResolver({})\n\n    def test_returns_empty_list_when_listing_for_different_partition(self):\n        resolver = regions.EndpointResolver(self._template())\n        self.assertEqual([], resolver.get_available_endpoints('ec2', 'bar'))\n\n    def test_returns_empty_list_when_no_service_found(self):\n        resolver = regions.EndpointResolver(self._template())\n        self.assertEqual([], resolver.get_available_endpoints('what?'))\n\n    def test_gets_endpoint_names(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.get_available_endpoints(\n            'ec2', allow_non_regional=True\n        )\n        self.assertEqual(\n            ['d', 'eu-baz', 'us-bar', 'us-dep', 'us-fizz', 'us-foo'],\n            sorted(result),\n        )\n\n    def test_gets_endpoint_names_for_partition(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.get_available_endpoints(\n            'ec2', allow_non_regional=True, partition_name='foo'\n        )\n        self.assertEqual(['foo-1', 'foo-2', 'foo-3'], sorted(result))\n\n    def test_list_regional_endpoints_only(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.get_available_endpoints(\n            'ec2', allow_non_regional=False\n        )\n        self.assertEqual(['eu-baz', 'us-bar', 'us-foo'], sorted(result))\n\n    def test_returns_none_when_no_match(self):\n        resolver = regions.EndpointResolver(self._template())\n        self.assertIsNone(resolver.construct_endpoint('foo', 'baz'))\n\n    def test_constructs_regionalized_endpoints_for_exact_matches(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint('not-regionalized', 'eu-baz')\n        self.assertEqual(\n            'not-regionalized.eu-baz.amazonaws.com', result['hostname']\n        )\n        self.assertEqual('aws', result['partition'])\n        self.assertEqual('eu-baz', result['endpointName'])\n\n    def test_constructs_partition_endpoints_for_real_partition_region(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint('not-regionalized', 'us-bar')\n        self.assertEqual('not-regionalized', result['hostname'])\n        self.assertEqual('aws', result['partition'])\n        self.assertEqual('aws', result['endpointName'])\n\n    def test_constructs_partition_endpoints_for_regex_match(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint('not-regionalized', 'us-abc')\n        self.assertEqual('not-regionalized', result['hostname'])\n\n    def test_constructs_endpoints_for_regionalized_regex_match(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint('s3', 'us-abc')\n        self.assertEqual('s3.us-abc.amazonaws.com', result['hostname'])\n\n    def test_constructs_endpoints_for_unknown_service_but_known_region(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint('unknown', 'us-foo')\n        self.assertEqual('unknown.us-foo.amazonaws.com', result['hostname'])\n\n    def test_merges_service_keys(self):\n        resolver = regions.EndpointResolver(self._template())\n        us_foo = resolver.construct_endpoint('merge', 'us-foo')\n        us_bar = resolver.construct_endpoint('merge', 'us-bar')\n        self.assertEqual(['http'], us_foo['protocols'])\n        self.assertEqual(['v4'], us_foo['signatureVersions'])\n        self.assertEqual(['https'], us_bar['protocols'])\n        self.assertEqual(['v2'], us_bar['signatureVersions'])\n\n    def test_merges_partition_default_keys_with_no_overwrite(self):\n        resolver = regions.EndpointResolver(self._template())\n        resolved = resolver.construct_endpoint('ec2', 'foo-1')\n        self.assertEqual('baz', resolved['foo'])\n        self.assertEqual(['http'], resolved['protocols'])\n\n    def test_merges_partition_default_keys_with_overwrite(self):\n        resolver = regions.EndpointResolver(self._template())\n        resolved = resolver.construct_endpoint('ec2', 'foo-2')\n        self.assertEqual('bar', resolved['foo'])\n        self.assertEqual(['http'], resolved['protocols'])\n\n    def test_gives_hostname_and_common_name_unaltered(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint('s3', 'eu-baz')\n        self.assertEqual('s3.eu-baz.amazonaws.com', result['sslCommonName'])\n        self.assertEqual('foo', result['hostname'])\n\n    def tests_uses_partition_endpoint_when_no_region_provided(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint('not-regionalized')\n        self.assertEqual('not-regionalized', result['hostname'])\n        self.assertEqual('aws', result['endpointName'])\n\n    def test_returns_dns_suffix_if_available(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint('not-regionalized')\n        self.assertEqual(result['dnsSuffix'], 'amazonaws.com')\n\n    def test_construct_dualstack_from_endpoint_variant(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            'ec2', 'us-foo', use_dualstack_endpoint=True\n        )\n        self.assertEqual(result['hostname'], 'ec2.foo.api.aws')\n        self.assertEqual(result['dnsSuffix'], 'api.aws')\n\n    def test_construct_dualstack_endpoint_from_service_default_variant(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            'ec2', 'us-bar', use_dualstack_endpoint=True\n        )\n        self.assertEqual(result['hostname'], 'api.ec2.us-bar.api.aws')\n        self.assertEqual(result['dnsSuffix'], 'api.aws')\n\n    def test_construct_dualstack_endpoint_from_partition_default_variant(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            'dynamodb', 'us-bar', use_dualstack_endpoint=True\n        )\n        self.assertEqual(result['hostname'], 'dynamodb.us-bar.api.aws')\n        self.assertEqual(result['dnsSuffix'], 'api.aws')\n\n    def test_constructs_dualstack_endpoint_no_hostname_in_variant(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            's3', 'us-fizz', use_dualstack_endpoint=True\n        )\n        self.assertEqual('s3.dualstack.us-fizz.api.aws', result['hostname'])\n\n    def test_constructs_endpoint_dualstack_no_variant_dns_suffix(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            's3', 'us-bar', use_dualstack_endpoint=True\n        )\n        self.assertEqual('s3.dualstack.us-bar.api.aws', result['hostname'])\n\n    def test_construct_dualstack_endpoint_iso_partition_raise_exception(self):\n        with self.assertRaises(EndpointVariantError):\n            resolver = regions.EndpointResolver(self._template())\n            resolver.construct_endpoint(\n                'foo', 'foo-1', 'aws-iso', use_dualstack_endpoint=True\n            )\n\n    def test_get_partition_dns_suffix_no_tags(self):\n        resolver = regions.EndpointResolver(self._template())\n        self.assertEqual(\n            resolver.get_partition_dns_suffix('aws'), 'amazonaws.com'\n        )\n\n    def test_get_partition_dualstack_dns_suffix(self):\n        resolver = regions.EndpointResolver(self._template())\n        self.assertEqual(\n            resolver.get_partition_dns_suffix('aws', ['dualstack']), 'api.aws'\n        )\n\n    def test_get_partition_dualstack_dns_suffix_does_not_exist(self):\n        resolver = regions.EndpointResolver(self._template())\n        self.assertIsNone(\n            resolver.get_partition_dns_suffix('foo', ['dualstack'])\n        )\n\n    def test_get_available_fips_endpoints(self):\n        resolver = regions.EndpointResolver(self._template())\n        fips_endpoints = resolver.get_available_endpoints(\n            'ec2', endpoint_variant_tags=['fips']\n        )\n        self.assertEqual(fips_endpoints, ['us-foo'])\n\n    def test_get_available_dualstack_endpoints(self):\n        resolver = regions.EndpointResolver(self._template())\n        dualstack_endpoints = resolver.get_available_endpoints(\n            'ec2', endpoint_variant_tags=['dualstack']\n        )\n        self.assertEqual(dualstack_endpoints, ['us-foo'])\n\n    def test_get_available_fips_and_dualstack_endpoints(self):\n        resolver = regions.EndpointResolver(self._template())\n        fips_and_dualstack_endpoints = resolver.get_available_endpoints(\n            'ec2', endpoint_variant_tags=['fips', 'dualstack']\n        )\n        self.assertEqual(fips_and_dualstack_endpoints, ['us-foo'])\n\n    def test_get_available_fips_endpoints_none(self):\n        resolver = regions.EndpointResolver(self._template())\n        fips_endpoints = resolver.get_available_endpoints(\n            'ec2', 'foo', endpoint_variant_tags=['fips']\n        )\n        self.assertEqual(fips_endpoints, [])\n\n    def test_get_available_dualstack_endpoints_none(self):\n        resolver = regions.EndpointResolver(self._template())\n        dualstack_endpoints = resolver.get_available_endpoints(\n            'ec2', 'foo', endpoint_variant_tags=['dualstack']\n        )\n        self.assertEqual(dualstack_endpoints, [])\n\n    def test_get_available_fips_and_dualstack_endpoints_none(self):\n        resolver = regions.EndpointResolver(self._template())\n        fips_and_dualstack_endpoints = resolver.get_available_endpoints(\n            'ec2', 'foo', endpoint_variant_tags=['fips', 'dualstack']\n        )\n        self.assertEqual(fips_and_dualstack_endpoints, [])\n\n    def test_construct_deprecated_endpoint_raises_warning(self):\n        resolver = regions.EndpointResolver(self._template())\n        with self.assertLogs('botocore.regions', level='WARNING') as log:\n            result = resolver.construct_endpoint(\n                'ec2',\n                'us-dep',\n                use_fips_endpoint=True,\n            )\n            self.assertIn('deprecated endpoint', log.output[0])\n            self.assertEqual(\n                result['hostname'], 'ec2-fips.us-dep.amazonaws.com'\n            )\n            self.assertEqual(result['dnsSuffix'], 'amazonaws.com')\n\n    def test_construct_fips_from_endpoint_variant(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            'ec2', 'us-foo', use_fips_endpoint=True\n        )\n        self.assertEqual(result['hostname'], 'ec2-fips.foo.amazonaws.com')\n        self.assertEqual(result['dnsSuffix'], 'amazonaws.com')\n\n    def test_construct_fips_endpoint_from_service_default_variant(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            'ec2', 'us-bar', use_fips_endpoint=True\n        )\n        self.assertEqual(result['hostname'], 'ec2-fips.us-bar.amazonaws.com')\n        self.assertEqual(result['dnsSuffix'], 'amazonaws.com')\n\n    def test_construct_fips_endpoint_from_partition_default_variant(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            'dynamodb', 'us-bar', use_fips_endpoint=True\n        )\n        self.assertEqual(\n            result['hostname'], 'dynamodb-fips.us-bar.amazonaws.com'\n        )\n        self.assertEqual(result['dnsSuffix'], 'amazonaws.com')\n\n    def test_constructs_fips_endpoint_no_hostname_in_variant(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            's3', 'us-fizz', use_fips_endpoint=True\n        )\n        self.assertEqual('s3-fips.us-fizz.amazonaws.com', result['hostname'])\n\n    def test_construct_dualstack_and_fips_from_endpoint_variant(self):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            'ec2',\n            'us-foo',\n            use_dualstack_endpoint=True,\n            use_fips_endpoint=True,\n        )\n        self.assertEqual(result['hostname'], 'ec2-fips.foo.api.aws')\n        self.assertEqual(result['dnsSuffix'], 'api.aws')\n\n    def test_construct_dualstack_and_fips_endpoint_from_service_default_variant(\n        self,\n    ):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            'ec2',\n            'us-bar',\n            use_dualstack_endpoint=True,\n            use_fips_endpoint=True,\n        )\n        self.assertEqual(result['hostname'], 'ec2-fips.us-bar.api.aws')\n        self.assertEqual(result['dnsSuffix'], 'api.aws')\n\n    def test_construct_dualstack_and_fips_endpoint_from_partition_default_variant(\n        self,\n    ):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            'dynamodb',\n            'us-bar',\n            use_dualstack_endpoint=True,\n            use_fips_endpoint=True,\n        )\n        self.assertEqual(result['hostname'], 'dynamodb-fips.us-bar.api.aws')\n        self.assertEqual(result['dnsSuffix'], 'api.aws')\n\n    def test_constructs_dualstack_and_fips_endpoint_no_hostname_in_variant(\n        self,\n    ):\n        resolver = regions.EndpointResolver(self._template())\n        result = resolver.construct_endpoint(\n            's3',\n            'us-fizz',\n            use_dualstack_endpoint=True,\n            use_fips_endpoint=True,\n        )\n        self.assertEqual(\n            's3-fips.dualstack.us-fizz.api.aws', result['hostname']\n        )\n\n    def test_construct_fips_endpoint_no_variant_raise_exception(self):\n        with self.assertRaises(EndpointVariantError):\n            resolver = regions.EndpointResolver(self._template())\n            resolver.construct_endpoint(\n                'ec2', 'foo-1', 'foo', use_fips_endpoint=True\n            )\n\n    def test_construct_dualstack_endpoint_no_variant_raise_exception(self):\n        with self.assertRaises(EndpointVariantError):\n            resolver = regions.EndpointResolver(self._template())\n            resolver.construct_endpoint(\n                'ec2', 'foo-1', 'foo', use_dualstack_endpoint=True\n            )\n\n    def test_construct_dualstack_and_fips_endpoint_no_variant_raise_exception(\n        self,\n    ):\n        with self.assertRaises(EndpointVariantError):\n            resolver = regions.EndpointResolver(self._template())\n            resolver.construct_endpoint(\n                'ec2',\n                'foo-1',\n                'foo',\n                use_dualstack_endpoint=True,\n                use_fips_endpoint=True,\n            )\n\n\ndef _variant_test_definitions():\n    return [\n        {\n            \"service\": \"default-pattern-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"default-pattern-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"default-pattern-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"default-pattern-service-fips.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"default-pattern-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"default-pattern-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"default-pattern-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"default-pattern-service-fips.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"global-service\",\n            \"region\": \"aws-global\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"global-service.amazonaws.com\",\n        },\n        {\n            \"service\": \"global-service\",\n            \"region\": \"aws-global\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"global-service-fips.amazonaws.com\",\n        },\n        {\n            \"service\": \"global-service\",\n            \"region\": \"foo\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"global-service.amazonaws.com\",\n        },\n        {\n            \"service\": \"global-service\",\n            \"region\": \"foo\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"global-service-fips.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"fips.override-variant-service.us-west-2.new.dns.suffix\",\n        },\n        {\n            \"service\": \"override-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"fips.override-variant-service.af-south-1.new.dns.suffix\",\n        },\n        {\n            \"service\": \"override-variant-dns-suffix-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-dns-suffix-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-dns-suffix-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-dns-suffix-service-fips.us-west-2.new.dns.suffix\",\n        },\n        {\n            \"service\": \"override-variant-dns-suffix-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-dns-suffix-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-dns-suffix-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-dns-suffix-service-fips.af-south-1.new.dns.suffix\",\n        },\n        {\n            \"service\": \"override-variant-hostname-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-hostname-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-hostname-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"fips.override-variant-hostname-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-hostname-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-hostname-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-hostname-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"fips.override-variant-hostname-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-endpoint-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-endpoint-variant-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-endpoint-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"fips.override-endpoint-variant-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-endpoint-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-endpoint-variant-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-endpoint-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"override-endpoint-variant-service-fips.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"default-pattern-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"default-pattern-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"default-pattern-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"default-pattern-service.us-west-2.api.aws\",\n        },\n        {\n            \"service\": \"default-pattern-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"default-pattern-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"default-pattern-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"default-pattern-service.af-south-1.api.aws\",\n        },\n        {\n            \"service\": \"global-service\",\n            \"region\": \"aws-global\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"global-service.amazonaws.com\",\n        },\n        {\n            \"service\": \"global-service\",\n            \"region\": \"aws-global\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"global-service.api.aws\",\n        },\n        {\n            \"service\": \"global-service\",\n            \"region\": \"foo\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"global-service.amazonaws.com\",\n        },\n        {\n            \"service\": \"global-service\",\n            \"region\": \"foo\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"global-service.api.aws\",\n        },\n        {\n            \"service\": \"override-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"override-variant-service.dualstack.us-west-2.new.dns.suffix\",\n        },\n        {\n            \"service\": \"override-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"override-variant-service.dualstack.af-south-1.new.dns.suffix\",\n        },\n        {\n            \"service\": \"override-variant-dns-suffix-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-dns-suffix-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-dns-suffix-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"override-variant-dns-suffix-service.us-west-2.new.dns.suffix\",\n        },\n        {\n            \"service\": \"override-variant-dns-suffix-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-dns-suffix-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-dns-suffix-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"override-variant-dns-suffix-service.af-south-1.new.dns.suffix\",\n        },\n        {\n            \"service\": \"override-variant-hostname-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-hostname-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-hostname-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"override-variant-hostname-service.dualstack.us-west-2.api.aws\",\n        },\n        {\n            \"service\": \"override-variant-hostname-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-variant-hostname-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-variant-hostname-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"override-variant-hostname-service.dualstack.af-south-1.api.aws\",\n        },\n        {\n            \"service\": \"override-endpoint-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-endpoint-variant-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-endpoint-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"override-endpoint-variant-service.dualstack.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-endpoint-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"override-endpoint-variant-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"override-endpoint-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"override-endpoint-variant-service.af-south-1.api.aws\",\n        },\n        {\n            \"service\": \"multi-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"multi-variant-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"multi-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"multi-variant-service.dualstack.us-west-2.api.aws\",\n        },\n        {\n            \"service\": \"multi-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"fips.multi-variant-service.us-west-2.amazonaws.com\",\n        },\n        {\n            \"service\": \"multi-variant-service\",\n            \"region\": \"us-west-2\",\n            \"fips\": True,\n            \"dualstack\": True,\n            \"endpoint\": \"fips.multi-variant-service.dualstack.us-west-2.new.dns.suffix\",\n        },\n        {\n            \"service\": \"multi-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": False,\n            \"endpoint\": \"multi-variant-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"multi-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": False,\n            \"dualstack\": True,\n            \"endpoint\": \"multi-variant-service.dualstack.af-south-1.api.aws\",\n        },\n        {\n            \"service\": \"multi-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": True,\n            \"dualstack\": False,\n            \"endpoint\": \"fips.multi-variant-service.af-south-1.amazonaws.com\",\n        },\n        {\n            \"service\": \"multi-variant-service\",\n            \"region\": \"af-south-1\",\n            \"fips\": True,\n            \"dualstack\": True,\n            \"endpoint\": \"fips.multi-variant-service.dualstack.af-south-1.new.dns.suffix\",\n        },\n    ]\n\n\ndef _modeled_variants_template():\n    return {\n        \"partitions\": [\n            {\n                \"defaults\": {\n                    \"hostname\": \"{service}.{region}.{dnsSuffix}\",\n                    \"protocols\": [\"https\"],\n                    \"signatureVersions\": [\"v4\"],\n                    \"variants\": [\n                        {\n                            \"dnsSuffix\": \"amazonaws.com\",\n                            \"hostname\": \"{service}-fips.{region}.{dnsSuffix}\",\n                            \"tags\": [\"fips\"],\n                        },\n                        {\n                            \"dnsSuffix\": \"api.aws\",\n                            \"hostname\": \"{service}.{region}.{dnsSuffix}\",\n                            \"tags\": [\"dualstack\"],\n                        },\n                        {\n                            \"dnsSuffix\": \"api.aws\",\n                            \"hostname\": \"{service}-fips.{region}.{dnsSuffix}\",\n                            \"tags\": [\"dualstack\", \"fips\"],\n                        },\n                    ],\n                },\n                \"dnsSuffix\": \"amazonaws.com\",\n                \"partition\": \"aws\",\n                \"regionRegex\": \"^(us|eu|ap|sa|ca|me|af)\\\\-\\\\w+\\\\-\\\\d+$\",\n                \"regions\": {\n                    \"af-south-1\": {\"description\": \"Africa (Cape Town)\"},\n                    \"us-west-2\": {\"description\": \"US West (Oregon)\"},\n                },\n                \"services\": {\n                    \"default-pattern-service\": {\n                        \"endpoints\": {\n                            \"af-south-1\": {},\n                            \"us-west-2\": {\n                                \"variants\": [\n                                    {\n                                        \"hostname\": \"default-pattern-service-fips.us-west-2.amazonaws.com\",\n                                        \"tags\": [\"fips\"],\n                                    },\n                                    {\n                                        \"hostname\": \"default-pattern-service.us-west-2.api.aws\",\n                                        \"tags\": [\"dualstack\"],\n                                    },\n                                ]\n                            },\n                        }\n                    },\n                    \"global-service\": {\n                        \"endpoints\": {\n                            \"aws-global\": {\n                                \"credentialScope\": {\"region\": \"us-east-1\"},\n                                \"hostname\": \"global-service.amazonaws.com\",\n                                \"variants\": [\n                                    {\n                                        \"hostname\": \"global-service-fips.amazonaws.com\",\n                                        \"tags\": [\"fips\"],\n                                    },\n                                    {\n                                        \"hostname\": \"global-service.api.aws\",\n                                        \"tags\": [\"dualstack\"],\n                                    },\n                                ],\n                            }\n                        },\n                        \"isRegionalized\": False,\n                        \"partitionEndpoint\": \"aws-global\",\n                    },\n                    \"override-variant-service\": {\n                        \"defaults\": {\n                            \"variants\": [\n                                {\n                                    \"hostname\": \"fips.{service}.{region}.{dnsSuffix}\",\n                                    \"dnsSuffix\": \"new.dns.suffix\",\n                                    \"tags\": [\"fips\"],\n                                },\n                                {\n                                    \"hostname\": \"{service}.dualstack.{region}.{dnsSuffix}\",\n                                    \"dnsSuffix\": \"new.dns.suffix\",\n                                    \"tags\": [\"dualstack\"],\n                                },\n                            ]\n                        },\n                        \"endpoints\": {\n                            \"af-south-1\": {},\n                            \"us-west-2\": {\n                                \"variants\": [\n                                    {\n                                        \"hostname\": \"fips.override-variant-service.us-west-2.new.dns.suffix\",\n                                        \"tags\": [\"fips\"],\n                                    },\n                                    {\n                                        \"hostname\": \"override-variant-service.dualstack.us-west-2.new.dns.suffix\",\n                                        \"tags\": [\"dualstack\"],\n                                    },\n                                ]\n                            },\n                        },\n                    },\n                    \"override-variant-dns-suffix-service\": {\n                        \"defaults\": {\n                            \"variants\": [\n                                {\n                                    \"dnsSuffix\": \"new.dns.suffix\",\n                                    \"tags\": [\"fips\"],\n                                },\n                                {\n                                    \"dnsSuffix\": \"new.dns.suffix\",\n                                    \"tags\": [\"dualstack\"],\n                                },\n                            ]\n                        },\n                        \"endpoints\": {\n                            \"af-south-1\": {},\n                            \"us-west-2\": {\n                                \"variants\": [\n                                    {\n                                        \"hostname\": \"override-variant-dns-suffix-service-fips.us-west-2.new.dns.suffix\",\n                                        \"tags\": [\"fips\"],\n                                    },\n                                    {\n                                        \"hostname\": \"override-variant-dns-suffix-service.us-west-2.new.dns.suffix\",\n                                        \"tags\": [\"dualstack\"],\n                                    },\n                                ]\n                            },\n                        },\n                    },\n                    \"override-variant-hostname-service\": {\n                        \"defaults\": {\n                            \"variants\": [\n                                {\n                                    \"hostname\": \"fips.{service}.{region}.{dnsSuffix}\",\n                                    \"tags\": [\"fips\"],\n                                },\n                                {\n                                    \"hostname\": \"{service}.dualstack.{region}.{dnsSuffix}\",\n                                    \"tags\": [\"dualstack\"],\n                                },\n                            ]\n                        },\n                        \"endpoints\": {\n                            \"af-south-1\": {},\n                            \"us-west-2\": {\n                                \"variants\": [\n                                    {\n                                        \"hostname\": \"fips.override-variant-hostname-service.us-west-2.amazonaws.com\",\n                                        \"tags\": [\"fips\"],\n                                    },\n                                    {\n                                        \"hostname\": \"override-variant-hostname-service.dualstack.us-west-2.api.aws\",\n                                        \"tags\": [\"dualstack\"],\n                                    },\n                                ]\n                            },\n                        },\n                    },\n                    \"override-endpoint-variant-service\": {\n                        \"endpoints\": {\n                            \"af-south-1\": {},\n                            \"us-west-2\": {\n                                \"variants\": [\n                                    {\n                                        \"hostname\": \"fips.override-endpoint-variant-service.us-west-2.amazonaws.com\",\n                                        \"tags\": [\"fips\"],\n                                    },\n                                    {\n                                        \"hostname\": \"override-endpoint-variant-service.dualstack.us-west-2.amazonaws.com\",\n                                        \"tags\": [\"dualstack\"],\n                                    },\n                                ]\n                            },\n                        }\n                    },\n                    \"multi-variant-service\": {\n                        \"defaults\": {\n                            \"variants\": [\n                                {\n                                    \"hostname\": \"fips.{service}.{region}.{dnsSuffix}\",\n                                    \"tags\": [\"fips\"],\n                                },\n                                {\n                                    \"hostname\": \"{service}.dualstack.{region}.{dnsSuffix}\",\n                                    \"tags\": [\"dualstack\"],\n                                },\n                                {\n                                    \"dnsSuffix\": \"new.dns.suffix\",\n                                    \"hostname\": \"fips.{service}.dualstack.{region}.{dnsSuffix}\",\n                                    \"tags\": [\"fips\", \"dualstack\"],\n                                },\n                            ]\n                        },\n                        \"endpoints\": {\n                            \"af-south-1\": {\"deprecated\": True},\n                            \"us-west-2\": {\n                                \"variants\": [\n                                    {\n                                        \"hostname\": \"fips.multi-variant-service.dualstack.us-west-2.new.dns.suffix\",\n                                        \"tags\": [\"fips\", \"dualstack\"],\n                                    }\n                                ]\n                            },\n                        },\n                    },\n                },\n            },\n            {\n                \"defaults\": {\n                    \"hostname\": \"{service}.{region}.{dnsSuffix}\",\n                    \"protocols\": [\"https\"],\n                    \"signatureVersions\": [\"v4\"],\n                },\n                \"dnsSuffix\": \"c2s.ic.gov\",\n                \"partition\": \"aws-iso\",\n                \"regionRegex\": \"^us\\\\-iso\\\\-\\\\w+\\\\-\\\\d+$\",\n                \"regions\": {\"us-iso-east-1\": {\"description\": \"US ISO East\"}},\n                \"services\": {\n                    \"some-service\": {\"endpoints\": {\"us-iso-east-1\": {}}}\n                },\n            },\n        ],\n        \"version\": 3,\n    }\n\n\n@pytest.mark.parametrize(\"test_case\", _variant_test_definitions())\ndef test_modeled_variants(test_case):\n    _verify_expected_endpoint(**test_case)\n\n\ndef _verify_expected_endpoint(service, region, fips, dualstack, endpoint):\n    resolver = regions.EndpointResolver(_modeled_variants_template())\n    resolved = resolver.construct_endpoint(\n        service,\n        region,\n        use_dualstack_endpoint=dualstack,\n        use_fips_endpoint=fips,\n    )\n    # If we can't resolve the region, we attempt to get a\n    # global endpoint.\n    if not resolved:\n        resolved = resolver.construct_endpoint(\n            service,\n            region,\n            partition_name='aws',\n            use_dualstack_endpoint=dualstack,\n            use_fips_endpoint=fips,\n        )\n    assert resolved['hostname'] == endpoint\n\n\ndef test_additional_endpoint_data_exists_with_variants():\n    resolver = regions.EndpointResolver(_modeled_variants_template())\n    resolved = resolver.construct_endpoint(\n        'global-service',\n        'aws-global',\n        use_fips_endpoint=True,\n    )\n    assert resolved['credentialScope'] == {'region': 'us-east-1'}\n", "tests/unit/test_protocols.py": "#!/usr/bin/env python\n# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Test runner for the JSON models compliance tests\n\nThis is a test runner for all the JSON tests defined in\n``tests/unit/protocols/``, including both the input/output tests.\n\nYou can use the normal ``python -m pytest tests/unit/test_protocols.py``\nto run this test.  In addition, there are several env vars you can use during\ndevelopment.\n\nTests are broken down by filename, test suite, testcase.  When a test fails\nyou'll see the protocol (filename), test suite, and test case number of the\nfailed test.\n\n::\n\n    Description           : Scalar members (0:0)  <--- (suite_id:test_id)\n    Protocol:             : ec2                  <--- test file (ec2.json)\n    Given                 : ...\n    Response              : ...\n    Expected serialization: ...\n    Actual serialization  : ...\n    Assertion message     : ...\n\nTo run tests from only a single file, you can set the\nBOTOCORE_TEST env var::\n\n    BOTOCORE_TEST=tests/unit/compliance/input/json.json pytest tests/unit/test_protocols.py\n\nTo run a single test suite you can set the BOTOCORE_TEST_ID env var:\n\n    BOTOCORE_TEST=tests/unit/compliance/input/json.json BOTOCORE_TEST_ID=5 \\\n        pytest tests/unit/test_protocols.py\n\nTo run a single test case in a suite (useful when debugging a single test), you\ncan set the BOTOCORE_TEST_ID env var with the ``suite_id:test_id`` syntax.\n\n    BOTOCORE_TEST_ID=5:1 pytest test/unit/test_protocols.py\n\n\"\"\"\nimport copy\nimport os\nfrom base64 import b64decode\nfrom calendar import timegm\nfrom enum import Enum\n\nimport pytest\nfrom dateutil.tz import tzutc\n\nfrom botocore.awsrequest import HeadersDict, prepare_request_dict\nfrom botocore.compat import OrderedDict, json, urlsplit\nfrom botocore.eventstream import EventStream\nfrom botocore.model import NoShapeFoundError, OperationModel, ServiceModel\nfrom botocore.parsers import (\n    EC2QueryParser,\n    JSONParser,\n    QueryParser,\n    RestJSONParser,\n    RestXMLParser,\n)\nfrom botocore.serialize import (\n    EC2Serializer,\n    JSONSerializer,\n    QuerySerializer,\n    RestJSONSerializer,\n    RestXMLSerializer,\n)\nfrom botocore.utils import parse_timestamp, percent_encode_sequence\n\nTEST_DIR = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), 'protocols'\n)\nNOT_SPECIFIED = object()\nPROTOCOL_SERIALIZERS = {\n    'ec2': EC2Serializer,\n    'query': QuerySerializer,\n    'json': JSONSerializer,\n    'rest-json': RestJSONSerializer,\n    'rest-xml': RestXMLSerializer,\n}\nPROTOCOL_PARSERS = {\n    'ec2': EC2QueryParser,\n    'query': QueryParser,\n    'json': JSONParser,\n    'rest-json': RestJSONParser,\n    'rest-xml': RestXMLParser,\n}\nPROTOCOL_TEST_BLACKLIST = ['Idempotency token auto fill']\n\n\nclass TestType(Enum):\n    # Tell test runner to ignore this class\n    __test__ = False\n\n    INPUT = \"input\"\n    OUTPUT = \"output\"\n\n\ndef _compliance_tests(test_type=None):\n    inp = test_type is None or test_type is TestType.INPUT\n    out = test_type is None or test_type is TestType.OUTPUT\n\n    for full_path in _walk_files():\n        if full_path.endswith('.json'):\n            for model, case, basename in _load_cases(full_path):\n                if model.get('description') in PROTOCOL_TEST_BLACKLIST:\n                    continue\n                if 'params' in case and inp:\n                    yield model, case, basename\n                elif 'response' in case and out:\n                    yield model, case, basename\n\n\n@pytest.mark.parametrize(\n    \"json_description, case, basename\", _compliance_tests(TestType.INPUT)\n)\ndef test_input_compliance(json_description, case, basename):\n    service_description = copy.deepcopy(json_description)\n    service_description['operations'] = {\n        case.get('name', 'OperationName'): case,\n    }\n    model = ServiceModel(service_description)\n    protocol_type = model.metadata['protocol']\n    try:\n        protocol_serializer = PROTOCOL_SERIALIZERS[protocol_type]\n    except KeyError:\n        raise RuntimeError(\"Unknown protocol: %s\" % protocol_type)\n    serializer = protocol_serializer()\n    serializer.MAP_TYPE = OrderedDict\n    operation_model = OperationModel(case['given'], model)\n    request = serializer.serialize_to_request(case['params'], operation_model)\n    _serialize_request_description(request)\n    client_endpoint = service_description.get('clientEndpoint')\n    try:\n        _assert_request_body_is_bytes(request['body'])\n        _assert_requests_equal(request, case['serialized'])\n        _assert_endpoints_equal(request, case['serialized'], client_endpoint)\n    except AssertionError as e:\n        _input_failure_message(protocol_type, case, request, e)\n\n\ndef _assert_request_body_is_bytes(body):\n    if not isinstance(body, bytes):\n        raise AssertionError(\n            \"Expected body to be serialized as type \"\n            \"bytes(), instead got: %s\" % type(body)\n        )\n\n\ndef _assert_endpoints_equal(actual, expected, endpoint):\n    if 'host' not in expected:\n        return\n    prepare_request_dict(actual, endpoint)\n    actual_host = urlsplit(actual['url']).netloc\n    assert_equal(actual_host, expected['host'], 'Host')\n\n\nclass MockRawResponse:\n    def __init__(self, data):\n        self._data = b64decode(data)\n\n    def stream(self):\n        yield self._data\n\n\n@pytest.mark.parametrize(\n    \"json_description, case, basename\", _compliance_tests(TestType.OUTPUT)\n)\ndef test_output_compliance(json_description, case, basename):\n    service_description = copy.deepcopy(json_description)\n    operation_name = case.get('name', 'OperationName')\n    service_description['operations'] = {\n        operation_name: case,\n    }\n    case['response']['context'] = {'operation_name': operation_name}\n    try:\n        model = ServiceModel(service_description)\n        operation_model = OperationModel(case['given'], model)\n        parser = PROTOCOL_PARSERS[model.metadata['protocol']](\n            timestamp_parser=_compliance_timestamp_parser\n        )\n        # We load the json as utf-8, but the response parser is at the\n        # botocore boundary, so it expects to work with bytes.\n        body_bytes = case['response']['body'].encode('utf-8')\n        case['response']['body'] = body_bytes\n        # We need the headers to be case insensitive\n        headers = HeadersDict(case['response']['headers'])\n        case['response']['headers'] = headers\n        # If this is an event stream fake the raw streamed response\n        if operation_model.has_event_stream_output:\n            case['response']['body'] = MockRawResponse(body_bytes)\n        if 'error' in case:\n            output_shape = operation_model.output_shape\n            parsed = parser.parse(case['response'], output_shape)\n            try:\n                error_shape = model.shape_for(parsed['Error']['Code'])\n            except NoShapeFoundError:\n                error_shape = None\n            if error_shape is not None:\n                error_parse = parser.parse(case['response'], error_shape)\n                parsed.update(error_parse)\n        else:\n            output_shape = operation_model.output_shape\n            parsed = parser.parse(case['response'], output_shape)\n        parsed = _fixup_parsed_result(parsed)\n    except Exception as e:\n        msg = (\n            \"\\nFailed to run test  : %s\\n\"\n            \"Protocol            : %s\\n\"\n            \"Description         : %s (%s:%s)\\n\"\n            % (\n                e,\n                model.metadata['protocol'],\n                case['description'],\n                case['suite_id'],\n                case['test_id'],\n            )\n        )\n        raise AssertionError(msg)\n    try:\n        if 'error' in case:\n            expected_result = {\n                'Error': {\n                    'Code': case.get('errorCode', ''),\n                    'Message': case.get('errorMessage', ''),\n                }\n            }\n            expected_result.update(case['error'])\n        else:\n            expected_result = case['result']\n        assert_equal(parsed, expected_result, \"Body\")\n    except Exception as e:\n        _output_failure_message(\n            model.metadata['protocol'], case, parsed, expected_result, e\n        )\n\n\ndef _fixup_parsed_result(parsed):\n    # This function contains all the transformation we need\n    # to do from the response _our_ response parsers give\n    # vs. the expected responses in the protocol tests.\n    # These are implementation specific changes, not any\n    # \"we're not following the spec\"-type changes.\n\n    # 1. RequestMetadata.  We parse this onto the returned dict, but compliance\n    # tests don't have any specs for how to deal with request metadata.\n    if 'ResponseMetadata' in parsed:\n        del parsed['ResponseMetadata']\n    # 2. Binary blob types.  In the protocol test, blob types, when base64\n    # decoded, always decode to something that can be expressed via utf-8.\n    # This is not always the case.  In python3, the blob type is designed to\n    # return a bytes (not str) object.  However, for these tests we'll work for\n    # any bytes type, and decode it as utf-8 because we know that's safe for\n    # the compliance tests.\n    parsed = _convert_bytes_to_str(parsed)\n    # 3. We need to expand the event stream object into the list of events\n    for key, value in parsed.items():\n        if isinstance(value, EventStream):\n            parsed[key] = _convert_bytes_to_str(list(value))\n            break\n    # 4. We parse the entire error body into the \"Error\" field for rest-xml\n    # which causes some modeled fields in the response to be placed under the\n    # error key. We don't have enough information in the test suite to assert\n    # these properly, and they probably shouldn't be there in the first place.\n    if 'Error' in parsed:\n        error_keys = list(parsed['Error'].keys())\n        for key in error_keys:\n            if key not in ['Code', 'Message']:\n                del parsed['Error'][key]\n    return parsed\n\n\ndef _convert_bytes_to_str(parsed):\n    if isinstance(parsed, dict):\n        new_dict = {}\n        for key, value in parsed.items():\n            new_dict[key] = _convert_bytes_to_str(value)\n        return new_dict\n    elif isinstance(parsed, bytes):\n        return parsed.decode('utf-8')\n    elif isinstance(parsed, list):\n        new_list = []\n        for item in parsed:\n            new_list.append(_convert_bytes_to_str(item))\n        return new_list\n    else:\n        return parsed\n\n\ndef _compliance_timestamp_parser(value):\n    datetime = parse_timestamp(value)\n    # Convert from our time zone to UTC\n    datetime = datetime.astimezone(tzutc())\n    # Convert to epoch.\n    return int(timegm(datetime.timetuple()))\n\n\ndef _output_failure_message(\n    protocol_type, case, actual_parsed, expected_result, error\n):\n    j = _try_json_dump\n    error_message = (\n        \"\\nDescription           : %s (%s:%s)\\n\"\n        \"Protocol:             : %s\\n\"\n        \"Given                 : %s\\n\"\n        \"Response              : %s\\n\"\n        \"Expected serialization: %s\\n\"\n        \"Actual serialization  : %s\\n\"\n        \"Assertion message     : %s\\n\"\n        % (\n            case['description'],\n            case['suite_id'],\n            case['test_id'],\n            protocol_type,\n            j(case['given']),\n            j(case['response']),\n            j(expected_result),\n            j(actual_parsed),\n            error,\n        )\n    )\n    raise AssertionError(error_message)\n\n\ndef _input_failure_message(protocol_type, case, actual_request, error):\n    j = _try_json_dump\n    error_message = (\n        \"\\nDescription           : %s (%s:%s)\\n\"\n        \"Protocol:             : %s\\n\"\n        \"Given                 : %s\\n\"\n        \"Params                : %s\\n\"\n        \"Expected serialization: %s\\n\"\n        \"Actual serialization  : %s\\n\"\n        \"Assertion message     : %s\\n\"\n        % (\n            case['description'],\n            case['suite_id'],\n            case['test_id'],\n            protocol_type,\n            j(case['given']),\n            j(case['params']),\n            j(case['serialized']),\n            j(actual_request),\n            error,\n        )\n    )\n    raise AssertionError(error_message)\n\n\ndef _try_json_dump(obj):\n    try:\n        return json.dumps(obj)\n    except (ValueError, TypeError):\n        return str(obj)\n\n\ndef assert_equal(first, second, prefix):\n    # A better assert equals.  It allows you to just provide\n    # prefix instead of the entire message.\n    try:\n        assert first == second\n    except Exception:\n        try:\n            better = \"{} (actual != expected)\\n{} !=\\n{}\".format(\n                prefix,\n                json.dumps(first, indent=2),\n                json.dumps(second, indent=2),\n            )\n        except (ValueError, TypeError):\n            better = \"{} (actual != expected)\\n{} !=\\n{}\".format(\n                prefix, first, second\n            )\n        raise AssertionError(better)\n\n\ndef _serialize_request_description(request_dict):\n    if isinstance(request_dict.get('body'), dict):\n        # urlencode the request body.\n        encoded = percent_encode_sequence(request_dict['body']).encode('utf-8')\n        request_dict['body'] = encoded\n    if isinstance(request_dict.get('query_string'), dict):\n        encoded = percent_encode_sequence(request_dict.get('query_string'))\n        if encoded:\n            # 'requests' automatically handle this, but we in the\n            # test runner we need to handle the case where the url_path\n            # already has query params.\n            if '?' not in request_dict['url_path']:\n                request_dict['url_path'] += '?%s' % encoded\n            else:\n                request_dict['url_path'] += '&%s' % encoded\n\n\ndef _assert_requests_equal(actual, expected):\n    assert_equal(\n        actual['body'], expected.get('body', '').encode('utf-8'), 'Body value'\n    )\n    actual_headers = HeadersDict(actual['headers'])\n    expected_headers = HeadersDict(expected.get('headers', {}))\n    excluded_headers = expected.get('forbidHeaders', [])\n    _assert_expected_headers_in_request(\n        actual_headers, expected_headers, excluded_headers\n    )\n    assert_equal(actual['url_path'], expected.get('uri', ''), \"URI\")\n    if 'method' in expected:\n        assert_equal(actual['method'], expected['method'], \"Method\")\n\n\ndef _assert_expected_headers_in_request(actual, expected, excluded_headers):\n    for header, value in expected.items():\n        assert header in actual\n        assert actual[header] == value\n    for header in excluded_headers:\n        assert header not in actual\n\n\ndef _walk_files():\n    # Check for a shortcut when running the tests interactively.\n    # If a BOTOCORE_TEST env var is defined, that file is used as the\n    # only test to run.  Useful when doing feature development.\n    single_file = os.environ.get('BOTOCORE_TEST')\n    if single_file is not None:\n        yield os.path.abspath(single_file)\n    else:\n        for root, _, filenames in os.walk(TEST_DIR):\n            for filename in filenames:\n                yield os.path.join(root, filename)\n\n\ndef _load_cases(full_path):\n    # During developement, you can set the BOTOCORE_TEST_ID\n    # to run a specific test suite or even a specific test case.\n    # The format is BOTOCORE_TEST_ID=suite_id:test_id or\n    # BOTOCORE_TEST_ID=suite_id\n    suite_id, test_id = _get_suite_test_id()\n    all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict)\n    basename = os.path.basename(full_path)\n    for i, test_data in enumerate(all_test_data):\n        if suite_id is not None and i != suite_id:\n            continue\n        cases = test_data.pop('cases')\n        description = test_data['description']\n        for j, case in enumerate(cases):\n            if test_id is not None and j != test_id:\n                continue\n            case['description'] = description\n            case['suite_id'] = i\n            case['test_id'] = j\n            yield (test_data, case, basename)\n\n\ndef _get_suite_test_id():\n    if 'BOTOCORE_TEST_ID' not in os.environ:\n        return None, None\n    test_id = None\n    suite_id = None\n    split = os.environ['BOTOCORE_TEST_ID'].split(':')\n    try:\n        if len(split) == 2:\n            suite_id, test_id = int(split[0]), int(split[1])\n        else:\n            suite_id = int(split([0]))\n    except TypeError:\n        # Same exception, just give a better error message.\n        raise TypeError(\n            \"Invalid format for BOTOCORE_TEST_ID, should be \"\n            \"suite_id[:test_id], and both values should be \"\n            \"integers.\"\n        )\n    return suite_id, test_id\n", "tests/unit/test_http_session.py": "import socket\n\nimport pytest\nfrom urllib3.exceptions import NewConnectionError, ProtocolError, ProxyError\n\nfrom botocore.awsrequest import (\n    AWSHTTPConnectionPool,\n    AWSHTTPSConnectionPool,\n    AWSRequest,\n)\nfrom botocore.exceptions import (\n    ConnectionClosedError,\n    EndpointConnectionError,\n    ProxyConnectionError,\n)\nfrom botocore.httpsession import (\n    ProxyConfiguration,\n    URLLib3Session,\n    get_cert_path,\n    mask_proxy_url,\n)\nfrom tests import mock, unittest\n\n\nclass TestProxyConfiguration(unittest.TestCase):\n    def setUp(self):\n        self.url = 'http://localhost/'\n        self.auth_url = 'http://user:pass@localhost/'\n        self.proxy_config = ProxyConfiguration(\n            proxies={'http': 'http://localhost:8081/'}\n        )\n\n    def update_http_proxy(self, url):\n        self.proxy_config = ProxyConfiguration(proxies={'http': url})\n\n    def test_construct_proxy_headers_with_auth(self):\n        headers = self.proxy_config.proxy_headers_for(self.auth_url)\n        proxy_auth = headers.get('Proxy-Authorization')\n        self.assertEqual('Basic dXNlcjpwYXNz', proxy_auth)\n\n    def test_construct_proxy_headers_without_auth(self):\n        headers = self.proxy_config.proxy_headers_for(self.url)\n        self.assertEqual({}, headers)\n\n    def test_proxy_for_url_no_slashes(self):\n        self.update_http_proxy('localhost:8081/')\n        proxy_url = self.proxy_config.proxy_url_for(self.url)\n        self.assertEqual('http://localhost:8081/', proxy_url)\n\n    def test_proxy_for_url_no_protocol(self):\n        self.update_http_proxy('//localhost:8081/')\n        proxy_url = self.proxy_config.proxy_url_for(self.url)\n        self.assertEqual('http://localhost:8081/', proxy_url)\n\n    def test_fix_proxy_url_has_protocol_http(self):\n        proxy_url = self.proxy_config.proxy_url_for(self.url)\n        self.assertEqual('http://localhost:8081/', proxy_url)\n\n\nclass TestHttpSessionUtils(unittest.TestCase):\n    def test_get_cert_path_path(self):\n        path = '/some/path'\n        cert_path = get_cert_path(path)\n        self.assertEqual(path, cert_path)\n\n    def test_get_cert_path_certifi_or_default(self):\n        with mock.patch('botocore.httpsession.where') as where:\n            path = '/bundle/path'\n            where.return_value = path\n            cert_path = get_cert_path(True)\n            self.assertEqual(path, cert_path)\n\n\n@pytest.mark.parametrize(\n    'proxy_url, expected_mask_url',\n    (\n        ('http://myproxy.amazonaws.com', 'http://myproxy.amazonaws.com'),\n        (\n            'http://user@myproxy.amazonaws.com',\n            'http://***@myproxy.amazonaws.com',\n        ),\n        (\n            'http://user:pass@myproxy.amazonaws.com',\n            'http://***:***@myproxy.amazonaws.com',\n        ),\n        (\n            'https://user:pass@myproxy.amazonaws.com',\n            'https://***:***@myproxy.amazonaws.com',\n        ),\n        ('http://user:pass@localhost', 'http://***:***@localhost'),\n        ('http://user:pass@localhost:80', 'http://***:***@localhost:80'),\n        ('http://user:pass@userpass.com', 'http://***:***@userpass.com'),\n        ('http://user:pass@192.168.1.1', 'http://***:***@192.168.1.1'),\n        ('http://user:pass@[::1]', 'http://***:***@[::1]'),\n        ('http://user:pass@[::1]:80', 'http://***:***@[::1]:80'),\n    ),\n)\ndef test_mask_proxy_url(proxy_url, expected_mask_url):\n    assert mask_proxy_url(proxy_url) == expected_mask_url\n\n\nclass TestURLLib3Session(unittest.TestCase):\n    def setUp(self):\n        self.request = AWSRequest(\n            method='GET',\n            url='http://example.com/',\n            headers={},\n            data=b'',\n        )\n\n        self.response = mock.Mock()\n        self.response.headers = {}\n        self.response.stream.return_value = b''\n\n        self.pool_manager = mock.Mock()\n        self.connection = mock.Mock()\n        self.connection.urlopen.return_value = self.response\n        self.pool_manager.connection_from_url.return_value = self.connection\n\n        self.pool_patch = mock.patch('botocore.httpsession.PoolManager')\n        self.proxy_patch = mock.patch('botocore.httpsession.proxy_from_url')\n        self.pool_manager_cls = self.pool_patch.start()\n        self.proxy_manager_fun = self.proxy_patch.start()\n        self.pool_manager_cls.return_value = self.pool_manager\n        self.proxy_manager_fun.return_value = self.pool_manager\n\n    def tearDown(self):\n        self.pool_patch.stop()\n        self.proxy_patch.stop()\n\n    def assert_request_sent(\n        self, headers=None, body=None, url='/', chunked=False\n    ):\n        if headers is None:\n            headers = {}\n\n        self.connection.urlopen.assert_called_once_with(\n            method=self.request.method,\n            url=url,\n            body=body,\n            headers=headers,\n            retries=mock.ANY,\n            assert_same_host=False,\n            preload_content=False,\n            decode_content=False,\n            chunked=chunked,\n        )\n\n    def _assert_manager_call(self, manager, *assert_args, **assert_kwargs):\n        call_kwargs = {\n            'maxsize': mock.ANY,\n            'timeout': mock.ANY,\n            'ssl_context': mock.ANY,\n            'socket_options': [],\n            'cert_file': None,\n            'key_file': None,\n        }\n        call_kwargs.update(assert_kwargs)\n        manager.assert_called_with(*assert_args, **call_kwargs)\n\n    def assert_pool_manager_call(self, *args, **kwargs):\n        self._assert_manager_call(self.pool_manager_cls, *args, **kwargs)\n\n    def assert_proxy_manager_call(self, *args, **kwargs):\n        self._assert_manager_call(self.proxy_manager_fun, *args, **kwargs)\n\n    def test_forwards_max_pool_size(self):\n        URLLib3Session(max_pool_connections=22)\n        self.assert_pool_manager_call(maxsize=22)\n\n    def test_forwards_client_cert(self):\n        URLLib3Session(client_cert='/some/cert')\n        self.assert_pool_manager_call(cert_file='/some/cert', key_file=None)\n\n    def test_forwards_client_cert_and_key_tuple(self):\n        cert = ('/some/cert', '/some/key')\n        URLLib3Session(client_cert=cert)\n        self.assert_pool_manager_call(cert_file=cert[0], key_file=cert[1])\n\n    def test_proxies_config_settings(self):\n        proxies = {'http': 'http://proxy.com'}\n        proxies_config = {\n            'proxy_ca_bundle': 'path/to/bundle',\n            'proxy_client_cert': ('path/to/cert', 'path/to/key'),\n            'proxy_use_forwarding_for_https': False,\n        }\n        use_forwarding = proxies_config['proxy_use_forwarding_for_https']\n        with mock.patch('botocore.httpsession.create_urllib3_context'):\n            session = URLLib3Session(\n                proxies=proxies, proxies_config=proxies_config\n            )\n            self.request.url = 'http://example.com/'\n            session.send(self.request.prepare())\n            self.assert_proxy_manager_call(\n                proxies['http'],\n                proxy_headers={},\n                proxy_ssl_context=mock.ANY,\n                use_forwarding_for_https=use_forwarding,\n            )\n        self.assert_request_sent(url=self.request.url)\n\n    def test_proxies_config_settings_unknown_config(self):\n        proxies = {'http': 'http://proxy.com'}\n        proxies_config = {\n            'proxy_ca_bundle': None,\n            'proxy_client_cert': None,\n            'proxy_use_forwarding_for_https': True,\n            'proxy_not_a_real_arg': 'do not pass',\n        }\n        use_forwarding = proxies_config['proxy_use_forwarding_for_https']\n        session = URLLib3Session(\n            proxies=proxies, proxies_config=proxies_config\n        )\n        self.request.url = 'http://example.com/'\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(\n            proxies['http'],\n            proxy_headers={},\n            use_forwarding_for_https=use_forwarding,\n        )\n        self.assertNotIn(\n            'proxy_not_a_real_arg', self.proxy_manager_fun.call_args\n        )\n        self.assert_request_sent(url=self.request.url)\n\n    def test_http_proxy_scheme_with_http_url(self):\n        proxies = {'http': 'http://proxy.com'}\n        session = URLLib3Session(proxies=proxies)\n        self.request.url = 'http://example.com/'\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(\n            proxies['http'],\n            proxy_headers={},\n        )\n        self.assert_request_sent(url=self.request.url)\n\n    def test_http_proxy_scheme_with_https_url(self):\n        proxies = {'https': 'http://proxy.com'}\n        session = URLLib3Session(proxies=proxies)\n        self.request.url = 'https://example.com/'\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(\n            proxies['https'],\n            proxy_headers={},\n        )\n        self.assert_request_sent()\n\n    def test_https_proxy_scheme_with_http_url(self):\n        proxies = {'http': 'https://proxy.com'}\n        session = URLLib3Session(proxies=proxies)\n        self.request.url = 'http://example.com/'\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(\n            proxies['http'],\n            proxy_headers={},\n        )\n        self.assert_request_sent(url=self.request.url)\n\n    def test_https_proxy_scheme_tls_in_tls(self):\n        proxies = {'https': 'https://proxy.com'}\n        session = URLLib3Session(proxies=proxies)\n        self.request.url = 'https://example.com/'\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(\n            proxies['https'],\n            proxy_headers={},\n        )\n        self.assert_request_sent()\n\n    def test_https_proxy_scheme_forwarding_https_url(self):\n        proxies = {'https': 'https://proxy.com'}\n        proxies_config = {\"proxy_use_forwarding_for_https\": True}\n        session = URLLib3Session(\n            proxies=proxies, proxies_config=proxies_config\n        )\n        self.request.url = 'https://example.com/'\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(\n            proxies['https'],\n            proxy_headers={},\n            use_forwarding_for_https=True,\n        )\n        self.assert_request_sent(url=self.request.url)\n\n    def test_basic_https_proxy_with_client_cert(self):\n        proxies = {'https': 'http://proxy.com'}\n        session = URLLib3Session(proxies=proxies, client_cert='/some/cert')\n        self.request.url = 'https://example.com/'\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(\n            proxies['https'],\n            proxy_headers={},\n            cert_file='/some/cert',\n            key_file=None,\n        )\n        self.assert_request_sent()\n\n    def test_basic_https_proxy_with_client_cert_and_key(self):\n        cert = ('/some/cert', '/some/key')\n        proxies = {'https': 'http://proxy.com'}\n        session = URLLib3Session(proxies=proxies, client_cert=cert)\n        self.request.url = 'https://example.com/'\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(\n            proxies['https'],\n            proxy_headers={},\n            cert_file=cert[0],\n            key_file=cert[1],\n        )\n        self.assert_request_sent()\n\n    def test_urllib3_proxies_kwargs_included(self):\n        cert = ('/some/cert', '/some/key')\n        proxies = {'https': 'https://proxy.com'}\n        proxies_config = {'proxy_client_cert': \"path/to/cert\"}\n        with mock.patch('botocore.httpsession.create_urllib3_context'):\n            session = URLLib3Session(\n                proxies=proxies,\n                client_cert=cert,\n                proxies_config=proxies_config,\n            )\n            self.request.url = 'https://example.com/'\n            session.send(self.request.prepare())\n            self.assert_proxy_manager_call(\n                proxies['https'],\n                proxy_headers={},\n                cert_file=cert[0],\n                key_file=cert[1],\n                proxy_ssl_context=mock.ANY,\n            )\n            self.assert_request_sent()\n\n    def test_proxy_ssl_context_uses_check_hostname(self):\n        cert = ('/some/cert', '/some/key')\n        proxies = {'https': 'https://proxy.com'}\n        proxies_config = {'proxy_client_cert': \"path/to/cert\"}\n        with mock.patch('botocore.httpsession.create_urllib3_context'):\n            session = URLLib3Session(\n                proxies=proxies,\n                client_cert=cert,\n                proxies_config=proxies_config,\n            )\n            self.request.url = 'https://example.com/'\n            session.send(self.request.prepare())\n            last_call = self.proxy_manager_fun.call_args[-1]\n            self.assertIs(last_call['ssl_context'].check_hostname, True)\n\n    def test_proxy_ssl_context_does_not_use_check_hostname_if_ip_address(self):\n        cert = ('/some/cert', '/some/key')\n        proxies_config = {'proxy_client_cert': \"path/to/cert\"}\n        urls = [\n            'https://1.2.3.4:5678',\n            'https://4.6.0.0',\n            'https://[FE80::8939:7684:D84b:a5A4%251]:1234',\n            'https://[FE80::8939:7684:D84b:a5A4%251]',\n            'https://[FE80::8939:7684:D84b:a5A4]:999',\n            'https://[FE80::8939:7684:D84b:a5A4]',\n            'https://[::1]:789',\n        ]\n        for proxy_url in urls:\n            with mock.patch('botocore.httpsession.SSLContext'):\n                proxies = {'https': proxy_url}\n                session = URLLib3Session(\n                    proxies=proxies,\n                    client_cert=cert,\n                    proxies_config=proxies_config,\n                )\n                self.request.url = 'https://example.com/'\n                session.send(self.request.prepare())\n                last_call = self.proxy_manager_fun.call_args[-1]\n                self.assertIs(last_call['ssl_context'].check_hostname, False)\n\n    def test_basic_request(self):\n        session = URLLib3Session()\n        session.send(self.request.prepare())\n        self.assert_request_sent()\n        self.response.stream.assert_called_once_with()\n\n    def test_basic_streaming_request(self):\n        session = URLLib3Session()\n        self.request.stream_output = True\n        session.send(self.request.prepare())\n        self.assert_request_sent()\n        self.response.stream.assert_not_called()\n\n    def test_basic_https_request(self):\n        session = URLLib3Session()\n        self.request.url = 'https://example.com/'\n        session.send(self.request.prepare())\n        self.assert_request_sent()\n\n    def test_basic_https_proxy_request(self):\n        proxies = {'https': 'http://proxy.com'}\n        session = URLLib3Session(proxies=proxies)\n        self.request.url = 'https://example.com/'\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(proxies['https'], proxy_headers={})\n        self.assert_request_sent()\n\n    def test_basic_proxy_request_caches_manager(self):\n        proxies = {'https': 'http://proxy.com'}\n        session = URLLib3Session(proxies=proxies)\n        self.request.url = 'https://example.com/'\n        session.send(self.request.prepare())\n        # assert we created the proxy manager\n        self.assert_proxy_manager_call(proxies['https'], proxy_headers={})\n        session.send(self.request.prepare())\n        # assert that we did not create another proxy manager\n        self.assertEqual(self.proxy_manager_fun.call_count, 1)\n\n    def test_basic_http_proxy_request(self):\n        proxies = {'http': 'http://proxy.com'}\n        session = URLLib3Session(proxies=proxies)\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(proxies['http'], proxy_headers={})\n        self.assert_request_sent(url=self.request.url)\n\n    def test_ssl_context_is_explicit(self):\n        session = URLLib3Session()\n        session.send(self.request.prepare())\n        _, manager_kwargs = self.pool_manager_cls.call_args\n        self.assertIsNotNone(manager_kwargs.get('ssl_context'))\n\n    def test_proxy_request_ssl_context_is_explicit(self):\n        proxies = {'http': 'http://proxy.com'}\n        session = URLLib3Session(proxies=proxies)\n        session.send(self.request.prepare())\n        _, proxy_kwargs = self.proxy_manager_fun.call_args\n        self.assertIsNotNone(proxy_kwargs.get('ssl_context'))\n\n    def test_session_forwards_socket_options_to_pool_manager(self):\n        socket_options = [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)]\n        URLLib3Session(socket_options=socket_options)\n        self.assert_pool_manager_call(socket_options=socket_options)\n\n    def test_session_forwards_socket_options_to_proxy_manager(self):\n        proxies = {'http': 'http://proxy.com'}\n        socket_options = [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)]\n        session = URLLib3Session(\n            proxies=proxies,\n            socket_options=socket_options,\n        )\n        session.send(self.request.prepare())\n        self.assert_proxy_manager_call(\n            proxies['http'],\n            proxy_headers={},\n            socket_options=socket_options,\n        )\n\n    def make_request_with_error(self, error):\n        self.connection.urlopen.side_effect = error\n        session = URLLib3Session()\n        session.send(self.request.prepare())\n\n    def test_catches_new_connection_error(self):\n        error = NewConnectionError(None, None)\n        with pytest.raises(EndpointConnectionError):\n            self.make_request_with_error(error)\n\n    def test_catches_bad_status_line(self):\n        error = ProtocolError(None)\n        with pytest.raises(ConnectionClosedError):\n            self.make_request_with_error(error)\n\n    def test_catches_proxy_error(self):\n        self.connection.urlopen.side_effect = ProxyError('test', None)\n        session = URLLib3Session(\n            proxies={'http': 'http://user:pass@proxy.com'}\n        )\n        with pytest.raises(ProxyConnectionError) as e:\n            session.send(self.request.prepare())\n        assert 'user:pass' not in str(e.value)\n        assert 'http://***:***@proxy.com' in str(e.value)\n\n    def test_aws_connection_classes_are_used(self):\n        session = URLLib3Session()  # noqa\n        # ensure the pool manager is using the correct classes\n        http_class = self.pool_manager.pool_classes_by_scheme.get('http')\n        self.assertIs(http_class, AWSHTTPConnectionPool)\n        https_class = self.pool_manager.pool_classes_by_scheme.get('https')\n        self.assertIs(https_class, AWSHTTPSConnectionPool)\n\n    def test_chunked_encoding_is_set_with_header(self):\n        session = URLLib3Session()\n        self.request.headers['Transfer-Encoding'] = 'chunked'\n\n        session.send(self.request.prepare())\n        self.assert_request_sent(\n            chunked=True,\n            headers={'Transfer-Encoding': 'chunked'},\n        )\n\n    def test_chunked_encoding_is_not_set_without_header(self):\n        session = URLLib3Session()\n\n        session.send(self.request.prepare())\n        self.assert_request_sent(chunked=False)\n\n    def test_close(self):\n        session = URLLib3Session()\n        session.close()\n        self.pool_manager.clear.assert_called_once_with()\n\n    def test_close_proxied(self):\n        proxies = {'https': 'http://proxy.com', 'http': 'http://proxy2.com'}\n        session = URLLib3Session(proxies=proxies)\n        for proxy, proxy_url in proxies.items():\n            self.request.url = '%s://example.com/' % proxy\n            session.send(self.request.prepare())\n\n        session.close()\n        self.proxy_manager_fun.return_value.clear.assert_called_with()\n        # One call for pool manager, one call for each of the proxies\n        self.assertEqual(\n            self.proxy_manager_fun.return_value.clear.call_count,\n            1 + len(proxies),\n        )\n", "tests/unit/test_monitoring.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport re\nimport socket\nimport time\n\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.exceptions import ConnectionError\nfrom botocore.hooks import HierarchicalEmitter\nfrom botocore.model import OperationModel, ServiceModel\nfrom botocore.monitoring import (\n    APICallAttemptEvent,\n    APICallEvent,\n    BaseMonitorEvent,\n    CSMSerializer,\n    Monitor,\n    MonitorEventAdapter,\n    SocketPublisher,\n)\nfrom tests import mock, unittest\n\n\nclass PublishingException(Exception):\n    pass\n\n\nclass TestMonitor(unittest.TestCase):\n    def setUp(self):\n        self.adapter = mock.Mock(MonitorEventAdapter)\n        self.publisher = mock.Mock(SocketPublisher)\n        self.handler = Monitor(self.adapter, self.publisher)\n\n    def test_register(self):\n        event_emitter = mock.Mock(HierarchicalEmitter)\n        self.handler.register(event_emitter)\n        self.assertEqual(\n            event_emitter.register_last.call_args_list,\n            [\n                mock.call('before-parameter-build', self.handler.capture),\n                mock.call('request-created', self.handler.capture),\n                mock.call('response-received', self.handler.capture),\n                mock.call('after-call', self.handler.capture),\n                mock.call('after-call-error', self.handler.capture),\n            ],\n        )\n\n    def test_handle(self):\n        event = object()\n        self.adapter.feed.return_value = event\n        self.handler.capture('event-name', event_parameter='event-value')\n        self.adapter.feed.assert_called_with(\n            'event-name', {'event_parameter': 'event-value'}\n        )\n        self.publisher.publish.assert_called_with(event)\n\n    def test_handle_no_publish(self):\n        self.adapter.feed.return_value = None\n        self.handler.capture('event-name', event_parameter='event-value')\n        self.publisher.publish.assert_not_called()\n\n    def test_handle_catches_exceptions(self):\n        self.publisher.publish.side_effect = PublishingException()\n        try:\n            self.handler.capture('event-name', event_parameter='event-value')\n        except PublishingException:\n            self.fail(\n                'The publishing exception should have been caught '\n                'in the handler'\n            )\n\n\nclass TestMonitorEventAdapter(unittest.TestCase):\n    def setUp(self):\n        self.mock_time = mock.Mock(time.time)\n        self.mock_time.return_value = 0\n        self.adapter = MonitorEventAdapter(self.mock_time)\n\n        self.context = {}\n        self.wire_name = 'MyOperation'\n        self.operation_model = mock.Mock(OperationModel)\n        self.operation_model.wire_name = self.wire_name\n        self.service_id = 'MyService'\n        self.service_model = mock.Mock(ServiceModel)\n        self.service_model.service_id = self.service_id\n        self.operation_model.service_model = self.service_model\n\n        self.url = 'https://us-east-1.myservice.amazonaws.com'\n        self.request_headers = {}\n        self.request = mock.Mock(AWSRequest)\n        self.request.url = self.url\n        self.request.headers = self.request_headers\n        self.request.context = self.context\n\n        self.http_status_code = 200\n        self.response_headers = {}\n\n    def feed_before_parameter_build_event(self, current_time=0):\n        self.mock_time.return_value = current_time\n        self.adapter.feed(\n            'before-parameter-build',\n            {'model': self.operation_model, 'context': self.context},\n        )\n\n    def feed_request_created_event(self, current_time=0):\n        self.mock_time.return_value = current_time\n        self.adapter.feed(\n            'request-created',\n            {\n                'request': self.request,\n            },\n        )\n\n    def test_feed_before_parameter_build_returns_no_event(self):\n        self.assertIsNone(\n            self.adapter.feed(\n                'before-parameter-build',\n                {'model': self.operation_model, 'context': self.context},\n            )\n        )\n\n    def test_feed_request_created_returns_no_event(self):\n        self.adapter.feed(\n            'before-parameter-build',\n            {'model': self.operation_model, 'context': self.context},\n        )\n        self.assertIsNone(\n            self.adapter.feed(\n                'request-created',\n                {\n                    'request': self.request,\n                },\n            )\n        )\n\n    def test_feed_with_successful_response(self):\n        self.feed_before_parameter_build_event(current_time=1)\n        self.feed_request_created_event(current_time=2)\n\n        self.mock_time.return_value = 3\n        attempt_event = self.adapter.feed(\n            'response-received',\n            {\n                'parsed_response': {\n                    'ResponseMetadata': {\n                        'HTTPStatusCode': self.http_status_code,\n                        'HTTPHeaders': self.response_headers,\n                    }\n                },\n                'context': self.context,\n                'exception': None,\n            },\n        )\n        self.assertEqual(\n            attempt_event,\n            APICallAttemptEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=2000,\n                latency=1000,\n                url=self.url,\n                request_headers=self.request_headers,\n                http_status_code=self.http_status_code,\n                response_headers=self.response_headers,\n            ),\n        )\n\n        self.mock_time.return_value = 4\n        call_event = self.adapter.feed(\n            'after-call',\n            {\n                'parsed': {\n                    'ResponseMetadata': {\n                        'HTTPStatusCode': self.http_status_code,\n                        'HTTPHeaders': self.response_headers,\n                    }\n                },\n                'context': self.context,\n            },\n        )\n        self.assertEqual(\n            call_event,\n            APICallEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=1000,\n                latency=3000,\n                attempts=[attempt_event],\n            ),\n        )\n\n    def test_feed_with_retries(self):\n        self.feed_before_parameter_build_event(current_time=1)\n        self.feed_request_created_event(current_time=2)\n\n        self.mock_time.return_value = 3\n        first_attempt_event = self.adapter.feed(\n            'response-received',\n            {\n                'parsed_response': {\n                    'ResponseMetadata': {\n                        'HTTPStatusCode': 500,\n                        'HTTPHeaders': self.response_headers,\n                    }\n                },\n                'context': self.context,\n                'exception': None,\n            },\n        )\n        self.assertEqual(\n            first_attempt_event,\n            APICallAttemptEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=2000,\n                latency=1000,\n                url=self.url,\n                request_headers=self.request_headers,\n                http_status_code=500,\n                response_headers=self.response_headers,\n            ),\n        )\n\n        self.feed_request_created_event(current_time=5)\n        self.mock_time.return_value = 6\n        second_attempt_event = self.adapter.feed(\n            'response-received',\n            {\n                'parsed_response': {\n                    'ResponseMetadata': {\n                        'HTTPStatusCode': 200,\n                        'HTTPHeaders': self.response_headers,\n                    }\n                },\n                'context': self.context,\n                'exception': None,\n            },\n        )\n        self.assertEqual(\n            second_attempt_event,\n            APICallAttemptEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=5000,\n                latency=1000,\n                url=self.url,\n                request_headers=self.request_headers,\n                http_status_code=200,\n                response_headers=self.response_headers,\n            ),\n        )\n\n        self.mock_time.return_value = 7\n        call_event = self.adapter.feed(\n            'after-call',\n            {\n                'parsed': {\n                    'ResponseMetadata': {\n                        'HTTPStatusCode': 200,\n                        'HTTPHeaders': self.response_headers,\n                    }\n                },\n                'context': self.context,\n            },\n        )\n        self.assertEqual(\n            call_event,\n            APICallEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=1000,\n                latency=6000,\n                attempts=[first_attempt_event, second_attempt_event],\n            ),\n        )\n\n    def test_feed_with_retries_exceeded(self):\n        self.feed_before_parameter_build_event(current_time=1)\n        self.feed_request_created_event(current_time=2)\n\n        self.mock_time.return_value = 3\n        first_attempt_event = self.adapter.feed(\n            'response-received',\n            {\n                'parsed_response': {\n                    'ResponseMetadata': {\n                        'HTTPStatusCode': 500,\n                        'HTTPHeaders': self.response_headers,\n                    }\n                },\n                'context': self.context,\n                'exception': None,\n            },\n        )\n        self.feed_request_created_event(current_time=5)\n        self.mock_time.return_value = 6\n        second_attempt_event = self.adapter.feed(\n            'response-received',\n            {\n                'parsed_response': {\n                    'ResponseMetadata': {\n                        'HTTPStatusCode': 200,\n                        'HTTPHeaders': self.response_headers,\n                        'MaxAttemptsReached': True,\n                    }\n                },\n                'context': self.context,\n                'exception': None,\n            },\n        )\n        self.mock_time.return_value = 7\n        call_event = self.adapter.feed(\n            'after-call',\n            {\n                'parsed': {\n                    'ResponseMetadata': {\n                        'HTTPStatusCode': 200,\n                        'HTTPHeaders': self.response_headers,\n                        'MaxAttemptsReached': True,\n                    }\n                },\n                'context': self.context,\n            },\n        )\n        self.assertEqual(\n            call_event,\n            APICallEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=1000,\n                latency=6000,\n                attempts=[first_attempt_event, second_attempt_event],\n                retries_exceeded=True,\n            ),\n        )\n\n    def test_feed_with_parsed_error(self):\n        self.feed_before_parameter_build_event(current_time=1)\n        self.feed_request_created_event(current_time=2)\n\n        self.mock_time.return_value = 3\n        parsed_error = {'Code': 'MyErrorCode', 'Message': 'MyMessage'}\n        parsed_response = {\n            'Error': parsed_error,\n            'ResponseMetadata': {\n                'HTTPStatusCode': 400,\n                'HTTPHeaders': self.response_headers,\n            },\n        }\n        attempt_event = self.adapter.feed(\n            'response-received',\n            {\n                'parsed_response': parsed_response,\n                'context': self.context,\n                'exception': None,\n            },\n        )\n        self.assertEqual(\n            attempt_event,\n            APICallAttemptEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=2000,\n                latency=1000,\n                url=self.url,\n                request_headers=self.request_headers,\n                http_status_code=400,\n                response_headers=self.response_headers,\n                parsed_error=parsed_error,\n            ),\n        )\n\n        self.mock_time.return_value = 4\n        call_event = self.adapter.feed(\n            'after-call', {'parsed': parsed_response, 'context': self.context}\n        )\n        self.assertEqual(\n            call_event,\n            APICallEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=1000,\n                latency=3000,\n                attempts=[attempt_event],\n            ),\n        )\n\n    def test_feed_with_wire_exception(self):\n        self.feed_before_parameter_build_event(current_time=1)\n        self.feed_request_created_event(current_time=2)\n\n        self.mock_time.return_value = 3\n        wire_exception = Exception('Some wire exception')\n        attempt_event = self.adapter.feed(\n            'response-received',\n            {\n                'parsed_response': None,\n                'context': self.context,\n                'exception': wire_exception,\n            },\n        )\n        self.assertEqual(\n            attempt_event,\n            APICallAttemptEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=2000,\n                latency=1000,\n                url=self.url,\n                request_headers=self.request_headers,\n                wire_exception=wire_exception,\n            ),\n        )\n\n        self.mock_time.return_value = 4\n        call_event = self.adapter.feed(\n            'after-call-error',\n            {'exception': wire_exception, 'context': self.context},\n        )\n        self.assertEqual(\n            call_event,\n            APICallEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=1000,\n                latency=3000,\n                attempts=[attempt_event],\n            ),\n        )\n\n    def test_feed_with_wire_exception_retries_exceeded(self):\n        self.feed_before_parameter_build_event(current_time=1)\n        self.feed_request_created_event(current_time=2)\n\n        self.mock_time.return_value = 3\n        # Connection errors are retryable\n        wire_exception = ConnectionError(error='connection issue')\n        attempt_event = self.adapter.feed(\n            'response-received',\n            {\n                'parsed_response': None,\n                'context': self.context,\n                'exception': wire_exception,\n            },\n        )\n        self.mock_time.return_value = 4\n        call_event = self.adapter.feed(\n            'after-call-error',\n            {'exception': wire_exception, 'context': self.context},\n        )\n        self.assertEqual(\n            call_event,\n            APICallEvent(\n                service=self.service_id,\n                operation=self.wire_name,\n                timestamp=1000,\n                latency=3000,\n                attempts=[attempt_event],\n                retries_exceeded=True,\n            ),\n        )\n\n\nclass TestBaseMonitorEvent(unittest.TestCase):\n    def test_init_self(self):\n        event = BaseMonitorEvent(\n            service='MyService', operation='MyOperation', timestamp=1000\n        )\n        self.assertEqual(event.service, 'MyService')\n        self.assertEqual(event.operation, 'MyOperation')\n        self.assertEqual(event.timestamp, 1000)\n\n    def test_eq(self):\n        self.assertEqual(\n            BaseMonitorEvent(\n                service='MyService', operation='MyOperation', timestamp=1000\n            ),\n            BaseMonitorEvent(\n                service='MyService', operation='MyOperation', timestamp=1000\n            ),\n        )\n\n    def test_not_eq_different_classes(self):\n        self.assertNotEqual(\n            BaseMonitorEvent(\n                service='MyService', operation='MyOperation', timestamp=1000\n            ),\n            object(),\n        )\n\n    def test_not_eq_different_attrs(self):\n        self.assertNotEqual(\n            BaseMonitorEvent(\n                service='MyService', operation='MyOperation', timestamp=1000\n            ),\n            BaseMonitorEvent(\n                service='DifferentService',\n                operation='DifferentOperation',\n                timestamp=0,\n            ),\n        )\n\n\nclass TestAPICallEvent(unittest.TestCase):\n    def test_init(self):\n        event = APICallEvent(\n            service='MyService',\n            operation='MyOperation',\n            timestamp=1000,\n            latency=2000,\n            attempts=[],\n        )\n        self.assertEqual(event.service, 'MyService')\n        self.assertEqual(event.operation, 'MyOperation')\n        self.assertEqual(event.timestamp, 1000)\n        self.assertEqual(event.latency, 2000)\n        self.assertEqual(event.attempts, [])\n\n    def test_new_api_call_attempt_event(self):\n        event = APICallEvent(\n            service='MyService',\n            operation='MyOperation',\n            timestamp=1000,\n            latency=2000,\n            attempts=[],\n        )\n        attempt_event = event.new_api_call_attempt(timestamp=2000)\n        self.assertEqual(\n            attempt_event,\n            APICallAttemptEvent(\n                service='MyService', operation='MyOperation', timestamp=2000\n            ),\n        )\n        self.assertEqual(event.attempts, [attempt_event])\n\n\nclass TestAPICallAttemptEvent(unittest.TestCase):\n    def test_init(self):\n        url = 'https://us-east-1.myservice.amazonaws.com'\n        parsed_error = {'Code': 'ErrorCode', 'Message': 'ErrorMessage'}\n        wire_exception = Exception('Some wire exception')\n        event = APICallAttemptEvent(\n            service='MyService',\n            operation='MyOperation',\n            timestamp=1000,\n            latency=2000,\n            url=url,\n            http_status_code=200,\n            request_headers={},\n            response_headers={},\n            parsed_error=parsed_error,\n            wire_exception=wire_exception,\n        )\n        self.assertEqual(event.service, 'MyService')\n        self.assertEqual(event.operation, 'MyOperation')\n        self.assertEqual(event.timestamp, 1000)\n        self.assertEqual(event.latency, 2000)\n        self.assertEqual(event.url, url)\n        self.assertEqual(event.http_status_code, 200)\n        self.assertEqual(event.request_headers, {})\n        self.assertEqual(event.response_headers, {})\n        self.assertEqual(event.parsed_error, parsed_error)\n        self.assertEqual(event.wire_exception, wire_exception)\n\n\nclass TestCSMSerializer(unittest.TestCase):\n    def setUp(self):\n        self.csm_client_id = 'MyId'\n        self.serializer = CSMSerializer(self.csm_client_id)\n        self.service = 'MyService'\n        self.operation = 'MyOperation'\n        self.user_agent = 'my-user-agent'\n        self.fqdn = 'us-east-1.myservice.amazonaws.com'\n        self.url = 'https://' + self.fqdn\n        self.timestamp = 1000\n        self.latency = 2000\n        self.request_headers = {'User-Agent': self.user_agent}\n\n    def get_serialized_event_dict(self, event):\n        serialized_event = self.serializer.serialize(event)\n        return json.loads(serialized_event.decode('utf-8'))\n\n    def test_validates_csm_client_id(self):\n        max_client_id_len = 255\n        with self.assertRaises(ValueError):\n            CSMSerializer('a' * (max_client_id_len + 1))\n\n    def test_serialize_produces_bytes(self):\n        event = APICallEvent(\n            service=self.service, operation=self.operation, timestamp=1000\n        )\n        serialized_event = self.serializer.serialize(event)\n        self.assertIsInstance(serialized_event, bytes)\n\n    def test_serialize_does_not_add_whitespace(self):\n        event = APICallEvent(\n            service=self.service, operation=self.operation, timestamp=1000\n        )\n        serialized_event = self.serializer.serialize(event)\n        self.assertIsNone(re.match(r'\\s', serialized_event.decode('utf-8')))\n\n    def test_serialize_api_call_event(self):\n        event = APICallEvent(\n            service=self.service, operation=self.operation, timestamp=1000\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(\n            serialized_event_dict,\n            {\n                'Version': 1,\n                'Type': 'ApiCall',\n                'Service': self.service,\n                'Api': self.operation,\n                'ClientId': self.csm_client_id,\n                'MaxRetriesExceeded': 0,\n                'Timestamp': 1000,\n                'AttemptCount': 0,\n            },\n        )\n\n    def test_serialize_api_call_event_with_latency(self):\n        event = APICallEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=1000,\n            latency=2000,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['Latency'], self.latency)\n\n    def test_serialize_api_call_event_with_attempts(self):\n        event = APICallEvent(\n            service=self.service, operation=self.operation, timestamp=1000\n        )\n        event.new_api_call_attempt(2000)\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['AttemptCount'], 1)\n\n    def test_serialize_api_call_event_region(self):\n        event = APICallEvent(\n            service=self.service, operation=self.operation, timestamp=1000\n        )\n        attempt = event.new_api_call_attempt(2000)\n        auth_value = (\n            'AWS4-HMAC-SHA256 '\n            'Credential=myaccesskey/20180523/my-region-1/ec2/aws4_request,'\n            'SignedHeaders=content-type;host;x-amz-date, '\n            'Signature=somesignature'\n        )\n        self.request_headers['Authorization'] = auth_value\n        attempt.request_headers = self.request_headers\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['Region'], 'my-region-1')\n\n    def test_serialize_api_call_event_user_agent(self):\n        event = APICallEvent(\n            service=self.service, operation=self.operation, timestamp=1000\n        )\n        attempt = event.new_api_call_attempt(2000)\n        attempt.request_headers = {'User-Agent': self.user_agent}\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['UserAgent'], self.user_agent)\n\n    def test_serialize_api_call_event_http_status_code(self):\n        event = APICallEvent(\n            service=self.service, operation=self.operation, timestamp=1000\n        )\n        attempt = event.new_api_call_attempt(2000)\n        attempt.http_status_code = 200\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['FinalHttpStatusCode'], 200)\n\n    def test_serialize_api_call_event_parsed_error(self):\n        event = APICallEvent(\n            service=self.service, operation=self.operation, timestamp=1000\n        )\n        attempt = event.new_api_call_attempt(2000)\n        attempt.parsed_error = {\n            'Code': 'MyErrorCode',\n            'Message': 'My error message',\n        }\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(\n            serialized_event_dict['FinalAwsException'], 'MyErrorCode'\n        )\n        self.assertEqual(\n            serialized_event_dict['FinalAwsExceptionMessage'],\n            'My error message',\n        )\n\n    def test_serialize_api_call_event_wire_exception(self):\n        event = APICallEvent(\n            service=self.service, operation=self.operation, timestamp=1000\n        )\n        attempt = event.new_api_call_attempt(2000)\n        attempt.wire_exception = Exception('Error on the wire')\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(\n            serialized_event_dict['FinalSdkException'], 'Exception'\n        )\n        self.assertEqual(\n            serialized_event_dict['FinalSdkExceptionMessage'],\n            'Error on the wire',\n        )\n\n    def test_serialize_api_call_event_with_retries_exceeded(self):\n        event = APICallEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=1000,\n            retries_exceeded=True,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['MaxRetriesExceeded'], 1)\n\n    def test_serialize_api_call_attempt_event(self):\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(\n            serialized_event_dict,\n            {\n                'Version': 1,\n                'Type': 'ApiCallAttempt',\n                'Service': self.service,\n                'Api': self.operation,\n                'ClientId': self.csm_client_id,\n                'Timestamp': self.timestamp,\n            },\n        )\n\n    def test_serialize_api_call_attempt_event_with_latency(self):\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            latency=self.latency,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['AttemptLatency'], self.latency)\n\n    def test_serialize_with_user_agent(self):\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            request_headers={'User-Agent': self.user_agent},\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['UserAgent'], self.user_agent)\n\n    def test_serialize_with_url(self):\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            url=self.url,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['Fqdn'], self.fqdn)\n\n    def test_serialize_with_s3_signing(self):\n        auth_value = 'AWS myaccesskey:somesignature'\n        self.request_headers['Authorization'] = auth_value\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            request_headers=self.request_headers,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['AccessKey'], 'myaccesskey')\n\n    def test_serialize_with_sigv4_sigining(self):\n        auth_value = (\n            'AWS4-HMAC-SHA256 '\n            'Credential=myaccesskey/20180523/my-region-1/ec2/aws4_request,'\n            'SignedHeaders=content-type;host;x-amz-date, '\n            'Signature=somesignature'\n        )\n        self.request_headers['Authorization'] = auth_value\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            request_headers=self.request_headers,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['AccessKey'], 'myaccesskey')\n\n    def test_serialize_with_session_token(self):\n        self.request_headers['X-Amz-Security-Token'] = 'my-security-token'\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            request_headers=self.request_headers,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(\n            serialized_event_dict['SessionToken'], 'my-security-token'\n        )\n\n    def test_serialize_with_path_parameters_in_url(self):\n        self.url = 'https://' + self.fqdn + '/resource'\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            url=self.url,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['Fqdn'], self.fqdn)\n\n    def test_serialize_with_request_id_headers(self):\n        response_headers = {\n            'x-amzn-requestid': 'id1',\n            'x-amz-request-id': 'id2',\n            'x-amz-id-2': 'id3',\n        }\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            response_headers=response_headers,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['XAmznRequestId'], 'id1')\n        self.assertEqual(serialized_event_dict['XAmzRequestId'], 'id2')\n        self.assertEqual(serialized_event_dict['XAmzId2'], 'id3')\n\n    def test_serialize_filters_unwanted_response_headers(self):\n        response_headers = {'filter-out': 'do-not-include-this'}\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            response_headers=response_headers,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(\n            serialized_event_dict,\n            {\n                'Version': 1,\n                'Type': 'ApiCallAttempt',\n                'Service': self.service,\n                'Api': self.operation,\n                'ClientId': self.csm_client_id,\n                'Timestamp': self.timestamp,\n            },\n        )\n\n    def test_serialize_with_status_code(self):\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            http_status_code=200,\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['HttpStatusCode'], 200)\n\n    def test_serialize_with_service_error(self):\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            parsed_error={\n                'Code': 'MyErrorCode',\n                'Message': 'My error message',\n            },\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['AwsException'], 'MyErrorCode')\n        self.assertEqual(\n            serialized_event_dict['AwsExceptionMessage'], 'My error message'\n        )\n\n    def test_serialize_with_wire_exception(self):\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            wire_exception=Exception('Error on the wire'),\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(serialized_event_dict['SdkException'], 'Exception')\n        self.assertEqual(\n            serialized_event_dict['SdkExceptionMessage'], 'Error on the wire'\n        )\n\n    def test_serialize_truncates_long_user_agent(self):\n        max_user_agent_length = 256\n        user_agent = 'a' * (max_user_agent_length + 1)\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            request_headers={'User-Agent': user_agent},\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(\n            serialized_event_dict['UserAgent'],\n            user_agent[:max_user_agent_length],\n        )\n\n    def test_serialize_truncates_long_service_error(self):\n        max_error_code_length = 128\n        max_error_message_length = 512\n        long_error_code = 'c' * (max_error_code_length + 1)\n        long_error_message = 'm' * (max_error_message_length + 1)\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            parsed_error={\n                'Code': long_error_code,\n                'Message': long_error_message,\n            },\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n        self.assertEqual(\n            serialized_event_dict['AwsException'],\n            long_error_code[:max_error_code_length],\n        )\n        self.assertEqual(\n            serialized_event_dict['AwsExceptionMessage'],\n            long_error_message[:max_error_message_length],\n        )\n\n    def test_serialize_truncates_long_wire_exception(self):\n        max_class_name_length = 128\n        max_error_message_length = 512\n        long_class_name = 'W' * (max_class_name_length + 1)\n        wire_class = type(long_class_name, (Exception,), {})\n        long_error_message = 'm' * (max_error_message_length + 1)\n        event = APICallAttemptEvent(\n            service=self.service,\n            operation=self.operation,\n            timestamp=self.timestamp,\n            wire_exception=wire_class(long_error_message),\n        )\n        serialized_event_dict = self.get_serialized_event_dict(event)\n\n        self.assertEqual(\n            serialized_event_dict['SdkException'],\n            long_class_name[:max_class_name_length],\n        )\n        self.assertEqual(\n            serialized_event_dict['SdkExceptionMessage'],\n            long_error_message[:max_error_message_length],\n        )\n\n\nclass TestSocketPublisher(unittest.TestCase):\n    def setUp(self):\n        self.socket = mock.Mock(socket.socket)\n        self.host = '127.0.0.1'\n        self.port = 31000\n        self.serializer = mock.Mock(CSMSerializer)\n        self.publisher = SocketPublisher(\n            self.socket, self.host, self.port, self.serializer\n        )\n\n    def test_publish(self):\n        event = object()\n        self.serializer.serialize.return_value = b'serialized event'\n        self.publisher.publish(event)\n        self.serializer.serialize.assert_called_with(event)\n        self.socket.sendto.assert_called_with(\n            b'serialized event', (self.host, self.port)\n        )\n\n    def test_skips_publishing_over_max_size(self):\n        event = mock.Mock(APICallAttemptEvent)\n        max_event_size = 8 * 1024\n        self.serializer.serialize.return_value = b'a' * (max_event_size + 1)\n        self.publisher.publish(event)\n        self.socket.sendto.assert_not_called()\n", "tests/unit/test_parsers.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\nimport itertools\n\nimport pytest\nfrom dateutil.tz import tzutc\n\nfrom botocore import model, parsers\nfrom botocore.compat import MutableMapping, json\nfrom tests import RawResponse, unittest\n\n\n# HTTP responses will typically return a custom HTTP\n# dict.  We want to ensure we're able to work with any\n# kind of mutable mapping implementation.\nclass CustomHeaderDict(MutableMapping):\n    def __init__(self, original_dict):\n        self._d = original_dict\n\n    def __getitem__(self, item):\n        return self._d[item]\n\n    def __setitem__(self, item, value):\n        self._d[item] = value\n\n    def __delitem__(self, item):\n        del self._d[item]\n\n    def __iter__(self):\n        return iter(self._d)\n\n    def __len__(self):\n        return len(self._d)\n\n\n# These tests contain botocore specific tests that either\n# don't make sense in the protocol tests or haven't been added\n# yet.\nclass TestResponseMetadataParsed(unittest.TestCase):\n    def test_response_metadata_parsed_for_query_service(self):\n        parser = parsers.QueryParser()\n        response = (\n            b'<OperationNameResponse>'\n            b'  <OperationNameResult><Str>myname</Str></OperationNameResult>'\n            b'  <ResponseMetadata>'\n            b'    <RequestId>request-id</RequestId>'\n            b'  </ResponseMetadata>'\n            b'</OperationNameResponse>'\n        )\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'resultWrapper': 'OperationNameResult',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    },\n                    'Num': {\n                        'shape': 'IntegerType',\n                    },\n                },\n            },\n            model.ShapeResolver(\n                {\n                    'StringType': {\n                        'type': 'string',\n                    },\n                    'IntegerType': {\n                        'type': 'integer',\n                    },\n                }\n            ),\n        )\n        parsed = parser.parse(\n            {'body': response, 'headers': {}, 'status_code': 200}, output_shape\n        )\n        self.assertEqual(\n            parsed,\n            {\n                'Str': 'myname',\n                'ResponseMetadata': {\n                    'RequestId': 'request-id',\n                    'HTTPStatusCode': 200,\n                    'HTTPHeaders': {},\n                },\n            },\n        )\n\n    def test_metadata_always_exists_for_query(self):\n        # ResponseMetadata is used for more than just the request id. It\n        # should always get populated, even if the request doesn't seem to\n        # have an id.\n        parser = parsers.QueryParser()\n        response = (\n            b'<OperationNameResponse>'\n            b'  <OperationNameResult><Str>myname</Str></OperationNameResult>'\n            b'</OperationNameResponse>'\n        )\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'resultWrapper': 'OperationNameResult',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    },\n                    'Num': {\n                        'shape': 'IntegerType',\n                    },\n                },\n            },\n            model.ShapeResolver(\n                {\n                    'StringType': {\n                        'type': 'string',\n                    },\n                    'IntegerType': {\n                        'type': 'integer',\n                    },\n                }\n            ),\n        )\n        parsed = parser.parse(\n            {'body': response, 'headers': {}, 'status_code': 200}, output_shape\n        )\n        expected = {\n            'Str': 'myname',\n            'ResponseMetadata': {'HTTPStatusCode': 200, 'HTTPHeaders': {}},\n        }\n        self.assertEqual(parsed, expected)\n\n    def test_response_metadata_parsed_for_ec2(self):\n        parser = parsers.EC2QueryParser()\n        response = (\n            b'<OperationNameResponse>'\n            b'  <Str>myname</Str>'\n            b'  <requestId>request-id</requestId>'\n            b'</OperationNameResponse>'\n        )\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    }\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n        parsed = parser.parse(\n            {'headers': {}, 'body': response, 'status_code': 200}, output_shape\n        )\n        # Note that the response metadata is normalized to match the query\n        # protocol, even though this is not how it appears in the output.\n        self.assertEqual(\n            parsed,\n            {\n                'Str': 'myname',\n                'ResponseMetadata': {\n                    'RequestId': 'request-id',\n                    'HTTPStatusCode': 200,\n                    'HTTPHeaders': {},\n                },\n            },\n        )\n\n    def test_metadata_always_exists_for_ec2(self):\n        # ResponseMetadata is used for more than just the request id. It\n        # should always get populated, even if the request doesn't seem to\n        # have an id.\n        parser = parsers.EC2QueryParser()\n        response = (\n            b'<OperationNameResponse>'\n            b'  <Str>myname</Str>'\n            b'</OperationNameResponse>'\n        )\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    }\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n        parsed = parser.parse(\n            {'headers': {}, 'body': response, 'status_code': 200}, output_shape\n        )\n        expected = {\n            'Str': 'myname',\n            'ResponseMetadata': {'HTTPStatusCode': 200, 'HTTPHeaders': {}},\n        }\n        self.assertEqual(parsed, expected)\n\n    def test_response_metadata_on_json_request(self):\n        parser = parsers.JSONParser()\n        response = b'{\"Str\": \"mystring\"}'\n        headers = {'x-amzn-requestid': 'request-id'}\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    }\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n        parsed = parser.parse(\n            {'body': response, 'headers': headers, 'status_code': 200},\n            output_shape,\n        )\n        # Note that the response metadata is normalized to match the query\n        # protocol, even though this is not how it appears in the output.\n        self.assertEqual(\n            parsed,\n            {\n                'Str': 'mystring',\n                'ResponseMetadata': {\n                    'RequestId': 'request-id',\n                    'HTTPStatusCode': 200,\n                    'HTTPHeaders': headers,\n                },\n            },\n        )\n\n    def test_response_no_initial_event_stream(self):\n        parser = parsers.JSONParser()\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'members': {'Payload': {'shape': 'Payload'}},\n            },\n            model.ShapeResolver(\n                {\n                    'Payload': {\n                        'type': 'structure',\n                        'members': [],\n                        'eventstream': True,\n                    }\n                }\n            ),\n        )\n        with self.assertRaises(parsers.ResponseParserError):\n            response_dict = {\n                'status_code': 200,\n                'headers': {},\n                'body': RawResponse(b''),\n                'context': {'operation_name': 'TestOperation'},\n            }\n            parser.parse(response_dict, output_shape)\n\n    def test_metadata_always_exists_for_json(self):\n        # ResponseMetadata is used for more than just the request id. It\n        # should always get populated, even if the request doesn't seem to\n        # have an id.\n        parser = parsers.JSONParser()\n        response = b'{\"Str\": \"mystring\"}'\n        headers = {}\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    }\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n        parsed = parser.parse(\n            {'body': response, 'headers': headers, 'status_code': 200},\n            output_shape,\n        )\n        expected = {\n            'Str': 'mystring',\n            'ResponseMetadata': {\n                'HTTPStatusCode': 200,\n                'HTTPHeaders': headers,\n            },\n        }\n        self.assertEqual(parsed, expected)\n\n    def test_response_metadata_on_rest_json_response(self):\n        parser = parsers.RestJSONParser()\n        response = b'{\"Str\": \"mystring\"}'\n        headers = {'x-amzn-requestid': 'request-id'}\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    }\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n        parsed = parser.parse(\n            {'body': response, 'headers': headers, 'status_code': 200},\n            output_shape,\n        )\n        # Note that the response metadata is normalized to match the query\n        # protocol, even though this is not how it appears in the output.\n        self.assertEqual(\n            parsed,\n            {\n                'Str': 'mystring',\n                'ResponseMetadata': {\n                    'RequestId': 'request-id',\n                    'HTTPStatusCode': 200,\n                    'HTTPHeaders': headers,\n                },\n            },\n        )\n\n    def test_metadata_always_exists_on_rest_json_response(self):\n        # ResponseMetadata is used for more than just the request id. It\n        # should always get populated, even if the request doesn't seem to\n        # have an id.\n        parser = parsers.RestJSONParser()\n        response = b'{\"Str\": \"mystring\"}'\n        headers = {}\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    }\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n        parsed = parser.parse(\n            {'body': response, 'headers': headers, 'status_code': 200},\n            output_shape,\n        )\n        expected = {\n            'Str': 'mystring',\n            'ResponseMetadata': {\n                'HTTPStatusCode': 200,\n                'HTTPHeaders': headers,\n            },\n        }\n        self.assertEqual(parsed, expected)\n\n    def test_response_metadata_from_s3_response(self):\n        # Even though s3 is a rest-xml service, it's response metadata\n        # is slightly different.  It has two request ids, both come from\n        # the response headers, are both are named differently from other\n        # rest-xml responses.\n        headers = {'x-amz-id-2': 'second-id', 'x-amz-request-id': 'request-id'}\n        parser = parsers.RestXMLParser()\n        parsed = parser.parse(\n            {'body': '', 'headers': headers, 'status_code': 200}, None\n        )\n        self.assertEqual(\n            parsed,\n            {\n                'ResponseMetadata': {\n                    'RequestId': 'request-id',\n                    'HostId': 'second-id',\n                    'HTTPStatusCode': 200,\n                    'HTTPHeaders': headers,\n                }\n            },\n        )\n\n    def test_metadata_always_exists_on_rest_xml_response(self):\n        # ResponseMetadata is used for more than just the request id. It\n        # should always get populated, even if the request doesn't seem to\n        # have an id.\n        headers = {}\n        parser = parsers.RestXMLParser()\n        parsed = parser.parse(\n            {'body': '', 'headers': headers, 'status_code': 200}, None\n        )\n        expected = {\n            'ResponseMetadata': {'HTTPStatusCode': 200, 'HTTPHeaders': headers}\n        }\n        self.assertEqual(parsed, expected)\n\n    def test_checksum_metadata_parsed_from_response_context(self):\n        headers = {}\n        response_dict = {\n            'status_code': 200,\n            'headers': headers,\n            'body': b'',\n            'context': {'checksum': {'response_algorithm': 'crc32'}},\n        }\n        parser = parsers.RestXMLParser()\n        parsed = parser.parse(response_dict, None)\n        expected_algorithm = 'crc32'\n        actual_algorithm = parsed['ResponseMetadata']['ChecksumAlgorithm']\n        self.assertEqual(actual_algorithm, expected_algorithm)\n\n\nclass TestTaggedUnions(unittest.TestCase):\n    def assert_tagged_union_response_with_unknown_member(\n        self,\n        parser,\n        response,\n        output_shape,\n        expected_parsed_response,\n        expected_log,\n    ):\n        with self.assertLogs() as captured_log:\n            parsed = parser.parse(response, output_shape)\n            self.assertEqual(parsed, expected_parsed_response)\n            self.assertEqual(len(captured_log.records), 1)\n            self.assertIn(\n                (\n                    'Received a tagged union response with member '\n                    'unknown to client'\n                ),\n                captured_log.records[0].getMessage(),\n            )\n\n    def test_base_json_parser_handles_unknown_member(self):\n        parser = parsers.JSONParser()\n        response = b'{\"Foo\": \"mystring\"}'\n        headers = {'x-amzn-requestid': 'request-id'}\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'union': True,\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    }\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n        response = {'body': response, 'headers': headers, 'status_code': 200}\n        # Parsed response omits data from service since it is not\n        # modeled in the client\n        expected_parsed_response = {\n            'SDK_UNKNOWN_MEMBER': {'name': 'Foo'},\n            'ResponseMetadata': {\n                'RequestId': 'request-id',\n                'HTTPStatusCode': 200,\n                'HTTPHeaders': {'x-amzn-requestid': 'request-id'},\n            },\n        }\n        expected_log = \"Received a response with an unknown member Foo set\"\n        self.assert_tagged_union_response_with_unknown_member(\n            parser,\n            response,\n            output_shape,\n            expected_parsed_response,\n            expected_log,\n        )\n\n    def test_base_xml_parser_handles_unknown_member(self):\n        parser = parsers.QueryParser()\n        response = (\n            b'<OperationNameResponse>'\n            b'  <OperationNameResult><Foo>mystring</Foo></OperationNameResult>'\n            b'  <ResponseMetadata>'\n            b'    <RequestId>request-id</RequestId>'\n            b'  </ResponseMetadata>'\n            b'</OperationNameResponse>'\n        )\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'union': True,\n                'resultWrapper': 'OperationNameResult',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    },\n                },\n            },\n            model.ShapeResolver(\n                {\n                    'StringType': {\n                        'type': 'string',\n                    },\n                }\n            ),\n        )\n        response = {'body': response, 'headers': {}, 'status_code': 200}\n        # Parsed response omits data from service since it is not\n        # modeled in the client\n        expected_parsed_response = {\n            'SDK_UNKNOWN_MEMBER': {'name': 'Foo'},\n            'ResponseMetadata': {\n                'RequestId': 'request-id',\n                'HTTPStatusCode': 200,\n                'HTTPHeaders': {},\n            },\n        }\n        expected_log = \"Received a response with an unknown member Foo set\"\n        self.assert_tagged_union_response_with_unknown_member(\n            parser,\n            response,\n            output_shape,\n            expected_parsed_response,\n            expected_log,\n        )\n\n    def test_parser_errors_out_when_multiple_members_set(self):\n        parser = parsers.JSONParser()\n        response = b'{\"Foo\": \"mystring\", \"Bar\": \"mystring2\"}'\n        headers = {'x-amzn-requestid': 'request-id'}\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'union': True,\n                'members': {\n                    'Foo': {\n                        'shape': 'StringType',\n                    },\n                    'Bar': {\n                        'shape': 'StringType',\n                    },\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n\n        response = {'body': response, 'headers': headers, 'status_code': 200}\n        with self.assertRaises(parsers.ResponseParserError):\n            parser.parse(response, output_shape)\n\n    def test_parser_accepts_type_metadata_with_union(self):\n        parser = parsers.JSONParser()\n        response = b'{\"Foo\": \"mystring\", \"__type\": \"mytype\"}'\n        headers = {'x-amzn-requestid': 'request-id'}\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'union': True,\n                'members': {\n                    'Foo': {\n                        'shape': 'StringType',\n                    },\n                    'Bar': {\n                        'shape': 'StringType',\n                    },\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n\n        response = {\n            'body': response,\n            'headers': headers,\n            'status_code': 200,\n        }\n        parsed = parser.parse(response, output_shape)\n        self.assertEqual(parsed['Foo'], 'mystring')\n\n\nclass TestHeaderResponseInclusion(unittest.TestCase):\n    def create_parser(self):\n        return parsers.JSONParser()\n\n    def create_arbitary_output_shape(self):\n        output_shape = model.StructureShape(\n            'OutputShape',\n            {\n                'type': 'structure',\n                'members': {\n                    'Str': {\n                        'shape': 'StringType',\n                    }\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n        return output_shape\n\n    def test_can_add_errors_into_response(self):\n        parser = self.create_parser()\n        headers = {\n            'x-amzn-requestid': 'request-id',\n            'Header1': 'foo',\n            'Header2': 'bar',\n        }\n        output_shape = self.create_arbitary_output_shape()\n        parsed = parser.parse(\n            {'body': b'{}', 'headers': headers, 'status_code': 200},\n            output_shape,\n        )\n        # The mapped header's keys should all be lower cased\n        parsed_headers = {\n            'x-amzn-requestid': 'request-id',\n            'header1': 'foo',\n            'header2': 'bar',\n        }\n        # Response headers should be mapped as HTTPHeaders.\n        self.assertEqual(\n            parsed['ResponseMetadata']['HTTPHeaders'], parsed_headers\n        )\n\n    def test_can_always_json_serialize_headers(self):\n        parser = self.create_parser()\n        original_headers = {\n            'x-amzn-requestid': 'request-id',\n            'Header1': 'foo',\n        }\n        headers = CustomHeaderDict(original_headers)\n        output_shape = self.create_arbitary_output_shape()\n        parsed = parser.parse(\n            {'body': b'{}', 'headers': headers, 'status_code': 200},\n            output_shape,\n        )\n        metadata = parsed['ResponseMetadata']\n        # We've had the contract that you can json serialize a\n        # response.  So we want to ensure that despite using a CustomHeaderDict\n        # we can always JSON dumps the response metadata.\n        self.assertEqual(\n            json.loads(json.dumps(metadata))['HTTPHeaders']['header1'], 'foo'\n        )\n\n\nclass TestResponseParsingDatetimes(unittest.TestCase):\n    def test_can_parse_float_timestamps(self):\n        # The type \"timestamp\" can come back as both an integer or as a float.\n        # We need to make sure we handle the case where the timestamp comes\n        # back as a float.  It might make sense to move this to protocol tests.\n        output_shape = model.Shape(\n            shape_name='datetime', shape_model={'type': 'timestamp'}\n        )\n        parser = parsers.JSONParser()\n        timestamp_as_float = b'1407538750.49'\n        expected_parsed = datetime.datetime(\n            2014, 8, 8, 22, 59, 10, 490000, tzinfo=tzutc()\n        )\n        parsed = parser.parse(\n            {'body': timestamp_as_float, 'headers': [], 'status_code': 200},\n            output_shape,\n        )\n        self.assertEqual(parsed, expected_parsed)\n\n\nclass TestResponseParserFactory(unittest.TestCase):\n    def setUp(self):\n        self.factory = parsers.ResponseParserFactory()\n\n    def test_rest_parser(self):\n        parser = self.factory.create_parser('rest-xml')\n        self.assertTrue(isinstance(parser, parsers.BaseRestParser))\n        self.assertTrue(isinstance(parser, parsers.BaseXMLResponseParser))\n\n    def test_json_parser(self):\n        parser = self.factory.create_parser('json')\n        self.assertTrue(isinstance(parser, parsers.BaseJSONParser))\n\n\nclass TestCanDecorateResponseParsing(unittest.TestCase):\n    def setUp(self):\n        self.factory = parsers.ResponseParserFactory()\n\n    def create_request_dict(self, with_body):\n        return {'body': with_body, 'headers': [], 'status_code': 200}\n\n    def test_normal_blob_parsing(self):\n        output_shape = model.Shape(\n            shape_name='BlobType', shape_model={'type': 'blob'}\n        )\n        parser = self.factory.create_parser('json')\n\n        hello_world_b64 = b'\"aGVsbG8gd29ybGQ=\"'\n        expected_parsed = b'hello world'\n        parsed = parser.parse(\n            self.create_request_dict(with_body=hello_world_b64), output_shape\n        )\n        self.assertEqual(parsed, expected_parsed)\n\n    def test_can_decorate_scalar_parsing(self):\n        output_shape = model.Shape(\n            shape_name='BlobType', shape_model={'type': 'blob'}\n        )\n        # Here we're overriding the blob parser so that\n        # we can change it to a noop parser.\n        self.factory.set_parser_defaults(blob_parser=lambda x: x)\n        parser = self.factory.create_parser('json')\n\n        hello_world_b64 = b'\"aGVsbG8gd29ybGQ=\"'\n        expected_parsed = \"aGVsbG8gd29ybGQ=\"\n        parsed = parser.parse(\n            self.create_request_dict(with_body=hello_world_b64), output_shape\n        )\n        self.assertEqual(parsed, expected_parsed)\n\n    def test_can_decorate_timestamp_parser(self):\n        output_shape = model.Shape(\n            shape_name='datetime', shape_model={'type': 'timestamp'}\n        )\n        # Here we're overriding the timestamp parser so that\n        # we can change it to just convert a string to an integer\n        # instead of converting to a datetime.\n        self.factory.set_parser_defaults(timestamp_parser=lambda x: int(x))\n        parser = self.factory.create_parser('json')\n\n        timestamp_as_int = b'1407538750'\n        expected_parsed = int(timestamp_as_int)\n        parsed = parser.parse(\n            self.create_request_dict(with_body=timestamp_as_int), output_shape\n        )\n        self.assertEqual(parsed, expected_parsed)\n\n\nclass TestHandlesNoOutputShape(unittest.TestCase):\n    \"\"\"Verify that each protocol handles no output shape properly.\"\"\"\n\n    def test_empty_rest_json_response(self):\n        headers = {'x-amzn-requestid': 'request-id'}\n        parser = parsers.RestJSONParser()\n        output_shape = None\n        parsed = parser.parse(\n            {'body': b'', 'headers': headers, 'status_code': 200}, output_shape\n        )\n        self.assertEqual(\n            parsed,\n            {\n                'ResponseMetadata': {\n                    'RequestId': 'request-id',\n                    'HTTPStatusCode': 200,\n                    'HTTPHeaders': headers,\n                }\n            },\n        )\n\n    def test_empty_rest_xml_response(self):\n        # This is the format used by cloudfront, route53.\n        headers = {'x-amzn-requestid': 'request-id'}\n        parser = parsers.RestXMLParser()\n        output_shape = None\n        parsed = parser.parse(\n            {'body': b'', 'headers': headers, 'status_code': 200}, output_shape\n        )\n        self.assertEqual(\n            parsed,\n            {\n                'ResponseMetadata': {\n                    'RequestId': 'request-id',\n                    'HTTPStatusCode': 200,\n                    'HTTPHeaders': headers,\n                }\n            },\n        )\n\n    def test_empty_query_response(self):\n        body = (\n            b'<DeleteTagsResponse xmlns=\"http://autoscaling.amazonaws.com/\">'\n            b'  <ResponseMetadata>'\n            b'    <RequestId>request-id</RequestId>'\n            b'  </ResponseMetadata>'\n            b'</DeleteTagsResponse>'\n        )\n        parser = parsers.QueryParser()\n        output_shape = None\n        parsed = parser.parse(\n            {'body': body, 'headers': {}, 'status_code': 200}, output_shape\n        )\n        self.assertEqual(\n            parsed,\n            {\n                'ResponseMetadata': {\n                    'RequestId': 'request-id',\n                    'HTTPStatusCode': 200,\n                    'HTTPHeaders': {},\n                }\n            },\n        )\n\n    def test_empty_json_response(self):\n        headers = {'x-amzn-requestid': 'request-id'}\n        # Output shape of None represents no output shape in the model.\n        output_shape = None\n        parser = parsers.JSONParser()\n        parsed = parser.parse(\n            {'body': b'', 'headers': headers, 'status_code': 200}, output_shape\n        )\n        self.assertEqual(\n            parsed,\n            {\n                'ResponseMetadata': {\n                    'RequestId': 'request-id',\n                    'HTTPStatusCode': 200,\n                    'HTTPHeaders': headers,\n                }\n            },\n        )\n\n\nclass TestHandlesInvalidXMLResponses(unittest.TestCase):\n    def test_invalid_xml_shown_in_error_message(self):\n        # Missing the closing XML tags.\n        invalid_xml = (\n            b'<DeleteTagsResponse xmlns=\"http://autoscaling.amazonaws.com/\">'\n            b'  <ResponseMetadata>'\n        )\n        parser = parsers.QueryParser()\n        output_shape = None\n        # The XML body should be in the error message.\n        with self.assertRaisesRegex(\n            parsers.ResponseParserError, '<DeleteTagsResponse'\n        ):\n            parser.parse(\n                {'body': invalid_xml, 'headers': {}, 'status_code': 200},\n                output_shape,\n            )\n\n\nclass TestRESTXMLResponses(unittest.TestCase):\n    def test_multiple_structures_list_returns_struture(self):\n        # This is to handle the scenario when something is modeled\n        # as a structure and instead a list of structures is returned.\n        # For this case, a single element from the list should be parsed\n        # For botocore, this will be the first element.\n        # Currently, this logic may happen in s3's GetBucketLifecycle\n        # operation.\n        headers = {}\n        parser = parsers.RestXMLParser()\n        body = (\n            '<?xml version=\"1.0\" ?>'\n            '<OperationName xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">'\n            '\t<Foo><Bar>first_value</Bar></Foo>'\n            '\t<Foo><Bar>middle_value</Bar></Foo>'\n            '\t<Foo><Bar>last_value</Bar></Foo>'\n            '</OperationName>'\n        )\n        builder = model.DenormalizedStructureBuilder()\n        output_shape = builder.with_members(\n            {\n                'Foo': {\n                    'type': 'structure',\n                    'members': {\n                        'Bar': {\n                            'type': 'string',\n                        }\n                    },\n                }\n            }\n        ).build_model()\n        parsed = parser.parse(\n            {'body': body, 'headers': headers, 'status_code': 200},\n            output_shape,\n        )\n        # Ensure the first element is used out of the list.\n        self.assertEqual(parsed['Foo'], {'Bar': 'first_value'})\n\n\nclass TestEventStreamParsers(unittest.TestCase):\n    def setUp(self):\n        self.parser = parsers.EventStreamXMLParser()\n        self.output_shape = model.StructureShape(\n            'EventStream',\n            {\n                'eventstream': True,\n                'type': 'structure',\n                'members': {\n                    'EventA': {'shape': 'EventAStructure'},\n                    'EventB': {'shape': 'EventBStructure'},\n                    'EventC': {'shape': 'EventCStructure'},\n                    'EventD': {'shape': 'EventDStructure'},\n                    'EventException': {'shape': 'ExceptionShape'},\n                },\n            },\n            model.ShapeResolver(\n                {\n                    'EventAStructure': {\n                        'event': True,\n                        'type': 'structure',\n                        'members': {\n                            'Stats': {\n                                'shape': 'StatsStructure',\n                                'eventpayload': True,\n                            },\n                            'Header': {\n                                'shape': 'IntShape',\n                                'eventheader': True,\n                            },\n                        },\n                    },\n                    'EventBStructure': {\n                        'event': True,\n                        'type': 'structure',\n                        'members': {\n                            'Body': {\n                                'shape': 'BlobShape',\n                                'eventpayload': True,\n                            }\n                        },\n                    },\n                    'EventCStructure': {\n                        'event': True,\n                        'type': 'structure',\n                        'members': {\n                            'Body': {\n                                'shape': 'StringShape',\n                                'eventpayload': True,\n                            }\n                        },\n                    },\n                    'EventDStructure': {\n                        'event': True,\n                        'type': 'structure',\n                        'members': {\n                            'StringField': {'shape': 'StringShape'},\n                            'IntField': {'shape': 'IntShape'},\n                            'Header': {\n                                'shape': 'IntShape',\n                                'eventheader': True,\n                            },\n                        },\n                    },\n                    'StatsStructure': {\n                        'type': 'structure',\n                        'members': {\n                            'StringField': {'shape': 'StringShape'},\n                            'IntField': {'shape': 'IntShape'},\n                        },\n                    },\n                    'BlobShape': {'type': 'blob'},\n                    'StringShape': {'type': 'string'},\n                    'IntShape': {'type': 'integer'},\n                    'ExceptionShape': {\n                        'exception': True,\n                        'type': 'structure',\n                        'members': {'message': {'shape': 'StringShape'}},\n                    },\n                }\n            ),\n        )\n\n    def parse_event(self, headers=None, body=None, status_code=200):\n        response_dict = {\n            'body': body,\n            'headers': headers,\n            'status_code': status_code,\n        }\n        return self.parser.parse(response_dict, self.output_shape)\n\n    def test_parses_event_xml(self):\n        headers = {'Header': 123, ':event-type': 'EventA'}\n        body = (\n            b'<Stats xmlns=\"\">'\n            b'  <StringField>abcde</StringField>'\n            b'  <IntField>1234</IntField>'\n            b'</Stats>'\n        )\n        parsed = self.parse_event(headers, body)\n        expected = {\n            'EventA': {\n                'Header': 123,\n                'Stats': {'StringField': 'abcde', 'IntField': 1234},\n            }\n        }\n        self.assertEqual(parsed, expected)\n\n    def test_parses_event_bad_xml(self):\n        headers = {'Header': 123, ':event-type': 'EventA'}\n        parsed = self.parse_event(headers, b'')\n        expected = {'EventA': {'Header': 123, 'Stats': {}}}\n        self.assertEqual(parsed, expected)\n\n    def test_parses_event_blob(self):\n        headers = {':event-type': 'EventB'}\n        parsed = self.parse_event(headers, b'blob')\n        expected = {'EventB': {'Body': b'blob'}}\n        self.assertEqual(parsed, expected)\n\n    def test_parses_event_string(self):\n        headers = {':event-type': 'EventC'}\n        parsed = self.parse_event(headers, b'blob')\n        expected = {'EventC': {'Body': 'blob'}}\n        self.assertEqual(parsed, expected)\n\n    def test_parses_payload_implicit(self):\n        headers = {'Header': 123, ':event-type': 'EventD'}\n        body = (\n            b'<EventD xmlns=\"\">'\n            b'  <StringField>abcde</StringField>'\n            b'  <IntField>1234</IntField>'\n            b'</EventD>'\n        )\n        parsed = self.parse_event(headers, body)\n        expected = {\n            'EventD': {'Header': 123, 'StringField': 'abcde', 'IntField': 1234}\n        }\n        self.assertEqual(parsed, expected)\n\n    def test_parses_error_event(self):\n        error_code = 'client/SomeError'\n        error_message = 'You did something wrong'\n        headers = {\n            ':message-type': 'error',\n            ':error-code': error_code,\n            ':error-message': error_message,\n        }\n        body = b''\n        parsed = self.parse_event(headers, body, status_code=400)\n        expected = {'Error': {'Code': error_code, 'Message': error_message}}\n        self.assertEqual(parsed, expected)\n\n    def test_parses_exception_event(self):\n        self.parser = parsers.EventStreamJSONParser()\n        error_code = 'EventException'\n        headers = {\n            ':message-type': 'exception',\n            ':exception-type': error_code,\n        }\n        body = b'{\"message\": \"You did something wrong\"}'\n        parsed = self.parse_event(headers, body, status_code=400)\n        expected = {\n            'Error': {'Code': error_code, 'Message': 'You did something wrong'}\n        }\n        self.assertEqual(parsed, expected)\n\n    def test_parses_event_json(self):\n        self.parser = parsers.EventStreamJSONParser()\n        headers = {':event-type': 'EventD'}\n        body = b'{' b'  \"StringField\": \"abcde\",' b'  \"IntField\": 1234' b'}'\n        parsed = self.parse_event(headers, body)\n        expected = {'EventD': {'StringField': 'abcde', 'IntField': 1234}}\n        self.assertEqual(parsed, expected)\n\n\nclass TestParseErrorResponses(unittest.TestCase):\n    # This class consolidates all the error parsing tests\n    # across all the protocols.  We may potentially pull\n    # this into the shared protocol tests in the future,\n    # so consolidating them into a single class will make\n    # this easier.\n    def setUp(self):\n        self.error_shape = model.StructureShape(\n            'ErrorShape',\n            {\n                'type': 'structure',\n                'exception': True,\n                'members': {\n                    'ModeledField': {\n                        'shape': 'StringType',\n                    }\n                },\n            },\n            model.ShapeResolver({'StringType': {'type': 'string'}}),\n        )\n\n    def test_response_metadata_errors_for_json_protocol(self):\n        parser = parsers.JSONParser()\n        response = {\n            \"body\": b\"\"\"\n                {\"__type\":\"amazon.foo.validate#ValidationException\",\n                 \"message\":\"this is a message\"}\n                \"\"\",\n            \"status_code\": 400,\n            \"headers\": {\"x-amzn-requestid\": \"request-id\"},\n        }\n        parsed = parser.parse(response, None)\n        # Even (especially) on an error condition, the\n        # ResponseMetadata should be populated.\n        self.assertIn('ResponseMetadata', parsed)\n        self.assertEqual(parsed['ResponseMetadata']['RequestId'], 'request-id')\n\n        self.assertIn('Error', parsed)\n        self.assertEqual(parsed['Error']['Message'], 'this is a message')\n        self.assertEqual(parsed['Error']['Code'], 'ValidationException')\n\n    def test_response_metadata_errors_alternate_form_json_protocol(self):\n        # Sometimes there is no '#' in the __type.  We need to be\n        # able to parse this error message as well.\n        parser = parsers.JSONParser()\n        response = {\n            \"body\": b\"\"\"\n                {\"__type\":\"ValidationException\",\n                 \"message\":\"this is a message\"}\n                \"\"\",\n            \"status_code\": 400,\n            \"headers\": {\"x-amzn-requestid\": \"request-id\"},\n        }\n        parsed = parser.parse(response, None)\n        self.assertIn('Error', parsed)\n        self.assertEqual(parsed['Error']['Message'], 'this is a message')\n        self.assertEqual(parsed['Error']['Code'], 'ValidationException')\n\n    def test_response_with_query_error_for_json_protocol(self):\n        parser = parsers.JSONParser()\n        response = {\n            \"body\": b\"\"\"\n                {\"__type\":\"amazon.foo.validate#ValidationException\",\n                 \"message\":\"this is a message\"}\n                \"\"\",\n            \"status_code\": 400,\n            \"headers\": {\n                \"x-amzn-requestid\": \"request-id\",\n                \"x-amzn-query-error\": \"AWS.SimpleQueueService.NonExistentQueue;Sender\",\n            },\n        }\n        parsed = parser.parse(response, None)\n        # ResponseMetadata should always be populated.\n        self.assertIn('ResponseMetadata', parsed)\n        self.assertEqual(parsed['ResponseMetadata']['RequestId'], 'request-id')\n\n        self.assertIn('Error', parsed)\n        self.assertEqual(parsed['Error']['Message'], 'this is a message')\n        self.assertEqual(\n            parsed['Error']['Code'], 'AWS.SimpleQueueService.NonExistentQueue'\n        )\n        self.assertEqual(\n            parsed['Error']['QueryErrorCode'], \"ValidationException\"\n        )\n        self.assertEqual(parsed['Error']['Type'], 'Sender')\n\n    def test_response_with_invalid_query_error_for_json_protocol(self):\n        parser = parsers.JSONParser()\n        response = {\n            \"body\": b\"\"\"\n                {\"__type\":\"amazon.foo.validate#ValidationException\",\n                 \"message\":\"this is a message\"}\n                \"\"\",\n            \"status_code\": 400,\n            \"headers\": {\n                \"x-amzn-requestid\": \"request-id\",\n                \"x-amzn-query-error\": \"AWS.SimpleQueueService.NonExistentQueue;sender;400\",\n            },\n        }\n        parsed = parser.parse(response, None)\n        self.assertIn('ResponseMetadata', parsed)\n        self.assertEqual(parsed['ResponseMetadata']['RequestId'], 'request-id')\n\n        self.assertIn('Error', parsed)\n        self.assertEqual(parsed['Error']['Message'], 'this is a message')\n        self.assertEqual(parsed['Error']['Code'], 'ValidationException')\n        self.assertNotIn('QueryErrorCode', parsed['Error'])\n        self.assertNotIn('Type', parsed['Error'])\n\n    def test_response_with_incomplete_query_error_for_json_protocol(self):\n        parser = parsers.JSONParser()\n        response = {\n            \"body\": b\"\"\"\n                {\"__type\":\"amazon.foo.validate#ValidationException\",\n                 \"message\":\"this is a message\"}\n                \"\"\",\n            \"status_code\": 400,\n            \"headers\": {\n                \"x-amzn-requestid\": \"request-id\",\n                \"x-amzn-query-error\": \";sender\",\n            },\n        }\n        parsed = parser.parse(response, None)\n        self.assertIn('ResponseMetadata', parsed)\n        self.assertEqual(parsed['ResponseMetadata']['RequestId'], 'request-id')\n\n        self.assertIn('Error', parsed)\n        self.assertEqual(parsed['Error']['Message'], 'this is a message')\n        self.assertEqual(parsed['Error']['Code'], 'ValidationException')\n        self.assertNotIn('QueryErrorCode', parsed['Error'])\n        self.assertNotIn('Type', parsed['Error'])\n\n    def test_response_with_empty_query_errors_for_json_protocol(self):\n        parser = parsers.JSONParser()\n        response = {\n            \"body\": b\"\"\"\n                {\"__type\":\"amazon.foo.validate#ValidationException\",\n                 \"message\":\"this is a message\"}\n                \"\"\",\n            \"status_code\": 400,\n            \"headers\": {\n                \"x-amzn-requestid\": \"request-id\",\n                \"x-amzn-query-error\": \"\",\n            },\n        }\n        parsed = parser.parse(response, None)\n        self.assertIn('ResponseMetadata', parsed)\n        self.assertEqual(parsed['ResponseMetadata']['RequestId'], 'request-id')\n\n        self.assertIn('Error', parsed)\n        self.assertEqual(parsed['Error']['Message'], 'this is a message')\n        self.assertEqual(parsed['Error']['Code'], 'ValidationException')\n        self.assertNotIn('QueryErrorCode', parsed['Error'])\n        self.assertNotIn('Type', parsed['Error'])\n\n    def test_parse_error_response_for_query_protocol(self):\n        body = (\n            b'<ErrorResponse xmlns=\"https://iam.amazonaws.com/doc/2010-05-08/\">'\n            b'  <Error>'\n            b'    <Type>Sender</Type>'\n            b'    <Code>InvalidInput</Code>'\n            b'    <Message>ARN asdf is not valid.</Message>'\n            b'  </Error>'\n            b'  <RequestId>request-id</RequestId>'\n            b'</ErrorResponse>'\n        )\n        parser = parsers.QueryParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': {}, 'status_code': 400}, None\n        )\n        self.assertIn('Error', parsed)\n        self.assertEqual(\n            parsed['Error'],\n            {\n                'Code': 'InvalidInput',\n                'Message': 'ARN asdf is not valid.',\n                'Type': 'Sender',\n            },\n        )\n\n    def test_can_parse_sdb_error_response_query_protocol(self):\n        body = (\n            b'<OperationNameResponse>'\n            b'    <Errors>'\n            b'        <Error>'\n            b'            <Code>1</Code>'\n            b'            <Message>msg</Message>'\n            b'        </Error>'\n            b'    </Errors>'\n            b'    <RequestId>abc-123</RequestId>'\n            b'</OperationNameResponse>'\n        )\n        parser = parsers.QueryParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': {}, 'status_code': 500}, None\n        )\n        self.assertIn('Error', parsed)\n        self.assertEqual(parsed['Error'], {'Code': '1', 'Message': 'msg'})\n        self.assertEqual(\n            parsed['ResponseMetadata'],\n            {'RequestId': 'abc-123', 'HTTPStatusCode': 500, 'HTTPHeaders': {}},\n        )\n\n    def test_can_parser_ec2_errors(self):\n        body = (\n            b'<Response>'\n            b'  <Errors>'\n            b'    <Error>'\n            b'      <Code>InvalidInstanceID.NotFound</Code>'\n            b'      <Message>The instance ID i-12345 does not exist</Message>'\n            b'    </Error>'\n            b'  </Errors>'\n            b'  <RequestID>06f382b0-d521-4bb6-988c-ca49d5ae6070</RequestID>'\n            b'</Response>'\n        )\n        parser = parsers.EC2QueryParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': {}, 'status_code': 400}, None\n        )\n        self.assertIn('Error', parsed)\n        self.assertEqual(\n            parsed['Error'],\n            {\n                'Code': 'InvalidInstanceID.NotFound',\n                'Message': 'The instance ID i-12345 does not exist',\n            },\n        )\n\n    def test_can_parse_rest_xml_errors(self):\n        body = (\n            b'<ErrorResponse xmlns=\"https://route53.amazonaws.com/doc/2013-04-01/\">'\n            b'  <Error>'\n            b'    <Type>Sender</Type>'\n            b'    <Code>NoSuchHostedZone</Code>'\n            b'    <Message>No hosted zone found with ID: foobar</Message>'\n            b'  </Error>'\n            b'  <RequestId>bc269cf3-d44f-11e5-8779-2d21c30eb3f1</RequestId>'\n            b'</ErrorResponse>'\n        )\n        parser = parsers.RestXMLParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': {}, 'status_code': 400}, None\n        )\n        self.assertIn('Error', parsed)\n        self.assertEqual(\n            parsed['Error'],\n            {\n                'Code': 'NoSuchHostedZone',\n                'Message': 'No hosted zone found with ID: foobar',\n                'Type': 'Sender',\n            },\n        )\n\n    def test_can_parse_rest_json_errors(self):\n        body = b'{\"Message\":\"Function not found: foo\",\"Type\":\"User\"}'\n        headers = {\n            'x-amzn-requestid': 'request-id',\n            'x-amzn-errortype': 'ResourceNotFoundException:http://url/',\n        }\n        parser = parsers.RestJSONParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': headers, 'status_code': 400}, None\n        )\n        self.assertIn('Error', parsed)\n        self.assertEqual(\n            parsed['Error'],\n            {\n                'Code': 'ResourceNotFoundException',\n                'Message': 'Function not found: foo',\n            },\n        )\n\n    def test_error_response_with_no_body_rest_json(self):\n        parser = parsers.RestJSONParser()\n        response = b''\n        headers = {'content-length': '0', 'connection': 'keep-alive'}\n        output_shape = None\n        parsed = parser.parse(\n            {'body': response, 'headers': headers, 'status_code': 504},\n            output_shape,\n        )\n\n        self.assertIn('Error', parsed)\n        self.assertEqual(\n            parsed['Error'], {'Code': '504', 'Message': 'Gateway Timeout'}\n        )\n        self.assertEqual(\n            parsed['ResponseMetadata'],\n            {'HTTPStatusCode': 504, 'HTTPHeaders': headers},\n        )\n\n    def test_error_response_with_string_body_rest_json(self):\n        parser = parsers.RestJSONParser()\n        response = b'HTTP content length exceeded 1049600 bytes.'\n        headers = {'content-length': '0', 'connection': 'keep-alive'}\n        output_shape = None\n        parsed = parser.parse(\n            {'body': response, 'headers': headers, 'status_code': 413},\n            output_shape,\n        )\n\n        self.assertIn('Error', parsed)\n        self.assertEqual(\n            parsed['Error'],\n            {'Code': '413', 'Message': response.decode('utf-8')},\n        )\n        self.assertEqual(\n            parsed['ResponseMetadata'],\n            {'HTTPStatusCode': 413, 'HTTPHeaders': headers},\n        )\n\n    def test_error_response_with_xml_body_rest_json(self):\n        parser = parsers.RestJSONParser()\n        response = (\n            b'<AccessDeniedException>'\n            b'   <Message>Unable to determine service/operation name to be authorized</Message>'\n            b'</AccessDeniedException>'\n        )\n        headers = {'content-length': '0', 'connection': 'keep-alive'}\n        output_shape = None\n        parsed = parser.parse(\n            {'body': response, 'headers': headers, 'status_code': 403},\n            output_shape,\n        )\n\n        self.assertIn('Error', parsed)\n        self.assertEqual(\n            parsed['Error'],\n            {'Code': '403', 'Message': response.decode('utf-8')},\n        )\n        self.assertEqual(\n            parsed['ResponseMetadata'],\n            {'HTTPStatusCode': 403, 'HTTPHeaders': headers},\n        )\n\n    def test_s3_error_response(self):\n        body = (\n            b'<Error>'\n            b'  <Code>NoSuchBucket</Code>'\n            b'  <Message>error message</Message>'\n            b'  <BucketName>asdf</BucketName>'\n            b'  <RequestId>EF1EF43A74415102</RequestId>'\n            b'  <HostId>hostid</HostId>'\n            b'</Error>'\n        )\n        headers = {'x-amz-id-2': 'second-id', 'x-amz-request-id': 'request-id'}\n        parser = parsers.RestXMLParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': headers, 'status_code': 400}, None\n        )\n        self.assertIn('Error', parsed)\n        self.assertEqual(\n            parsed['Error'],\n            {\n                'Code': 'NoSuchBucket',\n                'Message': 'error message',\n                'BucketName': 'asdf',\n                # We don't want the RequestId/HostId because they're already\n                # present in the ResponseMetadata key.\n            },\n        )\n        self.assertEqual(\n            parsed['ResponseMetadata'],\n            {\n                'RequestId': 'request-id',\n                'HostId': 'second-id',\n                'HTTPStatusCode': 400,\n                'HTTPHeaders': headers,\n            },\n        )\n\n    def test_s3_error_response_with_no_body(self):\n        # If you try to HeadObject a key that does not exist,\n        # you will get an empty body.  When this happens\n        # we expect that we will use Code/Message from the\n        # HTTP status code.\n        body = ''\n        headers = {'x-amz-id-2': 'second-id', 'x-amz-request-id': 'request-id'}\n        parser = parsers.RestXMLParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': headers, 'status_code': 404}, None\n        )\n        self.assertIn('Error', parsed)\n        self.assertEqual(\n            parsed['Error'],\n            {\n                'Code': '404',\n                'Message': 'Not Found',\n            },\n        )\n        self.assertEqual(\n            parsed['ResponseMetadata'],\n            {\n                'RequestId': 'request-id',\n                'HostId': 'second-id',\n                'HTTPStatusCode': 404,\n                'HTTPHeaders': headers,\n            },\n        )\n\n    def test_can_parse_glacier_error_response(self):\n        body = (\n            b'{\"code\":\"AccessDeniedException\",\"type\":\"Client\",\"message\":'\n            b'\"Access denied\"}'\n        )\n        headers = {'x-amzn-requestid': 'request-id'}\n        parser = parsers.RestJSONParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': headers, 'status_code': 400}, None\n        )\n        self.assertEqual(\n            parsed['Error'],\n            {'Message': 'Access denied', 'Code': 'AccessDeniedException'},\n        )\n\n    def test_can_parse_restjson_error_code(self):\n        body = b'''{\n            \"status\": \"error\",\n            \"errors\": [{\"message\": \"[*Deprecated*: blah\"}],\n            \"adds\": 0,\n            \"__type\": \"#WasUnableToParseThis\",\n            \"message\": \"blah\",\n            \"deletes\": 0}'''\n        headers = {'x-amzn-requestid': 'request-id'}\n        parser = parsers.RestJSONParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': headers, 'status_code': 400}, None\n        )\n        self.assertEqual(\n            parsed['Error'],\n            {'Message': 'blah', 'Code': 'WasUnableToParseThis'},\n        )\n\n    def test_can_parse_with_case_insensitive_keys(self):\n        body = (\n            b'{\"Code\":\"AccessDeniedException\",\"type\":\"Client\",\"Message\":'\n            b'\"Access denied\"}'\n        )\n        headers = {'x-amzn-requestid': 'request-id'}\n        parser = parsers.RestJSONParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': headers, 'status_code': 400}, None\n        )\n        self.assertEqual(\n            parsed['Error'],\n            {'Message': 'Access denied', 'Code': 'AccessDeniedException'},\n        )\n\n    def test_can_parse_rest_json_modeled_fields(self):\n        body = (\n            b'{\"ModeledField\":\"Some modeled field\",'\n            b'\"Message\":\"Some message\"}'\n        )\n        parser = parsers.RestJSONParser()\n        response_dict = {\n            'status_code': 400,\n            'headers': {},\n            'body': body,\n        }\n        parsed = parser.parse(response_dict, self.error_shape)\n        expected_parsed = {\n            'ModeledField': 'Some modeled field',\n        }\n        self.assertEqual(parsed, expected_parsed)\n\n    def test_can_parse_rest_xml_modeled_fields(self):\n        parser = parsers.RestXMLParser()\n        body = (\n            b'<?xml version=\"1.0\"?>\\n<ErrorResponse xmlns=\"http://foo.bar\">'\n            b'<Error><Type>Sender</Type><Code>NoSuchDistribution</Code>'\n            b'<Message>The specified distribution does not exist.</Message>'\n            b'<ModeledField>Some modeled field</ModeledField>'\n            b'</Error>'\n            b'</ErrorResponse>'\n        )\n        response_dict = {\n            'status_code': 400,\n            'headers': {},\n            'body': body,\n        }\n        parsed = parser.parse(response_dict, self.error_shape)\n        expected_parsed = {\n            'ModeledField': 'Some modeled field',\n        }\n        self.assertEqual(parsed, expected_parsed)\n\n    def test_can_parse_ec2_modeled_fields(self):\n        body = (\n            b'<Response><Errors><Error>'\n            b'<Code>ExceptionShape</Code>'\n            b'<Message>Foo message</Message>'\n            b'<ModeledField>Some modeled field</ModeledField>'\n            b'</Error></Errors></Response>'\n        )\n        parser = parsers.EC2QueryParser()\n        response_dict = {\n            'status_code': 400,\n            'headers': {},\n            'body': body,\n        }\n        parsed = parser.parse(response_dict, self.error_shape)\n        expected_parsed = {\n            'ModeledField': 'Some modeled field',\n        }\n        self.assertEqual(parsed, expected_parsed)\n\n    def test_can_parse_query_modeled_fields(self):\n        parser = parsers.QueryParser()\n        body = (\n            b'<?xml version=\"1.0\"?>\\n<ErrorResponse xmlns=\"http://foo.bar\">'\n            b'<Error><Type>Sender</Type><Code>SomeCode</Code>'\n            b'<Message>A message</Message>'\n            b'<ModeledField>Some modeled field</ModeledField>'\n            b'</Error>'\n            b'</ErrorResponse>'\n        )\n        response_dict = {\n            'status_code': 400,\n            'headers': {},\n            'body': body,\n        }\n        parsed = parser.parse(response_dict, self.error_shape)\n        expected_parsed = {\n            'ModeledField': 'Some modeled field',\n        }\n        self.assertEqual(parsed, expected_parsed)\n\n    def test_can_parse_json_modeled_fields(self):\n        body = (\n            b'{\"ModeledField\":\"Some modeled field\",'\n            b'\"Message\":\"Some message\",'\n            b'\"__type\": \"Prefix#SomeError\"}'\n        )\n        parser = parsers.JSONParser()\n        response_dict = {\n            'status_code': 400,\n            'headers': {},\n            'body': body,\n        }\n        parsed = parser.parse(response_dict, self.error_shape)\n        expected_parsed = {\n            'ModeledField': 'Some modeled field',\n        }\n        self.assertEqual(parsed, expected_parsed)\n\n    def test_can_parse_route53_with_missing_message(self):\n        # The message isn't always in the XML response (or even the headers).\n        # We should be able to handle this gracefully and still at least\n        # populate a \"Message\" key so that consumers don't have to\n        # conditionally check for this.\n        body = (\n            b'<ErrorResponse>'\n            b'  <Error>'\n            b'    <Type>Sender</Type>'\n            b'    <Code>InvalidInput</Code>'\n            b'  </Error>'\n            b'  <RequestId>id</RequestId>'\n            b'</ErrorResponse>'\n        )\n        parser = parsers.RestXMLParser()\n        parsed = parser.parse(\n            {'body': body, 'headers': {}, 'status_code': 400}, None\n        )\n        error = parsed['Error']\n        self.assertEqual(error['Code'], 'InvalidInput')\n        # Even though there's no <Message /> we should\n        # still populate an empty string.\n        self.assertEqual(error['Message'], '')\n\n\ndef _generic_test_bodies():\n    generic_html_body = (\n        b'<html><body><b>Http/1.1 Service Unavailable</b></body></html>'\n    )\n    empty_body = b''\n    none_body = None\n\n    return [generic_html_body, empty_body, none_body]\n\n\n@pytest.mark.parametrize(\n    \"parser, body\",\n    itertools.product(\n        parsers.PROTOCOL_PARSERS.values(), _generic_test_bodies()\n    ),\n)\ndef test_can_handle_generic_error_message(parser, body):\n    # There are times when you can get a service to respond with a generic\n    # html error page.  We should be able to handle this case.\n    parsed = parser().parse(\n        {'body': body, 'headers': {}, 'status_code': 503}, None\n    )\n    assert parsed['Error'] == {'Code': '503', 'Message': 'Service Unavailable'}\n    assert parsed['ResponseMetadata']['HTTPStatusCode'] == 503\n", "tests/unit/test_endpoint_provider.py": "# Copyright 2012-2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport json\nimport logging\nimport os\nfrom unittest.mock import Mock\n\nimport pytest\n\nfrom botocore.endpoint_provider import (\n    EndpointProvider,\n    EndpointRule,\n    ErrorRule,\n    RuleCreator,\n    RuleSet,\n    RuleSetStandardLibrary,\n    TreeRule,\n)\nfrom botocore.exceptions import (\n    EndpointResolutionError,\n    MissingDependencyException,\n    UnknownSignatureVersionError,\n)\nfrom botocore.loaders import Loader\nfrom botocore.regions import EndpointRulesetResolver\nfrom tests import requires_crt\n\nREGION_TEMPLATE = \"{Region}\"\nREGION_REF = {\"ref\": \"Region\"}\nBUCKET_ARN_REF = {\"ref\": \"bucketArn\"}\nPARSE_ARN_FUNC = {\n    \"fn\": \"aws.parseArn\",\n    \"argv\": [{\"ref\": \"Bucket\"}],\n    \"assign\": \"bucketArn\",\n}\nSTRING_EQUALS_FUNC = {\n    \"fn\": \"stringEquals\",\n    \"argv\": [\n        {\n            \"fn\": \"getAttr\",\n            \"argv\": [BUCKET_ARN_REF, \"region\"],\n            \"assign\": \"bucketRegion\",\n        },\n        \"\",\n    ],\n}\nDNS_SUFFIX_TEMPLATE = \"{PartitionResults#dnsSuffix}\"\nURL_TEMPLATE = (\n    f\"https://{REGION_TEMPLATE}.myGreatService.{DNS_SUFFIX_TEMPLATE}\"\n)\nENDPOINT_DICT = {\n    \"url\": URL_TEMPLATE,\n    \"properties\": {\n        \"authSchemes\": [\n            {\n                \"signingName\": \"s3\",\n                \"signingScope\": REGION_TEMPLATE,\n                \"name\": \"s3v4\",\n            }\n        ],\n    },\n    \"headers\": {\n        \"x-amz-region-set\": [\n            REGION_REF,\n            {\n                \"fn\": \"getAttr\",\n                \"argv\": [BUCKET_ARN_REF, \"region\"],\n            },\n            \"us-east-2\",\n        ],\n    },\n}\n\n\n@pytest.fixture(scope=\"module\")\ndef loader():\n    return Loader()\n\n\n@pytest.fixture(scope=\"module\")\ndef partitions(loader):\n    return loader.load_data(\"partitions\")\n\n\n@pytest.fixture(scope=\"module\")\ndef rule_lib(partitions):\n    return RuleSetStandardLibrary(partitions)\n\n\n@pytest.fixture(scope=\"module\")\ndef ruleset_dict():\n    path = os.path.join(\n        os.path.dirname(__file__),\n        \"data\",\n        \"endpoints\",\n        \"valid-rules\",\n        \"deprecated-param.json\",\n    )\n    with open(path) as f:\n        return json.load(f)\n\n\n@pytest.fixture(scope=\"module\")\ndef endpoint_provider(ruleset_dict, partitions):\n    return EndpointProvider(ruleset_dict, partitions)\n\n\n@pytest.fixture(scope=\"module\")\ndef endpoint_rule():\n    return EndpointRule(\n        endpoint=ENDPOINT_DICT,\n        conditions=[\n            PARSE_ARN_FUNC,\n            {\n                \"fn\": \"not\",\n                \"argv\": [STRING_EQUALS_FUNC],\n            },\n            {\n                \"fn\": \"aws.partition\",\n                \"argv\": [REGION_REF],\n                \"assign\": \"PartitionResults\",\n            },\n        ],\n    )\n\n\ndef ruleset_testcases():\n    filenames = [\n        \"aws-region\",\n        \"default-values\",\n        \"eventbridge\",\n        \"fns\",\n        \"headers\",\n        \"is-virtual-hostable-s3-bucket\",\n        \"local-region-override\",\n        \"parse-arn\",\n        \"parse-url\",\n        \"substring\",\n        \"uri-encode\",\n        \"valid-hostlabel\",\n    ]\n    error_cases = []\n    endpoint_cases = []\n    base_path = os.path.join(os.path.dirname(__file__), \"data\", \"endpoints\")\n    for name in filenames:\n        with open(os.path.join(base_path, \"valid-rules\", f\"{name}.json\")) as f:\n            ruleset = json.load(f)\n        with open(os.path.join(base_path, \"test-cases\", f\"{name}.json\")) as f:\n            tests = json.load(f)\n\n        for test in tests[\"testCases\"]:\n            input_params = test[\"params\"]\n            expected_object = test[\"expect\"]\n            if \"error\" in expected_object:\n                error_cases.append(\n                    (ruleset, input_params, expected_object[\"error\"])\n                )\n            elif \"endpoint\" in expected_object:\n                endpoint_cases.append(\n                    (ruleset, input_params, expected_object[\"endpoint\"])\n                )\n            else:\n                raise ValueError(\"Expected `error` or `endpoint` in test case\")\n    return error_cases, endpoint_cases\n\n\nERROR_TEST_CASES, ENDPOINT_TEST_CASES = ruleset_testcases()\n\n\n@pytest.mark.parametrize(\n    \"ruleset,input_params,expected_error\",\n    ERROR_TEST_CASES,\n)\ndef test_endpoint_resolution_raises(\n    partitions, ruleset, input_params, expected_error\n):\n    endpoint_provider = EndpointProvider(ruleset, partitions)\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        endpoint_provider.resolve_endpoint(**input_params)\n    assert str(exc_info.value) == expected_error\n\n\n@pytest.mark.parametrize(\n    \"ruleset,input_params,expected_endpoint\",\n    ENDPOINT_TEST_CASES,\n)\ndef test_endpoint_resolution(\n    partitions, ruleset, input_params, expected_endpoint\n):\n    endpoint_provider = EndpointProvider(ruleset, partitions)\n    endpoint = endpoint_provider.resolve_endpoint(**input_params)\n    assert endpoint.url == expected_endpoint[\"url\"]\n    assert endpoint.properties == expected_endpoint.get(\"properties\", {})\n    assert endpoint.headers == expected_endpoint.get(\"headers\", {})\n\n\ndef test_none_returns_default_partition(rule_lib):\n    partition_dict = rule_lib.aws_partition(None)\n    assert partition_dict['name'] == \"aws\"\n\n\ndef test_no_match_region_returns_default_partition(rule_lib):\n    partition_dict = rule_lib.aws_partition(\"invalid-region-42\")\n    assert partition_dict['name'] == \"aws\"\n\n\ndef test_invalid_arn_returns_none(rule_lib):\n    assert rule_lib.aws_parse_arn(\"arn:aws:this-is-not-an-arn:foo\") is None\n\n\n@pytest.mark.parametrize(\n    \"arn, expected_resource_id\",\n    [\n        (\n            \"arn:aws:s3:::myBucket/key\",\n            {\n                \"partition\": \"aws\",\n                \"service\": \"s3\",\n                \"region\": \"\",\n                \"accountId\": \"\",\n                \"resourceId\": [\"myBucket\", \"key\"],\n            },\n        ),\n        (\n            \"arn:aws:kinesis:us-east-1:1234567890:stream/mystream:foo\",\n            {\n                \"partition\": \"aws\",\n                \"service\": \"kinesis\",\n                \"region\": \"us-east-1\",\n                \"accountId\": \"1234567890\",\n                \"resourceId\": [\"stream\", \"mystream\", \"foo\"],\n            },\n        ),\n        (\n            \"arn:aws:s3:::myBucket\",\n            {\n                \"partition\": \"aws\",\n                \"service\": \"s3\",\n                \"region\": \"\",\n                \"accountId\": \"\",\n                \"region\": \"\",\n                \"resourceId\": [\"myBucket\"],\n            },\n        ),\n    ],\n)\ndef test_parse_arn_delimiters(rule_lib, arn, expected_resource_id):\n    parsed_arn = rule_lib.aws_parse_arn(arn)\n    assert parsed_arn == expected_resource_id\n\n\ndef test_uri_encode_none_returns_none(rule_lib):\n    assert rule_lib.uri_encode(None) is None\n\n\ndef test_parse_url_none_return_none(rule_lib):\n    assert rule_lib.parse_url(None) is None\n\n\ndef test_string_equals_wrong_type_raises(rule_lib):\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        rule_lib.string_equals(1, 2)\n    assert \"Both values must be strings\" in str(exc_info.value)\n\n\ndef test_boolean_equals_wrong_type_raises(rule_lib):\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        rule_lib.boolean_equals(1, 2)\n    assert \"Both arguments must be bools\" in str(exc_info.value)\n\n\ndef test_substring_wrong_type_raises(rule_lib):\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        rule_lib.substring([\"h\", \"e\", \"l\", \"l\", \"o\"], 0, 5, False)\n    assert \"Input must be a string\" in str(exc_info.value)\n\n\ndef test_creator_unknown_type_raises():\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        RuleCreator.create(type=\"foo\")\n    assert \"Unknown rule type: foo.\" in str(exc_info.value)\n\n\ndef test_parameter_wrong_type_raises(endpoint_provider):\n    param = endpoint_provider.ruleset.parameters[\"Region\"]\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        param.validate_input(1)\n    assert \"Value (Region) is the wrong type\" in str(exc_info.value)\n\n\ndef test_deprecated_parameter_logs(endpoint_provider, caplog):\n    caplog.set_level(logging.INFO)\n    param = endpoint_provider.ruleset.parameters[\"Region\"]\n    param.validate_input(\"foo\")\n    assert \"Region has been deprecated.\" in caplog.text\n\n\ndef test_no_endpoint_found_error(endpoint_provider):\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        endpoint_provider.resolve_endpoint(\n            **{\"Endpoint\": \"mygreatendpoint.com\", \"Bucket\": \"mybucket\"}\n        )\n    assert \"No endpoint found for parameters\" in str(exc_info.value)\n\n\n@pytest.mark.parametrize(\n    \"rule_dict,expected_rule_type\",\n    [\n        (\n            {\n                \"type\": \"endpoint\",\n                \"conditions\": [],\n                \"endpoint\": {\n                    \"url\": (\n                        \"https://{Region}.myGreatService.\"\n                        \"{PartitionResult#dualStackDnsSuffix}\"\n                    ),\n                    \"properties\": {},\n                    \"headers\": {},\n                },\n            },\n            EndpointRule,\n        ),\n        (\n            {\n                \"type\": \"error\",\n                \"conditions\": [],\n                \"error\": (\n                    \"Dualstack is enabled but this partition \"\n                    \"does not support DualStack\"\n                ),\n            },\n            ErrorRule,\n        ),\n        ({\"type\": \"tree\", \"conditions\": [], \"rules\": []}, TreeRule),\n    ],\n)\ndef test_rule_creation(rule_dict, expected_rule_type):\n    rule = RuleCreator.create(**rule_dict)\n    assert isinstance(rule, expected_rule_type)\n\n\ndef test_assign_existing_scope_var_raises(rule_lib):\n    rule = EndpointRule(\n        conditions=[\n            {\n                'fn': 'aws.parseArn',\n                'argv': ['{Bucket}'],\n                'assign': 'bucketArn',\n            },\n            {\n                'fn': 'aws.parseArn',\n                'argv': ['{Bucket}'],\n                'assign': 'bucketArn',\n            },\n        ],\n        endpoint={'url': 'foo.bar'},\n    )\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        rule.evaluate_conditions(\n            scope_vars={\n                'Bucket': 'arn:aws:s3:us-east-1:123456789012:mybucket'\n            },\n            rule_lib=rule_lib,\n        )\n    assert str(exc_info.value) == (\n        \"Assignment bucketArn already exists in \"\n        \"scoped variables and cannot be overwritten\"\n    )\n\n\ndef test_ruleset_unknown_parameter_type_raises(partitions):\n    with pytest.raises(EndpointResolutionError) as exc_info:\n        RuleSet(\n            version='1.0',\n            parameters={\n                'Bucket': {\"type\": \"list\"},\n            },\n            rules=[],\n            partitions=partitions,\n        )\n    assert \"Unknown parameter type: list.\" in str(exc_info.value)\n\n\n@pytest.fixture()\ndef empty_resolver():\n    return EndpointRulesetResolver(\n        endpoint_ruleset_data={\n            'version': '1.0',\n            'parameters': {},\n            'rules': [],\n        },\n        partition_data={},\n        service_model=None,\n        builtins={},\n        client_context=None,\n        event_emitter=None,\n        use_ssl=True,\n        requested_auth_scheme=None,\n    )\n\n\ndef test_auth_schemes_conversion_sigv4(empty_resolver):\n    auth_schemes = [\n        {\n            'name': 'sigv4',\n            'signingName': 'dynamodb',\n            'signingRegion': 'my-region-1',\n            'disableDoubleEncoding': True,\n            'otherParameter': 'otherValue',\n        }\n    ]\n    at, sc = empty_resolver.auth_schemes_to_signing_ctx(auth_schemes)\n    assert at == 'v4'\n    assert sc == {\n        'region': 'my-region-1',\n        'signing_name': 'dynamodb',\n        'disableDoubleEncoding': True,\n    }\n\n\n@requires_crt()\ndef test_auth_schemes_conversion_sigv4a_with_crt(monkeypatch, empty_resolver):\n    monkeypatch.setattr('botocore.regions.HAS_CRT', True)\n    auth_schemes = [\n        {'name': 'sigv4a', 'signingName': 's3', 'signingRegionSet': ['*']}\n    ]\n    at, sc = empty_resolver.auth_schemes_to_signing_ctx(auth_schemes)\n    assert at == 'v4a'\n    assert sc == {'region': '*', 'signing_name': 's3'}\n\n\ndef test_auth_schemes_conversion_sigv4a_without_crt(\n    monkeypatch, empty_resolver\n):\n    monkeypatch.setattr('botocore.regions.HAS_CRT', False)\n    monkeypatch.setattr('botocore.regions.AUTH_TYPE_MAPS', {})\n    auth_schemes = [\n        {'name': 'sigv4a', 'signingName': 's3', 'signingRegionSet': ['*']}\n    ]\n    with pytest.raises(MissingDependencyException):\n        empty_resolver.auth_schemes_to_signing_ctx(auth_schemes)\n\n\ndef test_auth_schemes_conversion_no_known_auth_types(empty_resolver):\n    auth_schemes = [\n        {'name': 'foo', 'signingName': 's3', 'signingRegion': 'ap-south-2'},\n        {'name': 'bar', 'otherParamKey': 'otherParamVal'},\n    ]\n    with pytest.raises(UnknownSignatureVersionError):\n        empty_resolver.auth_schemes_to_signing_ctx(auth_schemes)\n\n\ndef test_auth_schemes_conversion_first_authtype_unknown(\n    monkeypatch, empty_resolver\n):\n    monkeypatch.setattr('botocore.regions.HAS_CRT', False)\n    monkeypatch.setattr('botocore.regions.AUTH_TYPE_MAPS', {'bar': None})\n    auth_schemes = [\n        {'name': 'foo', 'signingName': 's3', 'signingRegion': 'ap-south-1'},\n        {'name': 'bar', 'signingName': 's3', 'signingRegion': 'ap-south-2'},\n    ]\n    at, sc = empty_resolver.auth_schemes_to_signing_ctx(auth_schemes)\n    assert at == 'bar'\n    assert sc == {'region': 'ap-south-2', 'signing_name': 's3'}\n\n\ndef test_endpoint_resolution_caches(endpoint_provider, monkeypatch):\n    mock_evaluate = Mock()\n    monkeypatch.setattr(RuleSet, \"evaluate\", mock_evaluate)\n    for _ in range(5):\n        endpoint_provider.resolve_endpoint(Region=\"us-east-2\")\n    mock_evaluate.assert_called_once_with({\"Region\": \"us-east-2\"})\n\n\ndef test_endpoint_reevaluates_result(endpoint_provider, monkeypatch):\n    regions = [\"us-east-1\", \"us-west-2\"]\n    mock_evaluate = Mock()\n    monkeypatch.setattr(RuleSet, \"evaluate\", mock_evaluate)\n    for region in regions:\n        endpoint_provider.resolve_endpoint(Region=region)\n    assert mock_evaluate.call_count == 2\n\n\n@pytest.mark.parametrize(\n    \"bucket, expected_value\",\n    [\n        (\"mybucket\", True),\n        (\"ab\", False),\n        (\"a.b\", True),\n        (\"my.great.bucket.aws.com\", True),\n        (\"mY.GREAT.bucket.aws.com\", False),\n        (\"192.168.1.1\", False),\n    ],\n)\ndef test_aws_is_virtual_hostable_s3_bucket_allow_subdomains(\n    rule_lib, bucket, expected_value\n):\n    assert (\n        rule_lib.aws_is_virtual_hostable_s3_bucket(bucket, True)\n        == expected_value\n    )\n", "tests/unit/test_loaders.py": "# Copyright (c) 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\nimport contextlib\nimport copy\nimport os\n\nfrom botocore.exceptions import DataNotFoundError, UnknownServiceError\nfrom botocore.loaders import (\n    ExtrasProcessor,\n    JSONFileLoader,\n    Loader,\n    create_loader,\n)\nfrom tests import BaseEnvVar, mock\n\n\nclass TestJSONFileLoader(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.data_path = os.path.join(os.path.dirname(__file__), 'data')\n        self.file_loader = JSONFileLoader()\n        self.valid_file_path = os.path.join(self.data_path, 'foo')\n        self.compressed_file_path = os.path.join(self.data_path, 'compressed')\n\n    def test_load_file(self):\n        data = self.file_loader.load_file(self.valid_file_path)\n        self.assertEqual(len(data), 3)\n        self.assertTrue('test_key_1' in data)\n\n    def test_load_compressed_file(self):\n        data = self.file_loader.load_file(self.compressed_file_path)\n        self.assertEqual(len(data), 3)\n        self.assertTrue('test_key_1' in data)\n\n    def test_load_compressed_file_exists_check(self):\n        self.assertTrue(self.file_loader.exists(self.compressed_file_path))\n\n    def test_load_json_file_does_not_exist_returns_none(self):\n        # None is used to indicate that the loader could not find a\n        # file to load.\n        self.assertIsNone(self.file_loader.load_file('fooasdfasdfasdf'))\n\n    def test_file_exists_check(self):\n        self.assertTrue(self.file_loader.exists(self.valid_file_path))\n\n    def test_file_does_not_exist_returns_false(self):\n        self.assertFalse(\n            self.file_loader.exists(\n                os.path.join(self.data_path, 'does', 'not', 'exist')\n            )\n        )\n\n    def test_file_with_non_ascii(self):\n        try:\n            filename = os.path.join(self.data_path, 'non_ascii')\n            self.assertTrue(self.file_loader.load_file(filename) is not None)\n        except UnicodeDecodeError:\n            self.fail('Fail to handle data file with non-ascii characters')\n\n\nclass TestLoader(BaseEnvVar):\n    def test_default_search_paths(self):\n        loader = Loader()\n        self.assertEqual(len(loader.search_paths), 2)\n        # We should also have ~/.aws/models added to\n        # the search path.  To deal with cross platform\n        # issues we'll just check for a path that ends\n        # with .aws/models.\n        home_dir_path = os.path.join('.aws', 'models')\n        self.assertTrue(\n            any(p.endswith(home_dir_path) for p in loader.search_paths)\n        )\n\n    def test_can_add_to_search_path(self):\n        loader = Loader()\n        loader.search_paths.append('mypath')\n        self.assertIn('mypath', loader.search_paths)\n\n    def test_can_initialize_with_search_paths(self):\n        loader = Loader(extra_search_paths=['foo', 'bar'])\n        # Note that the extra search paths are before\n        # the customer/builtin data paths.\n        self.assertEqual(\n            loader.search_paths,\n            [\n                'foo',\n                'bar',\n                loader.CUSTOMER_DATA_PATH,\n                loader.BUILTIN_DATA_PATH,\n            ],\n        )\n\n    # The file loader isn't consulted unless the current\n    # search path exists, so we're patching isdir to always\n    # say that a directory exists.\n    @mock.patch('os.path.isdir', mock.Mock(return_value=True))\n    def test_load_data_uses_loader(self):\n        search_paths = ['foo', 'bar', 'baz']\n\n        class FakeLoader:\n            def load_file(self, name):\n                expected_ending = os.path.join('bar', 'baz')\n                if name.endswith(expected_ending):\n                    return ['loaded data']\n\n        loader = Loader(\n            extra_search_paths=search_paths, file_loader=FakeLoader()\n        )\n        loaded = loader.load_data('baz')\n        self.assertEqual(loaded, ['loaded data'])\n\n    @mock.patch('os.path.isdir', mock.Mock(return_value=True))\n    def test_load_data_with_path(self):\n        search_paths = ['foo', 'bar', 'baz']\n\n        class FakeLoader:\n            def load_file(self, name):\n                expected_ending = os.path.join('bar', 'abc')\n                if name.endswith(expected_ending):\n                    return ['loaded data']\n\n        loader = Loader(\n            extra_search_paths=search_paths, file_loader=FakeLoader()\n        )\n        loaded, path = loader.load_data_with_path('abc')\n        self.assertEqual(loaded, ['loaded data'])\n        self.assertEqual(path, os.path.join('bar', 'abc'))\n\n    def test_data_not_found_raises_exception(self):\n        class FakeLoader:\n            def load_file(self, name):\n                # Returning None indicates that the\n                # loader couldn't find anything.\n                return None\n\n        loader = Loader(file_loader=FakeLoader())\n        with self.assertRaises(DataNotFoundError):\n            loader.load_data('baz')\n\n    def test_data_not_found_raises_exception_load_data_with_path(self):\n        class FakeLoader:\n            def load_file(self, name):\n                return None\n\n        loader = Loader(file_loader=FakeLoader())\n        with self.assertRaises(DataNotFoundError):\n            loader.load_data_with_path('baz')\n\n    @mock.patch('os.path.isdir', mock.Mock(return_value=True))\n    def test_error_raised_if_service_does_not_exist(self):\n        loader = Loader(\n            extra_search_paths=[], include_default_search_paths=False\n        )\n        with self.assertRaises(DataNotFoundError):\n            loader.determine_latest_version('unknownservice', 'service-2')\n\n    @mock.patch('os.path.isdir', mock.Mock(return_value=True))\n    def test_load_service_model(self):\n        class FakeLoader:\n            def load_file(self, name):\n                return ['loaded data']\n\n        loader = Loader(\n            extra_search_paths=['foo'],\n            file_loader=FakeLoader(),\n            include_default_search_paths=False,\n            include_default_extras=False,\n        )\n        loader.determine_latest_version = mock.Mock(return_value='2015-03-01')\n        loader.list_available_services = mock.Mock(return_value=['baz'])\n        loaded = loader.load_service_model('baz', type_name='service-2')\n        self.assertEqual(loaded, ['loaded data'])\n\n    @mock.patch('os.path.isdir', mock.Mock(return_value=True))\n    def test_load_service_model_enforces_case(self):\n        class FakeLoader:\n            def load_file(self, name):\n                return ['loaded data']\n\n        loader = Loader(\n            extra_search_paths=['foo'],\n            file_loader=FakeLoader(),\n            include_default_search_paths=False,\n        )\n        loader.determine_latest_version = mock.Mock(return_value='2015-03-01')\n        loader.list_available_services = mock.Mock(return_value=['baz'])\n\n        # Should have a) the unknown service name and b) list of valid\n        # service names.\n        with self.assertRaisesRegex(\n            UnknownServiceError, 'Unknown service.*BAZ.*baz'\n        ):\n            loader.load_service_model('BAZ', type_name='service-2')\n\n    def test_load_service_model_uses_provided_type_name(self):\n        loader = Loader(\n            extra_search_paths=['foo'],\n            file_loader=mock.Mock(),\n            include_default_search_paths=False,\n        )\n        loader.list_available_services = mock.Mock(return_value=['baz'])\n\n        # Should have a) the unknown service name and b) list of valid\n        # service names.\n        provided_type_name = 'not-service-2'\n        with self.assertRaisesRegex(\n            UnknownServiceError, 'Unknown service.*BAZ.*baz'\n        ):\n            loader.load_service_model('BAZ', type_name=provided_type_name)\n\n        loader.list_available_services.assert_called_with(provided_type_name)\n\n    def test_create_loader_parses_data_path(self):\n        search_path = os.pathsep.join(['foo', 'bar', 'baz'])\n        loader = create_loader(search_path)\n        self.assertIn('foo', loader.search_paths)\n        self.assertIn('bar', loader.search_paths)\n        self.assertIn('baz', loader.search_paths)\n\n    def test_is_builtin_path(self):\n        loader = Loader()\n        path_in_builtins = os.path.join(loader.BUILTIN_DATA_PATH, \"foo.txt\")\n        path_elsewhere = __file__\n        self.assertTrue(loader.is_builtin_path(path_in_builtins))\n        self.assertFalse(loader.is_builtin_path(path_elsewhere))\n\n\nclass TestMergeExtras(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.file_loader = mock.Mock()\n        self.data_loader = Loader(\n            extra_search_paths=['datapath'],\n            file_loader=self.file_loader,\n            include_default_search_paths=False,\n        )\n        self.data_loader.determine_latest_version = mock.Mock(\n            return_value='2015-03-01'\n        )\n        self.data_loader.list_available_services = mock.Mock(\n            return_value=['myservice']\n        )\n\n        isdir_mock = mock.Mock(return_value=True)\n        self.isdir_patch = mock.patch('os.path.isdir', isdir_mock)\n        self.isdir_patch.start()\n\n    def tearDown(self):\n        super().tearDown()\n        self.isdir_patch.stop()\n\n    def test_merge_extras(self):\n        service_data = {'foo': 'service', 'bar': 'service'}\n        sdk_extras = {'merge': {'foo': 'sdk'}}\n        self.file_loader.load_file.side_effect = [service_data, sdk_extras]\n\n        loaded = self.data_loader.load_service_model('myservice', 'service-2')\n        expected = {'foo': 'sdk', 'bar': 'service'}\n        self.assertEqual(loaded, expected)\n\n        call_args = self.file_loader.load_file.call_args_list\n        call_args = [c[0][0] for c in call_args]\n        base_path = os.path.join('datapath', 'myservice', '2015-03-01')\n        expected_call_args = [\n            os.path.join(base_path, 'service-2'),\n            os.path.join(base_path, 'service-2.sdk-extras'),\n        ]\n        self.assertEqual(call_args, expected_call_args)\n\n    def test_extras_not_found(self):\n        service_data = {'foo': 'service', 'bar': 'service'}\n        service_data_copy = copy.copy(service_data)\n        self.file_loader.load_file.side_effect = [service_data, None]\n\n        loaded = self.data_loader.load_service_model('myservice', 'service-2')\n        self.assertEqual(loaded, service_data_copy)\n\n    def test_no_merge_in_extras(self):\n        service_data = {'foo': 'service', 'bar': 'service'}\n        service_data_copy = copy.copy(service_data)\n        self.file_loader.load_file.side_effect = [service_data, {}]\n\n        loaded = self.data_loader.load_service_model('myservice', 'service-2')\n        self.assertEqual(loaded, service_data_copy)\n\n    def test_include_default_extras(self):\n        self.data_loader = Loader(\n            extra_search_paths=['datapath'],\n            file_loader=self.file_loader,\n            include_default_search_paths=False,\n            include_default_extras=False,\n        )\n        self.data_loader.determine_latest_version = mock.Mock(\n            return_value='2015-03-01'\n        )\n        self.data_loader.list_available_services = mock.Mock(\n            return_value=['myservice']\n        )\n\n        service_data = {'foo': 'service', 'bar': 'service'}\n        service_data_copy = copy.copy(service_data)\n        sdk_extras = {'merge': {'foo': 'sdk'}}\n        self.file_loader.load_file.side_effect = [service_data, sdk_extras]\n\n        loaded = self.data_loader.load_service_model('myservice', 'service-2')\n        self.assertEqual(loaded, service_data_copy)\n\n    def test_append_extra_type(self):\n        service_data = {'foo': 'service', 'bar': 'service'}\n        sdk_extras = {'merge': {'foo': 'sdk'}}\n        cli_extras = {'merge': {'cli': True}}\n        self.file_loader.load_file.side_effect = [\n            service_data,\n            sdk_extras,\n            cli_extras,\n        ]\n\n        self.data_loader.extras_types.append('cli')\n\n        loaded = self.data_loader.load_service_model('myservice', 'service-2')\n        expected = {'foo': 'sdk', 'bar': 'service', 'cli': True}\n        self.assertEqual(loaded, expected)\n\n        call_args = self.file_loader.load_file.call_args_list\n        call_args = [c[0][0] for c in call_args]\n        base_path = os.path.join('datapath', 'myservice', '2015-03-01')\n        expected_call_args = [\n            os.path.join(base_path, 'service-2'),\n            os.path.join(base_path, 'service-2.sdk-extras'),\n            os.path.join(base_path, 'service-2.cli-extras'),\n        ]\n        self.assertEqual(call_args, expected_call_args)\n\n    def test_sdk_empty_extras_skipped(self):\n        service_data = {'foo': 'service', 'bar': 'service'}\n        cli_extras = {'merge': {'foo': 'cli'}}\n        self.file_loader.load_file.side_effect = [\n            service_data,\n            None,\n            cli_extras,\n        ]\n\n        self.data_loader.extras_types.append('cli')\n\n        loaded = self.data_loader.load_service_model('myservice', 'service-2')\n        expected = {'foo': 'cli', 'bar': 'service'}\n        self.assertEqual(loaded, expected)\n\n\nclass TestExtrasProcessor(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.processor = ExtrasProcessor()\n        self.service_data = {\n            'shapes': {\n                'StringShape': {'type': 'string'},\n            }\n        }\n        self.service_data_copy = copy.deepcopy(self.service_data)\n\n    def test_process_empty_list(self):\n        self.processor.process(self.service_data, [])\n        self.assertEqual(self.service_data, self.service_data_copy)\n\n    def test_process_empty_extras(self):\n        self.processor.process(self.service_data, [{}])\n        self.assertEqual(self.service_data, self.service_data_copy)\n\n    def test_process_merge_key(self):\n        extras = {'merge': {'shapes': {'BooleanShape': {'type': 'boolean'}}}}\n        self.processor.process(self.service_data, [extras])\n        self.assertNotEqual(self.service_data, self.service_data_copy)\n\n        boolean_shape = self.service_data['shapes'].get('BooleanShape')\n        self.assertEqual(boolean_shape, {'type': 'boolean'})\n\n    def test_process_in_order(self):\n        extras = [\n            {'merge': {'shapes': {'BooleanShape': {'type': 'boolean'}}}},\n            {'merge': {'shapes': {'BooleanShape': {'type': 'string'}}}},\n        ]\n        self.processor.process(self.service_data, extras)\n        self.assertNotEqual(self.service_data, self.service_data_copy)\n\n        boolean_shape = self.service_data['shapes'].get('BooleanShape')\n        self.assertEqual(boolean_shape, {'type': 'string'})\n\n\nclass TestLoadersWithDirectorySearching(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.fake_directories = {}\n\n    def tearDown(self):\n        super().tearDown()\n\n    @contextlib.contextmanager\n    def loader_with_fake_dirs(self):\n        mock_file_loader = mock.Mock()\n        mock_file_loader.exists = self.fake_exists\n        search_paths = list(self.fake_directories)\n        loader = Loader(\n            extra_search_paths=search_paths,\n            include_default_search_paths=False,\n            file_loader=mock_file_loader,\n        )\n        with mock.patch('os.listdir', self.fake_listdir):\n            with mock.patch('os.path.isdir', mock.Mock(return_value=True)):\n                yield loader\n\n    def fake_listdir(self, dirname):\n        parts = dirname.split(os.path.sep)\n        result = self.fake_directories\n        while parts:\n            current = parts.pop(0)\n            result = result[current]\n        return list(result)\n\n    def fake_exists(self, path):\n        parts = path.split(os.sep)\n        result = self.fake_directories\n        while len(parts) > 1:\n            current = parts.pop(0)\n            result = result[current]\n        return parts[0] in result\n\n    def test_list_available_services(self):\n        self.fake_directories = {\n            'foo': {\n                'ec2': {\n                    '2010-01-01': ['service-2'],\n                    '2014-10-01': ['service-1'],\n                },\n                'dynamodb': {\n                    '2010-01-01': ['service-2'],\n                },\n            },\n            'bar': {\n                'ec2': {\n                    '2015-03-01': ['service-1'],\n                },\n                'rds': {\n                    # This will not show up in\n                    # list_available_services() for type\n                    # service-2 because it does not contains\n                    # a service-2.\n                    '2012-01-01': ['resource-1'],\n                },\n            },\n        }\n        with self.loader_with_fake_dirs() as loader:\n            self.assertEqual(\n                loader.list_available_services(type_name='service-2'),\n                ['dynamodb', 'ec2'],\n            )\n            self.assertEqual(\n                loader.list_available_services(type_name='resource-1'), ['rds']\n            )\n\n    def test_determine_latest(self):\n        # Fake mapping of directories to subdirectories.\n        # In this example, we can see that the 'bar' directory\n        # contains the latest EC2 API version, 2015-03-01,\n        # so loader.determine_latest('ec2') should return\n        # this value 2015-03-01.\n        self.fake_directories = {\n            'foo': {\n                'ec2': {\n                    '2010-01-01': ['service-2'],\n                    # This directory contains the latest API version\n                    # for EC2 because its the highest API directory\n                    # that contains a service-2.\n                    '2014-10-01': ['service-2'],\n                },\n            },\n            'bar': {\n                'ec2': {\n                    '2012-01-01': ['service-2'],\n                    # 2015-03-1 is *not* the latest for service-2,\n                    # because its directory only has service-1.json.\n                    '2015-03-01': ['service-1'],\n                },\n            },\n        }\n        with self.loader_with_fake_dirs() as loader:\n            loader.determine_latest_version('ec2', 'service-2')\n            self.assertEqual(\n                loader.determine_latest_version('ec2', 'service-2'),\n                '2014-10-01',\n            )\n            self.assertEqual(\n                loader.determine_latest_version('ec2', 'service-1'),\n                '2015-03-01',\n            )\n", "tests/unit/test_waiters.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport io\nimport os\n\nimport botocore\nfrom botocore.exceptions import ClientError, WaiterConfigError, WaiterError\nfrom botocore.loaders import Loader\nfrom botocore.model import ServiceModel\nfrom botocore.waiter import (\n    NormalizedOperationMethod,\n    SingleWaiterConfig,\n    Waiter,\n    WaiterModel,\n    create_waiter_with_client,\n)\nfrom tests import BaseEnvVar, mock, unittest\n\n\nclass TestWaiterModel(unittest.TestCase):\n    def setUp(self):\n        self.boiler_plate_config = {\n            'description': 'Waiter description',\n            'operation': 'HeadBucket',\n            'delay': 5,\n            'maxAttempts': 20,\n        }\n\n    def create_acceptor_function(self, for_config):\n        single_waiter = {'acceptors': [for_config]}\n        single_waiter.update(self.boiler_plate_config)\n        config = SingleWaiterConfig(single_waiter)\n        return config.acceptors[0].matcher_func\n\n    def test_waiter_version(self):\n        self.assertEqual(WaiterModel({'version': 2, 'waiters': {}}).version, 2)\n\n    def test_wont_load_missing_version_in_config(self):\n        # We only load waiter configs if we know for sure that we're\n        # loading version 2 of the format.\n        waiters = {\n            # Missing the 'version' key.\n            'waiters': {}\n        }\n        with self.assertRaises(WaiterConfigError):\n            WaiterModel(waiters)\n\n    def test_unsupported_waiter_version(self):\n        waiters = {'version': 1, 'waiters': {}}\n        with self.assertRaises(WaiterConfigError):\n            WaiterModel(waiters)\n\n    def test_waiter_names(self):\n        waiters = {\n            'version': 2,\n            'waiters': {\n                'BarWaiter': {},\n                'FooWaiter': {},\n            },\n        }\n        self.assertEqual(\n            WaiterModel(waiters).waiter_names, ['BarWaiter', 'FooWaiter']\n        )\n\n    def test_get_single_waiter_config(self):\n        single_waiter = {\n            'description': 'Waiter description',\n            'operation': 'HeadBucket',\n            'delay': 5,\n            'maxAttempts': 20,\n            'acceptors': [\n                {'state': 'success', 'matcher': 'status', 'expected': 200},\n                {'state': 'retry', 'matcher': 'status', 'expected': 404},\n            ],\n        }\n        waiters = {\n            'version': 2,\n            'waiters': {\n                'BucketExists': single_waiter,\n            },\n        }\n        model = WaiterModel(waiters)\n        config = model.get_waiter('BucketExists')\n        self.assertEqual(config.operation, 'HeadBucket')\n\n    def test_get_waiter_does_not_exist(self):\n        waiters = {'version': 2, 'waiters': {}}\n        model = WaiterModel(waiters)\n        with self.assertRaises(ValueError):\n            model.get_waiter('UnknownWaiter')\n\n    def test_single_waiter_config_attributes(self):\n        single_waiter = {\n            'description': 'Waiter description',\n            'operation': 'HeadBucket',\n            'delay': 5,\n            'maxAttempts': 20,\n            'acceptors': [],\n        }\n        config = SingleWaiterConfig(single_waiter)\n        self.assertEqual(config.description, 'Waiter description')\n        self.assertEqual(config.operation, 'HeadBucket')\n        self.assertEqual(config.delay, 5)\n        self.assertEqual(config.max_attempts, 20)\n\n    def test_single_waiter_acceptors_built_with_matcher_func(self):\n        # When the list of acceptors are requested, we actually will transform\n        # them into values that are easier to use.\n        single_waiter = {\n            'acceptors': [\n                {'state': 'success', 'matcher': 'status', 'expected': 200},\n            ],\n        }\n        single_waiter.update(self.boiler_plate_config)\n        config = SingleWaiterConfig(single_waiter)\n        success_acceptor = config.acceptors[0]\n\n        self.assertEqual(success_acceptor.state, 'success')\n        self.assertEqual(success_acceptor.matcher, 'status')\n        self.assertEqual(success_acceptor.expected, 200)\n        self.assertTrue(callable(success_acceptor.matcher_func))\n\n    def test_single_waiter_acceptor_matches_jmespath(self):\n        single_waiter = {\n            'acceptors': [\n                {\n                    'state': 'success',\n                    'matcher': 'path',\n                    'argument': 'Table.TableStatus',\n                    'expected': 'ACCEPTED',\n                },\n            ],\n        }\n        single_waiter.update(self.boiler_plate_config)\n        config = SingleWaiterConfig(single_waiter)\n        success_acceptor = config.acceptors[0].matcher_func\n        # success_acceptor is a callable that takes a response dict and returns\n        # True or False.\n        self.assertTrue(\n            success_acceptor({'Table': {'TableStatus': 'ACCEPTED'}})\n        )\n        self.assertFalse(\n            success_acceptor({'Table': {'TableStatus': 'CREATING'}})\n        )\n\n    def test_single_waiter_supports_status_code(self):\n        single_waiter = {\n            'acceptors': [\n                {'state': 'success', 'matcher': 'status', 'expected': 200}\n            ],\n        }\n        single_waiter.update(self.boiler_plate_config)\n        config = SingleWaiterConfig(single_waiter)\n        success_acceptor = config.acceptors[0].matcher_func\n        self.assertTrue(\n            success_acceptor({'ResponseMetadata': {'HTTPStatusCode': 200}})\n        )\n        self.assertFalse(\n            success_acceptor({'ResponseMetadata': {'HTTPStatusCode': 404}})\n        )\n\n    def test_single_waiter_supports_error(self):\n        single_waiter = {\n            'acceptors': [\n                {\n                    'state': 'success',\n                    'matcher': 'error',\n                    'expected': 'DoesNotExistError',\n                }\n            ],\n        }\n        single_waiter.update(self.boiler_plate_config)\n        config = SingleWaiterConfig(single_waiter)\n        success_acceptor = config.acceptors[0].matcher_func\n        self.assertTrue(\n            success_acceptor({'Error': {'Code': 'DoesNotExistError'}})\n        )\n        self.assertFalse(\n            success_acceptor({'Error': {'Code': 'DoesNotExistErorr'}})\n        )\n\n    def test_unknown_matcher(self):\n        unknown_type = 'arbitrary_type'\n        single_waiter = {\n            'acceptors': [\n                {\n                    'state': 'success',\n                    'matcher': unknown_type,\n                    'expected': 'foo',\n                }\n            ]\n        }\n        single_waiter.update(self.boiler_plate_config)\n        config = SingleWaiterConfig(single_waiter)\n        with self.assertRaises(WaiterConfigError):\n            config.acceptors\n\n    def test_single_waiter_supports_path_all(self):\n        matches = self.create_acceptor_function(\n            for_config={\n                'state': 'success',\n                'matcher': 'pathAll',\n                'argument': 'Tables[].State',\n                'expected': 'GOOD',\n            }\n        )\n        self.assertTrue(matches({'Tables': [{\"State\": \"GOOD\"}]}))\n        self.assertTrue(\n            matches({'Tables': [{\"State\": \"GOOD\"}, {\"State\": \"GOOD\"}]})\n        )\n\n    def test_single_waiter_supports_path_any(self):\n        matches = self.create_acceptor_function(\n            for_config={\n                'state': 'failure',\n                'matcher': 'pathAny',\n                'argument': 'Tables[].State',\n                'expected': 'FAIL',\n            }\n        )\n        self.assertTrue(matches({'Tables': [{\"State\": \"FAIL\"}]}))\n        self.assertTrue(\n            matches({'Tables': [{\"State\": \"GOOD\"}, {\"State\": \"FAIL\"}]})\n        )\n\n    def test_waiter_handles_error_responses_with_path_matchers(self):\n        path_any = self.create_acceptor_function(\n            for_config={\n                'state': 'success',\n                'matcher': 'pathAny',\n                'argument': 'length(Tables) > `0`',\n                'expected': True,\n            }\n        )\n        path_all = self.create_acceptor_function(\n            for_config={\n                'state': 'success',\n                'matcher': 'pathAll',\n                'argument': 'length(Tables) > `0`',\n                'expected': True,\n            }\n        )\n        path = self.create_acceptor_function(\n            for_config={\n                'state': 'success',\n                'matcher': 'path',\n                'argument': 'length(Tables) > `0`',\n                'expected': True,\n            }\n        )\n        self.assertFalse(path_any({'Error': {'Code': 'DoesNotExist'}}))\n        self.assertFalse(path_all({'Error': {'Code': 'DoesNotExist'}}))\n        self.assertFalse(path({'Error': {'Code': 'DoesNotExist'}}))\n\n    def test_single_waiter_does_not_match_path_all(self):\n        matches = self.create_acceptor_function(\n            for_config={\n                'state': 'success',\n                'matcher': 'pathAll',\n                'argument': 'Tables[].State',\n                'expected': 'GOOD',\n            }\n        )\n        self.assertFalse(\n            matches({'Tables': [{\"State\": \"GOOD\"}, {\"State\": \"BAD\"}]})\n        )\n        self.assertFalse(\n            matches({'Tables': [{\"State\": \"BAD\"}, {\"State\": \"GOOD\"}]})\n        )\n        self.assertFalse(\n            matches({'Tables': [{\"State\": \"BAD\"}, {\"State\": \"BAD\"}]})\n        )\n        self.assertFalse(matches({'Tables': []}))\n        self.assertFalse(\n            matches(\n                {\n                    'Tables': [\n                        {\"State\": \"BAD\"},\n                        {\"State\": \"BAD\"},\n                        {\"State\": \"BAD\"},\n                        {\"State\": \"BAD\"},\n                    ]\n                }\n            )\n        )\n\n    def test_path_all_missing_field(self):\n        matches = self.create_acceptor_function(\n            for_config={\n                'state': 'success',\n                'matcher': 'pathAll',\n                'argument': 'Tables[].State',\n                'expected': 'GOOD',\n            }\n        )\n        self.assertFalse(\n            matches({'Tables': [{\"NotState\": \"GOOD\"}, {\"NotState\": \"BAD\"}]})\n        )\n\n    def test_path_all_matcher_does_not_receive_list(self):\n        matches = self.create_acceptor_function(\n            for_config={\n                'state': 'success',\n                'matcher': 'pathAll',\n                'argument': 'Tables[].State',\n                'expected': 'GOOD',\n            }\n        )\n        self.assertFalse(matches({\"NotTables\": []}))\n\n    def test_single_waiter_supports_all_three_states(self):\n        single_waiter = {\n            'acceptors': [\n                {\n                    'state': 'success',\n                    'matcher': 'error',\n                    'expected': 'DoesNotExistError',\n                },\n                {'state': 'success', 'matcher': 'status', 'expected': 200},\n                {\n                    'state': 'success',\n                    'matcher': 'path',\n                    'argument': 'Foo.Bar',\n                    'expected': 'baz',\n                },\n            ],\n        }\n        single_waiter.update(self.boiler_plate_config)\n        config = SingleWaiterConfig(single_waiter)\n        acceptors = config.acceptors\n        # Each acceptors should be able to handle not matching\n        # any type of response.\n        matches_nothing = {}\n        self.assertFalse(acceptors[0].matcher_func(matches_nothing))\n        self.assertFalse(acceptors[1].matcher_func(matches_nothing))\n        self.assertFalse(acceptors[2].matcher_func(matches_nothing))\n\n\nclass TestWaitersObjects(unittest.TestCase):\n    def setUp(self):\n        pass\n\n    def client_responses_are(self, *responses, **kwargs):\n        operation_method = kwargs['for_operation']\n        operation_method.side_effect = responses\n        return operation_method\n\n    def create_waiter_config(\n        self, operation='MyOperation', delay=0, max_attempts=3, acceptors=None\n    ):\n        if acceptors is None:\n            # Create some arbitrary acceptor that will never match.\n            acceptors = [\n                {'state': 'success', 'matcher': 'status', 'expected': 1000}\n            ]\n        waiter_config = {\n            'operation': operation,\n            'delay': delay,\n            'maxAttempts': max_attempts,\n            'acceptors': acceptors,\n        }\n        config = SingleWaiterConfig(waiter_config)\n        return config\n\n    def test_waiter_waits_until_acceptor_matches(self):\n        config = self.create_waiter_config(\n            max_attempts=3,\n            acceptors=[\n                {\n                    'state': 'success',\n                    'matcher': 'path',\n                    'argument': 'Foo',\n                    'expected': 'SUCCESS',\n                }\n            ],\n        )\n        # Simulate the client having two calls that don't\n        # match followed by a third call that matches the\n        # acceptor.\n        operation_method = mock.Mock()\n        waiter = Waiter('MyWaiter', config, operation_method)\n        self.client_responses_are(\n            {'Foo': 'FAILURE'},\n            {'Foo': 'FAILURE'},\n            {'Foo': 'SUCCESS'},\n            for_operation=operation_method,\n        )\n        waiter.wait()\n        self.assertEqual(operation_method.call_count, 3)\n\n    def test_waiter_matches_with_invalid_error_response(self):\n        # Verify that the call will not raise WaiterError\n        # because of 'Error' key in success response.\n        config = self.create_waiter_config(\n            max_attempts=3,\n            acceptors=[\n                {\n                    'state': 'success',\n                    'matcher': 'path',\n                    'argument': 'Foo',\n                    'expected': 'SUCCESS',\n                }\n            ],\n        )\n        operation_method = mock.Mock()\n        waiter = Waiter('MyWaiter', config, operation_method)\n        self.client_responses_are(\n            {'Foo': 'SUCCESS', 'Error': 'foo'}, for_operation=operation_method\n        )\n        waiter.wait()\n        self.assertEqual(operation_method.call_count, 1)\n\n    def test_waiter_never_matches(self):\n        # Verify that a matcher will fail after max_attempts\n        # is exceeded.\n        config = self.create_waiter_config(max_attempts=3)\n        operation_method = mock.Mock()\n        self.client_responses_are(\n            {'Foo': 'FAILURE'},\n            {'Foo': 'FAILURE'},\n            {'Foo': 'FAILURE'},\n            for_operation=operation_method,\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n        with self.assertRaises(WaiterError):\n            waiter.wait()\n\n    def test_unspecified_errors_stops_waiter(self):\n        # If a waiter receives an error response, then the\n        # waiter immediately stops.\n        config = self.create_waiter_config()\n        operation_method = mock.Mock()\n        self.client_responses_are(\n            # This is an unknown error that's not called out\n            # in any of the waiter config, so when the\n            # waiter encounters this response it will transition\n            # to the failure state.\n            {'Error': {'Code': 'UnknownError', 'Message': 'bad error'}},\n            for_operation=operation_method,\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n        with self.assertRaises(WaiterError):\n            waiter.wait()\n\n    def test_last_response_available_on_waiter_error(self):\n        last_response = {\n            'Error': {'Code': 'UnknownError', 'Message': 'bad error'}\n        }\n        config = self.create_waiter_config()\n        operation_method = mock.Mock()\n        self.client_responses_are(\n            last_response, for_operation=operation_method\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n        with self.assertRaises(WaiterError) as e:\n            waiter.wait()\n        self.assertEqual(e.exception.last_response, last_response)\n\n    def test_unspecified_errors_propagate_error_code(self):\n        # If a waiter receives an error response, then the\n        # waiter should pass along the error code\n        config = self.create_waiter_config()\n        operation_method = mock.Mock()\n        error_code = 'error_message'\n        error_message = 'error_message'\n        self.client_responses_are(\n            # This is an unknown error that's not called out\n            # in any of the waiter config, so when the\n            # waiter encounters this response it will transition\n            # to the failure state.\n            {'Error': {'Code': error_code, 'Message': error_message}},\n            for_operation=operation_method,\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n\n        with self.assertRaisesRegex(WaiterError, error_message):\n            waiter.wait()\n\n    def _assert_failure_state_error_raised(\n        self, acceptors, responses, expected_msg\n    ):\n        config = self.create_waiter_config(acceptors=acceptors)\n        operation_method = mock.Mock()\n        waiter = Waiter('MyWaiter', config, operation_method)\n        self.client_responses_are(*responses, for_operation=operation_method)\n        with self.assertRaisesRegex(WaiterError, expected_msg):\n            waiter.wait()\n\n    def test_waiter_failure_state_error(self):\n        test_cases = [\n            (\n                [\n                    {\n                        'state': 'failure',\n                        'matcher': 'path',\n                        'argument': 'Foo',\n                        'expected': 'FAILURE',\n                    }\n                ],\n                [{'Foo': 'FAILURE'}],\n                'FAILURE',\n            ),\n            (\n                [\n                    {\n                        'state': 'failure',\n                        'matcher': 'pathAll',\n                        'argument': 'Tables[].State',\n                        'expected': 'FAILURE',\n                    }\n                ],\n                [{'Tables': [{\"State\": \"FAILURE\"}]}],\n                'FAILURE',\n            ),\n            (\n                [\n                    {\n                        'state': 'failure',\n                        'matcher': 'pathAny',\n                        'argument': 'Tables[].State',\n                        'expected': 'FAILURE',\n                    }\n                ],\n                [{'Tables': [{\"State\": \"FAILURE\"}]}],\n                'FAILURE',\n            ),\n            (\n                [{'state': 'failure', 'matcher': 'status', 'expected': 404}],\n                [{'ResponseMetadata': {'HTTPStatusCode': 404}}],\n                '404',\n            ),\n            (\n                [\n                    {\n                        'state': 'failure',\n                        'matcher': 'error',\n                        'expected': 'FailError',\n                    }\n                ],\n                [{'Error': {'Code': 'FailError', 'Message': 'foo'}}],\n                'FailError',\n            ),\n            (\n                [\n                    {\n                        'state': 'retry',\n                        'matcher': 'error',\n                        'expected': 'RetryMe',\n                    }\n                ],\n                [{'Success': False}] * 4,\n                'Max attempts exceeded',\n            ),\n            (\n                [\n                    {'state': 'success', 'matcher': 'status', 'expected': 200},\n                    {\n                        'state': 'retry',\n                        'matcher': 'error',\n                        'expected': 'RetryMe',\n                    },\n                ],\n                [\n                    {'Success': False},\n                    {'Error': {'Code': 'RetryMe', 'Message': 'foo'}},\n                    {'Success': False},\n                    {'Success': False},\n                ],\n                'Previously accepted state',\n            ),\n        ]\n\n        for acceptors, responses, expected_msg in test_cases:\n            self._assert_failure_state_error_raised(\n                acceptors, responses, expected_msg\n            )\n\n    def test_waiter_transitions_to_failure_state(self):\n        acceptors = [\n            # A success state that will never be hit.\n            {'state': 'success', 'matcher': 'status', 'expected': 1000},\n            {'state': 'failure', 'matcher': 'error', 'expected': 'FailError'},\n        ]\n        config = self.create_waiter_config(acceptors=acceptors)\n        operation_method = mock.Mock()\n        self.client_responses_are(\n            {'Nothing': 'foo'},\n            # And on the second attempt, a FailError is seen, which\n            # causes the waiter to fail fast.\n            {'Error': {'Code': 'FailError', 'Message': 'foo'}},\n            {'WillNeverGetCalled': True},\n            for_operation=operation_method,\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n        with self.assertRaises(WaiterError):\n            waiter.wait()\n        # Not only should we raise an exception, but we should have\n        # only called the operation_method twice because the second\n        # response triggered a fast fail.\n        self.assertEqual(operation_method.call_count, 2)\n\n    def test_waiter_handles_retry_state(self):\n        acceptor_with_retry_state = [\n            {'state': 'success', 'matcher': 'status', 'expected': 200},\n            {'state': 'retry', 'matcher': 'error', 'expected': 'RetryMe'},\n        ]\n        config = self.create_waiter_config(acceptors=acceptor_with_retry_state)\n        operation_method = mock.Mock()\n        self.client_responses_are(\n            {'Nothing': 'foo'},\n            {'Error': {'Code': 'RetryMe', 'Message': 'foo'}},\n            {'Success': True, 'ResponseMetadata': {'HTTPStatusCode': 200}},\n            {'NeverCalled': True},\n            for_operation=operation_method,\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n        waiter.wait()\n        self.assertEqual(operation_method.call_count, 3)\n\n    def test_kwargs_are_passed_through(self):\n        acceptors = [\n            {'state': 'success', 'matcher': 'error', 'expected': 'MyError'},\n        ]\n        config = self.create_waiter_config(acceptors=acceptors)\n        operation_method = mock.Mock()\n        self.client_responses_are(\n            {'Error': {'Code': 'MyError'}}, for_operation=operation_method\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n        waiter.wait(Foo='foo', Bar='bar', Baz='baz')\n\n        operation_method.assert_called_with(Foo='foo', Bar='bar', Baz='baz')\n\n    @mock.patch('time.sleep')\n    def test_waiter_honors_delay_time_between_retries(self, sleep_mock):\n        delay_time = 5\n        config = self.create_waiter_config(delay=delay_time)\n        operation_method = mock.Mock()\n        self.client_responses_are(\n            # This is an unknown error that's not called out\n            # in any of the waiter config, so when the\n            # waiter encounters this response it will transition\n            # to the failure state.\n            {'Success': False},\n            {'Success': False},\n            {'Success': False},\n            for_operation=operation_method,\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n        with self.assertRaises(WaiterError):\n            waiter.wait()\n\n        # We attempt three times, which means we need to sleep\n        # twice, once before each subsequent request.\n        self.assertEqual(sleep_mock.call_count, 2)\n        sleep_mock.assert_called_with(delay_time)\n\n    @mock.patch('time.sleep')\n    def test_waiter_invocation_config_honors_delay(self, sleep_mock):\n        config = self.create_waiter_config()\n        operation_method = mock.Mock()\n        self.client_responses_are(\n            {'Success': False},\n            {'Success': False},\n            {'Success': False},\n            for_operation=operation_method,\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n        custom_delay = 3\n        with self.assertRaises(WaiterError):\n            waiter.wait(WaiterConfig={'Delay': custom_delay})\n\n        # We attempt three times, which means we need to sleep\n        # twice, once before each subsequent request.\n        self.assertEqual(sleep_mock.call_count, 2)\n        sleep_mock.assert_called_with(custom_delay)\n\n    def test_waiter_invocation_config_honors_max_attempts(self):\n        config = self.create_waiter_config()\n        operation_method = mock.Mock()\n        self.client_responses_are(\n            {'Success': False},\n            {'Success': False},\n            for_operation=operation_method,\n        )\n        waiter = Waiter('MyWaiter', config, operation_method)\n        custom_max = 2\n        with self.assertRaises(WaiterError):\n            waiter.wait(WaiterConfig={'MaxAttempts': custom_max})\n\n        self.assertEqual(operation_method.call_count, 2)\n\n\nclass TestCreateWaiter(unittest.TestCase):\n    def setUp(self):\n        self.waiter_config = {\n            'version': 2,\n            'waiters': {\n                'WaiterName': {\n                    'operation': 'Foo',\n                    'delay': 1,\n                    'maxAttempts': 1,\n                    'acceptors': [],\n                },\n            },\n        }\n        self.waiter_model = WaiterModel(self.waiter_config)\n        self.service_json_model = {\n            'metadata': {'serviceFullName': 'Amazon MyService'},\n            'operations': {\n                'Foo': {\n                    'name': 'Foo',\n                    'input': {'shape': 'FooInputOutput'},\n                    'output': {'shape': 'FooInputOutput'},\n                }\n            },\n            'shapes': {\n                'FooInputOutput': {\n                    'type': 'structure',\n                    'members': {\n                        'bar': {\n                            'shape': 'String',\n                            'documentation': 'Documents bar',\n                        }\n                    },\n                },\n                'String': {'type': 'string'},\n            },\n        }\n        self.service_model = ServiceModel(self.service_json_model, 'myservice')\n        self.client = mock.Mock()\n        self.client.meta.service_model = self.service_model\n\n    def test_can_create_waiter_from_client(self):\n        waiter_name = 'WaiterName'\n        waiter = create_waiter_with_client(\n            waiter_name, self.waiter_model, self.client\n        )\n        self.assertIsInstance(waiter, Waiter)\n\n    def test_waiter_class_name(self):\n        waiter_name = 'WaiterName'\n        waiter = create_waiter_with_client(\n            waiter_name, self.waiter_model, self.client\n        )\n        self.assertEqual(\n            waiter.__class__.__name__, 'MyService.Waiter.WaiterName'\n        )\n\n    def test_waiter_help_documentation(self):\n        waiter_name = 'WaiterName'\n        waiter = create_waiter_with_client(\n            waiter_name, self.waiter_model, self.client\n        )\n        with mock.patch('sys.stdout', io.StringIO()) as mock_stdout:\n            help(waiter.wait)\n        content = mock_stdout.getvalue()\n        lines = [\n            (\n                '    Polls :py:meth:`MyService.Client.foo` every 1 '\n                'seconds until a successful state is reached. An error '\n                'is returned after 1 failed checks.'\n            ),\n            '    **Request Syntax**',\n            '    ::',\n            '      waiter.wait(',\n            \"          bar='string'\",\n            '      )',\n            '    :type bar: string',\n            '    :param bar: Documents bar',\n            '    :returns: None',\n        ]\n        for line in lines:\n            self.assertIn(line, content)\n\n\nclass TestOperationMethods(unittest.TestCase):\n    def test_normalized_op_method_makes_call(self):\n        client_method = mock.Mock()\n        op = NormalizedOperationMethod(client_method)\n        op(Foo='a', Bar='b')\n\n        client_method.assert_called_with(Foo='a', Bar='b')\n\n    def test_normalized_op_returns_error_response(self):\n        # Client objects normally throw exceptions when an error\n        # occurs, but we need to return the parsed error response.\n        client_method = mock.Mock()\n        op = NormalizedOperationMethod(client_method)\n        parsed_response = {'Error': {'Code': 'Foo', 'Message': 'bar'}}\n        exception = ClientError(parsed_response, 'OperationName')\n        client_method.side_effect = exception\n        actual_response = op(Foo='a', Bar='b')\n        self.assertEqual(actual_response, parsed_response)\n\n\nclass ServiceWaiterFunctionalTest(BaseEnvVar):\n    \"\"\"\n    This class is used as a base class if you want to functionally test the\n    waiters for a specific service.\n    \"\"\"\n\n    def setUp(self):\n        super().setUp()\n        self.data_path = os.path.join(\n            os.path.dirname(botocore.__file__), 'data'\n        )\n        self.environ['AWS_DATA_PATH'] = self.data_path\n        self.loader = Loader([self.data_path])\n\n    def get_waiter_model(self, service, api_version=None):\n        \"\"\"Get the waiter model for the service.\"\"\"\n        with mock.patch(\n            'botocore.loaders.Loader.list_available_services',\n            return_value=[service],\n        ):\n            return WaiterModel(\n                self.loader.load_service_model(\n                    service, type_name='waiters-2', api_version=api_version\n                )\n            )\n\n    def get_service_model(self, service, api_version=None):\n        \"\"\"Get the service model for the service.\"\"\"\n        with mock.patch(\n            'botocore.loaders.Loader.list_available_services',\n            return_value=[service],\n        ):\n            return ServiceModel(\n                self.loader.load_service_model(\n                    service, type_name='service-2', api_version=api_version\n                ),\n                service_name=service,\n            )\n\n\nclass CloudFrontWaitersTest(ServiceWaiterFunctionalTest):\n    def setUp(self):\n        super().setUp()\n        self.client = mock.Mock()\n        self.service = 'cloudfront'\n        self.old_api_versions = ['2014-05-31']\n\n    def assert_distribution_deployed_call_count(self, api_version=None):\n        waiter_name = 'DistributionDeployed'\n        waiter_model = self.get_waiter_model(self.service, api_version)\n        self.client.meta.service_model = self.get_service_model(\n            self.service, api_version\n        )\n        self.client.get_distribution.side_effect = [\n            {'Distribution': {'Status': 'Deployed'}}\n        ]\n        waiter = create_waiter_with_client(\n            waiter_name, waiter_model, self.client\n        )\n        waiter.wait()\n        self.assertEqual(self.client.get_distribution.call_count, 1)\n\n    def assert_invalidation_completed_call_count(self, api_version=None):\n        waiter_name = 'InvalidationCompleted'\n        waiter_model = self.get_waiter_model(self.service, api_version)\n        self.client.meta.service_model = self.get_service_model(\n            self.service, api_version\n        )\n        self.client.get_invalidation.side_effect = [\n            {'Invalidation': {'Status': 'Completed'}}\n        ]\n        waiter = create_waiter_with_client(\n            waiter_name, waiter_model, self.client\n        )\n        waiter.wait()\n        self.assertEqual(self.client.get_invalidation.call_count, 1)\n\n    def assert_streaming_distribution_deployed_call_count(\n        self, api_version=None\n    ):\n        waiter_name = 'StreamingDistributionDeployed'\n        waiter_model = self.get_waiter_model(self.service, api_version)\n        self.client.meta.service_model = self.get_service_model(\n            self.service, api_version\n        )\n        self.client.get_streaming_distribution.side_effect = [\n            {'StreamingDistribution': {'Status': 'Deployed'}}\n        ]\n        waiter = create_waiter_with_client(\n            waiter_name, waiter_model, self.client\n        )\n        waiter.wait()\n        self.assertEqual(self.client.get_streaming_distribution.call_count, 1)\n\n    def test_distribution_deployed(self):\n        # Test the latest version.\n        self.assert_distribution_deployed_call_count()\n        self.client.reset_mock()\n\n        # Test previous api versions.\n        for api_version in self.old_api_versions:\n            self.assert_distribution_deployed_call_count(api_version)\n            self.client.reset_mock()\n\n    def test_invalidation_completed(self):\n        # Test the latest version.\n        self.assert_invalidation_completed_call_count()\n        self.client.reset_mock()\n\n        # Test previous api versions.\n        for api_version in self.old_api_versions:\n            self.assert_invalidation_completed_call_count(api_version)\n            self.client.reset_mock()\n\n    def test_streaming_distribution_deployed(self):\n        # Test the latest version.\n        self.assert_streaming_distribution_deployed_call_count()\n        self.client.reset_mock()\n\n        # Test previous api versions.\n        for api_version in self.old_api_versions:\n            self.assert_streaming_distribution_deployed_call_count(api_version)\n            self.client.reset_mock()\n", "tests/unit/test_awsrequest.py": "#!/usr/bin/env\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport io\nimport os\nimport shutil\nimport socket\nimport tempfile\n\nimport pytest\nfrom urllib3.connectionpool import HTTPConnectionPool, HTTPSConnectionPool\n\nfrom botocore.awsrequest import (\n    AWSHTTPConnection,\n    AWSHTTPSConnection,\n    AWSRequest,\n    AWSResponse,\n    HeadersDict,\n    create_request_object,\n    prepare_request_dict,\n)\nfrom botocore.compat import file_type\nfrom botocore.exceptions import UnseekableStreamError\nfrom tests import mock, unittest\n\n\nclass IgnoreCloseBytesIO(io.BytesIO):\n    def close(self):\n        pass\n\n\nclass FakeSocket:\n    def __init__(self, read_data, fileclass=IgnoreCloseBytesIO):\n        self.sent_data = b''\n        self.read_data = read_data\n        self.fileclass = fileclass\n        self._fp_object = None\n\n    def sendall(self, data):\n        self.sent_data += data\n\n    def makefile(self, mode, bufsize=None):\n        if self._fp_object is None:\n            self._fp_object = self.fileclass(self.read_data)\n        return self._fp_object\n\n    def close(self):\n        pass\n\n    def settimeout(self, value):\n        pass\n\n\nclass BytesIOWithLen(io.BytesIO):\n    def __len__(self):\n        return len(self.getvalue())\n\n\nclass Unseekable(file_type):\n    def __init__(self, stream):\n        self._stream = stream\n\n    def read(self):\n        return self._stream.read()\n\n    def seek(self, offset, whence):\n        # This is a case where seek() exists as part of the object's interface,\n        # but it doesn't actually work (for example socket.makefile(), which\n        # will raise an io.* error on python3).\n        raise ValueError(\"Underlying stream does not support seeking.\")\n\n\nclass Seekable:\n    \"\"\"This class represents a bare-bones,seekable file-like object\n\n    Note it does not include some of the other attributes of other\n    file-like objects such as StringIO's getvalue() and file object's fileno\n    property. If the file-like object does not have either of these attributes\n    requests will not calculate the content length even though it is still\n    possible to calculate it.\n    \"\"\"\n\n    def __init__(self, stream):\n        self._stream = stream\n\n    def __iter__(self):\n        return iter(self._stream)\n\n    def read(self):\n        return self._stream.read()\n\n    def seek(self, offset, whence=0):\n        self._stream.seek(offset, whence)\n\n    def tell(self):\n        return self._stream.tell()\n\n\nclass TestAWSRequest(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'foo')\n        self.request = AWSRequest(method='GET', url='http://example.com')\n        self.prepared_request = self.request.prepare()\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def test_prepared_request_repr(self):\n        expected_repr = (\n            '<AWSPreparedRequest stream_output=False, method=GET, '\n            'url=http://example.com, headers={}>'\n        )\n        request_repr = repr(self.prepared_request)\n        self.assertEqual(request_repr, expected_repr)\n\n    def test_can_prepare_url_params(self):\n        request = AWSRequest(url='http://example.com/', params={'foo': 'bar'})\n        prepared_request = request.prepare()\n        self.assertEqual(prepared_request.url, 'http://example.com/?foo=bar')\n\n    def test_can_prepare_url_params_with_existing_query(self):\n        request = AWSRequest(\n            url='http://example.com/?bar=foo', params={'foo': 'bar'}\n        )\n        prepared_request = request.prepare()\n        self.assertEqual(\n            prepared_request.url, 'http://example.com/?bar=foo&foo=bar'\n        )\n\n    def test_can_prepare_dict_body(self):\n        body = {'dead': 'beef'}\n        request = AWSRequest(url='http://example.com/', data=body)\n        prepared_request = request.prepare()\n        self.assertEqual(prepared_request.body, 'dead=beef')\n\n    def test_can_prepare_dict_body_unicode_values(self):\n        body = {'Text': '\\u30c6\\u30b9\\u30c8 string'}\n        expected_body = 'Text=%E3%83%86%E3%82%B9%E3%83%88+string'\n        request = AWSRequest(url='http://example.com/', data=body)\n        prepared_request = request.prepare()\n        self.assertEqual(prepared_request.body, expected_body)\n\n    def test_can_prepare_dict_body_unicode_keys(self):\n        body = {'\\u30c6\\u30b9\\u30c8': 'string'}\n        expected_body = '%E3%83%86%E3%82%B9%E3%83%88=string'\n        request = AWSRequest(url='http://example.com/', data=body)\n        prepared_request = request.prepare()\n        self.assertEqual(prepared_request.body, expected_body)\n\n    def test_can_prepare_empty_body(self):\n        request = AWSRequest(url='http://example.com/', data=b'')\n        prepared_request = request.prepare()\n        self.assertEqual(prepared_request.body, None)\n        content_length = prepared_request.headers.get('content-length')\n        self.assertEqual(content_length, '0')\n\n    def test_request_body_is_prepared(self):\n        request = AWSRequest(url='http://example.com/', data='body')\n        self.assertEqual(request.body, b'body')\n\n    def test_prepare_body_content_adds_content_length(self):\n        content = b'foobarbaz'\n        expected_len = str(len(content))\n        with open(self.filename, 'wb') as f:\n            f.write(content)\n        with open(self.filename, 'rb') as f:\n            data = Seekable(f)\n            self.request.data = data\n            self.request.method = 'POST'\n            prepared_request = self.request.prepare()\n            calculated_len = prepared_request.headers['Content-Length']\n            self.assertEqual(calculated_len, expected_len)\n\n    def test_prepare_body_doesnt_override_content_length(self):\n        self.request.method = 'PUT'\n        self.request.headers['Content-Length'] = '20'\n        self.request.data = b'asdf'\n        prepared_request = self.request.prepare()\n        self.assertEqual(prepared_request.headers['Content-Length'], '20')\n\n    def test_prepare_body_doesnt_set_content_length_head(self):\n        self.request.method = 'HEAD'\n        self.request.data = b'thisshouldntbehere'\n        prepared_request = self.request.prepare()\n        self.assertEqual(prepared_request.headers.get('Content-Length'), None)\n\n    def test_prepare_body_doesnt_set_content_length_get(self):\n        self.request.method = 'GET'\n        self.request.data = b'thisshouldntbehere'\n        prepared_request = self.request.prepare()\n        self.assertEqual(prepared_request.headers.get('Content-Length'), None)\n\n    def test_prepare_body_doesnt_set_content_length_options(self):\n        self.request.method = 'OPTIONS'\n        self.request.data = b'thisshouldntbehere'\n        prepared_request = self.request.prepare()\n        self.assertEqual(prepared_request.headers.get('Content-Length'), None)\n\n    def test_can_reset_stream_handles_binary(self):\n        contents = b'notastream'\n        self.prepared_request.body = contents\n        self.prepared_request.reset_stream()\n        # assert the request body doesn't change after reset_stream is called\n        self.assertEqual(self.prepared_request.body, contents)\n\n    def test_can_reset_stream_handles_bytearray(self):\n        contents = bytearray(b'notastream')\n        self.prepared_request.body = contents\n        self.prepared_request.reset_stream()\n        # assert the request body doesn't change after reset_stream is called\n        self.assertEqual(self.prepared_request.body, contents)\n\n    def test_can_reset_stream(self):\n        contents = b'foobarbaz'\n        with open(self.filename, 'wb') as f:\n            f.write(contents)\n        with open(self.filename, 'rb') as body:\n            self.prepared_request.body = body\n            # pretend the request body was partially sent\n            body.read()\n            self.assertNotEqual(body.tell(), 0)\n            # have the prepared request reset its stream\n            self.prepared_request.reset_stream()\n            # the stream should be reset\n            self.assertEqual(body.tell(), 0)\n\n    def test_cannot_reset_stream_raises_error(self):\n        contents = b'foobarbaz'\n        with open(self.filename, 'wb') as f:\n            f.write(contents)\n        with open(self.filename, 'rb') as body:\n            self.prepared_request.body = Unseekable(body)\n            # pretend the request body was partially sent\n            body.read()\n            self.assertNotEqual(body.tell(), 0)\n            # reset stream should fail\n            with self.assertRaises(UnseekableStreamError):\n                self.prepared_request.reset_stream()\n\n    def test_duck_type_for_file_check(self):\n        # As part of determining whether or not we can rewind a stream\n        # we first need to determine if the thing is a file like object.\n        # We should not be using an isinstance check.  Instead, we should\n        # be using duck type checks.\n\n        class LooksLikeFile:\n            def __init__(self):\n                self.seek_called = False\n\n            def read(self, amount=None):\n                pass\n\n            def seek(self, where):\n                self.seek_called = True\n\n        looks_like_file = LooksLikeFile()\n        self.prepared_request.body = looks_like_file\n        self.prepared_request.reset_stream()\n        # The stream should now be reset.\n        self.assertTrue(looks_like_file.seek_called)\n\n\n@pytest.mark.parametrize(\n    \"test_input,expected_output\",\n    [\n        ([('foo', None)], 'http://example.com/?foo=None'),\n        ([(None, None)], 'http://example.com/?None=None'),\n        (\n            [('foo', 'bar'), ('foo', 'baz'), ('fizz', 'buzz')],\n            'http://example.com/?foo=bar&foo=baz&fizz=buzz',\n        ),\n        (\n            [('foo', 'bar'), ('foo', None)],\n            'http://example.com/?foo=bar&foo=None',\n        ),\n        ([('foo', 'bar')], 'http://example.com/?foo=bar'),\n        (\n            [('foo', 'bar'), ('foo', 'bar'), ('fizz', 'buzz')],\n            'http://example.com/?foo=bar&foo=bar&fizz=buzz',\n        ),\n        ([('', 'bar')], 'http://example.com/?=bar'),\n        ([(1, 'bar')], 'http://example.com/?1=bar'),\n    ],\n)\ndef test_can_use_list_tuples_for_params(test_input, expected_output):\n    request = AWSRequest(url='http://example.com/', params=test_input)\n    prepared_request = request.prepare()\n    assert prepared_request.url == expected_output\n\n\ndef test_empty_list_tuples_value_error_for_params():\n    request = AWSRequest(url='http://example.com/', params=[()])\n    with pytest.raises(ValueError):\n        request.prepare()\n\n\nclass TestAWSResponse(unittest.TestCase):\n    def setUp(self):\n        self.response = AWSResponse('http://url.com', 200, HeadersDict(), None)\n        self.response.raw = mock.Mock()\n\n    def set_raw_stream(self, blobs):\n        def stream(*args, **kwargs):\n            yield from blobs\n\n        self.response.raw.stream.return_value = stream()\n\n    def test_content_property(self):\n        self.set_raw_stream([b'some', b'data'])\n        self.assertEqual(self.response.content, b'somedata')\n        self.assertEqual(self.response.content, b'somedata')\n        # assert that stream was not called more than once\n        self.assertEqual(self.response.raw.stream.call_count, 1)\n\n    def test_text_property(self):\n        self.set_raw_stream([b'\\xe3\\x82\\xb8\\xe3\\x83\\xa7\\xe3\\x82\\xb0'])\n        self.response.headers['content-type'] = 'text/plain; charset=utf-8'\n        self.assertEqual(self.response.text, '\\u30b8\\u30e7\\u30b0')\n\n    def test_text_property_defaults_utf8(self):\n        self.set_raw_stream([b'\\xe3\\x82\\xb8\\xe3\\x83\\xa7\\xe3\\x82\\xb0'])\n        self.assertEqual(self.response.text, '\\u30b8\\u30e7\\u30b0')\n\n\nclass TestAWSHTTPConnection(unittest.TestCase):\n    def create_tunneled_connection(self, url, port, response):\n        s = FakeSocket(response)\n        conn = AWSHTTPConnection(url, port)\n        conn.sock = s\n        conn._tunnel_host = url\n        conn._tunnel_port = port\n        conn._tunnel_headers = {'key': 'value'}\n\n        # Create a mock response.\n        self.mock_response = mock.Mock()\n        self.mock_response.fp = mock.Mock()\n\n        # Imitate readline function by creating a list to be sent as\n        # a side effect of the mocked readline to be able to track how the\n        # response is processed in ``_tunnel()``.\n        delimeter = b'\\r\\n'\n        side_effect = []\n        response_components = response.split(delimeter)\n        for i in range(len(response_components)):\n            new_component = response_components[i]\n            # Only add the delimeter on if it is not the last component\n            # which should be an empty string.\n            if i != len(response_components) - 1:\n                new_component += delimeter\n            side_effect.append(new_component)\n\n        self.mock_response.fp.readline.side_effect = side_effect\n\n        response_components = response.split(b' ')\n        self.mock_response._read_status.return_value = (\n            response_components[0],\n            int(response_components[1]),\n            response_components[2],\n        )\n        conn.response_class = mock.Mock()\n        conn.response_class.return_value = self.mock_response\n        return conn\n\n    def test_expect_100_continue_returned(self):\n        with mock.patch('urllib3.util.wait_for_read') as wait_mock:\n            # Shows the server first sending a 100 continue response\n            # then a 200 ok response.\n            s = FakeSocket(b'HTTP/1.1 100 Continue\\r\\n\\r\\nHTTP/1.1 200 OK\\r\\n')\n            conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n            conn.sock = s\n            wait_mock.return_value = True\n            conn.request(\n                'GET', '/bucket/foo', b'body', {'Expect': b'100-continue'}\n            )\n            response = conn.getresponse()\n            # Assert that we waited for the 100-continue response\n            self.assertEqual(wait_mock.call_count, 1)\n            # Now we should verify that our final response is the 200 OK\n            self.assertEqual(response.status, 200)\n\n    def test_handles_expect_100_with_different_reason_phrase(self):\n        with mock.patch('urllib3.util.wait_for_read') as wait_mock:\n            # Shows the server first sending a 100 continue response\n            # then a 200 ok response.\n            s = FakeSocket(\n                b'HTTP/1.1 100 (Continue)\\r\\n\\r\\nHTTP/1.1 200 OK\\r\\n'\n            )\n            conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n            conn.sock = s\n            wait_mock.return_value = True\n            conn.request(\n                'GET',\n                '/bucket/foo',\n                io.BytesIO(b'body'),\n                {'Expect': b'100-continue', 'Content-Length': b'4'},\n            )\n            response = conn.getresponse()\n            # Now we should verify that our final response is the 200 OK.\n            self.assertEqual(response.status, 200)\n            # Assert that we waited for the 100-continue response\n            self.assertEqual(wait_mock.call_count, 1)\n            # Verify that we went the request body because we got a 100\n            # continue.\n            self.assertIn(b'body', s.sent_data)\n\n    def test_expect_100_sends_connection_header(self):\n        # When using squid as an HTTP proxy, it will also send\n        # a Connection: keep-alive header back with the 100 continue\n        # response.  We need to ensure we handle this case.\n        with mock.patch('urllib3.util.wait_for_read') as wait_mock:\n            # Shows the server first sending a 100 continue response\n            # then a 500 response.  We're picking 500 to confirm we\n            # actually parse the response instead of getting the\n            # default status of 200 which happens when we can't parse\n            # the response.\n            s = FakeSocket(\n                b'HTTP/1.1 100 Continue\\r\\n'\n                b'Connection: keep-alive\\r\\n'\n                b'\\r\\n'\n                b'HTTP/1.1 500 Internal Service Error\\r\\n'\n            )\n            conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n            conn.sock = s\n            wait_mock.return_value = True\n            conn.request(\n                'GET', '/bucket/foo', b'body', {'Expect': b'100-continue'}\n            )\n            # Assert that we waited for the 100-continue response\n            self.assertEqual(wait_mock.call_count, 1)\n            response = conn.getresponse()\n            self.assertEqual(response.status, 500)\n\n    def test_expect_100_continue_sends_307(self):\n        # This is the case where we send a 100 continue and the server\n        # immediately sends a 307\n        with mock.patch('urllib3.util.wait_for_read') as wait_mock:\n            # Shows the server first sending a 100 continue response\n            # then a 200 ok response.\n            s = FakeSocket(\n                b'HTTP/1.1 307 Temporary Redirect\\r\\n'\n                b'Location: http://example.org\\r\\n'\n            )\n            conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n            conn.sock = s\n            wait_mock.return_value = True\n            conn.request(\n                'GET', '/bucket/foo', b'body', {'Expect': b'100-continue'}\n            )\n            # Assert that we waited for the 100-continue response\n            self.assertEqual(wait_mock.call_count, 1)\n            response = conn.getresponse()\n            # Now we should verify that our final response is the 307.\n            self.assertEqual(response.status, 307)\n\n    def test_expect_100_continue_no_response_from_server(self):\n        with mock.patch('urllib3.util.wait_for_read') as wait_mock:\n            # Shows the server first sending a 100 continue response\n            # then a 200 ok response.\n            s = FakeSocket(\n                b'HTTP/1.1 307 Temporary Redirect\\r\\n'\n                b'Location: http://example.org\\r\\n'\n            )\n            conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n            conn.sock = s\n            # By settings wait_mock to return False, this indicates\n            # that the server did not send any response.  In this situation\n            # we should just send the request anyways.\n            wait_mock.return_value = False\n            conn.request(\n                'GET', '/bucket/foo', b'body', {'Expect': b'100-continue'}\n            )\n            # Assert that we waited for the 100-continue response\n            self.assertEqual(wait_mock.call_count, 1)\n            response = conn.getresponse()\n            self.assertEqual(response.status, 307)\n\n    def test_message_body_is_file_like_object(self):\n        # Shows the server first sending a 100 continue response\n        # then a 200 ok response.\n        body = BytesIOWithLen(b'body contents')\n        s = FakeSocket(b'HTTP/1.1 200 OK\\r\\n')\n        conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n        conn.sock = s\n        conn.request('GET', '/bucket/foo', body)\n        response = conn.getresponse()\n        self.assertEqual(response.status, 200)\n\n    def test_no_expect_header_set(self):\n        # Shows the server first sending a 100 continue response\n        # then a 200 ok response.\n        s = FakeSocket(b'HTTP/1.1 200 OK\\r\\n')\n        conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n        conn.sock = s\n        conn.request('GET', '/bucket/foo', b'body')\n        response = conn.getresponse()\n        self.assertEqual(response.status, 200)\n\n    def test_tunnel_readline_none_bugfix(self):\n        # Tests whether ``_tunnel`` function is able to work around the\n        # py26 bug of avoiding infinite while loop if nothing is returned.\n        conn = self.create_tunneled_connection(\n            url='s3.amazonaws.com',\n            port=443,\n            response=b'HTTP/1.1 200 OK\\r\\n',\n        )\n        conn._tunnel()\n        # Ensure proper amount of readline calls were made.\n        self.assertEqual(self.mock_response.fp.readline.call_count, 2)\n\n    def test_tunnel_readline_normal(self):\n        # Tests that ``_tunnel`` function behaves normally when it comes\n        # across the usual http ending.\n        conn = self.create_tunneled_connection(\n            url='s3.amazonaws.com',\n            port=443,\n            response=b'HTTP/1.1 200 OK\\r\\n\\r\\n',\n        )\n        conn._tunnel()\n        # Ensure proper amount of readline calls were made.\n        self.assertEqual(self.mock_response.fp.readline.call_count, 2)\n\n    def test_tunnel_raises_socket_error(self):\n        # Tests that ``_tunnel`` function throws appropriate error when\n        # not 200 status.\n        conn = self.create_tunneled_connection(\n            url='s3.amazonaws.com',\n            port=443,\n            response=b'HTTP/1.1 404 Not Found\\r\\n\\r\\n',\n        )\n        with self.assertRaises(socket.error):\n            conn._tunnel()\n\n    def test_tunnel_uses_std_lib(self):\n        s = FakeSocket(b'HTTP/1.1 200 OK\\r\\n')\n        conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n        conn.sock = s\n        # Test that the standard library method was used by patching out\n        # the ``_tunnel`` method and seeing if the std lib method was called.\n        with mock.patch(\n            'urllib3.connection.HTTPConnection._tunnel'\n        ) as mock_tunnel:\n            conn._tunnel()\n            self.assertTrue(mock_tunnel.called)\n\n    def test_encodes_unicode_method_line(self):\n        s = FakeSocket(b'HTTP/1.1 200 OK\\r\\n')\n        conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n        conn.sock = s\n        # Note the combination of unicode 'GET' and\n        # bytes 'Utf8-Header' value.\n        conn.request(\n            'GET',\n            '/bucket/foo',\n            b'body',\n            headers={\"Utf8-Header\": b\"\\xe5\\xb0\\x8f\"},\n        )\n        response = conn.getresponse()\n        self.assertEqual(response.status, 200)\n\n    def test_state_reset_on_connection_close(self):\n        # This simulates what urllib3 does with connections\n        # in its connection pool logic.\n        with mock.patch('urllib3.util.wait_for_read') as wait_mock:\n            # First fast fail with a 500 response when we first\n            # send the expect header.\n            s = FakeSocket(b'HTTP/1.1 500 Internal Server Error\\r\\n')\n            conn = AWSHTTPConnection('s3.amazonaws.com', 443)\n            conn.sock = s\n            wait_mock.return_value = True\n\n            conn.request(\n                'GET', '/bucket/foo', b'body', {'Expect': b'100-continue'}\n            )\n            self.assertEqual(wait_mock.call_count, 1)\n            response = conn.getresponse()\n            self.assertEqual(response.status, 500)\n\n            # Now what happens in urllib3 is that when the next\n            # request comes along and this conection gets checked\n            # out.  We see that the connection needs to be\n            # reset.  So first the connection is closed.\n            conn.close()\n\n            # And then a new connection is established.\n            new_conn = FakeSocket(\n                b'HTTP/1.1 100 (Continue)\\r\\n\\r\\nHTTP/1.1 200 OK\\r\\n'\n            )\n            conn.sock = new_conn\n\n            # And we make a request, we should see the 200 response\n            # that was sent back.\n            wait_mock.return_value = True\n\n            conn.request(\n                'GET', '/bucket/foo', b'body', {'Expect': b'100-continue'}\n            )\n            # Assert that we waited for the 100-continue response\n            self.assertEqual(wait_mock.call_count, 2)\n            response = conn.getresponse()\n            # This should be 200.  If it's a 500 then\n            # the prior response was leaking into our\n            # current response.,\n            self.assertEqual(response.status, 200)\n\n\nclass TestAWSHTTPConnectionPool(unittest.TestCase):\n    def test_global_urllib3_pool_is_unchanged(self):\n        http_connection_class = HTTPConnectionPool.ConnectionCls\n        self.assertIsNot(http_connection_class, AWSHTTPConnection)\n        https_connection_class = HTTPSConnectionPool.ConnectionCls\n        self.assertIsNot(https_connection_class, AWSHTTPSConnection)\n\n\nclass TestPrepareRequestDict(unittest.TestCase):\n    def setUp(self):\n        self.user_agent = 'botocore/1.0'\n        self.endpoint_url = 'https://s3.amazonaws.com'\n        self.base_request_dict = {\n            'body': '',\n            'headers': {},\n            'method': 'GET',\n            'query_string': '',\n            'url_path': '/',\n            'context': {},\n        }\n\n    def prepare_base_request_dict(\n        self, request_dict, endpoint_url=None, user_agent=None, context=None\n    ):\n        self.base_request_dict.update(request_dict)\n        context = context or {}\n        if user_agent is None:\n            user_agent = self.user_agent\n        if endpoint_url is None:\n            endpoint_url = self.endpoint_url\n        prepare_request_dict(\n            self.base_request_dict,\n            endpoint_url=endpoint_url,\n            user_agent=user_agent,\n            context=context,\n        )\n\n    def test_prepare_request_dict_for_get(self):\n        request_dict = {'method': 'GET', 'url_path': '/'}\n        self.prepare_base_request_dict(\n            request_dict, endpoint_url='https://s3.amazonaws.com'\n        )\n        self.assertEqual(self.base_request_dict['method'], 'GET')\n        self.assertEqual(\n            self.base_request_dict['url'], 'https://s3.amazonaws.com/'\n        )\n        self.assertEqual(\n            self.base_request_dict['headers']['User-Agent'], self.user_agent\n        )\n\n    def test_prepare_request_dict_for_get_no_user_agent(self):\n        self.user_agent = None\n        request_dict = {'method': 'GET', 'url_path': '/'}\n        self.prepare_base_request_dict(\n            request_dict, endpoint_url='https://s3.amazonaws.com'\n        )\n        self.assertNotIn('User-Agent', self.base_request_dict['headers'])\n\n    def test_prepare_request_dict_with_context(self):\n        context = {'foo': 'bar'}\n        self.prepare_base_request_dict({}, context=context)\n        self.assertEqual(self.base_request_dict['context'], context)\n\n    def test_query_string_serialized_to_url(self):\n        request_dict = {\n            'method': 'GET',\n            'query_string': {'prefix': 'foo'},\n            'url_path': '/mybucket',\n        }\n        self.prepare_base_request_dict(request_dict)\n        self.assertEqual(\n            self.base_request_dict['url'],\n            'https://s3.amazonaws.com/mybucket?prefix=foo',\n        )\n\n    def test_url_path_combined_with_endpoint_url(self):\n        # This checks the case where a user specifies and\n        # endpoint_url that has a path component, and the\n        # serializer gives us a request_dict that has a url\n        # component as well (say from a rest-* service).\n        request_dict = {\n            'query_string': {'prefix': 'foo'},\n            'url_path': '/mybucket',\n        }\n        endpoint_url = 'https://custom.endpoint/foo/bar'\n        self.prepare_base_request_dict(request_dict, endpoint_url)\n        self.assertEqual(\n            self.base_request_dict['url'],\n            'https://custom.endpoint/foo/bar/mybucket?prefix=foo',\n        )\n\n    def test_url_path_with_trailing_slash(self):\n        self.prepare_base_request_dict(\n            {'url_path': '/mybucket'},\n            endpoint_url='https://custom.endpoint/foo/bar/',\n        )\n\n        self.assertEqual(\n            self.base_request_dict['url'],\n            'https://custom.endpoint/foo/bar/mybucket',\n        )\n\n    def test_url_path_is_slash(self):\n        self.prepare_base_request_dict(\n            {'url_path': '/'}, endpoint_url='https://custom.endpoint/foo/bar/'\n        )\n\n        self.assertEqual(\n            self.base_request_dict['url'], 'https://custom.endpoint/foo/bar/'\n        )\n\n    def test_url_path_is_slash_with_endpoint_url_no_slash(self):\n        self.prepare_base_request_dict(\n            {'url_path': '/'}, endpoint_url='https://custom.endpoint/foo/bar'\n        )\n\n        self.assertEqual(\n            self.base_request_dict['url'], 'https://custom.endpoint/foo/bar'\n        )\n\n    def test_custom_endpoint_with_query_string(self):\n        self.prepare_base_request_dict(\n            {'url_path': '/baz', 'query_string': {'x': 'y'}},\n            endpoint_url='https://custom.endpoint/foo/bar?foo=bar',\n        )\n\n        self.assertEqual(\n            self.base_request_dict['url'],\n            'https://custom.endpoint/foo/bar/baz?foo=bar&x=y',\n        )\n\n\nclass TestCreateRequestObject(unittest.TestCase):\n    def setUp(self):\n        self.request_dict = {\n            'method': 'GET',\n            'query_string': {'prefix': 'foo'},\n            'url_path': '/mybucket',\n            'headers': {'User-Agent': 'my-agent'},\n            'body': 'my body',\n            'url': 'https://s3.amazonaws.com/mybucket?prefix=foo',\n            'context': {'signing': {'region': 'us-west-2'}},\n        }\n\n    def test_create_request_object(self):\n        request = create_request_object(self.request_dict)\n        self.assertEqual(request.method, self.request_dict['method'])\n        self.assertEqual(request.url, self.request_dict['url'])\n        self.assertEqual(request.data, self.request_dict['body'])\n        self.assertEqual(request.context, self.request_dict['context'])\n        self.assertIn('User-Agent', request.headers)\n\n\nclass TestHeadersDict(unittest.TestCase):\n    def setUp(self):\n        self.headers = HeadersDict()\n\n    def test_get_insensitive(self):\n        self.headers['foo'] = 'bar'\n        self.assertEqual(self.headers['FOO'], 'bar')\n\n    def test_set_insensitive(self):\n        self.headers['foo'] = 'bar'\n        self.headers['FOO'] = 'baz'\n        self.assertEqual(self.headers['foo'], 'baz')\n\n    def test_del_insensitive(self):\n        self.headers['foo'] = 'bar'\n        self.assertEqual(self.headers['FOO'], 'bar')\n        del self.headers['FoO']\n        with self.assertRaises(KeyError):\n            self.headers['foo']\n\n    def test_iteration(self):\n        self.headers['FOO'] = 'bar'\n        self.headers['dead'] = 'beef'\n        self.assertIn('FOO', list(self.headers))\n        self.assertIn('dead', list(self.headers))\n        headers_items = list(self.headers.items())\n        self.assertIn(('FOO', 'bar'), headers_items)\n        self.assertIn(('dead', 'beef'), headers_items)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/unit/test_auth_sigv4.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.auth import SigV4Auth\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.credentials import Credentials\nfrom tests import unittest\n\nSECRET_KEY = \"wJalrXUtnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY\"\nACCESS_KEY = 'AKIDEXAMPLE'\n\n\nclass TestSigV4Auth(unittest.TestCase):\n    def setUp(self):\n        self.credentials = Credentials(ACCESS_KEY, SECRET_KEY)\n        self.sigv4 = SigV4Auth(self.credentials, 'host', 'us-weast-1')\n\n    def test_signed_host_is_lowercase(self):\n        endpoint = 'https://S5.Us-WeAsT-2.AmAZonAwS.com'\n        expected_host = 's5.us-weast-2.amazonaws.com'\n        request = AWSRequest(method='GET', url=endpoint)\n        headers_to_sign = self.sigv4.headers_to_sign(request)\n        self.assertEqual(expected_host, headers_to_sign.get('host'))\n\n    def test_signed_host_is_ipv6_without_port(self):\n        endpoint = 'http://[::1]'\n        expected_host = '[::1]'\n        request = AWSRequest(method='GET', url=endpoint)\n        headers_to_sign = self.sigv4.headers_to_sign(request)\n        self.assertEqual(expected_host, headers_to_sign.get('host'))\n\n    def test_signed_host_is_ipv6_with_default_port(self):\n        endpoint = 'http://[::1]:80'\n        expected_host = '[::1]'\n        request = AWSRequest(method='GET', url=endpoint)\n        headers_to_sign = self.sigv4.headers_to_sign(request)\n        self.assertEqual(expected_host, headers_to_sign.get('host'))\n\n    def test_signed_host_is_ipv6_with_explicit_port(self):\n        endpoint = 'http://[::1]:6789'\n        expected_host = '[::1]:6789'\n        request = AWSRequest(method='GET', url=endpoint)\n        headers_to_sign = self.sigv4.headers_to_sign(request)\n        self.assertEqual(expected_host, headers_to_sign.get('host'))\n", "tests/unit/test_response.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\nfrom io import BytesIO\n\nfrom dateutil.tz import tzutc\nfrom urllib3.exceptions import ProtocolError as URLLib3ProtocolError\nfrom urllib3.exceptions import ReadTimeoutError as URLLib3ReadTimeoutError\n\nimport botocore\nfrom botocore import response\nfrom botocore.awsrequest import AWSResponse\nfrom botocore.exceptions import (\n    IncompleteReadError,\n    ReadTimeoutError,\n    ResponseStreamingError,\n)\nfrom tests import unittest\nfrom tests.unit import BaseResponseTest\n\nXMLBODY1 = (\n    b'<?xml version=\"1.0\" encoding=\"UTF-8\"?><Error>'\n    b'<Code>AccessDenied</Code>'\n    b'<Message>Access Denied</Message>'\n    b'<RequestId>XXXXXXXXXXXXXXXX</RequestId>'\n    b'<HostId>AAAAAAAAAAAAAAAAAAA</HostId>'\n    b'</Error>'\n)\n\nXMLBODY2 = (\n    b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n    b'<ListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">'\n    b'<Name>mybucket</Name><Prefix></Prefix><Marker></Marker>'\n    b'<MaxKeys>1000</MaxKeys><IsTruncated>false</IsTruncated>'\n    b'<Contents><Key>test.png</Key><LastModified>2014-03-01T17:06:40.000Z</LastModified>'\n    b'<ETag>&quot;00000000000000000000000000000000&quot;</ETag><Size>6702</Size>'\n    b'<Owner><ID>AAAAAAAAAAAAAAAAAAA</ID>'\n    b'<DisplayName>dummy</DisplayName></Owner>'\n    b'<StorageClass>STANDARD</StorageClass></Contents></ListBucketResult>'\n)\n\n\nclass TestStreamWrapper(unittest.TestCase):\n    def assert_lines(self, line_iterator, expected_lines):\n        for expected_line in expected_lines:\n            self.assertEqual(\n                next(line_iterator),\n                expected_line,\n            )\n        # We should have exhausted the iterator.\n        with self.assertRaises(StopIteration):\n            next(line_iterator)\n\n    def test_streaming_wrapper_validates_content_length(self):\n        body = BytesIO(b'1234567890')\n        stream = response.StreamingBody(body, content_length=10)\n        self.assertEqual(stream.read(), b'1234567890')\n\n    def test_streaming_body_with_invalid_length(self):\n        body = BytesIO(b'123456789')\n        stream = response.StreamingBody(body, content_length=10)\n        with self.assertRaises(IncompleteReadError):\n            self.assertEqual(stream.read(9), b'123456789')\n            # The next read will have nothing returned and raise\n            # an IncompleteReadError because we were expectd 10 bytes, not 9.\n            stream.read()\n\n    def test_streaming_body_readable(self):\n        body = BytesIO(b'1234567890')\n        stream = response.StreamingBody(body, content_length=10)\n        self.assertTrue(stream.readable())\n        stream.close()\n        with self.assertRaises(ValueError):\n            stream.readable()\n\n    def test_streaming_body_with_zero_read(self):\n        body = BytesIO(b'1234567890')\n        stream = response.StreamingBody(body, content_length=10)\n        chunk = stream.read(0)\n        self.assertEqual(chunk, b'')\n        self.assertEqual(stream.read(), b'1234567890')\n\n    def test_streaming_body_with_single_read(self):\n        body = BytesIO(b'123456789')\n        stream = response.StreamingBody(body, content_length=10)\n        with self.assertRaises(IncompleteReadError):\n            stream.read()\n\n    def test_streaming_body_readline(self):\n        body = BytesIO(b'1234567890\\n1234567\\n12345\\n')\n        stream = response.StreamingBody(body, content_length=25)\n        chunk = stream.readline()\n        self.assertEqual(chunk, b'1234567890\\n')\n        chunk = stream.readline()\n        self.assertEqual(chunk, b'1234567\\n')\n\n    def test_streaming_body_readlines(self):\n        body = BytesIO(b'1234567890\\n1234567890\\n12345')\n        stream = response.StreamingBody(body, content_length=27)\n        chunks = [b'1234567890\\n', b'1234567890\\n', b'12345']\n        self.assertEqual(stream.readlines(), chunks)\n\n    def test_streaming_body_tell(self):\n        body = BytesIO(b'1234567890')\n        stream = response.StreamingBody(body, content_length=10)\n        self.assertEqual(stream.tell(), 0)\n        stream.read(5)\n        self.assertEqual(stream.tell(), 5)\n\n    def test_streaming_body_closes(self):\n        body = BytesIO(b'1234567890')\n        stream = response.StreamingBody(body, content_length=10)\n        self.assertFalse(body.closed)\n        stream.close()\n        self.assertTrue(body.closed)\n\n    def test_default_iter_behavior(self):\n        body = BytesIO(b'a' * 2048)\n        stream = response.StreamingBody(body, content_length=2048)\n        chunks = list(stream)\n        self.assertEqual(len(chunks), 2)\n        self.assertEqual(chunks, [b'a' * 1024, b'a' * 1024])\n\n    def test_streaming_body_is_an_iterator(self):\n        body = BytesIO(b'a' * 1024 + b'b' * 1024 + b'c' * 2)\n        stream = response.StreamingBody(body, content_length=2050)\n        self.assertEqual(b'a' * 1024, next(stream))\n        self.assertEqual(b'b' * 1024, next(stream))\n        self.assertEqual(b'c' * 2, next(stream))\n        with self.assertRaises(StopIteration):\n            next(stream)\n\n    def test_iter_chunks_single_byte(self):\n        body = BytesIO(b'abcde')\n        stream = response.StreamingBody(body, content_length=5)\n        chunks = list(stream.iter_chunks(chunk_size=1))\n        self.assertEqual(chunks, [b'a', b'b', b'c', b'd', b'e'])\n\n    def test_iter_chunks_with_leftover(self):\n        body = BytesIO(b'abcde')\n        stream = response.StreamingBody(body, content_length=5)\n        chunks = list(stream.iter_chunks(chunk_size=2))\n        self.assertEqual(chunks, [b'ab', b'cd', b'e'])\n\n    def test_iter_chunks_single_chunk(self):\n        body = BytesIO(b'abcde')\n        stream = response.StreamingBody(body, content_length=5)\n        chunks = list(stream.iter_chunks(chunk_size=1024))\n        self.assertEqual(chunks, [b'abcde'])\n\n    def test_streaming_line_iterator(self):\n        body = BytesIO(b'1234567890\\n1234567890\\n12345')\n        stream = response.StreamingBody(body, content_length=27)\n        self.assert_lines(\n            stream.iter_lines(),\n            [b'1234567890', b'1234567890', b'12345'],\n        )\n\n    def test_streaming_line_iterator_ends_newline(self):\n        body = BytesIO(b'1234567890\\n1234567890\\n12345\\n')\n        stream = response.StreamingBody(body, content_length=28)\n        self.assert_lines(\n            stream.iter_lines(),\n            [b'1234567890', b'1234567890', b'12345'],\n        )\n\n    def test_streaming_line_iter_chunk_sizes(self):\n        for chunk_size in range(1, 30):\n            body = BytesIO(b'1234567890\\n1234567890\\n12345')\n            stream = response.StreamingBody(body, content_length=27)\n            self.assert_lines(\n                stream.iter_lines(chunk_size),\n                [b'1234567890', b'1234567890', b'12345'],\n            )\n\n    def test_streaming_line_iterator_keepends(self):\n        body = BytesIO(b'1234567890\\n1234567890\\n12345')\n        stream = response.StreamingBody(body, content_length=27)\n        self.assert_lines(\n            stream.iter_lines(keepends=True),\n            [b'1234567890\\n', b'1234567890\\n', b'12345'],\n        )\n\n    def test_catches_urllib3_read_timeout(self):\n        class TimeoutBody:\n            def read(*args, **kwargs):\n                raise URLLib3ReadTimeoutError(None, None, None)\n\n            def geturl(*args, **kwargs):\n                return 'http://example.com'\n\n        stream = response.StreamingBody(TimeoutBody(), content_length=None)\n        with self.assertRaises(ReadTimeoutError):\n            stream.read()\n\n    def test_catches_urllib3_protocol_error(self):\n        class ProtocolErrorBody:\n            def read(*args, **kwargs):\n                raise URLLib3ProtocolError(None, None, None)\n\n            def geturl(*args, **kwargs):\n                return 'http://example.com'\n\n        stream = response.StreamingBody(\n            ProtocolErrorBody(), content_length=None\n        )\n        with self.assertRaises(ResponseStreamingError):\n            stream.read()\n\n    def test_streaming_line_abstruse_newline_standard(self):\n        for chunk_size in range(1, 30):\n            body = BytesIO(b'1234567890\\r\\n1234567890\\r\\n12345\\r\\n')\n            stream = response.StreamingBody(body, content_length=31)\n            self.assert_lines(\n                stream.iter_lines(chunk_size),\n                [b'1234567890', b'1234567890', b'12345'],\n            )\n\n    def test_streaming_line_empty_body(self):\n        stream = response.StreamingBody(\n            BytesIO(b''),\n            content_length=0,\n        )\n        self.assert_lines(stream.iter_lines(), [])\n\n    def test_streaming_body_as_context_manager(self):\n        body = BytesIO(b'1234567890')\n        with response.StreamingBody(body, content_length=10) as stream:\n            self.assertEqual(stream.read(), b'1234567890')\n            self.assertFalse(body.closed)\n        self.assertTrue(body.closed)\n\n\nclass FakeRawResponse(BytesIO):\n    def stream(self, amt=1024, decode_content=None):\n        while True:\n            chunk = self.read(amt)\n            if not chunk:\n                break\n            yield chunk\n\n\nclass TestGetResponse(BaseResponseTest):\n    maxDiff = None\n\n    def test_get_response_streaming_ok(self):\n        headers = {\n            'content-type': 'image/png',\n            'server': 'AmazonS3',\n            'AcceptRanges': 'bytes',\n            'transfer-encoding': 'chunked',\n            'ETag': '\"00000000000000000000000000000000\"',\n        }\n        raw = FakeRawResponse(b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00')\n\n        http_response = AWSResponse(None, 200, headers, raw)\n\n        session = botocore.session.get_session()\n        service_model = session.get_service_model('s3')\n        operation_model = service_model.operation_model('GetObject')\n\n        res = response.get_response(operation_model, http_response)\n        self.assertTrue(isinstance(res[1]['Body'], response.StreamingBody))\n        self.assertEqual(res[1]['ETag'], '\"00000000000000000000000000000000\"')\n\n    def test_get_response_streaming_ng(self):\n        headers = {\n            'content-type': 'application/xml',\n            'date': 'Sat, 08 Mar 2014 12:05:44 GMT',\n            'server': 'AmazonS3',\n            'transfer-encoding': 'chunked',\n            'x-amz-id-2': 'AAAAAAAAAAAAAAAAAAA',\n            'x-amz-request-id': 'XXXXXXXXXXXXXXXX',\n        }\n        raw = FakeRawResponse(XMLBODY1)\n        http_response = AWSResponse(None, 403, headers, raw)\n\n        session = botocore.session.get_session()\n        service_model = session.get_service_model('s3')\n        operation_model = service_model.operation_model('GetObject')\n\n        self.assert_response_with_subset_metadata(\n            response.get_response(operation_model, http_response)[1],\n            {\n                'Error': {'Message': 'Access Denied', 'Code': 'AccessDenied'},\n                'ResponseMetadata': {\n                    'HostId': 'AAAAAAAAAAAAAAAAAAA',\n                    'RequestId': 'XXXXXXXXXXXXXXXX',\n                    'HTTPStatusCode': 403,\n                },\n            },\n        )\n\n    def test_get_response_nonstreaming_ok(self):\n        headers = {\n            'content-type': 'application/xml',\n            'date': 'Sun, 09 Mar 2014 02:55:43 GMT',\n            'server': 'AmazonS3',\n            'transfer-encoding': 'chunked',\n            'x-amz-id-2': 'AAAAAAAAAAAAAAAAAAA',\n            'x-amz-request-id': 'XXXXXXXXXXXXXXXX',\n        }\n        raw = FakeRawResponse(XMLBODY1)\n        http_response = AWSResponse(None, 403, headers, raw)\n\n        session = botocore.session.get_session()\n        service_model = session.get_service_model('s3')\n        operation_model = service_model.operation_model('ListObjects')\n\n        self.assert_response_with_subset_metadata(\n            response.get_response(operation_model, http_response)[1],\n            {\n                'ResponseMetadata': {\n                    'RequestId': 'XXXXXXXXXXXXXXXX',\n                    'HostId': 'AAAAAAAAAAAAAAAAAAA',\n                    'HTTPStatusCode': 403,\n                },\n                'Error': {'Message': 'Access Denied', 'Code': 'AccessDenied'},\n            },\n        )\n\n    def test_get_response_nonstreaming_ng(self):\n        headers = {\n            'content-type': 'application/xml',\n            'date': 'Sat, 08 Mar 2014 12:05:44 GMT',\n            'server': 'AmazonS3',\n            'transfer-encoding': 'chunked',\n            'x-amz-id-2': 'AAAAAAAAAAAAAAAAAAA',\n            'x-amz-request-id': 'XXXXXXXXXXXXXXXX',\n        }\n        raw = FakeRawResponse(XMLBODY2)\n        http_response = AWSResponse(None, 200, headers, raw)\n\n        session = botocore.session.get_session()\n        service_model = session.get_service_model('s3')\n        operation_model = service_model.operation_model('ListObjects')\n\n        self.assert_response_with_subset_metadata(\n            response.get_response(operation_model, http_response)[1],\n            {\n                'Contents': [\n                    {\n                        'ETag': '\"00000000000000000000000000000000\"',\n                        'Key': 'test.png',\n                        'LastModified': datetime.datetime(\n                            2014, 3, 1, 17, 6, 40, tzinfo=tzutc()\n                        ),\n                        'Owner': {\n                            'DisplayName': 'dummy',\n                            'ID': 'AAAAAAAAAAAAAAAAAAA',\n                        },\n                        'Size': 6702,\n                        'StorageClass': 'STANDARD',\n                    }\n                ],\n                'IsTruncated': False,\n                'Marker': \"\",\n                'MaxKeys': 1000,\n                'Name': 'mybucket',\n                'Prefix': \"\",\n                'ResponseMetadata': {\n                    'RequestId': 'XXXXXXXXXXXXXXXX',\n                    'HostId': 'AAAAAAAAAAAAAAAAAAA',\n                    'HTTPStatusCode': 200,\n                },\n            },\n        )\n", "tests/unit/test_session_legacy.py": "#!/usr/bin/env\n# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\nimport shutil\nimport tempfile\n\nimport pytest\n\nimport botocore.config\nimport botocore.exceptions\nimport botocore.loaders\nimport botocore.session\nfrom botocore import client\nfrom botocore.hooks import HierarchicalEmitter\nfrom botocore.model import ServiceModel\nfrom botocore.paginate import PaginatorModel\nfrom botocore.waiter import WaiterModel\nfrom tests import create_session, mock, temporary_file, unittest\n\n\n# This is an old version of the session tests to ensure backwards compatibility\n# there is a new unit/test_session.py set of tests for the new config interface\n# which should be prefered. When backwards compatibility can be dropped then\n# this test should be removed.\nclass BaseSessionTest(unittest.TestCase):\n    def setUp(self):\n        self.env_vars = {\n            'profile': (None, 'FOO_PROFILE', None, None),\n            'region': ('foo_region', 'FOO_REGION', None, None),\n            'data_path': ('data_path', 'FOO_DATA_PATH', None, None),\n            'config_file': (None, 'FOO_CONFIG_FILE', None, None),\n            'credentials_file': (None, None, '/tmp/nowhere', None),\n            'ca_bundle': ('foo_ca_bundle', 'FOO_AWS_CA_BUNDLE', None, None),\n            'api_versions': ('foo_api_versions', None, {}, None),\n        }\n        self.environ = {}\n        self.environ_patch = mock.patch('os.environ', self.environ)\n        self.environ_patch.start()\n        self.environ['FOO_PROFILE'] = 'foo'\n        self.environ['FOO_REGION'] = 'us-west-11'\n        data_path = os.path.join(os.path.dirname(__file__), 'data')\n        self.environ['FOO_DATA_PATH'] = data_path\n        config_path = os.path.join(\n            os.path.dirname(__file__), 'cfg', 'foo_config'\n        )\n        self.environ['FOO_CONFIG_FILE'] = config_path\n        self.session = create_session(session_vars=self.env_vars)\n\n    def tearDown(self):\n        self.environ_patch.stop()\n\n\nclass SessionTest(BaseSessionTest):\n    def close_log_file_handler(self, tempdir, filename):\n        logger = logging.getLogger('botocore')\n        handlers = logger.handlers\n        for handler in handlers[:]:\n            if hasattr(handler, 'stream') and handler.stream.name == filename:\n                handler.stream.close()\n                logger.removeHandler(handler)\n                os.remove(filename)\n                # logging has an atexit handler that will try to flush/close\n                # the file.  By setting this flag to False, we'll prevent it\n                # from raising an exception, which is fine because we're\n                # handling the closing of the file ourself.\n                logging.raiseExceptions = False\n        shutil.rmtree(tempdir)\n\n    def test_supports_multiple_env_vars_for_single_logical_name(self):\n        env_vars = {\n            'profile': (\n                None,\n                ['BAR_DEFAULT_PROFILE', 'BAR_PROFILE'],\n                None,\n                None,\n            ),\n        }\n        session = create_session(session_vars=env_vars)\n        self.environ['BAR_DEFAULT_PROFILE'] = 'first'\n        self.environ['BAR_PROFILE'] = 'second'\n        self.assertEqual(session.get_config_variable('profile'), 'first')\n\n    def test_profile_when_set_explicitly(self):\n        session = create_session(session_vars=self.env_vars, profile='asdf')\n        self.assertEqual(session.profile, 'asdf')\n\n    def test_profile_when_pulled_from_env(self):\n        self.environ['FOO_PROFILE'] = 'bar'\n        # Even though we didn't explicitly pass in a profile, the\n        # profile property will still look this up for us.\n        self.assertEqual(self.session.profile, 'bar')\n\n    def test_multiple_env_vars_uses_second_var(self):\n        env_vars = {\n            'profile': (\n                None,\n                ['BAR_DEFAULT_PROFILE', 'BAR_PROFILE'],\n                None,\n                None,\n            ),\n        }\n        session = create_session(session_vars=env_vars)\n        self.environ.pop('BAR_DEFAULT_PROFILE', None)\n        self.environ['BAR_PROFILE'] = 'second'\n        self.assertEqual(session.get_config_variable('profile'), 'second')\n\n    def test_profile(self):\n        self.assertEqual(self.session.get_config_variable('profile'), 'foo')\n        self.assertEqual(\n            self.session.get_config_variable('region'), 'us-west-11'\n        )\n        self.session.get_config_variable('profile') == 'default'\n        saved_region = self.environ['FOO_REGION']\n        del self.environ['FOO_REGION']\n        saved_profile = self.environ['FOO_PROFILE']\n        del self.environ['FOO_PROFILE']\n        session = create_session(session_vars=self.env_vars)\n        self.assertEqual(session.get_config_variable('profile'), None)\n        self.assertEqual(session.get_config_variable('region'), 'us-west-1')\n        self.environ['FOO_REGION'] = saved_region\n        self.environ['FOO_PROFILE'] = saved_profile\n\n    def test_profile_does_not_exist_raises_exception(self):\n        # Given we have no profile:\n        self.environ['FOO_PROFILE'] = 'profile_that_does_not_exist'\n        session = create_session(session_vars=self.env_vars)\n        with self.assertRaises(botocore.exceptions.ProfileNotFound):\n            session.get_scoped_config()\n\n    def test_variable_does_not_exist(self):\n        session = create_session(session_vars=self.env_vars)\n        self.assertIsNone(session.get_config_variable('foo/bar'))\n\n    def test_get_aws_services_in_alphabetical_order(self):\n        session = create_session(session_vars=self.env_vars)\n        services = session.get_available_services()\n        self.assertEqual(sorted(services), services)\n\n    def test_profile_does_not_exist_with_default_profile(self):\n        session = create_session(session_vars=self.env_vars)\n        config = session.get_scoped_config()\n        # We should have loaded this properly, and we'll check\n        # that foo_access_key which is defined in the config\n        # file should be present in the loaded config dict.\n        self.assertIn('aws_access_key_id', config)\n\n    def test_type_conversions_occur_when_specified(self):\n        # Specify that we can retrieve the var from the\n        # FOO_TIMEOUT env var, with a conversion function\n        # of int().\n        self.env_vars['metadata_service_timeout'] = (\n            None,\n            'FOO_TIMEOUT',\n            None,\n            int,\n        )\n        # Environment variables are always strings.\n        self.environ['FOO_TIMEOUT'] = '10'\n        session = create_session(session_vars=self.env_vars)\n        # But we should type convert this to a string.\n        self.assertEqual(\n            session.get_config_variable('metadata_service_timeout'), 10\n        )\n\n    def test_default_profile_specified_raises_exception(self):\n        # If you explicity set the default profile and you don't\n        # have that in your config file, an exception is raised.\n        config_path = os.path.join(\n            os.path.dirname(__file__), 'cfg', 'boto_config_empty'\n        )\n        self.environ['FOO_CONFIG_FILE'] = config_path\n        self.environ['FOO_PROFILE'] = 'default'\n        session = create_session(session_vars=self.env_vars)\n        # In this case, even though we specified default, because\n        # the boto_config_empty config file does not have a default\n        # profile, we should be raising an exception.\n        with self.assertRaises(botocore.exceptions.ProfileNotFound):\n            session.get_scoped_config()\n\n    def test_file_logger(self):\n        tempdir = tempfile.mkdtemp()\n        temp_file = os.path.join(tempdir, 'file_logger')\n        self.session.set_file_logger(logging.DEBUG, temp_file)\n        self.addCleanup(self.close_log_file_handler, tempdir, temp_file)\n        self.session.get_credentials()\n        self.assertTrue(os.path.isfile(temp_file))\n        with open(temp_file) as logfile:\n            s = logfile.read()\n        self.assertTrue('Looking for credentials' in s)\n\n    def test_full_config_property(self):\n        full_config = self.session.full_config\n        self.assertTrue('foo' in full_config['profiles'])\n        self.assertTrue('default' in full_config['profiles'])\n\n    def test_full_config_merges_creds_file_data(self):\n        with temporary_file('w') as f:\n            self.session.set_config_variable('credentials_file', f.name)\n            f.write('[newprofile]\\n')\n            f.write('aws_access_key_id=FROM_CREDS_FILE_1\\n')\n            f.write('aws_secret_access_key=FROM_CREDS_FILE_2\\n')\n            f.flush()\n\n            full_config = self.session.full_config\n            self.assertEqual(\n                full_config['profiles']['newprofile'],\n                {\n                    'aws_access_key_id': 'FROM_CREDS_FILE_1',\n                    'aws_secret_access_key': 'FROM_CREDS_FILE_2',\n                },\n            )\n\n    def test_path_not_in_available_profiles(self):\n        with temporary_file('w') as f:\n            self.session.set_config_variable('credentials_file', f.name)\n            f.write('[newprofile]\\n')\n            f.write('aws_access_key_id=FROM_CREDS_FILE_1\\n')\n            f.write('aws_secret_access_key=FROM_CREDS_FILE_2\\n')\n            f.flush()\n\n            profiles = self.session.available_profiles\n            self.assertEqual(set(profiles), {'foo', 'default', 'newprofile'})\n\n    def test_emit_delegates_to_emitter(self):\n        calls = []\n        handler = lambda **kwargs: calls.append(kwargs)\n        self.session.register('foo', handler)\n        self.session.emit('foo')\n        self.assertEqual(len(calls), 1)\n        self.assertEqual(calls[0]['event_name'], 'foo')\n\n    def test_emitter_can_be_passed_in(self):\n        events = HierarchicalEmitter()\n        session = create_session(\n            session_vars=self.env_vars, event_hooks=events\n        )\n        calls = []\n        handler = lambda **kwargs: calls.append(kwargs)\n        events.register('foo', handler)\n\n        session.emit('foo')\n        self.assertEqual(len(calls), 1)\n\n    def test_emit_first_non_none(self):\n        session = create_session(session_vars=self.env_vars)\n        session.register('foo', lambda **kwargs: None)\n        session.register('foo', lambda **kwargs: 'first')\n        session.register('foo', lambda **kwargs: 'second')\n        response = session.emit_first_non_none_response('foo')\n        self.assertEqual(response, 'first')\n\n    @mock.patch('logging.getLogger')\n    @mock.patch('logging.FileHandler')\n    def test_logger_name_can_be_passed_in(self, file_handler, get_logger):\n        self.session.set_debug_logger('botocore.hooks')\n        get_logger.assert_called_with('botocore.hooks')\n\n        self.session.set_file_logger('DEBUG', 'debuglog', 'botocore.service')\n        get_logger.assert_called_with('botocore.service')\n        file_handler.assert_called_with('debuglog')\n\n    @mock.patch('logging.getLogger')\n    @mock.patch('logging.StreamHandler')\n    @mock.patch('logging.Formatter')\n    def test_general_purpose_logger(self, formatter, file_handler, get_logger):\n        self.session.set_stream_logger('foo.bar', 'ERROR', format_string='foo')\n        get_logger.assert_called_with('foo.bar')\n        get_logger.return_value.setLevel.assert_called_with(logging.DEBUG)\n        formatter.assert_called_with('foo')\n\n    def test_register_with_unique_id(self):\n        calls = []\n        handler = lambda **kwargs: calls.append(kwargs)\n        self.session.register('foo', handler, unique_id='bar')\n        self.session.emit('foo')\n        self.assertEqual(calls[0]['event_name'], 'foo')\n        calls = []\n        self.session.unregister('foo', unique_id='bar')\n        self.session.emit('foo')\n        self.assertEqual(calls, [])\n\n\nclass TestBuiltinEventHandlers(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.builtin_handlers = [\n            ('foo', self.on_foo),\n        ]\n        self.foo_called = False\n        self.handler_patch = mock.patch(\n            'botocore.handlers.BUILTIN_HANDLERS', self.builtin_handlers\n        )\n        self.handler_patch.start()\n\n    def on_foo(self, **kwargs):\n        self.foo_called = True\n\n    def tearDown(self):\n        super().tearDown()\n        self.handler_patch.stop()\n\n    def test_registered_builtin_handlers(self):\n        session = botocore.session.Session(\n            self.env_vars, None, include_builtin_handlers=True\n        )\n        session.emit('foo')\n        self.assertTrue(self.foo_called)\n\n\nclass TestSessionConfigurationVars(BaseSessionTest):\n    def test_per_session_config_vars(self):\n        self.session.session_var_map['foobar'] = (\n            None,\n            'FOOBAR',\n            'default',\n            None,\n        )\n        # Default value.\n        self.assertEqual(self.session.get_config_variable('foobar'), 'default')\n        # Retrieve from os environment variable.\n        self.environ['FOOBAR'] = 'fromenv'\n        self.assertEqual(self.session.get_config_variable('foobar'), 'fromenv')\n\n        # Explicit override.\n        self.session.set_config_variable('foobar', 'session-instance')\n        self.assertEqual(\n            self.session.get_config_variable('foobar'), 'session-instance'\n        )\n\n        # Can disable this check via the ``methods`` arg.\n        del self.environ['FOOBAR']\n        self.assertEqual(\n            self.session.get_config_variable(\n                'foobar', methods=('env', 'config')\n            ),\n            'default',\n        )\n\n    def test_default_value_can_be_overriden(self):\n        self.session.session_var_map['foobar'] = (\n            None,\n            'FOOBAR',\n            'default',\n            None,\n        )\n        self.assertEqual(self.session.get_config_variable('foobar'), 'default')\n\n    def test_can_get_session_vars_info_from_default_session(self):\n        # This test is to ensure that you can still reach the session_vars_map\n        # information from the session and that it has the expected value.\n        self.session = create_session()\n        self.assertEqual(\n            self.session.session_var_map['region'],\n            ('region', 'AWS_DEFAULT_REGION', None, None),\n        )\n        self.assertEqual(\n            self.session.session_var_map['profile'],\n            (None, ['AWS_DEFAULT_PROFILE', 'AWS_PROFILE'], None, None),\n        )\n        self.assertEqual(\n            self.session.session_var_map['data_path'],\n            ('data_path', 'AWS_DATA_PATH', None, None),\n        )\n        self.assertEqual(\n            self.session.session_var_map['config_file'],\n            (None, 'AWS_CONFIG_FILE', '~/.aws/config', None),\n        )\n        self.assertEqual(\n            self.session.session_var_map['ca_bundle'],\n            ('ca_bundle', 'AWS_CA_BUNDLE', None, None),\n        )\n        self.assertEqual(\n            self.session.session_var_map['api_versions'],\n            ('api_versions', None, {}, None),\n        )\n        self.assertEqual(\n            self.session.session_var_map['credentials_file'],\n            (None, 'AWS_SHARED_CREDENTIALS_FILE', '~/.aws/credentials', None),\n        )\n        self.assertEqual(\n            self.session.session_var_map['metadata_service_timeout'],\n            (\n                'metadata_service_timeout',\n                'AWS_METADATA_SERVICE_TIMEOUT',\n                1,\n                int,\n            ),\n        )\n        self.assertEqual(\n            self.session.session_var_map['metadata_service_num_attempts'],\n            (\n                'metadata_service_num_attempts',\n                'AWS_METADATA_SERVICE_NUM_ATTEMPTS',\n                1,\n                int,\n            ),\n        )\n        self.assertEqual(\n            self.session.session_var_map['parameter_validation'],\n            ('parameter_validation', None, True, None),\n        )\n\n\nclass TestSessionPartitionFiles(BaseSessionTest):\n    def test_lists_partitions_on_disk(self):\n        mock_resolver = mock.Mock()\n        mock_resolver.get_available_partitions.return_value = ['foo']\n        self.session._register_internal_component(\n            'endpoint_resolver', mock_resolver\n        )\n        self.assertEqual(['foo'], self.session.get_available_partitions())\n\n    def test_proxies_list_endpoints_to_resolver(self):\n        resolver = mock.Mock()\n        resolver.get_available_endpoints.return_value = ['a', 'b']\n        self.session._register_internal_component(\n            'endpoint_resolver', resolver\n        )\n        self.session.get_available_regions('foo', 'bar', True)\n\n    def test_provides_empty_list_for_unknown_service_regions(self):\n        regions = self.session.get_available_regions('__foo__')\n        self.assertEqual([], regions)\n\n\nclass TestSessionUserAgent(BaseSessionTest):\n    def test_can_change_user_agent_name(self):\n        self.session.user_agent_name = 'something-else'\n        self.assertTrue(self.session.user_agent().startswith('something-else'))\n\n    def test_can_change_user_agent_version(self):\n        self.session.user_agent_version = '24.0'\n        self.assertTrue(self.session.user_agent().startswith('Botocore/24.0'))\n\n    def test_can_append_to_user_agent(self):\n        self.session.user_agent_extra = 'custom-thing/other'\n        self.assertTrue(\n            self.session.user_agent().endswith('custom-thing/other')\n        )\n\n    def test_execution_env_not_set(self):\n        self.assertFalse(self.session.user_agent().endswith('FooEnv'))\n\n    def test_execution_env_set(self):\n        self.environ['AWS_EXECUTION_ENV'] = 'FooEnv'\n        self.assertTrue(self.session.user_agent().endswith(' exec-env/FooEnv'))\n\n    def test_agent_extra_and_exec_env(self):\n        self.session.user_agent_extra = 'custom-thing/other'\n        self.environ['AWS_EXECUTION_ENV'] = 'FooEnv'\n        user_agent = self.session.user_agent()\n        self.assertTrue(user_agent.endswith('custom-thing/other'))\n        self.assertIn('exec-env/FooEnv', user_agent)\n\n\nclass TestConfigLoaderObject(BaseSessionTest):\n    def test_config_loader_delegation(self):\n        session = create_session(\n            session_vars=self.env_vars, profile='credfile-profile'\n        )\n        with temporary_file('w') as f:\n            f.write('[credfile-profile]\\naws_access_key_id=a\\n')\n            f.write('aws_secret_access_key=b\\n')\n            f.flush()\n            session.set_config_variable('credentials_file', f.name)\n            # Now trying to retrieve the scoped config should pull in\n            # values from the shared credentials file.\n            self.assertEqual(\n                session.get_scoped_config(),\n                {'aws_access_key_id': 'a', 'aws_secret_access_key': 'b'},\n            )\n\n\nclass TestGetServiceModel(BaseSessionTest):\n    def test_get_service_model(self):\n        loader = mock.Mock()\n        loader.load_service_model.return_value = {\n            'metadata': {'serviceId': 'foo'}\n        }\n        self.session.register_component('data_loader', loader)\n        model = self.session.get_service_model('made_up')\n        self.assertIsInstance(model, ServiceModel)\n        self.assertEqual(model.service_name, 'made_up')\n\n\nclass TestGetPaginatorModel(BaseSessionTest):\n    def test_get_paginator_model(self):\n        loader = mock.Mock()\n        loader.load_service_model.return_value = {\"pagination\": {}}\n        self.session.register_component('data_loader', loader)\n\n        model = self.session.get_paginator_model('foo')\n\n        # Verify we get a PaginatorModel back\n        self.assertIsInstance(model, PaginatorModel)\n        # Verify we called the loader correctly.\n        loader.load_service_model.assert_called_with(\n            'foo', 'paginators-1', None\n        )\n\n\nclass TestGetWaiterModel(BaseSessionTest):\n    def test_get_waiter_model(self):\n        loader = mock.Mock()\n        loader.load_service_model.return_value = {\"version\": 2, \"waiters\": {}}\n        self.session.register_component('data_loader', loader)\n\n        model = self.session.get_waiter_model('foo')\n\n        # Verify we (1) get the expected return data,\n        self.assertIsInstance(model, WaiterModel)\n        self.assertEqual(model.waiter_names, [])\n        # and (2) call the loader correctly.\n        loader.load_service_model.assert_called_with('foo', 'waiters-2', None)\n\n\nclass TestCreateClient(BaseSessionTest):\n    def test_can_create_client(self):\n        sts_client = self.session.create_client('sts', 'us-west-2')\n        self.assertIsInstance(sts_client, client.BaseClient)\n\n    def test_credential_provider_not_called_when_creds_provided(self):\n        cred_provider = mock.Mock()\n        self.session.register_component('credential_provider', cred_provider)\n        self.session.create_client(\n            'sts',\n            'us-west-2',\n            aws_access_key_id='foo',\n            aws_secret_access_key='bar',\n            aws_session_token='baz',\n        )\n        self.assertFalse(\n            cred_provider.load_credentials.called,\n            \"Credential provider was called even though \"\n            \"explicit credentials were provided to the \"\n            \"create_client call.\",\n        )\n\n    def test_cred_provider_called_when_partial_creds_provided(self):\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            self.session.create_client(\n                'sts',\n                'us-west-2',\n                aws_access_key_id='foo',\n                aws_secret_access_key=None,\n            )\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            self.session.create_client(\n                'sts',\n                'us-west-2',\n                aws_access_key_id=None,\n                aws_secret_access_key='foo',\n            )\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_config_passed_to_client_creator(self, client_creator):\n        # Make sure there is no default set\n        self.assertEqual(self.session.get_default_client_config(), None)\n\n        # The config passed to the client should be the one that is used\n        # in creating the client.\n        config = botocore.config.Config(region_name='us-west-2')\n        self.session.create_client('sts', config=config)\n        client_creator.return_value.create_client.assert_called_with(\n            service_name=mock.ANY,\n            region_name=mock.ANY,\n            is_secure=mock.ANY,\n            endpoint_url=mock.ANY,\n            verify=mock.ANY,\n            credentials=mock.ANY,\n            scoped_config=mock.ANY,\n            client_config=config,\n            api_version=mock.ANY,\n            auth_token=mock.ANY,\n        )\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_default_client_config(self, client_creator):\n        config = botocore.config.Config()\n        self.session.set_default_client_config(config)\n        self.session.create_client('sts')\n\n        client_creator.return_value.create_client.assert_called_with(\n            service_name=mock.ANY,\n            region_name=mock.ANY,\n            is_secure=mock.ANY,\n            endpoint_url=mock.ANY,\n            verify=mock.ANY,\n            credentials=mock.ANY,\n            scoped_config=mock.ANY,\n            client_config=config,\n            api_version=mock.ANY,\n            auth_token=mock.ANY,\n        )\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_merging_client_configs(self, client_creator):\n        config = botocore.config.Config(region_name='us-west-2')\n        other_config = botocore.config.Config(region_name='us-east-1')\n        self.session.set_default_client_config(config)\n        self.session.create_client('sts', config=other_config)\n\n        # Grab the client config used in creating the client\n        used_client_config = (\n            client_creator.return_value.create_client.call_args[1][\n                'client_config'\n            ]\n        )\n        # Check that the client configs were merged\n        self.assertEqual(used_client_config.region_name, 'us-east-1')\n        # Make sure that the client config used is not the default client\n        # config or the one passed in. It should be a new config.\n        self.assertIsNot(used_client_config, config)\n        self.assertIsNot(used_client_config, other_config)\n\n    def test_create_client_with_region(self):\n        ec2_client = self.session.create_client('ec2', 'us-west-2')\n        self.assertEqual(ec2_client.meta.region_name, 'us-west-2')\n\n    def test_create_client_with_region_and_client_config(self):\n        config = botocore.config.Config()\n        # Use a client config with no region configured.\n        ec2_client = self.session.create_client(\n            'ec2', region_name='us-west-2', config=config\n        )\n        self.assertEqual(ec2_client.meta.region_name, 'us-west-2')\n\n        # If the region name is changed, it should not change the\n        # region of the client\n        config.region_name = 'us-east-1'\n        self.assertEqual(ec2_client.meta.region_name, 'us-west-2')\n\n        # Now make a new client with the updated client config.\n        ec2_client = self.session.create_client('ec2', config=config)\n        self.assertEqual(ec2_client.meta.region_name, 'us-east-1')\n\n    def test_create_client_no_region_and_no_client_config(self):\n        ec2_client = self.session.create_client('ec2')\n        self.assertEqual(ec2_client.meta.region_name, 'us-west-11')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_ca_bundle_from_config(self, client_creator):\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            self.session = create_session(session_vars=self.env_vars)\n            f.write('[default]\\n')\n            f.write('foo_ca_bundle=config-certs.pem\\n')\n            f.flush()\n\n            self.session.create_client('ec2', 'us-west-2')\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(call_kwargs['verify'], 'config-certs.pem')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_ca_bundle_from_env_var(self, client_creator):\n        self.environ['FOO_AWS_CA_BUNDLE'] = 'env-certs.pem'\n        self.session.create_client('ec2', 'us-west-2')\n        call_kwargs = client_creator.return_value.create_client.call_args[1]\n        self.assertEqual(call_kwargs['verify'], 'env-certs.pem')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_verify_param(self, client_creator):\n        self.session.create_client(\n            'ec2', 'us-west-2', verify='verify-certs.pem'\n        )\n        call_kwargs = client_creator.return_value.create_client.call_args[1]\n        self.assertEqual(call_kwargs['verify'], 'verify-certs.pem')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_verify_param_overrides_all(self, client_creator):\n        with temporary_file('w') as f:\n            # Set the ca cert using the config file\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            self.session = create_session(session_vars=self.env_vars)\n            f.write('[default]\\n')\n            f.write('foo_ca_bundle=config-certs.pem\\n')\n            f.flush()\n\n            # Set the ca cert with an environment variable\n            self.environ['FOO_AWS_CA_BUNDLE'] = 'env-certs.pem'\n\n            # Set the ca cert using the verify parameter\n            self.session.create_client(\n                'ec2', 'us-west-2', verify='verify-certs.pem'\n            )\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            # The verify parameter should override all the other\n            # configurations\n            self.assertEqual(call_kwargs['verify'], 'verify-certs.pem')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_use_no_api_version_by_default(self, client_creator):\n        self.session.create_client('myservice', 'us-west-2')\n        call_kwargs = client_creator.return_value.create_client.call_args[1]\n        self.assertEqual(call_kwargs['api_version'], None)\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_uses_api_version_from_config(self, client_creator):\n        config_api_version = '2012-01-01'\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            self.session = create_session(session_vars=self.env_vars)\n            f.write('[default]\\n')\n            f.write(\n                'foo_api_versions =\\n'\n                '    myservice = %s\\n' % config_api_version\n            )\n            f.flush()\n\n            self.session.create_client('myservice', 'us-west-2')\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(call_kwargs['api_version'], config_api_version)\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_can_specify_multiple_versions_from_config(self, client_creator):\n        config_api_version = '2012-01-01'\n        second_config_api_version = '2013-01-01'\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            self.session = create_session(session_vars=self.env_vars)\n            f.write('[default]\\n')\n            f.write(\n                f'foo_api_versions =\\n'\n                f'    myservice = {config_api_version}\\n'\n                f'    myservice2 = {second_config_api_version}\\n'\n            )\n            f.flush()\n\n            self.session.create_client('myservice', 'us-west-2')\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(call_kwargs['api_version'], config_api_version)\n\n            self.session.create_client('myservice2', 'us-west-2')\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(\n                call_kwargs['api_version'], second_config_api_version\n            )\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_param_api_version_overrides_config_value(self, client_creator):\n        config_api_version = '2012-01-01'\n        override_api_version = '2014-01-01'\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            self.session = create_session(session_vars=self.env_vars)\n            f.write('[default]\\n')\n            f.write(\n                'foo_api_versions =\\n'\n                '    myservice = %s\\n' % config_api_version\n            )\n            f.flush()\n\n            self.session.create_client(\n                'myservice', 'us-west-2', api_version=override_api_version\n            )\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(call_kwargs['api_version'], override_api_version)\n\n\nclass TestSessionComponent(BaseSessionTest):\n    def test_internal_component(self):\n        component = object()\n        self.session._register_internal_component('internal', component)\n        self.assertIs(\n            self.session._get_internal_component('internal'), component\n        )\n        with self.assertRaises(ValueError):\n            self.session.get_component('internal')\n\n    def test_internal_endpoint_resolver_is_same_as_deprecated_public(self):\n        endpoint_resolver = self.session._get_internal_component(\n            'endpoint_resolver'\n        )\n        # get_component has been deprecated to the public\n        with pytest.warns(DeprecationWarning):\n            self.assertIs(\n                self.session.get_component('endpoint_resolver'),\n                endpoint_resolver,\n            )\n\n    def test_internal_exceptions_factory_is_same_as_deprecated_public(self):\n        exceptions_factory = self.session._get_internal_component(\n            'exceptions_factory'\n        )\n        # get_component has been deprecated to the public\n        with pytest.warns(DeprecationWarning):\n            self.assertIs(\n                self.session.get_component('exceptions_factory'),\n                exceptions_factory,\n            )\n\n\nclass TestComponentLocator(unittest.TestCase):\n    def setUp(self):\n        self.components = botocore.session.ComponentLocator()\n\n    def test_unknown_component_raises_exception(self):\n        with self.assertRaises(ValueError):\n            self.components.get_component('unknown-component')\n\n    def test_can_register_and_retrieve_component(self):\n        component = object()\n        self.components.register_component('foo', component)\n        self.assertIs(self.components.get_component('foo'), component)\n\n    def test_last_registration_wins(self):\n        first = object()\n        second = object()\n        self.components.register_component('foo', first)\n        self.components.register_component('foo', second)\n        self.assertIs(self.components.get_component('foo'), second)\n\n    def test_can_lazy_register_a_component(self):\n        component = object()\n        lazy = lambda: component\n        self.components.lazy_register_component('foo', lazy)\n        self.assertIs(self.components.get_component('foo'), component)\n\n    def test_latest_registration_wins_even_if_lazy(self):\n        first = object()\n        second = object()\n        lazy_second = lambda: second\n        self.components.register_component('foo', first)\n        self.components.lazy_register_component('foo', lazy_second)\n        self.assertIs(self.components.get_component('foo'), second)\n\n    def test_latest_registration_overrides_lazy(self):\n        first = object()\n        second = object()\n        lazy_first = lambda: first\n        self.components.lazy_register_component('foo', lazy_first)\n        self.components.register_component('foo', second)\n        self.assertIs(self.components.get_component('foo'), second)\n\n    def test_lazy_registration_factory_does_not_remove_from_list_on_error(\n        self,\n    ):\n        class ArbitraryError(Exception):\n            pass\n\n        def bad_factory():\n            raise ArbitraryError(\"Factory raises an exception.\")\n\n        self.components.lazy_register_component('foo', bad_factory)\n\n        with self.assertRaises(ArbitraryError):\n            self.components.get_component('foo')\n\n        # Trying again should raise the same exception,\n        # not an ValueError(\"Unknown component\")\n        with self.assertRaises(ArbitraryError):\n            self.components.get_component('foo')\n\n\nclass TestDefaultClientConfig(BaseSessionTest):\n    def test_new_session_has_no_default_client_config(self):\n        self.assertEqual(self.session.get_default_client_config(), None)\n\n    def test_set_and_get_client_config(self):\n        client_config = botocore.config.Config()\n        self.session.set_default_client_config(client_config)\n        self.assertIs(self.session.get_default_client_config(), client_config)\n", "tests/unit/test_model.py": "import pytest\n\nfrom botocore import model\nfrom botocore.compat import OrderedDict\nfrom tests import unittest\n\n\n@pytest.mark.parametrize(\"property_name\", ['api_version', 'protocol'])\ndef test_missing_model_attribute_raises_exception(property_name):\n    service_model = model.ServiceModel({'metadata': {'endpointPrefix': 'foo'}})\n    with pytest.raises(model.UndefinedModelAttributeError):\n        getattr(service_model, property_name)\n\n\nclass TestServiceId(unittest.TestCase):\n    def test_hypenize_replaces_spaces(self):\n        self.assertEqual(\n            model.ServiceId('my service').hyphenize(), 'my-service'\n        )\n\n    def test_hyphenize_lower_cases(self):\n        self.assertEqual(model.ServiceId('MyService').hyphenize(), 'myservice')\n\n\nclass TestServiceModel(unittest.TestCase):\n    def setUp(self):\n        self.model = {\n            'metadata': {\n                'protocol': 'query',\n                'endpointPrefix': 'endpoint-prefix',\n                'serviceId': 'MyService',\n            },\n            'documentation': 'Documentation value',\n            'operations': {},\n            'shapes': {'StringShape': {'type': 'string'}},\n        }\n        self.error_shapes = {\n            'ExceptionOne': {\n                'exception': True,\n                'type': 'structure',\n                'members': {},\n            },\n            'ExceptionTwo': {\n                'exception': True,\n                'type': 'structure',\n                'members': {},\n                'error': {'code': 'FooCode'},\n            },\n        }\n        self.service_model = model.ServiceModel(self.model)\n\n    def test_metadata_available(self):\n        # You should be able to access the metadata in a service description\n        # through the service model object.\n        self.assertEqual(self.service_model.metadata.get('protocol'), 'query')\n\n    def test_service_name_can_be_overriden(self):\n        service_model = model.ServiceModel(\n            self.model, service_name='myservice'\n        )\n        self.assertEqual(service_model.service_name, 'myservice')\n\n    def test_service_name_defaults_to_endpoint_prefix(self):\n        self.assertEqual(self.service_model.service_name, 'endpoint-prefix')\n\n    def test_service_id(self):\n        self.assertEqual(self.service_model.service_id, 'MyService')\n\n    def test_hyphenize_service_id(self):\n        self.assertEqual(\n            self.service_model.service_id.hyphenize(), 'myservice'\n        )\n\n    def test_service_id_does_not_exist(self):\n        service_model = {\n            'metadata': {\n                'protocol': 'query',\n                'endpointPrefix': 'endpoint-prefix',\n            },\n            'documentation': 'Documentation value',\n            'operations': {},\n            'shapes': {'StringShape': {'type': 'string'}},\n        }\n        service_name = 'myservice'\n        service_model = model.ServiceModel(service_model, service_name)\n        with self.assertRaisesRegex(\n            model.UndefinedModelAttributeError, service_name\n        ):\n            service_model.service_id\n\n    def test_operation_does_not_exist(self):\n        with self.assertRaises(model.OperationNotFoundError):\n            self.service_model.operation_model('NoExistOperation')\n\n    def test_signing_name_defaults_to_endpoint_prefix(self):\n        self.assertEqual(self.service_model.signing_name, 'endpoint-prefix')\n\n    def test_documentation_exposed_as_property(self):\n        self.assertEqual(\n            self.service_model.documentation, 'Documentation value'\n        )\n\n    def test_shape_names(self):\n        self.assertEqual(self.service_model.shape_names, ['StringShape'])\n\n    def test_repr_has_service_name(self):\n        self.assertEqual(\n            repr(self.service_model), 'ServiceModel(endpoint-prefix)'\n        )\n\n    def test_shape_for_error_code(self):\n        self.model['shapes'].update(self.error_shapes)\n        self.service_model = model.ServiceModel(self.model)\n        shape = self.service_model.shape_for_error_code('ExceptionOne')\n        self.assertEqual(shape.name, 'ExceptionOne')\n        shape = self.service_model.shape_for_error_code('FooCode')\n        self.assertEqual(shape.name, 'ExceptionTwo')\n\n    def test_error_shapes(self):\n        self.model['shapes'].update(self.error_shapes)\n        self.service_model = model.ServiceModel(self.model)\n        error_shapes = self.service_model.error_shapes\n        error_shape_names = [shape.name for shape in error_shapes]\n        self.assertEqual(len(error_shape_names), 2)\n        self.assertIn('ExceptionOne', error_shape_names)\n        self.assertIn('ExceptionTwo', error_shape_names)\n\n    def test_client_context_params(self):\n        service_model = model.ServiceModel(\n            {\n                'metadata': {\n                    'protocol': 'query',\n                    'endpointPrefix': 'endpoint-prefix',\n                    'serviceId': 'MyServiceWithClientContextParams',\n                },\n                'documentation': 'Documentation value',\n                'operations': {},\n                'shapes': {},\n                'clientContextParams': {\n                    'stringClientContextParam': {\n                        'type': 'string',\n                        'documentation': 'str-valued',\n                    },\n                    'booleanClientContextParam': {\n                        'type': 'boolean',\n                        'documentation': 'bool-valued',\n                    },\n                },\n            }\n        )\n        self.assertEqual(len(service_model.client_context_parameters), 2)\n        client_ctx_param1 = service_model.client_context_parameters[0]\n        client_ctx_param2 = service_model.client_context_parameters[1]\n        self.assertEqual(client_ctx_param1.name, 'stringClientContextParam')\n        self.assertEqual(client_ctx_param1.type, 'string')\n        self.assertEqual(client_ctx_param1.documentation, 'str-valued')\n        self.assertEqual(client_ctx_param2.name, 'booleanClientContextParam')\n\n    def test_client_context_params_absent(self):\n        self.assertIsInstance(\n            self.service_model.client_context_parameters, list\n        )\n        self.assertEqual(len(self.service_model.client_context_parameters), 0)\n\n\nclass TestOperationModelFromService(unittest.TestCase):\n    def setUp(self):\n        self.model = {\n            'metadata': {'protocol': 'query', 'endpointPrefix': 'foo'},\n            'documentation': '',\n            'operations': {\n                'OperationName': {\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'name': 'OperationName',\n                    'input': {'shape': 'OperationNameRequest'},\n                    'output': {\n                        'shape': 'OperationNameResponse',\n                    },\n                    'errors': [{'shape': 'NoSuchResourceException'}],\n                    'documentation': 'Docs for OperationName',\n                    'authtype': 'v4',\n                },\n                'OperationTwo': {\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'name': 'OperationTwo',\n                    'input': {'shape': 'OperationNameRequest'},\n                    'output': {\n                        'shape': 'OperationNameResponse',\n                    },\n                    'errors': [{'shape': 'NoSuchResourceException'}],\n                    'documentation': 'Docs for OperationTwo',\n                },\n                'PayloadOperation': {\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'name': 'PayloadOperation',\n                    'input': {'shape': 'PayloadOperationRequest'},\n                    'output': {\n                        'shape': 'PayloadOperationResponse',\n                    },\n                    'errors': [{'shape': 'NoSuchResourceException'}],\n                    'documentation': 'Docs for PayloadOperation',\n                    'requestcompression': {'encodings': ['gzip']},\n                },\n                'NoBodyOperation': {\n                    'http': {\n                        'method': 'GET',\n                        'requestUri': '/',\n                    },\n                    'name': 'NoBodyOperation',\n                    'input': {'shape': 'NoBodyOperationRequest'},\n                    'output': {\n                        'shape': 'OperationNameResponse',\n                    },\n                    'errors': [{'shape': 'NoSuchResourceException'}],\n                    'documentation': 'Docs for NoBodyOperation',\n                },\n                'ContextParamOperation': {\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'name': 'ContextParamOperation',\n                    'input': {'shape': 'ContextParamOperationRequest'},\n                    'output': {'shape': 'OperationNameResponse'},\n                    'errors': [{'shape': 'NoSuchResourceException'}],\n                    'documentation': 'Docs for ContextParamOperation',\n                    'staticContextParams': {\n                        'stringStaticContextParam': {\n                            'value': 'Static Context Param Value',\n                        },\n                        'booleanStaticContextParam': {\n                            'value': True,\n                        },\n                    },\n                },\n            },\n            'shapes': {\n                'OperationNameRequest': {\n                    'type': 'structure',\n                    'members': {\n                        'Arg1': {'shape': 'stringType'},\n                        'Arg2': {'shape': 'stringType'},\n                    },\n                },\n                'OperationNameResponse': {\n                    'type': 'structure',\n                    'members': {\n                        'String': {\n                            'shape': 'stringType',\n                        }\n                    },\n                },\n                'PayloadOperationRequest': {\n                    'type': 'structure',\n                    'members': {\n                        'Arg1': {'shape': 'TestConfig'},\n                        'Arg2': {'shape': 'stringType'},\n                    },\n                    'payload': 'Arg1',\n                },\n                'PayloadOperationResponse': {\n                    'type': 'structure',\n                    'members': {\n                        'String': {\n                            'shape': 'stringType',\n                        }\n                    },\n                    'payload': 'String',\n                },\n                'NoBodyOperationRequest': {\n                    'type': 'structure',\n                    'members': {\n                        'data': {\n                            'location': 'header',\n                            'locationName': 'x-amz-data',\n                            'shape': 'stringType',\n                        }\n                    },\n                },\n                'ContextParamOperationRequest': {\n                    'type': 'structure',\n                    'members': {\n                        'ContextParamArg': {\n                            'shape': 'stringType',\n                            'contextParam': {\n                                'name': 'contextParamName',\n                            },\n                        }\n                    },\n                    'payload': 'ContextParamArg',\n                },\n                'NoSuchResourceException': {\n                    'type': 'structure',\n                    'members': {},\n                },\n                'stringType': {\n                    'type': 'string',\n                },\n                'TestConfig': {\n                    'type': 'structure',\n                    'members': {'timeout': {'shape': 'stringType'}},\n                },\n            },\n        }\n        self.service_model = model.ServiceModel(self.model)\n\n    def test_wire_name_always_matches_model(self):\n        service_model = model.ServiceModel(self.model)\n        operation = model.OperationModel(\n            self.model['operations']['OperationName'], service_model, 'Foo'\n        )\n        self.assertEqual(operation.name, 'Foo')\n        self.assertEqual(operation.wire_name, 'OperationName')\n\n    def test_operation_name_in_repr(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertIn('OperationName', repr(operation))\n\n    def test_name_and_wire_name_defaults_to_same_value(self):\n        service_model = model.ServiceModel(self.model)\n        operation = model.OperationModel(\n            self.model['operations']['OperationName'], service_model\n        )\n        self.assertEqual(operation.name, 'OperationName')\n        self.assertEqual(operation.wire_name, 'OperationName')\n\n    def test_name_from_service(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertEqual(operation.name, 'OperationName')\n\n    def test_name_from_service_model_when_differs_from_name(self):\n        self.model['operations']['Foo'] = self.model['operations'][\n            'OperationName'\n        ]\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('Foo')\n        self.assertEqual(operation.name, 'Foo')\n\n    def test_operation_input_model(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertEqual(operation.name, 'OperationName')\n        # Operations should also have a reference to the top level metadata.\n        self.assertEqual(operation.metadata['protocol'], 'query')\n        self.assertEqual(operation.http['method'], 'POST')\n        self.assertEqual(operation.http['requestUri'], '/')\n        shape = operation.input_shape\n        self.assertEqual(shape.name, 'OperationNameRequest')\n        self.assertEqual(list(sorted(shape.members)), ['Arg1', 'Arg2'])\n\n    def test_has_documentation_property(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertEqual(operation.documentation, 'Docs for OperationName')\n\n    def test_service_model_available_from_operation_model(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        # This is an identity comparison because we don't implement\n        # __eq__, so we may need to change this in the future.\n        self.assertEqual(operation.service_model, service_model)\n\n    def test_operation_output_model(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        output = operation.output_shape\n        self.assertEqual(list(output.members), ['String'])\n        self.assertFalse(operation.has_streaming_output)\n\n    def test_operation_shape_not_required(self):\n        # It's ok if there's no output shape. We'll just get a return value of\n        # None.\n        del self.model['operations']['OperationName']['output']\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        output_shape = operation.output_shape\n        self.assertIsNone(output_shape)\n\n    def test_error_shapes(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        # OperationName only has a NoSuchResourceException\n        self.assertEqual(len(operation.error_shapes), 1)\n        self.assertEqual(\n            operation.error_shapes[0].name, 'NoSuchResourceException'\n        )\n\n    def test_has_auth_type(self):\n        operation = self.service_model.operation_model('OperationName')\n        self.assertEqual(operation.auth_type, 'v4')\n\n    def test_auth_type_not_set(self):\n        operation = self.service_model.operation_model('OperationTwo')\n        self.assertIsNone(operation.auth_type)\n\n    def test_deprecated_present(self):\n        self.model['operations']['OperationName']['deprecated'] = True\n        service_model = model.ServiceModel(self.model)\n        operation_name = service_model.operation_model('OperationName')\n        self.assertTrue(operation_name.deprecated)\n\n    def test_deprecated_present_false(self):\n        self.model['operations']['OperationName']['deprecated'] = False\n        service_model = model.ServiceModel(self.model)\n        operation_name = service_model.operation_model('OperationName')\n        self.assertFalse(operation_name.deprecated)\n\n    def test_deprecated_absent(self):\n        service_model = model.ServiceModel(self.model)\n        operation_two = service_model.operation_model('OperationTwo')\n        self.assertFalse(operation_two.deprecated)\n\n    def test_endpoint_operation_present(self):\n        self.model['operations']['OperationName']['endpointoperation'] = True\n        service_model = model.ServiceModel(self.model)\n        operation_name = service_model.operation_model('OperationName')\n        self.assertTrue(operation_name.is_endpoint_discovery_operation)\n\n    def test_endpoint_operation_present_false(self):\n        self.model['operations']['OperationName']['endpointoperation'] = False\n        service_model = model.ServiceModel(self.model)\n        operation_name = service_model.operation_model('OperationName')\n        self.assertFalse(operation_name.is_endpoint_discovery_operation)\n\n    def test_endpoint_operation_absent(self):\n        operation_two = self.service_model.operation_model('OperationName')\n        self.assertFalse(operation_two.is_endpoint_discovery_operation)\n\n    def test_endpoint_discovery_required(self):\n        operation = self.model['operations']['OperationName']\n        operation['endpointdiscovery'] = {'required': True}\n        service_model = model.ServiceModel(self.model)\n        self.assertTrue(service_model.endpoint_discovery_required)\n\n    def test_endpoint_discovery_required_false(self):\n        self.model['operations']['OperationName']['endpointdiscovery'] = {}\n        service_model = model.ServiceModel(self.model)\n        self.assertFalse(service_model.endpoint_discovery_required)\n\n    def test_endpoint_discovery_required_no_value(self):\n        operation = self.model['operations']['OperationName']\n        self.assertTrue(operation.get('endpointdiscovery') is None)\n        service_model = model.ServiceModel(self.model)\n        self.assertFalse(service_model.endpoint_discovery_required)\n\n    def test_endpoint_discovery_present(self):\n        operation = self.model['operations']['OperationName']\n        operation['endpointdiscovery'] = {'required': True}\n        service_model = model.ServiceModel(self.model)\n        operation_name = service_model.operation_model('OperationName')\n        self.assertTrue(operation_name.endpoint_discovery.get('required'))\n\n    def test_endpoint_discovery_absent(self):\n        operation_name = self.service_model.operation_model('OperationName')\n        self.assertIsNone(operation_name.endpoint_discovery)\n\n    def test_http_checksum_absent(self):\n        operation_name = self.service_model.operation_model('OperationName')\n        self.assertEqual(operation_name.http_checksum, {})\n\n    def test_http_checksum_present(self):\n        operation = self.model['operations']['OperationName']\n        operation['httpChecksum'] = {\n            \"requestChecksumRequired\": True,\n            \"requestAlgorithmMember\": \"ChecksumAlgorithm\",\n            \"requestValidationModeMember\": \"ChecksumMode\",\n            \"responseAlgorithms\": [\"crc32\", \"crc32c\", \"sha256\", \"sha1\"],\n        }\n        service_model = model.ServiceModel(self.model)\n        operation_model = service_model.operation_model('OperationName')\n        http_checksum = operation_model.http_checksum\n        self.assertEqual(\n            http_checksum[\"requestChecksumRequired\"],\n            True,\n        )\n        self.assertEqual(\n            http_checksum[\"requestAlgorithmMember\"],\n            \"ChecksumAlgorithm\",\n        )\n        self.assertEqual(\n            http_checksum[\"requestValidationModeMember\"],\n            \"ChecksumMode\",\n        )\n        self.assertEqual(\n            http_checksum[\"responseAlgorithms\"],\n            [\"crc32\", \"crc32c\", \"sha256\", \"sha1\"],\n        )\n\n    def test_context_parameter_present(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('ContextParamOperation')\n        self.assertEqual(len(operation.context_parameters), 1)\n        context_param = operation.context_parameters[0]\n        self.assertEqual(context_param.name, 'contextParamName')\n        self.assertEqual(context_param.member_name, 'ContextParamArg')\n\n    def test_context_parameter_absent(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationTwo')\n        self.assertIsInstance(operation.context_parameters, list)\n        self.assertEqual(len(operation.context_parameters), 0)\n\n    def test_static_context_parameter_present(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('ContextParamOperation')\n        self.assertEqual(len(operation.static_context_parameters), 2)\n        static_ctx_param1 = operation.static_context_parameters[0]\n        static_ctx_param2 = operation.static_context_parameters[1]\n        self.assertEqual(static_ctx_param1.name, 'stringStaticContextParam')\n        self.assertEqual(static_ctx_param1.value, 'Static Context Param Value')\n        self.assertEqual(static_ctx_param2.name, 'booleanStaticContextParam')\n        self.assertEqual(static_ctx_param2.value, True)\n\n    def test_static_context_parameter_absent(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationTwo')\n        self.assertIsInstance(operation.static_context_parameters, list)\n        self.assertEqual(len(operation.static_context_parameters), 0)\n\n    def test_request_compression(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('PayloadOperation')\n        self.assertEqual(\n            operation.request_compression, {'encodings': ['gzip']}\n        )\n\n\nclass TestOperationModelEventStreamTypes(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n        self.model = {\n            'metadata': {'protocol': 'rest-xml', 'endpointPrefix': 'foo'},\n            'documentation': '',\n            'operations': {\n                'OperationName': {\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'name': 'OperationName',\n                    'input': {'shape': 'OperationRequest'},\n                    'output': {'shape': 'OperationResponse'},\n                }\n            },\n            'shapes': {\n                'NormalStructure': {\n                    'type': 'structure',\n                    'members': {'Input': {'shape': 'StringType'}},\n                },\n                'OperationRequest': {\n                    'type': 'structure',\n                    'members': {\n                        'String': {'shape': 'StringType'},\n                        \"Body\": {'shape': 'EventStreamStructure'},\n                    },\n                    'payload': 'Body',\n                },\n                'OperationResponse': {\n                    'type': 'structure',\n                    'members': {\n                        'String': {'shape': 'StringType'},\n                        \"Body\": {'shape': 'EventStreamStructure'},\n                    },\n                    'payload': 'Body',\n                },\n                'StringType': {'type': 'string'},\n                'BlobType': {'type': 'blob'},\n                'EventStreamStructure': {\n                    'eventstream': True,\n                    'type': 'structure',\n                    'members': {\n                        'EventA': {'shape': 'EventAStructure'},\n                        'EventB': {'shape': 'EventBStructure'},\n                    },\n                },\n                'EventAStructure': {\n                    'event': True,\n                    'type': 'structure',\n                    'members': {\n                        'Payload': {'shape': 'BlobType', 'eventpayload': True},\n                        'Header': {'shape': 'StringType', 'eventheader': True},\n                    },\n                },\n                'EventBStructure': {\n                    'event': True,\n                    'type': 'structure',\n                    'members': {'Records': {'shape': 'StringType'}},\n                },\n            },\n        }\n\n    def update_operation(self, **kwargs):\n        operation = self.model['operations']['OperationName']\n        operation.update(kwargs)\n\n    def test_event_stream_input_for_operation(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertTrue(operation.has_event_stream_input)\n        event_stream_input = operation.get_event_stream_input()\n        self.assertEqual(event_stream_input.name, 'EventStreamStructure')\n\n    def test_no_event_stream_input_for_operation(self):\n        self.update_operation(input={'shape': 'NormalStructure'})\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertFalse(operation.has_event_stream_input)\n        self.assertEqual(operation.get_event_stream_input(), None)\n\n    def test_event_stream_output_for_operation(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertTrue(operation.has_event_stream_output)\n        output = operation.get_event_stream_output()\n        self.assertEqual(output.name, 'EventStreamStructure')\n\n    def test_no_event_stream_output_for_operation(self):\n        self.update_operation(output={'shape': 'NormalStructure'})\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertFalse(operation.has_event_stream_output)\n        self.assertEqual(operation.get_event_stream_output(), None)\n\n    def test_no_output_shape(self):\n        self.update_operation(output=None)\n        del self.model['operations']['OperationName']['output']\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertFalse(operation.has_event_stream_output)\n        self.assertEqual(operation.get_event_stream_output(), None)\n\n\nclass TestOperationModelStreamingTypes(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n        self.model = {\n            'metadata': {'protocol': 'query', 'endpointPrefix': 'foo'},\n            'documentation': '',\n            'operations': {\n                'OperationName': {\n                    'name': 'OperationName',\n                    'input': {\n                        'shape': 'OperationRequest',\n                    },\n                    'output': {\n                        'shape': 'OperationResponse',\n                    },\n                }\n            },\n            'shapes': {\n                'OperationRequest': {\n                    'type': 'structure',\n                    'members': {\n                        'String': {\n                            'shape': 'stringType',\n                        },\n                        \"Body\": {\n                            'shape': 'blobType',\n                        },\n                    },\n                    'payload': 'Body',\n                },\n                'OperationResponse': {\n                    'type': 'structure',\n                    'members': {\n                        'String': {\n                            'shape': 'stringType',\n                        },\n                        \"Body\": {\n                            'shape': 'blobType',\n                        },\n                    },\n                    'payload': 'Body',\n                },\n                'stringType': {\n                    'type': 'string',\n                },\n                'blobType': {'type': 'blob'},\n            },\n        }\n\n    def remove_payload(self, type):\n        self.model['shapes']['Operation' + type].pop('payload')\n\n    def test_streaming_input_for_operation(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertTrue(operation.has_streaming_input)\n        self.assertEqual(operation.get_streaming_input().name, 'blobType')\n\n    def test_not_streaming_input_for_operation(self):\n        self.remove_payload('Request')\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertFalse(operation.has_streaming_input)\n        self.assertEqual(operation.get_streaming_input(), None)\n\n    def test_streaming_output_for_operation(self):\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertTrue(operation.has_streaming_output)\n        self.assertEqual(operation.get_streaming_output().name, 'blobType')\n\n    def test_not_streaming_output_for_operation(self):\n        self.remove_payload('Response')\n        service_model = model.ServiceModel(self.model)\n        operation = service_model.operation_model('OperationName')\n        self.assertFalse(operation.has_streaming_output)\n        self.assertEqual(operation.get_streaming_output(), None)\n\n\nclass TestDeepMerge(unittest.TestCase):\n    def setUp(self):\n        self.shapes = {\n            'SetQueueAttributes': {\n                'type': 'structure',\n                'members': {\n                    'MapExample': {\n                        'shape': 'StrToStrMap',\n                        'locationName': 'Attribute',\n                    },\n                },\n            },\n            'SetQueueAttributes2': {\n                'type': 'structure',\n                'members': {\n                    'MapExample': {\n                        'shape': 'StrToStrMap',\n                        'locationName': 'Attribute2',\n                    },\n                },\n            },\n            'StrToStrMap': {\n                'type': 'map',\n                'key': {'shape': 'StringType', 'locationName': 'Name'},\n                'value': {'shape': 'StringType', 'locationName': 'Value'},\n                'flattened': True,\n                'name': 'NotAttribute',\n            },\n            'StringType': {'type': 'string'},\n        }\n        self.shape_resolver = model.ShapeResolver(self.shapes)\n\n    def test_deep_merge(self):\n        shape = self.shape_resolver.get_shape_by_name('SetQueueAttributes')\n        map_merged = shape.members['MapExample']\n        # map_merged has a serialization as a member trait as well as\n        # in the StrToStrMap.\n        # The member trait should have precedence.\n        self.assertEqual(\n            map_merged.serialization,\n            # member beats the definition.\n            {\n                'name': 'Attribute',\n                # From the definition.\n                'flattened': True,\n            },\n        )\n        # Ensure we don't merge/mutate the original dicts.\n        self.assertEqual(map_merged.key.serialization['name'], 'Name')\n        self.assertEqual(map_merged.value.serialization['name'], 'Value')\n        self.assertEqual(map_merged.key.serialization['name'], 'Name')\n\n    def test_merges_copy_dict(self):\n        shape = self.shape_resolver.get_shape_by_name('SetQueueAttributes')\n        map_merged = shape.members['MapExample']\n        self.assertEqual(map_merged.serialization.get('name'), 'Attribute')\n\n        shape2 = self.shape_resolver.get_shape_by_name('SetQueueAttributes2')\n        map_merged2 = shape2.members['MapExample']\n        self.assertEqual(map_merged2.serialization.get('name'), 'Attribute2')\n\n\nclass TestShapeResolver(unittest.TestCase):\n    def test_get_shape_by_name(self):\n        shape_map = {\n            'Foo': {\n                'type': 'structure',\n                'members': {\n                    'Bar': {'shape': 'StringType'},\n                    'Baz': {'shape': 'StringType'},\n                },\n            },\n            \"StringType\": {\"type\": \"string\"},\n        }\n        resolver = model.ShapeResolver(shape_map)\n        shape = resolver.get_shape_by_name('Foo')\n        self.assertEqual(shape.name, 'Foo')\n        self.assertEqual(shape.type_name, 'structure')\n\n    def test_resolve_shape_reference(self):\n        shape_map = {\n            'Foo': {\n                'type': 'structure',\n                'members': {\n                    'Bar': {'shape': 'StringType'},\n                    'Baz': {'shape': 'StringType'},\n                },\n            },\n            \"StringType\": {\"type\": \"string\"},\n        }\n        resolver = model.ShapeResolver(shape_map)\n        shape = resolver.resolve_shape_ref({'shape': 'StringType'})\n        self.assertEqual(shape.name, 'StringType')\n        self.assertEqual(shape.type_name, 'string')\n\n    def test_resolve_shape_references_with_member_traits(self):\n        shape_map = {\n            'Foo': {\n                'type': 'structure',\n                'members': {\n                    'Bar': {'shape': 'StringType'},\n                    'Baz': {'shape': 'StringType', 'locationName': 'other'},\n                },\n            },\n            \"StringType\": {\"type\": \"string\"},\n        }\n        resolver = model.ShapeResolver(shape_map)\n        shape = resolver.resolve_shape_ref(\n            {'shape': 'StringType', 'locationName': 'other'}\n        )\n        self.assertEqual(shape.serialization['name'], 'other')\n        self.assertEqual(shape.name, 'StringType')\n\n    def test_serialization_cache(self):\n        shape_map = {\n            'Foo': {\n                'type': 'structure',\n                'members': {\n                    'Baz': {'shape': 'StringType', 'locationName': 'other'},\n                },\n            },\n            \"StringType\": {\"type\": \"string\"},\n        }\n        resolver = model.ShapeResolver(shape_map)\n        shape = resolver.resolve_shape_ref(\n            {'shape': 'StringType', 'locationName': 'other'}\n        )\n        self.assertEqual(shape.serialization['name'], 'other')\n        # serialization is computed on demand, and a cache is kept.\n        # This is just verifying that trying to access serialization again\n        # gives the same result.  We don't actually care that it's cached,\n        # we just care that the cache doesn't mess with correctness.\n        self.assertEqual(shape.serialization['name'], 'other')\n\n    def test_shape_overrides(self):\n        shape_map = {\n            \"StringType\": {\n                \"type\": \"string\",\n                \"documentation\": \"Original documentation\",\n            }\n        }\n        resolver = model.ShapeResolver(shape_map)\n        shape = resolver.get_shape_by_name('StringType')\n        self.assertEqual(shape.documentation, 'Original documentation')\n\n        shape = resolver.resolve_shape_ref(\n            {'shape': 'StringType', 'documentation': 'override'}\n        )\n        self.assertEqual(shape.documentation, 'override')\n\n    def test_shape_type_structure(self):\n        shapes = {\n            'ChangePasswordRequest': {\n                'type': 'structure',\n                'members': {\n                    'OldPassword': {'shape': 'passwordType'},\n                    'NewPassword': {'shape': 'passwordType'},\n                },\n            },\n            'passwordType': {\n                \"type\": \"string\",\n            },\n        }\n        resolver = model.ShapeResolver(shapes)\n        shape = resolver.get_shape_by_name('ChangePasswordRequest')\n        self.assertEqual(shape.type_name, 'structure')\n        self.assertEqual(shape.name, 'ChangePasswordRequest')\n        self.assertEqual(\n            list(sorted(shape.members)), ['NewPassword', 'OldPassword']\n        )\n        self.assertEqual(shape.members['OldPassword'].name, 'passwordType')\n        self.assertEqual(shape.members['OldPassword'].type_name, 'string')\n        self.assertEqual(shape.error_code, None)\n\n    def test_exception_error_code(self):\n        shapes = {\n            'FooException': {\n                'exception': True,\n                'type': 'structure',\n                'members': {},\n            }\n        }\n        # Test without explicit error code\n        resolver = model.ShapeResolver(shapes)\n        shape = resolver.get_shape_by_name('FooException')\n        self.assertTrue(shape.metadata['exception'])\n        self.assertEqual(shape.error_code, 'FooException')\n        # Test with explicit error code\n        shapes['FooException']['error'] = {'code': 'ExceptionCode'}\n        resolver = model.ShapeResolver(shapes)\n        shape = resolver.get_shape_by_name('FooException')\n        self.assertTrue(shape.metadata['exception'])\n        self.assertEqual(shape.error_code, 'ExceptionCode')\n\n    def test_shape_metadata(self):\n        shapes = {\n            'ChangePasswordRequest': {\n                'type': 'structure',\n                'required': ['OldPassword', 'NewPassword'],\n                'members': {\n                    'OldPassword': {'shape': 'passwordType'},\n                    'NewPassword': {'shape': 'passwordType'},\n                },\n            },\n            'passwordType': {\n                \"type\": \"string\",\n                \"min\": 1,\n                \"max\": 128,\n                \"pattern\": \".*\",\n                \"sensitive\": True,\n            },\n        }\n        resolver = model.ShapeResolver(shapes)\n        shape = resolver.get_shape_by_name('ChangePasswordRequest')\n        self.assertEqual(\n            shape.metadata['required'], ['OldPassword', 'NewPassword']\n        )\n        member = shape.members['OldPassword']\n        self.assertEqual(member.metadata['min'], 1)\n        self.assertEqual(member.metadata['max'], 128)\n        self.assertEqual(member.metadata['pattern'], '.*')\n        self.assertEqual(member.metadata['sensitive'], True)\n\n    def test_error_shape_metadata(self):\n        shapes = {\n            'ResourceNotFoundException': {\n                'type': 'structure',\n                'members': {\n                    'message': {\n                        'shape': 'ErrorMessage',\n                    }\n                },\n                'exception': True,\n                'retryable': {'throttling': True},\n            }\n        }\n        resolver = model.ShapeResolver(shapes)\n        shape = resolver.get_shape_by_name('ResourceNotFoundException')\n        self.assertEqual(\n            shape.metadata,\n            {'exception': True, 'retryable': {'throttling': True}},\n        )\n\n    def test_shape_list(self):\n        shapes = {\n            'mfaDeviceListType': {\n                \"type\": \"list\",\n                \"member\": {\"shape\": \"MFADevice\"},\n            },\n            'MFADevice': {\n                'type': 'structure',\n                'members': {'UserName': {'shape': 'userNameType'}},\n            },\n            'userNameType': {'type': 'string'},\n        }\n        resolver = model.ShapeResolver(shapes)\n        shape = resolver.get_shape_by_name('mfaDeviceListType')\n        self.assertEqual(shape.member.type_name, 'structure')\n        self.assertEqual(shape.member.name, 'MFADevice')\n        self.assertEqual(list(shape.member.members), ['UserName'])\n\n    def test_shape_does_not_exist(self):\n        resolver = model.ShapeResolver({})\n        with self.assertRaises(model.NoShapeFoundError):\n            resolver.get_shape_by_name('NoExistShape')\n\n    def test_missing_type_key(self):\n        shapes = {'UnknownType': {'NotTheTypeKey': 'someUnknownType'}}\n        resolver = model.ShapeResolver(shapes)\n        with self.assertRaises(model.InvalidShapeError):\n            resolver.get_shape_by_name('UnknownType')\n\n    def test_bad_shape_ref(self):\n        # This is an example of a denormalized model,\n        # which should raise an exception.\n        shapes = {\n            'Struct': {\n                'type': 'structure',\n                'members': {\n                    'A': {'type': 'string'},\n                    'B': {'type': 'string'},\n                },\n            }\n        }\n        resolver = model.ShapeResolver(shapes)\n        with self.assertRaises(model.InvalidShapeReferenceError):\n            struct = resolver.get_shape_by_name('Struct')\n            # Resolving the members will fail because\n            # the 'A' and 'B' members are not shape refs.\n            struct.members\n\n    def test_shape_name_in_repr(self):\n        shapes = {\n            'StringType': {\n                'type': 'string',\n            }\n        }\n        resolver = model.ShapeResolver(shapes)\n        self.assertIn(\n            'StringType', repr(resolver.get_shape_by_name('StringType'))\n        )\n\n\nclass TestBuilders(unittest.TestCase):\n    def test_structure_shape_builder_with_scalar_types(self):\n        b = model.DenormalizedStructureBuilder()\n        shape = b.with_members(\n            {\n                'A': {'type': 'string'},\n                'B': {'type': 'integer'},\n            }\n        ).build_model()\n        self.assertIsInstance(shape, model.StructureShape)\n        self.assertEqual(sorted(list(shape.members)), ['A', 'B'])\n        self.assertEqual(shape.members['A'].type_name, 'string')\n        self.assertEqual(shape.members['B'].type_name, 'integer')\n\n    def test_structure_shape_with_structure_type(self):\n        b = model.DenormalizedStructureBuilder()\n        shape = b.with_members(\n            {\n                'A': {\n                    'type': 'structure',\n                    'members': {\n                        'A-1': {'type': 'string'},\n                    },\n                },\n            }\n        ).build_model()\n        self.assertIsInstance(shape, model.StructureShape)\n        self.assertEqual(list(shape.members), ['A'])\n        self.assertEqual(shape.members['A'].type_name, 'structure')\n        self.assertEqual(list(shape.members['A'].members), ['A-1'])\n\n    def test_structure_shape_with_list(self):\n        b = model.DenormalizedStructureBuilder()\n        shape = b.with_members(\n            {\n                'A': {'type': 'list', 'member': {'type': 'string'}},\n            }\n        ).build_model()\n        self.assertIsInstance(shape.members['A'], model.ListShape)\n        self.assertEqual(shape.members['A'].member.type_name, 'string')\n\n    def test_structure_shape_with_map_type(self):\n        b = model.DenormalizedStructureBuilder()\n        shape = b.with_members(\n            {\n                'A': {\n                    'type': 'map',\n                    'key': {'type': 'string'},\n                    'value': {'type': 'string'},\n                }\n            }\n        ).build_model()\n        self.assertIsInstance(shape.members['A'], model.MapShape)\n        map_shape = shape.members['A']\n        self.assertEqual(map_shape.key.type_name, 'string')\n        self.assertEqual(map_shape.value.type_name, 'string')\n\n    def test_nested_structure(self):\n        b = model.DenormalizedStructureBuilder()\n        shape = b.with_members(\n            {\n                'A': {\n                    'type': 'structure',\n                    'members': {\n                        'B': {\n                            'type': 'structure',\n                            'members': {\n                                'C': {\n                                    'type': 'string',\n                                }\n                            },\n                        }\n                    },\n                }\n            }\n        ).build_model()\n        self.assertEqual(\n            shape.members['A'].members['B'].members['C'].type_name, 'string'\n        )\n\n    def test_enum_values_on_string_used(self):\n        b = model.DenormalizedStructureBuilder()\n        enum_values = ['foo', 'bar', 'baz']\n        shape = b.with_members(\n            {\n                'A': {\n                    'type': 'string',\n                    'enum': enum_values,\n                },\n            }\n        ).build_model()\n        self.assertIsInstance(shape, model.StructureShape)\n        string_shape = shape.members['A']\n        self.assertIsInstance(string_shape, model.StringShape)\n        self.assertEqual(string_shape.metadata['enum'], enum_values)\n        self.assertEqual(string_shape.enum, enum_values)\n\n    def test_documentation_on_shape_used(self):\n        b = model.DenormalizedStructureBuilder()\n        shape = b.with_members(\n            {\n                'A': {\n                    'type': 'string',\n                    'documentation': 'MyDocs',\n                },\n            }\n        ).build_model()\n        self.assertEqual(shape.members['A'].documentation, 'MyDocs')\n\n    def test_min_max_used_in_metadata(self):\n        b = model.DenormalizedStructureBuilder()\n        shape = b.with_members(\n            {\n                'A': {\n                    'type': 'string',\n                    'documentation': 'MyDocs',\n                    'min': 2,\n                    'max': 3,\n                },\n            }\n        ).build_model()\n        metadata = shape.members['A'].metadata\n        self.assertEqual(metadata.get('min'), 2)\n        self.assertEqual(metadata.get('max'), 3)\n\n    def test_use_shape_name_when_provided(self):\n        b = model.DenormalizedStructureBuilder()\n        shape = b.with_members(\n            {\n                'A': {\n                    'type': 'string',\n                    'shape_name': 'MyStringShape',\n                },\n            }\n        ).build_model()\n        self.assertEqual(shape.members['A'].name, 'MyStringShape')\n\n    def test_unknown_shape_type(self):\n        b = model.DenormalizedStructureBuilder()\n        with self.assertRaises(model.InvalidShapeError):\n            b.with_members(\n                {\n                    'A': {\n                        'type': 'brand-new-shape-type',\n                    },\n                }\n            ).build_model()\n\n    def test_ordered_shape_builder(self):\n        b = model.DenormalizedStructureBuilder()\n        shape = b.with_members(\n            OrderedDict(\n                [\n                    ('A', {'type': 'string'}),\n                    (\n                        'B',\n                        {\n                            'type': 'structure',\n                            'members': OrderedDict(\n                                [\n                                    ('C', {'type': 'string'}),\n                                    ('D', {'type': 'string'}),\n                                ]\n                            ),\n                        },\n                    ),\n                ]\n            )\n        ).build_model()\n\n        # Members should be in order\n        self.assertEqual(['A', 'B'], list(shape.members.keys()))\n\n        # Nested structure members should *also* stay ordered\n        self.assertEqual(['C', 'D'], list(shape.members['B'].members.keys()))\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/unit/test_configloader.py": "#!/usr/bin/env\n# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport shutil\nimport tempfile\n\nimport botocore.exceptions\nfrom botocore.configloader import (\n    load_config,\n    multi_file_load_config,\n    raw_config_parse,\n)\nfrom tests import mock, unittest\n\n\ndef path(filename):\n    directory = os.path.join(os.path.dirname(__file__), 'cfg')\n    if isinstance(filename, bytes):\n        directory = directory.encode('latin-1')\n    return os.path.join(directory, filename)\n\n\nclass TestConfigLoader(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def create_config_file(self, filename):\n        contents = (\n            '[default]\\n'\n            'aws_access_key_id = foo\\n'\n            'aws_secret_access_key = bar\\n\\n'\n            '[profile \"personal\"]\\n'\n            'aws_access_key_id = fie\\n'\n            'aws_secret_access_key = baz\\n'\n            'aws_security_token = fiebaz\\n'\n        )\n\n        directory = self.tempdir\n        if isinstance(filename, bytes):\n            directory = directory.encode('latin-1')\n        full_path = os.path.join(directory, filename)\n\n        with open(full_path, 'w') as f:\n            f.write(contents)\n        return full_path\n\n    def test_config_not_found(self):\n        with self.assertRaises(botocore.exceptions.ConfigNotFound):\n            raw_config_parse(path('aws_config_notfound'))\n\n    def test_config_parse_error(self):\n        filename = path('aws_config_bad')\n        with self.assertRaises(botocore.exceptions.ConfigParseError):\n            raw_config_parse(filename)\n\n    def test_config_parse_error_bad_unicode(self):\n        filename = path('aws_config_badbytes')\n        with self.assertRaises(botocore.exceptions.ConfigParseError):\n            raw_config_parse(filename)\n\n    def test_config_parse_error_filesystem_encoding_none(self):\n        filename = path('aws_config_bad')\n        with mock.patch('sys.getfilesystemencoding') as encoding:\n            encoding.return_value = None\n            with self.assertRaises(botocore.exceptions.ConfigParseError):\n                raw_config_parse(filename)\n\n    def test_config(self):\n        loaded_config = raw_config_parse(path('aws_config'))\n        self.assertIn('default', loaded_config)\n        self.assertIn('profile \"personal\"', loaded_config)\n\n    def test_profile_map_conversion(self):\n        loaded_config = load_config(path('aws_config'))\n        self.assertIn('profiles', loaded_config)\n        self.assertEqual(\n            sorted(loaded_config['profiles'].keys()), ['default', 'personal']\n        )\n\n    def test_bad_profiles_are_ignored(self):\n        filename = path('aws_bad_profile')\n        loaded_config = load_config(filename)\n        self.assertEqual(len(loaded_config['profiles']), 3)\n        profiles = loaded_config['profiles']\n        self.assertIn('my profile', profiles)\n        self.assertIn('personal1', profiles)\n        self.assertIn('default', profiles)\n\n    def test_nested_hierarchy_parsing(self):\n        filename = path('aws_config_nested')\n        loaded_config = load_config(filename)\n        config = loaded_config['profiles']['default']\n        self.assertEqual(config['aws_access_key_id'], 'foo')\n        self.assertEqual(config['region'], 'us-west-2')\n        self.assertEqual(config['s3']['signature_version'], 's3v4')\n        self.assertEqual(config['cloudwatch']['signature_version'], 'v4')\n\n    def test_nested_hierarchy_with_no_subsection_parsing(self):\n        filename = path('aws_config_nested')\n        raw_config = raw_config_parse(filename, False)['default']\n        self.assertEqual(raw_config['aws_access_key_id'], 'foo')\n        self.assertEqual(raw_config['region'], 'us-west-2')\n        # Specifying False for pase_subsections in raw_config_parse\n        # will make sure that indented sections such as singature_version\n        # will not be treated as another subsection but rather\n        # its literal value.\n        self.assertEqual(raw_config['cloudwatch'], '\\nsignature_version = v4')\n        self.assertEqual(\n            raw_config['s3'],\n            '\\nsignature_version = s3v4' '\\naddressing_style = path',\n        )\n\n    def test_nested_bad_config(self):\n        filename = path('aws_config_nested_bad')\n        with self.assertRaises(botocore.exceptions.ConfigParseError):\n            load_config(filename)\n\n    def test_nested_bad_config_filesystem_encoding_none(self):\n        filename = path('aws_config_nested_bad')\n        with mock.patch('sys.getfilesystemencoding') as encoding:\n            encoding.return_value = None\n            with self.assertRaises(botocore.exceptions.ConfigParseError):\n                load_config(filename)\n\n    def test_multi_file_load(self):\n        filenames = [\n            path('aws_config_other'),\n            path('aws_config'),\n            path('aws_third_config'),\n            path('aws_config_notfound'),\n        ]\n        loaded_config = multi_file_load_config(*filenames)\n        config = loaded_config['profiles']['default']\n        self.assertEqual(config['aws_access_key_id'], 'other_foo')\n        self.assertEqual(config['aws_secret_access_key'], 'other_bar')\n        second_config = loaded_config['profiles']['personal']\n        self.assertEqual(second_config['aws_access_key_id'], 'fie')\n        self.assertEqual(second_config['aws_secret_access_key'], 'baz')\n        self.assertEqual(second_config['aws_security_token'], 'fiebaz')\n        third_config = loaded_config['profiles']['third']\n        self.assertEqual(third_config['aws_access_key_id'], 'third_fie')\n        self.assertEqual(third_config['aws_secret_access_key'], 'third_baz')\n        self.assertEqual(third_config['aws_security_token'], 'third_fiebaz')\n\n    def test_unicode_bytes_path_not_found(self):\n        with self.assertRaises(botocore.exceptions.ConfigNotFound):\n            with mock.patch('sys.getfilesystemencoding') as encoding:\n                encoding.return_value = 'utf-8'\n                load_config(path(b'\\xe2\\x9c\\x93'))\n\n    def test_unicode_bytes_path_not_found_filesystem_encoding_none(self):\n        with mock.patch('sys.getfilesystemencoding') as encoding:\n            encoding.return_value = None\n            with self.assertRaises(botocore.exceptions.ConfigNotFound):\n                load_config(path(b'\\xe2\\x9c\\x93'))\n\n    def test_unicode_bytes_path(self):\n        filename = self.create_config_file(b'aws_config_unicode\\xe2\\x9c\\x93')\n        with mock.patch('sys.getfilesystemencoding') as encoding:\n            encoding.return_value = 'utf-8'\n            loaded_config = load_config(filename)\n        self.assertIn('default', loaded_config['profiles'])\n        self.assertIn('personal', loaded_config['profiles'])\n\n    def test_sso_session_config(self):\n        filename = path('aws_sso_session_config')\n        loaded_config = load_config(filename)\n        self.assertIn('profiles', loaded_config)\n        self.assertIn('default', loaded_config['profiles'])\n        self.assertIn('sso_sessions', loaded_config)\n        self.assertIn('sso', loaded_config['sso_sessions'])\n        sso_config = loaded_config['sso_sessions']['sso']\n        self.assertEqual(sso_config['sso_region'], 'us-east-1')\n        self.assertEqual(sso_config['sso_start_url'], 'https://example.com')\n\n    def test_services_config(self):\n        filename = path('aws_services_config')\n        loaded_config = load_config(filename)\n        self.assertIn('profiles', loaded_config)\n        self.assertIn('default', loaded_config['profiles'])\n        self.assertIn('services', loaded_config)\n        self.assertIn('my-services', loaded_config['services'])\n        services_config = loaded_config['services']['my-services']\n        self.assertIn('s3', services_config)\n        self.assertIn('dynamodb', services_config)\n        self.assertEqual(\n            services_config['s3']['endpoint_url'], 'https://localhost:5678/'\n        )\n        self.assertEqual(\n            services_config['dynamodb']['endpoint_url'],\n            'https://localhost:8888/',\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/unit/test_errorfactory.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.errorfactory import BaseClientExceptions, ClientExceptionsFactory\nfrom botocore.exceptions import ClientError\nfrom botocore.model import ServiceModel\nfrom tests import unittest\n\n\nclass TestBaseClientExceptions(unittest.TestCase):\n    def setUp(self):\n        self.code_to_exception = {}\n        self.exceptions = BaseClientExceptions(self.code_to_exception)\n\n    def test_has_client_error(self):\n        self.assertIs(self.exceptions.ClientError, ClientError)\n\n    def test_from_code(self):\n        exception_cls = type('MyException', (ClientError,), {})\n        self.code_to_exception['MyExceptionCode'] = exception_cls\n        self.assertIs(\n            self.exceptions.from_code('MyExceptionCode'), exception_cls\n        )\n\n    def test_from_code_nonmatch_defaults_to_client_error(self):\n        self.assertIs(\n            self.exceptions.from_code('SomeUnknownErrorCode'), ClientError\n        )\n\n    def test_gettattr_message(self):\n        exception_cls = type('MyException', (ClientError,), {})\n        self.code_to_exception['MyExceptionCode'] = exception_cls\n        with self.assertRaisesRegex(\n            AttributeError, 'Valid exceptions are: MyException'\n        ):\n            self.exceptions.SomeUnmodeledError\n\n\nclass TestClientExceptionsFactory(unittest.TestCase):\n    def setUp(self):\n        self.model = {\n            \"metadata\": {\n                'endpointPrefix': 'myservice',\n                'serviceFullName': 'MyService',\n            },\n            'operations': {\n                'OperationName': {\n                    'name': 'OperationName',\n                    'errors': [\n                        {'shape': 'ExceptionMissingCode'},\n                        {'shape': 'ExceptionWithModeledCode'},\n                    ],\n                },\n                'AnotherOperationName': {\n                    'name': 'AnotherOperationName',\n                    'errors': [\n                        {'shape': 'ExceptionForAnotherOperation'},\n                        {'shape': 'ExceptionWithModeledCode'},\n                    ],\n                },\n            },\n            'shapes': {\n                'ExceptionWithModeledCode': {\n                    'type': 'structure',\n                    'members': {},\n                    'error': {'code': 'ModeledCode'},\n                    'exception': True,\n                },\n                'ExceptionMissingCode': {\n                    'type': 'structure',\n                    'members': {},\n                    'exception': True,\n                },\n                'ExceptionForAnotherOperation': {\n                    'type': 'structure',\n                    'members': {},\n                    'exception': True,\n                },\n            },\n        }\n        self.service_model = ServiceModel(self.model)\n        self.exceptions_factory = ClientExceptionsFactory()\n\n    def test_class_name(self):\n        exceptions = self.exceptions_factory.create_client_exceptions(\n            self.service_model\n        )\n        self.assertEqual(exceptions.__class__.__name__, 'MyServiceExceptions')\n\n    def test_creates_modeled_exception(self):\n        exceptions = self.exceptions_factory.create_client_exceptions(\n            self.service_model\n        )\n        self.assertTrue(hasattr(exceptions, 'ExceptionWithModeledCode'))\n        modeled_exception = exceptions.ExceptionWithModeledCode\n        self.assertEqual(\n            modeled_exception.__name__, 'ExceptionWithModeledCode'\n        )\n        self.assertTrue(issubclass(modeled_exception, ClientError))\n\n    def test_collects_modeled_exceptions_for_all_operations(self):\n        exceptions = self.exceptions_factory.create_client_exceptions(\n            self.service_model\n        )\n        # Make sure exceptions were added for all operations by checking\n        # an exception only found on an a different operation.\n        self.assertTrue(hasattr(exceptions, 'ExceptionForAnotherOperation'))\n        modeled_exception = exceptions.ExceptionForAnotherOperation\n        self.assertEqual(\n            modeled_exception.__name__, 'ExceptionForAnotherOperation'\n        )\n        self.assertTrue(issubclass(modeled_exception, ClientError))\n\n    def test_creates_modeled_exception_mapping_that_has_code(self):\n        exceptions = self.exceptions_factory.create_client_exceptions(\n            self.service_model\n        )\n        exception = exceptions.from_code('ModeledCode')\n        self.assertEqual(exception.__name__, 'ExceptionWithModeledCode')\n        self.assertTrue(issubclass(exception, ClientError))\n\n    def test_creates_modeled_exception_mapping_that_has_no_code(self):\n        exceptions = self.exceptions_factory.create_client_exceptions(\n            self.service_model\n        )\n        # For exceptions that do not have an explicit code associated to them,\n        # the code is the name of the exception.\n        exception = exceptions.from_code('ExceptionMissingCode')\n        self.assertEqual(exception.__name__, 'ExceptionMissingCode')\n        self.assertTrue(issubclass(exception, ClientError))\n", "tests/unit/test_credentials.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport shutil\nimport subprocess\nimport tempfile\nimport time\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nimport pytest\nfrom dateutil.tz import tzlocal, tzutc\n\nimport botocore.exceptions\nimport botocore.session\nfrom botocore import credentials\nfrom botocore.compat import json\nfrom botocore.configprovider import ConfigValueStore\nfrom botocore.credentials import (\n    AssumeRoleProvider,\n    AssumeRoleWithWebIdentityProvider,\n    ConfigProvider,\n    CredentialProvider,\n    Credentials,\n    EnvProvider,\n    ProcessProvider,\n    ProfileProviderBuilder,\n    SharedCredentialProvider,\n    SSOCredentialFetcher,\n    SSOProvider,\n    create_assume_role_refresher,\n)\nfrom botocore.session import Session\nfrom botocore.stub import Stubber\nfrom botocore.utils import (\n    ContainerMetadataFetcher,\n    FileWebIdentityTokenLoader,\n    SSOTokenLoader,\n    datetime2timestamp,\n)\nfrom tests import (\n    BaseEnvVar,\n    IntegerRefresher,\n    mock,\n    skip_if_windows,\n    temporary_file,\n    unittest,\n)\n\n# Passed to session to keep it from finding default config file\nTESTENVVARS = {'config_file': (None, 'AWS_CONFIG_FILE', None)}\n\n\nraw_metadata = {\n    'foobar': {\n        'Code': 'Success',\n        'LastUpdated': '2012-12-03T14:38:21Z',\n        'AccessKeyId': 'foo',\n        'SecretAccessKey': 'bar',\n        'Token': 'foobar',\n        'Expiration': '2012-12-03T20:48:03Z',\n        'Type': 'AWS-HMAC',\n    }\n}\npost_processed_metadata = {\n    'role_name': 'foobar',\n    'access_key': raw_metadata['foobar']['AccessKeyId'],\n    'secret_key': raw_metadata['foobar']['SecretAccessKey'],\n    'token': raw_metadata['foobar']['Token'],\n    'expiry_time': raw_metadata['foobar']['Expiration'],\n}\n\n\ndef path(filename):\n    return os.path.join(os.path.dirname(__file__), 'cfg', filename)\n\n\nclass TestCredentials(BaseEnvVar):\n    def _ensure_credential_is_normalized_as_unicode(self, access, secret):\n        c = credentials.Credentials(access, secret)\n        self.assertTrue(isinstance(c.access_key, str))\n        self.assertTrue(isinstance(c.secret_key, str))\n\n    def test_detect_nonascii_character(self):\n        self._ensure_credential_is_normalized_as_unicode(\n            'foo\\xe2\\x80\\x99', 'bar\\xe2\\x80\\x99'\n        )\n\n    def test_unicode_input(self):\n        self._ensure_credential_is_normalized_as_unicode('foo', 'bar')\n\n\nclass TestRefreshableCredentials(TestCredentials):\n    def setUp(self):\n        super().setUp()\n        self.refresher = mock.Mock()\n        self.future_time = datetime.now(tzlocal()) + timedelta(hours=24)\n        self.expiry_time = datetime.now(tzlocal()) - timedelta(minutes=30)\n        self.metadata = {\n            'access_key': 'NEW-ACCESS',\n            'secret_key': 'NEW-SECRET',\n            'token': 'NEW-TOKEN',\n            'expiry_time': self.future_time.isoformat(),\n            'role_name': 'rolename',\n        }\n        self.refresher.return_value = self.metadata\n        self.mock_time = mock.Mock()\n        self.creds = credentials.RefreshableCredentials(\n            'ORIGINAL-ACCESS',\n            'ORIGINAL-SECRET',\n            'ORIGINAL-TOKEN',\n            self.expiry_time,\n            self.refresher,\n            'iam-role',\n            time_fetcher=self.mock_time,\n        )\n\n    def test_refresh_needed(self):\n        # The expiry time was set for 30 minutes ago, so if we\n        # say the current time is utcnow(), then we should need\n        # a refresh.\n        self.mock_time.return_value = datetime.now(tzlocal())\n        self.assertTrue(self.creds.refresh_needed())\n        # We should refresh creds, if we try to access \"access_key\"\n        # or any of the cred vars.\n        self.assertEqual(self.creds.access_key, 'NEW-ACCESS')\n        self.assertEqual(self.creds.secret_key, 'NEW-SECRET')\n        self.assertEqual(self.creds.token, 'NEW-TOKEN')\n\n    def test_no_expiration(self):\n        creds = credentials.RefreshableCredentials(\n            'ORIGINAL-ACCESS',\n            'ORIGINAL-SECRET',\n            'ORIGINAL-TOKEN',\n            None,\n            self.refresher,\n            'iam-role',\n            time_fetcher=self.mock_time,\n        )\n        self.assertFalse(creds.refresh_needed())\n\n    def test_no_refresh_needed(self):\n        # The expiry time was 30 minutes ago, let's say it's an hour\n        # ago currently.  That would mean we don't need a refresh.\n        self.mock_time.return_value = datetime.now(tzlocal()) - timedelta(\n            minutes=60\n        )\n        self.assertTrue(not self.creds.refresh_needed())\n\n        self.assertEqual(self.creds.access_key, 'ORIGINAL-ACCESS')\n        self.assertEqual(self.creds.secret_key, 'ORIGINAL-SECRET')\n        self.assertEqual(self.creds.token, 'ORIGINAL-TOKEN')\n\n    def test_get_credentials_set(self):\n        # We need to return a consistent set of credentials to use during the\n        # signing process.\n        self.mock_time.return_value = datetime.now(tzlocal()) - timedelta(\n            minutes=60\n        )\n        self.assertTrue(not self.creds.refresh_needed())\n        credential_set = self.creds.get_frozen_credentials()\n        self.assertEqual(credential_set.access_key, 'ORIGINAL-ACCESS')\n        self.assertEqual(credential_set.secret_key, 'ORIGINAL-SECRET')\n        self.assertEqual(credential_set.token, 'ORIGINAL-TOKEN')\n\n    def test_refresh_returns_empty_dict(self):\n        self.refresher.return_value = {}\n        self.mock_time.return_value = datetime.now(tzlocal())\n        self.assertTrue(self.creds.refresh_needed())\n\n        with self.assertRaises(botocore.exceptions.CredentialRetrievalError):\n            self.creds.access_key\n\n    def test_refresh_returns_none(self):\n        self.refresher.return_value = None\n        self.mock_time.return_value = datetime.now(tzlocal())\n        self.assertTrue(self.creds.refresh_needed())\n\n        with self.assertRaises(botocore.exceptions.CredentialRetrievalError):\n            self.creds.access_key\n\n    def test_refresh_returns_partial_credentials(self):\n        self.refresher.return_value = {'access_key': 'akid'}\n        self.mock_time.return_value = datetime.now(tzlocal())\n        self.assertTrue(self.creds.refresh_needed())\n\n        with self.assertRaises(botocore.exceptions.CredentialRetrievalError):\n            self.creds.access_key\n\n\nclass TestDeferredRefreshableCredentials(unittest.TestCase):\n    def setUp(self):\n        self.refresher = mock.Mock()\n        self.future_time = datetime.now(tzlocal()) + timedelta(hours=24)\n        self.metadata = {\n            'access_key': 'NEW-ACCESS',\n            'secret_key': 'NEW-SECRET',\n            'token': 'NEW-TOKEN',\n            'expiry_time': self.future_time.isoformat(),\n            'role_name': 'rolename',\n        }\n        self.refresher.return_value = self.metadata\n        self.mock_time = mock.Mock()\n        self.mock_time.return_value = datetime.now(tzlocal())\n\n    def test_refresh_using_called_on_first_access(self):\n        creds = credentials.DeferredRefreshableCredentials(\n            self.refresher, 'iam-role', self.mock_time\n        )\n\n        # The credentials haven't been accessed, so there should be no calls.\n        self.refresher.assert_not_called()\n\n        # Now that the object has been accessed, it should have called the\n        # refresher\n        creds.get_frozen_credentials()\n        self.assertEqual(self.refresher.call_count, 1)\n\n    def test_refresh_only_called_once(self):\n        creds = credentials.DeferredRefreshableCredentials(\n            self.refresher, 'iam-role', self.mock_time\n        )\n\n        for _ in range(5):\n            creds.get_frozen_credentials()\n\n        # The credentials were accessed several times in a row, but only\n        # should call refresh once.\n        self.assertEqual(self.refresher.call_count, 1)\n\n\nclass TestAssumeRoleCredentialFetcher(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.source_creds = credentials.Credentials('a', 'b', 'c')\n        self.role_arn = 'myrole'\n\n    def create_client_creator(self, with_response):\n        # Create a mock sts client that returns a specific response\n        # for assume_role.\n        client = mock.Mock()\n        if isinstance(with_response, list):\n            client.assume_role.side_effect = with_response\n        else:\n            client.assume_role.return_value = with_response\n        return mock.Mock(return_value=client)\n\n    def get_expected_creds_from_response(self, response):\n        expiration = response['Credentials']['Expiration']\n        if isinstance(expiration, datetime):\n            expiration = expiration.isoformat()\n        return {\n            'access_key': response['Credentials']['AccessKeyId'],\n            'secret_key': response['Credentials']['SecretAccessKey'],\n            'token': response['Credentials']['SessionToken'],\n            'expiry_time': expiration,\n        }\n\n    def some_future_time(self):\n        timeobj = datetime.now(tzlocal())\n        return timeobj + timedelta(hours=24)\n\n    def test_no_cache(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator, self.source_creds, self.role_arn\n        )\n\n        expected_response = self.get_expected_creds_from_response(response)\n        response = refresher.fetch_credentials()\n\n        self.assertEqual(response, expected_response)\n\n    def test_expiration_in_datetime_format(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                # Note the lack of isoformat(), we're using\n                # a datetime.datetime type.  This will ensure\n                # we test both parsing as well as serializing\n                # from a given datetime because the credentials\n                # are immediately expired.\n                'Expiration': self.some_future_time(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator, self.source_creds, self.role_arn\n        )\n\n        expected_response = self.get_expected_creds_from_response(response)\n        response = refresher.fetch_credentials()\n\n        self.assertEqual(response, expected_response)\n\n    def test_retrieves_from_cache(self):\n        date_in_future = datetime.utcnow() + timedelta(seconds=1000)\n        utc_timestamp = date_in_future.isoformat() + 'Z'\n        cache_key = '793d6e2f27667ab2da104824407e486bfec24a47'\n        cache = {\n            cache_key: {\n                'Credentials': {\n                    'AccessKeyId': 'foo-cached',\n                    'SecretAccessKey': 'bar-cached',\n                    'SessionToken': 'baz-cached',\n                    'Expiration': utc_timestamp,\n                }\n            }\n        }\n        client_creator = mock.Mock()\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator, self.source_creds, self.role_arn, cache=cache\n        )\n\n        expected_response = self.get_expected_creds_from_response(\n            cache[cache_key]\n        )\n        response = refresher.fetch_credentials()\n\n        self.assertEqual(response, expected_response)\n        client_creator.assert_not_called()\n\n    def test_cache_key_is_windows_safe(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        cache = {}\n        client_creator = self.create_client_creator(with_response=response)\n\n        role_arn = 'arn:aws:iam::role/foo-role'\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator, self.source_creds, role_arn, cache=cache\n        )\n\n        refresher.fetch_credentials()\n\n        # On windows, you cannot use a a ':' in the filename, so\n        # we need to make sure that it doesn't make it into the cache key.\n        cache_key = '75c539f0711ba78c5b9e488d0add95f178a54d74'\n        self.assertIn(cache_key, cache)\n        self.assertEqual(cache[cache_key], response)\n\n    def test_cache_key_with_role_session_name(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        cache = {}\n        client_creator = self.create_client_creator(with_response=response)\n        role_session_name = 'my_session_name'\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator,\n            self.source_creds,\n            self.role_arn,\n            cache=cache,\n            extra_args={'RoleSessionName': role_session_name},\n        )\n        refresher.fetch_credentials()\n\n        # This is the sha256 hex digest of the expected assume role args.\n        cache_key = '2964201f5648c8be5b9460a9cf842d73a266daf2'\n        self.assertIn(cache_key, cache)\n        self.assertEqual(cache[cache_key], response)\n\n    def test_cache_key_with_policy(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        cache = {}\n        client_creator = self.create_client_creator(with_response=response)\n        policy = json.dumps(\n            {\n                \"Version\": \"2012-10-17\",\n                \"Statement\": [\n                    {\"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\"}\n                ],\n            }\n        )\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator,\n            self.source_creds,\n            self.role_arn,\n            cache=cache,\n            extra_args={'Policy': policy},\n        )\n        refresher.fetch_credentials()\n\n        # This is the sha256 hex digest of the expected assume role args.\n        cache_key = '176f223d915e82456c253545e192aa21d68f5ab8'\n        self.assertIn(cache_key, cache)\n        self.assertEqual(cache[cache_key], response)\n\n    def test_assume_role_in_cache_but_expired(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        cache = {\n            'development--myrole': {\n                'Credentials': {\n                    'AccessKeyId': 'foo-cached',\n                    'SecretAccessKey': 'bar-cached',\n                    'SessionToken': 'baz-cached',\n                    'Expiration': datetime.now(tzlocal()),\n                }\n            }\n        }\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator, self.source_creds, self.role_arn, cache=cache\n        )\n        expected = self.get_expected_creds_from_response(response)\n        response = refresher.fetch_credentials()\n\n        self.assertEqual(response, expected)\n\n    def test_role_session_name_can_be_provided(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        role_session_name = 'myname'\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator,\n            self.source_creds,\n            self.role_arn,\n            extra_args={'RoleSessionName': role_session_name},\n        )\n        refresher.fetch_credentials()\n\n        client = client_creator.return_value\n        client.assume_role.assert_called_with(\n            RoleArn=self.role_arn, RoleSessionName=role_session_name\n        )\n\n    def test_external_id_can_be_provided(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        external_id = 'my_external_id'\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator,\n            self.source_creds,\n            self.role_arn,\n            extra_args={'ExternalId': external_id},\n        )\n        refresher.fetch_credentials()\n\n        client = client_creator.return_value\n        client.assume_role.assert_called_with(\n            RoleArn=self.role_arn,\n            ExternalId=external_id,\n            RoleSessionName=mock.ANY,\n        )\n\n    def test_policy_can_be_provided(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        policy = json.dumps(\n            {\n                \"Version\": \"2012-10-17\",\n                \"Statement\": [\n                    {\"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\"}\n                ],\n            }\n        )\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator,\n            self.source_creds,\n            self.role_arn,\n            extra_args={'Policy': policy},\n        )\n        refresher.fetch_credentials()\n\n        client = client_creator.return_value\n        client.assume_role.assert_called_with(\n            RoleArn=self.role_arn, Policy=policy, RoleSessionName=mock.ANY\n        )\n\n    def test_duration_seconds_can_be_provided(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        duration = 1234\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator,\n            self.source_creds,\n            self.role_arn,\n            extra_args={'DurationSeconds': duration},\n        )\n        refresher.fetch_credentials()\n\n        client = client_creator.return_value\n        client.assume_role.assert_called_with(\n            RoleArn=self.role_arn,\n            DurationSeconds=duration,\n            RoleSessionName=mock.ANY,\n        )\n\n    def test_mfa(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        prompter = mock.Mock(return_value='token-code')\n        mfa_serial = 'mfa'\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator,\n            self.source_creds,\n            self.role_arn,\n            extra_args={'SerialNumber': mfa_serial},\n            mfa_prompter=prompter,\n        )\n        refresher.fetch_credentials()\n\n        client = client_creator.return_value\n        # In addition to the normal assume role args, we should also\n        # inject the serial number from the config as well as the\n        # token code that comes from prompting the user (the prompter\n        # object).\n        client.assume_role.assert_called_with(\n            RoleArn='myrole',\n            RoleSessionName=mock.ANY,\n            SerialNumber='mfa',\n            TokenCode='token-code',\n        )\n\n    def test_refreshes(self):\n        responses = [\n            {\n                'Credentials': {\n                    'AccessKeyId': 'foo',\n                    'SecretAccessKey': 'bar',\n                    'SessionToken': 'baz',\n                    # We're creating an expiry time in the past so as\n                    # soon as we try to access the credentials, the\n                    # refresh behavior will be triggered.\n                    'Expiration': (\n                        datetime.now(tzlocal()) - timedelta(seconds=100)\n                    ).isoformat(),\n                },\n            },\n            {\n                'Credentials': {\n                    'AccessKeyId': 'foo',\n                    'SecretAccessKey': 'bar',\n                    'SessionToken': 'baz',\n                    'Expiration': self.some_future_time().isoformat(),\n                }\n            },\n        ]\n        client_creator = self.create_client_creator(with_response=responses)\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator, self.source_creds, self.role_arn\n        )\n\n        # The first call will simply use whatever credentials it is given.\n        # The second will check the cache, and only make a call if the\n        # cached credentials are expired.\n        refresher.fetch_credentials()\n        refresher.fetch_credentials()\n\n        client = client_creator.return_value\n        assume_role_calls = client.assume_role.call_args_list\n        self.assertEqual(len(assume_role_calls), 2, assume_role_calls)\n\n    def test_mfa_refresh_enabled(self):\n        responses = [\n            {\n                'Credentials': {\n                    'AccessKeyId': 'foo',\n                    'SecretAccessKey': 'bar',\n                    'SessionToken': 'baz',\n                    # We're creating an expiry time in the past so as\n                    # soon as we try to access the credentials, the\n                    # refresh behavior will be triggered.\n                    'Expiration': (\n                        datetime.now(tzlocal()) - timedelta(seconds=100)\n                    ).isoformat(),\n                },\n            },\n            {\n                'Credentials': {\n                    'AccessKeyId': 'foo',\n                    'SecretAccessKey': 'bar',\n                    'SessionToken': 'baz',\n                    'Expiration': self.some_future_time().isoformat(),\n                }\n            },\n        ]\n        client_creator = self.create_client_creator(with_response=responses)\n\n        token_code = 'token-code-1'\n        prompter = mock.Mock(side_effect=[token_code])\n        mfa_serial = 'mfa'\n\n        refresher = credentials.AssumeRoleCredentialFetcher(\n            client_creator,\n            self.source_creds,\n            self.role_arn,\n            extra_args={'SerialNumber': mfa_serial},\n            mfa_prompter=prompter,\n        )\n\n        # This is will refresh credentials if they're expired. Because\n        # we set the expiry time to something in the past, this will\n        # trigger the refresh behavior.\n        refresher.fetch_credentials()\n\n        assume_role = client_creator.return_value.assume_role\n        calls = [c[1] for c in assume_role.call_args_list]\n        expected_calls = [\n            {\n                'RoleArn': self.role_arn,\n                'RoleSessionName': mock.ANY,\n                'SerialNumber': mfa_serial,\n                'TokenCode': token_code,\n            }\n        ]\n        self.assertEqual(calls, expected_calls)\n\n\nclass TestAssumeRoleWithWebIdentityCredentialFetcher(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.role_arn = 'myrole'\n\n    def load_token(self):\n        return 'totally.a.token'\n\n    def some_future_time(self):\n        timeobj = datetime.now(tzlocal())\n        return timeobj + timedelta(hours=24)\n\n    def create_client_creator(self, with_response):\n        # Create a mock sts client that returns a specific response\n        # for assume_role.\n        client = mock.Mock()\n        if isinstance(with_response, list):\n            client.assume_role_with_web_identity.side_effect = with_response\n        else:\n            client.assume_role_with_web_identity.return_value = with_response\n        return mock.Mock(return_value=client)\n\n    def get_expected_creds_from_response(self, response):\n        expiration = response['Credentials']['Expiration']\n        if isinstance(expiration, datetime):\n            expiration = expiration.isoformat()\n        return {\n            'access_key': response['Credentials']['AccessKeyId'],\n            'secret_key': response['Credentials']['SecretAccessKey'],\n            'token': response['Credentials']['SessionToken'],\n            'expiry_time': expiration,\n        }\n\n    def test_no_cache(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        refresher = credentials.AssumeRoleWithWebIdentityCredentialFetcher(\n            client_creator, self.load_token, self.role_arn\n        )\n        expected_response = self.get_expected_creds_from_response(response)\n        response = refresher.fetch_credentials()\n\n        self.assertEqual(response, expected_response)\n\n    def test_retrieves_from_cache(self):\n        date_in_future = datetime.utcnow() + timedelta(seconds=1000)\n        utc_timestamp = date_in_future.isoformat() + 'Z'\n        cache_key = '793d6e2f27667ab2da104824407e486bfec24a47'\n        cache = {\n            cache_key: {\n                'Credentials': {\n                    'AccessKeyId': 'foo-cached',\n                    'SecretAccessKey': 'bar-cached',\n                    'SessionToken': 'baz-cached',\n                    'Expiration': utc_timestamp,\n                }\n            }\n        }\n        client_creator = mock.Mock()\n        refresher = credentials.AssumeRoleWithWebIdentityCredentialFetcher(\n            client_creator, self.load_token, self.role_arn, cache=cache\n        )\n        expected_response = self.get_expected_creds_from_response(\n            cache[cache_key]\n        )\n        response = refresher.fetch_credentials()\n\n        self.assertEqual(response, expected_response)\n        client_creator.assert_not_called()\n\n    def test_assume_role_in_cache_but_expired(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        cache = {\n            'development--myrole': {\n                'Credentials': {\n                    'AccessKeyId': 'foo-cached',\n                    'SecretAccessKey': 'bar-cached',\n                    'SessionToken': 'baz-cached',\n                    'Expiration': datetime.now(tzlocal()),\n                }\n            }\n        }\n\n        refresher = credentials.AssumeRoleWithWebIdentityCredentialFetcher(\n            client_creator, self.load_token, self.role_arn, cache=cache\n        )\n        expected = self.get_expected_creds_from_response(response)\n        response = refresher.fetch_credentials()\n\n        self.assertEqual(response, expected)\n\n\nclass TestAssumeRoleWithWebIdentityCredentialProvider(unittest.TestCase):\n    def setUp(self):\n        self.profile_name = 'some-profile'\n        self.config = {\n            'role_arn': 'arn:aws:iam::123:role/role-name',\n            'web_identity_token_file': '/some/path/token.jwt',\n        }\n\n    def create_client_creator(self, with_response):\n        # Create a mock sts client that returns a specific response\n        # for assume_role.\n        client = mock.Mock()\n        if isinstance(with_response, list):\n            client.assume_role_with_web_identity.side_effect = with_response\n        else:\n            client.assume_role_with_web_identity.return_value = with_response\n        return mock.Mock(return_value=client)\n\n    def some_future_time(self):\n        timeobj = datetime.now(tzlocal())\n        return timeobj + timedelta(hours=24)\n\n    def _mock_loader_cls(self, token=''):\n        mock_loader = mock.Mock(spec=FileWebIdentityTokenLoader)\n        mock_loader.return_value = token\n        mock_cls = mock.Mock()\n        mock_cls.return_value = mock_loader\n        return mock_cls\n\n    def _load_config(self):\n        return {\n            'profiles': {\n                self.profile_name: self.config,\n            }\n        }\n\n    def test_assume_role_with_no_cache(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        mock_loader_cls = self._mock_loader_cls('totally.a.token')\n        provider = credentials.AssumeRoleWithWebIdentityProvider(\n            load_config=self._load_config,\n            client_creator=client_creator,\n            cache={},\n            profile_name=self.profile_name,\n            token_loader_cls=mock_loader_cls,\n        )\n\n        creds = provider.load()\n\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n        mock_loader_cls.assert_called_with('/some/path/token.jwt')\n\n    def test_assume_role_retrieves_from_cache(self):\n        date_in_future = datetime.utcnow() + timedelta(seconds=1000)\n        utc_timestamp = date_in_future.isoformat() + 'Z'\n\n        cache_key = 'c29461feeacfbed43017d20612606ff76abc073d'\n        cache = {\n            cache_key: {\n                'Credentials': {\n                    'AccessKeyId': 'foo-cached',\n                    'SecretAccessKey': 'bar-cached',\n                    'SessionToken': 'baz-cached',\n                    'Expiration': utc_timestamp,\n                }\n            }\n        }\n        mock_loader_cls = self._mock_loader_cls('totally.a.token')\n        client_creator = mock.Mock()\n        provider = credentials.AssumeRoleWithWebIdentityProvider(\n            load_config=self._load_config,\n            client_creator=client_creator,\n            cache=cache,\n            profile_name=self.profile_name,\n            token_loader_cls=mock_loader_cls,\n        )\n\n        creds = provider.load()\n\n        self.assertEqual(creds.access_key, 'foo-cached')\n        self.assertEqual(creds.secret_key, 'bar-cached')\n        self.assertEqual(creds.token, 'baz-cached')\n        client_creator.assert_not_called()\n\n    def test_assume_role_in_cache_but_expired(self):\n        expired_creds = datetime.now(tzlocal())\n        valid_creds = expired_creds + timedelta(hours=1)\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': valid_creds,\n            },\n        }\n        cache = {\n            'development--myrole': {\n                'Credentials': {\n                    'AccessKeyId': 'foo-cached',\n                    'SecretAccessKey': 'bar-cached',\n                    'SessionToken': 'baz-cached',\n                    'Expiration': expired_creds,\n                }\n            }\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        mock_loader_cls = self._mock_loader_cls('totally.a.token')\n        provider = credentials.AssumeRoleWithWebIdentityProvider(\n            load_config=self._load_config,\n            client_creator=client_creator,\n            cache=cache,\n            profile_name=self.profile_name,\n            token_loader_cls=mock_loader_cls,\n        )\n\n        creds = provider.load()\n\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n        mock_loader_cls.assert_called_with('/some/path/token.jwt')\n\n    def test_role_session_name_provided(self):\n        self.config['role_session_name'] = 'myname'\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        mock_loader_cls = self._mock_loader_cls('totally.a.token')\n        provider = credentials.AssumeRoleWithWebIdentityProvider(\n            load_config=self._load_config,\n            client_creator=client_creator,\n            cache={},\n            profile_name=self.profile_name,\n            token_loader_cls=mock_loader_cls,\n        )\n        # The credentials won't actually be assumed until they're requested.\n        provider.load().get_frozen_credentials()\n\n        client = client_creator.return_value\n        client.assume_role_with_web_identity.assert_called_with(\n            RoleArn='arn:aws:iam::123:role/role-name',\n            RoleSessionName='myname',\n            WebIdentityToken='totally.a.token',\n        )\n\n    def test_role_arn_not_set(self):\n        del self.config['role_arn']\n        client_creator = self.create_client_creator(with_response={})\n        provider = credentials.AssumeRoleWithWebIdentityProvider(\n            load_config=self._load_config,\n            client_creator=client_creator,\n            cache={},\n            profile_name=self.profile_name,\n        )\n        # If the role arn isn't set but the token path is raise an error\n        with self.assertRaises(botocore.exceptions.InvalidConfigError):\n            provider.load()\n\n\nclass TestEnvVar(BaseEnvVar):\n    def test_envvars_are_found_no_token(self):\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.method, 'env')\n\n    def test_envvars_found_with_security_token(self):\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_SECURITY_TOKEN': 'baz',\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n        self.assertEqual(creds.method, 'env')\n\n    def test_envvars_found_with_session_token(self):\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_SESSION_TOKEN': 'baz',\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n        self.assertEqual(creds.method, 'env')\n\n    def test_envvars_not_found(self):\n        provider = credentials.EnvProvider(environ={})\n        creds = provider.load()\n        self.assertIsNone(creds)\n\n    def test_envvars_empty_string(self):\n        environ = {\n            'AWS_ACCESS_KEY_ID': '',\n            'AWS_SECRET_ACCESS_KEY': '',\n            'AWS_SECURITY_TOKEN': '',\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n        self.assertIsNone(creds)\n\n    def test_expiry_omitted_if_envvar_empty(self):\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_SESSION_TOKEN': 'baz',\n            'AWS_CREDENTIAL_EXPIRATION': '',\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n        # Because we treat empty env vars the same as not being provided,\n        # we should return static credentials and not a refreshable\n        # credential.\n        self.assertNotIsInstance(creds, credentials.RefreshableCredentials)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n\n    def test_error_when_expiry_required_but_empty(self):\n        expiry_time = datetime.now(tzlocal()) - timedelta(hours=1)\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_CREDENTIAL_EXPIRATION': expiry_time.isoformat(),\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n\n        del environ['AWS_CREDENTIAL_EXPIRATION']\n\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            creds.get_frozen_credentials()\n\n    def test_can_override_env_var_mapping(self):\n        # We can change the env var provider to\n        # use our specified env var names.\n        environ = {\n            'FOO_ACCESS_KEY': 'foo',\n            'FOO_SECRET_KEY': 'bar',\n            'FOO_SESSION_TOKEN': 'baz',\n        }\n        mapping = {\n            'access_key': 'FOO_ACCESS_KEY',\n            'secret_key': 'FOO_SECRET_KEY',\n            'token': 'FOO_SESSION_TOKEN',\n        }\n        provider = credentials.EnvProvider(environ, mapping)\n        creds = provider.load()\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n\n    def test_can_override_partial_env_var_mapping(self):\n        # Only changing the access key mapping.\n        # The other 2 use the default values of\n        # AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN\n        # use our specified env var names.\n        environ = {\n            'FOO_ACCESS_KEY': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_SESSION_TOKEN': 'baz',\n        }\n        provider = credentials.EnvProvider(\n            environ, {'access_key': 'FOO_ACCESS_KEY'}\n        )\n        creds = provider.load()\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n\n    def test_can_override_expiry_env_var_mapping(self):\n        expiry_time = datetime.now(tzlocal()) - timedelta(hours=1)\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_SESSION_TOKEN': 'baz',\n            'FOO_EXPIRY': expiry_time.isoformat(),\n        }\n        provider = credentials.EnvProvider(\n            environ, {'expiry_time': 'FOO_EXPIRY'}\n        )\n        creds = provider.load()\n\n        # Since the credentials are expired, we'll trigger a refresh whenever\n        # we try to access them. Since the environment credentials are still\n        # expired, this will raise an error.\n        error_message = (\n            \"Credentials were refreshed, but the refreshed credentials are \"\n            \"still expired.\"\n        )\n        with self.assertRaisesRegex(RuntimeError, error_message):\n            creds.get_frozen_credentials()\n\n    def test_partial_creds_is_an_error(self):\n        # If the user provides an access key, they must also\n        # provide a secret key.  Not doing so will generate an\n        # error.\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            # Missing the AWS_SECRET_ACCESS_KEY\n        }\n        provider = credentials.EnvProvider(environ)\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            provider.load()\n\n    def test_partial_creds_is_an_error_empty_string(self):\n        # If the user provides an access key, they must also\n        # provide a secret key.  Not doing so will generate an\n        # error.\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': '',\n        }\n        provider = credentials.EnvProvider(environ)\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            provider.load()\n\n    def test_missing_access_key_id_raises_error(self):\n        expiry_time = datetime.now(tzlocal()) - timedelta(hours=1)\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_CREDENTIAL_EXPIRATION': expiry_time.isoformat(),\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n\n        del environ['AWS_ACCESS_KEY_ID']\n\n        # Since the credentials are expired, we'll trigger a refresh\n        # whenever we try to access them. At that refresh time, the relevant\n        # environment variables are incomplete, so an error will be raised.\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            creds.get_frozen_credentials()\n\n    def test_credentials_refresh(self):\n        # First initialize the credentials with an expired credential set.\n        expiry_time = datetime.now(tzlocal()) - timedelta(hours=1)\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_SESSION_TOKEN': 'baz',\n            'AWS_CREDENTIAL_EXPIRATION': expiry_time.isoformat(),\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n        self.assertIsInstance(creds, credentials.RefreshableCredentials)\n\n        # Since the credentials are expired, we'll trigger a refresh whenever\n        # we try to access them. But at this point the environment hasn't been\n        # updated, so when it refreshes it will trigger an exception because\n        # the new creds are still expired.\n        error_message = (\n            \"Credentials were refreshed, but the refreshed credentials are \"\n            \"still expired.\"\n        )\n        with self.assertRaisesRegex(RuntimeError, error_message):\n            creds.get_frozen_credentials()\n\n        # Now we update the environment with non-expired credentials,\n        # so when we access the creds it will refresh and grab the new ones.\n        expiry_time = datetime.now(tzlocal()) + timedelta(hours=1)\n        environ.update(\n            {\n                'AWS_ACCESS_KEY_ID': 'bin',\n                'AWS_SECRET_ACCESS_KEY': 'bam',\n                'AWS_SESSION_TOKEN': 'biz',\n                'AWS_CREDENTIAL_EXPIRATION': expiry_time.isoformat(),\n            }\n        )\n\n        frozen = creds.get_frozen_credentials()\n        self.assertEqual(frozen.access_key, 'bin')\n        self.assertEqual(frozen.secret_key, 'bam')\n        self.assertEqual(frozen.token, 'biz')\n\n    def test_credentials_only_refresh_when_needed(self):\n        expiry_time = datetime.now(tzlocal()) + timedelta(hours=2)\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_SESSION_TOKEN': 'baz',\n            'AWS_CREDENTIAL_EXPIRATION': expiry_time.isoformat(),\n        }\n        provider = credentials.EnvProvider(environ)\n\n        # Perform the initial credential load\n        creds = provider.load()\n\n        # Now that the initial load has been performed, we go ahead and\n        # change the environment. If the credentials were expired,\n        # they would immediately refresh upon access and we'd get the new\n        # ones. Since they've got plenty of time, they shouldn't refresh.\n        expiry_time = datetime.now(tzlocal()) + timedelta(hours=3)\n        environ.update(\n            {\n                'AWS_ACCESS_KEY_ID': 'bin',\n                'AWS_SECRET_ACCESS_KEY': 'bam',\n                'AWS_SESSION_TOKEN': 'biz',\n                'AWS_CREDENTIAL_EXPIRATION': expiry_time.isoformat(),\n            }\n        )\n\n        frozen = creds.get_frozen_credentials()\n        self.assertEqual(frozen.access_key, 'foo')\n        self.assertEqual(frozen.secret_key, 'bar')\n        self.assertEqual(frozen.token, 'baz')\n\n    def test_credentials_not_refreshable_if_no_expiry_present(self):\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_SESSION_TOKEN': 'baz',\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n        self.assertNotIsInstance(creds, credentials.RefreshableCredentials)\n        self.assertIsInstance(creds, credentials.Credentials)\n\n    def test_credentials_do_not_become_refreshable(self):\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_SESSION_TOKEN': 'baz',\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n        frozen = creds.get_frozen_credentials()\n        self.assertEqual(frozen.access_key, 'foo')\n        self.assertEqual(frozen.secret_key, 'bar')\n        self.assertEqual(frozen.token, 'baz')\n\n        expiry_time = datetime.now(tzlocal()) - timedelta(hours=1)\n        environ.update(\n            {\n                'AWS_ACCESS_KEY_ID': 'bin',\n                'AWS_SECRET_ACCESS_KEY': 'bam',\n                'AWS_SESSION_TOKEN': 'biz',\n                'AWS_CREDENTIAL_EXPIRATION': expiry_time.isoformat(),\n            }\n        )\n\n        frozen = creds.get_frozen_credentials()\n        self.assertEqual(frozen.access_key, 'foo')\n        self.assertEqual(frozen.secret_key, 'bar')\n        self.assertEqual(frozen.token, 'baz')\n        self.assertNotIsInstance(creds, credentials.RefreshableCredentials)\n\n    def test_credentials_throw_error_if_expiry_goes_away(self):\n        expiry_time = datetime.now(tzlocal()) - timedelta(hours=1)\n        environ = {\n            'AWS_ACCESS_KEY_ID': 'foo',\n            'AWS_SECRET_ACCESS_KEY': 'bar',\n            'AWS_CREDENTIAL_EXPIRATION': expiry_time.isoformat(),\n        }\n        provider = credentials.EnvProvider(environ)\n        creds = provider.load()\n\n        del environ['AWS_CREDENTIAL_EXPIRATION']\n\n        with self.assertRaises(credentials.PartialCredentialsError):\n            creds.get_frozen_credentials()\n\n\nclass TestSharedCredentialsProvider(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.ini_parser = mock.Mock()\n\n    def test_credential_file_exists_default_profile(self):\n        self.ini_parser.return_value = {\n            'default': {\n                'aws_access_key_id': 'foo',\n                'aws_secret_access_key': 'bar',\n            }\n        }\n        provider = credentials.SharedCredentialProvider(\n            creds_filename='~/.aws/creds',\n            profile_name='default',\n            ini_parser=self.ini_parser,\n        )\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertIsNone(creds.token)\n        self.assertEqual(creds.method, 'shared-credentials-file')\n\n    def test_partial_creds_raise_error(self):\n        self.ini_parser.return_value = {\n            'default': {\n                'aws_access_key_id': 'foo',\n                # Missing 'aws_secret_access_key'.\n            }\n        }\n        provider = credentials.SharedCredentialProvider(\n            creds_filename='~/.aws/creds',\n            profile_name='default',\n            ini_parser=self.ini_parser,\n        )\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            provider.load()\n\n    def test_credentials_file_exists_with_session_token(self):\n        self.ini_parser.return_value = {\n            'default': {\n                'aws_access_key_id': 'foo',\n                'aws_secret_access_key': 'bar',\n                'aws_session_token': 'baz',\n            }\n        }\n        provider = credentials.SharedCredentialProvider(\n            creds_filename='~/.aws/creds',\n            profile_name='default',\n            ini_parser=self.ini_parser,\n        )\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n        self.assertEqual(creds.method, 'shared-credentials-file')\n\n    def test_credentials_file_with_multiple_profiles(self):\n        self.ini_parser.return_value = {\n            # Here the user has a 'default' and a 'dev' profile.\n            'default': {\n                'aws_access_key_id': 'a',\n                'aws_secret_access_key': 'b',\n                'aws_session_token': 'c',\n            },\n            'dev': {\n                'aws_access_key_id': 'd',\n                'aws_secret_access_key': 'e',\n                'aws_session_token': 'f',\n            },\n        }\n        # And we specify a profile_name of 'dev'.\n        provider = credentials.SharedCredentialProvider(\n            creds_filename='~/.aws/creds',\n            profile_name='dev',\n            ini_parser=self.ini_parser,\n        )\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'd')\n        self.assertEqual(creds.secret_key, 'e')\n        self.assertEqual(creds.token, 'f')\n        self.assertEqual(creds.method, 'shared-credentials-file')\n\n    def test_credentials_file_does_not_exist_returns_none(self):\n        # It's ok if the credentials file does not exist, we should\n        # just catch the appropriate errors and return None.\n        self.ini_parser.side_effect = botocore.exceptions.ConfigNotFound(\n            path='foo'\n        )\n        provider = credentials.SharedCredentialProvider(\n            creds_filename='~/.aws/creds',\n            profile_name='dev',\n            ini_parser=self.ini_parser,\n        )\n        creds = provider.load()\n        self.assertIsNone(creds)\n\n\nclass TestConfigFileProvider(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        profile_config = {\n            'aws_access_key_id': 'a',\n            'aws_secret_access_key': 'b',\n            'aws_session_token': 'c',\n            # Non creds related configs can be in a session's # config.\n            'region': 'us-west-2',\n            'output': 'json',\n        }\n        parsed = {'profiles': {'default': profile_config}}\n        parser = mock.Mock()\n        parser.return_value = parsed\n        self.parser = parser\n\n    def test_config_file_exists(self):\n        provider = credentials.ConfigProvider(\n            'cli.cfg', 'default', self.parser\n        )\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertEqual(creds.token, 'c')\n        self.assertEqual(creds.method, 'config-file')\n\n    def test_config_file_missing_profile_config(self):\n        # Referring to a profile that's not in the config file\n        # will result in session.config returning an empty dict.\n        profile_name = 'NOT-default'\n        provider = credentials.ConfigProvider(\n            'cli.cfg', profile_name, self.parser\n        )\n        creds = provider.load()\n        self.assertIsNone(creds)\n\n    def test_config_file_errors_ignored(self):\n        # We should move on to the next provider if the config file\n        # can't be found.\n        self.parser.side_effect = botocore.exceptions.ConfigNotFound(\n            path='cli.cfg'\n        )\n        provider = credentials.ConfigProvider(\n            'cli.cfg', 'default', self.parser\n        )\n        creds = provider.load()\n        self.assertIsNone(creds)\n\n    def test_partial_creds_is_error(self):\n        profile_config = {\n            'aws_access_key_id': 'a',\n            # Missing aws_secret_access_key\n        }\n        parsed = {'profiles': {'default': profile_config}}\n        parser = mock.Mock()\n        parser.return_value = parsed\n        provider = credentials.ConfigProvider('cli.cfg', 'default', parser)\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            provider.load()\n\n\nclass TestBotoProvider(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.ini_parser = mock.Mock()\n\n    def test_boto_config_file_exists_in_home_dir(self):\n        environ = {}\n        self.ini_parser.return_value = {\n            'Credentials': {\n                # boto's config file does not support a session token\n                # so we only test for access_key/secret_key.\n                'aws_access_key_id': 'a',\n                'aws_secret_access_key': 'b',\n            }\n        }\n        provider = credentials.BotoProvider(\n            environ=environ, ini_parser=self.ini_parser\n        )\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertIsNone(creds.token)\n        self.assertEqual(creds.method, 'boto-config')\n\n    def test_env_var_set_for_boto_location(self):\n        environ = {'BOTO_CONFIG': 'alternate-config.cfg'}\n        self.ini_parser.return_value = {\n            'Credentials': {\n                # boto's config file does not support a session token\n                # so we only test for access_key/secret_key.\n                'aws_access_key_id': 'a',\n                'aws_secret_access_key': 'b',\n            }\n        }\n        provider = credentials.BotoProvider(\n            environ=environ, ini_parser=self.ini_parser\n        )\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertIsNone(creds.token)\n        self.assertEqual(creds.method, 'boto-config')\n\n        # Assert that the parser was called with the filename specified\n        # in the env var.\n        self.ini_parser.assert_called_with('alternate-config.cfg')\n\n    def test_no_boto_config_file_exists(self):\n        self.ini_parser.side_effect = botocore.exceptions.ConfigNotFound(\n            path='foo'\n        )\n        provider = credentials.BotoProvider(\n            environ={}, ini_parser=self.ini_parser\n        )\n        creds = provider.load()\n        self.assertIsNone(creds)\n\n    def test_partial_creds_is_error(self):\n        ini_parser = mock.Mock()\n        ini_parser.return_value = {\n            'Credentials': {\n                'aws_access_key_id': 'a',\n                # Missing aws_secret_access_key.\n            }\n        }\n        provider = credentials.BotoProvider(environ={}, ini_parser=ini_parser)\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            provider.load()\n\n\nclass TestOriginalEC2Provider(BaseEnvVar):\n    def test_load_ec2_credentials_file_not_exist(self):\n        provider = credentials.OriginalEC2Provider(environ={})\n        creds = provider.load()\n        self.assertIsNone(creds)\n\n    def test_load_ec2_credentials_file_exists(self):\n        environ = {\n            'AWS_CREDENTIAL_FILE': 'foo.cfg',\n        }\n        parser = mock.Mock()\n        parser.return_value = {\n            'AWSAccessKeyId': 'a',\n            'AWSSecretKey': 'b',\n        }\n        provider = credentials.OriginalEC2Provider(\n            environ=environ, parser=parser\n        )\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertIsNone(creds.token)\n        self.assertEqual(creds.method, 'ec2-credentials-file')\n\n\nclass TestInstanceMetadataProvider(BaseEnvVar):\n    def test_load_from_instance_metadata(self):\n        timeobj = datetime.now(tzlocal())\n        timestamp = (timeobj + timedelta(hours=24)).isoformat()\n        fetcher = mock.Mock()\n        fetcher.retrieve_iam_role_credentials.return_value = {\n            'access_key': 'a',\n            'secret_key': 'b',\n            'token': 'c',\n            'expiry_time': timestamp,\n            'role_name': 'myrole',\n        }\n        provider = credentials.InstanceMetadataProvider(\n            iam_role_fetcher=fetcher\n        )\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertEqual(creds.token, 'c')\n        self.assertEqual(creds.method, 'iam-role')\n\n    def test_no_role_creds_exist(self):\n        fetcher = mock.Mock()\n        fetcher.retrieve_iam_role_credentials.return_value = {}\n        provider = credentials.InstanceMetadataProvider(\n            iam_role_fetcher=fetcher\n        )\n        creds = provider.load()\n        self.assertIsNone(creds)\n        fetcher.retrieve_iam_role_credentials.assert_called_with()\n\n\nclass CredentialResolverTest(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.provider1 = mock.Mock()\n        self.provider1.METHOD = 'provider1'\n        self.provider1.CANONICAL_NAME = 'CustomProvider1'\n        self.provider2 = mock.Mock()\n        self.provider2.METHOD = 'provider2'\n        self.provider2.CANONICAL_NAME = 'CustomProvider2'\n        self.fake_creds = credentials.Credentials('a', 'b', 'c')\n\n    def test_load_credentials_single_provider(self):\n        self.provider1.load.return_value = self.fake_creds\n        resolver = credentials.CredentialResolver(providers=[self.provider1])\n        creds = resolver.load_credentials()\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertEqual(creds.token, 'c')\n\n    def test_get_provider_by_name(self):\n        resolver = credentials.CredentialResolver(providers=[self.provider1])\n        result = resolver.get_provider('provider1')\n        self.assertIs(result, self.provider1)\n\n    def test_get_unknown_provider_raises_error(self):\n        resolver = credentials.CredentialResolver(providers=[self.provider1])\n        with self.assertRaises(botocore.exceptions.UnknownCredentialError):\n            resolver.get_provider('unknown-foo')\n\n    def test_first_credential_non_none_wins(self):\n        self.provider1.load.return_value = None\n        self.provider2.load.return_value = self.fake_creds\n        resolver = credentials.CredentialResolver(\n            providers=[self.provider1, self.provider2]\n        )\n        creds = resolver.load_credentials()\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertEqual(creds.token, 'c')\n        self.provider1.load.assert_called_with()\n        self.provider2.load.assert_called_with()\n\n    def test_no_creds_loaded(self):\n        self.provider1.load.return_value = None\n        self.provider2.load.return_value = None\n        resolver = credentials.CredentialResolver(\n            providers=[self.provider1, self.provider2]\n        )\n        creds = resolver.load_credentials()\n        self.assertIsNone(creds)\n\n    def test_inject_additional_providers_after_existing(self):\n        self.provider1.load.return_value = None\n        self.provider2.load.return_value = self.fake_creds\n        resolver = credentials.CredentialResolver(\n            providers=[self.provider1, self.provider2]\n        )\n        # Now, if we were to call resolver.load() now, provider2 would\n        # win because it's returning a non None response.\n        # However we can inject a new provider before provider2 to\n        # override this process.\n        # Providers can be added by the METHOD name of each provider.\n        new_provider = mock.Mock()\n        new_provider.METHOD = 'new_provider'\n        new_provider.load.return_value = credentials.Credentials('d', 'e', 'f')\n\n        resolver.insert_after('provider1', new_provider)\n\n        creds = resolver.load_credentials()\n        self.assertIsNotNone(creds)\n\n        self.assertEqual(creds.access_key, 'd')\n        self.assertEqual(creds.secret_key, 'e')\n        self.assertEqual(creds.token, 'f')\n        # Provider 1 should have been called, but provider2 should\n        # *not* have been called because new_provider already returned\n        # a non-None response.\n        self.provider1.load.assert_called_with()\n        self.assertTrue(not self.provider2.called)\n\n    def test_inject_provider_before_existing(self):\n        new_provider = mock.Mock()\n        new_provider.METHOD = 'override'\n        new_provider.load.return_value = credentials.Credentials('x', 'y', 'z')\n\n        resolver = credentials.CredentialResolver(\n            providers=[self.provider1, self.provider2]\n        )\n        resolver.insert_before(self.provider1.METHOD, new_provider)\n        creds = resolver.load_credentials()\n        self.assertEqual(creds.access_key, 'x')\n        self.assertEqual(creds.secret_key, 'y')\n        self.assertEqual(creds.token, 'z')\n\n    def test_can_remove_providers(self):\n        self.provider1.load.return_value = credentials.Credentials(\n            'a', 'b', 'c'\n        )\n        self.provider2.load.return_value = credentials.Credentials(\n            'd', 'e', 'f'\n        )\n        resolver = credentials.CredentialResolver(\n            providers=[self.provider1, self.provider2]\n        )\n        resolver.remove('provider1')\n        creds = resolver.load_credentials()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'd')\n        self.assertEqual(creds.secret_key, 'e')\n        self.assertEqual(creds.token, 'f')\n        self.assertTrue(not self.provider1.load.called)\n        self.provider2.load.assert_called_with()\n\n    def test_provider_unknown(self):\n        resolver = credentials.CredentialResolver(\n            providers=[self.provider1, self.provider2]\n        )\n        # No error is raised if you try to remove an unknown provider.\n        resolver.remove('providerFOO')\n        # But an error IS raised if you try to insert after an unknown\n        # provider.\n        with self.assertRaises(botocore.exceptions.UnknownCredentialError):\n            resolver.insert_after('providerFoo', None)\n\n\nclass TestCreateCredentialResolver(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n\n        self.session = mock.Mock(spec=botocore.session.Session)\n        self.session.get_component = self.fake_get_component\n\n        self.fake_instance_variables = {\n            'credentials_file': 'a',\n            'legacy_config_file': 'b',\n            'config_file': 'c',\n            'metadata_service_timeout': 1,\n            'metadata_service_num_attempts': 1,\n            'imds_use_ipv6': 'false',\n        }\n        self.config_loader = ConfigValueStore()\n        for name, value in self.fake_instance_variables.items():\n            self.config_loader.set_config_variable(name, value)\n\n        self.session.get_config_variable = (\n            self.config_loader.get_config_variable\n        )\n        self.session.set_config_variable = self.fake_set_config_variable\n        self.session.instance_variables = self.fake_instance_variable_lookup\n\n    def fake_get_component(self, key):\n        if key == 'config_provider':\n            return self.config_loader\n        return None\n\n    def fake_instance_variable_lookup(self):\n        return self.fake_instance_variables\n\n    def fake_set_config_variable(self, logical_name, value):\n        self.fake_instance_variables[logical_name] = value\n\n    def test_create_credential_resolver(self):\n        resolver = credentials.create_credential_resolver(self.session)\n        self.assertIsInstance(resolver, credentials.CredentialResolver)\n\n    def test_explicit_profile_ignores_env_provider(self):\n        self.session.set_config_variable('profile', 'dev')\n        resolver = credentials.create_credential_resolver(self.session)\n\n        self.assertTrue(\n            all(not isinstance(p, EnvProvider) for p in resolver.providers)\n        )\n\n    def test_no_profile_checks_env_provider(self):\n        # If no profile is provided,\n        self.config_loader.set_config_variable('profile', None)\n        resolver = credentials.create_credential_resolver(self.session)\n        # Then an EnvProvider should be part of our credential lookup chain.\n        self.assertTrue(\n            any(isinstance(p, EnvProvider) for p in resolver.providers)\n        )\n\n    def test_default_cache(self):\n        resolver = credentials.create_credential_resolver(self.session)\n        cache = resolver.get_provider('assume-role').cache\n        self.assertIsInstance(cache, dict)\n        self.assertEqual(cache, {})\n\n    def test_custom_cache(self):\n        custom_cache = credentials.JSONFileCache()\n        resolver = credentials.create_credential_resolver(\n            self.session, custom_cache\n        )\n        cache = resolver.get_provider('assume-role').cache\n        self.assertIs(cache, custom_cache)\n\n\nclass TestCanonicalNameSourceProvider(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.custom_provider1 = mock.Mock(spec=CredentialProvider)\n        self.custom_provider1.METHOD = 'provider1'\n        self.custom_provider1.CANONICAL_NAME = 'CustomProvider1'\n        self.custom_provider2 = mock.Mock(spec=CredentialProvider)\n        self.custom_provider2.METHOD = 'provider2'\n        self.custom_provider2.CANONICAL_NAME = 'CustomProvider2'\n        self.fake_creds = credentials.Credentials('a', 'b', 'c')\n\n    def test_load_source_credentials(self):\n        provider = credentials.CanonicalNameCredentialSourcer(\n            providers=[self.custom_provider1, self.custom_provider2]\n        )\n        self.custom_provider1.load.return_value = self.fake_creds\n        result = provider.source_credentials('CustomProvider1')\n        self.assertIs(result, self.fake_creds)\n\n    def test_load_source_credentials_case_insensitive(self):\n        provider = credentials.CanonicalNameCredentialSourcer(\n            providers=[self.custom_provider1, self.custom_provider2]\n        )\n        self.custom_provider1.load.return_value = self.fake_creds\n        result = provider.source_credentials('cUsToMpRoViDeR1')\n        self.assertIs(result, self.fake_creds)\n\n    def test_load_unknown_canonical_name_raises_error(self):\n        provider = credentials.CanonicalNameCredentialSourcer(\n            providers=[self.custom_provider1]\n        )\n        with self.assertRaises(botocore.exceptions.UnknownCredentialError):\n            provider.source_credentials('CustomUnknown')\n\n    def _assert_assume_role_creds_returned_with_shared_file(self, provider):\n        assume_role_provider = mock.Mock(spec=AssumeRoleProvider)\n        assume_role_provider.METHOD = 'assume-role'\n        assume_role_provider.CANONICAL_NAME = None\n\n        source = credentials.CanonicalNameCredentialSourcer(\n            providers=[assume_role_provider, provider]\n        )\n\n        # If the assume role provider returns credentials, those should be\n        # what is returned.\n        assume_role_provider.load.return_value = self.fake_creds\n        provider.load.return_value = credentials.Credentials('d', 'e', 'f')\n\n        creds = source.source_credentials(provider.CANONICAL_NAME)\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertEqual(creds.token, 'c')\n        self.assertFalse(provider.load.called)\n\n    def _assert_returns_creds_if_assume_role_not_used(self, provider):\n        assume_role_provider = mock.Mock(spec=AssumeRoleProvider)\n        assume_role_provider.METHOD = 'assume-role'\n        assume_role_provider.CANONICAL_NAME = None\n\n        source = credentials.CanonicalNameCredentialSourcer(\n            providers=[assume_role_provider, provider]\n        )\n\n        # If the assume role provider returns nothing, then whatever is in\n        # the config provider should be returned.\n        assume_role_provider.load.return_value = None\n        provider.load.return_value = credentials.Credentials('d', 'e', 'f')\n\n        creds = source.source_credentials(provider.CANONICAL_NAME)\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'd')\n        self.assertEqual(creds.secret_key, 'e')\n        self.assertEqual(creds.token, 'f')\n        self.assertTrue(assume_role_provider.load.called)\n\n    def test_assume_role_creds_returned_with_config_file(self):\n        provider = mock.Mock(spec=ConfigProvider)\n        provider.METHOD = 'config-file'\n        provider.CANONICAL_NAME = 'SharedConfig'\n        self._assert_assume_role_creds_returned_with_shared_file(provider)\n\n    def test_config_file_returns_creds_if_assume_role_not_used(self):\n        provider = mock.Mock(spec=ConfigProvider)\n        provider.METHOD = 'config-file'\n        provider.CANONICAL_NAME = 'SharedConfig'\n        self._assert_returns_creds_if_assume_role_not_used(provider)\n\n    def test_assume_role_creds_returned_with_cred_file(self):\n        provider = mock.Mock(spec=SharedCredentialProvider)\n        provider.METHOD = 'credentials-file'\n        provider.CANONICAL_NAME = 'SharedCredentials'\n        self._assert_assume_role_creds_returned_with_shared_file(provider)\n\n    def test_creds_file_returns_creds_if_assume_role_not_used(self):\n        provider = mock.Mock(spec=SharedCredentialProvider)\n        provider.METHOD = 'credentials-file'\n        provider.CANONICAL_NAME = 'SharedCredentials'\n        self._assert_returns_creds_if_assume_role_not_used(provider)\n\n    def test_get_canonical_assume_role_without_shared_files(self):\n        assume_role_provider = mock.Mock(spec=AssumeRoleProvider)\n        assume_role_provider.METHOD = 'assume-role'\n        assume_role_provider.CANONICAL_NAME = None\n        assume_role_provider.load.return_value = self.fake_creds\n\n        provider = credentials.CanonicalNameCredentialSourcer(\n            providers=[assume_role_provider]\n        )\n\n        creds = provider.source_credentials('SharedConfig')\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertEqual(creds.token, 'c')\n\n        creds = provider.source_credentials('SharedCredentials')\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'a')\n        self.assertEqual(creds.secret_key, 'b')\n        self.assertEqual(creds.token, 'c')\n\n    def test_get_canonical_shared_files_without_assume_role(self):\n        provider = credentials.CanonicalNameCredentialSourcer(\n            providers=[self.custom_provider1]\n        )\n        with self.assertRaises(botocore.exceptions.UnknownCredentialError):\n            provider.source_credentials('SharedConfig')\n        with self.assertRaises(botocore.exceptions.UnknownCredentialError):\n            provider.source_credentials('SharedCredentials')\n\n\nclass TestAssumeRoleCredentialProvider(unittest.TestCase):\n    maxDiff = None\n\n    def setUp(self):\n        self.fake_config = {\n            'profiles': {\n                'development': {\n                    'role_arn': 'myrole',\n                    'source_profile': 'longterm',\n                },\n                'longterm': {\n                    'aws_access_key_id': 'akid',\n                    'aws_secret_access_key': 'skid',\n                },\n                'non-static': {\n                    'role_arn': 'myrole',\n                    'credential_source': 'Environment',\n                },\n                'chained': {\n                    'role_arn': 'chained-role',\n                    'source_profile': 'development',\n                },\n            }\n        }\n\n    def create_config_loader(self, with_config=None):\n        if with_config is None:\n            with_config = self.fake_config\n        load_config = mock.Mock()\n        load_config.return_value = with_config\n        return load_config\n\n    def create_client_creator(self, with_response):\n        # Create a mock sts client that returns a specific response\n        # for assume_role.\n        client = mock.Mock()\n        if isinstance(with_response, list):\n            client.assume_role.side_effect = with_response\n        else:\n            client.assume_role.return_value = with_response\n        return mock.Mock(return_value=client)\n\n    def some_future_time(self):\n        timeobj = datetime.now(tzlocal())\n        return timeobj + timedelta(hours=24)\n\n    def test_assume_role_with_no_cache(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n        )\n\n        creds = provider.load()\n\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n\n    def test_assume_role_with_datetime(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                # Note the lack of isoformat(), we're using\n                # a datetime.datetime type.  This will ensure\n                # we test both parsing as well as serializing\n                # from a given datetime because the credentials\n                # are immediately expired.\n                'Expiration': datetime.now(tzlocal()) + timedelta(hours=20),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n        )\n\n        creds = provider.load()\n\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n\n    def test_assume_role_refresher_serializes_datetime(self):\n        client = mock.Mock()\n        time_zone = tzutc()\n        expiration = datetime(\n            year=2016, month=11, day=6, hour=1, minute=30, tzinfo=time_zone\n        )\n        client.assume_role.return_value = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': expiration,\n            }\n        }\n        refresh = create_assume_role_refresher(client, {})\n        expiry_time = refresh()['expiry_time']\n        self.assertEqual(expiry_time, '2016-11-06T01:30:00UTC')\n\n    def test_assume_role_retrieves_from_cache(self):\n        date_in_future = datetime.utcnow() + timedelta(seconds=1000)\n        utc_timestamp = date_in_future.isoformat() + 'Z'\n        self.fake_config['profiles']['development']['role_arn'] = 'myrole'\n\n        cache_key = '793d6e2f27667ab2da104824407e486bfec24a47'\n        cache = {\n            cache_key: {\n                'Credentials': {\n                    'AccessKeyId': 'foo-cached',\n                    'SecretAccessKey': 'bar-cached',\n                    'SessionToken': 'baz-cached',\n                    'Expiration': utc_timestamp,\n                }\n            }\n        }\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            mock.Mock(),\n            cache=cache,\n            profile_name='development',\n        )\n\n        creds = provider.load()\n\n        self.assertEqual(creds.access_key, 'foo-cached')\n        self.assertEqual(creds.secret_key, 'bar-cached')\n        self.assertEqual(creds.token, 'baz-cached')\n\n    def test_chain_prefers_cache(self):\n        date_in_future = datetime.utcnow() + timedelta(seconds=1000)\n        utc_timestamp = date_in_future.isoformat() + 'Z'\n\n        # The profile we will be using has a cache entry, but the profile it\n        # is sourcing from does not. This should result in the cached\n        # credentials being used, and the source profile not being called.\n        cache_key = '3d440bf424caf7a5ee664fbf89139a84409f95c2'\n        cache = {\n            cache_key: {\n                'Credentials': {\n                    'AccessKeyId': 'foo-cached',\n                    'SecretAccessKey': 'bar-cached',\n                    'SessionToken': 'baz-cached',\n                    'Expiration': utc_timestamp,\n                }\n            }\n        }\n\n        client_creator = self.create_client_creator(\n            [Exception(\"Attempted to call assume role when not needed.\")]\n        )\n\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache=cache,\n            profile_name='chained',\n        )\n\n        creds = provider.load()\n\n        self.assertEqual(creds.access_key, 'foo-cached')\n        self.assertEqual(creds.secret_key, 'bar-cached')\n        self.assertEqual(creds.token, 'baz-cached')\n\n    def test_cache_key_is_windows_safe(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        cache = {}\n        self.fake_config['profiles']['development'][\n            'role_arn'\n        ] = 'arn:aws:iam::foo-role'\n\n        client_creator = self.create_client_creator(with_response=response)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache=cache,\n            profile_name='development',\n        )\n\n        provider.load().get_frozen_credentials()\n        # On windows, you cannot use a a ':' in the filename, so\n        # we need to make sure it doesn't come up in the cache key.\n        cache_key = '3f8e35c8dca6211d496e830a2de723b2387921e3'\n        self.assertIn(cache_key, cache)\n        self.assertEqual(cache[cache_key], response)\n\n    def test_cache_key_with_role_session_name(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        cache = {}\n        self.fake_config['profiles']['development'][\n            'role_arn'\n        ] = 'arn:aws:iam::foo-role'\n        self.fake_config['profiles']['development'][\n            'role_session_name'\n        ] = 'foo_role_session_name'\n\n        client_creator = self.create_client_creator(with_response=response)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache=cache,\n            profile_name='development',\n        )\n\n        # The credentials won't actually be assumed until they're requested.\n        provider.load().get_frozen_credentials()\n\n        cache_key = '5e75ce21b6a64ab183b29c4a159b6f0248121d51'\n        self.assertIn(cache_key, cache)\n        self.assertEqual(cache[cache_key], response)\n\n    def test_assume_role_in_cache_but_expired(self):\n        expired_creds = datetime.now(tzlocal())\n        valid_creds = expired_creds + timedelta(hours=1)\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': valid_creds,\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        cache = {\n            'development--myrole': {\n                'Credentials': {\n                    'AccessKeyId': 'foo-cached',\n                    'SecretAccessKey': 'bar-cached',\n                    'SessionToken': 'baz-cached',\n                    'Expiration': expired_creds,\n                }\n            }\n        }\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache=cache,\n            profile_name='development',\n        )\n\n        creds = provider.load()\n\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n\n    def test_role_session_name_provided(self):\n        dev_profile = self.fake_config['profiles']['development']\n        dev_profile['role_session_name'] = 'myname'\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n        )\n\n        # The credentials won't actually be assumed until they're requested.\n        provider.load().get_frozen_credentials()\n\n        client = client_creator.return_value\n        client.assume_role.assert_called_with(\n            RoleArn='myrole', RoleSessionName='myname'\n        )\n\n    def test_external_id_provided(self):\n        self.fake_config['profiles']['development']['external_id'] = 'myid'\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n        )\n\n        # The credentials won't actually be assumed until they're requested.\n        provider.load().get_frozen_credentials()\n\n        client = client_creator.return_value\n        client.assume_role.assert_called_with(\n            RoleArn='myrole', ExternalId='myid', RoleSessionName=mock.ANY\n        )\n\n    def test_assume_role_with_duration(self):\n        self.fake_config['profiles']['development']['duration_seconds'] = 7200\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n        )\n\n        # The credentials won't actually be assumed until they're requested.\n        provider.load().get_frozen_credentials()\n\n        client = client_creator.return_value\n        client.assume_role.assert_called_with(\n            RoleArn='myrole', RoleSessionName=mock.ANY, DurationSeconds=7200\n        )\n\n    def test_assume_role_with_bad_duration(self):\n        self.fake_config['profiles']['development'][\n            'duration_seconds'\n        ] = 'garbage value'\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n        )\n\n        # The credentials won't actually be assumed until they're requested.\n        provider.load().get_frozen_credentials()\n\n        client = client_creator.return_value\n        client.assume_role.assert_called_with(\n            RoleArn='myrole', RoleSessionName=mock.ANY\n        )\n\n    def test_assume_role_with_mfa(self):\n        self.fake_config['profiles']['development']['mfa_serial'] = 'mfa'\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        prompter = mock.Mock(return_value='token-code')\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n            prompter=prompter,\n        )\n\n        # The credentials won't actually be assumed until they're requested.\n        provider.load().get_frozen_credentials()\n\n        client = client_creator.return_value\n        # In addition to the normal assume role args, we should also\n        # inject the serial number from the config as well as the\n        # token code that comes from prompting the user (the prompter\n        # object).\n        client.assume_role.assert_called_with(\n            RoleArn='myrole',\n            RoleSessionName=mock.ANY,\n            SerialNumber='mfa',\n            TokenCode='token-code',\n        )\n\n    def test_assume_role_populates_session_name_on_refresh(self):\n        expiration_time = self.some_future_time()\n        next_expiration_time = expiration_time + timedelta(hours=4)\n        responses = [\n            {\n                'Credentials': {\n                    'AccessKeyId': 'foo',\n                    'SecretAccessKey': 'bar',\n                    'SessionToken': 'baz',\n                    # We're creating an expiry time in the past so as\n                    # soon as we try to access the credentials, the\n                    # refresh behavior will be triggered.\n                    'Expiration': expiration_time.isoformat(),\n                },\n            },\n            {\n                'Credentials': {\n                    'AccessKeyId': 'foo',\n                    'SecretAccessKey': 'bar',\n                    'SessionToken': 'baz',\n                    'Expiration': next_expiration_time.isoformat(),\n                }\n            },\n        ]\n        client_creator = self.create_client_creator(with_response=responses)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n            prompter=mock.Mock(return_value='token-code'),\n        )\n\n        local_now = mock.Mock(return_value=datetime.now(tzlocal()))\n        with mock.patch('botocore.credentials._local_now', local_now):\n            # This will trigger the first assume_role() call.  It returns\n            # credentials that are expired and will trigger a refresh.\n            creds = provider.load()\n            creds.get_frozen_credentials()\n\n            # This will trigger the second assume_role() call because\n            # a refresh is needed.\n            local_now.return_value = expiration_time\n            creds.get_frozen_credentials()\n\n        client = client_creator.return_value\n        assume_role_calls = client.assume_role.call_args_list\n        self.assertEqual(len(assume_role_calls), 2, assume_role_calls)\n        # The args should be identical.  That is, the second\n        # assume_role call should have the exact same args as the\n        # initial assume_role call.\n        self.assertEqual(assume_role_calls[0], assume_role_calls[1])\n\n    def test_assume_role_mfa_cannot_refresh_credentials(self):\n        # Note: we should look into supporting optional behavior\n        # in the future that allows for reprompting for credentials.\n        # But for now, if we get temp creds with MFA then when those\n        # creds expire, we can't refresh the credentials.\n        self.fake_config['profiles']['development']['mfa_serial'] = 'mfa'\n        expiration_time = self.some_future_time()\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                # We're creating an expiry time in the past so as\n                # soon as we try to access the credentials, the\n                # refresh behavior will be triggered.\n                'Expiration': expiration_time.isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n            prompter=mock.Mock(return_value='token-code'),\n        )\n\n        local_now = mock.Mock(return_value=datetime.now(tzlocal()))\n        with mock.patch('botocore.credentials._local_now', local_now):\n            # Loads the credentials, resulting in the first assume role call.\n            creds = provider.load()\n            creds.get_frozen_credentials()\n\n            local_now.return_value = expiration_time\n            with self.assertRaises(credentials.RefreshWithMFAUnsupportedError):\n                # access_key is a property that will refresh credentials\n                # if they're expired.  Because we set the expiry time to\n                # something in the past, this will trigger the refresh\n                # behavior, with with MFA will currently raise an exception.\n                creds.access_key\n\n    def test_no_config_is_noop(self):\n        self.fake_config['profiles']['development'] = {\n            'aws_access_key_id': 'foo',\n            'aws_secret_access_key': 'bar',\n        }\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            mock.Mock(),\n            cache={},\n            profile_name='development',\n        )\n\n        # Because a role_arn was not specified, the AssumeRoleProvider\n        # is a noop and will not return credentials (which means we\n        # move on to the next provider).\n        creds = provider.load()\n        self.assertIsNone(creds)\n\n    def test_source_profile_not_provided(self):\n        del self.fake_config['profiles']['development']['source_profile']\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            mock.Mock(),\n            cache={},\n            profile_name='development',\n        )\n\n        # source_profile is required, we shoudl get an error.\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            provider.load()\n\n    def test_source_profile_does_not_exist(self):\n        dev_profile = self.fake_config['profiles']['development']\n        dev_profile['source_profile'] = 'does-not-exist'\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            mock.Mock(),\n            cache={},\n            profile_name='development',\n        )\n\n        # source_profile is required, we shoudl get an error.\n        with self.assertRaises(botocore.exceptions.InvalidConfigError):\n            provider.load()\n\n    def test_incomplete_source_credentials_raises_error(self):\n        del self.fake_config['profiles']['longterm']['aws_access_key_id']\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            mock.Mock(),\n            cache={},\n            profile_name='development',\n        )\n\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            provider.load()\n\n    def test_source_profile_and_credential_source_provided(self):\n        profile = self.fake_config['profiles']['development']\n        profile['credential_source'] = 'SomeCredentialProvider'\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            mock.Mock(),\n            cache={},\n            profile_name='development',\n        )\n\n        with self.assertRaises(botocore.exceptions.InvalidConfigError):\n            provider.load()\n\n    def test_credential_source_with_no_resolver_configured(self):\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            mock.Mock(),\n            cache={},\n            profile_name='non-static',\n        )\n\n        with self.assertRaises(botocore.exceptions.InvalidConfigError):\n            provider.load()\n\n    def test_credential_source_with_no_providers_configured(self):\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            mock.Mock(),\n            cache={},\n            profile_name='non-static',\n            credential_sourcer=credentials.CanonicalNameCredentialSourcer([]),\n        )\n\n        with self.assertRaises(botocore.exceptions.InvalidConfigError):\n            provider.load()\n\n    def test_credential_source_not_among_providers(self):\n        fake_provider = mock.Mock()\n        fake_provider.CANONICAL_NAME = 'CustomFakeProvider'\n\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            mock.Mock(),\n            cache={},\n            profile_name='non-static',\n            credential_sourcer=credentials.CanonicalNameCredentialSourcer(\n                [fake_provider]\n            ),\n        )\n\n        # We configured the assume role provider with a single fake source\n        # provider, CustomFakeProvider. The profile we are attempting to use\n        # calls for the Environment credential provider as the credentials\n        # source. Since that isn't one of the configured source providers,\n        # an error is thrown.\n        with self.assertRaises(botocore.exceptions.InvalidConfigError):\n            provider.load()\n\n    def test_assume_role_with_credential_source(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n\n        config = {\n            'profiles': {\n                'sourced': {\n                    'role_arn': 'myrole',\n                    'credential_source': 'CustomMockProvider',\n                }\n            }\n        }\n        config_loader = self.create_config_loader(with_config=config)\n\n        fake_provider = mock.Mock()\n        fake_provider.CANONICAL_NAME = 'CustomMockProvider'\n        fake_creds = credentials.Credentials('akid', 'skid', 'token')\n        fake_provider.load.return_value = fake_creds\n\n        provider = credentials.AssumeRoleProvider(\n            config_loader,\n            client_creator,\n            cache={},\n            profile_name='sourced',\n            credential_sourcer=credentials.CanonicalNameCredentialSourcer(\n                [fake_provider]\n            ),\n        )\n\n        creds = provider.load()\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n        client_creator.assert_called_with(\n            'sts',\n            aws_access_key_id=fake_creds.access_key,\n            aws_secret_access_key=fake_creds.secret_key,\n            aws_session_token=fake_creds.token,\n        )\n\n    def test_credential_source_returns_none(self):\n        config = {\n            'profiles': {\n                'sourced': {\n                    'role_arn': 'myrole',\n                    'credential_source': 'CustomMockProvider',\n                }\n            }\n        }\n        config_loader = self.create_config_loader(with_config=config)\n\n        fake_provider = mock.Mock()\n        fake_provider.CANONICAL_NAME = 'CustomMockProvider'\n        fake_provider.load.return_value = None\n\n        provider = credentials.AssumeRoleProvider(\n            config_loader,\n            mock.Mock(),\n            cache={},\n            profile_name='sourced',\n            credential_sourcer=credentials.CanonicalNameCredentialSourcer(\n                [fake_provider]\n            ),\n        )\n\n        with self.assertRaises(botocore.exceptions.CredentialRetrievalError):\n            provider.load()\n\n    def test_source_profile_can_reference_self(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n\n        config = {\n            'profiles': {\n                'self-referencial': {\n                    'aws_access_key_id': 'akid',\n                    'aws_secret_access_key': 'skid',\n                    'role_arn': 'myrole',\n                    'source_profile': 'self-referencial',\n                }\n            }\n        }\n\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(config),\n            client_creator,\n            cache={},\n            profile_name='self-referencial',\n        )\n\n        creds = provider.load()\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n\n    def test_infinite_looping_profiles_raises_error(self):\n        config = {\n            'profiles': {\n                'first': {'role_arn': 'first', 'source_profile': 'second'},\n                'second': {'role_arn': 'second', 'source_profile': 'first'},\n            }\n        }\n\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(config),\n            mock.Mock(),\n            cache={},\n            profile_name='first',\n        )\n\n        with self.assertRaises(botocore.credentials.InfiniteLoopConfigError):\n            provider.load()\n\n    def test_recursive_assume_role(self):\n        assume_responses = [\n            Credentials('foo', 'bar', 'baz'),\n            Credentials('spam', 'eggs', 'spamandegss'),\n        ]\n        responses = []\n        for credential_set in assume_responses:\n            responses.append(\n                {\n                    'Credentials': {\n                        'AccessKeyId': credential_set.access_key,\n                        'SecretAccessKey': credential_set.secret_key,\n                        'SessionToken': credential_set.token,\n                        'Expiration': self.some_future_time().isoformat(),\n                    }\n                }\n            )\n        client_creator = self.create_client_creator(with_response=responses)\n\n        static_credentials = Credentials('akid', 'skid')\n        config = {\n            'profiles': {\n                'first': {'role_arn': 'first', 'source_profile': 'second'},\n                'second': {'role_arn': 'second', 'source_profile': 'third'},\n                'third': {\n                    'aws_access_key_id': static_credentials.access_key,\n                    'aws_secret_access_key': static_credentials.secret_key,\n                },\n            }\n        }\n\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(config),\n            client_creator,\n            cache={},\n            profile_name='first',\n        )\n\n        creds = provider.load()\n        expected_creds = assume_responses[-1]\n        self.assertEqual(creds.access_key, expected_creds.access_key)\n        self.assertEqual(creds.secret_key, expected_creds.secret_key)\n        self.assertEqual(creds.token, expected_creds.token)\n\n        client_creator.assert_has_calls(\n            [\n                mock.call(\n                    'sts',\n                    aws_access_key_id=static_credentials.access_key,\n                    aws_secret_access_key=static_credentials.secret_key,\n                    aws_session_token=static_credentials.token,\n                ),\n                mock.call(\n                    'sts',\n                    aws_access_key_id=assume_responses[0].access_key,\n                    aws_secret_access_key=assume_responses[0].secret_key,\n                    aws_session_token=assume_responses[0].token,\n                ),\n            ]\n        )\n\n    def test_assume_role_with_profile_provider(self):\n        response = {\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': self.some_future_time().isoformat(),\n            },\n        }\n        client_creator = self.create_client_creator(with_response=response)\n        mock_builder = mock.Mock(spec=ProfileProviderBuilder)\n        mock_builder.providers.return_value = [ProfileProvider('foo-profile')]\n\n        provider = credentials.AssumeRoleProvider(\n            self.create_config_loader(),\n            client_creator,\n            cache={},\n            profile_name='development',\n            profile_provider_builder=mock_builder,\n        )\n\n        creds = provider.load().get_frozen_credentials()\n\n        self.assertEqual(client_creator.call_count, 1)\n        client_creator.assert_called_with(\n            'sts',\n            aws_access_key_id='foo-profile-access-key',\n            aws_secret_access_key='foo-profile-secret-key',\n            aws_session_token='foo-profile-token',\n        )\n\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n\n\nclass ProfileProvider:\n    METHOD = 'fake'\n\n    def __init__(self, profile_name):\n        self._profile_name = profile_name\n\n    def load(self):\n        return Credentials(\n            '%s-access-key' % self._profile_name,\n            '%s-secret-key' % self._profile_name,\n            '%s-token' % self._profile_name,\n            self.METHOD,\n        )\n\n\nclass TestJSONCache(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        self.cache = credentials.JSONFileCache(self.tempdir)\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def test_supports_contains_check(self):\n        # By default the cache is empty because we're\n        # using a new temp dir everytime.\n        self.assertTrue('mykey' not in self.cache)\n\n    def test_add_key_and_contains_check(self):\n        self.cache['mykey'] = {'foo': 'bar'}\n        self.assertTrue('mykey' in self.cache)\n\n    def test_added_key_can_be_retrieved(self):\n        self.cache['mykey'] = {'foo': 'bar'}\n        self.assertEqual(self.cache['mykey'], {'foo': 'bar'})\n\n    def test_only_accepts_json_serializable_data(self):\n        with self.assertRaises(ValueError):\n            # set()'s cannot be serialized to a JSON string.\n            self.cache['mykey'] = set()\n\n    def test_can_override_existing_values(self):\n        self.cache['mykey'] = {'foo': 'bar'}\n        self.cache['mykey'] = {'baz': 'newvalue'}\n        self.assertEqual(self.cache['mykey'], {'baz': 'newvalue'})\n\n    def test_can_delete_existing_values(self):\n        key_path = Path(os.path.join(self.tempdir, 'deleteme.json'))\n        self.cache['deleteme'] = {'foo': 'bar'}\n        assert self.cache['deleteme'] == {'foo': 'bar'}\n        assert key_path.exists()\n\n        del self.cache['deleteme']\n        # Validate key removed\n        with pytest.raises(KeyError):\n            self.cache['deleteme']\n        # Validate file removed\n        assert not key_path.exists()\n\n        self.cache['deleteme'] = {'bar': 'baz'}\n        assert self.cache['deleteme'] == {'bar': 'baz'}\n\n    def test_can_delete_missing_values(self):\n        key_path = Path(os.path.join(self.tempdir, 'deleteme.json'))\n        assert not key_path.exists()\n\n        with pytest.raises(KeyError):\n            del self.cache['deleteme']\n\n    def test_can_add_multiple_keys(self):\n        self.cache['mykey'] = {'foo': 'bar'}\n        self.cache['mykey2'] = {'baz': 'qux'}\n        self.assertEqual(self.cache['mykey'], {'foo': 'bar'})\n        self.assertEqual(self.cache['mykey2'], {'baz': 'qux'})\n\n    def test_working_dir_does_not_exist(self):\n        working_dir = os.path.join(self.tempdir, 'foo')\n        cache = credentials.JSONFileCache(working_dir)\n        cache['foo'] = {'bar': 'baz'}\n        self.assertEqual(cache['foo'], {'bar': 'baz'})\n\n    def test_key_error_raised_when_cache_key_does_not_exist(self):\n        with self.assertRaises(KeyError):\n            self.cache['foo']\n\n    def test_file_is_truncated_before_writing(self):\n        self.cache['mykey'] = {\n            'really long key in the cache': 'really long value in cache'\n        }\n        # Now overwrite it with a smaller value.\n        self.cache['mykey'] = {'a': 'b'}\n        self.assertEqual(self.cache['mykey'], {'a': 'b'})\n\n    @skip_if_windows('File permissions tests not supported on Windows.')\n    def test_permissions_for_file_restricted(self):\n        self.cache['mykey'] = {'foo': 'bar'}\n        filename = os.path.join(self.tempdir, 'mykey.json')\n        self.assertEqual(os.stat(filename).st_mode & 0xFFF, 0o600)\n\n    def test_cache_with_custom_dumps_func(self):\n        def _custom_serializer(obj):\n            return \"custom foo\"\n\n        def _custom_dumps(obj):\n            return json.dumps(obj, default=_custom_serializer)\n\n        custom_dir = os.path.join(self.tempdir, 'custom')\n        custom_cache = credentials.JSONFileCache(\n            custom_dir, dumps_func=_custom_dumps\n        )\n        custom_cache['test'] = {'bar': object()}\n        self.assertEqual(custom_cache['test'], {'bar': 'custom foo'})\n\n\nclass TestRefreshLogic(unittest.TestCase):\n    def test_mandatory_refresh_needed(self):\n        creds = IntegerRefresher(\n            # These values will immediately trigger\n            # a manadatory refresh.\n            creds_last_for=2,\n            mandatory_refresh=3,\n            advisory_refresh=3,\n        )\n        temp = creds.get_frozen_credentials()\n        self.assertEqual(temp, credentials.ReadOnlyCredentials('1', '1', '1'))\n\n    def test_advisory_refresh_needed(self):\n        creds = IntegerRefresher(\n            # These values will immediately trigger\n            # a manadatory refresh.\n            creds_last_for=4,\n            mandatory_refresh=2,\n            advisory_refresh=5,\n        )\n        temp = creds.get_frozen_credentials()\n        self.assertEqual(temp, credentials.ReadOnlyCredentials('1', '1', '1'))\n\n    def test_refresh_fails_is_not_an_error_during_advisory_period(self):\n        fail_refresh = mock.Mock(side_effect=Exception(\"refresh failed\"))\n        creds = IntegerRefresher(\n            creds_last_for=5,\n            advisory_refresh=7,\n            mandatory_refresh=3,\n            refresh_function=fail_refresh,\n        )\n        temp = creds.get_frozen_credentials()\n        # We should have called the refresh function.\n        self.assertTrue(fail_refresh.called)\n        # The fail_refresh function will raise an exception.\n        # Because we're in the advisory period we'll not propogate\n        # the exception and return the current set of credentials\n        # (generation '1').\n        self.assertEqual(temp, credentials.ReadOnlyCredentials('0', '0', '0'))\n\n    def test_exception_propogated_on_error_during_mandatory_period(self):\n        fail_refresh = mock.Mock(side_effect=Exception(\"refresh failed\"))\n        creds = IntegerRefresher(\n            creds_last_for=5,\n            advisory_refresh=10,\n            # Note we're in the mandatory period now (5 < 7< 10).\n            mandatory_refresh=7,\n            refresh_function=fail_refresh,\n        )\n        with self.assertRaisesRegex(Exception, 'refresh failed'):\n            creds.get_frozen_credentials()\n\n    def test_exception_propogated_on_expired_credentials(self):\n        fail_refresh = mock.Mock(side_effect=Exception(\"refresh failed\"))\n        creds = IntegerRefresher(\n            # Setting this to 0 mean the credentials are immediately\n            # expired.\n            creds_last_for=0,\n            advisory_refresh=10,\n            mandatory_refresh=7,\n            refresh_function=fail_refresh,\n        )\n        with self.assertRaisesRegex(Exception, 'refresh failed'):\n            # Because credentials are actually expired, any\n            # failure to refresh should be propagated.\n            creds.get_frozen_credentials()\n\n    def test_refresh_giving_expired_credentials_raises_exception(self):\n        # This verifies an edge cases where refreshed credentials\n        # still give expired credentials:\n        # 1. We see credentials are expired.\n        # 2. We try to refresh the credentials.\n        # 3. The \"refreshed\" credentials are still expired.\n        #\n        # In this case, we hard fail and let the user know what\n        # happened.\n        creds = IntegerRefresher(\n            # Negative number indicates that the credentials\n            # have already been expired for 2 seconds, even\n            # on refresh.\n            creds_last_for=-2,\n        )\n        err_msg = 'refreshed credentials are still expired'\n        with self.assertRaisesRegex(RuntimeError, err_msg):\n            # Because credentials are actually expired, any\n            # failure to refresh should be propagated.\n            creds.get_frozen_credentials()\n\n\nclass TestContainerProvider(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.tempdir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        super().tearDown()\n        shutil.rmtree(self.tempdir)\n\n    def test_noop_if_env_var_is_not_set(self):\n        # The 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI' env var\n        # is not present as an env var.\n        environ = {}\n        provider = credentials.ContainerProvider(environ)\n        creds = provider.load()\n        self.assertIsNone(creds)\n\n    def full_url(self, url):\n        return f'http://{ContainerMetadataFetcher.IP_ADDRESS}{url}'\n\n    def create_fetcher(self):\n        fetcher = mock.Mock(spec=ContainerMetadataFetcher)\n        fetcher.full_url = self.full_url\n        return fetcher\n\n    def test_retrieve_from_provider_if_env_var_present(self):\n        environ = {\n            'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/latest/credentials?id=foo'\n        }\n        fetcher = self.create_fetcher()\n        timeobj = datetime.now(tzlocal())\n        timestamp = (timeobj + timedelta(hours=24)).isoformat()\n        fetcher.retrieve_full_uri.return_value = {\n            \"AccessKeyId\": \"access_key\",\n            \"SecretAccessKey\": \"secret_key\",\n            \"Token\": \"token\",\n            \"Expiration\": timestamp,\n        }\n        provider = credentials.ContainerProvider(environ, fetcher)\n        creds = provider.load()\n\n        fetcher.retrieve_full_uri.assert_called_with(\n            self.full_url('/latest/credentials?id=foo'), headers=None\n        )\n        self.assertEqual(creds.access_key, 'access_key')\n        self.assertEqual(creds.secret_key, 'secret_key')\n        self.assertEqual(creds.token, 'token')\n        self.assertEqual(creds.method, 'container-role')\n\n    def test_creds_refresh_when_needed(self):\n        environ = {\n            'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/latest/credentials?id=foo'\n        }\n        fetcher = mock.Mock(spec=credentials.ContainerMetadataFetcher)\n        timeobj = datetime.now(tzlocal())\n        expired_timestamp = (timeobj - timedelta(hours=23)).isoformat()\n        future_timestamp = (timeobj + timedelta(hours=1)).isoformat()\n        fetcher.retrieve_full_uri.side_effect = [\n            {\n                \"AccessKeyId\": \"access_key_old\",\n                \"SecretAccessKey\": \"secret_key_old\",\n                \"Token\": \"token_old\",\n                \"Expiration\": expired_timestamp,\n            },\n            {\n                \"AccessKeyId\": \"access_key_new\",\n                \"SecretAccessKey\": \"secret_key_new\",\n                \"Token\": \"token_new\",\n                \"Expiration\": future_timestamp,\n            },\n        ]\n        provider = credentials.ContainerProvider(environ, fetcher)\n        creds = provider.load()\n        frozen_creds = creds.get_frozen_credentials()\n        self.assertEqual(frozen_creds.access_key, 'access_key_new')\n        self.assertEqual(frozen_creds.secret_key, 'secret_key_new')\n        self.assertEqual(frozen_creds.token, 'token_new')\n\n    def test_http_error_propagated(self):\n        environ = {\n            'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/latest/credentials?id=foo'\n        }\n        fetcher = mock.Mock(spec=credentials.ContainerMetadataFetcher)\n        exception = botocore.exceptions.CredentialRetrievalError\n        fetcher.retrieve_full_uri.side_effect = exception(\n            provider='ecs-role', error_msg='fake http error'\n        )\n        provider = credentials.ContainerProvider(environ, fetcher)\n\n        with self.assertRaises(exception):\n            provider.load()\n\n    def test_http_error_propagated_on_refresh(self):\n        # We should ensure errors are still propagated even in the\n        # case of a failed refresh.\n        environ = {\n            'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/latest/credentials?id=foo'\n        }\n        fetcher = mock.Mock(spec=credentials.ContainerMetadataFetcher)\n        timeobj = datetime.now(tzlocal())\n        expired_timestamp = (timeobj - timedelta(hours=23)).isoformat()\n        http_exception = botocore.exceptions.MetadataRetrievalError\n        raised_exception = botocore.exceptions.CredentialRetrievalError\n        fetcher.retrieve_full_uri.side_effect = [\n            {\n                \"AccessKeyId\": \"access_key_old\",\n                \"SecretAccessKey\": \"secret_key_old\",\n                \"Token\": \"token_old\",\n                \"Expiration\": expired_timestamp,\n            },\n            http_exception(error_msg='HTTP connection timeout'),\n        ]\n        provider = credentials.ContainerProvider(environ, fetcher)\n        # First time works with no issues.\n        creds = provider.load()\n        # Second time with a refresh should propagate an error.\n        with self.assertRaises(raised_exception):\n            creds.get_frozen_credentials()\n\n    def test_can_use_full_url(self):\n        environ = {\n            'AWS_CONTAINER_CREDENTIALS_FULL_URI': 'http://localhost/foo'\n        }\n        fetcher = self.create_fetcher()\n        timeobj = datetime.now(tzlocal())\n        timestamp = (timeobj + timedelta(hours=24)).isoformat()\n        fetcher.retrieve_full_uri.return_value = {\n            \"AccessKeyId\": \"access_key\",\n            \"SecretAccessKey\": \"secret_key\",\n            \"Token\": \"token\",\n            \"Expiration\": timestamp,\n        }\n        provider = credentials.ContainerProvider(environ, fetcher)\n        creds = provider.load()\n\n        fetcher.retrieve_full_uri.assert_called_with(\n            'http://localhost/foo', headers=None\n        )\n        self.assertEqual(creds.access_key, 'access_key')\n        self.assertEqual(creds.secret_key, 'secret_key')\n        self.assertEqual(creds.token, 'token')\n        self.assertEqual(creds.method, 'container-role')\n\n    def test_can_pass_basic_auth_token(self):\n        environ = {\n            'AWS_CONTAINER_CREDENTIALS_FULL_URI': 'http://localhost/foo',\n            'AWS_CONTAINER_AUTHORIZATION_TOKEN': 'Basic auth-token',\n        }\n        fetcher = self.create_fetcher()\n        timeobj = datetime.now(tzlocal())\n        timestamp = (timeobj + timedelta(hours=24)).isoformat()\n        fetcher.retrieve_full_uri.return_value = {\n            \"AccessKeyId\": \"access_key\",\n            \"SecretAccessKey\": \"secret_key\",\n            \"Token\": \"token\",\n            \"Expiration\": timestamp,\n        }\n        provider = credentials.ContainerProvider(environ, fetcher)\n        creds = provider.load()\n\n        fetcher.retrieve_full_uri.assert_called_with(\n            'http://localhost/foo',\n            headers={'Authorization': 'Basic auth-token'},\n        )\n        self.assertEqual(creds.access_key, 'access_key')\n        self.assertEqual(creds.secret_key, 'secret_key')\n        self.assertEqual(creds.token, 'token')\n        self.assertEqual(creds.method, 'container-role')\n\n    def test_can_pass_auth_token_from_file(self):\n        with temporary_file('w') as f:\n            f.write('Basic auth-token')\n            f.flush()\n            environ = {\n                'AWS_CONTAINER_CREDENTIALS_FULL_URI': 'http://localhost/foo',\n                'AWS_CONTAINER_AUTHORIZATION_TOKEN_FILE': f.name,\n            }\n            fetcher = self.create_fetcher()\n            timeobj = datetime.now(tzlocal())\n            timestamp = (timeobj + timedelta(hours=24)).isoformat()\n            fetcher.retrieve_full_uri.return_value = {\n                \"AccessKeyId\": \"access_key\",\n                \"SecretAccessKey\": \"secret_key\",\n                \"Token\": \"token\",\n                \"Expiration\": timestamp,\n            }\n            provider = credentials.ContainerProvider(environ, fetcher)\n            creds = provider.load()\n\n            fetcher.retrieve_full_uri.assert_called_with(\n                'http://localhost/foo',\n                headers={'Authorization': 'Basic auth-token'},\n            )\n            self.assertEqual(creds.access_key, 'access_key')\n            self.assertEqual(creds.secret_key, 'secret_key')\n            self.assertEqual(creds.token, 'token')\n            self.assertEqual(creds.method, 'container-role')\n\n    def test_reloads_auth_token_from_file(self):\n        with temporary_file('w') as f:\n            f.write('First auth token')\n            f.flush()\n            environ = {\n                'AWS_CONTAINER_CREDENTIALS_FULL_URI': 'http://localhost/foo',\n                'AWS_CONTAINER_AUTHORIZATION_TOKEN_FILE': f.name,\n            }\n            fetcher = self.create_fetcher()\n            timeobj = datetime.now(tzlocal())\n            fetcher.retrieve_full_uri.side_effect = [\n                {\n                    \"AccessKeyId\": \"first_key\",\n                    \"SecretAccessKey\": \"first_secret\",\n                    \"Token\": \"first_token\",\n                    \"Expiration\": (timeobj + timedelta(seconds=1)).isoformat(),\n                },\n                {\n                    \"AccessKeyId\": \"second_key\",\n                    \"SecretAccessKey\": \"second_secret\",\n                    \"Token\": \"second_token\",\n                    \"Expiration\": (timeobj + timedelta(minutes=5)).isoformat(),\n                },\n            ]\n            provider = credentials.ContainerProvider(environ, fetcher)\n            creds = provider.load()\n            fetcher.retrieve_full_uri.assert_called_with(\n                'http://localhost/foo',\n                headers={'Authorization': 'First auth token'},\n            )\n            time.sleep(1.5)\n            f.seek(0)\n            f.truncate()\n            f.write('Second auth token')\n            f.flush()\n            creds._refresh()\n            fetcher.retrieve_full_uri.assert_called_with(\n                'http://localhost/foo',\n                headers={'Authorization': 'Second auth token'},\n            )\n\n    def test_throws_error_on_invalid_token_file(self):\n        token_file_path = '/some/path/token.jwt'\n        environ = {\n            'AWS_CONTAINER_CREDENTIALS_FULL_URI': 'http://localhost/foo',\n            'AWS_CONTAINER_AUTHORIZATION_TOKEN_FILE': token_file_path,\n        }\n        fetcher = self.create_fetcher()\n        provider = credentials.ContainerProvider(environ, fetcher)\n\n        with self.assertRaises(FileNotFoundError):\n            provider.load()\n\n    def test_throws_error_on_illegal_header(self):\n        environ = {\n            'AWS_CONTAINER_CREDENTIALS_FULL_URI': 'http://localhost/foo',\n            'AWS_CONTAINER_AUTHORIZATION_TOKEN': 'invalid\\r\\ntoken',\n        }\n        fetcher = self.create_fetcher()\n        provider = credentials.ContainerProvider(environ, fetcher)\n\n        with self.assertRaises(ValueError):\n            provider.load()\n\n\nclass TestProcessProvider(BaseEnvVar):\n    def setUp(self):\n        super().setUp()\n        self.loaded_config = {}\n        self.load_config = mock.Mock(return_value=self.loaded_config)\n        self.invoked_process = mock.Mock()\n        self.popen_mock = mock.Mock(\n            return_value=self.invoked_process, spec=subprocess.Popen\n        )\n\n    def create_process_provider(self, profile_name='default'):\n        provider = ProcessProvider(\n            profile_name, self.load_config, popen=self.popen_mock\n        )\n        return provider\n\n    def _get_output(self, stdout, stderr=''):\n        return json.dumps(stdout).encode('utf-8'), stderr.encode('utf-8')\n\n    def _set_process_return_value(self, stdout, stderr='', rc=0):\n        output = self._get_output(stdout, stderr)\n        self.invoked_process.communicate.return_value = output\n        self.invoked_process.returncode = rc\n\n    def test_process_not_invoked_if_profile_does_not_exist(self):\n        # self.loaded_config is an empty dictionary with no profile\n        # information.\n        provider = self.create_process_provider()\n        self.assertIsNone(provider.load())\n\n    def test_process_not_invoked_if_not_configured_for_empty_config(self):\n        # No credential_process configured so we skip this provider.\n        self.loaded_config['profiles'] = {'default': {}}\n        provider = self.create_process_provider()\n        self.assertIsNone(provider.load())\n\n    def test_can_retrieve_via_process(self):\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        self._set_process_return_value(\n            {\n                'Version': 1,\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': '2999-01-01T00:00:00Z',\n            }\n        )\n\n        provider = self.create_process_provider()\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n        self.assertEqual(creds.method, 'custom-process')\n        self.popen_mock.assert_called_with(\n            ['my-process'], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n\n    def test_can_pass_arguments_through(self):\n        self.loaded_config['profiles'] = {\n            'default': {\n                'credential_process': 'my-process --foo --bar \"one two\"'\n            }\n        }\n        self._set_process_return_value(\n            {\n                'Version': 1,\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': '2999-01-01T00:00:00Z',\n            }\n        )\n\n        provider = self.create_process_provider()\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.popen_mock.assert_called_with(\n            ['my-process', '--foo', '--bar', 'one two'],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n\n    def test_can_refresh_credentials(self):\n        # We given a time that's already expired so .access_key\n        # will trigger the refresh worfklow.  We just need to verify\n        # that the refresh function gives the same result as the\n        # initial retrieval.\n        expired_date = '2016-01-01T00:00:00Z'\n        future_date = str(datetime.now(tzlocal()) + timedelta(hours=24))\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        old_creds = self._get_output(\n            {\n                'Version': 1,\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': expired_date,\n            }\n        )\n        new_creds = self._get_output(\n            {\n                'Version': 1,\n                'AccessKeyId': 'foo2',\n                'SecretAccessKey': 'bar2',\n                'SessionToken': 'baz2',\n                'Expiration': future_date,\n            }\n        )\n        self.invoked_process.communicate.side_effect = [old_creds, new_creds]\n        self.invoked_process.returncode = 0\n\n        provider = self.create_process_provider()\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo2')\n        self.assertEqual(creds.secret_key, 'bar2')\n        self.assertEqual(creds.token, 'baz2')\n        self.assertEqual(creds.method, 'custom-process')\n\n    def test_non_zero_rc_raises_exception(self):\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        self._set_process_return_value('', 'Error Message', 1)\n\n        provider = self.create_process_provider()\n        exception = botocore.exceptions.CredentialRetrievalError\n        with self.assertRaisesRegex(exception, 'Error Message'):\n            provider.load()\n\n    def test_unsupported_version_raises_mismatch(self):\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        bad_version = 100\n        self._set_process_return_value(\n            {\n                'Version': bad_version,\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': '2999-01-01T00:00:00Z',\n            }\n        )\n\n        provider = self.create_process_provider()\n        exception = botocore.exceptions.CredentialRetrievalError\n        with self.assertRaisesRegex(exception, 'Unsupported version'):\n            provider.load()\n\n    def test_missing_version_in_payload_returned_raises_exception(self):\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        self._set_process_return_value(\n            {\n                # Let's say they forget a 'Version' key.\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': '2999-01-01T00:00:00Z',\n            }\n        )\n\n        provider = self.create_process_provider()\n        exception = botocore.exceptions.CredentialRetrievalError\n        with self.assertRaisesRegex(exception, 'Unsupported version'):\n            provider.load()\n\n    def test_missing_access_key_raises_exception(self):\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        self._set_process_return_value(\n            {\n                'Version': 1,\n                # Missing access key.\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': '2999-01-01T00:00:00Z',\n            }\n        )\n\n        provider = self.create_process_provider()\n        exception = botocore.exceptions.CredentialRetrievalError\n        with self.assertRaisesRegex(exception, 'Missing required key'):\n            provider.load()\n\n    def test_missing_secret_key_raises_exception(self):\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        self._set_process_return_value(\n            {\n                'Version': 1,\n                'AccessKeyId': 'foo',\n                # Missing secret key.\n                'SessionToken': 'baz',\n                'Expiration': '2999-01-01T00:00:00Z',\n            }\n        )\n\n        provider = self.create_process_provider()\n        exception = botocore.exceptions.CredentialRetrievalError\n        with self.assertRaisesRegex(exception, 'Missing required key'):\n            provider.load()\n\n    def test_missing_session_token(self):\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        self._set_process_return_value(\n            {\n                'Version': 1,\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                # Missing session token.\n                'Expiration': '2999-01-01T00:00:00Z',\n            }\n        )\n\n        provider = self.create_process_provider()\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertIsNone(creds.token)\n        self.assertEqual(creds.method, 'custom-process')\n\n    def test_missing_expiration(self):\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        self._set_process_return_value(\n            {\n                'Version': 1,\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                # Missing expiration.\n            }\n        )\n\n        provider = self.create_process_provider()\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertEqual(creds.token, 'baz')\n        self.assertEqual(creds.method, 'custom-process')\n\n    def test_missing_expiration_and_session_token(self):\n        self.loaded_config['profiles'] = {\n            'default': {'credential_process': 'my-process'}\n        }\n        self._set_process_return_value(\n            {\n                'Version': 1,\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                # Missing session token and expiration\n            }\n        )\n\n        provider = self.create_process_provider()\n        creds = provider.load()\n        self.assertIsNotNone(creds)\n        self.assertEqual(creds.access_key, 'foo')\n        self.assertEqual(creds.secret_key, 'bar')\n        self.assertIsNone(creds.token)\n        self.assertEqual(creds.method, 'custom-process')\n\n\nclass TestProfileProviderBuilder(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n        self.mock_session = mock.Mock(spec=Session)\n        self.builder = ProfileProviderBuilder(self.mock_session)\n\n    def test_profile_provider_builder_order(self):\n        providers = self.builder.providers('some-profile')\n        expected_providers = [\n            AssumeRoleWithWebIdentityProvider,\n            SSOProvider,\n            SharedCredentialProvider,\n            ProcessProvider,\n            ConfigProvider,\n        ]\n        self.assertEqual(len(providers), len(expected_providers))\n        zipped_providers = zip(providers, expected_providers)\n        for provider, expected_type in zipped_providers:\n            self.assertTrue(isinstance(provider, expected_type))\n\n\nclass TestSSOCredentialFetcher(unittest.TestCase):\n    def setUp(self):\n        self.sso = Session().create_client('sso', region_name='us-east-1')\n        self.stubber = Stubber(self.sso)\n        self.mock_session = mock.Mock(spec=Session)\n        self.mock_session.create_client.return_value = self.sso\n\n        self.cache = {}\n        self.sso_region = 'us-east-1'\n        self.start_url = 'https://d-92671207e4.awsapps.com/start'\n        self.role_name = 'test-role'\n        self.account_id = '1234567890'\n        self.access_token = {\n            'accessToken': 'some.sso.token',\n        }\n        # This is just an arbitrary point in time we can pin to\n        self.now = datetime(2008, 9, 23, 12, 26, 40, tzinfo=tzutc())\n        # The SSO endpoint uses ms whereas the OIDC endpoint uses seconds\n        self.now_timestamp = 1222172800000\n\n        self.loader = mock.Mock(spec=SSOTokenLoader)\n        self.loader.return_value = self.access_token\n        self.fetcher = SSOCredentialFetcher(\n            self.start_url,\n            self.sso_region,\n            self.role_name,\n            self.account_id,\n            self.mock_session.create_client,\n            token_loader=self.loader,\n            cache=self.cache,\n        )\n\n    def test_can_fetch_credentials(self):\n        expected_params = {\n            'roleName': self.role_name,\n            'accountId': self.account_id,\n            'accessToken': self.access_token['accessToken'],\n        }\n        expected_response = {\n            'roleCredentials': {\n                'accessKeyId': 'foo',\n                'secretAccessKey': 'bar',\n                'sessionToken': 'baz',\n                'expiration': self.now_timestamp + 1000000,\n            }\n        }\n        self.stubber.add_response(\n            'get_role_credentials',\n            expected_response,\n            expected_params=expected_params,\n        )\n        with self.stubber:\n            credentials = self.fetcher.fetch_credentials()\n        self.assertEqual(credentials['access_key'], 'foo')\n        self.assertEqual(credentials['secret_key'], 'bar')\n        self.assertEqual(credentials['token'], 'baz')\n        self.assertEqual(credentials['expiry_time'], '2008-09-23T12:43:20Z')\n        cache_key = '048db75bbe50955c16af7aba6ff9c41a3131bb7e'\n        expected_cached_credentials = {\n            'ProviderType': 'sso',\n            'Credentials': {\n                'AccessKeyId': 'foo',\n                'SecretAccessKey': 'bar',\n                'SessionToken': 'baz',\n                'Expiration': '2008-09-23T12:43:20Z',\n            },\n        }\n        self.assertEqual(self.cache[cache_key], expected_cached_credentials)\n\n    def test_raises_helpful_message_on_unauthorized_exception(self):\n        expected_params = {\n            'roleName': self.role_name,\n            'accountId': self.account_id,\n            'accessToken': self.access_token['accessToken'],\n        }\n        self.stubber.add_client_error(\n            'get_role_credentials',\n            service_error_code='UnauthorizedException',\n            expected_params=expected_params,\n        )\n        with self.assertRaises(botocore.exceptions.UnauthorizedSSOTokenError):\n            with self.stubber:\n                self.fetcher.fetch_credentials()\n\n\nclass TestSSOProvider(unittest.TestCase):\n    def setUp(self):\n        self.sso = Session().create_client('sso', region_name='us-east-1')\n        self.stubber = Stubber(self.sso)\n        self.mock_session = mock.Mock(spec=Session)\n        self.mock_session.create_client.return_value = self.sso\n\n        self.sso_region = 'us-east-1'\n        self.start_url = 'https://d-92671207e4.awsapps.com/start'\n        self.role_name = 'test-role'\n        self.account_id = '1234567890'\n        self.access_token = 'some.sso.token'\n\n        self.profile_name = 'sso-profile'\n        self.config = {\n            'sso_region': self.sso_region,\n            'sso_start_url': self.start_url,\n            'sso_role_name': self.role_name,\n            'sso_account_id': self.account_id,\n        }\n        self.expires_at = datetime.now(tzutc()) + timedelta(hours=24)\n        self.cached_creds_key = '048db75bbe50955c16af7aba6ff9c41a3131bb7e'\n        self.cached_token_key = '13f9d35043871d073ab260e020f0ffde092cb14b'\n        self.cache = {\n            self.cached_token_key: {\n                'accessToken': self.access_token,\n                'expiresAt': self.expires_at.strftime('%Y-%m-%dT%H:%M:%S%Z'),\n            }\n        }\n        self.provider = SSOProvider(\n            load_config=self._mock_load_config,\n            client_creator=self.mock_session.create_client,\n            profile_name=self.profile_name,\n            cache=self.cache,\n            token_cache=self.cache,\n        )\n\n        self.expected_get_role_credentials_params = {\n            'roleName': self.role_name,\n            'accountId': self.account_id,\n            'accessToken': self.access_token,\n        }\n        expiration = datetime2timestamp(self.expires_at)\n        self.expected_get_role_credentials_response = {\n            'roleCredentials': {\n                'accessKeyId': 'foo',\n                'secretAccessKey': 'bar',\n                'sessionToken': 'baz',\n                'expiration': int(expiration * 1000),\n            }\n        }\n\n    def _mock_load_config(self):\n        return {\n            'profiles': {\n                self.profile_name: self.config,\n            }\n        }\n\n    def _add_get_role_credentials_response(self):\n        self.stubber.add_response(\n            'get_role_credentials',\n            self.expected_get_role_credentials_response,\n            self.expected_get_role_credentials_params,\n        )\n\n    def test_load_sso_credentials_without_cache(self):\n        self._add_get_role_credentials_response()\n        with self.stubber:\n            credentials = self.provider.load()\n            self.assertEqual(credentials.access_key, 'foo')\n            self.assertEqual(credentials.secret_key, 'bar')\n            self.assertEqual(credentials.token, 'baz')\n\n    def test_load_sso_credentials_with_cache(self):\n        cached_creds = {\n            'Credentials': {\n                'AccessKeyId': 'cached-akid',\n                'SecretAccessKey': 'cached-sak',\n                'SessionToken': 'cached-st',\n                'Expiration': self.expires_at.strftime('%Y-%m-%dT%H:%M:%S%Z'),\n            }\n        }\n        self.cache[self.cached_creds_key] = cached_creds\n        credentials = self.provider.load()\n        self.assertEqual(credentials.access_key, 'cached-akid')\n        self.assertEqual(credentials.secret_key, 'cached-sak')\n        self.assertEqual(credentials.token, 'cached-st')\n\n    def test_load_sso_credentials_with_cache_expired(self):\n        cached_creds = {\n            'Credentials': {\n                'AccessKeyId': 'expired-akid',\n                'SecretAccessKey': 'expired-sak',\n                'SessionToken': 'expired-st',\n                'Expiration': '2002-10-22T20:52:11UTC',\n            }\n        }\n        self.cache[self.cached_creds_key] = cached_creds\n\n        self._add_get_role_credentials_response()\n        with self.stubber:\n            credentials = self.provider.load()\n            self.assertEqual(credentials.access_key, 'foo')\n            self.assertEqual(credentials.secret_key, 'bar')\n            self.assertEqual(credentials.token, 'baz')\n\n    def test_required_config_not_set(self):\n        del self.config['sso_start_url']\n        # If any required configuration is missing we should get an error\n        with self.assertRaises(botocore.exceptions.InvalidConfigError):\n            self.provider.load()\n", "tests/unit/test_history.py": "from botocore.history import (\n    BaseHistoryHandler,\n    HistoryRecorder,\n    get_global_history_recorder,\n)\nfrom tests import mock, unittest\n\n\nclass TerribleError(Exception):\n    pass\n\n\nclass ExceptionThrowingHandler(BaseHistoryHandler):\n    def emit(self, event_type, payload, source):\n        raise TerribleError('Bad behaving handler')\n\n\nclass TestHistoryRecorder(unittest.TestCase):\n    def test_can_attach_and_call_handler_emit(self):\n        mock_handler = mock.Mock(spec=BaseHistoryHandler)\n        recorder = HistoryRecorder()\n        recorder.enable()\n        recorder.add_handler(mock_handler)\n        recorder.record('foo', 'bar', source='source')\n\n        mock_handler.emit.assert_called_with('foo', 'bar', 'source')\n\n    def test_can_call_multiple_handlers(self):\n        first_handler = mock.Mock(spec=BaseHistoryHandler)\n        second_handler = mock.Mock(spec=BaseHistoryHandler)\n        recorder = HistoryRecorder()\n        recorder.enable()\n        recorder.add_handler(first_handler)\n        recorder.add_handler(second_handler)\n        recorder.record('foo', 'bar', source='source')\n\n        first_handler.emit.assert_called_with('foo', 'bar', 'source')\n        second_handler.emit.assert_called_with('foo', 'bar', 'source')\n\n    def test_does_use_botocore_source_by_default(self):\n        mock_handler = mock.Mock(spec=BaseHistoryHandler)\n        recorder = HistoryRecorder()\n        recorder.enable()\n        recorder.add_handler(mock_handler)\n        recorder.record('foo', 'bar')\n\n        mock_handler.emit.assert_called_with('foo', 'bar', 'BOTOCORE')\n\n    def test_does_not_call_handlers_when_never_enabled(self):\n        mock_handler = mock.Mock(spec=BaseHistoryHandler)\n        recorder = HistoryRecorder()\n        recorder.add_handler(mock_handler)\n        recorder.record('foo', 'bar')\n\n        mock_handler.emit.assert_not_called()\n\n    def test_does_not_call_handlers_when_disabled(self):\n        mock_handler = mock.Mock(spec=BaseHistoryHandler)\n        recorder = HistoryRecorder()\n        recorder.enable()\n        recorder.disable()\n        recorder.add_handler(mock_handler)\n        recorder.record('foo', 'bar')\n\n        mock_handler.emit.assert_not_called()\n\n    def test_can_ignore_handler_exceptions(self):\n        mock_handler = mock.Mock(spec=BaseHistoryHandler)\n        recorder = HistoryRecorder()\n        recorder.enable()\n        bad_handler = ExceptionThrowingHandler()\n        recorder.add_handler(bad_handler)\n        recorder.add_handler(mock_handler)\n        try:\n            recorder.record('foo', 'bar')\n        except TerribleError:\n            self.fail('Should not have raised a TerribleError')\n        mock_handler.emit.assert_called_with('foo', 'bar', 'BOTOCORE')\n\n\nclass TestGetHistoryRecorder(unittest.TestCase):\n    def test_can_get_history_recorder(self):\n        recorder = get_global_history_recorder()\n        self.assertTrue(isinstance(recorder, HistoryRecorder))\n\n    def test_does_reuse_history_recorder(self):\n        recorder_1 = get_global_history_recorder()\n        recorder_2 = get_global_history_recorder()\n        self.assertIs(recorder_1, recorder_2)\n", "tests/unit/test_eventstream.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Unit tests for the binary event stream decoder. \"\"\"\nimport pytest\n\nfrom botocore.eventstream import (\n    ChecksumMismatch,\n    DecodeUtils,\n    DuplicateHeader,\n    EventStream,\n    EventStreamBuffer,\n    EventStreamHeaderParser,\n    EventStreamMessage,\n    InvalidHeadersLength,\n    InvalidPayloadLength,\n    MessagePrelude,\n    NoInitialResponseError,\n)\nfrom botocore.exceptions import EventStreamError\nfrom botocore.parsers import EventStreamXMLParser\nfrom tests import mock\n\nEMPTY_MESSAGE = (\n    b'\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x05\\xc2H\\xeb}\\x98\\xc8\\xff',\n    EventStreamMessage(\n        prelude=MessagePrelude(\n            total_length=0x10,\n            headers_length=0,\n            crc=0x05C248EB,\n        ),\n        headers={},\n        payload=b'',\n        crc=0x7D98C8FF,\n    ),\n)\n\nINT8_HEADER = (\n    (\n        b\"\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\x07)\\x86\\x01X\\x04\"\n        b\"byte\\x02\\xff\\xc2\\xf8i\\xdc\"\n    ),\n    EventStreamMessage(\n        prelude=MessagePrelude(\n            total_length=0x17,\n            headers_length=0x7,\n            crc=0x29860158,\n        ),\n        headers={'byte': -1},\n        payload=b'',\n        crc=0xC2F869DC,\n    ),\n)\n\nINT16_HEADER = (\n    (\n        b\"\\x00\\x00\\x00\\x19\\x00\\x00\\x00\\tq\\x0e\\x92>\\x05\"\n        b\"short\\x03\\xff\\xff\\xb2|\\xb6\\xcc\"\n    ),\n    EventStreamMessage(\n        prelude=MessagePrelude(\n            total_length=0x19,\n            headers_length=0x9,\n            crc=0x710E923E,\n        ),\n        headers={'short': -1},\n        payload=b'',\n        crc=0xB27CB6CC,\n    ),\n)\n\nINT32_HEADER = (\n    (\n        b\"\\x00\\x00\\x00\\x1d\\x00\\x00\\x00\\r\\x83\\xe3\\xf0\\xe7\\x07\"\n        b\"integer\\x04\\xff\\xff\\xff\\xff\\x8b\\x8e\\x12\\xeb\"\n    ),\n    EventStreamMessage(\n        prelude=MessagePrelude(\n            total_length=0x1D,\n            headers_length=0xD,\n            crc=0x83E3F0E7,\n        ),\n        headers={'integer': -1},\n        payload=b'',\n        crc=0x8B8E12EB,\n    ),\n)\n\nINT64_HEADER = (\n    (\n        b\"\\x00\\x00\\x00\\x1e\\x00\\x00\\x00\\x0e]J\\xdb\\x8d\\x04\"\n        b\"long\\x05\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xffK\\xc22\\xda\"\n    ),\n    EventStreamMessage(\n        prelude=MessagePrelude(\n            total_length=0x1E,\n            headers_length=0xE,\n            crc=0x5D4ADB8D,\n        ),\n        headers={'long': -1},\n        payload=b'',\n        crc=0x4BC232DA,\n    ),\n)\n\nPAYLOAD_NO_HEADERS = (\n    b\"\\x00\\x00\\x00\\x1d\\x00\\x00\\x00\\x00\\xfdR\\x8cZ{'foo':'bar'}\\xc3e96\",\n    EventStreamMessage(\n        prelude=MessagePrelude(\n            total_length=0x1D,\n            headers_length=0,\n            crc=0xFD528C5A,\n        ),\n        headers={},\n        payload=b\"{'foo':'bar'}\",\n        crc=0xC3653936,\n    ),\n)\n\nPAYLOAD_ONE_STR_HEADER = (\n    (\n        b\"\\x00\\x00\\x00=\\x00\\x00\\x00 \\x07\\xfd\\x83\\x96\\x0ccontent-type\\x07\\x00\\x10\"\n        b\"application/json{'foo':'bar'}\\x8d\\x9c\\x08\\xb1\"\n    ),\n    EventStreamMessage(\n        prelude=MessagePrelude(\n            total_length=0x3D,\n            headers_length=0x20,\n            crc=0x07FD8396,\n        ),\n        headers={'content-type': 'application/json'},\n        payload=b\"{'foo':'bar'}\",\n        crc=0x8D9C08B1,\n    ),\n)\n\nALL_HEADERS_TYPES = (\n    (\n        b\"\\x00\\x00\\x00\\x62\\x00\\x00\\x00\\x52\\x03\\xb5\\xcb\\x9c\"\n        b\"\\x010\\x00\\x011\\x01\\x012\\x02\\x02\\x013\\x03\\x00\\x03\"\n        b\"\\x014\\x04\\x00\\x00\\x00\\x04\\x015\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\"\n        b\"\\x016\\x06\\x00\\x05bytes\\x017\\x07\\x00\\x04utf8\"\n        b\"\\x018\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x019\\x090123456789abcdef\"\n        b\"\\x63\\x35\\x36\\x71\"\n    ),\n    EventStreamMessage(\n        prelude=MessagePrelude(\n            total_length=0x62,\n            headers_length=0x52,\n            crc=0x03B5CB9C,\n        ),\n        headers={\n            '0': True,\n            '1': False,\n            '2': 0x02,\n            '3': 0x03,\n            '4': 0x04,\n            '5': 0x05,\n            '6': b'bytes',\n            '7': 'utf8',\n            '8': 0x08,\n            '9': b'0123456789abcdef',\n        },\n        payload=b\"\",\n        crc=0x63353671,\n    ),\n)\n\nERROR_EVENT_MESSAGE = (\n    (\n        b\"\\x00\\x00\\x00\\x52\\x00\\x00\\x00\\x42\\xbf\\x23\\x63\\x7e\"\n        b\"\\x0d:message-type\\x07\\x00\\x05error\"\n        b\"\\x0b:error-code\\x07\\x00\\x04code\"\n        b\"\\x0e:error-message\\x07\\x00\\x07message\"\n        b\"\\x6b\\x6c\\xea\\x3d\"\n    ),\n    EventStreamMessage(\n        prelude=MessagePrelude(\n            total_length=0x52,\n            headers_length=0x42,\n            crc=0xBF23637E,\n        ),\n        headers={\n            ':message-type': 'error',\n            ':error-code': 'code',\n            ':error-message': 'message',\n        },\n        payload=b'',\n        crc=0x6B6CEA3D,\n    ),\n)\n\n# Tuples of encoded messages and their expected decoded output\nPOSITIVE_CASES = [\n    EMPTY_MESSAGE,\n    INT8_HEADER,\n    INT16_HEADER,\n    INT32_HEADER,\n    INT64_HEADER,\n    PAYLOAD_NO_HEADERS,\n    PAYLOAD_ONE_STR_HEADER,\n    ALL_HEADERS_TYPES,\n    ERROR_EVENT_MESSAGE,\n]\n\nCORRUPTED_HEADER_LENGTH = (\n    (\n        b\"\\x00\\x00\\x00=\\xFF\\x00\\x01\\x02\\x07\\xfd\\x83\\x96\\x0ccontent-type\\x07\\x00\"\n        b\"\\x10application/json{'foo':'bar'}\\x8d\\x9c\\x08\\xb1\"\n    ),\n    InvalidHeadersLength,\n)\n\nCORRUPTED_HEADERS = (\n    (\n        b\"\\x00\\x00\\x00=\\x00\\x00\\x00 \\x07\\xfd\\x83\\x96\\x0ccontent+type\\x07\\x00\\x10\"\n        b\"application/json{'foo':'bar'}\\x8d\\x9c\\x08\\xb1\"\n    ),\n    ChecksumMismatch,\n)\n\nCORRUPTED_LENGTH = (\n    b\"\\x01\\x00\\x00\\x1d\\x00\\x00\\x00\\x00\\xfdR\\x8cZ{'foo':'bar'}\\xc3e96\",\n    InvalidPayloadLength,\n)\n\nCORRUPTED_PAYLOAD = (\n    b\"\\x00\\x00\\x00\\x1d\\x00\\x00\\x00\\x00\\xfdR\\x8cZ{'foo':'bar'\\x8d\\xc3e96\",\n    ChecksumMismatch,\n)\n\nDUPLICATE_HEADER = (\n    (\n        b\"\\x00\\x00\\x00\\x24\\x00\\x00\\x00\\x14\\x4b\\xb9\\x82\\xd0\"\n        b\"\\x04test\\x04asdf\\x04test\\x04asdf\\xf3\\xf4\\x75\\x63\"\n    ),\n    DuplicateHeader,\n)\n\n# Tuples of encoded messages and their expected exception\nNEGATIVE_CASES = [\n    CORRUPTED_LENGTH,\n    CORRUPTED_PAYLOAD,\n    CORRUPTED_HEADERS,\n    CORRUPTED_HEADER_LENGTH,\n    DUPLICATE_HEADER,\n]\n\n\ndef assert_message_equal(message_a, message_b):\n    \"\"\"Asserts all fields for two messages are equal.\"\"\"\n    assert message_a.prelude.total_length == message_b.prelude.total_length\n    assert message_a.prelude.headers_length == message_b.prelude.headers_length\n    assert message_a.prelude.crc == message_b.prelude.crc\n    assert message_a.headers == message_b.headers\n    assert message_a.payload == message_b.payload\n    assert message_a.crc == message_b.crc\n\n\ndef test_partial_message():\n    \"\"\"Ensure that we can receive partial payloads.\"\"\"\n    data = EMPTY_MESSAGE[0]\n    event_buffer = EventStreamBuffer()\n    # This mid point is an arbitrary break in the middle of the headers\n    mid_point = 15\n    event_buffer.add_data(data[:mid_point])\n    messages = list(event_buffer)\n    assert messages == []\n    event_buffer.add_data(data[mid_point : len(data)])\n    for message in event_buffer:\n        assert_message_equal(message, EMPTY_MESSAGE[1])\n\n\ndef check_message_decodes(encoded, decoded):\n    \"\"\"Ensure the message decodes to what we expect.\"\"\"\n    event_buffer = EventStreamBuffer()\n    event_buffer.add_data(encoded)\n    messages = list(event_buffer)\n    assert len(messages) == 1\n    assert_message_equal(messages[0], decoded)\n\n\n@pytest.mark.parametrize(\"encoded, decoded\", POSITIVE_CASES)\ndef test_positive_cases(encoded, decoded):\n    \"\"\"Test that all positive cases decode how we expect.\"\"\"\n    check_message_decodes(encoded, decoded)\n\n\ndef test_all_positive_cases():\n    \"\"\"Test all positive cases can be decoded on the same buffer.\"\"\"\n    event_buffer = EventStreamBuffer()\n    # add all positive test cases to the same buffer\n    for encoded, _ in POSITIVE_CASES:\n        event_buffer.add_data(encoded)\n    # collect all of the expected messages\n    expected_messages = [decoded for (_, decoded) in POSITIVE_CASES]\n    # collect all of the decoded messages\n    decoded_messages = list(event_buffer)\n    # assert all messages match what we expect\n    for expected, decoded in zip(expected_messages, decoded_messages):\n        assert_message_equal(expected, decoded)\n\n\n@pytest.mark.parametrize(\"encoded, exception\", NEGATIVE_CASES)\ndef test_negative_cases(encoded, exception):\n    \"\"\"Test that all negative cases raise the expected exception.\"\"\"\n    with pytest.raises(exception):\n        check_message_decodes(encoded, None)\n\n\ndef test_header_parser():\n    \"\"\"Test that the header parser supports all header types.\"\"\"\n    headers_data = (\n        b\"\\x010\\x00\\x011\\x01\\x012\\x02\\x02\\x013\\x03\\x00\\x03\"\n        b\"\\x014\\x04\\x00\\x00\\x00\\x04\\x015\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\"\n        b\"\\x016\\x06\\x00\\x05bytes\\x017\\x07\\x00\\x04utf8\"\n        b\"\\x018\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x019\\x090123456789abcdef\"\n    )\n\n    expected_headers = {\n        '0': True,\n        '1': False,\n        '2': 0x02,\n        '3': 0x03,\n        '4': 0x04,\n        '5': 0x05,\n        '6': b'bytes',\n        '7': 'utf8',\n        '8': 0x08,\n        '9': b'0123456789abcdef',\n    }\n\n    parser = EventStreamHeaderParser()\n    headers = parser.parse(headers_data)\n    assert headers == expected_headers\n\n\ndef test_message_prelude_properties():\n    \"\"\"Test that calculated properties from the payload are correct.\"\"\"\n    # Total length: 40, Headers Length: 15, random crc\n    prelude = MessagePrelude(40, 15, 0x00000000)\n    assert prelude.payload_length == 9\n    assert prelude.headers_end == 27\n    assert prelude.payload_end == 36\n\n\ndef test_message_to_response_dict():\n    response_dict = PAYLOAD_ONE_STR_HEADER[1].to_response_dict()\n    assert response_dict['status_code'] == 200\n\n    expected_headers = {'content-type': 'application/json'}\n    assert response_dict['headers'] == expected_headers\n    assert response_dict['body'] == b\"{'foo':'bar'}\"\n\n\ndef test_message_to_response_dict_error():\n    response_dict = ERROR_EVENT_MESSAGE[1].to_response_dict()\n    assert response_dict['status_code'] == 400\n    headers = {\n        ':message-type': 'error',\n        ':error-code': 'code',\n        ':error-message': 'message',\n    }\n    assert response_dict['headers'] == headers\n    assert response_dict['body'] == b''\n\n\ndef test_unpack_uint8():\n    (value, bytes_consumed) = DecodeUtils.unpack_uint8(b'\\xDE')\n    assert bytes_consumed == 1\n    assert value == 0xDE\n\n\ndef test_unpack_uint32():\n    (value, bytes_consumed) = DecodeUtils.unpack_uint32(b'\\xDE\\xAD\\xBE\\xEF')\n    assert bytes_consumed == 4\n    assert value == 0xDEADBEEF\n\n\ndef test_unpack_int8():\n    (value, bytes_consumed) = DecodeUtils.unpack_int8(b'\\xFE')\n    assert bytes_consumed == 1\n    assert value == -2\n\n\ndef test_unpack_int16():\n    (value, bytes_consumed) = DecodeUtils.unpack_int16(b'\\xFF\\xFE')\n    assert bytes_consumed == 2\n    assert value == -2\n\n\ndef test_unpack_int32():\n    (value, bytes_consumed) = DecodeUtils.unpack_int32(b'\\xFF\\xFF\\xFF\\xFE')\n    assert bytes_consumed == 4\n    assert value == -2\n\n\ndef test_unpack_int64():\n    test_bytes = b'\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFE'\n    (value, bytes_consumed) = DecodeUtils.unpack_int64(test_bytes)\n    assert bytes_consumed == 8\n    assert value == -2\n\n\ndef test_unpack_array_short():\n    test_bytes = b'\\x00\\x10application/json'\n    (value, bytes_consumed) = DecodeUtils.unpack_byte_array(test_bytes)\n    assert bytes_consumed == 18\n    assert value == b'application/json'\n\n\ndef test_unpack_byte_array_int():\n    (value, array_bytes_consumed) = DecodeUtils.unpack_byte_array(\n        b'\\x00\\x00\\x00\\x10application/json', length_byte_size=4\n    )\n    assert array_bytes_consumed == 20\n    assert value == b'application/json'\n\n\ndef test_unpack_utf8_string():\n    length = b'\\x00\\x09'\n    utf8_string = b'\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e'\n    encoded = length + utf8_string\n    (value, bytes_consumed) = DecodeUtils.unpack_utf8_string(encoded)\n    assert bytes_consumed == 11\n    assert value == utf8_string.decode('utf-8')\n\n\ndef test_unpack_prelude():\n    data = b'\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03'\n    prelude = DecodeUtils.unpack_prelude(data)\n    assert prelude == ((1, 2, 3), 12)\n\n\ndef create_mock_raw_stream(*data):\n    raw_stream = mock.Mock()\n\n    def generator():\n        yield from data\n\n    raw_stream.stream = generator\n    return raw_stream\n\n\ndef test_event_stream_wrapper_iteration():\n    raw_stream = create_mock_raw_stream(\n        b\"\\x00\\x00\\x00+\\x00\\x00\\x00\\x0e4\\x8b\\xec{\\x08event-id\\x04\\x00\",\n        b\"\\x00\\xa0\\x0c{'foo':'bar'}\\xd3\\x89\\x02\\x85\",\n    )\n    parser = mock.Mock(spec=EventStreamXMLParser)\n    output_shape = mock.Mock()\n    event_stream = EventStream(raw_stream, output_shape, parser, '')\n    events = list(event_stream)\n    assert len(events) == 1\n\n    response_dict = {\n        'headers': {'event-id': 0x0000A00C},\n        'body': b\"{'foo':'bar'}\",\n        'status_code': 200,\n    }\n    parser.parse.assert_called_with(response_dict, output_shape)\n\n\ndef test_eventstream_wrapper_iteration_error():\n    raw_stream = create_mock_raw_stream(ERROR_EVENT_MESSAGE[0])\n    parser = mock.Mock(spec=EventStreamXMLParser)\n    parser.parse.return_value = {}\n    output_shape = mock.Mock()\n    event_stream = EventStream(raw_stream, output_shape, parser, '')\n    with pytest.raises(EventStreamError):\n        list(event_stream)\n\n\ndef test_event_stream_wrapper_close():\n    raw_stream = mock.Mock()\n    event_stream = EventStream(raw_stream, None, None, '')\n    event_stream.close()\n    raw_stream.close.assert_called_once_with()\n\n\ndef test_event_stream_initial_response():\n    raw_stream = create_mock_raw_stream(\n        b'\\x00\\x00\\x00~\\x00\\x00\\x00O\\xc5\\xa3\\xdd\\xc6\\r:message-type\\x07\\x00',\n        b'\\x05event\\x0b:event-type\\x07\\x00\\x10initial-response\\r:content-type',\n        b'\\x07\\x00\\ttext/json{\"InitialResponse\": \"sometext\"}\\xf6\\x98$\\x83',\n    )\n    parser = mock.Mock(spec=EventStreamXMLParser)\n    output_shape = mock.Mock()\n    event_stream = EventStream(raw_stream, output_shape, parser, '')\n    event = event_stream.get_initial_response()\n    headers = {\n        ':message-type': 'event',\n        ':event-type': 'initial-response',\n        ':content-type': 'text/json',\n    }\n    payload = b'{\"InitialResponse\": \"sometext\"}'\n    assert event.headers == headers\n    assert event.payload == payload\n\n\ndef test_event_stream_initial_response_wrong_type():\n    raw_stream = create_mock_raw_stream(\n        b\"\\x00\\x00\\x00+\\x00\\x00\\x00\\x0e4\\x8b\\xec{\\x08event-id\\x04\\x00\",\n        b\"\\x00\\xa0\\x0c{'foo':'bar'}\\xd3\\x89\\x02\\x85\",\n    )\n    parser = mock.Mock(spec=EventStreamXMLParser)\n    output_shape = mock.Mock()\n    event_stream = EventStream(raw_stream, output_shape, parser, '')\n    with pytest.raises(NoInitialResponseError):\n        event_stream.get_initial_response()\n\n\ndef test_event_stream_initial_response_no_event():\n    raw_stream = create_mock_raw_stream(b'')\n    parser = mock.Mock(spec=EventStreamXMLParser)\n    output_shape = mock.Mock()\n    event_stream = EventStream(raw_stream, output_shape, parser, '')\n    with pytest.raises(NoInitialResponseError):\n        event_stream.get_initial_response()\n", "tests/unit/test_stub.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore import hooks\nfrom botocore.exceptions import ParamValidationError, UnStubbedResponseError\nfrom botocore.model import ServiceModel\nfrom botocore.stub import Stubber\nfrom tests import mock, unittest\n\n\nclass TestStubber(unittest.TestCase):\n    def setUp(self):\n        self.event_emitter = hooks.HierarchicalEmitter()\n        self.client = mock.Mock()\n        self.client.meta.events = self.event_emitter\n        self.client.meta.method_to_api_mapping.get.return_value = 'foo'\n        self.stubber = Stubber(self.client)\n        self.validate_parameters_mock = mock.Mock()\n        self.validate_parameters_patch = mock.patch(\n            'botocore.stub.validate_parameters', self.validate_parameters_mock\n        )\n        self.validate_parameters_patch.start()\n\n    def tearDown(self):\n        self.validate_parameters_patch.stop()\n\n    def emit_get_response_event(\n        self, model=None, request_dict=None, signer=None, context=None\n    ):\n        if model is None:\n            model = mock.Mock()\n            model.name = 'foo'\n\n        handler, response = self.event_emitter.emit_until_response(\n            event_name='before-call.myservice.foo',\n            model=model,\n            params=request_dict,\n            request_signer=signer,\n            context=context,\n        )\n\n        return response\n\n    def test_stubber_registers_events(self):\n        self.event_emitter = mock.Mock()\n        self.client.meta.events = self.event_emitter\n        self.stubber.activate()\n        # This just ensures that we register at the correct event\n        # and nothing more\n        self.event_emitter.register_first.assert_called_with(\n            'before-parameter-build.*.*', mock.ANY, unique_id=mock.ANY\n        )\n        self.event_emitter.register.assert_called_with(\n            'before-call.*.*', mock.ANY, unique_id=mock.ANY\n        )\n\n    def test_stubber_unregisters_events(self):\n        self.event_emitter = mock.Mock()\n        self.client.meta.events = self.event_emitter\n        self.stubber.activate()\n        self.stubber.deactivate()\n        self.event_emitter.unregister.assert_any_call(\n            'before-parameter-build.*.*', mock.ANY, unique_id=mock.ANY\n        )\n        self.event_emitter.unregister.assert_any_call(\n            'before-call.*.*', mock.ANY, unique_id=mock.ANY\n        )\n\n    def test_context_manager(self):\n        self.event_emitter = mock.Mock()\n        self.client.meta.events = self.event_emitter\n\n        with self.stubber:\n            # Ensure events are registered in context\n            self.event_emitter.register_first.assert_called_with(\n                'before-parameter-build.*.*', mock.ANY, unique_id=mock.ANY\n            )\n            self.event_emitter.register.assert_called_with(\n                'before-call.*.*', mock.ANY, unique_id=mock.ANY\n            )\n\n        # Ensure events are no longer registered once we leave the context\n        self.event_emitter.unregister.assert_any_call(\n            'before-parameter-build.*.*', mock.ANY, unique_id=mock.ANY\n        )\n        self.event_emitter.unregister.assert_any_call(\n            'before-call.*.*', mock.ANY, unique_id=mock.ANY\n        )\n\n    def test_add_response(self):\n        response = {'foo': 'bar'}\n        self.stubber.add_response('foo', response)\n\n        with self.assertRaises(AssertionError):\n            self.stubber.assert_no_pending_responses()\n\n    def test_add_response_fails_when_missing_client_method(self):\n        del self.client.foo\n        with self.assertRaises(ValueError):\n            self.stubber.add_response('foo', {})\n\n    def test_validates_service_response(self):\n        self.stubber.add_response('foo', {})\n        self.assertTrue(self.validate_parameters_mock.called)\n\n    def test_validate_ignores_response_metadata(self):\n        service_response = {'ResponseMetadata': {'foo': 'bar'}}\n        service_model = ServiceModel(\n            {\n                'documentation': '',\n                'operations': {\n                    'foo': {\n                        'name': 'foo',\n                        'input': {'shape': 'StringShape'},\n                        'output': {'shape': 'StringShape'},\n                    }\n                },\n                'shapes': {'StringShape': {'type': 'string'}},\n            }\n        )\n        op_name = service_model.operation_names[0]\n        output_shape = service_model.operation_model(op_name).output_shape\n\n        self.client.meta.service_model = service_model\n        self.stubber.add_response('TestOperation', service_response)\n        self.validate_parameters_mock.assert_called_with({}, output_shape)\n\n        # Make sure service response hasn't been mutated\n        self.assertEqual(\n            service_response, {'ResponseMetadata': {'foo': 'bar'}}\n        )\n\n    def test_validates_on_empty_output_shape(self):\n        service_model = ServiceModel(\n            {'documentation': '', 'operations': {'foo': {'name': 'foo'}}}\n        )\n        self.client.meta.service_model = service_model\n\n        with self.assertRaises(ParamValidationError):\n            self.stubber.add_response('TestOperation', {'foo': 'bar'})\n\n    def test_get_response(self):\n        service_response = {'bar': 'baz'}\n        self.stubber.add_response('foo', service_response)\n        self.stubber.activate()\n        response = self.emit_get_response_event()\n        self.assertEqual(response[1], service_response)\n        self.assertEqual(response[0].status_code, 200)\n\n    def test_get_client_error_response(self):\n        error_code = \"foo\"\n        service_message = \"bar\"\n        self.stubber.add_client_error('foo', error_code, service_message)\n        self.stubber.activate()\n        response = self.emit_get_response_event()\n        self.assertEqual(response[1]['Error']['Message'], service_message)\n        self.assertEqual(response[1]['Error']['Code'], error_code)\n\n    def test_get_client_error_with_extra_error_meta(self):\n        error_code = \"foo\"\n        error_message = \"bar\"\n        error_meta = {\n            \"Endpoint\": \"https://foo.bar.baz\",\n        }\n        self.stubber.add_client_error(\n            'foo',\n            error_code,\n            error_message,\n            http_status_code=301,\n            service_error_meta=error_meta,\n        )\n        with self.stubber:\n            response = self.emit_get_response_event()\n        error = response[1]['Error']\n        self.assertIn('Endpoint', error)\n        self.assertEqual(error['Endpoint'], \"https://foo.bar.baz\")\n\n    def test_get_client_error_with_extra_response_meta(self):\n        error_code = \"foo\"\n        error_message = \"bar\"\n        stub_response_meta = {\n            \"RequestId\": \"79104EXAMPLEB723\",\n        }\n        self.stubber.add_client_error(\n            'foo',\n            error_code,\n            error_message,\n            http_status_code=301,\n            response_meta=stub_response_meta,\n        )\n        with self.stubber:\n            response = self.emit_get_response_event()\n        actual_response_meta = response[1]['ResponseMetadata']\n        self.assertIn('RequestId', actual_response_meta)\n        self.assertEqual(actual_response_meta['RequestId'], \"79104EXAMPLEB723\")\n\n    def test_get_client_error_with_modeled_fields(self):\n        error_code = 'foo'\n        error_message = 'bar'\n        modeled_fields = {'Extra': 'foobar'}\n        self.stubber.add_client_error(\n            'foo',\n            error_code,\n            error_message,\n            http_status_code=301,\n            modeled_fields=modeled_fields,\n        )\n        with self.stubber:\n            response = self.emit_get_response_event()\n        self.assertIn('Extra', response[1])\n        self.assertEqual(response[1]['Extra'], 'foobar')\n\n    def test_get_response_errors_with_no_stubs(self):\n        self.stubber.activate()\n        with self.assertRaises(UnStubbedResponseError):\n            self.emit_get_response_event()\n\n    def test_assert_no_responses_remaining(self):\n        self.stubber.add_response('foo', {})\n        with self.assertRaises(AssertionError):\n            self.stubber.assert_no_pending_responses()\n", "tests/unit/test_compat.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\n\nimport pytest\n\nfrom botocore.compat import (\n    HAS_CRT,\n    compat_shell_split,\n    ensure_bytes,\n    get_md5,\n    get_tzinfo_options,\n    total_seconds,\n    unquote_str,\n)\nfrom botocore.exceptions import MD5UnavailableError\nfrom tests import BaseEnvVar, mock, unittest\n\n\nclass TotalSecondsTest(BaseEnvVar):\n    def test_total_seconds(self):\n        delta = datetime.timedelta(days=1, seconds=45)\n        remaining = total_seconds(delta)\n        self.assertEqual(remaining, 86445.0)\n\n        delta = datetime.timedelta(seconds=33, microseconds=772)\n        remaining = total_seconds(delta)\n        self.assertEqual(remaining, 33.000772)\n\n\nclass TestUnquoteStr(unittest.TestCase):\n    def test_unquote_str(self):\n        value = '%E2%9C%93'\n        # Note: decoded to unicode and utf-8 decoded as well.\n        # This would work in python2 and python3.\n        self.assertEqual(unquote_str(value), '\\u2713')\n\n    def test_unquote_normal(self):\n        value = 'foo'\n        # Note: decoded to unicode and utf-8 decoded as well.\n        # This would work in python2 and python3.\n        self.assertEqual(unquote_str(value), 'foo')\n\n    def test_unquote_with_spaces(self):\n        value = 'foo+bar'\n        # Note: decoded to unicode and utf-8 decoded as well.\n        # This would work in python2 and python3.\n        self.assertEqual(unquote_str(value), 'foo bar')\n\n\nclass TestEnsureBytes(unittest.TestCase):\n    def test_string(self):\n        value = 'foo'\n        response = ensure_bytes(value)\n        self.assertIsInstance(response, bytes)\n        self.assertEqual(response, b'foo')\n\n    def test_binary(self):\n        value = b'bar'\n        response = ensure_bytes(value)\n        self.assertIsInstance(response, bytes)\n        self.assertEqual(response, b'bar')\n\n    def test_unicode(self):\n        value = 'baz'\n        response = ensure_bytes(value)\n        self.assertIsInstance(response, bytes)\n        self.assertEqual(response, b'baz')\n\n    def test_non_ascii(self):\n        value = '\\u2713'\n        response = ensure_bytes(value)\n        self.assertIsInstance(response, bytes)\n        self.assertEqual(response, b'\\xe2\\x9c\\x93')\n\n    def test_non_string_or_bytes_raises_error(self):\n        value = 500\n        with self.assertRaises(ValueError):\n            ensure_bytes(value)\n\n\nclass TestGetMD5(unittest.TestCase):\n    def test_available(self):\n        md5 = mock.Mock()\n        with mock.patch('botocore.compat.MD5_AVAILABLE', True):\n            with mock.patch('hashlib.md5', mock.Mock(return_value=md5)):\n                self.assertEqual(get_md5(), md5)\n\n    def test_unavailable_raises_error(self):\n        with mock.patch('botocore.compat.MD5_AVAILABLE', False):\n            with self.assertRaises(MD5UnavailableError):\n                get_md5()\n\n\n@pytest.fixture\ndef shell_split_runner():\n    # Single runner fixture for all tests\n    return ShellSplitTestRunner()\n\n\ndef get_windows_test_cases():\n    windows_cases = {\n        r'': [],\n        r'spam \\\\': [r'spam', '\\\\\\\\'],\n        r'spam ': [r'spam'],\n        r' spam': [r'spam'],\n        'spam eggs': [r'spam', r'eggs'],\n        'spam\\teggs': [r'spam', r'eggs'],\n        'spam\\neggs': ['spam\\neggs'],\n        '\"\"': [''],\n        '\" \"': [' '],\n        '\"\\t\"': ['\\t'],\n        '\\\\\\\\': ['\\\\\\\\'],\n        '\\\\\\\\ ': ['\\\\\\\\'],\n        '\\\\\\\\\\t': ['\\\\\\\\'],\n        r'\\\"': ['\"'],\n        # The following four test cases are official test cases given in\n        # Microsoft's documentation.\n        r'\"abc\" d e': [r'abc', r'd', r'e'],\n        r'a\\\\b d\"e f\"g h': [r'a\\\\b', r'de fg', r'h'],\n        r'a\\\\\\\"b c d': [r'a\\\"b', r'c', r'd'],\n        r'a\\\\\\\\\"b c\" d e': [r'a\\\\b c', r'd', r'e'],\n    }\n    return windows_cases.items()\n\n\n@pytest.mark.parametrize(\n    \"input_string, expected_output\", get_windows_test_cases()\n)\ndef test_compat_shell_split_windows(\n    shell_split_runner, input_string, expected_output\n):\n    shell_split_runner.assert_equal(input_string, expected_output, \"win32\")\n\n\ndef test_compat_shell_split_windows_raises_error(shell_split_runner):\n    shell_split_runner.assert_raises(r'\"', ValueError, \"win32\")\n\n\ndef get_unix_test_cases():\n    unix_cases = {\n        r'': [],\n        r'spam \\\\': [r'spam', '\\\\'],\n        r'spam ': [r'spam'],\n        r' spam': [r'spam'],\n        'spam eggs': [r'spam', r'eggs'],\n        'spam\\teggs': [r'spam', r'eggs'],\n        'spam\\neggs': ['spam', 'eggs'],\n        '\"\"': [''],\n        '\" \"': [' '],\n        '\"\\t\"': ['\\t'],\n        '\\\\\\\\': ['\\\\'],\n        '\\\\\\\\ ': ['\\\\'],\n        '\\\\\\\\\\t': ['\\\\'],\n        r'\\\"': ['\"'],\n        # The following four test cases are official test cases given in\n        # Microsoft's documentation, but adapted to unix shell splitting.\n        r'\"abc\" d e': [r'abc', r'd', r'e'],\n        r'a\\\\b d\"e f\"g h': [r'a\\b', r'de fg', r'h'],\n        r'a\\\\\\\"b c d': [r'a\\\"b', r'c', r'd'],\n        r'a\\\\\\\\\"b c\" d e': [r'a\\\\b c', r'd', r'e'],\n    }\n    return unix_cases.items()\n\n\n@pytest.mark.parametrize(\n    \"input_string, expected_output\", get_unix_test_cases()\n)\ndef test_compat_shell_split_unix_linux2(\n    shell_split_runner, input_string, expected_output\n):\n    shell_split_runner.assert_equal(input_string, expected_output, \"linux2\")\n\n\n@pytest.mark.parametrize(\n    \"input_string, expected_output\", get_unix_test_cases()\n)\ndef test_compat_shell_split_unix_darwin(\n    shell_split_runner, input_string, expected_output\n):\n    shell_split_runner.assert_equal(input_string, expected_output, \"darwin\")\n\n\ndef test_compat_shell_split_unix_linux2_raises_error(shell_split_runner):\n    shell_split_runner.assert_raises(r'\"', ValueError, \"linux2\")\n\n\ndef test_compat_shell_split_unix_darwin_raises_error(shell_split_runner):\n    shell_split_runner.assert_raises(r'\"', ValueError, \"darwin\")\n\n\nclass ShellSplitTestRunner:\n    def assert_equal(self, s, expected, platform):\n        assert compat_shell_split(s, platform) == expected\n\n    def assert_raises(self, s, exception_cls, platform):\n        with pytest.raises(exception_cls):\n            compat_shell_split(s, platform)\n\n\nclass TestTimezoneOperations(unittest.TestCase):\n    def test_get_tzinfo_options(self):\n        options = get_tzinfo_options()\n        self.assertTrue(len(options) > 0)\n\n        for tzinfo in options:\n            self.assertIsInstance(tzinfo(), datetime.tzinfo)\n\n\nclass TestCRTIntegration(unittest.TestCase):\n    def test_has_crt_global(self):\n        try:\n            import awscrt.auth  # noqa\n\n            assert HAS_CRT\n        except ImportError:\n            assert not HAS_CRT\n", "tests/unit/test_hooks.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport functools\nfrom functools import partial\n\nfrom botocore.hooks import (\n    EventAliaser,\n    HierarchicalEmitter,\n    first_non_none_response,\n)\nfrom tests import unittest\n\n\nclass TestHierarchicalEventEmitter(unittest.TestCase):\n    def setUp(self):\n        self.emitter = HierarchicalEmitter()\n        self.hook_calls = []\n\n    def hook(self, **kwargs):\n        self.hook_calls.append(kwargs)\n\n    def test_non_dot_behavior(self):\n        self.emitter.register('no-dot', self.hook)\n        self.emitter.emit('no-dot')\n        self.assertEqual(len(self.hook_calls), 1)\n\n    def test_with_dots(self):\n        self.emitter.register('foo.bar.baz', self.hook)\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(len(self.hook_calls), 1)\n\n    def test_catch_all_hook(self):\n        self.emitter.register('foo', self.hook)\n        self.emitter.register('foo.bar', self.hook)\n        self.emitter.register('foo.bar.baz', self.hook)\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(len(self.hook_calls), 3, self.hook_calls)\n        # The hook is called with the same event name three times.\n        self.assertEqual(\n            [e['event_name'] for e in self.hook_calls],\n            ['foo.bar.baz', 'foo.bar.baz', 'foo.bar.baz'],\n        )\n\n    def test_hook_called_in_proper_order(self):\n        # We should call the hooks from most specific to least\n        # specific.\n        calls = []\n        self.emitter.register('foo', lambda **kwargs: calls.append('foo'))\n        self.emitter.register(\n            'foo.bar', lambda **kwargs: calls.append('foo.bar')\n        )\n        self.emitter.register(\n            'foo.bar.baz', lambda **kwargs: calls.append('foo.bar.baz')\n        )\n\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(calls, ['foo.bar.baz', 'foo.bar', 'foo'])\n\n\nclass TestAliasedEmitter(unittest.TestCase):\n    def setUp(self):\n        self.hook_calls = []\n\n    def hook(self, **kwargs):\n        self.hook_calls.append(kwargs)\n\n    def get_emitter(self, event_aliases):\n        emitter = HierarchicalEmitter()\n        return EventAliaser(emitter, event_aliases)\n\n    def test_event_emitted(self):\n        aliases = {'bar': 'bear'}\n        emitter = self.get_emitter(event_aliases=aliases)\n        emitter.register('foo.bear.baz', self.hook)\n        emitter.emit('foo.bear.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, ['foo.bear.baz'])\n\n    def test_aliased_event_emitted(self):\n        aliases = {'bar': 'bear'}\n        emitter = self.get_emitter(event_aliases=aliases)\n        emitter.register('foo.bear.baz', self.hook)\n        emitter.emit('foo.bar.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, ['foo.bear.baz'])\n\n    def test_alias_with_dots_emitted(self):\n        aliases = {'api.bar': 'bear'}\n        emitter = self.get_emitter(event_aliases=aliases)\n        emitter.register('foo.bear.baz', self.hook)\n        emitter.emit('foo.api.bar.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, ['foo.bear.baz'])\n\n    def test_aliased_event_registered(self):\n        aliases = {'bar': 'bear'}\n        emitter = self.get_emitter(event_aliases=aliases)\n        emitter.register('foo.bar.baz', self.hook)\n        emitter.emit('foo.bear.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, ['foo.bear.baz'])\n\n    def test_aliased_event_with_dots_registered(self):\n        aliases = {'api.bar': 'bear'}\n        emitter = self.get_emitter(event_aliases=aliases)\n        emitter.register('foo.api.bar.baz', self.hook)\n        emitter.emit('foo.bear.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, ['foo.bear.baz'])\n\n    def test_event_unregistered(self):\n        aliases = {'bar': 'bear'}\n        emitter = self.get_emitter(event_aliases=aliases)\n\n        emitter.register('foo.bar.baz', self.hook)\n        emitter.emit('foo.bear.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, ['foo.bear.baz'])\n\n        self.hook_calls = []\n        emitter.unregister('foo.bear.baz', self.hook)\n        emitter.emit('foo.bear.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, [])\n\n    def test_aliased_event_unregistered(self):\n        aliases = {'bar': 'bear'}\n        emitter = self.get_emitter(event_aliases=aliases)\n\n        emitter.register('foo.bar.baz', self.hook)\n        emitter.emit('foo.bear.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, ['foo.bear.baz'])\n\n        self.hook_calls = []\n        emitter.unregister('foo.bar.baz', self.hook)\n        emitter.emit('foo.bear.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, [])\n\n    def test_aliased_event_with_dots_unregistered(self):\n        aliases = {'api.bar': 'bear'}\n        emitter = self.get_emitter(event_aliases=aliases)\n\n        emitter.register('foo.api.bar.baz', self.hook)\n        emitter.emit('foo.bear.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, ['foo.bear.baz'])\n\n        self.hook_calls = []\n        emitter.unregister('foo.api.bar.baz', self.hook)\n        emitter.emit('foo.bear.baz')\n        calls = [e['event_name'] for e in self.hook_calls]\n        self.assertEqual(calls, [])\n\n\nclass TestStopProcessing(unittest.TestCase):\n    def setUp(self):\n        self.emitter = HierarchicalEmitter()\n        self.hook_calls = []\n\n    def hook1(self, **kwargs):\n        self.hook_calls.append('hook1')\n\n    def hook2(self, **kwargs):\n        self.hook_calls.append('hook2')\n        return 'hook2-response'\n\n    def hook3(self, **kwargs):\n        self.hook_calls.append('hook3')\n        return 'hook3-response'\n\n    def test_all_hooks(self):\n        # Here we register three hooks and sanity check\n        # that all three would be called by a normal emit.\n        # This ensures our hook calls are setup properly for\n        # later tests.\n        self.emitter.register('foo', self.hook1)\n        self.emitter.register('foo', self.hook2)\n        self.emitter.register('foo', self.hook3)\n        self.emitter.emit('foo')\n\n        self.assertEqual(self.hook_calls, ['hook1', 'hook2', 'hook3'])\n\n    def test_stop_processing_after_first_response(self):\n        # Here we register three hooks, but only the first\n        # two should ever execute.\n        self.emitter.register('foo', self.hook1)\n        self.emitter.register('foo', self.hook2)\n        self.emitter.register('foo', self.hook3)\n        handler, response = self.emitter.emit_until_response('foo')\n\n        self.assertEqual(response, 'hook2-response')\n        self.assertEqual(self.hook_calls, ['hook1', 'hook2'])\n\n    def test_no_responses(self):\n        # Here we register a handler that will not return a response\n        # and ensure we get back proper values.\n        self.emitter.register('foo', self.hook1)\n        responses = self.emitter.emit('foo')\n\n        self.assertEqual(self.hook_calls, ['hook1'])\n        self.assertEqual(responses, [(self.hook1, None)])\n\n    def test_no_handlers(self):\n        # Here we have no handlers, but still expect a tuple of return\n        # values.\n        handler, response = self.emitter.emit_until_response('foo')\n\n        self.assertIsNone(handler)\n        self.assertIsNone(response)\n\n\nclass TestFirstNonNoneResponse(unittest.TestCase):\n    def test_all_none(self):\n        self.assertIsNone(first_non_none_response([]))\n\n    def test_first_non_none(self):\n        correct_value = 'correct_value'\n        wrong_value = 'wrong_value'\n        # The responses are tuples of (handler, response),\n        # and we don't care about the handler so we just use a value of\n        # None.\n        responses = [(None, None), (None, correct_value), (None, wrong_value)]\n        self.assertEqual(first_non_none_response(responses), correct_value)\n\n    def test_default_value_if_non_none_found(self):\n        responses = [(None, None), (None, None)]\n        # If no response is found and a default value is passed in, it will\n        # be returned.\n        self.assertEqual(\n            first_non_none_response(responses, default='notfound'), 'notfound'\n        )\n\n\nclass TestWildcardHandlers(unittest.TestCase):\n    def setUp(self):\n        self.emitter = HierarchicalEmitter()\n        self.hook_calls = []\n\n    def hook(self, **kwargs):\n        self.hook_calls.append(kwargs)\n\n    def register(self, event_name):\n        func = partial(self.hook, registered_with=event_name)\n        self.emitter.register(event_name, func)\n        return func\n\n    def assert_hook_is_called_given_event(self, event):\n        starting = len(self.hook_calls)\n        self.emitter.emit(event)\n        after = len(self.hook_calls)\n        if not after > starting:\n            self.fail(\"Handler was not called for event: %s\" % event)\n        self.assertEqual(self.hook_calls[-1]['event_name'], event)\n\n    def assert_hook_is_not_called_given_event(self, event):\n        starting = len(self.hook_calls)\n        self.emitter.emit(event)\n        after = len(self.hook_calls)\n        if not after == starting:\n            self.fail(\n                \"Handler was called for event but was not \"\n                \"suppose to be called: %s, last_event: %s\"\n                % (event, self.hook_calls[-1])\n            )\n\n    def test_one_level_wildcard_handler(self):\n        self.emitter.register('foo.*.baz', self.hook)\n        # Also register for a number of other events to check\n        # for false positives.\n        self.emitter.register('other.bar.baz', self.hook)\n        self.emitter.register('qqq.baz', self.hook)\n        self.emitter.register('dont.call.me', self.hook)\n        self.emitter.register('dont', self.hook)\n        # These calls should trigger our hook.\n        self.assert_hook_is_called_given_event('foo.bar.baz')\n        self.assert_hook_is_called_given_event('foo.qux.baz')\n        self.assert_hook_is_called_given_event('foo.anything.baz')\n\n        # These calls should not match our hook.\n        self.assert_hook_is_not_called_given_event('foo')\n        self.assert_hook_is_not_called_given_event('foo.bar')\n        self.assert_hook_is_not_called_given_event('bar.qux.baz')\n        self.assert_hook_is_not_called_given_event('foo-bar')\n\n    def test_hierarchical_wildcard_handler(self):\n        self.emitter.register('foo.*.baz', self.hook)\n        self.assert_hook_is_called_given_event('foo.bar.baz.qux')\n        self.assert_hook_is_called_given_event('foo.bar.baz.qux.foo')\n        self.assert_hook_is_called_given_event('foo.qux.baz.qux')\n        self.assert_hook_is_called_given_event('foo.qux.baz.qux.foo')\n\n        self.assert_hook_is_not_called_given_event('bar.qux.baz.foo')\n\n    def test_multiple_wildcard_events(self):\n        self.emitter.register('foo.*.*.baz', self.hook)\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz')\n        self.assert_hook_is_called_given_event('foo.ANY.THING.baz')\n        self.assert_hook_is_called_given_event('foo.AT.ALL.baz')\n\n        # More specific than what we registered for.\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz.extra')\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz.extra.stuff')\n\n        # Too short:\n        self.assert_hook_is_not_called_given_event('foo')\n        self.assert_hook_is_not_called_given_event('foo.bar')\n        self.assert_hook_is_not_called_given_event('foo.bar.baz')\n\n        # Bad ending segment.\n        self.assert_hook_is_not_called_given_event('foo.ANY.THING.notbaz')\n        self.assert_hook_is_not_called_given_event('foo.ANY.THING.stillnotbaz')\n\n    def test_can_unregister_for_wildcard_events(self):\n        self.emitter.register('foo.*.*.baz', self.hook)\n        # Call multiple times to verify caching behavior.\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz')\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz')\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz')\n\n        self.emitter.unregister('foo.*.*.baz', self.hook)\n        self.assert_hook_is_not_called_given_event('foo.bar.baz.baz')\n        self.assert_hook_is_not_called_given_event('foo.bar.baz.baz')\n        self.assert_hook_is_not_called_given_event('foo.bar.baz.baz')\n\n        self.emitter.register('foo.*.*.baz', self.hook)\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz')\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz')\n\n    def test_unregister_does_not_exist(self):\n        self.emitter.register('foo.*.*.baz', self.hook)\n        self.emitter.unregister('foo.*.*.baz', self.hook)\n        self.emitter.unregister('foo.*.*.baz', self.hook)\n        self.assert_hook_is_not_called_given_event('foo.bar.baz.baz')\n\n    def test_cache_cleared_properly(self):\n        self.emitter.register('foo.*.*.baz', self.hook)\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz')\n\n        self.emitter.register('foo.*.*.bar', self.hook)\n        self.assert_hook_is_called_given_event('foo.bar.baz.baz')\n        self.assert_hook_is_called_given_event('foo.bar.baz.bar')\n\n        self.emitter.unregister('foo.*.*.baz', self.hook)\n        self.assert_hook_is_called_given_event('foo.bar.baz.bar')\n        self.assert_hook_is_not_called_given_event('foo.bar.baz.baz')\n\n    def test_complicated_register_unregister(self):\n        r = self.emitter.register\n        u = partial(self.emitter.unregister, handler=self.hook)\n        r('foo.bar.baz.qux', self.hook)\n        r('foo.bar.baz', self.hook)\n        r('foo.bar', self.hook)\n        r('foo', self.hook)\n\n        u('foo.bar.baz')\n        u('foo')\n        u('foo.bar')\n\n        self.assert_hook_is_called_given_event('foo.bar.baz.qux')\n\n        self.assert_hook_is_not_called_given_event('foo.bar.baz')\n        self.assert_hook_is_not_called_given_event('foo.bar')\n        self.assert_hook_is_not_called_given_event('foo')\n\n    def test_register_multiple_handlers_for_same_event(self):\n        self.emitter.register('foo.bar.baz', self.hook)\n        self.emitter.register('foo.bar.baz', self.hook)\n\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(len(self.hook_calls), 2)\n\n    def test_register_with_unique_id(self):\n        self.emitter.register('foo.bar.baz', self.hook, unique_id='foo')\n        # Since we're using the same unique_id, this registration is ignored.\n        self.emitter.register('foo.bar.baz', self.hook, unique_id='foo')\n        # This also works across event names, so this registration is ignored\n        # as well.\n        self.emitter.register('foo.other', self.hook, unique_id='foo')\n\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(len(self.hook_calls), 1)\n\n        self.hook_calls = []\n\n        self.emitter.emit('foo.other')\n        self.assertEqual(len(self.hook_calls), 0)\n\n    def test_remove_handler_with_unique_id(self):\n        hook2 = lambda **kwargs: self.hook_calls.append(kwargs)\n        self.emitter.register('foo.bar.baz', self.hook, unique_id='foo')\n        self.emitter.register('foo.bar.baz', hook2)\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(len(self.hook_calls), 2)\n\n        # Reset the hook calls.\n        self.hook_calls = []\n\n        self.emitter.unregister('foo.bar.baz', hook2)\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(len(self.hook_calls), 1)\n\n        self.hook_calls = []\n\n        # Can provide the unique_id to unregister.\n        self.emitter.unregister('foo.bar.baz', unique_id='foo')\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(len(self.hook_calls), 0)\n\n        # Same as with not specifying a unique_id, you can call\n        # unregister multiple times and not get an exception.\n        self.emitter.unregister('foo.bar.baz', unique_id='foo')\n\n    def test_remove_handler_with_and_without_unique_id(self):\n        self.emitter.register('foo.bar.baz', self.hook, unique_id='foo')\n        self.emitter.register('foo.bar.baz', self.hook)\n\n        self.emitter.unregister('foo.bar.baz', self.hook)\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(len(self.hook_calls), 1)\n\n        self.hook_calls = []\n\n        self.emitter.unregister('foo.bar.baz', self.hook)\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(len(self.hook_calls), 0)\n\n    def test_register_with_uses_count_initially(self):\n        self.emitter.register(\n            'foo', self.hook, unique_id='foo', unique_id_uses_count=True\n        )\n        # Subsequent calls must set ``unique_id_uses_count`` to True.\n        with self.assertRaises(ValueError):\n            self.emitter.register('foo', self.hook, unique_id='foo')\n\n    def test_register_with_uses_count_not_initially(self):\n        self.emitter.register('foo', self.hook, unique_id='foo')\n        # Subsequent calls must set ``unique_id_uses_count`` to False.\n        with self.assertRaises(ValueError):\n            self.emitter.register(\n                'foo', self.hook, unique_id='foo', unique_id_uses_count=True\n            )\n\n    def test_register_with_uses_count_unregister(self):\n        self.emitter.register(\n            'foo', self.hook, unique_id='foo', unique_id_uses_count=True\n        )\n        self.emitter.register(\n            'foo', self.hook, unique_id='foo', unique_id_uses_count=True\n        )\n        # Event was registered to use a count so it must be specified\n        # that a count is used when unregistering\n        with self.assertRaises(ValueError):\n            self.emitter.unregister('foo', self.hook, unique_id='foo')\n        # Event should not have been unregistered.\n        self.emitter.emit('foo')\n        self.assertEqual(len(self.hook_calls), 1)\n        self.emitter.unregister(\n            'foo', self.hook, unique_id='foo', unique_id_uses_count=True\n        )\n        # Event still should not be unregistered.\n        self.hook_calls = []\n        self.emitter.emit('foo')\n        self.assertEqual(len(self.hook_calls), 1)\n        self.emitter.unregister(\n            'foo', self.hook, unique_id='foo', unique_id_uses_count=True\n        )\n        # Now the event should be unregistered.\n        self.hook_calls = []\n        self.emitter.emit('foo')\n        self.assertEqual(len(self.hook_calls), 0)\n\n    def test_register_with_no_uses_count_unregister(self):\n        self.emitter.register('foo', self.hook, unique_id='foo')\n        # The event was not registered to use a count initially\n        with self.assertRaises(ValueError):\n            self.emitter.unregister(\n                'foo', self.hook, unique_id='foo', unique_id_uses_count=True\n            )\n\n    def test_handlers_called_in_order(self):\n        def handler(call_number, **kwargs):\n            kwargs['call_number'] = call_number\n            self.hook_calls.append(kwargs)\n\n        self.emitter.register('foo', partial(handler, call_number=1))\n        self.emitter.register('foo', partial(handler, call_number=2))\n        self.emitter.emit('foo')\n        self.assertEqual([k['call_number'] for k in self.hook_calls], [1, 2])\n\n    def test_handler_call_order_with_hierarchy(self):\n        def handler(call_number, **kwargs):\n            kwargs['call_number'] = call_number\n            self.hook_calls.append(kwargs)\n\n        # We go from most specific to least specific, and each level is called\n        # in the order they were registered for that particular hierarchy\n        # level.\n        self.emitter.register('foo.bar.baz', partial(handler, call_number=1))\n        self.emitter.register('foo.bar', partial(handler, call_number=3))\n        self.emitter.register('foo', partial(handler, call_number=5))\n        self.emitter.register('foo.bar.baz', partial(handler, call_number=2))\n        self.emitter.register('foo.bar', partial(handler, call_number=4))\n        self.emitter.register('foo', partial(handler, call_number=6))\n\n        self.emitter.emit('foo.bar.baz')\n        self.assertEqual(\n            [k['call_number'] for k in self.hook_calls], [1, 2, 3, 4, 5, 6]\n        )\n\n    def test_register_first_single_level(self):\n        def handler(call_number, **kwargs):\n            kwargs['call_number'] = call_number\n            self.hook_calls.append(kwargs)\n\n        # Handlers registered through register_first() are always called\n        # before handlers registered with register().\n        self.emitter.register('foo', partial(handler, call_number=3))\n        self.emitter.register('foo', partial(handler, call_number=4))\n        self.emitter.register_first('foo', partial(handler, call_number=1))\n        self.emitter.register_first('foo', partial(handler, call_number=2))\n        self.emitter.register('foo', partial(handler, call_number=5))\n\n        self.emitter.emit('foo')\n        self.assertEqual(\n            [k['call_number'] for k in self.hook_calls], [1, 2, 3, 4, 5]\n        )\n\n    def test_register_first_hierarchy(self):\n        def handler(call_number, **kwargs):\n            kwargs['call_number'] = call_number\n            self.hook_calls.append(kwargs)\n\n        self.emitter.register('foo', partial(handler, call_number=5))\n        self.emitter.register('foo.bar', partial(handler, call_number=2))\n\n        self.emitter.register_first('foo', partial(handler, call_number=4))\n        self.emitter.register_first('foo.bar', partial(handler, call_number=1))\n\n        self.emitter.register('foo', partial(handler, call_number=6))\n        self.emitter.register('foo.bar', partial(handler, call_number=3))\n\n        self.emitter.emit('foo.bar')\n        self.assertEqual(\n            [k['call_number'] for k in self.hook_calls], [1, 2, 3, 4, 5, 6]\n        )\n\n    def test_register_last_hierarchy(self):\n        def handler(call_number, **kwargs):\n            kwargs['call_number'] = call_number\n            self.hook_calls.append(kwargs)\n\n        self.emitter.register_last('foo', partial(handler, call_number=3))\n        self.emitter.register('foo', partial(handler, call_number=2))\n        self.emitter.register_first('foo', partial(handler, call_number=1))\n        self.emitter.emit('foo')\n        self.assertEqual(\n            [k['call_number'] for k in self.hook_calls], [1, 2, 3]\n        )\n\n    def test_register_unregister_first_last(self):\n        self.emitter.register('foo', self.hook)\n        self.emitter.register_last('foo.bar', self.hook)\n        self.emitter.register_first('foo.bar.baz', self.hook)\n\n        self.emitter.unregister('foo.bar.baz', self.hook)\n        self.emitter.unregister('foo.bar', self.hook)\n        self.emitter.unregister('foo', self.hook)\n\n        self.emitter.emit('foo')\n        self.assertEqual(self.hook_calls, [])\n\n    def test_copy_emitter(self):\n        # Here we're not testing copy directly, we're testing\n        # the observable behavior from copying an event emitter.\n        first = []\n\n        def first_handler(id_name, **kwargs):\n            first.append(id_name)\n\n        second = []\n\n        def second_handler(id_name, **kwargs):\n            second.append(id_name)\n\n        self.emitter.register('foo.bar.baz', first_handler)\n        # First time we emit, only the first handler should be called.\n        self.emitter.emit('foo.bar.baz', id_name='first-time')\n        self.assertEqual(first, ['first-time'])\n        self.assertEqual(second, [])\n\n        copied_emitter = copy.copy(self.emitter)\n        # If we emit from the copied emitter, we should still\n        # only see the first handler called.\n        copied_emitter.emit('foo.bar.baz', id_name='second-time')\n        self.assertEqual(first, ['first-time', 'second-time'])\n        self.assertEqual(second, [])\n\n        # However, if we register an event handler with the copied\n        # emitter, the first emitter will not see this.\n        copied_emitter.register('foo.bar.baz', second_handler)\n\n        copied_emitter.emit('foo.bar.baz', id_name='third-time')\n        self.assertEqual(first, ['first-time', 'second-time', 'third-time'])\n        # And now the second handler is called.\n        self.assertEqual(second, ['third-time'])\n\n        # And vice-versa, emitting from the original emitter\n        # will not trigger the second_handler.\n        # We'll double check this by unregistering/re-registering\n        # the event handler.\n        self.emitter.unregister('foo.bar.baz', first_handler)\n        self.emitter.register('foo.bar.baz', first_handler)\n        self.emitter.emit('foo.bar.baz', id_name='last-time')\n        self.assertEqual(second, ['third-time'])\n\n    def test_copy_emitter_with_unique_id_event(self):\n        # Here we're not testing copy directly, we're testing\n        # the observable behavior from copying an event emitter.\n        first = []\n\n        def first_handler(id_name, **kwargs):\n            first.append(id_name)\n\n        second = []\n\n        def second_handler(id_name, **kwargs):\n            second.append(id_name)\n\n        self.emitter.register('foo', first_handler, 'bar')\n        self.emitter.emit('foo', id_name='first-time')\n        self.assertEqual(first, ['first-time'])\n        self.assertEqual(second, [])\n\n        copied_emitter = copy.copy(self.emitter)\n\n        # If we register an event handler with the copied\n        # emitter, the event should not get registered again\n        # because the unique id was already used.\n        copied_emitter.register('foo', second_handler, 'bar')\n        copied_emitter.emit('foo', id_name='second-time')\n        self.assertEqual(first, ['first-time', 'second-time'])\n        self.assertEqual(second, [])\n\n        # If we unregister the first event from the copied emitter,\n        # We should be able to register the second handler.\n        copied_emitter.unregister('foo', first_handler, 'bar')\n        copied_emitter.register('foo', second_handler, 'bar')\n        copied_emitter.emit('foo', id_name='third-time')\n        self.assertEqual(first, ['first-time', 'second-time'])\n        self.assertEqual(second, ['third-time'])\n\n        # The original event emitter should have the unique id event still\n        # registered though.\n        self.emitter.emit('foo', id_name='fourth-time')\n        self.assertEqual(first, ['first-time', 'second-time', 'fourth-time'])\n        self.assertEqual(second, ['third-time'])\n\n    def test_copy_events_with_partials(self):\n        # There's a bug in python2.6 where you can't deepcopy\n        # a partial object.  We want to ensure that doesn't\n        # break when a partial is hooked up as an event handler.\n        def handler(a, b, **kwargs):\n            return b\n\n        f = functools.partial(handler, 1)\n        self.emitter.register('a.b', f)\n        copied = copy.copy(self.emitter)\n        self.assertEqual(\n            copied.emit_until_response('a.b', b='return-val')[1], 'return-val'\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/unit/test_handlers.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport base64\nimport copy\nimport io\nimport json\nimport os\n\nimport pytest\n\nimport botocore\nimport botocore.session\nfrom botocore import handlers\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.compat import OrderedDict, quote\nfrom botocore.config import Config\nfrom botocore.credentials import Credentials\nfrom botocore.docs.bcdoc.restdoc import DocumentStructure\nfrom botocore.docs.example import RequestExampleDocumenter\nfrom botocore.docs.params import RequestParamsDocumenter\nfrom botocore.exceptions import (\n    AliasConflictParameterError,\n    MD5UnavailableError,\n    ParamValidationError,\n)\nfrom botocore.hooks import HierarchicalEmitter\nfrom botocore.model import (\n    DenormalizedStructureBuilder,\n    OperationModel,\n    ServiceId,\n    ServiceModel,\n)\nfrom botocore.signers import RequestSigner\nfrom botocore.utils import conditionally_calculate_md5\nfrom tests import BaseSessionTest, mock, unittest\n\n\nclass TestHandlers(BaseSessionTest):\n    def test_get_console_output(self):\n        parsed = {'Output': base64.b64encode(b'foobar').decode('utf-8')}\n        handlers.decode_console_output(parsed)\n        self.assertEqual(parsed['Output'], 'foobar')\n\n    def test_get_console_output_cant_be_decoded(self):\n        parsed = {'Output': 1}\n        handlers.decode_console_output(parsed)\n        self.assertEqual(parsed['Output'], 1)\n\n    def test_get_console_output_bad_unicode_errors(self):\n        original = base64.b64encode(b'before\\xffafter').decode('utf-8')\n        parsed = {'Output': original}\n        handlers.decode_console_output(parsed)\n        self.assertEqual(parsed['Output'], 'before\\ufffdafter')\n\n    def test_noop_if_output_key_does_not_exist(self):\n        original = {'foo': 'bar'}\n        parsed = original.copy()\n\n        handlers.decode_console_output(parsed)\n        # Should be unchanged because the 'Output'\n        # key is not in the output.\n        self.assertEqual(parsed, original)\n\n    def test_decode_quoted_jsondoc(self):\n        value = quote('{\"foo\":\"bar\"}')\n        converted_value = handlers.decode_quoted_jsondoc(value)\n        self.assertEqual(converted_value, {'foo': 'bar'})\n\n    def test_cant_decode_quoted_jsondoc(self):\n        value = quote('{\"foo\": \"missing end quote}')\n        converted_value = handlers.decode_quoted_jsondoc(value)\n        self.assertEqual(converted_value, value)\n\n    def test_disable_signing(self):\n        self.assertEqual(handlers.disable_signing(), botocore.UNSIGNED)\n\n    def test_only_quote_url_path_not_version_id(self):\n        params = {'CopySource': '/foo/bar++baz?versionId=123'}\n        handlers.handle_copy_source_param(params)\n        self.assertEqual(\n            params['CopySource'], '/foo/bar%2B%2Bbaz?versionId=123'\n        )\n\n    def test_only_version_id_is_special_cased(self):\n        params = {'CopySource': '/foo/bar++baz?notVersion=foo+'}\n        handlers.handle_copy_source_param(params)\n        self.assertEqual(\n            params['CopySource'], '/foo/bar%2B%2Bbaz%3FnotVersion%3Dfoo%2B'\n        )\n\n    def test_copy_source_with_multiple_questions(self):\n        params = {'CopySource': '/foo/bar+baz?a=baz+?versionId=a+'}\n        handlers.handle_copy_source_param(params)\n        self.assertEqual(\n            params['CopySource'], '/foo/bar%2Bbaz%3Fa%3Dbaz%2B?versionId=a+'\n        )\n\n    def test_copy_source_supports_dict(self):\n        params = {'CopySource': {'Bucket': 'foo', 'Key': 'keyname+'}}\n        handlers.handle_copy_source_param(params)\n        self.assertEqual(params['CopySource'], 'foo/keyname%2B')\n\n    def test_copy_source_ignored_if_not_dict(self):\n        params = {'CopySource': 'stringvalue'}\n        handlers.handle_copy_source_param(params)\n        self.assertEqual(params['CopySource'], 'stringvalue')\n\n    def test_copy_source_supports_optional_version_id(self):\n        params = {\n            'CopySource': {\n                'Bucket': 'foo',\n                'Key': 'keyname+',\n                'VersionId': 'asdf+',\n            }\n        }\n        handlers.handle_copy_source_param(params)\n        self.assertEqual(\n            params['CopySource'],\n            # Note, versionId is not url encoded.\n            'foo/keyname%2B?versionId=asdf+',\n        )\n\n    def test_copy_source_has_validation_failure(self):\n        with self.assertRaisesRegex(ParamValidationError, 'Key'):\n            handlers.handle_copy_source_param(\n                {'CopySource': {'Bucket': 'foo'}}\n            )\n\n    def test_quote_source_header_needs_no_changes(self):\n        params = {'CopySource': '/foo/bar?versionId=123'}\n        handlers.handle_copy_source_param(params)\n        self.assertEqual(params['CopySource'], '/foo/bar?versionId=123')\n\n    def test_presigned_url_already_present_ec2(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopySnapshot'\n        params = {'body': {'PresignedUrl': 'https://foo'}}\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('ec2'),\n            'us-east-1',\n            'ec2',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        handlers.inject_presigned_url_ec2(\n            params, request_signer, operation_model\n        )\n        self.assertEqual(params['body']['PresignedUrl'], 'https://foo')\n\n    def test_presigned_url_with_source_region_ec2(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopySnapshot'\n        params = {\n            'body': {\n                'PresignedUrl': 'https://foo',\n                'SourceRegion': 'us-east-1',\n            }\n        }\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('ec2'),\n            'us-east-1',\n            'ec2',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        handlers.inject_presigned_url_ec2(\n            params, request_signer, operation_model\n        )\n        self.assertEqual(params['body']['PresignedUrl'], 'https://foo')\n        self.assertEqual(params['body']['SourceRegion'], 'us-east-1')\n\n    def test_presigned_url_already_present_rds(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopyDBSnapshot'\n        params = {'body': {'PreSignedUrl': 'https://foo'}}\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('rds'),\n            'us-east-1',\n            'rds',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        handlers.inject_presigned_url_rds(\n            params, request_signer, operation_model\n        )\n        self.assertEqual(params['body']['PreSignedUrl'], 'https://foo')\n\n    def test_presigned_url_with_source_region_rds(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopyDBSnapshot'\n        params = {\n            'body': {\n                'PreSignedUrl': 'https://foo',\n                'SourceRegion': 'us-east-1',\n            }\n        }\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('rds'),\n            'us-east-1',\n            'rds',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        handlers.inject_presigned_url_rds(\n            params, request_signer, operation_model\n        )\n        self.assertEqual(params['body']['PreSignedUrl'], 'https://foo')\n        self.assertNotIn('SourceRegion', params['body'])\n\n    def test_inject_presigned_url_ec2(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopySnapshot'\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('ec2'),\n            'us-east-1',\n            'ec2',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        request_dict = {}\n        params = {'SourceRegion': 'us-west-2'}\n        request_dict['body'] = params\n        request_dict['url'] = 'https://ec2.us-east-1.amazonaws.com'\n        request_dict['method'] = 'POST'\n        request_dict['headers'] = {}\n        request_dict['context'] = {}\n\n        handlers.inject_presigned_url_ec2(\n            request_dict, request_signer, operation_model\n        )\n\n        self.assertIn(\n            'https://ec2.us-west-2.amazonaws.com?', params['PresignedUrl']\n        )\n        self.assertIn('X-Amz-Signature', params['PresignedUrl'])\n        self.assertIn('DestinationRegion', params['PresignedUrl'])\n        # We should also populate the DestinationRegion with the\n        # region_name of the endpoint object.\n        self.assertEqual(params['DestinationRegion'], 'us-east-1')\n\n    def test_use_event_operation_name(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'FakeOperation'\n        request_signer = mock.Mock()\n        request_signer._region_name = 'us-east-1'\n        request_dict = {}\n        params = {'SourceRegion': 'us-west-2'}\n        request_dict['body'] = params\n        request_dict['url'] = 'https://myservice.us-east-1.amazonaws.com'\n        request_dict['method'] = 'POST'\n        request_dict['headers'] = {}\n        request_dict['context'] = {}\n\n        handlers.inject_presigned_url_ec2(\n            request_dict, request_signer, operation_model\n        )\n\n        call_args = request_signer.generate_presigned_url.call_args\n        operation_name = call_args[1].get('operation_name')\n        self.assertEqual(operation_name, 'FakeOperation')\n\n    def test_destination_region_always_changed(self):\n        # If the user provides a destination region, we will still\n        # override the DesinationRegion with the region_name from\n        # the endpoint object.\n        actual_region = 'us-west-1'\n        operation_model = mock.Mock()\n        operation_model.name = 'CopySnapshot'\n\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('ec2'),\n            actual_region,\n            'ec2',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        request_dict = {}\n        params = {\n            'SourceRegion': 'us-west-2',\n            'DestinationRegion': 'us-east-1',\n        }\n        request_dict['body'] = params\n        request_dict['url'] = 'https://ec2.us-west-1.amazonaws.com'\n        request_dict['method'] = 'POST'\n        request_dict['headers'] = {}\n        request_dict['context'] = {}\n\n        # The user provides us-east-1, but we will override this to\n        # endpoint.region_name, of 'us-west-1' in this case.\n        handlers.inject_presigned_url_ec2(\n            request_dict, request_signer, operation_model\n        )\n\n        self.assertIn(\n            'https://ec2.us-west-2.amazonaws.com?', params['PresignedUrl']\n        )\n\n        # Always use the DestinationRegion from the endpoint, regardless of\n        # whatever value the user provides.\n        self.assertEqual(params['DestinationRegion'], actual_region)\n\n    def test_inject_presigned_url_rds(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopyDBSnapshot'\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('rds'),\n            'us-east-1',\n            'rds',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        request_dict = {}\n        params = {'SourceRegion': 'us-west-2'}\n        request_dict['body'] = params\n        request_dict['url'] = 'https://rds.us-east-1.amazonaws.com'\n        request_dict['method'] = 'POST'\n        request_dict['headers'] = {}\n        request_dict['context'] = {}\n\n        handlers.inject_presigned_url_rds(\n            request_dict, request_signer, operation_model\n        )\n\n        self.assertIn(\n            'https://rds.us-west-2.amazonaws.com?', params['PreSignedUrl']\n        )\n        self.assertIn('X-Amz-Signature', params['PreSignedUrl'])\n        self.assertIn('DestinationRegion', params['PreSignedUrl'])\n        # We should not populate the destination region for rds\n        self.assertNotIn('DestinationRegion', params)\n\n    def test_source_region_removed(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopyDBSnapshot'\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('rds'),\n            'us-east-1',\n            'rds',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        request_dict = {}\n        params = {'SourceRegion': 'us-west-2'}\n        request_dict['body'] = params\n        request_dict['url'] = 'https://rds.us-east-1.amazonaws.com'\n        request_dict['method'] = 'POST'\n        request_dict['headers'] = {}\n        request_dict['context'] = {}\n\n        handlers.inject_presigned_url_rds(\n            params=request_dict,\n            request_signer=request_signer,\n            model=operation_model,\n        )\n\n        self.assertNotIn('SourceRegion', params)\n\n    def test_source_region_removed_when_presigned_url_provided_for_rds(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopyDBSnapshot'\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('rds'),\n            'us-east-1',\n            'rds',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        request_dict = {}\n        params = {'SourceRegion': 'us-west-2', 'PreSignedUrl': 'https://foo'}\n        request_dict['body'] = params\n        request_dict['url'] = 'https://rds.us-east-1.amazonaws.com'\n        request_dict['method'] = 'POST'\n        request_dict['headers'] = {}\n        request_dict['context'] = {}\n\n        handlers.inject_presigned_url_rds(\n            params=request_dict,\n            request_signer=request_signer,\n            model=operation_model,\n        )\n\n        self.assertNotIn('SourceRegion', params)\n\n    def test_dest_region_removed(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopyDBSnapshot'\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('rds'),\n            'us-east-1',\n            'rds',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        request_dict = {}\n        params = {'SourceRegion': 'us-west-2'}\n        request_dict['body'] = params\n        request_dict['url'] = 'https://rds.us-east-1.amazonaws.com'\n        request_dict['method'] = 'POST'\n        request_dict['headers'] = {}\n        request_dict['context'] = {}\n\n        handlers.inject_presigned_url_rds(\n            params=request_dict,\n            request_signer=request_signer,\n            model=operation_model,\n        )\n\n        self.assertNotIn('DestinationRegion', params)\n\n    def test_presigned_url_already_present_for_rds(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopyDBSnapshot'\n        params = {'body': {'PresignedUrl': 'https://foo'}}\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('rds'),\n            'us-east-1',\n            'rds',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        handlers.inject_presigned_url_rds(\n            params=params, request_signer=request_signer, model=operation_model\n        )\n        self.assertEqual(params['body']['PresignedUrl'], 'https://foo')\n\n    def test_presigned_url_casing_changed_for_rds(self):\n        operation_model = mock.Mock()\n        operation_model.name = 'CopyDBSnapshot'\n        credentials = Credentials('key', 'secret')\n        event_emitter = HierarchicalEmitter()\n        request_signer = RequestSigner(\n            ServiceId('rds'),\n            'us-east-1',\n            'rds',\n            'v4',\n            credentials,\n            event_emitter,\n        )\n        request_dict = {}\n        params = {'SourceRegion': 'us-west-2'}\n        request_dict['body'] = params\n        request_dict['url'] = 'https://rds.us-east-1.amazonaws.com'\n        request_dict['method'] = 'POST'\n        request_dict['headers'] = {}\n        request_dict['context'] = {}\n\n        handlers.inject_presigned_url_rds(\n            params=request_dict,\n            request_signer=request_signer,\n            model=operation_model,\n        )\n\n        self.assertNotIn('PresignedUrl', params)\n        self.assertIn(\n            'https://rds.us-west-2.amazonaws.com?', params['PreSignedUrl']\n        )\n        self.assertIn('X-Amz-Signature', params['PreSignedUrl'])\n\n    def test_500_status_code_set_for_200_response(self):\n        http_response = mock.Mock()\n        http_response.status_code = 200\n        http_response.content = \"\"\"\n            <Error>\n              <Code>AccessDenied</Code>\n              <Message>Access Denied</Message>\n              <RequestId>id</RequestId>\n              <HostId>hostid</HostId>\n            </Error>\n        \"\"\"\n        handlers.check_for_200_error((http_response, {}))\n        self.assertEqual(http_response.status_code, 500)\n\n    def test_200_response_with_no_error_left_untouched(self):\n        http_response = mock.Mock()\n        http_response.status_code = 200\n        http_response.content = \"<NotAnError></NotAnError>\"\n        handlers.check_for_200_error((http_response, {}))\n        # We don't touch the status code since there are no errors present.\n        self.assertEqual(http_response.status_code, 200)\n\n    def test_500_response_can_be_none(self):\n        # A 500 response can raise an exception, which means the response\n        # object is None.  We need to handle this case.\n        handlers.check_for_200_error(None)\n\n    def test_route53_resource_id(self):\n        event = 'before-parameter-build.route53.GetHostedZone'\n        params = {\n            'Id': '/hostedzone/ABC123',\n            'HostedZoneId': '/hostedzone/ABC123',\n            'ResourceId': '/hostedzone/DEF456',\n            'DelegationSetId': '/hostedzone/GHI789',\n            'ChangeId': '/hostedzone/JKL012',\n            'Other': '/hostedzone/foo',\n        }\n        operation_def = {\n            'name': 'GetHostedZone',\n            'input': {'shape': 'GetHostedZoneInput'},\n        }\n        service_def = {\n            'metadata': {},\n            'shapes': {\n                'GetHostedZoneInput': {\n                    'type': 'structure',\n                    'members': {\n                        'Id': {'shape': 'ResourceId'},\n                        'HostedZoneId': {'shape': 'ResourceId'},\n                        'ResourceId': {'shape': 'ResourceId'},\n                        'DelegationSetId': {'shape': 'DelegationSetId'},\n                        'ChangeId': {'shape': 'ChangeId'},\n                        'Other': {'shape': 'String'},\n                    },\n                },\n                'ResourceId': {'type': 'string'},\n                'DelegationSetId': {'type': 'string'},\n                'ChangeId': {'type': 'string'},\n                'String': {'type': 'string'},\n            },\n        }\n        model = OperationModel(operation_def, ServiceModel(service_def))\n        self.session.emit(event, params=params, model=model)\n\n        self.assertEqual(params['Id'], 'ABC123')\n        self.assertEqual(params['HostedZoneId'], 'ABC123')\n        self.assertEqual(params['ResourceId'], 'DEF456')\n        self.assertEqual(params['DelegationSetId'], 'GHI789')\n        self.assertEqual(params['ChangeId'], 'JKL012')\n\n        # This one should have been left alone\n        self.assertEqual(params['Other'], '/hostedzone/foo')\n\n    def test_route53_resource_id_missing_input_shape(self):\n        event = 'before-parameter-build.route53.GetHostedZone'\n        params = {'HostedZoneId': '/hostedzone/ABC123'}\n        operation_def = {'name': 'GetHostedZone'}\n        service_def = {'metadata': {}, 'shapes': {}}\n        model = OperationModel(operation_def, ServiceModel(service_def))\n        self.session.emit(event, params=params, model=model)\n\n        self.assertEqual(params['HostedZoneId'], '/hostedzone/ABC123')\n\n    def test_run_instances_userdata(self):\n        user_data = 'This is a test'\n        b64_user_data = base64.b64encode(user_data.encode('latin-1')).decode(\n            'utf-8'\n        )\n        params = dict(\n            ImageId='img-12345678', MinCount=1, MaxCount=5, UserData=user_data\n        )\n        handlers.base64_encode_user_data(params=params)\n        result = {\n            'ImageId': 'img-12345678',\n            'MinCount': 1,\n            'MaxCount': 5,\n            'UserData': b64_user_data,\n        }\n        self.assertEqual(params, result)\n\n    def test_run_instances_userdata_blob(self):\n        # Ensure that binary can be passed in as user data.\n        # This is valid because you can send gzip compressed files as\n        # user data.\n        user_data = b'\\xc7\\xa9This is a test'\n        b64_user_data = base64.b64encode(user_data).decode('utf-8')\n        params = dict(\n            ImageId='img-12345678', MinCount=1, MaxCount=5, UserData=user_data\n        )\n        handlers.base64_encode_user_data(params=params)\n        result = {\n            'ImageId': 'img-12345678',\n            'MinCount': 1,\n            'MaxCount': 5,\n            'UserData': b64_user_data,\n        }\n        self.assertEqual(params, result)\n\n    def test_get_template_has_error_response(self):\n        original = {'Error': {'Code': 'Message'}}\n        handler_input = copy.deepcopy(original)\n        handlers.json_decode_template_body(parsed=handler_input)\n        # The handler should not have changed the response because it's\n        # an error response.\n        self.assertEqual(original, handler_input)\n\n    def test_does_decode_template_body_in_order(self):\n        expected_ordering = OrderedDict(\n            [\n                ('TemplateVersion', 1.0),\n                ('APropertyOfSomeKind', 'a value'),\n                ('list', [1, 2, 3]),\n                ('nested', OrderedDict([('key', 'value'), ('foo', 'bar')])),\n            ]\n        )\n        template_string = json.dumps(expected_ordering)\n        parsed_response = {'TemplateBody': template_string}\n\n        handlers.json_decode_template_body(parsed=parsed_response)\n        result = parsed_response['TemplateBody']\n\n        self.assertTrue(isinstance(result, OrderedDict))\n        for element, expected_element in zip(result, expected_ordering):\n            self.assertEqual(element, expected_element)\n\n    def test_decode_json_policy(self):\n        parsed = {\n            'Document': '{\"foo\": \"foobarbaz\"}',\n            'Other': 'bar',\n        }\n        service_def = {\n            'operations': {\n                'Foo': {\n                    'output': {'shape': 'PolicyOutput'},\n                }\n            },\n            'shapes': {\n                'PolicyOutput': {\n                    'type': 'structure',\n                    'members': {\n                        'Document': {'shape': 'policyDocumentType'},\n                        'Other': {'shape': 'stringType'},\n                    },\n                },\n                'policyDocumentType': {'type': 'string'},\n                'stringType': {'type': 'string'},\n            },\n        }\n        model = ServiceModel(service_def)\n        op_model = model.operation_model('Foo')\n        handlers.json_decode_policies(parsed, op_model)\n        self.assertEqual(parsed['Document'], {'foo': 'foobarbaz'})\n\n        no_document = {'Other': 'bar'}\n        handlers.json_decode_policies(no_document, op_model)\n        self.assertEqual(no_document, {'Other': 'bar'})\n\n    def test_inject_account_id(self):\n        params = {}\n        handlers.inject_account_id(params)\n        self.assertEqual(params['accountId'], '-')\n\n    def test_account_id_not_added_if_present(self):\n        params = {'accountId': 'foo'}\n        handlers.inject_account_id(params)\n        self.assertEqual(params['accountId'], 'foo')\n\n    def test_glacier_version_header_added(self):\n        request_dict = {'headers': {}}\n        model = ServiceModel({'metadata': {'apiVersion': '2012-01-01'}})\n        handlers.add_glacier_version(model, request_dict)\n        self.assertEqual(\n            request_dict['headers']['x-amz-glacier-version'], '2012-01-01'\n        )\n\n    def test_application_json_header_added(self):\n        request_dict = {'headers': {}}\n        handlers.add_accept_header(None, request_dict)\n        self.assertEqual(request_dict['headers']['Accept'], 'application/json')\n\n    def test_accept_header_not_added_if_present(self):\n        request_dict = {'headers': {'Accept': 'application/yaml'}}\n        handlers.add_accept_header(None, request_dict)\n        self.assertEqual(request_dict['headers']['Accept'], 'application/yaml')\n\n    def test_glacier_checksums_added(self):\n        request_dict = {\n            'headers': {},\n            'body': io.BytesIO(b'hello world'),\n        }\n        handlers.add_glacier_checksums(request_dict)\n        self.assertIn('x-amz-content-sha256', request_dict['headers'])\n        self.assertIn('x-amz-sha256-tree-hash', request_dict['headers'])\n        self.assertEqual(\n            request_dict['headers']['x-amz-content-sha256'],\n            'b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9',\n        )\n        self.assertEqual(\n            request_dict['headers']['x-amz-sha256-tree-hash'],\n            'b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9',\n        )\n        # And verify that the body can still be read.\n        self.assertEqual(request_dict['body'].read(), b'hello world')\n\n    def test_tree_hash_added_only_if_not_exists(self):\n        request_dict = {\n            'headers': {\n                'x-amz-sha256-tree-hash': 'pre-exists',\n            },\n            'body': io.BytesIO(b'hello world'),\n        }\n        handlers.add_glacier_checksums(request_dict)\n        self.assertEqual(\n            request_dict['headers']['x-amz-sha256-tree-hash'], 'pre-exists'\n        )\n\n    def test_checksum_added_only_if_not_exists(self):\n        request_dict = {\n            'headers': {\n                'x-amz-content-sha256': 'pre-exists',\n            },\n            'body': io.BytesIO(b'hello world'),\n        }\n        handlers.add_glacier_checksums(request_dict)\n        self.assertEqual(\n            request_dict['headers']['x-amz-content-sha256'], 'pre-exists'\n        )\n\n    def test_glacier_checksums_support_raw_bytes(self):\n        request_dict = {\n            'headers': {},\n            'body': b'hello world',\n        }\n        handlers.add_glacier_checksums(request_dict)\n        self.assertEqual(\n            request_dict['headers']['x-amz-content-sha256'],\n            'b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9',\n        )\n        self.assertEqual(\n            request_dict['headers']['x-amz-sha256-tree-hash'],\n            'b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9',\n        )\n\n    def test_switch_host_with_param(self):\n        request = AWSRequest()\n        url = 'https://machinelearning.us-east-1.amazonaws.com'\n        new_endpoint = 'https://my-custom-endpoint.amazonaws.com'\n        data = '{\"PredictEndpoint\":\"%s\"}' % new_endpoint\n        request.data = data.encode('utf-8')\n        request.url = url\n        handlers.switch_host_with_param(request, 'PredictEndpoint')\n        self.assertEqual(request.url, new_endpoint)\n\n    def test_invalid_char_in_bucket_raises_exception(self):\n        params = {\n            'Bucket': 'bad/bucket/name',\n            'Key': 'foo',\n            'Body': b'asdf',\n        }\n        with self.assertRaises(ParamValidationError):\n            handlers.validate_bucket_name(params)\n\n    def test_bucket_too_long_raises_exception(self):\n        params = {\n            'Bucket': 'a' * 300,\n            'Key': 'foo',\n            'Body': b'asdf',\n        }\n        with self.assertRaises(ParamValidationError):\n            handlers.validate_bucket_name(params)\n\n    def test_not_dns_compat_but_still_valid_bucket_name(self):\n        params = {\n            'Bucket': 'foasdf......bar--baz-a_b_CD10',\n            'Key': 'foo',\n            'Body': b'asdf',\n        }\n        self.assertIsNone(handlers.validate_bucket_name(params))\n\n    def test_valid_bucket_name_hyphen(self):\n        self.assertIsNone(\n            handlers.validate_bucket_name({'Bucket': 'my-bucket-name'})\n        )\n\n    def test_valid_bucket_name_underscore(self):\n        self.assertIsNone(\n            handlers.validate_bucket_name({'Bucket': 'my_bucket_name'})\n        )\n\n    def test_valid_bucket_name_period(self):\n        self.assertIsNone(\n            handlers.validate_bucket_name({'Bucket': 'my.bucket.name'})\n        )\n\n    def test_validation_is_noop_if_no_bucket_param_exists(self):\n        self.assertIsNone(handlers.validate_bucket_name(params={}))\n\n    def test_validation_is_s3_accesspoint_arn(self):\n        try:\n            arn = 'arn:aws:s3:us-west-2:123456789012:accesspoint:endpoint'\n            handlers.validate_bucket_name({'Bucket': arn})\n        except ParamValidationError:\n            self.fail('The s3 arn: %s should pass validation' % arn)\n\n    def test_validation_is_s3_outpost_arn(self):\n        try:\n            arn = (\n                'arn:aws:s3-outposts:us-west-2:123456789012:outpost:'\n                'op-01234567890123456:accesspoint:myaccesspoint'\n            )\n            handlers.validate_bucket_name({'Bucket': arn})\n        except ParamValidationError:\n            self.fail('The s3 arn: %s should pass validation' % arn)\n\n    def test_validation_is_global_s3_bucket_arn(self):\n        with self.assertRaises(ParamValidationError):\n            arn = 'arn:aws:s3:::mybucket'\n            handlers.validate_bucket_name({'Bucket': arn})\n\n    def test_validation_is_other_service_arn(self):\n        with self.assertRaises(ParamValidationError):\n            arn = 'arn:aws:ec2:us-west-2:123456789012:instance:myinstance'\n            handlers.validate_bucket_name({'Bucket': arn})\n\n    def test_validate_non_ascii_metadata_values(self):\n        with self.assertRaises(ParamValidationError):\n            handlers.validate_ascii_metadata({'Metadata': {'foo': '\\u2713'}})\n\n    def test_validate_non_ascii_metadata_keys(self):\n        with self.assertRaises(ParamValidationError):\n            handlers.validate_ascii_metadata({'Metadata': {'\\u2713': 'bar'}})\n\n    def test_validate_non_triggered_when_no_md_specified(self):\n        original = {'NotMetadata': ''}\n        copied = original.copy()\n        handlers.validate_ascii_metadata(copied)\n        self.assertEqual(original, copied)\n\n    def test_validation_passes_when_all_ascii_chars(self):\n        original = {'Metadata': {'foo': 'bar'}}\n        copied = original.copy()\n        handlers.validate_ascii_metadata(original)\n        self.assertEqual(original, copied)\n\n    def test_set_encoding_type(self):\n        params = {}\n        context = {}\n        handlers.set_list_objects_encoding_type_url(params, context=context)\n        self.assertEqual(params['EncodingType'], 'url')\n        self.assertTrue(context['encoding_type_auto_set'])\n\n        params['EncodingType'] = 'new_value'\n        handlers.set_list_objects_encoding_type_url(params, context={})\n        self.assertEqual(params['EncodingType'], 'new_value')\n\n    def test_decode_list_objects(self):\n        parsed = {\n            'Contents': [{'Key': \"%C3%A7%C3%B6s%25asd%08\"}],\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object(parsed, context=context)\n        self.assertEqual(parsed['Contents'][0]['Key'], '\\xe7\\xf6s%asd\\x08')\n\n    def test_decode_list_objects_does_not_decode_without_context(self):\n        parsed = {\n            'Contents': [{'Key': \"%C3%A7%C3%B6s%25asd\"}],\n            'EncodingType': 'url',\n        }\n        handlers.decode_list_object(parsed, context={})\n        self.assertEqual(parsed['Contents'][0]['Key'], '%C3%A7%C3%B6s%25asd')\n\n    def test_decode_list_objects_with_marker(self):\n        parsed = {\n            'Marker': \"%C3%A7%C3%B6s%25%20asd%08+c\",\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object(parsed, context=context)\n        self.assertEqual(parsed['Marker'], '\\xe7\\xf6s% asd\\x08 c')\n\n    def test_decode_list_objects_with_nextmarker(self):\n        parsed = {\n            'NextMarker': \"%C3%A7%C3%B6s%25%20asd%08+c\",\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object(parsed, context=context)\n        self.assertEqual(parsed['NextMarker'], '\\xe7\\xf6s% asd\\x08 c')\n\n    def test_decode_list_objects_with_common_prefixes(self):\n        parsed = {\n            'CommonPrefixes': [{'Prefix': \"%C3%A7%C3%B6s%25%20asd%08+c\"}],\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object(parsed, context=context)\n        self.assertEqual(\n            parsed['CommonPrefixes'][0]['Prefix'], '\\xe7\\xf6s% asd\\x08 c'\n        )\n\n    def test_decode_list_objects_with_delimiter(self):\n        parsed = {\n            'Delimiter': \"%C3%A7%C3%B6s%25%20asd%08+c\",\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object(parsed, context=context)\n        self.assertEqual(parsed['Delimiter'], '\\xe7\\xf6s% asd\\x08 c')\n\n    def test_decode_list_objects_v2(self):\n        parsed = {\n            'Contents': [{'Key': \"%C3%A7%C3%B6s%25asd%08\"}],\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object_v2(parsed, context=context)\n        self.assertEqual(parsed['Contents'][0]['Key'], '\\xe7\\xf6s%asd\\x08')\n\n    def test_decode_list_objects_v2_does_not_decode_without_context(self):\n        parsed = {\n            'Contents': [{'Key': \"%C3%A7%C3%B6s%25asd\"}],\n            'EncodingType': 'url',\n        }\n        handlers.decode_list_object_v2(parsed, context={})\n        self.assertEqual(parsed['Contents'][0]['Key'], '%C3%A7%C3%B6s%25asd')\n\n    def test_decode_list_objects_v2_with_delimiter(self):\n        parsed = {\n            'Delimiter': \"%C3%A7%C3%B6s%25%20asd%08+c\",\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object_v2(parsed, context=context)\n        self.assertEqual(parsed['Delimiter'], '\\xe7\\xf6s% asd\\x08 c')\n\n    def test_decode_list_objects_v2_with_prefix(self):\n        parsed = {\n            'Prefix': \"%C3%A7%C3%B6s%25%20asd%08+c\",\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object_v2(parsed, context=context)\n        self.assertEqual(parsed['Prefix'], '\\xe7\\xf6s% asd\\x08 c')\n\n    def test_decode_list_objects_v2_does_not_decode_continuationtoken(self):\n        parsed = {\n            'ContinuationToken': \"%C3%A7%C3%B6s%25%20asd%08+c\",\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object_v2(parsed, context=context)\n        self.assertEqual(\n            parsed['ContinuationToken'], \"%C3%A7%C3%B6s%25%20asd%08+c\"\n        )\n\n    def test_decode_list_objects_v2_with_startafter(self):\n        parsed = {\n            'StartAfter': \"%C3%A7%C3%B6s%25%20asd%08+c\",\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object_v2(parsed, context=context)\n        self.assertEqual(parsed['StartAfter'], '\\xe7\\xf6s% asd\\x08 c')\n\n    def test_decode_list_objects_v2_with_common_prefixes(self):\n        parsed = {\n            'CommonPrefixes': [{'Prefix': \"%C3%A7%C3%B6s%25%20asd%08+c\"}],\n            'EncodingType': 'url',\n        }\n        context = {'encoding_type_auto_set': True}\n        handlers.decode_list_object_v2(parsed, context=context)\n        self.assertEqual(\n            parsed['CommonPrefixes'][0]['Prefix'], '\\xe7\\xf6s% asd\\x08 c'\n        )\n\n    def test_set_operation_specific_signer_no_auth_type(self):\n        signing_name = 'myservice'\n        context = {'auth_type': None}\n        response = handlers.set_operation_specific_signer(\n            context=context, signing_name=signing_name\n        )\n        self.assertIsNone(response)\n\n    def test_set_operation_specific_signer_unsigned(self):\n        signing_name = 'myservice'\n        context = {'auth_type': 'none'}\n        response = handlers.set_operation_specific_signer(\n            context=context, signing_name=signing_name\n        )\n        self.assertEqual(response, botocore.UNSIGNED)\n\n    def test_set_operation_specific_signer_v4(self):\n        signing_name = 'myservice'\n        context = {'auth_type': 'v4'}\n        response = handlers.set_operation_specific_signer(\n            context=context, signing_name=signing_name\n        )\n        self.assertEqual(response, 'v4')\n\n    def test_set_operation_specific_signer_v4a(self):\n        signing_name = 'myservice'\n        context = {'auth_type': 'v4a'}\n        response = handlers.set_operation_specific_signer(\n            context=context, signing_name=signing_name\n        )\n        self.assertEqual(response, 'v4a')\n        # for v4a, context gets updated in place\n        self.assertIsNotNone(context.get('signing'))\n        self.assertEqual(context['signing']['region'], '*')\n        self.assertEqual(context['signing']['signing_name'], signing_name)\n\n    def test_set_operation_specific_signer_v4a_existing_signing_context(self):\n        signing_name = 'myservice'\n        context = {\n            'auth_type': 'v4a',\n            'signing': {'foo': 'bar', 'region': 'abc'},\n        }\n        handlers.set_operation_specific_signer(\n            context=context, signing_name=signing_name\n        )\n        # region has been updated\n        self.assertEqual(context['signing']['region'], '*')\n        # signing_name has been added\n        self.assertEqual(context['signing']['signing_name'], signing_name)\n        # foo remained untouched\n        self.assertEqual(context['signing']['foo'], 'bar')\n\n    def test_set_operation_specific_signer_v4_unsinged_payload(self):\n        signing_name = 'myservice'\n        context = {'auth_type': 'v4-unsigned-body'}\n        response = handlers.set_operation_specific_signer(\n            context=context, signing_name=signing_name\n        )\n        self.assertEqual(response, 'v4')\n        self.assertEqual(context.get('payload_signing_enabled'), False)\n\n    def test_set_operation_specific_signer_s3v4_unsigned_payload(self):\n        signing_name = 's3'\n        context = {\n            'auth_type': 'v4-unsigned-body',\n            'signing': {\n                'foo': 'bar',\n                'region': 'abc',\n                'disableDoubleEncoding': True,\n            },\n        }\n        response = handlers.set_operation_specific_signer(\n            context=context, signing_name=signing_name\n        )\n        self.assertEqual(response, 's3v4')\n        self.assertEqual(context.get('payload_signing_enabled'), False)\n\n\n@pytest.mark.parametrize(\n    'auth_type, expected_response', [('v4', 's3v4'), ('v4a', 's3v4a')]\n)\ndef test_set_operation_specific_signer_s3v4(auth_type, expected_response):\n    signing_name = 's3'\n    context = {\n        'auth_type': auth_type,\n        'signing': {'disableDoubleEncoding': True},\n    }\n    response = handlers.set_operation_specific_signer(\n        context=context, signing_name=signing_name\n    )\n    assert response == expected_response\n\n\nclass TestConvertStringBodyToFileLikeObject(BaseSessionTest):\n    def assert_converts_to_file_like_object_with_bytes(self, body, body_bytes):\n        params = {'Body': body}\n        handlers.convert_body_to_file_like_object(params)\n        self.assertTrue(hasattr(params['Body'], 'read'))\n        contents = params['Body'].read()\n        self.assertIsInstance(contents, bytes)\n        self.assertEqual(contents, body_bytes)\n\n    def test_string(self):\n        self.assert_converts_to_file_like_object_with_bytes('foo', b'foo')\n\n    def test_binary(self):\n        body = os.urandom(500)\n        body_bytes = body\n        self.assert_converts_to_file_like_object_with_bytes(body, body_bytes)\n\n    def test_file(self):\n        body = io.StringIO()\n        params = {'Body': body}\n        handlers.convert_body_to_file_like_object(params)\n        self.assertEqual(params['Body'], body)\n\n    def test_unicode(self):\n        self.assert_converts_to_file_like_object_with_bytes('bar', b'bar')\n\n    def test_non_ascii_characters(self):\n        self.assert_converts_to_file_like_object_with_bytes(\n            '\\u2713', b'\\xe2\\x9c\\x93'\n        )\n\n\nclass TestRetryHandlerOrder(BaseSessionTest):\n    def get_handler_names(self, responses):\n        names = []\n        for response in responses:\n            handler = response[0]\n            if hasattr(handler, '__name__'):\n                names.append(handler.__name__)\n            elif hasattr(handler, '__class__'):\n                names.append(handler.__class__.__name__)\n            else:\n                names.append(str(handler))\n        return names\n\n    def test_s3_special_case_is_before_other_retry(self):\n        client = self.session.create_client('s3')\n        service_model = self.session.get_service_model('s3')\n        operation = service_model.operation_model('CopyObject')\n        responses = client.meta.events.emit(\n            'needs-retry.s3.CopyObject',\n            request_dict={'context': {}},\n            response=(mock.Mock(), mock.Mock()),\n            endpoint=mock.Mock(),\n            operation=operation,\n            attempts=1,\n            caught_exception=None,\n        )\n        # This is implementation specific, but we're trying to verify that\n        # the check_for_200_error is before any of the retry logic in\n        # botocore.retryhandlers.\n        # Technically, as long as the relative order is preserved, we don't\n        # care about the absolute order.\n        names = self.get_handler_names(responses)\n        self.assertIn('check_for_200_error', names)\n        self.assertIn('RetryHandler', names)\n        s3_200_handler = names.index('check_for_200_error')\n        general_retry_handler = names.index('RetryHandler')\n        self.assertTrue(\n            s3_200_handler < general_retry_handler,\n            \"S3 200 error handler was supposed to be before \"\n            \"the general retry handler, but it was not.\",\n        )\n\n\nclass BaseMD5Test(BaseSessionTest):\n    def setUp(self, **environ):\n        super().setUp(**environ)\n        self.md5_object = mock.Mock()\n        self.md5_digest = mock.Mock(return_value=b'foo')\n        self.md5_object.digest = self.md5_digest\n        md5_builder = mock.Mock(return_value=self.md5_object)\n        self.md5_patch = mock.patch('hashlib.md5', md5_builder)\n        self.md5_patch.start()\n        self._md5_available_patch = None\n        self.set_md5_available()\n\n    def tearDown(self):\n        super().tearDown()\n        self.md5_patch.stop()\n        if self._md5_available_patch:\n            self._md5_available_patch.stop()\n\n    def set_md5_available(self, is_available=True):\n        if self._md5_available_patch:\n            self._md5_available_patch.stop()\n\n        self._md5_available_patch = mock.patch(\n            'botocore.compat.MD5_AVAILABLE', is_available\n        )\n        self._md5_available_patch.start()\n\n\nclass TestSSEMD5(BaseMD5Test):\n    def test_raises_error_when_md5_unavailable(self):\n        self.set_md5_available(False)\n\n        with self.assertRaises(MD5UnavailableError):\n            handlers.sse_md5({'SSECustomerKey': b'foo'})\n\n        with self.assertRaises(MD5UnavailableError):\n            handlers.copy_source_sse_md5({'CopySourceSSECustomerKey': b'foo'})\n\n    def test_sse_params(self):\n        for op in (\n            'HeadObject',\n            'GetObject',\n            'PutObject',\n            'CopyObject',\n            'CreateMultipartUpload',\n            'UploadPart',\n            'UploadPartCopy',\n            'SelectObjectContent',\n        ):\n            event = 'before-parameter-build.s3.%s' % op\n            params = {\n                'SSECustomerKey': b'bar',\n                'SSECustomerAlgorithm': 'AES256',\n            }\n            self.session.emit(\n                event, params=params, model=mock.MagicMock(), context={}\n            )\n            self.assertEqual(params['SSECustomerKey'], 'YmFy')\n            self.assertEqual(params['SSECustomerKeyMD5'], 'Zm9v')\n\n    def test_sse_params_as_str(self):\n        event = 'before-parameter-build.s3.PutObject'\n        params = {'SSECustomerKey': 'bar', 'SSECustomerAlgorithm': 'AES256'}\n        self.session.emit(\n            event, params=params, model=mock.MagicMock(), context={}\n        )\n        self.assertEqual(params['SSECustomerKey'], 'YmFy')\n        self.assertEqual(params['SSECustomerKeyMD5'], 'Zm9v')\n\n    def test_copy_source_sse_params(self):\n        for op in ['CopyObject', 'UploadPartCopy']:\n            event = 'before-parameter-build.s3.%s' % op\n            params = {\n                'CopySourceSSECustomerKey': b'bar',\n                'CopySourceSSECustomerAlgorithm': 'AES256',\n            }\n            self.session.emit(\n                event, params=params, model=mock.MagicMock(), context={}\n            )\n            self.assertEqual(params['CopySourceSSECustomerKey'], 'YmFy')\n            self.assertEqual(params['CopySourceSSECustomerKeyMD5'], 'Zm9v')\n\n    def test_copy_source_sse_params_as_str(self):\n        event = 'before-parameter-build.s3.CopyObject'\n        params = {\n            'CopySourceSSECustomerKey': 'bar',\n            'CopySourceSSECustomerAlgorithm': 'AES256',\n        }\n        self.session.emit(\n            event, params=params, model=mock.MagicMock(), context={}\n        )\n        self.assertEqual(params['CopySourceSSECustomerKey'], 'YmFy')\n        self.assertEqual(params['CopySourceSSECustomerKeyMD5'], 'Zm9v')\n\n\nclass TestAddMD5(BaseMD5Test):\n    def get_context(self, s3_config=None):\n        if s3_config is None:\n            s3_config = {}\n        return {'client_config': Config(s3=s3_config)}\n\n    def test_adds_md5_when_v4(self):\n        credentials = Credentials('key', 'secret')\n        request_signer = RequestSigner(\n            ServiceId('s3'), 'us-east-1', 's3', 'v4', credentials, mock.Mock()\n        )\n        request_dict = {\n            'body': b'bar',\n            'url': 'https://s3.us-east-1.amazonaws.com',\n            'method': 'PUT',\n            'headers': {},\n        }\n        context = self.get_context()\n        conditionally_calculate_md5(\n            request_dict, request_signer=request_signer, context=context\n        )\n        self.assertTrue('Content-MD5' in request_dict['headers'])\n\n    def test_adds_md5_when_s3v4(self):\n        credentials = Credentials('key', 'secret')\n        request_signer = RequestSigner(\n            ServiceId('s3'),\n            'us-east-1',\n            's3',\n            's3v4',\n            credentials,\n            mock.Mock(),\n        )\n        request_dict = {\n            'body': b'bar',\n            'url': 'https://s3.us-east-1.amazonaws.com',\n            'method': 'PUT',\n            'headers': {},\n        }\n        context = self.get_context({'payload_signing_enabled': False})\n        conditionally_calculate_md5(\n            request_dict, request_signer=request_signer, context=context\n        )\n        self.assertTrue('Content-MD5' in request_dict['headers'])\n\n    def test_conditional_does_not_add_when_md5_unavailable(self):\n        credentials = Credentials('key', 'secret')\n        request_signer = RequestSigner(\n            's3', 'us-east-1', 's3', 's3', credentials, mock.Mock()\n        )\n        request_dict = {\n            'body': b'bar',\n            'url': 'https://s3.us-east-1.amazonaws.com',\n            'method': 'PUT',\n            'headers': {},\n        }\n\n        context = self.get_context()\n        self.set_md5_available(False)\n        with mock.patch('botocore.utils.MD5_AVAILABLE', False):\n            conditionally_calculate_md5(\n                request_dict, request_signer=request_signer, context=context\n            )\n            self.assertFalse('Content-MD5' in request_dict['headers'])\n\n    def test_add_md5_raises_error_when_md5_unavailable(self):\n        credentials = Credentials('key', 'secret')\n        request_signer = RequestSigner(\n            ServiceId('s3'), 'us-east-1', 's3', 's3', credentials, mock.Mock()\n        )\n        request_dict = {\n            'body': b'bar',\n            'url': 'https://s3.us-east-1.amazonaws.com',\n            'method': 'PUT',\n            'headers': {},\n        }\n\n        self.set_md5_available(False)\n        with self.assertRaises(MD5UnavailableError):\n            conditionally_calculate_md5(\n                request_dict, request_signer=request_signer\n            )\n\n    def test_adds_md5_when_s3v2(self):\n        credentials = Credentials('key', 'secret')\n        request_signer = RequestSigner(\n            ServiceId('s3'), 'us-east-1', 's3', 's3', credentials, mock.Mock()\n        )\n        request_dict = {\n            'body': b'bar',\n            'url': 'https://s3.us-east-1.amazonaws.com',\n            'method': 'PUT',\n            'headers': {},\n        }\n        context = self.get_context()\n        conditionally_calculate_md5(\n            request_dict, request_signer=request_signer, context=context\n        )\n        self.assertTrue('Content-MD5' in request_dict['headers'])\n\n    def test_add_md5_with_file_like_body(self):\n        request_dict = {'body': io.BytesIO(b'foobar'), 'headers': {}}\n        self.md5_digest.return_value = b'8X\\xf6\"0\\xac<\\x91_0\\x0cfC\\x12\\xc6?'\n        conditionally_calculate_md5(request_dict)\n        self.assertEqual(\n            request_dict['headers']['Content-MD5'], 'OFj2IjCsPJFfMAxmQxLGPw=='\n        )\n\n    def test_add_md5_with_bytes_object(self):\n        request_dict = {'body': b'foobar', 'headers': {}}\n        self.md5_digest.return_value = b'8X\\xf6\"0\\xac<\\x91_0\\x0cfC\\x12\\xc6?'\n        conditionally_calculate_md5(request_dict)\n        self.assertEqual(\n            request_dict['headers']['Content-MD5'], 'OFj2IjCsPJFfMAxmQxLGPw=='\n        )\n\n    def test_add_md5_with_empty_body(self):\n        request_dict = {'body': b'', 'headers': {}}\n        self.md5_digest.return_value = b'8X\\xf6\"0\\xac<\\x91_0\\x0cfC\\x12\\xc6?'\n        conditionally_calculate_md5(request_dict)\n        self.assertEqual(\n            request_dict['headers']['Content-MD5'], 'OFj2IjCsPJFfMAxmQxLGPw=='\n        )\n\n    def test_add_md5_with_bytearray_object(self):\n        request_dict = {'body': bytearray(b'foobar'), 'headers': {}}\n        self.md5_digest.return_value = b'8X\\xf6\"0\\xac<\\x91_0\\x0cfC\\x12\\xc6?'\n        conditionally_calculate_md5(request_dict)\n        self.assertEqual(\n            request_dict['headers']['Content-MD5'], 'OFj2IjCsPJFfMAxmQxLGPw=='\n        )\n\n    def test_skip_md5_when_flexible_checksum_context(self):\n        request_dict = {\n            'body': io.BytesIO(b'foobar'),\n            'headers': {},\n            'context': {\n                'checksum': {\n                    'request_algorithm': {\n                        'in': 'header',\n                        'algorithm': 'crc32',\n                        'name': 'x-amz-checksum-crc32',\n                    }\n                }\n            },\n        }\n        conditionally_calculate_md5(request_dict)\n        self.assertNotIn('Content-MD5', request_dict['headers'])\n\n    def test_skip_md5_when_flexible_checksum_explicit_header(self):\n        request_dict = {\n            'body': io.BytesIO(b'foobar'),\n            'headers': {'x-amz-checksum-crc32': 'foo'},\n        }\n        conditionally_calculate_md5(request_dict)\n        self.assertNotIn('Content-MD5', request_dict['headers'])\n\n\nclass TestParameterAlias(unittest.TestCase):\n    def setUp(self):\n        self.original_name = 'original'\n        self.alias_name = 'alias'\n        self.parameter_alias = handlers.ParameterAlias(\n            self.original_name, self.alias_name\n        )\n\n        self.operation_model = mock.Mock()\n        request_shape = (\n            DenormalizedStructureBuilder()\n            .with_members({self.original_name: {'type': 'string'}})\n            .build_model()\n        )\n        self.operation_model.input_shape = request_shape\n        self.sample_section = DocumentStructure('')\n        self.event_emitter = HierarchicalEmitter()\n\n    def test_alias_parameter_in_call(self):\n        value = 'value'\n        params = {self.alias_name: value}\n        self.parameter_alias.alias_parameter_in_call(\n            params, self.operation_model\n        )\n        self.assertEqual(params, {self.original_name: value})\n\n    def test_alias_parameter_and_original_in_call(self):\n        params = {\n            self.original_name: 'orginal_value',\n            self.alias_name: 'alias_value',\n        }\n        with self.assertRaises(AliasConflictParameterError):\n            self.parameter_alias.alias_parameter_in_call(\n                params, self.operation_model\n            )\n\n    def test_alias_parameter_in_call_does_not_touch_original(self):\n        value = 'value'\n        params = {self.original_name: value}\n        self.parameter_alias.alias_parameter_in_call(\n            params, self.operation_model\n        )\n        self.assertEqual(params, {self.original_name: value})\n\n    def test_does_not_alias_parameter_for_no_input_shape(self):\n        value = 'value'\n        params = {self.alias_name: value}\n        self.operation_model.input_shape = None\n        self.parameter_alias.alias_parameter_in_call(\n            params, self.operation_model\n        )\n        self.assertEqual(params, {self.alias_name: value})\n\n    def test_does_not_alias_parameter_for_not_modeled_member(self):\n        value = 'value'\n        params = {self.alias_name: value}\n\n        request_shape = (\n            DenormalizedStructureBuilder()\n            .with_members({'foo': {'type': 'string'}})\n            .build_model()\n        )\n        self.operation_model.input_shape = request_shape\n        self.parameter_alias.alias_parameter_in_call(\n            params, self.operation_model\n        )\n        self.assertEqual(params, {self.alias_name: value})\n\n    def test_alias_parameter_in_documentation_request_params(self):\n        RequestParamsDocumenter(\n            'myservice', 'myoperation', self.event_emitter\n        ).document_params(\n            self.sample_section, self.operation_model.input_shape\n        )\n        self.parameter_alias.alias_parameter_in_documentation(\n            'docs.request-params.myservice.myoperation.complete-section',\n            self.sample_section,\n        )\n        contents = self.sample_section.flush_structure().decode('utf-8')\n        self.assertIn(':type ' + self.alias_name + ':', contents)\n        self.assertIn(':param ' + self.alias_name + ':', contents)\n        self.assertNotIn(':type ' + self.original_name + ':', contents)\n        self.assertNotIn(':param ' + self.original_name + ':', contents)\n\n    def test_alias_parameter_in_documentation_request_example(self):\n        RequestExampleDocumenter(\n            'myservice', 'myoperation', self.event_emitter\n        ).document_example(\n            self.sample_section, self.operation_model.input_shape\n        )\n        self.parameter_alias.alias_parameter_in_documentation(\n            'docs.request-example.myservice.myoperation.complete-section',\n            self.sample_section,\n        )\n        contents = self.sample_section.flush_structure().decode('utf-8')\n        self.assertIn(self.alias_name + '=', contents)\n        self.assertNotIn(self.original_name + '=', contents)\n\n\nclass TestCommandAlias(unittest.TestCase):\n    def test_command_alias(self):\n        alias = handlers.ClientMethodAlias('foo')\n        client = mock.Mock()\n        client.foo.return_value = 'bar'\n\n        response = alias(client=client)()\n        self.assertEqual(response, 'bar')\n\n\nclass TestPrependToHost(unittest.TestCase):\n    def setUp(self):\n        self.hoister = handlers.HeaderToHostHoister('test-header')\n\n    def _prepend_to_host(self, url, prepend_string):\n        params = {\n            'headers': {\n                'test-header': prepend_string,\n            },\n            'url': url,\n        }\n        self.hoister.hoist(params=params)\n        return params['url']\n\n    def test_does_prepend_to_host(self):\n        prepended = self._prepend_to_host('https://bar.example.com/', 'foo')\n        self.assertEqual(prepended, 'https://foo.bar.example.com/')\n\n    def test_does_prepend_to_host_with_http(self):\n        prepended = self._prepend_to_host('http://bar.example.com/', 'foo')\n        self.assertEqual(prepended, 'http://foo.bar.example.com/')\n\n    def test_does_prepend_to_host_with_path(self):\n        prepended = self._prepend_to_host(\n            'https://bar.example.com/path', 'foo'\n        )\n        self.assertEqual(prepended, 'https://foo.bar.example.com/path')\n\n    def test_does_prepend_to_host_with_more_components(self):\n        prepended = self._prepend_to_host(\n            'https://bar.baz.example.com/path', 'foo'\n        )\n        self.assertEqual(prepended, 'https://foo.bar.baz.example.com/path')\n\n    def test_does_validate_long_host(self):\n        with self.assertRaises(ParamValidationError):\n            self._prepend_to_host('https://example.com/path', 'toolong' * 100)\n\n    def test_does_validate_host_with_illegal_char(self):\n        with self.assertRaises(ParamValidationError):\n            self._prepend_to_host('https://example.com/path', 'host#name')\n\n\n@pytest.mark.parametrize(\n    'environ, header_before, header_after',\n    [\n        ({}, {}, {}),\n        ({'AWS_LAMBDA_FUNCTION_NAME': 'some-function'}, {}, {}),\n        (\n            {\n                '_X_AMZN_TRACE_ID': (\n                    'Root=1-5759e988-bd862e3fe1be46a994272793;Parent=53995c3f42cd8ad8;'\n                    'Sampled=1;lineage=a87bd80c:0,68fd508a:5,c512fbe3:2'\n                )\n            },\n            {},\n            {},\n        ),\n        (\n            {\n                'AWS_LAMBDA_FUNCTION_NAME': 'some-function',\n                '_X_AMZN_TRACE_ID': (\n                    'Root=1-5759e988-bd862e3fe1be46a994272793;Parent=53995c3f42cd8ad8;'\n                    'Sampled=1;lineage=a87bd80c:0,68fd508a:5,c512fbe3:2'\n                ),\n            },\n            {},\n            {\n                'X-Amzn-Trace-Id': (\n                    'Root=1-5759e988-bd862e3fe1be46a994272793;Parent=53995c3f42cd8ad8;'\n                    'Sampled=1;lineage=a87bd80c:0,68fd508a:5,c512fbe3:2'\n                )\n            },\n        ),\n        (\n            {\n                'AWS_LAMBDA_FUNCTION_NAME': 'some-function',\n                '_X_AMZN_TRACE_ID': 'EnvValue',\n            },\n            {'X-Amzn-Trace-Id': 'OriginalValue'},\n            {'X-Amzn-Trace-Id': 'OriginalValue'},\n        ),\n        (\n            {\n                'AWS_LAMBDA_FUNCTION_NAME': 'foo',\n                '_X_AMZN_TRACE_ID': 'first\\nsecond',\n            },\n            {},\n            {'X-Amzn-Trace-Id': 'first%0Asecond'},\n        ),\n        (\n            {\n                'AWS_LAMBDA_FUNCTION_NAME': 'foo',\n                '_X_AMZN_TRACE_ID': 'test123-=;:+&[]{}\\\"\\'',\n            },\n            {},\n            {'X-Amzn-Trace-Id': 'test123-=;:+&[]{}\\\"\\''},\n        ),\n    ],\n)\ndef test_add_recursion_detection_header(environ, header_before, header_after):\n    request_dict = {'headers': header_before}\n    with mock.patch('os.environ', environ):\n        handlers.add_recursion_detection_header(request_dict)\n        assert request_dict['headers'] == header_after\n\n\n@pytest.mark.parametrize(\n    'auth_path_in, auth_path_expected',\n    [\n        # access points should be stripped\n        (\n            '/arn%3Aaws%3As3%3Aus-west-2%3A1234567890%3Aaccesspoint%2Fmy-ap/object.txt',\n            '/object.txt',\n        ),\n        (\n            '/arn%3Aaws%3As3%3Aus-west-2%3A1234567890%3Aaccesspoint%2Fmy-ap/foo/foo/foo/object.txt',\n            '/foo/foo/foo/object.txt',\n        ),\n        # regular bucket names should not be stripped\n        (\n            '/mybucket/object.txt',\n            '/mybucket/object.txt',\n        ),\n        (\n            '/mybucket/foo/foo/foo/object.txt',\n            '/mybucket/foo/foo/foo/object.txt',\n        ),\n        (\n            '/arn-is-a-valid-bucketname/object.txt',\n            '/arn-is-a-valid-bucketname/object.txt',\n        ),\n        # non-bucket cases\n        (\n            '',\n            '',\n        ),\n        (\n            None,\n            None,\n        ),\n        (\n            123,\n            123,\n        ),\n    ],\n)\ndef test_remove_arn_from_signing_path(auth_path_in, auth_path_expected):\n    request = AWSRequest(method='GET', auth_path=auth_path_in)\n    # the handler modifies the request in place\n    handlers.remove_arn_from_signing_path(\n        request=request, some='other', kwarg='values'\n    )\n    assert request.auth_path == auth_path_expected\n\n\n@pytest.mark.parametrize(\n    'request_uri_before, request_uri_after, auth_path',\n    [\n        ('/{Bucket}', '', '/{Bucket}/'),\n        ('/{Bucket}?query', '?query', '/{Bucket}/'),\n        ('/{Bucket}/123', '/123', '/{Bucket}/123'),\n        ('/{Bucket}/123?query', '/123?query', '/{Bucket}/123'),\n    ],\n)\ndef test_remove_bucket_from_url_paths_from_model(\n    request_uri_before, request_uri_after, auth_path\n):\n    operation_def = {\n        'name': 'TestOp',\n        'http': {\n            'method': 'GET',\n            'requestUri': request_uri_before,\n            'responseCode': 200,\n        },\n        'input': {'shape': 'TestOpInput'},\n    }\n    service_def = {\n        'metadata': {},\n        'shapes': {\n            'TestOpInput': {\n                'type': 'structure',\n                'required': ['Bucket'],\n                'members': {\n                    'Bucket': {\n                        'shape': 'String',\n                        'contextParam': {'name': 'Bucket'},\n                        'location': 'uri',\n                        'locationName': 'Bucket',\n                    },\n                },\n            },\n        },\n    }\n    model = OperationModel(operation_def, ServiceModel(service_def))\n    # the handler modifies ``model`` in place\n    handlers.remove_bucket_from_url_paths_from_model(\n        params=None, model=model, context=None\n    )\n    assert model.http['requestUri'] == request_uri_after\n    assert model.http['authPath'] == auth_path\n", "tests/unit/test_exceptions.py": "#!/usr/bin/env\n# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pickle\n\nimport botocore.awsrequest\nimport botocore.session\nfrom botocore import exceptions\nfrom tests import unittest\n\n\ndef test_client_error_can_handle_missing_code_or_message():\n    response = {'Error': {}}\n    expect = 'An error occurred (Unknown) when calling the blackhole operation: Unknown'\n    assert str(exceptions.ClientError(response, 'blackhole')) == expect\n\n\ndef test_client_error_has_operation_name_set():\n    response = {'Error': {}}\n    exception = exceptions.ClientError(response, 'blackhole')\n    assert hasattr(exception, 'operation_name')\n\n\ndef test_client_error_set_correct_operation_name():\n    response = {'Error': {}}\n    exception = exceptions.ClientError(response, 'blackhole')\n    assert exception.operation_name == 'blackhole'\n\n\ndef test_retry_info_added_when_present():\n    response = {\n        'Error': {},\n        'ResponseMetadata': {\n            'MaxAttemptsReached': True,\n            'RetryAttempts': 3,\n        },\n    }\n    error_msg = str(exceptions.ClientError(response, 'operation'))\n    if '(reached max retries: 3)' not in error_msg:\n        raise AssertionError(\n            \"retry information not inject into error \"\n            \"message: %s\" % error_msg\n        )\n\n\ndef test_retry_info_not_added_if_retry_attempts_not_present():\n    response = {\n        'Error': {},\n        'ResponseMetadata': {\n            'MaxAttemptsReached': True,\n        },\n    }\n    # Because RetryAttempts is missing, retry info is not\n    # in the error message.\n    error_msg = str(exceptions.ClientError(response, 'operation'))\n    if 'max retries' in error_msg:\n        raise AssertionError(\n            \"Retry information should not be in exception \"\n            \"message when retry attempts not in response \"\n            \"metadata: %s\" % error_msg\n        )\n\n\ndef test_can_handle_when_response_missing_error_key():\n    response = {\n        'ResponseMetadata': {\n            'HTTPHeaders': {},\n            'HTTPStatusCode': 503,\n            'MaxAttemptsReached': True,\n            'RetryAttempts': 4,\n        }\n    }\n    e = exceptions.ClientError(response, 'SomeOperation')\n    if 'An error occurred (Unknown)' not in str(e):\n        raise AssertionError(\n            \"Error code should default to 'Unknown' \"\n            \"when missing error response, instead got: %s\" % str(e)\n        )\n\n\nclass TestPickleExceptions(unittest.TestCase):\n    def test_single_kwarg_botocore_error(self):\n        exception = botocore.exceptions.DataNotFoundError(data_path='mypath')\n        unpickled_exception = pickle.loads(pickle.dumps(exception))\n        self.assertIsInstance(\n            unpickled_exception, botocore.exceptions.DataNotFoundError\n        )\n        self.assertEqual(str(unpickled_exception), str(exception))\n        self.assertEqual(unpickled_exception.kwargs, exception.kwargs)\n\n    def test_multiple_kwarg_botocore_error(self):\n        exception = botocore.exceptions.UnknownServiceError(\n            service_name='myservice', known_service_names=['s3']\n        )\n        unpickled_exception = pickle.loads(pickle.dumps(exception))\n        self.assertIsInstance(\n            unpickled_exception, botocore.exceptions.UnknownServiceError\n        )\n        self.assertEqual(str(unpickled_exception), str(exception))\n        self.assertEqual(unpickled_exception.kwargs, exception.kwargs)\n\n    def test_client_error(self):\n        exception = botocore.exceptions.ClientError(\n            error_response={\n                'Error': {'Code': 'MyCode', 'Message': 'MyMessage'}\n            },\n            operation_name='myoperation',\n        )\n        unpickled_exception = pickle.loads(pickle.dumps(exception))\n        self.assertIsInstance(\n            unpickled_exception, botocore.exceptions.ClientError\n        )\n        self.assertEqual(str(unpickled_exception), str(exception))\n        self.assertEqual(\n            unpickled_exception.operation_name, exception.operation_name\n        )\n        self.assertEqual(unpickled_exception.response, exception.response)\n\n    def test_dynamic_client_error(self):\n        session = botocore.session.Session()\n        client = session.create_client('s3', 'us-west-2')\n        exception = client.exceptions.NoSuchKey(\n            error_response={\n                'Error': {'Code': 'NoSuchKey', 'Message': 'Not Found'}\n            },\n            operation_name='myoperation',\n        )\n        unpickled_exception = pickle.loads(pickle.dumps(exception))\n        self.assertIsInstance(\n            unpickled_exception, botocore.exceptions.ClientError\n        )\n        self.assertEqual(str(unpickled_exception), str(exception))\n        self.assertEqual(\n            unpickled_exception.operation_name, exception.operation_name\n        )\n        self.assertEqual(unpickled_exception.response, exception.response)\n\n    def test_http_client_error(self):\n        exception = botocore.exceptions.HTTPClientError(\n            botocore.awsrequest.AWSRequest(),\n            botocore.awsrequest.AWSResponse(\n                url='https://foo.com', status_code=400, headers={}, raw=b''\n            ),\n            error='error',\n        )\n        unpickled_exception = pickle.loads(pickle.dumps(exception))\n        self.assertIsInstance(\n            unpickled_exception, botocore.exceptions.HTTPClientError\n        )\n        self.assertEqual(str(unpickled_exception), str(exception))\n        self.assertEqual(unpickled_exception.kwargs, exception.kwargs)\n        # The request/response properties on the HTTPClientError do not have\n        # __eq__ defined so we want to make sure properties are at least\n        # of the expected type\n        self.assertIsInstance(\n            unpickled_exception.request, botocore.awsrequest.AWSRequest\n        )\n        self.assertIsInstance(\n            unpickled_exception.response, botocore.awsrequest.AWSResponse\n        )\n", "tests/unit/test_s3_addressing.py": "#!/usr/bin/env python\n# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport os\n\nfrom botocore.compat import OrderedDict\nfrom botocore.handlers import set_list_objects_encoding_type_url\nfrom tests import BaseSessionTest, ClientHTTPStubber\n\n\nclass TestS3Addressing(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.region_name = 'us-east-1'\n        self.signature_version = 's3'\n\n        self.session.unregister(\n            'before-parameter-build.s3.ListObjects',\n            set_list_objects_encoding_type_url,\n        )\n\n    def get_prepared_request(self, operation, params, force_hmacv1=False):\n        if force_hmacv1:\n            self.session.register('choose-signer', self.enable_hmacv1)\n        client = self.session.create_client('s3', self.region_name)\n        with ClientHTTPStubber(client) as http_stubber:\n            http_stubber.add_response()\n            getattr(client, operation)(**params)\n            # Return the request that was sent over the wire.\n            return http_stubber.requests[0]\n\n    def enable_hmacv1(self, **kwargs):\n        return 's3'\n\n    def test_list_objects_dns_name(self):\n        params = {'Bucket': 'safename'}\n        prepared_request = self.get_prepared_request(\n            'list_objects', params, force_hmacv1=True\n        )\n        self.assertEqual(\n            prepared_request.url, 'https://safename.s3.amazonaws.com/'\n        )\n\n    def test_list_objects_non_dns_name(self):\n        params = {'Bucket': 'un_safe_name'}\n        prepared_request = self.get_prepared_request(\n            'list_objects', params, force_hmacv1=True\n        )\n        self.assertEqual(\n            prepared_request.url, 'https://s3.amazonaws.com/un_safe_name'\n        )\n\n    def test_list_objects_dns_name_non_classic(self):\n        self.region_name = 'us-west-2'\n        params = {'Bucket': 'safename'}\n        prepared_request = self.get_prepared_request(\n            'list_objects', params, force_hmacv1=True\n        )\n        self.assertEqual(\n            prepared_request.url,\n            'https://safename.s3.us-west-2.amazonaws.com/',\n        )\n\n    def test_list_objects_unicode_query_string_eu_central_1(self):\n        self.region_name = 'eu-central-1'\n        params = OrderedDict(\n            [('Bucket', 'safename'), ('Marker', '\\xe4\\xf6\\xfc-01.txt')]\n        )\n        prepared_request = self.get_prepared_request('list_objects', params)\n        self.assertEqual(\n            prepared_request.url,\n            (\n                'https://safename.s3.eu-central-1.amazonaws.com/'\n                '?marker=%C3%A4%C3%B6%C3%BC-01.txt'\n            ),\n        )\n\n    def test_list_objects_in_restricted_regions(self):\n        self.region_name = 'us-gov-west-1'\n        params = {'Bucket': 'safename'}\n        prepared_request = self.get_prepared_request('list_objects', params)\n        # Note how we keep the region specific endpoint here.\n        self.assertEqual(\n            prepared_request.url,\n            'https://safename.s3.us-gov-west-1.amazonaws.com/',\n        )\n\n    def test_list_objects_in_fips(self):\n        self.region_name = 'fips-us-gov-west-1'\n        params = {'Bucket': 'safename'}\n        prepared_request = self.get_prepared_request('list_objects', params)\n        # Note how we keep the region specific endpoint here.\n        self.assertEqual(\n            prepared_request.url,\n            'https://safename.s3-fips.us-gov-west-1.amazonaws.com/',\n        )\n\n    def test_list_objects_non_dns_name_non_classic(self):\n        self.region_name = 'us-west-2'\n        params = {'Bucket': 'un_safe_name'}\n        prepared_request = self.get_prepared_request('list_objects', params)\n        self.assertEqual(\n            prepared_request.url,\n            'https://s3.us-west-2.amazonaws.com/un_safe_name',\n        )\n\n    def test_put_object_dns_name_non_classic(self):\n        self.region_name = 'us-west-2'\n        file_path = os.path.join(os.path.dirname(__file__), 'put_object_data')\n        with open(file_path, 'rb') as fp:\n            params = {\n                'Bucket': 'my.valid.name',\n                'Key': 'mykeyname',\n                'Body': fp,\n                'ACL': 'public-read',\n                'ContentLanguage': 'piglatin',\n                'ContentType': 'text/plain',\n            }\n            prepared_request = self.get_prepared_request('put_object', params)\n            self.assertEqual(\n                prepared_request.url,\n                'https://s3.us-west-2.amazonaws.com/my.valid.name/mykeyname',\n            )\n\n    def test_put_object_dns_name_classic(self):\n        self.region_name = 'us-east-1'\n        file_path = os.path.join(os.path.dirname(__file__), 'put_object_data')\n        with open(file_path, 'rb') as fp:\n            params = {\n                'Bucket': 'my.valid.name',\n                'Key': 'mykeyname',\n                'Body': fp,\n                'ACL': 'public-read',\n                'ContentLanguage': 'piglatin',\n                'ContentType': 'text/plain',\n            }\n            prepared_request = self.get_prepared_request('put_object', params)\n            self.assertEqual(\n                prepared_request.url,\n                'https://s3.amazonaws.com/my.valid.name/mykeyname',\n            )\n\n    def test_put_object_dns_name_single_letter_non_classic(self):\n        self.region_name = 'us-west-2'\n        file_path = os.path.join(os.path.dirname(__file__), 'put_object_data')\n        with open(file_path, 'rb') as fp:\n            params = {\n                'Bucket': 'a.valid.name',\n                'Key': 'mykeyname',\n                'Body': fp,\n                'ACL': 'public-read',\n                'ContentLanguage': 'piglatin',\n                'ContentType': 'text/plain',\n            }\n            prepared_request = self.get_prepared_request('put_object', params)\n            self.assertEqual(\n                prepared_request.url,\n                'https://s3.us-west-2.amazonaws.com/a.valid.name/mykeyname',\n            )\n\n    def test_get_object_non_dns_name_non_classic(self):\n        self.region_name = 'us-west-2'\n        params = {'Bucket': 'AnInvalidName', 'Key': 'mykeyname'}\n        prepared_request = self.get_prepared_request('get_object', params)\n        self.assertEqual(\n            prepared_request.url,\n            'https://s3.us-west-2.amazonaws.com/AnInvalidName/mykeyname',\n        )\n\n    def test_get_object_non_dns_name_classic(self):\n        self.region_name = 'us-east-1'\n        params = {'Bucket': 'AnInvalidName', 'Key': 'mykeyname'}\n        prepared_request = self.get_prepared_request('get_object', params)\n        self.assertEqual(\n            prepared_request.url,\n            'https://s3.amazonaws.com/AnInvalidName/mykeyname',\n        )\n\n    def test_get_object_ip_address_name_non_classic(self):\n        self.region_name = 'us-west-2'\n        params = {'Bucket': '192.168.5.4', 'Key': 'mykeyname'}\n        prepared_request = self.get_prepared_request('get_object', params)\n        self.assertEqual(\n            prepared_request.url,\n            'https://s3.us-west-2.amazonaws.com/192.168.5.4/mykeyname',\n        )\n\n    def test_get_object_almost_an_ip_address_name_non_classic(self):\n        self.region_name = 'us-west-2'\n        params = {'Bucket': '192.168.5.256', 'Key': 'mykeyname'}\n        prepared_request = self.get_prepared_request('get_object', params)\n        self.assertEqual(\n            prepared_request.url,\n            'https://s3.us-west-2.amazonaws.com/192.168.5.256/mykeyname',\n        )\n\n    def test_invalid_endpoint_raises_exception(self):\n        with self.assertRaisesRegex(ValueError, 'Invalid region'):\n            self.session.create_client('s3', 'Invalid region')\n\n    def test_non_existent_region(self):\n        # If I ask for a region that does not\n        # exist on a global endpoint, such as:\n        client = self.session.create_client('s3', 'us-west-111')\n        # Then the default endpoint heuristic will apply and we'll\n        # get the region name as specified.\n        self.assertEqual(client.meta.region_name, 'us-west-111')\n        # Why not fixed this?  Well backwards compatibility for one thing.\n        # The other reason is because it was intended to accommodate this\n        # use case.  Let's say I have us-west-2 set as my default region,\n        # possibly through an env var or config variable.  Well, by default,\n        # we'd make a call like:\n        client = self.session.create_client('iam', 'us-west-2')\n        # Instead of giving the user an error, we should instead give\n        # them the partition-global endpoint.\n        self.assertEqual(client.meta.region_name, 'aws-global')\n        # But if they request an endpoint that we *do* know about, we use\n        # that specific endpoint.\n        client = self.session.create_client('iam', 'aws-us-gov-global')\n        self.assertEqual(client.meta.region_name, 'aws-us-gov-global')\n", "tests/unit/test_validate.py": "import decimal\nimport io\nfrom datetime import datetime\n\nfrom botocore.model import ShapeResolver\nfrom botocore.validate import ParamValidator\nfrom tests import unittest\n\nBOILER_PLATE_SHAPES = {'StringType': {'type': 'string'}}\n\n\nclass BaseTestValidate(unittest.TestCase):\n    def assert_has_validation_errors(self, given_shapes, input_params, errors):\n        # Given the shape definitions ``given_shape`` and the user input\n        # parameters ``input_params``, verify that the validation has\n        # validation errors containing the list of ``errors``.\n        # Also, this assumes the input shape name is \"Input\".\n        errors_found = self.get_validation_error_message(\n            given_shapes, input_params\n        )\n        self.assertTrue(errors_found.has_errors())\n        error_message = errors_found.generate_report()\n        for error in errors:\n            self.assertIn(error, error_message)\n\n    def get_validation_error_message(self, given_shapes, input_params):\n        s = ShapeResolver(given_shapes)\n        input_shape = s.get_shape_by_name('Input')\n        validator = ParamValidator()\n        errors_found = validator.validate(input_params, input_shape)\n        return errors_found\n\n\nclass TestValidateRequiredParams(BaseTestValidate):\n    def test_validate_required_params(self):\n        self.assert_has_validation_errors(\n            given_shapes={\n                'Input': {\n                    'type': 'structure',\n                    'required': ['A', 'B'],\n                    'members': {\n                        'A': {'shape': 'StringType'},\n                        'B': {'shape': 'StringType'},\n                    },\n                },\n                'StringType': {'type': 'string'},\n            },\n            input_params={'A': 'foo'},\n            errors=['Missing required parameter'],\n        )\n\n    def test_validate_nested_required_param(self):\n        self.assert_has_validation_errors(\n            given_shapes={\n                'Input': {\n                    'type': 'structure',\n                    'members': {'A': {'shape': 'SubStruct'}},\n                },\n                'SubStruct': {\n                    'type': 'structure',\n                    'required': ['B', 'C'],\n                    'members': {\n                        'B': {'shape': 'StringType'},\n                        'C': {'shape': 'StringType'},\n                    },\n                },\n                'StringType': {\n                    'type': 'string',\n                },\n            },\n            input_params={'A': {'B': 'foo'}},\n            errors=['Missing required parameter'],\n        )\n\n    def test_validate_unknown_param(self):\n        self.assert_has_validation_errors(\n            given_shapes={\n                'Input': {\n                    'type': 'structure',\n                    'required': ['A'],\n                    'members': {\n                        'A': {'shape': 'StringType'},\n                    },\n                },\n                'StringType': {'type': 'string'},\n            },\n            input_params={'A': 'foo', 'B': 'bar'},\n            errors=['Unknown parameter'],\n        )\n\n\nclass TestValidateJSONValueTrait(BaseTestValidate):\n    def test_accepts_jsonvalue_string(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'json': {\n                        'shape': 'StrType',\n                        'jsonvalue': True,\n                        'location': 'header',\n                        'locationName': 'header-name',\n                    }\n                },\n            },\n            'StrType': {'type': 'string'},\n        }\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={\n                'json': {'data': [1, 2.3, '3'], 'unicode': '\\u2713'}\n            },\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_validate_jsonvalue_string(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'json': {\n                        'shape': 'StrType',\n                        'jsonvalue': True,\n                        'location': 'header',\n                        'locationName': 'header-name',\n                    }\n                },\n            },\n            'StrType': {'type': 'string'},\n        }\n\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={'json': {'date': datetime(2017, 4, 27, 0, 0)}},\n            errors=[('Invalid parameter json must be json serializable: ')],\n        )\n\n\nclass TestValidateDocumentType(BaseTestValidate):\n    def test_accepts_document_type_string(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'inlineDocument': {\n                        'shape': 'DocumentType',\n                    }\n                },\n            },\n            'DocumentType': {'type': 'structure', 'document': True},\n        }\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={\n                'inlineDocument': {\n                    'data': [1, 2.3, '3', {'foo': None}],\n                    'unicode': '\\u2713',\n                }\n            },\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_validate_document_type_string(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'inlineDocument': {\n                        'shape': 'DocumentType',\n                    }\n                },\n            },\n            'DocumentType': {'type': 'structure', 'document': True},\n        }\n\n        invalid_document = object()\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={\n                'inlineDocument': {\n                    'number': complex(1j),\n                    'date': datetime(2017, 4, 27, 0, 0),\n                    'list': [invalid_document],\n                    'dict': {'foo': (1, 2, 3)},\n                }\n            },\n            errors=[\n                ('Invalid type for document parameter number'),\n                ('Invalid type for document parameter date'),\n                ('Invalid type for document parameter list[0]'),\n                ('Invalid type for document parameter foo'),\n            ],\n        )\n\n\nclass TestValidateTaggedUnion(BaseTestValidate):\n    def test_accepts_one_member(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'taggedUnion': {\n                        'shape': 'TaggedUnionType',\n                    }\n                },\n            },\n            'TaggedUnionType': {\n                'type': 'structure',\n                'union': True,\n                'members': {\n                    'Foo': {'shape': 'StringType'},\n                    'Bar': {'shape': 'StringType'},\n                },\n            },\n            'StringType': {'type': 'string'},\n        }\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={'taggedUnion': {'Foo': \"mystring\"}},\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_validate_one_member_is_set(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'taggedUnion': {\n                        'shape': 'TaggedUnionType',\n                    }\n                },\n            },\n            'TaggedUnionType': {\n                'type': 'structure',\n                'union': True,\n                'members': {\n                    'Foo': {'shape': 'StringType'},\n                    'Bar': {'shape': 'StringType'},\n                },\n            },\n            'StringType': {'type': 'string'},\n        }\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={\n                'taggedUnion': {'Foo': \"mystring\", 'Bar': \"mystring2\"}\n            },\n        )\n        error_msg = errors.generate_report()\n        self.assertIn(\n            'Invalid number of parameters set for tagged union structure',\n            error_msg,\n        )\n\n    def test_validate_known_member_is_set(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'taggedUnion': {\n                        'shape': 'TaggedUnionType',\n                    }\n                },\n            },\n            'TaggedUnionType': {\n                'type': 'structure',\n                'union': True,\n                'members': {\n                    'Foo': {'shape': 'StringType'},\n                    'Bar': {'shape': 'StringType'},\n                },\n            },\n            'StringType': {'type': 'string'},\n        }\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={'taggedUnion': {'unknown': \"mystring\"}},\n        )\n        error_msg = errors.generate_report()\n        self.assertIn('Unknown parameter in taggedUnion', error_msg)\n\n    def test_validate_structure_is_not_empty(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'taggedUnion': {\n                        'shape': 'TaggedUnionType',\n                    }\n                },\n            },\n            'TaggedUnionType': {\n                'type': 'structure',\n                'union': True,\n                'members': {\n                    'Foo': {'shape': 'StringType'},\n                    'Bar': {'shape': 'StringType'},\n                },\n            },\n            'StringType': {'type': 'string'},\n        }\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes, input_params={'taggedUnion': {}}\n        )\n        error_msg = errors.generate_report()\n        self.assertIn('Must set one of the following keys', error_msg)\n\n\nclass TestValidateTypes(BaseTestValidate):\n    def setUp(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'Str': {'shape': 'StrType'},\n                    'Int': {'shape': 'IntType'},\n                    'Bool': {'shape': 'BoolType'},\n                    'List': {'shape': 'ListType'},\n                    'Struct': {'shape': 'StructType'},\n                    'Double': {'shape': 'DoubleType'},\n                    'Long': {'shape': 'LongType'},\n                    'Map': {'shape': 'MapType'},\n                    'Timestamp': {'shape': 'TimeType'},\n                },\n            },\n            'StrType': {'type': 'string'},\n            'IntType': {'type': 'integer'},\n            'BoolType': {'type': 'boolean'},\n            'ListType': {'type': 'list'},\n            'StructType': {'type': 'structure'},\n            'DoubleType': {'type': 'double'},\n            'LongType': {'type': 'long'},\n            'MapType': {'type': 'map'},\n            'TimeType': {'type': 'timestamp'},\n        }\n\n    def test_validate_string(self):\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={\n                'Str': 24,\n                'Int': 'notInt',\n                'Bool': 'notBool',\n                'List': 'notList',\n                'Struct': 'notDict',\n                'Double': 'notDouble',\n                'Long': 'notLong',\n                'Map': 'notDict',\n                'Timestamp': 'notTimestamp',\n            },\n            errors=[\n                'Invalid type for parameter Str',\n                'Invalid type for parameter Int',\n                'Invalid type for parameter Bool',\n                'Invalid type for parameter List',\n                'Invalid type for parameter Struct',\n                'Invalid type for parameter Double',\n                'Invalid type for parameter Long',\n                'Invalid type for parameter Map',\n                'Invalid type for parameter Timestamp',\n            ],\n        )\n\n    def test_datetime_type_accepts_datetime_obj(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={'Timestamp': datetime.now()},\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_datetime_accepts_string_timestamp(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={'Timestamp': '2014-01-01 12:00:00'},\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_can_handle_none_datetimes(self):\n        # This is specifically to test a workaround a bug in dateutil\n        # where low level exceptions can propogate back up to\n        # us.\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes, input_params={'Timestamp': None}\n        )\n        error_msg = errors.generate_report()\n        self.assertIn('Invalid type for parameter Timestamp', error_msg)\n\n\nclass TestValidateRanges(BaseTestValidate):\n    def setUp(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'Int': {'shape': 'IntType'},\n                    'Long': {'shape': 'IntType'},\n                    'String': {'shape': 'StringType'},\n                    'List': {'shape': 'ListType'},\n                    'OnlyMin': {'shape': 'MinStrOnly'},\n                    'OnlyMax': {'shape': 'MaxStrOnly'},\n                },\n            },\n            'IntType': {\n                'type': 'integer',\n                'min': 0,\n                'max': 1000,\n            },\n            'LongType': {\n                'type': 'long',\n                'min': 0,\n                'max': 1000,\n            },\n            'StringType': {\n                'type': 'string',\n                'min': 1,\n                'max': 10,\n            },\n            'MinStrOnly': {'type': 'string', 'min': 1},\n            'MaxStrOnly': {'type': 'string', 'max': 10},\n            'ListType': {\n                'type': 'list',\n                'min': 1,\n                'max': 5,\n                'member': {'shape': 'StringType'},\n            },\n        }\n\n    def test_less_than_range(self):\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={\n                'Int': -10,\n                'Long': -10,\n            },\n            errors=[\n                'Invalid value for parameter Int',\n                'Invalid value for parameter Long',\n            ],\n        )\n\n    def test_does_not_validate_greater_than_range(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={\n                'Int': 100000000,\n                'Long': 100000000,\n            },\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_within_range(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes, input_params={'Int': 10}\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_string_min_length_contraint(self):\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={\n                'String': '',\n            },\n            errors=[\n                'Invalid length for parameter String',\n            ],\n        )\n\n    def test_does_not_validate_string_max_length_contraint(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={\n                'String': 'more than ten characters',\n            },\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_list_min_length_constraint(self):\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={\n                'List': [],\n            },\n            errors=[\n                'Invalid length for parameter List',\n            ],\n        )\n\n    def test_does_not_validate_list_max_length_constraint(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={\n                'List': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'],\n            },\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_only_min_value_specified(self):\n        # min anx max don't have to both be provided.\n        # It's valid to just have min with no max, and\n        # vice versa.\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={\n                'OnlyMin': '',\n            },\n            errors=[\n                'Invalid length for parameter OnlyMin',\n            ],\n        )\n\n    def test_does_not_validate_max_when_only_max_value_specified(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={\n                'OnlyMax': 'more than ten characters',\n            },\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n\nclass TestValidateMapType(BaseTestValidate):\n    def setUp(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'Map': {'shape': 'MapType'},\n                },\n            },\n            'MapType': {\n                'type': 'map',\n                'key': {'shape': 'StringType'},\n                'value': {'shape': 'StringType'},\n            },\n            'StringType': {\n                'type': 'string',\n                'min': 2,\n            },\n        }\n\n    def test_validate_keys_and_values(self):\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={'Map': {'foo': '', 'a': 'foobar'}},\n            errors=[\n                'Invalid length for parameter Map',\n            ],\n        )\n\n\nclass TestValidationFloatType(BaseTestValidate):\n    def setUp(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'Float': {'shape': 'FloatType'},\n                },\n            },\n            'FloatType': {\n                'type': 'float',\n                'min': 2,\n                'max': 5,\n            },\n        }\n\n    def test_range_float(self):\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={\n                'Float': 1,\n            },\n            errors=[\n                'Invalid value for parameter Float',\n            ],\n        )\n\n    def test_decimal_allowed(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={'Float': decimal.Decimal('2.12345')},\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_decimal_still_validates_range(self):\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={\n                'Float': decimal.Decimal('1'),\n            },\n            errors=[\n                'Invalid value for parameter Float',\n            ],\n        )\n\n\nclass TestValidateTypeBlob(BaseTestValidate):\n    def setUp(self):\n        self.shapes = {\n            'Input': {\n                'type': 'structure',\n                'members': {\n                    'Blob': {'shape': 'BlobType'},\n                },\n            },\n            'BlobType': {\n                'type': 'blob',\n                'min': 2,\n                'max': 5,\n            },\n        }\n\n    def test_validates_bytes(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes, input_params={'Blob': b'12345'}\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_validates_bytearray(self):\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={'Blob': bytearray(b'12345')},\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_validates_file_like_object(self):\n        value = io.BytesIO(b'foo')\n\n        errors = self.get_validation_error_message(\n            given_shapes=self.shapes,\n            input_params={'Blob': value},\n        )\n        error_msg = errors.generate_report()\n        self.assertEqual(error_msg, '')\n\n    def test_validate_type(self):\n        self.assert_has_validation_errors(\n            given_shapes=self.shapes,\n            input_params={\n                'Blob': 24,\n            },\n            errors=[\n                'Invalid type for parameter Blob',\n            ],\n        )\n", "tests/unit/test_translate.py": "# Copyright 2012-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore import translate\nfrom tests import unittest\n\n\nclass TestBuildRetryConfig(unittest.TestCase):\n    def setUp(self):\n        self.retry = {\n            \"definitions\": {\"def_name\": {\"from\": {\"definition\": \"file\"}}},\n            \"retry\": {\n                \"__default__\": {\n                    \"max_attempts\": 5,\n                    \"delay\": \"global_delay\",\n                    \"policies\": {\n                        \"global_one\": \"global\",\n                        \"override_me\": \"global\",\n                    },\n                },\n                \"sts\": {\n                    \"__default__\": {\n                        \"delay\": \"service_specific_delay\",\n                        \"policies\": {\n                            \"service_one\": \"service\",\n                            \"override_me\": \"service\",\n                        },\n                    },\n                    \"AssumeRole\": {\n                        \"policies\": {\n                            \"name\": \"policy\",\n                            \"other\": {\"$ref\": \"def_name\"},\n                        }\n                    },\n                },\n            },\n        }\n\n    def test_inject_retry_config(self):\n        retry = translate.build_retry_config(\n            'sts', self.retry['retry'], self.retry['definitions']\n        )\n        self.assertIn('__default__', retry)\n        self.assertEqual(\n            retry['__default__'],\n            {\n                \"max_attempts\": 5,\n                \"delay\": \"service_specific_delay\",\n                \"policies\": {\n                    \"global_one\": \"global\",\n                    \"override_me\": \"service\",\n                    \"service_one\": \"service\",\n                },\n            },\n        )\n        # Policies should be merged.\n        operation_config = retry['AssumeRole']\n        self.assertEqual(operation_config['policies']['name'], 'policy')\n\n    def test_resolve_reference(self):\n        retry = translate.build_retry_config(\n            'sts', self.retry['retry'], self.retry['definitions']\n        )\n        operation_config = retry['AssumeRole']\n        # And we should resolve references.\n        self.assertEqual(\n            operation_config['policies']['other'],\n            {\"from\": {\"definition\": \"file\"}},\n        )\n\n    def test_service_specific_defaults_no_mutate_default_retry(self):\n        retry = translate.build_retry_config(\n            'sts', self.retry['retry'], self.retry['definitions']\n        )\n        # sts has a specific policy\n        self.assertEqual(\n            retry['__default__'],\n            {\n                \"max_attempts\": 5,\n                \"delay\": \"service_specific_delay\",\n                \"policies\": {\n                    \"global_one\": \"global\",\n                    \"override_me\": \"service\",\n                    \"service_one\": \"service\",\n                },\n            },\n        )\n\n        # The general defaults for the upstream model should not have been\n        # mutated from building the retry config\n        self.assertEqual(\n            self.retry['retry']['__default__'],\n            {\n                \"max_attempts\": 5,\n                \"delay\": \"global_delay\",\n                \"policies\": {\n                    \"global_one\": \"global\",\n                    \"override_me\": \"global\",\n                },\n            },\n        )\n\n    def test_client_override_max_attempts(self):\n        retry = translate.build_retry_config(\n            'sts',\n            self.retry['retry'],\n            self.retry['definitions'],\n            client_retry_config={'max_attempts': 9},\n        )\n        self.assertEqual(retry['__default__']['max_attempts'], 10)\n        # But it should not mutate the original retry model\n        self.assertEqual(self.retry['retry']['__default__']['max_attempts'], 5)\n", "tests/unit/test_client.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport io\nfrom contextlib import closing\n\nimport botocore\nimport botocore.config\nfrom botocore import client, exceptions, hooks\nfrom botocore.auth import AUTH_TYPE_MAPS, BaseSigner\nfrom botocore.client import ClientEndpointBridge\nfrom botocore.configprovider import (\n    ChainProvider,\n    ConfigValueStore,\n    EnvironmentProvider,\n)\nfrom botocore.credentials import Credentials\nfrom botocore.endpoint import DEFAULT_TIMEOUT\nfrom botocore.errorfactory import ClientExceptionsFactory\nfrom botocore.exceptions import (\n    InvalidMaxRetryAttemptsError,\n    InvalidRetryConfigurationError,\n    InvalidRetryModeError,\n    InvalidS3AddressingStyleError,\n    ParamValidationError,\n    UnknownSignatureVersionError,\n)\nfrom botocore.stub import Stubber\nfrom botocore.useragent import UserAgentString\nfrom tests import mock, unittest\n\n\nclass TestAutoGeneratedClient(unittest.TestCase):\n    def setUp(self):\n        self.service_description = {\n            'metadata': {\n                'serviceFullName': 'AWS MyService',\n                'apiVersion': '2014-01-01',\n                'endpointPrefix': 'myservice',\n                'signatureVersion': 'v4',\n                'protocol': 'query',\n                'serviceId': 'MyService',\n            },\n            'operations': {\n                'TestOperation': {\n                    'name': 'TestOperation',\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'input': {'shape': 'TestOperationRequest'},\n                    'errors': [{'shape': 'TestOperationException'}],\n                    'documentation': 'Documents TestOperation',\n                }\n            },\n            'shapes': {\n                'TestOperationRequest': {\n                    'type': 'structure',\n                    'required': ['Foo'],\n                    'members': {\n                        'Foo': {\n                            'shape': 'StringType',\n                            'documentation': 'Documents Foo',\n                        },\n                        'Bar': {\n                            'shape': 'StringType',\n                            'documentation': 'Documents Bar',\n                        },\n                    },\n                },\n                \"TestOperationException\": {\n                    'type': 'structure',\n                    'exception': True,\n                    'error': {'code': 'TestOperationErrorCode'},\n                },\n                'StringType': {'type': 'string'},\n            },\n        }\n        self.endpoint_ruleset = {\n            \"version\": \"1.0\",\n            \"parameters\": {},\n            \"rules\": [\n                {\n                    \"conditions\": [],\n                    \"type\": \"endpoint\",\n                    \"endpoint\": {\n                        \"url\": \"https://foo.bar\",\n                        \"properties\": {},\n                        \"headers\": {},\n                    },\n                }\n            ],\n        }\n        self.retry_config = {\n            \"retry\": {\n                \"__default__\": {\n                    \"max_attempts\": 5,\n                    \"delay\": {\n                        \"type\": \"exponential\",\n                        \"base\": \"rand\",\n                        \"growth_factor\": 2,\n                    },\n                    \"policies\": {},\n                }\n            }\n        }\n\n        def load_service_mock(*args, **kwargs):\n            if args[1] == \"service-2\":\n                return self.service_description\n\n        self.loader = mock.Mock()\n        self.loader.load_service_model.side_effect = load_service_mock\n        self.loader.load_data.return_value = self.retry_config\n\n        self.credentials = Credentials('access-key', 'secret-key')\n\n        self.endpoint_creator_patch = mock.patch(\n            'botocore.args.EndpointCreator'\n        )\n        self.endpoint_creator_cls = self.endpoint_creator_patch.start()\n        self.endpoint_creator = self.endpoint_creator_cls.return_value\n\n        self.endpoint = mock.Mock()\n        self.endpoint.host = 'https://myservice.amazonaws.com'\n        self.endpoint.make_request.return_value = (\n            mock.Mock(status_code=200),\n            {},\n        )\n        self.endpoint_creator.create_endpoint.return_value = self.endpoint\n\n        self.resolver = mock.Mock()\n        self.endpoint_data = {\n            'partition': 'aws',\n            'hostname': 'foo',\n            'endpointName': 'us-west-2',\n            'signatureVersions': ['v4'],\n        }\n        self.resolver.construct_endpoint.return_value = self.endpoint_data\n        self.resolver.get_available_endpoints.return_value = ['us-west-2']\n        self.config_store = ConfigValueStore()\n\n    def tearDown(self):\n        self.endpoint_creator_patch.stop()\n\n    def create_mock_emitter(self, responses=None):\n        if responses is None:\n            responses = []\n\n        emitter = mock.Mock()\n        emitter.emit.return_value = responses\n        return emitter\n\n    def create_client_creator(\n        self,\n        endpoint_creator=None,\n        event_emitter=None,\n        retry_handler_factory=None,\n        retry_config_translator=None,\n        response_parser_factory=None,\n        endpoint_prefix=None,\n        exceptions_factory=None,\n        config_store=None,\n        user_agent_creator=None,\n    ):\n        if event_emitter is None:\n            event_emitter = hooks.HierarchicalEmitter()\n        if retry_handler_factory is None:\n            retry_handler_factory = botocore.retryhandler\n        if retry_config_translator is None:\n            retry_config_translator = botocore.translate\n        if endpoint_prefix is not None:\n            self.service_description['metadata'][\n                'endpointPrefix'\n            ] = endpoint_prefix\n\n        if endpoint_creator is not None:\n            self.endpoint_creator_cls.return_value = endpoint_creator\n        if exceptions_factory is None:\n            exceptions_factory = ClientExceptionsFactory()\n        if config_store is None:\n            config_store = self.config_store\n        if user_agent_creator is None:\n            user_agent_creator = (\n                UserAgentString.from_environment().set_session_config(\n                    session_user_agent_name='MyUserAgent',\n                    session_user_agent_version='1.2.3-rc5',\n                    session_user_agent_extra=None,\n                )\n            )\n        creator = client.ClientCreator(\n            self.loader,\n            self.resolver,\n            'user-agent',\n            event_emitter,\n            retry_handler_factory,\n            retry_config_translator,\n            response_parser_factory,\n            exceptions_factory,\n            config_store,\n            user_agent_creator,\n        )\n        return creator\n\n    def assert_no_param_error_raised(self, client):\n        try:\n            self.make_api_call_with_missing_param(client)\n        except ParamValidationError:\n            self.fail(\n                \"ParamValidationError shouldn't be raised \"\n                \"with validation disabled\"\n            )\n\n    def make_api_call_with_missing_param(self, service_client):\n        # Missing required 'Foo' param.\n        service_client.test_operation(Bar='two')\n\n    def test_client_name(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertTrue(service_client.__class__.__name__, 'MyService')\n\n    def test_client_name_with_amazon(self):\n        self.service_description['metadata'][\n            'serviceFullName'\n        ] = 'Amazon MyService'\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertTrue(service_client.__class__.__name__, 'MyService')\n\n    def test_client_name_using_abreviation(self):\n        self.service_description['metadata'][\n            'serviceAbbreviation'\n        ] = 'Abbreviation'\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertTrue(service_client.__class__.__name__, 'Abbreviation')\n\n    def test_client_name_with_non_alphabet_characters(self):\n        self.service_description['metadata'][\n            'serviceFullName'\n        ] = 'Amazon My-Service'\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertTrue(service_client.__class__.__name__, 'MyService')\n\n    def test_client_name_with_no_full_name_or_abbreviation(self):\n        del self.service_description['metadata']['serviceFullName']\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertTrue(service_client.__class__.__name__, 'myservice')\n\n    def test_client_generated_from_model(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertTrue(hasattr(service_client, 'test_operation'))\n\n    def test_client_with_nonstandard_signature_version(self):\n        self.service_description['metadata']['signatureVersion'] = 'foo'\n        creator = self.create_client_creator()\n        foo_signer = mock.Mock(spec=BaseSigner)\n\n        auth_types = AUTH_TYPE_MAPS.copy()\n        auth_types['foo'] = foo_signer\n\n        with mock.patch('botocore.client.AUTH_TYPE_MAPS', auth_types):\n            service_client = creator.create_client(\n                'myservice', 'us-west-2', credentials=self.credentials\n            )\n        assert service_client.meta.config.signature_version == 'foo'\n\n    def test_client_method_docstring(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        method_docstring = str(service_client.test_operation.__doc__)\n        ref_docstring_lines = [\n            'Documents TestOperation',\n            '**Request Syntax**',\n            '  response = client.test_operation(',\n            '      Bar=\\'string\\'',\n            '      Foo=\\'string\\'',\n            '  )',\n            ':type Bar: string',\n            ':param Bar: Documents Bar',\n            ':type Foo: string',\n            ':param Foo: **[REQUIRED]** Documents Foo',\n        ]\n        for line in ref_docstring_lines:\n            self.assertIn(line, method_docstring)\n\n    def test_client_method_help(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        with mock.patch('sys.stdout', io.StringIO()) as mock_stdout:\n            help(service_client.test_operation)\n        method_docstring = mock_stdout.getvalue()\n        ref_docstring_lines = [\n            'Documents TestOperation',\n            '**Request Syntax**',\n            '  response = client.test_operation(',\n            '      Bar=\\'string\\'',\n            '      Foo=\\'string\\'',\n            '  )',\n            ':type Bar: string',\n            ':param Bar: Documents Bar',\n            ':type Foo: string',\n            ':param Foo: **[REQUIRED]** Documents Foo',\n        ]\n        for line in ref_docstring_lines:\n            self.assertIn(line, method_docstring)\n\n    def test_client_create_unicode(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertTrue(hasattr(service_client, 'test_operation'))\n\n    def test_client_has_region_name_on_meta(self):\n        creator = self.create_client_creator()\n        region_name = 'us-west-2'\n        self.endpoint.region_name = region_name\n        service_client = creator.create_client(\n            'myservice', region_name, credentials=self.credentials\n        )\n        self.assertEqual(service_client.meta.region_name, region_name)\n\n    def test_client_has_endpoint_url_on_meta(self):\n        creator = self.create_client_creator()\n        self.endpoint.host = 'https://foo.bar'\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertEqual(service_client.meta.endpoint_url, 'https://foo.bar')\n\n    def test_client_has_standard_partition_on_meta(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertEqual(service_client.meta.partition, 'aws')\n\n    def test_client_has_non_standard_partition_on_meta(self):\n        creator = self.create_client_creator()\n        self.resolver.construct_endpoint.return_value = {\n            'partition': 'aws-cn',\n            'hostname': 'foo',\n            'endpointName': 'cn-north-1',\n            'signatureVersions': ['v4'],\n        }\n        service_client = creator.create_client(\n            'myservice', 'cn-north-1', credentials=self.credentials\n        )\n        self.assertEqual(service_client.meta.partition, 'aws-cn')\n\n    def test_client_has_exceptions_attribute(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertTrue(hasattr(service_client, 'exceptions'))\n\n    def test_client_has_modeled_exceptions(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertTrue(\n            issubclass(\n                service_client.exceptions.TestOperationException,\n                client.ClientError,\n            )\n        )\n\n    def test_client_fips_region_transformation(self):\n        creator = self.create_client_creator()\n        with self.assertLogs('botocore.client', level='WARNING') as log:\n            creator.create_client(\n                'myservice', 'fips-us-west-2', credentials=self.credentials\n            )\n            self.assertIn('fips-us-west-2 to us-west-2', log.output[0])\n\n    def test_api_version_is_passed_to_loader_if_provided(self):\n        creator = self.create_client_creator()\n        self.endpoint.host = 'https://foo.bar'\n        specific_api_version = '2014-03-01'\n        creator.create_client(\n            'myservice',\n            'us-west-2',\n            credentials=self.credentials,\n            api_version=specific_api_version,\n        )\n        calls = [\n            mock.call(\n                'myservice', 'service-2', api_version=specific_api_version\n            ),\n            mock.call(\n                'myservice',\n                'endpoint-rule-set-1',\n                api_version=specific_api_version,\n            ),\n        ]\n        self.loader.load_service_model.assert_has_calls(calls)\n\n    def test_create_client_class_creates_class(self):\n        creator = self.create_client_creator()\n        client_class = creator.create_client_class('myservice')\n        self.assertTrue(hasattr(client_class, 'test_operation'))\n\n    def test_create_client_class_forwards_api_version(self):\n        creator = self.create_client_creator()\n        specific_api_version = '2014-03-01'\n        creator.create_client_class(\n            'myservice', api_version=specific_api_version\n        )\n        self.loader.load_service_model.assert_called_with(\n            'myservice', 'service-2', api_version=specific_api_version\n        )\n\n    def test_signing_region_does_not_change_client_region(self):\n        with mock.patch('botocore.args.RequestSigner') as mock_signer:\n            credential_scope_region = 'us-east-1'\n            self.resolver.construct_endpoint.return_value = {\n                'partition': 'aws',\n                'hostname': 'endpoint.url',\n                'endpointName': 'us-west-2',\n                'signatureVersions': ['v4'],\n                'credentialScope': {'region': credential_scope_region},\n            }\n            creator = self.create_client_creator()\n            service_client = creator.create_client(\n                'myservice', 'us-west-2', credentials=self.credentials\n            )\n            self.assertEqual(service_client.meta.region_name, 'us-west-2')\n            call_args = mock_signer.call_args\n            self.assertEqual(credential_scope_region, call_args[0][1])\n\n    def test_client_uses_signing_region_from_credential_scope(self):\n        with mock.patch('botocore.args.RequestSigner') as mock_signer:\n            credential_scope_region = 'us-east-1'\n            self.resolver.construct_endpoint.return_value = {\n                'partition': 'aws',\n                'endpointName': 'us-west-2',\n                'hostname': 'endpoint.url',\n                'signatureVersions': ['v4'],\n                'credentialScope': {'region': credential_scope_region},\n            }\n            creator = self.create_client_creator()\n            service_client = creator.create_client(\n                service_name='myservice',\n                region_name='us-west-2',\n                credentials=self.credentials,\n            )\n            # Use the resolved region as the region value.\n            self.assertEqual(service_client.meta.region_name, 'us-west-2')\n            # Ensure that we use the credential scope region for signing,\n            # and not the resolved region name.\n            call_args = mock_signer.call_args\n            self.assertEqual(credential_scope_region, call_args[0][1])\n\n    def test_client_uses_signing_name_from_credential_scope(self):\n        with mock.patch('botocore.args.RequestSigner') as mock_signer:\n            self.resolver.construct_endpoint.return_value = {\n                'partition': 'aws',\n                'endpointName': 'us-west-2',\n                'hostname': 'endpoint.url',\n                'signatureVersions': ['v4'],\n                'credentialScope': {'service': 'override'},\n            }\n            creator = self.create_client_creator()\n            creator.create_client(\n                service_name='myservice',\n                region_name='us-west-2',\n                credentials=self.credentials,\n            )\n            call_args = mock_signer.call_args\n            self.assertEqual('MyService', call_args[0][0])\n            self.assertEqual('override', call_args[0][2])\n\n    def test_client_uses_given_region_name_and_endpoint_url_when_present(self):\n        with mock.patch('botocore.args.RequestSigner') as mock_signer:\n            credential_scope_region = 'us-east-1'\n            self.resolver.construct_endpoint.return_value = {\n                'partition': 'aws',\n                'endpointName': 'us-west-2',\n                'hostname': 'endpoint.url',\n                'signatureVersions': ['v4'],\n                'credentialScope': {'region': credential_scope_region},\n            }\n            creator = self.create_client_creator()\n            service_client = creator.create_client(\n                service_name='myservice',\n                region_name='us-west-2',\n                credentials=self.credentials,\n                endpoint_url='https://foo',\n            )\n            self.assertEqual(service_client.meta.region_name, 'us-west-2')\n            call_args = mock_signer.call_args\n            self.assertEqual('us-west-2', call_args[0][1])\n\n    def test_client_uses_signing_name_from_model_if_present_if_resolved(self):\n        self.service_description['metadata']['signingName'] = 'otherName'\n        with mock.patch('botocore.args.RequestSigner') as mock_signer:\n            self.resolver.construct_endpoint.return_value = {\n                'partition': 'aws',\n                'endpointName': 'us-west-2',\n                'hostname': 'endpoint.url',\n                'signatureVersions': ['v4'],\n            }\n            creator = self.create_client_creator()\n            service_client = creator.create_client(\n                service_name='myservice',\n                region_name='us-west-2',\n                credentials=self.credentials,\n                endpoint_url='https://foo',\n            )\n            self.assertEqual(service_client.meta.region_name, 'us-west-2')\n            call_args = mock_signer.call_args[0]\n            self.assertEqual('otherName', call_args[2])\n\n    def test_client_uses_signing_name_even_with_no_resolve(self):\n        self.service_description['metadata']['signingName'] = 'otherName'\n        with mock.patch('botocore.args.RequestSigner') as mock_signer:\n            self.resolver.construct_endpoint.return_value = {}\n            creator = self.create_client_creator()\n            service_client = creator.create_client(\n                service_name='myservice',\n                region_name='us-west-2',\n                credentials=self.credentials,\n                endpoint_url='https://foo',\n            )\n            self.assertEqual(service_client.meta.region_name, 'us-west-2')\n            call_args = mock_signer.call_args[0]\n            self.assertEqual('otherName', call_args[2])\n\n    @mock.patch('botocore.args.RequestSigner')\n    def test_client_signature_no_override(self, request_signer):\n        creator = self.create_client_creator()\n        creator.create_client(\n            'myservice',\n            'us-west-2',\n            credentials=self.credentials,\n            scoped_config={},\n        )\n        request_signer.assert_called_with(\n            mock.ANY,\n            mock.ANY,\n            mock.ANY,\n            'v4',\n            mock.ANY,\n            mock.ANY,\n            mock.ANY,\n        )\n\n    @mock.patch('botocore.args.RequestSigner')\n    def test_client_signature_override_config_file(self, request_signer):\n        creator = self.create_client_creator()\n        config = {'myservice': {'signature_version': 'foo'}}\n        creator.create_client(\n            'myservice',\n            'us-west-2',\n            credentials=self.credentials,\n            scoped_config=config,\n        )\n        request_signer.assert_called_with(\n            mock.ANY,\n            mock.ANY,\n            mock.ANY,\n            'foo',\n            mock.ANY,\n            mock.ANY,\n            mock.ANY,\n        )\n\n    @mock.patch('botocore.args.RequestSigner')\n    def test_client_signature_override_arg(self, request_signer):\n        creator = self.create_client_creator()\n        config = botocore.config.Config(signature_version='foo')\n        creator.create_client(\n            'myservice',\n            'us-west-2',\n            credentials=self.credentials,\n            client_config=config,\n        )\n        request_signer.assert_called_with(\n            mock.ANY,\n            mock.ANY,\n            mock.ANY,\n            'foo',\n            mock.ANY,\n            mock.ANY,\n            mock.ANY,\n        )\n\n    def test_client_method_to_api_mapping(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        self.assertEqual(\n            service_client.meta.method_to_api_mapping,\n            {'test_operation': 'TestOperation'},\n        )\n\n    def test_anonymous_client_request(self):\n        creator = self.create_client_creator()\n        config = botocore.config.Config(signature_version=botocore.UNSIGNED)\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', client_config=config\n        )\n\n        service_client.test_operation(Foo='one')\n\n        # Make sure a request has been attempted\n        self.assertTrue(self.endpoint.make_request.called)\n\n        # Make sure the request parameters do NOT include auth\n        # information. The service defined above for these tests\n        # uses sigv4 by default (which we disable).\n        params = {\n            k.lower(): v\n            for k, v in self.endpoint.make_request.call_args[0][1].items()\n        }\n        self.assertNotIn('authorization', params)\n        self.assertNotIn('x-amz-signature', params)\n\n    def test_client_user_agent_in_request(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n\n        service_client.test_operation(Foo='one')\n\n        self.assertTrue(self.endpoint.make_request.called)\n        params = {\n            k.lower(): v\n            for k, v in self.endpoint.make_request.call_args[0][1].items()\n        }\n        self.assertIn('MyUserAgent/1.2.3', params['headers']['User-Agent'])\n\n    def test_client_custom_user_agent_in_request(self):\n        creator = self.create_client_creator()\n        config = botocore.config.Config(user_agent='baz')\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', client_config=config\n        )\n\n        service_client.test_operation(Foo='one')\n\n        self.assertTrue(self.endpoint.make_request.called)\n        params = {\n            k.lower(): v\n            for k, v in self.endpoint.make_request.call_args[0][1].items()\n        }\n        self.assertEqual(params['headers']['User-Agent'], 'baz')\n\n    def test_client_custom_user_agent_extra_in_request(self):\n        creator = self.create_client_creator()\n        config = botocore.config.Config(user_agent_extra='extrastuff')\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', client_config=config\n        )\n        service_client.test_operation(Foo='one')\n        headers = self.endpoint.make_request.call_args[0][1]['headers']\n        self.assertTrue(headers['User-Agent'].endswith('extrastuff'))\n\n    def test_client_registers_request_created_handler(self):\n        event_emitter = self.create_mock_emitter()\n        creator = self.create_client_creator(event_emitter=event_emitter)\n        creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertIn(\n            mock.call('request-created.myservice', mock.ANY),\n            event_emitter.register.call_args_list,\n        )\n\n    def test_client_makes_call(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        self.assertTrue(self.endpoint_creator.create_endpoint.called)\n\n        response = service_client.test_operation(Foo='one', Bar='two')\n        self.assertEqual(response, {})\n\n    def test_client_error_message_for_positional_args(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        with self.assertRaisesRegex(\n            TypeError, 'only accepts keyword arguments'\n        ):\n            service_client.test_operation('foo')\n\n    @mock.patch('botocore.args.RequestSigner.sign')\n    def test_client_signs_call(self, signer_mock):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        request = mock.Mock()\n\n        # Emit the request created event to see if it would be signed.\n        # We tested above to ensure this event is registered when\n        # a client is created. This prevents testing the entire client\n        # call logic.\n        service_client.meta.events.emit(\n            'request-created.myservice.test_operation',\n            request=request,\n            operation_name='test_operation',\n        )\n\n        signer_mock.assert_called_with('test_operation', request)\n\n    def test_client_validates_params_by_default(self):\n        creator = self.create_client_creator()\n\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        with self.assertRaises(ParamValidationError):\n            self.make_api_call_with_missing_param(service_client)\n\n    def test_client_doesnt_validate_params_when_validation_disabled(self):\n        creator = self.create_client_creator()\n\n        client_config = botocore.config.Config()\n        client_config.parameter_validation = False\n        service_client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            credentials=self.credentials,\n            client_config=client_config,\n        )\n\n        self.assert_no_param_error_raised(service_client)\n\n    def test_can_disable_param_validation_from_scoped_config(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            credentials=self.credentials,\n            scoped_config={'parameter_validation': False},\n        )\n        self.assert_no_param_error_raised(service_client)\n\n    def test_client_config_trumps_scoped_config(self):\n        creator = self.create_client_creator()\n        scoped_config = {'parameter_validation': True}\n        client_config = botocore.config.Config(parameter_validation=False)\n        # Client config should win and param validation is disabled.\n        service_client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            credentials=self.credentials,\n            scoped_config=scoped_config,\n            client_config=client_config,\n        )\n        self.assert_no_param_error_raised(service_client)\n\n    def test_client_with_custom_both_timeout(self):\n        self.create_client_creator().create_client(\n            'myservice',\n            'us-west-2',\n            client_config=botocore.config.Config(\n                connect_timeout=123, read_timeout=234\n            ),\n        )\n        call_kwargs = self.endpoint_creator.create_endpoint.call_args[1]\n        self.assertEqual(call_kwargs['timeout'], (123, 234))\n\n    def test_client_with_custom_connect_timeout(self):\n        self.create_client_creator().create_client(\n            'myservice',\n            'us-west-2',\n            client_config=botocore.config.Config(connect_timeout=123),\n        )\n        call_kwargs = self.endpoint_creator.create_endpoint.call_args[1]\n        self.assertEqual(call_kwargs['timeout'], (123, DEFAULT_TIMEOUT))\n\n    def test_client_with_custom_read_timeout(self):\n        self.create_client_creator().create_client(\n            'myservice',\n            'us-west-2',\n            client_config=botocore.config.Config(read_timeout=234),\n        )\n        call_kwargs = self.endpoint_creator.create_endpoint.call_args[1]\n        self.assertEqual(call_kwargs['timeout'], (DEFAULT_TIMEOUT, 234))\n\n    def test_client_with_custom_neither_timeout(self):\n        self.create_client_creator().create_client('myservice', 'us-west-2')\n        call_kwargs = self.endpoint_creator.create_endpoint.call_args[1]\n        self.assertEqual(\n            call_kwargs['timeout'], (DEFAULT_TIMEOUT, DEFAULT_TIMEOUT)\n        )\n\n    def test_client_with_custom_params(self):\n        creator = self.create_client_creator()\n        creator.create_client(\n            'myservice', 'us-west-2', is_secure=False, verify=False\n        )\n        call_kwargs = self.endpoint_creator.create_endpoint.call_args[1]\n        self.assertFalse(call_kwargs['verify'])\n        self.assertNotIn('is_secure', call_kwargs)\n\n    def test_client_with_custom_proxy_config(self):\n        proxies_config = {\n            'proxy_ca_bundle': 'foo_ca_bundle',\n            'proxy_client_cert': 'foo_cert',\n            'proxy_use_forwarding_for_https': False,\n        }\n        self.create_client_creator().create_client(\n            'myservice',\n            'us-west-2',\n            client_config=botocore.config.Config(\n                proxies_config=proxies_config\n            ),\n        )\n        call_kwargs = self.endpoint_creator.create_endpoint.call_args[1]\n        self.assertEqual(call_kwargs['proxies_config'], proxies_config)\n\n    def test_client_with_endpoint_url(self):\n        creator = self.create_client_creator()\n        creator.create_client(\n            'myservice', 'us-west-2', endpoint_url='http://custom.foo'\n        )\n        call_kwargs = self.endpoint_creator.create_endpoint.call_args[1]\n        self.assertEqual(call_kwargs['endpoint_url'], 'http://custom.foo')\n\n    def test_client_can_use_guessed_endpoints(self):\n        # Ensure the resolver returns None (meaning a guess is made)\n        self.resolver.construct_endpoint.return_value = None\n        creator = self.create_client_creator()\n        client = creator.create_client('myservice', region_name='invalid')\n        self.assertEqual('invalid', client.meta.region_name)\n\n    def test_client_with_response_parser_factory(self):\n        factory = mock.Mock()\n        creator = self.create_client_creator(response_parser_factory=factory)\n        creator.create_client('myservice', 'us-west-2')\n        call_kwargs = self.endpoint_creator.create_endpoint.call_args[1]\n        self.assertEqual(call_kwargs['response_parser_factory'], factory)\n\n    def test_operation_cannot_paginate(self):\n        pagination_config = {\n            'pagination': {\n                # Note that there's no pagination config for\n                # 'TestOperation', indicating that TestOperation\n                # is not pageable.\n                'SomeOtherOperation': {\n                    \"input_token\": \"Marker\",\n                    \"output_token\": \"Marker\",\n                    \"more_results\": \"IsTruncated\",\n                    \"limit_key\": \"MaxItems\",\n                    \"result_key\": \"Users\",\n                }\n            }\n        }\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            pagination_config,\n        ]\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        self.assertFalse(service_client.can_paginate('test_operation'))\n\n    def test_operation_can_paginate(self):\n        pagination_config = {\n            'pagination': {\n                'TestOperation': {\n                    \"input_token\": \"Marker\",\n                    \"output_token\": \"Marker\",\n                    \"more_results\": \"IsTruncated\",\n                    \"limit_key\": \"MaxItems\",\n                    \"result_key\": \"Users\",\n                }\n            }\n        }\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            pagination_config,\n        ]\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        self.assertTrue(service_client.can_paginate('test_operation'))\n        # Also, the config is cached, but we want to make sure we get\n        # the same answer when we ask again.\n        self.assertTrue(service_client.can_paginate('test_operation'))\n\n    def test_service_has_no_pagination_configs(self):\n        # This is the case where there is an actual *.paginator.json, file,\n        # but the specific operation itself is not actually pageable.\n        # If the loader cannot load pagination configs, it communicates\n        # this by raising a DataNotFoundError.\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            exceptions.DataNotFoundError(data_path='/foo'),\n        ]\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        self.assertFalse(service_client.can_paginate('test_operation'))\n\n    def test_waiter_config_uses_service_name_not_endpoint_prefix(self):\n        waiter_config = {'version': 2, 'waiters': {}}\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            waiter_config,\n        ]\n        creator = self.create_client_creator()\n        # We're going to verify that the loader loads a service called\n        # 'other-service-name', and even though the endpointPrefix is\n        # 'myservice', we use 'other-service-name' for waiters/paginators, etc.\n        service_client = creator.create_client(\n            'other-service-name', 'us-west-2'\n        )\n        self.assertEqual(service_client.waiter_names, [])\n        # Note we're using other-service-name, not\n        # 'myservice', which is the endpointPrefix.\n        self.loader.load_service_model.assert_called_with(\n            'other-service-name', 'waiters-2', '2014-01-01'\n        )\n\n    def test_service_has_waiter_configs(self):\n        waiter_config = {\n            'version': 2,\n            'waiters': {\n                \"Waiter1\": {\n                    'operation': 'TestOperation',\n                    'delay': 5,\n                    'maxAttempts': 20,\n                    'acceptors': [],\n                },\n                \"Waiter2\": {\n                    'operation': 'TestOperation',\n                    'delay': 5,\n                    'maxAttempts': 20,\n                    'acceptors': [],\n                },\n            },\n        }\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            waiter_config,\n        ]\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        self.assertEqual(\n            sorted(service_client.waiter_names), sorted(['waiter1', 'waiter2'])\n        )\n        self.assertTrue(hasattr(service_client.get_waiter('waiter1'), 'wait'))\n\n    def test_service_has_no_waiter_configs(self):\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            exceptions.DataNotFoundError(data_path='/foo'),\n        ]\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        self.assertEqual(service_client.waiter_names, [])\n        with self.assertRaises(ValueError):\n            service_client.get_waiter(\"unknown_waiter\")\n\n    def test_service_has_retry_event(self):\n        # A retry event should be registered for the service.\n        event_emitter = self.create_mock_emitter()\n        creator = self.create_client_creator(event_emitter=event_emitter)\n        creator.create_client('myservice', 'us-west-2')\n\n        event_emitter.register.assert_any_call(\n            'needs-retry.myservice',\n            mock.ANY,\n            unique_id='retry-config-myservice',\n        )\n\n    def test_service_creates_retryhandler(self):\n        # A retry handler with the expected configuration should be\n        # created when instantiating a client.\n        retry_handler_factory = mock.Mock()\n        creator = self.create_client_creator(\n            retry_handler_factory=retry_handler_factory\n        )\n        creator.create_client('myservice', 'us-west-2')\n\n        retry_handler_factory.create_retry_handler.assert_called_with(\n            {\n                '__default__': {\n                    'delay': {\n                        'growth_factor': 2,\n                        'base': 'rand',\n                        'type': 'exponential',\n                    },\n                    'policies': {},\n                    'max_attempts': 5,\n                }\n            },\n            'myservice',\n        )\n\n    def test_service_registers_retry_handler(self):\n        # The retry handler returned from ``create_retry_handler``\n        # that was tested above needs to be set as the handler for\n        # the event emitter.\n        retry_handler_factory = mock.Mock()\n        handler = mock.Mock()\n        event_emitter = self.create_mock_emitter()\n        retry_handler_factory.create_retry_handler.return_value = handler\n\n        creator = self.create_client_creator(\n            event_emitter=event_emitter,\n            retry_handler_factory=retry_handler_factory,\n        )\n        creator.create_client('myservice', 'us-west-2')\n\n        event_emitter.register.assert_any_call(\n            mock.ANY, handler, unique_id=mock.ANY\n        )\n\n    def test_service_retry_missing_config(self):\n        # No config means we should never see any retry events registered.\n        self.loader.load_data.return_value = {}\n\n        event_emitter = self.create_mock_emitter()\n        creator = self.create_client_creator(event_emitter=event_emitter)\n        creator.create_client('myservice', 'us-west-2')\n\n        for call in event_emitter.register.call_args_list:\n            self.assertNotIn('needs-retry', call[0][0])\n\n    def test_emits_after_call_error(self):\n        event_emitter = hooks.HierarchicalEmitter()\n\n        recorded_kwargs = []\n\n        def record(event_name, **kwargs):\n            recorded_kwargs.append(kwargs)\n\n        event_emitter.register(\n            'after-call-error.myservice.TestOperation', record\n        )\n\n        raised_error = RuntimeError('Unexpected error')\n        self.endpoint.make_request.side_effect = raised_error\n        creator = self.create_client_creator(event_emitter=event_emitter)\n        client = creator.create_client('myservice', 'us-west-2')\n        with self.assertRaises(RuntimeError):\n            client.test_operation(Foo='one', Bar='two')\n        self.assertEqual(\n            recorded_kwargs, [{'exception': raised_error, 'context': mock.ANY}]\n        )\n\n    def test_can_override_max_attempts(self):\n        retry_handler_factory = mock.Mock(botocore.retryhandler)\n        creator = self.create_client_creator(\n            retry_handler_factory=retry_handler_factory\n        )\n        creator.create_client(\n            'myservice',\n            'us-west-2',\n            client_config=botocore.config.Config(\n                retries={'max_attempts': 9, 'mode': 'legacy'}\n            ),\n        )\n\n        retry_handler_factory.create_retry_handler.assert_called_with(\n            {\n                '__default__': {\n                    'delay': {\n                        'growth_factor': 2,\n                        'base': 'rand',\n                        'type': 'exponential',\n                    },\n                    'policies': {},\n                    'max_attempts': 10,\n                }\n            },\n            'myservice',\n        )\n\n    def test_can_register_standard_retry_mode(self):\n        with mock.patch('botocore.client.standard') as standard:\n            creator = self.create_client_creator()\n            creator.create_client(\n                'myservice',\n                'us-west-2',\n                client_config=botocore.config.Config(\n                    retries={'mode': 'standard'}\n                ),\n            )\n        self.assertTrue(standard.register_retry_handler.called)\n\n    def test_can_register_standard_retry_mode_from_config_store(self):\n        fake_env = {'AWS_RETRY_MODE': 'standard'}\n        config_store = ConfigValueStore(\n            mapping={\n                'retry_mode': ChainProvider(\n                    [\n                        EnvironmentProvider('AWS_RETRY_MODE', fake_env),\n                    ]\n                )\n            }\n        )\n        creator = self.create_client_creator(config_store=config_store)\n        with mock.patch('botocore.client.standard') as standard:\n            creator.create_client('myservice', 'us-west-2')\n        self.assertTrue(standard.register_retry_handler.called)\n\n    def test_try_to_paginate_non_paginated(self):\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            exceptions.DataNotFoundError(data_path='/foo'),\n        ]\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        with self.assertRaises(exceptions.OperationNotPageableError):\n            service_client.get_paginator('test_operation')\n\n    def test_successful_pagination_object_created(self):\n        pagination_config = {\n            'pagination': {\n                'TestOperation': {\n                    \"input_token\": \"Marker\",\n                    \"output_token\": \"Marker\",\n                    \"more_results\": \"IsTruncated\",\n                    \"limit_key\": \"MaxItems\",\n                    \"result_key\": \"Users\",\n                }\n            }\n        }\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            pagination_config,\n        ]\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        paginator = service_client.get_paginator('test_operation')\n        # The pagination logic itself is tested elsewhere (test_paginate.py),\n        # but we can at least make sure it looks like a paginator.\n        self.assertTrue(hasattr(paginator, 'paginate'))\n\n    def test_paginator_class_name_from_client(self):\n        pagination_config = {\n            'pagination': {\n                'TestOperation': {\n                    \"input_token\": \"Marker\",\n                    \"output_token\": \"Marker\",\n                    \"more_results\": \"IsTruncated\",\n                    \"limit_key\": \"MaxItems\",\n                    \"result_key\": \"Users\",\n                }\n            }\n        }\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            pagination_config,\n        ]\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        paginator = service_client.get_paginator('test_operation')\n        self.assertEqual(\n            paginator.__class__.__name__, 'MyService.Paginator.TestOperation'\n        )\n\n    def test_paginator_help_from_client(self):\n        pagination_config = {\n            'pagination': {\n                'TestOperation': {\n                    \"input_token\": \"Marker\",\n                    \"output_token\": \"Marker\",\n                    \"more_results\": \"IsTruncated\",\n                    \"limit_key\": \"MaxItems\",\n                    \"result_key\": \"Users\",\n                }\n            }\n        }\n        self.loader.load_service_model.side_effect = [\n            self.service_description,\n            self.endpoint_ruleset,\n            pagination_config,\n        ]\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        paginator = service_client.get_paginator('test_operation')\n        with mock.patch('sys.stdout', io.StringIO()) as mock_stdout:\n            help(paginator.paginate)\n        contents = mock_stdout.getvalue()\n        lines = [\n            (\n                '    Creates an iterator that will paginate through responses '\n                'from :py:meth:`MyService.Client.test_operation`.'\n            ),\n            '    **Request Syntax**',\n            '    ::',\n            '      response_iterator = paginator.paginate(',\n            \"          Foo='string',\",\n            \"          Bar='string',\",\n            '          PaginationConfig={',\n            \"              'MaxItems': 123,\",\n            \"              'PageSize': 123,\",\n            \"              'StartingToken': 'string'\",\n            '          }',\n            '      )',\n            '    :type Foo: string',\n            '    :param Foo: **[REQUIRED]** Documents Foo',\n            '    :type Bar: string',\n            '    :param Bar: Documents Bar',\n            '    :type PaginationConfig: dict',\n            '    :param PaginationConfig:',\n            (\n                '      A dictionary that provides parameters to control '\n                'pagination.'\n            ),\n            '      - **MaxItems** *(integer) --*',\n            (\n                '        The total number of items to return. If the total '\n                'number of items available is more than the value specified '\n                'in max-items then a ``NextToken`` will be provided in the '\n                'output that you can use to resume pagination.'\n            ),\n            '      - **PageSize** *(integer) --*',\n            '        The size of each page.',\n            '      - **StartingToken** *(string) --*',\n            (\n                '        A token to specify where to start paginating. This is '\n                'the ``NextToken`` from a previous response.'\n            ),\n            '    :returns: None',\n        ]\n        for line in lines:\n            self.assertIn(line, contents)\n\n    def test_can_set_credentials_in_client_init(self):\n        creator = self.create_client_creator()\n        credentials = Credentials(\n            access_key='access_key',\n            secret_key='secret_key',\n            token='session_token',\n        )\n        client = creator.create_client(\n            'myservice', 'us-west-2', credentials=credentials\n        )\n\n        # Verify that we create an endpoint with a credentials object\n        # matching our creds arguments.\n        self.assertEqual(client._request_signer._credentials, credentials)\n\n    def test_event_emitted_when_invoked(self):\n        event_emitter = hooks.HierarchicalEmitter()\n        creator = self.create_client_creator(event_emitter=event_emitter)\n\n        calls = []\n        handler = lambda **kwargs: calls.append(kwargs)\n        event_emitter.register('before-call', handler)\n\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        service_client.test_operation(Foo='one', Bar='two')\n        self.assertEqual(len(calls), 1)\n\n    def test_events_are_per_client(self):\n        event_emitter = hooks.HierarchicalEmitter()\n        creator = self.create_client_creator(event_emitter=event_emitter)\n\n        first_calls = []\n        first_handler = lambda **kwargs: first_calls.append(kwargs)\n\n        second_calls = []\n        second_handler = lambda **kwargs: second_calls.append(kwargs)\n\n        first_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        second_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        first_client.meta.events.register('before-call', first_handler)\n        second_client.meta.events.register('before-call', second_handler)\n\n        # Now, if we invoke an operation from either client, only\n        # the handlers registered with the specific client will be invoked.\n        # So if we invoke the first client.\n        first_client.test_operation(Foo='one', Bar='two')\n        # Only first_calls is populated, not second_calls.\n        self.assertEqual(len(first_calls), 1)\n        self.assertEqual(len(second_calls), 0)\n\n        # If we invoke an operation from the second client,\n        # only second_calls will be populated, not first_calls.\n        second_client.test_operation(Foo='one', Bar='two')\n        # first_calls == 1 from the previous first_client.test_operation()\n        # call.\n        self.assertEqual(len(first_calls), 1)\n        self.assertEqual(len(second_calls), 1)\n\n    def test_clients_inherit_handlers_from_session(self):\n        # Even though clients get their own event emitters, they still\n        # inherit any handlers that were registered on the event emitter\n        # at the time the client was created.\n        event_emitter = hooks.HierarchicalEmitter()\n        creator = self.create_client_creator(event_emitter=event_emitter)\n\n        # So if an event handler is registered before any clients are created:\n\n        base_calls = []\n        base_handler = lambda **kwargs: base_calls.append(kwargs)\n        event_emitter.register('before-call', base_handler)\n\n        # Then any client created from this point forward from the\n        # event_emitter passed into the ClientCreator will have this\n        # handler.\n        first_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        first_client.test_operation(Foo='one', Bar='two')\n        self.assertEqual(len(base_calls), 1)\n\n        # Same thing if we create another client.\n        second_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        second_client.test_operation(Foo='one', Bar='two')\n        self.assertEqual(len(base_calls), 2)\n\n    def test_clients_inherit_only_at_create_time(self):\n        # If event handlers are added to the copied event emitter\n        # _after_ a client is created, we don't pick those up.\n        event_emitter = hooks.HierarchicalEmitter()\n        creator = self.create_client_creator(event_emitter=event_emitter)\n\n        # 1. Create a client.\n        first_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        # 2. Now register an event handler from the originating event emitter.\n        base_calls = []\n        base_handler = lambda **kwargs: base_calls.append(kwargs)\n        event_emitter.register('before-call', base_handler)\n\n        # 3. The client will _not_ see this because it already has its\n        #    own copy of the event handlers.\n        first_client.test_operation(Foo='one', Bar='two')\n        self.assertEqual(len(base_calls), 0)\n\n    def test_clients_have_meta_object(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client('myservice', 'us-west-2')\n        self.assertTrue(hasattr(service_client, 'meta'))\n        self.assertTrue(hasattr(service_client.meta, 'events'))\n        # Sanity check the event emitter has an .emit() method.\n        self.assertTrue(hasattr(service_client.meta.events, 'emit'))\n\n    def test_client_register_seperate_unique_id_event(self):\n        event_emitter = hooks.HierarchicalEmitter()\n        creator = self.create_client_creator(event_emitter=event_emitter)\n\n        client1 = creator.create_client('myservice', 'us-west-2')\n        client2 = creator.create_client('myservice', 'us-west-2')\n\n        def ping(**kwargs):\n            return 'foo'\n\n        client1.meta.events.register('some-event', ping, 'my-unique-id')\n        client2.meta.events.register('some-event', ping, 'my-unique-id')\n\n        # Ensure both clients can register a function with an unique id\n        client1_responses = client1.meta.events.emit('some-event')\n        self.assertEqual(len(client1_responses), 1)\n        self.assertEqual(client1_responses[0][1], 'foo')\n\n        client2_responses = client2.meta.events.emit('some-event')\n        self.assertEqual(len(client2_responses), 1)\n        self.assertEqual(client2_responses[0][1], 'foo')\n\n        # Ensure when a client is unregistered the other client has\n        # the unique-id event still registered.\n        client1.meta.events.unregister('some-event', ping, 'my-unique-id')\n        client1_responses = client1.meta.events.emit('some-event')\n        self.assertEqual(len(client1_responses), 0)\n\n        client2_responses = client2.meta.events.emit('some-event')\n        self.assertEqual(len(client2_responses), 1)\n        self.assertEqual(client2_responses[0][1], 'foo')\n\n        # Ensure that the other client can unregister the event\n        client2.meta.events.unregister('some-event', ping, 'my-unique-id')\n        client2_responses = client2.meta.events.emit('some-event')\n        self.assertEqual(len(client2_responses), 0)\n\n    def test_client_created_emits_events(self):\n        called = []\n\n        def on_client_create(class_attributes, **kwargs):\n            called.append(class_attributes)\n\n        event_emitter = hooks.HierarchicalEmitter()\n        event_emitter.register('creating-client-class', on_client_create)\n\n        creator = self.create_client_creator(event_emitter=event_emitter)\n        creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        self.assertEqual(len(called), 1)\n        self.assertIn('test_operation', called[0])\n\n    def test_client_method_called_event(self):\n        event_emitter = hooks.HierarchicalEmitter()\n\n        def inject_params(params, **kwargs):\n            new_params = params.copy()\n            new_params['Foo'] = 'zero'\n            return new_params\n\n        event_emitter.register(\n            'provide-client-params.myservice.TestOperation', inject_params\n        )\n\n        wrapped_emitter = mock.Mock(wraps=event_emitter)\n        creator = self.create_client_creator(event_emitter=wrapped_emitter)\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        params = {'Foo': 'one', 'Bar': 'two'}\n        service_client.test_operation(**params)\n\n        # Ensure that the initial params were not modified in the handler\n        self.assertEqual(params, {'Foo': 'one', 'Bar': 'two'})\n\n        # Ensure the handler passed on the correct param values.\n        body = self.endpoint.make_request.call_args[0][1]['body']\n        self.assertEqual(body['Foo'], 'zero')\n\n    def test_client_default_for_s3_addressing_style(self):\n        creator = self.create_client_creator()\n        client = creator.create_client('myservice', 'us-west-2')\n        self.assertEqual(client.meta.config.s3, None)\n\n    def test_client_s3_addressing_style_with_config(self):\n        creator = self.create_client_creator()\n        my_client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            client_config=botocore.config.Config(\n                s3={'addressing_style': 'auto'}\n            ),\n        )\n        self.assertEqual(my_client.meta.config.s3['addressing_style'], 'auto')\n\n    def test_client_s3_addressing_style_with_bad_value(self):\n        creator = self.create_client_creator()\n        client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            scoped_config={'s3': ''},\n        )\n        self.assertIsNone(client.meta.config.s3)\n\n    def test_client_s3_addressing_style_with_config_store(self):\n        self.config_store.set_config_variable(\n            's3', {'addressing_style': 'virtual'}\n        )\n        creator = self.create_client_creator()\n        client = creator.create_client('myservice', 'us-west-2')\n        self.assertEqual(client.meta.config.s3['addressing_style'], 'virtual')\n\n    def test_client_s3_addressing_style_with_incorrect_style(self):\n        with self.assertRaises(InvalidS3AddressingStyleError):\n            botocore.config.Config(s3={'addressing_style': 'foo'})\n\n    def test_client_s3_addressing_style_config_overrides_config_store(self):\n        self.config_store.set_config_variable(\n            's3', {'addressing_style': 'virtual'}\n        )\n        creator = self.create_client_creator()\n        my_client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            client_config=botocore.config.Config(\n                s3={'addressing_style': 'auto'}\n            ),\n        )\n        self.assertEqual(my_client.meta.config.s3['addressing_style'], 'auto')\n\n    def test_client_payload_signing_from_config_store(self):\n        self.config_store.set_config_variable(\n            's3', {'payload_signing_enabled': True}\n        )\n        creator = self.create_client_creator()\n        my_client = creator.create_client('myservice', 'us-west-2')\n        self.assertEqual(\n            my_client.meta.config.s3['payload_signing_enabled'], True\n        )\n\n    def test_client_payload_signing_from_client_config(self):\n        creator = self.create_client_creator()\n        my_client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            client_config=client.Config(s3={'payload_signing_enabled': True}),\n        )\n        self.assertEqual(\n            my_client.meta.config.s3['payload_signing_enabled'], True\n        )\n\n    def test_client_payload_signing_client_config_overrides_scoped(self):\n        creator = self.create_client_creator()\n        my_client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            scoped_config={'s3': {'payload_signing_enabled': False}},\n            client_config=client.Config(s3={'payload_signing_enabled': True}),\n        )\n        self.assertEqual(\n            my_client.meta.config.s3['payload_signing_enabled'], True\n        )\n\n    def test_client_s3_accelerate_from_config_store(self):\n        self.config_store.set_config_variable(\n            's3', {'use_accelerate_endpoint': True}\n        )\n        creator = self.create_client_creator()\n        my_client = creator.create_client('myservice', 'us-west-2')\n        self.assertEqual(\n            my_client.meta.config.s3['use_accelerate_endpoint'], True\n        )\n\n    def test_client_s3_accelerate_from_client_config(self):\n        creator = self.create_client_creator()\n        my_client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            client_config=client.Config(s3={'use_accelerate_endpoint': True}),\n        )\n        self.assertEqual(\n            my_client.meta.config.s3['use_accelerate_endpoint'], True\n        )\n\n    def test_client_s3_accelerate_client_config_overrides_config_store(self):\n        self.config_store.set_config_variable(\n            's3', {'use_accelerate_endpoint': False}\n        )\n        creator = self.create_client_creator()\n        my_client = creator.create_client(\n            'myservice',\n            'us-west-2',\n            client_config=client.Config(s3={'use_accelerate_endpoint': True}),\n        )\n        self.assertEqual(\n            my_client.meta.config.s3['use_accelerate_endpoint'], True\n        )\n\n    def test_before_call_short_circuits_request(self):\n        def return_mock_tuple(**kwargs):\n            http_mock = mock.Mock()\n            http_mock.status_code = 200\n            return http_mock, mock.Mock()\n\n        emitter = hooks.HierarchicalEmitter()\n        emitter.register_last('before-call.*.*', return_mock_tuple)\n        creator = self.create_client_creator(event_emitter=emitter)\n        service_client = creator.create_client('myservice', 'us-west-2')\n\n        service_client.test_operation(Foo='one')\n        self.assertFalse(self.endpoint.make_request.called)\n\n    def test_getattr_emits_event(self):\n        emitter = self.create_mock_emitter()\n        emitter.emit_until_response.return_value = (None, None)\n\n        creator = self.create_client_creator(event_emitter=emitter)\n        service_client = creator.create_client('myservice', 'us-west-2')\n\n        # Assert that the event hasn't fired yet\n        emitter.emit_until_response.assert_not_called()\n\n        with self.assertRaises(AttributeError):\n            service_client.attribute_that_does_not_exist\n\n        emitter.emit_until_response.assert_called_once_with(\n            'getattr.myservice.attribute_that_does_not_exist',\n            client=service_client,\n        )\n\n    def test_getattr_event_returns_response(self):\n        emitter = self.create_mock_emitter()\n        emitter.emit_until_response.return_value = (None, 'success')\n\n        creator = self.create_client_creator(event_emitter=emitter)\n        service_client = creator.create_client('myservice', 'us-west-2')\n\n        value = service_client.attribute_that_does_not_exist\n\n        self.assertEqual(value, 'success')\n\n    def _create_hostname_binding_client(self, *args, **kwargs):\n        test_operation = self.service_description['operations'][\n            'TestOperation'\n        ]\n        test_operation['endpoint'] = {'hostPrefix': '{Foo}.'}\n        test_shape = self.service_description['shapes']['TestOperationRequest']\n        test_shape['members']['Foo']['hostLabel'] = True\n\n        creator = self.create_client_creator()\n        return creator.create_client('myservice', *args, **kwargs)\n\n    def test_client_operation_hostname_binding(self):\n        client = self._create_hostname_binding_client('us-west-2')\n        client.test_operation(Foo='bound')\n\n        expected_url = 'https://bound.myservice.amazonaws.com/'\n        self.assertTrue(self.endpoint.make_request.called)\n        request_dict = self.endpoint.make_request.call_args[0][1]\n        self.assertEqual(request_dict['url'], expected_url)\n\n    def test_client_operation_hostname_binding_validation(self):\n        client = self._create_hostname_binding_client('us-west-2')\n        with self.assertRaises(ParamValidationError):\n            client.test_operation(Foo='')\n\n    def test_client_operation_hostname_binding_configuration(self):\n        config = botocore.config.Config(inject_host_prefix=False)\n        client = self._create_hostname_binding_client(\n            'us-west-2',\n            client_config=config,\n        )\n\n        client.test_operation(Foo='baz')\n        expected_url = 'https://myservice.amazonaws.com/'\n        self.assertTrue(self.endpoint.make_request.called)\n        request_dict = self.endpoint.make_request.call_args[0][1]\n        self.assertEqual(request_dict['url'], expected_url)\n\n    def test_client_close(self):\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        service_client.close()\n        self.endpoint.close.assert_called_once_with()\n\n    def test_client_close_context_manager(self):\n        creator = self.create_client_creator()\n        with closing(\n            creator.create_client(\n                'myservice', 'us-west-2', credentials=self.credentials\n            )\n        ) as service_client:\n            service_client.test_operation(Foo='baz')\n\n        self.endpoint.close.assert_called_once_with()\n\n    def test_client_internal_credential_shim(self):\n        \"\"\"This test exercises the internal credential shim exposed on clients.\n        It's here to ensure we don't unintentionally regress behavior used with\n        our upload/download operations in s3transfer with the CRT.\n        \"\"\"\n        creator = self.create_client_creator()\n        service_client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        self.assertEqual(service_client._get_credentials(), self.credentials)\n\n\nclass TestClientErrors(TestAutoGeneratedClient):\n    def add_error_response(self, error_response):\n        self.endpoint.make_request.return_value = (\n            mock.Mock(status_code=400),\n            error_response,\n        )\n\n    def test_client_makes_call_with_error(self):\n        creator = self.create_client_creator()\n        client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        with Stubber(client) as stub:\n            stub.add_client_error(\n                'test_operation', 'TestOperationErrorCode', 'error occurred'\n            )\n            with self.assertRaises(client.exceptions.TestOperationException):\n                client.test_operation(Foo='one', Bar='two')\n\n    def test_error_with_no_wire_code(self):\n        creator = self.create_client_creator()\n        client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        with Stubber(client) as stub:\n            stub.add_client_error('test_operation', '404', 'Not Found')\n            try:\n                client.test_operation(Foo='one', Bar='two')\n            except client.exceptions.ClientError as e:\n                # This is needed becasue the error could be a subclass of\n                # ClientError.\n                # We explicitly want it to be a generic ClientError though\n                self.assertEqual(e.__class__, exceptions.ClientError)\n\n    def test_error_with_dot_separated_code(self):\n        creator = self.create_client_creator()\n        client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        with Stubber(client) as stub:\n            stub.add_client_error(\n                'test_operation', 'InvalidAddress.NotFound', 'Not Found'\n            )\n            try:\n                client.test_operation(Foo='one', Bar='two')\n            except client.exceptions.ClientError as e:\n                # This is needed becasue the error could be a subclass of\n                # ClientError.\n                # We explicitly want it to be a generic ClientError though\n                self.assertEqual(e.__class__, exceptions.ClientError)\n\n    def test_error_with_empty_message(self):\n        creator = self.create_client_creator()\n        client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        with Stubber(client) as stub:\n            stub.add_client_error('test_operation', 'TestOperationErrorCode')\n            with self.assertRaises(client.exceptions.TestOperationException):\n                client.test_operation(Foo='one', Bar='two')\n\n    def test_error_with_empty_code(self):\n        creator = self.create_client_creator()\n        client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        with Stubber(client) as stub:\n            stub.add_client_error('test_operation')\n            try:\n                client.test_operation(Foo='one', Bar='two')\n            except client.exceptions.ClientError as e:\n                # This is needed becasue the error could be a subclass of\n                # ClientError.\n                # We explicitly want it to be a generic ClientError though\n                self.assertEqual(e.__class__, exceptions.ClientError)\n\n    def test_error_with_missing_code(self):\n        error_response = {'Error': {'Message': 'error occurred'}}\n        # The stubber is not being used because it will always populate the\n        # the message and code.\n        self.endpoint.make_request.return_value = (\n            mock.Mock(status_code=400),\n            error_response,\n        )\n\n        creator = self.create_client_creator()\n        client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        try:\n            client.test_operation(Foo='one', Bar='two')\n        except client.exceptions.ClientError as e:\n            # This is needed becasue the error could be a subclass of\n            # ClientError.\n            # We explicitly want it to be a generic ClientError though\n            self.assertEqual(e.__class__, exceptions.ClientError)\n\n    def test_error_with_empty_contents(self):\n        error_response = {'Error': {}}\n        # The stubber is not being used because it will always populate the\n        # the message and code.\n        self.endpoint.make_request.return_value = (\n            mock.Mock(status_code=400),\n            error_response,\n        )\n\n        creator = self.create_client_creator()\n        client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        try:\n            client.test_operation(Foo='one', Bar='two')\n        except client.exceptions.ClientError as e:\n            # This is needed becasue the error could be a subclass of\n            # ClientError.\n            # We explicitly want it to be a generic ClientError though\n            self.assertEqual(e.__class__, exceptions.ClientError)\n\n    def test_exception_classes_across_clients_are_the_same(self):\n        creator = self.create_client_creator(\n            exceptions_factory=ClientExceptionsFactory()\n        )\n        client = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n        client2 = creator.create_client(\n            'myservice', 'us-west-2', credentials=self.credentials\n        )\n\n        with Stubber(client) as stub:\n            stub.add_client_error(\n                'test_operation', 'TestOperationErrorCode', 'error occurred'\n            )\n            try:\n                client.test_operation(Foo='one', Bar='two')\n            except client2.exceptions.TestOperationException as e:\n                # Caught exception should as well be an instance of the\n                # other client's TestOperationException\n                self.assertIsInstance(\n                    e, client.exceptions.TestOperationException\n                )\n\n\nclass TestConfig(unittest.TestCase):\n    def test_can_use_args_to_construct(self):\n        config = botocore.config.Config(\n            *botocore.config.Config.OPTION_DEFAULTS.values()\n        )\n        for (\n            option,\n            default_value,\n        ) in botocore.config.Config.OPTION_DEFAULTS.items():\n            self.assertTrue(hasattr(config, option))\n            self.assertEqual(getattr(config, option), default_value)\n\n    def test_can_use_kwargs_to_construct(self):\n        config = botocore.config.Config(\n            **botocore.config.Config.OPTION_DEFAULTS\n        )\n        for (\n            option,\n            default_value,\n        ) in botocore.config.Config.OPTION_DEFAULTS.items():\n            self.assertTrue(hasattr(config, option))\n            self.assertEqual(getattr(config, option), default_value)\n\n    def test_can_use_mix_of_args_and_kwargs(self):\n        config = botocore.config.Config('us-east-1', read_timeout=50)\n        self.assertEqual(config.region_name, 'us-east-1')\n        self.assertEqual(config.read_timeout, 50)\n\n    def test_invalid_kwargs(self):\n        with self.assertRaisesRegex(TypeError, 'Got unexpected keyword'):\n            botocore.config.Config(foo='foo')\n\n    def test_pass_invalid_length_of_args(self):\n        with self.assertRaisesRegex(TypeError, 'Takes at most'):\n            botocore.config.Config(\n                'foo', *botocore.config.Config.OPTION_DEFAULTS.values()\n            )\n\n    def test_create_with_multiple_kwargs(self):\n        with self.assertRaisesRegex(TypeError, 'Got multiple values'):\n            botocore.config.Config('us-east-1', region_name='us-east-1')\n\n    def test_merge_returns_new_config_object(self):\n        config = botocore.config.Config()\n        other_config = botocore.config.Config()\n        new_config = config.merge(other_config)\n        # Check the type is correct\n        self.assertIsInstance(new_config, botocore.config.Config)\n        # Make sure the new config is a brand new config object\n        self.assertIsNot(new_config, config)\n        self.assertIsNot(new_config, other_config)\n\n    def test_general_merge_keeps_default_values(self):\n        config = botocore.config.Config()\n        other_config = botocore.config.Config()\n        config_properties = vars(config)\n        new_config = config.merge(other_config)\n        # Ensure that the values all stayed the same in the new config\n        self.assertEqual(config_properties, vars(new_config))\n\n    def test_merge_overrides_values(self):\n        config = botocore.config.Config(region_name='us-east-1')\n        other_config = botocore.config.Config(region_name='us-west-2')\n        new_config = config.merge(other_config)\n        self.assertEqual(new_config.region_name, 'us-west-2')\n\n    def test_merge_overrides_values_even_when_using_default(self):\n        config = botocore.config.Config(region_name='us-east-1')\n        other_config = botocore.config.Config(region_name=None)\n        new_config = config.merge(other_config)\n        self.assertEqual(new_config.region_name, None)\n\n    def test_merge_overrides_values_even_when_using_default_timeout(self):\n        config = botocore.config.Config(read_timeout=30)\n        other_config = botocore.config.Config(read_timeout=DEFAULT_TIMEOUT)\n        new_config = config.merge(other_config)\n        self.assertEqual(new_config.read_timeout, DEFAULT_TIMEOUT)\n\n    def test_merge_overrides_only_when_user_provided_values(self):\n        config = botocore.config.Config(\n            region_name='us-east-1', signature_version='s3v4'\n        )\n        other_config = botocore.config.Config(region_name='us-west-2')\n        new_config = config.merge(other_config)\n        self.assertEqual(new_config.region_name, 'us-west-2')\n        self.assertEqual(new_config.signature_version, 's3v4')\n\n    def test_can_set_retry_max_attempts(self):\n        config = botocore.config.Config(retries={'max_attempts': 15})\n        self.assertEqual(config.retries['max_attempts'], 15)\n\n    def test_validates_retry_config(self):\n        with self.assertRaisesRegex(\n            InvalidRetryConfigurationError,\n            'Cannot provide retry configuration for \"not-allowed\"',\n        ):\n            botocore.config.Config(retries={'not-allowed': True})\n\n    def test_validates_max_retry_attempts(self):\n        with self.assertRaises(InvalidMaxRetryAttemptsError):\n            botocore.config.Config(retries={'max_attempts': -1})\n\n    def test_validates_total_max_attempts(self):\n        with self.assertRaises(InvalidMaxRetryAttemptsError):\n            botocore.config.Config(retries={'total_max_attempts': 0})\n\n    def test_validaties_retry_mode(self):\n        with self.assertRaises(InvalidRetryModeError):\n            botocore.config.Config(retries={'mode': 'turbo-mode'})\n\n\nclass TestClientEndpointBridge(unittest.TestCase):\n    def setUp(self):\n        self.resolver = mock.Mock()\n        self.boilerplate_response = {\n            'endpointName': 'us-east-1',\n            'hostname': 's3.amazonaws.com',\n            'partition': 'aws',\n            'protocols': ['http', 'https'],\n            'dnsSuffix': 'amazonaws.com',\n            'signatureVersions': ['s3', 's3v4'],\n        }\n        self.resolver.construct_endpoint.return_value = (\n            self.boilerplate_response\n        )\n\n    def test_guesses_endpoint_as_last_resort(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = None\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('myservice', region_name='guess')\n        self.assertEqual('guess', resolved['region_name'])\n        self.assertEqual('guess', resolved['signing_region'])\n        self.assertEqual('myservice', resolved['signing_name'])\n        self.assertEqual('myservice', resolved['service_name'])\n        self.assertEqual('v4', resolved['signature_version'])\n        self.assertEqual(\n            'https://myservice.guess.amazonaws.com', resolved['endpoint_url']\n        )\n\n    def test_uses_us_east_1_by_default_for_s3(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 's3.amazonaws.com',\n            'endpointName': 'us-east-1',\n            'signatureVersions': ['s3', 's3v4'],\n            'protocols': ['https'],\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('s3')\n        self.assertEqual('us-east-1', resolved['region_name'])\n        self.assertEqual('us-east-1', resolved['signing_region'])\n        self.assertEqual('https://s3.amazonaws.com', resolved['endpoint_url'])\n\n    def test_uses_region_from_client_config_if_available(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = None\n        client_config = mock.Mock()\n        client_config.region_name = 'us-foo-bar'\n        bridge = ClientEndpointBridge(resolver, client_config=client_config)\n        resolved = bridge.resolve('test')\n        self.assertEqual('us-foo-bar', resolved['region_name'])\n        self.assertEqual('us-foo-bar', resolved['signing_region'])\n        self.assertEqual(\n            'https://test.us-foo-bar.amazonaws.com', resolved['endpoint_url']\n        )\n\n    def test_can_guess_endpoint_and_use_given_endpoint_url(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = None\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve(\n            'test', 'guess', endpoint_url='http://test.com'\n        )\n        self.assertEqual('guess', resolved['region_name'])\n        self.assertEqual('guess', resolved['signing_region'])\n        self.assertEqual('http://test.com', resolved['endpoint_url'])\n\n    def test_can_use_endpoint_url_with_resolved_endpoint(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'do-not-use-this',\n            'endpointName': 'us-west-2',\n            'signatureVersions': ['v2'],\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve(\n            'ec2', 'us-west-2', endpoint_url='https://foo'\n        )\n        self.assertEqual('us-west-2', resolved['region_name'])\n        self.assertEqual('us-west-2', resolved['signing_region'])\n        self.assertEqual('https://foo', resolved['endpoint_url'])\n        self.assertEqual('v2', resolved['signature_version'])\n\n    def test_can_create_http_urls(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'host.com',\n            'signatureVersions': ['v4'],\n            'endpointName': 'us-foo-baz',\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('myservice', 'us-foo-baz', is_secure=False)\n        self.assertEqual('http://host.com', resolved['endpoint_url'])\n\n    def test_credential_scope_overrides_signing_region(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'host.com',\n            'endpointName': 'us-foo-baz',\n            'signatureVersions': ['v4'],\n            'credentialScope': {'region': 'override'},\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('myservice', 'us-foo-baz')\n        self.assertEqual('us-foo-baz', resolved['region_name'])\n        self.assertEqual('override', resolved['signing_region'])\n\n    def test_cred_scope_does_not_override_signing_region_if_endpoint_url(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'will-not-use.com',\n            'endpointName': 'us-foo-baz',\n            'signatureVersions': ['v4'],\n            'credentialScope': {'region': 'override'},\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve(\n            'myservice', 'us-foo-baz', endpoint_url='https://override.com'\n        )\n        self.assertEqual('us-foo-baz', resolved['region_name'])\n        self.assertEqual('us-foo-baz', resolved['signing_region'])\n        self.assertEqual('https://override.com', resolved['endpoint_url'])\n\n    def test_resolved_region_overrides_region_when_no_endpoint_url(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'host.com',\n            'signatureVersions': ['v4'],\n            'endpointName': 'override',\n            'protocols': ['https'],\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('myservice', 'will-not-be-there')\n        self.assertEqual('override', resolved['region_name'])\n        self.assertEqual('override', resolved['signing_region'])\n        self.assertEqual('https://host.com', resolved['endpoint_url'])\n\n    def test_does_not_use_https_if_not_available(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'host.com',\n            'signatureVersions': ['v4'],\n            'endpointName': 'foo',\n            # Note: http, not https\n            'protocols': ['http'],\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('myservice')\n        # We should resolve to http://, not https://\n        self.assertEqual('http://host.com', resolved['endpoint_url'])\n\n    def test_uses_signature_version_from_client_config(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test.com',\n            'endpointName': 'us-west-2',\n            'signatureVersions': ['v2'],\n        }\n        client_config = mock.Mock()\n        client_config.signature_version = 's3'\n        bridge = ClientEndpointBridge(resolver, client_config=client_config)\n        resolved = bridge.resolve('test', 'us-west-2')\n        self.assertEqual('s3', resolved['signature_version'])\n\n    def test_uses_signature_version_from_client_config_when_guessing(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = None\n        client_config = mock.Mock()\n        client_config.signature_version = 's3v4'\n        bridge = ClientEndpointBridge(resolver, client_config=client_config)\n        resolved = bridge.resolve('test', 'us-west-2')\n        self.assertEqual('s3v4', resolved['signature_version'])\n\n    def test_uses_signature_version_from_scoped_config(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test.com',\n            'endpointName': 'us-west-2',\n            'signatureVersions': ['v2'],\n        }\n        scoped_config = mock.Mock()\n        scoped_config.get.return_value = {'signature_version': 's3'}\n        bridge = ClientEndpointBridge(resolver, scoped_config)\n        resolved = bridge.resolve('test', 'us-west-2')\n        self.assertEqual('s3', resolved['signature_version'])\n\n    def test_uses_s3v4_over_s3_for_s3(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test.com',\n            'endpointName': 'us-west-2',\n            'signatureVersions': ['s3v4', 's3'],\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('s3', 'us-west-2')\n        self.assertEqual('s3v4', resolved['signature_version'])\n\n    def test_uses_s3v4_over_others_for_s3(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test.com',\n            'endpointName': 'us-west-2',\n            'signatureVersions': ['s3v4', 'v4'],\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('s3', 'us-west-2')\n        self.assertEqual('s3v4', resolved['signature_version'])\n\n    def test_uses_v4_over_other_signers(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test',\n            'signatureVersions': ['v2', 'v4'],\n            'endpointName': 'us-west-2',\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('test', 'us-west-2')\n        self.assertEqual('v4', resolved['signature_version'])\n\n    def test_uses_known_signers_from_list_of_signature_versions(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test',\n            'signatureVersions': ['foo', 'baz', 'v3https'],\n            'endpointName': 'us-west-2',\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('test', 'us-west-2')\n        self.assertEqual('v3https', resolved['signature_version'])\n\n    def test_raises_when_signature_version_is_unknown(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test',\n            'endpointName': 'us-west-2',\n            'signatureVersions': ['foo'],\n        }\n        bridge = ClientEndpointBridge(resolver)\n        with self.assertRaises(UnknownSignatureVersionError):\n            bridge.resolve('test', 'us-west-2')\n\n    def test_uses_first_known_signature_version(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test',\n            'endpointName': 'us-west-2',\n            'signatureVersions': ['foo', 'bar', 'baz', 's3v4', 'v2'],\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('test', 'us-west-2')\n        self.assertEqual('s3v4', resolved['signature_version'])\n\n    def test_raises_when_signature_version_is_not_found(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test',\n            'endpointName': 'us-west-2',\n        }\n        bridge = ClientEndpointBridge(resolver)\n        with self.assertRaises(UnknownSignatureVersionError):\n            bridge.resolve('test', 'us-west-2')\n\n    def test_uses_service_name_as_signing_name(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test',\n            'signatureVersions': ['v4'],\n            'endpointName': 'us-west-2',\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('test', 'us-west-2')\n        self.assertEqual('test', resolved['signing_name'])\n\n    def test_uses_credential_scope_signing_name(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test',\n            'endpointName': 'us-west-2',\n            'signatureVersions': ['v4'],\n            'credentialScope': {'service': 'override'},\n        }\n        bridge = ClientEndpointBridge(resolver)\n        resolved = bridge.resolve('test', 'us-west-2')\n        self.assertEqual('override', resolved['signing_name'])\n\n    def test_uses_service_signing_name_when_present_and_no_cred_scope(self):\n        resolver = mock.Mock()\n        resolver.construct_endpoint.return_value = {\n            'partition': 'aws',\n            'hostname': 'test',\n            'signatureVersions': ['v4'],\n            'endpointName': 'us-west-2',\n        }\n        bridge = ClientEndpointBridge(resolver, service_signing_name='foo')\n        resolved = bridge.resolve('test', 'us-west-2')\n        self.assertEqual('foo', resolved['signing_name'])\n\n    def test_disable_dualstack_explicitly(self):\n        scoped_config = {'s3': {'use_dualstack_endpoint': True}}\n        config = botocore.config.Config(s3={'use_dualstack_endpoint': False})\n        bridge = ClientEndpointBridge(\n            self.resolver, scoped_config, client_config=config\n        )\n        resolved = bridge.resolve('s3', 'us-east-1')\n        self.assertEqual(resolved['endpoint_url'], 'https://s3.amazonaws.com')\n\n    def test_use_dualstack_endpoint(self):\n        config = botocore.config.Config(use_dualstack_endpoint=True)\n        bridge = ClientEndpointBridge(self.resolver, client_config=config)\n        bridge.resolve('ec2', 'us-west-2')\n        self.resolver.construct_endpoint.assert_called_with(\n            'ec2',\n            'us-west-2',\n            use_dualstack_endpoint=True,\n            use_fips_endpoint=False,\n        )\n\n    def test_use_fips_endpoint(self):\n        config = botocore.config.Config(use_fips_endpoint=True)\n        bridge = ClientEndpointBridge(self.resolver, client_config=config)\n        bridge.resolve('ec2', 'us-west-2')\n        self.resolver.construct_endpoint.assert_called_with(\n            'ec2',\n            'us-west-2',\n            use_dualstack_endpoint=False,\n            use_fips_endpoint=True,\n        )\n\n    def test_use_dualstack_endpoint_omits_s3(self):\n        config = botocore.config.Config(\n            use_dualstack_endpoint=True, s3={'use_dualstack_endpoint': False}\n        )\n        bridge = ClientEndpointBridge(self.resolver, client_config=config)\n        bridge.resolve('s3', 'us-west-2')\n        self.resolver.construct_endpoint.assert_called_with(\n            's3',\n            'us-west-2',\n            use_dualstack_endpoint=False,\n            use_fips_endpoint=False,\n        )\n\n    def test_modeled_endpoint_variants_client_config_trumps_scoped_config(\n        self,\n    ):\n        scoped_config = {\n            'use_dualstack_endpoint': True,\n            'use_fips_endpoint': True,\n        }\n        config = botocore.config.Config(\n            use_dualstack_endpoint=False, use_fips_endpoint=False\n        )\n        bridge = ClientEndpointBridge(\n            self.resolver, scoped_config, client_config=config\n        )\n        bridge.resolve('ec2', 'us-west-2')\n        self.resolver.construct_endpoint.assert_called_with(\n            'ec2',\n            'us-west-2',\n            use_dualstack_endpoint=False,\n            use_fips_endpoint=False,\n        )\n\n    def test_modeled_endpoint_variants_tags_using_config_store(self):\n        config_store = mock.Mock()\n        config_store.get_config_variable.return_value = True\n        bridge = ClientEndpointBridge(self.resolver, config_store=config_store)\n        bridge.resolve('ec2', 'us-west-2')\n        self.resolver.construct_endpoint.assert_called_with(\n            'ec2',\n            'us-west-2',\n            use_dualstack_endpoint=True,\n            use_fips_endpoint=True,\n        )\n", "tests/unit/test_discovery.py": "import time\n\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.client import ClientMeta\nfrom botocore.discovery import (\n    EndpointDiscoveryHandler,\n    EndpointDiscoveryManager,\n    EndpointDiscoveryRefreshFailed,\n    EndpointDiscoveryRequired,\n    block_endpoint_discovery_required_operations,\n)\nfrom botocore.exceptions import ConnectionError\nfrom botocore.handlers import inject_api_version_header_if_needed\nfrom botocore.hooks import HierarchicalEmitter\nfrom botocore.model import ServiceModel\nfrom tests import mock, unittest\n\n\nclass BaseEndpointDiscoveryTest(unittest.TestCase):\n    def setUp(self):\n        self.service_description = {\n            'version': '2.0',\n            'metadata': {\n                'apiVersion': '2018-08-31',\n                'endpointPrefix': 'fooendpoint',\n                'jsonVersion': '1.1',\n                'protocol': 'json',\n                'serviceAbbreviation': 'FooService',\n                'serviceId': 'FooService',\n                'serviceFullName': 'AwsFooService',\n                'signatureVersion': 'v4',\n                'signingName': 'awsfooservice',\n                'targetPrefix': 'awsfooservice',\n            },\n            'operations': {\n                'DescribeEndpoints': {\n                    'name': 'DescribeEndpoints',\n                    'http': {'method': 'POST', 'requestUri': '/'},\n                    'input': {'shape': 'DescribeEndpointsRequest'},\n                    'output': {'shape': 'DescribeEndpointsResponse'},\n                    'endpointoperation': True,\n                },\n                'TestDiscoveryRequired': {\n                    'name': 'TestDiscoveryRequired',\n                    'http': {'method': 'POST', 'requestUri': '/'},\n                    'input': {'shape': 'TestDiscoveryIdsRequest'},\n                    'output': {'shape': 'EmptyStruct'},\n                    'endpointdiscovery': {'required': True},\n                },\n                'TestDiscoveryOptional': {\n                    'name': 'TestDiscoveryOptional',\n                    'http': {'method': 'POST', 'requestUri': '/'},\n                    'input': {'shape': 'TestDiscoveryIdsRequest'},\n                    'output': {'shape': 'EmptyStruct'},\n                    'endpointdiscovery': {},\n                },\n                'TestDiscovery': {\n                    'name': 'TestDiscovery',\n                    'http': {'method': 'POST', 'requestUri': '/'},\n                    'input': {'shape': 'EmptyStruct'},\n                    'output': {'shape': 'EmptyStruct'},\n                    'endpointdiscovery': {},\n                },\n            },\n            'shapes': {\n                'Boolean': {'type': 'boolean'},\n                'DescribeEndpointsRequest': {\n                    'type': 'structure',\n                    'members': {\n                        'Operation': {'shape': 'String'},\n                        'Identifiers': {'shape': 'Identifiers'},\n                    },\n                },\n                'DescribeEndpointsResponse': {\n                    'type': 'structure',\n                    'required': ['Endpoints'],\n                    'members': {'Endpoints': {'shape': 'Endpoints'}},\n                },\n                'Endpoint': {\n                    'type': 'structure',\n                    'required': ['Address', 'CachePeriodInMinutes'],\n                    'members': {\n                        'Address': {'shape': 'String'},\n                        'CachePeriodInMinutes': {'shape': 'Long'},\n                    },\n                },\n                'Endpoints': {'type': 'list', 'member': {'shape': 'Endpoint'}},\n                'Identifiers': {\n                    'type': 'map',\n                    'key': {'shape': 'String'},\n                    'value': {'shape': 'String'},\n                },\n                'Long': {'type': 'long'},\n                'String': {'type': 'string'},\n                'TestDiscoveryIdsRequest': {\n                    'type': 'structure',\n                    'required': ['Foo', 'Nested'],\n                    'members': {\n                        'Foo': {\n                            'shape': 'String',\n                            'endpointdiscoveryid': True,\n                        },\n                        'Baz': {'shape': 'String'},\n                        'Nested': {'shape': 'Nested'},\n                    },\n                },\n                'EmptyStruct': {'type': 'structure', 'members': {}},\n                'Nested': {\n                    'type': 'structure',\n                    'required': 'Bar',\n                    'members': {\n                        'Bar': {\n                            'shape': 'String',\n                            'endpointdiscoveryid': True,\n                        }\n                    },\n                },\n            },\n        }\n\n\nclass TestEndpointDiscoveryManager(BaseEndpointDiscoveryTest):\n    def setUp(self):\n        super().setUp()\n        self.construct_manager()\n\n    def construct_manager(self, cache=None, time=None, side_effect=None):\n        self.service_model = ServiceModel(self.service_description)\n        self.meta = mock.Mock(spec=ClientMeta)\n        self.meta.service_model = self.service_model\n        self.client = mock.Mock()\n        if side_effect is None:\n            side_effect = [\n                {\n                    'Endpoints': [\n                        {\n                            'Address': 'new.com',\n                            'CachePeriodInMinutes': 2,\n                        }\n                    ]\n                }\n            ]\n        self.client.describe_endpoints.side_effect = side_effect\n        self.client.meta = self.meta\n        self.manager = EndpointDiscoveryManager(\n            self.client, cache=cache, current_time=time\n        )\n\n    def test_injects_api_version_if_endpoint_operation(self):\n        model = self.service_model.operation_model('DescribeEndpoints')\n        params = {'headers': {}}\n        inject_api_version_header_if_needed(model, params)\n        self.assertEqual(\n            params['headers'].get('x-amz-api-version'), '2018-08-31'\n        )\n\n    def test_no_inject_api_version_if_not_endpoint_operation(self):\n        model = self.service_model.operation_model('TestDiscoveryRequired')\n        params = {'headers': {}}\n        inject_api_version_header_if_needed(model, params)\n        self.assertNotIn('x-amz-api-version', params['headers'])\n\n    def test_gather_identifiers(self):\n        params = {'Foo': 'value1', 'Nested': {'Bar': 'value2'}}\n        operation = self.service_model.operation_model('TestDiscoveryRequired')\n        ids = self.manager.gather_identifiers(operation, params)\n        self.assertEqual(ids, {'Foo': 'value1', 'Bar': 'value2'})\n\n    def test_gather_identifiers_none(self):\n        operation = self.service_model.operation_model('TestDiscovery')\n        ids = self.manager.gather_identifiers(operation, {})\n        self.assertEqual(ids, {})\n\n    def test_describe_endpoint(self):\n        kwargs = {\n            'Operation': 'FooBar',\n            'Identifiers': {'Foo': 'value1', 'Bar': 'value2'},\n        }\n        self.manager.describe_endpoint(**kwargs)\n        self.client.describe_endpoints.assert_called_with(**kwargs)\n\n    def test_describe_endpoint_no_input(self):\n        describe = self.service_description['operations']['DescribeEndpoints']\n        del describe['input']\n        self.construct_manager()\n        self.manager.describe_endpoint(Operation='FooBar', Identifiers={})\n        self.client.describe_endpoints.assert_called_with()\n\n    def test_describe_endpoint_empty_input(self):\n        describe = self.service_description['operations']['DescribeEndpoints']\n        describe['input'] = {'shape': 'EmptyStruct'}\n        self.construct_manager()\n        self.manager.describe_endpoint(Operation='FooBar', Identifiers={})\n        self.client.describe_endpoints.assert_called_with()\n\n    def test_describe_endpoint_ids_and_operation(self):\n        cache = {}\n        self.construct_manager(cache=cache)\n        ids = {'Foo': 'value1', 'Bar': 'value2'}\n        kwargs = {\n            'Operation': 'TestDiscoveryRequired',\n            'Identifiers': ids,\n        }\n        self.manager.describe_endpoint(**kwargs)\n        self.client.describe_endpoints.assert_called_with(**kwargs)\n        key = ((('Bar', 'value2'), ('Foo', 'value1')), 'TestDiscoveryRequired')\n        self.assertIn(key, cache)\n        self.assertEqual(cache[key][0]['Address'], 'new.com')\n        self.manager.describe_endpoint(**kwargs)\n        call_count = self.client.describe_endpoints.call_count\n        self.assertEqual(call_count, 1)\n\n    def test_describe_endpoint_no_ids_or_operation(self):\n        cache = {}\n        describe = self.service_description['operations']['DescribeEndpoints']\n        describe['input'] = {'shape': 'EmptyStruct'}\n        self.construct_manager(cache=cache)\n        self.manager.describe_endpoint(\n            Operation='TestDiscoveryRequired', Identifiers={}\n        )\n        self.client.describe_endpoints.assert_called_with()\n        key = ()\n        self.assertIn(key, cache)\n        self.assertEqual(cache[key][0]['Address'], 'new.com')\n        self.manager.describe_endpoint(\n            Operation='TestDiscoveryRequired', Identifiers={}\n        )\n        call_count = self.client.describe_endpoints.call_count\n        self.assertEqual(call_count, 1)\n\n    def test_describe_endpoint_expired_entry(self):\n        current_time = time.time()\n        key = ()\n        cache = {\n            key: [{'Address': 'old.com', 'Expiration': current_time - 10}]\n        }\n        self.construct_manager(cache=cache)\n        kwargs = {\n            'Identifiers': {},\n            'Operation': 'TestDiscoveryRequired',\n        }\n        self.manager.describe_endpoint(**kwargs)\n        self.client.describe_endpoints.assert_called_with()\n        self.assertIn(key, cache)\n        self.assertEqual(cache[key][0]['Address'], 'new.com')\n        self.manager.describe_endpoint(**kwargs)\n        call_count = self.client.describe_endpoints.call_count\n        self.assertEqual(call_count, 1)\n\n    def test_describe_endpoint_cache_expiration(self):\n        def _time():\n            return float(0)\n\n        cache = {}\n        self.construct_manager(cache=cache, time=_time)\n        self.manager.describe_endpoint(\n            Operation='TestDiscoveryRequired', Identifiers={}\n        )\n        key = ()\n        self.assertIn(key, cache)\n        self.assertEqual(cache[key][0]['Expiration'], float(120))\n\n    def test_delete_endpoints_present(self):\n        key = ()\n        cache = {key: [{'Address': 'old.com', 'Expiration': 0}]}\n        self.construct_manager(cache=cache)\n        kwargs = {\n            'Identifiers': {},\n            'Operation': 'TestDiscoveryRequired',\n        }\n        self.manager.delete_endpoints(**kwargs)\n        self.assertEqual(cache, {})\n\n    def test_delete_endpoints_absent(self):\n        cache = {}\n        self.construct_manager(cache=cache)\n        kwargs = {\n            'Identifiers': {},\n            'Operation': 'TestDiscoveryRequired',\n        }\n        self.manager.delete_endpoints(**kwargs)\n        self.assertEqual(cache, {})\n\n    def test_describe_endpoint_optional_fails_no_cache(self):\n        side_effect = [ConnectionError(error=None)]\n        self.construct_manager(side_effect=side_effect)\n        kwargs = {'Operation': 'TestDiscoveryOptional'}\n        endpoint = self.manager.describe_endpoint(**kwargs)\n        self.assertIsNone(endpoint)\n        # This second call should be blocked as we just failed\n        endpoint = self.manager.describe_endpoint(**kwargs)\n        self.assertIsNone(endpoint)\n        self.client.describe_endpoints.call_args_list == [mock.call()]\n\n    def test_describe_endpoint_optional_fails_stale_cache(self):\n        key = ()\n        cache = {key: [{'Address': 'old.com', 'Expiration': 0}]}\n        side_effect = [ConnectionError(error=None)] * 2\n        self.construct_manager(cache=cache, side_effect=side_effect)\n        kwargs = {'Operation': 'TestDiscoveryOptional'}\n        endpoint = self.manager.describe_endpoint(**kwargs)\n        self.assertEqual(endpoint, 'old.com')\n        # This second call shouldn't go through as we just failed\n        endpoint = self.manager.describe_endpoint(**kwargs)\n        self.assertEqual(endpoint, 'old.com')\n        self.client.describe_endpoints.call_args_list == [mock.call()]\n\n    def test_describe_endpoint_required_fails_no_cache(self):\n        side_effect = [ConnectionError(error=None)] * 2\n        self.construct_manager(side_effect=side_effect)\n        kwargs = {'Operation': 'TestDiscoveryRequired'}\n        with self.assertRaises(EndpointDiscoveryRefreshFailed):\n            self.manager.describe_endpoint(**kwargs)\n        # This second call should go through, as we have no cache\n        with self.assertRaises(EndpointDiscoveryRefreshFailed):\n            self.manager.describe_endpoint(**kwargs)\n        describe_count = self.client.describe_endpoints.call_count\n        self.assertEqual(describe_count, 2)\n\n    def test_describe_endpoint_required_fails_stale_cache(self):\n        key = ()\n        cache = {key: [{'Address': 'old.com', 'Expiration': 0}]}\n        side_effect = [ConnectionError(error=None)] * 2\n        self.construct_manager(cache=cache, side_effect=side_effect)\n        kwargs = {'Operation': 'TestDiscoveryRequired'}\n        endpoint = self.manager.describe_endpoint(**kwargs)\n        self.assertEqual(endpoint, 'old.com')\n        # We have a stale endpoint, so this shouldn't fail or force a refresh\n        endpoint = self.manager.describe_endpoint(**kwargs)\n        self.assertEqual(endpoint, 'old.com')\n        self.client.describe_endpoints.call_args_list == [mock.call()]\n\n    def test_describe_endpoint_required_force_refresh_success(self):\n        side_effect = [\n            ConnectionError(error=None),\n            {\n                'Endpoints': [\n                    {\n                        'Address': 'new.com',\n                        'CachePeriodInMinutes': 2,\n                    }\n                ]\n            },\n        ]\n        self.construct_manager(side_effect=side_effect)\n        kwargs = {'Operation': 'TestDiscoveryRequired'}\n        # First call will fail\n        with self.assertRaises(EndpointDiscoveryRefreshFailed):\n            self.manager.describe_endpoint(**kwargs)\n        self.client.describe_endpoints.call_args_list == [mock.call()]\n        # Force a refresh if the cache is empty but discovery is required\n        endpoint = self.manager.describe_endpoint(**kwargs)\n        self.assertEqual(endpoint, 'new.com')\n\n    def test_describe_endpoint_retries_after_failing(self):\n        fake_time = mock.Mock()\n        fake_time.side_effect = [0, 100, 200]\n        side_effect = [\n            ConnectionError(error=None),\n            {\n                'Endpoints': [\n                    {\n                        'Address': 'new.com',\n                        'CachePeriodInMinutes': 2,\n                    }\n                ]\n            },\n        ]\n        self.construct_manager(side_effect=side_effect, time=fake_time)\n        kwargs = {'Operation': 'TestDiscoveryOptional'}\n        endpoint = self.manager.describe_endpoint(**kwargs)\n        self.assertIsNone(endpoint)\n        self.client.describe_endpoints.call_args_list == [mock.call()]\n        # Second time should try again as enough time has elapsed\n        endpoint = self.manager.describe_endpoint(**kwargs)\n        self.assertEqual(endpoint, 'new.com')\n\n\nclass TestEndpointDiscoveryHandler(BaseEndpointDiscoveryTest):\n    def setUp(self):\n        super().setUp()\n        self.manager = mock.Mock(spec=EndpointDiscoveryManager)\n        self.handler = EndpointDiscoveryHandler(self.manager)\n        self.service_model = ServiceModel(self.service_description)\n\n    def test_register_handler(self):\n        events = mock.Mock(spec=HierarchicalEmitter)\n        self.handler.register(events, 'foo-bar')\n        events.register.assert_any_call(\n            'before-parameter-build.foo-bar', self.handler.gather_identifiers\n        )\n        events.register.assert_any_call(\n            'needs-retry.foo-bar', self.handler.handle_retries\n        )\n        events.register_first.assert_called_with(\n            'request-created.foo-bar', self.handler.discover_endpoint\n        )\n\n    def test_discover_endpoint(self):\n        request = AWSRequest()\n        request.context = {'discovery': {'identifiers': {}}}\n        self.manager.describe_endpoint.return_value = 'https://new.foo'\n        self.handler.discover_endpoint(request, 'TestOperation')\n        self.assertEqual(request.url, 'https://new.foo')\n        self.manager.describe_endpoint.assert_called_with(\n            Operation='TestOperation', Identifiers={}\n        )\n\n    def test_discover_endpoint_fails(self):\n        request = AWSRequest()\n        request.context = {'discovery': {'identifiers': {}}}\n        request.url = 'old.com'\n        self.manager.describe_endpoint.return_value = None\n        self.handler.discover_endpoint(request, 'TestOperation')\n        self.assertEqual(request.url, 'old.com')\n        self.manager.describe_endpoint.assert_called_with(\n            Operation='TestOperation', Identifiers={}\n        )\n\n    def test_discover_endpoint_no_protocol(self):\n        request = AWSRequest()\n        request.context = {'discovery': {'identifiers': {}}}\n        self.manager.describe_endpoint.return_value = 'new.foo'\n        self.handler.discover_endpoint(request, 'TestOperation')\n        self.assertEqual(request.url, 'https://new.foo')\n        self.manager.describe_endpoint.assert_called_with(\n            Operation='TestOperation', Identifiers={}\n        )\n\n    def test_inject_no_context(self):\n        request = AWSRequest(url='https://original.foo')\n        self.handler.discover_endpoint(request, 'TestOperation')\n        self.assertEqual(request.url, 'https://original.foo')\n        self.manager.describe_endpoint.assert_not_called()\n\n    def test_gather_identifiers(self):\n        context = {}\n        params = {'Foo': 'value1', 'Nested': {'Bar': 'value2'}}\n        ids = {'Foo': 'value1', 'Bar': 'value2'}\n        model = self.service_model.operation_model('TestDiscoveryRequired')\n        self.manager.gather_identifiers.return_value = ids\n        self.handler.gather_identifiers(params, model, context)\n        self.assertEqual(context['discovery']['identifiers'], ids)\n\n    def test_gather_identifiers_not_discoverable(self):\n        context = {}\n        model = self.service_model.operation_model('DescribeEndpoints')\n        self.handler.gather_identifiers({}, model, context)\n        self.assertEqual(context, {})\n\n    def test_discovery_disabled_but_required(self):\n        model = self.service_model.operation_model('TestDiscoveryRequired')\n        with self.assertRaises(EndpointDiscoveryRequired):\n            block_endpoint_discovery_required_operations(model)\n\n    def test_discovery_disabled_but_optional(self):\n        context = {}\n        model = self.service_model.operation_model('TestDiscoveryOptional')\n        block_endpoint_discovery_required_operations(model, context=context)\n        self.assertEqual(context, {})\n\n    def test_does_not_retry_no_response(self):\n        retry = self.handler.handle_retries(None, None, None)\n        self.assertIsNone(retry)\n\n    def test_does_not_retry_other_errors(self):\n        parsed_response = {'ResponseMetadata': {'HTTPStatusCode': 200}}\n        response = (None, parsed_response)\n        retry = self.handler.handle_retries(None, response, None)\n        self.assertIsNone(retry)\n\n    def test_does_not_retry_if_no_context(self):\n        request_dict = {'context': {}}\n        parsed_response = {'ResponseMetadata': {'HTTPStatusCode': 421}}\n        response = (None, parsed_response)\n        retry = self.handler.handle_retries(request_dict, response, None)\n        self.assertIsNone(retry)\n\n    def _assert_retries(self, parsed_response):\n        request_dict = {'context': {'discovery': {'identifiers': {}}}}\n        response = (None, parsed_response)\n        model = self.service_model.operation_model('TestDiscoveryOptional')\n        retry = self.handler.handle_retries(request_dict, response, model)\n        self.assertEqual(retry, 0)\n        self.manager.delete_endpoints.assert_called_with(\n            Operation='TestDiscoveryOptional', Identifiers={}\n        )\n\n    def test_retries_421_status_code(self):\n        parsed_response = {'ResponseMetadata': {'HTTPStatusCode': 421}}\n        self._assert_retries(parsed_response)\n\n    def test_retries_invalid_endpoint_exception(self):\n        parsed_response = {'Error': {'Code': 'InvalidEndpointException'}}\n        self._assert_retries(parsed_response)\n", "tests/unit/test_useragent.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport platform\n\nimport pytest\n\nimport botocore.useragent\nfrom botocore import __version__ as botocore_version\nfrom botocore.config import Config\nfrom botocore.useragent import (\n    UserAgentString,\n    sanitize_user_agent_string_component,\n)\nfrom tests import mock\n\nfrom .. import requires_crt\n\n\n# Returns a list of unmodified User-Agent components.\ndef unmodified_components(components):\n    return components\n\n\n@pytest.mark.parametrize(\n    'raw_str, allow_hash, expected_str',\n    [\n        ('foo', False, 'foo'),\n        ('foo', True, 'foo'),\n        ('ExampleFramework (1.2.3)', False, 'ExampleFramework--1.2.3-'),\n        ('foo#1.2.3', False, 'foo-1.2.3'),\n        ('foo#1.2.3', True, 'foo#1.2.3'),\n        ('', False, ''),\n        ('', True, ''),\n        ('', False, ''),\n        ('#', False, '-'),\n        ('#', True, '#'),\n        (' ', False, '-'),\n        ('  ', False, '--'),\n        ('@=[]{ }/\\\\\u00f8\u00df\u00a9', True, '------------'),\n        (\n            'Java_HotSpot_(TM)_64-Bit_Server_VM/25.151-b12',\n            True,\n            'Java_HotSpot_-TM-_64-Bit_Server_VM-25.151-b12',\n        ),\n    ],\n)\ndef test_sanitize_ua_string_component(raw_str, allow_hash, expected_str):\n    actual_str = sanitize_user_agent_string_component(raw_str, allow_hash)\n    assert actual_str == expected_str\n\n\n@mock.patch.object(\n    botocore.useragent, 'modify_components', unmodified_components\n)\ndef test_basic_user_agent_string():\n    ua = UserAgentString(\n        platform_name='linux',\n        platform_version='1.2.3-foo',\n        platform_machine='x86_64',\n        python_version='3.8.20',\n        python_implementation='Dpython',\n        execution_env='AWS_Lambda_python3.8',\n        crt_version='Unknown',\n    ).with_client_config(\n        Config(retries={'mode': 'legacy'}, user_agent_appid='fooapp')\n    )\n\n    actual = ua.to_string()\n    expected = (\n        f'Botocore/{botocore_version} '\n        'md/awscrt#Unknown '\n        'ua/2.0 '\n        'os/linux#1.2.3-foo '\n        'md/arch#x86_64 '\n        'lang/python#3.8.20 '\n        'md/pyimpl#Dpython '\n        'exec-env/AWS_Lambda_python3.8 '\n        'cfg/retry-mode#legacy '\n        'app/fooapp'\n    )\n    assert actual == expected\n\n\ndef test_shared_test_case():\n    # This test case is shared across AWS SDKs.\n\n    uas = UserAgentString(\n        platform_name=\"Linux\",\n        platform_version=\"5.4.228-131.415.AMZN2.X86_64\",\n        platform_machine=\"\",\n        python_version=\"4.3.2\",\n        python_implementation=None,\n        execution_env='lambda',\n    ).with_client_config(\n        Config(user_agent_appid='123456', retries={'mode': 'standard'})\n    )\n    actual = uas.to_string().split(' ')\n    expected_in_exact_order = [\n        f\"Botocore/{botocore_version}\",\n        \"ua/2.0\",\n        \"os/linux#5.4.228-131.415.AMZN2.X86_64\",\n        \"lang/python#4.3.2\",\n        \"exec-env/lambda\",\n    ]\n    expected_in_any_order = [\n        \"cfg/retry-mode#standard\",\n        \"app/123456\",\n    ]\n    for el in [*expected_in_exact_order, *expected_in_any_order]:\n        assert el in actual\n\n    indices = [actual.index(el) for el in expected_in_exact_order]\n    assert indices == list(sorted(indices)), 'Elements were found out of order'\n\n\n@mock.patch.object(\n    botocore.useragent, 'modify_components', unmodified_components\n)\ndef test_user_agent_string_with_missing_information():\n    # Even when collecting information from the environment fails completely,\n    # some minimal string should be generated.\n    uas = UserAgentString(\n        platform_name=None,\n        platform_version=None,\n        platform_machine=None,\n        python_version=None,\n        python_implementation=None,\n        execution_env=None,\n        crt_version=None,\n    ).with_client_config(Config())\n    actual = uas.to_string()\n    assert actual == f'Botocore/{botocore_version} ua/2.0 os/other lang/python'\n\n\ndef test_from_environment(monkeypatch):\n    monkeypatch.setenv('AWS_EXECUTION_ENV', 'lambda')\n    monkeypatch.setattr(platform, 'system', lambda: 'Linux')\n    monkeypatch.setattr(\n        platform, 'release', lambda: '5.4.228-131.415.AMZN2.X86_64'\n    )\n    monkeypatch.setattr(platform, 'python_version', lambda: '4.3.2')\n    monkeypatch.setattr(platform, 'python_implementation', lambda: 'Cpython')\n\n    uas = UserAgentString.from_environment()\n\n    assert uas._execution_env == 'lambda'\n    assert uas._platform_name == 'Linux'\n    assert uas._platform_version == '5.4.228-131.415.AMZN2.X86_64'\n    assert uas._python_version == '4.3.2'\n    assert uas._python_implementation == 'Cpython'\n\n\n@requires_crt()\ndef test_from_environment_can_read_crt_version(monkeypatch):\n    import awscrt\n\n    monkeypatch.setattr(awscrt, '__version__', 'a.b.c')\n    uas = UserAgentString.from_environment()\n    assert uas._crt_version == 'a.b.c'\n\n\ndef test_from_environment_with_most_values_not_available(monkeypatch):\n    # Asserts that ``None`` values are properly passed through to the\n    # UserAgentString class. There are separate tests to assert that\n    # ``UserAgentString.to_string()`` can handle ``None`` values.\n    monkeypatch.delenv('AWS_EXECUTION_ENV', raising=False)\n    monkeypatch.setattr(platform, 'system', lambda: None)\n    monkeypatch.setattr(platform, 'release', lambda: None)\n    monkeypatch.setattr(platform, 'python_version', lambda: None)\n    monkeypatch.setattr(platform, 'python_implementation', lambda: None)\n\n    uas = UserAgentString.from_environment()\n\n    assert uas._execution_env is None\n    assert uas._platform_name is None\n    assert uas._platform_version is None\n    assert uas._python_version is None\n    assert uas._python_implementation is None\n\n\ndef test_from_environment_unknown_platform(monkeypatch):\n    monkeypatch.setattr(platform, 'system', lambda: 'FooOS')\n    monkeypatch.setattr(platform, 'release', lambda: '0.0.1')\n    uas = UserAgentString.from_environment()\n    assert ' os/other md/FooOS#0.0.1 ' in uas.to_string()\n", "tests/unit/test_serialize.py": "\"\"\"Additional tests for request serialization.\n\nWhile there are compliance tests in tests/unit/protocols where\nthe majority of the request serialization/response parsing is tested,\nthis test module contains additional tests that go above and beyond the\nspec.  This can happen for a number of reasons:\n\n* We are testing python specific behavior that doesn't make sense as a\n  compliance test.\n* We are testing behavior that is not strictly part of the spec.  These\n  may result in a a coverage gap that would otherwise be untested.\n\n\"\"\"\nimport base64\nimport datetime\nimport io\nimport json\n\nimport dateutil.tz\n\nfrom botocore import serialize\nfrom botocore.exceptions import ParamValidationError\nfrom botocore.model import ServiceModel\nfrom tests import unittest\n\n\nclass BaseModelWithBlob(unittest.TestCase):\n    def setUp(self):\n        self.model = {\n            'metadata': {'protocol': 'query', 'apiVersion': '2014-01-01'},\n            'documentation': '',\n            'operations': {\n                'TestOperation': {\n                    'name': 'TestOperation',\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'input': {'shape': 'InputShape'},\n                }\n            },\n            'shapes': {\n                'InputShape': {\n                    'type': 'structure',\n                    'members': {\n                        'Blob': {'shape': 'BlobType'},\n                    },\n                },\n                'BlobType': {\n                    'type': 'blob',\n                },\n            },\n        }\n\n    def serialize_to_request(self, input_params):\n        service_model = ServiceModel(self.model)\n        request_serializer = serialize.create_serializer(\n            service_model.metadata['protocol']\n        )\n        return request_serializer.serialize_to_request(\n            input_params, service_model.operation_model('TestOperation')\n        )\n\n    def assert_serialized_blob_equals(self, request, blob_bytes):\n        # This method handles all the details of the base64 decoding.\n        encoded = base64.b64encode(blob_bytes)\n        # Now the serializers actually have the base64 encoded contents\n        # as str types so we need to decode back.  We know that this is\n        # ascii so it's safe to use the ascii encoding.\n        expected = encoded.decode('ascii')\n        self.assertEqual(request['body']['Blob'], expected)\n\n\nclass TestBinaryTypes(BaseModelWithBlob):\n    def test_blob_accepts_bytes_type(self):\n        body = b'bytes body'\n        request = self.serialize_to_request(input_params={'Blob': body})\n        self.assert_serialized_blob_equals(request, blob_bytes=body)\n\n    def test_blob_accepts_str_type(self):\n        body = 'ascii text'\n        request = self.serialize_to_request(input_params={'Blob': body})\n        self.assert_serialized_blob_equals(\n            request, blob_bytes=body.encode('ascii')\n        )\n\n    def test_blob_handles_unicode_chars(self):\n        body = '\\u2713'\n        request = self.serialize_to_request(input_params={'Blob': body})\n        self.assert_serialized_blob_equals(\n            request, blob_bytes=body.encode('utf-8')\n        )\n\n\nclass TestBinaryTypesJSON(BaseModelWithBlob):\n    def setUp(self):\n        super().setUp()\n        self.model['metadata'] = {\n            'protocol': 'json',\n            'apiVersion': '2014-01-01',\n            'jsonVersion': '1.1',\n            'targetPrefix': 'foo',\n        }\n\n    def test_blob_accepts_bytes_type(self):\n        body = b'bytes body'\n        request = self.serialize_to_request(input_params={'Blob': body})\n        serialized_blob = json.loads(request['body'].decode('utf-8'))['Blob']\n        self.assertEqual(\n            base64.b64encode(body).decode('ascii'), serialized_blob\n        )\n\n\nclass TestBinaryTypesWithRestXML(BaseModelWithBlob):\n    def setUp(self):\n        super().setUp()\n        self.model['metadata'] = {\n            'protocol': 'rest-xml',\n            'apiVersion': '2014-01-01',\n        }\n        self.model['operations']['TestOperation']['input'] = {\n            'shape': 'InputShape',\n            'locationName': 'OperationRequest',\n            'payload': 'Blob',\n        }\n\n    def test_blob_serialization_with_file_like_object(self):\n        body = io.BytesIO(b'foobar')\n        request = self.serialize_to_request(input_params={'Blob': body})\n        self.assertEqual(request['body'], body)\n\n    def test_blob_serialization_when_payload_is_unicode(self):\n        # When the body is a text type, we should encode the\n        # text to bytes.\n        body = '\\u2713'\n        request = self.serialize_to_request(input_params={'Blob': body})\n        self.assertEqual(request['body'], body.encode('utf-8'))\n\n    def test_blob_serialization_when_payload_is_bytes(self):\n        body = b'bytes body'\n        request = self.serialize_to_request(input_params={'Blob': body})\n        self.assertEqual(request['body'], body)\n\n\nclass TestTimestampHeadersWithRestXML(unittest.TestCase):\n    def setUp(self):\n        self.model = {\n            'metadata': {'protocol': 'rest-xml', 'apiVersion': '2014-01-01'},\n            'documentation': '',\n            'operations': {\n                'TestOperation': {\n                    'name': 'TestOperation',\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'input': {'shape': 'InputShape'},\n                }\n            },\n            'shapes': {\n                'InputShape': {\n                    'type': 'structure',\n                    'members': {\n                        'TimestampHeader': {\n                            'shape': 'TimestampType',\n                            'location': 'header',\n                            'locationName': 'x-timestamp',\n                        },\n                    },\n                },\n                'TimestampType': {\n                    'type': 'timestamp',\n                },\n            },\n        }\n        self.service_model = ServiceModel(self.model)\n\n    def serialize_to_request(self, input_params):\n        request_serializer = serialize.create_serializer(\n            self.service_model.metadata['protocol']\n        )\n        return request_serializer.serialize_to_request(\n            input_params, self.service_model.operation_model('TestOperation')\n        )\n\n    def test_accepts_datetime_object(self):\n        request = self.serialize_to_request(\n            {\n                'TimestampHeader': datetime.datetime(\n                    2014, 1, 1, 12, 12, 12, tzinfo=dateutil.tz.tzutc()\n                )\n            }\n        )\n        self.assertEqual(\n            request['headers']['x-timestamp'], 'Wed, 01 Jan 2014 12:12:12 GMT'\n        )\n\n    def test_accepts_iso_8601_format(self):\n        request = self.serialize_to_request(\n            {'TimestampHeader': '2014-01-01T12:12:12+00:00'}\n        )\n        self.assertEqual(\n            request['headers']['x-timestamp'], 'Wed, 01 Jan 2014 12:12:12 GMT'\n        )\n\n    def test_accepts_iso_8601_format_non_utc(self):\n        request = self.serialize_to_request(\n            {'TimestampHeader': '2014-01-01T07:12:12-05:00'}\n        )\n        self.assertEqual(\n            request['headers']['x-timestamp'], 'Wed, 01 Jan 2014 12:12:12 GMT'\n        )\n\n    def test_accepts_rfc_822_format(self):\n        request = self.serialize_to_request(\n            {'TimestampHeader': 'Wed, 01 Jan 2014 12:12:12 GMT'}\n        )\n        self.assertEqual(\n            request['headers']['x-timestamp'], 'Wed, 01 Jan 2014 12:12:12 GMT'\n        )\n\n    def test_accepts_unix_timestamp_integer(self):\n        request = self.serialize_to_request({'TimestampHeader': 1388578332})\n        self.assertEqual(\n            request['headers']['x-timestamp'], 'Wed, 01 Jan 2014 12:12:12 GMT'\n        )\n\n\nclass TestTimestamps(unittest.TestCase):\n    def setUp(self):\n        self.model = {\n            'metadata': {'protocol': 'query', 'apiVersion': '2014-01-01'},\n            'documentation': '',\n            'operations': {\n                'TestOperation': {\n                    'name': 'TestOperation',\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'input': {'shape': 'InputShape'},\n                }\n            },\n            'shapes': {\n                'InputShape': {\n                    'type': 'structure',\n                    'members': {\n                        'Timestamp': {'shape': 'TimestampType'},\n                    },\n                },\n                'TimestampType': {\n                    'type': 'timestamp',\n                },\n            },\n        }\n        self.service_model = ServiceModel(self.model)\n\n    def serialize_to_request(self, input_params):\n        request_serializer = serialize.create_serializer(\n            self.service_model.metadata['protocol']\n        )\n        return request_serializer.serialize_to_request(\n            input_params, self.service_model.operation_model('TestOperation')\n        )\n\n    def test_accepts_datetime_object(self):\n        request = self.serialize_to_request(\n            {\n                'Timestamp': datetime.datetime(\n                    2014, 1, 1, 12, 12, 12, tzinfo=dateutil.tz.tzutc()\n                )\n            }\n        )\n        self.assertEqual(request['body']['Timestamp'], '2014-01-01T12:12:12Z')\n\n    def test_accepts_naive_datetime_object(self):\n        request = self.serialize_to_request(\n            {'Timestamp': datetime.datetime(2014, 1, 1, 12, 12, 12)}\n        )\n        self.assertEqual(request['body']['Timestamp'], '2014-01-01T12:12:12Z')\n\n    def test_accepts_iso_8601_format(self):\n        request = self.serialize_to_request(\n            {'Timestamp': '2014-01-01T12:12:12Z'}\n        )\n        self.assertEqual(request['body']['Timestamp'], '2014-01-01T12:12:12Z')\n\n    def test_accepts_timestamp_without_tz_info(self):\n        # If a timezone/utc is not specified, assume they meant\n        # UTC.  This is also the previous behavior from older versions\n        # of botocore so we want to make sure we preserve this behavior.\n        request = self.serialize_to_request(\n            {'Timestamp': '2014-01-01T12:12:12'}\n        )\n        self.assertEqual(request['body']['Timestamp'], '2014-01-01T12:12:12Z')\n\n    def test_microsecond_timestamp_without_tz_info(self):\n        request = self.serialize_to_request(\n            {'Timestamp': '2014-01-01T12:12:12.123456'}\n        )\n        self.assertEqual(\n            request['body']['Timestamp'], '2014-01-01T12:12:12.123456Z'\n        )\n\n\nclass TestJSONTimestampSerialization(unittest.TestCase):\n    def setUp(self):\n        self.model = {\n            'metadata': {\n                'protocol': 'json',\n                'apiVersion': '2014-01-01',\n                'jsonVersion': '1.1',\n                'targetPrefix': 'foo',\n            },\n            'documentation': '',\n            'operations': {\n                'TestOperation': {\n                    'name': 'TestOperation',\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'input': {'shape': 'InputShape'},\n                }\n            },\n            'shapes': {\n                'InputShape': {\n                    'type': 'structure',\n                    'members': {\n                        'Timestamp': {'shape': 'TimestampType'},\n                    },\n                },\n                'TimestampType': {\n                    'type': 'timestamp',\n                },\n            },\n        }\n        self.service_model = ServiceModel(self.model)\n\n    def serialize_to_request(self, input_params):\n        request_serializer = serialize.create_serializer(\n            self.service_model.metadata['protocol']\n        )\n        return request_serializer.serialize_to_request(\n            input_params, self.service_model.operation_model('TestOperation')\n        )\n\n    def test_accepts_iso_8601_format(self):\n        body = json.loads(\n            self.serialize_to_request({'Timestamp': '1970-01-01T00:00:00'})[\n                'body'\n            ].decode('utf-8')\n        )\n        self.assertEqual(body['Timestamp'], 0)\n\n    def test_accepts_epoch(self):\n        body = json.loads(\n            self.serialize_to_request({'Timestamp': '0'})['body'].decode(\n                'utf-8'\n            )\n        )\n        self.assertEqual(body['Timestamp'], 0)\n        # Can also be an integer 0.\n        body = json.loads(\n            self.serialize_to_request({'Timestamp': 0})['body'].decode('utf-8')\n        )\n        self.assertEqual(body['Timestamp'], 0)\n\n    def test_accepts_partial_iso_format(self):\n        body = json.loads(\n            self.serialize_to_request({'Timestamp': '1970-01-01'})[\n                'body'\n            ].decode('utf-8')\n        )\n        self.assertEqual(body['Timestamp'], 0)\n\n\nclass TestInstanceCreation(unittest.TestCase):\n    def setUp(self):\n        self.model = {\n            'metadata': {'protocol': 'query', 'apiVersion': '2014-01-01'},\n            'documentation': '',\n            'operations': {\n                'TestOperation': {\n                    'name': 'TestOperation',\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'input': {'shape': 'InputShape'},\n                }\n            },\n            'shapes': {\n                'InputShape': {\n                    'type': 'structure',\n                    'members': {\n                        'Timestamp': {'shape': 'StringTestType'},\n                    },\n                },\n                'StringTestType': {'type': 'string', 'min': 15},\n            },\n        }\n        self.service_model = ServiceModel(self.model)\n\n    def assert_serialize_valid_parameter(self, request_serializer):\n        valid_string = 'valid_string_with_min_15_chars'\n        request = request_serializer.serialize_to_request(\n            {'Timestamp': valid_string},\n            self.service_model.operation_model('TestOperation'),\n        )\n\n        self.assertEqual(request['body']['Timestamp'], valid_string)\n\n    def assert_serialize_invalid_parameter(self, request_serializer):\n        invalid_string = 'short string'\n        request = request_serializer.serialize_to_request(\n            {'Timestamp': invalid_string},\n            self.service_model.operation_model('TestOperation'),\n        )\n\n        self.assertEqual(request['body']['Timestamp'], invalid_string)\n\n    def test_instantiate_without_validation(self):\n        request_serializer = serialize.create_serializer(\n            self.service_model.metadata['protocol'], False\n        )\n\n        try:\n            self.assert_serialize_valid_parameter(request_serializer)\n        except ParamValidationError as e:\n            self.fail(\n                \"Shouldn't fail serializing valid parameter without \"\n                \"validation: {}\".format(e)\n            )\n\n        try:\n            self.assert_serialize_invalid_parameter(request_serializer)\n        except ParamValidationError as e:\n            self.fail(\n                \"Shouldn't fail serializing invalid parameter without \"\n                \"validation: {}\".format(e)\n            )\n\n    def test_instantiate_with_validation(self):\n        request_serializer = serialize.create_serializer(\n            self.service_model.metadata['protocol'], True\n        )\n        try:\n            self.assert_serialize_valid_parameter(request_serializer)\n        except ParamValidationError as e:\n            self.fail(\n                \"Shouldn't fail serializing invalid parameter without \"\n                \"validation: {}\".format(e)\n            )\n\n        with self.assertRaises(ParamValidationError):\n            self.assert_serialize_invalid_parameter(request_serializer)\n\n\nclass TestHeaderSerialization(BaseModelWithBlob):\n    def setUp(self):\n        self.model = {\n            'metadata': {'protocol': 'rest-xml', 'apiVersion': '2014-01-01'},\n            'documentation': '',\n            'operations': {\n                'TestOperation': {\n                    'name': 'TestOperation',\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'input': {'shape': 'InputShape'},\n                }\n            },\n            'shapes': {\n                'InputShape': {\n                    'type': 'structure',\n                    'members': {\n                        'ContentLength': {\n                            'shape': 'Integer',\n                            'location': 'header',\n                            'locationName': 'Content-Length',\n                        },\n                    },\n                },\n                'Integer': {'type': 'integer'},\n            },\n        }\n        self.service_model = ServiceModel(self.model)\n\n    def test_always_serialized_as_str(self):\n        request = self.serialize_to_request({'ContentLength': 100})\n        self.assertEqual(request['headers']['Content-Length'], '100')\n\n\nclass TestRestXMLUnicodeSerialization(unittest.TestCase):\n    def setUp(self):\n        self.model = {\n            'metadata': {'protocol': 'rest-xml', 'apiVersion': '2014-01-01'},\n            'documentation': '',\n            'operations': {\n                'TestOperation': {\n                    'name': 'TestOperation',\n                    'http': {\n                        'method': 'POST',\n                        'requestUri': '/',\n                    },\n                    'input': {'shape': 'InputShape'},\n                }\n            },\n            'shapes': {\n                'InputShape': {\n                    'type': 'structure',\n                    'members': {\n                        'Foo': {'shape': 'FooShape', 'locationName': 'Foo'},\n                    },\n                    'payload': 'Foo',\n                },\n                'FooShape': {\n                    'type': 'list',\n                    'member': {'shape': 'StringShape'},\n                },\n                'StringShape': {\n                    'type': 'string',\n                },\n            },\n        }\n        self.service_model = ServiceModel(self.model)\n\n    def serialize_to_request(self, input_params):\n        request_serializer = serialize.create_serializer(\n            self.service_model.metadata['protocol']\n        )\n        return request_serializer.serialize_to_request(\n            input_params, self.service_model.operation_model('TestOperation')\n        )\n\n    def test_restxml_serializes_unicode(self):\n        params = {'Foo': ['\\u65e5\\u672c\\u8a9e\\u3067\\u304a\\uff4b']}\n        try:\n            self.serialize_to_request(params)\n        except UnicodeEncodeError:\n            self.fail(\"RestXML serializer failed to serialize unicode text.\")\n", "tests/unit/test_config_provider.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport os\n\nimport pytest\n\nimport botocore.configprovider\nimport botocore.session as session\nfrom botocore.configprovider import (\n    BaseProvider,\n    ChainProvider,\n    ConfigChainFactory,\n    ConfiguredEndpointProvider,\n    ConfigValueStore,\n    ConstantProvider,\n    DefaultConfigResolver,\n    EnvironmentProvider,\n    InstanceVarProvider,\n    ScopedConfigProvider,\n    SectionConfigProvider,\n    SmartDefaultsConfigStoreFactory,\n)\nfrom botocore.exceptions import ConnectTimeoutError\nfrom botocore.utils import IMDSRegionProvider\nfrom tests import mock, unittest\n\n\nclass TestConfigChainFactory(unittest.TestCase):\n    def assert_chain_does_provide(\n        self,\n        instance_map,\n        environ_map,\n        scoped_config_map,\n        create_config_chain_args,\n        expected_value,\n    ):\n        fake_session = mock.Mock(spec=session.Session)\n        fake_session.get_scoped_config.return_value = scoped_config_map\n        fake_session.instance_variables.return_value = instance_map\n        builder = ConfigChainFactory(fake_session, environ=environ_map)\n        chain = builder.create_config_chain(**create_config_chain_args)\n        value = chain.provide()\n        self.assertEqual(value, expected_value)\n\n    def test_chain_builder_can_provide_instance(self):\n        self.assert_chain_does_provide(\n            instance_map={'instance_var': 'from-instance'},\n            environ_map={},\n            scoped_config_map={},\n            create_config_chain_args={\n                'instance_name': 'instance_var',\n            },\n            expected_value='from-instance',\n        )\n\n    def test_chain_builder_can_skip_instance(self):\n        self.assert_chain_does_provide(\n            instance_map={'wrong_instance_var': 'instance'},\n            environ_map={'ENV_VAR': 'env'},\n            scoped_config_map={},\n            create_config_chain_args={\n                'instance_name': 'instance_var',\n                'env_var_names': 'ENV_VAR',\n            },\n            expected_value='env',\n        )\n\n    def test_chain_builder_can_provide_env_var(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={'ENV_VAR': 'from-env'},\n            scoped_config_map={},\n            create_config_chain_args={\n                'env_var_names': 'ENV_VAR',\n            },\n            expected_value='from-env',\n        )\n\n    def test_does_provide_none_if_no_variable_exists_in_env_var_list(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={},\n            create_config_chain_args={\n                'env_var_names': ['FOO'],\n            },\n            expected_value=None,\n        )\n\n    def test_does_provide_value_if_variable_exists_in_env_var_list(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={'FOO': 'bar'},\n            scoped_config_map={},\n            create_config_chain_args={\n                'env_var_names': ['FOO'],\n            },\n            expected_value='bar',\n        )\n\n    def test_does_provide_first_non_none_value_first_in_env_var_list(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={'FOO': 'baz'},\n            scoped_config_map={},\n            create_config_chain_args={\n                'env_var_names': ['FOO', 'BAR'],\n            },\n            expected_value='baz',\n        )\n\n    def test_does_provide_first_non_none_value_second_in_env_var_list(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={'BAR': 'baz'},\n            scoped_config_map={},\n            create_config_chain_args={\n                'env_var_names': ['FOO', 'BAR'],\n            },\n            expected_value='baz',\n        )\n\n    def test_does_provide_none_if_all_list_env_vars_are_none(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={},\n            create_config_chain_args={\n                'env_var_names': ['FOO', 'BAR'],\n            },\n            expected_value=None,\n        )\n\n    def test_does_provide_first_value_when_both_env_vars_exist(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={'FOO': 'baz', 'BAR': 'buz'},\n            scoped_config_map={},\n            create_config_chain_args={\n                'env_var_names': ['FOO', 'BAR'],\n            },\n            expected_value='baz',\n        )\n\n    def test_chain_builder_can_provide_config_var(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={'config_var': 'from-config'},\n            create_config_chain_args={\n                'config_property_names': 'config_var',\n            },\n            expected_value='from-config',\n        )\n\n    def test_chain_builder_can_provide_nested_config_var(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={'config_var': {'nested-key': 'nested-val'}},\n            create_config_chain_args={\n                'config_property_names': ('config_var', 'nested-key'),\n            },\n            expected_value='nested-val',\n        )\n\n    def test_provide_value_from_config_list(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={'var': 'val'},\n            create_config_chain_args={\n                'config_property_names': ['var'],\n            },\n            expected_value='val',\n        )\n\n    def test_provide_value_from_config_list_looks_for_non_none_vals(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={'non_none_var': 'non_none_val'},\n            create_config_chain_args={\n                'config_property_names': ['none_var', 'non_none_var'],\n            },\n            expected_value='non_none_val',\n        )\n\n    def test_provide_value_from_config_list_retrieves_first_non_none_val(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={'first': 'first_val', 'second': 'second_val'},\n            create_config_chain_args={\n                'config_property_names': ['first', 'second'],\n            },\n            expected_value='first_val',\n        )\n\n    def test_provide_value_from_config_list_if_all_vars_are_none(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={},\n            create_config_chain_args={\n                'config_property_names': ['config1', 'config2'],\n            },\n            expected_value=None,\n        )\n\n    def test_provide_value_from_list_with_nested_var(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={'section': {'nested_var': 'nested_val'}},\n            create_config_chain_args={\n                'config_property_names': [('section', 'nested_var')],\n            },\n            expected_value='nested_val',\n        )\n\n    def test_chain_builder_can_provide_default(self):\n        self.assert_chain_does_provide(\n            instance_map={},\n            environ_map={},\n            scoped_config_map={},\n            create_config_chain_args={'default': 'from-default'},\n            expected_value='from-default',\n        )\n\n    def test_chain_provider_does_follow_priority_instance_var(self):\n        self.assert_chain_does_provide(\n            instance_map={'instance_var': 'from-instance'},\n            environ_map={'ENV_VAR': 'from-env'},\n            scoped_config_map={'config_var': 'from-config'},\n            create_config_chain_args={\n                'instance_name': 'instance_var',\n                'env_var_names': 'ENV_VAR',\n                'config_property_names': 'config_var',\n                'default': 'from-default',\n            },\n            expected_value='from-instance',\n        )\n\n    def test_chain_provider_does_follow_priority_env_var(self):\n        self.assert_chain_does_provide(\n            instance_map={'wrong_instance_var': 'from-instance'},\n            environ_map={'ENV_VAR': 'from-env'},\n            scoped_config_map={'config_var': 'from-confi'},\n            create_config_chain_args={\n                'instance_name': 'instance_var',\n                'env_var_names': 'ENV_VAR',\n                'config_property_names': 'config_var',\n                'default': 'from-default',\n            },\n            expected_value='from-env',\n        )\n\n    def test_chain_provider_does_follow_priority_config(self):\n        self.assert_chain_does_provide(\n            instance_map={'wrong_instance_var': 'from-instance'},\n            environ_map={'WRONG_ENV_VAR': 'from-env'},\n            scoped_config_map={'config_var': 'from-config'},\n            create_config_chain_args={\n                'instance_name': 'instance_var',\n                'env_var_names': 'ENV_VAR',\n                'config_property_names': 'config_var',\n                'default': 'from-default',\n            },\n            expected_value='from-config',\n        )\n\n    def test_chain_provider_does_follow_priority_default(self):\n        self.assert_chain_does_provide(\n            instance_map={'wrong_instance_var': 'from-instance'},\n            environ_map={'WRONG_ENV_VAR': 'from-env'},\n            scoped_config_map={'wrong_config_var': 'from-config'},\n            create_config_chain_args={\n                'instance_name': 'instance_var',\n                'env_var_names': 'ENV_VAR',\n                'config_property_names': 'config_var',\n                'default': 'from-default',\n            },\n            expected_value='from-default',\n        )\n\n\nclass TestConfigValueStore(unittest.TestCase):\n    def test_does_provide_none_if_no_variable_exists(self):\n        provider = ConfigValueStore()\n        value = provider.get_config_variable('fake_variable')\n        self.assertIsNone(value)\n\n    def test_does_provide_value_if_variable_exists(self):\n        mock_value_provider = mock.Mock(spec=BaseProvider)\n        mock_value_provider.provide.return_value = 'foo'\n        provider = ConfigValueStore(\n            mapping={\n                'fake_variable': mock_value_provider,\n            }\n        )\n        value = provider.get_config_variable('fake_variable')\n        self.assertEqual(value, 'foo')\n\n    def test_can_set_variable(self):\n        provider = ConfigValueStore()\n        provider.set_config_variable('fake_variable', 'foo')\n        value = provider.get_config_variable('fake_variable')\n        self.assertEqual(value, 'foo')\n\n    def test_can_set_config_provider(self):\n        foo_value_provider = mock.Mock(spec=BaseProvider)\n        foo_value_provider.provide.return_value = 'foo'\n        provider = ConfigValueStore(\n            mapping={\n                'fake_variable': foo_value_provider,\n            }\n        )\n\n        value = provider.get_config_variable('fake_variable')\n        self.assertEqual(value, 'foo')\n\n        bar_value_provider = mock.Mock(spec=BaseProvider)\n        bar_value_provider.provide.return_value = 'bar'\n        provider.set_config_provider('fake_variable', bar_value_provider)\n\n        value = provider.get_config_variable('fake_variable')\n        self.assertEqual(value, 'bar')\n\n    def test_can_get_config_provider(self):\n        chain_provider = ChainProvider(\n            providers=[ConstantProvider(value='bar')]\n        )\n        config_value_store = ConfigValueStore(\n            mapping={\n                'fake_variable': chain_provider,\n            }\n        )\n        provider = config_value_store.get_config_provider('fake_variable')\n        value = config_value_store.get_config_variable('fake_variable')\n        self.assertIsInstance(provider, ChainProvider)\n        self.assertEqual(value, 'bar')\n\n    def test_can_get_config_provider_non_chain_provider(self):\n        constant_provider = ConstantProvider(value='bar')\n        config_value_store = ConfigValueStore(\n            mapping={\n                'fake_variable': constant_provider,\n            }\n        )\n        provider = config_value_store.get_config_provider('fake_variable')\n        value = config_value_store.get_config_variable('fake_variable')\n        self.assertIsInstance(provider, ConstantProvider)\n        self.assertEqual(value, 'bar')\n\n    def test_deepcopy_preserves_overrides(self):\n        provider = ConstantProvider(100)\n        config_store = ConfigValueStore(mapping={'fake_variable': provider})\n        config_store.set_config_variable('fake_variable', 'override-value')\n\n        config_store_deepcopy = copy.deepcopy(config_store)\n\n        value = config_store_deepcopy.get_config_variable('fake_variable')\n        self.assertEqual(value, 'override-value')\n\n    def test_copy_preserves_provider_identities(self):\n        fake_variable_provider = ConstantProvider(100)\n        config_store = ConfigValueStore(\n            mapping={\n                'fake_variable': fake_variable_provider,\n            }\n        )\n\n        config_store_copy = copy.copy(config_store)\n\n        self.assertIs(\n            config_store.get_config_provider('fake_variable'),\n            config_store_copy.get_config_provider('fake_variable'),\n        )\n\n    def test_copy_preserves_overrides(self):\n        provider = ConstantProvider(100)\n        config_store = ConfigValueStore(mapping={'fake_variable': provider})\n        config_store.set_config_variable('fake_variable', 'override-value')\n\n        config_store_copy = copy.copy(config_store)\n\n        value = config_store_copy.get_config_variable('fake_variable')\n        self.assertEqual(value, 'override-value')\n\n    def test_copy_update_does_not_mutate_source_config_store(self):\n        fake_variable_provider = ConstantProvider(100)\n        config_store = ConfigValueStore(\n            mapping={\n                'fake_variable': fake_variable_provider,\n            }\n        )\n\n        config_store_copy = copy.copy(config_store)\n\n        another_variable_provider = ConstantProvider('ABC')\n\n        config_store_copy.set_config_provider(\n            'fake_variable', another_variable_provider\n        )\n\n        assert config_store.get_config_variable('fake_variable') == 100\n        assert config_store_copy.get_config_variable('fake_variable') == 'ABC'\n\n\nclass TestInstanceVarProvider(unittest.TestCase):\n    def assert_provides_value(self, name, instance_map, expected_value):\n        fake_session = mock.Mock(spec=session.Session)\n        fake_session.instance_variables.return_value = instance_map\n\n        provider = InstanceVarProvider(\n            instance_var=name,\n            session=fake_session,\n        )\n        value = provider.provide()\n        self.assertEqual(value, expected_value)\n\n    def test_can_provide_value(self):\n        self.assert_provides_value(\n            name='foo',\n            instance_map={'foo': 'bar'},\n            expected_value='bar',\n        )\n\n    def test_does_provide_none_if_value_not_in_dict(self):\n        self.assert_provides_value(\n            name='foo',\n            instance_map={},\n            expected_value=None,\n        )\n\n\nclass TestEnvironmentProvider(unittest.TestCase):\n    def assert_does_provide(self, env, name, expected_value):\n        provider = EnvironmentProvider(name=name, env=env)\n        value = provider.provide()\n        self.assertEqual(value, expected_value)\n\n    def test_does_provide_none_if_no_variable_exists(self):\n        self.assert_does_provide(\n            name='FOO',\n            env={},\n            expected_value=None,\n        )\n\n    def test_does_provide_value_if_variable_exists(self):\n        self.assert_does_provide(\n            name='FOO',\n            env={\n                'FOO': 'bar',\n            },\n            expected_value='bar',\n        )\n\n\nclass TestScopedConfigProvider(unittest.TestCase):\n    def assert_provides_value(\n        self, config_file_values, config_var_name, expected_value\n    ):\n        fake_session = mock.Mock(spec=session.Session)\n        fake_session.get_scoped_config.return_value = config_file_values\n        property_provider = ScopedConfigProvider(\n            config_var_name=config_var_name,\n            session=fake_session,\n        )\n        value = property_provider.provide()\n        self.assertEqual(value, expected_value)\n\n    def test_can_provide_value(self):\n        self.assert_provides_value(\n            config_file_values={'foo': 'bar'},\n            config_var_name='foo',\n            expected_value='bar',\n        )\n\n    def test_does_provide_none_if_var_not_in_config(self):\n        self.assert_provides_value(\n            config_file_values={'foo': 'bar'},\n            config_var_name='no_such_var',\n            expected_value=None,\n        )\n\n    def test_provide_nested_value(self):\n        self.assert_provides_value(\n            config_file_values={'section': {'nested_var': 'nested_val'}},\n            config_var_name=('section', 'nested_var'),\n            expected_value='nested_val',\n        )\n\n    def test_provide_nested_value_but_not_section(self):\n        self.assert_provides_value(\n            config_file_values={'section': 'not-nested'},\n            config_var_name=('section', 'nested_var'),\n            expected_value=None,\n        )\n\n\ndef _make_provider_that_returns(return_value):\n    provider = mock.Mock(spec=BaseProvider)\n    provider.provide.return_value = return_value\n    return provider\n\n\ndef _make_providers_that_return(return_values):\n    mocks = []\n    for return_value in return_values:\n        provider = _make_provider_that_returns(return_value)\n        mocks.append(provider)\n    return mocks\n\n\ndef assert_chain_does_provide(providers, expected_value):\n    provider = ChainProvider(\n        providers=providers,\n    )\n    assert provider.provide() == expected_value\n\n\n@pytest.mark.parametrize(\n    'case',\n    (\n        (None, []),\n        (None, [None]),\n        ('foo', ['foo']),\n        ('foo', ['foo', 'bar']),\n        ('bar', [None, 'bar']),\n        ('foo', ['foo', None]),\n        ('baz', [None, None, 'baz']),\n        ('bar', [None, 'bar', None]),\n        ('foo', ['foo', 'bar', None]),\n        ('foo', ['foo', 'bar', 'baz']),\n    ),\n)\ndef test_chain_provider(case):\n    # Each case is a tuple with the first element being the expected return\n    # value from the ChainProvider. The second value being a list of return\n    # values from the individual providers that are in the chain.\n    assert_chain_does_provide(_make_providers_that_return(case[1]), case[0])\n\n\nclass TestChainProvider(unittest.TestCase):\n    def test_can_convert_provided_value(self):\n        chain_provider = ChainProvider(\n            providers=_make_providers_that_return(['1']),\n            conversion_func=int,\n        )\n        value = chain_provider.provide()\n        self.assertIsInstance(value, int)\n        self.assertEqual(value, 1)\n\n\nclass TestConstantProvider(unittest.TestCase):\n    def test_can_provide_value(self):\n        provider = ConstantProvider(value='foo')\n        value = provider.provide()\n        self.assertEqual(value, 'foo')\n\n\nclass TestSectionConfigProvider(unittest.TestCase):\n    def assert_provides_value(\n        self,\n        config_file_values,\n        section_name,\n        expected_value,\n        override_providers=None,\n    ):\n        fake_session = mock.Mock(spec=session.Session)\n        fake_session.get_scoped_config.return_value = config_file_values\n        provider = SectionConfigProvider(\n            section_name=section_name,\n            session=fake_session,\n            override_providers=override_providers,\n        )\n        value = provider.provide()\n        self.assertEqual(value, expected_value)\n\n    def test_provide_section_config(self):\n        self.assert_provides_value(\n            config_file_values={'mysection': {'section_var': 'section_val'}},\n            section_name='mysection',\n            expected_value={'section_var': 'section_val'},\n        )\n\n    def test_provide_service_config_missing_service(self):\n        self.assert_provides_value(\n            config_file_values={},\n            section_name='mysection',\n            expected_value=None,\n        )\n\n    def test_provide_service_config_not_a_section(self):\n        self.assert_provides_value(\n            config_file_values={'myservice': 'not-a-section'},\n            section_name='mysection',\n            expected_value=None,\n        )\n\n    def test_provide_section_config_with_overrides(self):\n        self.assert_provides_value(\n            config_file_values={\n                'mysection': {\n                    'override_var': 'from_config_file',\n                    'no_override_var': 'from_config_file',\n                }\n            },\n            section_name='mysection',\n            override_providers={'override_var': ConstantProvider('override')},\n            expected_value={\n                'override_var': 'override',\n                'no_override_var': 'from_config_file',\n            },\n        )\n\n    def test_provide_section_config_with_only_overrides(self):\n        self.assert_provides_value(\n            config_file_values={},\n            section_name='mysection',\n            override_providers={'override_var': ConstantProvider('override')},\n            expected_value={\n                'override_var': 'override',\n            },\n        )\n\n\nclass TestSmartDefaults:\n    def _template(self):\n        return {\n            \"base\": {\n                \"retryMode\": \"standard\",\n                \"stsRegionalEndpoints\": \"regional\",\n                \"s3UsEast1RegionalEndpoints\": \"regional\",\n                \"connectTimeoutInMillis\": 1000,\n                \"tlsNegotiationTimeoutInMillis\": 1000,\n            },\n            \"modes\": {\n                \"standard\": {\n                    \"connectTimeoutInMillis\": {\"multiply\": 2},\n                    \"tlsNegotiationTimeoutInMillis\": {\"multiply\": 2},\n                },\n                \"in-region\": {\n                    \"connectTimeoutInMillis\": {\"multiply\": 1},\n                    \"tlsNegotiationTimeoutInMillis\": {\"multiply\": 1},\n                },\n                \"cross-region\": {\n                    \"connectTimeoutInMillis\": {\"multiply\": 2.8},\n                    \"tlsNegotiationTimeoutInMillis\": {\"multiply\": 2.8},\n                },\n                \"mobile\": {\n                    \"connectTimeoutInMillis\": {\"override\": 10000},\n                    \"tlsNegotiationTimeoutInMillis\": {\"add\": 10000},\n                    \"retryMode\": {\"override\": \"adaptive\"},\n                },\n            },\n        }\n\n    def _create_default_config_resolver(self):\n        return DefaultConfigResolver(self._template())\n\n    @pytest.fixture\n    def smart_defaults_factory(self):\n        fake_session = mock.Mock(spec=session.Session)\n        fake_session.get_scoped_config.return_value = {}\n        default_config_resolver = self._create_default_config_resolver()\n        return SmartDefaultsConfigStoreFactory(\n            default_config_resolver, imds_region_provider=mock.Mock()\n        )\n\n    @pytest.fixture\n    def fake_session(self):\n        fake_session = mock.Mock(spec=session.Session)\n        fake_session.get_scoped_config.return_value = {}\n        return fake_session\n\n    def _create_config_value_store(self, s3_mapping={}, **override_kwargs):\n        constant_provider = ConstantProvider(value='my_sts_regional_endpoint')\n        environment_provider = EnvironmentProvider(\n            name='AWS_RETRY_MODE', env={'AWS_RETRY_MODE': None}\n        )\n        fake_session = mock.Mock(spec=session.Session)\n        fake_session.get_scoped_config.return_value = {}\n        # Testing with three different providers to validate\n        # SmartDefaultsConfigStoreFactory._get_new_chain_provider\n        mapping = {\n            'sts_regional_endpoints': ChainProvider(\n                providers=[constant_provider]\n            ),\n            'retry_mode': ChainProvider(providers=[environment_provider]),\n            's3': SectionConfigProvider('s3', fake_session, s3_mapping),\n        }\n        mapping.update(**override_kwargs)\n        config_store = ConfigValueStore(mapping=mapping)\n        return config_store\n\n    def _create_os_environ_patcher(self):\n        return mock.patch.object(\n            botocore.configprovider.os, 'environ', mock.Mock(wraps=os.environ)\n        )\n\n    def test_config_store_deepcopy(self):\n        config_store = ConfigValueStore()\n        config_store.set_config_provider(\n            'constant_value', ConstantProvider('ABC')\n        )\n        config_store_copy = copy.deepcopy(config_store)\n        config_store_copy.set_config_provider(\n            'constant_value_copy', ConstantProvider('123')\n        )\n        assert config_store.get_config_variable('constant_value_copy') is None\n        assert config_store_copy.get_config_variable('constant_value') == 'ABC'\n\n    def _create_config_value_store_to_test_merge(self):\n        environment_provider = EnvironmentProvider(\n            name='AWS_S3_US_EAST_1_REGIONAL_ENDPOINT',\n            env={},\n        )\n\n        s3_mapping = {\n            'us_east_1_regional_endpoint': ChainProvider(\n                providers=[environment_provider]\n            )\n        }\n\n        override_kwargs = {'connect_timeout': ConstantProvider(value=None)}\n\n        config_value_store = self._create_config_value_store(\n            s3_mapping=s3_mapping, **override_kwargs\n        )\n\n        return config_value_store\n\n    @pytest.mark.parametrize(\n        'config_variable,expected_value_before,expected_value_after',\n        [\n            ['retry_mode', None, 'standard'],\n            ['sts_regional_endpoints', 'my_sts_regional_endpoint', 'regional'],\n            ['connect_timeout', None, 2],\n            ['s3', None, {'us_east_1_regional_endpoint': 'regional'}],\n        ],\n    )\n    def test_config_store_providers_not_mutated_after_merge(\n        self,\n        config_variable,\n        expected_value_before,\n        expected_value_after,\n        smart_defaults_factory,\n    ):\n        \"\"\"Test uses the standard default mode from the template\"\"\"\n\n        config_value_store = self._create_config_value_store_to_test_merge()\n\n        provider = config_value_store.get_config_provider(config_variable)\n\n        smart_defaults_factory.merge_smart_defaults(\n            config_value_store, 'standard', 'some-region'\n        )\n\n        assert provider.provide() == expected_value_before\n\n        assert (\n            config_value_store.get_config_variable(config_variable)\n            == expected_value_after\n        )\n\n    @pytest.mark.parametrize(\n        'defaults_mode, retry_mode, sts_regional_endpoints,'\n        ' us_east_1_regional_endpoint, connect_timeout',\n        [\n            ('standard', 'standard', 'regional', 'regional', 2000),\n            ('in-region', 'standard', 'regional', 'regional', 1000),\n            ('cross-region', 'standard', 'regional', 'regional', 2800),\n            ('mobile', 'adaptive', 'regional', 'regional', 10000),\n        ],\n    )\n    def test_get_defualt_config_values(\n        self,\n        defaults_mode,\n        retry_mode,\n        sts_regional_endpoints,\n        us_east_1_regional_endpoint,\n        connect_timeout,\n    ):\n        default_config_resolver = self._create_default_config_resolver()\n        default_values = default_config_resolver.get_default_config_values(\n            defaults_mode\n        )\n        assert default_values['retryMode'] == retry_mode\n        assert default_values['stsRegionalEndpoints'] == sts_regional_endpoints\n        assert (\n            default_values['s3UsEast1RegionalEndpoints']\n            == us_east_1_regional_endpoint\n        )\n        assert default_values['connectTimeoutInMillis'] == connect_timeout\n\n    def test_resolve_default_values_on_config(\n        self, smart_defaults_factory, fake_session\n    ):\n        config_store = self._create_config_value_store()\n        smart_defaults_factory.merge_smart_defaults(\n            config_store, 'standard', 'foo'\n        )\n        s3_config = config_store.get_config_variable('s3')\n        assert s3_config['us_east_1_regional_endpoint'] == 'regional'\n        assert config_store.get_config_variable('retry_mode') == 'standard'\n        assert (\n            config_store.get_config_variable('sts_regional_endpoints')\n            == 'regional'\n        )\n        assert config_store.get_config_variable('connect_timeout') == 2\n\n    def test_no_resolve_default_s3_values_on_config(\n        self, smart_defaults_factory\n    ):\n        environment_provider = EnvironmentProvider(\n            name='AWS_S3_US_EAST_1_REGIONAL_ENDPOINT',\n            env={'AWS_S3_US_EAST_1_REGIONAL_ENDPOINT': 'legacy'},\n        )\n        s3_mapping = {\n            'us_east_1_regional_endpoint': ChainProvider(\n                providers=[environment_provider]\n            )\n        }\n        config_store = self._create_config_value_store(s3_mapping=s3_mapping)\n        smart_defaults_factory.merge_smart_defaults(\n            config_store, 'standard', 'foo'\n        )\n        s3_config = config_store.get_config_variable('s3')\n        assert s3_config['us_east_1_regional_endpoint'] == 'legacy'\n        assert config_store.get_config_variable('retry_mode') == 'standard'\n        assert (\n            config_store.get_config_variable('sts_regional_endpoints')\n            == 'regional'\n        )\n        assert config_store.get_config_variable('connect_timeout') == 2\n\n    def test_resolve_default_s3_values_on_config(\n        self, smart_defaults_factory, fake_session\n    ):\n        s3_mapping = {\n            'use_arn_region': ChainProvider(\n                providers=[ConstantProvider(value=False)]\n            )\n        }\n        config_store = self._create_config_value_store(s3_mapping=s3_mapping)\n        smart_defaults_factory.merge_smart_defaults(\n            config_store, 'standard', 'foo'\n        )\n        s3_config = config_store.get_config_variable('s3')\n        assert s3_config['us_east_1_regional_endpoint'] == 'regional'\n        assert config_store.get_config_variable('retry_mode') == 'standard'\n        assert (\n            config_store.get_config_variable('sts_regional_endpoints')\n            == 'regional'\n        )\n        assert config_store.get_config_variable('connect_timeout') == 2\n\n    @pytest.mark.parametrize(\n        'execution_env_var, region_env_var, default_region_env_var, '\n        'imds_region, client_region, resolved_mode',\n        [\n            (\n                'AWS_Lambda_python3.6',\n                'us-east-1',\n                None,\n                None,\n                'us-east-1',\n                'in-region',\n            ),\n            (\n                'AWS_Lambda_python3.6',\n                'us-west-2',\n                'us-west-2',\n                None,\n                'us-east-1',\n                'cross-region',\n            ),\n            (\n                'AWS_Lambda_python3.6',\n                None,\n                None,\n                'us-west-2',\n                'us-east-1',\n                'cross-region',\n            ),\n            (None, None, 'us-east-1', 'us-east-1', 'us-east-1', 'in-region'),\n            (None, None, None, 'us-west-2', 'us-east-1', 'cross-region'),\n            (None, None, None, None, 'us-west-2', 'standard'),\n        ],\n    )\n    def test_resolve_auto_mode(\n        self,\n        execution_env_var,\n        region_env_var,\n        default_region_env_var,\n        imds_region,\n        client_region,\n        resolved_mode,\n    ):\n        imds_region_provider = mock.Mock(spec=IMDSRegionProvider)\n        imds_region_provider.provide.return_value = imds_region\n        default_config_resolver = mock.Mock()\n        with mock.patch.object(\n            botocore.configprovider.os, 'environ', mock.Mock(wraps=os.environ)\n        ) as os_environ_patcher:\n            os_environ_patcher.get.side_effect = [\n                execution_env_var,\n                default_region_env_var,\n                region_env_var,\n            ]\n            smart_defaults_factory = SmartDefaultsConfigStoreFactory(\n                default_config_resolver, imds_region_provider\n            )\n            mode = smart_defaults_factory.resolve_auto_mode(client_region)\n            assert mode == resolved_mode\n\n    def test_resolve_auto_mode_imds_region_provider_connect_timeout(self):\n        imds_region_provider = mock.Mock(spec=IMDSRegionProvider)\n        imds_region_provider.provide.side_effect = ConnectTimeoutError(\n            endpoint_url='foo'\n        )\n        default_config_resolver = mock.Mock()\n        with mock.patch.object(\n            botocore.configprovider.os, 'environ', mock.Mock(wraps=os.environ)\n        ) as os_environ_patcher:\n            os_environ_patcher.get.side_effect = [None] * 3\n            smart_defaults_factory = SmartDefaultsConfigStoreFactory(\n                default_config_resolver, imds_region_provider\n            )\n            mode = smart_defaults_factory.resolve_auto_mode('us-west-2')\n            assert mode == 'standard'\n\n\ndef create_cases():\n    service = 'batch'\n\n    return [\n        dict(\n            service=service,\n            environ_map={},\n            full_config_map={},\n            expected_value=None,\n        ),\n        dict(\n            service=service,\n            environ_map={'AWS_ENDPOINT_URL': 'global-from-env'},\n            full_config_map={},\n            expected_value='global-from-env',\n        ),\n        dict(\n            service=service,\n            environ_map={\n                f'AWS_ENDPOINT_URL_{service.upper()}': 'service-from-env',\n                'AWS_ENDPOINT_URL': 'global-from-env',\n            },\n            full_config_map={},\n            expected_value='service-from-env',\n        ),\n        dict(\n            service=service,\n            environ_map={\n                'AWS_ENDPOINT_URL': 'global-from-env',\n                'AWS_ENDPOINT_URL_S3': 's3-endpoint-url',\n            },\n            full_config_map={},\n            expected_value='global-from-env',\n        ),\n        dict(\n            service=service,\n            environ_map={},\n            full_config_map={\n                'profiles': {'default': {'endpoint_url': 'global-from-config'}}\n            },\n            expected_value='global-from-config',\n        ),\n        dict(\n            service=service,\n            environ_map={},\n            full_config_map={\n                'profiles': {\n                    'default': {\n                        'services': 'my-services',\n                    }\n                },\n                'services': {\n                    'my-services': {\n                        service: {'endpoint_url': \"service-from-config\"}\n                    }\n                },\n            },\n            expected_value='service-from-config',\n        ),\n        dict(\n            service=service,\n            environ_map={},\n            full_config_map={\n                'profiles': {\n                    'default': {\n                        'services': 'my-services',\n                        'endpoint_url': 'global-from-config',\n                    }\n                },\n                'services': {\n                    'my-services': {\n                        service: {'endpoint_url': \"service-from-config\"}\n                    }\n                },\n            },\n            expected_value='service-from-config',\n        ),\n        dict(\n            service=service,\n            environ_map={\n                'AWS_ENDPOINT_URL': 'global-from-env',\n            },\n            full_config_map={\n                'profiles': {\n                    'default': {\n                        'endpoint_url': 'global-from-config',\n                    }\n                },\n            },\n            expected_value='global-from-env',\n        ),\n        dict(\n            service=service,\n            environ_map={\n                f'AWS_ENDPOINT_URL_{service.upper()}': 'service-from-env',\n            },\n            full_config_map={\n                'profiles': {\n                    'default': {\n                        'endpoint_url': 'global-from-config',\n                    }\n                },\n            },\n            expected_value='service-from-env',\n        ),\n        dict(\n            service='s3',\n            environ_map={},\n            full_config_map={\n                'profiles': {\n                    'default': {\n                        'services': 'my-services',\n                        'endpoint_url': 'global-from-config',\n                    }\n                },\n                'services': {\n                    'my-services': {\n                        service: {'endpoint_url': \"service-from-config\"}\n                    }\n                },\n            },\n            expected_value='global-from-config',\n        ),\n        dict(\n            service='runtime.sagemaker',\n            environ_map={},\n            full_config_map={\n                'profiles': {\n                    'default': {\n                        'services': 'my-services',\n                    }\n                },\n                'services': {\n                    'my-services': {\n                        'sagemaker_runtime': {\n                            'endpoint_url': \"service-from-config\"\n                        }\n                    }\n                },\n            },\n            expected_value='service-from-config',\n        ),\n        dict(\n            service='apigateway',\n            environ_map={},\n            full_config_map={\n                'profiles': {\n                    'default': {\n                        'services': 'my-services',\n                    }\n                },\n                'services': {\n                    'my-services': {\n                        'api_gateway': {'endpoint_url': \"service-from-config\"}\n                    }\n                },\n            },\n            expected_value='service-from-config',\n        ),\n    ]\n\n\nclass TestConfiguredEndpointProvider:\n    def assert_does_provide(\n        self,\n        service,\n        environ_map,\n        full_config_map,\n        expected_value,\n    ):\n        scoped_config_map = full_config_map.get('profiles', {}).get(\n            'default', {}\n        )\n\n        chain = ConfiguredEndpointProvider(\n            scoped_config=scoped_config_map,\n            full_config=full_config_map,\n            client_name=service,\n            environ=environ_map,\n        )\n        value = chain.provide()\n        assert value == expected_value\n\n    @pytest.mark.parametrize('test_case', create_cases())\n    def test_does_provide(self, test_case):\n        self.assert_does_provide(**test_case)\n\n    def test_is_deepcopyable(self):\n        env = {'AWS_ENDPOINT_URL_BATCH': 'https://endpoint-override'}\n        provider = ConfiguredEndpointProvider(\n            full_config={}, scoped_config={}, client_name='batch', environ=env\n        )\n\n        provider_deepcopy = copy.deepcopy(provider)\n        assert provider is not provider_deepcopy\n        assert provider.provide() == 'https://endpoint-override'\n        assert provider_deepcopy.provide() == 'https://endpoint-override'\n\n        env['AWS_ENDPOINT_URL_BATCH'] = 'https://another-new-endpoint-override'\n        assert provider.provide() == 'https://another-new-endpoint-override'\n        assert provider_deepcopy.provide() == 'https://endpoint-override'\n", "tests/unit/test_args.py": "#!/usr/bin/env\n# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport socket\n\nfrom botocore import args, exceptions\nfrom botocore.client import ClientEndpointBridge\nfrom botocore.config import Config\nfrom botocore.configprovider import ConfigValueStore\nfrom botocore.hooks import HierarchicalEmitter\nfrom botocore.model import ServiceModel\nfrom botocore.useragent import UserAgentString\nfrom tests import mock, unittest\n\n\nclass TestCreateClientArgs(unittest.TestCase):\n    def setUp(self):\n        self.event_emitter = mock.Mock(HierarchicalEmitter)\n        self.config_store = ConfigValueStore()\n        user_agent_creator = UserAgentString(\n            platform_name=None,\n            platform_version=None,\n            platform_machine=None,\n            python_version=None,\n            python_implementation=None,\n            execution_env=None,\n            crt_version=None,\n        )\n        self.args_create = args.ClientArgsCreator(\n            event_emitter=self.event_emitter,\n            user_agent=None,\n            response_parser_factory=None,\n            loader=None,\n            exceptions_factory=None,\n            config_store=self.config_store,\n            user_agent_creator=user_agent_creator,\n        )\n        self.service_name = 'ec2'\n        self.region = 'us-west-2'\n        self.endpoint_url = 'https://ec2/'\n        self.service_model = self._get_service_model()\n        self.bridge = mock.Mock(ClientEndpointBridge)\n        self._set_endpoint_bridge_resolve()\n        self._set_resolver_uses_builtin()\n        self.default_socket_options = [\n            (socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        ]\n\n    def _get_service_model(self, service_name=None):\n        if service_name is None:\n            service_name = self.service_name\n        service_model = mock.Mock(ServiceModel)\n        service_model.service_name = service_name\n        service_model.endpoint_prefix = service_name\n        service_model.metadata = {\n            'serviceFullName': 'MyService',\n            'protocol': 'query',\n        }\n        service_model.operation_names = []\n        return service_model\n\n    def _set_endpoint_bridge_resolve(self, **override_kwargs):\n        ret_val = {\n            'region_name': self.region,\n            'signature_version': 'v4',\n            'endpoint_url': self.endpoint_url,\n            'signing_name': self.service_name,\n            'signing_region': self.region,\n            'metadata': {},\n        }\n        ret_val.update(**override_kwargs)\n        self.bridge.resolve.return_value = ret_val\n\n    def _set_resolver_uses_builtin(self, uses_builtin=True):\n        self.bridge.resolver_uses_builtin_data.return_value = uses_builtin\n\n    def call_get_client_args(self, **override_kwargs):\n        call_kwargs = {\n            'service_model': self.service_model,\n            'region_name': self.region,\n            'is_secure': True,\n            'endpoint_url': self.endpoint_url,\n            'verify': True,\n            'credentials': None,\n            'scoped_config': {},\n            'client_config': None,\n            'endpoint_bridge': self.bridge,\n            'endpoints_ruleset_data': {\n                'version': '1.0',\n                'parameters': {},\n                'rules': [],\n            },\n            'partition_data': {},\n        }\n        call_kwargs.update(**override_kwargs)\n        return self.args_create.get_client_args(**call_kwargs)\n\n    def assert_create_endpoint_call(self, mock_endpoint, **override_kwargs):\n        call_kwargs = {\n            'endpoint_url': self.endpoint_url,\n            'region_name': self.region,\n            'response_parser_factory': None,\n            'timeout': (60, 60),\n            'verify': True,\n            'max_pool_connections': 10,\n            'proxies': None,\n            'proxies_config': None,\n            'socket_options': self.default_socket_options,\n            'client_cert': None,\n        }\n        call_kwargs.update(**override_kwargs)\n        mock_endpoint.return_value.create_endpoint.assert_called_with(\n            self.service_model, **call_kwargs\n        )\n\n    def test_compute_s3_configuration(self):\n        self.assertIsNone(self.args_create.compute_s3_config(None))\n\n    def test_compute_s3_config_only_config_store(self):\n        self.config_store.set_config_variable(\n            's3', {'use_accelerate_endpoint': True}\n        )\n        self.assertEqual(\n            self.args_create.compute_s3_config(None),\n            {'use_accelerate_endpoint': True},\n        )\n\n    def test_client_s3_accelerate_from_client_config(self):\n        self.assertEqual(\n            self.args_create.compute_s3_config(\n                client_config=Config(s3={'use_accelerate_endpoint': True})\n            ),\n            {'use_accelerate_endpoint': True},\n        )\n\n    def test_client_s3_accelerate_client_config_overrides_config_store(self):\n        self.config_store.set_config_variable(\n            's3', {'use_accelerate_endpoint': False}\n        )\n        self.assertEqual(\n            self.args_create.compute_s3_config(\n                client_config=Config(s3={'use_accelerate_endpoint': True})\n            ),\n            # client_config beats scoped_config\n            {'use_accelerate_endpoint': True},\n        )\n\n    def test_max_pool_from_client_config_forwarded_to_endpoint_creator(self):\n        config = Config(max_pool_connections=20)\n        with mock.patch('botocore.args.EndpointCreator') as m:\n            self.call_get_client_args(client_config=config)\n            self.assert_create_endpoint_call(m, max_pool_connections=20)\n\n    def test_proxies_from_client_config_forwarded_to_endpoint_creator(self):\n        proxies = {\n            'http': 'http://foo.bar:1234',\n            'https': 'https://foo.bar:4321',\n        }\n        config = Config(proxies=proxies)\n        with mock.patch('botocore.args.EndpointCreator') as m:\n            self.call_get_client_args(client_config=config)\n            self.assert_create_endpoint_call(m, proxies=proxies)\n\n    def test_s3_with_endpoint_url_still_resolves_region(self):\n        self.service_model.endpoint_prefix = 's3'\n        self.service_model.metadata = {'protocol': 'rest-xml'}\n        self.bridge.resolve.side_effect = [\n            {\n                'region_name': None,\n                'signature_version': 's3v4',\n                'endpoint_url': 'http://other.com/',\n                'signing_name': 's3',\n                'signing_region': None,\n                'metadata': {},\n            },\n            {\n                'region_name': 'us-west-2',\n                'signature_version': 's3v4',\n                'endpoint_url': 'https://s3-us-west-2.amazonaws.com',\n                'signing_name': 's3',\n                'signing_region': 'us-west-2',\n                'metadata': {},\n            },\n        ]\n        client_args = self.call_get_client_args(\n            endpoint_url='http://other.com/'\n        )\n        self.assertEqual(client_args['client_config'].region_name, 'us-west-2')\n\n    def test_region_does_not_resolve_if_not_s3_and_endpoint_url_provided(self):\n        self.service_model.endpoint_prefix = 'ec2'\n        self.service_model.metadata = {'protocol': 'query'}\n        self.bridge.resolve.side_effect = [\n            {\n                'region_name': None,\n                'signature_version': 'v4',\n                'endpoint_url': 'http://other.com/',\n                'signing_name': 'ec2',\n                'signing_region': None,\n                'metadata': {},\n            }\n        ]\n        client_args = self.call_get_client_args(\n            endpoint_url='http://other.com/'\n        )\n        self.assertEqual(client_args['client_config'].region_name, None)\n\n    def test_tcp_keepalive_enabled_scoped_config(self):\n        scoped_config = {'tcp_keepalive': 'true'}\n        with mock.patch('botocore.args.EndpointCreator') as m:\n            self.call_get_client_args(scoped_config=scoped_config)\n            self.assert_create_endpoint_call(\n                m,\n                socket_options=self.default_socket_options\n                + [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)],\n            )\n\n    def test_tcp_keepalive_not_specified(self):\n        with mock.patch('botocore.args.EndpointCreator') as m:\n            self.call_get_client_args(scoped_config={}, client_config=None)\n            self.assert_create_endpoint_call(\n                m, socket_options=self.default_socket_options\n            )\n            self.call_get_client_args(\n                scoped_config=None, client_config=Config()\n            )\n            self.assert_create_endpoint_call(\n                m, socket_options=self.default_socket_options\n            )\n\n    def test_tcp_keepalive_enabled_if_set_anywhere(self):\n        with mock.patch('botocore.args.EndpointCreator') as m:\n            self.call_get_client_args(\n                scoped_config={'tcp_keepalive': 'true'},\n                client_config=Config(tcp_keepalive=False),\n            )\n            self.assert_create_endpoint_call(\n                m,\n                socket_options=self.default_socket_options\n                + [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)],\n            )\n            self.call_get_client_args(\n                scoped_config={'tcp_keepalive': 'false'},\n                client_config=Config(tcp_keepalive=True),\n            )\n            self.assert_create_endpoint_call(\n                m,\n                socket_options=self.default_socket_options\n                + [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)],\n            )\n\n    def test_tcp_keepalive_explicitly_disabled(self):\n        scoped_config = {'tcp_keepalive': 'false'}\n        with mock.patch('botocore.args.EndpointCreator') as m:\n            self.call_get_client_args(scoped_config=scoped_config)\n            self.assert_create_endpoint_call(\n                m, socket_options=self.default_socket_options\n            )\n\n    def test_tcp_keepalive_enabled_case_insensitive(self):\n        scoped_config = {'tcp_keepalive': 'True'}\n        with mock.patch('botocore.args.EndpointCreator') as m:\n            self.call_get_client_args(scoped_config=scoped_config)\n            self.assert_create_endpoint_call(\n                m,\n                socket_options=self.default_socket_options\n                + [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)],\n            )\n\n    def test_client_config_has_use_dualstack_endpoint_flag(self):\n        self._set_endpoint_bridge_resolve(metadata={'tags': ['dualstack']})\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('ec2'),\n        )\n        self.assertTrue(client_args['client_config'].use_dualstack_endpoint)\n\n    def test_client_config_has_use_fips_endpoint_flag(self):\n        self._set_endpoint_bridge_resolve(metadata={'tags': ['fips']})\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('ec2'),\n        )\n        self.assertTrue(client_args['client_config'].use_fips_endpoint)\n\n    def test_client_config_has_both_use_fips_and_use_dualstack__endpoint_flags(\n        self,\n    ):\n        self._set_endpoint_bridge_resolve(\n            metadata={'tags': ['fips', 'dualstack']}\n        )\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('ec2'),\n        )\n        self.assertTrue(client_args['client_config'].use_fips_endpoint)\n        self.assertTrue(client_args['client_config'].use_dualstack_endpoint)\n\n    def test_s3_override_use_dualstack_endpoint_flag(self):\n        self._set_endpoint_bridge_resolve(metadata={'tags': ['dualstack']})\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('s3'),\n        )\n        self.assertTrue(\n            client_args['client_config'].s3['use_dualstack_endpoint']\n        )\n\n    def test_sts_override_resolved_endpoint_for_legacy_region(self):\n        self.config_store.set_config_variable(\n            'sts_regional_endpoints', 'legacy'\n        )\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('sts'),\n            region_name='us-west-2',\n            endpoint_url=None,\n        )\n        self.assertEqual(\n            client_args['endpoint'].host, 'https://sts.amazonaws.com'\n        )\n        self.assertEqual(\n            client_args['request_signer'].region_name, 'us-east-1'\n        )\n\n    def test_sts_use_resolved_endpoint_for_nonlegacy_region(self):\n        resolved_endpoint = 'https://resolved-endpoint'\n        resolved_region = 'resolved-region'\n        self._set_endpoint_bridge_resolve(\n            endpoint_url=resolved_endpoint, signing_region=resolved_region\n        )\n        self.config_store.set_config_variable(\n            'sts_regional_endpoints', 'legacy'\n        )\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('sts'),\n            region_name='ap-east-1',\n            endpoint_url=None,\n        )\n        self.assertEqual(client_args['endpoint'].host, resolved_endpoint)\n        self.assertEqual(\n            client_args['request_signer'].region_name, resolved_region\n        )\n\n    def test_sts_use_resolved_endpoint_for_regional_configuration(self):\n        resolved_endpoint = 'https://resolved-endpoint'\n        resolved_region = 'resolved-region'\n        self._set_endpoint_bridge_resolve(\n            endpoint_url=resolved_endpoint, signing_region=resolved_region\n        )\n        self.config_store.set_config_variable(\n            'sts_regional_endpoints', 'regional'\n        )\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('sts'),\n            region_name='us-west-2',\n            endpoint_url=None,\n        )\n        self.assertEqual(client_args['endpoint'].host, resolved_endpoint)\n        self.assertEqual(\n            client_args['request_signer'].region_name, resolved_region\n        )\n\n    def test_sts_with_endpoint_override_and_legacy_configured(self):\n        override_endpoint = 'https://override-endpoint'\n        self._set_endpoint_bridge_resolve(endpoint_url=override_endpoint)\n        self.config_store.set_config_variable(\n            'sts_regional_endpoints', 'legacy'\n        )\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('sts'),\n            region_name='us-west-2',\n            endpoint_url=override_endpoint,\n        )\n        self.assertEqual(client_args['endpoint'].host, override_endpoint)\n\n    def test_sts_http_scheme_for_override_endpoint(self):\n        self.config_store.set_config_variable(\n            'sts_regional_endpoints', 'legacy'\n        )\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('sts'),\n            region_name='us-west-2',\n            endpoint_url=None,\n            is_secure=False,\n        )\n        self.assertEqual(\n            client_args['endpoint'].host, 'http://sts.amazonaws.com'\n        )\n\n    def test_sts_regional_endpoints_defaults_to_legacy_if_not_set(self):\n        self.config_store.set_config_variable('sts_regional_endpoints', None)\n        client_args = self.call_get_client_args(\n            service_model=self._get_service_model('sts'),\n            region_name='us-west-2',\n            endpoint_url=None,\n        )\n        self.assertEqual(\n            client_args['endpoint'].host, 'https://sts.amazonaws.com'\n        )\n        self.assertEqual(\n            client_args['request_signer'].region_name, 'us-east-1'\n        )\n\n    def test_invalid_sts_regional_endpoints(self):\n        self.config_store.set_config_variable(\n            'sts_regional_endpoints', 'invalid'\n        )\n        with self.assertRaises(\n            exceptions.InvalidSTSRegionalEndpointsConfigError\n        ):\n            self.call_get_client_args(\n                service_model=self._get_service_model('sts'),\n                region_name='us-west-2',\n                endpoint_url=None,\n            )\n\n    def test_provides_total_max_attempts(self):\n        config = Config(retries={'total_max_attempts': 10})\n        client_args = self.call_get_client_args(client_config=config)\n        self.assertEqual(\n            client_args['client_config'].retries['total_max_attempts'], 10\n        )\n\n    def test_provides_total_max_attempts_has_precedence(self):\n        config = Config(retries={'total_max_attempts': 10, 'max_attempts': 5})\n        client_args = self.call_get_client_args(client_config=config)\n        self.assertEqual(\n            client_args['client_config'].retries['total_max_attempts'], 10\n        )\n        self.assertNotIn('max_attempts', client_args['client_config'].retries)\n\n    def test_provide_retry_config_maps_total_max_attempts(self):\n        config = Config(retries={'max_attempts': 10})\n        client_args = self.call_get_client_args(client_config=config)\n        self.assertEqual(\n            client_args['client_config'].retries['total_max_attempts'], 11\n        )\n        self.assertNotIn('max_attempts', client_args['client_config'].retries)\n\n    def test_can_merge_max_attempts(self):\n        self.config_store.set_config_variable('max_attempts', 4)\n        config = self.call_get_client_args()['client_config']\n        self.assertEqual(config.retries['total_max_attempts'], 4)\n\n    def test_uses_config_value_if_present_for_max_attempts(self):\n        config = self.call_get_client_args(\n            client_config=Config(retries={'max_attempts': 2})\n        )['client_config']\n        self.assertEqual(config.retries['total_max_attempts'], 3)\n\n    def test_uses_client_config_over_config_store_max_attempts(self):\n        self.config_store.set_config_variable('max_attempts', 4)\n        config = self.call_get_client_args(\n            client_config=Config(retries={'max_attempts': 2})\n        )['client_config']\n        self.assertEqual(config.retries['total_max_attempts'], 3)\n\n    def test_uses_client_config_total_over_config_store_max_attempts(self):\n        self.config_store.set_config_variable('max_attempts', 4)\n        config = self.call_get_client_args(\n            client_config=Config(retries={'total_max_attempts': 2})\n        )['client_config']\n        self.assertEqual(config.retries['total_max_attempts'], 2)\n\n    def test_max_attempts_unset_if_retries_is_none(self):\n        config = self.call_get_client_args(client_config=Config(retries=None))[\n            'client_config'\n        ]\n        self.assertEqual(config.retries, {'mode': 'legacy'})\n\n    def test_retry_mode_set_on_config_store(self):\n        self.config_store.set_config_variable('retry_mode', 'standard')\n        config = self.call_get_client_args()['client_config']\n        self.assertEqual(config.retries['mode'], 'standard')\n\n    def test_retry_mode_set_on_client_config(self):\n        config = self.call_get_client_args(\n            client_config=Config(retries={'mode': 'standard'})\n        )['client_config']\n        self.assertEqual(config.retries['mode'], 'standard')\n\n    def test_connect_timeout_set_on_config_store(self):\n        self.config_store.set_config_variable('connect_timeout', 10)\n        config = self.call_get_client_args(\n            client_config=Config(defaults_mode='standard')\n        )['client_config']\n        self.assertEqual(config.connect_timeout, 10)\n\n    def test_connnect_timeout_set_on_client_config(self):\n        config = self.call_get_client_args(\n            client_config=Config(connect_timeout=10)\n        )['client_config']\n        self.assertEqual(config.connect_timeout, 10)\n\n    def test_connnect_timeout_set_to_client_config_default(self):\n        config = self.call_get_client_args()['client_config']\n        self.assertEqual(config.connect_timeout, 60)\n\n    def test_client_config_beats_config_store(self):\n        self.config_store.set_config_variable('retry_mode', 'adaptive')\n        config = self.call_get_client_args(\n            client_config=Config(retries={'mode': 'standard'})\n        )['client_config']\n        self.assertEqual(config.retries['mode'], 'standard')\n\n    def test_creates_ruleset_resolver_if_given_data(self):\n        with mock.patch('botocore.args.EndpointRulesetResolver') as m:\n            self.call_get_client_args(\n                service_model=self._get_service_model('s3'),\n                endpoints_ruleset_data={\n                    'version': '1.0',\n                    'parameters': {},\n                    'rules': [],\n                },\n            )\n            m.assert_called_once()\n\n    def test_doesnt_create_ruleset_resolver_if_not_given_data(self):\n        with mock.patch('botocore.args.EndpointRulesetResolver') as m:\n            self.call_get_client_args(\n                service_model=self._get_service_model('s3'),\n                endpoints_ruleset_data=None,\n            )\n            m.assert_not_called()\n\n    def test_request_compression_client_config(self):\n        input_config = Config(\n            disable_request_compression=True,\n            request_min_compression_size_bytes=100,\n        )\n        client_args = self.call_get_client_args(client_config=input_config)\n        config = client_args['client_config']\n        self.assertEqual(config.request_min_compression_size_bytes, 100)\n        self.assertTrue(config.disable_request_compression)\n\n    def test_request_compression_config_store(self):\n        self.config_store.set_config_variable(\n            'request_min_compression_size_bytes', 100\n        )\n        self.config_store.set_config_variable(\n            'disable_request_compression', True\n        )\n        config = self.call_get_client_args()['client_config']\n        self.assertEqual(config.request_min_compression_size_bytes, 100)\n        self.assertTrue(config.disable_request_compression)\n\n    def test_request_compression_client_config_overrides_config_store(self):\n        self.config_store.set_config_variable(\n            'request_min_compression_size_bytes', 100\n        )\n        self.config_store.set_config_variable(\n            'disable_request_compression', True\n        )\n        input_config = Config(\n            disable_request_compression=False,\n            request_min_compression_size_bytes=1,\n        )\n        client_args = self.call_get_client_args(client_config=input_config)\n        config = client_args['client_config']\n        self.assertEqual(config.request_min_compression_size_bytes, 1)\n        self.assertFalse(config.disable_request_compression)\n\n    def test_coercible_value_request_min_compression_size_bytes(self):\n        config = Config(request_min_compression_size_bytes='100')\n        client_args = self.call_get_client_args(client_config=config)\n        config = client_args['client_config']\n        self.assertEqual(config.request_min_compression_size_bytes, 100)\n\n    def test_coercible_value_disable_request_compression(self):\n        config = Config(disable_request_compression='true')\n        client_args = self.call_get_client_args(client_config=config)\n        config = client_args['client_config']\n        self.assertTrue(config.disable_request_compression)\n\n    def test_bad_type_request_min_compression_size_bytes(self):\n        with self.assertRaises(exceptions.InvalidConfigError):\n            config = Config(request_min_compression_size_bytes='foo')\n            self.call_get_client_args(client_config=config)\n        self.config_store.set_config_variable(\n            'request_min_compression_size_bytes', 'foo'\n        )\n        with self.assertRaises(exceptions.InvalidConfigError):\n            self.call_get_client_args()\n\n    def test_low_min_request_min_compression_size_bytes(self):\n        with self.assertRaises(exceptions.InvalidConfigError):\n            config = Config(request_min_compression_size_bytes=0)\n            self.call_get_client_args(client_config=config)\n        self.config_store.set_config_variable(\n            'request_min_compression_size_bytes', 0\n        )\n        with self.assertRaises(exceptions.InvalidConfigError):\n            self.call_get_client_args()\n\n    def test_high_max_request_min_compression_size_bytes(self):\n        with self.assertRaises(exceptions.InvalidConfigError):\n            config = Config(request_min_compression_size_bytes=9999999)\n            self.call_get_client_args(client_config=config)\n        self.config_store.set_config_variable(\n            'request_min_compression_size_bytes', 9999999\n        )\n        with self.assertRaises(exceptions.InvalidConfigError):\n            self.call_get_client_args()\n\n    def test_bad_value_disable_request_compression(self):\n        input_config = Config(disable_request_compression='foo')\n        client_args = self.call_get_client_args(client_config=input_config)\n        config = client_args['client_config']\n        self.assertFalse(config.disable_request_compression)\n\n\nclass TestEndpointResolverBuiltins(unittest.TestCase):\n    def setUp(self):\n        event_emitter = mock.Mock(HierarchicalEmitter)\n        self.config_store = ConfigValueStore()\n        user_agent_creator = UserAgentString(\n            platform_name=None,\n            platform_version=None,\n            platform_machine=None,\n            python_version=None,\n            python_implementation=None,\n            execution_env=None,\n            crt_version=None,\n        )\n        self.args_create = args.ClientArgsCreator(\n            event_emitter=event_emitter,\n            user_agent=None,\n            response_parser_factory=None,\n            loader=None,\n            exceptions_factory=None,\n            config_store=self.config_store,\n            user_agent_creator=user_agent_creator,\n        )\n        self.bridge = ClientEndpointBridge(\n            endpoint_resolver=mock.Mock(),\n            scoped_config=None,\n            client_config=Config(),\n            default_endpoint=None,\n            service_signing_name=None,\n            config_store=self.config_store,\n        )\n        # assume a legacy endpoint resolver that uses the builtin\n        # endpoints.json file\n        self.bridge.endpoint_resolver.uses_builtin_data = True\n\n    def call_compute_endpoint_resolver_builtin_defaults(self, **overrides):\n        defaults = {\n            'region_name': 'ca-central-1',\n            'service_name': 'fooservice',\n            's3_config': {},\n            'endpoint_bridge': self.bridge,\n            'client_endpoint_url': None,\n            'legacy_endpoint_url': 'https://my.legacy.endpoint.com',\n        }\n        kwargs = {**defaults, **overrides}\n        return self.args_create.compute_endpoint_resolver_builtin_defaults(\n            **kwargs\n        )\n\n    def test_builtins_defaults(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults()\n\n        self.assertEqual(bins['AWS::Region'], 'ca-central-1')\n        self.assertEqual(bins['AWS::UseFIPS'], False)\n        self.assertEqual(bins['AWS::UseDualStack'], False)\n        self.assertEqual(bins['AWS::STS::UseGlobalEndpoint'], True)\n        self.assertEqual(bins['AWS::S3::UseGlobalEndpoint'], False)\n        self.assertEqual(bins['AWS::S3::Accelerate'], False)\n        self.assertEqual(bins['AWS::S3::ForcePathStyle'], False)\n        self.assertEqual(bins['AWS::S3::UseArnRegion'], True)\n        self.assertEqual(bins['AWS::S3Control::UseArnRegion'], False)\n        self.assertEqual(\n            bins['AWS::S3::DisableMultiRegionAccessPoints'], False\n        )\n        self.assertEqual(bins['SDK::Endpoint'], None)\n\n    def test_aws_region(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            region_name='my-region-1',\n        )\n        self.assertEqual(bins['AWS::Region'], 'my-region-1')\n\n    def test_aws_use_fips_when_config_is_set_true(self):\n        self.config_store.set_config_variable('use_fips_endpoint', True)\n        bins = self.call_compute_endpoint_resolver_builtin_defaults()\n        self.assertEqual(bins['AWS::UseFIPS'], True)\n\n    def test_aws_use_fips_when_config_is_set_false(self):\n        self.config_store.set_config_variable('use_fips_endpoint', False)\n        bins = self.call_compute_endpoint_resolver_builtin_defaults()\n        self.assertEqual(bins['AWS::UseFIPS'], False)\n\n    def test_aws_use_dualstack_when_config_is_set_true(self):\n        self.bridge.client_config = Config(s3={'use_dualstack_endpoint': True})\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            service_name='s3-control'\n        )\n        self.assertEqual(bins['AWS::UseDualStack'], True)\n\n    def test_aws_use_dualstack_when_config_is_set_false(self):\n        self.bridge.client_config = Config(\n            s3={'use_dualstack_endpoint': False}\n        )\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            service_name='s3-control'\n        )\n        self.assertEqual(bins['AWS::UseDualStack'], False)\n\n    def test_aws_use_dualstack_when_non_dualstack_service(self):\n        self.bridge.client_config = Config(s3={'use_dualstack_endpoint': True})\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            service_name='other-service'\n        )\n        self.assertEqual(bins['AWS::UseDualStack'], False)\n\n    def test_aws_sts_global_endpoint_with_default_and_legacy_region(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            region_name='us-west-2',\n        )\n        self.assertEqual(bins['AWS::STS::UseGlobalEndpoint'], True)\n\n    def test_aws_sts_global_endpoint_with_default_and_nonlegacy_region(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            region_name='eu-south-1',\n        )\n        self.assertEqual(bins['AWS::STS::UseGlobalEndpoint'], False)\n\n    def test_aws_sts_global_endpoint_with_nondefault_config(self):\n        self.config_store.set_config_variable(\n            'sts_regional_endpoints', 'regional'\n        )\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            region_name='us-west-2',\n        )\n        self.assertEqual(bins['AWS::STS::UseGlobalEndpoint'], False)\n\n    def test_s3_global_endpoint(self):\n        # The only reason for this builtin to not have the default value\n        # (False) is that the ``_should_force_s3_global`` method\n        # returns True.\n        self.args_create._should_force_s3_global = mock.Mock(return_value=True)\n        bins = self.call_compute_endpoint_resolver_builtin_defaults()\n        self.assertTrue(bins['AWS::S3::UseGlobalEndpoint'])\n        self.args_create._should_force_s3_global.assert_called_once()\n\n    def test_s3_accelerate_with_config_set_true(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            s3_config={'use_accelerate_endpoint': True},\n        )\n        self.assertEqual(bins['AWS::S3::Accelerate'], True)\n\n    def test_s3_accelerate_with_config_set_false(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            s3_config={'use_accelerate_endpoint': False},\n        )\n        self.assertEqual(bins['AWS::S3::Accelerate'], False)\n\n    def test_force_path_style_with_config_set_to_path(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            s3_config={'addressing_style': 'path'},\n        )\n        self.assertEqual(bins['AWS::S3::ForcePathStyle'], True)\n\n    def test_force_path_style_with_config_set_to_auto(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            s3_config={'addressing_style': 'auto'},\n        )\n        self.assertEqual(bins['AWS::S3::ForcePathStyle'], False)\n\n    def test_force_path_style_with_config_set_to_virtual(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            s3_config={'addressing_style': 'virtual'},\n        )\n        self.assertEqual(bins['AWS::S3::ForcePathStyle'], False)\n\n    def test_use_arn_region_with_config_set_false(self):\n        # These two builtins both take their value from the ``use_arn_region``\n        # in the S3 configuration, but have different default values.\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            s3_config={'use_arn_region': False},\n        )\n        self.assertEqual(bins['AWS::S3::UseArnRegion'], False)\n        self.assertEqual(bins['AWS::S3Control::UseArnRegion'], False)\n\n    def test_use_arn_region_with_config_set_true(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            s3_config={'use_arn_region': True},\n        )\n        self.assertEqual(bins['AWS::S3::UseArnRegion'], True)\n        self.assertEqual(bins['AWS::S3Control::UseArnRegion'], True)\n\n    def test_disable_mrap_with_config_set_true(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            s3_config={'s3_disable_multiregion_access_points': True},\n        )\n        self.assertEqual(bins['AWS::S3::DisableMultiRegionAccessPoints'], True)\n\n    def test_disable_mrap_with_config_set_false(self):\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            s3_config={'s3_disable_multiregion_access_points': False},\n        )\n        self.assertEqual(\n            bins['AWS::S3::DisableMultiRegionAccessPoints'], False\n        )\n\n    def test_sdk_endpoint_both_inputs_set(self):\n        # assume a legacy endpoint resolver that uses a customized\n        # endpoints.json file\n        self.bridge.endpoint_resolver.uses_builtin_data = False\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            client_endpoint_url='https://my.client.endpoint.com',\n            legacy_endpoint_url='https://my.legacy.endpoint.com',\n        )\n        self.assertEqual(\n            bins['SDK::Endpoint'], 'https://my.client.endpoint.com'\n        )\n\n    def test_sdk_endpoint_legacy_set_with_builtin_data(self):\n        # assume a legacy endpoint resolver that uses a customized\n        # endpoints.json file\n        self.bridge.endpoint_resolver.uses_builtin_data = False\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            client_endpoint_url=None,\n            legacy_endpoint_url='https://my.legacy.endpoint.com',\n        )\n        self.assertEqual(\n            bins['SDK::Endpoint'], 'https://my.legacy.endpoint.com'\n        )\n\n    def test_sdk_endpoint_legacy_set_without_builtin_data(self):\n        # assume a legacy endpoint resolver that uses the builtin\n        # endpoints.json file\n        self.bridge.endpoint_resolver.uses_builtin_data = True\n        bins = self.call_compute_endpoint_resolver_builtin_defaults(\n            client_endpoint_url=None,\n            legacy_endpoint_url='https://my.legacy.endpoint.com',\n        )\n        self.assertEqual(bins['SDK::Endpoint'], None)\n", "tests/unit/__init__.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\n\nfrom tests import unittest\n\n\nclass BaseResponseTest(unittest.TestCase):\n    def assert_response_with_subset_metadata(\n        self, actual_response, expected_response\n    ):\n        \"\"\"\n        Compares two parsed service responses. For ResponseMetadata, it will\n        only assert that the expected is a proper subset of the actual. This\n        is useful so that when new keys are added to the metadata, tests don't\n        break.\n        \"\"\"\n        actual = copy.copy(actual_response)\n        expected = copy.copy(expected_response)\n\n        actual_metadata = actual.pop('ResponseMetadata', {})\n        expected_metadata = expected.pop('ResponseMetadata', {})\n\n        self.assertEqual(actual, expected)\n        self.assert_dict_is_proper_subset(actual_metadata, expected_metadata)\n\n    def assert_dict_is_proper_subset(self, superset, subset):\n        \"\"\"\n        Asserts that a dictionary is a proper subset of another.\n        \"\"\"\n        self.assertTrue(\n            all(\n                (k in superset and superset[k] == v) for k, v in subset.items()\n            )\n        )\n", "tests/unit/test_compress.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport gzip\nimport io\nimport sys\n\nimport pytest\n\nimport botocore\nfrom botocore.compress import COMPRESSION_MAPPING, maybe_compress_request\nfrom botocore.config import Config\nfrom tests import mock\n\n\ndef _make_op(\n    request_compression=None,\n    has_streaming_input=False,\n    streaming_metadata=None,\n):\n    op = mock.Mock()\n    op.request_compression = request_compression\n    op.has_streaming_input = has_streaming_input\n    if streaming_metadata is not None:\n        streaming_shape = mock.Mock()\n        streaming_shape.metadata = streaming_metadata\n        op.get_streaming_input.return_value = streaming_shape\n    return op\n\n\nOP_NO_COMPRESSION = _make_op()\nOP_WITH_COMPRESSION = _make_op({'encodings': ['gzip']})\nOP_UNKNOWN_COMPRESSION = _make_op({'encodings': ['foo']})\nOP_MULTIPLE_COMPRESSIONS = _make_op({'encodings': ['gzip', 'foo']})\nSTREAMING_OP_WITH_COMPRESSION = _make_op(\n    {'encodings': ['gzip']},\n    True,\n    {},\n)\nSTREAMING_OP_WITH_COMPRESSION_REQUIRES_LENGTH = _make_op(\n    {'encodings': ['gzip']},\n    True,\n    {'requiresLength': True},\n)\n\n\nREQUEST_BODY = (\n    b'Action=PutMetricData&Version=2010-08-01&Namespace=Namespace'\n    b'&MetricData.member.1.MetricName=metric&MetricData.member.1.Unit=Bytes'\n    b'&MetricData.member.1.Value=128'\n)\nREQUEST_BODY_COMPRESSED = (\n    b'\\x1f\\x8b\\x08\\x00\\x01\\x00\\x00\\x00\\x02\\xffsL.\\xc9\\xcc\\xcf\\xb3\\r(-\\xf1M-)'\n    b'\\xcaLvI,IT\\x0bK-*\\x06\\x89\\x1a\\x19\\x18\\x1a\\xe8\\x1aX\\xe8\\x1a\\x18\\xaa\\xf9%'\n    b'\\xe6\\xa6\\x16\\x17$&\\xa7\\xda\\xc2Yj\\x08\\x1dz\\xb9\\xa9\\xb9I\\xa9Ez\\x86z\\x101\\x90'\n    b'\\x1a\\xdb\\\\0\\x13\\xab\\xaa\\xd0\\xbc\\xcc\\x12[\\xa7\\xca\\x92\\xd4b\\xac\\xd2a\\x899\\xa5'\n    b'\\xa9\\xb6\\x86F\\x16\\x00\\x1e\\xdd\\t\\xfd\\x9e\\x00\\x00\\x00'\n)\n\n\nCOMPRESSION_CONFIG_128_BYTES = Config(\n    disable_request_compression=False,\n    request_min_compression_size_bytes=128,\n)\nCOMPRESSION_CONFIG_1_BYTE = Config(\n    disable_request_compression=False,\n    request_min_compression_size_bytes=1,\n)\n\n\nclass NonSeekableStream:\n    def __init__(self, buffer):\n        self._buffer = buffer\n\n    def read(self, size=None):\n        return self._buffer.read(size)\n\n\ndef _request_dict(body=REQUEST_BODY, headers=None):\n    if headers is None:\n        headers = {}\n\n    return {\n        'body': body,\n        'headers': headers,\n    }\n\n\ndef request_dict_non_seekable_text_stream():\n    stream = NonSeekableStream(io.StringIO(REQUEST_BODY.decode('utf-8')))\n    return _request_dict(stream)\n\n\ndef request_dict_non_seekable_bytes_stream():\n    return _request_dict(NonSeekableStream(io.BytesIO(REQUEST_BODY)))\n\n\nclass StaticGzipFile(gzip.GzipFile):\n    def __init__(self, *args, **kwargs):\n        kwargs['mtime'] = 1\n        super().__init__(*args, **kwargs)\n\n\ndef static_compress(*args, **kwargs):\n    kwargs['mtime'] = 1\n    return gzip.compress(*args, **kwargs)\n\n\ndef _bad_compression(body):\n    raise ValueError('Reached unintended compression algorithm \"foo\"')\n\n\nMOCK_COMPRESSION = {'foo': _bad_compression}\nMOCK_COMPRESSION.update(COMPRESSION_MAPPING)\n\n\ndef _assert_compression_body(compressed_body, expected_body):\n    data = compressed_body\n    if hasattr(compressed_body, 'read'):\n        data = compressed_body.read()\n    assert data == expected_body\n\n\ndef _assert_compression_header(headers, encoding='gzip'):\n    assert 'Content-Encoding' in headers\n    assert encoding in headers['Content-Encoding']\n\n\ndef assert_request_compressed(request_dict, expected_body):\n    _assert_compression_body(request_dict['body'], expected_body)\n    _assert_compression_header(request_dict['headers'])\n\n\n@pytest.mark.parametrize(\n    'request_dict, operation_model',\n    [\n        (\n            _request_dict(),\n            OP_WITH_COMPRESSION,\n        ),\n        (\n            _request_dict(),\n            OP_MULTIPLE_COMPRESSIONS,\n        ),\n        (\n            _request_dict(),\n            STREAMING_OP_WITH_COMPRESSION,\n        ),\n        (\n            _request_dict(bytearray(REQUEST_BODY)),\n            OP_WITH_COMPRESSION,\n        ),\n        (\n            _request_dict(headers={'Content-Encoding': 'identity'}),\n            OP_WITH_COMPRESSION,\n        ),\n        (\n            _request_dict(REQUEST_BODY.decode('utf-8')),\n            OP_WITH_COMPRESSION,\n        ),\n        (\n            _request_dict(io.BytesIO(REQUEST_BODY)),\n            OP_WITH_COMPRESSION,\n        ),\n        (\n            _request_dict(io.StringIO(REQUEST_BODY.decode('utf-8'))),\n            OP_WITH_COMPRESSION,\n        ),\n        (\n            request_dict_non_seekable_bytes_stream(),\n            STREAMING_OP_WITH_COMPRESSION,\n        ),\n        (\n            request_dict_non_seekable_text_stream(),\n            STREAMING_OP_WITH_COMPRESSION,\n        ),\n    ],\n)\n@mock.patch.object(botocore.compress, 'GzipFile', StaticGzipFile)\n@mock.patch.object(botocore.compress, 'gzip_compress', static_compress)\n@pytest.mark.skipif(\n    sys.version_info < (3, 8), reason='requires python3.8 or higher'\n)\ndef test_compression(request_dict, operation_model):\n    maybe_compress_request(\n        COMPRESSION_CONFIG_128_BYTES, request_dict, operation_model\n    )\n    assert_request_compressed(request_dict, REQUEST_BODY_COMPRESSED)\n\n\n@pytest.mark.parametrize(\n    'config, request_dict, operation_model',\n    [\n        (\n            Config(\n                disable_request_compression=True,\n                request_min_compression_size_bytes=1,\n            ),\n            _request_dict(),\n            OP_WITH_COMPRESSION,\n        ),\n        (\n            Config(\n                disable_request_compression=False,\n                request_min_compression_size_bytes=256,\n            ),\n            _request_dict(),\n            OP_WITH_COMPRESSION,\n        ),\n        (\n            Config(\n                disable_request_compression=False,\n                request_min_compression_size_bytes=1,\n                signature_version='v2',\n            ),\n            _request_dict(),\n            OP_WITH_COMPRESSION,\n        ),\n        (\n            COMPRESSION_CONFIG_128_BYTES,\n            _request_dict(),\n            STREAMING_OP_WITH_COMPRESSION_REQUIRES_LENGTH,\n        ),\n        (\n            COMPRESSION_CONFIG_128_BYTES,\n            _request_dict(),\n            OP_NO_COMPRESSION,\n        ),\n        (\n            COMPRESSION_CONFIG_128_BYTES,\n            _request_dict(),\n            OP_UNKNOWN_COMPRESSION,\n        ),\n        (\n            COMPRESSION_CONFIG_128_BYTES,\n            _request_dict(headers={'Content-Encoding': 'identity'}),\n            OP_UNKNOWN_COMPRESSION,\n        ),\n        (\n            COMPRESSION_CONFIG_128_BYTES,\n            request_dict_non_seekable_bytes_stream(),\n            OP_WITH_COMPRESSION,\n        ),\n    ],\n)\ndef test_no_compression(config, request_dict, operation_model):\n    ce_header = request_dict['headers'].get('Content-Encoding')\n    original_body = request_dict['body']\n    maybe_compress_request(config, request_dict, operation_model)\n    assert request_dict['body'] == original_body\n    assert ce_header == request_dict['headers'].get('Content-Encoding')\n\n\n@pytest.mark.parametrize(\n    'operation_model, expected_body',\n    [\n        (\n            OP_WITH_COMPRESSION,\n            (\n                b'\\x1f\\x8b\\x08\\x00\\x01\\x00\\x00\\x00\\x02\\xffK\\xcb'\n                b'\\xcf\\xb7MJ,\\x02\\x00v\\x8e5\\x1c\\x07\\x00\\x00\\x00'\n            ),\n        ),\n        (OP_NO_COMPRESSION, {'foo': 'bar'}),\n    ],\n)\n@mock.patch.object(botocore.compress, 'gzip_compress', static_compress)\n@pytest.mark.skipif(\n    sys.version_info < (3, 8), reason='requires python3.8 or higher'\n)\ndef test_dict_compression(operation_model, expected_body):\n    request_dict = _request_dict({'foo': 'bar'})\n    maybe_compress_request(\n        COMPRESSION_CONFIG_1_BYTE, request_dict, operation_model\n    )\n    body = request_dict['body']\n    assert body == expected_body\n\n\n@pytest.mark.parametrize('body', [1, object(), True, 1.0])\ndef test_maybe_compress_bad_types(body):\n    request_dict = _request_dict(body)\n    maybe_compress_request(\n        COMPRESSION_CONFIG_1_BYTE, request_dict, OP_WITH_COMPRESSION\n    )\n    assert request_dict['body'] == body\n\n\n@mock.patch.object(botocore.compress, 'GzipFile', StaticGzipFile)\ndef test_body_streams_position_reset():\n    request_dict = _request_dict(io.BytesIO(REQUEST_BODY))\n    maybe_compress_request(\n        COMPRESSION_CONFIG_128_BYTES,\n        request_dict,\n        OP_WITH_COMPRESSION,\n    )\n    assert request_dict['body'].tell() == 0\n    assert_request_compressed(request_dict, REQUEST_BODY_COMPRESSED)\n\n\n@mock.patch.object(botocore.compress, 'gzip_compress', static_compress)\n@mock.patch.object(botocore.compress, 'COMPRESSION_MAPPING', MOCK_COMPRESSION)\n@pytest.mark.skipif(\n    sys.version_info < (3, 8), reason='requires python3.8 or higher'\n)\ndef test_only_compress_once():\n    request_dict = _request_dict()\n    maybe_compress_request(\n        COMPRESSION_CONFIG_128_BYTES,\n        request_dict,\n        OP_MULTIPLE_COMPRESSIONS,\n    )\n    assert_request_compressed(request_dict, REQUEST_BODY_COMPRESSED)\n", "tests/unit/test_httpchecksum.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport unittest\nfrom io import BytesIO\n\nfrom botocore.awsrequest import AWSResponse\nfrom botocore.compat import HAS_CRT\nfrom botocore.exceptions import (\n    AwsChunkedWrapperError,\n    FlexibleChecksumError,\n    MissingDependencyException,\n)\nfrom botocore.httpchecksum import (\n    _CHECKSUM_CLS,\n    AwsChunkedWrapper,\n    Crc32Checksum,\n    CrtCrc32cChecksum,\n    CrtCrc32Checksum,\n    Sha1Checksum,\n    Sha256Checksum,\n    StreamingChecksumBody,\n    apply_request_checksum,\n    handle_checksum_body,\n    resolve_request_checksum_algorithm,\n    resolve_response_checksum_algorithms,\n)\nfrom botocore.model import OperationModel\nfrom tests import mock, requires_crt\n\n\nclass TestHttpChecksumHandlers(unittest.TestCase):\n    def _make_operation_model(\n        self,\n        http_checksum=None,\n        streaming_output=False,\n        streaming_input=False,\n        required=False,\n    ):\n        operation = mock.Mock(spec=OperationModel)\n        if http_checksum is None:\n            http_checksum = {}\n        operation.http_checksum = http_checksum\n        operation.http_checksum_required = required\n        operation.has_streaming_output = streaming_output\n        operation.has_streaming_input = streaming_input\n        return operation\n\n    def _make_http_response(\n        self,\n        body,\n        headers=None,\n        context=None,\n        streaming=False,\n    ):\n        if context is None:\n            context = {}\n\n        if headers is None:\n            headers = {}\n\n        http_response = mock.Mock(spec=AWSResponse)\n        http_response.raw = BytesIO(body)\n        http_response.content = body\n        http_response.status_code = 200\n        http_response.headers = headers\n        response_dict = {\n            \"headers\": http_response.headers,\n            \"status_code\": http_response.status_code,\n            \"context\": context,\n        }\n        if streaming:\n            response_dict[\"body\"] = BytesIO(body)\n        else:\n            response_dict[\"body\"] = body\n        return http_response, response_dict\n\n    def _build_request(self, body):\n        request = {\n            \"headers\": {},\n            \"body\": body,\n            \"context\": {},\n            \"url\": \"https://example.com\",\n        }\n        return request\n\n    def test_request_checksum_algorithm_no_model(self):\n        request = self._build_request(b\"\")\n        operation_model = self._make_operation_model()\n        params = {}\n        resolve_request_checksum_algorithm(request, operation_model, params)\n        self.assertNotIn(\"checksum\", request[\"context\"])\n\n    def test_request_checksum_algorithm_model_opt_in(self):\n        operation_model = self._make_operation_model(\n            http_checksum={\"requestAlgorithmMember\": \"Algorithm\"}\n        )\n\n        # Param is not present, no checksum will be set\n        params = {}\n        request = self._build_request(b\"\")\n        resolve_request_checksum_algorithm(request, operation_model, params)\n        self.assertNotIn(\"checksum\", request[\"context\"])\n\n        # Param is present, crc32 checksum will be set\n        params = {\"Algorithm\": \"crc32\"}\n        request = self._build_request(b\"\")\n        resolve_request_checksum_algorithm(request, operation_model, params)\n        expected_algorithm = {\n            \"algorithm\": \"crc32\",\n            \"in\": \"header\",\n            \"name\": \"x-amz-checksum-crc32\",\n        }\n        actual_algorithm = request[\"context\"][\"checksum\"][\"request_algorithm\"]\n        self.assertEqual(actual_algorithm, expected_algorithm)\n\n        # Param present but header already set, checksum should be skipped\n        params = {\"Algorithm\": \"crc32\"}\n        request = self._build_request(b\"\")\n        request[\"headers\"][\"x-amz-checksum-crc32\"] = \"foo\"\n        resolve_request_checksum_algorithm(request, operation_model, params)\n        self.assertNotIn(\"checksum\", request[\"context\"])\n\n    def test_request_checksum_algorithm_model_opt_in_streaming(self):\n        request = self._build_request(b\"\")\n        operation_model = self._make_operation_model(\n            http_checksum={\"requestAlgorithmMember\": \"Algorithm\"},\n            streaming_input=True,\n        )\n\n        # Param is not present, no checksum will be set\n        params = {}\n        resolve_request_checksum_algorithm(request, operation_model, params)\n        self.assertNotIn(\"checksum\", request[\"context\"])\n\n        # Param is present, crc32 checksum will be set in the trailer\n        params = {\"Algorithm\": \"crc32\"}\n        resolve_request_checksum_algorithm(request, operation_model, params)\n        expected_algorithm = {\n            \"algorithm\": \"crc32\",\n            \"in\": \"trailer\",\n            \"name\": \"x-amz-checksum-crc32\",\n        }\n        actual_algorithm = request[\"context\"][\"checksum\"][\"request_algorithm\"]\n        self.assertEqual(actual_algorithm, expected_algorithm)\n\n        # Trailer should not be used for http endpoints\n        request = self._build_request(b\"\")\n        request[\"url\"] = \"http://example.com\"\n        resolve_request_checksum_algorithm(request, operation_model, params)\n        expected_algorithm = {\n            \"algorithm\": \"crc32\",\n            \"in\": \"header\",\n            \"name\": \"x-amz-checksum-crc32\",\n        }\n        actual_algorithm = request[\"context\"][\"checksum\"][\"request_algorithm\"]\n        self.assertEqual(actual_algorithm, expected_algorithm)\n\n    def test_request_checksum_algorithm_model_unsupported_algorithm(self):\n        request = self._build_request(b\"\")\n        operation_model = self._make_operation_model(\n            http_checksum={\"requestAlgorithmMember\": \"Algorithm\"},\n        )\n        params = {\"Algorithm\": \"sha256\"}\n\n        with self.assertRaises(FlexibleChecksumError):\n            resolve_request_checksum_algorithm(\n                request, operation_model, params, supported_algorithms=[]\n            )\n\n    @unittest.skipIf(HAS_CRT, \"Error only expected when CRT is not available\")\n    def test_request_checksum_algorithm_model_no_crt_crc32c_unsupported(self):\n        request = self._build_request(b\"\")\n        operation_model = self._make_operation_model(\n            http_checksum={\"requestAlgorithmMember\": \"Algorithm\"},\n        )\n        params = {\"Algorithm\": \"crc32c\"}\n        with self.assertRaises(MissingDependencyException) as context:\n            resolve_request_checksum_algorithm(\n                request, operation_model, params\n            )\n            self.assertIn(\n                \"Using CRC32C requires an additional dependency\",\n                str(context.exception),\n            )\n\n    def test_request_checksum_algorithm_model_legacy_md5(self):\n        request = self._build_request(b\"\")\n        operation_model = self._make_operation_model(required=True)\n        params = {}\n\n        resolve_request_checksum_algorithm(request, operation_model, params)\n        actual_algorithm = request[\"context\"][\"checksum\"][\"request_algorithm\"]\n        expected_algorithm = \"conditional-md5\"\n        self.assertEqual(actual_algorithm, expected_algorithm)\n\n    def test_request_checksum_algorithm_model_new_md5(self):\n        request = self._build_request(b\"\")\n        operation_model = self._make_operation_model(\n            http_checksum={\"requestChecksumRequired\": True}\n        )\n        params = {}\n\n        resolve_request_checksum_algorithm(request, operation_model, params)\n        actual_algorithm = request[\"context\"][\"checksum\"][\"request_algorithm\"]\n        expected_algorithm = \"conditional-md5\"\n        self.assertEqual(actual_algorithm, expected_algorithm)\n\n    def test_apply_request_checksum_handles_no_checksum_context(self):\n        request = self._build_request(b\"\")\n        apply_request_checksum(request)\n        # Build another request and assert the original request is the same\n        expected_request = self._build_request(b\"\")\n        self.assertEqual(request, expected_request)\n\n    def test_apply_request_checksum_handles_invalid_context(self):\n        request = self._build_request(b\"\")\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": {\n                \"in\": \"http-trailer\",\n                \"algorithm\": \"crc32\",\n                \"name\": \"x-amz-checksum-crc32\",\n            }\n        }\n        with self.assertRaises(FlexibleChecksumError):\n            apply_request_checksum(request)\n\n    def test_apply_request_checksum_conditional_md5(self):\n        request = self._build_request(b\"\")\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": \"conditional-md5\"\n        }\n        apply_request_checksum(request)\n        self.assertIn(\"Content-MD5\", request[\"headers\"])\n\n    def test_apply_request_checksum_flex_header_bytes(self):\n        request = self._build_request(b\"\")\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": {\n                \"in\": \"header\",\n                \"algorithm\": \"crc32\",\n                \"name\": \"x-amz-checksum-crc32\",\n            }\n        }\n        apply_request_checksum(request)\n        self.assertIn(\"x-amz-checksum-crc32\", request[\"headers\"])\n\n    def test_apply_request_checksum_flex_header_readable(self):\n        request = self._build_request(BytesIO(b\"\"))\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": {\n                \"in\": \"header\",\n                \"algorithm\": \"crc32\",\n                \"name\": \"x-amz-checksum-crc32\",\n            }\n        }\n        apply_request_checksum(request)\n        self.assertIn(\"x-amz-checksum-crc32\", request[\"headers\"])\n\n    def test_apply_request_checksum_flex_header_explicit_digest(self):\n        request = self._build_request(b\"\")\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": {\n                \"in\": \"header\",\n                \"algorithm\": \"crc32\",\n                \"name\": \"x-amz-checksum-crc32\",\n            }\n        }\n        request[\"headers\"][\"x-amz-checksum-crc32\"] = \"foo\"\n        apply_request_checksum(request)\n        # The checksum should not have been modified\n        self.assertEqual(request[\"headers\"][\"x-amz-checksum-crc32\"], \"foo\")\n\n    def test_apply_request_checksum_flex_trailer_bytes(self):\n        request = self._build_request(b\"\")\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": {\n                \"in\": \"trailer\",\n                \"algorithm\": \"crc32\",\n                \"name\": \"x-amz-checksum-crc32\",\n            }\n        }\n        apply_request_checksum(request)\n        self.assertNotIn(\"x-amz-checksum-crc32\", request[\"headers\"])\n        self.assertIsInstance(request[\"body\"], AwsChunkedWrapper)\n\n    def test_apply_request_checksum_flex_trailer_readable(self):\n        request = self._build_request(BytesIO(b\"\"))\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": {\n                \"in\": \"trailer\",\n                \"algorithm\": \"crc32\",\n                \"name\": \"x-amz-checksum-crc32\",\n            }\n        }\n        apply_request_checksum(request)\n        self.assertNotIn(\"x-amz-checksum-crc32\", request[\"headers\"])\n        self.assertIsInstance(request[\"body\"], AwsChunkedWrapper)\n\n    def test_apply_request_checksum_flex_header_trailer_explicit_digest(self):\n        request = self._build_request(b\"\")\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": {\n                \"in\": \"trailer\",\n                \"algorithm\": \"crc32\",\n                \"name\": \"x-amz-checksum-crc32\",\n            }\n        }\n        request[\"headers\"][\"x-amz-checksum-crc32\"] = \"foo\"\n        apply_request_checksum(request)\n        # The checksum should not have been modified\n        self.assertEqual(request[\"headers\"][\"x-amz-checksum-crc32\"], \"foo\")\n        # The body should not have been wrapped\n        self.assertIsInstance(request[\"body\"], bytes)\n\n    def test_apply_request_checksum_content_encoding_preset(self):\n        request = self._build_request(b\"\")\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": {\n                \"in\": \"trailer\",\n                \"algorithm\": \"crc32\",\n                \"name\": \"x-amz-checksum-crc32\",\n            }\n        }\n        request[\"headers\"][\"Content-Encoding\"] = \"foo\"\n        apply_request_checksum(request)\n        # The content encoding should only have been appended\n        self.assertEqual(\n            request[\"headers\"][\"Content-Encoding\"], \"foo,aws-chunked\"\n        )\n\n    def test_apply_request_checksum_content_encoding_default(self):\n        request = self._build_request(b\"\")\n        request[\"context\"][\"checksum\"] = {\n            \"request_algorithm\": {\n                \"in\": \"trailer\",\n                \"algorithm\": \"crc32\",\n                \"name\": \"x-amz-checksum-crc32\",\n            }\n        }\n        apply_request_checksum(request)\n        self.assertEqual(request[\"headers\"][\"Content-Encoding\"], \"aws-chunked\")\n\n    def test_response_checksum_algorithm_no_model(self):\n        request = self._build_request(b\"\")\n        operation_model = self._make_operation_model()\n        params = {}\n        resolve_response_checksum_algorithms(request, operation_model, params)\n        self.assertNotIn(\"checksum\", request[\"context\"])\n\n    def test_response_checksum_algorithm_model_opt_in(self):\n        request = self._build_request(b\"\")\n        operation_model = self._make_operation_model(\n            http_checksum={\n                \"responseAlgorithms\": [\"crc32\", \"sha1\", \"sha256\"],\n                \"requestValidationModeMember\": \"ChecksumMode\",\n            }\n        )\n\n        # Param is not present, no algorithms will be set\n        params = {}\n        resolve_response_checksum_algorithms(request, operation_model, params)\n        self.assertNotIn(\"checksum\", request[\"context\"])\n\n        # Param is present, algorithms will be set\n        params = {\"ChecksumMode\": \"enabled\"}\n        resolve_response_checksum_algorithms(\n            request,\n            operation_model,\n            params,\n            supported_algorithms=[\"sha1\", \"sha256\"],\n        )\n        # CRC32 should have been filtered it out as it was not supported\n        expected_algorithms = [\"sha1\", \"sha256\"]\n        actual_algorithms = request[\"context\"][\"checksum\"][\n            \"response_algorithms\"\n        ]\n        self.assertEqual(actual_algorithms, expected_algorithms)\n\n    def test_handle_checksum_body_checksum(self):\n        context = {\"checksum\": {\"response_algorithms\": [\"sha1\", \"crc32\"]}}\n        headers = {\"x-amz-checksum-crc32\": \"DUoRhQ==\"}\n        http_response, response_dict = self._make_http_response(\n            b\"hello world\",\n            headers=headers,\n            context=context,\n        )\n        operation_model = self._make_operation_model()\n        handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n        body = response_dict[\"body\"]\n        self.assertEqual(body, b\"hello world\")\n        algorithm = response_dict[\"context\"][\"checksum\"][\"response_algorithm\"]\n        self.assertEqual(algorithm, \"crc32\")\n\n        headers = {\"x-amz-checksum-crc32\": \"WrOonG==\"}\n        http_response, response_dict = self._make_http_response(\n            b\"hello world\",\n            headers=headers,\n            context=context,\n        )\n        with self.assertRaises(FlexibleChecksumError):\n            handle_checksum_body(\n                http_response,\n                response_dict,\n                context,\n                operation_model,\n            )\n\n        # This header should not be checked, we won't calculate a checksum\n        # but a proper body should still come out at the end\n        headers = {\"x-amz-checksum-foo\": \"FOO==\"}\n        http_response, response_dict = self._make_http_response(\n            b\"hello world\",\n            headers=headers,\n            context=context,\n        )\n        handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n        body = response_dict[\"body\"]\n        self.assertEqual(body, b\"hello world\")\n        algorithm = response_dict[\"context\"][\"checksum\"][\"response_algorithm\"]\n        self.assertEqual(algorithm, \"crc32\")\n\n    def test_handle_checksum_body_checksum_streaming(self):\n        context = {\"checksum\": {\"response_algorithms\": [\"sha1\", \"crc32\"]}}\n        headers = {\"x-amz-checksum-crc32\": \"DUoRhQ==\"}\n        http_response, response_dict = self._make_http_response(\n            b\"hello world\",\n            headers=headers,\n            context=context,\n            streaming=True,\n        )\n        operation_model = self._make_operation_model(streaming_output=True)\n        handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n        body = response_dict[\"body\"]\n        self.assertEqual(body.read(), b\"hello world\")\n        algorithm = response_dict[\"context\"][\"checksum\"][\"response_algorithm\"]\n        self.assertEqual(algorithm, \"crc32\")\n\n        headers = {\"x-amz-checksum-crc32\": \"WrOonG==\"}\n        http_response, response_dict = self._make_http_response(\n            b\"hello world\",\n            headers=headers,\n            context=context,\n            streaming=True,\n        )\n        handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n        body = response_dict[\"body\"]\n        with self.assertRaises(FlexibleChecksumError):\n            body.read()\n\n        # This header should not be checked, we won't calculate a checksum\n        # but a proper body should still come out at the end\n        headers = {\"x-amz-checksum-foo\": \"FOOO==\"}\n        http_response, response_dict = self._make_http_response(\n            b\"hello world\",\n            headers=headers,\n            context=context,\n            streaming=True,\n        )\n        handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n        body = response_dict[\"body\"]\n        self.assertEqual(body.read(), b\"hello world\")\n        algorithm = response_dict[\"context\"][\"checksum\"][\"response_algorithm\"]\n        self.assertEqual(algorithm, \"crc32\")\n\n    def test_handle_checksum_body_checksum_skip_non_streaming(self):\n        context = {\"checksum\": {\"response_algorithms\": [\"sha1\", \"crc32\"]}}\n        # S3 will return checksums over the checksums of parts which are a\n        # special case that end with -#. These cannot be validated and are\n        # instead skipped\n        headers = {\"x-amz-checksum-crc32\": \"FOOO==-123\"}\n        http_response, response_dict = self._make_http_response(\n            b\"hello world\",\n            headers=headers,\n            context=context,\n        )\n        operation_model = self._make_operation_model()\n        handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n        body = response_dict[\"body\"]\n        self.assertEqual(body, b\"hello world\")\n\n    def test_handle_checksum_body_checksum_skip_streaming(self):\n        context = {\"checksum\": {\"response_algorithms\": [\"sha1\", \"crc32\"]}}\n        # S3 will return checksums over the checksums of parts which are a\n        # special case that end with -#. These cannot be validated and are\n        # instead skipped\n        headers = {\"x-amz-checksum-crc32\": \"FOOO==-123\"}\n        http_response, response_dict = self._make_http_response(\n            b\"hello world\",\n            headers=headers,\n            context=context,\n            streaming=True,\n        )\n        operation_model = self._make_operation_model(streaming_output=True)\n        handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n        body = response_dict[\"body\"]\n        self.assertEqual(body.read(), b\"hello world\")\n\n\nclass TestAwsChunkedWrapper(unittest.TestCase):\n    def test_single_chunk_body(self):\n        # Test a small body that fits in a single chunk\n        bytes = BytesIO(b\"abcdefghijklmnopqrstuvwxyz\")\n        wrapper = AwsChunkedWrapper(bytes)\n        body = wrapper.read()\n        expected = b\"1a\\r\\n\" b\"abcdefghijklmnopqrstuvwxyz\\r\\n\" b\"0\\r\\n\\r\\n\"\n        self.assertEqual(body, expected)\n\n    def test_multi_chunk_body(self):\n        # Test a body that requires multiple chunks\n        bytes = BytesIO(b\"abcdefghijklmnopqrstuvwxyz\")\n        wrapper = AwsChunkedWrapper(bytes, chunk_size=10)\n        body = wrapper.read()\n        expected = (\n            b\"a\\r\\n\"\n            b\"abcdefghij\\r\\n\"\n            b\"a\\r\\n\"\n            b\"klmnopqrst\\r\\n\"\n            b\"6\\r\\n\"\n            b\"uvwxyz\\r\\n\"\n            b\"0\\r\\n\\r\\n\"\n        )\n        self.assertEqual(body, expected)\n\n    def test_read_returns_less_data(self):\n        class OneLessBytesIO(BytesIO):\n            def read(self, size=-1):\n                # Return 1 less byte than was asked for\n                return super().read(size - 1)\n\n        bytes = OneLessBytesIO(b\"abcdefghijklmnopqrstuvwxyz\")\n        wrapper = AwsChunkedWrapper(bytes, chunk_size=10)\n        body = wrapper.read()\n        # NOTE: This particular body is not important, but it is important that\n        # the actual size of the chunk matches the length sent which may not\n        # always be the configured chunk_size if the read does not return that\n        # much data.\n        expected = (\n            b\"9\\r\\n\"\n            b\"abcdefghi\\r\\n\"\n            b\"9\\r\\n\"\n            b\"jklmnopqr\\r\\n\"\n            b\"8\\r\\n\"\n            b\"stuvwxyz\\r\\n\"\n            b\"0\\r\\n\\r\\n\"\n        )\n        self.assertEqual(body, expected)\n\n    def test_single_chunk_body_with_checksum(self):\n        wrapper = AwsChunkedWrapper(\n            BytesIO(b\"hello world\"),\n            checksum_cls=Crc32Checksum,\n            checksum_name=\"checksum\",\n        )\n        body = wrapper.read()\n        expected = (\n            b\"b\\r\\n\" b\"hello world\\r\\n\" b\"0\\r\\n\" b\"checksum:DUoRhQ==\\r\\n\\r\\n\"\n        )\n        self.assertEqual(body, expected)\n\n    def test_multi_chunk_body_with_checksum(self):\n        wrapper = AwsChunkedWrapper(\n            BytesIO(b\"hello world\"),\n            chunk_size=5,\n            checksum_cls=Crc32Checksum,\n            checksum_name=\"checksum\",\n        )\n        body = wrapper.read()\n        expected = (\n            b\"5\\r\\n\"\n            b\"hello\\r\\n\"\n            b\"5\\r\\n\"\n            b\" worl\\r\\n\"\n            b\"1\\r\\n\"\n            b\"d\\r\\n\"\n            b\"0\\r\\n\"\n            b\"checksum:DUoRhQ==\\r\\n\\r\\n\"\n        )\n        self.assertEqual(body, expected)\n\n    def test_multi_chunk_body_with_checksum_iter(self):\n        wrapper = AwsChunkedWrapper(\n            BytesIO(b\"hello world\"),\n            chunk_size=5,\n            checksum_cls=Crc32Checksum,\n            checksum_name=\"checksum\",\n        )\n        expected_chunks = [\n            b\"5\\r\\nhello\\r\\n\",\n            b\"5\\r\\n worl\\r\\n\",\n            b\"1\\r\\nd\\r\\n\",\n            b\"0\\r\\nchecksum:DUoRhQ==\\r\\n\\r\\n\",\n        ]\n        self.assertListEqual(expected_chunks, list(wrapper))\n\n    def test_wrapper_can_be_reset(self):\n        wrapper = AwsChunkedWrapper(\n            BytesIO(b\"hello world\"),\n            chunk_size=5,\n            checksum_cls=Crc32Checksum,\n            checksum_name=\"checksum\",\n        )\n        first_read = wrapper.read()\n        self.assertEqual(b\"\", wrapper.read())\n        wrapper.seek(0)\n        second_read = wrapper.read()\n        self.assertEqual(first_read, second_read)\n        self.assertIn(b\"checksum:DUoRhQ==\", first_read)\n\n    def test_wrapper_can_only_seek_to_start(self):\n        wrapper = AwsChunkedWrapper(BytesIO())\n        with self.assertRaises(AwsChunkedWrapperError):\n            wrapper.seek(1)\n        with self.assertRaises(AwsChunkedWrapperError):\n            wrapper.seek(0, whence=1)\n        with self.assertRaises(AwsChunkedWrapperError):\n            wrapper.seek(1, whence=2)\n\n\nclass TestChecksumImplementations(unittest.TestCase):\n    def assert_base64_checksum(self, checksum_cls, expected_digest):\n        checksum = checksum_cls()\n        checksum.update(b\"hello world\")\n        actual_digest = checksum.b64digest()\n        self.assertEqual(actual_digest, expected_digest)\n\n    def test_crc32(self):\n        self.assert_base64_checksum(Crc32Checksum, \"DUoRhQ==\")\n\n    def test_sha1(self):\n        self.assert_base64_checksum(\n            Sha1Checksum,\n            \"Kq5sNclPz7QV2+lfQIuc6R7oRu0=\",\n        )\n\n    def test_sha256(self):\n        self.assert_base64_checksum(\n            Sha256Checksum,\n            \"uU0nuZNNPgilLlLX2n2r+sSE7+N6U4DukIj3rOLvzek=\",\n        )\n\n    @requires_crt()\n    def test_crt_crc32(self):\n        self.assert_base64_checksum(CrtCrc32Checksum, \"DUoRhQ==\")\n\n    @requires_crt()\n    def test_crt_crc32c(self):\n        self.assert_base64_checksum(CrtCrc32cChecksum, \"yZRlqg==\")\n\n\nclass TestCrtChecksumOverrides(unittest.TestCase):\n    @requires_crt()\n    def test_crt_crc32_available(self):\n        actual_cls = _CHECKSUM_CLS.get(\"crc32\")\n        self.assertEqual(actual_cls, CrtCrc32Checksum)\n\n    @requires_crt()\n    def test_crt_crc32c_available(self):\n        actual_cls = _CHECKSUM_CLS.get(\"crc32c\")\n        self.assertEqual(actual_cls, CrtCrc32cChecksum)\n\n\nclass TestStreamingChecksumBody(unittest.TestCase):\n    def setUp(self):\n        self.raw_bytes = b\"hello world\"\n        self.fake_body = BytesIO(self.raw_bytes)\n        self._make_wrapper(\"DUoRhQ==\")\n\n    def _make_wrapper(self, checksum):\n        self.wrapper = StreamingChecksumBody(\n            self.fake_body,\n            None,\n            Crc32Checksum(),\n            checksum,\n        )\n\n    def test_basic_read_good(self):\n        actual = self.wrapper.read()\n        self.assertEqual(actual, self.raw_bytes)\n\n    def test_many_reads_good(self):\n        actual = b\"\"\n        actual += self.wrapper.read(5)\n        actual += self.wrapper.read(5)\n        actual += self.wrapper.read(1)\n        self.assertEqual(actual, self.raw_bytes)\n\n    def test_basic_read_bad(self):\n        self._make_wrapper(\"duorhq==\")\n        with self.assertRaises(FlexibleChecksumError):\n            self.wrapper.read()\n\n    def test_many_reads_bad(self):\n        self._make_wrapper(\"duorhq==\")\n        self.wrapper.read(5)\n        self.wrapper.read(6)\n        # Whole body has been read, next read signals the end of the stream and\n        # validates the checksum of the body contents read\n        with self.assertRaises(FlexibleChecksumError):\n            self.wrapper.read(1)\n\n    def test_handles_variable_padding(self):\n        # This digest is equivalent but with more padding\n        self._make_wrapper(\"DUoRhQ=====\")\n        actual = self.wrapper.read()\n        self.assertEqual(actual, self.raw_bytes)\n\n    def test_iter_raises_error(self):\n        self._make_wrapper(\"duorhq==\")\n        with self.assertRaises(FlexibleChecksumError):\n            for chunk in self.wrapper:\n                pass\n", "tests/unit/test_tokens.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport dateutil.parser\nimport pytest\n\nfrom botocore.exceptions import (\n    InvalidConfigError,\n    SSOTokenLoadError,\n    TokenRetrievalError,\n)\nfrom botocore.session import Session\nfrom botocore.tokens import SSOTokenProvider\nfrom tests import mock\n\n\ndef parametrize(cases):\n    return pytest.mark.parametrize(\n        \"test_case\",\n        cases,\n        ids=[c[\"documentation\"] for c in cases],\n    )\n\n\nsso_provider_resolution_cases = [\n    {\n        \"documentation\": \"Full valid profile\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"sso_session\": \"admin\"}},\n            \"sso_sessions\": {\n                \"admin\": {\n                    \"sso_region\": \"us-east-1\",\n                    \"sso_start_url\": \"https://d-abc123.awsapps.com/start\",\n                }\n            },\n        },\n        \"resolves\": True,\n    },\n    {\n        \"documentation\": \"Non-SSO profiles are skipped\",\n        \"config\": {\"profiles\": {\"test\": {\"region\": \"us-west-2\"}}},\n        \"resolves\": False,\n    },\n    {\n        \"documentation\": \"Only start URL is invalid\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"sso_session\": \"admin\"}},\n            \"sso_sessions\": {\n                \"admin\": {\n                    \"sso_start_url\": \"https://d-abc123.awsapps.com/start\"\n                }\n            },\n        },\n        \"resolves\": False,\n        \"expectedException\": InvalidConfigError,\n    },\n    {\n        \"documentation\": \"Only sso_region is invalid\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"sso_session\": \"admin\"}},\n            \"sso_sessions\": {\"admin\": {\"sso_region\": \"us-east-1\"}},\n        },\n        \"resolves\": False,\n        \"expectedException\": InvalidConfigError,\n    },\n    {\n        \"documentation\": \"Specified sso-session must exist\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"sso_session\": \"dev\"}},\n            \"sso_sessions\": {\"admin\": {\"sso_region\": \"us-east-1\"}},\n        },\n        \"resolves\": False,\n        \"expectedException\": InvalidConfigError,\n    },\n    {\n        \"documentation\": \"The sso_session must be specified\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"region\": \"us-west-2\"}},\n            \"sso_sessions\": {\n                \"admin\": {\n                    \"sso_region\": \"us-east-1\",\n                    \"sso_start_url\": \"https://d-abc123.awsapps.com/start\",\n                }\n            },\n        },\n        \"resolves\": False,\n    },\n]\n\n\ndef _create_mock_session(config):\n    mock_session = mock.Mock(spec=Session)\n    mock_session.get_config_variable.return_value = \"test\"\n    mock_session.full_config = config\n    return mock_session\n\n\ndef _run_token_provider_test_case(provider, test_case):\n    expected_exception = test_case.get(\"expectedException\")\n    if expected_exception is not None:\n        with pytest.raises(expected_exception):\n            auth_token = provider.load_token()\n        return\n\n    auth_token = provider.load_token()\n    if test_case[\"resolves\"]:\n        assert auth_token is not None\n    else:\n        assert auth_token is None\n\n\n@parametrize(sso_provider_resolution_cases)\ndef test_sso_token_provider_resolution(test_case):\n    mock_session = _create_mock_session(test_case[\"config\"])\n    resolver = SSOTokenProvider(mock_session)\n\n    _run_token_provider_test_case(resolver, test_case)\n\n\n@parametrize(sso_provider_resolution_cases)\ndef test_sso_token_provider_profile_name_overrides_session_profile(test_case):\n    mock_session = _create_mock_session(test_case[\"config\"])\n    mock_session.get_config_variable.return_value = \"default\"\n    resolver = SSOTokenProvider(mock_session, profile_name='test')\n\n    _run_token_provider_test_case(resolver, test_case)\n\n\nsso_provider_refresh_cases = [\n    {\n        \"documentation\": \"Valid token with all fields\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T21:30:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n            \"refreshToken\": \"cachedrefreshtoken\",\n        },\n        \"expectedToken\": {\n            \"token\": \"cachedtoken\",\n            \"expiration\": \"2021-12-25T21:30:00Z\",\n        },\n    },\n    {\n        \"documentation\": \"Minimal valid cached token\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T21:30:00Z\",\n        },\n        \"expectedToken\": {\n            \"token\": \"cachedtoken\",\n            \"expiration\": \"2021-12-25T21:30:00Z\",\n        },\n    },\n    {\n        \"documentation\": \"Minimal expired cached token\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T13:00:00Z\",\n        },\n        \"expectedException\": TokenRetrievalError,\n    },\n    {\n        \"documentation\": \"Token missing the expiresAt field\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\"accessToken\": \"cachedtoken\"},\n        \"expectedException\": SSOTokenLoadError,\n    },\n    {\n        \"documentation\": \"Token missing the accessToken field\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\"expiresAt\": \"2021-12-25T13:00:00Z\"},\n        \"expectedException\": SSOTokenLoadError,\n    },\n    {\n        \"documentation\": \"Expired token refresh with refresh token\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T13:00:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n            \"refreshToken\": \"cachedrefreshtoken\",\n        },\n        \"refreshResponse\": {\n            \"tokenType\": \"Bearer\",\n            \"accessToken\": \"newtoken\",\n            \"expiresIn\": 28800,\n            \"refreshToken\": \"newrefreshtoken\",\n        },\n        \"expectedTokenWriteback\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"newtoken\",\n            \"expiresAt\": \"2021-12-25T21:30:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n            \"refreshToken\": \"newrefreshtoken\",\n        },\n        \"expectedToken\": {\n            \"token\": \"newtoken\",\n            \"expiration\": \"2021-12-25T21:30:00Z\",\n        },\n    },\n    {\n        \"documentation\": \"Expired token refresh without new refresh token\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T13:00:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n            \"refreshToken\": \"cachedrefreshtoken\",\n        },\n        \"refreshResponse\": {\n            \"tokenType\": \"Bearer\",\n            \"accessToken\": \"newtoken\",\n            \"expiresIn\": 28800,\n        },\n        \"expectedTokenWriteback\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"newtoken\",\n            \"expiresAt\": \"2021-12-25T21:30:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n        },\n        \"expectedToken\": {\n            \"token\": \"newtoken\",\n            \"expiration\": \"2021-12-25T21:30:00Z\",\n        },\n    },\n    {\n        \"documentation\": \"Expired token and expired client registration\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-10-25T13:00:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2021-11-25T13:30:00Z\",\n            \"refreshToken\": \"cachedrefreshtoken\",\n        },\n        \"expectedException\": TokenRetrievalError,\n    },\n]\n\n\n@parametrize(sso_provider_refresh_cases)\ndef test_sso_token_provider_refresh(test_case):\n    config = {\n        \"profiles\": {\"test\": {\"sso_session\": \"admin\"}},\n        \"sso_sessions\": {\n            \"admin\": {\n                \"sso_region\": \"us-west-2\",\n                \"sso_start_url\": \"https://d-123.awsapps.com/start\",\n            }\n        },\n    }\n    cache_key = \"d033e22ae348aeb5660fc2140aec35850c4da997\"\n    token_cache = {}\n\n    # Prepopulate the token cache\n    cached_token = test_case.pop(\"cachedToken\", None)\n    if cached_token:\n        token_cache[cache_key] = cached_token\n\n    mock_session = _create_mock_session(config)\n    mock_sso_oidc = mock.Mock()\n    mock_session.create_client.return_value = mock_sso_oidc\n\n    refresh_response = test_case.pop(\"refreshResponse\", None)\n    mock_sso_oidc.create_token.return_value = refresh_response\n\n    current_time = dateutil.parser.parse(test_case.pop(\"currentTime\"))\n\n    def _time_fetcher():\n        return current_time\n\n    resolver = SSOTokenProvider(\n        mock_session,\n        token_cache,\n        time_fetcher=_time_fetcher,\n    )\n\n    auth_token = resolver.load_token()\n\n    actual_exception = None\n    try:\n        actual_token = auth_token.get_frozen_token()\n    except Exception as e:\n        actual_exception = e\n\n    expected_exception = test_case.pop(\"expectedException\", None)\n    if expected_exception is not None:\n        assert isinstance(actual_exception, expected_exception)\n    elif actual_exception is not None:\n        raise actual_exception\n\n    expected_token = test_case.pop(\"expectedToken\", {})\n    raw_token = expected_token.get(\"token\")\n    if raw_token is not None:\n        assert actual_token.token == raw_token\n\n    raw_expiration = expected_token.get(\"expiration\")\n    if raw_expiration is not None:\n        expected_expiration = dateutil.parser.parse(raw_expiration)\n        assert actual_token.expiration == expected_expiration\n\n    expected_token_write_back = test_case.pop(\"expectedTokenWriteback\", None)\n    if expected_token_write_back:\n        mock_sso_oidc.create_token.assert_called_with(\n            grantType=\"refresh_token\",\n            clientId=cached_token[\"clientId\"],\n            clientSecret=cached_token[\"clientSecret\"],\n            refreshToken=cached_token[\"refreshToken\"],\n        )\n        raw_expiration = expected_token_write_back[\"expiresAt\"]\n        # The in-memory cache doesn't serialize to JSON so expect a datetime\n        expected_expiration = dateutil.parser.parse(raw_expiration)\n        expected_token_write_back[\"expiresAt\"] = expected_expiration\n        assert expected_token_write_back == token_cache[cache_key]\n\n    # Pop the documentation to ensure all test fields are handled\n    test_case.pop(\"documentation\")\n    assert not test_case.keys(), \"All fields of test case should be handled\"\n", "tests/unit/test_session.py": "#!/usr/bin/env\n# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\nimport shutil\nimport tempfile\n\nimport pytest\n\nimport botocore.config\nimport botocore.exceptions\nimport botocore.loaders\nimport botocore.session\nfrom botocore import (\n    UNSIGNED,\n    client,\n    register_initializer,\n    unregister_initializer,\n)\nfrom botocore.configprovider import ConfigChainFactory\nfrom botocore.hooks import HierarchicalEmitter\nfrom botocore.model import ServiceModel\nfrom botocore.paginate import PaginatorModel\nfrom botocore.waiter import WaiterModel\nfrom tests import create_session, mock, requires_crt, temporary_file, unittest\n\n\nclass BaseSessionTest(unittest.TestCase):\n    def setUp(self):\n        self.environ = {}\n        self.environ_patch = mock.patch('os.environ', self.environ)\n        self.environ_patch.start()\n        self.environ['FOO_PROFILE'] = 'foo'\n        self.environ['FOO_REGION'] = 'us-west-11'\n        data_path = os.path.join(os.path.dirname(__file__), 'data')\n        self.environ['FOO_DATA_PATH'] = data_path\n        config_path = os.path.join(\n            os.path.dirname(__file__), 'cfg', 'foo_config'\n        )\n        self.environ['FOO_CONFIG_FILE'] = config_path\n        self.session = create_session()\n        config_chain_builder = ConfigChainFactory(\n            session=self.session,\n            environ=self.environ,\n        )\n        config_store = self.session.get_component('config_store')\n        config_updates = {\n            'profile': config_chain_builder.create_config_chain(\n                instance_name='profile',\n                env_var_names='FOO_PROFILE',\n            ),\n            'region': config_chain_builder.create_config_chain(\n                instance_name='region',\n                env_var_names='FOO_REGION',\n                config_property_names='foo_region',\n            ),\n            'data_path': config_chain_builder.create_config_chain(\n                instance_name='data_path',\n                env_var_names='FOO_DATA_PATH',\n                config_property_names='data_path',\n            ),\n            'config_file': config_chain_builder.create_config_chain(\n                instance_name='config_file',\n                env_var_names='FOO_CONFIG_FILE',\n            ),\n            'credentials_file': config_chain_builder.create_config_chain(\n                instance_name='credentials_file',\n                default='/tmp/nowhere',\n            ),\n            'ca_bundle': config_chain_builder.create_config_chain(\n                instance_name='ca_bundle',\n                env_var_names='FOO_AWS_CA_BUNDLE',\n                config_property_names='foo_ca_bundle',\n            ),\n            'api_versions': config_chain_builder.create_config_chain(\n                instance_name='api_versions',\n                config_property_names='foo_api_versions',\n                default={},\n            ),\n        }\n        for name, provider in config_updates.items():\n            config_store.set_config_provider(name, provider)\n\n    def update_session_config_mapping(self, logical_name, **kwargs):\n        config_chain_builder = ConfigChainFactory(\n            session=self.session,\n            environ=self.environ,\n        )\n        self.session.get_component('config_store').set_config_provider(\n            logical_name,\n            config_chain_builder.create_config_chain(**kwargs),\n        )\n\n    def tearDown(self):\n        self.environ_patch.stop()\n\n\nclass SessionTest(BaseSessionTest):\n    def close_log_file_handler(self, tempdir, filename):\n        logger = logging.getLogger('botocore')\n        handlers = logger.handlers\n        for handler in handlers[:]:\n            if hasattr(handler, 'stream') and handler.stream.name == filename:\n                handler.stream.close()\n                logger.removeHandler(handler)\n                os.remove(filename)\n                # logging has an atexit handler that will try to flush/close\n                # the file.  By setting this flag to False, we'll prevent it\n                # from raising an exception, which is fine because we're\n                # handling the closing of the file ourself.\n                logging.raiseExceptions = False\n        shutil.rmtree(tempdir)\n\n    def test_supports_multiple_env_vars_for_single_logical_name(self):\n        self.update_session_config_mapping(\n            'profile', env_var_names=['BAR_DEFAULT_PROFILE', 'BAR_PROFILE']\n        )\n        self.environ['BAR_DEFAULT_PROFILE'] = 'first'\n        self.environ['BAR_PROFILE'] = 'second'\n        self.assertEqual(self.session.get_config_variable('profile'), 'first')\n\n    def test_profile_when_set_explicitly(self):\n        session = create_session(profile='asdf')\n        self.assertEqual(session.profile, 'asdf')\n\n    def test_profile_when_pulled_from_env(self):\n        self.environ['FOO_PROFILE'] = 'bar'\n        # Even though we didn't explicitly pass in a profile, the\n        # profile property will still look this up for us.\n        self.assertEqual(self.session.profile, 'bar')\n\n    def test_multiple_env_vars_uses_second_var(self):\n        self.update_session_config_mapping(\n            'profile', env_var_names=['BAR_DEFAULT_PROFILE', 'BAR_PROFILE']\n        )\n        self.environ.pop('BAR_DEFAULT_PROFILE', None)\n        self.environ['BAR_PROFILE'] = 'second'\n        self.assertEqual(self.session.get_config_variable('profile'), 'second')\n\n    def test_profile_does_not_exist_raises_exception(self):\n        # Given we have no profile:\n        self.environ['FOO_PROFILE'] = 'profile_that_does_not_exist'\n        with self.assertRaises(botocore.exceptions.ProfileNotFound):\n            self.session.get_scoped_config()\n\n    def test_variable_does_not_exist(self):\n        self.assertIsNone(self.session.get_config_variable('foo/bar'))\n\n    def test_get_aws_services_in_alphabetical_order(self):\n        services = self.session.get_available_services()\n        self.assertEqual(sorted(services), services)\n\n    def test_profile_does_not_exist_with_default_profile(self):\n        config = self.session.get_scoped_config()\n        # We should have loaded this properly, and we'll check\n        # that foo_access_key which is defined in the config\n        # file should be present in the loaded config dict.\n        self.assertIn('aws_access_key_id', config)\n\n    def test_type_conversions_occur_when_specified(self):\n        # Specify that we can retrieve the var from the\n        # FOO_TIMEOUT env var, with a conversion function\n        # of int().\n        self.update_session_config_mapping(\n            'metadata_service_timeout',\n            env_var_names='FOO_TIMEOUT',\n            conversion_func=int,\n        )\n        # Environment variables are always strings.\n        self.environ['FOO_TIMEOUT'] = '10'\n        # But we should type convert this to a string.\n        self.assertEqual(\n            self.session.get_config_variable('metadata_service_timeout'), 10\n        )\n\n    def test_default_profile_specified_raises_exception(self):\n        # If you explicity set the default profile and you don't\n        # have that in your config file, an exception is raised.\n        config_path = os.path.join(\n            os.path.dirname(__file__), 'cfg', 'boto_config_empty'\n        )\n        self.environ['FOO_CONFIG_FILE'] = config_path\n        self.environ['FOO_PROFILE'] = 'default'\n        # In this case, even though we specified default, because\n        # the boto_config_empty config file does not have a default\n        # profile, we should be raising an exception.\n        with self.assertRaises(botocore.exceptions.ProfileNotFound):\n            self.session.get_scoped_config()\n\n    def test_file_logger(self):\n        tempdir = tempfile.mkdtemp()\n        temp_file = os.path.join(tempdir, 'file_logger')\n        self.session.set_file_logger(logging.DEBUG, temp_file)\n        self.addCleanup(self.close_log_file_handler, tempdir, temp_file)\n        self.session.get_credentials()\n        self.assertTrue(os.path.isfile(temp_file))\n        with open(temp_file) as logfile:\n            s = logfile.read()\n        self.assertTrue('Looking for credentials' in s)\n\n    def test_full_config_property(self):\n        full_config = self.session.full_config\n        self.assertTrue('foo' in full_config['profiles'])\n        self.assertTrue('default' in full_config['profiles'])\n\n    def test_full_config_merges_creds_file_data(self):\n        with temporary_file('w') as f:\n            self.session.set_config_variable('credentials_file', f.name)\n            f.write('[newprofile]\\n')\n            f.write('aws_access_key_id=FROM_CREDS_FILE_1\\n')\n            f.write('aws_secret_access_key=FROM_CREDS_FILE_2\\n')\n            f.flush()\n\n            full_config = self.session.full_config\n            self.assertEqual(\n                full_config['profiles']['newprofile'],\n                {\n                    'aws_access_key_id': 'FROM_CREDS_FILE_1',\n                    'aws_secret_access_key': 'FROM_CREDS_FILE_2',\n                },\n            )\n\n    def test_path_not_in_available_profiles(self):\n        with temporary_file('w') as f:\n            self.session.set_config_variable('credentials_file', f.name)\n            f.write('[newprofile]\\n')\n            f.write('aws_access_key_id=FROM_CREDS_FILE_1\\n')\n            f.write('aws_secret_access_key=FROM_CREDS_FILE_2\\n')\n            f.flush()\n\n            profiles = self.session.available_profiles\n            self.assertEqual(set(profiles), {'foo', 'default', 'newprofile'})\n\n    def test_emit_delegates_to_emitter(self):\n        calls = []\n        handler = lambda **kwargs: calls.append(kwargs)\n        self.session.register('foo', handler)\n        self.session.emit('foo')\n        self.assertEqual(len(calls), 1)\n        self.assertEqual(calls[0]['event_name'], 'foo')\n\n    def test_emitter_can_be_passed_in(self):\n        events = HierarchicalEmitter()\n        session = create_session(event_hooks=events)\n        calls = []\n        handler = lambda **kwargs: calls.append(kwargs)\n        events.register('foo', handler)\n\n        session.emit('foo')\n        self.assertEqual(len(calls), 1)\n\n    def test_emit_first_non_none(self):\n        self.session.register('foo', lambda **kwargs: None)\n        self.session.register('foo', lambda **kwargs: 'first')\n        self.session.register('foo', lambda **kwargs: 'second')\n        response = self.session.emit_first_non_none_response('foo')\n        self.assertEqual(response, 'first')\n\n    @mock.patch('logging.getLogger')\n    @mock.patch('logging.FileHandler')\n    def test_logger_name_can_be_passed_in(self, file_handler, get_logger):\n        self.session.set_debug_logger('botocore.hooks')\n        get_logger.assert_called_with('botocore.hooks')\n\n        self.session.set_file_logger('DEBUG', 'debuglog', 'botocore.service')\n        get_logger.assert_called_with('botocore.service')\n        file_handler.assert_called_with('debuglog')\n\n    @mock.patch('logging.getLogger')\n    @mock.patch('logging.StreamHandler')\n    @mock.patch('logging.Formatter')\n    def test_general_purpose_logger(self, formatter, file_handler, get_logger):\n        self.session.set_stream_logger('foo.bar', 'ERROR', format_string='foo')\n        get_logger.assert_called_with('foo.bar')\n        get_logger.return_value.setLevel.assert_called_with(logging.DEBUG)\n        formatter.assert_called_with('foo')\n\n    def test_register_with_unique_id(self):\n        calls = []\n        handler = lambda **kwargs: calls.append(kwargs)\n        self.session.register('foo', handler, unique_id='bar')\n        self.session.emit('foo')\n        self.assertEqual(calls[0]['event_name'], 'foo')\n        calls = []\n        self.session.unregister('foo', unique_id='bar')\n        self.session.emit('foo')\n        self.assertEqual(calls, [])\n\n\nclass TestBuiltinEventHandlers(BaseSessionTest):\n    def setUp(self):\n        super().setUp()\n        self.builtin_handlers = [\n            ('foo', self.on_foo),\n        ]\n        self.foo_called = False\n        self.handler_patch = mock.patch(\n            'botocore.handlers.BUILTIN_HANDLERS', self.builtin_handlers\n        )\n        self.handler_patch.start()\n\n    def on_foo(self, **kwargs):\n        self.foo_called = True\n\n    def tearDown(self):\n        super().tearDown()\n        self.handler_patch.stop()\n\n    def test_registered_builtin_handlers(self):\n        session = create_session(include_builtin_handlers=True)\n        session.emit('foo')\n        self.assertTrue(self.foo_called)\n\n\nclass TestSessionConfigurationVars(BaseSessionTest):\n    def test_per_session_config_vars(self):\n        self.update_session_config_mapping(\n            'foobar',\n            instance_name='foobar',\n            env_var_names='FOOBAR',\n            default='default',\n        )\n        # Default value.\n        self.assertEqual(self.session.get_config_variable('foobar'), 'default')\n        # Retrieve from os environment variable.\n        self.environ['FOOBAR'] = 'fromenv'\n        self.assertEqual(self.session.get_config_variable('foobar'), 'fromenv')\n\n        # Explicit override.\n        self.session.set_config_variable('foobar', 'session-instance')\n        self.assertEqual(\n            self.session.get_config_variable('foobar'), 'session-instance'\n        )\n\n        # Back to default value.\n        del self.environ['FOOBAR']\n        self.session.set_config_variable('foobar', None)\n        self.assertEqual(self.session.get_config_variable('foobar'), 'default')\n\n    def test_default_value_can_be_overriden(self):\n        self.update_session_config_mapping(\n            'foobar',\n            instance_name='foobar',\n            env_var_names='FOOBAR',\n            default='default',\n        )\n        self.assertEqual(self.session.get_config_variable('foobar'), 'default')\n\n    def test_can_get_with_methods(self):\n        self.environ['AWS_DEFAULT_REGION'] = 'env-var'\n        self.session.set_config_variable('region', 'instance-var')\n        value = self.session.get_config_variable('region')\n        self.assertEqual(value, 'instance-var')\n\n        value = self.session.get_config_variable('region', methods=('env',))\n        self.assertEqual(value, 'env-var')\n\n\nclass TestSessionPartitionFiles(BaseSessionTest):\n    def test_lists_partitions_on_disk(self):\n        mock_resolver = mock.Mock()\n        mock_resolver.get_available_partitions.return_value = ['foo']\n        self.session._register_internal_component(\n            'endpoint_resolver', mock_resolver\n        )\n        self.assertEqual(['foo'], self.session.get_available_partitions())\n\n    def test_proxies_list_endpoints_to_resolver(self):\n        resolver = mock.Mock()\n        resolver.get_available_endpoints.return_value = ['a', 'b']\n        self.session._register_internal_component(\n            'endpoint_resolver', resolver\n        )\n        self.session.get_available_regions('foo', 'bar', True)\n\n    def test_provides_empty_list_for_unknown_service_regions(self):\n        regions = self.session.get_available_regions('__foo__')\n        self.assertEqual([], regions)\n\n    def test_provides_correct_partition_for_region(self):\n        partition = self.session.get_partition_for_region('us-west-2')\n        self.assertEqual(partition, 'aws')\n\n    def test_provides_correct_partition_for_region_regex(self):\n        partition = self.session.get_partition_for_region('af-south-99')\n        self.assertEqual(partition, 'aws')\n\n    def test_provides_correct_partition_for_region_non_default(self):\n        partition = self.session.get_partition_for_region('cn-north-1')\n        self.assertEqual(partition, 'aws-cn')\n\n    def test_raises_exception_for_invalid_region(self):\n        with self.assertRaises(botocore.exceptions.UnknownRegionError):\n            self.session.get_partition_for_region('no-good-1')\n\n\nclass TestSessionUserAgent(BaseSessionTest):\n    def test_can_change_user_agent_name(self):\n        self.session.user_agent_name = 'something-else'\n        self.assertTrue(self.session.user_agent().startswith('something-else'))\n\n    def test_can_change_user_agent_version(self):\n        self.session.user_agent_version = '24.0'\n        self.assertTrue(self.session.user_agent().startswith('Botocore/24.0'))\n\n    def test_can_append_to_user_agent(self):\n        self.session.user_agent_extra = 'custom-thing/other'\n        self.assertTrue(\n            self.session.user_agent().endswith('custom-thing/other')\n        )\n\n    def test_execution_env_not_set(self):\n        self.assertFalse(self.session.user_agent().endswith('FooEnv'))\n\n    def test_execution_env_set(self):\n        self.environ['AWS_EXECUTION_ENV'] = 'FooEnv'\n        self.assertTrue(self.session.user_agent().endswith(' exec-env/FooEnv'))\n\n    def test_agent_extra_and_exec_env(self):\n        self.session.user_agent_extra = 'custom-thing/other'\n        self.environ['AWS_EXECUTION_ENV'] = 'FooEnv'\n        user_agent = self.session.user_agent()\n        self.assertTrue(user_agent.endswith('custom-thing/other'))\n        self.assertIn('exec-env/FooEnv', user_agent)\n\n    @requires_crt()\n    def test_crt_user_agent_appended(self):\n        user_agent = self.session.user_agent()\n        self.assertIn(' awscrt/', user_agent)\n        self.assertNotIn('awscrt/Unknown', user_agent)\n\n    @requires_crt()\n    def test_crt_and_extra_user_agent(self):\n        user_agent = self.session.user_agent()\n        self.assertIn(' awscrt/', user_agent)\n        self.assertNotIn('custom-thing/other', user_agent)\n        self.session.user_agent_extra = 'custom-thing/other'\n        user_agent_w_extra = self.session.user_agent()\n        self.assertIn(' awscrt/', user_agent)\n        self.assertTrue(user_agent_w_extra.endswith('custom-thing/other'))\n\n\nclass TestConfigLoaderObject(BaseSessionTest):\n    def test_config_loader_delegation(self):\n        session = create_session(profile='credfile-profile')\n        with temporary_file('w') as f:\n            f.write('[credfile-profile]\\naws_access_key_id=a\\n')\n            f.write('aws_secret_access_key=b\\n')\n            f.flush()\n            session.set_config_variable('credentials_file', f.name)\n            # Now trying to retrieve the scoped config should pull in\n            # values from the shared credentials file.\n            self.assertEqual(\n                session.get_scoped_config(),\n                {'aws_access_key_id': 'a', 'aws_secret_access_key': 'b'},\n            )\n\n\nclass TestGetServiceModel(BaseSessionTest):\n    def test_get_service_model(self):\n        loader = mock.Mock()\n        loader.load_service_model.return_value = {\n            'metadata': {'serviceId': 'foo'}\n        }\n        self.session.register_component('data_loader', loader)\n        model = self.session.get_service_model('made_up')\n        self.assertIsInstance(model, ServiceModel)\n        self.assertEqual(model.service_name, 'made_up')\n\n\nclass TestGetPaginatorModel(BaseSessionTest):\n    def test_get_paginator_model(self):\n        loader = mock.Mock()\n        loader.load_service_model.return_value = {\"pagination\": {}}\n        self.session.register_component('data_loader', loader)\n\n        model = self.session.get_paginator_model('foo')\n\n        # Verify we get a PaginatorModel back\n        self.assertIsInstance(model, PaginatorModel)\n        # Verify we called the loader correctly.\n        loader.load_service_model.assert_called_with(\n            'foo', 'paginators-1', None\n        )\n\n\nclass TestGetWaiterModel(BaseSessionTest):\n    def test_get_waiter_model(self):\n        loader = mock.Mock()\n        loader.load_service_model.return_value = {\"version\": 2, \"waiters\": {}}\n        self.session.register_component('data_loader', loader)\n\n        model = self.session.get_waiter_model('foo')\n\n        # Verify we (1) get the expected return data,\n        self.assertIsInstance(model, WaiterModel)\n        self.assertEqual(model.waiter_names, [])\n        # and (2) call the loader correctly.\n        loader.load_service_model.assert_called_with('foo', 'waiters-2', None)\n\n\nclass TestCreateClient(BaseSessionTest):\n    def test_can_create_client(self):\n        sts_client = self.session.create_client('sts', 'us-west-2')\n        self.assertIsInstance(sts_client, client.BaseClient)\n\n    def test_credential_provider_not_called_when_creds_provided(self):\n        cred_provider = mock.Mock()\n        self.session.register_component('credential_provider', cred_provider)\n        self.session.create_client(\n            'sts',\n            'us-west-2',\n            aws_access_key_id='foo',\n            aws_secret_access_key='bar',\n            aws_session_token='baz',\n        )\n        self.assertFalse(\n            cred_provider.load_credentials.called,\n            \"Credential provider was called even though \"\n            \"explicit credentials were provided to the \"\n            \"create_client call.\",\n        )\n\n    def test_cred_provider_called_when_partial_creds_provided(self):\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            self.session.create_client(\n                'sts',\n                'us-west-2',\n                aws_access_key_id='foo',\n                aws_secret_access_key=None,\n            )\n        with self.assertRaises(botocore.exceptions.PartialCredentialsError):\n            self.session.create_client(\n                'sts',\n                'us-west-2',\n                aws_access_key_id=None,\n                aws_secret_access_key='foo',\n            )\n\n    def test_cred_provider_not_called_on_unsigned_client(self):\n        cred_provider = mock.Mock()\n        self.session.register_component('credential_provider', cred_provider)\n        config = botocore.config.Config(signature_version=UNSIGNED)\n        self.session.create_client('sts', 'us-west-2', config=config)\n        self.assertFalse(cred_provider.load_credentials.called)\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_config_passed_to_client_creator(self, client_creator):\n        # Make sure there is no default set\n        self.assertEqual(self.session.get_default_client_config(), None)\n\n        # The config passed to the client should be the one that is used\n        # in creating the client.\n        config = botocore.config.Config(region_name='us-west-2')\n        self.session.create_client('sts', config=config)\n        client_creator.return_value.create_client.assert_called_with(\n            service_name=mock.ANY,\n            region_name=mock.ANY,\n            is_secure=mock.ANY,\n            endpoint_url=mock.ANY,\n            verify=mock.ANY,\n            credentials=mock.ANY,\n            scoped_config=mock.ANY,\n            client_config=config,\n            api_version=mock.ANY,\n            auth_token=mock.ANY,\n        )\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_default_client_config(self, client_creator):\n        config = botocore.config.Config()\n        self.session.set_default_client_config(config)\n        self.session.create_client('sts')\n\n        client_creator.return_value.create_client.assert_called_with(\n            service_name=mock.ANY,\n            region_name=mock.ANY,\n            is_secure=mock.ANY,\n            endpoint_url=mock.ANY,\n            verify=mock.ANY,\n            credentials=mock.ANY,\n            scoped_config=mock.ANY,\n            client_config=config,\n            api_version=mock.ANY,\n            auth_token=mock.ANY,\n        )\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_merging_client_configs(self, client_creator):\n        config = botocore.config.Config(region_name='us-west-2')\n        other_config = botocore.config.Config(region_name='us-east-1')\n        self.session.set_default_client_config(config)\n        self.session.create_client('sts', config=other_config)\n\n        # Grab the client config used in creating the client\n        used_client_config = (\n            client_creator.return_value.create_client.call_args[1][\n                'client_config'\n            ]\n        )\n        # Check that the client configs were merged\n        self.assertEqual(used_client_config.region_name, 'us-east-1')\n        # Make sure that the client config used is not the default client\n        # config or the one passed in. It should be a new config.\n        self.assertIsNot(used_client_config, config)\n        self.assertIsNot(used_client_config, other_config)\n\n    def test_create_client_with_region(self):\n        ec2_client = self.session.create_client('ec2', 'us-west-2')\n        self.assertEqual(ec2_client.meta.region_name, 'us-west-2')\n\n    def test_create_client_with_region_and_client_config(self):\n        config = botocore.config.Config()\n        # Use a client config with no region configured.\n        ec2_client = self.session.create_client(\n            'ec2', region_name='us-west-2', config=config\n        )\n        self.assertEqual(ec2_client.meta.region_name, 'us-west-2')\n\n        # If the region name is changed, it should not change the\n        # region of the client\n        config.region_name = 'us-east-1'\n        self.assertEqual(ec2_client.meta.region_name, 'us-west-2')\n\n        # Now make a new client with the updated client config.\n        ec2_client = self.session.create_client('ec2', config=config)\n        self.assertEqual(ec2_client.meta.region_name, 'us-east-1')\n\n    def test_create_client_no_region_and_no_client_config(self):\n        ec2_client = self.session.create_client('ec2')\n        self.assertEqual(ec2_client.meta.region_name, 'us-west-11')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_ca_bundle_from_config(self, client_creator):\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            f.write('[default]\\n')\n            f.write('foo_ca_bundle=config-certs.pem\\n')\n            f.flush()\n\n            self.session.create_client('ec2', 'us-west-2')\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(call_kwargs['verify'], 'config-certs.pem')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_ca_bundle_from_env_var(self, client_creator):\n        self.environ['FOO_AWS_CA_BUNDLE'] = 'env-certs.pem'\n        self.session.create_client('ec2', 'us-west-2')\n        call_kwargs = client_creator.return_value.create_client.call_args[1]\n        self.assertEqual(call_kwargs['verify'], 'env-certs.pem')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_with_verify_param(self, client_creator):\n        self.session.create_client(\n            'ec2', 'us-west-2', verify='verify-certs.pem'\n        )\n        call_kwargs = client_creator.return_value.create_client.call_args[1]\n        self.assertEqual(call_kwargs['verify'], 'verify-certs.pem')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_verify_param_overrides_all(self, client_creator):\n        with temporary_file('w') as f:\n            # Set the ca cert using the config file\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            f.write('[default]\\n')\n            f.write('foo_ca_bundle=config-certs.pem\\n')\n            f.flush()\n\n            # Set the ca cert with an environment variable\n            self.environ['FOO_AWS_CA_BUNDLE'] = 'env-certs.pem'\n\n            # Set the ca cert using the verify parameter\n            self.session.create_client(\n                'ec2', 'us-west-2', verify='verify-certs.pem'\n            )\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            # The verify parameter should override all the other\n            # configurations\n            self.assertEqual(call_kwargs['verify'], 'verify-certs.pem')\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_use_no_api_version_by_default(self, client_creator):\n        self.session.create_client('myservice', 'us-west-2')\n        call_kwargs = client_creator.return_value.create_client.call_args[1]\n        self.assertEqual(call_kwargs['api_version'], None)\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_create_client_uses_api_version_from_config(self, client_creator):\n        config_api_version = '2012-01-01'\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            f.write('[default]\\n')\n            f.write(\n                'foo_api_versions =\\n'\n                '    myservice = %s\\n' % config_api_version\n            )\n            f.flush()\n\n            self.session.create_client('myservice', 'us-west-2')\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(call_kwargs['api_version'], config_api_version)\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_can_specify_multiple_versions_from_config(self, client_creator):\n        config_api_version = '2012-01-01'\n        second_config_api_version = '2013-01-01'\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            f.write('[default]\\n')\n            f.write(\n                f'foo_api_versions =\\n'\n                f'    myservice = {config_api_version}\\n'\n                f'    myservice2 = {second_config_api_version}\\n'\n            )\n            f.flush()\n\n            self.session.create_client('myservice', 'us-west-2')\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(call_kwargs['api_version'], config_api_version)\n\n            self.session.create_client('myservice2', 'us-west-2')\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(\n                call_kwargs['api_version'], second_config_api_version\n            )\n\n    @mock.patch('botocore.client.ClientCreator')\n    def test_param_api_version_overrides_config_value(self, client_creator):\n        config_api_version = '2012-01-01'\n        override_api_version = '2014-01-01'\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            f.write('[default]\\n')\n            f.write(\n                'foo_api_versions =\\n'\n                '    myservice = %s\\n' % config_api_version\n            )\n            f.flush()\n\n            self.session.create_client(\n                'myservice', 'us-west-2', api_version=override_api_version\n            )\n            call_kwargs = client_creator.return_value.create_client.call_args[\n                1\n            ]\n            self.assertEqual(call_kwargs['api_version'], override_api_version)\n\n\nclass TestSessionComponent(BaseSessionTest):\n    def test_internal_component(self):\n        component = object()\n        self.session._register_internal_component('internal', component)\n        self.assertIs(\n            self.session._get_internal_component('internal'), component\n        )\n        with self.assertRaises(ValueError):\n            self.session.get_component('internal')\n\n    def test_internal_endpoint_resolver_is_same_as_deprecated_public(self):\n        endpoint_resolver = self.session._get_internal_component(\n            'endpoint_resolver'\n        )\n        # get_component has been deprecated to the public\n        with pytest.warns(DeprecationWarning):\n            self.assertIs(\n                self.session.get_component('endpoint_resolver'),\n                endpoint_resolver,\n            )\n\n    def test_internal_exceptions_factory_is_same_as_deprecated_public(self):\n        exceptions_factory = self.session._get_internal_component(\n            'exceptions_factory'\n        )\n        # get_component has been deprecated to the public\n        with pytest.warns(DeprecationWarning):\n            self.assertIs(\n                self.session.get_component('exceptions_factory'),\n                exceptions_factory,\n            )\n\n\nclass TestClientMonitoring(BaseSessionTest):\n    def assert_created_client_is_monitored(self, session):\n        with mock.patch(\n            'botocore.monitoring.Monitor', spec=True\n        ) as mock_monitor:\n            client = session.create_client('ec2', 'us-west-2')\n        mock_monitor.return_value.register.assert_called_with(\n            client.meta.events\n        )\n\n    def assert_monitoring_host_and_port(self, session, host, port):\n        with mock.patch(\n            'botocore.monitoring.SocketPublisher', spec=True\n        ) as mock_publisher:\n            session.create_client('ec2', 'us-west-2')\n        self.assertEqual(mock_publisher.call_count, 1)\n        _, args, kwargs = mock_publisher.mock_calls[0]\n        self.assertEqual(kwargs.get('host'), host)\n        self.assertEqual(kwargs.get('port'), port)\n\n    def assert_created_client_is_not_monitored(self, session):\n        with mock.patch(\n            'botocore.session.monitoring.Monitor', spec=True\n        ) as mock_monitor:\n            session.create_client('ec2', 'us-west-2')\n            mock_monitor.return_value.register.assert_not_called()\n\n    def test_with_csm_enabled_from_config(self):\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            f.write('[default]\\n')\n            f.write('csm_enabled=true\\n')\n            f.flush()\n            self.assert_created_client_is_monitored(self.session)\n\n    def test_with_csm_enabled_from_env(self):\n        self.environ['AWS_CSM_ENABLED'] = 'true'\n        self.assert_created_client_is_monitored(self.session)\n\n    def test_with_csm_host(self):\n        custom_host = '10.13.37.1'\n        self.environ['AWS_CSM_ENABLED'] = 'true'\n        self.environ['AWS_CSM_HOST'] = custom_host\n        self.assert_monitoring_host_and_port(self.session, custom_host, 31000)\n\n    def test_with_csm_port(self):\n        custom_port = '1234'\n        self.environ['AWS_CSM_ENABLED'] = 'true'\n        self.environ['AWS_CSM_PORT'] = custom_port\n        self.assert_monitoring_host_and_port(\n            self.session,\n            '127.0.0.1',\n            int(custom_port),\n        )\n\n    def test_with_csm_disabled_from_config(self):\n        with temporary_file('w') as f:\n            del self.environ['FOO_PROFILE']\n            self.environ['FOO_CONFIG_FILE'] = f.name\n            f.write('[default]\\n')\n            f.write('csm_enabled=false\\n')\n            f.flush()\n            self.assert_created_client_is_not_monitored(self.session)\n\n    def test_with_csm_disabled_from_env(self):\n        self.environ['AWS_CSM_ENABLED'] = 'false'\n        self.assert_created_client_is_not_monitored(self.session)\n\n    def test_csm_not_configured(self):\n        self.assert_created_client_is_not_monitored(self.session)\n\n\nclass TestComponentLocator(unittest.TestCase):\n    def setUp(self):\n        self.components = botocore.session.ComponentLocator()\n\n    def test_unknown_component_raises_exception(self):\n        with self.assertRaises(ValueError):\n            self.components.get_component('unknown-component')\n\n    def test_can_register_and_retrieve_component(self):\n        component = object()\n        self.components.register_component('foo', component)\n        self.assertIs(self.components.get_component('foo'), component)\n\n    def test_last_registration_wins(self):\n        first = object()\n        second = object()\n        self.components.register_component('foo', first)\n        self.components.register_component('foo', second)\n        self.assertIs(self.components.get_component('foo'), second)\n\n    def test_can_lazy_register_a_component(self):\n        component = object()\n        lazy = lambda: component\n        self.components.lazy_register_component('foo', lazy)\n        self.assertIs(self.components.get_component('foo'), component)\n\n    def test_latest_registration_wins_even_if_lazy(self):\n        first = object()\n        second = object()\n        lazy_second = lambda: second\n        self.components.register_component('foo', first)\n        self.components.lazy_register_component('foo', lazy_second)\n        self.assertIs(self.components.get_component('foo'), second)\n\n    def test_latest_registration_overrides_lazy(self):\n        first = object()\n        second = object()\n        lazy_first = lambda: first\n        self.components.lazy_register_component('foo', lazy_first)\n        self.components.register_component('foo', second)\n        self.assertIs(self.components.get_component('foo'), second)\n\n    def test_lazy_registration_factory_does_not_remove_from_list_on_error(\n        self,\n    ):\n        class ArbitraryError(Exception):\n            pass\n\n        def bad_factory():\n            raise ArbitraryError(\"Factory raises an exception.\")\n\n        self.components.lazy_register_component('foo', bad_factory)\n\n        with self.assertRaises(ArbitraryError):\n            self.components.get_component('foo')\n\n        # Trying again should raise the same exception,\n        # not an ValueError(\"Unknown component\")\n        with self.assertRaises(ArbitraryError):\n            self.components.get_component('foo')\n\n\nclass TestDefaultClientConfig(BaseSessionTest):\n    def test_new_session_has_no_default_client_config(self):\n        self.assertEqual(self.session.get_default_client_config(), None)\n\n    def test_set_and_get_client_config(self):\n        client_config = botocore.config.Config()\n        self.session.set_default_client_config(client_config)\n        self.assertIs(self.session.get_default_client_config(), client_config)\n\n\nclass TestSessionRegionSetup(BaseSessionTest):\n    def test_new_session_with_valid_region(self):\n        s3_client = self.session.create_client('s3', 'us-west-2')\n        self.assertIsInstance(s3_client, client.BaseClient)\n        self.assertEqual(s3_client.meta.region_name, 'us-west-2')\n\n    def test_new_session_with_unknown_region(self):\n        s3_client = self.session.create_client('s3', 'MyCustomRegion1')\n        self.assertIsInstance(s3_client, client.BaseClient)\n        self.assertEqual(s3_client.meta.region_name, 'MyCustomRegion1')\n\n    def test_new_session_with_invalid_region(self):\n        with self.assertRaises(botocore.exceptions.InvalidRegionError):\n            self.session.create_client('s3', 'not.a.real#region')\n\n    def test_new_session_with_none_region(self):\n        s3_client = self.session.create_client('s3', region_name=None)\n        self.assertIsInstance(s3_client, client.BaseClient)\n        self.assertTrue(s3_client.meta.region_name is not None)\n\n\nclass TestInitializationHooks(BaseSessionTest):\n    def test_can_register_init_hook(self):\n        call_args = []\n\n        def init_hook(session):\n            call_args.append(session)\n\n        register_initializer(init_hook)\n        self.addCleanup(unregister_initializer, init_hook)\n        session = create_session()\n        self.assertEqual(call_args, [session])\n\n    def test_can_unregister_hook(self):\n        call_args = []\n\n        def init_hook(session):\n            call_args.append(session)\n\n        register_initializer(init_hook)\n        unregister_initializer(init_hook)\n        create_session()\n        self.assertEqual(call_args, [])\n\n    def test_unregister_hook_raises_value_error(self):\n        not_registered = lambda session: None\n        with self.assertRaises(ValueError):\n            self.assertRaises(unregister_initializer(not_registered))\n", "tests/unit/test_auth_bearer.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport pytest\n\nfrom botocore.auth import BearerAuth\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.exceptions import NoAuthTokenError\nfrom botocore.tokens import FrozenAuthToken\n\ncases = [\n    {\n        \"documentation\": \"Minimal bearer auth case\",\n        \"headers\": {},\n        \"token\": \"mF_9.B5f-4.1JqM\",\n        \"expectedHeaders\": {\"Authorization\": \"Bearer mF_9.B5f-4.1JqM\"},\n    },\n    {\n        \"documentation\": \"Longer token case\",\n        \"headers\": {},\n        \"token\": \"eW91J3JlIG5vdCBzdXBwb3NlZCB0byBkZWNvZGUgdGhpcyE=\",\n        \"expectedHeaders\": {\n            \"Authorization\": \"Bearer eW91J3JlIG5vdCBzdXBwb3NlZCB0byBkZWNvZGUgdGhpcyE=\"\n        },\n    },\n    {\n        \"documentation\": \"Signer should override existing header\",\n        \"headers\": {\"Authorization\": \"Bearer foo\"},\n        \"token\": \"mF_9.B5f-4.1JqM\",\n        \"expectedHeaders\": {\"Authorization\": \"Bearer mF_9.B5f-4.1JqM\"},\n    },\n    {\n        \"documentation\": \"Signer requires a token\",\n        \"headers\": {},\n        \"token\": None,\n        \"expectedException\": NoAuthTokenError,\n    },\n]\n\n\n@pytest.mark.parametrize(\"test_case\", cases)\ndef test_bearer_auth(test_case):\n    url = \"https://example.com\"\n    headers = test_case.get(\"headers\", {})\n    request = AWSRequest(method=\"GET\", url=url, headers=headers)\n\n    auth_token = None\n    raw_token = test_case[\"token\"]\n    if raw_token:\n        auth_token = FrozenAuthToken(test_case[\"token\"], expiration=None)\n\n    bearer_auth = BearerAuth(auth_token)\n    expected_headers = test_case.get(\"expectedHeaders\")\n    expected_exception = test_case.get(\"expectedException\")\n    if expected_headers:\n        bearer_auth.add_auth(request)\n        for name in expected_headers:\n            actual_header = request.headers[name]\n            expected_header = expected_headers[name]\n            assert actual_header == expected_header\n    elif expected_exception:\n        with pytest.raises(expected_exception):\n            bearer_auth.add_auth(request)\n", "tests/unit/test_idempotency.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport re\n\nfrom botocore.handlers import generate_idempotent_uuid\nfrom tests import mock, unittest\n\n\nclass TestIdempotencyInjection(unittest.TestCase):\n    def setUp(self):\n        self.mock_model = mock.MagicMock()\n        self.mock_model.idempotent_members = ['RequiredKey']\n        self.uuid_pattern = re.compile(\n            '^[0-9a-f]{8}-([0-9a-f]{4}-){3}[0-9a-f]{12}$', re.I\n        )\n\n    def test_injection(self):\n        # No parameters are provided, RequiredKey should be autofilled\n        params = {}\n        generate_idempotent_uuid(params, self.mock_model)\n        self.assertIn('RequiredKey', params)\n        self.assertIsNotNone(self.uuid_pattern.match(params['RequiredKey']))\n\n    def test_provided(self):\n        # RequiredKey is provided, should not be replaced\n        params = {'RequiredKey': 'already populated'}\n        generate_idempotent_uuid(params, self.mock_model)\n        self.assertEqual(params['RequiredKey'], 'already populated')\n", "tests/unit/test_retryhandler.py": "#!/usr/bin/env\n# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore import retryhandler\nfrom botocore.exceptions import (\n    ChecksumError,\n    EndpointConnectionError,\n    ReadTimeoutError,\n)\nfrom tests import mock, unittest\n\nHTTP_500_RESPONSE = mock.Mock()\nHTTP_500_RESPONSE.status_code = 500\n\nHTTP_400_RESPONSE = mock.Mock()\nHTTP_400_RESPONSE.status_code = 400\n\nHTTP_200_RESPONSE = mock.Mock()\nHTTP_200_RESPONSE.status_code = 200\n\nREQUEST_DICT = {'context': {}}\n\n\nclass TestRetryCheckers(unittest.TestCase):\n    def construct_checker_kwargs(\n        self, response, attempt_number, caught_exception\n    ):\n        checker_kwargs = {\n            'attempt_number': attempt_number,\n            'response': response,\n            'caught_exception': caught_exception,\n        }\n        if isinstance(self.checker, retryhandler.MaxAttemptsDecorator):\n            checker_kwargs.update({'retries_context': REQUEST_DICT['context']})\n\n        return checker_kwargs\n\n    def assert_should_be_retried(\n        self, response, attempt_number=1, caught_exception=None\n    ):\n        checker_kwargs = self.construct_checker_kwargs(\n            response, attempt_number, caught_exception\n        )\n\n        self.assertTrue(self.checker(**checker_kwargs))\n\n    def assert_should_not_be_retried(\n        self, response, attempt_number=1, caught_exception=None\n    ):\n        checker_kwargs = self.construct_checker_kwargs(\n            response, attempt_number, caught_exception\n        )\n\n        self.assertFalse(self.checker(**checker_kwargs))\n\n    def test_status_code_checker(self):\n        self.checker = retryhandler.HTTPStatusCodeChecker(500)\n        self.assert_should_be_retried(response=(HTTP_500_RESPONSE, {}))\n\n    def test_max_attempts(self):\n        self.checker = retryhandler.MaxAttemptsDecorator(\n            retryhandler.HTTPStatusCodeChecker(500), max_attempts=3\n        )\n        response = {'ResponseMetadata': {}}\n\n        # Retry up to three times.\n        self.assert_should_be_retried(\n            (HTTP_500_RESPONSE, response), attempt_number=1\n        )\n        self.assert_should_be_retried(\n            (HTTP_500_RESPONSE, {}), attempt_number=2\n        )\n        # On the third failed response, we've reached the\n        # max attempts so we should return False.\n        self.assert_should_not_be_retried(\n            (HTTP_500_RESPONSE, response), attempt_number=3\n        )\n        self.assertTrue(response['ResponseMetadata']['MaxAttemptsReached'])\n\n    def test_max_attempts_successful(self):\n        self.checker = retryhandler.MaxAttemptsDecorator(\n            retryhandler.HTTPStatusCodeChecker(500), max_attempts=3\n        )\n\n        self.assert_should_be_retried(\n            (HTTP_500_RESPONSE, {}), attempt_number=1\n        )\n        # The second retry is successful.\n        self.assert_should_not_be_retried(\n            (HTTP_200_RESPONSE, {}), attempt_number=2\n        )\n\n        # But now we can reuse this object.\n        self.assert_should_be_retried(\n            (HTTP_500_RESPONSE, {}), attempt_number=1\n        )\n        self.assert_should_be_retried(\n            (HTTP_500_RESPONSE, {}), attempt_number=2\n        )\n        self.assert_should_not_be_retried(\n            (HTTP_500_RESPONSE, {}), attempt_number=3\n        )\n\n    def test_error_code_checker(self):\n        self.checker = retryhandler.ServiceErrorCodeChecker(\n            status_code=400, error_code='Throttled'\n        )\n        response = (HTTP_400_RESPONSE, {'Error': {'Code': 'Throttled'}})\n        self.assert_should_be_retried(response)\n\n    def test_error_code_checker_does_not_match(self):\n        self.checker = retryhandler.ServiceErrorCodeChecker(\n            status_code=400, error_code='Throttled'\n        )\n        response = (HTTP_400_RESPONSE, {'Error': {'Code': 'NotThrottled'}})\n        self.assert_should_not_be_retried(response)\n\n    def test_error_code_checker_ignore_caught_exception(self):\n        self.checker = retryhandler.ServiceErrorCodeChecker(\n            status_code=400, error_code='Throttled'\n        )\n        self.assert_should_not_be_retried(\n            response=None, caught_exception=RuntimeError()\n        )\n\n    def test_multi_checker(self):\n        checker = retryhandler.ServiceErrorCodeChecker(\n            status_code=400, error_code='Throttled'\n        )\n        checker2 = retryhandler.HTTPStatusCodeChecker(500)\n        self.checker = retryhandler.MultiChecker([checker, checker2])\n        self.assert_should_be_retried((HTTP_500_RESPONSE, {}))\n        self.assert_should_be_retried(\n            response=(HTTP_400_RESPONSE, {'Error': {'Code': 'Throttled'}})\n        )\n        self.assert_should_not_be_retried(response=(HTTP_200_RESPONSE, {}))\n\n    def test_exception_checker_ignores_response(self):\n        self.checker = retryhandler.ExceptionRaiser()\n        self.assert_should_not_be_retried(\n            response=(HTTP_200_RESPONSE, {}), caught_exception=None\n        )\n\n    def test_value_error_raised_when_missing_response_and_exception(self):\n        self.checker = retryhandler.ExceptionRaiser()\n        with self.assertRaises(ValueError):\n            self.checker(1, response=None, caught_exception=None)\n\n\nclass TestCreateRetryConfiguration(unittest.TestCase):\n    def setUp(self):\n        self.retry_config = {\n            '__default__': {\n                'max_attempts': 5,\n                'delay': {\n                    'type': 'exponential',\n                    'base': 1,\n                    'growth_factor': 2,\n                },\n                'policies': {\n                    'throttling': {\n                        'applies_when': {\n                            'response': {\n                                'service_error_code': 'Throttling',\n                                'http_status_code': 400,\n                            }\n                        }\n                    }\n                },\n            },\n            'OperationFoo': {\n                'policies': {\n                    'crc32check': {\n                        'applies_when': {\n                            'response': {\n                                'crc32body': 'x-amz-crc32',\n                            }\n                        }\n                    }\n                }\n            },\n            'OperationBar': {\n                'policies': {\n                    'socket_errors': {\n                        'applies_when': {\n                            'socket_errors': [\"GENERAL_CONNECTION_ERROR\"],\n                        }\n                    }\n                }\n            },\n        }\n\n    def test_create_retry_single_checker_service_level(self):\n        checker = retryhandler.create_checker_from_retry_config(\n            self.retry_config, operation_name=None\n        )\n        self.assertIsInstance(checker, retryhandler.MaxAttemptsDecorator)\n        # We're reaching into internal fields here, but only to check\n        # that the object is created properly.\n        self.assertEqual(checker._max_attempts, 5)\n        self.assertIsInstance(\n            checker._checker, retryhandler.ServiceErrorCodeChecker\n        )\n        self.assertEqual(checker._checker._error_code, 'Throttling')\n        self.assertEqual(checker._checker._status_code, 400)\n\n    def test_create_retry_for_operation(self):\n        checker = retryhandler.create_checker_from_retry_config(\n            self.retry_config, operation_name='OperationFoo'\n        )\n        self.assertIsInstance(checker, retryhandler.MaxAttemptsDecorator)\n        self.assertEqual(checker._max_attempts, 5)\n        self.assertIsInstance(checker._checker, retryhandler.MultiChecker)\n\n    def test_retry_with_socket_errors(self):\n        checker = retryhandler.create_checker_from_retry_config(\n            self.retry_config, operation_name='OperationBar'\n        )\n        self.assertIsInstance(checker, retryhandler.BaseChecker)\n        all_checkers = checker._checker._checkers\n        self.assertIsInstance(\n            all_checkers[0], retryhandler.ServiceErrorCodeChecker\n        )\n        self.assertIsInstance(all_checkers[1], retryhandler.ExceptionRaiser)\n\n    def test_create_retry_handler_with_socket_errors(self):\n        handler = retryhandler.create_retry_handler(\n            self.retry_config, operation_name='OperationBar'\n        )\n        exception = EndpointConnectionError(endpoint_url='')\n        with self.assertRaises(EndpointConnectionError):\n            handler(\n                response=None,\n                attempts=10,\n                caught_exception=exception,\n                request_dict=REQUEST_DICT,\n            )\n        # No connection error raised because attempts < max_attempts.\n        sleep_time = handler(\n            response=None,\n            attempts=1,\n            caught_exception=exception,\n            request_dict=REQUEST_DICT,\n        )\n        self.assertEqual(sleep_time, 1)\n        # But any other exception should be raised even if\n        # attempts < max_attempts.\n        with self.assertRaises(ValueError):\n            sleep_time = handler(\n                response=None,\n                attempts=1,\n                caught_exception=ValueError(),\n                request_dict=REQUEST_DICT,\n            )\n\n    def test_connection_timeouts_are_retried(self):\n        # If a connection times out, we get a Timout exception\n        # from requests.  We should be retrying those.\n        handler = retryhandler.create_retry_handler(\n            self.retry_config, operation_name='OperationBar'\n        )\n        sleep_time = handler(\n            response=None,\n            attempts=1,\n            caught_exception=ReadTimeoutError(endpoint_url=''),\n            request_dict=REQUEST_DICT,\n        )\n        self.assertEqual(sleep_time, 1)\n\n    def test_create_retry_handler_with_no_operation(self):\n        handler = retryhandler.create_retry_handler(\n            self.retry_config, operation_name=None\n        )\n        self.assertIsInstance(handler, retryhandler.RetryHandler)\n        # No good way to test for the delay function as the action\n        # other than to just invoke it.\n        self.assertEqual(handler._action(attempts=2), 2)\n        self.assertEqual(handler._action(attempts=3), 4)\n\n    def test_crc32_check_propogates_error(self):\n        handler = retryhandler.create_retry_handler(\n            self.retry_config, operation_name='OperationFoo'\n        )\n        http_response = mock.Mock()\n        http_response.status_code = 200\n        # This is not the crc32 of b'foo', so this should\n        # fail the crc32 check.\n        http_response.headers = {'x-amz-crc32': 2356372768}\n        http_response.content = b'foo'\n        # The first 10 attempts we get a retry.\n        self.assertEqual(\n            handler(\n                response=(http_response, {}),\n                attempts=1,\n                caught_exception=None,\n                request_dict=REQUEST_DICT,\n            ),\n            1,\n        )\n        with self.assertRaises(ChecksumError):\n            handler(\n                response=(http_response, {}),\n                attempts=10,\n                caught_exception=None,\n                request_dict=REQUEST_DICT,\n            )\n\n\nclass TestRetryHandler(unittest.TestCase):\n    def test_action_tied_to_policy(self):\n        # When a retry rule matches we should return the\n        # amount of time to sleep, otherwise we should return None.\n        delay_function = retryhandler.create_exponential_delay_function(1, 2)\n        checker = retryhandler.HTTPStatusCodeChecker(500)\n        handler = retryhandler.RetryHandler(checker, delay_function)\n        response = (HTTP_500_RESPONSE, {})\n\n        self.assertEqual(\n            handler(response=response, attempts=1, caught_exception=None), 1\n        )\n        self.assertEqual(\n            handler(response=response, attempts=2, caught_exception=None), 2\n        )\n        self.assertEqual(\n            handler(response=response, attempts=3, caught_exception=None), 4\n        )\n        self.assertEqual(\n            handler(response=response, attempts=4, caught_exception=None), 8\n        )\n\n    def test_none_response_when_no_matches(self):\n        delay_function = retryhandler.create_exponential_delay_function(1, 2)\n        checker = retryhandler.HTTPStatusCodeChecker(500)\n        handler = retryhandler.RetryHandler(checker, delay_function)\n        response = (HTTP_200_RESPONSE, {})\n\n        self.assertIsNone(\n            handler(response=response, attempts=1, caught_exception=None)\n        )\n\n\nclass TestCRC32Checker(unittest.TestCase):\n    def setUp(self):\n        self.checker = retryhandler.CRC32Checker('x-amz-crc32')\n\n    def test_crc32_matches(self):\n        http_response = mock.Mock()\n        http_response.status_code = 200\n        # This is the crc32 of b'foo', so this should\n        # pass the crc32 check.\n        http_response.headers = {'x-amz-crc32': 2356372769}\n        http_response.content = b'foo'\n        self.assertIsNone(\n            self.checker(\n                response=(http_response, {}),\n                attempt_number=1,\n                caught_exception=None,\n            )\n        )\n\n    def test_crc32_missing(self):\n        # It's not an error is the crc32 header is missing.\n        http_response = mock.Mock()\n        http_response.status_code = 200\n        http_response.headers = {}\n        self.assertIsNone(\n            self.checker(\n                response=(http_response, {}),\n                attempt_number=1,\n                caught_exception=None,\n            )\n        )\n\n    def test_crc32_check_fails(self):\n        http_response = mock.Mock()\n        http_response.status_code = 200\n        # This is not the crc32 of b'foo', so this should\n        # fail the crc32 check.\n        http_response.headers = {'x-amz-crc32': 2356372768}\n        http_response.content = b'foo'\n        with self.assertRaises(ChecksumError):\n            self.checker(\n                response=(http_response, {}),\n                attempt_number=1,\n                caught_exception=None,\n            )\n\n\nclass TestDelayExponential(unittest.TestCase):\n    def test_delay_with_numeric_base(self):\n        self.assertEqual(\n            retryhandler.delay_exponential(\n                base=3, growth_factor=2, attempts=3\n            ),\n            12,\n        )\n\n    def test_delay_with_rand_string(self):\n        delay = retryhandler.delay_exponential(\n            base='rand', growth_factor=2, attempts=3\n        )\n        # 2 ** (3 - 1) == 4, so the retry is between 0, 4.\n        self.assertTrue(0 <= delay <= 4)\n\n    def test_value_error_raised_with_non_positive_number(self):\n        with self.assertRaises(ValueError):\n            retryhandler.delay_exponential(\n                base=-1, growth_factor=2, attempts=3\n            )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/unit/test_http_client_exception_mapping.py": "import pytest\n\nfrom botocore import exceptions as botocore_exceptions\nfrom botocore.vendored.requests import exceptions as requests_exceptions\nfrom botocore.vendored.requests.packages.urllib3 import (\n    exceptions as urllib3_exceptions,\n)\n\n\n@pytest.mark.parametrize(\n    \"new_exception, old_exception\",\n    (\n        (\n            botocore_exceptions.ReadTimeoutError,\n            requests_exceptions.ReadTimeout,\n        ),\n        (\n            botocore_exceptions.ReadTimeoutError,\n            urllib3_exceptions.ReadTimeoutError,\n        ),\n        (\n            botocore_exceptions.ConnectTimeoutError,\n            requests_exceptions.ConnectTimeout,\n        ),\n        (\n            botocore_exceptions.ProxyConnectionError,\n            requests_exceptions.ProxyError,\n        ),\n        (botocore_exceptions.SSLError, requests_exceptions.SSLError),\n    ),\n)\ndef test_http_client_exception_mapping(new_exception, old_exception):\n    # assert that the new exception can still be caught by the old vendored one\n    with pytest.raises(old_exception):\n        raise new_exception(endpoint_url=None, proxy_url=None, error=None)\n", "tests/unit/test_endpoint.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\nimport io\nimport socket\n\nimport pytest\n\nimport botocore.endpoint\nfrom botocore.config import Config\nfrom botocore.endpoint import DEFAULT_TIMEOUT, Endpoint, EndpointCreator\nfrom botocore.exceptions import HTTPClientError\nfrom botocore.httpsession import URLLib3Session\nfrom botocore.model import (\n    OperationModel,\n    ServiceId,\n    ServiceModel,\n    StructureShape,\n)\nfrom tests import mock, unittest\n\n\ndef request_dict(**kwargs):\n    base = {\n        'headers': {},\n        'body': '',\n        'url_path': '/',\n        'query_string': '',\n        'method': 'POST',\n        'url': 'https://example.com',\n        'context': {},\n    }\n    base.update(kwargs)\n    return base\n\n\nclass RecordStreamResets(io.StringIO):\n    def __init__(self, value):\n        io.StringIO.__init__(self, value)\n        self.total_resets = 0\n\n    def seek(self, where, whence=0):\n        self.total_resets += 1\n        io.StringIO.seek(self, where, whence)\n\n\nclass TestEndpointBase(unittest.TestCase):\n    def setUp(self):\n        self.op = mock.Mock()\n        self.op.has_streaming_output = False\n        self.op.has_event_stream_output = False\n        self.op.metadata = {'protocol': 'json'}\n        self.event_emitter = mock.Mock()\n        self.event_emitter.emit.return_value = []\n        self.factory_patch = mock.patch(\n            'botocore.parsers.ResponseParserFactory'\n        )\n        self.factory = self.factory_patch.start()\n        self.endpoint = Endpoint(\n            'https://ec2.us-west-2.amazonaws.com/',\n            endpoint_prefix='ec2',\n            event_emitter=self.event_emitter,\n        )\n        self.http_session = mock.Mock()\n        self.http_session.send.return_value = mock.Mock(\n            status_code=200,\n            headers={},\n            content=b'{\"Foo\": \"bar\"}',\n        )\n        self.endpoint.http_session = self.http_session\n\n    def tearDown(self):\n        self.factory_patch.stop()\n\n    def get_emitter_responses(self, num_retries=0, sleep_time=0):\n        emitter_responses = []\n        response_request_emitter_responses = [\n            [(None, None)],  # emit() response for request-created\n            [(None, None)],  # emit() response for before-send\n            [(None, None)],  # emit() response for response-received\n        ]\n        for _ in range(num_retries):\n            emitter_responses.extend(response_request_emitter_responses)\n            # emit() response for retry for sleep time\n            emitter_responses.append([(None, sleep_time)])\n        emitter_responses.extend(response_request_emitter_responses)\n        # emit() response for no retry\n        emitter_responses.append([(None, None)])\n        return emitter_responses\n\n    def get_events_emitted(self, event_emitter):\n        return [\n            call_arg[0][0] for call_arg in event_emitter.emit.call_args_list\n        ]\n\n\nclass TestEndpointFeatures(TestEndpointBase):\n    def test_make_request_with_no_auth(self):\n        self.endpoint.auth = None\n        self.endpoint.make_request(self.op, request_dict())\n\n        # http_session should be used to send the request.\n        self.assertTrue(self.http_session.send.called)\n        prepared_request = self.http_session.send.call_args[0][0]\n        self.assertNotIn('Authorization', prepared_request.headers)\n\n    def test_make_request_no_signature_version(self):\n        self.endpoint.make_request(self.op, request_dict())\n\n        # http_session should be used to send the request.\n        self.assertTrue(self.http_session.send.called)\n        prepared_request = self.http_session.send.call_args[0][0]\n        self.assertNotIn('Authorization', prepared_request.headers)\n\n    def test_make_request_with_context(self):\n        r = request_dict()\n        r['context'] = {'signing': {'region': 'us-west-2'}}\n        with mock.patch(\n            'botocore.endpoint.Endpoint.prepare_request'\n        ) as prepare:\n            self.endpoint.make_request(self.op, r)\n        request = prepare.call_args[0][0]\n        self.assertEqual(request.context['signing']['region'], 'us-west-2')\n\n    def test_make_request_sets_retries_config_in_context(self):\n        r = request_dict()\n        r['context'] = {'signing': {'region': 'us-west-2'}}\n        with mock.patch(\n            'botocore.endpoint.Endpoint.prepare_request'\n        ) as prepare:\n            self.endpoint.make_request(self.op, r)\n        request = prepare.call_args[0][0]\n        self.assertIn('retries', request.context)\n\n    def test_exception_caught_when_constructing_retries_context(self):\n        r = request_dict()\n        datetime_patcher = mock.patch.object(\n            botocore.endpoint.datetime,\n            'datetime',\n            mock.Mock(wraps=datetime.datetime),\n        )\n        r['context'] = {'signing': {'region': 'us-west-2'}}\n        with mock.patch(\n            'botocore.endpoint.Endpoint.prepare_request'\n        ) as prepare:\n            mocked_datetime = datetime_patcher.start()\n            mocked_datetime.side_effect = Exception()\n            self.endpoint.make_request(self.op, r)\n            datetime_patcher.stop()\n        request = prepare.call_args[0][0]\n        self.assertIn('retries', request.context)\n\n    def test_parses_modeled_exception_fields(self):\n        # Setup the service model to have exceptions to generate the mapping\n        self.service_model = mock.Mock(spec=ServiceModel)\n        self.op.service_model = self.service_model\n        self.exception_shape = mock.Mock(spec=StructureShape)\n        shape_for_error_code = self.service_model.shape_for_error_code\n        shape_for_error_code.return_value = self.exception_shape\n\n        r = request_dict()\n        self.http_session.send.return_value = mock.Mock(\n            status_code=400,\n            headers={},\n            content=b'',\n        )\n        parser = mock.Mock()\n        parser.parse.side_effect = [\n            {\n                'Error': {\n                    'Code': 'ExceptionShape',\n                    'Message': 'Some message',\n                }\n            },\n            {'SomeField': 'Foo'},\n        ]\n        self.factory.return_value.create_parser.return_value = parser\n        _, response = self.endpoint.make_request(self.op, r)\n        # The parser should be called twice, once for the original\n        # error parse and once again for the modeled exception parse\n        self.assertEqual(parser.parse.call_count, 2)\n        parse_calls = parser.parse.call_args_list\n        self.assertEqual(parse_calls[1][0][1], self.exception_shape)\n        self.assertEqual(parse_calls[0][0][1], self.op.output_shape)\n        expected_response = {\n            'Error': {\n                'Code': 'ExceptionShape',\n                'Message': 'Some message',\n            },\n            'SomeField': 'Foo',\n        }\n        self.assertEqual(response, expected_response)\n\n    def test_close(self):\n        self.endpoint.close()\n        self.endpoint.http_session.close.assert_called_once_with()\n\n\nclass TestRetryInterface(TestEndpointBase):\n    def setUp(self):\n        super().setUp()\n        self.retried_on_exception = None\n        self._operation = mock.Mock(spec=OperationModel)\n        self._operation.name = 'DescribeInstances'\n        self._operation.metadata = {'protocol': 'query'}\n        self._operation.service_model.service_id = ServiceId('EC2')\n        self._operation.has_streaming_output = False\n        self._operation.has_event_stream_output = False\n\n    def assert_events_emitted(self, event_emitter, expected_events):\n        self.assertEqual(\n            self.get_events_emitted(event_emitter), expected_events\n        )\n\n    def test_retry_events_are_emitted(self):\n        self.endpoint.make_request(self._operation, request_dict())\n        call_args = self.event_emitter.emit.call_args\n        self.assertEqual(call_args[0][0], 'needs-retry.ec2.DescribeInstances')\n\n    def test_retry_events_can_alter_behavior(self):\n        self.event_emitter.emit.side_effect = self.get_emitter_responses(\n            num_retries=1\n        )\n        r = request_dict()\n        r['context']['client_config'] = Config()\n        self.endpoint.make_request(self._operation, r)\n        self.assert_events_emitted(\n            self.event_emitter,\n            expected_events=[\n                'request-created.ec2.DescribeInstances',\n                'before-send.ec2.DescribeInstances',\n                'response-received.ec2.DescribeInstances',\n                'needs-retry.ec2.DescribeInstances',\n            ]\n            * 2,\n        )\n\n    def test_retry_on_socket_errors(self):\n        self.event_emitter.emit.side_effect = self.get_emitter_responses(\n            num_retries=1\n        )\n        self.http_session.send.side_effect = HTTPClientError(error='wrapped')\n        with self.assertRaises(HTTPClientError):\n            self.endpoint.make_request(self._operation, request_dict())\n        self.assert_events_emitted(\n            self.event_emitter,\n            expected_events=[\n                'request-created.ec2.DescribeInstances',\n                'before-send.ec2.DescribeInstances',\n                'response-received.ec2.DescribeInstances',\n                'needs-retry.ec2.DescribeInstances',\n            ]\n            * 2,\n        )\n\n    def test_retry_attempts_added_to_response_metadata(self):\n        self.event_emitter.emit.side_effect = self.get_emitter_responses(\n            num_retries=1\n        )\n        parser = mock.Mock()\n        parser.parse.return_value = {'ResponseMetadata': {}}\n        self.factory.return_value.create_parser.return_value = parser\n        r = request_dict()\n        r['context']['client_config'] = Config()\n        response = self.endpoint.make_request(self._operation, r)\n        self.assertEqual(response[1]['ResponseMetadata']['RetryAttempts'], 1)\n\n    def test_retry_attempts_is_zero_when_not_retried(self):\n        self.event_emitter.emit.side_effect = self.get_emitter_responses(\n            num_retries=0\n        )\n        parser = mock.Mock()\n        parser.parse.return_value = {'ResponseMetadata': {}}\n        self.factory.return_value.create_parser.return_value = parser\n        response = self.endpoint.make_request(self._operation, request_dict())\n        self.assertEqual(response[1]['ResponseMetadata']['RetryAttempts'], 0)\n\n\nclass TestS3ResetStreamOnRetry(TestEndpointBase):\n    def setUp(self):\n        super().setUp()\n\n    def max_attempts_retry_handler(self, attempts, **kwargs):\n        # Simulate a max requests of 3.\n        self.total_calls += 1\n        if attempts == 3:\n            return None\n        else:\n            # Returning anything non-None will trigger a retry,\n            # but 0 here is so that time.sleep(0) happens.\n            return 0\n\n    def test_reset_stream_on_retry(self):\n        op = mock.Mock()\n        body = RecordStreamResets('foobar')\n        op.name = 'PutObject'\n        op.has_streaming_output = True\n        op.has_event_stream_output = False\n        op.metadata = {'protocol': 'rest-xml'}\n        request = request_dict()\n        request['body'] = body\n        request['context']['client_config'] = Config()\n        self.event_emitter.emit.side_effect = self.get_emitter_responses(\n            num_retries=2\n        )\n        self.endpoint.make_request(op, request)\n        # 2 seeks for the resets and 6 (2 per creation) for content-length\n        self.assertEqual(body.total_resets, 8)\n\n\nclass TestEventStreamBody(TestEndpointBase):\n    def test_event_stream_body_is_streaming(self):\n        self.op.has_event_stream_output = True\n        request = request_dict()\n        self.endpoint.make_request(self.op, request)\n        sent_request = self.http_session.send.call_args[0][0]\n        self.assertTrue(sent_request.stream_output)\n\n\nclass TestEndpointCreator(unittest.TestCase):\n    def setUp(self):\n        self.service_model = mock.Mock(\n            endpoint_prefix='ec2', signature_version='v2', signing_name='ec2'\n        )\n        self.environ = {}\n        self.environ_patch = mock.patch('os.environ', self.environ)\n        self.environ_patch.start()\n        self.creator = EndpointCreator(mock.Mock())\n        self.mock_session = mock.Mock(spec=URLLib3Session)\n\n    def tearDown(self):\n        self.environ_patch.stop()\n\n    def test_creates_endpoint_with_configured_url(self):\n        endpoint = self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-east-1',\n            endpoint_url='https://endpoint.url',\n        )\n        self.assertEqual(endpoint.host, 'https://endpoint.url')\n\n    def test_creates_endpoint_with_ipv4_url(self):\n        endpoint = self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-east-1',\n            endpoint_url='https://192.168.0.0',\n        )\n        self.assertEqual(endpoint.host, 'https://192.168.0.0')\n\n    def test_creates_endpoint_with_ipv6_url(self):\n        endpoint = self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-east-1',\n            endpoint_url='https://[100:0:2::61]:7480',\n        )\n        self.assertEqual(endpoint.host, 'https://[100:0:2::61]:7480')\n\n    def test_raises_error_with_invalid_url(self):\n        with pytest.raises(ValueError):\n            self.creator.create_endpoint(\n                self.service_model,\n                region_name='us-east-1',\n                endpoint_url='https://*.aws.amazon.com/',\n            )\n\n    def test_create_endpoint_with_default_timeout(self):\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            http_session_cls=self.mock_session,\n        )\n        session_args = self.mock_session.call_args[1]\n        self.assertEqual(session_args.get('timeout'), DEFAULT_TIMEOUT)\n\n    def test_create_endpoint_with_customized_timeout(self):\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            timeout=123,\n            http_session_cls=self.mock_session,\n        )\n        session_args = self.mock_session.call_args[1]\n        self.assertEqual(session_args.get('timeout'), 123)\n\n    def test_get_endpoint_default_verify_ssl(self):\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            http_session_cls=self.mock_session,\n        )\n        session_args = self.mock_session.call_args[1]\n        self.assertTrue(session_args.get('verify'))\n\n    def test_verify_ssl_can_be_disabled(self):\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            verify=False,\n            http_session_cls=self.mock_session,\n        )\n        session_args = self.mock_session.call_args[1]\n        self.assertFalse(session_args.get('verify'))\n\n    def test_verify_ssl_can_specify_cert_bundle(self):\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            verify='/path/cacerts.pem',\n            http_session_cls=self.mock_session,\n        )\n        session_args = self.mock_session.call_args[1]\n        self.assertEqual(session_args.get('verify'), '/path/cacerts.pem')\n\n    def test_client_cert_can_specify_path(self):\n        client_cert = '/some/path/cert'\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            client_cert=client_cert,\n            http_session_cls=self.mock_session,\n        )\n        session_args = self.mock_session.call_args[1]\n        self.assertEqual(session_args.get('client_cert'), '/some/path/cert')\n\n    def test_honor_cert_bundle_env_var(self):\n        self.environ['REQUESTS_CA_BUNDLE'] = '/env/cacerts.pem'\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            http_session_cls=self.mock_session,\n        )\n        session_args = self.mock_session.call_args[1]\n        self.assertEqual(session_args.get('verify'), '/env/cacerts.pem')\n\n    def test_env_ignored_if_explicitly_passed(self):\n        self.environ['REQUESTS_CA_BUNDLE'] = '/env/cacerts.pem'\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            verify='/path/cacerts.pem',\n            http_session_cls=self.mock_session,\n        )\n        session_args = self.mock_session.call_args[1]\n        # /path/cacerts.pem wins over the value from the env var.\n        self.assertEqual(session_args.get('verify'), '/path/cacerts.pem')\n\n    def test_can_specify_max_pool_conns(self):\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            max_pool_connections=100,\n            http_session_cls=self.mock_session,\n        )\n        session_args = self.mock_session.call_args[1]\n        self.assertEqual(session_args.get('max_pool_connections'), 100)\n\n    def test_socket_options(self):\n        socket_options = [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)]\n        self.creator.create_endpoint(\n            self.service_model,\n            region_name='us-west-2',\n            endpoint_url='https://example.com',\n            http_session_cls=self.mock_session,\n            socket_options=socket_options,\n        )\n        session_args = self.mock_session.call_args[1]\n        self.assertEqual(session_args.get('socket_options'), socket_options)\n", "tests/unit/test_paginate.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom botocore import model\nfrom botocore.exceptions import PaginationError\nfrom botocore.paginate import (\n    Paginator,\n    PaginatorModel,\n    TokenDecoder,\n    TokenEncoder,\n)\nfrom tests import mock, unittest\n\n\ndef encode_token(token):\n    return TokenEncoder().encode(token)\n\n\nclass TestTokenDecoder(unittest.TestCase):\n    def setUp(self):\n        self.decoder = TokenDecoder()\n\n    def test_decode(self):\n        token = 'eyJmb28iOiAiYmFyIn0='\n        expected = {'foo': 'bar'}\n        self.assertEqual(self.decoder.decode(token), expected)\n\n    def test_decode_with_bytes(self):\n        token = (\n            'eyJib3RvX2VuY29kZWRfa2V5cyI6IFtbImZvbyJdXSwgImZvbyI6ICJZbUZ5In0='\n        )\n        expected = {'foo': b'bar'}\n        self.assertEqual(self.decoder.decode(token), expected)\n\n    def test_decode_with_nested_bytes(self):\n        token = (\n            'eyJmb28iOiB7ImJhciI6ICJZbUY2In0sICJib3RvX2VuY29kZWRfa2V5cyI6'\n            'IFtbImZvbyIsICJiYXIiXV19'\n        )\n        expected = {'foo': {'bar': b'baz'}}\n        self.assertEqual(self.decoder.decode(token), expected)\n\n    def test_decode_with_listed_bytes(self):\n        token = (\n            'eyJib3RvX2VuY29kZWRfa2V5cyI6IFtbImZvbyIsICJiYXIiLCAxXV0sICJmb28i'\n            'OiB7ImJhciI6IFsiYmF6IiwgIlltbHUiXX19'\n        )\n        expected = {'foo': {'bar': ['baz', b'bin']}}\n        self.assertEqual(self.decoder.decode(token), expected)\n\n    def test_decode_with_multiple_bytes_values(self):\n        token = (\n            'eyJib3RvX2VuY29kZWRfa2V5cyI6IFtbImZvbyIsICJiaW4iXSwgWyJmb28iLCAi'\n            'YmFyIl1dLCAiZm9vIjogeyJiaW4iOiAiWW1GdCIsICJiYXIiOiAiWW1GNiJ9fQ=='\n        )\n        expected = {'foo': {'bar': b'baz', 'bin': b'bam'}}\n        self.assertEqual(self.decoder.decode(token), expected)\n\n\nclass TestPaginatorModel(unittest.TestCase):\n    def setUp(self):\n        self.paginator_config = {}\n        self.paginator_config['pagination'] = {\n            'ListFoos': {\n                'output_token': 'NextToken',\n                'input_token': 'NextToken',\n                'result_key': 'Foo',\n            }\n        }\n        self.paginator_model = PaginatorModel(self.paginator_config)\n\n    def test_get_paginator(self):\n        paginator_config = self.paginator_model.get_paginator('ListFoos')\n        self.assertEqual(\n            paginator_config,\n            {\n                'output_token': 'NextToken',\n                'input_token': 'NextToken',\n                'result_key': 'Foo',\n            },\n        )\n\n    def test_get_paginator_no_exists(self):\n        with self.assertRaises(ValueError):\n            self.paginator_model.get_paginator('ListBars')\n\n\nclass TestPagination(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        self.paginate_config = {\n            'output_token': 'NextToken',\n            'input_token': 'NextToken',\n            'result_key': 'Foo',\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_result_key_available(self):\n        self.assertEqual(\n            [rk.expression for rk in self.paginator.result_keys], ['Foo']\n        )\n\n    def test_no_next_token(self):\n        response = {'not_the_next_token': 'foobar'}\n        self.method.return_value = response\n        actual = list(self.paginator.paginate())\n        self.assertEqual(actual, [{'not_the_next_token': 'foobar'}])\n\n    def test_next_token_in_response(self):\n        responses = [\n            {'NextToken': 'token1'},\n            {'NextToken': 'token2'},\n            {'not_next_token': 'foo'},\n        ]\n        self.method.side_effect = responses\n        actual = list(self.paginator.paginate())\n        self.assertEqual(actual, responses)\n        # The first call has no next token, the second and third call should\n        # have 'token1' and 'token2' respectively.\n        self.assertEqual(\n            self.method.call_args_list,\n            [\n                mock.call(),\n                mock.call(NextToken='token1'),\n                mock.call(NextToken='token2'),\n            ],\n        )\n\n    def test_next_token_is_string(self):\n        self.paginate_config = {\n            \"output_token\": \"Marker\",\n            \"input_token\": \"Marker\",\n            \"result_key\": \"Users\",\n            \"limit_key\": \"MaxKeys\",\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        result = self.paginator.paginate(PaginationConfig={'MaxItems': 1})\n        result = result.build_full_result()\n        token = result.get('NextToken')\n        self.assertIsInstance(token, (str,))\n\n    def test_any_passed_in_args_are_unmodified(self):\n        responses = [\n            {'NextToken': 'token1'},\n            {'NextToken': 'token2'},\n            {'not_next_token': 'foo'},\n        ]\n        self.method.side_effect = responses\n        actual = list(self.paginator.paginate(Foo='foo', Bar='bar'))\n        self.assertEqual(actual, responses)\n        self.assertEqual(\n            self.method.call_args_list,\n            [\n                mock.call(Foo='foo', Bar='bar'),\n                mock.call(Foo='foo', Bar='bar', NextToken='token1'),\n                mock.call(Foo='foo', Bar='bar', NextToken='token2'),\n            ],\n        )\n\n    def test_exception_raised_if_same_next_token(self):\n        responses = [\n            {'NextToken': 'token1'},\n            {'NextToken': 'token2'},\n            {'NextToken': 'token2'},\n        ]\n        self.method.side_effect = responses\n        with self.assertRaises(PaginationError):\n            list(self.paginator.paginate())\n\n    def test_next_token_with_or_expression(self):\n        self.pagination_config = {\n            'output_token': 'NextToken || NextToken2',\n            'input_token': 'NextToken',\n            'result_key': 'Foo',\n        }\n        self.paginator = Paginator(\n            self.method, self.pagination_config, self.model\n        )\n        # Verify that despite varying between NextToken and NextToken2\n        # we still can extract the right next tokens.\n        responses = [\n            {'NextToken': 'token1'},\n            {'NextToken2': 'token2'},\n            # The first match found wins, so because NextToken is\n            # listed before NextToken2 in the 'output_tokens' config,\n            # 'token3' is chosen over 'token4'.\n            {'NextToken': 'token3', 'NextToken2': 'token4'},\n            {'not_next_token': 'foo'},\n        ]\n        self.method.side_effect = responses\n        list(self.paginator.paginate())\n        self.assertEqual(\n            self.method.call_args_list,\n            [\n                mock.call(),\n                mock.call(NextToken='token1'),\n                mock.call(NextToken='token2'),\n                mock.call(NextToken='token3'),\n            ],\n        )\n\n    def test_more_tokens(self):\n        # Some pagination configs have a 'more_token' key that\n        # indicate whether or not the results are being paginated.\n        self.paginate_config = {\n            'more_results': 'IsTruncated',\n            'output_token': 'NextToken',\n            'input_token': 'NextToken',\n            'result_key': 'Foo',\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        responses = [\n            {'Foo': [1], 'IsTruncated': True, 'NextToken': 'token1'},\n            {'Foo': [2], 'IsTruncated': True, 'NextToken': 'token2'},\n            {'Foo': [3], 'IsTruncated': False, 'NextToken': 'token3'},\n            {'Foo': [4], 'not_next_token': 'foo'},\n        ]\n        self.method.side_effect = responses\n        list(self.paginator.paginate())\n        self.assertEqual(\n            self.method.call_args_list,\n            [\n                mock.call(),\n                mock.call(NextToken='token1'),\n                mock.call(NextToken='token2'),\n            ],\n        )\n\n    def test_more_tokens_is_path_expression(self):\n        self.paginate_config = {\n            'more_results': 'Foo.IsTruncated',\n            'output_token': 'NextToken',\n            'input_token': 'NextToken',\n            'result_key': 'Bar',\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        responses = [\n            {'Foo': {'IsTruncated': True}, 'NextToken': 'token1'},\n            {'Foo': {'IsTruncated': False}, 'NextToken': 'token2'},\n        ]\n        self.method.side_effect = responses\n        list(self.paginator.paginate())\n        self.assertEqual(\n            self.method.call_args_list,\n            [mock.call(), mock.call(NextToken='token1')],\n        )\n\n    def test_page_size(self):\n        self.paginate_config = {\n            \"output_token\": \"Marker\",\n            \"input_token\": \"Marker\",\n            \"result_key\": \"Users\",\n            \"limit_key\": \"MaxKeys\",\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        users = []\n        for page in self.paginator.paginate(PaginationConfig={'PageSize': 1}):\n            users += page['Users']\n        self.assertEqual(\n            self.method.call_args_list,\n            [\n                mock.call(MaxKeys=1),\n                mock.call(Marker='m1', MaxKeys=1),\n                mock.call(Marker='m2', MaxKeys=1),\n            ],\n        )\n\n    def test_with_empty_markers(self):\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"\"},\n            {\"Users\": [\"User1\"], \"Marker\": \"\"},\n            {\"Users\": [\"User1\"], \"Marker\": \"\"},\n        ]\n        self.method.side_effect = responses\n        users = []\n        for page in self.paginator.paginate():\n            users += page['Users']\n        # We want to stop paginating if the next token is empty.\n        self.assertEqual(self.method.call_args_list, [mock.call()])\n        self.assertEqual(users, ['User1'])\n\n    def test_build_full_result_with_single_key(self):\n        self.paginate_config = {\n            \"output_token\": \"Marker\",\n            \"input_token\": \"Marker\",\n            \"result_key\": \"Users\",\n            \"limit_key\": \"MaxKeys\",\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate()\n        complete = pages.build_full_result()\n        self.assertEqual(complete, {'Users': ['User1', 'User2', 'User3']})\n\n    def test_build_multiple_results(self):\n        self.paginate_config = {\n            \"output_token\": \"Marker\",\n            \"input_token\": \"Marker\",\n            \"result_key\": \"Users\",\n            \"limit_key\": \"MaxKeys\",\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n        max_items = 3\n        page_size = 2\n\n        responses = [\n            {\"Users\": [\"User1\", \"User2\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User3\", \"User4\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\", \"User4\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User5\", \"User6\", \"User7\"], \"Marker\": \"m3\"},\n        ]\n        self.method.side_effect = responses\n\n        pages = self.paginator.paginate(\n            PaginationConfig={'PageSize': page_size, 'MaxItems': max_items}\n        )\n        result = pages.build_full_result()\n\n        pages = self.paginator.paginate(\n            PaginationConfig={\n                'MaxItems': max_items,\n                'PageSize': page_size,\n                'StartingToken': result['NextToken'],\n            }\n        )\n        result = pages.build_full_result()\n\n        expected_token = encode_token(\n            {\n                'Marker': 'm2',\n                'boto_truncate_amount': 2,\n            }\n        )\n        self.assertEqual(expected_token, result['NextToken'])\n\n\nclass TestPaginatorPageSize(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        self.paginate_config = {\n            \"output_token\": \"Marker\",\n            \"input_token\": \"Marker\",\n            \"result_key\": [\"Users\", \"Groups\"],\n            'limit_key': 'MaxKeys',\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        self.endpoint = mock.Mock()\n\n    def test_no_page_size(self):\n        kwargs = {'arg1': 'foo', 'arg2': 'bar'}\n        ref_kwargs = {'arg1': 'foo', 'arg2': 'bar'}\n        pages = self.paginator.paginate(**kwargs)\n        pages._inject_starting_params(kwargs)\n        self.assertEqual(kwargs, ref_kwargs)\n\n    def test_page_size(self):\n        kwargs = {\n            'arg1': 'foo',\n            'arg2': 'bar',\n            'PaginationConfig': {'PageSize': 5},\n        }\n        extracted_kwargs = {'arg1': 'foo', 'arg2': 'bar'}\n        # Note that ``MaxKeys`` in ``setUp()`` is the parameter used for\n        # the page size for pagination.\n        ref_kwargs = {'arg1': 'foo', 'arg2': 'bar', 'MaxKeys': 5}\n        pages = self.paginator.paginate(**kwargs)\n        pages._inject_starting_params(extracted_kwargs)\n        self.assertEqual(extracted_kwargs, ref_kwargs)\n\n    def test_page_size_incorrectly_provided(self):\n        kwargs = {\n            'arg1': 'foo',\n            'arg2': 'bar',\n            'PaginationConfig': {'PageSize': 5},\n        }\n        del self.paginate_config['limit_key']\n        paginator = Paginator(self.method, self.paginate_config, self.model)\n\n        with self.assertRaises(PaginationError):\n            paginator.paginate(**kwargs)\n\n\nclass TestPaginatorWithPathExpressions(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        # This is something we'd see in s3 pagination.\n        self.paginate_config = {\n            'output_token': [\n                'NextMarker || ListBucketResult.Contents[-1].Key'\n            ],\n            'input_token': 'next_marker',\n            'result_key': 'Contents',\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_s3_list_objects(self):\n        responses = [\n            {'NextMarker': 'token1'},\n            {'NextMarker': 'token2'},\n            {'not_next_token': 'foo'},\n        ]\n        self.method.side_effect = responses\n        list(self.paginator.paginate())\n        self.assertEqual(\n            self.method.call_args_list,\n            [\n                mock.call(),\n                mock.call(next_marker='token1'),\n                mock.call(next_marker='token2'),\n            ],\n        )\n\n    def test_s3_list_object_complex(self):\n        responses = [\n            {'NextMarker': 'token1'},\n            {\n                'ListBucketResult': {\n                    'Contents': [{\"Key\": \"first\"}, {\"Key\": \"Last\"}]\n                }\n            },\n            {'not_next_token': 'foo'},\n        ]\n        self.method.side_effect = responses\n        list(self.paginator.paginate())\n        self.assertEqual(\n            self.method.call_args_list,\n            [\n                mock.call(),\n                mock.call(next_marker='token1'),\n                mock.call(next_marker='Last'),\n            ],\n        )\n\n\nclass TestBinaryTokens(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        self.paginate_config = {\n            \"output_token\": \"Marker\",\n            \"input_token\": \"Marker\",\n            \"result_key\": \"Users\",\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_build_full_result_with_bytes(self):\n        responses = [\n            {\"Users\": [\"User1\", \"User2\"], \"Marker\": b'\\xff'},\n            {\"Users\": [\"User3\", \"User4\"], \"Marker\": b'\\xfe'},\n            {\"Users\": [\"User5\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate(PaginationConfig={'MaxItems': 3})\n        complete = pages.build_full_result()\n        expected_token = encode_token(\n            {\n                \"Marker\": b'\\xff',\n                \"boto_truncate_amount\": 1,\n            }\n        )\n        expected_response = {\n            \"Users\": [\"User1\", \"User2\", \"User3\"],\n            \"NextToken\": expected_token,\n        }\n        self.assertEqual(complete, expected_response)\n\n    def test_build_full_result_with_nested_bytes(self):\n        responses = [\n            {\"Users\": [\"User1\", \"User2\"], \"Marker\": {'key': b'\\xff'}},\n            {\"Users\": [\"User3\", \"User4\"], \"Marker\": {'key': b'\\xfe'}},\n            {\"Users\": [\"User5\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate(PaginationConfig={'MaxItems': 3})\n        complete = pages.build_full_result()\n        expected_token = encode_token(\n            {\n                \"Marker\": {'key': b'\\xff'},\n                \"boto_truncate_amount\": 1,\n            }\n        )\n        expected_response = {\n            \"Users\": [\"User1\", \"User2\", \"User3\"],\n            \"NextToken\": expected_token,\n        }\n        self.assertEqual(complete, expected_response)\n\n    def test_build_full_result_with_listed_bytes(self):\n        responses = [\n            {\"Users\": [\"User1\", \"User2\"], \"Marker\": {'key': ['foo', b'\\xff']}},\n            {\"Users\": [\"User3\", \"User4\"], \"Marker\": {'key': ['foo', b'\\xfe']}},\n            {\"Users\": [\"User5\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate(PaginationConfig={'MaxItems': 3})\n        complete = pages.build_full_result()\n        expected_token = encode_token(\n            {\n                \"Marker\": {'key': ['foo', b'\\xff']},\n                \"boto_truncate_amount\": 1,\n            }\n        )\n        expected_response = {\n            \"Users\": [\"User1\", \"User2\", \"User3\"],\n            \"NextToken\": expected_token,\n        }\n        self.assertEqual(complete, expected_response)\n\n    def test_build_full_result_with_multiple_bytes_values(self):\n        responses = [\n            {\n                \"Users\": [\"User1\", \"User2\"],\n                \"Marker\": {'key': b'\\xff', 'key2': b'\\xef'},\n            },\n            {\n                \"Users\": [\"User3\", \"User4\"],\n                \"Marker\": {'key': b'\\xfe', 'key2': b'\\xee'},\n            },\n            {\"Users\": [\"User5\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate(PaginationConfig={'MaxItems': 3})\n        complete = pages.build_full_result()\n        expected_token = encode_token(\n            {\n                \"Marker\": {'key': b'\\xff', 'key2': b'\\xef'},\n                \"boto_truncate_amount\": 1,\n            }\n        )\n        expected_response = {\n            \"Users\": [\"User1\", \"User2\", \"User3\"],\n            \"NextToken\": expected_token,\n        }\n        self.assertEqual(complete, expected_response)\n\n    def test_resume_with_bytes(self):\n        responses = [\n            {\"Users\": [\"User3\", \"User4\"], \"Marker\": b'\\xfe'},\n            {\"Users\": [\"User5\"]},\n        ]\n        self.method.side_effect = responses\n        starting_token = encode_token(\n            {\n                \"Marker\": b'\\xff',\n                \"boto_truncate_amount\": 1,\n            }\n        )\n        pages = self.paginator.paginate(\n            PaginationConfig={'StartingToken': starting_token}\n        )\n        complete = pages.build_full_result()\n        expected_response = {\"Users\": [\"User4\", \"User5\"]}\n        self.assertEqual(complete, expected_response)\n        self.method.assert_any_call(Marker=b'\\xff')\n\n    def test_resume_with_nested_bytes(self):\n        responses = [\n            {\"Users\": [\"User3\", \"User4\"], \"Marker\": {'key': b'\\xfe'}},\n            {\"Users\": [\"User5\"]},\n        ]\n        self.method.side_effect = responses\n        starting_token = encode_token(\n            {\n                \"Marker\": {'key': b'\\xff'},\n                \"boto_truncate_amount\": 1,\n            }\n        )\n        pages = self.paginator.paginate(\n            PaginationConfig={'StartingToken': starting_token}\n        )\n        complete = pages.build_full_result()\n        expected_response = {\"Users\": [\"User4\", \"User5\"]}\n        self.assertEqual(complete, expected_response)\n        self.method.assert_any_call(Marker={'key': b'\\xff'})\n\n    def test_resume_with_listed_bytes(self):\n        responses = [\n            {\"Users\": [\"User3\", \"User4\"], \"Marker\": {'key': ['bar', b'\\xfe']}},\n            {\"Users\": [\"User5\"]},\n        ]\n        self.method.side_effect = responses\n        starting_token = encode_token(\n            {\n                \"Marker\": {'key': ['foo', b'\\xff']},\n                \"boto_truncate_amount\": 1,\n            }\n        )\n        pages = self.paginator.paginate(\n            PaginationConfig={'StartingToken': starting_token}\n        )\n        complete = pages.build_full_result()\n        expected_response = {\"Users\": [\"User4\", \"User5\"]}\n        self.assertEqual(complete, expected_response)\n        self.method.assert_any_call(Marker={'key': ['foo', b'\\xff']})\n\n    def test_resume_with_multiple_bytes_values(self):\n        responses = [\n            {\n                \"Users\": [\"User3\", \"User4\"],\n                \"Marker\": {'key': b'\\xfe', 'key2': b'\\xee'},\n            },\n            {\"Users\": [\"User5\"]},\n        ]\n        self.method.side_effect = responses\n        starting_token = encode_token(\n            {\n                \"Marker\": {'key': b'\\xff', 'key2': b'\\xef'},\n                \"boto_truncate_amount\": 1,\n            }\n        )\n        pages = self.paginator.paginate(\n            PaginationConfig={'StartingToken': starting_token}\n        )\n        complete = pages.build_full_result()\n        expected_response = {\"Users\": [\"User4\", \"User5\"]}\n        self.assertEqual(complete, expected_response)\n        self.method.assert_any_call(Marker={'key': b'\\xfe', 'key2': b'\\xee'})\n\n\nclass TestMultipleTokens(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        # This is something we'd see in s3 pagination.\n        self.paginate_config = {\n            \"output_token\": [\n                \"ListBucketResults.NextKeyMarker\",\n                \"ListBucketResults.NextUploadIdMarker\",\n            ],\n            \"input_token\": [\"key_marker\", \"upload_id_marker\"],\n            \"result_key\": 'Foo',\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_s3_list_multipart_uploads(self):\n        responses = [\n            {\n                \"Foo\": [1],\n                \"ListBucketResults\": {\n                    \"NextKeyMarker\": \"key1\",\n                    \"NextUploadIdMarker\": \"up1\",\n                },\n            },\n            {\n                \"Foo\": [2],\n                \"ListBucketResults\": {\n                    \"NextKeyMarker\": \"key2\",\n                    \"NextUploadIdMarker\": \"up2\",\n                },\n            },\n            {\n                \"Foo\": [3],\n                \"ListBucketResults\": {\n                    \"NextKeyMarker\": \"key3\",\n                    \"NextUploadIdMarker\": \"up3\",\n                },\n            },\n            {},\n        ]\n        self.method.side_effect = responses\n        list(self.paginator.paginate())\n        self.assertEqual(\n            self.method.call_args_list,\n            [\n                mock.call(),\n                mock.call(key_marker='key1', upload_id_marker='up1'),\n                mock.call(key_marker='key2', upload_id_marker='up2'),\n                mock.call(key_marker='key3', upload_id_marker='up3'),\n            ],\n        )\n\n\nclass TestOptionalTokens(unittest.TestCase):\n    \"\"\"\n    Tests a paginator with an optional output token.\n\n    The Route53 ListResourceRecordSets paginator includes three output tokens,\n    one of which only appears in certain records. If this gets left in the\n    request params from a previous page, the API will skip over a record.\n\n    \"\"\"\n\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        # This is based on Route53 pagination.\n        self.paginate_config = {\n            \"output_token\": [\n                \"NextRecordName\",\n                \"NextRecordType\",\n                \"NextRecordIdentifier\",\n            ],\n            \"input_token\": [\n                \"StartRecordName\",\n                \"StartRecordType\",\n                \"StartRecordIdentifier\",\n            ],\n            \"result_key\": 'Foo',\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_clean_token(self):\n        responses = [\n            {\n                \"Foo\": [1],\n                \"IsTruncated\": True,\n                \"NextRecordName\": \"aaa.example.com\",\n                \"NextRecordType\": \"A\",\n                \"NextRecordIdentifier\": \"id\",\n            },\n            {\n                \"Foo\": [2],\n                \"IsTruncated\": True,\n                \"NextRecordName\": \"bbb.example.com\",\n                \"NextRecordType\": \"A\",\n            },\n            {\"Foo\": [3], \"IsTruncated\": False},\n        ]\n        self.method.side_effect = responses\n        list(self.paginator.paginate())\n        self.assertEqual(\n            self.method.call_args_list,\n            [\n                mock.call(),\n                mock.call(\n                    StartRecordName='aaa.example.com',\n                    StartRecordType='A',\n                    StartRecordIdentifier='id',\n                ),\n                mock.call(\n                    StartRecordName='bbb.example.com', StartRecordType='A'\n                ),\n            ],\n        )\n\n\nclass TestKeyIterators(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        # This is something we'd see in s3 pagination.\n        self.paginate_config = {\n            \"output_token\": \"Marker\",\n            \"input_token\": \"Marker\",\n            \"result_key\": \"Users\",\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_result_key_iters(self):\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate()\n        iterators = pages.result_key_iters()\n        self.assertEqual(len(iterators), 1)\n        self.assertEqual(list(iterators[0]), [\"User1\", \"User2\", \"User3\"])\n\n    def test_build_full_result_with_single_key(self):\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate()\n        complete = pages.build_full_result()\n        self.assertEqual(complete, {'Users': ['User1', 'User2', 'User3']})\n\n    def test_max_items_can_be_specified(self):\n        paginator = Paginator(self.method, self.paginate_config, self.model)\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        expected_token = encode_token({\"Marker\": \"m1\"})\n        self.assertEqual(\n            paginator.paginate(\n                PaginationConfig={'MaxItems': 1}\n            ).build_full_result(),\n            {'Users': ['User1'], 'NextToken': expected_token},\n        )\n\n    def test_max_items_as_strings(self):\n        # Some services (route53) model MaxItems as a string type.\n        # We need to be able to handle this case.\n        paginator = Paginator(self.method, self.paginate_config, self.model)\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        expected_token = encode_token({\"Marker\": \"m1\"})\n        self.assertEqual(\n            # Note MaxItems is a string here.\n            paginator.paginate(\n                PaginationConfig={'MaxItems': '1'}\n            ).build_full_result(),\n            {'Users': ['User1'], 'NextToken': expected_token},\n        )\n\n    def test_next_token_on_page_boundary(self):\n        paginator = Paginator(self.method, self.paginate_config, self.model)\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        expected_token = encode_token({\"Marker\": \"m2\"})\n        self.assertEqual(\n            paginator.paginate(\n                PaginationConfig={'MaxItems': 2}\n            ).build_full_result(),\n            {'Users': ['User1', 'User2'], 'NextToken': expected_token},\n        )\n\n    def test_max_items_can_be_specified_truncates_response(self):\n        # We're saying we only want 4 items, but notice that the second\n        # page of results returns users 4-6 so we have to truncated\n        # part of that second page.\n        paginator = Paginator(self.method, self.paginate_config, self.model)\n        responses = [\n            {\"Users\": [\"User1\", \"User2\", \"User3\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User4\", \"User5\", \"User6\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User7\"]},\n        ]\n        self.method.side_effect = responses\n        expected_token = encode_token(\n            {\"Marker\": \"m1\", \"boto_truncate_amount\": 1}\n        )\n        self.assertEqual(\n            paginator.paginate(\n                PaginationConfig={'MaxItems': 4}\n            ).build_full_result(),\n            {\n                'Users': ['User1', 'User2', 'User3', 'User4'],\n                'NextToken': expected_token,\n            },\n        )\n\n    def test_resume_next_marker_mid_page(self):\n        # This is a simulation of picking up from the response\n        # from test_MaxItems_can_be_specified_truncates_response\n        # We got the first 4 users, when we pick up we should get\n        # User5 - User7.\n        paginator = Paginator(self.method, self.paginate_config, self.model)\n        responses = [\n            {\"Users\": [\"User4\", \"User5\", \"User6\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User7\"]},\n        ]\n        self.method.side_effect = responses\n        starting_token = encode_token(\n            {\"Marker\": \"m1\", \"boto_truncate_amount\": 1}\n        )\n        pagination_config = {'StartingToken': starting_token}\n        self.assertEqual(\n            paginator.paginate(\n                PaginationConfig=pagination_config\n            ).build_full_result(),\n            {'Users': ['User5', 'User6', 'User7']},\n        )\n        self.assertEqual(\n            self.method.call_args_list,\n            [mock.call(Marker='m1'), mock.call(Marker='m2')],\n        )\n\n    def test_max_items_exceeds_actual_amount(self):\n        # Because MaxItems=10 > number of users (3), we should just return\n        # all of the users.\n        paginator = Paginator(self.method, self.paginate_config, self.model)\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        self.assertEqual(\n            paginator.paginate(\n                PaginationConfig={'MaxItems': 10}\n            ).build_full_result(),\n            {'Users': ['User1', 'User2', 'User3']},\n        )\n\n    def test_bad_input_tokens(self):\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        with self.assertRaisesRegex(ValueError, 'Bad starting token'):\n            pagination_config = {'StartingToken': 'does___not___work'}\n            self.paginator.paginate(\n                PaginationConfig=pagination_config\n            ).build_full_result()\n\n\nclass TestMultipleResultKeys(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        # This is something we'd see in s3 pagination.\n        self.paginate_config = {\n            \"output_token\": \"Marker\",\n            \"input_token\": \"Marker\",\n            \"result_key\": [\"Users\", \"Groups\"],\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_build_full_result_with_multiple_result_keys(self):\n        responses = [\n            {\"Users\": [\"User1\"], \"Groups\": [\"Group1\"], \"Marker\": \"m1\"},\n            {\"Users\": [\"User2\"], \"Groups\": [\"Group2\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User3\"], \"Groups\": [\"Group3\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate()\n        complete = pages.build_full_result()\n        self.assertEqual(\n            complete,\n            {\n                \"Users\": ['User1', 'User2', 'User3'],\n                \"Groups\": ['Group1', 'Group2', 'Group3'],\n            },\n        )\n\n    def test_build_full_result_with_different_length_result_keys(self):\n        responses = [\n            {\"Users\": [\"User1\"], \"Groups\": [\"Group1\"], \"Marker\": \"m1\"},\n            # Then we stop getting \"Users\" output, but we get more \"Groups\"\n            {\"Users\": [], \"Groups\": [\"Group2\"], \"Marker\": \"m2\"},\n            {\"Users\": [], \"Groups\": [\"Group3\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate()\n        complete = pages.build_full_result()\n        self.assertEqual(\n            complete,\n            {\"Users\": ['User1'], \"Groups\": ['Group1', 'Group2', 'Group3']},\n        )\n\n    def test_build_full_result_with_zero_length_result_key(self):\n        responses = [\n            # In this case the 'Users' key is always empty but we should\n            # have a 'Users' key in the output, it should just have an\n            # empty list for a value.\n            {\"Users\": [], \"Groups\": [\"Group1\"], \"Marker\": \"m1\"},\n            {\"Users\": [], \"Groups\": [\"Group2\"], \"Marker\": \"m2\"},\n            {\"Users\": [], \"Groups\": [\"Group3\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate()\n        complete = pages.build_full_result()\n        self.assertEqual(\n            complete, {\"Users\": [], \"Groups\": ['Group1', 'Group2', 'Group3']}\n        )\n\n    def test_build_result_with_secondary_keys(self):\n        responses = [\n            {\n                \"Users\": [\"User1\", \"User2\"],\n                \"Groups\": [\"Group1\", \"Group2\"],\n                \"Marker\": \"m1\",\n            },\n            {\"Users\": [\"User3\"], \"Groups\": [\"Group3\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User4\"], \"Groups\": [\"Group4\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate(PaginationConfig={'MaxItems': 1})\n        complete = pages.build_full_result()\n        expected_token = encode_token(\n            {\"Marker\": None, \"boto_truncate_amount\": 1}\n        )\n        self.assertEqual(\n            complete,\n            {\n                \"Users\": [\"User1\"],\n                \"Groups\": [\"Group1\", \"Group2\"],\n                \"NextToken\": expected_token,\n            },\n        )\n\n    def test_resume_with_secondary_keys(self):\n        # This is simulating a continutation of the previous test,\n        # test_build_result_with_secondary_keys.  We use the\n        # token specified in the response \"None___1\" to continue where we\n        # left off.\n        responses = [\n            {\n                \"Users\": [\"User1\", \"User2\"],\n                \"Groups\": [\"Group1\", \"Group2\"],\n                \"Marker\": \"m1\",\n            },\n            {\"Users\": [\"User3\"], \"Groups\": [\"Group3\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User4\"], \"Groups\": [\"Group4\"]},\n        ]\n        self.method.side_effect = responses\n        starting_token = encode_token(\n            {\"Marker\": None, \"boto_truncate_amount\": 1}\n        )\n        pages = self.paginator.paginate(\n            PaginationConfig={'MaxItems': 1, 'StartingToken': starting_token}\n        )\n        complete = pages.build_full_result()\n        # Note that the secondary keys (\"Groups\") are all truncated because\n        # they were in the original (first) response.\n        expected_token = encode_token({\"Marker\": \"m1\"})\n        self.assertEqual(\n            complete,\n            {\"Users\": [\"User2\"], \"Groups\": [], \"NextToken\": expected_token},\n        )\n\n    def test_resume_with_secondary_result_as_string(self):\n        self.method.return_value = {\"Users\": [\"User1\", \"User2\"], \"Groups\": \"a\"}\n        starting_token = encode_token(\n            {\"Marker\": None, \"boto_truncate_amount\": 1}\n        )\n        pages = self.paginator.paginate(\n            PaginationConfig={'MaxItems': 1, 'StartingToken': starting_token}\n        )\n        complete = pages.build_full_result()\n        # Note that the secondary keys (\"Groups\") becomes empty string because\n        # they were in the original (first) response.\n        self.assertEqual(complete, {\"Users\": [\"User2\"], \"Groups\": \"\"})\n\n    def test_resume_with_secondary_result_as_integer(self):\n        self.method.return_value = {\"Users\": [\"User1\", \"User2\"], \"Groups\": 123}\n        starting_token = encode_token(\n            {\"Marker\": None, \"boto_truncate_amount\": 1}\n        )\n        pages = self.paginator.paginate(\n            PaginationConfig={'MaxItems': 1, 'StartingToken': starting_token}\n        )\n        complete = pages.build_full_result()\n        # Note that the secondary keys (\"Groups\") becomes zero because\n        # they were in the original (first) response.\n        self.assertEqual(complete, {\"Users\": [\"User2\"], \"Groups\": 0})\n\n\nclass TestMultipleInputKeys(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        # Probably the most complicated example we'll see:\n        # multiple input/output/result keys.\n        self.paginate_config = {\n            \"output_token\": [\"Marker1\", \"Marker2\"],\n            \"input_token\": [\"InMarker1\", \"InMarker2\"],\n            \"result_key\": [\"Users\", \"Groups\"],\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_build_full_result_with_multiple_input_keys(self):\n        responses = [\n            {\n                \"Users\": [\"User1\", \"User2\"],\n                \"Groups\": [\"Group1\"],\n                \"Marker1\": \"m1\",\n                \"Marker2\": \"m2\",\n            },\n            {\n                \"Users\": [\"User3\", \"User4\"],\n                \"Groups\": [\"Group2\"],\n                \"Marker1\": \"m3\",\n                \"Marker2\": \"m4\",\n            },\n            {\"Users\": [\"User5\"], \"Groups\": [\"Group3\"]},\n        ]\n        self.method.side_effect = responses\n        pages = self.paginator.paginate(PaginationConfig={'MaxItems': 3})\n        complete = pages.build_full_result()\n        expected_token = encode_token(\n            {\"InMarker1\": \"m1\", \"InMarker2\": \"m2\", \"boto_truncate_amount\": 1}\n        )\n        self.assertEqual(\n            complete,\n            {\n                \"Users\": ['User1', 'User2', 'User3'],\n                \"Groups\": ['Group1', 'Group2'],\n                \"NextToken\": expected_token,\n            },\n        )\n\n    def test_resume_with_multiple_input_keys(self):\n        responses = [\n            {\n                \"Users\": [\"User3\", \"User4\"],\n                \"Groups\": [\"Group2\"],\n                \"Marker1\": \"m3\",\n                \"Marker2\": \"m4\",\n            },\n            {\"Users\": [\"User5\"], \"Groups\": [\"Group3\"]},\n        ]\n        self.method.side_effect = responses\n        starting_token = encode_token(\n            {\"InMarker1\": \"m1\", \"InMarker2\": \"m2\", \"boto_truncate_amount\": 1}\n        )\n        pages = self.paginator.paginate(\n            PaginationConfig={'MaxItems': 1, 'StartingToken': starting_token}\n        )\n        complete = pages.build_full_result()\n        expected_token = encode_token({\"InMarker1\": \"m3\", \"InMarker2\": \"m4\"})\n        self.assertEqual(\n            complete,\n            {\"Users\": ['User4'], \"Groups\": [], \"NextToken\": expected_token},\n        )\n        self.assertEqual(\n            self.method.call_args_list,\n            [mock.call(InMarker1='m1', InMarker2='m2')],\n        )\n\n    def test_resume_encounters_an_empty_payload(self):\n        response = {\"not_a_result_key\": \"it happens in some service\"}\n        self.method.return_value = response\n        starting_token = encode_token(\n            {\"Marker\": None, \"boto_truncate_amount\": 1}\n        )\n        complete = self.paginator.paginate(\n            PaginationConfig={'StartingToken': starting_token}\n        ).build_full_result()\n        self.assertEqual(complete, {})\n\n    def test_result_key_exposed_on_paginator(self):\n        self.assertEqual(\n            [rk.expression for rk in self.paginator.result_keys],\n            ['Users', 'Groups'],\n        )\n\n    def test_result_key_exposed_on_page_iterator(self):\n        pages = self.paginator.paginate(MaxItems=3)\n        self.assertEqual(\n            [rk.expression for rk in pages.result_keys], ['Users', 'Groups']\n        )\n\n\nclass TestExpressionKeyIterators(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        # This is something like what we'd see in RDS.\n        self.paginate_config = {\n            \"input_token\": \"Marker\",\n            \"output_token\": \"Marker\",\n            \"limit_key\": \"MaxRecords\",\n            \"result_key\": \"EngineDefaults.Parameters\",\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        self.responses = [\n            {\"EngineDefaults\": {\"Parameters\": [\"One\", \"Two\"]}, \"Marker\": \"m1\"},\n            {\n                \"EngineDefaults\": {\"Parameters\": [\"Three\", \"Four\"]},\n                \"Marker\": \"m2\",\n            },\n            {\"EngineDefaults\": {\"Parameters\": [\"Five\"]}},\n        ]\n\n    def test_result_key_iters(self):\n        self.method.side_effect = self.responses\n        pages = self.paginator.paginate()\n        iterators = pages.result_key_iters()\n        self.assertEqual(len(iterators), 1)\n        self.assertEqual(\n            list(iterators[0]), ['One', 'Two', 'Three', 'Four', 'Five']\n        )\n\n    def test_build_full_result_with_single_key(self):\n        self.method.side_effect = self.responses\n        pages = self.paginator.paginate()\n        complete = pages.build_full_result()\n        self.assertEqual(\n            complete,\n            {\n                'EngineDefaults': {\n                    'Parameters': ['One', 'Two', 'Three', 'Four', 'Five']\n                },\n            },\n        )\n\n\nclass TestIncludeResultKeys(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        self.paginate_config = {\n            'output_token': 'Marker',\n            'input_token': 'Marker',\n            'result_key': ['ResultKey', 'Count', 'Log'],\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_different_kinds_of_result_key(self):\n        self.method.side_effect = [\n            {'ResultKey': ['a'], 'Count': 1, 'Log': 'x', 'Marker': 'a'},\n            {'not_a_result_key': 'this page will be ignored', 'Marker': '_'},\n            {'ResultKey': ['b', 'c'], 'Count': 2, 'Log': 'y', 'Marker': 'b'},\n            {'ResultKey': ['d', 'e', 'f'], 'Count': 3, 'Log': 'z'},\n        ]\n        pages = self.paginator.paginate()\n        expected = {\n            'ResultKey': ['a', 'b', 'c', 'd', 'e', 'f'],\n            'Count': 6,\n            'Log': 'xyz',\n        }\n        self.assertEqual(pages.build_full_result(), expected)\n\n    def test_result_key_is_missing(self):\n        self.method.side_effect = [\n            {'not_a_result_key': 'this page will be ignored', 'Marker': '_'},\n            {'neither_this_one': 'this page will be ignored, too'},\n        ]\n        pages = self.paginator.paginate()\n        expected = {}\n        self.assertEqual(pages.build_full_result(), expected)\n\n\nclass TestIncludeNonResultKeys(unittest.TestCase):\n    maxDiff = None\n\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        self.paginate_config = {\n            'output_token': 'NextToken',\n            'input_token': 'NextToken',\n            'result_key': 'ResultKey',\n            'non_aggregate_keys': ['NotResultKey'],\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_include_non_aggregate_keys(self):\n        self.method.side_effect = [\n            {'ResultKey': ['foo'], 'NotResultKey': 'a', 'NextToken': 't1'},\n            {'ResultKey': ['bar'], 'NotResultKey': 'a', 'NextToken': 't2'},\n            {'ResultKey': ['baz'], 'NotResultKey': 'a'},\n        ]\n        pages = self.paginator.paginate()\n        actual = pages.build_full_result()\n        self.assertEqual(pages.non_aggregate_part, {'NotResultKey': 'a'})\n        expected = {\n            'ResultKey': ['foo', 'bar', 'baz'],\n            'NotResultKey': 'a',\n        }\n        self.assertEqual(actual, expected)\n\n    def test_include_with_multiple_result_keys(self):\n        self.paginate_config['result_key'] = ['ResultKey1', 'ResultKey2']\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        self.method.side_effect = [\n            {\n                'ResultKey1': ['a', 'b'],\n                'ResultKey2': ['u', 'v'],\n                'NotResultKey': 'a',\n                'NextToken': 'token1',\n            },\n            {\n                'ResultKey1': ['c', 'd'],\n                'ResultKey2': ['w', 'x'],\n                'NotResultKey': 'a',\n                'NextToken': 'token2',\n            },\n            {\n                'ResultKey1': ['e', 'f'],\n                'ResultKey2': ['y', 'z'],\n                'NotResultKey': 'a',\n            },\n        ]\n        pages = self.paginator.paginate()\n        actual = pages.build_full_result()\n        expected = {\n            'ResultKey1': ['a', 'b', 'c', 'd', 'e', 'f'],\n            'ResultKey2': ['u', 'v', 'w', 'x', 'y', 'z'],\n            'NotResultKey': 'a',\n        }\n        self.assertEqual(actual, expected)\n\n    def test_include_with_nested_result_keys(self):\n        self.paginate_config['result_key'] = 'Result.Key'\n        self.paginate_config['non_aggregate_keys'] = [\n            'Outer',\n            'Result.Inner',\n        ]\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        self.method.side_effect = [\n            # The non result keys shows hypothetical\n            # example.  This doesn't actually happen,\n            # but in the case where the non result keys\n            # are different across pages, we use the values\n            # from the first page.\n            {\n                'Result': {'Key': ['foo'], 'Inner': 'v1'},\n                'Outer': 'v2',\n                'NextToken': 't1',\n            },\n            {\n                'Result': {'Key': ['bar', 'baz'], 'Inner': 'v3'},\n                'Outer': 'v4',\n                'NextToken': 't2',\n            },\n            {'Result': {'Key': ['qux'], 'Inner': 'v5'}, 'Outer': 'v6'},\n        ]\n        pages = self.paginator.paginate()\n        actual = pages.build_full_result()\n        self.assertEqual(\n            pages.non_aggregate_part,\n            {'Outer': 'v2', 'Result': {'Inner': 'v1'}},\n        )\n        expected = {\n            'Result': {'Key': ['foo', 'bar', 'baz', 'qux'], 'Inner': 'v1'},\n            'Outer': 'v2',\n        }\n        self.assertEqual(actual, expected)\n\n\nclass TestSearchOverResults(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n        self.paginate_config = {\n            'more_results': 'IsTruncated',\n            'output_token': 'NextToken',\n            'input_token': 'NextToken',\n            'result_key': 'Foo',\n        }\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n        responses = [\n            {\n                'Foo': [{'a': 1}, {'b': 2}],\n                'IsTruncated': True,\n                'NextToken': '1',\n            },\n            {\n                'Foo': [{'a': 3}, {'b': 4}],\n                'IsTruncated': True,\n                'NextToken': '2',\n            },\n            {'Foo': [{'a': 5}], 'IsTruncated': False, 'NextToken': '3'},\n        ]\n        self.method.side_effect = responses\n\n    def test_yields_non_list_values(self):\n        result = list(self.paginator.paginate().search('Foo[0].a'))\n        self.assertEqual([1, 3, 5], result)\n\n    def test_yields_individual_list_values(self):\n        result = list(self.paginator.paginate().search('Foo[].*[]'))\n        self.assertEqual([1, 2, 3, 4, 5], result)\n\n    def test_empty_when_no_match(self):\n        result = list(self.paginator.paginate().search('Foo[].qux'))\n        self.assertEqual([], result)\n\n    def test_no_yield_when_no_match_on_page(self):\n        result = list(self.paginator.paginate().search('Foo[].b'))\n        self.assertEqual([2, 4], result)\n\n\nclass TestDeprecatedStartingToken(unittest.TestCase):\n    def setUp(self):\n        self.method = mock.Mock()\n        self.model = mock.Mock()\n\n    def create_paginator(self, multiple_tokens=False):\n        if multiple_tokens:\n            paginator_config = {\n                \"output_token\": [\"Marker1\", \"Marker2\"],\n                \"input_token\": [\"InMarker1\", \"InMarker2\"],\n                \"result_key\": [\"Users\", \"Groups\"],\n            }\n        else:\n            paginator_config = {\n                'output_token': 'Marker',\n                'input_token': 'Marker',\n                'result_key': 'Users',\n            }\n        return Paginator(self.method, paginator_config, self.model)\n\n    def assert_pagination_result(\n        self, expected, pagination_config, multiple_tokens=False\n    ):\n        paginator = self.create_paginator(multiple_tokens)\n        try:\n            actual = paginator.paginate(\n                PaginationConfig=pagination_config\n            ).build_full_result()\n            self.assertEqual(actual, expected)\n        except ValueError:\n            self.fail(\"Deprecated paginator failed.\")\n\n    def test_deprecated_starting_token(self):\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m3\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        pagination_config = {'StartingToken': 'm1___0'}\n        expected = {'Users': ['User1', 'User2', 'User3']}\n        self.assert_pagination_result(expected, pagination_config)\n\n    def test_deprecated_multiple_starting_token(self):\n        responses = [\n            {\n                \"Users\": [\"User1\", \"User2\"],\n                \"Groups\": [\"Group1\"],\n                \"Marker1\": \"m1\",\n                \"Marker2\": \"m2\",\n            },\n            {\n                \"Users\": [\"User3\", \"User4\"],\n                \"Groups\": [\"Group2\"],\n                \"Marker1\": \"m3\",\n                \"Marker2\": \"m4\",\n            },\n            {\"Users\": [\"User5\"], \"Groups\": [\"Group3\"]},\n        ]\n        self.method.side_effect = responses\n        pagination_config = {'StartingToken': 'm0___m0___1'}\n        expected = {\n            'Groups': ['Group2', 'Group3'],\n            'Users': ['User2', 'User3', 'User4', 'User5'],\n        }\n        self.assert_pagination_result(\n            expected, pagination_config, multiple_tokens=True\n        )\n\n    def test_deprecated_starting_token_returns_new_style_next_token(self):\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m3\"},\n            {\"Users\": [\"User3\"], \"Marker\": \"m4\"},\n        ]\n        self.method.side_effect = responses\n        pagination_config = {'StartingToken': 'm1___0', 'MaxItems': 3}\n        expected = {\n            'Users': ['User1', 'User2', 'User3'],\n            'NextToken': encode_token({'Marker': 'm4'}),\n        }\n        self.assert_pagination_result(expected, pagination_config)\n\n    def test_deprecated_starting_token_without_all_input_set_to_none(self):\n        responses = [\n            {\n                \"Users\": [\"User1\", \"User2\"],\n                \"Groups\": [\"Group1\"],\n                \"Marker1\": \"m1\",\n                \"Marker2\": \"m2\",\n            },\n            {\n                \"Users\": [\"User3\", \"User4\"],\n                \"Groups\": [\"Group2\"],\n                \"Marker1\": \"m3\",\n                \"Marker2\": \"m4\",\n            },\n            {\"Users\": [\"User5\"], \"Groups\": [\"Group3\"]},\n        ]\n        self.method.side_effect = responses\n        pagination_config = {'StartingToken': 'm0'}\n        expected = {\n            'Groups': ['Group2', 'Group3'],\n            'Users': ['User1', 'User2', 'User3', 'User4', 'User5'],\n        }\n        self.assert_pagination_result(\n            expected, pagination_config, multiple_tokens=True\n        )\n\n    def test_deprecated_starting_token_rejects_too_many_input_tokens(self):\n        responses = [\n            {\"Users\": [\"User1\"], \"Marker\": \"m2\"},\n            {\"Users\": [\"User2\"], \"Marker\": \"m3\"},\n            {\"Users\": [\"User3\"]},\n        ]\n        self.method.side_effect = responses\n        pagination_config = {'StartingToken': 'm1___m4___0'}\n        expected = {'Users': ['User1', 'User2', 'User3']}\n\n        paginator = self.create_paginator()\n        with self.assertRaises(ValueError):\n            actual = paginator.paginate(\n                PaginationConfig=pagination_config\n            ).build_full_result()\n            self.assertEqual(actual, expected)\n\n\nclass TestStringPageSize(unittest.TestCase):\n    def setUp(self):\n        self.service_model = {\n            'metadata': {'protocol': 'query', 'endpointPrefix': 'prefix'},\n            'documentation': 'best service ever',\n            'operations': {\n                'ListStuff': {\n                    'name': 'ListStuff',\n                    'http': {'method': 'GET', 'requestUri': '/things'},\n                    'input': {'shape': 'ListStuffInputShape'},\n                    'output': {'shape': 'ListStuffOutputShape'},\n                    'errors': [],\n                    'documentation': 'Lists stuff',\n                }\n            },\n            'shapes': {\n                'String': {'type': 'string'},\n                'ListOfStuff': {'type': 'list', 'member': {'type': 'string'}},\n                'ListStuffInputShape': {\n                    'type': 'structure',\n                    'required': [],\n                    'members': {\n                        'NextToken': {'shape': 'String'},\n                        'MaxItems': {'shape': 'String'},\n                    },\n                },\n                'ListStuffOutputShape': {\n                    'type': 'structure',\n                    'required': [],\n                    'members': {\n                        'NextToken': {'shape': 'String'},\n                        'Stuff': {'shape': 'ListOfStuff'},\n                        'IsTruncated': {'type': 'boolean'},\n                    },\n                },\n            },\n        }\n        self.paginate_config = {\n            'input_token': 'NextToken',\n            'output_token': 'NextToken',\n            'limit_key': 'MaxItems',\n            'result_key': 'Stuff',\n        }\n        self.service = model.ServiceModel(self.service_model)\n        self.model = self.service.operation_model('ListStuff')\n        self.method = mock.Mock()\n        self.method.side_effect = [{}]\n        self.paginator = Paginator(\n            self.method, self.paginate_config, self.model\n        )\n\n    def test_int_page_size(self):\n        list(self.paginator.paginate(PaginationConfig={'PageSize': 1}))\n        self.method.assert_called_with(MaxItems='1')\n\n    def test_str_page_size(self):\n        list(self.paginator.paginate(PaginationConfig={'PageSize': '1'}))\n        self.method.assert_called_with(MaxItems='1')\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/unit/test_utils.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport datetime\nimport io\nimport operator\nfrom contextlib import contextmanager\nfrom sys import getrefcount\n\nimport pytest\nfrom dateutil.tz import tzoffset, tzutc\n\nimport botocore\nfrom botocore import xform_name\nfrom botocore.awsrequest import AWSRequest, HeadersDict\nfrom botocore.compat import json\nfrom botocore.config import Config\nfrom botocore.endpoint_provider import RuleSetEndpoint\nfrom botocore.exceptions import (\n    ClientError,\n    ConfigNotFound,\n    ConnectionClosedError,\n    ConnectTimeoutError,\n    InvalidDNSNameError,\n    InvalidExpressionError,\n    InvalidIMDSEndpointError,\n    InvalidIMDSEndpointModeError,\n    MetadataRetrievalError,\n    ReadTimeoutError,\n    SSOTokenLoadError,\n    UnsupportedOutpostResourceError,\n    UnsupportedS3AccesspointConfigurationError,\n    UnsupportedS3ArnError,\n)\nfrom botocore.model import (\n    DenormalizedStructureBuilder,\n    OperationModel,\n    ServiceModel,\n    ShapeResolver,\n)\nfrom botocore.regions import EndpointRulesetResolver\nfrom botocore.session import Session\nfrom botocore.utils import (\n    ArgumentGenerator,\n    ArnParser,\n    CachedProperty,\n    ContainerMetadataFetcher,\n    IMDSRegionProvider,\n    InstanceMetadataFetcher,\n    InstanceMetadataRegionFetcher,\n    InvalidArnException,\n    S3ArnParamHandler,\n    S3EndpointSetter,\n    S3RegionRedirectorv2,\n    SSOTokenLoader,\n    calculate_sha256,\n    calculate_tree_hash,\n    datetime2timestamp,\n    deep_merge,\n    determine_content_length,\n    ensure_boolean,\n    fix_s3_host,\n    get_encoding_from_headers,\n    get_service_module_name,\n    has_header,\n    instance_cache,\n    is_json_value_header,\n    is_s3_accelerate_url,\n    is_valid_endpoint_url,\n    is_valid_ipv6_endpoint_url,\n    is_valid_uri,\n    lowercase_dict,\n    lru_cache_weakref,\n    merge_dicts,\n    normalize_url_path,\n    parse_key_val_file,\n    parse_key_val_file_contents,\n    parse_timestamp,\n    parse_to_aware_datetime,\n    percent_encode,\n    percent_encode_sequence,\n    remove_dot_segments,\n    resolve_imds_endpoint_mode,\n    set_value_from_jmespath,\n    switch_host_s3_accelerate,\n    switch_to_virtual_host_style,\n    validate_jmespath_for_set,\n)\nfrom tests import FreezeTime, RawResponse, create_session, mock, unittest\n\nDATE = datetime.datetime(2021, 12, 10, 00, 00, 00)\nDT_FORMAT = \"%Y-%m-%dT%H:%M:%SZ\"\n\n\nclass TestEnsureBoolean(unittest.TestCase):\n    def test_boolean_true(self):\n        self.assertEqual(ensure_boolean(True), True)\n\n    def test_boolean_false(self):\n        self.assertEqual(ensure_boolean(False), False)\n\n    def test_string_true(self):\n        self.assertEqual(ensure_boolean('True'), True)\n\n    def test_string_false(self):\n        self.assertEqual(ensure_boolean('False'), False)\n\n    def test_string_lowercase_true(self):\n        self.assertEqual(ensure_boolean('true'), True)\n\n    def test_invalid_type_false(self):\n        self.assertEqual(ensure_boolean({'foo': 'bar'}), False)\n\n\nclass TestResolveIMDSEndpointMode(unittest.TestCase):\n    def create_session_with_config(self, endpoint_mode, imds_use_IPv6):\n        session = create_session()\n        session.set_config_variable(\n            'ec2_metadata_service_endpoint_mode', endpoint_mode\n        )\n        session.set_config_variable('imds_use_ipv6', imds_use_IPv6)\n        return session\n\n    def test_resolve_endpoint_mode_no_config(self):\n        session = self.create_session_with_config(None, None)\n        self.assertEqual(resolve_imds_endpoint_mode(session), 'ipv4')\n\n    def test_resolve_endpoint_mode_IPv6(self):\n        session = self.create_session_with_config('IPv6', None)\n        self.assertEqual(resolve_imds_endpoint_mode(session), 'ipv6')\n\n    def test_resolve_endpoint_mode_IPv4(self):\n        session = self.create_session_with_config('IPv4', None)\n        self.assertEqual(resolve_imds_endpoint_mode(session), 'ipv4')\n\n    def test_resolve_endpoint_mode_none_use_IPv6_true(self):\n        session = self.create_session_with_config(None, True)\n        self.assertEqual(resolve_imds_endpoint_mode(session), 'ipv6')\n\n    def test_resolve_endpoint_mode_none_use_IPv6_false(self):\n        session = self.create_session_with_config(None, False)\n        self.assertEqual(resolve_imds_endpoint_mode(session), 'ipv4')\n\n    def test_resolve_endpoint_mode_IPv6_use_IPv6_false(self):\n        session = self.create_session_with_config('IPv6', False)\n        self.assertEqual(resolve_imds_endpoint_mode(session), 'ipv6')\n\n    def test_resolve_endpoint_mode_IPv4_use_IPv6_true(self):\n        session = self.create_session_with_config('IPv4', True)\n        self.assertEqual(resolve_imds_endpoint_mode(session), 'ipv4')\n\n    def test_resolve_endpoint_mode_IPv6_use_IPv6_true(self):\n        session = self.create_session_with_config('IPv6', True)\n        self.assertEqual(resolve_imds_endpoint_mode(session), 'ipv6')\n\n    def test_resolve_endpoint_mode_IPv6_mixed_casing_use_IPv6_true(self):\n        session = self.create_session_with_config('iPv6', None)\n        self.assertEqual(resolve_imds_endpoint_mode(session), 'ipv6')\n\n    def test_resolve_endpoint_mode_invalid_input(self):\n        session = self.create_session_with_config('IPv3', True)\n        with self.assertRaises(InvalidIMDSEndpointModeError):\n            resolve_imds_endpoint_mode(session)\n\n\nclass TestIsJSONValueHeader(unittest.TestCase):\n    def test_no_serialization_section(self):\n        shape = mock.Mock()\n        shape.type_name = 'string'\n        self.assertFalse(is_json_value_header(shape))\n\n    def test_non_jsonvalue_shape(self):\n        shape = mock.Mock()\n        shape.serialization = {'location': 'header'}\n        shape.type_name = 'string'\n        self.assertFalse(is_json_value_header(shape))\n\n    def test_non_header_jsonvalue_shape(self):\n        shape = mock.Mock()\n        shape.serialization = {'jsonvalue': True}\n        shape.type_name = 'string'\n        self.assertFalse(is_json_value_header(shape))\n\n    def test_non_string_jsonvalue_shape(self):\n        shape = mock.Mock()\n        shape.serialization = {'location': 'header', 'jsonvalue': True}\n        shape.type_name = 'integer'\n        self.assertFalse(is_json_value_header(shape))\n\n    def test_json_value_header(self):\n        shape = mock.Mock()\n        shape.serialization = {'jsonvalue': True, 'location': 'header'}\n        shape.type_name = 'string'\n        self.assertTrue(is_json_value_header(shape))\n\n\nclass TestURINormalization(unittest.TestCase):\n    def test_remove_dot_segments(self):\n        self.assertEqual(remove_dot_segments('../foo'), 'foo')\n        self.assertEqual(remove_dot_segments('../../foo'), 'foo')\n        self.assertEqual(remove_dot_segments('./foo'), 'foo')\n        self.assertEqual(remove_dot_segments('/./'), '/')\n        self.assertEqual(remove_dot_segments('/../'), '/')\n        self.assertEqual(\n            remove_dot_segments('/foo/bar/baz/../qux'), '/foo/bar/qux'\n        )\n        self.assertEqual(remove_dot_segments('/foo/..'), '/')\n        self.assertEqual(remove_dot_segments('foo/bar/baz'), 'foo/bar/baz')\n        self.assertEqual(remove_dot_segments('..'), '')\n        self.assertEqual(remove_dot_segments('.'), '')\n        self.assertEqual(remove_dot_segments('/.'), '/')\n        self.assertEqual(remove_dot_segments('/.foo'), '/.foo')\n        self.assertEqual(remove_dot_segments('/..foo'), '/..foo')\n        self.assertEqual(remove_dot_segments(''), '')\n        self.assertEqual(remove_dot_segments('/a/b/c/./../../g'), '/a/g')\n        self.assertEqual(remove_dot_segments('mid/content=5/../6'), 'mid/6')\n        # I don't think this is RFC compliant...\n        self.assertEqual(remove_dot_segments('//foo//'), '/foo/')\n\n    def test_empty_url_normalization(self):\n        self.assertEqual(normalize_url_path(''), '/')\n\n\nclass TestTransformName(unittest.TestCase):\n    def test_upper_camel_case(self):\n        self.assertEqual(xform_name('UpperCamelCase'), 'upper_camel_case')\n        self.assertEqual(xform_name('UpperCamelCase', '-'), 'upper-camel-case')\n\n    def test_lower_camel_case(self):\n        self.assertEqual(xform_name('lowerCamelCase'), 'lower_camel_case')\n        self.assertEqual(xform_name('lowerCamelCase', '-'), 'lower-camel-case')\n\n    def test_consecutive_upper_case(self):\n        self.assertEqual(xform_name('HTTPHeaders'), 'http_headers')\n        self.assertEqual(xform_name('HTTPHeaders', '-'), 'http-headers')\n\n    def test_consecutive_upper_case_middle_string(self):\n        self.assertEqual(xform_name('MainHTTPHeaders'), 'main_http_headers')\n        self.assertEqual(\n            xform_name('MainHTTPHeaders', '-'), 'main-http-headers'\n        )\n\n    def test_s3_prefix(self):\n        self.assertEqual(xform_name('S3BucketName'), 's3_bucket_name')\n\n    def test_already_snake_cased(self):\n        self.assertEqual(xform_name('leave_alone'), 'leave_alone')\n        self.assertEqual(xform_name('s3_bucket_name'), 's3_bucket_name')\n        self.assertEqual(xform_name('bucket_s3_name'), 'bucket_s3_name')\n\n    def test_special_cases(self):\n        # Some patterns don't actually match the rules we expect.\n        self.assertEqual(\n            xform_name('SwapEnvironmentCNAMEs'), 'swap_environment_cnames'\n        )\n        self.assertEqual(\n            xform_name('SwapEnvironmentCNAMEs', '-'), 'swap-environment-cnames'\n        )\n        self.assertEqual(\n            xform_name('CreateCachediSCSIVolume', '-'),\n            'create-cached-iscsi-volume',\n        )\n        self.assertEqual(\n            xform_name('DescribeCachediSCSIVolumes', '-'),\n            'describe-cached-iscsi-volumes',\n        )\n        self.assertEqual(\n            xform_name('DescribeStorediSCSIVolumes', '-'),\n            'describe-stored-iscsi-volumes',\n        )\n        self.assertEqual(\n            xform_name('CreateStorediSCSIVolume', '-'),\n            'create-stored-iscsi-volume',\n        )\n        self.assertEqual(\n            xform_name('sourceServerIDs', '-'), 'source-server-ids'\n        )\n\n    def test_special_case_ends_with_s(self):\n        self.assertEqual(xform_name('GatewayARNs', '-'), 'gateway-arns')\n\n    def test_partial_rename(self):\n        transformed = xform_name('IPV6', '-')\n        self.assertEqual(transformed, 'ipv6')\n        transformed = xform_name('IPV6', '_')\n        self.assertEqual(transformed, 'ipv6')\n\n    def test_s3_partial_rename(self):\n        transformed = xform_name('s3Resources', '-')\n        self.assertEqual(transformed, 's3-resources')\n        transformed = xform_name('s3Resources', '_')\n        self.assertEqual(transformed, 's3_resources')\n\n\nclass TestValidateJMESPathForSet(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n        self.data = {\n            'Response': {\n                'Thing': {\n                    'Id': 1,\n                    'Name': 'Thing #1',\n                }\n            },\n            'Marker': 'some-token',\n        }\n\n    def test_invalid_exp(self):\n        with self.assertRaises(InvalidExpressionError):\n            validate_jmespath_for_set('Response.*.Name')\n\n        with self.assertRaises(InvalidExpressionError):\n            validate_jmespath_for_set('Response.Things[0]')\n\n        with self.assertRaises(InvalidExpressionError):\n            validate_jmespath_for_set('')\n\n        with self.assertRaises(InvalidExpressionError):\n            validate_jmespath_for_set('.')\n\n\nclass TestSetValueFromJMESPath(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n        self.data = {\n            'Response': {\n                'Thing': {\n                    'Id': 1,\n                    'Name': 'Thing #1',\n                }\n            },\n            'Marker': 'some-token',\n        }\n\n    def test_single_depth_existing(self):\n        set_value_from_jmespath(self.data, 'Marker', 'new-token')\n        self.assertEqual(self.data['Marker'], 'new-token')\n\n    def test_single_depth_new(self):\n        self.assertFalse('Limit' in self.data)\n        set_value_from_jmespath(self.data, 'Limit', 100)\n        self.assertEqual(self.data['Limit'], 100)\n\n    def test_multiple_depth_existing(self):\n        set_value_from_jmespath(self.data, 'Response.Thing.Name', 'New Name')\n        self.assertEqual(self.data['Response']['Thing']['Name'], 'New Name')\n\n    def test_multiple_depth_new(self):\n        self.assertFalse('Brand' in self.data)\n        set_value_from_jmespath(self.data, 'Brand.New', {'abc': 123})\n        self.assertEqual(self.data['Brand']['New']['abc'], 123)\n\n\nclass TestParseEC2CredentialsFile(unittest.TestCase):\n    def test_parse_ec2_content(self):\n        contents = \"AWSAccessKeyId=a\\nAWSSecretKey=b\\n\"\n        self.assertEqual(\n            parse_key_val_file_contents(contents),\n            {'AWSAccessKeyId': 'a', 'AWSSecretKey': 'b'},\n        )\n\n    def test_parse_ec2_content_empty(self):\n        contents = \"\"\n        self.assertEqual(parse_key_val_file_contents(contents), {})\n\n    def test_key_val_pair_with_blank_lines(self):\n        # The \\n\\n has an extra blank between the access/secret keys.\n        contents = \"AWSAccessKeyId=a\\n\\nAWSSecretKey=b\\n\"\n        self.assertEqual(\n            parse_key_val_file_contents(contents),\n            {'AWSAccessKeyId': 'a', 'AWSSecretKey': 'b'},\n        )\n\n    def test_key_val_parser_lenient(self):\n        # Ignore any line that does not have a '=' char in it.\n        contents = \"AWSAccessKeyId=a\\nNOTKEYVALLINE\\nAWSSecretKey=b\\n\"\n        self.assertEqual(\n            parse_key_val_file_contents(contents),\n            {'AWSAccessKeyId': 'a', 'AWSSecretKey': 'b'},\n        )\n\n    def test_multiple_equals_on_line(self):\n        contents = \"AWSAccessKeyId=a\\nAWSSecretKey=secret_key_with_equals=b\\n\"\n        self.assertEqual(\n            parse_key_val_file_contents(contents),\n            {\n                'AWSAccessKeyId': 'a',\n                'AWSSecretKey': 'secret_key_with_equals=b',\n            },\n        )\n\n    def test_os_error_raises_config_not_found(self):\n        mock_open = mock.Mock()\n        mock_open.side_effect = OSError()\n        with self.assertRaises(ConfigNotFound):\n            parse_key_val_file('badfile', _open=mock_open)\n\n\nclass TestParseTimestamps(unittest.TestCase):\n    def test_parse_iso8601(self):\n        self.assertEqual(\n            parse_timestamp('1970-01-01T00:10:00.000Z'),\n            datetime.datetime(1970, 1, 1, 0, 10, tzinfo=tzutc()),\n        )\n\n    def test_parse_epoch(self):\n        self.assertEqual(\n            parse_timestamp(1222172800),\n            datetime.datetime(2008, 9, 23, 12, 26, 40, tzinfo=tzutc()),\n        )\n\n    def test_parse_epoch_zero_time(self):\n        self.assertEqual(\n            parse_timestamp(0),\n            datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc()),\n        )\n\n    def test_parse_epoch_negative_time(self):\n        self.assertEqual(\n            parse_timestamp(-2208988800),\n            datetime.datetime(1900, 1, 1, 0, 0, 0, tzinfo=tzutc()),\n        )\n\n    def test_parse_epoch_beyond_2038(self):\n        self.assertEqual(\n            parse_timestamp(2524608000),\n            datetime.datetime(2050, 1, 1, 0, 0, 0, tzinfo=tzutc()),\n        )\n\n    def test_parse_epoch_as_string(self):\n        self.assertEqual(\n            parse_timestamp('1222172800'),\n            datetime.datetime(2008, 9, 23, 12, 26, 40, tzinfo=tzutc()),\n        )\n\n    def test_parse_rfc822(self):\n        self.assertEqual(\n            parse_timestamp('Wed, 02 Oct 2002 13:00:00 GMT'),\n            datetime.datetime(2002, 10, 2, 13, 0, tzinfo=tzutc()),\n        )\n\n    def test_parse_gmt_in_uk_time(self):\n        # In the UK the time switches from GMT to BST and back as part of\n        # their daylight savings time. time.tzname will therefore report\n        # both time zones. dateutil sees that the time zone is a local time\n        # zone and so parses it as local time, but it ends up being BST\n        # instead of GMT. To remedy this issue we can provide a time zone\n        # context which will enforce GMT == UTC.\n        with mock.patch('time.tzname', ('GMT', 'BST')):\n            self.assertEqual(\n                parse_timestamp('Wed, 02 Oct 2002 13:00:00 GMT'),\n                datetime.datetime(2002, 10, 2, 13, 0, tzinfo=tzutc()),\n            )\n\n    def test_parse_invalid_timestamp(self):\n        with self.assertRaises(ValueError):\n            parse_timestamp('invalid date')\n\n    def test_parse_timestamp_fails_with_bad_tzinfo(self):\n        mock_tzinfo = mock.Mock()\n        mock_tzinfo.__name__ = 'tzinfo'\n        mock_tzinfo.side_effect = OSError()\n        mock_get_tzinfo_options = mock.MagicMock(return_value=(mock_tzinfo,))\n\n        with mock.patch(\n            'botocore.utils.get_tzinfo_options', mock_get_tzinfo_options\n        ):\n            with self.assertRaises(RuntimeError):\n                parse_timestamp(0)\n\n    @contextmanager\n    def mocked_fromtimestamp_that_raises(self, exception_type):\n        class MockDatetime(datetime.datetime):\n            @classmethod\n            def fromtimestamp(cls, *args, **kwargs):\n                raise exception_type()\n\n        mock_fromtimestamp = mock.Mock()\n        mock_fromtimestamp.side_effect = OverflowError()\n\n        with mock.patch('datetime.datetime', MockDatetime):\n            yield\n\n    def test_parse_timestamp_succeeds_with_fromtimestamp_overflowerror(self):\n        # ``datetime.fromtimestamp()`` fails with OverflowError on some systems\n        # for timestamps beyond 2038. See\n        # https://docs.python.org/3/library/datetime.html#datetime.datetime.fromtimestamp\n        # This test mocks fromtimestamp() to always raise an OverflowError and\n        # checks that the fallback method returns the same time and timezone\n        # as fromtimestamp.\n        wout_fallback = parse_timestamp(0)\n        with self.mocked_fromtimestamp_that_raises(OverflowError):\n            with_fallback = parse_timestamp(0)\n            self.assertEqual(with_fallback, wout_fallback)\n            self.assertEqual(with_fallback.tzinfo, wout_fallback.tzinfo)\n\n    def test_parse_timestamp_succeeds_with_fromtimestamp_oserror(self):\n        # Same as test_parse_timestamp_succeeds_with_fromtimestamp_overflowerror\n        # but for systems where datetime.fromtimestamp() fails with OSerror for\n        # negative timestamps that represent times before 1970.\n        wout_fallback = parse_timestamp(0)\n        with self.mocked_fromtimestamp_that_raises(OSError):\n            with_fallback = parse_timestamp(0)\n            self.assertEqual(with_fallback, wout_fallback)\n            self.assertEqual(with_fallback.tzinfo, wout_fallback.tzinfo)\n\n\nclass TestDatetime2Timestamp(unittest.TestCase):\n    def test_datetime2timestamp_naive(self):\n        self.assertEqual(\n            datetime2timestamp(datetime.datetime(1970, 1, 2)), 86400\n        )\n\n    def test_datetime2timestamp_aware(self):\n        tzinfo = tzoffset(\"BRST\", -10800)\n        self.assertEqual(\n            datetime2timestamp(datetime.datetime(1970, 1, 2, tzinfo=tzinfo)),\n            97200,\n        )\n\n\nclass TestParseToUTCDatetime(unittest.TestCase):\n    def test_handles_utc_time(self):\n        original = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc())\n        self.assertEqual(parse_to_aware_datetime(original), original)\n\n    def test_handles_other_timezone(self):\n        tzinfo = tzoffset(\"BRST\", -10800)\n        original = datetime.datetime(2014, 1, 1, 0, 0, 0, tzinfo=tzinfo)\n        self.assertEqual(parse_to_aware_datetime(original), original)\n\n    def test_handles_naive_datetime(self):\n        original = datetime.datetime(1970, 1, 1, 0, 0, 0)\n        expected = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc())\n        self.assertEqual(parse_to_aware_datetime(original), expected)\n\n    def test_handles_string_epoch(self):\n        expected = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc())\n        self.assertEqual(parse_to_aware_datetime('0'), expected)\n\n    def test_handles_int_epoch(self):\n        expected = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc())\n        self.assertEqual(parse_to_aware_datetime(0), expected)\n\n    def test_handles_full_iso_8601(self):\n        expected = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc())\n        self.assertEqual(\n            parse_to_aware_datetime('1970-01-01T00:00:00Z'), expected\n        )\n\n    def test_year_only_iso_8601(self):\n        expected = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc())\n        self.assertEqual(parse_to_aware_datetime('1970-01-01'), expected)\n\n\nclass TestCachedProperty(unittest.TestCase):\n    def test_cached_property_same_value(self):\n        class CacheMe:\n            @CachedProperty\n            def foo(self):\n                return 'foo'\n\n        c = CacheMe()\n        self.assertEqual(c.foo, 'foo')\n        self.assertEqual(c.foo, 'foo')\n\n    def test_cached_property_only_called_once(self):\n        # Note: you would normally never want to cache\n        # a property that returns a new value each time,\n        # but this is done to demonstrate the caching behavior.\n\n        class NoIncrement:\n            def __init__(self):\n                self.counter = 0\n\n            @CachedProperty\n            def current_value(self):\n                self.counter += 1\n                return self.counter\n\n        c = NoIncrement()\n        self.assertEqual(c.current_value, 1)\n        # If the property wasn't cached, the next value should be\n        # be 2, but because it's cached, we know the value will be 1.\n        self.assertEqual(c.current_value, 1)\n\n\nclass TestArgumentGenerator(unittest.TestCase):\n    def setUp(self):\n        self.arg_generator = ArgumentGenerator()\n\n    def assert_skeleton_from_model_is(self, model, generated_skeleton):\n        shape = (\n            DenormalizedStructureBuilder().with_members(model).build_model()\n        )\n        actual = self.arg_generator.generate_skeleton(shape)\n        self.assertEqual(actual, generated_skeleton)\n\n    def test_generate_string(self):\n        self.assert_skeleton_from_model_is(\n            model={'A': {'type': 'string'}}, generated_skeleton={'A': ''}\n        )\n\n    def test_generate_string_enum(self):\n        enum_values = ['A', 'B', 'C']\n        model = {'A': {'type': 'string', 'enum': enum_values}}\n        shape = (\n            DenormalizedStructureBuilder().with_members(model).build_model()\n        )\n        actual = self.arg_generator.generate_skeleton(shape)\n\n        self.assertIn(actual['A'], enum_values)\n\n    def test_generate_scalars(self):\n        self.assert_skeleton_from_model_is(\n            model={\n                'A': {'type': 'string'},\n                'B': {'type': 'integer'},\n                'C': {'type': 'float'},\n                'D': {'type': 'boolean'},\n                'E': {'type': 'timestamp'},\n                'F': {'type': 'double'},\n            },\n            generated_skeleton={\n                'A': '',\n                'B': 0,\n                'C': 0.0,\n                'D': True,\n                'E': datetime.datetime(1970, 1, 1, 0, 0, 0),\n                'F': 0.0,\n            },\n        )\n\n    def test_will_use_member_names_for_string_values(self):\n        self.arg_generator = ArgumentGenerator(use_member_names=True)\n        self.assert_skeleton_from_model_is(\n            model={\n                'A': {'type': 'string'},\n                'B': {'type': 'integer'},\n                'C': {'type': 'float'},\n                'D': {'type': 'boolean'},\n            },\n            generated_skeleton={\n                'A': 'A',\n                'B': 0,\n                'C': 0.0,\n                'D': True,\n            },\n        )\n\n    def test_will_use_member_names_for_string_values_of_list(self):\n        self.arg_generator = ArgumentGenerator(use_member_names=True)\n        # We're not using assert_skeleton_from_model_is\n        # because we can't really control the name of strings shapes\n        # being used in the DenormalizedStructureBuilder. We can only\n        # control the name of structures and list shapes.\n        shape_map = ShapeResolver(\n            {\n                'InputShape': {\n                    'type': 'structure',\n                    'members': {\n                        'StringList': {'shape': 'StringList'},\n                    },\n                },\n                'StringList': {\n                    'type': 'list',\n                    'member': {'shape': 'StringType'},\n                },\n                'StringType': {\n                    'type': 'string',\n                },\n            }\n        )\n        shape = shape_map.get_shape_by_name('InputShape')\n        actual = self.arg_generator.generate_skeleton(shape)\n\n        expected = {'StringList': ['StringType']}\n        self.assertEqual(actual, expected)\n\n    def test_generate_nested_structure(self):\n        self.assert_skeleton_from_model_is(\n            model={\n                'A': {\n                    'type': 'structure',\n                    'members': {\n                        'B': {'type': 'string'},\n                    },\n                }\n            },\n            generated_skeleton={'A': {'B': ''}},\n        )\n\n    def test_generate_scalar_list(self):\n        self.assert_skeleton_from_model_is(\n            model={\n                'A': {'type': 'list', 'member': {'type': 'string'}},\n            },\n            generated_skeleton={\n                'A': [''],\n            },\n        )\n\n    def test_generate_scalar_map(self):\n        self.assert_skeleton_from_model_is(\n            model={\n                'A': {\n                    'type': 'map',\n                    'key': {'type': 'string'},\n                    'value': {'type': 'string'},\n                }\n            },\n            generated_skeleton={\n                'A': {\n                    'KeyName': '',\n                }\n            },\n        )\n\n    def test_handles_recursive_shapes(self):\n        # We're not using assert_skeleton_from_model_is\n        # because we can't use a DenormalizedStructureBuilder,\n        # we need a normalized model to represent recursive\n        # shapes.\n        shape_map = ShapeResolver(\n            {\n                'InputShape': {\n                    'type': 'structure',\n                    'members': {\n                        'A': {'shape': 'RecursiveStruct'},\n                        'B': {'shape': 'StringType'},\n                    },\n                },\n                'RecursiveStruct': {\n                    'type': 'structure',\n                    'members': {\n                        'C': {'shape': 'RecursiveStruct'},\n                        'D': {'shape': 'StringType'},\n                    },\n                },\n                'StringType': {\n                    'type': 'string',\n                },\n            }\n        )\n        shape = shape_map.get_shape_by_name('InputShape')\n        actual = self.arg_generator.generate_skeleton(shape)\n        expected = {\n            'A': {\n                'C': {\n                    # For recurisve shapes, we'll just show\n                    # an empty dict.\n                },\n                'D': '',\n            },\n            'B': '',\n        }\n        self.assertEqual(actual, expected)\n\n\nclass TestChecksums(unittest.TestCase):\n    def test_empty_hash(self):\n        self.assertEqual(\n            calculate_sha256(io.BytesIO(b''), as_hex=True),\n            'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855',\n        )\n\n    def test_as_hex(self):\n        self.assertEqual(\n            calculate_sha256(io.BytesIO(b'hello world'), as_hex=True),\n            'b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9',\n        )\n\n    def test_as_binary(self):\n        self.assertEqual(\n            calculate_sha256(io.BytesIO(b'hello world'), as_hex=False),\n            (\n                b\"\\xb9M'\\xb9\\x93M>\\x08\\xa5.R\\xd7\\xda}\\xab\\xfa\\xc4\\x84\\xef\"\n                b\"\\xe3zS\\x80\\xee\\x90\\x88\\xf7\\xac\\xe2\\xef\\xcd\\xe9\"\n            ),\n        )\n\n\nclass TestTreeHash(unittest.TestCase):\n    # Note that for these tests I've independently verified\n    # what the expected tree hashes should be from other\n    # SDK implementations.\n\n    def test_empty_tree_hash(self):\n        self.assertEqual(\n            calculate_tree_hash(io.BytesIO(b'')),\n            'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855',\n        )\n\n    def test_tree_hash_less_than_one_mb(self):\n        one_k = io.BytesIO(b'a' * 1024)\n        self.assertEqual(\n            calculate_tree_hash(one_k),\n            '2edc986847e209b4016e141a6dc8716d3207350f416969382d431539bf292e4a',\n        )\n\n    def test_tree_hash_exactly_one_mb(self):\n        one_meg_bytestring = b'a' * (1 * 1024 * 1024)\n        one_meg = io.BytesIO(one_meg_bytestring)\n        self.assertEqual(\n            calculate_tree_hash(one_meg),\n            '9bc1b2a288b26af7257a36277ae3816a7d4f16e89c1e7e77d0a5c48bad62b360',\n        )\n\n    def test_tree_hash_multiple_of_one_mb(self):\n        four_mb = io.BytesIO(b'a' * (4 * 1024 * 1024))\n        self.assertEqual(\n            calculate_tree_hash(four_mb),\n            '9491cb2ed1d4e7cd53215f4017c23ec4ad21d7050a1e6bb636c4f67e8cddb844',\n        )\n\n    def test_tree_hash_offset_of_one_mb_multiple(self):\n        offset_four_mb = io.BytesIO(b'a' * (4 * 1024 * 1024) + b'a' * 20)\n        self.assertEqual(\n            calculate_tree_hash(offset_four_mb),\n            '12f3cbd6101b981cde074039f6f728071da8879d6f632de8afc7cdf00661b08f',\n        )\n\n\nclass TestIsValidEndpointURL(unittest.TestCase):\n    def test_dns_name_is_valid(self):\n        self.assertTrue(is_valid_endpoint_url('https://s3.amazonaws.com/'))\n\n    def test_ip_address_is_allowed(self):\n        self.assertTrue(is_valid_endpoint_url('https://10.10.10.10/'))\n\n    def test_path_component_ignored(self):\n        self.assertTrue(\n            is_valid_endpoint_url('https://foo.bar.com/other/path/')\n        )\n\n    def test_can_have_port(self):\n        self.assertTrue(is_valid_endpoint_url('https://foo.bar.com:12345/'))\n\n    def test_ip_can_have_port(self):\n        self.assertTrue(is_valid_endpoint_url('https://10.10.10.10:12345/'))\n\n    def test_cannot_have_spaces(self):\n        self.assertFalse(is_valid_endpoint_url('https://my invalid name/'))\n\n    def test_missing_scheme(self):\n        self.assertFalse(is_valid_endpoint_url('foo.bar.com'))\n\n    def test_no_new_lines(self):\n        self.assertFalse(is_valid_endpoint_url('https://foo.bar.com\\nbar/'))\n\n    def test_long_hostname(self):\n        long_hostname = 'htps://%s.com' % ('a' * 256)\n        self.assertFalse(is_valid_endpoint_url(long_hostname))\n\n    def test_hostname_can_end_with_dot(self):\n        self.assertTrue(is_valid_endpoint_url('https://foo.bar.com./'))\n\n    def test_hostname_no_dots(self):\n        self.assertTrue(is_valid_endpoint_url('https://foo/'))\n\n\nclass TestFixS3Host(unittest.TestCase):\n    def test_fix_s3_host_initial(self):\n        request = AWSRequest(\n            method='PUT',\n            headers={},\n            url='https://s3-us-west-2.amazonaws.com/bucket/key.txt',\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3'\n        fix_s3_host(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        self.assertEqual(\n            request.url, 'https://bucket.s3-us-west-2.amazonaws.com/key.txt'\n        )\n        self.assertEqual(request.auth_path, '/bucket/key.txt')\n\n    def test_fix_s3_host_only_applied_once(self):\n        request = AWSRequest(\n            method='PUT',\n            headers={},\n            url='https://s3.us-west-2.amazonaws.com/bucket/key.txt',\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3'\n        fix_s3_host(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        # Calling the handler again should not affect the end result:\n        fix_s3_host(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        self.assertEqual(\n            request.url, 'https://bucket.s3.us-west-2.amazonaws.com/key.txt'\n        )\n        # This was a bug previously.  We want to make sure that\n        # calling fix_s3_host() again does not alter the auth_path.\n        # Otherwise we'll get signature errors.\n        self.assertEqual(request.auth_path, '/bucket/key.txt')\n\n    def test_dns_style_not_used_for_get_bucket_location(self):\n        original_url = 'https://s3-us-west-2.amazonaws.com/bucket?location'\n        request = AWSRequest(\n            method='GET',\n            headers={},\n            url=original_url,\n        )\n        signature_version = 's3'\n        region_name = 'us-west-2'\n        fix_s3_host(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        # The request url should not have been modified because this is\n        # a request for GetBucketLocation.\n        self.assertEqual(request.url, original_url)\n\n    def test_can_provide_default_endpoint_url(self):\n        request = AWSRequest(\n            method='PUT',\n            headers={},\n            url='https://s3-us-west-2.amazonaws.com/bucket/key.txt',\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3'\n        fix_s3_host(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n            default_endpoint_url='foo.s3.amazonaws.com',\n        )\n        self.assertEqual(\n            request.url, 'https://bucket.foo.s3.amazonaws.com/key.txt'\n        )\n\n    def test_no_endpoint_url_uses_request_url(self):\n        request = AWSRequest(\n            method='PUT',\n            headers={},\n            url='https://s3-us-west-2.amazonaws.com/bucket/key.txt',\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3'\n        fix_s3_host(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n            # A value of None means use the url in the current request.\n            default_endpoint_url=None,\n        )\n        self.assertEqual(\n            request.url, 'https://bucket.s3-us-west-2.amazonaws.com/key.txt'\n        )\n\n\nclass TestSwitchToVirtualHostStyle(unittest.TestCase):\n    def test_switch_to_virtual_host_style(self):\n        request = AWSRequest(\n            method='PUT',\n            headers={},\n            url='https://foo.amazonaws.com/bucket/key.txt',\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3'\n        switch_to_virtual_host_style(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        self.assertEqual(\n            request.url, 'https://bucket.foo.amazonaws.com/key.txt'\n        )\n        self.assertEqual(request.auth_path, '/bucket/key.txt')\n\n    def test_uses_default_endpoint(self):\n        request = AWSRequest(\n            method='PUT',\n            headers={},\n            url='https://foo.amazonaws.com/bucket/key.txt',\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3'\n        switch_to_virtual_host_style(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n            default_endpoint_url='s3.amazonaws.com',\n        )\n        self.assertEqual(\n            request.url, 'https://bucket.s3.amazonaws.com/key.txt'\n        )\n        self.assertEqual(request.auth_path, '/bucket/key.txt')\n\n    def test_throws_invalid_dns_name_error(self):\n        request = AWSRequest(\n            method='PUT',\n            headers={},\n            url='https://foo.amazonaws.com/mybucket.foo/key.txt',\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3'\n        with self.assertRaises(InvalidDNSNameError):\n            switch_to_virtual_host_style(\n                request=request,\n                signature_version=signature_version,\n                region_name=region_name,\n            )\n\n    def test_fix_s3_host_only_applied_once(self):\n        request = AWSRequest(\n            method='PUT',\n            headers={},\n            url='https://foo.amazonaws.com/bucket/key.txt',\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3'\n        switch_to_virtual_host_style(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        # Calling the handler again should not affect the end result:\n        switch_to_virtual_host_style(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        self.assertEqual(\n            request.url, 'https://bucket.foo.amazonaws.com/key.txt'\n        )\n        # This was a bug previously.  We want to make sure that\n        # calling fix_s3_host() again does not alter the auth_path.\n        # Otherwise we'll get signature errors.\n        self.assertEqual(request.auth_path, '/bucket/key.txt')\n\n    def test_virtual_host_style_for_make_bucket(self):\n        request = AWSRequest(\n            method='PUT', headers={}, url='https://foo.amazonaws.com/bucket'\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3'\n        switch_to_virtual_host_style(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        self.assertEqual(request.url, 'https://bucket.foo.amazonaws.com/')\n\n    def test_virtual_host_style_not_used_for_get_bucket_location(self):\n        original_url = 'https://foo.amazonaws.com/bucket?location'\n        request = AWSRequest(\n            method='GET',\n            headers={},\n            url=original_url,\n        )\n        signature_version = 's3'\n        region_name = 'us-west-2'\n        switch_to_virtual_host_style(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        # The request url should not have been modified because this is\n        # a request for GetBucketLocation.\n        self.assertEqual(request.url, original_url)\n\n    def test_virtual_host_style_not_used_for_list_buckets(self):\n        original_url = 'https://foo.amazonaws.com/'\n        request = AWSRequest(\n            method='GET',\n            headers={},\n            url=original_url,\n        )\n        signature_version = 's3'\n        region_name = 'us-west-2'\n        switch_to_virtual_host_style(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n        )\n        # The request url should not have been modified because this is\n        # a request for GetBucketLocation.\n        self.assertEqual(request.url, original_url)\n\n    def test_is_unaffected_by_sigv4(self):\n        request = AWSRequest(\n            method='PUT',\n            headers={},\n            url='https://foo.amazonaws.com/bucket/key.txt',\n        )\n        region_name = 'us-west-2'\n        signature_version = 's3v4'\n        switch_to_virtual_host_style(\n            request=request,\n            signature_version=signature_version,\n            region_name=region_name,\n            default_endpoint_url='s3.amazonaws.com',\n        )\n        self.assertEqual(\n            request.url, 'https://bucket.s3.amazonaws.com/key.txt'\n        )\n\n\ndef test_chunked_encoding_used_for_stream_like_object():\n    class BufferedStream(io.BufferedIOBase):\n        \"\"\"Class to ensure seek/tell don't work, but read is implemented.\"\"\"\n\n        def __init__(self, value):\n            self.value = io.BytesIO(value)\n\n        def read(self, size=-1):\n            return self.value.read(size)\n\n    request = AWSRequest(\n        method='POST',\n        headers={},\n        data=BufferedStream(b\"some initial binary data\"),\n        url='https://foo.amazonaws.com/bucket/key.txt',\n    )\n    prepared_request = request.prepare()\n    assert prepared_request.headers == {'Transfer-Encoding': 'chunked'}\n\n\nclass TestInstanceCache(unittest.TestCase):\n    class DummyClass:\n        def __init__(self, cache):\n            self._instance_cache = cache\n\n        @instance_cache\n        def add(self, x, y):\n            return x + y\n\n        @instance_cache\n        def sub(self, x, y):\n            return x - y\n\n    def setUp(self):\n        self.cache = {}\n\n    def test_cache_single_method_call(self):\n        adder = self.DummyClass(self.cache)\n        self.assertEqual(adder.add(2, 1), 3)\n        # This should result in one entry in the cache.\n        self.assertEqual(len(self.cache), 1)\n        # When we call the method with the same args,\n        # we should reuse the same entry in the cache.\n        self.assertEqual(adder.add(2, 1), 3)\n        self.assertEqual(len(self.cache), 1)\n\n    def test_can_cache_multiple_methods(self):\n        adder = self.DummyClass(self.cache)\n        adder.add(2, 1)\n\n        # A different method results in a new cache entry,\n        # so now there should be two elements in the cache.\n        self.assertEqual(adder.sub(2, 1), 1)\n        self.assertEqual(len(self.cache), 2)\n        self.assertEqual(adder.sub(2, 1), 1)\n\n    def test_can_cache_kwargs(self):\n        adder = self.DummyClass(self.cache)\n        adder.add(x=2, y=1)\n        self.assertEqual(adder.add(x=2, y=1), 3)\n        self.assertEqual(len(self.cache), 1)\n\n\nclass TestMergeDicts(unittest.TestCase):\n    def test_merge_dicts_overrides(self):\n        first = {\n            'foo': {'bar': {'baz': {'one': 'ORIGINAL', 'two': 'ORIGINAL'}}}\n        }\n        second = {'foo': {'bar': {'baz': {'one': 'UPDATE'}}}}\n\n        merge_dicts(first, second)\n        # The value from the second dict wins.\n        self.assertEqual(first['foo']['bar']['baz']['one'], 'UPDATE')\n        # And we still preserve the other attributes.\n        self.assertEqual(first['foo']['bar']['baz']['two'], 'ORIGINAL')\n\n    def test_merge_dicts_new_keys(self):\n        first = {\n            'foo': {'bar': {'baz': {'one': 'ORIGINAL', 'two': 'ORIGINAL'}}}\n        }\n        second = {'foo': {'bar': {'baz': {'three': 'UPDATE'}}}}\n\n        merge_dicts(first, second)\n        self.assertEqual(first['foo']['bar']['baz']['one'], 'ORIGINAL')\n        self.assertEqual(first['foo']['bar']['baz']['two'], 'ORIGINAL')\n        self.assertEqual(first['foo']['bar']['baz']['three'], 'UPDATE')\n\n    def test_merge_empty_dict_does_nothing(self):\n        first = {'foo': {'bar': 'baz'}}\n        merge_dicts(first, {})\n        self.assertEqual(first, {'foo': {'bar': 'baz'}})\n\n    def test_more_than_one_sub_dict(self):\n        first = {\n            'one': {'inner': 'ORIGINAL', 'inner2': 'ORIGINAL'},\n            'two': {'inner': 'ORIGINAL', 'inner2': 'ORIGINAL'},\n        }\n        second = {'one': {'inner': 'UPDATE'}, 'two': {'inner': 'UPDATE'}}\n\n        merge_dicts(first, second)\n        self.assertEqual(first['one']['inner'], 'UPDATE')\n        self.assertEqual(first['one']['inner2'], 'ORIGINAL')\n\n        self.assertEqual(first['two']['inner'], 'UPDATE')\n        self.assertEqual(first['two']['inner2'], 'ORIGINAL')\n\n    def test_new_keys(self):\n        first = {'one': {'inner': 'ORIGINAL'}, 'two': {'inner': 'ORIGINAL'}}\n        second = {'three': {'foo': {'bar': 'baz'}}}\n        # In this case, second has no keys in common, but we'd still expect\n        # this to get merged.\n        merge_dicts(first, second)\n        self.assertEqual(first['three']['foo']['bar'], 'baz')\n\n    def test_list_values_no_append(self):\n        dict1 = {'Foo': ['old_foo_value']}\n        dict2 = {'Foo': ['new_foo_value']}\n        merge_dicts(dict1, dict2)\n        self.assertEqual(dict1, {'Foo': ['new_foo_value']})\n\n    def test_list_values_append(self):\n        dict1 = {'Foo': ['old_foo_value']}\n        dict2 = {'Foo': ['new_foo_value']}\n        merge_dicts(dict1, dict2, append_lists=True)\n        self.assertEqual(dict1, {'Foo': ['old_foo_value', 'new_foo_value']})\n\n    def test_list_values_mismatching_types(self):\n        dict1 = {'Foo': 'old_foo_value'}\n        dict2 = {'Foo': ['new_foo_value']}\n        merge_dicts(dict1, dict2, append_lists=True)\n        self.assertEqual(dict1, {'Foo': ['new_foo_value']})\n\n    def test_list_values_missing_key(self):\n        dict1 = {}\n        dict2 = {'Foo': ['foo_value']}\n        merge_dicts(dict1, dict2, append_lists=True)\n        self.assertEqual(dict1, {'Foo': ['foo_value']})\n\n\nclass TestLowercaseDict(unittest.TestCase):\n    def test_lowercase_dict_empty(self):\n        original = {}\n        copy = lowercase_dict(original)\n        self.assertEqual(original, copy)\n\n    def test_lowercase_dict_original_keys_lower(self):\n        original = {\n            'lower_key1': 1,\n            'lower_key2': 2,\n        }\n        copy = lowercase_dict(original)\n        self.assertEqual(original, copy)\n\n    def test_lowercase_dict_original_keys_mixed(self):\n        original = {\n            'SOME_KEY': 'value',\n            'AnOTher_OnE': 'anothervalue',\n        }\n        copy = lowercase_dict(original)\n        expected = {\n            'some_key': 'value',\n            'another_one': 'anothervalue',\n        }\n        self.assertEqual(expected, copy)\n\n\nclass TestGetServiceModuleName(unittest.TestCase):\n    def setUp(self):\n        self.service_description = {\n            'metadata': {\n                'serviceFullName': 'AWS MyService',\n                'apiVersion': '2014-01-01',\n                'endpointPrefix': 'myservice',\n                'signatureVersion': 'v4',\n                'protocol': 'query',\n            },\n            'operations': {},\n            'shapes': {},\n        }\n        self.service_model = ServiceModel(\n            self.service_description, 'myservice'\n        )\n\n    def test_default(self):\n        self.assertEqual(\n            get_service_module_name(self.service_model), 'MyService'\n        )\n\n    def test_client_name_with_amazon(self):\n        self.service_description['metadata'][\n            'serviceFullName'\n        ] = 'Amazon MyService'\n        self.assertEqual(\n            get_service_module_name(self.service_model), 'MyService'\n        )\n\n    def test_client_name_using_abreviation(self):\n        self.service_description['metadata'][\n            'serviceAbbreviation'\n        ] = 'Abbreviation'\n        self.assertEqual(\n            get_service_module_name(self.service_model), 'Abbreviation'\n        )\n\n    def test_client_name_with_non_alphabet_characters(self):\n        self.service_description['metadata'][\n            'serviceFullName'\n        ] = 'Amazon My-Service'\n        self.assertEqual(\n            get_service_module_name(self.service_model), 'MyService'\n        )\n\n    def test_client_name_with_no_full_name_or_abbreviation(self):\n        del self.service_description['metadata']['serviceFullName']\n        self.assertEqual(\n            get_service_module_name(self.service_model), 'myservice'\n        )\n\n\nclass TestPercentEncodeSequence(unittest.TestCase):\n    def test_percent_encode_empty(self):\n        self.assertEqual(percent_encode_sequence({}), '')\n\n    def test_percent_encode_special_chars(self):\n        self.assertEqual(\n            percent_encode_sequence({'k1': 'with spaces++/'}),\n            'k1=with%20spaces%2B%2B%2F',\n        )\n\n    def test_percent_encode_string_string_tuples(self):\n        self.assertEqual(\n            percent_encode_sequence([('k1', 'v1'), ('k2', 'v2')]),\n            'k1=v1&k2=v2',\n        )\n\n    def test_percent_encode_dict_single_pair(self):\n        self.assertEqual(percent_encode_sequence({'k1': 'v1'}), 'k1=v1')\n\n    def test_percent_encode_dict_string_string(self):\n        self.assertEqual(\n            percent_encode_sequence({'k1': 'v1', 'k2': 'v2'}), 'k1=v1&k2=v2'\n        )\n\n    def test_percent_encode_single_list_of_values(self):\n        self.assertEqual(\n            percent_encode_sequence({'k1': ['a', 'b', 'c']}), 'k1=a&k1=b&k1=c'\n        )\n\n    def test_percent_encode_list_values_of_string(self):\n        self.assertEqual(\n            percent_encode_sequence(\n                {'k1': ['a', 'list'], 'k2': ['another', 'list']}\n            ),\n            'k1=a&k1=list&k2=another&k2=list',\n        )\n\n\nclass TestPercentEncode(unittest.TestCase):\n    def test_percent_encode_obj(self):\n        self.assertEqual(percent_encode(1), '1')\n\n    def test_percent_encode_text(self):\n        self.assertEqual(percent_encode(''), '')\n        self.assertEqual(percent_encode('a'), 'a')\n        self.assertEqual(percent_encode('\\u0000'), '%00')\n        # Codepoint > 0x7f\n        self.assertEqual(percent_encode('\\u2603'), '%E2%98%83')\n        # Codepoint > 0xffff\n        self.assertEqual(percent_encode('\\U0001f32e'), '%F0%9F%8C%AE')\n\n    def test_percent_encode_bytes(self):\n        self.assertEqual(percent_encode(b''), '')\n        self.assertEqual(percent_encode(b'a'), 'a')\n        self.assertEqual(percent_encode(b'\\x00'), '%00')\n        # UTF-8 Snowman\n        self.assertEqual(percent_encode(b'\\xe2\\x98\\x83'), '%E2%98%83')\n        # Arbitrary bytes (not valid UTF-8).\n        self.assertEqual(percent_encode(b'\\x80\\x00'), '%80%00')\n\n\nclass TestSwitchHostS3Accelerate(unittest.TestCase):\n    def setUp(self):\n        self.original_url = 'https://s3.amazonaws.com/foo/key.txt'\n        self.request = AWSRequest(\n            method='PUT', headers={}, url=self.original_url\n        )\n        self.client_config = Config()\n        self.request.context['client_config'] = self.client_config\n\n    def test_switch_host(self):\n        switch_host_s3_accelerate(self.request, 'PutObject')\n        self.assertEqual(\n            self.request.url, 'https://s3-accelerate.amazonaws.com/foo/key.txt'\n        )\n\n    def test_do_not_switch_black_listed_operations(self):\n        # It should not get switched for ListBuckets, DeleteBucket, and\n        # CreateBucket\n        blacklist_ops = ['ListBuckets', 'DeleteBucket', 'CreateBucket']\n        for op_name in blacklist_ops:\n            switch_host_s3_accelerate(self.request, op_name)\n            self.assertEqual(self.request.url, self.original_url)\n\n    def test_uses_original_endpoint_scheme(self):\n        self.request.url = 'http://s3.amazonaws.com/foo/key.txt'\n        switch_host_s3_accelerate(self.request, 'PutObject')\n        self.assertEqual(\n            self.request.url, 'http://s3-accelerate.amazonaws.com/foo/key.txt'\n        )\n\n    def test_uses_dualstack(self):\n        self.client_config.s3 = {'use_dualstack_endpoint': True}\n        self.original_url = 'https://s3.dualstack.amazonaws.com/foo/key.txt'\n        self.request = AWSRequest(\n            method='PUT', headers={}, url=self.original_url\n        )\n        self.request.context['client_config'] = self.client_config\n        switch_host_s3_accelerate(self.request, 'PutObject')\n        self.assertEqual(\n            self.request.url,\n            'https://s3-accelerate.dualstack.amazonaws.com/foo/key.txt',\n        )\n\n\nclass TestDeepMerge(unittest.TestCase):\n    def test_simple_merge(self):\n        a = {'key': 'value'}\n        b = {'otherkey': 'othervalue'}\n        deep_merge(a, b)\n\n        expected = {'key': 'value', 'otherkey': 'othervalue'}\n        self.assertEqual(a, expected)\n\n    def test_merge_list(self):\n        # Lists are treated as opaque data and so no effort should be made to\n        # combine them.\n        a = {'key': ['original']}\n        b = {'key': ['new']}\n        deep_merge(a, b)\n        self.assertEqual(a, {'key': ['new']})\n\n    def test_merge_number(self):\n        # The value from b is always taken\n        a = {'key': 10}\n        b = {'key': 45}\n        deep_merge(a, b)\n        self.assertEqual(a, {'key': 45})\n\n        a = {'key': 45}\n        b = {'key': 10}\n        deep_merge(a, b)\n        self.assertEqual(a, {'key': 10})\n\n    def test_merge_boolean(self):\n        # The value from b is always taken\n        a = {'key': False}\n        b = {'key': True}\n        deep_merge(a, b)\n        self.assertEqual(a, {'key': True})\n\n        a = {'key': True}\n        b = {'key': False}\n        deep_merge(a, b)\n        self.assertEqual(a, {'key': False})\n\n    def test_merge_string(self):\n        a = {'key': 'value'}\n        b = {'key': 'othervalue'}\n        deep_merge(a, b)\n        self.assertEqual(a, {'key': 'othervalue'})\n\n    def test_merge_overrides_value(self):\n        # The value from b is always taken, even when it's a different type\n        a = {'key': 'original'}\n        b = {'key': {'newkey': 'newvalue'}}\n        deep_merge(a, b)\n        self.assertEqual(a, {'key': {'newkey': 'newvalue'}})\n\n        a = {'key': {'anotherkey': 'value'}}\n        b = {'key': 'newvalue'}\n        deep_merge(a, b)\n        self.assertEqual(a, {'key': 'newvalue'})\n\n    def test_deep_merge(self):\n        a = {\n            'first': {\n                'second': {'key': 'value', 'otherkey': 'othervalue'},\n                'key': 'value',\n            }\n        }\n        b = {\n            'first': {\n                'second': {\n                    'otherkey': 'newvalue',\n                    'yetanotherkey': 'yetanothervalue',\n                }\n            }\n        }\n        deep_merge(a, b)\n\n        expected = {\n            'first': {\n                'second': {\n                    'key': 'value',\n                    'otherkey': 'newvalue',\n                    'yetanotherkey': 'yetanothervalue',\n                },\n                'key': 'value',\n            }\n        }\n        self.assertEqual(a, expected)\n\n\nclass TestS3RegionRedirector(unittest.TestCase):\n    def setUp(self):\n        self.client = mock.Mock()\n        self.client._ruleset_resolver = EndpointRulesetResolver(\n            endpoint_ruleset_data={\n                'version': '1.0',\n                'parameters': {},\n                'rules': [],\n            },\n            partition_data={},\n            service_model=None,\n            builtins={},\n            client_context=None,\n            event_emitter=None,\n            use_ssl=True,\n            requested_auth_scheme=None,\n        )\n        self.client._ruleset_resolver.construct_endpoint = mock.Mock(\n            return_value=RuleSetEndpoint(\n                url='https://new-endpoint.amazonaws.com',\n                properties={},\n                headers={},\n            )\n        )\n        self.cache = {}\n        self.redirector = S3RegionRedirectorv2(None, self.client)\n        self.set_client_response_headers({})\n        self.operation = mock.Mock()\n        self.operation.name = 'foo'\n\n    def set_client_response_headers(self, headers):\n        error_response = ClientError(\n            {\n                'Error': {'Code': '', 'Message': ''},\n                'ResponseMetadata': {'HTTPHeaders': headers},\n            },\n            'HeadBucket',\n        )\n        success_response = {'ResponseMetadata': {'HTTPHeaders': headers}}\n        self.client.head_bucket.side_effect = [\n            error_response,\n            success_response,\n        ]\n\n    def test_set_request_url(self):\n        old_url = 'https://us-west-2.amazonaws.com/foo'\n        new_endpoint = 'https://eu-central-1.amazonaws.com'\n        new_url = self.redirector.set_request_url(old_url, new_endpoint)\n        self.assertEqual(new_url, 'https://eu-central-1.amazonaws.com/foo')\n\n    def test_set_request_url_keeps_old_scheme(self):\n        old_url = 'http://us-west-2.amazonaws.com/foo'\n        new_endpoint = 'https://eu-central-1.amazonaws.com'\n        new_url = self.redirector.set_request_url(old_url, new_endpoint)\n        self.assertEqual(new_url, 'http://eu-central-1.amazonaws.com/foo')\n\n    def test_sets_signing_context_from_cache(self):\n        self.cache['foo'] = 'new-region-1'\n        self.redirector = S3RegionRedirectorv2(\n            None, self.client, cache=self.cache\n        )\n        params = {'Bucket': 'foo'}\n        builtins = {'AWS::Region': 'old-region-1'}\n        self.redirector.redirect_from_cache(builtins, params)\n        self.assertEqual(builtins.get('AWS::Region'), 'new-region-1')\n\n    def test_only_changes_context_if_bucket_in_cache(self):\n        self.cache['foo'] = 'new-region-1'\n        self.redirector = S3RegionRedirectorv2(\n            None, self.client, cache=self.cache\n        )\n        params = {'Bucket': 'bar'}\n        builtins = {'AWS::Region': 'old-region-1'}\n        self.redirector.redirect_from_cache(builtins, params)\n        self.assertEqual(builtins.get('AWS::Region'), 'old-region-1')\n\n    def test_redirect_from_error(self):\n        request_dict = {\n            'context': {\n                's3_redirect': {\n                    'bucket': 'foo',\n                    'redirected': False,\n                    'params': {'Bucket': 'foo'},\n                },\n                'signing': {\n                    'region': 'us-west-2',\n                },\n            },\n            'url': 'https://us-west-2.amazonaws.com/foo',\n        }\n        response = (\n            None,\n            {\n                'Error': {\n                    'Code': 'PermanentRedirect',\n                    'Endpoint': 'foo.eu-central-1.amazonaws.com',\n                    'Bucket': 'foo',\n                },\n                'ResponseMetadata': {\n                    'HTTPHeaders': {'x-amz-bucket-region': 'eu-central-1'}\n                },\n            },\n        )\n\n        self.client._ruleset_resolver.construct_endpoint.return_value = (\n            RuleSetEndpoint(\n                url='https://eu-central-1.amazonaws.com/foo',\n                properties={\n                    'authSchemes': [\n                        {\n                            'name': 'sigv4',\n                            'signingRegion': 'eu-central-1',\n                            'disableDoubleEncoding': True,\n                        }\n                    ]\n                },\n                headers={},\n            )\n        )\n\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n\n        # The response needs to be 0 so that there is no retry delay\n        self.assertEqual(redirect_response, 0)\n\n        self.assertEqual(\n            request_dict['url'], 'https://eu-central-1.amazonaws.com/foo'\n        )\n\n        expected_signing_context = {\n            'region': 'eu-central-1',\n            'disableDoubleEncoding': True,\n        }\n        signing_context = request_dict['context'].get('signing')\n        self.assertEqual(signing_context, expected_signing_context)\n        self.assertTrue(\n            request_dict['context']['s3_redirect'].get('redirected')\n        )\n\n    def test_does_not_redirect_if_previously_redirected(self):\n        request_dict = {\n            'context': {\n                'signing': {'bucket': 'foo', 'region': 'us-west-2'},\n                's3_redirected': True,\n            },\n            'url': 'https://us-west-2.amazonaws.com/foo',\n        }\n        response = (\n            None,\n            {\n                'Error': {\n                    'Code': '400',\n                    'Message': 'Bad Request',\n                },\n                'ResponseMetadata': {\n                    'HTTPHeaders': {'x-amz-bucket-region': 'us-west-2'}\n                },\n            },\n        )\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        self.assertIsNone(redirect_response)\n\n    def test_does_not_redirect_unless_permanentredirect_recieved(self):\n        request_dict = {}\n        response = (None, {})\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        self.assertIsNone(redirect_response)\n        self.assertEqual(request_dict, {})\n\n    def test_does_not_redirect_if_region_cannot_be_found(self):\n        request_dict = {\n            'url': 'https://us-west-2.amazonaws.com/foo',\n            'context': {\n                's3_redirect': {\n                    'bucket': 'foo',\n                    'redirected': False,\n                    'params': {'Bucket': 'foo'},\n                },\n                'signing': {},\n            },\n        }\n        response = (\n            None,\n            {\n                'Error': {\n                    'Code': 'PermanentRedirect',\n                    'Endpoint': 'foo.eu-central-1.amazonaws.com',\n                    'Bucket': 'foo',\n                },\n                'ResponseMetadata': {'HTTPHeaders': {}},\n            },\n        )\n\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n\n        self.assertIsNone(redirect_response)\n\n    def test_redirects_301(self):\n        request_dict = {\n            'url': 'https://us-west-2.amazonaws.com/foo',\n            'context': {\n                's3_redirect': {\n                    'bucket': 'foo',\n                    'redirected': False,\n                    'params': {'Bucket': 'foo'},\n                },\n                'signing': {},\n            },\n        }\n        response = (\n            None,\n            {\n                'Error': {'Code': '301', 'Message': 'Moved Permanently'},\n                'ResponseMetadata': {\n                    'HTTPHeaders': {'x-amz-bucket-region': 'eu-central-1'}\n                },\n            },\n        )\n\n        self.operation.name = 'HeadObject'\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        self.assertEqual(redirect_response, 0)\n\n        self.operation.name = 'ListObjects'\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        self.assertIsNone(redirect_response)\n\n    def test_redirects_400_head_bucket(self):\n        request_dict = {\n            'url': 'https://us-west-2.amazonaws.com/foo',\n            'context': {\n                's3_redirect': {\n                    'bucket': 'foo',\n                    'redirected': False,\n                    'params': {'Bucket': 'foo'},\n                },\n                'signing': {},\n            },\n        }\n        response = (\n            None,\n            {\n                'Error': {'Code': '400', 'Message': 'Bad Request'},\n                'ResponseMetadata': {\n                    'HTTPHeaders': {'x-amz-bucket-region': 'eu-central-1'}\n                },\n            },\n        )\n\n        self.operation.name = 'HeadObject'\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        self.assertEqual(redirect_response, 0)\n\n        self.operation.name = 'ListObjects'\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        self.assertIsNone(redirect_response)\n\n    def test_does_not_redirect_400_head_bucket_no_region_header(self):\n        # We should not redirect a 400 Head* if the region header is not\n        # present as this will lead to infinitely calling HeadBucket.\n        request_dict = {\n            'url': 'https://us-west-2.amazonaws.com/foo',\n            'context': {'signing': {'bucket': 'foo'}},\n        }\n        response = (\n            None,\n            {\n                'Error': {'Code': '400', 'Message': 'Bad Request'},\n                'ResponseMetadata': {'HTTPHeaders': {}},\n            },\n        )\n\n        self.operation.name = 'HeadBucket'\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        head_bucket_calls = self.client.head_bucket.call_count\n        self.assertIsNone(redirect_response)\n        # We should not have made an additional head bucket call\n        self.assertEqual(head_bucket_calls, 0)\n\n    def test_does_not_redirect_if_None_response(self):\n        request_dict = {\n            'url': 'https://us-west-2.amazonaws.com/foo',\n            'context': {'signing': {'bucket': 'foo'}},\n        }\n        response = None\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        self.assertIsNone(redirect_response)\n\n    def test_get_region_from_response(self):\n        response = (\n            None,\n            {\n                'Error': {\n                    'Code': 'PermanentRedirect',\n                    'Endpoint': 'foo.eu-central-1.amazonaws.com',\n                    'Bucket': 'foo',\n                },\n                'ResponseMetadata': {\n                    'HTTPHeaders': {'x-amz-bucket-region': 'eu-central-1'}\n                },\n            },\n        )\n        region = self.redirector.get_bucket_region('foo', response)\n        self.assertEqual(region, 'eu-central-1')\n\n    def test_get_region_from_response_error_body(self):\n        response = (\n            None,\n            {\n                'Error': {\n                    'Code': 'PermanentRedirect',\n                    'Endpoint': 'foo.eu-central-1.amazonaws.com',\n                    'Bucket': 'foo',\n                    'Region': 'eu-central-1',\n                },\n                'ResponseMetadata': {'HTTPHeaders': {}},\n            },\n        )\n        region = self.redirector.get_bucket_region('foo', response)\n        self.assertEqual(region, 'eu-central-1')\n\n    def test_get_region_from_head_bucket_error(self):\n        self.set_client_response_headers(\n            {'x-amz-bucket-region': 'eu-central-1'}\n        )\n        response = (\n            None,\n            {\n                'Error': {\n                    'Code': 'PermanentRedirect',\n                    'Endpoint': 'foo.eu-central-1.amazonaws.com',\n                    'Bucket': 'foo',\n                },\n                'ResponseMetadata': {'HTTPHeaders': {}},\n            },\n        )\n        region = self.redirector.get_bucket_region('foo', response)\n        self.assertEqual(region, 'eu-central-1')\n\n    def test_get_region_from_head_bucket_success(self):\n        success_response = {\n            'ResponseMetadata': {\n                'HTTPHeaders': {'x-amz-bucket-region': 'eu-central-1'}\n            }\n        }\n        self.client.head_bucket.side_effect = None\n        self.client.head_bucket.return_value = success_response\n        response = (\n            None,\n            {\n                'Error': {\n                    'Code': 'PermanentRedirect',\n                    'Endpoint': 'foo.eu-central-1.amazonaws.com',\n                    'Bucket': 'foo',\n                },\n                'ResponseMetadata': {'HTTPHeaders': {}},\n            },\n        )\n        region = self.redirector.get_bucket_region('foo', response)\n        self.assertEqual(region, 'eu-central-1')\n\n    def test_no_redirect_from_error_for_accesspoint(self):\n        request_dict = {\n            'url': (\n                'https://myendpoint-123456789012.s3-accesspoint.'\n                'us-west-2.amazonaws.com/key'\n            ),\n            'context': {\n                's3_redirect': {\n                    'redirected': False,\n                    'bucket': 'arn:aws:s3:us-west-2:123456789012:myendpoint',\n                    'params': {},\n                }\n            },\n        }\n        response = (\n            None,\n            {\n                'Error': {'Code': '400', 'Message': 'Bad Request'},\n                'ResponseMetadata': {\n                    'HTTPHeaders': {'x-amz-bucket-region': 'eu-central-1'}\n                },\n            },\n        )\n\n        self.operation.name = 'HeadObject'\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        self.assertEqual(redirect_response, None)\n\n    def test_no_redirect_from_error_for_mrap_accesspoint(self):\n        mrap_arn = 'arn:aws:s3::123456789012:accesspoint:mfzwi23gnjvgw.mrap'\n        request_dict = {\n            'url': (\n                'https://mfzwi23gnjvgw.mrap.accesspoint.'\n                's3-global.amazonaws.com'\n            ),\n            'context': {\n                's3_redirect': {\n                    'redirected': False,\n                    'bucket': mrap_arn,\n                    'params': {},\n                }\n            },\n        }\n        response = (\n            None,\n            {\n                'Error': {'Code': '400', 'Message': 'Bad Request'},\n                'ResponseMetadata': {\n                    'HTTPHeaders': {'x-amz-bucket-region': 'eu-central-1'}\n                },\n            },\n        )\n\n        self.operation.name = 'HeadObject'\n        redirect_response = self.redirector.redirect_from_error(\n            request_dict, response, self.operation\n        )\n        self.assertEqual(redirect_response, None)\n\n\nclass TestArnParser(unittest.TestCase):\n    def setUp(self):\n        self.parser = ArnParser()\n\n    def test_parse(self):\n        arn = 'arn:aws:s3:us-west-2:1023456789012:myresource'\n        self.assertEqual(\n            self.parser.parse_arn(arn),\n            {\n                'partition': 'aws',\n                'service': 's3',\n                'region': 'us-west-2',\n                'account': '1023456789012',\n                'resource': 'myresource',\n            },\n        )\n\n    def test_parse_invalid_arn(self):\n        with self.assertRaises(InvalidArnException):\n            self.parser.parse_arn('arn:aws:s3')\n\n    def test_parse_arn_with_resource_type(self):\n        arn = 'arn:aws:s3:us-west-2:1023456789012:bucket_name:mybucket'\n        self.assertEqual(\n            self.parser.parse_arn(arn),\n            {\n                'partition': 'aws',\n                'service': 's3',\n                'region': 'us-west-2',\n                'account': '1023456789012',\n                'resource': 'bucket_name:mybucket',\n            },\n        )\n\n    def test_parse_arn_with_empty_elements(self):\n        arn = 'arn:aws:s3:::mybucket'\n        self.assertEqual(\n            self.parser.parse_arn(arn),\n            {\n                'partition': 'aws',\n                'service': 's3',\n                'region': '',\n                'account': '',\n                'resource': 'mybucket',\n            },\n        )\n\n\nclass TestS3ArnParamHandler(unittest.TestCase):\n    def setUp(self):\n        self.arn_handler = S3ArnParamHandler()\n        self.model = mock.Mock(OperationModel)\n        self.model.name = 'GetObject'\n\n    def test_register(self):\n        event_emitter = mock.Mock()\n        self.arn_handler.register(event_emitter)\n        event_emitter.register.assert_called_with(\n            'before-parameter-build.s3', self.arn_handler.handle_arn\n        )\n\n    def test_accesspoint_arn(self):\n        params = {\n            'Bucket': 'arn:aws:s3:us-west-2:123456789012:accesspoint/endpoint'\n        }\n        context = {}\n        self.arn_handler.handle_arn(params, self.model, context)\n        self.assertEqual(params, {'Bucket': 'endpoint'})\n        self.assertEqual(\n            context,\n            {\n                's3_accesspoint': {\n                    'name': 'endpoint',\n                    'account': '123456789012',\n                    'region': 'us-west-2',\n                    'partition': 'aws',\n                    'service': 's3',\n                }\n            },\n        )\n\n    def test_accesspoint_arn_with_colon(self):\n        params = {\n            'Bucket': 'arn:aws:s3:us-west-2:123456789012:accesspoint:endpoint'\n        }\n        context = {}\n        self.arn_handler.handle_arn(params, self.model, context)\n        self.assertEqual(params, {'Bucket': 'endpoint'})\n        self.assertEqual(\n            context,\n            {\n                's3_accesspoint': {\n                    'name': 'endpoint',\n                    'account': '123456789012',\n                    'region': 'us-west-2',\n                    'partition': 'aws',\n                    'service': 's3',\n                }\n            },\n        )\n\n    def test_errors_for_non_accesspoint_arn(self):\n        params = {\n            'Bucket': 'arn:aws:s3:us-west-2:123456789012:unsupported:resource'\n        }\n        context = {}\n        with self.assertRaises(UnsupportedS3ArnError):\n            self.arn_handler.handle_arn(params, self.model, context)\n\n    def test_outpost_arn_with_colon(self):\n        params = {\n            'Bucket': (\n                'arn:aws:s3-outposts:us-west-2:123456789012:outpost:'\n                'op-01234567890123456:accesspoint:myaccesspoint'\n            )\n        }\n        context = {}\n        self.arn_handler.handle_arn(params, self.model, context)\n        self.assertEqual(params, {'Bucket': 'myaccesspoint'})\n        self.assertEqual(\n            context,\n            {\n                's3_accesspoint': {\n                    'name': 'myaccesspoint',\n                    'outpost_name': 'op-01234567890123456',\n                    'account': '123456789012',\n                    'region': 'us-west-2',\n                    'partition': 'aws',\n                    'service': 's3-outposts',\n                }\n            },\n        )\n\n    def test_outpost_arn_with_slash(self):\n        params = {\n            'Bucket': (\n                'arn:aws:s3-outposts:us-west-2:123456789012:outpost/'\n                'op-01234567890123456/accesspoint/myaccesspoint'\n            )\n        }\n        context = {}\n        self.arn_handler.handle_arn(params, self.model, context)\n        self.assertEqual(params, {'Bucket': 'myaccesspoint'})\n        self.assertEqual(\n            context,\n            {\n                's3_accesspoint': {\n                    'name': 'myaccesspoint',\n                    'outpost_name': 'op-01234567890123456',\n                    'account': '123456789012',\n                    'region': 'us-west-2',\n                    'partition': 'aws',\n                    'service': 's3-outposts',\n                }\n            },\n        )\n\n    def test_outpost_arn_errors_for_missing_fields(self):\n        params = {\n            'Bucket': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost/'\n            'op-01234567890123456/accesspoint'\n        }\n        with self.assertRaises(UnsupportedOutpostResourceError):\n            self.arn_handler.handle_arn(params, self.model, {})\n\n    def test_outpost_arn_errors_for_empty_fields(self):\n        params = {\n            'Bucket': 'arn:aws:s3-outposts:us-west-2:123456789012:outpost/'\n            '/accesspoint/myaccesspoint'\n        }\n        with self.assertRaises(UnsupportedOutpostResourceError):\n            self.arn_handler.handle_arn(params, self.model, {})\n\n    def test_ignores_bucket_names(self):\n        params = {'Bucket': 'mybucket'}\n        context = {}\n        self.arn_handler.handle_arn(params, self.model, context)\n        self.assertEqual(params, {'Bucket': 'mybucket'})\n        self.assertEqual(context, {})\n\n    def test_ignores_create_bucket(self):\n        arn = 'arn:aws:s3:us-west-2:123456789012:accesspoint/endpoint'\n        params = {'Bucket': arn}\n        context = {}\n        self.model.name = 'CreateBucket'\n        self.arn_handler.handle_arn(params, self.model, context)\n        self.assertEqual(params, {'Bucket': arn})\n        self.assertEqual(context, {})\n\n\nclass TestS3EndpointSetter(unittest.TestCase):\n    def setUp(self):\n        self.operation_name = 'GetObject'\n        self.signature_version = 's3v4'\n        self.region_name = 'us-west-2'\n        self.service = 's3'\n        self.account = '123456789012'\n        self.bucket = 'mybucket'\n        self.key = 'key.txt'\n        self.accesspoint_name = 'myaccesspoint'\n        self.outpost_name = 'op-123456789012'\n        self.partition = 'aws'\n        self.endpoint_resolver = mock.Mock()\n        self.dns_suffix = 'amazonaws.com'\n        self.endpoint_resolver.construct_endpoint.return_value = {\n            'dnsSuffix': self.dns_suffix\n        }\n        self.endpoint_setter = self.get_endpoint_setter()\n\n    def get_endpoint_setter(self, **kwargs):\n        setter_kwargs = {\n            'endpoint_resolver': self.endpoint_resolver,\n            'region': self.region_name,\n        }\n        setter_kwargs.update(kwargs)\n        return S3EndpointSetter(**setter_kwargs)\n\n    def get_s3_request(\n        self, bucket=None, key=None, scheme='https://', querystring=None\n    ):\n        url = scheme + 's3.us-west-2.amazonaws.com/'\n        if bucket:\n            url += bucket\n        if key:\n            url += '/%s' % key\n        if querystring:\n            url += '?%s' % querystring\n        return AWSRequest(method='GET', headers={}, url=url)\n\n    def get_s3_outpost_request(self, **s3_request_kwargs):\n        request = self.get_s3_request(\n            self.accesspoint_name, **s3_request_kwargs\n        )\n        accesspoint_context = self.get_s3_accesspoint_context(\n            name=self.accesspoint_name, outpost_name=self.outpost_name\n        )\n        request.context['s3_accesspoint'] = accesspoint_context\n        return request\n\n    def get_s3_accesspoint_request(\n        self,\n        accesspoint_name=None,\n        accesspoint_context=None,\n        **s3_request_kwargs,\n    ):\n        if not accesspoint_name:\n            accesspoint_name = self.accesspoint_name\n        request = self.get_s3_request(accesspoint_name, **s3_request_kwargs)\n        if accesspoint_context is None:\n            accesspoint_context = self.get_s3_accesspoint_context(\n                name=accesspoint_name\n            )\n        request.context['s3_accesspoint'] = accesspoint_context\n        return request\n\n    def get_s3_accesspoint_context(self, **overrides):\n        accesspoint_context = {\n            'name': self.accesspoint_name,\n            'account': self.account,\n            'region': self.region_name,\n            'partition': self.partition,\n            'service': self.service,\n        }\n        accesspoint_context.update(overrides)\n        return accesspoint_context\n\n    def call_set_endpoint(self, endpoint_setter, request, **kwargs):\n        set_endpoint_kwargs = {\n            'request': request,\n            'operation_name': self.operation_name,\n            'signature_version': self.signature_version,\n            'region_name': self.region_name,\n        }\n        set_endpoint_kwargs.update(kwargs)\n        endpoint_setter.set_endpoint(**set_endpoint_kwargs)\n\n    def test_register(self):\n        event_emitter = mock.Mock()\n        self.endpoint_setter.register(event_emitter)\n        event_emitter.register.assert_has_calls(\n            [\n                mock.call('before-sign.s3', self.endpoint_setter.set_endpoint),\n                mock.call('choose-signer.s3', self.endpoint_setter.set_signer),\n                mock.call(\n                    'before-call.s3.WriteGetObjectResponse',\n                    self.endpoint_setter.update_endpoint_to_s3_object_lambda,\n                ),\n            ]\n        )\n\n    def test_outpost_endpoint(self):\n        request = self.get_s3_outpost_request()\n        self.call_set_endpoint(self.endpoint_setter, request=request)\n        expected_url = 'https://{}-{}.{}.s3-outposts.{}.amazonaws.com/'.format(\n            self.accesspoint_name,\n            self.account,\n            self.outpost_name,\n            self.region_name,\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_outpost_endpoint_preserves_key_in_path(self):\n        request = self.get_s3_outpost_request(key=self.key)\n        self.call_set_endpoint(self.endpoint_setter, request=request)\n        expected_url = (\n            'https://{}-{}.{}.s3-outposts.{}.amazonaws.com/{}'.format(\n                self.accesspoint_name,\n                self.account,\n                self.outpost_name,\n                self.region_name,\n                self.key,\n            )\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_accesspoint_endpoint(self):\n        request = self.get_s3_accesspoint_request()\n        self.call_set_endpoint(self.endpoint_setter, request=request)\n        expected_url = 'https://{}-{}.s3-accesspoint.{}.amazonaws.com/'.format(\n            self.accesspoint_name, self.account, self.region_name\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_accesspoint_preserves_key_in_path(self):\n        request = self.get_s3_accesspoint_request(key=self.key)\n        self.call_set_endpoint(self.endpoint_setter, request=request)\n        expected_url = (\n            'https://{}-{}.s3-accesspoint.{}.amazonaws.com/{}'.format(\n                self.accesspoint_name, self.account, self.region_name, self.key\n            )\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_accesspoint_preserves_scheme(self):\n        request = self.get_s3_accesspoint_request(scheme='http://')\n        self.call_set_endpoint(self.endpoint_setter, request=request)\n        expected_url = 'http://{}-{}.s3-accesspoint.{}.amazonaws.com/'.format(\n            self.accesspoint_name,\n            self.account,\n            self.region_name,\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_accesspoint_preserves_query_string(self):\n        request = self.get_s3_accesspoint_request(querystring='acl')\n        self.call_set_endpoint(self.endpoint_setter, request=request)\n        expected_url = (\n            'https://{}-{}.s3-accesspoint.{}.amazonaws.com/?acl'.format(\n                self.accesspoint_name,\n                self.account,\n                self.region_name,\n            )\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_uses_resolved_dns_suffix(self):\n        self.endpoint_resolver.construct_endpoint.return_value = {\n            'dnsSuffix': 'mysuffix.com'\n        }\n        request = self.get_s3_accesspoint_request()\n        self.call_set_endpoint(self.endpoint_setter, request=request)\n        expected_url = 'https://{}-{}.s3-accesspoint.{}.mysuffix.com/'.format(\n            self.accesspoint_name,\n            self.account,\n            self.region_name,\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_uses_region_of_client_if_use_arn_disabled(self):\n        client_region = 'client-region'\n        self.endpoint_setter = self.get_endpoint_setter(\n            region=client_region, s3_config={'use_arn_region': False}\n        )\n        request = self.get_s3_accesspoint_request()\n        self.call_set_endpoint(self.endpoint_setter, request=request)\n        expected_url = 'https://{}-{}.s3-accesspoint.{}.amazonaws.com/'.format(\n            self.accesspoint_name,\n            self.account,\n            client_region,\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_accesspoint_supports_custom_endpoint(self):\n        endpoint_setter = self.get_endpoint_setter(\n            endpoint_url='https://custom.com'\n        )\n        request = self.get_s3_accesspoint_request()\n        self.call_set_endpoint(endpoint_setter, request=request)\n        expected_url = 'https://{}-{}.custom.com/'.format(\n            self.accesspoint_name,\n            self.account,\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_errors_for_mismatching_partition(self):\n        endpoint_setter = self.get_endpoint_setter(partition='aws-cn')\n        accesspoint_context = self.get_s3_accesspoint_context(partition='aws')\n        request = self.get_s3_accesspoint_request(\n            accesspoint_context=accesspoint_context\n        )\n        with self.assertRaises(UnsupportedS3AccesspointConfigurationError):\n            self.call_set_endpoint(endpoint_setter, request=request)\n\n    def test_errors_for_mismatching_partition_when_using_client_region(self):\n        endpoint_setter = self.get_endpoint_setter(\n            s3_config={'use_arn_region': False}, partition='aws-cn'\n        )\n        accesspoint_context = self.get_s3_accesspoint_context(partition='aws')\n        request = self.get_s3_accesspoint_request(\n            accesspoint_context=accesspoint_context\n        )\n        with self.assertRaises(UnsupportedS3AccesspointConfigurationError):\n            self.call_set_endpoint(endpoint_setter, request=request)\n\n    def test_set_endpoint_for_auto(self):\n        endpoint_setter = self.get_endpoint_setter(\n            s3_config={'addressing_style': 'auto'}\n        )\n        request = self.get_s3_request(self.bucket, self.key)\n        self.call_set_endpoint(endpoint_setter, request)\n        expected_url = 'https://{}.s3.us-west-2.amazonaws.com/{}'.format(\n            self.bucket, self.key\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_set_endpoint_for_virtual(self):\n        endpoint_setter = self.get_endpoint_setter(\n            s3_config={'addressing_style': 'virtual'}\n        )\n        request = self.get_s3_request(self.bucket, self.key)\n        self.call_set_endpoint(endpoint_setter, request)\n        expected_url = 'https://{}.s3.us-west-2.amazonaws.com/{}'.format(\n            self.bucket, self.key\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_set_endpoint_for_path(self):\n        endpoint_setter = self.get_endpoint_setter(\n            s3_config={'addressing_style': 'path'}\n        )\n        request = self.get_s3_request(self.bucket, self.key)\n        self.call_set_endpoint(endpoint_setter, request)\n        expected_url = 'https://s3.us-west-2.amazonaws.com/{}/{}'.format(\n            self.bucket, self.key\n        )\n        self.assertEqual(request.url, expected_url)\n\n    def test_set_endpoint_for_accelerate(self):\n        endpoint_setter = self.get_endpoint_setter(\n            s3_config={'use_accelerate_endpoint': True}\n        )\n        request = self.get_s3_request(self.bucket, self.key)\n        self.call_set_endpoint(endpoint_setter, request)\n        expected_url = 'https://{}.s3-accelerate.amazonaws.com/{}'.format(\n            self.bucket, self.key\n        )\n        self.assertEqual(request.url, expected_url)\n\n\nclass TestContainerMetadataFetcher(unittest.TestCase):\n    def setUp(self):\n        self.responses = []\n        self.http = mock.Mock()\n        self.sleep = mock.Mock()\n\n    def create_fetcher(self):\n        return ContainerMetadataFetcher(self.http, sleep=self.sleep)\n\n    def fake_response(self, status_code, body):\n        response = mock.Mock()\n        response.status_code = status_code\n        response.content = body\n        return response\n\n    def set_http_responses_to(self, *responses):\n        http_responses = []\n        for response in responses:\n            if isinstance(response, Exception):\n                # Simulating an error condition.\n                http_response = response\n            elif hasattr(response, 'status_code'):\n                # It's a precreated fake_response.\n                http_response = response\n            else:\n                http_response = self.fake_response(\n                    status_code=200, body=json.dumps(response).encode('utf-8')\n                )\n            http_responses.append(http_response)\n        self.http.send.side_effect = http_responses\n\n    def assert_request(self, method, url, headers):\n        request = self.http.send.call_args[0][0]\n        self.assertEqual(request.method, method)\n        self.assertEqual(request.url, url)\n        self.assertEqual(request.headers, headers)\n\n    def assert_can_retrieve_metadata_from(self, full_uri):\n        response_body = {'foo': 'bar'}\n        self.set_http_responses_to(response_body)\n        fetcher = self.create_fetcher()\n        response = fetcher.retrieve_full_uri(full_uri)\n        self.assertEqual(response, response_body)\n        self.assert_request('GET', full_uri, {'Accept': 'application/json'})\n\n    def assert_host_is_not_allowed(self, full_uri):\n        response_body = {'foo': 'bar'}\n        self.set_http_responses_to(response_body)\n        fetcher = self.create_fetcher()\n        with self.assertRaisesRegex(ValueError, 'Unsupported host'):\n            fetcher.retrieve_full_uri(full_uri)\n        self.assertFalse(self.http.send.called)\n\n    def test_can_specify_extra_headers_are_merged(self):\n        headers = {\n            # The 'Accept' header will override the\n            # default Accept header of application/json.\n            'Accept': 'application/not-json',\n            'X-Other-Header': 'foo',\n        }\n        self.set_http_responses_to({'foo': 'bar'})\n        fetcher = self.create_fetcher()\n        fetcher.retrieve_full_uri('http://localhost', headers)\n        self.assert_request('GET', 'http://localhost', headers)\n\n    def test_can_retrieve_uri(self):\n        json_body = {\n            \"AccessKeyId\": \"a\",\n            \"SecretAccessKey\": \"b\",\n            \"Token\": \"c\",\n            \"Expiration\": \"d\",\n        }\n        self.set_http_responses_to(json_body)\n\n        fetcher = self.create_fetcher()\n        response = fetcher.retrieve_uri('/foo?id=1')\n\n        self.assertEqual(response, json_body)\n        # Ensure we made calls to the right endpoint.\n        headers = {'Accept': 'application/json'}\n        self.assert_request('GET', 'http://169.254.170.2/foo?id=1', headers)\n\n    def test_can_retry_requests(self):\n        success_response = {\n            \"AccessKeyId\": \"a\",\n            \"SecretAccessKey\": \"b\",\n            \"Token\": \"c\",\n            \"Expiration\": \"d\",\n        }\n        self.set_http_responses_to(\n            # First response is a connection error, should\n            # be retried.\n            ConnectionClosedError(endpoint_url=''),\n            # Second response is the successful JSON response\n            # with credentials.\n            success_response,\n        )\n        fetcher = self.create_fetcher()\n        response = fetcher.retrieve_uri('/foo?id=1')\n        self.assertEqual(response, success_response)\n\n    def test_propagates_credential_error_on_http_errors(self):\n        self.set_http_responses_to(\n            # In this scenario, we never get a successful response.\n            ConnectionClosedError(endpoint_url=''),\n            ConnectionClosedError(endpoint_url=''),\n            ConnectionClosedError(endpoint_url=''),\n            ConnectionClosedError(endpoint_url=''),\n            ConnectionClosedError(endpoint_url=''),\n        )\n        # As a result, we expect an appropriate error to be raised.\n        fetcher = self.create_fetcher()\n        with self.assertRaises(MetadataRetrievalError):\n            fetcher.retrieve_uri('/foo?id=1')\n        self.assertEqual(self.http.send.call_count, fetcher.RETRY_ATTEMPTS)\n\n    def test_error_raised_on_non_200_response(self):\n        self.set_http_responses_to(\n            self.fake_response(status_code=404, body=b'Error not found'),\n            self.fake_response(status_code=404, body=b'Error not found'),\n            self.fake_response(status_code=404, body=b'Error not found'),\n        )\n        fetcher = self.create_fetcher()\n        with self.assertRaises(MetadataRetrievalError):\n            fetcher.retrieve_uri('/foo?id=1')\n        # Should have tried up to RETRY_ATTEMPTS.\n        self.assertEqual(self.http.send.call_count, fetcher.RETRY_ATTEMPTS)\n\n    def test_error_raised_on_no_json_response(self):\n        # If the service returns a sucess response but with a body that\n        # does not contain JSON, we should still retry up to RETRY_ATTEMPTS,\n        # but after exhausting retries we propagate the exception.\n        self.set_http_responses_to(\n            self.fake_response(status_code=200, body=b'Not JSON'),\n            self.fake_response(status_code=200, body=b'Not JSON'),\n            self.fake_response(status_code=200, body=b'Not JSON'),\n        )\n        fetcher = self.create_fetcher()\n        with self.assertRaises(MetadataRetrievalError) as e:\n            fetcher.retrieve_uri('/foo?id=1')\n        self.assertNotIn('Not JSON', str(e.exception))\n        # Should have tried up to RETRY_ATTEMPTS.\n        self.assertEqual(self.http.send.call_count, fetcher.RETRY_ATTEMPTS)\n\n    def test_can_retrieve_full_uri_with_fixed_ip(self):\n        uri = f'http://{ContainerMetadataFetcher.IP_ADDRESS}/foo?id=1'\n        self.assert_can_retrieve_metadata_from(uri)\n\n    def test_localhost_http_is_allowed(self):\n        self.assert_can_retrieve_metadata_from('http://localhost/foo')\n\n    def test_localhost_with_port_http_is_allowed(self):\n        self.assert_can_retrieve_metadata_from('http://localhost:8000/foo')\n\n    def test_localhost_https_is_allowed(self):\n        self.assert_can_retrieve_metadata_from('https://localhost/foo')\n\n    def test_can_use_127_ip_addr(self):\n        self.assert_can_retrieve_metadata_from('https://127.0.0.1/foo')\n\n    def test_can_use_127_ip_addr_with_port(self):\n        self.assert_can_retrieve_metadata_from('https://127.0.0.1:8080/foo')\n\n    def test_can_use_eks_ipv4_addr(self):\n        uri = 'http://169.254.170.23/credentials'\n        self.assert_can_retrieve_metadata_from(uri)\n\n    def test_can_use_eks_ipv6_addr(self):\n        uri = 'http://[fd00:ec2::23]/credentials'\n        self.assert_can_retrieve_metadata_from(uri)\n\n    def test_can_use_eks_ipv6_addr_with_port(self):\n        uri = 'https://[fd00:ec2::23]:8000'\n        self.assert_can_retrieve_metadata_from(uri)\n\n    def test_can_use_loopback_v6_uri(self):\n        self.assert_can_retrieve_metadata_from('http://[::1]/credentials')\n\n    def test_link_local_http_is_not_allowed(self):\n        self.assert_host_is_not_allowed('http://169.254.0.1/foo')\n\n    def test_link_local_https_is_not_allowed(self):\n        self.assert_host_is_not_allowed('https://169.254.0.1/foo')\n\n    def test_non_link_local_nonallowed_url(self):\n        self.assert_host_is_not_allowed('http://169.1.2.3/foo')\n\n    def test_error_raised_on_nonallowed_url(self):\n        self.assert_host_is_not_allowed('http://somewhere.com/foo')\n\n    def test_external_host_not_allowed_if_https(self):\n        self.assert_host_is_not_allowed('https://somewhere.com/foo')\n\n\nclass TestUnsigned(unittest.TestCase):\n    def test_copy_returns_same_object(self):\n        self.assertIs(botocore.UNSIGNED, copy.copy(botocore.UNSIGNED))\n\n    def test_deepcopy_returns_same_object(self):\n        self.assertIs(botocore.UNSIGNED, copy.deepcopy(botocore.UNSIGNED))\n\n\nclass TestInstanceMetadataFetcher(unittest.TestCase):\n    def setUp(self):\n        urllib3_session_send = 'botocore.httpsession.URLLib3Session.send'\n        self._urllib3_patch = mock.patch(urllib3_session_send)\n        self._send = self._urllib3_patch.start()\n        self._imds_responses = []\n        self._send.side_effect = self.get_imds_response\n        self._role_name = 'role-name'\n        self._creds = {\n            'AccessKeyId': 'spam',\n            'SecretAccessKey': 'eggs',\n            'Token': 'spam-token',\n            'Expiration': 'something',\n        }\n        self._expected_creds = {\n            'access_key': self._creds['AccessKeyId'],\n            'secret_key': self._creds['SecretAccessKey'],\n            'token': self._creds['Token'],\n            'expiry_time': self._creds['Expiration'],\n            'role_name': self._role_name,\n        }\n\n    def tearDown(self):\n        self._urllib3_patch.stop()\n\n    def add_imds_response(self, body, status_code=200):\n        response = botocore.awsrequest.AWSResponse(\n            url='http://169.254.169.254/',\n            status_code=status_code,\n            headers={},\n            raw=RawResponse(body),\n        )\n        self._imds_responses.append(response)\n\n    def add_get_role_name_imds_response(self, role_name=None):\n        if role_name is None:\n            role_name = self._role_name\n        self.add_imds_response(body=role_name.encode('utf-8'))\n\n    def add_get_credentials_imds_response(self, creds=None):\n        if creds is None:\n            creds = self._creds\n        self.add_imds_response(body=json.dumps(creds).encode('utf-8'))\n\n    def add_get_token_imds_response(self, token, status_code=200):\n        self.add_imds_response(\n            body=token.encode('utf-8'), status_code=status_code\n        )\n\n    def add_metadata_token_not_supported_response(self):\n        self.add_imds_response(b'', status_code=404)\n\n    def add_imds_connection_error(self, exception):\n        self._imds_responses.append(exception)\n\n    def add_default_imds_responses(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n    def get_imds_response(self, request):\n        response = self._imds_responses.pop(0)\n        if isinstance(response, Exception):\n            raise response\n        return response\n\n    def _test_imds_base_url(self, config, expected_url):\n        self.add_default_imds_responses()\n\n        fetcher = InstanceMetadataFetcher(config=config)\n        result = fetcher.retrieve_iam_role_credentials()\n\n        self.assertEqual(result, self._expected_creds)\n        self.assertEqual(fetcher.get_base_url(), expected_url)\n\n    def test_disabled_by_environment(self):\n        env = {'AWS_EC2_METADATA_DISABLED': 'true'}\n        fetcher = InstanceMetadataFetcher(env=env)\n        result = fetcher.retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n        self._send.assert_not_called()\n\n    def test_disabled_by_environment_mixed_case(self):\n        env = {'AWS_EC2_METADATA_DISABLED': 'tRuE'}\n        fetcher = InstanceMetadataFetcher(env=env)\n        result = fetcher.retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n        self._send.assert_not_called()\n\n    def test_disabling_env_var_not_true(self):\n        url = 'https://example.com/'\n        env = {'AWS_EC2_METADATA_DISABLED': 'false'}\n\n        self.add_default_imds_responses()\n\n        fetcher = InstanceMetadataFetcher(base_url=url, env=env)\n        result = fetcher.retrieve_iam_role_credentials()\n\n        self.assertEqual(result, self._expected_creds)\n\n    def test_ec2_metadata_endpoint_service_mode(self):\n        configs = [\n            (\n                {'ec2_metadata_service_endpoint_mode': 'ipv6'},\n                'http://[fd00:ec2::254]/',\n            ),\n            (\n                {'ec2_metadata_service_endpoint_mode': 'ipv6'},\n                'http://[fd00:ec2::254]/',\n            ),\n            (\n                {'ec2_metadata_service_endpoint_mode': 'ipv4'},\n                'http://169.254.169.254/',\n            ),\n            (\n                {'ec2_metadata_service_endpoint_mode': 'foo'},\n                'http://169.254.169.254/',\n            ),\n            (\n                {\n                    'ec2_metadata_service_endpoint_mode': 'ipv6',\n                    'ec2_metadata_service_endpoint': 'http://[fd00:ec2::010]/',\n                },\n                'http://[fd00:ec2::010]/',\n            ),\n        ]\n\n        for config, expected_url in configs:\n            self._test_imds_base_url(config, expected_url)\n\n    def test_metadata_endpoint(self):\n        urls = [\n            'http://fd00:ec2:0000:0000:0000:0000:0000:0000/',\n            'http://[fd00:ec2::010]/',\n            'http://192.168.1.1/',\n        ]\n        for url in urls:\n            self.assertTrue(is_valid_uri(url))\n\n    def test_ipv6_endpoint_no_brackets_env_var_set(self):\n        url = 'http://fd00:ec2::010/'\n        self.assertFalse(is_valid_ipv6_endpoint_url(url))\n\n    def test_ipv6_invalid_endpoint(self):\n        url = 'not.a:valid:dom@in'\n        config = {'ec2_metadata_service_endpoint': url}\n        with self.assertRaises(InvalidIMDSEndpointError):\n            InstanceMetadataFetcher(config=config)\n\n    def test_ipv6_endpoint_env_var_set_and_args(self):\n        url = 'http://[fd00:ec2::254]/'\n        url_arg = 'http://fd00:ec2:0000:0000:0000:8a2e:0370:7334/'\n        config = {'ec2_metadata_service_endpoint': url}\n\n        self.add_default_imds_responses()\n\n        fetcher = InstanceMetadataFetcher(config=config, base_url=url_arg)\n        result = fetcher.retrieve_iam_role_credentials()\n\n        self.assertEqual(result, self._expected_creds)\n        self.assertEqual(fetcher.get_base_url(), url_arg)\n\n    def test_ipv6_imds_not_allocated(self):\n        url = 'http://fd00:ec2:0000:0000:0000:0000:0000:0000/'\n        config = {'ec2_metadata_service_endpoint': url}\n\n        self.add_imds_response(status_code=400, body=b'{}')\n\n        fetcher = InstanceMetadataFetcher(config=config)\n        result = fetcher.retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n\n    def test_ipv6_imds_empty_config(self):\n        configs = [\n            ({'ec2_metadata_service_endpoint': ''}, 'http://169.254.169.254/'),\n            (\n                {'ec2_metadata_service_endpoint_mode': ''},\n                'http://169.254.169.254/',\n            ),\n            ({}, 'http://169.254.169.254/'),\n            (None, 'http://169.254.169.254/'),\n        ]\n\n        for config, expected_url in configs:\n            self._test_imds_base_url(config, expected_url)\n\n    def test_includes_user_agent_header(self):\n        user_agent = 'my-user-agent'\n        self.add_default_imds_responses()\n\n        InstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        self.assertEqual(self._send.call_count, 3)\n        for call in self._send.calls:\n            self.assertTrue(call[0][0].headers['User-Agent'], user_agent)\n\n    def test_non_200_response_for_role_name_is_retried(self):\n        # Response for role name that have a non 200 status code should\n        # be retried.\n        self.add_get_token_imds_response(token='token')\n        self.add_imds_response(\n            status_code=429, body=b'{\"message\": \"Slow down\"}'\n        )\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n        result = InstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    def test_http_connection_error_for_role_name_is_retried(self):\n        # Connection related errors should be retried\n        self.add_get_token_imds_response(token='token')\n        self.add_imds_connection_error(ConnectionClosedError(endpoint_url=''))\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n        result = InstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    def test_empty_response_for_role_name_is_retried(self):\n        # Response for role name that have a non 200 status code should\n        # be retried.\n        self.add_get_token_imds_response(token='token')\n        self.add_imds_response(body=b'')\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n        result = InstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    def test_non_200_response_is_retried(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Response for creds that has a 200 status code but has an empty\n        # body should be retried.\n        self.add_imds_response(\n            status_code=429, body=b'{\"message\": \"Slow down\"}'\n        )\n        self.add_get_credentials_imds_response()\n        result = InstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    def test_http_connection_errors_is_retried(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Connection related errors should be retried\n        self.add_imds_connection_error(ConnectionClosedError(endpoint_url=''))\n        self.add_get_credentials_imds_response()\n        result = InstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    def test_empty_response_is_retried(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Response for creds that has a 200 status code but is empty.\n        # This should be retried.\n        self.add_imds_response(body=b'')\n        self.add_get_credentials_imds_response()\n        result = InstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    def test_invalid_json_is_retried(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Response for creds that has a 200 status code but is invalid JSON.\n        # This should be retried.\n        self.add_imds_response(body=b'{\"AccessKey\":')\n        self.add_get_credentials_imds_response()\n        result = InstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    def test_exhaust_retries_on_role_name_request(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_imds_response(status_code=400, body=b'')\n        result = InstanceMetadataFetcher(\n            num_attempts=1\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n\n    def test_exhaust_retries_on_credentials_request(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        self.add_imds_response(status_code=400, body=b'')\n        result = InstanceMetadataFetcher(\n            num_attempts=1\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n\n    def test_missing_fields_in_credentials_response(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Response for creds that has a 200 status code and a JSON body\n        # representing an error. We do not necessarily want to retry this.\n        self.add_imds_response(\n            body=b'{\"Code\":\"AssumeRoleUnauthorizedAccess\",\"Message\":\"error\"}'\n        )\n        result = InstanceMetadataFetcher().retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n\n    def test_token_is_included(self):\n        user_agent = 'my-user-agent'\n        self.add_default_imds_responses()\n\n        result = InstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        # Check that subsequent calls after getting the token include the token.\n        self.assertEqual(self._send.call_count, 3)\n        for call in self._send.call_args_list[1:]:\n            self.assertEqual(\n                call[0][0].headers['x-aws-ec2-metadata-token'], 'token'\n            )\n        self.assertEqual(result, self._expected_creds)\n\n    def test_metadata_token_not_supported_404(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_response(b'', status_code=404)\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = InstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    def test_metadata_token_not_supported_403(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_response(b'', status_code=403)\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = InstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    def test_metadata_token_not_supported_405(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_response(b'', status_code=405)\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = InstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    def test_metadata_token_not_supported_timeout(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_connection_error(ReadTimeoutError(endpoint_url='url'))\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = InstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    def test_token_not_supported_exhaust_retries(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_connection_error(ConnectTimeoutError(endpoint_url='url'))\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = InstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    def test_metadata_token_bad_request_yields_no_credentials(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_response(b'', status_code=400)\n        result = InstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n\n    def test_v1_disabled_by_config(self):\n        config = {'ec2_metadata_v1_disabled': True}\n        self.add_imds_response(b'', status_code=404)\n        fetcher = InstanceMetadataFetcher(num_attempts=1, config=config)\n        with self.assertRaises(MetadataRetrievalError):\n            fetcher.retrieve_iam_role_credentials()\n\n    def _get_datetime(self, dt=None, offset=None, offset_func=operator.add):\n        if dt is None:\n            dt = datetime.datetime.utcnow()\n        if offset is not None:\n            dt = offset_func(dt, offset)\n\n        return dt\n\n    def _get_default_creds(self, overrides=None):\n        if overrides is None:\n            overrides = {}\n\n        creds = {\n            'AccessKeyId': 'access',\n            'SecretAccessKey': 'secret',\n            'Token': 'token',\n            'Expiration': '1970-01-01T00:00:00',\n        }\n        creds.update(overrides)\n        return creds\n\n    def _convert_creds_to_imds_fetcher(self, creds):\n        return {\n            'access_key': creds['AccessKeyId'],\n            'secret_key': creds['SecretAccessKey'],\n            'token': creds['Token'],\n            'expiry_time': creds['Expiration'],\n            'role_name': self._role_name,\n        }\n\n    def _add_default_imds_response(self, status_code=200, creds=''):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        self.add_imds_response(\n            status_code=200, body=json.dumps(creds).encode('utf-8')\n        )\n\n    def mock_randint(self, int_val=600):\n        randint_mock = mock.Mock()\n        randint_mock.return_value = int_val\n        return randint_mock\n\n    @FreezeTime(module=botocore.utils.datetime, date=DATE)\n    def test_expiry_time_extension(self):\n        current_time = self._get_datetime()\n        expiration_time = self._get_datetime(\n            dt=current_time, offset=datetime.timedelta(seconds=14 * 60)\n        )\n        new_expiration = self._get_datetime(\n            dt=current_time, offset=datetime.timedelta(seconds=20 * 60)\n        )\n\n        creds = self._get_default_creds(\n            {\"Expiration\": expiration_time.strftime(DT_FORMAT)}\n        )\n        expected_data = self._convert_creds_to_imds_fetcher(creds)\n        expected_data[\"expiry_time\"] = new_expiration.strftime(DT_FORMAT)\n\n        self._add_default_imds_response(200, creds)\n\n        with mock.patch(\"random.randint\", self.mock_randint()):\n            fetcher = InstanceMetadataFetcher()\n            result = fetcher.retrieve_iam_role_credentials()\n            assert result == expected_data\n\n    @FreezeTime(module=botocore.utils.datetime, date=DATE)\n    def test_expired_expiry_extension(self):\n        current_time = self._get_datetime()\n        expiration_time = self._get_datetime(\n            dt=current_time,\n            offset=datetime.timedelta(seconds=14 * 60),\n            offset_func=operator.sub,\n        )\n        new_expiration = self._get_datetime(\n            dt=current_time, offset=datetime.timedelta(seconds=20 * 60)\n        )\n        assert current_time > expiration_time\n        assert new_expiration > current_time\n\n        creds = self._get_default_creds(\n            {\"Expiration\": expiration_time.strftime(DT_FORMAT)}\n        )\n        expected_data = self._convert_creds_to_imds_fetcher(creds)\n        expected_data[\"expiry_time\"] = new_expiration.strftime(DT_FORMAT)\n\n        self._add_default_imds_response(200, creds)\n\n        with mock.patch(\"random.randint\", self.mock_randint()):\n            fetcher = InstanceMetadataFetcher()\n            result = fetcher.retrieve_iam_role_credentials()\n            assert result == expected_data\n\n    @FreezeTime(module=botocore.utils.datetime, date=DATE)\n    def test_expiry_extension_with_config(self):\n        current_time = self._get_datetime()\n        expiration_time = self._get_datetime(\n            dt=current_time,\n            offset=datetime.timedelta(seconds=14 * 60),\n            offset_func=operator.sub,\n        )\n        new_expiration = self._get_datetime(\n            dt=current_time, offset=datetime.timedelta(seconds=25 * 60)\n        )\n        assert current_time > expiration_time\n        assert new_expiration > current_time\n\n        creds = self._get_default_creds(\n            {\"Expiration\": expiration_time.strftime(DT_FORMAT)}\n        )\n        expected_data = self._convert_creds_to_imds_fetcher(creds)\n        expected_data[\"expiry_time\"] = new_expiration.strftime(DT_FORMAT)\n\n        self._add_default_imds_response(200, creds)\n\n        with mock.patch(\"random.randint\", self.mock_randint()):\n            fetcher = InstanceMetadataFetcher(\n                config={\"ec2_credential_refresh_window\": 15 * 60}\n            )\n            result = fetcher.retrieve_iam_role_credentials()\n            assert result == expected_data\n\n    @FreezeTime(module=botocore.utils.datetime, date=DATE)\n    def test_expiry_extension_with_bad_datetime(self):\n        bad_datetime = \"May 20th, 2020 19:00:00\"\n        creds = self._get_default_creds({\"Expiration\": bad_datetime})\n        self._add_default_imds_response(200, creds)\n\n        fetcher = InstanceMetadataFetcher(\n            config={\"ec2_credential_refresh_window\": 15 * 60}\n        )\n        results = fetcher.retrieve_iam_role_credentials()\n        assert results['expiry_time'] == bad_datetime\n\n\nclass TestIMDSRegionProvider(unittest.TestCase):\n    def setUp(self):\n        self.environ = {}\n        self.environ_patch = mock.patch('os.environ', self.environ)\n        self.environ_patch.start()\n\n    def tearDown(self):\n        self.environ_patch.stop()\n\n    def assert_does_provide_expected_value(\n        self,\n        fetcher_region=None,\n        expected_result=None,\n    ):\n        fake_session = mock.Mock(spec=Session)\n        fake_fetcher = mock.Mock(spec=InstanceMetadataRegionFetcher)\n        fake_fetcher.retrieve_region.return_value = fetcher_region\n        provider = IMDSRegionProvider(fake_session, fetcher=fake_fetcher)\n        value = provider.provide()\n        self.assertEqual(value, expected_result)\n\n    def test_does_provide_region_when_present(self):\n        self.assert_does_provide_expected_value(\n            fetcher_region='us-mars-2',\n            expected_result='us-mars-2',\n        )\n\n    def test_does_provide_none(self):\n        self.assert_does_provide_expected_value(\n            fetcher_region=None,\n            expected_result=None,\n        )\n\n    @mock.patch('botocore.httpsession.URLLib3Session.send')\n    def test_use_truncated_user_agent(self, send):\n        session = Session()\n        session = Session()\n        session.user_agent_version = '3.0'\n        provider = IMDSRegionProvider(session)\n        provider.provide()\n        args, _ = send.call_args\n        self.assertIn('Botocore/3.0', args[0].headers['User-Agent'])\n\n    @mock.patch('botocore.httpsession.URLLib3Session.send')\n    def test_can_use_ipv6(self, send):\n        session = Session()\n        session.set_config_variable('imds_use_ipv6', True)\n        provider = IMDSRegionProvider(session)\n        provider.provide()\n        args, _ = send.call_args\n        self.assertIn('[fd00:ec2::254]', args[0].url)\n\n    @mock.patch('botocore.httpsession.URLLib3Session.send')\n    def test_use_ipv4_by_default(self, send):\n        session = Session()\n        provider = IMDSRegionProvider(session)\n        provider.provide()\n        args, _ = send.call_args\n        self.assertIn('169.254.169.254', args[0].url)\n\n    @mock.patch('botocore.httpsession.URLLib3Session.send')\n    def test_can_set_imds_endpoint_mode_to_ipv4(self, send):\n        session = Session()\n        session.set_config_variable(\n            'ec2_metadata_service_endpoint_mode', 'ipv4'\n        )\n        provider = IMDSRegionProvider(session)\n        provider.provide()\n        args, _ = send.call_args\n        self.assertIn('169.254.169.254', args[0].url)\n\n    @mock.patch('botocore.httpsession.URLLib3Session.send')\n    def test_can_set_imds_endpoint_mode_to_ipv6(self, send):\n        session = Session()\n        session.set_config_variable(\n            'ec2_metadata_service_endpoint_mode', 'ipv6'\n        )\n        provider = IMDSRegionProvider(session)\n        provider.provide()\n        args, _ = send.call_args\n        self.assertIn('[fd00:ec2::254]', args[0].url)\n\n    @mock.patch('botocore.httpsession.URLLib3Session.send')\n    def test_can_set_imds_service_endpoint(self, send):\n        session = Session()\n        session.set_config_variable(\n            'ec2_metadata_service_endpoint', 'http://myendpoint/'\n        )\n        provider = IMDSRegionProvider(session)\n        provider.provide()\n        args, _ = send.call_args\n        self.assertIn('http://myendpoint/', args[0].url)\n\n    @mock.patch('botocore.httpsession.URLLib3Session.send')\n    def test_can_set_imds_service_endpoint_custom(self, send):\n        session = Session()\n        session.set_config_variable(\n            'ec2_metadata_service_endpoint', 'http://myendpoint'\n        )\n        provider = IMDSRegionProvider(session)\n        provider.provide()\n        args, _ = send.call_args\n        self.assertIn('http://myendpoint/latest/meta-data', args[0].url)\n\n    @mock.patch('botocore.httpsession.URLLib3Session.send')\n    def test_imds_service_endpoint_overrides_ipv6_endpoint(self, send):\n        session = Session()\n        session.set_config_variable(\n            'ec2_metadata_service_endpoint_mode', 'ipv6'\n        )\n        session.set_config_variable(\n            'ec2_metadata_service_endpoint', 'http://myendpoint/'\n        )\n        provider = IMDSRegionProvider(session)\n        provider.provide()\n        args, _ = send.call_args\n        self.assertIn('http://myendpoint/', args[0].url)\n\n\nclass TestSSOTokenLoader(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n        self.session_name = 'admin'\n        self.start_url = 'https://d-abc123.awsapps.com/start'\n        self.cache_key = '40a89917e3175433e361b710a9d43528d7f1890a'\n        self.session_cache_key = 'd033e22ae348aeb5660fc2140aec35850c4da997'\n        self.access_token = 'totally.a.token'\n        self.cached_token = {\n            'accessToken': self.access_token,\n            'expiresAt': '2002-10-18T03:52:38UTC',\n        }\n        self.cache = {}\n        self.loader = SSOTokenLoader(cache=self.cache)\n\n    def test_can_load_token_exists(self):\n        self.cache[self.cache_key] = self.cached_token\n        access_token = self.loader(self.start_url)\n        self.assertEqual(self.cached_token, access_token)\n\n    def test_can_handle_does_not_exist(self):\n        with self.assertRaises(SSOTokenLoadError):\n            self.loader(self.start_url)\n\n    def test_can_handle_invalid_cache(self):\n        self.cache[self.cache_key] = {}\n        with self.assertRaises(SSOTokenLoadError):\n            self.loader(self.start_url)\n\n    def test_can_save_token(self):\n        self.loader.save_token(self.start_url, self.cached_token)\n        access_token = self.loader(self.start_url)\n        self.assertEqual(self.cached_token, access_token)\n\n    def test_can_save_token_sso_session(self):\n        self.loader.save_token(\n            self.start_url,\n            self.cached_token,\n            session_name=self.session_name,\n        )\n        access_token = self.loader(\n            self.start_url,\n            session_name=self.session_name,\n        )\n        self.assertEqual(self.cached_token, access_token)\n\n    def test_can_load_token_exists_sso_session_name(self):\n        self.cache[self.session_cache_key] = self.cached_token\n        access_token = self.loader(\n            self.start_url,\n            session_name=self.session_name,\n        )\n        self.assertEqual(self.cached_token, access_token)\n\n\n@pytest.mark.parametrize(\n    'header_name, headers, expected',\n    (\n        ('test_header', {'test_header': 'foo'}, True),\n        ('Test_Header', {'test_header': 'foo'}, True),\n        ('test_header', {'Test_Header': 'foo'}, True),\n        ('missing_header', {'Test_Header': 'foo'}, False),\n        (None, {'Test_Header': 'foo'}, False),\n        ('test_header', HeadersDict({'test_header': 'foo'}), True),\n        ('Test_Header', HeadersDict({'test_header': 'foo'}), True),\n        ('test_header', HeadersDict({'Test_Header': 'foo'}), True),\n        ('missing_header', HeadersDict({'Test_Header': 'foo'}), False),\n        (None, HeadersDict({'Test_Header': 'foo'}), False),\n    ),\n)\ndef test_has_header(header_name, headers, expected):\n    assert has_header(header_name, headers) is expected\n\n\nclass TestDetermineContentLength(unittest.TestCase):\n    def test_basic_bytes(self):\n        length = determine_content_length(b'hello')\n        self.assertEqual(length, 5)\n\n    def test_empty_bytes(self):\n        length = determine_content_length(b'')\n        self.assertEqual(length, 0)\n\n    def test_buffered_io_base(self):\n        length = determine_content_length(io.BufferedIOBase())\n        self.assertIsNone(length)\n\n    def test_none(self):\n        length = determine_content_length(None)\n        self.assertEqual(length, 0)\n\n    def test_basic_len_obj(self):\n        class HasLen:\n            def __len__(self):\n                return 12\n\n        length = determine_content_length(HasLen())\n        self.assertEqual(length, 12)\n\n    def test_non_seekable_fileobj(self):\n        class Readable:\n            def read(self, *args, **kwargs):\n                pass\n\n        length = determine_content_length(Readable())\n        self.assertIsNone(length)\n\n    def test_seekable_fileobj(self):\n        class Seekable:\n            _pos = 0\n\n            def read(self, *args, **kwargs):\n                pass\n\n            def tell(self, *args, **kwargs):\n                return self._pos\n\n            def seek(self, *args, **kwargs):\n                self._pos = 50\n\n        length = determine_content_length(Seekable())\n        self.assertEqual(length, 50)\n\n\n@pytest.mark.parametrize(\n    'url, expected',\n    (\n        ('https://s3-accelerate.amazonaws.com', True),\n        ('https://s3-accelerate.amazonaws.com/', True),\n        ('https://s3-accelerate.amazonaws.com/key', True),\n        ('http://s3-accelerate.amazonaws.com/key', True),\n        ('https://s3-accelerate.foo.amazonaws.com/key', False),\n        # bucket prefixes are not allowed\n        ('https://bucket.s3-accelerate.amazonaws.com/key', False),\n        # S3 accelerate can be combined with dualstack\n        ('https://s3-accelerate.dualstack.amazonaws.com/key', True),\n        ('https://bucket.s3-accelerate.dualstack.amazonaws.com/key', False),\n        ('https://s3-accelerate.dualstack.dualstack.amazonaws.com/key', False),\n        ('https://s3-accelerate.dualstack.foo.amazonaws.com/key', False),\n        ('https://dualstack.s3-accelerate.amazonaws.com/key', False),\n        # assorted other ways for URLs to not be valid for s3-accelerate\n        ('ftp://s3-accelerate.dualstack.foo.amazonaws.com/key', False),\n        ('https://s3-accelerate.dualstack.foo.c2s.ic.gov/key', False),\n        # None-valued url is accepted\n        (None, False),\n    ),\n)\ndef test_is_s3_accelerate_url(url, expected):\n    assert is_s3_accelerate_url(url) == expected\n\n\n@pytest.mark.parametrize(\n    'headers, default, expected',\n    (\n        ({}, 'ISO-8859-1', None),\n        ({'Content-Type': 'text/html; charset=utf-8'}, 'default', 'utf-8'),\n        ({'Content-Type': 'text/html; charset=\"utf-8\"'}, 'default', 'utf-8'),\n        ({'Content-Type': 'text/html'}, 'ascii', 'ascii'),\n        ({'Content-Type': 'application/json'}, 'ISO-8859-1', None),\n    ),\n)\ndef test_get_encoding_from_headers(headers, default, expected):\n    charset = get_encoding_from_headers(HeadersDict(headers), default=default)\n    assert charset == expected\n\n\ndef test_lru_cache_weakref():\n    class ClassWithCachedMethod:\n        @lru_cache_weakref(maxsize=10)\n        def cached_fn(self, a, b):\n            return a + b\n\n    cls1 = ClassWithCachedMethod()\n    cls2 = ClassWithCachedMethod()\n\n    assert cls1.cached_fn.cache_info().currsize == 0\n    assert getrefcount(cls1) == 2\n    assert getrefcount(cls2) == 2\n    # \"The count returned is generally one higher than you might expect, because\n    # it includes the (temporary) reference as an argument to getrefcount().\"\n    # https://docs.python.org/3.8/library/sys.html#getrefcount\n\n    cls1.cached_fn(1, 1)\n    cls2.cached_fn(1, 1)\n\n    # The cache now has two entries, but the reference count remains the same as\n    # before.\n    assert cls1.cached_fn.cache_info().currsize == 2\n    assert getrefcount(cls1) == 2\n    assert getrefcount(cls2) == 2\n\n    # Deleting one of the objects does not interfere with the cache entries\n    # related to the other object.\n    del cls1\n    assert cls2.cached_fn.cache_info().currsize == 2\n    assert cls2.cached_fn.cache_info().hits == 0\n    assert cls2.cached_fn.cache_info().misses == 2\n    cls2.cached_fn(1, 1)\n    assert cls2.cached_fn.cache_info().currsize == 2\n    assert cls2.cached_fn.cache_info().hits == 1  # the call was a cache hit\n    assert cls2.cached_fn.cache_info().misses == 2\n", "tests/unit/response_parsing/test_response_parsing.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport difflib\nimport glob\nimport json\nimport logging\nimport os\nimport pprint\n\nimport pytest\n\nimport botocore.session\nfrom botocore import parsers, xform_name\nfrom tests import create_session\n\nlog = logging.getLogger(__name__)\n\n\nSPECIAL_CASES = [\n    'iam-get-user-policy.xml',  # Needs the JSON decode from handlers.py\n    'iam-list-roles.xml',  # Needs the JSON decode from handlers.py for the policy\n    's3-get-bucket-location.xml',  # Confirmed, this will need a special handler\n    # 's3-list-multipart-uploads.xml',  # Bug in model, missing delimeter\n    'cloudformation-get-template.xml',  # Need to JSON decode the template body.\n]\n\n\ndef _get_expected_parsed_result(filename):\n    dirname, filename = os.path.split(filename)\n    basename = os.path.splitext(filename)[0]\n    jsonfile = os.path.join(dirname, basename + '.json')\n    with open(jsonfile) as f:\n        return json.load(f)\n\n\ndef _get_raw_response_body(xmlfile):\n    with open(xmlfile, 'rb') as f:\n        return f.read()\n\n\ndef _get_operation_model(service_model, filename):\n    dirname, filename = os.path.split(filename)\n    basename = os.path.splitext(filename)[0]\n    sn, opname = basename.split('-', 1)\n    # In order to have multiple tests for the same\n    # operation a '#' char is used to separate\n    # operation names from some other suffix so that\n    # the tests have a different filename, e.g\n    # my-operation#1.xml, my-operation#2.xml.\n    opname = opname.split('#')[0]\n    operation_names = service_model.operation_names\n    for operation_name in operation_names:\n        if xform_name(operation_name) == opname.replace('-', '_'):\n            return service_model.operation_model(operation_name)\n\n\ndef _test_parsed_response(xmlfile, operation_model, expected):\n    response_body = _get_raw_response_body(xmlfile)\n    response = {'body': response_body, 'status_code': 200, 'headers': {}}\n    for case in SPECIAL_CASES:\n        if case in xmlfile:\n            print(\"SKIP: %s\" % xmlfile)\n            return\n    if 'errors' in xmlfile:\n        response['status_code'] = 400\n    # Handle the special cased __headers__ key if it exists.\n    if b'__headers__' in response_body:\n        loaded = json.loads(response_body.decode('utf-8'))\n        response['headers'] = loaded.pop('__headers__')\n        response['body'] = json.dumps(loaded).encode('utf-8')\n\n    protocol = operation_model.service_model.protocol\n    parser_cls = parsers.PROTOCOL_PARSERS[protocol]\n    parser = parser_cls(timestamp_parser=lambda x: x)\n    parsed = parser.parse(response, operation_model.output_shape)\n    parsed = _convert_bytes_to_str(parsed)\n    expected['ResponseMetadata']['HTTPStatusCode'] = response['status_code']\n    expected['ResponseMetadata']['HTTPHeaders'] = response['headers']\n\n    d2 = parsed\n    d1 = expected\n\n    if d1 != d2:\n        log.debug('-' * 40)\n        log.debug(\"XML FILE:\\n\" + xmlfile)\n        log.debug('-' * 40)\n        log.debug(\"ACTUAL:\\n\" + pprint.pformat(parsed))\n        log.debug('-' * 40)\n        log.debug(\"EXPECTED:\\n\" + pprint.pformat(expected))\n    if not d1 == d2:\n        # Borrowed from assertDictEqual, though this doesn't\n        # handle the case when unicode literals are used in one\n        # dict but not in the other (and we want to consider them\n        # as being equal).\n        print(d1)\n        print()\n        print(d2)\n        pretty_d1 = pprint.pformat(d1, width=1).splitlines()\n        pretty_d2 = pprint.pformat(d2, width=1).splitlines()\n        diff = '\\n' + '\\n'.join(difflib.ndiff(pretty_d1, pretty_d2))\n        raise AssertionError(\"Dicts are not equal:\\n%s\" % diff)\n\n\ndef _convert_bytes_to_str(parsed):\n    if isinstance(parsed, dict):\n        new_dict = {}\n        for key, value in parsed.items():\n            new_dict[key] = _convert_bytes_to_str(value)\n        return new_dict\n    elif isinstance(parsed, bytes):\n        return parsed.decode('utf-8')\n    elif isinstance(parsed, list):\n        new_list = []\n        for item in parsed:\n            new_list.append(_convert_bytes_to_str(item))\n        return new_list\n    else:\n        return parsed\n\n\ndef _xml_test_cases():\n    session = create_session()\n    test_cases = []\n    for dp in ['responses', 'errors']:\n        data_path = os.path.join(os.path.dirname(__file__), 'xml')\n        data_path = os.path.join(data_path, dp)\n        xml_files = glob.glob('%s/*.xml' % data_path)\n        service_names = set()\n        for fn in xml_files:\n            service_names.add(os.path.split(fn)[1].split('-')[0])\n        for service_name in service_names:\n            service_model = session.get_service_model(service_name)\n            service_xml_files = glob.glob(f'{data_path}/{service_name}-*.xml')\n            for xmlfile in service_xml_files:\n                expected = _get_expected_parsed_result(xmlfile)\n                operation_model = _get_operation_model(service_model, xmlfile)\n                test_cases.append((xmlfile, operation_model, expected))\n    return sorted(test_cases)\n\n\n@pytest.mark.parametrize(\n    \"xmlfile, operation_model, expected\", _xml_test_cases()\n)\ndef test_xml_parsing(xmlfile, operation_model, expected):\n    _test_parsed_response(xmlfile, operation_model, expected)\n\n\ndef _json_test_cases():\n    # The outputs/ directory has sample output responses\n    # For each file in outputs/ there's a corresponding file\n    # in expected/ that has the expected parsed response.\n    base_dir = os.path.join(os.path.dirname(__file__), 'json')\n    json_responses_dir = os.path.join(base_dir, 'errors')\n    expected_parsed_dir = os.path.join(base_dir, 'expected')\n    session = botocore.session.get_session()\n    json_test_cases = []\n    for json_response_file in os.listdir(json_responses_dir):\n        # Files look like: 'datapipeline-create-pipeline.json'\n        service_name, operation_name = os.path.splitext(json_response_file)[\n            0\n        ].split('-', 1)\n        expected_parsed_response = os.path.join(\n            expected_parsed_dir, json_response_file\n        )\n        raw_response_file = os.path.join(\n            json_responses_dir, json_response_file\n        )\n        with open(expected_parsed_response) as f:\n            expected = json.load(f)\n        service_model = session.get_service_model(service_name)\n        operation_names = service_model.operation_names\n        operation_model = None\n        for op_name in operation_names:\n            if xform_name(op_name) == operation_name.replace('-', '_'):\n                operation_model = service_model.operation_model(op_name)\n        json_test_cases.append((raw_response_file, operation_model, expected))\n    return sorted(json_test_cases)\n\n\n@pytest.mark.parametrize(\n    \"raw_response_file, operation_model, expected\", _json_test_cases()\n)\ndef test_json_errors_parsing(raw_response_file, operation_model, expected):\n    _test_parsed_response(raw_response_file, operation_model, expected)\n", "tests/unit/response_parsing/__init__.py": "# Copyright (c) 2012 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/unit/docs/test_waiter.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.waiter import WaiterDocumenter\nfrom botocore.waiter import WaiterModel\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass TestWaiterDocumenter(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Biz', 'String')\n        self.setup_client()\n        waiter_model = WaiterModel(self.waiter_json_model)\n        self.waiter_documenter = WaiterDocumenter(\n            client=self.client,\n            service_waiter_model=waiter_model,\n            root_docs_path=self.root_services_path,\n        )\n\n    def test_document_waiters(self):\n        self.waiter_documenter.document_waiters(self.doc_structure)\n        self.assert_contains_lines_in_order(\n            [\n                '=======',\n                'Waiters',\n                '=======',\n                'The available waiters are:',\n                'waiter/SampleOperationComplete',\n            ]\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:class:: MyService.Waiter.SampleOperationComplete',\n                '  ::',\n                '    waiter = client.get_waiter(\\'sample_operation_complete\\')',\n                '  .. py:method:: wait(**kwargs)',\n                (\n                    '    Polls :py:meth:`MyService.Client.sample_operation` '\n                    'every 15 seconds until a successful state is reached. An error '\n                    'is returned after 40 failed checks.'\n                ),\n                '    **Request Syntax**',\n                '    ::',\n                '      waiter.wait(',\n                '          Biz=\\'string\\'',\n                '      )',\n                '    :type Biz: string',\n                '    :param Biz:',\n                '    :type WaiterConfig: dict',\n                '    :param WaiterConfig:',\n                (\n                    'A dictionary that provides parameters to control waiting '\n                    'behavior.'\n                ),\n                '     - **Delay** *(integer) --*',\n                (\n                    '        The amount of time in seconds to wait between attempts. '\n                    'Default: 15'\n                ),\n                '      - **MaxAttempts** *(integer) --*',\n                '        The maximum number of attempts to be made. Default: 40',\n                '    :returns: None',\n            ],\n            self.get_nested_service_contents(\n                'myservice', 'waiter', 'SampleOperationComplete'\n            ),\n        )\n", "tests/unit/docs/test_sharedexample.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.compat import OrderedDict\nfrom botocore.docs.sharedexample import (\n    SharedExampleDocumenter,\n    document_shared_examples,\n)\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass TestDocumentSharedExamples(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape({\"foo\": {\"type\": \"string\"}})\n        self.add_shape({\"nested\": {\"type\": \"string\"}})\n        self.add_shape(\n            {\n                \"other\": {\n                    \"type\": \"structure\",\n                    \"members\": {\"nested\": {\"shape\": \"nested\"}},\n                }\n            }\n        )\n        self.add_shape(\n            {\"aloha\": {\"type\": \"list\", \"member\": {\"shape\": \"other\"}}}\n        )\n        self.add_shape_to_params('foo', 'foo')\n        self.add_shape_to_params('aloha', 'aloha')\n        self._examples = [\n            {\n                \"id\": \"sample-id\",\n                \"title\": \"sample-title\",\n                \"description\": \"Sample Description.\",\n                \"input\": OrderedDict(\n                    [\n                        (\"aloha\", [\"other\", {\"nested\": \"fun!\"}]),\n                        (\"foo\", \"bar\"),\n                    ]\n                ),\n                \"output\": OrderedDict(\n                    [\n                        (\"foo\", \"baz\"),\n                    ]\n                ),\n                \"comments\": {\n                    \"input\": {\"aloha\": \"mahalo\"},\n                    \"output\": {\"foo\": \"Sample Comment\"},\n                },\n            }\n        ]\n\n    def test_default(self):\n        document_shared_examples(\n            self.doc_structure,\n            self.operation_model,\n            'response = client.foo',\n            self._examples,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                \"**Examples**\",\n                \"Sample Description.\",\n                \"::\",\n                \"  response = client.foo(\",\n                \"      # mahalo\",\n                \"      aloha=[\",\n                \"          'other',\",\n                \"          {\",\n                \"              'nested': 'fun!',\",\n                \"          },\",\n                \"      ],\",\n                \"      foo='bar',\",\n                \"  )\",\n                \"  print(response)\",\n                \"Expected Output:\",\n                \"::\",\n                \"  {\",\n                \"      # Sample Comment\",\n                \"      'foo': 'baz',\",\n                \"      'ResponseMetadata': {\",\n                \"          '...': '...',\",\n                \"      },\",\n                \"  }\",\n            ]\n        )\n\n\nclass TestSharedExampleDocumenter(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.documenter = SharedExampleDocumenter()\n\n    def test_is_input(self):\n        self.add_shape_to_params('foo', 'String')\n        self.documenter.document_shared_example(\n            example={'input': {'foo': 'bar'}},\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order([\"foo.bar(\", \"    foo='bar'\", \")\"])\n\n    def test_dict_example(self):\n        self.add_shape(\n            {\n                'bar': {\n                    \"type\": \"structure\",\n                    \"members\": {\"bar\": {\"shape\": \"String\"}},\n                }\n            }\n        )\n        self.add_shape_to_params('foo', 'bar')\n        self.documenter.document_shared_example(\n            example={'input': {'foo': {'bar': 'baz'}}},\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order(\n            [\"foo.bar(\", \"    foo={\", \"        'bar': 'baz',\", \"    },\", \")\"]\n        )\n\n    def test_list_example(self):\n        self.add_shape(\n            {\"foo\": {\"type\": \"list\", \"member\": {\"shape\": \"String\"}}}\n        )\n        self.add_shape_to_params('foo', 'foo')\n        self.documenter.document_shared_example(\n            example={'input': {'foo': ['bar']}},\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order(\n            [\"foo.bar(\", \"    foo=[\", \"        'bar',\", \"    ],\", \")\"]\n        )\n\n    def test_can_handle_no_input_key(self):\n        self.add_shape_to_params('foo', 'String')\n        self.documenter.document_shared_example(\n            example={},\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order([\"foo.bar(\", \")\"])\n\n    def test_unicode_string_example(self):\n        self.add_shape_to_params('foo', 'String')\n        self.documenter.document_shared_example(\n            example={'input': {'foo': 'bar'}},\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order([\"foo.bar(\", \"    foo='bar'\", \")\"])\n\n    def test_timestamp_example(self):\n        self.add_shape({'foo': {'type': 'timestamp'}})\n        self.add_shape_to_params('foo', 'foo')\n        self.documenter.document_shared_example(\n            example={'input': {'foo': 'Fri, 20 Nov 2015 21:13:12 GMT'}},\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                \"foo.bar(\",\n                \"    foo=datetime(2015, 11, 20, 21, 13, 12, 4, 324, 0)\",\n                \")\",\n            ]\n        )\n\n    def test_map_example(self):\n        self.add_shape({\"baz\": {\"type\": \"string\"}})\n        self.add_shape(\n            {\n                'bar': {\n                    \"type\": \"map\",\n                    \"key\": {\"shape\": \"baz\"},\n                    \"value\": {\"shape\": \"baz\"},\n                }\n            }\n        )\n        self.add_shape_to_params('foo', 'bar')\n        self.documenter.document_shared_example(\n            example={'input': {'foo': {'bar': 'baz'}}},\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order(\n            [\"foo.bar(\", \"    foo={\", \"        'bar': 'baz',\", \"    },\", \")\"]\n        )\n\n    def test_add_comment(self):\n        self.add_shape_to_params('foo', 'String')\n        self.documenter.document_shared_example(\n            example={\n                'input': {'foo': 'bar'},\n                'comments': {'input': {'foo': 'baz'}},\n            },\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order(\n            [\"foo.bar(\", \"    # baz\", \"    foo='bar',\", \")\"]\n        )\n\n    def test_unicode_exammple(self):\n        self.add_shape_to_params('foo', 'String')\n        self.documenter.document_shared_example(\n            example={'input': {'foo': '\\u2713'}},\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order(\n            [\"foo.bar(\", \"    foo='\\u2713'\", \")\"]\n        )\n\n    def test_escape_character_example(self):\n        self.add_shape_to_params('foo', 'String')\n        self.documenter.document_shared_example(\n            example={'output': {'foo': 'good\\n\\rintentions!\\n\\r'}},\n            prefix='foo.bar',\n            section=self.doc_structure,\n            operation_model=self.operation_model,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                \"Expected Output:\",\n                \"  {\",\n                \"      'foo': 'good\\\\n\\\\rintentions!\\\\n\\\\r',\",\n                \"  }\",\n            ]\n        )\n", "tests/unit/docs/test_example.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.example import (\n    RequestExampleDocumenter,\n    ResponseExampleDocumenter,\n)\nfrom botocore.docs.utils import DocumentedShape\nfrom botocore.hooks import HierarchicalEmitter\nfrom tests import mock\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass BaseExampleDocumenterTest(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.event_emitter = HierarchicalEmitter()\n        self.request_example = RequestExampleDocumenter(\n            service_name='myservice',\n            operation_name='SampleOperation',\n            event_emitter=self.event_emitter,\n        )\n        self.response_example = ResponseExampleDocumenter(\n            service_name='myservice',\n            operation_name='SampleOperation',\n            event_emitter=self.event_emitter,\n        )\n\n\nclass TestDocumentDefaultValue(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  response = myclient.call(',\n                '      Foo=\\'string\\'',\n                '  )',\n            ]\n        )\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n        )\n        self.assert_contains_lines_in_order(\n            ['::', '  {', '      \\'Foo\\': \\'string\\'', '  }']\n        )\n\n\nclass TestDocumentNoMembers(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n        )\n        self.assert_contains_lines_in_order(\n            ['::', '  response = myclient.call()']\n        )\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n        )\n        self.assert_contains_lines_in_order(['::', '  {}'])\n\n\nclass TestTraverseAndDocumentShape(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n        self.event_emitter = mock.Mock()\n        self.request_example = RequestExampleDocumenter(\n            service_name='myservice',\n            operation_name='SampleOperation',\n            event_emitter=self.event_emitter,\n        )\n        self.response_example = ResponseExampleDocumenter(\n            service_name='myservice',\n            operation_name='SampleOperation',\n            event_emitter=self.event_emitter,\n        )\n\n    def test_events_emitted_response_example(self):\n        self.response_example.traverse_and_document_shape(\n            section=self.doc_structure,\n            shape=self.operation_model.input_shape,\n            history=[],\n        )\n        structure_section = self.doc_structure.get_section('structure-value')\n        print(self.event_emitter.emit.call_args_list[0][1]['section'].name)\n        self.assertEqual(\n            self.event_emitter.emit.call_args_list,\n            [\n                mock.call(\n                    'docs.response-example.myservice.SampleOperation.Foo',\n                    section=structure_section.get_section('Foo').get_section(\n                        'member-value'\n                    ),\n                ),\n                mock.call(\n                    (\n                        'docs.response-example.myservice.SampleOperation'\n                        '.complete-section'\n                    ),\n                    section=self.doc_structure,\n                ),\n            ],\n        )\n\n    def test_events_emitted_request_example(self):\n        self.request_example.traverse_and_document_shape(\n            section=self.doc_structure,\n            shape=self.operation_model.input_shape,\n            history=[],\n        )\n        structure_section = self.doc_structure.get_section('structure-value')\n        self.assertEqual(\n            self.event_emitter.emit.call_args_list,\n            [\n                mock.call(\n                    'docs.request-example.myservice.SampleOperation.Foo',\n                    section=structure_section.get_section('Foo').get_section(\n                        'member-value'\n                    ),\n                ),\n                mock.call(\n                    (\n                        'docs.request-example.myservice.SampleOperation'\n                        '.complete-section'\n                    ),\n                    section=self.doc_structure,\n                ),\n            ],\n        )\n\n\nclass TestDocumentEnumValue(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape(\n            {'EnumString': {'type': 'string', 'enum': ['foo', 'bar']}}\n        )\n        self.add_shape_to_params('Foo', 'EnumString', 'This describes foo.')\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  response = myclient.call(',\n                '      Foo=\\'foo\\'|\\'bar\\'',\n                '  )',\n            ]\n        )\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n        )\n        self.assert_contains_lines_in_order(\n            ['::', '  {', '      \\'Foo\\': \\'foo\\'|\\'bar\\'', '  }']\n        )\n\n\nclass TestDocumentMultipleDefaultValues(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n        self.add_shape_to_params(\n            'Bar', 'String', 'This describes bar.', is_required=True\n        )\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  response = myclient.call(',\n                '      Foo=\\'string\\',',\n                '      Bar=\\'string\\'',\n                '  )',\n            ]\n        )\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  {',\n                '      \\'Foo\\': \\'string\\',',\n                '      \\'Bar\\': \\'string\\'',\n                '  }',\n            ]\n        )\n\n\nclass TestDocumentInclude(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n        self.include_params = [\n            DocumentedShape(\n                name='Baz',\n                type_name='integer',\n                documentation='This describes baz.',\n            )\n        ]\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n            include=self.include_params,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  response = myclient.call(',\n                '      Foo=\\'string\\',',\n                '      Baz=123',\n                '  )',\n            ]\n        )\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            include=self.include_params,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  {',\n                '      \\'Foo\\': \\'string\\',',\n                '      \\'Baz\\': 123',\n                '  }',\n            ]\n        )\n\n\nclass TestDocumentExclude(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n        self.add_shape_to_params(\n            'Bar', 'String', 'This describes bar.', is_required=True\n        )\n        self.exclude_params = ['Foo']\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n            exclude=self.exclude_params,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  response = myclient.call(',\n                '      Bar=\\'string\\'',\n                '  )',\n            ]\n        )\n        self.assert_not_contains_line('      Foo=\\'string\\'')\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            exclude=self.exclude_params,\n        )\n        self.assert_contains_lines_in_order(\n            ['::', '  {', '      \\'Bar\\': \\'string\\'', '  }']\n        )\n        self.assert_not_contains_line('\\'Foo\\': \\'string\\',')\n\n\nclass TestDocumentList(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape(\n            {\n                'List': {\n                    'type': 'list',\n                    'member': {\n                        'shape': 'String',\n                        'documentation': 'A string element',\n                    },\n                }\n            }\n        )\n        self.add_shape_to_params('Foo', 'List', 'This describes the list.')\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  response = myclient.call(',\n                '      Foo=[',\n                '          \\'string\\',',\n                '      ]',\n                '  )',\n            ]\n        )\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  {',\n                '      \\'Foo\\': [',\n                '          \\'string\\',',\n                '      ]',\n                '  }',\n            ]\n        )\n\n\nclass TestDocumentMap(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape(\n            {\n                'Map': {\n                    'type': 'map',\n                    'key': {'shape': 'String'},\n                    'value': {'shape': 'String'},\n                }\n            }\n        )\n        self.add_shape_to_params('Foo', 'Map', 'This describes the map.')\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  response = myclient.call(',\n                '      Foo={',\n                '          \\'string\\': \\'string\\'',\n                '      }',\n                '  )',\n            ]\n        )\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  {',\n                '      \\'Foo\\': {',\n                '          \\'string\\': \\'string\\'',\n                '      }',\n                '  }',\n            ]\n        )\n\n\nclass TestDocumentStructure(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape(\n            {\n                'Structure': {\n                    'type': 'structure',\n                    'members': {\n                        'Member': {\n                            'shape': 'String',\n                            'documentation': 'This is its member.',\n                        }\n                    },\n                }\n            }\n        )\n        self.add_shape_to_params(\n            'Foo', 'Structure', 'This describes the structure.'\n        )\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  response = myclient.call(',\n                '      Foo={',\n                '          \\'Member\\': \\'string\\'',\n                '      }',\n                '  )',\n            ]\n        )\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  {',\n                '      \\'Foo\\': {',\n                '          \\'Member\\': \\'string\\'',\n                '      }',\n                '  }',\n            ]\n        )\n\n\nclass TestDocumentRecursiveShape(BaseExampleDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape(\n            {\n                'Structure': {\n                    'type': 'structure',\n                    'members': {\n                        'Foo': {\n                            'shape': 'Structure',\n                            'documentation': 'This is a recursive structure.',\n                        }\n                    },\n                }\n            }\n        )\n        self.add_shape_to_params(\n            'Foo', 'Structure', 'This describes the structure.'\n        )\n\n    def test_request_example(self):\n        self.request_example.document_example(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            prefix='response = myclient.call',\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  response = myclient.call(',\n                '      Foo={',\n                '          \\'Foo\\': {\\'... recursive ...\\'}',\n                '      }',\n                '  )',\n            ]\n        )\n\n    def test_response_example(self):\n        self.response_example.document_example(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '::',\n                '  {',\n                '      \\'Foo\\': {',\n                '          \\'Foo\\': {\\'... recursive ...\\'}',\n                '      }',\n                '  }',\n            ]\n        )\n", "tests/unit/docs/test_service.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nfrom botocore.docs.service import ServiceDocumenter\nfrom botocore.session import get_session\nfrom tests import mock\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass TestServiceDocumenter(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.setup_documenter()\n\n    def setup_documenter(self):\n        self.add_shape_to_params('Biz', 'String')\n        self.setup_client()\n        with mock.patch(\n            'botocore.session.create_loader', return_value=self.loader\n        ):\n            session = get_session()\n            self.service_documenter = ServiceDocumenter(\n                'myservice', session, self.root_services_path\n            )\n\n    def test_document_service(self):\n        # Note that not everything will be included as it is just\n        # a smoke test to make sure all of the main parts are inluded.\n        contents = self.service_documenter.document_service().decode('utf-8')\n        lines = [\n            '*********',\n            'MyService',\n            '*********',\n            '======',\n            'Client',\n            '======',\n            '.. py:class:: MyService.Client',\n            '  A low-level client representing AWS MyService',\n            '  AWS MyService Description',\n            '    client = session.create_client(\\'myservice\\')',\n            'These are the available methods:',\n            '  myservice/client/sample_operation',\n            '=================',\n            'Client Exceptions',\n            '=================',\n            'Client exceptions are available on a client instance ',\n            'via the ``exceptions`` property. For more detailed instructions ',\n            'and examples on the exact usage of client exceptions, see the ',\n            'error handling ',\n            'Client exceptions are available',\n            '==========',\n            'Paginators',\n            '==========',\n            'Paginators are available on a client instance',\n            'via the ``get_paginator`` method. For more detailed instructions ',\n            'and examples on the usage of paginators, see the paginators',\n            'The available paginators are:',\n            '  myservice/paginator/SampleOperation',\n            '=======',\n            'Waiters',\n            '=======',\n            'Waiters are available on a client instance ',\n            'via the ``get_waiter`` method. For more detailed instructions ',\n            'and examples on the usage or waiters, see the waiters',\n            '  myservice/waiter/SampleOperationComplete',\n        ]\n        for line in lines:\n            self.assertIn(line, contents)\n\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:method:: MyService.Client.sample_operation(**kwargs)',\n                '  **Examples**',\n                '  Sample Description.',\n                '  ::',\n                '    response = client.sample_operation(',\n            ],\n            self.get_nested_service_contents(\n                'myservice', 'client', 'sample_operation'\n            ),\n        )\n\n    def test_document_service_no_paginator(self):\n        os.remove(self.paginator_model_file)\n        contents = self.service_documenter.document_service().decode('utf-8')\n        self.assertNotIn('Paginators', contents)\n\n    def test_document_service_no_waiter(self):\n        os.remove(self.waiter_model_file)\n        contents = self.service_documenter.document_service().decode('utf-8')\n        self.assertNotIn('Waiters', contents)\n\n    def test_document_service_no_context_params(self):\n        contents = self.service_documenter.document_service().decode('utf-8')\n        self.assertNotIn('Client Context Parameters', contents)\n\n    def test_document_service_context_params(self):\n        self.json_model['clientContextParams'] = {\n            'ClientContextParam1': {\n                'type': 'string',\n                'documentation': 'A client context param',\n            },\n            'ClientContextParam2': {\n                'type': 'boolean',\n                'documentation': 'A second client context param',\n            },\n        }\n        self.setup_documenter()\n        contents = self.service_documenter.document_service().decode('utf-8')\n        self.assertIn('Client Context Parameters', contents)\n", "tests/unit/docs/test_params.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.params import (\n    RequestParamsDocumenter,\n    ResponseParamsDocumenter,\n)\nfrom botocore.docs.utils import DocumentedShape\nfrom botocore.hooks import HierarchicalEmitter\nfrom tests import mock\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass BaseParamsDocumenterTest(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.event_emitter = HierarchicalEmitter()\n        self.request_params = RequestParamsDocumenter(\n            service_name='myservice',\n            operation_name='SampleOperation',\n            event_emitter=self.event_emitter,\n        )\n        self.response_params = ResponseParamsDocumenter(\n            service_name='myservice',\n            operation_name='SampleOperation',\n            event_emitter=self.event_emitter,\n        )\n\n\nclass TestDocumentDefaultValue(BaseParamsDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n\n    def test_request_params(self):\n        self.request_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [':type Foo: string', ':param Foo: This describes foo.']\n        )\n\n    def test_response_params(self):\n        self.response_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            ['- *(dict) --*', '  - **Foo** *(string) --* This describes foo.']\n        )\n\n\nclass TestTraverseAndDocumentShape(BaseParamsDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n        self.event_emitter = mock.Mock()\n        self.request_params = RequestParamsDocumenter(\n            service_name='myservice',\n            operation_name='SampleOperation',\n            event_emitter=self.event_emitter,\n        )\n        self.response_params = ResponseParamsDocumenter(\n            service_name='myservice',\n            operation_name='SampleOperation',\n            event_emitter=self.event_emitter,\n        )\n\n    def test_events_emitted_response_params(self):\n        self.response_params.traverse_and_document_shape(\n            section=self.doc_structure,\n            shape=self.operation_model.input_shape,\n            history=[],\n        )\n        self.assertEqual(\n            self.event_emitter.emit.call_args_list,\n            [\n                mock.call(\n                    'docs.response-params.myservice.SampleOperation.Foo',\n                    section=self.doc_structure.get_section('Foo'),\n                ),\n                mock.call(\n                    (\n                        'docs.response-params.myservice.SampleOperation'\n                        '.complete-section'\n                    ),\n                    section=self.doc_structure,\n                ),\n            ],\n        )\n\n    def test_events_emitted_request_params(self):\n        self.request_params.traverse_and_document_shape(\n            section=self.doc_structure,\n            shape=self.operation_model.input_shape,\n            history=[],\n        )\n        self.assertEqual(\n            self.event_emitter.emit.call_args_list,\n            [\n                mock.call(\n                    'docs.request-params.myservice.SampleOperation.Foo',\n                    section=self.doc_structure.get_section('Foo'),\n                ),\n                mock.call(\n                    (\n                        'docs.request-params.myservice.SampleOperation'\n                        '.complete-section'\n                    ),\n                    section=self.doc_structure,\n                ),\n            ],\n        )\n\n\nclass TestDocumentMultipleDefaultValues(BaseParamsDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n        self.add_shape_to_params(\n            'Bar', 'String', 'This describes bar.', is_required=True\n        )\n\n    def test_request_params(self):\n        self.request_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                ':type Foo: string',\n                ':param Foo: This describes foo.',\n                ':type Bar: string',\n                ':param Bar: **[REQUIRED]** This describes bar.',\n            ]\n        )\n\n    def test_response_params(self):\n        self.response_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '- *(dict) --*',\n                '  - **Foo** *(string) --* This describes foo.',\n                '  - **Bar** *(string) --* This describes bar.',\n            ]\n        )\n\n\nclass TestDocumentInclude(BaseParamsDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n        self.include_params = [\n            DocumentedShape(\n                name='Baz',\n                type_name='integer',\n                documentation='This describes baz.',\n            )\n        ]\n\n    def test_request_params(self):\n        self.request_params.document_params(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            include=self.include_params,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                ':type Foo: string',\n                ':param Foo: This describes foo.',\n                ':type Baz: int',\n                ':param Baz: This describes baz.',\n            ]\n        )\n\n    def test_response_params(self):\n        self.response_params.document_params(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            include=self.include_params,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '- *(dict) --*',\n                '  - **Foo** *(string) --* This describes foo.',\n                '  - **Baz** *(integer) --* This describes baz.',\n            ]\n        )\n\n\nclass TestDocumentExclude(BaseParamsDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String', 'This describes foo.')\n        self.add_shape_to_params(\n            'Bar', 'String', 'This describes bar.', is_required=True\n        )\n        self.exclude_params = ['Foo']\n\n    def test_request_params(self):\n        self.request_params.document_params(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            exclude=self.exclude_params,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                ':type Bar: string',\n                ':param Bar: **[REQUIRED]** This describes bar.',\n            ]\n        )\n        self.assert_not_contains_lines(\n            [':type Foo: string', ':param Foo: This describes foo.']\n        )\n\n    def test_response_params(self):\n        self.response_params.document_params(\n            self.doc_structure,\n            self.operation_model.input_shape,\n            exclude=self.exclude_params,\n        )\n        self.assert_contains_lines_in_order(\n            ['- *(dict) --*', '  - **Bar** *(string) --* This describes bar.']\n        )\n        self.assert_not_contains_line(\n            '  - **Foo** *(string) --* This describes foo.'\n        )\n\n\nclass TestDocumentList(BaseParamsDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape(\n            {\n                'List': {\n                    'type': 'list',\n                    'member': {\n                        'shape': 'String',\n                        'documentation': 'A string element',\n                    },\n                }\n            }\n        )\n        self.add_shape_to_params(\n            'Foo',\n            'List',\n            'This describes the list. Each element of this list is a string.',\n        )\n\n    def test_request_params(self):\n        self.request_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                ':type Foo: list',\n                ':param Foo: This describes the list.',\n                '  - *(string) --* A string element',\n            ]\n        )\n\n    def test_response_params(self):\n        self.response_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '- *(dict) --*',\n                (\n                    '  - **Foo** *(list) --* This describes the list. '\n                    'Each element of this list is a string.'\n                ),\n                '    - *(string) --* A string element',\n            ]\n        )\n\n\nclass TestDocumentMap(BaseParamsDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape(\n            {\n                'Map': {\n                    'type': 'map',\n                    'key': {'shape': 'String'},\n                    'value': {'shape': 'String'},\n                }\n            }\n        )\n        self.add_shape_to_params('Foo', 'Map', 'This describes the map.')\n\n    def test_request_params(self):\n        self.request_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                ':type Foo: dict',\n                ':param Foo: This describes the map.',\n                '  - *(string) --*',\n                '    - *(string) --*',\n            ]\n        )\n\n    def test_response_params(self):\n        self.response_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '- *(dict) --*',\n                '  - **Foo** *(dict) --* This describes the map.',\n                '    - *(string) --*',\n                '      - *(string) --*',\n            ]\n        )\n\n\nclass TestDocumentStructure(BaseParamsDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape(\n            {\n                'Structure': {\n                    'type': 'structure',\n                    'members': {\n                        'Member': {\n                            'shape': 'String',\n                            'documentation': 'This is its member.',\n                        }\n                    },\n                }\n            }\n        )\n        self.add_shape_to_params(\n            'Foo', 'Structure', 'This describes the structure.'\n        )\n\n    def test_request_params(self):\n        self.request_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                ':type Foo: dict',\n                ':param Foo: This describes the structure.',\n                '  - **Member** *(string) --* This is its member.',\n            ]\n        )\n\n    def test_response_params(self):\n        self.response_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '- *(dict) --*',\n                '  - **Foo** *(dict) --* This describes the structure.',\n                '    - **Member** *(string) --* This is its member.',\n            ]\n        )\n\n\nclass TestDocumentRecursiveShape(BaseParamsDocumenterTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape(\n            {\n                'Structure': {\n                    'type': 'structure',\n                    'members': {\n                        'Foo': {\n                            'shape': 'Structure',\n                            'documentation': 'This is a recursive structure.',\n                        }\n                    },\n                }\n            }\n        )\n        self.add_shape_to_params(\n            'Foo', 'Structure', 'This describes the structure.'\n        )\n\n    def test_request_params(self):\n        self.request_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                ':type Foo: dict',\n                ':param Foo: This describes the structure.',\n                '  - **Foo** *(dict) --* This is a recursive structure.',\n            ]\n        )\n\n    def test_response_params(self):\n        self.response_params.document_params(\n            self.doc_structure, self.operation_model.input_shape\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '- *(dict) --*',\n                '  - **Foo** *(dict) --* This is a recursive structure.',\n            ]\n        )\n", "tests/unit/docs/test_docs.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nfrom botocore.docs import generate_docs\nfrom botocore.session import get_session\nfrom tests import mock\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass TestGenerateDocs(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.loader_patch = mock.patch(\n            'botocore.session.create_loader', return_value=self.loader\n        )\n        self.available_service_patch = mock.patch(\n            'botocore.session.Session.get_available_services',\n            return_value=['myservice'],\n        )\n        self.loader_patch.start()\n        self.available_service_patch.start()\n\n    def tearDown(self):\n        super().tearDown()\n        self.loader_patch.stop()\n        self.available_service_patch.stop()\n\n    def test_generate_docs(self):\n        session = get_session()\n        # Have the rst files get written to the temporary directory\n        generate_docs(self.docs_root_dir, session)\n\n        reference_service_path = os.path.join(\n            self.root_services_path, 'myservice.rst'\n        )\n        self.assertTrue(os.path.exists(reference_service_path))\n\n        # Make sure the rst file has some the expected contents.\n        with open(reference_service_path) as f:\n            contents = f.read()\n            self.assertIn('AWS MyService', contents)\n            self.assertIn('Client', contents)\n            self.assertIn('Paginators', contents)\n            self.assertIn('Waiters', contents)\n", "tests/unit/docs/test_client.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.client import (\n    ClientContextParamsDocumenter,\n    ClientDocumenter,\n    ClientExceptionsDocumenter,\n)\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass TestClientDocumenter(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        exception_shape = {\n            'SomeException': {\n                'exception': True,\n                'type': 'structure',\n                'members': {'Message': {'shape': 'String'}},\n            }\n        }\n        self.add_shape(exception_shape)\n        self.add_shape_to_params('Biz', 'String')\n        self.add_shape_to_errors('SomeException')\n        self.setup_client()\n        self.client_documenter = ClientDocumenter(\n            self.client, self.root_services_path\n        )\n\n    def test_document_client(self):\n        self.client_documenter.document_client(self.doc_structure)\n        self.assert_contains_lines_in_order(\n            [\n                '======',\n                'Client',\n                '======',\n                '.. py:class:: MyService.Client',\n                '  A low-level client representing AWS MyService',\n                '  AWS MyService Description',\n                '    client = session.create_client(\\'myservice\\')',\n                'These are the available methods:',\n                '  myservice/client/can_paginate',\n                '  myservice/client/get_paginator',\n                '  myservice/client/get_waiter',\n                '  myservice/client/sample_operation',\n            ]\n        )\n        self.assert_contains_lines_in_order(\n            ['.. py:method:: MyService.Client.can_paginate(operation_name)'],\n            self.get_nested_service_contents(\n                'myservice', 'client', 'can_paginate'\n            ),\n        )\n        self.assert_contains_lines_in_order(\n            ['.. py:method:: MyService.Client.get_paginator(operation_name)'],\n            self.get_nested_service_contents(\n                'myservice', 'client', 'get_paginator'\n            ),\n        )\n        self.assert_contains_lines_in_order(\n            ['.. py:method:: MyService.Client.get_waiter(waiter_name)'],\n            self.get_nested_service_contents(\n                'myservice', 'client', 'get_waiter'\n            ),\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:method:: MyService.Client.sample_operation(**kwargs)',\n                '  **Request Syntax**',\n                '  ::',\n                '    response = client.sample_operation(',\n                '        Biz=\\'string\\'',\n                '    )',\n                '  :type Biz: string',\n                '  :param Biz:',\n                '  :rtype: dict',\n                '  :returns:',\n                '    **Response Syntax**',\n                '    ::',\n                '      {',\n                '          \\'Biz\\': \\'string\\'',\n                '      }',\n                '    **Response Structure**',\n                '    - *(dict) --*',\n                '      - **Biz** *(string) --*',\n                '**Exceptions**',\n                '*   :py:class:`MyService.Client.exceptions.SomeException`',\n            ],\n            self.get_nested_service_contents(\n                'myservice', 'client', 'sample_operation'\n            ),\n        )\n\n\nclass TestClientExceptionsDocumenter(BaseDocsTest):\n    def setup_documenter(self):\n        self.setup_client()\n        self.exceptions_documenter = ClientExceptionsDocumenter(\n            self.client, self.root_services_path\n        )\n\n    def test_no_modeled_exceptions(self):\n        self.setup_documenter()\n        self.exceptions_documenter.document_exceptions(self.doc_structure)\n        self.assert_contains_lines_in_order(\n            [\n                '=================',\n                'Client Exceptions',\n                '=================',\n                'Client exceptions are available',\n                'This client has no modeled exception classes.',\n            ]\n        )\n\n    def test_modeled_exceptions(self):\n        exception_shape = {\n            'SomeException': {\n                'exception': True,\n                'type': 'structure',\n                'members': {'Message': {'shape': 'String'}},\n            }\n        }\n        self.add_shape(exception_shape)\n        self.setup_documenter()\n        self.exceptions_documenter.document_exceptions(self.doc_structure)\n        self.assert_contains_lines_in_order(\n            [\n                '=================',\n                'Client Exceptions',\n                '=================',\n                'Client exceptions are available',\n                'The available client exceptions are:',\n                '.. toctree::',\n                ':maxdepth: 1',\n                ':titlesonly:',\n                '  myservice/client/exceptions/SomeException',\n            ]\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:class:: MyService.Client.exceptions.SomeException',\n                '**Example**',\n                '::',\n                'except client.exceptions.SomeException as e:',\n                '.. py:attribute:: response',\n                '**Syntax**',\n                '{',\n                \"'Message': 'string',\",\n                \"'Error': {\",\n                \"'Code': 'string',\",\n                \"'Message': 'string'\",\n                '}',\n                '}',\n                '**Structure**',\n                '- *(dict) --*',\n                '- **Message** *(string) --* ',\n                '- **Error** *(dict) --* ',\n                '- **Code** *(string) --* ',\n                '- **Message** *(string) --* ',\n            ],\n            self.get_nested_service_contents(\n                'myservice', 'client/exceptions', 'SomeException'\n            ),\n        )\n\n\nclass TestClientContextParamsDocumenter(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.json_model['clientContextParams'] = {\n            'ClientContextParam1': {\n                'type': 'string',\n                'documentation': 'A client context param',\n            },\n            'ClientContextParam2': {\n                'type': 'boolean',\n                'documentation': 'A second client context param',\n            },\n        }\n        self.setup_client()\n        service_model = self.client.meta.service_model\n        self.context_params_documenter = ClientContextParamsDocumenter(\n            service_model.service_name, service_model.client_context_parameters\n        )\n\n    def test_client_context_params(self):\n        self.context_params_documenter.document_context_params(\n            self.doc_structure\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '========================',\n                'Client Context Parameters',\n                '========================',\n                'Client context parameters are configurable',\n                'The available ``myservice`` client context params are:',\n                '* ``client_context_param1`` (string) - A client context param',\n                '* ``client_context_param2`` (boolean) - A second client context param',\n            ]\n        )\n", "tests/unit/docs/test_paginator.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.paginator import PaginatorDocumenter\nfrom botocore.paginate import PaginatorModel\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass TestPaginatorDocumenter(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Biz', 'String')\n        self.extra_setup()\n\n    def extra_setup(self):\n        self.setup_client()\n        paginator_model = PaginatorModel(self.paginator_json_model)\n        self.paginator_documenter = PaginatorDocumenter(\n            client=self.client,\n            service_paginator_model=paginator_model,\n            root_docs_path=self.root_services_path,\n        )\n\n    def test_document_paginators(self):\n        self.paginator_documenter.document_paginators(self.doc_structure)\n        self.assert_contains_lines_in_order(\n            [\n                '==========',\n                'Paginators',\n                '==========',\n                'The available paginators are:',\n                'paginator/SampleOperation',\n            ]\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:class:: MyService.Paginator.SampleOperation',\n                '  ::',\n                '    paginator = client.get_paginator(\\'sample_operation\\')',\n                '  .. py:method:: paginate(**kwargs)',\n                (\n                    '    Creates an iterator that will paginate through responses'\n                    ' from :py:meth:`MyService.Client.sample_operation`.'\n                ),\n                '    **Request Syntax**',\n                '    ::',\n                '      response_iterator = paginator.paginate(',\n                '          Biz=\\'string\\',',\n                '          PaginationConfig={',\n                '              \\'MaxItems\\': 123,',\n                '              \\'PageSize\\': 123,',\n                '              \\'StartingToken\\': \\'string\\'',\n                '          }',\n                '      )',\n                '    :type Biz: string',\n                '    :param Biz:',\n                '    :type PaginationConfig: dict',\n                '    :param PaginationConfig:',\n                (\n                    '      A dictionary that provides parameters to '\n                    'control pagination.'\n                ),\n                '      - **MaxItems** *(integer) --*',\n                '      - **PageSize** *(integer) --*',\n                '      - **StartingToken** *(string) --*',\n                '    :rtype: dict',\n                '    :returns:',\n                '      **Response Syntax**',\n                '      ::',\n                '        {',\n                '            \\'Biz\\': \\'string\\',',\n                '            \\'NextToken\\': \\'string\\'',\n                '        }',\n                '      **Response Structure**',\n                '      - *(dict) --*',\n                '        - **Biz** *(string) --*',\n                '        - **NextToken** *(string) --*',\n            ],\n            self.get_nested_service_contents(\n                'myservice', 'paginator', 'SampleOperation'\n            ),\n        )\n\n    def test_no_page_size_if_no_limit_key(self):\n        paginator = self.paginator_json_model[\"pagination\"]\n        operation = paginator[\"SampleOperation\"]\n        del operation[\"limit_key\"]\n\n        self.paginator_documenter.document_paginators(self.doc_structure)\n        self.assert_not_contains_lines(\n            [\n                '              \\'PageSize\\': 123,',\n                '      - **PageSize** *(integer) --*',\n            ]\n        )\n", "tests/unit/docs/test_docstring.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.docstring import (\n    ClientMethodDocstring,\n    LazyLoadedDocstring,\n    PaginatorDocstring,\n    WaiterDocstring,\n)\nfrom tests import mock, unittest\n\n\nclass MockedLazyLoadedDocstring(LazyLoadedDocstring):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mocked_writer_method = mock.Mock()\n\n    def _write_docstring(self, *args, **kwargs):\n        self.mocked_writer_method(*args, **kwargs)\n\n\nclass TestLazyLoadedDocstring(unittest.TestCase):\n    def test_raises_not_implemented(self):\n        with self.assertRaises(NotImplementedError):\n            str(LazyLoadedDocstring())\n\n    def test_expandtabs(self):\n        docstring = MockedLazyLoadedDocstring()\n        docstring.mocked_writer_method.side_effect = (\n            lambda section: section.write('foo\\t')\n        )\n        self.assertEqual('foo ', docstring.expandtabs(1))\n\n    def test_str(self):\n        docstring = MockedLazyLoadedDocstring()\n        docstring.mocked_writer_method.side_effect = (\n            lambda section: section.write('foo')\n        )\n        self.assertEqual('foo', str(docstring))\n\n    def test_repr(self):\n        docstring = MockedLazyLoadedDocstring()\n        docstring.mocked_writer_method.side_effect = (\n            lambda section: section.write('foo')\n        )\n        self.assertEqual('foo', repr(docstring))\n\n    def test_is_lazy_loaded(self):\n        docstring = MockedLazyLoadedDocstring()\n        str(docstring)\n        str(docstring)\n        # The mock.ANY represents the DocumentStructure that is filled out.\n        docstring.mocked_writer_method.assert_called_once_with(mock.ANY)\n\n    def test_args_kwargs_passed(self):\n        args = ['foo', 'bar']\n        kwargs = {'biz': 'baz'}\n        docstring = MockedLazyLoadedDocstring(*args, **kwargs)\n        str(docstring)\n        # The mock.ANY represents the DocumentStructure that is filled out.\n        docstring.mocked_writer_method.assert_called_with(\n            mock.ANY, *args, **kwargs\n        )\n\n\nclass TestClientMethodDocstring(unittest.TestCase):\n    def test_use_correct_docstring_writer(self):\n        with mock.patch(\n            'botocore.docs.docstring' '.document_model_driven_method'\n        ) as mock_writer:\n            docstring = ClientMethodDocstring()\n            str(docstring)\n            self.assertTrue(mock_writer.called)\n\n\nclass TestWaiterDocstring(unittest.TestCase):\n    def test_use_correct_docstring_writer(self):\n        with mock.patch(\n            'botocore.docs.docstring' '.document_wait_method'\n        ) as mock_writer:\n            docstring = WaiterDocstring()\n            str(docstring)\n            self.assertTrue(mock_writer.called)\n\n\nclass TestPaginatorDocstring(unittest.TestCase):\n    def test_use_correct_docstring_writer(self):\n        with mock.patch(\n            'botocore.docs.docstring' '.document_paginate_method'\n        ) as mock_writer:\n            docstring = PaginatorDocstring()\n            str(docstring)\n            self.assertTrue(mock_writer.called)\n", "tests/unit/docs/__init__.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport os\nimport shutil\nimport tempfile\n\nfrom botocore.client import ClientCreator\nfrom botocore.compat import OrderedDict\nfrom botocore.configprovider import ConfigValueStore\nfrom botocore.docs.bcdoc.restdoc import DocumentStructure\nfrom botocore.hooks import HierarchicalEmitter\nfrom botocore.loaders import Loader\nfrom botocore.model import OperationModel, ServiceModel\nfrom tests import mock, unittest\n\n\nclass BaseDocsTest(unittest.TestCase):\n    def setUp(self):\n        self.root_dir = tempfile.mkdtemp()\n        self.version_dirs = os.path.join(\n            self.root_dir, 'myservice', '2014-01-01'\n        )\n        os.makedirs(self.version_dirs)\n        self.model_file = os.path.join(self.version_dirs, 'service-2.json')\n        self.waiter_model_file = os.path.join(\n            self.version_dirs, 'waiters-2.json'\n        )\n        self.paginator_model_file = os.path.join(\n            self.version_dirs, 'paginators-1.json'\n        )\n        self.example_model_file = os.path.join(\n            self.version_dirs, 'examples-1.json'\n        )\n        self.docs_root_dir = tempfile.mkdtemp()\n        self.root_services_path = os.path.join(\n            self.docs_root_dir, 'reference', 'services'\n        )\n\n        self.json_model = {}\n        self.nested_json_model = {}\n        self._setup_models()\n        self.build_models()\n        self.events = HierarchicalEmitter()\n        self.setup_client()\n        self.doc_name = 'MyDoc'\n        self.doc_structure = DocumentStructure(self.doc_name, target='html')\n\n    def tearDown(self):\n        shutil.rmtree(self.root_dir)\n        shutil.rmtree(self.docs_root_dir)\n\n    def setup_client(self):\n        with open(self.example_model_file, 'w') as f:\n            json.dump(self.example_json_model, f)\n\n        with open(self.waiter_model_file, 'w') as f:\n            json.dump(self.waiter_json_model, f)\n\n        with open(self.paginator_model_file, 'w') as f:\n            json.dump(self.paginator_json_model, f)\n\n        with open(self.model_file, 'w') as f:\n            json.dump(self.json_model, f)\n\n        myservice_endpoint_rule_set = {\n            \"version\": \"1.3\",\n            \"parameters\": {},\n            \"rules\": [\n                {\n                    \"conditions\": [],\n                    \"endpoint\": {\n                        \"url\": \"https://example.com\",\n                        \"properties\": {},\n                        \"headers\": {},\n                    },\n                    \"type\": \"endpoint\",\n                }\n            ],\n        }\n\n        def load_service_mock(*args, **kwargs):\n            if args[1] == \"endpoint-rule-set-1\":\n                return myservice_endpoint_rule_set\n            else:\n                return Loader(\n                    extra_search_paths=[self.root_dir]\n                ).load_service_model(*args, **kwargs)\n\n        self.loader = Loader(extra_search_paths=[self.root_dir])\n        self.loader.load_service_model = mock.Mock()\n        self.loader.load_service_model.side_effect = load_service_mock\n\n        endpoint_resolver = mock.Mock()\n        endpoint_resolver.construct_endpoint.return_value = {\n            'hostname': 'foo.us-east-1',\n            'partition': 'aws',\n            'endpointName': 'us-east-1',\n            'signatureVersions': ['v4'],\n        }\n\n        self.creator = ClientCreator(\n            loader=self.loader,\n            endpoint_resolver=endpoint_resolver,\n            user_agent='user-agent',\n            event_emitter=self.events,\n            retry_handler_factory=mock.Mock(),\n            retry_config_translator=mock.Mock(),\n            exceptions_factory=mock.Mock(),\n            config_store=ConfigValueStore(),\n        )\n\n        self.client = self.creator.create_client('myservice', 'us-east-1')\n\n    def _setup_models(self):\n        self.json_model = {\n            'metadata': {\n                'apiVersion': '2014-01-01',\n                'endpointPrefix': 'myservice',\n                'signatureVersion': 'v4',\n                'serviceFullName': 'AWS MyService',\n                'uid': 'myservice-2014-01-01',\n                'protocol': 'query',\n                'serviceId': 'MyService',\n            },\n            'operations': {\n                'SampleOperation': {\n                    'name': 'SampleOperation',\n                    'input': {'shape': 'SampleOperationInputOutput'},\n                    'output': {'shape': 'SampleOperationInputOutput'},\n                }\n            },\n            'shapes': {\n                'SampleOperationInputOutput': {\n                    'type': 'structure',\n                    'members': OrderedDict(),\n                },\n                'String': {'type': 'string'},\n            },\n            'documentation': 'AWS MyService Description',\n        }\n\n        self.waiter_json_model = {\n            \"version\": 2,\n            \"waiters\": {\n                \"SampleOperationComplete\": {\n                    \"delay\": 15,\n                    \"operation\": \"SampleOperation\",\n                    \"maxAttempts\": 40,\n                    \"acceptors\": [\n                        {\n                            \"expected\": \"complete\",\n                            \"matcher\": \"pathAll\",\n                            \"state\": \"success\",\n                            \"argument\": \"Biz\",\n                        },\n                        {\n                            \"expected\": \"failed\",\n                            \"matcher\": \"pathAny\",\n                            \"state\": \"failure\",\n                            \"argument\": \"Biz\",\n                        },\n                    ],\n                }\n            },\n        }\n\n        self.paginator_json_model = {\n            \"pagination\": {\n                \"SampleOperation\": {\n                    \"input_token\": \"NextResult\",\n                    \"output_token\": \"NextResult\",\n                    \"limit_key\": \"MaxResults\",\n                    \"result_key\": \"Biz\",\n                }\n            }\n        }\n\n        self.example_json_model = {\n            \"version\": 1,\n            \"examples\": {\n                \"SampleOperation\": [\n                    {\n                        \"id\": \"sample-id\",\n                        \"title\": \"sample-title\",\n                        \"description\": \"Sample Description.\",\n                        \"input\": OrderedDict(\n                            [\n                                (\"Biz\", \"foo\"),\n                            ]\n                        ),\n                        \"comments\": {\n                            \"input\": {\"Biz\": \"bar\"},\n                        },\n                    }\n                ]\n            },\n        }\n\n    def get_nested_service_contents(self, service, type, name):\n        service_file_path = os.path.join(\n            self.root_services_path, service, type, f'{name}.rst'\n        )\n        with open(service_file_path, 'rb') as f:\n            return f.read().decode('utf-8')\n\n    def build_models(self):\n        self.service_model = ServiceModel(self.json_model)\n        self.operation_model = OperationModel(\n            self.json_model['operations']['SampleOperation'],\n            self.service_model,\n        )\n\n    def add_shape(self, shape):\n        shape_name = list(shape.keys())[0]\n        self.json_model['shapes'][shape_name] = shape[shape_name]\n\n    def add_shape_to_params(\n        self, param_name, shape_name, documentation=None, is_required=False\n    ):\n        params_shape = self.json_model['shapes']['SampleOperationInputOutput']\n        member = {'shape': shape_name}\n        if documentation is not None:\n            member['documentation'] = documentation\n        params_shape['members'][param_name] = member\n\n        if is_required:\n            required_list = params_shape.get('required', [])\n            required_list.append(param_name)\n            params_shape['required'] = required_list\n\n    def add_shape_to_errors(self, shape_name):\n        operation = self.json_model['operations']['SampleOperation']\n        errors = operation.get('errors', [])\n        errors.append({'shape': shape_name})\n        operation['errors'] = errors\n\n    def assert_contains_line(self, line):\n        contents = self.doc_structure.flush_structure().decode('utf-8')\n        self.assertIn(line, contents)\n\n    def assert_contains_lines_in_order(self, lines, contents=None):\n        if contents is None:\n            contents = self.doc_structure.flush_structure().decode('utf-8')\n        for line in lines:\n            self.assertIn(line, contents)\n            beginning = contents.find(line)\n            contents = contents[(beginning + len(line)) :]\n\n    def assert_not_contains_line(self, line):\n        contents = self.doc_structure.flush_structure().decode('utf-8')\n        self.assertNotIn(line, contents)\n\n    def assert_not_contains_lines(self, lines):\n        contents = self.doc_structure.flush_structure().decode('utf-8')\n        for line in lines:\n            self.assertNotIn(line, contents)\n", "tests/unit/docs/test_method.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.method import (\n    document_custom_method,\n    document_custom_signature,\n    document_model_driven_method,\n    document_model_driven_signature,\n    get_instance_public_methods,\n)\nfrom botocore.docs.utils import DocumentedShape\nfrom botocore.hooks import HierarchicalEmitter\nfrom tests import unittest\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass TestGetInstanceMethods(unittest.TestCase):\n    class MySampleClass:\n        def _internal_method(self):\n            pass\n\n        def public_method(self):\n            pass\n\n    def test_get_instance_methods(self):\n        instance = self.MySampleClass()\n        instance_methods = get_instance_public_methods(instance)\n        self.assertEqual(len(instance_methods), 1)\n        self.assertIn('public_method', instance_methods)\n        self.assertEqual(\n            instance.public_method, instance_methods['public_method']\n        )\n\n\nclass TestDocumentModelDrivenSignature(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.add_shape_to_params('Foo', 'String')\n        self.add_shape_to_params('Bar', 'String', is_required=True)\n        self.add_shape_to_params('Baz', 'String')\n\n    def test_document_signature(self):\n        document_model_driven_signature(\n            self.doc_structure, 'my_method', self.operation_model\n        )\n        self.assert_contains_line('.. py:method:: my_method(**kwargs)')\n\n    def test_document_signature_exclude_all_kwargs(self):\n        exclude_params = ['Foo', 'Bar', 'Baz']\n        document_model_driven_signature(\n            self.doc_structure,\n            'my_method',\n            self.operation_model,\n            exclude=exclude_params,\n        )\n        self.assert_contains_line('.. py:method:: my_method()')\n\n    def test_document_signature_exclude_and_include(self):\n        exclude_params = ['Foo', 'Bar', 'Baz']\n        include_params = [\n            DocumentedShape(\n                name='Biz', type_name='integer', documentation='biz docs'\n            )\n        ]\n        document_model_driven_signature(\n            self.doc_structure,\n            'my_method',\n            self.operation_model,\n            include=include_params,\n            exclude=exclude_params,\n        )\n        self.assert_contains_line('.. py:method:: my_method(**kwargs)')\n\n\nclass TestDocumentCustomSignature(BaseDocsTest):\n    def sample_method(self, foo, bar='bar', baz=None):\n        pass\n\n    def test_document_signature(self):\n        document_custom_signature(\n            self.doc_structure, 'my_method', self.sample_method\n        )\n        self.assert_contains_line(\n            '.. py:method:: my_method(foo, bar=\\'bar\\', baz=None)'\n        )\n\n\nclass TestDocumentCustomMethod(BaseDocsTest):\n    def custom_method(self, foo):\n        \"\"\"This is a custom method\n\n        :type foo: string\n        :param foo: The foo parameter\n        \"\"\"\n        pass\n\n    def test_document_custom_signature(self):\n        document_custom_method(\n            self.doc_structure, 'my_method', self.custom_method\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:method:: my_method(foo)',\n                '  This is a custom method',\n                '  :type foo: string',\n                '  :param foo: The foo parameter',\n            ]\n        )\n\n\nclass TestDocumentModelDrivenMethod(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.event_emitter = HierarchicalEmitter()\n        self.add_shape_to_params('Bar', 'String')\n\n    def test_default(self):\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n        )\n        cross_ref_link = (\n            'See also: `AWS API Documentation '\n            '<https://docs.aws.amazon.com/goto/WebAPI'\n            '/myservice-2014-01-01/SampleOperation>'\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:method:: foo(**kwargs)',\n                '  This describes the foo method.',\n                cross_ref_link,\n                '  **Request Syntax**',\n                '  ::',\n                '    response = client.foo(',\n                '        Bar=\\'string\\'',\n                '    )',\n                '  :type Bar: string',\n                '  :param Bar:',\n                '  :rtype: dict',\n                '  :returns:',\n                '    **Response Syntax**',\n                '    ::',\n                '      {',\n                '          \\'Bar\\': \\'string\\'',\n                '      }',\n                '    **Response Structure**',\n                '    - *(dict) --*',\n                '      - **Bar** *(string) --*',\n            ]\n        )\n\n    def test_no_input_output_shape(self):\n        del self.json_model['operations']['SampleOperation']['input']\n        del self.json_model['operations']['SampleOperation']['output']\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:method:: foo()',\n                '  This describes the foo method.',\n                '  **Request Syntax**',\n                '  ::',\n                '    response = client.foo()',\n                '  :returns: None',\n            ]\n        )\n\n    def test_include_input(self):\n        include_params = [\n            DocumentedShape(\n                name='Biz', type_name='string', documentation='biz docs'\n            )\n        ]\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n            include_input=include_params,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:method:: foo(**kwargs)',\n                '  This describes the foo method.',\n                '  **Request Syntax**',\n                '  ::',\n                '    response = client.foo(',\n                '        Bar=\\'string\\',',\n                '        Biz=\\'string\\'',\n                '    )',\n                '  :type Bar: string',\n                '  :param Bar:',\n                '  :type Biz: string',\n                '  :param Biz: biz docs',\n                '  :rtype: dict',\n                '  :returns:',\n                '    **Response Syntax**',\n                '    ::',\n                '      {',\n                '          \\'Bar\\': \\'string\\'',\n                '      }',\n                '    **Response Structure**',\n                '    - *(dict) --*',\n                '      - **Bar** *(string) --*',\n            ]\n        )\n\n    def test_include_output(self):\n        include_params = [\n            DocumentedShape(\n                name='Biz', type_name='string', documentation='biz docs'\n            )\n        ]\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n            include_output=include_params,\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:method:: foo(**kwargs)',\n                '  This describes the foo method.',\n                '  **Request Syntax**',\n                '  ::',\n                '    response = client.foo(',\n                '        Bar=\\'string\\'',\n                '    )',\n                '  :type Bar: string',\n                '  :param Bar:',\n                '  :rtype: dict',\n                '  :returns:',\n                '    **Response Syntax**',\n                '    ::',\n                '      {',\n                '          \\'Bar\\': \\'string\\'',\n                '          \\'Biz\\': \\'string\\'',\n                '      }',\n                '    **Response Structure**',\n                '    - *(dict) --*',\n                '      - **Bar** *(string) --*',\n                '      - **Biz** *(string) --*',\n            ]\n        )\n\n    def test_exclude_input(self):\n        self.add_shape_to_params('Biz', 'String')\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n            exclude_input=['Bar'],\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:method:: foo(**kwargs)',\n                '  This describes the foo method.',\n                '  **Request Syntax**',\n                '  ::',\n                '    response = client.foo(',\n                '        Biz=\\'string\\'',\n                '    )',\n                '  :type Biz: string',\n                '  :param Biz:',\n                '  :rtype: dict',\n                '  :returns:',\n                '    **Response Syntax**',\n                '    ::',\n                '      {',\n                '          \\'Bar\\': \\'string\\'',\n                '          \\'Biz\\': \\'string\\'',\n                '      }',\n                '    **Response Structure**',\n                '    - *(dict) --*',\n                '      - **Bar** *(string) --*',\n                '      - **Biz** *(string) --*',\n            ]\n        )\n        self.assert_not_contains_lines(\n            [':param Bar: string', 'Bar=\\'string\\'']\n        )\n\n    def test_exclude_output(self):\n        self.add_shape_to_params('Biz', 'String')\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n            exclude_output=['Bar'],\n        )\n        self.assert_contains_lines_in_order(\n            [\n                '.. py:method:: foo(**kwargs)',\n                '  This describes the foo method.',\n                '  **Request Syntax**',\n                '  ::',\n                '    response = client.foo(',\n                '        Bar=\\'string\\'',\n                '        Biz=\\'string\\'',\n                '    )',\n                '  :type Biz: string',\n                '  :param Biz:',\n                '  :rtype: dict',\n                '  :returns:',\n                '    **Response Syntax**',\n                '    ::',\n                '      {',\n                '          \\'Biz\\': \\'string\\'',\n                '      }',\n                '    **Response Structure**',\n                '    - *(dict) --*',\n                '      - **Biz** *(string) --*',\n            ]\n        )\n        self.assert_not_contains_lines(\n            [\n                '\\'Bar\\': \\'string\\'',\n                '- **Bar** *(string) --*',\n            ]\n        )\n\n    def test_streaming_body_in_output(self):\n        self.add_shape_to_params('Body', 'Blob')\n        self.json_model['shapes']['Blob'] = {'type': 'blob'}\n        self.json_model['shapes']['SampleOperationInputOutput'][\n            'payload'\n        ] = 'Body'\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n        )\n        self.assert_contains_line('**Body** (:class:`.StreamingBody`)')\n\n    def test_event_stream_body_in_output(self):\n        self.add_shape_to_params('Payload', 'EventStream')\n        self.json_model['shapes']['SampleOperationInputOutput'][\n            'payload'\n        ] = 'Payload'\n        self.json_model['shapes']['EventStream'] = {\n            'type': 'structure',\n            'eventstream': True,\n            'members': {'Event': {'shape': 'Event'}},\n        }\n        self.json_model['shapes']['Event'] = {\n            'type': 'structure',\n            'event': True,\n            'members': {\n                'Fields': {\n                    'shape': 'EventFields',\n                    'eventpayload': True,\n                }\n            },\n        }\n        self.json_model['shapes']['EventFields'] = {\n            'type': 'structure',\n            'members': {'Field': {'shape': 'EventField'}},\n        }\n        self.json_model['shapes']['EventField'] = {'type': 'blob'}\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n        )\n        self.assert_contains_lines_in_order(\n            [\n                \"this operation contains an :class:`.EventStream`\",\n                \"'Payload': EventStream({\",\n                \"'Event': {\",\n                \"'Fields': {\",\n                \"'Field': b'bytes'\",\n                \"**Payload** (:class:`.EventStream`)\",\n                \"**Event** *(dict)\",\n                \"**Fields** *(dict)\",\n                \"**Field** *(bytes)\",\n            ]\n        )\n\n    def test_streaming_body_in_input(self):\n        del self.json_model['operations']['SampleOperation']['output']\n        self.add_shape_to_params('Body', 'Blob')\n        self.json_model['shapes']['Blob'] = {'type': 'blob'}\n        self.json_model['shapes']['SampleOperationInputOutput'][\n            'payload'\n        ] = 'Body'\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n        )\n        # The line in the example\n        self.assert_contains_line('Body=b\\'bytes\\'|file')\n        # The line in the parameter description\n        self.assert_contains_line(\n            ':type Body: bytes or seekable file-like object'\n        )\n\n    def test_deprecated(self):\n        self.json_model['operations']['SampleOperation']['deprecated'] = True\n        document_model_driven_method(\n            self.doc_structure,\n            'foo',\n            self.operation_model,\n            event_emitter=self.event_emitter,\n            method_description='This describes the foo method.',\n            example_prefix='response = client.foo',\n        )\n        # The line in the example\n        self.assert_contains_lines_in_order(\n            [\n                '  .. danger::',\n                '        This operation is deprecated and may not function as '\n                'expected. This operation should not be used going forward and is '\n                'only kept for the purpose of backwards compatiblity.',\n            ]\n        )\n", "tests/unit/docs/test_utils.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.utils import (\n    AppendParamDocumentation,\n    AutoPopulatedParam,\n    HideParamFromOperations,\n    escape_controls,\n    get_official_service_name,\n    py_default,\n    py_type_name,\n)\nfrom tests import unittest\nfrom tests.unit.docs import BaseDocsTest\n\n\nclass TestPythonTypeName(unittest.TestCase):\n    def test_structure(self):\n        self.assertEqual('dict', py_type_name('structure'))\n\n    def test_list(self):\n        self.assertEqual('list', py_type_name('list'))\n\n    def test_map(self):\n        self.assertEqual('dict', py_type_name('map'))\n\n    def test_string(self):\n        self.assertEqual('string', py_type_name('string'))\n\n    def test_character(self):\n        self.assertEqual('string', py_type_name('character'))\n\n    def test_blob(self):\n        self.assertEqual('bytes', py_type_name('blob'))\n\n    def test_timestamp(self):\n        self.assertEqual('datetime', py_type_name('timestamp'))\n\n    def test_integer(self):\n        self.assertEqual('integer', py_type_name('integer'))\n\n    def test_long(self):\n        self.assertEqual('integer', py_type_name('long'))\n\n    def test_float(self):\n        self.assertEqual('float', py_type_name('float'))\n\n    def test_double(self):\n        self.assertEqual('float', py_type_name('double'))\n\n\nclass TestPythonDefault(unittest.TestCase):\n    def test_structure(self):\n        self.assertEqual('{...}', py_default('structure'))\n\n    def test_list(self):\n        self.assertEqual('[...]', py_default('list'))\n\n    def test_map(self):\n        self.assertEqual('{...}', py_default('map'))\n\n    def test_string(self):\n        self.assertEqual('\\'string\\'', py_default('string'))\n\n    def test_blob(self):\n        self.assertEqual('b\\'bytes\\'', py_default('blob'))\n\n    def test_timestamp(self):\n        self.assertEqual('datetime(2015, 1, 1)', py_default('timestamp'))\n\n    def test_integer(self):\n        self.assertEqual('123', py_default('integer'))\n\n    def test_long(self):\n        self.assertEqual('123', py_default('long'))\n\n    def test_double(self):\n        self.assertEqual('123.0', py_default('double'))\n\n\nclass TestGetOfficialServiceName(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.service_model.metadata = {'serviceFullName': 'Official Name'}\n\n    def test_no_short_name(self):\n        self.assertEqual(\n            'Official Name', get_official_service_name(self.service_model)\n        )\n\n    def test_aws_short_name(self):\n        self.service_model.metadata['serviceAbbreviation'] = 'AWS Foo'\n        self.assertEqual(\n            'Official Name (Foo)',\n            get_official_service_name(self.service_model),\n        )\n\n    def test_amazon_short_name(self):\n        self.service_model.metadata['serviceAbbreviation'] = 'Amazon Foo'\n        self.assertEqual(\n            'Official Name (Foo)',\n            get_official_service_name(self.service_model),\n        )\n\n    def test_short_name_in_official_name(self):\n        self.service_model.metadata['serviceFullName'] = 'The Foo Service'\n        self.service_model.metadata['serviceAbbreviation'] = 'Amazon Foo'\n        self.assertEqual(\n            'The Foo Service', get_official_service_name(self.service_model)\n        )\n\n\nclass TestAutopopulatedParam(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.name = 'MyMember'\n        self.param = AutoPopulatedParam(self.name)\n\n    def test_request_param_not_required(self):\n        section = self.doc_structure.add_new_section(self.name)\n        section.add_new_section('param-documentation')\n        self.param.document_auto_populated_param(\n            'docs.request-params', self.doc_structure\n        )\n        self.assert_contains_line('this parameter is automatically populated')\n\n    def test_request_param_required(self):\n        section = self.doc_structure.add_new_section(self.name)\n        is_required_section = section.add_new_section('is-required')\n        section.add_new_section('param-documentation')\n        is_required_section.write('**[REQUIRED]**')\n        self.param.document_auto_populated_param(\n            'docs.request-params', self.doc_structure\n        )\n        self.assert_not_contains_line('**[REQUIRED]**')\n        self.assert_contains_line('this parameter is automatically populated')\n\n    def test_non_default_param_description(self):\n        description = 'This is a custom description'\n        self.param = AutoPopulatedParam(self.name, description)\n        section = self.doc_structure.add_new_section(self.name)\n        section.add_new_section('param-documentation')\n        self.param.document_auto_populated_param(\n            'docs.request-params', self.doc_structure\n        )\n        self.assert_contains_line(description)\n\n    def test_request_example(self):\n        top_section = self.doc_structure.add_new_section('structure-value')\n        section = top_section.add_new_section(self.name)\n        example = 'MyMember: \\'string\\''\n        section.write(example)\n        self.assert_contains_line(example)\n        self.param.document_auto_populated_param(\n            'docs.request-example', self.doc_structure\n        )\n        self.assert_not_contains_line(example)\n\n    def test_param_not_in_section_request_param(self):\n        self.doc_structure.add_new_section('Foo')\n        self.param.document_auto_populated_param(\n            'docs.request-params', self.doc_structure\n        )\n        self.assertEqual(\n            '', self.doc_structure.flush_structure().decode('utf-8')\n        )\n\n    def test_param_not_in_section_request_example(self):\n        top_section = self.doc_structure.add_new_section('structure-value')\n        section = top_section.add_new_section('Foo')\n        example = 'Foo: \\'string\\''\n        section.write(example)\n        self.assert_contains_line(example)\n        self.param.document_auto_populated_param(\n            'docs.request-example', self.doc_structure\n        )\n        self.assert_contains_line(example)\n\n\nclass TestHideParamFromOperations(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.name = 'MyMember'\n        self.param = HideParamFromOperations(\n            's3', self.name, ['SampleOperation']\n        )\n\n    def test_hides_params_from_doc_string(self):\n        section = self.doc_structure.add_new_section(self.name)\n        param_signature = ':param %s: ' % self.name\n        section.write(param_signature)\n        self.assert_contains_line(param_signature)\n        self.param.hide_param(\n            'docs.request-params.s3.SampleOperation.complete-section',\n            self.doc_structure,\n        )\n        self.assert_not_contains_line(param_signature)\n\n    def test_hides_param_from_example(self):\n        structure = self.doc_structure.add_new_section('structure-value')\n        section = structure.add_new_section(self.name)\n        example = '%s: \\'string\\'' % self.name\n        section.write(example)\n        self.assert_contains_line(example)\n        self.param.hide_param(\n            'docs.request-example.s3.SampleOperation.complete-section',\n            self.doc_structure,\n        )\n        self.assert_not_contains_line(example)\n\n\nclass TestAppendParamDocumentation(BaseDocsTest):\n    def setUp(self):\n        super().setUp()\n        self.name = 'MyMember'\n        self.param = AppendParamDocumentation(self.name, 'hello!')\n\n    def test_appends_documentation(self):\n        section = self.doc_structure.add_new_section(self.name)\n        param_section = section.add_new_section('param-documentation')\n        param_section.writeln('foo')\n        self.param.append_documentation(\n            'docs.request-params', self.doc_structure\n        )\n        self.assert_contains_line('foo\\n')\n        self.assert_contains_line('hello!')\n\n\nclass TestEscapeControls(unittest.TestCase):\n    def test_escapes_controls(self):\n        escaped = escape_controls('\\na\\rb\\tc\\fd\\be')\n        self.assertEqual(escaped, '\\\\na\\\\rb\\\\tc\\\\fd\\\\be')\n", "tests/unit/docs/bcdoc/test_docstringparser.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\nimport pytest\n\nimport botocore.docs.bcdoc.docstringparser as parser\nfrom botocore.docs.bcdoc.restdoc import ReSTDocument\nfrom tests import mock, unittest\n\n\nclass TestDocStringParser(unittest.TestCase):\n    def parse(self, html):\n        docstring_parser = parser.DocStringParser(ReSTDocument())\n        docstring_parser.feed(html)\n        docstring_parser.close()\n        return docstring_parser.doc.getvalue()\n\n    def assert_contains_exact_lines_in_order(self, actual, expected):\n        # Get each line and filter out empty lines\n        contents = actual.split(b'\\n')\n        contents = [line for line in contents if line and not line.isspace()]\n\n        for line in expected:\n            self.assertIn(line, contents)\n            beginning = contents.index(line)\n            contents = contents[beginning:]\n\n    def test_tag_with_collapsible_spaces(self):\n        html = \"<p>  a       bcd efg </p>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(result, [b'a bcd efg'])\n\n    def test_nested_lists(self):\n        html = \"<ul><li>Wello</li><ul><li>Horld</li></ul></ul>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(\n            result, [b'* Wello', b'  * Horld']\n        )\n\n    def test_nested_lists_with_extra_white_space(self):\n        html = \"<ul> <li> Wello</li><ul> <li> Horld</li></ul></ul>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(\n            result, [b'* Wello', b'  * Horld']\n        )\n\n    def test_link_with_no_period(self):\n        html = \"<p>This is a test <a href='https://testing.com'>Link</a></p>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(\n            result, [b'This is a test `Link <https://testing.com>`__']\n        )\n\n    def test_link_with_period(self):\n        html = \"<p>This is a test <a href='https://testing.com'>Link</a>.</p>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(\n            result, [b'This is a test `Link <https://testing.com>`__.']\n        )\n\n    def test_code_with_empty_link(self):\n        html = \"<p>Foo <code> <a>Link</a> </code></p>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(result, [b'Foo ``Link``'])\n\n    def test_code_with_link_spaces(self):\n        html = \"<p>Foo <code> <a href=\\\"https://aws.dev\\\">Link</a> </code></p>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(result, [b'Foo ``Link``'])\n\n    def test_code_with_link_no_spaces(self):\n        html = \"<p>Foo <code><a href=\\\"https://aws.dev\\\">Link</a></code></p>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(result, [b'Foo ``Link``'])\n\n    def test_href_with_spaces(self):\n        html = \"<p><a href=\\\" https://testing.com\\\">Link</a></p>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(\n            result, [b' `Link <https://testing.com>`__']\n        )\n\n    def test_bold_with_nested_formatting(self):\n        html = \"<b><code>Test</code>test<a href=\\\" https://testing.com\\\">Link</a></b>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(\n            result, [b'``Test``test `Link <https://testing.com>`__']\n        )\n\n    def test_link_with_nested_formatting(self):\n        html = \"<a href=\\\"https://testing.com\\\"><code>Test</code></a>\"\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(\n            result, [b'`Test <https://testing.com>`__']\n        )\n\n    def test_indentation_with_spaces_between_tags(self):\n        # Leading spaces inserted as padding between HTML tags can lead to\n        # unexpected indentation in RST. For example, consider the space between\n        # ``<p>`` and ``<b>`` in the third line of this example:\n        html = (\n            \"<p>First paragraph </p> \"\n            \"<note> <p> Second paragraph in note </p> </note> \"\n            \"<p> <b>Bold statement:</b> Third paragraph</p>\"\n            \"<p>Last paragraph</p> \"\n        )\n        # If kept, it will appear as indentation in RST and have the effect of\n        # pulling the third paragraph into the note:\n        #\n        # ```\n        # First paragraph\n        #\n        # ..note::\n        #   Second paragraph in note <-- intentionally indented\n        #\n        #  **Bold statement:** Third paragraph <-- unintentionally indented\n        #\n        # Last paragraph <-- first non-indented paragraph after note\n        # ```\n        result = self.parse(html)\n        self.assert_contains_exact_lines_in_order(\n            result,\n            #  \u2193 no whitespace here\n            [b'**Bold statement:** Third paragraph'],\n        )\n\n\nclass TestHTMLTree(unittest.TestCase):\n    def setUp(self):\n        self.style = mock.Mock()\n        self.doc = mock.Mock()\n        self.doc.style = self.style\n        self.tree = parser.HTMLTree(self.doc)\n\n    def test_add_tag(self):\n        self.tree.add_tag('foo')\n        self.assertIsInstance(self.tree.current_node, parser.TagNode)\n        self.assertEqual(self.tree.current_node.tag, 'foo')\n\n    def test_add_unsupported_tag(self):\n        del self.style.start_foo\n        del self.style.end_foo\n        self.tree.add_tag('foo')\n        self.assertIn('foo', self.tree.unhandled_tags)\n\n    def test_add_data(self):\n        self.tree.add_data('foo')\n        self.assertNotIsInstance(self.tree.current_node, parser.DataNode)\n        node = self.tree.head.children[0]\n        self.assertIsInstance(node, parser.DataNode)\n        self.assertEqual(node.data, 'foo')\n\n\nclass TestStemNode(unittest.TestCase):\n    def setUp(self):\n        self.style = mock.Mock()\n        self.doc = mock.Mock()\n        self.doc.style = self.style\n        self.node = parser.StemNode()\n\n    def test_add_child(self):\n        child = parser.StemNode()\n        self.node.add_child(child)\n        self.assertIn(child, self.node.children)\n        self.assertEqual(child.parent, self.node)\n\n    def test_write(self):\n        self.node.add_child(mock.Mock())\n        self.node.add_child(mock.Mock())\n\n        self.node.write(mock.Mock())\n        for child in self.node.children:\n            self.assertTrue(child.write.called)\n\n\nclass TestTagNode(unittest.TestCase):\n    def setUp(self):\n        self.style = mock.Mock()\n        self.doc = mock.Mock()\n        self.doc.style = self.style\n        self.tag = 'foo'\n        self.node = parser.TagNode(self.tag)\n\n    def test_write_calls_style(self):\n        self.node.write(self.doc)\n        self.assertTrue(self.style.start_foo.called)\n        self.assertTrue(self.style.end_foo.called)\n\n    def test_write_unsupported_tag(self):\n        del self.style.start_foo\n        del self.style.end_foo\n\n        try:\n            self.node.write(self.doc)\n        except AttributeError as e:\n            self.fail(str(e))\n\n\nclass TestDataNode(unittest.TestCase):\n    def setUp(self):\n        self.style = mock.Mock()\n        self.doc = mock.Mock()\n        self.doc.style = self.style\n        self.doc.translate_words.return_value = []\n\n    def test_string_data(self):\n        node = parser.DataNode('foo')\n        self.assertEqual(node.data, 'foo')\n\n    def test_non_string_data_raises_error(self):\n        with self.assertRaises(ValueError):\n            parser.DataNode(5)\n\n    def test_lstrip(self):\n        node = parser.DataNode(' foo')\n        node.lstrip()\n        self.assertEqual(node.data, 'foo')\n\n    def test_write(self):\n        node = parser.DataNode('foo   bar baz')\n        self.doc.translate_words.return_value = ['foo', 'bar', 'baz']\n        node.write(self.doc)\n        self.doc.handle_data.assert_called_once_with('foo bar baz')\n\n    def test_write_empty_string(self):\n        node = parser.DataNode('')\n        node.write(self.doc)\n        self.assertFalse(self.doc.handle_data.called)\n\n\n@pytest.mark.parametrize(\n    'data, lstrip, rstrip, both',\n    [\n        ('foo', 'foo', 'foo', 'foo'),\n        (' foo', 'foo', ' foo', 'foo'),\n        ('   foo', 'foo', '   foo', 'foo'),\n        ('\\tfoo', 'foo', '\\tfoo', 'foo'),\n        ('\\t \\t foo', 'foo', '\\t \\t foo', 'foo'),\n        ('foo ', 'foo ', 'foo', 'foo'),\n        ('foo  ', 'foo  ', 'foo', 'foo'),\n        ('foo\\t\\t', 'foo\\t\\t', 'foo', 'foo'),\n    ],\n)\ndef test_datanode_stripping(data, lstrip, rstrip, both):\n    doc = mock.Mock()\n    doc.style = mock.Mock()\n    doc.translate_words.side_effect = lambda words: words\n\n    node = parser.DataNode(data)\n    node.lstrip()\n    node.write(doc)\n    doc.handle_data.assert_called_once_with(lstrip)\n    doc.handle_data.reset_mock()\n\n    node = parser.DataNode(data)\n    node.rstrip()\n    node.write(doc)\n    doc.handle_data.assert_called_once_with(rstrip)\n    doc.handle_data.reset_mock()\n\n    node = parser.DataNode(data)\n    node.lstrip()\n    node.rstrip()\n    node.write(doc)\n    doc.handle_data.assert_called_once_with(both)\n\n\n@pytest.mark.parametrize(\n    'data',\n    [\n        (' '),\n        ('  '),\n        ('\\t'),\n        ('\\t \\t '),\n    ],\n)\ndef test_datanode_stripping_empty_string(data):\n    doc = mock.Mock()\n    doc.style = mock.Mock()\n    doc.translate_words.side_effect = lambda words: words\n    node = parser.DataNode(data)\n    node.lstrip()\n    node.write(doc)\n    doc.handle_data.assert_not_called()\n\n\n@pytest.mark.parametrize(\n    'html, expected_lines',\n    [\n        ('<p>  foo</p>', [b'foo']),\n        ('<p>\\tfoo</p>', [b'foo']),\n        ('<p>  <span>  </span> <span> <span> foo</span></span></p>', [b'foo']),\n        ('<p>foo  </p>', [b'foo']),\n        ('<p>foo\\t</p>', [b'foo']),\n        ('<p>  foo  </p>', [b'foo']),\n        ('<p>  <span>foo</span>  </p>', [b'foo']),\n        ('<p>  <span>foo  </span>  </p>', [b'foo']),\n        ('<p>  <span>  foo</span>  </p>', [b'foo']),\n        ('<p>  <span>  foo  </span>  </p>', [b'foo']),\n        # various nested markup examples\n        ('<i>italic</i>', [b'*italic*']),\n        ('<p><i>italic</i></p>', [b'*italic*']),\n        ('<p><i>italic</i> </p>', [b'*italic*']),\n        ('<p><i>italic </i></p>', [b'*italic*']),\n        ('<p>foo <i> italic </i> bar</p>', [b'foo *italic* bar']),\n        ('<p>  <span> foo <i> bar</i> </span>  </p>', [b'foo *bar*']),\n        ('<p>  <span> foo<i> bar</i> </span>  </p>', [b'foo* bar*']),\n        ('<p>  <span> foo <i>bar</i> </span>  </p>', [b'foo *bar*']),\n        # links\n        ('<a href=\"url\">foo</a> <i>bar</i>', [b'`foo <url>`__ *bar*']),\n        # ReST does not support link text starting with whitespace\n        ('<p>abc<a href=\"url\"> foo</a></p>', [b'abc `foo <url>`__']),\n        ('<p>abc<a href=\"url\"> foo </a> bar</p>', [b'abc `foo <url>`__ bar']),\n        # code-in-a removed and whitespace removed\n        ('<a href=\"url\"> <code>foo</code> </a> bar', [b'`foo <url>`__ bar']),\n        # list items\n        ('<li>  foo</li>', [b'* foo']),\n        ('<li>  <foo>  </foo><foo> foo</foo></li>', [b'* foo']),\n        ('<li>  <foo> foo</foo><foo>  </foo></li>', [b'* foo']),\n        ('<li><foo>  </foo><foo> foo</foo> <foo>bar</li>', [b'* foo bar']),\n        ('<li><foo>  </foo><foo> foo</foo> <foo> bar</li>', [b'* foo bar']),\n        ('<li><foo>  </foo><foo> foo</foo><foo> bar</li>', [b'* foo bar']),\n        # multiple block tags in sequence are each left and right stripped\n        ('<p>  foo</p><p>  bar\\t</p>', [b'foo', b'bar']),\n        ('<p>  foo</p><li>  bar  </li>', [b'foo', b'* bar']),\n        # nested block tags also work\n        (\n            '<p> <p> foo </p> <p> <span> bar </span> </p> </p>',\n            [b'foo', b'bar'],\n        ),\n    ],\n)\ndef test_whitespace_collapsing(html, expected_lines):\n    docstring_parser = parser.DocStringParser(ReSTDocument())\n    docstring_parser.feed(html)\n    docstring_parser.close()\n    actual = docstring_parser.doc.getvalue()\n\n    # Get each line and filter out empty lines\n    contents = actual.split(b'\\n')\n    contents = [line for line in contents if line and not line.isspace()]\n    for line in expected_lines:\n        assert line in contents\n        beginning = contents.index(line)\n        contents = contents[beginning:]\n", "tests/unit/docs/bcdoc/test_style.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nfrom botocore.docs.bcdoc.restdoc import ReSTDocument\nfrom botocore.docs.bcdoc.style import ReSTStyle\nfrom tests import unittest\n\n\nclass TestStyle(unittest.TestCase):\n    def test_spaces(self):\n        style = ReSTStyle(None, 4)\n        self.assertEqual(style.spaces(), '')\n        style.indent()\n        self.assertEqual(style.spaces(), '    ')\n        style.indent()\n        self.assertEqual(style.spaces(), '        ')\n        style.dedent()\n        self.assertEqual(style.spaces(), '    ')\n        style.dedent()\n        self.assertEqual(style.spaces(), '')\n        style.dedent()\n        self.assertEqual(style.spaces(), '')\n\n    def test_bold(self):\n        style = ReSTStyle(ReSTDocument())\n        style.bold('foobar')\n        self.assertEqual(style.doc.getvalue(), b'**foobar**')\n\n    def test_empty_bold(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_b()\n        style.end_b()\n        self.assertEqual(style.doc.getvalue(), b'')\n\n    def test_italics(self):\n        style = ReSTStyle(ReSTDocument())\n        style.italics('foobar')\n        self.assertEqual(style.doc.getvalue(), b'*foobar*')\n\n    def test_empty_italics(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_i()\n        style.end_i()\n        self.assertEqual(style.doc.getvalue(), b'')\n\n    def test_p(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_p()\n        style.doc.write('foo')\n        style.end_p()\n        self.assertEqual(style.doc.getvalue(), b'\\n\\nfoo\\n\\n')\n\n    def test_code(self):\n        style = ReSTStyle(ReSTDocument())\n        style.code('foobar')\n        self.assertEqual(style.doc.getvalue(), b'``foobar``')\n\n    def test_empty_code(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_code()\n        style.end_code()\n        self.assertEqual(style.doc.getvalue(), b'')\n\n    def test_h1(self):\n        style = ReSTStyle(ReSTDocument())\n        style.h1('foobar fiebaz')\n        self.assertEqual(\n            style.doc.getvalue(),\n            b'\\n\\n*************\\nfoobar fiebaz\\n*************\\n\\n',\n        )\n\n    def test_h2(self):\n        style = ReSTStyle(ReSTDocument())\n        style.h2('foobar fiebaz')\n        self.assertEqual(\n            style.doc.getvalue(),\n            b'\\n\\n=============\\nfoobar fiebaz\\n=============\\n\\n',\n        )\n\n    def test_h3(self):\n        style = ReSTStyle(ReSTDocument())\n        style.h3('foobar fiebaz')\n        self.assertEqual(\n            style.doc.getvalue(),\n            b'\\n\\n-------------\\nfoobar fiebaz\\n-------------\\n\\n',\n        )\n\n    def test_ref(self):\n        style = ReSTStyle(ReSTDocument())\n        style.ref('foobar', 'http://foo.bar.com')\n        self.assertEqual(\n            style.doc.getvalue(), b':doc:`foobar <http://foo.bar.com>`'\n        )\n\n    def test_examples(self):\n        style = ReSTStyle(ReSTDocument())\n        self.assertTrue(style.doc.keep_data)\n        style.start_examples()\n        self.assertFalse(style.doc.keep_data)\n        style.end_examples()\n        self.assertTrue(style.doc.keep_data)\n\n    def test_codeblock(self):\n        style = ReSTStyle(ReSTDocument())\n        style.codeblock('foobar')\n        self.assertEqual(style.doc.getvalue(), b'::\\n\\n  foobar\\n\\n\\n')\n\n    def test_important(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_important()\n        style.end_important()\n        self.assertEqual(style.doc.getvalue(), b'\\n\\n.. warning::\\n\\n  \\n\\n')\n\n    def test_note(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_note()\n        style.end_note()\n        self.assertEqual(style.doc.getvalue(), b'\\n\\n.. note::\\n\\n  \\n\\n')\n\n    def test_danger(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_danger()\n        style.end_danger()\n        self.assertEqual(style.doc.getvalue(), b'\\n\\n.. danger::\\n\\n  \\n\\n')\n\n    def test_toctree_html(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'html'\n        style.toctree()\n        style.tocitem('foo')\n        style.tocitem('bar')\n        self.assertEqual(\n            style.doc.getvalue(),\n            (\n                b'\\n.. toctree::\\n  :maxdepth: 1'\n                b'\\n  :titlesonly:\\n\\n  foo\\n  bar\\n'\n            ),\n        )\n\n    def test_toctree_man(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'man'\n        style.toctree()\n        style.tocitem('foo')\n        style.tocitem('bar')\n        self.assertEqual(style.doc.getvalue(), b'\\n\\n\\n* foo\\n\\n\\n* bar\\n\\n')\n\n    def test_hidden_toctree_html(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'html'\n        style.hidden_toctree()\n        style.hidden_tocitem('foo')\n        style.hidden_tocitem('bar')\n        self.assertEqual(\n            style.doc.getvalue(),\n            (\n                b'\\n.. toctree::\\n  :maxdepth: 1'\n                b'\\n  :hidden:\\n\\n  foo\\n  bar\\n'\n            ),\n        )\n\n    def test_hidden_toctree_non_html(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'man'\n        style.hidden_toctree()\n        style.hidden_tocitem('foo')\n        style.hidden_tocitem('bar')\n        self.assertEqual(style.doc.getvalue(), b'')\n\n    def test_href_link(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_a(attrs=[('href', 'http://example.org')])\n        style.doc.write('example')\n        style.end_a()\n        self.assertEqual(\n            style.doc.getvalue(), b'`example <http://example.org>`__'\n        )\n\n    def test_escape_href_link(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_a(attrs=[('href', 'http://example.org')])\n        style.doc.write('foo: the next bar')\n        style.end_a()\n        self.assertEqual(\n            style.doc.getvalue(),\n            b'`foo\\\\: the next bar <http://example.org>`__',\n        )\n\n    def test_handle_no_text_hrefs(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_a(attrs=[('href', 'http://example.org')])\n        style.end_a()\n        self.assertEqual(style.doc.getvalue(), b'`<http://example.org>`__')\n\n    def test_sphinx_reference_label_html(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'html'\n        style.sphinx_reference_label('foo', 'bar')\n        self.assertEqual(style.doc.getvalue(), b':ref:`bar <foo>`')\n\n    def test_sphinx_reference_label_html_no_text(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'html'\n        style.sphinx_reference_label('foo')\n        self.assertEqual(style.doc.getvalue(), b':ref:`foo <foo>`')\n\n    def test_sphinx_reference_label_non_html(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'man'\n        style.sphinx_reference_label('foo', 'bar')\n        self.assertEqual(style.doc.getvalue(), b'bar')\n\n    def test_sphinx_reference_label_non_html_no_text(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'man'\n        style.sphinx_reference_label('foo')\n        self.assertEqual(style.doc.getvalue(), b'foo')\n\n    def test_table_of_contents(self):\n        style = ReSTStyle(ReSTDocument())\n        style.table_of_contents()\n        self.assertEqual(style.doc.getvalue(), b'.. contents:: ')\n\n    def test_table_of_contents_with_title(self):\n        style = ReSTStyle(ReSTDocument())\n        style.table_of_contents(title='Foo')\n        self.assertEqual(style.doc.getvalue(), b'.. contents:: Foo\\n')\n\n    def test_table_of_contents_with_title_and_depth(self):\n        style = ReSTStyle(ReSTDocument())\n        style.table_of_contents(title='Foo', depth=2)\n        self.assertEqual(\n            style.doc.getvalue(), b'.. contents:: Foo\\n   :depth: 2\\n'\n        )\n\n    def test_sphinx_py_class(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_sphinx_py_class('FooClass')\n        style.end_sphinx_py_class()\n        self.assertEqual(\n            style.doc.getvalue(), b'\\n\\n.. py:class:: FooClass\\n\\n  \\n\\n'\n        )\n\n    def test_sphinx_py_method(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_sphinx_py_method('method')\n        style.end_sphinx_py_method()\n        self.assertEqual(\n            style.doc.getvalue(), b'\\n\\n.. py:method:: method\\n\\n  \\n\\n'\n        )\n\n    def test_sphinx_py_method_with_params(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_sphinx_py_method('method', 'foo=None')\n        style.end_sphinx_py_method()\n        self.assertEqual(\n            style.doc.getvalue(),\n            b'\\n\\n.. py:method:: method(foo=None)\\n\\n  \\n\\n',\n        )\n\n    def test_sphinx_py_attr(self):\n        style = ReSTStyle(ReSTDocument())\n        style.start_sphinx_py_attr('Foo')\n        style.end_sphinx_py_attr()\n        self.assertEqual(\n            style.doc.getvalue(), b'\\n\\n.. py:attribute:: Foo\\n\\n  \\n\\n'\n        )\n\n    def test_write_py_doc_string(self):\n        style = ReSTStyle(ReSTDocument())\n        docstring = (\n            'This describes a function\\n'\n            ':param foo: Describes foo\\n'\n            'returns: None'\n        )\n        style.write_py_doc_string(docstring)\n        self.assertEqual(\n            style.doc.getvalue(), (docstring + '\\n').encode('latin-1')\n        )\n\n    def test_new_line(self):\n        style = ReSTStyle(ReSTDocument())\n        style.new_line()\n        self.assertEqual(style.doc.getvalue(), b'\\n')\n\n        style.do_p = False\n        style.new_line()\n        self.assertEqual(style.doc.getvalue(), b'\\n\\n')\n\n    def test_list(self):\n        style = ReSTStyle(ReSTDocument())\n        style.li('foo')\n        self.assertEqual(style.doc.getvalue(), b'\\n* foo\\n\\n')\n\n    def test_non_top_level_lists_are_indented(self):\n        style = ReSTStyle(ReSTDocument())\n\n        # Start the top level list\n        style.start_ul()\n\n        # Write one list element\n        style.start_li()\n        style.doc.handle_data('foo')\n        style.end_li()\n\n        self.assertEqual(style.doc.getvalue(), b\"\\n\\n\\n* foo\\n\")\n\n        # Start the nested list\n        style.start_ul()\n\n        # Write an element to the nested list\n        style.start_li()\n        style.doc.handle_data('bar')\n        style.end_li()\n\n        self.assertEqual(\n            style.doc.getvalue(), b\"\\n\\n\\n* foo\\n\\n\\n  \\n  * bar\\n  \"\n        )\n\n    def test_external_link(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'html'\n        style.external_link('MyLink', 'http://example.com/foo')\n        self.assertEqual(\n            style.doc.getvalue(), b'`MyLink <http://example.com/foo>`_'\n        )\n\n    def test_external_link_in_man_page(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'man'\n        style.external_link('MyLink', 'http://example.com/foo')\n        self.assertEqual(style.doc.getvalue(), b'MyLink')\n\n    def test_internal_link(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'html'\n        style.internal_link('MyLink', '/index')\n        self.assertEqual(style.doc.getvalue(), b':doc:`MyLink </index>`')\n\n    def test_internal_link_in_man_page(self):\n        style = ReSTStyle(ReSTDocument())\n        style.doc.target = 'man'\n        style.internal_link('MyLink', '/index')\n        self.assertEqual(style.doc.getvalue(), b'MyLink')\n", "tests/unit/docs/bcdoc/__init__.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/unit/docs/bcdoc/test_document.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nfrom botocore.docs.bcdoc.restdoc import DocumentStructure, ReSTDocument\nfrom tests import unittest\n\n\nclass TestReSTDocument(unittest.TestCase):\n    def test_write(self):\n        doc = ReSTDocument()\n        doc.write('foo')\n        self.assertEqual(doc.getvalue(), b'foo')\n\n    def test_writeln(self):\n        doc = ReSTDocument()\n        doc.writeln('foo')\n        self.assertEqual(doc.getvalue(), b'foo\\n')\n\n    def test_include_doc_string(self):\n        doc = ReSTDocument()\n        doc.include_doc_string('<p>this is a <code>test</code></p>')\n        self.assertEqual(doc.getvalue(), b'\\n\\nthis is a ``test``\\n\\n')\n\n    def test_remove_doc_string(self):\n        doc = ReSTDocument()\n        doc.writeln('foo')\n        doc.include_doc_string('<p>this is a <code>test</code></p>')\n        doc.remove_last_doc_string()\n        self.assertEqual(doc.getvalue(), b'foo\\n')\n\n    def test_add_links(self):\n        doc = ReSTDocument()\n        doc.hrefs['foo'] = 'https://example.com/'\n        self.assertEqual(\n            doc.getvalue(), b'\\n\\n.. _foo: https://example.com/\\n'\n        )\n\n\nclass TestDocumentStructure(unittest.TestCase):\n    def setUp(self):\n        self.name = 'mydoc'\n        self.doc_structure = DocumentStructure(self.name)\n\n    def test_name(self):\n        self.assertEqual(self.doc_structure.name, self.name)\n\n    def test_path(self):\n        self.assertEqual(self.doc_structure.path, [self.name])\n        self.doc_structure.path = ['foo']\n        self.assertEqual(self.doc_structure.path, ['foo'])\n\n    def test_add_new_section(self):\n        section = self.doc_structure.add_new_section('mysection')\n\n        # Ensure the name of the section is correct\n        self.assertEqual(section.name, 'mysection')\n\n        # Ensure we can get the section.\n        self.assertEqual(self.doc_structure.get_section('mysection'), section)\n\n        # Ensure the path is correct\n        self.assertEqual(section.path, ['mydoc', 'mysection'])\n\n        # Ensure some of the necessary attributes are passed to the\n        # the section.\n        self.assertEqual(\n            section.style.indentation, self.doc_structure.style.indentation\n        )\n        self.assertEqual(\n            section.translation_map, self.doc_structure.translation_map\n        )\n        self.assertEqual(section.hrefs, self.doc_structure.hrefs)\n\n    def test_delete_section(self):\n        section = self.doc_structure.add_new_section('mysection')\n        self.assertEqual(self.doc_structure.get_section('mysection'), section)\n        self.doc_structure.delete_section('mysection')\n        with self.assertRaises(KeyError):\n            section.get_section('mysection')\n\n    def test_create_sections_at_instantiation(self):\n        sections = ['intro', 'middle', 'end']\n        self.doc_structure = DocumentStructure(\n            self.name, section_names=sections\n        )\n        # Ensure the sections are attached to the new document structure.\n        for section_name in sections:\n            section = self.doc_structure.get_section(section_name)\n            self.assertEqual(section.name, section_name)\n\n    def test_flush_structure(self):\n        section = self.doc_structure.add_new_section('mysection')\n        subsection = section.add_new_section('mysubsection')\n        self.doc_structure.writeln('1')\n        section.writeln('2')\n        subsection.writeln('3')\n        second_section = self.doc_structure.add_new_section('mysection2')\n        second_section.writeln('4')\n        contents = self.doc_structure.flush_structure()\n\n        # Ensure the contents were flushed out correctly\n        self.assertEqual(contents, b'1\\n2\\n3\\n4\\n')\n\n    def test_flush_structure_hrefs(self):\n        section = self.doc_structure.add_new_section('mysection')\n        section.writeln('section contents')\n        self.doc_structure.hrefs['foo'] = 'www.foo.com'\n        section.hrefs['bar'] = 'www.bar.com'\n        contents = self.doc_structure.flush_structure()\n        self.assertIn(b'.. _foo: www.foo.com', contents)\n        self.assertIn(b'.. _bar: www.bar.com', contents)\n\n    def test_available_sections(self):\n        self.doc_structure.add_new_section('mysection')\n        self.doc_structure.add_new_section('mysection2')\n        self.assertEqual(\n            self.doc_structure.available_sections, ['mysection', 'mysection2']\n        )\n\n    def test_context(self):\n        context = {'Foo': 'Bar'}\n        section = self.doc_structure.add_new_section(\n            'mysection', context=context\n        )\n        self.assertEqual(section.context, context)\n\n        # Make sure if context is not specified it is empty.\n        section = self.doc_structure.add_new_section('mysection2')\n        self.assertEqual(section.context, {})\n\n    def test_remove_all_sections(self):\n        self.doc_structure.add_new_section('mysection2')\n        self.doc_structure.remove_all_sections()\n        self.assertEqual(self.doc_structure.available_sections, [])\n\n    def test_clear_text(self):\n        self.doc_structure.write('Foo')\n        self.doc_structure.clear_text()\n        self.assertEqual(self.doc_structure.flush_structure(), b'')\n", "tests/unit/crt/__init__.py": "", "tests/unit/crt/auth/test_crt_signers.py": "import botocore\nfrom botocore.compat import HAS_CRT\nfrom tests import requires_crt\nfrom tests.unit.auth.test_signers import (\n    TestS3SigV4Auth,\n    TestSigV4Presign,\n    TestSigV4Resign,\n)\n\n\n@requires_crt()\ndef test_crt_supported_auth_types_list():\n    # The list CRT_SUPPORTED_AUTH_TYPES is available even when awscrt is not\n    # installed. Its entries must always match the keys of the\n    # CRT_AUTH_TYPE_MAPS dict which is only available when CRT is installed.\n    with_crt_list = set(botocore.crt.CRT_SUPPORTED_AUTH_TYPES)\n    without_crt_list = set(botocore.crt.auth.CRT_AUTH_TYPE_MAPS.keys())\n    assert with_crt_list == without_crt_list\n\n\n@requires_crt()\nclass TestCrtS3SigV4Auth(TestS3SigV4Auth):\n    # Repeat TestS3SigV4Auth tests, but using CRT signer\n    if HAS_CRT:\n        AuthClass = botocore.crt.auth.CrtS3SigV4Auth\n\n\n@requires_crt()\nclass TestCrtSigV4Resign(TestSigV4Resign):\n    # Run same tests against CRT auth\n    if HAS_CRT:\n        AuthClass = botocore.crt.auth.CrtSigV4Auth\n\n\n@requires_crt()\nclass TestCrtSigV4Presign(TestSigV4Presign):\n    # Run same tests against CRT auth\n    if HAS_CRT:\n        AuthClass = botocore.crt.auth.CrtSigV4QueryAuth\n", "tests/unit/crt/auth/test_crt_sigv4.py": "import pytest\n\nimport botocore\nfrom tests import FreezeTime, requires_crt\nfrom tests.unit.auth.test_sigv4 import (\n    DATE,\n    REGION,\n    SERVICE,\n    SignatureTestCase,\n    assert_equal,\n    create_request_from_raw_request,\n    generate_test_cases,\n)\n\n\ndef _test_crt_signature_version_4(test_case):\n    test_case = SignatureTestCase(test_case)\n    request = create_request_from_raw_request(test_case.raw_request)\n\n    # Use CRT logging to diagnose interim steps (canonical request, etc)\n    # import awscrt.io\n    # awscrt.io.init_logging(awscrt.io.LogLevel.Trace, 'stdout')\n    auth = botocore.crt.auth.CrtSigV4Auth(\n        test_case.credentials, SERVICE, REGION\n    )\n    auth.add_auth(request)\n    actual_auth_header = request.headers[\"Authorization\"]\n    assert_equal(\n        actual_auth_header,\n        test_case.authorization_header,\n        test_case.raw_request,\n        \"authheader\",\n    )\n\n\n@requires_crt()\n@pytest.mark.parametrize(\"test_case\", generate_test_cases())\n@FreezeTime(module=botocore.auth.datetime, date=DATE)\ndef test_signature_version_4(test_case):\n    _test_crt_signature_version_4(test_case)\n", "tests/unit/crt/auth/__init__.py": "", "tests/unit/auth/test_signers.py": "#!/usr/bin/env\n# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport datetime\nimport io\nimport json\nimport time\n\nimport botocore.auth\nimport botocore.credentials\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.compat import HTTPHeaders, parse_qs, urlsplit\nfrom tests import mock, unittest\n\n\nclass BaseTestWithFixedDate(unittest.TestCase):\n    def setUp(self):\n        self.fixed_date = datetime.datetime(2014, 3, 10, 17, 2, 55, 0)\n        self.datetime_patch = mock.patch('botocore.auth.datetime.datetime')\n        self.datetime_mock = self.datetime_patch.start()\n        self.datetime_mock.utcnow.return_value = self.fixed_date\n        self.datetime_mock.strptime.return_value = self.fixed_date\n\n    def tearDown(self):\n        self.datetime_patch.stop()\n\n\nclass TestHMACV1(unittest.TestCase):\n    maxDiff = None\n\n    def setUp(self):\n        access_key = '44CF9590006BF252F707'\n        secret_key = 'OtxrzxIsfpFjA7SwPzILwy8Bw21TLhquhboDYROV'\n        self.credentials = botocore.credentials.Credentials(\n            access_key, secret_key\n        )\n        self.hmacv1 = botocore.auth.HmacV1Auth(self.credentials, None, None)\n        self.date_mock = mock.patch('botocore.auth.formatdate')\n        self.formatdate = self.date_mock.start()\n        self.formatdate.return_value = 'Thu, 17 Nov 2005 18:49:58 GMT'\n\n    def tearDown(self):\n        self.date_mock.stop()\n\n    def test_put(self):\n        headers = {\n            'Date': 'Thu, 17 Nov 2005 18:49:58 GMT',\n            'Content-Md5': 'c8fdb181845a4ca6b8fec737b3581d76',\n            'Content-Type': 'text/html',\n            'X-Amz-Meta-Author': 'foo@bar.com',\n            'X-Amz-Magic': 'abracadabra',\n        }\n        http_headers = HTTPHeaders.from_dict(headers)\n        split = urlsplit('/quotes/nelson')\n        cs = self.hmacv1.canonical_string('PUT', split, http_headers)\n        expected_canonical = (\n            \"PUT\\nc8fdb181845a4ca6b8fec737b3581d76\\ntext/html\\n\"\n            \"Thu, 17 Nov 2005 18:49:58 GMT\\nx-amz-magic:abracadabra\\n\"\n            \"x-amz-meta-author:foo@bar.com\\n/quotes/nelson\"\n        )\n        expected_signature = 'jZNOcbfWmD/A/f3hSvVzXZjM2HU='\n        self.assertEqual(cs, expected_canonical)\n        sig = self.hmacv1.get_signature('PUT', split, http_headers)\n        self.assertEqual(sig, expected_signature)\n\n    def test_duplicate_headers(self):\n        pairs = [\n            ('Date', 'Thu, 17 Nov 2005 18:49:58 GMT'),\n            ('Content-Md5', 'c8fdb181845a4ca6b8fec737b3581d76'),\n            ('Content-Type', 'text/html'),\n            ('X-Amz-Meta-Author', 'bar@baz.com'),\n            ('X-Amz-Meta-Author', 'foo@bar.com'),\n            ('X-Amz-Magic', 'abracadabra'),\n        ]\n\n        http_headers = HTTPHeaders.from_pairs(pairs)\n        split = urlsplit('/quotes/nelson')\n        sig = self.hmacv1.get_signature('PUT', split, http_headers)\n        self.assertEqual(sig, 'kIdMxyiYB+F+83zYGR6sSb3ICcE=')\n\n    def test_query_string(self):\n        split = urlsplit('/quotes/nelson?uploads')\n        pairs = [('Date', 'Thu, 17 Nov 2005 18:49:58 GMT')]\n        sig = self.hmacv1.get_signature(\n            'PUT', split, HTTPHeaders.from_pairs(pairs)\n        )\n        self.assertEqual(sig, 'P7pBz3Z4p3GxysRSJ/gR8nk7D4o=')\n\n    def test_bucket_operations(self):\n        # Check that the standard operations on buckets that are\n        # specified as query strings end up in the canonical resource.\n        operations = (\n            'acl',\n            'cors',\n            'lifecycle',\n            'policy',\n            'notification',\n            'logging',\n            'tagging',\n            'requestPayment',\n            'versioning',\n            'website',\n            'object-lock',\n        )\n        for operation in operations:\n            url = '/quotes?%s' % operation\n            split = urlsplit(url)\n            cr = self.hmacv1.canonical_resource(split)\n            self.assertEqual(cr, '/quotes?%s' % operation)\n\n    def test_sign_with_token(self):\n        credentials = botocore.credentials.Credentials(\n            access_key='foo', secret_key='bar', token='baz'\n        )\n        auth = botocore.auth.HmacV1Auth(credentials)\n        request = AWSRequest()\n        request.headers['Date'] = 'Thu, 17 Nov 2005 18:49:58 GMT'\n        request.headers['Content-Type'] = 'text/html'\n        request.method = 'PUT'\n        request.url = 'https://s3.amazonaws.com/bucket/key'\n        auth.add_auth(request)\n        self.assertIn('Authorization', request.headers)\n        # We're not actually checking the signature here, we're\n        # just making sure the auth header has the right format.\n        self.assertTrue(request.headers['Authorization'].startswith('AWS '))\n\n    def test_resign_with_token(self):\n        credentials = botocore.credentials.Credentials(\n            access_key='foo', secret_key='bar', token='baz'\n        )\n        auth = botocore.auth.HmacV1Auth(credentials)\n        request = AWSRequest()\n        request.headers['Date'] = 'Thu, 17 Nov 2005 18:49:58 GMT'\n        request.headers['Content-Type'] = 'text/html'\n        request.method = 'PUT'\n        request.url = 'https://s3.amazonaws.com/bucket/key'\n\n        auth.add_auth(request)\n        original_auth = request.headers['Authorization']\n        # Resigning the request shouldn't change the authorization\n        # header.  We are also ensuring that the date stays the same\n        # because we're mocking out the formatdate() call.  There's\n        # another unit test that verifies we use the latest time\n        # when we sign the request.\n        auth.add_auth(request)\n        self.assertEqual(\n            request.headers.get_all('Authorization'), [original_auth]\n        )\n\n    def test_resign_uses_most_recent_date(self):\n        dates = [\n            'Thu, 17 Nov 2005 18:49:58 GMT',\n            'Thu, 17 Nov 2014 20:00:00 GMT',\n        ]\n        self.formatdate.side_effect = dates\n\n        request = AWSRequest()\n        request.headers['Content-Type'] = 'text/html'\n        request.method = 'PUT'\n        request.url = 'https://s3.amazonaws.com/bucket/key'\n\n        self.hmacv1.add_auth(request)\n        original_date = request.headers['Date']\n\n        self.hmacv1.add_auth(request)\n        modified_date = request.headers['Date']\n\n        # Each time we sign a request, we make another call to formatdate()\n        # so we should have a different date header each time.\n        self.assertEqual(original_date, dates[0])\n        self.assertEqual(modified_date, dates[1])\n\n\nclass TestSigV2(unittest.TestCase):\n    maxDiff = None\n\n    def setUp(self):\n        access_key = 'foo'\n        secret_key = 'bar'\n        self.credentials = botocore.credentials.Credentials(\n            access_key, secret_key\n        )\n        self.signer = botocore.auth.SigV2Auth(self.credentials)\n        self.time_patcher = mock.patch.object(\n            botocore.auth.time, 'gmtime', mock.Mock(wraps=time.gmtime)\n        )\n        mocked_time = self.time_patcher.start()\n        mocked_time.return_value = time.struct_time(\n            [2014, 6, 20, 8, 40, 23, 4, 171, 0]\n        )\n\n    def tearDown(self):\n        self.time_patcher.stop()\n\n    def test_put(self):\n        request = mock.Mock()\n        request.url = '/'\n        request.method = 'POST'\n        params = {'Foo': '\\u2713'}\n        result = self.signer.calc_signature(request, params)\n        self.assertEqual(\n            result,\n            ('Foo=%E2%9C%93', 'VCtWuwaOL0yMffAT8W4y0AFW3W4KUykBqah9S40rB+Q='),\n        )\n\n    def test_fields(self):\n        request = AWSRequest()\n        request.url = '/'\n        request.method = 'POST'\n        request.data = {'Foo': '\\u2713'}\n        self.signer.add_auth(request)\n        self.assertEqual(request.data['AWSAccessKeyId'], 'foo')\n        self.assertEqual(request.data['Foo'], '\\u2713')\n        self.assertEqual(request.data['Timestamp'], '2014-06-20T08:40:23Z')\n        self.assertEqual(\n            request.data['Signature'],\n            'Tiecw+t51tok4dTT8B4bg47zxHEM/KcD55f2/x6K22o=',\n        )\n        self.assertEqual(request.data['SignatureMethod'], 'HmacSHA256')\n        self.assertEqual(request.data['SignatureVersion'], '2')\n\n    def test_resign(self):\n        # Make sure that resigning after e.g. retries works\n        request = AWSRequest()\n        request.url = '/'\n        request.method = 'POST'\n        params = {\n            'Foo': '\\u2713',\n            'Signature': 'VCtWuwaOL0yMffAT8W4y0AFW3W4KUykBqah9S40rB+Q=',\n        }\n        result = self.signer.calc_signature(request, params)\n        self.assertEqual(\n            result,\n            ('Foo=%E2%9C%93', 'VCtWuwaOL0yMffAT8W4y0AFW3W4KUykBqah9S40rB+Q='),\n        )\n\n    def test_get(self):\n        request = AWSRequest()\n        request.url = '/'\n        request.method = 'GET'\n        request.params = {'Foo': '\\u2713'}\n        self.signer.add_auth(request)\n        self.assertEqual(request.params['AWSAccessKeyId'], 'foo')\n        self.assertEqual(request.params['Foo'], '\\u2713')\n        self.assertEqual(request.params['Timestamp'], '2014-06-20T08:40:23Z')\n        self.assertEqual(\n            request.params['Signature'],\n            'Un97klqZCONP65bA1+Iv4H3AcB2I40I4DBvw5ZERFPw=',\n        )\n        self.assertEqual(request.params['SignatureMethod'], 'HmacSHA256')\n        self.assertEqual(request.params['SignatureVersion'], '2')\n\n\nclass TestSigV3(unittest.TestCase):\n    maxDiff = None\n\n    def setUp(self):\n        self.access_key = 'access_key'\n        self.secret_key = 'secret_key'\n        self.credentials = botocore.credentials.Credentials(\n            self.access_key, self.secret_key\n        )\n        self.auth = botocore.auth.SigV3Auth(self.credentials)\n        self.date_mock = mock.patch('botocore.auth.formatdate')\n        self.formatdate = self.date_mock.start()\n        self.formatdate.return_value = 'Thu, 17 Nov 2005 18:49:58 GMT'\n\n    def tearDown(self):\n        self.date_mock.stop()\n\n    def test_signature_with_date_headers(self):\n        request = AWSRequest()\n        request.headers = {'Date': 'Thu, 17 Nov 2005 18:49:58 GMT'}\n        request.url = 'https://route53.amazonaws.com'\n        self.auth.add_auth(request)\n        self.assertEqual(\n            request.headers['X-Amzn-Authorization'],\n            (\n                'AWS3-HTTPS AWSAccessKeyId=access_key,Algorithm=HmacSHA256,'\n                'Signature=M245fo86nVKI8rLpH4HgWs841sBTUKuwciiTpjMDgPs='\n            ),\n        )\n\n    def test_resign_with_token(self):\n        credentials = botocore.credentials.Credentials(\n            access_key='foo', secret_key='bar', token='baz'\n        )\n        auth = botocore.auth.SigV3Auth(credentials)\n        request = AWSRequest()\n        request.headers['Date'] = 'Thu, 17 Nov 2005 18:49:58 GMT'\n        request.method = 'PUT'\n        request.url = 'https://route53.amazonaws.com/'\n        auth.add_auth(request)\n        original_auth = request.headers['X-Amzn-Authorization']\n        # Resigning the request shouldn't change the authorization\n        # header.\n        auth.add_auth(request)\n        self.assertEqual(\n            request.headers.get_all('X-Amzn-Authorization'), [original_auth]\n        )\n\n\nclass TestS3SigV4Auth(BaseTestWithFixedDate):\n    AuthClass = botocore.auth.S3SigV4Auth\n    maxDiff = None\n\n    def setUp(self):\n        super().setUp()\n        self.credentials = botocore.credentials.Credentials(\n            access_key='foo', secret_key='bar', token='baz'\n        )\n        self.auth = self.AuthClass(self.credentials, 'ec2', 'eu-central-1')\n        self.request = AWSRequest(data=io.BytesIO(b\"foo bar baz\"))\n        self.request.method = 'PUT'\n        self.request.url = 'https://s3.eu-central-1.amazonaws.com/'\n\n        self.client_config = mock.Mock()\n        self.s3_config = {}\n        self.client_config.s3 = self.s3_config\n\n        self.request.context = {'client_config': self.client_config}\n\n    def test_resign_with_content_hash(self):\n        self.auth.add_auth(self.request)\n        original_auth = self.request.headers['Authorization']\n\n        self.auth.add_auth(self.request)\n        self.assertEqual(\n            self.request.headers.get_all('Authorization'), [original_auth]\n        )\n\n    def test_signature_is_not_normalized(self):\n        request = AWSRequest()\n        request.url = 'https://s3.amazonaws.com/bucket/foo/./bar/../bar'\n        request.method = 'GET'\n        credentials = botocore.credentials.Credentials(\n            'access_key', 'secret_key'\n        )\n        auth = self.AuthClass(credentials, 's3', 'us-east-1')\n        auth.add_auth(request)\n        self.assertTrue(\n            request.headers['Authorization'].startswith('AWS4-HMAC-SHA256')\n        )\n\n    def test_query_string_params_in_urls(self):\n        if not hasattr(self.AuthClass, 'canonical_query_string'):\n            raise unittest.SkipTest(\n                '%s does not expose interim steps' % self.AuthClass.__name__\n            )\n\n        request = AWSRequest()\n        request.url = (\n            'https://s3.amazonaws.com/bucket?'\n            'marker=%C3%A4%C3%B6%C3%BC-01.txt&prefix'\n        )\n        request.data = {'Action': 'MyOperation'}\n        request.method = 'GET'\n\n        # Check that the canonical query string is correct formatting\n        # by ensuring that query string paramters that are added to the\n        # canonical query string are correctly formatted.\n        cqs = self.auth.canonical_query_string(request)\n        self.assertEqual('marker=%C3%A4%C3%B6%C3%BC-01.txt&prefix=', cqs)\n\n    def _test_blacklist_header(self, header, value):\n        request = AWSRequest()\n        request.url = 'https://s3.amazonaws.com/bucket/foo'\n        request.method = 'PUT'\n        request.headers[header] = value\n        credentials = botocore.credentials.Credentials(\n            'access_key', 'secret_key'\n        )\n        auth = self.AuthClass(credentials, 's3', 'us-east-1')\n        auth.add_auth(request)\n        self.assertNotIn(header, request.headers['Authorization'])\n\n    def test_blacklist_expect_headers(self):\n        self._test_blacklist_header('expect', '100-continue')\n\n    def test_blacklist_trace_id(self):\n        self._test_blacklist_header(\n            'x-amzn-trace-id', 'Root=foo;Parent=bar;Sampleid=1'\n        )\n\n    def test_blacklist_headers(self):\n        self._test_blacklist_header('user-agent', 'botocore/1.4.11')\n\n    def test_uses_sha256_if_config_value_is_true(self):\n        self.client_config.s3['payload_signing_enabled'] = True\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertNotEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_does_not_use_sha256_if_config_value_is_false(self):\n        self.client_config.s3['payload_signing_enabled'] = False\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_uses_sha256_if_md5_unset(self):\n        self.request.context['has_streaming_input'] = True\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertNotEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_uses_sha256_if_not_https(self):\n        self.request.context['has_streaming_input'] = True\n        self.request.headers.add_header('Content-MD5', 'foo')\n        self.request.url = 'http://s3.amazonaws.com/bucket'\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertNotEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_uses_sha256_if_not_streaming_upload(self):\n        self.request.context['has_streaming_input'] = False\n        self.request.headers.add_header('Content-MD5', 'foo')\n        self.request.url = 'https://s3.amazonaws.com/bucket'\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertNotEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_does_not_use_sha256_if_md5_set(self):\n        self.request.context['has_streaming_input'] = True\n        self.request.headers.add_header('Content-MD5', 'foo')\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_does_not_use_sha256_if_checksum_set(self):\n        self.request.context['has_streaming_input'] = True\n        self.request.context['checksum'] = {\n            'request_algorithm': {\n                'in': 'header',\n                'name': 'x-amz-checksum-sha256',\n                'algorithm': 'sha256',\n            }\n        }\n        self.request.headers.add_header('X-Amz-Checksum-sha256', 'foo')\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_does_not_use_sha256_if_context_config_set(self):\n        self.request.context['payload_signing_enabled'] = False\n        self.request.headers.add_header('Content-MD5', 'foo')\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_sha256_if_context_set_on_http(self):\n        self.request.context['payload_signing_enabled'] = False\n        self.request.headers.add_header('Content-MD5', 'foo')\n        self.request.url = 'http://s3.amazonaws.com/bucket'\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertNotEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_sha256_if_context_set_without_md5(self):\n        self.request.context['payload_signing_enabled'] = False\n        self.request.url = 'https://s3.amazonaws.com/bucket'\n        self.auth.add_auth(self.request)\n        sha_header = self.request.headers['X-Amz-Content-SHA256']\n        self.assertNotEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n\nclass TestSigV4(unittest.TestCase):\n    def setUp(self):\n        self.credentials = botocore.credentials.Credentials(\n            access_key='foo', secret_key='bar'\n        )\n\n    def create_signer(self, service_name='myservice', region='us-west-2'):\n        auth = botocore.auth.SigV4Auth(self.credentials, service_name, region)\n        return auth\n\n    def test_canonical_query_string(self):\n        request = AWSRequest()\n        request.url = (\n            'https://search-testdomain1-j67dwxlet67gf7ghwfmik2c67i.us-west-2.'\n            'cloudsearch.amazonaws.com/'\n            '2013-01-01/search?format=sdk&pretty=true&'\n            'q.options=%7B%22defaultOperator%22%3A%20%22and%22%2C%20%22'\n            'fields%22%3A%5B%22directors%5E10%22%5D%7D&q=George%20Lucas'\n        )\n        request.method = 'GET'\n        auth = self.create_signer('cloudsearchdomain', 'us-west-2')\n        actual = auth.canonical_query_string(request)\n        # Here 'q' should come before 'q.options'.\n        expected = (\n            \"format=sdk&pretty=true&q=George%20Lucas&q.options=%7B%22\"\n            \"defaultOperator%22%3A%20%22and%22%2C%20%22fields%22%3A%5B\"\n            \"%22directors%5E10%22%5D%7D\"\n        )\n        self.assertEqual(actual, expected)\n\n    def test_thread_safe_timestamp(self):\n        request = AWSRequest()\n        request.url = (\n            'https://search-testdomain1-j67dwxlet67gf7ghwfmik2c67i.us-west-2.'\n            'cloudsearch.amazonaws.com/'\n            '2013-01-01/search?format=sdk&pretty=true&'\n            'q.options=%7B%22defaultOperator%22%3A%20%22and%22%2C%20%22'\n            'fields%22%3A%5B%22directors%5E10%22%5D%7D&q=George%20Lucas'\n        )\n        request.method = 'GET'\n        auth = self.create_signer('cloudsearchdomain', 'us-west-2')\n        with mock.patch.object(\n            botocore.auth.datetime,\n            'datetime',\n            mock.Mock(wraps=datetime.datetime),\n        ) as mock_datetime:\n            original_utcnow = datetime.datetime(2014, 1, 1, 0, 0)\n\n            mock_datetime.utcnow.return_value = original_utcnow\n            # Go through the add_auth process once. This will attach\n            # a timestamp to the request at the beginning of auth.\n            auth.add_auth(request)\n            self.assertEqual(request.context['timestamp'], '20140101T000000Z')\n            # Ensure the date is in the Authorization header\n            self.assertIn('20140101', request.headers['Authorization'])\n            # Now suppose the utc time becomes the next day all of a sudden\n            mock_datetime.utcnow.return_value = datetime.datetime(\n                2014, 1, 2, 0, 0\n            )\n            # Smaller methods like the canonical request and string_to_sign\n            # should  have the timestamp attached to the request in their\n            # body and not what the time is now mocked as. This is to ensure\n            # there is no mismatching in timestamps when signing.\n            cr = auth.canonical_request(request)\n            self.assertIn('x-amz-date:20140101T000000Z', cr)\n            self.assertNotIn('x-amz-date:20140102T000000Z', cr)\n\n            sts = auth.string_to_sign(request, cr)\n            self.assertIn('20140101T000000Z', sts)\n            self.assertNotIn('20140102T000000Z', sts)\n\n    def test_payload_is_binary_file(self):\n        request = AWSRequest()\n        request.data = io.BytesIO('\\u2713'.encode())\n        request.url = 'https://amazonaws.com'\n        auth = self.create_signer()\n        payload = auth.payload(request)\n        self.assertEqual(\n            payload,\n            '1dabba21cdad44541f6b15796f8d22978fc7ea10c46aeceeeeb66c23b3ac7604',\n        )\n\n    def test_payload_is_bytes_type(self):\n        request = AWSRequest()\n        request.data = '\\u2713'.encode()\n        request.url = 'https://amazonaws.com'\n        auth = self.create_signer()\n        payload = auth.payload(request)\n        self.assertEqual(\n            payload,\n            '1dabba21cdad44541f6b15796f8d22978fc7ea10c46aeceeeeb66c23b3ac7604',\n        )\n\n    def test_payload_not_signed_if_disabled_in_context(self):\n        request = AWSRequest()\n        request.data = '\\u2713'.encode()\n        request.url = 'https://amazonaws.com'\n        request.context['payload_signing_enabled'] = False\n        auth = self.create_signer()\n        payload = auth.payload(request)\n        self.assertEqual(payload, 'UNSIGNED-PAYLOAD')\n\n    def test_content_sha256_set_if_payload_signing_disabled(self):\n        request = AWSRequest()\n        request.data = io.BytesIO('\\u2713'.encode())\n        request.url = 'https://amazonaws.com'\n        request.context['payload_signing_enabled'] = False\n        request.method = 'PUT'\n        auth = self.create_signer()\n        auth.add_auth(request)\n        sha_header = request.headers['X-Amz-Content-SHA256']\n        self.assertEqual(sha_header, 'UNSIGNED-PAYLOAD')\n\n    def test_collapse_multiple_spaces(self):\n        auth = self.create_signer()\n        original = HTTPHeaders()\n        original['foo'] = 'double  space'\n        headers = auth.canonical_headers(original)\n        self.assertEqual(headers, 'foo:double space')\n\n    def test_trims_leading_trailing_spaces(self):\n        auth = self.create_signer()\n        original = HTTPHeaders()\n        original['foo'] = '  leading  and  trailing  '\n        headers = auth.canonical_headers(original)\n        self.assertEqual(headers, 'foo:leading and trailing')\n\n    def test_strips_http_default_port(self):\n        request = AWSRequest()\n        request.url = 'http://s3.us-west-2.amazonaws.com:80/'\n        request.method = 'GET'\n        auth = self.create_signer('s3', 'us-west-2')\n        actual = auth.headers_to_sign(request)['host']\n        expected = 's3.us-west-2.amazonaws.com'\n        self.assertEqual(actual, expected)\n\n    def test_strips_https_default_port(self):\n        request = AWSRequest()\n        request.url = 'https://s3.us-west-2.amazonaws.com:443/'\n        request.method = 'GET'\n        auth = self.create_signer('s3', 'us-west-2')\n        actual = auth.headers_to_sign(request)['host']\n        expected = 's3.us-west-2.amazonaws.com'\n        self.assertEqual(actual, expected)\n\n    def test_strips_http_auth(self):\n        request = AWSRequest()\n        request.url = 'https://username:password@s3.us-west-2.amazonaws.com/'\n        request.method = 'GET'\n        auth = self.create_signer('s3', 'us-west-2')\n        actual = auth.headers_to_sign(request)['host']\n        expected = 's3.us-west-2.amazonaws.com'\n        self.assertEqual(actual, expected)\n\n    def test_strips_default_port_and_http_auth(self):\n        request = AWSRequest()\n        request.url = 'http://username:password@s3.us-west-2.amazonaws.com:80/'\n        request.method = 'GET'\n        auth = self.create_signer('s3', 'us-west-2')\n        actual = auth.headers_to_sign(request)['host']\n        expected = 's3.us-west-2.amazonaws.com'\n        self.assertEqual(actual, expected)\n\n\nclass TestSigV4Resign(BaseTestWithFixedDate):\n    maxDiff = None\n    AuthClass = botocore.auth.SigV4Auth\n\n    def setUp(self):\n        super().setUp()\n        self.credentials = botocore.credentials.Credentials(\n            access_key='foo', secret_key='bar', token='baz'\n        )\n        self.auth = self.AuthClass(self.credentials, 'ec2', 'us-west-2')\n        self.request = AWSRequest()\n        self.request.method = 'PUT'\n        self.request.url = 'https://ec2.amazonaws.com/'\n\n    def test_resign_request_with_date(self):\n        self.request.headers['Date'] = 'Thu, 17 Nov 2005 18:49:58 GMT'\n        self.auth.add_auth(self.request)\n        original_auth = self.request.headers['Authorization']\n\n        self.auth.add_auth(self.request)\n        self.assertEqual(\n            self.request.headers.get_all('Authorization'), [original_auth]\n        )\n\n    def test_sigv4_without_date(self):\n        self.auth.add_auth(self.request)\n        original_auth = self.request.headers['Authorization']\n\n        self.auth.add_auth(self.request)\n        self.assertEqual(\n            self.request.headers.get_all('Authorization'), [original_auth]\n        )\n\n\nclass BasePresignTest(unittest.TestCase):\n    def get_parsed_query_string(self, request):\n        query_string_dict = parse_qs(urlsplit(request.url).query)\n        # Also, parse_qs sets each value in the dict to be a list, but\n        # because we know that we won't have repeated keys, we simplify\n        # the dict and convert it back to a single value.\n        for key in query_string_dict:\n            query_string_dict[key] = query_string_dict[key][0]\n        return query_string_dict\n\n\nclass TestS3SigV2Presign(BasePresignTest):\n    def setUp(self):\n        self.access_key = 'access_key'\n        self.secret_key = 'secret_key'\n        self.credentials = botocore.credentials.Credentials(\n            self.access_key, self.secret_key\n        )\n        self.expires = 3000\n        self.auth = botocore.auth.HmacV1QueryAuth(\n            self.credentials, expires=self.expires\n        )\n\n        self.current_epoch_time = 1427427247.465591\n        self.time_patch = mock.patch('time.time')\n        self.time_mock = self.time_patch.start()\n        self.time_mock.return_value = self.current_epoch_time\n\n        self.request = AWSRequest()\n        self.bucket = 'mybucket'\n        self.key = 'myobject'\n        self.path = 'https://s3.amazonaws.com/{}/{}'.format(\n            self.bucket, self.key\n        )\n        self.request.url = self.path\n        self.request.method = 'GET'\n\n    def tearDown(self):\n        self.time_patch.stop()\n        super().tearDown()\n\n    def test_presign_with_query_string(self):\n        self.request.url = (\n            'https://foo-bucket.s3.amazonaws.com/image.jpg'\n            '?response-content-disposition='\n            'attachment%3B%20filename%3D%22download.jpg%22'\n        )\n        self.auth.add_auth(self.request)\n        query_string = self.get_parsed_query_string(self.request)\n        # We should have still kept the response-content-disposition\n        # in the query string.\n        self.assertIn('response-content-disposition', query_string)\n        self.assertEqual(\n            query_string['response-content-disposition'],\n            'attachment; filename=\"download.jpg\"',\n        )\n        # But we should have also added the parts from the signer.\n        self.assertEqual(query_string['AWSAccessKeyId'], self.access_key)\n\n    def test_presign_no_headers(self):\n        self.auth.add_auth(self.request)\n        self.assertTrue(self.request.url.startswith(self.path + '?'))\n        query_string = self.get_parsed_query_string(self.request)\n        self.assertEqual(query_string['AWSAccessKeyId'], self.access_key)\n        self.assertEqual(\n            query_string['Expires'],\n            str(int(self.current_epoch_time) + self.expires),\n        )\n        self.assertEqual(\n            query_string['Signature'], 'ZRSgywstwIruKLTLt/Bcrf9H1K4='\n        )\n\n    def test_presign_with_x_amz_headers(self):\n        self.request.headers['x-amz-security-token'] = 'foo'\n        self.request.headers['x-amz-acl'] = 'read-only'\n        self.auth.add_auth(self.request)\n        query_string = self.get_parsed_query_string(self.request)\n        self.assertEqual(query_string['x-amz-security-token'], 'foo')\n        self.assertEqual(query_string['x-amz-acl'], 'read-only')\n        self.assertEqual(\n            query_string['Signature'], '5oyMAGiUk1E5Ry2BnFr6cIS3Gus='\n        )\n\n    def test_presign_with_content_headers(self):\n        self.request.headers['content-type'] = 'txt'\n        self.request.headers['content-md5'] = 'foo'\n        self.auth.add_auth(self.request)\n        query_string = self.get_parsed_query_string(self.request)\n        self.assertEqual(query_string['content-type'], 'txt')\n        self.assertEqual(query_string['content-md5'], 'foo')\n        self.assertEqual(\n            query_string['Signature'], '/YQRFdQGywXP74WrOx2ET/RUqz8='\n        )\n\n    def test_presign_with_unused_headers(self):\n        self.request.headers['user-agent'] = 'botocore'\n        self.auth.add_auth(self.request)\n        query_string = self.get_parsed_query_string(self.request)\n        self.assertNotIn('user-agent', query_string)\n        self.assertEqual(\n            query_string['Signature'], 'ZRSgywstwIruKLTLt/Bcrf9H1K4='\n        )\n\n\nclass TestSigV4Presign(BasePresignTest):\n    maxDiff = None\n    AuthClass = botocore.auth.SigV4QueryAuth\n\n    def setUp(self):\n        self.access_key = 'access_key'\n        self.secret_key = 'secret_key'\n        self.credentials = botocore.credentials.Credentials(\n            self.access_key, self.secret_key\n        )\n        self.service_name = 'myservice'\n        self.region_name = 'myregion'\n        self.auth = self.AuthClass(\n            self.credentials, self.service_name, self.region_name, expires=60\n        )\n        self.datetime_patcher = mock.patch.object(\n            botocore.auth.datetime,\n            'datetime',\n            mock.Mock(wraps=datetime.datetime),\n        )\n        mocked_datetime = self.datetime_patcher.start()\n        mocked_datetime.utcnow.return_value = datetime.datetime(\n            2014, 1, 1, 0, 0\n        )\n\n    def tearDown(self):\n        self.datetime_patcher.stop()\n        super().tearDown()\n\n    def test_presign_no_params(self):\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = 'https://ec2.us-east-1.amazonaws.com/'\n        self.auth.add_auth(request)\n        query_string = self.get_parsed_query_string(request)\n        self.assertEqual(\n            query_string,\n            {\n                'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n                'X-Amz-Credential': (\n                    'access_key/20140101/myregion/' 'myservice/aws4_request'\n                ),\n                'X-Amz-Date': '20140101T000000Z',\n                'X-Amz-Expires': '60',\n                'X-Amz-Signature': (\n                    'c70e0bcdb4cd3ee324f71c78195445b878'\n                    '8315af0800bbbdbbb6d05a616fb84c'\n                ),\n                'X-Amz-SignedHeaders': 'host',\n            },\n        )\n\n    def test_operation_params_before_auth_params(self):\n        # The spec is picky about this.\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = 'https://ec2.us-east-1.amazonaws.com/?Action=MyOperation'\n        self.auth.add_auth(request)\n        # Verify auth params come after the existing params.\n        self.assertIn('?Action=MyOperation&X-Amz', request.url)\n\n    def test_operation_params_before_auth_params_in_body(self):\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = 'https://ec2.us-east-1.amazonaws.com/'\n        request.data = {'Action': 'MyOperation'}\n        self.auth.add_auth(request)\n        # Same situation, the params from request.data come before the auth\n        # params in the query string.\n        self.assertIn('?Action=MyOperation&X-Amz', request.url)\n\n    def test_operation_params_before_auth_params_in_params(self):\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = 'https://ec2.us-east-1.amazonaws.com/'\n        request.params = {'Action': 'MyOperation'}\n        self.auth.add_auth(request)\n        # Same situation, the params from request.param come before the\n        # auth params in the query string.\n        self.assertIn('?Action=MyOperation&X-Amz', request.url)\n\n    def test_request_params_not_duplicated_in_prepare(self):\n        \"\"\"\n        params should be moved to query string in add_auth\n        and not rewritten at the end with request.prepare()\n        \"\"\"\n        request = AWSRequest(\n            method='GET',\n            url='https://ec2.us-east-1.amazonaws.com/',\n            params={'Action': 'MyOperation'},\n        )\n        self.auth.add_auth(request)\n        self.assertIn('?Action=MyOperation&X-Amz', request.url)\n        prep = request.prepare()\n        assert not prep.url.endswith('Action=MyOperation')\n\n    def test_presign_with_spaces_in_param(self):\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = 'https://ec2.us-east-1.amazonaws.com/'\n        request.data = {'Action': 'MyOperation', 'Description': 'With Spaces'}\n        self.auth.add_auth(request)\n        # Verify we encode spaces as '%20, and we don't use '+'.\n        self.assertIn('Description=With%20Spaces', request.url)\n\n    def test_presign_with_empty_param_value(self):\n        request = AWSRequest()\n        request.method = 'POST'\n        # actual URL format for creating a multipart upload\n        request.url = 'https://s3.amazonaws.com/mybucket/mykey?uploads'\n        self.auth.add_auth(request)\n        # verify that uploads param is still in URL\n        self.assertIn('uploads', request.url)\n\n    def test_s3_sigv4_presign(self):\n        auth = botocore.auth.S3SigV4QueryAuth(\n            self.credentials, self.service_name, self.region_name, expires=60\n        )\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = (\n            'https://s3.us-west-2.amazonaws.com/mybucket/keyname/.bar'\n        )\n        auth.add_auth(request)\n        query_string = self.get_parsed_query_string(request)\n        # We use a different payload:\n        self.assertEqual(auth.payload(request), 'UNSIGNED-PAYLOAD')\n        # which will result in a different X-Amz-Signature:\n        self.assertEqual(\n            query_string,\n            {\n                'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n                'X-Amz-Credential': (\n                    'access_key/20140101/myregion/' 'myservice/aws4_request'\n                ),\n                'X-Amz-Date': '20140101T000000Z',\n                'X-Amz-Expires': '60',\n                'X-Amz-Signature': (\n                    'ac1b8b9e47e8685c5c963d75e35e8741d55251'\n                    'cd955239cc1efad4dc7201db66'\n                ),\n                'X-Amz-SignedHeaders': 'host',\n            },\n        )\n\n    def test_presign_with_security_token(self):\n        self.credentials.token = 'security-token'\n        auth = botocore.auth.S3SigV4QueryAuth(\n            self.credentials, self.service_name, self.region_name, expires=60\n        )\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = 'https://ec2.us-east-1.amazonaws.com/'\n        auth.add_auth(request)\n        query_string = self.get_parsed_query_string(request)\n        self.assertEqual(\n            query_string['X-Amz-Security-Token'], 'security-token'\n        )\n\n    def test_presign_where_body_is_json_bytes(self):\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = 'https://myservice.us-east-1.amazonaws.com/'\n        request.data = b'{\"Param\": \"value\"}'\n        self.auth.add_auth(request)\n        query_string = self.get_parsed_query_string(request)\n        expected_query_string = {\n            'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n            'X-Amz-Credential': (\n                'access_key/20140101/myregion/myservice/aws4_request'\n            ),\n            'X-Amz-Expires': '60',\n            'X-Amz-Date': '20140101T000000Z',\n            'X-Amz-Signature': (\n                '8e1d372d168d532313ce6df8f64a7dc51d'\n                'e6f312a9cfba6e5b345d8a771e839c'\n            ),\n            'X-Amz-SignedHeaders': 'host',\n            'Param': 'value',\n        }\n        self.assertEqual(query_string, expected_query_string)\n\n    def test_presign_where_body_is_json_string(self):\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = 'https://myservice.us-east-1.amazonaws.com/'\n        request.data = '{\"Param\": \"value\"}'\n        self.auth.add_auth(request)\n        query_string = self.get_parsed_query_string(request)\n        expected_query_string = {\n            'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n            'X-Amz-Credential': (\n                'access_key/20140101/myregion/myservice/aws4_request'\n            ),\n            'X-Amz-Expires': '60',\n            'X-Amz-Date': '20140101T000000Z',\n            'X-Amz-Signature': (\n                '8e1d372d168d532313ce6df8f64a7dc51d'\n                'e6f312a9cfba6e5b345d8a771e839c'\n            ),\n            'X-Amz-SignedHeaders': 'host',\n            'Param': 'value',\n        }\n        self.assertEqual(query_string, expected_query_string)\n\n    def test_presign_content_type_form_encoded_not_signed(self):\n        request = AWSRequest()\n        request.method = 'GET'\n        request.url = 'https://myservice.us-east-1.amazonaws.com/'\n        request.headers[\n            'Content-Type'\n        ] = 'application/x-www-form-urlencoded; charset=utf-8'\n        self.auth.add_auth(request)\n        query_string = self.get_parsed_query_string(request)\n        signed_headers = query_string.get('X-Amz-SignedHeaders')\n        self.assertNotIn('content-type', signed_headers)\n\n\nclass BaseS3PresignPostTest(unittest.TestCase):\n    def setUp(self):\n        self.access_key = 'access_key'\n        self.secret_key = 'secret_key'\n        self.credentials = botocore.credentials.Credentials(\n            self.access_key, self.secret_key\n        )\n\n        self.service_name = 'myservice'\n        self.region_name = 'myregion'\n\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.policy = {\n            \"expiration\": \"2007-12-01T12:00:00.000Z\",\n            \"conditions\": [\n                {\"acl\": \"public-read\"},\n                {\"bucket\": self.bucket},\n                [\"starts-with\", \"$key\", self.key],\n            ],\n        }\n        self.fields = {\n            'key': self.key,\n            'acl': 'public-read',\n        }\n\n        self.request = AWSRequest()\n        self.request.url = 'https://s3.amazonaws.com/%s' % self.bucket\n        self.request.method = 'POST'\n\n        self.request.context['s3-presign-post-fields'] = self.fields\n        self.request.context['s3-presign-post-policy'] = self.policy\n\n\nclass TestS3SigV2Post(BaseS3PresignPostTest):\n    def setUp(self):\n        super().setUp()\n        self.auth = botocore.auth.HmacV1PostAuth(self.credentials)\n\n        self.current_epoch_time = 1427427247.465591\n        self.time_patch = mock.patch('time.time')\n        self.time_mock = self.time_patch.start()\n        self.time_mock.return_value = self.current_epoch_time\n\n    def tearDown(self):\n        self.time_patch.stop()\n        super().tearDown()\n\n    def test_presign_post(self):\n        self.auth.add_auth(self.request)\n        result_fields = self.request.context['s3-presign-post-fields']\n        self.assertEqual(\n            result_fields['AWSAccessKeyId'], self.credentials.access_key\n        )\n\n        result_policy = json.loads(\n            base64.b64decode(result_fields['policy']).decode('utf-8')\n        )\n        self.assertEqual(\n            result_policy['expiration'], '2007-12-01T12:00:00.000Z'\n        )\n        self.assertEqual(\n            result_policy['conditions'],\n            [\n                {\"acl\": \"public-read\"},\n                {\"bucket\": \"mybucket\"},\n                [\"starts-with\", \"$key\", \"mykey\"],\n            ],\n        )\n        self.assertIn('signature', result_fields)\n\n    def test_presign_post_with_security_token(self):\n        self.credentials.token = 'my-token'\n        self.auth = botocore.auth.HmacV1PostAuth(self.credentials)\n        self.auth.add_auth(self.request)\n        result_fields = self.request.context['s3-presign-post-fields']\n        self.assertEqual(result_fields['x-amz-security-token'], 'my-token')\n\n    def test_empty_fields_and_policy(self):\n        self.request = AWSRequest()\n        self.request.url = 'https://s3.amazonaws.com/%s' % self.bucket\n        self.request.method = 'POST'\n        self.auth.add_auth(self.request)\n\n        result_fields = self.request.context['s3-presign-post-fields']\n        self.assertEqual(\n            result_fields['AWSAccessKeyId'], self.credentials.access_key\n        )\n        result_policy = json.loads(\n            base64.b64decode(result_fields['policy']).decode('utf-8')\n        )\n        self.assertEqual(result_policy['conditions'], [])\n        self.assertIn('signature', result_fields)\n\n\nclass TestS3SigV4Post(BaseS3PresignPostTest):\n    def setUp(self):\n        super().setUp()\n        self.auth = botocore.auth.S3SigV4PostAuth(\n            self.credentials, self.service_name, self.region_name\n        )\n        self.datetime_patcher = mock.patch.object(\n            botocore.auth.datetime,\n            'datetime',\n            mock.Mock(wraps=datetime.datetime),\n        )\n        mocked_datetime = self.datetime_patcher.start()\n        mocked_datetime.utcnow.return_value = datetime.datetime(\n            2014, 1, 1, 0, 0\n        )\n\n    def tearDown(self):\n        self.datetime_patcher.stop()\n        super().tearDown()\n\n    def test_presign_post(self):\n        self.auth.add_auth(self.request)\n        result_fields = self.request.context['s3-presign-post-fields']\n        self.assertEqual(result_fields['x-amz-algorithm'], 'AWS4-HMAC-SHA256')\n        self.assertEqual(\n            result_fields['x-amz-credential'],\n            'access_key/20140101/myregion/myservice/aws4_request',\n        )\n        self.assertEqual(result_fields['x-amz-date'], '20140101T000000Z')\n\n        result_policy = json.loads(\n            base64.b64decode(result_fields['policy']).decode('utf-8')\n        )\n        self.assertEqual(\n            result_policy['expiration'], '2007-12-01T12:00:00.000Z'\n        )\n        self.assertEqual(\n            result_policy['conditions'],\n            [\n                {\"acl\": \"public-read\"},\n                {\"bucket\": \"mybucket\"},\n                [\"starts-with\", \"$key\", \"mykey\"],\n                {\"x-amz-algorithm\": \"AWS4-HMAC-SHA256\"},\n                {\n                    \"x-amz-credential\": \"access_key/20140101/myregion/myservice/aws4_request\"\n                },\n                {\"x-amz-date\": \"20140101T000000Z\"},\n            ],\n        )\n        self.assertIn('x-amz-signature', result_fields)\n\n    def test_presign_post_with_security_token(self):\n        self.credentials.token = 'my-token'\n        self.auth = botocore.auth.S3SigV4PostAuth(\n            self.credentials, self.service_name, self.region_name\n        )\n        self.auth.add_auth(self.request)\n        result_fields = self.request.context['s3-presign-post-fields']\n        self.assertEqual(result_fields['x-amz-security-token'], 'my-token')\n\n    def test_empty_fields_and_policy(self):\n        self.request = AWSRequest()\n        self.request.url = 'https://s3.amazonaws.com/%s' % self.bucket\n        self.request.method = 'POST'\n        self.auth.add_auth(self.request)\n\n        result_fields = self.request.context['s3-presign-post-fields']\n        self.assertEqual(result_fields['x-amz-algorithm'], 'AWS4-HMAC-SHA256')\n        self.assertEqual(\n            result_fields['x-amz-credential'],\n            'access_key/20140101/myregion/myservice/aws4_request',\n        )\n        self.assertEqual(result_fields['x-amz-date'], '20140101T000000Z')\n\n        result_policy = json.loads(\n            base64.b64decode(result_fields['policy']).decode('utf-8')\n        )\n        self.assertEqual(\n            result_policy['conditions'],\n            [\n                {\"x-amz-algorithm\": \"AWS4-HMAC-SHA256\"},\n                {\n                    \"x-amz-credential\": \"access_key/20140101/myregion/myservice/aws4_request\"\n                },\n                {\"x-amz-date\": \"20140101T000000Z\"},\n            ],\n        )\n        self.assertIn('x-amz-signature', result_fields)\n", "tests/unit/auth/test_sigv4.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\"\"\"Signature Version 4 test suite.\n\nAWS provides a test suite for signature version 4:\n\nhttps://github.com/awslabs/aws-c-auth/tree/v0.3.15/tests/aws-sig-v4-test-suite\n\nThis module contains logic to run these tests.  The test files were\nplaced in ./aws4_testsuite, and we're using those to dynamically\ngenerate testcases based on these files.\n\n\"\"\"\nimport datetime\nimport io\nimport logging\nimport os\nimport re\nfrom http.server import BaseHTTPRequestHandler\n\nimport pytest\n\nimport botocore.auth\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.compat import parse_qsl, urlsplit\nfrom botocore.credentials import Credentials\nfrom tests import FreezeTime\n\nSECRET_KEY = \"wJalrXUtnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY\"\nACCESS_KEY = 'AKIDEXAMPLE'\nDATE = datetime.datetime(2015, 8, 30, 12, 36, 0)\nSERVICE = 'service'\nREGION = 'us-east-1'\n\nTESTSUITE_DIR = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), 'aws4_testsuite'\n)\n\n# The following tests are not run.  Each test has a comment as\n# to why the test is being ignored.\nTESTS_TO_IGNORE = [\n    # Bad request-line syntax, python's HTTP parser chokes on this.\n    'normalize-path/get-space',\n    # Multiple query params of the same key not supported by the SDKs.\n    'get-vanilla-query-order-key-case',\n    'get-vanilla-query-order-key',\n    'get-vanilla-query-order-value',\n]\n\nlog = logging.getLogger(__name__)\n\n\nclass RawHTTPRequest(BaseHTTPRequestHandler):\n    def __init__(self, raw_request):\n        if isinstance(raw_request, str):\n            raw_request = raw_request.encode('utf-8')\n        self.rfile = io.BytesIO(raw_request)\n        self.raw_requestline = self.rfile.readline()\n        self.error_code = None\n        self.error_message = None\n        self.parse_request()\n\n    def send_error(self, code, message):\n        self.error_code = code\n        self.error_message = message\n\n\ndef generate_test_cases():\n    for dirpath, dirnames, filenames in os.walk(TESTSUITE_DIR):\n        if not any(f.endswith('.req') for f in filenames):\n            continue\n\n        test_case = os.path.relpath(dirpath, TESTSUITE_DIR).replace(\n            os.sep, '/'\n        )\n        if test_case in TESTS_TO_IGNORE:\n            log.debug(\"Skipping test: %s\", test_case)\n            continue\n\n        yield test_case\n\n\n@pytest.mark.parametrize(\"test_case\", generate_test_cases())\n@FreezeTime(module=botocore.auth.datetime, date=DATE)\ndef test_signature_version_4(test_case):\n    _test_signature_version_4(test_case)\n\n\ndef create_request_from_raw_request(raw_request):\n    request = AWSRequest()\n    raw = RawHTTPRequest(raw_request)\n    if raw.error_code is not None:\n        raise Exception(raw.error_message)\n    request.method = raw.command\n    datetime_now = DATE\n    request.context['timestamp'] = datetime_now.strftime('%Y%m%dT%H%M%SZ')\n    for key, val in raw.headers.items():\n        request.headers[key] = val\n    request.data = raw.rfile.read()\n    host = raw.headers.get('host', '')\n    # For whatever reason, the BaseHTTPRequestHandler encodes\n    # the first line of the response as 'iso-8859-1',\n    # so we need decode this into utf-8.\n    if isinstance(raw.path, str):\n        raw.path = raw.path.encode('iso-8859-1').decode('utf-8')\n    url = f'https://{host}{raw.path}'\n    if '?' in url:\n        split_url = urlsplit(url)\n        params = dict(parse_qsl(split_url.query))\n        request.url = split_url.path\n        request.params = params\n    else:\n        request.url = url\n    return request\n\n\ndef _test_signature_version_4(test_case):\n    test_case = SignatureTestCase(test_case)\n    request = create_request_from_raw_request(test_case.raw_request)\n\n    auth = botocore.auth.SigV4Auth(test_case.credentials, SERVICE, REGION)\n    actual_canonical_request = auth.canonical_request(request)\n    actual_string_to_sign = auth.string_to_sign(\n        request, actual_canonical_request\n    )\n    auth.add_auth(request)\n    actual_auth_header = request.headers['Authorization']\n\n    # Some stuff only works right when you go through auth.add_auth()\n    # So don't assert the interim steps unless the end result was wrong.\n    if actual_auth_header != test_case.authorization_header:\n        assert_equal(\n            actual_canonical_request,\n            test_case.canonical_request,\n            test_case.raw_request,\n            'canonical_request',\n        )\n\n        assert_equal(\n            actual_string_to_sign,\n            test_case.string_to_sign,\n            test_case.raw_request,\n            'string_to_sign',\n        )\n\n        assert_equal(\n            actual_auth_header,\n            test_case.authorization_header,\n            test_case.raw_request,\n            'authheader',\n        )\n\n\ndef assert_equal(actual, expected, raw_request, part):\n    if actual != expected:\n        message = \"The %s did not match\" % part\n        message += f\"\\nACTUAL:{actual!r} !=\\nEXPECT:{expected!r}\"\n        message += '\\nThe raw request was:\\n%s' % raw_request\n        raise AssertionError(message)\n\n\nclass SignatureTestCase:\n    def __init__(self, test_case):\n        filepath = os.path.join(\n            TESTSUITE_DIR, test_case, os.path.basename(test_case)\n        )\n        self.raw_request = open(filepath + '.req', encoding='utf-8').read()\n        self.canonical_request = (\n            open(filepath + '.creq', encoding='utf-8').read().replace('\\r', '')\n        )\n        self.string_to_sign = (\n            open(filepath + '.sts', encoding='utf-8').read().replace('\\r', '')\n        )\n        self.authorization_header = (\n            open(filepath + '.authz', encoding='utf-8')\n            .read()\n            .replace('\\r', '')\n        )\n        self.signed_request = open(filepath + '.sreq', encoding='utf-8').read()\n\n        token_pattern = r'^x-amz-security-token:(.*)$'\n        token_match = re.search(\n            token_pattern, self.canonical_request, re.MULTILINE\n        )\n        token = token_match.group(1) if token_match else None\n        self.credentials = Credentials(ACCESS_KEY, SECRET_KEY, token)\n", "tests/unit/auth/__init__.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/unit/retries/test_standard.py": "from collections import Counter\n\nimport pytest\n\nfrom botocore import model\nfrom botocore.awsrequest import AWSResponse\nfrom botocore.exceptions import (\n    ConnectionError,\n    HTTPClientError,\n    ReadTimeoutError,\n)\nfrom botocore.retries import quota, standard\nfrom tests import mock, unittest\n\nRETRYABLE_THROTTLED_RESPONSES = [\n    # From the spec under \"Throttling Errors\"\n    # The status codes technically don't matter here, but we're adding\n    # them for completeness.\n    # StatusCode, ErrorCode, Retryable?\n    (400, 'Throttling', True),\n    (400, 'ThrottlingException', True),\n    (400, 'ThrottledException', True),\n    (400, 'RequestThrottledException', True),\n    (400, 'TooManyRequestsException', True),\n    (400, 'ProvisionedThroughputExceededException', True),\n    (400, 'TransactionInProgressException', True),\n    (503, 'RequestLimitExceeded', True),\n    (509, 'BandwidthLimitExceeded', True),\n    (400, 'LimitExceededException', True),\n    (403, 'RequestThrottled', True),\n    (503, 'SlowDown', True),\n    (400, 'PriorRequestNotComplete', True),\n    (502, 'EC2ThrottledException', True),\n    # These are some negative test cases, not in the spec but we'll use\n    # to verify we can detect throttled errors correctly.\n    (400, 'NotAThrottlingError', False),\n    (500, 'InternalServerError', False),\n    # \"None\" here represents no parsed response we just have a plain\n    # HTTP response and a 400 status code response.\n    (400, None, False),\n    (500, None, False),\n    (200, None, False),\n]\n\nRETRYABLE_TRANSIENT_ERRORS = [\n    # StatusCode, Error, Retryable?\n    (400, 'RequestTimeout', True),\n    (400, 'RequestTimeoutException', True),\n    (400, 'PriorRequestNotComplete', True),\n    # \"Any HTTP response with an HTTP status code of 500, 502, 503, or 504\".\n    (500, None, True),\n    (502, None, True),\n    (503, None, True),\n    (504, None, True),\n    # We'll also add a few errors with an explicit error code to verify\n    # that the code doesn't matter.\n    (500, 'InternalServiceError', True),\n    (502, 'BadError', True),\n    # These are botocore specific errors that correspond to\n    # \"Any IO (socket) level error where we are unable to read an HTTP\n    # response.\n    (None, ConnectionError(error='unknown'), True),\n    (None, HTTPClientError(error='unknown'), True),\n    # Negative cases\n    (200, None, False),\n    # This is a throttling error not a transient error\n    (400, 'Throttling', False),\n    (400, None, False),\n]\n\n\n# These tests are intended to be paired with the\n# SERVICE_DESCRIPTION_WITH_RETRIES definition.\nRETRYABLE_MODELED_ERRORS = [\n    (400, 'ModeledThrottlingError', True),\n    (400, 'ModeledRetryableError', True),\n    # Note this is ErrorCodeRetryable, not ModeledRetryableErrorWithCode,\n    # because the shape has a error code defined for it.\n    (400, 'ErrorCodeRetryable', True),\n    (400, 'NonRetryableError', False),\n    (None, ConnectionError(error='unknown'), False),\n]\n\n\nSERVICE_DESCRIPTION_WITH_RETRIES = {\n    'metadata': {},\n    'operations': {\n        'TestOperation': {\n            'name': 'TestOperation',\n            'input': {'shape': 'FakeInputOutputShape'},\n            'output': {'shape': 'FakeInputOutputShape'},\n            'errors': [\n                {'shape': 'ModeledThrottlingError'},\n                {'shape': 'ModeledRetryableError'},\n                {'shape': 'ModeledRetryableErrorWithCode'},\n                {'shape': 'NonRetryableError'},\n            ],\n        }\n    },\n    'shapes': {\n        'FakeInputOutputShape': {\n            'type': 'structure',\n            'members': {},\n        },\n        'ModeledThrottlingError': {\n            'type': 'structure',\n            'members': {\n                'message': {\n                    'shape': 'ErrorMessage',\n                }\n            },\n            'exception': True,\n            'retryable': {'throttling': True},\n        },\n        'ModeledRetryableError': {\n            'type': 'structure',\n            'members': {\n                'message': {\n                    'shape': 'ErrorMessage',\n                }\n            },\n            'exception': True,\n            'retryable': {},\n        },\n        'ModeledRetryableErrorWithCode': {\n            'type': 'structure',\n            'members': {\n                'message': {\n                    'shape': 'ErrorMessage',\n                }\n            },\n            'error': {\n                'code': 'ErrorCodeRetryable',\n            },\n            'exception': True,\n            'retryable': {'throttling': True},\n        },\n        'NonRetryableError': {\n            'type': 'structure',\n            'members': {\n                'message': {\n                    'shape': 'ErrorMessage',\n                }\n            },\n            'exception': True,\n        },\n    },\n}\n\n\n@pytest.mark.parametrize('case', RETRYABLE_TRANSIENT_ERRORS)\ndef test_can_detect_retryable_transient_errors(case):\n    transient_checker = standard.TransientRetryableChecker()\n    _verify_retryable(transient_checker, None, *case)\n\n\n@pytest.mark.parametrize('case', RETRYABLE_THROTTLED_RESPONSES)\ndef test_can_detect_retryable_throttled_errors(case):\n    throttled_checker = standard.ThrottledRetryableChecker()\n    _verify_retryable(throttled_checker, None, *case)\n\n\n@pytest.mark.parametrize('case', RETRYABLE_MODELED_ERRORS)\ndef test_can_detect_modeled_retryable_errors(case):\n    modeled_retry_checker = standard.ModeledRetryableChecker()\n    _verify_retryable(\n        modeled_retry_checker, get_operation_model_with_retries(), *case\n    )\n\n\n@pytest.mark.parametrize(\n    'case',\n    [\n        case\n        for case in RETRYABLE_TRANSIENT_ERRORS\n        + RETRYABLE_THROTTLED_RESPONSES\n        + RETRYABLE_MODELED_ERRORS\n        if case[2]\n    ],\n)\ndef test_standard_retry_conditions(case):\n    \"\"\"This is verifying that the high level object used for checking\n    retry conditions still handles all the individual testcases.\n\n    It's possible that cases that are retryable for an individual checker\n    aren't retryable for a different checker.  We need to filter out all\n    the False cases (if case[2]).\n    \"\"\"\n    standard_checker = standard.StandardRetryConditions()\n    op_model = get_operation_model_with_retries()\n    _verify_retryable(standard_checker, op_model, *case)\n\n\ndef get_operation_model_with_retries():\n    service = model.ServiceModel(\n        SERVICE_DESCRIPTION_WITH_RETRIES, service_name='my-service'\n    )\n    return service.operation_model('TestOperation')\n\n\ndef _verify_retryable(\n    checker, operation_model, status_code, error, is_retryable\n):\n    http_response = AWSResponse(\n        status_code=status_code, raw=None, headers={}, url='https://foo/'\n    )\n    parsed_response = None\n    caught_exception = None\n    if error is not None:\n        if isinstance(error, Exception):\n            caught_exception = error\n        else:\n            parsed_response = {'Error': {'Code': error, 'Message': 'Error'}}\n    context = standard.RetryContext(\n        attempt_number=1,\n        operation_model=operation_model,\n        parsed_response=parsed_response,\n        http_response=http_response,\n        caught_exception=caught_exception,\n    )\n    assert checker.is_retryable(context) == is_retryable\n\n\ndef arbitrary_retry_context():\n    # Used when you just need a dummy retry context that looks like\n    # a failed request.\n    return standard.RetryContext(\n        attempt_number=1,\n        operation_model=None,\n        parsed_response={'Error': {'Code': 'ErrorCode', 'Message': 'message'}},\n        http_response=AWSResponse(\n            status_code=500, raw=None, headers={}, url='https://foo'\n        ),\n        caught_exception=None,\n    )\n\n\ndef test_can_honor_max_attempts():\n    checker = standard.MaxAttemptsChecker(max_attempts=3)\n    context = arbitrary_retry_context()\n    context.attempt_number = 1\n    assert checker.is_retryable(context) is True\n\n    context.attempt_number = 2\n    assert checker.is_retryable(context) is True\n\n    context.attempt_number = 3\n    assert checker.is_retryable(context) is False\n\n\ndef test_max_attempts_adds_metadata_key_when_reached():\n    checker = standard.MaxAttemptsChecker(max_attempts=3)\n    context = arbitrary_retry_context()\n    context.attempt_number = 3\n    assert checker.is_retryable(context) is False\n    assert context.get_retry_metadata() == {'MaxAttemptsReached': True}\n\n\ndef test_retries_context_not_on_request_context():\n    checker = standard.MaxAttemptsChecker(max_attempts=3)\n    context = arbitrary_retry_context()\n    context.attempt_number = 3\n    assert checker.is_retryable(context) is False\n    assert context.request_context == {}\n\n\ndef test_can_create_default_retry_handler():\n    mock_client = mock.Mock()\n    mock_client.meta.service_model.service_id = model.ServiceId('my-service')\n    assert isinstance(\n        standard.register_retry_handler(mock_client), standard.RetryHandler\n    )\n    call_args_list = mock_client.meta.events.register.call_args_list\n    # We should have registered the retry quota to after-calls\n    first_call = call_args_list[0][0]\n    second_call = call_args_list[1][0]\n    # Not sure if there's a way to verify the class associated with the\n    # bound method matches what we expect.\n    assert first_call[0] == 'after-call.my-service'\n    assert second_call[0] == 'needs-retry.my-service'\n\n\nclass TestRetryHandler(unittest.TestCase):\n    def setUp(self):\n        self.retry_policy = mock.Mock(spec=standard.RetryPolicy)\n        self.retry_event_adapter = mock.Mock(spec=standard.RetryEventAdapter)\n        self.retry_quota = mock.Mock(spec=standard.RetryQuotaChecker)\n        self.retry_handler = standard.RetryHandler(\n            retry_policy=self.retry_policy,\n            retry_event_adapter=self.retry_event_adapter,\n            retry_quota=self.retry_quota,\n        )\n\n    def test_does_need_retry(self):\n        self.retry_event_adapter.create_retry_context.return_value = (\n            mock.sentinel.retry_context\n        )\n        self.retry_policy.should_retry.return_value = True\n        self.retry_quota.acquire_retry_quota.return_value = True\n        self.retry_policy.compute_retry_delay.return_value = 1\n\n        self.assertEqual(self.retry_handler.needs_retry(fake_kwargs='foo'), 1)\n        self.retry_event_adapter.create_retry_context.assert_called_with(\n            fake_kwargs='foo'\n        )\n        self.retry_policy.should_retry.assert_called_with(\n            mock.sentinel.retry_context\n        )\n        self.retry_quota.acquire_retry_quota.assert_called_with(\n            mock.sentinel.retry_context\n        )\n        self.retry_policy.compute_retry_delay.assert_called_with(\n            mock.sentinel.retry_context\n        )\n\n    def test_does_not_need_retry(self):\n        self.retry_event_adapter.create_retry_context.return_value = (\n            mock.sentinel.retry_context\n        )\n        self.retry_policy.should_retry.return_value = False\n\n        self.assertIsNone(self.retry_handler.needs_retry(fake_kwargs='foo'))\n        # Shouldn't consult quota if we don't have a retryable condition.\n        self.assertFalse(self.retry_quota.acquire_retry_quota.called)\n\n    def test_needs_retry_but_not_enough_quota(self):\n        self.retry_event_adapter.create_retry_context.return_value = (\n            mock.sentinel.retry_context\n        )\n        self.retry_policy.should_retry.return_value = True\n        self.retry_quota.acquire_retry_quota.return_value = False\n\n        self.assertIsNone(self.retry_handler.needs_retry(fake_kwargs='foo'))\n\n    def test_retry_handler_adds_retry_metadata_to_response(self):\n        self.retry_event_adapter.create_retry_context.return_value = (\n            mock.sentinel.retry_context\n        )\n        self.retry_policy.should_retry.return_value = False\n        self.assertIsNone(self.retry_handler.needs_retry(fake_kwargs='foo'))\n        adapter = self.retry_event_adapter\n        adapter.adapt_retry_response_from_context.assert_called_with(\n            mock.sentinel.retry_context\n        )\n\n\nclass TestRetryEventAdapter(unittest.TestCase):\n    def setUp(self):\n        self.success_response = {'ResponseMetadata': {}, 'Foo': {}}\n        self.failed_response = {'ResponseMetadata': {}, 'Error': {}}\n        self.http_success = AWSResponse(\n            status_code=200, raw=None, headers={}, url='https://foo/'\n        )\n        self.http_failed = AWSResponse(\n            status_code=500, raw=None, headers={}, url='https://foo/'\n        )\n        self.caught_exception = ConnectionError(error='unknown')\n\n    def test_create_context_from_success_response(self):\n        context = standard.RetryEventAdapter().create_retry_context(\n            response=(self.http_success, self.success_response),\n            attempts=1,\n            caught_exception=None,\n            request_dict={'context': {'foo': 'bar'}},\n            operation=mock.sentinel.operation_model,\n        )\n\n        self.assertEqual(context.attempt_number, 1)\n        self.assertEqual(\n            context.operation_model, mock.sentinel.operation_model\n        )\n        self.assertEqual(context.parsed_response, self.success_response)\n        self.assertEqual(context.http_response, self.http_success)\n        self.assertEqual(context.caught_exception, None)\n        self.assertEqual(context.request_context, {'foo': 'bar'})\n\n    def test_create_context_from_service_error(self):\n        context = standard.RetryEventAdapter().create_retry_context(\n            response=(self.http_failed, self.failed_response),\n            attempts=1,\n            caught_exception=None,\n            request_dict={'context': {'foo': 'bar'}},\n            operation=mock.sentinel.operation_model,\n        )\n        # We already tested the other attributes in\n        # test_create_context_from_success_response so we're only checking\n        # the attributes relevant to this test.\n        self.assertEqual(context.parsed_response, self.failed_response)\n        self.assertEqual(context.http_response, self.http_failed)\n\n    def test_create_context_from_exception(self):\n        context = standard.RetryEventAdapter().create_retry_context(\n            response=None,\n            attempts=1,\n            caught_exception=self.caught_exception,\n            request_dict={'context': {'foo': 'bar'}},\n            operation=mock.sentinel.operation_model,\n        )\n        self.assertEqual(context.parsed_response, None)\n        self.assertEqual(context.http_response, None)\n        self.assertEqual(context.caught_exception, self.caught_exception)\n\n    def test_can_inject_metadata_back_to_context(self):\n        adapter = standard.RetryEventAdapter()\n        context = adapter.create_retry_context(\n            attempts=1,\n            operation=None,\n            caught_exception=None,\n            request_dict={'context': {}},\n            response=(self.http_failed, self.failed_response),\n        )\n        context.add_retry_metadata(MaxAttemptsReached=True)\n        adapter.adapt_retry_response_from_context(context)\n        self.assertEqual(\n            self.failed_response['ResponseMetadata']['MaxAttemptsReached'],\n            True,\n        )\n\n\nclass TestRetryPolicy(unittest.TestCase):\n    def setUp(self):\n        self.retry_checker = mock.Mock(spec=standard.StandardRetryConditions)\n        self.retry_backoff = mock.Mock(spec=standard.ExponentialBackoff)\n        self.retry_policy = standard.RetryPolicy(\n            retry_checker=self.retry_checker, retry_backoff=self.retry_backoff\n        )\n\n    def test_delegates_to_retry_checker(self):\n        self.retry_checker.is_retryable.return_value = True\n        self.assertTrue(self.retry_policy.should_retry(mock.sentinel.context))\n        self.retry_checker.is_retryable.assert_called_with(\n            mock.sentinel.context\n        )\n\n    def test_delegates_to_retry_backoff(self):\n        self.retry_backoff.delay_amount.return_value = 1\n        self.assertEqual(\n            self.retry_policy.compute_retry_delay(mock.sentinel.context), 1\n        )\n        self.retry_backoff.delay_amount.assert_called_with(\n            mock.sentinel.context\n        )\n\n\nclass TestExponentialBackoff(unittest.TestCase):\n    def setUp(self):\n        self.random = lambda: 1\n        self.backoff = standard.ExponentialBackoff(\n            max_backoff=20, random=self.random\n        )\n\n    def test_range_of_exponential_backoff(self):\n        backoffs = [\n            self.backoff.delay_amount(standard.RetryContext(attempt_number=i))\n            for i in range(1, 10)\n        ]\n        # Note that we're capped at 20 which is our max backoff.\n        self.assertEqual(backoffs, [1, 2, 4, 8, 16, 20, 20, 20, 20])\n\n    def test_exponential_backoff_with_jitter(self):\n        backoff = standard.ExponentialBackoff()\n        backoffs = [\n            backoff.delay_amount(standard.RetryContext(attempt_number=3))\n            for i in range(10)\n        ]\n        # For attempt number 3, we should have a max value of 4 (2 ^ 2),\n        # so we can assert all the backoff values are within that range.\n        for x in backoffs:\n            self.assertTrue(0 <= x <= 4)\n\n    def test_uniform_rand_dist_on_max_attempts(self):\n        backoff = standard.ExponentialBackoff()\n        num_datapoints = 10_000\n        backoffs = [\n            backoff.delay_amount(standard.RetryContext(attempt_number=10))\n            for i in range(num_datapoints)\n        ]\n        self._assert_looks_like_uniform_distribution(backoffs)\n\n    def _assert_looks_like_uniform_distribution(self, backoffs):\n        histogram = Counter(int(el) for el in backoffs)\n        expected_value = len(backoffs) / len(histogram)\n        # This is an arbitrarily chosen tolerance, but we're being fairly\n        # lenient here and giving a 20% tolerance.  We're only interested\n        # in cases where it's obviously broken and not a uniform distribution.\n        tolerance = 0.20\n        low = expected_value - (expected_value * tolerance)\n        high = expected_value + (expected_value * tolerance)\n        out_of_range = [\n            str(i) for i in histogram.values() if not low <= i <= high\n        ]\n        if out_of_range:\n            raise AssertionError(\n                \"Backoff values outside of uniform distribution range \"\n                f\"({low} - {high}): {', '.join(out_of_range)}\"\n            )\n\n\nclass TestRetryQuotaChecker(unittest.TestCase):\n    def setUp(self):\n        self.quota = quota.RetryQuota(500)\n        self.quota_checker = standard.RetryQuotaChecker(self.quota)\n        self.request_context = {}\n\n    def create_context(self, is_timeout_error=False, status_code=200):\n        caught_exception = None\n        if is_timeout_error:\n            caught_exception = ReadTimeoutError(endpoint_url='https://foo')\n        http_response = AWSResponse(\n            status_code=status_code, raw=None, headers={}, url='https://foo/'\n        )\n        context = standard.RetryContext(\n            attempt_number=1,\n            request_context=self.request_context,\n            caught_exception=caught_exception,\n            http_response=http_response,\n        )\n        return context\n\n    def test_can_acquire_quota_non_timeout_error(self):\n        self.assertTrue(\n            self.quota_checker.acquire_retry_quota(self.create_context())\n        )\n        self.assertEqual(self.request_context['retry_quota_capacity'], 5)\n\n    def test_can_acquire_quota_for_timeout_error(self):\n        self.assertTrue(\n            self.quota_checker.acquire_retry_quota(\n                self.create_context(is_timeout_error=True)\n            )\n        )\n        self.assertEqual(self.request_context['retry_quota_capacity'], 10)\n\n    def test_can_release_quota_based_on_context_value_on_success(self):\n        context = self.create_context()\n        # This is where we had to retry the request but eventually\n        # succeeded.\n        http_response = self.create_context(status_code=200).http_response\n        self.assertTrue(self.quota_checker.acquire_retry_quota(context))\n        self.assertEqual(self.quota.available_capacity, 495)\n        self.quota_checker.release_retry_quota(\n            context.request_context, http_response=http_response\n        )\n        self.assertEqual(self.quota.available_capacity, 500)\n\n    def test_dont_release_quota_if_all_retries_failed(self):\n        context = self.create_context()\n        # If max_attempts_reached is True, then it means we used up all\n        # our retry attempts and still failed.  In this case we shouldn't\n        # give any retry quota back.\n        http_response = self.create_context(status_code=500).http_response\n        self.assertTrue(self.quota_checker.acquire_retry_quota(context))\n        self.assertEqual(self.quota.available_capacity, 495)\n        self.quota_checker.release_retry_quota(\n            context.request_context, http_response=http_response\n        )\n        self.assertEqual(self.quota.available_capacity, 495)\n\n    def test_can_release_default_quota_if_not_in_context(self):\n        context = self.create_context()\n        self.assertTrue(self.quota_checker.acquire_retry_quota(context))\n        self.assertEqual(self.quota.available_capacity, 495)\n        # We're going to remove the quota amount from the request context.\n        # This represents a successful request with no retries.\n        self.request_context.pop('retry_quota_capacity')\n        self.quota_checker.release_retry_quota(\n            context.request_context, context.http_response\n        )\n        # We expect only 1 unit was released.\n        self.assertEqual(self.quota.available_capacity, 496)\n\n    def test_acquire_quota_fails(self):\n        quota_checker = standard.RetryQuotaChecker(\n            quota.RetryQuota(initial_capacity=5)\n        )\n        # The first one succeeds.\n        self.assertTrue(\n            quota_checker.acquire_retry_quota(self.create_context())\n        )\n        # But we should fail now because we're out of quota.\n        self.request_context.pop('retry_quota_capacity')\n        self.assertFalse(\n            quota_checker.acquire_retry_quota(self.create_context())\n        )\n        self.assertNotIn('retry_quota_capacity', self.request_context)\n\n    def test_quota_reached_adds_retry_metadata(self):\n        quota_checker = standard.RetryQuotaChecker(\n            quota.RetryQuota(initial_capacity=0)\n        )\n        context = self.create_context()\n        self.assertFalse(quota_checker.acquire_retry_quota(context))\n        self.assertEqual(\n            context.get_retry_metadata(), {'RetryQuotaReached': True}\n        )\n\n    def test_single_failed_request_does_not_give_back_quota(self):\n        context = self.create_context()\n        http_response = self.create_context(status_code=400).http_response\n        # First deduct some amount of the retry quota so we're not hitting\n        # the upper bound.\n        self.quota.acquire(50)\n        self.assertEqual(self.quota.available_capacity, 450)\n        self.quota_checker.release_retry_quota(\n            context.request_context, http_response=http_response\n        )\n        self.assertEqual(self.quota.available_capacity, 450)\n\n\nclass TestRetryContext(unittest.TestCase):\n    def test_can_get_error_code(self):\n        context = arbitrary_retry_context()\n        context.parsed_response['Error']['Code'] = 'MyErrorCode'\n        self.assertEqual(context.get_error_code(), 'MyErrorCode')\n\n    def test_no_error_code_if_no_parsed_response(self):\n        context = arbitrary_retry_context()\n        context.parsed_response = None\n        self.assertIsNone(context.get_error_code())\n\n    def test_no_error_code_returns_none(self):\n        context = arbitrary_retry_context()\n        context.parsed_response = {}\n        self.assertIsNone(context.get_error_code())\n\n    def test_can_add_retry_reason(self):\n        context = arbitrary_retry_context()\n        context.add_retry_metadata(MaxAttemptsReached=True)\n        self.assertEqual(\n            context.get_retry_metadata(), {'MaxAttemptsReached': True}\n        )\n\n    def test_handles_non_error_top_level_error_key_get_error_code(self):\n        response = AWSResponse(\n            status_code=200,\n            raw=None,\n            headers={},\n            url='https://foo',\n        )\n        # A normal response can have a top level \"Error\" key that doesn't map\n        # to an error code and should be ignored\n        context = standard.RetryContext(\n            attempt_number=1,\n            operation_model=None,\n            parsed_response={'Error': 'This is a 200 response body'},\n            http_response=response,\n            caught_exception=None,\n        )\n        self.assertEqual(context.get_error_code(), None)\n\n\nclass TestThrottlingErrorDetector(unittest.TestCase):\n    def setUp(self):\n        self.throttling_detector = standard.ThrottlingErrorDetector(\n            standard.RetryEventAdapter()\n        )\n\n    def create_needs_retry_kwargs(self, **kwargs):\n        retry_kwargs = {\n            'response': None,\n            'attempts': 1,\n            'operation': None,\n            'caught_exception': None,\n            'request_dict': {'context': {}},\n        }\n        retry_kwargs.update(kwargs)\n        return retry_kwargs\n\n    def test_can_check_error_from_code(self):\n        kwargs = self.create_needs_retry_kwargs()\n        kwargs['response'] = (None, {'Error': {'Code': 'ThrottledException'}})\n        self.assertTrue(self.throttling_detector.is_throttling_error(**kwargs))\n\n    def test_no_throttling_error(self):\n        kwargs = self.create_needs_retry_kwargs()\n        kwargs['response'] = (None, {'Error': {'Code': 'RandomError'}})\n        self.assertFalse(\n            self.throttling_detector.is_throttling_error(**kwargs)\n        )\n\n    def test_detects_modeled_errors(self):\n        kwargs = self.create_needs_retry_kwargs()\n        kwargs['response'] = (\n            None,\n            {'Error': {'Code': 'ModeledThrottlingError'}},\n        )\n        kwargs['operation'] = get_operation_model_with_retries()\n        self.assertTrue(self.throttling_detector.is_throttling_error(**kwargs))\n\n\nclass TestModeledRetryErrorDetector(unittest.TestCase):\n    def setUp(self):\n        self.modeled_error = standard.ModeledRetryErrorDetector()\n\n    def test_not_retryable(self):\n        context = arbitrary_retry_context()\n        self.assertIsNone(self.modeled_error.detect_error_type(context))\n\n    def test_transient_error(self):\n        context = arbitrary_retry_context()\n        context.parsed_response['Error']['Code'] = 'ModeledRetryableError'\n        context.operation_model = get_operation_model_with_retries()\n        self.assertEqual(\n            self.modeled_error.detect_error_type(context),\n            self.modeled_error.TRANSIENT_ERROR,\n        )\n\n    def test_throttling_error(self):\n        context = arbitrary_retry_context()\n        context.parsed_response['Error']['Code'] = 'ModeledThrottlingError'\n        context.operation_model = get_operation_model_with_retries()\n        self.assertEqual(\n            self.modeled_error.detect_error_type(context),\n            self.modeled_error.THROTTLING_ERROR,\n        )\n\n\nclass Yes(standard.BaseRetryableChecker):\n    def is_retryable(self, context):\n        return True\n\n\nclass No(standard.BaseRetryableChecker):\n    def is_retryable(self, context):\n        return False\n\n\nclass TestOrRetryChecker(unittest.TestCase):\n    def test_can_match_any_checker(self):\n        self.assertTrue(standard.OrRetryChecker([Yes(), No()]))\n        self.assertTrue(standard.OrRetryChecker([No(), Yes()]))\n        self.assertTrue(standard.OrRetryChecker([Yes(), Yes()]))\n\n    def test_false_if_no_checkers_match(self):\n        self.assertTrue(standard.OrRetryChecker([No(), No(), No()]))\n", "tests/unit/retries/test_special.py": "from botocore.awsrequest import AWSResponse\nfrom botocore.retries import special, standard\nfrom tests import mock, unittest\n\n\ndef create_fake_op_model(service_name):\n    model = mock.Mock()\n    model.service_model.service_name = service_name\n    return model\n\n\nclass TestRetryIDPCommunicationError(unittest.TestCase):\n    def setUp(self):\n        self.checker = special.RetryIDPCommunicationError()\n\n    def test_only_retries_error_for_sts(self):\n        context = standard.RetryContext(\n            attempt_number=1,\n            operation_model=create_fake_op_model('s3'),\n            parsed_response={\n                'Error': {\n                    'Code': 'IDPCommunicationError',\n                    'Message': 'message',\n                }\n            },\n            http_response=AWSResponse(\n                status_code=400, raw=None, headers={}, url='https://foo'\n            ),\n            caught_exception=None,\n        )\n        self.assertFalse(self.checker.is_retryable(context))\n\n    def test_can_retry_idp_communication_error(self):\n        context = standard.RetryContext(\n            attempt_number=1,\n            operation_model=create_fake_op_model('sts'),\n            parsed_response={\n                'Error': {\n                    'Code': 'IDPCommunicationError',\n                    'Message': 'message',\n                }\n            },\n            http_response=AWSResponse(\n                status_code=400, raw=None, headers={}, url='https://foo'\n            ),\n            caught_exception=None,\n        )\n        self.assertTrue(self.checker.is_retryable(context))\n\n    def test_not_idp_communication_error(self):\n        context = standard.RetryContext(\n            attempt_number=1,\n            operation_model=create_fake_op_model('sts'),\n            parsed_response={\n                'Error': {\n                    'Code': 'NotIDPCommunicationError',\n                    'Message': 'message',\n                }\n            },\n            http_response=AWSResponse(\n                status_code=400, raw=None, headers={}, url='https://foo'\n            ),\n            caught_exception=None,\n        )\n        self.assertFalse(self.checker.is_retryable(context))\n\n\nclass TestRetryDDBChecksumError(unittest.TestCase):\n    def setUp(self):\n        self.checker = special.RetryDDBChecksumError()\n\n    def raw_stream(self, contents):\n        raw = mock.Mock()\n        raw.stream.return_value = [contents]\n        return raw\n\n    def test_checksum_not_in_header(self):\n        context = standard.RetryContext(\n            attempt_number=1,\n            operation_model=create_fake_op_model('dynamodb'),\n            parsed_response={\n                'Anything': [\"foo\"],\n            },\n            http_response=AWSResponse(\n                status_code=200,\n                raw=self.raw_stream(b'foo'),\n                headers={},\n                url='https://foo',\n            ),\n            caught_exception=None,\n        )\n        self.assertFalse(self.checker.is_retryable(context))\n\n    def test_checksum_matches(self):\n        context = standard.RetryContext(\n            attempt_number=1,\n            operation_model=create_fake_op_model('dynamodb'),\n            parsed_response={\n                'Anything': [\"foo\"],\n            },\n            http_response=AWSResponse(\n                status_code=200,\n                raw=self.raw_stream(b'foo'),\n                headers={'x-amz-crc32': '2356372769'},\n                url='https://foo',\n            ),\n            caught_exception=None,\n        )\n        self.assertFalse(self.checker.is_retryable(context))\n\n    def test_checksum_not_matches(self):\n        context = standard.RetryContext(\n            attempt_number=1,\n            operation_model=create_fake_op_model('dynamodb'),\n            parsed_response={\n                'Anything': [\"foo\"],\n            },\n            http_response=AWSResponse(\n                status_code=200,\n                raw=self.raw_stream(b'foo'),\n                headers={'x-amz-crc32': '2356372768'},\n                url='https://foo',\n            ),\n            caught_exception=None,\n        )\n        self.assertTrue(self.checker.is_retryable(context))\n\n    def test_checksum_check_only_for_dynamodb(self):\n        context = standard.RetryContext(\n            attempt_number=1,\n            operation_model=create_fake_op_model('s3'),\n            parsed_response={\n                'Anything': [\"foo\"],\n            },\n            http_response=AWSResponse(\n                status_code=200,\n                raw=self.raw_stream(b'foo'),\n                headers={'x-amz-crc32': '2356372768'},\n                url='https://foo',\n            ),\n            caught_exception=None,\n        )\n        self.assertFalse(self.checker.is_retryable(context))\n", "tests/unit/retries/test_adaptive.py": "from botocore.retries import adaptive, bucket, standard, throttling\nfrom tests import mock, unittest\n\n\nclass FakeClock(bucket.Clock):\n    def __init__(self, timestamp_sequences):\n        self.timestamp_sequences = timestamp_sequences\n        self.sleep_call_amounts = []\n\n    def sleep(self, amount):\n        self.sleep_call_amounts.append(amount)\n\n    def current_time(self):\n        return self.timestamp_sequences.pop(0)\n\n\nclass TestCanCreateRetryHandler(unittest.TestCase):\n    def test_can_register_retry_handler(self):\n        client = mock.Mock()\n        limiter = adaptive.register_retry_handler(client)\n        self.assertEqual(\n            client.meta.events.register.call_args_list,\n            [\n                mock.call('before-send', limiter.on_sending_request),\n                mock.call('needs-retry', limiter.on_receiving_response),\n            ],\n        )\n\n\nclass TestClientRateLimiter(unittest.TestCase):\n    def setUp(self):\n        self.timestamp_sequences = [0]\n        self.clock = FakeClock(self.timestamp_sequences)\n        self.token_bucket = mock.Mock(spec=bucket.TokenBucket)\n        self.rate_adjustor = mock.Mock(spec=throttling.CubicCalculator)\n        self.rate_clocker = mock.Mock(spec=adaptive.RateClocker)\n        self.throttling_detector = mock.Mock(\n            spec=standard.ThrottlingErrorDetector\n        )\n\n    def create_client_limiter(self):\n        rate_limiter = adaptive.ClientRateLimiter(\n            rate_adjustor=self.rate_adjustor,\n            rate_clocker=self.rate_clocker,\n            token_bucket=self.token_bucket,\n            throttling_detector=self.throttling_detector,\n            clock=self.clock,\n        )\n        return rate_limiter\n\n    def test_bucket_bucket_acquisition_only_if_enabled(self):\n        rate_limiter = self.create_client_limiter()\n        rate_limiter.on_sending_request(request=mock.sentinel.request)\n        self.assertFalse(self.token_bucket.acquire.called)\n\n    def test_token_bucket_enabled_on_throttling_error(self):\n        rate_limiter = self.create_client_limiter()\n        self.throttling_detector.is_throttling_error.return_value = True\n        self.rate_clocker.record.return_value = 21\n        self.rate_adjustor.error_received.return_value = 17\n        rate_limiter.on_receiving_response()\n        # Now if we call on_receiving_response we should try to acquire\n        # token.\n        self.timestamp_sequences.append(1)\n        rate_limiter.on_sending_request(request=mock.sentinel.request)\n        self.assertTrue(self.token_bucket.acquire.called)\n\n    def test_max_rate_updated_on_success_response(self):\n        rate_limiter = self.create_client_limiter()\n        self.throttling_detector.is_throttling_error.return_value = False\n        self.rate_adjustor.success_received.return_value = 20\n        self.rate_clocker.record.return_value = 21\n        rate_limiter.on_receiving_response()\n        self.assertEqual(self.token_bucket.max_rate, 20)\n\n    def test_max_rate_cant_exceed_20_percent_max(self):\n        rate_limiter = self.create_client_limiter()\n        self.throttling_detector.is_throttling_error.return_value = False\n        # So if our actual measured sending rate is 20 TPS\n        self.rate_clocker.record.return_value = 20\n        # But the rate adjustor is telling us to go up to 100 TPS\n        self.rate_adjustor.success_received.return_value = 100\n\n        # The most we should go up is 2.0 * 20\n        rate_limiter.on_receiving_response()\n        self.assertEqual(self.token_bucket.max_rate, 2.0 * 20)\n\n\nclass TestRateClocker(unittest.TestCase):\n    def setUp(self):\n        self.timestamp_sequences = [0]\n        self.clock = FakeClock(self.timestamp_sequences)\n        self.rate_measure = adaptive.RateClocker(self.clock)\n        self.smoothing = 0.8\n\n    def test_initial_rate_is_0(self):\n        self.assertEqual(self.rate_measure.measured_rate, 0)\n\n    def test_time_updates_if_after_bucket_range(self):\n        self.timestamp_sequences.append(1)\n        # This should be 1 * 0.8 + 0 * 0.2, or just 0.8\n        self.assertEqual(self.rate_measure.record(), 0.8)\n\n    def test_can_measure_constant_rate(self):\n        # Timestamps of 1 every second indicate a rate of 1 TPS.\n        self.timestamp_sequences.extend(range(1, 21))\n        for _ in range(20):\n            self.rate_measure.record()\n        self.assertAlmostEqual(self.rate_measure.measured_rate, 1)\n\n    def test_uses_smoothing_to_favor_recent_weights(self):\n        self.timestamp_sequences.extend(\n            [\n                1,\n                1.5,\n                2,\n                2.5,\n                3,\n                3.5,\n                4,\n                # If we now wait 10 seconds (.1 TPS),\n                # our rate is somewhere between 2 TPS and .1 TPS.\n                14,\n            ]\n        )\n        for _ in range(7):\n            self.rate_measure.record()\n        # We should almost be at 2.0 but not quite.\n        self.assertGreaterEqual(self.rate_measure.measured_rate, 1.99)\n        self.assertLessEqual(self.rate_measure.measured_rate, 2.0)\n        # With our last recording we now drop down between 0.1 and 2\n        # depending on our smoothing factor.\n        self.rate_measure.record()\n        self.assertGreaterEqual(self.rate_measure.measured_rate, 0.1)\n        self.assertLessEqual(self.rate_measure.measured_rate, 2.0)\n\n    def test_noop_when_delta_t_is_0(self):\n        self.timestamp_sequences.extend([1, 1, 1, 2, 3])\n        for _ in range(5):\n            self.rate_measure.record()\n        self.assertGreaterEqual(self.rate_measure.measured_rate, 1.0)\n\n    def test_times_are_grouped_per_time_bucket(self):\n        # Using our default of 0.5 time buckets, we have:\n        self.timestamp_sequences.extend(\n            [\n                0.1,\n                0.2,\n                0.3,\n                0.4,\n                0.49,\n            ]\n        )\n        for _ in range(len(self.timestamp_sequences)):\n            self.rate_measure.record()\n        # This is showing the tradeoff we're making with measuring rates.\n        # we're currently in the window from 0 <= x < 0.5, which means\n        # we use the rate from the previous bucket, which is 0:\n        self.assertEqual(self.rate_measure.measured_rate, 0)\n        # However if we now add a new measurement that's in the next\n        # time bucket  0.5 <= x < 1.0\n        # we'll use the range from the previous bucket:\n        self.timestamp_sequences.append(0.5)\n        self.rate_measure.record()\n        # And our previous bucket will be:\n        # 12 * 0.8 + 0.2 * 0\n        self.assertEqual(self.rate_measure.measured_rate, 12 * 0.8)\n", "tests/unit/retries/test_throttling.py": "from botocore.retries import throttling\nfrom tests import unittest\n\n\nclass TestCubicCalculator(unittest.TestCase):\n    def create_cubic_calculator(\n        self, starting_max_rate=10, beta=0.7, scale_constant=0.4\n    ):\n        return throttling.CubicCalculator(\n            starting_max_rate=starting_max_rate,\n            scale_constant=scale_constant,\n            start_time=0,\n            beta=beta,\n        )\n\n    # For these tests, rather than duplicate the formulas in the tests,\n    # I want to check against a fixed set of inputs with by-hand verified\n    # values to ensure we're doing the calculations correctly.\n\n    def test_starting_params(self):\n        cubic = self.create_cubic_calculator(starting_max_rate=10)\n        self.assertAlmostEqual(\n            cubic.get_params_snapshot().k, 1.9574338205844317\n        )\n\n    def test_success_responses_until_max_hit(self):\n        # For this test we're interested in the behavior less so than\n        # the specific numbers.  There's a few cases we care about:\n        #\n        cubic = self.create_cubic_calculator(starting_max_rate=10)\n        params = cubic.get_params_snapshot()\n        start_k = params.k\n        start_w_max = params.w_max\n        # Before we get to t == start_k, our throttle is below our\n        # max w_max\n        assertLessEqual = self.assertLessEqual\n        assertLessEqual(cubic.success_received(start_k / 3.0), start_w_max)\n        assertLessEqual(cubic.success_received(start_k / 2.0), start_w_max)\n        assertLessEqual(cubic.success_received(start_k / 1.1), start_w_max)\n        # At t == start_k, we should be at w_max.\n        self.assertAlmostEqual(cubic.success_received(timestamp=start_k), 10.0)\n        # And once we pass start_k, we'll be above w_max.\n        self.assertGreaterEqual(\n            cubic.success_received(start_k * 1.1), start_w_max\n        )\n        self.assertGreaterEqual(\n            cubic.success_received(start_k * 2.0), start_w_max\n        )\n\n    def test_error_response_decreases_rate_by_beta(self):\n        # This is the default value here so we're just being explicit.\n        cubic = self.create_cubic_calculator(starting_max_rate=10, beta=0.7)\n\n        # So let's say we made it up to 8 TPS before we were throttled.\n        rate_when_throttled = 8\n        new_rate = cubic.error_received(\n            current_rate=rate_when_throttled, timestamp=1\n        )\n        self.assertAlmostEqual(new_rate, rate_when_throttled * 0.7)\n\n        new_params = cubic.get_params_snapshot()\n        self.assertEqual(\n            new_params,\n            throttling.CubicParams(\n                w_max=rate_when_throttled, k=1.8171205928321397, last_fail=1\n            ),\n        )\n\n    def test_t_0_should_match_beta_decrease(self):\n        # So if I have beta of 0.6\n        cubic = self.create_cubic_calculator(starting_max_rate=10, beta=0.6)\n        # When I get throttled I should decrease my rate by 60%.\n        new_rate = cubic.error_received(current_rate=10, timestamp=1)\n        self.assertEqual(new_rate, 6.0)\n        # And my starting rate at time t=1 should start at that new rate.\n        self.assertAlmostEqual(cubic.success_received(timestamp=1), 6.0)\n", "tests/unit/retries/test_quota.py": "from botocore.retries import quota\nfrom tests import unittest\n\n\nclass TestRetryQuota(unittest.TestCase):\n    def setUp(self):\n        self.retry_quota = quota.RetryQuota(50)\n\n    def test_can_acquire_amount(self):\n        self.assertTrue(self.retry_quota.acquire(5))\n        self.assertEqual(self.retry_quota.available_capacity, 45)\n\n    def test_can_release_amount(self):\n        self.assertTrue(self.retry_quota.acquire(5))\n        self.assertEqual(self.retry_quota.available_capacity, 45)\n        self.retry_quota.release(5)\n        self.assertEqual(self.retry_quota.available_capacity, 50)\n\n    def test_cant_exceed_max_capacity(self):\n        self.assertTrue(self.retry_quota.acquire(5))\n        self.assertEqual(self.retry_quota.available_capacity, 45)\n        self.retry_quota.release(10)\n        self.assertEqual(self.retry_quota.available_capacity, 50)\n\n    def test_noop_if_at_max_capacity(self):\n        self.retry_quota.release(10)\n        self.assertEqual(self.retry_quota.available_capacity, 50)\n\n    def test_cant_go_below_zero(self):\n        self.assertTrue(self.retry_quota.acquire(49))\n        self.assertEqual(self.retry_quota.available_capacity, 1)\n        self.assertFalse(self.retry_quota.acquire(10))\n        self.assertEqual(self.retry_quota.available_capacity, 1)\n", "tests/unit/retries/__init__.py": "", "tests/unit/retries/test_bucket.py": "from botocore.exceptions import CapacityNotAvailableError\nfrom botocore.retries import bucket\nfrom tests import unittest\n\n\nclass FakeClock(bucket.Clock):\n    def __init__(self, timestamp_sequences):\n        self.timestamp_sequences = timestamp_sequences\n        self.sleep_call_amounts = []\n\n    def sleep(self, amount):\n        self.sleep_call_amounts.append(amount)\n\n    def current_time(self):\n        return self.timestamp_sequences.pop(0)\n\n\nclass TestTokenBucket(unittest.TestCase):\n    def setUp(self):\n        self.timestamp_sequences = [0]\n        self.clock = FakeClock(self.timestamp_sequences)\n\n    def create_token_bucket(self, max_rate=10, min_rate=0.1):\n        return bucket.TokenBucket(\n            max_rate=max_rate, clock=self.clock, min_rate=min_rate\n        )\n\n    def test_can_acquire_amount(self):\n        self.timestamp_sequences.extend(\n            [\n                # Requests tokens every second, which is well below our\n                # 10 TPS fill rate.\n                1,\n                2,\n                3,\n                4,\n                5,\n            ]\n        )\n        token_bucket = self.create_token_bucket(max_rate=10)\n        for _ in range(5):\n            self.assertTrue(token_bucket.acquire(1, block=False))\n\n    def test_can_change_max_capacity_lower(self):\n        # Requests at 1 TPS.\n        self.timestamp_sequences.extend([1, 2, 3, 4, 5])\n        token_bucket = self.create_token_bucket(max_rate=10)\n        # Request the first 5 tokens with max_rate=10\n        for _ in range(5):\n            self.assertTrue(token_bucket.acquire(1, block=False))\n        # Now scale the max_rate down to 1 on the 5th second.\n        self.timestamp_sequences.append(5)\n        token_bucket.max_rate = 1\n        # And then from seconds 6-10 we request at one per second.\n        self.timestamp_sequences.extend([6, 7, 8, 9, 10])\n        for _ in range(5):\n            self.assertTrue(token_bucket.acquire(1, block=False))\n\n    def test_max_capacity_is_at_least_one(self):\n        token_bucket = self.create_token_bucket()\n        self.timestamp_sequences.append(1)\n        token_bucket.max_rate = 0.5\n        self.assertEqual(token_bucket.max_rate, 0.5)\n        self.assertEqual(token_bucket.max_capacity, 1)\n\n    def test_acquire_fails_on_non_block_mode_returns_false(self):\n        self.timestamp_sequences.extend(\n            [\n                # Initial creation time.\n                0,\n                # Requests a token 1 second later.\n                1,\n            ]\n        )\n        token_bucket = self.create_token_bucket(max_rate=10)\n        with self.assertRaises(CapacityNotAvailableError):\n            token_bucket.acquire(100, block=False)\n\n    def test_can_retrieve_at_max_send_rate(self):\n        self.timestamp_sequences.extend(\n            [\n                # Request a new token every 100ms (10 TPS) for 2 seconds.\n                1 + 0.1 * i\n                for i in range(20)\n            ]\n        )\n        token_bucket = self.create_token_bucket(max_rate=10)\n        for _ in range(20):\n            self.assertTrue(token_bucket.acquire(1, block=False))\n\n    def test_acquiring_blocks_when_capacity_reached(self):\n        # This is 1 token every 0.1 seconds.\n        token_bucket = self.create_token_bucket(max_rate=10)\n        self.timestamp_sequences.extend(\n            [\n                # The first acquire() happens after .1 seconds.\n                0.1,\n                # The second acquire() will fail because we get tokens at\n                # 1 per 0.1 seconds.  We will then sleep for 0.05 seconds until we\n                # get a new token.\n                0.15,\n                # And at 0.2 seconds we get our token.\n                0.2,\n                # And at 0.3 seconds we have no issues getting a token.\n                # Because we're using such small units (to avoid bloating the\n                # test run time), we have to go slightly over 0.3 seconds here.\n                0.300001,\n            ]\n        )\n        self.assertTrue(token_bucket.acquire(1, block=False))\n        self.assertEqual(token_bucket.available_capacity, 0)\n        self.assertTrue(token_bucket.acquire(1, block=True))\n        self.assertEqual(token_bucket.available_capacity, 0)\n        self.assertTrue(token_bucket.acquire(1, block=False))\n\n    def test_rate_cant_go_below_min(self):\n        token_bucket = self.create_token_bucket(max_rate=1, min_rate=0.2)\n        self.timestamp_sequences.append(1)\n        token_bucket.max_rate = 0.1\n        self.assertEqual(token_bucket.max_rate, 0.2)\n        self.assertEqual(token_bucket.max_capacity, 1)\n", "botocore/response.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\nfrom io import IOBase\n\nfrom urllib3.exceptions import ProtocolError as URLLib3ProtocolError\nfrom urllib3.exceptions import ReadTimeoutError as URLLib3ReadTimeoutError\n\nfrom botocore import parsers\nfrom botocore.compat import set_socket_timeout\nfrom botocore.exceptions import (\n    IncompleteReadError,\n    ReadTimeoutError,\n    ResponseStreamingError,\n)\n\n# Keep these imported.  There's pre-existing code that uses them.\nfrom botocore import ScalarTypes  # noqa\nfrom botocore.compat import XMLParseError  # noqa\nfrom botocore.hooks import first_non_none_response  # noqa\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass StreamingBody(IOBase):\n    \"\"\"Wrapper class for an http response body.\n\n    This provides a few additional conveniences that do not exist\n    in the urllib3 model:\n\n        * Set the timeout on the socket (i.e read() timeouts)\n        * Auto validation of content length, if the amount of bytes\n          we read does not match the content length, an exception\n          is raised.\n\n    \"\"\"\n\n    _DEFAULT_CHUNK_SIZE = 1024\n\n    def __init__(self, raw_stream, content_length):\n        self._raw_stream = raw_stream\n        self._content_length = content_length\n        self._amount_read = 0\n\n    def __del__(self):\n        # Extending destructor in order to preserve the underlying raw_stream.\n        # The ability to add custom cleanup logic introduced in Python3.4+.\n        # https://www.python.org/dev/peps/pep-0442/\n        pass\n\n    def set_socket_timeout(self, timeout):\n        \"\"\"Set the timeout seconds on the socket.\"\"\"\n        # The problem we're trying to solve is to prevent .read() calls from\n        # hanging.  This can happen in rare cases.  What we'd like to ideally\n        # do is set a timeout on the .read() call so that callers can retry\n        # the request.\n        # Unfortunately, this isn't currently possible in requests.\n        # See: https://github.com/kennethreitz/requests/issues/1803\n        # So what we're going to do is reach into the guts of the stream and\n        # grab the socket object, which we can set the timeout on.  We're\n        # putting in a check here so in case this interface goes away, we'll\n        # know.\n        try:\n            set_socket_timeout(self._raw_stream, timeout)\n        except AttributeError:\n            logger.error(\n                \"Cannot access the socket object of \"\n                \"a streaming response.  It's possible \"\n                \"the interface has changed.\",\n                exc_info=True,\n            )\n            raise\n\n    def readable(self):\n        try:\n            return self._raw_stream.readable()\n        except AttributeError:\n            return False\n\n    def read(self, amt=None):\n        \"\"\"Read at most amt bytes from the stream.\n\n        If the amt argument is omitted, read all data.\n        \"\"\"\n        try:\n            chunk = self._raw_stream.read(amt)\n        except URLLib3ReadTimeoutError as e:\n            # TODO: the url will be None as urllib3 isn't setting it yet\n            raise ReadTimeoutError(endpoint_url=e.url, error=e)\n        except URLLib3ProtocolError as e:\n            raise ResponseStreamingError(error=e)\n        self._amount_read += len(chunk)\n        if amt is None or (not chunk and amt > 0):\n            # If the server sends empty contents or\n            # we ask to read all of the contents, then we know\n            # we need to verify the content length.\n            self._verify_content_length()\n        return chunk\n\n    def readlines(self):\n        return self._raw_stream.readlines()\n\n    def __iter__(self):\n        \"\"\"Return an iterator to yield 1k chunks from the raw stream.\"\"\"\n        return self.iter_chunks(self._DEFAULT_CHUNK_SIZE)\n\n    def __next__(self):\n        \"\"\"Return the next 1k chunk from the raw stream.\"\"\"\n        current_chunk = self.read(self._DEFAULT_CHUNK_SIZE)\n        if current_chunk:\n            return current_chunk\n        raise StopIteration()\n\n    def __enter__(self):\n        return self._raw_stream\n\n    def __exit__(self, type, value, traceback):\n        self._raw_stream.close()\n\n    next = __next__\n\n    def iter_lines(self, chunk_size=_DEFAULT_CHUNK_SIZE, keepends=False):\n        \"\"\"Return an iterator to yield lines from the raw stream.\n\n        This is achieved by reading chunk of bytes (of size chunk_size) at a\n        time from the raw stream, and then yielding lines from there.\n        \"\"\"\n        pending = b''\n        for chunk in self.iter_chunks(chunk_size):\n            lines = (pending + chunk).splitlines(True)\n            for line in lines[:-1]:\n                yield line.splitlines(keepends)[0]\n            pending = lines[-1]\n        if pending:\n            yield pending.splitlines(keepends)[0]\n\n    def iter_chunks(self, chunk_size=_DEFAULT_CHUNK_SIZE):\n        \"\"\"Return an iterator to yield chunks of chunk_size bytes from the raw\n        stream.\n        \"\"\"\n        while True:\n            current_chunk = self.read(chunk_size)\n            if current_chunk == b\"\":\n                break\n            yield current_chunk\n\n    def _verify_content_length(self):\n        # See: https://github.com/kennethreitz/requests/issues/1855\n        # Basically, our http library doesn't do this for us, so we have\n        # to do this ourself.\n        if self._content_length is not None and self._amount_read != int(\n            self._content_length\n        ):\n            raise IncompleteReadError(\n                actual_bytes=self._amount_read,\n                expected_bytes=int(self._content_length),\n            )\n\n    def tell(self):\n        return self._raw_stream.tell()\n\n    def close(self):\n        \"\"\"Close the underlying http response stream.\"\"\"\n        self._raw_stream.close()\n\n\ndef get_response(operation_model, http_response):\n    protocol = operation_model.metadata['protocol']\n    response_dict = {\n        'headers': http_response.headers,\n        'status_code': http_response.status_code,\n    }\n    # TODO: Unfortunately, we have to have error logic here.\n    # If it looks like an error, in the streaming response case we\n    # need to actually grab the contents.\n    if response_dict['status_code'] >= 300:\n        response_dict['body'] = http_response.content\n    elif operation_model.has_streaming_output:\n        response_dict['body'] = StreamingBody(\n            http_response.raw, response_dict['headers'].get('content-length')\n        )\n    else:\n        response_dict['body'] = http_response.content\n\n    parser = parsers.create_parser(protocol)\n    return http_response, parser.parse(\n        response_dict, operation_model.output_shape\n    )\n", "botocore/tokens.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport logging\nimport os\nimport threading\nfrom datetime import datetime, timedelta\nfrom typing import NamedTuple, Optional\n\nimport dateutil.parser\nfrom dateutil.tz import tzutc\n\nfrom botocore import UNSIGNED\nfrom botocore.compat import total_seconds\nfrom botocore.config import Config\nfrom botocore.exceptions import (\n    ClientError,\n    InvalidConfigError,\n    TokenRetrievalError,\n)\nfrom botocore.utils import CachedProperty, JSONFileCache, SSOTokenLoader\n\nlogger = logging.getLogger(__name__)\n\n\ndef _utc_now():\n    return datetime.now(tzutc())\n\n\ndef create_token_resolver(session):\n    providers = [\n        SSOTokenProvider(session),\n    ]\n    return TokenProviderChain(providers=providers)\n\n\ndef _serialize_utc_timestamp(obj):\n    if isinstance(obj, datetime):\n        return obj.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    return obj\n\n\ndef _sso_json_dumps(obj):\n    return json.dumps(obj, default=_serialize_utc_timestamp)\n\n\nclass FrozenAuthToken(NamedTuple):\n    token: str\n    expiration: Optional[datetime] = None\n\n\nclass DeferredRefreshableToken:\n    # The time at which we'll attempt to refresh, but not block if someone else\n    # is refreshing.\n    _advisory_refresh_timeout = 15 * 60\n    # The time at which all threads will block waiting for a refreshed token\n    _mandatory_refresh_timeout = 10 * 60\n    # Refresh at most once every minute to avoid blocking every request\n    _attempt_timeout = 60\n\n    def __init__(self, method, refresh_using, time_fetcher=_utc_now):\n        self._time_fetcher = time_fetcher\n        self._refresh_using = refresh_using\n        self.method = method\n\n        # The frozen token is protected by this lock\n        self._refresh_lock = threading.Lock()\n        self._frozen_token = None\n        self._next_refresh = None\n\n    def get_frozen_token(self):\n        self._refresh()\n        return self._frozen_token\n\n    def _refresh(self):\n        # If we don't need to refresh just return\n        refresh_type = self._should_refresh()\n        if not refresh_type:\n            return None\n\n        # Block for refresh if we're in the mandatory refresh window\n        block_for_refresh = refresh_type == \"mandatory\"\n        if self._refresh_lock.acquire(block_for_refresh):\n            try:\n                self._protected_refresh()\n            finally:\n                self._refresh_lock.release()\n\n    def _protected_refresh(self):\n        # This should only be called after acquiring the refresh lock\n        # Another thread may have already refreshed, double check refresh\n        refresh_type = self._should_refresh()\n        if not refresh_type:\n            return None\n\n        try:\n            now = self._time_fetcher()\n            self._next_refresh = now + timedelta(seconds=self._attempt_timeout)\n            self._frozen_token = self._refresh_using()\n        except Exception:\n            logger.warning(\n                \"Refreshing token failed during the %s refresh period.\",\n                refresh_type,\n                exc_info=True,\n            )\n            if refresh_type == \"mandatory\":\n                # This refresh was mandatory, error must be propagated back\n                raise\n\n        if self._is_expired():\n            # Fresh credentials should never be expired\n            raise TokenRetrievalError(\n                provider=self.method,\n                error_msg=\"Token has expired and refresh failed\",\n            )\n\n    def _is_expired(self):\n        if self._frozen_token is None:\n            return False\n\n        expiration = self._frozen_token.expiration\n        remaining = total_seconds(expiration - self._time_fetcher())\n        return remaining <= 0\n\n    def _should_refresh(self):\n        if self._frozen_token is None:\n            # We don't have a token yet, mandatory refresh\n            return \"mandatory\"\n\n        expiration = self._frozen_token.expiration\n        if expiration is None:\n            # No expiration, so assume we don't need to refresh.\n            return None\n\n        now = self._time_fetcher()\n        if now < self._next_refresh:\n            return None\n\n        remaining = total_seconds(expiration - now)\n\n        if remaining < self._mandatory_refresh_timeout:\n            return \"mandatory\"\n        elif remaining < self._advisory_refresh_timeout:\n            return \"advisory\"\n\n        return None\n\n\nclass TokenProviderChain:\n    def __init__(self, providers=None):\n        if providers is None:\n            providers = []\n        self._providers = providers\n\n    def load_token(self):\n        for provider in self._providers:\n            token = provider.load_token()\n            if token is not None:\n                return token\n        return None\n\n\nclass SSOTokenProvider:\n    METHOD = \"sso\"\n    _REFRESH_WINDOW = 15 * 60\n    _SSO_TOKEN_CACHE_DIR = os.path.expanduser(\n        os.path.join(\"~\", \".aws\", \"sso\", \"cache\")\n    )\n    _SSO_CONFIG_VARS = [\n        \"sso_start_url\",\n        \"sso_region\",\n    ]\n    _GRANT_TYPE = \"refresh_token\"\n    DEFAULT_CACHE_CLS = JSONFileCache\n\n    def __init__(\n        self, session, cache=None, time_fetcher=_utc_now, profile_name=None\n    ):\n        self._session = session\n        if cache is None:\n            cache = self.DEFAULT_CACHE_CLS(\n                self._SSO_TOKEN_CACHE_DIR,\n                dumps_func=_sso_json_dumps,\n            )\n        self._now = time_fetcher\n        self._cache = cache\n        self._token_loader = SSOTokenLoader(cache=self._cache)\n        self._profile_name = (\n            profile_name\n            or self._session.get_config_variable(\"profile\")\n            or 'default'\n        )\n\n    def _load_sso_config(self):\n        loaded_config = self._session.full_config\n        profiles = loaded_config.get(\"profiles\", {})\n        sso_sessions = loaded_config.get(\"sso_sessions\", {})\n        profile_config = profiles.get(self._profile_name, {})\n\n        if \"sso_session\" not in profile_config:\n            return\n\n        sso_session_name = profile_config[\"sso_session\"]\n        sso_config = sso_sessions.get(sso_session_name, None)\n\n        if not sso_config:\n            error_msg = (\n                f'The profile \"{self._profile_name}\" is configured to use the SSO '\n                f'token provider but the \"{sso_session_name}\" sso_session '\n                f\"configuration does not exist.\"\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        missing_configs = []\n        for var in self._SSO_CONFIG_VARS:\n            if var not in sso_config:\n                missing_configs.append(var)\n\n        if missing_configs:\n            error_msg = (\n                f'The profile \"{self._profile_name}\" is configured to use the SSO '\n                f\"token provider but is missing the following configuration: \"\n                f\"{missing_configs}.\"\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        return {\n            \"session_name\": sso_session_name,\n            \"sso_region\": sso_config[\"sso_region\"],\n            \"sso_start_url\": sso_config[\"sso_start_url\"],\n        }\n\n    @CachedProperty\n    def _sso_config(self):\n        return self._load_sso_config()\n\n    @CachedProperty\n    def _client(self):\n        config = Config(\n            region_name=self._sso_config[\"sso_region\"],\n            signature_version=UNSIGNED,\n        )\n        return self._session.create_client(\"sso-oidc\", config=config)\n\n    def _attempt_create_token(self, token):\n        response = self._client.create_token(\n            grantType=self._GRANT_TYPE,\n            clientId=token[\"clientId\"],\n            clientSecret=token[\"clientSecret\"],\n            refreshToken=token[\"refreshToken\"],\n        )\n        expires_in = timedelta(seconds=response[\"expiresIn\"])\n        new_token = {\n            \"startUrl\": self._sso_config[\"sso_start_url\"],\n            \"region\": self._sso_config[\"sso_region\"],\n            \"accessToken\": response[\"accessToken\"],\n            \"expiresAt\": self._now() + expires_in,\n            # Cache the registration alongside the token\n            \"clientId\": token[\"clientId\"],\n            \"clientSecret\": token[\"clientSecret\"],\n            \"registrationExpiresAt\": token[\"registrationExpiresAt\"],\n        }\n        if \"refreshToken\" in response:\n            new_token[\"refreshToken\"] = response[\"refreshToken\"]\n        logger.info(\"SSO Token refresh succeeded\")\n        return new_token\n\n    def _refresh_access_token(self, token):\n        keys = (\n            \"refreshToken\",\n            \"clientId\",\n            \"clientSecret\",\n            \"registrationExpiresAt\",\n        )\n        missing_keys = [k for k in keys if k not in token]\n        if missing_keys:\n            msg = f\"Unable to refresh SSO token: missing keys: {missing_keys}\"\n            logger.info(msg)\n            return None\n\n        expiry = dateutil.parser.parse(token[\"registrationExpiresAt\"])\n        if total_seconds(expiry - self._now()) <= 0:\n            logger.info(f\"SSO token registration expired at {expiry}\")\n            return None\n\n        try:\n            return self._attempt_create_token(token)\n        except ClientError:\n            logger.warning(\"SSO token refresh attempt failed\", exc_info=True)\n            return None\n\n    def _refresher(self):\n        start_url = self._sso_config[\"sso_start_url\"]\n        session_name = self._sso_config[\"session_name\"]\n        logger.info(f\"Loading cached SSO token for {session_name}\")\n        token_dict = self._token_loader(start_url, session_name=session_name)\n        expiration = dateutil.parser.parse(token_dict[\"expiresAt\"])\n        logger.debug(f\"Cached SSO token expires at {expiration}\")\n\n        remaining = total_seconds(expiration - self._now())\n        if remaining < self._REFRESH_WINDOW:\n            new_token_dict = self._refresh_access_token(token_dict)\n            if new_token_dict is not None:\n                token_dict = new_token_dict\n                expiration = token_dict[\"expiresAt\"]\n                self._token_loader.save_token(\n                    start_url, token_dict, session_name=session_name\n                )\n\n        return FrozenAuthToken(\n            token_dict[\"accessToken\"], expiration=expiration\n        )\n\n    def load_token(self):\n        if self._sso_config is None:\n            return None\n\n        return DeferredRefreshableToken(\n            self.METHOD, self._refresher, time_fetcher=self._now\n        )\n", "botocore/parsers.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Response parsers for the various protocol types.\n\nThe module contains classes that can take an HTTP response, and given\nan output shape, parse the response into a dict according to the\nrules in the output shape.\n\nThere are many similarities amongst the different protocols with regard\nto response parsing, and the code is structured in a way to avoid\ncode duplication when possible.  The diagram below is a diagram\nshowing the inheritance hierarchy of the response classes.\n\n::\n\n\n\n                                 +--------------+\n                                 |ResponseParser|\n                                 +--------------+\n                                    ^    ^    ^\n               +--------------------+    |    +-------------------+\n               |                         |                        |\n    +----------+----------+       +------+-------+        +-------+------+\n    |BaseXMLResponseParser|       |BaseRestParser|        |BaseJSONParser|\n    +---------------------+       +--------------+        +--------------+\n              ^         ^          ^           ^           ^        ^\n              |         |          |           |           |        |\n              |         |          |           |           |        |\n              |        ++----------+-+       +-+-----------++       |\n              |        |RestXMLParser|       |RestJSONParser|       |\n        +-----+-----+  +-------------+       +--------------+  +----+-----+\n        |QueryParser|                                          |JSONParser|\n        +-----------+                                          +----------+\n\n\nThe diagram above shows that there is a base class, ``ResponseParser`` that\ncontains logic that is similar amongst all the different protocols (``query``,\n``json``, ``rest-json``, ``rest-xml``).  Amongst the various services there\nis shared logic that can be grouped several ways:\n\n* The ``query`` and ``rest-xml`` both have XML bodies that are parsed in the\n  same way.\n* The ``json`` and ``rest-json`` protocols both have JSON bodies that are\n  parsed in the same way.\n* The ``rest-json`` and ``rest-xml`` protocols have additional attributes\n  besides body parameters that are parsed the same (headers, query string,\n  status code).\n\nThis is reflected in the class diagram above.  The ``BaseXMLResponseParser``\nand the BaseJSONParser contain logic for parsing the XML/JSON body,\nand the BaseRestParser contains logic for parsing out attributes that\ncome from other parts of the HTTP response.  Classes like the\n``RestXMLParser`` inherit from the ``BaseXMLResponseParser`` to get the\nXML body parsing logic and the ``BaseRestParser`` to get the HTTP\nheader/status code/query string parsing.\n\nAdditionally, there are event stream parsers that are used by the other parsers\nto wrap streaming bodies that represent a stream of events. The\nBaseEventStreamParser extends from ResponseParser and defines the logic for\nparsing values from the headers and payload of a message from the underlying\nbinary encoding protocol. Currently, event streams support parsing bodies\nencoded as JSON and XML through the following hierarchy.\n\n\n                                  +--------------+\n                                  |ResponseParser|\n                                  +--------------+\n                                    ^    ^    ^\n               +--------------------+    |    +------------------+\n               |                         |                       |\n    +----------+----------+   +----------+----------+    +-------+------+\n    |BaseXMLResponseParser|   |BaseEventStreamParser|    |BaseJSONParser|\n    +---------------------+   +---------------------+    +--------------+\n                     ^                ^        ^                 ^\n                     |                |        |                 |\n                     |                |        |                 |\n                   +-+----------------+-+    +-+-----------------+-+\n                   |EventStreamXMLParser|    |EventStreamJSONParser|\n                   +--------------------+    +---------------------+\n\nReturn Values\n=============\n\nEach call to ``parse()`` returns a dict has this form::\n\n    Standard Response\n\n    {\n      \"ResponseMetadata\": {\"RequestId\": <requestid>}\n      <response keys>\n    }\n\n    Error response\n\n    {\n      \"ResponseMetadata\": {\"RequestId\": <requestid>}\n      \"Error\": {\n        \"Code\": <string>,\n        \"Message\": <string>,\n        \"Type\": <string>,\n        <additional keys>\n      }\n    }\n\n\"\"\"\nimport base64\nimport http.client\nimport json\nimport logging\nimport re\n\nfrom botocore.compat import ETree, XMLParseError\nfrom botocore.eventstream import EventStream, NoInitialResponseError\nfrom botocore.utils import (\n    is_json_value_header,\n    lowercase_dict,\n    merge_dicts,\n    parse_timestamp,\n)\n\nLOG = logging.getLogger(__name__)\n\nDEFAULT_TIMESTAMP_PARSER = parse_timestamp\n\n\nclass ResponseParserFactory:\n    def __init__(self):\n        self._defaults = {}\n\n    def set_parser_defaults(self, **kwargs):\n        \"\"\"Set default arguments when a parser instance is created.\n\n        You can specify any kwargs that are allowed by a ResponseParser\n        class.  There are currently two arguments:\n\n            * timestamp_parser - A callable that can parse a timestamp string\n            * blob_parser - A callable that can parse a blob type\n\n        \"\"\"\n        self._defaults.update(kwargs)\n\n    def create_parser(self, protocol_name):\n        parser_cls = PROTOCOL_PARSERS[protocol_name]\n        return parser_cls(**self._defaults)\n\n\ndef create_parser(protocol):\n    return ResponseParserFactory().create_parser(protocol)\n\n\ndef _text_content(func):\n    # This decorator hides the difference between\n    # an XML node with text or a plain string.  It's used\n    # to ensure that scalar processing operates only on text\n    # strings, which allows the same scalar handlers to be used\n    # for XML nodes from the body and HTTP headers.\n    def _get_text_content(self, shape, node_or_string):\n        if hasattr(node_or_string, 'text'):\n            text = node_or_string.text\n            if text is None:\n                # If an XML node is empty <foo></foo>,\n                # we want to parse that as an empty string,\n                # not as a null/None value.\n                text = ''\n        else:\n            text = node_or_string\n        return func(self, shape, text)\n\n    return _get_text_content\n\n\nclass ResponseParserError(Exception):\n    pass\n\n\nclass ResponseParser:\n    \"\"\"Base class for response parsing.\n\n    This class represents the interface that all ResponseParsers for the\n    various protocols must implement.\n\n    This class will take an HTTP response and a model shape and parse the\n    HTTP response into a dictionary.\n\n    There is a single public method exposed: ``parse``.  See the ``parse``\n    docstring for more info.\n\n    \"\"\"\n\n    DEFAULT_ENCODING = 'utf-8'\n    EVENT_STREAM_PARSER_CLS = None\n\n    def __init__(self, timestamp_parser=None, blob_parser=None):\n        if timestamp_parser is None:\n            timestamp_parser = DEFAULT_TIMESTAMP_PARSER\n        self._timestamp_parser = timestamp_parser\n        if blob_parser is None:\n            blob_parser = self._default_blob_parser\n        self._blob_parser = blob_parser\n        self._event_stream_parser = None\n        if self.EVENT_STREAM_PARSER_CLS is not None:\n            self._event_stream_parser = self.EVENT_STREAM_PARSER_CLS(\n                timestamp_parser, blob_parser\n            )\n\n    def _default_blob_parser(self, value):\n        # Blobs are always returned as bytes type (this matters on python3).\n        # We don't decode this to a str because it's entirely possible that the\n        # blob contains binary data that actually can't be decoded.\n        return base64.b64decode(value)\n\n    def parse(self, response, shape):\n        \"\"\"Parse the HTTP response given a shape.\n\n        :param response: The HTTP response dictionary.  This is a dictionary\n            that represents the HTTP request.  The dictionary must have the\n            following keys, ``body``, ``headers``, and ``status_code``.\n\n        :param shape: The model shape describing the expected output.\n        :return: Returns a dictionary representing the parsed response\n            described by the model.  In addition to the shape described from\n            the model, each response will also have a ``ResponseMetadata``\n            which contains metadata about the response, which contains at least\n            two keys containing ``RequestId`` and ``HTTPStatusCode``.  Some\n            responses may populate additional keys, but ``RequestId`` will\n            always be present.\n\n        \"\"\"\n        LOG.debug('Response headers: %r', response['headers'])\n        LOG.debug('Response body:\\n%r', response['body'])\n        if response['status_code'] >= 301:\n            if self._is_generic_error_response(response):\n                parsed = self._do_generic_error_parse(response)\n            elif self._is_modeled_error_shape(shape):\n                parsed = self._do_modeled_error_parse(response, shape)\n                # We don't want to decorate the modeled fields with metadata\n                return parsed\n            else:\n                parsed = self._do_error_parse(response, shape)\n        else:\n            parsed = self._do_parse(response, shape)\n\n        # We don't want to decorate event stream responses with metadata\n        if shape and shape.serialization.get('eventstream'):\n            return parsed\n\n        # Add ResponseMetadata if it doesn't exist and inject the HTTP\n        # status code and headers from the response.\n        if isinstance(parsed, dict):\n            response_metadata = parsed.get('ResponseMetadata', {})\n            response_metadata['HTTPStatusCode'] = response['status_code']\n            # Ensure that the http header keys are all lower cased. Older\n            # versions of urllib3 (< 1.11) would unintentionally do this for us\n            # (see urllib3#633). We need to do this conversion manually now.\n            headers = response['headers']\n            response_metadata['HTTPHeaders'] = lowercase_dict(headers)\n            parsed['ResponseMetadata'] = response_metadata\n            self._add_checksum_response_metadata(response, response_metadata)\n        return parsed\n\n    def _add_checksum_response_metadata(self, response, response_metadata):\n        checksum_context = response.get('context', {}).get('checksum', {})\n        algorithm = checksum_context.get('response_algorithm')\n        if algorithm:\n            response_metadata['ChecksumAlgorithm'] = algorithm\n\n    def _is_modeled_error_shape(self, shape):\n        return shape is not None and shape.metadata.get('exception', False)\n\n    def _is_generic_error_response(self, response):\n        # There are times when a service will respond with a generic\n        # error response such as:\n        # '<html><body><b>Http/1.1 Service Unavailable</b></body></html>'\n        #\n        # This can also happen if you're going through a proxy.\n        # In this case the protocol specific _do_error_parse will either\n        # fail to parse the response (in the best case) or silently succeed\n        # and treat the HTML above as an XML response and return\n        # non sensical parsed data.\n        # To prevent this case from happening we first need to check\n        # whether or not this response looks like the generic response.\n        if response['status_code'] >= 500:\n            if 'body' not in response or response['body'] is None:\n                return True\n\n            body = response['body'].strip()\n            return body.startswith(b'<html>') or not body\n\n    def _do_generic_error_parse(self, response):\n        # There's not really much we can do when we get a generic\n        # html response.\n        LOG.debug(\n            \"Received a non protocol specific error response from the \"\n            \"service, unable to populate error code and message.\"\n        )\n        return {\n            'Error': {\n                'Code': str(response['status_code']),\n                'Message': http.client.responses.get(\n                    response['status_code'], ''\n                ),\n            },\n            'ResponseMetadata': {},\n        }\n\n    def _do_parse(self, response, shape):\n        raise NotImplementedError(\"%s._do_parse\" % self.__class__.__name__)\n\n    def _do_error_parse(self, response, shape):\n        raise NotImplementedError(f\"{self.__class__.__name__}._do_error_parse\")\n\n    def _do_modeled_error_parse(self, response, shape, parsed):\n        raise NotImplementedError(\n            f\"{self.__class__.__name__}._do_modeled_error_parse\"\n        )\n\n    def _parse_shape(self, shape, node):\n        handler = getattr(\n            self, f'_handle_{shape.type_name}', self._default_handle\n        )\n        return handler(shape, node)\n\n    def _handle_list(self, shape, node):\n        # Enough implementations share list serialization that it's moved\n        # up here in the base class.\n        parsed = []\n        member_shape = shape.member\n        for item in node:\n            parsed.append(self._parse_shape(member_shape, item))\n        return parsed\n\n    def _default_handle(self, shape, value):\n        return value\n\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return EventStream(response['body'], shape, parser, name)\n\n    def _get_first_key(self, value):\n        return list(value)[0]\n\n    def _has_unknown_tagged_union_member(self, shape, value):\n        if shape.is_tagged_union:\n            cleaned_value = value.copy()\n            cleaned_value.pop(\"__type\", None)\n            if len(cleaned_value) != 1:\n                error_msg = (\n                    \"Invalid service response: %s must have one and only \"\n                    \"one member set.\"\n                )\n                raise ResponseParserError(error_msg % shape.name)\n            tag = self._get_first_key(cleaned_value)\n            if tag not in shape.members:\n                msg = (\n                    \"Received a tagged union response with member \"\n                    \"unknown to client: %s. Please upgrade SDK for full \"\n                    \"response support.\"\n                )\n                LOG.info(msg % tag)\n                return True\n        return False\n\n    def _handle_unknown_tagged_union_member(self, tag):\n        return {'SDK_UNKNOWN_MEMBER': {'name': tag}}\n\n\nclass BaseXMLResponseParser(ResponseParser):\n    def __init__(self, timestamp_parser=None, blob_parser=None):\n        super().__init__(timestamp_parser, blob_parser)\n        self._namespace_re = re.compile('{.*}')\n\n    def _handle_map(self, shape, node):\n        parsed = {}\n        key_shape = shape.key\n        value_shape = shape.value\n        key_location_name = key_shape.serialization.get('name') or 'key'\n        value_location_name = value_shape.serialization.get('name') or 'value'\n        if shape.serialization.get('flattened') and not isinstance(node, list):\n            node = [node]\n        for keyval_node in node:\n            for single_pair in keyval_node:\n                # Within each <entry> there's a <key> and a <value>\n                tag_name = self._node_tag(single_pair)\n                if tag_name == key_location_name:\n                    key_name = self._parse_shape(key_shape, single_pair)\n                elif tag_name == value_location_name:\n                    val_name = self._parse_shape(value_shape, single_pair)\n                else:\n                    raise ResponseParserError(\"Unknown tag: %s\" % tag_name)\n            parsed[key_name] = val_name\n        return parsed\n\n    def _node_tag(self, node):\n        return self._namespace_re.sub('', node.tag)\n\n    def _handle_list(self, shape, node):\n        # When we use _build_name_to_xml_node, repeated elements are aggregated\n        # into a list.  However, we can't tell the difference between a scalar\n        # value and a single element flattened list.  So before calling the\n        # real _handle_list, we know that \"node\" should actually be a list if\n        # it's flattened, and if it's not, then we make it a one element list.\n        if shape.serialization.get('flattened') and not isinstance(node, list):\n            node = [node]\n        return super()._handle_list(shape, node)\n\n    def _handle_structure(self, shape, node):\n        parsed = {}\n        members = shape.members\n        if shape.metadata.get('exception', False):\n            node = self._get_error_root(node)\n        xml_dict = self._build_name_to_xml_node(node)\n        if self._has_unknown_tagged_union_member(shape, xml_dict):\n            tag = self._get_first_key(xml_dict)\n            return self._handle_unknown_tagged_union_member(tag)\n        for member_name in members:\n            member_shape = members[member_name]\n            if (\n                'location' in member_shape.serialization\n                or member_shape.serialization.get('eventheader')\n            ):\n                # All members with locations have already been handled,\n                # so we don't need to parse these members.\n                continue\n            xml_name = self._member_key_name(member_shape, member_name)\n            member_node = xml_dict.get(xml_name)\n            if member_node is not None:\n                parsed[member_name] = self._parse_shape(\n                    member_shape, member_node\n                )\n            elif member_shape.serialization.get('xmlAttribute'):\n                attribs = {}\n                location_name = member_shape.serialization['name']\n                for key, value in node.attrib.items():\n                    new_key = self._namespace_re.sub(\n                        location_name.split(':')[0] + ':', key\n                    )\n                    attribs[new_key] = value\n                if location_name in attribs:\n                    parsed[member_name] = attribs[location_name]\n        return parsed\n\n    def _get_error_root(self, original_root):\n        if self._node_tag(original_root) == 'ErrorResponse':\n            for child in original_root:\n                if self._node_tag(child) == 'Error':\n                    return child\n        return original_root\n\n    def _member_key_name(self, shape, member_name):\n        # This method is needed because we have to special case flattened list\n        # with a serialization name.  If this is the case we use the\n        # locationName from the list's member shape as the key name for the\n        # surrounding structure.\n        if shape.type_name == 'list' and shape.serialization.get('flattened'):\n            list_member_serialized_name = shape.member.serialization.get(\n                'name'\n            )\n            if list_member_serialized_name is not None:\n                return list_member_serialized_name\n        serialized_name = shape.serialization.get('name')\n        if serialized_name is not None:\n            return serialized_name\n        return member_name\n\n    def _build_name_to_xml_node(self, parent_node):\n        # If the parent node is actually a list. We should not be trying\n        # to serialize it to a dictionary. Instead, return the first element\n        # in the list.\n        if isinstance(parent_node, list):\n            return self._build_name_to_xml_node(parent_node[0])\n        xml_dict = {}\n        for item in parent_node:\n            key = self._node_tag(item)\n            if key in xml_dict:\n                # If the key already exists, the most natural\n                # way to handle this is to aggregate repeated\n                # keys into a single list.\n                # <foo>1</foo><foo>2</foo> -> {'foo': [Node(1), Node(2)]}\n                if isinstance(xml_dict[key], list):\n                    xml_dict[key].append(item)\n                else:\n                    # Convert from a scalar to a list.\n                    xml_dict[key] = [xml_dict[key], item]\n            else:\n                xml_dict[key] = item\n        return xml_dict\n\n    def _parse_xml_string_to_dom(self, xml_string):\n        try:\n            parser = ETree.XMLParser(\n                target=ETree.TreeBuilder(), encoding=self.DEFAULT_ENCODING\n            )\n            parser.feed(xml_string)\n            root = parser.close()\n        except XMLParseError as e:\n            raise ResponseParserError(\n                \"Unable to parse response (%s), \"\n                \"invalid XML received. Further retries may succeed:\\n%s\"\n                % (e, xml_string)\n            )\n        return root\n\n    def _replace_nodes(self, parsed):\n        for key, value in parsed.items():\n            if list(value):\n                sub_dict = self._build_name_to_xml_node(value)\n                parsed[key] = self._replace_nodes(sub_dict)\n            else:\n                parsed[key] = value.text\n        return parsed\n\n    @_text_content\n    def _handle_boolean(self, shape, text):\n        if text == 'true':\n            return True\n        else:\n            return False\n\n    @_text_content\n    def _handle_float(self, shape, text):\n        return float(text)\n\n    @_text_content\n    def _handle_timestamp(self, shape, text):\n        return self._timestamp_parser(text)\n\n    @_text_content\n    def _handle_integer(self, shape, text):\n        return int(text)\n\n    @_text_content\n    def _handle_string(self, shape, text):\n        return text\n\n    @_text_content\n    def _handle_blob(self, shape, text):\n        return self._blob_parser(text)\n\n    _handle_character = _handle_string\n    _handle_double = _handle_float\n    _handle_long = _handle_integer\n\n\nclass QueryParser(BaseXMLResponseParser):\n    def _do_error_parse(self, response, shape):\n        xml_contents = response['body']\n        root = self._parse_xml_string_to_dom(xml_contents)\n        parsed = self._build_name_to_xml_node(root)\n        self._replace_nodes(parsed)\n        # Once we've converted xml->dict, we need to make one or two\n        # more adjustments to extract nested errors and to be consistent\n        # with ResponseMetadata for non-error responses:\n        # 1. {\"Errors\": {\"Error\": {...}}} -> {\"Error\": {...}}\n        # 2. {\"RequestId\": \"id\"} -> {\"ResponseMetadata\": {\"RequestId\": \"id\"}}\n        if 'Errors' in parsed:\n            parsed.update(parsed.pop('Errors'))\n        if 'RequestId' in parsed:\n            parsed['ResponseMetadata'] = {'RequestId': parsed.pop('RequestId')}\n        return parsed\n\n    def _do_modeled_error_parse(self, response, shape):\n        return self._parse_body_as_xml(response, shape, inject_metadata=False)\n\n    def _do_parse(self, response, shape):\n        return self._parse_body_as_xml(response, shape, inject_metadata=True)\n\n    def _parse_body_as_xml(self, response, shape, inject_metadata=True):\n        xml_contents = response['body']\n        root = self._parse_xml_string_to_dom(xml_contents)\n        parsed = {}\n        if shape is not None:\n            start = root\n            if 'resultWrapper' in shape.serialization:\n                start = self._find_result_wrapped_shape(\n                    shape.serialization['resultWrapper'], root\n                )\n            parsed = self._parse_shape(shape, start)\n        if inject_metadata:\n            self._inject_response_metadata(root, parsed)\n        return parsed\n\n    def _find_result_wrapped_shape(self, element_name, xml_root_node):\n        mapping = self._build_name_to_xml_node(xml_root_node)\n        return mapping[element_name]\n\n    def _inject_response_metadata(self, node, inject_into):\n        mapping = self._build_name_to_xml_node(node)\n        child_node = mapping.get('ResponseMetadata')\n        if child_node is not None:\n            sub_mapping = self._build_name_to_xml_node(child_node)\n            for key, value in sub_mapping.items():\n                sub_mapping[key] = value.text\n            inject_into['ResponseMetadata'] = sub_mapping\n\n\nclass EC2QueryParser(QueryParser):\n    def _inject_response_metadata(self, node, inject_into):\n        mapping = self._build_name_to_xml_node(node)\n        child_node = mapping.get('requestId')\n        if child_node is not None:\n            inject_into['ResponseMetadata'] = {'RequestId': child_node.text}\n\n    def _do_error_parse(self, response, shape):\n        # EC2 errors look like:\n        # <Response>\n        #   <Errors>\n        #     <Error>\n        #       <Code>InvalidInstanceID.Malformed</Code>\n        #       <Message>Invalid id: \"1343124\"</Message>\n        #     </Error>\n        #   </Errors>\n        #   <RequestID>12345</RequestID>\n        # </Response>\n        # This is different from QueryParser in that it's RequestID,\n        # not RequestId\n        original = super()._do_error_parse(response, shape)\n        if 'RequestID' in original:\n            original['ResponseMetadata'] = {\n                'RequestId': original.pop('RequestID')\n            }\n        return original\n\n    def _get_error_root(self, original_root):\n        for child in original_root:\n            if self._node_tag(child) == 'Errors':\n                for errors_child in child:\n                    if self._node_tag(errors_child) == 'Error':\n                        return errors_child\n        return original_root\n\n\nclass BaseJSONParser(ResponseParser):\n    def _handle_structure(self, shape, value):\n        final_parsed = {}\n        if shape.is_document_type:\n            final_parsed = value\n        else:\n            member_shapes = shape.members\n            if value is None:\n                # If the comes across the wire as \"null\" (None in python),\n                # we should be returning this unchanged, instead of as an\n                # empty dict.\n                return None\n            final_parsed = {}\n            if self._has_unknown_tagged_union_member(shape, value):\n                tag = self._get_first_key(value)\n                return self._handle_unknown_tagged_union_member(tag)\n            for member_name in member_shapes:\n                member_shape = member_shapes[member_name]\n                json_name = member_shape.serialization.get('name', member_name)\n                raw_value = value.get(json_name)\n                if raw_value is not None:\n                    final_parsed[member_name] = self._parse_shape(\n                        member_shapes[member_name], raw_value\n                    )\n        return final_parsed\n\n    def _handle_map(self, shape, value):\n        parsed = {}\n        key_shape = shape.key\n        value_shape = shape.value\n        for key, value in value.items():\n            actual_key = self._parse_shape(key_shape, key)\n            actual_value = self._parse_shape(value_shape, value)\n            parsed[actual_key] = actual_value\n        return parsed\n\n    def _handle_blob(self, shape, value):\n        return self._blob_parser(value)\n\n    def _handle_timestamp(self, shape, value):\n        return self._timestamp_parser(value)\n\n    def _do_error_parse(self, response, shape):\n        body = self._parse_body_as_json(response['body'])\n        error = {\"Error\": {\"Message\": '', \"Code\": ''}, \"ResponseMetadata\": {}}\n        headers = response['headers']\n        # Error responses can have slightly different structures for json.\n        # The basic structure is:\n        #\n        # {\"__type\":\"ConnectClientException\",\n        #  \"message\":\"The error message.\"}\n\n        # The error message can either come in the 'message' or 'Message' key\n        # so we need to check for both.\n        error['Error']['Message'] = body.get(\n            'message', body.get('Message', '')\n        )\n        # if the message did not contain an error code\n        # include the response status code\n        response_code = response.get('status_code')\n\n        code = body.get('__type', response_code and str(response_code))\n        if code is not None:\n            # code has a couple forms as well:\n            # * \"com.aws.dynamodb.vAPI#ProvisionedThroughputExceededException\"\n            # * \"ResourceNotFoundException\"\n            if '#' in code:\n                code = code.rsplit('#', 1)[1]\n            if 'x-amzn-query-error' in headers:\n                code = self._do_query_compatible_error_parse(\n                    code, headers, error\n                )\n            error['Error']['Code'] = code\n        self._inject_response_metadata(error, response['headers'])\n        return error\n\n    def _do_query_compatible_error_parse(self, code, headers, error):\n        \"\"\"\n        Error response may contain an x-amzn-query-error header to translate\n        errors codes from former `query` services into `json`. We use this to\n        do our lookup in the errorfactory for modeled errors.\n        \"\"\"\n        query_error = headers['x-amzn-query-error']\n        query_error_components = query_error.split(';')\n\n        if len(query_error_components) == 2 and query_error_components[0]:\n            error['Error']['QueryErrorCode'] = code\n            error['Error']['Type'] = query_error_components[1]\n            return query_error_components[0]\n        return code\n\n    def _inject_response_metadata(self, parsed, headers):\n        if 'x-amzn-requestid' in headers:\n            parsed.setdefault('ResponseMetadata', {})['RequestId'] = headers[\n                'x-amzn-requestid'\n            ]\n\n    def _parse_body_as_json(self, body_contents):\n        if not body_contents:\n            return {}\n        body = body_contents.decode(self.DEFAULT_ENCODING)\n        try:\n            original_parsed = json.loads(body)\n            return original_parsed\n        except ValueError:\n            # if the body cannot be parsed, include\n            # the literal string as the message\n            return {'message': body}\n\n\nclass BaseEventStreamParser(ResponseParser):\n    def _do_parse(self, response, shape):\n        final_parsed = {}\n        if shape.serialization.get('eventstream'):\n            event_type = response['headers'].get(':event-type')\n            event_shape = shape.members.get(event_type)\n            if event_shape:\n                final_parsed[event_type] = self._do_parse(\n                    response, event_shape\n                )\n        else:\n            self._parse_non_payload_attrs(\n                response, shape, shape.members, final_parsed\n            )\n            self._parse_payload(response, shape, shape.members, final_parsed)\n        return final_parsed\n\n    def _do_error_parse(self, response, shape):\n        exception_type = response['headers'].get(':exception-type')\n        exception_shape = shape.members.get(exception_type)\n        if exception_shape is not None:\n            original_parsed = self._initial_body_parse(response['body'])\n            body = self._parse_shape(exception_shape, original_parsed)\n            error = {\n                'Error': {\n                    'Code': exception_type,\n                    'Message': body.get('Message', body.get('message', '')),\n                }\n            }\n        else:\n            error = {\n                'Error': {\n                    'Code': response['headers'].get(':error-code', ''),\n                    'Message': response['headers'].get(':error-message', ''),\n                }\n            }\n        return error\n\n    def _parse_payload(self, response, shape, member_shapes, final_parsed):\n        if shape.serialization.get('event'):\n            for name in member_shapes:\n                member_shape = member_shapes[name]\n                if member_shape.serialization.get('eventpayload'):\n                    body = response['body']\n                    if member_shape.type_name == 'blob':\n                        parsed_body = body\n                    elif member_shape.type_name == 'string':\n                        parsed_body = body.decode(self.DEFAULT_ENCODING)\n                    else:\n                        raw_parse = self._initial_body_parse(body)\n                        parsed_body = self._parse_shape(\n                            member_shape, raw_parse\n                        )\n                    final_parsed[name] = parsed_body\n                    return\n            # If we didn't find an explicit payload, use the current shape\n            original_parsed = self._initial_body_parse(response['body'])\n            body_parsed = self._parse_shape(shape, original_parsed)\n            final_parsed.update(body_parsed)\n\n    def _parse_non_payload_attrs(\n        self, response, shape, member_shapes, final_parsed\n    ):\n        headers = response['headers']\n        for name in member_shapes:\n            member_shape = member_shapes[name]\n            if member_shape.serialization.get('eventheader'):\n                if name in headers:\n                    value = headers[name]\n                    if member_shape.type_name == 'timestamp':\n                        # Event stream timestamps are an in milleseconds so we\n                        # divide by 1000 to convert to seconds.\n                        value = self._timestamp_parser(value / 1000.0)\n                    final_parsed[name] = value\n\n    def _initial_body_parse(self, body_contents):\n        # This method should do the initial xml/json parsing of the\n        # body.  We we still need to walk the parsed body in order\n        # to convert types, but this method will do the first round\n        # of parsing.\n        raise NotImplementedError(\"_initial_body_parse\")\n\n\nclass EventStreamJSONParser(BaseEventStreamParser, BaseJSONParser):\n    def _initial_body_parse(self, body_contents):\n        return self._parse_body_as_json(body_contents)\n\n\nclass EventStreamXMLParser(BaseEventStreamParser, BaseXMLResponseParser):\n    def _initial_body_parse(self, xml_string):\n        if not xml_string:\n            return ETree.Element('')\n        return self._parse_xml_string_to_dom(xml_string)\n\n\nclass JSONParser(BaseJSONParser):\n    EVENT_STREAM_PARSER_CLS = EventStreamJSONParser\n\n    \"\"\"Response parser for the \"json\" protocol.\"\"\"\n\n    def _do_parse(self, response, shape):\n        parsed = {}\n        if shape is not None:\n            event_name = shape.event_stream_name\n            if event_name:\n                parsed = self._handle_event_stream(response, shape, event_name)\n            else:\n                parsed = self._handle_json_body(response['body'], shape)\n        self._inject_response_metadata(parsed, response['headers'])\n        return parsed\n\n    def _do_modeled_error_parse(self, response, shape):\n        return self._handle_json_body(response['body'], shape)\n\n    def _handle_event_stream(self, response, shape, event_name):\n        event_stream_shape = shape.members[event_name]\n        event_stream = self._create_event_stream(response, event_stream_shape)\n        try:\n            event = event_stream.get_initial_response()\n        except NoInitialResponseError:\n            error_msg = 'First event was not of type initial-response'\n            raise ResponseParserError(error_msg)\n        parsed = self._handle_json_body(event.payload, shape)\n        parsed[event_name] = event_stream\n        return parsed\n\n    def _handle_json_body(self, raw_body, shape):\n        # The json.loads() gives us the primitive JSON types,\n        # but we need to traverse the parsed JSON data to convert\n        # to richer types (blobs, timestamps, etc.\n        parsed_json = self._parse_body_as_json(raw_body)\n        return self._parse_shape(shape, parsed_json)\n\n\nclass BaseRestParser(ResponseParser):\n    def _do_parse(self, response, shape):\n        final_parsed = {}\n        final_parsed['ResponseMetadata'] = self._populate_response_metadata(\n            response\n        )\n        self._add_modeled_parse(response, shape, final_parsed)\n        return final_parsed\n\n    def _add_modeled_parse(self, response, shape, final_parsed):\n        if shape is None:\n            return final_parsed\n        member_shapes = shape.members\n        self._parse_non_payload_attrs(\n            response, shape, member_shapes, final_parsed\n        )\n        self._parse_payload(response, shape, member_shapes, final_parsed)\n\n    def _do_modeled_error_parse(self, response, shape):\n        final_parsed = {}\n        self._add_modeled_parse(response, shape, final_parsed)\n        return final_parsed\n\n    def _populate_response_metadata(self, response):\n        metadata = {}\n        headers = response['headers']\n        if 'x-amzn-requestid' in headers:\n            metadata['RequestId'] = headers['x-amzn-requestid']\n        elif 'x-amz-request-id' in headers:\n            metadata['RequestId'] = headers['x-amz-request-id']\n            # HostId is what it's called whenever this value is returned\n            # in an XML response body, so to be consistent, we'll always\n            # call is HostId.\n            metadata['HostId'] = headers.get('x-amz-id-2', '')\n        return metadata\n\n    def _parse_payload(self, response, shape, member_shapes, final_parsed):\n        if 'payload' in shape.serialization:\n            # If a payload is specified in the output shape, then only that\n            # shape is used for the body payload.\n            payload_member_name = shape.serialization['payload']\n            body_shape = member_shapes[payload_member_name]\n            if body_shape.serialization.get('eventstream'):\n                body = self._create_event_stream(response, body_shape)\n                final_parsed[payload_member_name] = body\n            elif body_shape.type_name in ['string', 'blob']:\n                # This is a stream\n                body = response['body']\n                if isinstance(body, bytes):\n                    body = body.decode(self.DEFAULT_ENCODING)\n                final_parsed[payload_member_name] = body\n            else:\n                original_parsed = self._initial_body_parse(response['body'])\n                final_parsed[payload_member_name] = self._parse_shape(\n                    body_shape, original_parsed\n                )\n        else:\n            original_parsed = self._initial_body_parse(response['body'])\n            body_parsed = self._parse_shape(shape, original_parsed)\n            final_parsed.update(body_parsed)\n\n    def _parse_non_payload_attrs(\n        self, response, shape, member_shapes, final_parsed\n    ):\n        headers = response['headers']\n        for name in member_shapes:\n            member_shape = member_shapes[name]\n            location = member_shape.serialization.get('location')\n            if location is None:\n                continue\n            elif location == 'statusCode':\n                final_parsed[name] = self._parse_shape(\n                    member_shape, response['status_code']\n                )\n            elif location == 'headers':\n                final_parsed[name] = self._parse_header_map(\n                    member_shape, headers\n                )\n            elif location == 'header':\n                header_name = member_shape.serialization.get('name', name)\n                if header_name in headers:\n                    final_parsed[name] = self._parse_shape(\n                        member_shape, headers[header_name]\n                    )\n\n    def _parse_header_map(self, shape, headers):\n        # Note that headers are case insensitive, so we .lower()\n        # all header names and header prefixes.\n        parsed = {}\n        prefix = shape.serialization.get('name', '').lower()\n        for header_name in headers:\n            if header_name.lower().startswith(prefix):\n                # The key name inserted into the parsed hash\n                # strips off the prefix.\n                name = header_name[len(prefix) :]\n                parsed[name] = headers[header_name]\n        return parsed\n\n    def _initial_body_parse(self, body_contents):\n        # This method should do the initial xml/json parsing of the\n        # body.  We we still need to walk the parsed body in order\n        # to convert types, but this method will do the first round\n        # of parsing.\n        raise NotImplementedError(\"_initial_body_parse\")\n\n    def _handle_string(self, shape, value):\n        parsed = value\n        if is_json_value_header(shape):\n            decoded = base64.b64decode(value).decode(self.DEFAULT_ENCODING)\n            parsed = json.loads(decoded)\n        return parsed\n\n    def _handle_list(self, shape, node):\n        location = shape.serialization.get('location')\n        if location == 'header' and not isinstance(node, list):\n            # List in headers may be a comma separated string as per RFC7230\n            node = [e.strip() for e in node.split(',')]\n        return super()._handle_list(shape, node)\n\n\nclass RestJSONParser(BaseRestParser, BaseJSONParser):\n    EVENT_STREAM_PARSER_CLS = EventStreamJSONParser\n\n    def _initial_body_parse(self, body_contents):\n        return self._parse_body_as_json(body_contents)\n\n    def _do_error_parse(self, response, shape):\n        error = super()._do_error_parse(response, shape)\n        self._inject_error_code(error, response)\n        return error\n\n    def _inject_error_code(self, error, response):\n        # The \"Code\" value can come from either a response\n        # header or a value in the JSON body.\n        body = self._initial_body_parse(response['body'])\n        if 'x-amzn-errortype' in response['headers']:\n            code = response['headers']['x-amzn-errortype']\n            # Could be:\n            # x-amzn-errortype: ValidationException:\n            code = code.split(':')[0]\n            error['Error']['Code'] = code\n        elif 'code' in body or 'Code' in body:\n            error['Error']['Code'] = body.get('code', body.get('Code', ''))\n\n    def _handle_integer(self, shape, value):\n        return int(value)\n\n    _handle_long = _handle_integer\n\n\nclass RestXMLParser(BaseRestParser, BaseXMLResponseParser):\n    EVENT_STREAM_PARSER_CLS = EventStreamXMLParser\n\n    def _initial_body_parse(self, xml_string):\n        if not xml_string:\n            return ETree.Element('')\n        return self._parse_xml_string_to_dom(xml_string)\n\n    def _do_error_parse(self, response, shape):\n        # We're trying to be service agnostic here, but S3 does have a slightly\n        # different response structure for its errors compared to other\n        # rest-xml serivces (route53/cloudfront).  We handle this by just\n        # trying to parse both forms.\n        # First:\n        # <ErrorResponse xmlns=\"...\">\n        #   <Error>\n        #     <Type>Sender</Type>\n        #     <Code>InvalidInput</Code>\n        #     <Message>Invalid resource type: foo</Message>\n        #   </Error>\n        #   <RequestId>request-id</RequestId>\n        # </ErrorResponse>\n        if response['body']:\n            # If the body ends up being invalid xml, the xml parser should not\n            # blow up. It should at least try to pull information about the\n            # the error response from other sources like the HTTP status code.\n            try:\n                return self._parse_error_from_body(response)\n            except ResponseParserError:\n                LOG.debug(\n                    'Exception caught when parsing error response body:',\n                    exc_info=True,\n                )\n        return self._parse_error_from_http_status(response)\n\n    def _parse_error_from_http_status(self, response):\n        return {\n            'Error': {\n                'Code': str(response['status_code']),\n                'Message': http.client.responses.get(\n                    response['status_code'], ''\n                ),\n            },\n            'ResponseMetadata': {\n                'RequestId': response['headers'].get('x-amz-request-id', ''),\n                'HostId': response['headers'].get('x-amz-id-2', ''),\n            },\n        }\n\n    def _parse_error_from_body(self, response):\n        xml_contents = response['body']\n        root = self._parse_xml_string_to_dom(xml_contents)\n        parsed = self._build_name_to_xml_node(root)\n        self._replace_nodes(parsed)\n        if root.tag == 'Error':\n            # This is an S3 error response.  First we'll populate the\n            # response metadata.\n            metadata = self._populate_response_metadata(response)\n            # The RequestId and the HostId are already in the\n            # ResponseMetadata, but are also duplicated in the XML\n            # body.  We don't need these values in both places,\n            # we'll just remove them from the parsed XML body.\n            parsed.pop('RequestId', '')\n            parsed.pop('HostId', '')\n            return {'Error': parsed, 'ResponseMetadata': metadata}\n        elif 'RequestId' in parsed:\n            # Other rest-xml services:\n            parsed['ResponseMetadata'] = {'RequestId': parsed.pop('RequestId')}\n        default = {'Error': {'Message': '', 'Code': ''}}\n        merge_dicts(default, parsed)\n        return default\n\n    @_text_content\n    def _handle_string(self, shape, text):\n        text = super()._handle_string(shape, text)\n        return text\n\n\nPROTOCOL_PARSERS = {\n    'ec2': EC2QueryParser,\n    'query': QueryParser,\n    'json': JSONParser,\n    'rest-json': RestJSONParser,\n    'rest-xml': RestXMLParser,\n}\n", "botocore/configloader.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport configparser\nimport copy\nimport os\nimport shlex\nimport sys\n\nimport botocore.exceptions\n\n\ndef multi_file_load_config(*filenames):\n    \"\"\"Load and combine multiple INI configs with profiles.\n\n    This function will take a list of filesnames and return\n    a single dictionary that represents the merging of the loaded\n    config files.\n\n    If any of the provided filenames does not exist, then that file\n    is ignored.  It is therefore ok to provide a list of filenames,\n    some of which may not exist.\n\n    Configuration files are **not** deep merged, only the top level\n    keys are merged.  The filenames should be passed in order of\n    precedence.  The first config file has precedence over the\n    second config file, which has precedence over the third config file,\n    etc.  The only exception to this is that the \"profiles\" key is\n    merged to combine profiles from multiple config files into a\n    single profiles mapping.  However, if a profile is defined in\n    multiple config files, then the config file with the highest\n    precedence is used.  Profile values themselves are not merged.\n    For example::\n\n        FileA              FileB                FileC\n        [foo]             [foo]                 [bar]\n        a=1               a=2                   a=3\n                          b=2\n\n        [bar]             [baz]                [profile a]\n        a=2               a=3                  region=e\n\n        [profile a]       [profile b]          [profile c]\n        region=c          region=d             region=f\n\n    The final result of ``multi_file_load_config(FileA, FileB, FileC)``\n    would be::\n\n        {\"foo\": {\"a\": 1}, \"bar\": {\"a\": 2}, \"baz\": {\"a\": 3},\n        \"profiles\": {\"a\": {\"region\": \"c\"}}, {\"b\": {\"region\": d\"}},\n                    {\"c\": {\"region\": \"f\"}}}\n\n    Note that the \"foo\" key comes from A, even though it's defined in both\n    FileA and FileB.  Because \"foo\" was defined in FileA first, then the values\n    for \"foo\" from FileA are used and the values for \"foo\" from FileB are\n    ignored.  Also note where the profiles originate from.  Profile \"a\"\n    comes FileA, profile \"b\" comes from FileB, and profile \"c\" comes\n    from FileC.\n\n    \"\"\"\n    configs = []\n    profiles = []\n    for filename in filenames:\n        try:\n            loaded = load_config(filename)\n        except botocore.exceptions.ConfigNotFound:\n            continue\n        profiles.append(loaded.pop('profiles'))\n        configs.append(loaded)\n    merged_config = _merge_list_of_dicts(configs)\n    merged_profiles = _merge_list_of_dicts(profiles)\n    merged_config['profiles'] = merged_profiles\n    return merged_config\n\n\ndef _merge_list_of_dicts(list_of_dicts):\n    merged_dicts = {}\n    for single_dict in list_of_dicts:\n        for key, value in single_dict.items():\n            if key not in merged_dicts:\n                merged_dicts[key] = value\n    return merged_dicts\n\n\ndef load_config(config_filename):\n    \"\"\"Parse a INI config with profiles.\n\n    This will parse an INI config file and map top level profiles\n    into a top level \"profile\" key.\n\n    If you want to parse an INI file and map all section names to\n    top level keys, use ``raw_config_parse`` instead.\n\n    \"\"\"\n    parsed = raw_config_parse(config_filename)\n    return build_profile_map(parsed)\n\n\ndef raw_config_parse(config_filename, parse_subsections=True):\n    \"\"\"Returns the parsed INI config contents.\n\n    Each section name is a top level key.\n\n    :param config_filename: The name of the INI file to parse\n\n    :param parse_subsections: If True, parse indented blocks as\n       subsections that represent their own configuration dictionary.\n       For example, if the config file had the contents::\n\n           s3 =\n              signature_version = s3v4\n              addressing_style = path\n\n        The resulting ``raw_config_parse`` would be::\n\n            {'s3': {'signature_version': 's3v4', 'addressing_style': 'path'}}\n\n       If False, do not try to parse subsections and return the indented\n       block as its literal value::\n\n            {'s3': '\\nsignature_version = s3v4\\naddressing_style = path'}\n\n    :returns: A dict with keys for each profile found in the config\n        file and the value of each key being a dict containing name\n        value pairs found in that profile.\n\n    :raises: ConfigNotFound, ConfigParseError\n    \"\"\"\n    config = {}\n    path = config_filename\n    if path is not None:\n        path = os.path.expandvars(path)\n        path = os.path.expanduser(path)\n        if not os.path.isfile(path):\n            raise botocore.exceptions.ConfigNotFound(path=_unicode_path(path))\n        cp = configparser.RawConfigParser()\n        try:\n            cp.read([path])\n        except (configparser.Error, UnicodeDecodeError) as e:\n            raise botocore.exceptions.ConfigParseError(\n                path=_unicode_path(path), error=e\n            ) from None\n        else:\n            for section in cp.sections():\n                config[section] = {}\n                for option in cp.options(section):\n                    config_value = cp.get(section, option)\n                    if parse_subsections and config_value.startswith('\\n'):\n                        # Then we need to parse the inner contents as\n                        # hierarchical.  We support a single level\n                        # of nesting for now.\n                        try:\n                            config_value = _parse_nested(config_value)\n                        except ValueError as e:\n                            raise botocore.exceptions.ConfigParseError(\n                                path=_unicode_path(path), error=e\n                            ) from None\n                    config[section][option] = config_value\n    return config\n\n\ndef _unicode_path(path):\n    if isinstance(path, str):\n        return path\n    # According to the documentation getfilesystemencoding can return None\n    # on unix in which case the default encoding is used instead.\n    filesystem_encoding = sys.getfilesystemencoding()\n    if filesystem_encoding is None:\n        filesystem_encoding = sys.getdefaultencoding()\n    return path.decode(filesystem_encoding, 'replace')\n\n\ndef _parse_nested(config_value):\n    # Given a value like this:\n    # \\n\n    # foo = bar\n    # bar = baz\n    # We need to parse this into\n    # {'foo': 'bar', 'bar': 'baz}\n    parsed = {}\n    for line in config_value.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        # The caller will catch ValueError\n        # and raise an appropriate error\n        # if this fails.\n        key, value = line.split('=', 1)\n        parsed[key.strip()] = value.strip()\n    return parsed\n\n\ndef _parse_section(key, values):\n    result = {}\n    try:\n        parts = shlex.split(key)\n    except ValueError:\n        return result\n    if len(parts) == 2:\n        result[parts[1]] = values\n    return result\n\n\ndef build_profile_map(parsed_ini_config):\n    \"\"\"Convert the parsed INI config into a profile map.\n\n    The config file format requires that every profile except the\n    default to be prepended with \"profile\", e.g.::\n\n        [profile test]\n        aws_... = foo\n        aws_... = bar\n\n        [profile bar]\n        aws_... = foo\n        aws_... = bar\n\n        # This is *not* a profile\n        [preview]\n        otherstuff = 1\n\n        # Neither is this\n        [foobar]\n        morestuff = 2\n\n    The build_profile_map will take a parsed INI config file where each top\n    level key represents a section name, and convert into a format where all\n    the profiles are under a single top level \"profiles\" key, and each key in\n    the sub dictionary is a profile name.  For example, the above config file\n    would be converted from::\n\n        {\"profile test\": {\"aws_...\": \"foo\", \"aws...\": \"bar\"},\n         \"profile bar\": {\"aws...\": \"foo\", \"aws...\": \"bar\"},\n         \"preview\": {\"otherstuff\": ...},\n         \"foobar\": {\"morestuff\": ...},\n         }\n\n    into::\n\n        {\"profiles\": {\"test\": {\"aws_...\": \"foo\", \"aws...\": \"bar\"},\n                      \"bar\": {\"aws...\": \"foo\", \"aws...\": \"bar\"},\n         \"preview\": {\"otherstuff\": ...},\n         \"foobar\": {\"morestuff\": ...},\n        }\n\n    If there are no profiles in the provided parsed INI contents, then\n    an empty dict will be the value associated with the ``profiles`` key.\n\n    .. note::\n\n        This will not mutate the passed in parsed_ini_config.  Instead it will\n        make a deepcopy and return that value.\n\n    \"\"\"\n    parsed_config = copy.deepcopy(parsed_ini_config)\n    profiles = {}\n    sso_sessions = {}\n    services = {}\n    final_config = {}\n    for key, values in parsed_config.items():\n        if key.startswith(\"profile\"):\n            profiles.update(_parse_section(key, values))\n        elif key.startswith(\"sso-session\"):\n            sso_sessions.update(_parse_section(key, values))\n        elif key.startswith(\"services\"):\n            services.update(_parse_section(key, values))\n        elif key == 'default':\n            # default section is special and is considered a profile\n            # name but we don't require you use 'profile \"default\"'\n            # as a section.\n            profiles[key] = values\n        else:\n            final_config[key] = values\n    final_config['profiles'] = profiles\n    final_config['sso_sessions'] = sso_sessions\n    final_config['services'] = services\n    return final_config\n", "botocore/discovery.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport time\nimport weakref\n\nfrom botocore import xform_name\nfrom botocore.exceptions import BotoCoreError, ConnectionError, HTTPClientError\nfrom botocore.model import OperationNotFoundError\nfrom botocore.utils import CachedProperty\n\nlogger = logging.getLogger(__name__)\n\n\nclass EndpointDiscoveryException(BotoCoreError):\n    pass\n\n\nclass EndpointDiscoveryRequired(EndpointDiscoveryException):\n    \"\"\"Endpoint Discovery is disabled but is required for this operation.\"\"\"\n\n    fmt = 'Endpoint Discovery is not enabled but this operation requires it.'\n\n\nclass EndpointDiscoveryRefreshFailed(EndpointDiscoveryException):\n    \"\"\"Endpoint Discovery failed to the refresh the known endpoints.\"\"\"\n\n    fmt = 'Endpoint Discovery failed to refresh the required endpoints.'\n\n\ndef block_endpoint_discovery_required_operations(model, **kwargs):\n    endpoint_discovery = model.endpoint_discovery\n    if endpoint_discovery and endpoint_discovery.get('required'):\n        raise EndpointDiscoveryRequired()\n\n\nclass EndpointDiscoveryModel:\n    def __init__(self, service_model):\n        self._service_model = service_model\n\n    @CachedProperty\n    def discovery_operation_name(self):\n        discovery_operation = self._service_model.endpoint_discovery_operation\n        return xform_name(discovery_operation.name)\n\n    @CachedProperty\n    def discovery_operation_keys(self):\n        discovery_operation = self._service_model.endpoint_discovery_operation\n        keys = []\n        if discovery_operation.input_shape:\n            keys = list(discovery_operation.input_shape.members.keys())\n        return keys\n\n    def discovery_required_for(self, operation_name):\n        try:\n            operation_model = self._service_model.operation_model(\n                operation_name\n            )\n            return operation_model.endpoint_discovery.get('required', False)\n        except OperationNotFoundError:\n            return False\n\n    def discovery_operation_kwargs(self, **kwargs):\n        input_keys = self.discovery_operation_keys\n        # Operation and Identifiers are only sent if there are Identifiers\n        if not kwargs.get('Identifiers'):\n            kwargs.pop('Operation', None)\n            kwargs.pop('Identifiers', None)\n        return {k: v for k, v in kwargs.items() if k in input_keys}\n\n    def gather_identifiers(self, operation, params):\n        return self._gather_ids(operation.input_shape, params)\n\n    def _gather_ids(self, shape, params, ids=None):\n        # Traverse the input shape and corresponding parameters, gathering\n        # any input fields labeled as an endpoint discovery id\n        if ids is None:\n            ids = {}\n        for member_name, member_shape in shape.members.items():\n            if member_shape.metadata.get('endpointdiscoveryid'):\n                ids[member_name] = params[member_name]\n            elif (\n                member_shape.type_name == 'structure' and member_name in params\n            ):\n                self._gather_ids(member_shape, params[member_name], ids)\n        return ids\n\n\nclass EndpointDiscoveryManager:\n    def __init__(\n        self, client, cache=None, current_time=None, always_discover=True\n    ):\n        if cache is None:\n            cache = {}\n        self._cache = cache\n        self._failed_attempts = {}\n        if current_time is None:\n            current_time = time.time\n        self._time = current_time\n        self._always_discover = always_discover\n\n        # This needs to be a weak ref in order to prevent memory leaks on\n        # python 2.6\n        self._client = weakref.proxy(client)\n        self._model = EndpointDiscoveryModel(client.meta.service_model)\n\n    def _parse_endpoints(self, response):\n        endpoints = response['Endpoints']\n        current_time = self._time()\n        for endpoint in endpoints:\n            cache_time = endpoint.get('CachePeriodInMinutes')\n            endpoint['Expiration'] = current_time + cache_time * 60\n        return endpoints\n\n    def _cache_item(self, value):\n        if isinstance(value, dict):\n            return tuple(sorted(value.items()))\n        else:\n            return value\n\n    def _create_cache_key(self, **kwargs):\n        kwargs = self._model.discovery_operation_kwargs(**kwargs)\n        return tuple(self._cache_item(v) for k, v in sorted(kwargs.items()))\n\n    def gather_identifiers(self, operation, params):\n        return self._model.gather_identifiers(operation, params)\n\n    def delete_endpoints(self, **kwargs):\n        cache_key = self._create_cache_key(**kwargs)\n        if cache_key in self._cache:\n            del self._cache[cache_key]\n\n    def _describe_endpoints(self, **kwargs):\n        # This is effectively a proxy to whatever name/kwargs the service\n        # supports for endpoint discovery.\n        kwargs = self._model.discovery_operation_kwargs(**kwargs)\n        operation_name = self._model.discovery_operation_name\n        discovery_operation = getattr(self._client, operation_name)\n        logger.debug('Discovering endpoints with kwargs: %s', kwargs)\n        return discovery_operation(**kwargs)\n\n    def _get_current_endpoints(self, key):\n        if key not in self._cache:\n            return None\n        now = self._time()\n        return [e for e in self._cache[key] if now < e['Expiration']]\n\n    def _refresh_current_endpoints(self, **kwargs):\n        cache_key = self._create_cache_key(**kwargs)\n        try:\n            response = self._describe_endpoints(**kwargs)\n            endpoints = self._parse_endpoints(response)\n            self._cache[cache_key] = endpoints\n            self._failed_attempts.pop(cache_key, None)\n            return endpoints\n        except (ConnectionError, HTTPClientError):\n            self._failed_attempts[cache_key] = self._time() + 60\n            return None\n\n    def _recently_failed(self, cache_key):\n        if cache_key in self._failed_attempts:\n            now = self._time()\n            if now < self._failed_attempts[cache_key]:\n                return True\n            del self._failed_attempts[cache_key]\n        return False\n\n    def _select_endpoint(self, endpoints):\n        return endpoints[0]['Address']\n\n    def describe_endpoint(self, **kwargs):\n        operation = kwargs['Operation']\n        discovery_required = self._model.discovery_required_for(operation)\n\n        if not self._always_discover and not discovery_required:\n            # Discovery set to only run on required operations\n            logger.debug(\n                'Optional discovery disabled. Skipping discovery for Operation: %s'\n                % operation\n            )\n            return None\n\n        # Get the endpoint for the provided operation and identifiers\n        cache_key = self._create_cache_key(**kwargs)\n        endpoints = self._get_current_endpoints(cache_key)\n        if endpoints:\n            return self._select_endpoint(endpoints)\n        # All known endpoints are stale\n        recently_failed = self._recently_failed(cache_key)\n        if not recently_failed:\n            # We haven't failed to discover recently, go ahead and refresh\n            endpoints = self._refresh_current_endpoints(**kwargs)\n            if endpoints:\n                return self._select_endpoint(endpoints)\n        # Discovery has failed recently, do our best to get an endpoint\n        logger.debug('Endpoint Discovery has failed for: %s', kwargs)\n        stale_entries = self._cache.get(cache_key, None)\n        if stale_entries:\n            # We have stale entries, use those while discovery is failing\n            return self._select_endpoint(stale_entries)\n        if discovery_required:\n            # It looks strange to be checking recently_failed again but,\n            # this informs us as to whether or not we tried to refresh earlier\n            if recently_failed:\n                # Discovery is required and we haven't already refreshed\n                endpoints = self._refresh_current_endpoints(**kwargs)\n                if endpoints:\n                    return self._select_endpoint(endpoints)\n            # No endpoints even refresh, raise hard error\n            raise EndpointDiscoveryRefreshFailed()\n        # Discovery is optional, just use the default endpoint for now\n        return None\n\n\nclass EndpointDiscoveryHandler:\n    def __init__(self, manager):\n        self._manager = manager\n\n    def register(self, events, service_id):\n        events.register(\n            'before-parameter-build.%s' % service_id, self.gather_identifiers\n        )\n        events.register_first(\n            'request-created.%s' % service_id, self.discover_endpoint\n        )\n        events.register('needs-retry.%s' % service_id, self.handle_retries)\n\n    def gather_identifiers(self, params, model, context, **kwargs):\n        endpoint_discovery = model.endpoint_discovery\n        # Only continue if the operation supports endpoint discovery\n        if endpoint_discovery is None:\n            return\n        ids = self._manager.gather_identifiers(model, params)\n        context['discovery'] = {'identifiers': ids}\n\n    def discover_endpoint(self, request, operation_name, **kwargs):\n        ids = request.context.get('discovery', {}).get('identifiers')\n        if ids is None:\n            return\n        endpoint = self._manager.describe_endpoint(\n            Operation=operation_name, Identifiers=ids\n        )\n        if endpoint is None:\n            logger.debug('Failed to discover and inject endpoint')\n            return\n        if not endpoint.startswith('http'):\n            endpoint = 'https://' + endpoint\n        logger.debug('Injecting discovered endpoint: %s', endpoint)\n        request.url = endpoint\n\n    def handle_retries(self, request_dict, response, operation, **kwargs):\n        if response is None:\n            return None\n\n        _, response = response\n        status = response.get('ResponseMetadata', {}).get('HTTPStatusCode')\n        error_code = response.get('Error', {}).get('Code')\n        if status != 421 and error_code != 'InvalidEndpointException':\n            return None\n\n        context = request_dict.get('context', {})\n        ids = context.get('discovery', {}).get('identifiers')\n        if ids is None:\n            return None\n\n        # Delete the cached endpoints, forcing a refresh on retry\n        # TODO: Improve eviction behavior to only evict the bad endpoint if\n        # there are multiple. This will almost certainly require a lock.\n        self._manager.delete_endpoints(\n            Operation=operation.name, Identifiers=ids\n        )\n        return 0\n", "botocore/model.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Abstractions to interact with service models.\"\"\"\nfrom collections import defaultdict\nfrom typing import NamedTuple, Union\n\nfrom botocore.compat import OrderedDict\nfrom botocore.exceptions import (\n    MissingServiceIdError,\n    UndefinedModelAttributeError,\n)\nfrom botocore.utils import CachedProperty, hyphenize_service_id, instance_cache\n\nNOT_SET = object()\n\n\nclass NoShapeFoundError(Exception):\n    pass\n\n\nclass InvalidShapeError(Exception):\n    pass\n\n\nclass OperationNotFoundError(Exception):\n    pass\n\n\nclass InvalidShapeReferenceError(Exception):\n    pass\n\n\nclass ServiceId(str):\n    def hyphenize(self):\n        return hyphenize_service_id(self)\n\n\nclass Shape:\n    \"\"\"Object representing a shape from the service model.\"\"\"\n\n    # To simplify serialization logic, all shape params that are\n    # related to serialization are moved from the top level hash into\n    # a 'serialization' hash.  This list below contains the names of all\n    # the attributes that should be moved.\n    SERIALIZED_ATTRS = [\n        'locationName',\n        'queryName',\n        'flattened',\n        'location',\n        'payload',\n        'streaming',\n        'timestampFormat',\n        'xmlNamespace',\n        'resultWrapper',\n        'xmlAttribute',\n        'eventstream',\n        'event',\n        'eventheader',\n        'eventpayload',\n        'jsonvalue',\n        'timestampFormat',\n        'hostLabel',\n    ]\n    METADATA_ATTRS = [\n        'required',\n        'min',\n        'max',\n        'pattern',\n        'sensitive',\n        'enum',\n        'idempotencyToken',\n        'error',\n        'exception',\n        'endpointdiscoveryid',\n        'retryable',\n        'document',\n        'union',\n        'contextParam',\n        'clientContextParams',\n        'requiresLength',\n    ]\n    MAP_TYPE = OrderedDict\n\n    def __init__(self, shape_name, shape_model, shape_resolver=None):\n        \"\"\"\n\n        :type shape_name: string\n        :param shape_name: The name of the shape.\n\n        :type shape_model: dict\n        :param shape_model: The shape model.  This would be the value\n            associated with the key in the \"shapes\" dict of the\n            service model (i.e ``model['shapes'][shape_name]``)\n\n        :type shape_resolver: botocore.model.ShapeResolver\n        :param shape_resolver: A shape resolver object.  This is used to\n            resolve references to other shapes.  For scalar shape types\n            (string, integer, boolean, etc.), this argument is not\n            required.  If a shape_resolver is not provided for a complex\n            type, then a ``ValueError`` will be raised when an attempt\n            to resolve a shape is made.\n\n        \"\"\"\n        self.name = shape_name\n        self.type_name = shape_model['type']\n        self.documentation = shape_model.get('documentation', '')\n        self._shape_model = shape_model\n        if shape_resolver is None:\n            # If a shape_resolver is not provided, we create an object\n            # that will throw errors if you attempt to resolve\n            # a shape.  This is actually ok for scalar shapes\n            # because they don't need to resolve shapes and shouldn't\n            # be required to provide an object they won't use.\n            shape_resolver = UnresolvableShapeMap()\n        self._shape_resolver = shape_resolver\n        self._cache = {}\n\n    @CachedProperty\n    def serialization(self):\n        \"\"\"Serialization information about the shape.\n\n        This contains information that may be needed for input serialization\n        or response parsing.  This can include:\n\n            * name\n            * queryName\n            * flattened\n            * location\n            * payload\n            * streaming\n            * xmlNamespace\n            * resultWrapper\n            * xmlAttribute\n            * jsonvalue\n            * timestampFormat\n\n        :rtype: dict\n        :return: Serialization information about the shape.\n\n        \"\"\"\n        model = self._shape_model\n        serialization = {}\n        for attr in self.SERIALIZED_ATTRS:\n            if attr in self._shape_model:\n                serialization[attr] = model[attr]\n        # For consistency, locationName is renamed to just 'name'.\n        if 'locationName' in serialization:\n            serialization['name'] = serialization.pop('locationName')\n        return serialization\n\n    @CachedProperty\n    def metadata(self):\n        \"\"\"Metadata about the shape.\n\n        This requires optional information about the shape, including:\n\n            * min\n            * max\n            * pattern\n            * enum\n            * sensitive\n            * required\n            * idempotencyToken\n            * document\n            * union\n            * contextParam\n            * clientContextParams\n            * requiresLength\n\n        :rtype: dict\n        :return: Metadata about the shape.\n\n        \"\"\"\n        model = self._shape_model\n        metadata = {}\n        for attr in self.METADATA_ATTRS:\n            if attr in self._shape_model:\n                metadata[attr] = model[attr]\n        return metadata\n\n    @CachedProperty\n    def required_members(self):\n        \"\"\"A list of members that are required.\n\n        A structure shape can define members that are required.\n        This value will return a list of required members.  If there\n        are no required members an empty list is returned.\n\n        \"\"\"\n        return self.metadata.get('required', [])\n\n    def _resolve_shape_ref(self, shape_ref):\n        return self._shape_resolver.resolve_shape_ref(shape_ref)\n\n    def __repr__(self):\n        return f\"<{self.__class__.__name__}({self.name})>\"\n\n    @property\n    def event_stream_name(self):\n        return None\n\n\nclass StructureShape(Shape):\n    @CachedProperty\n    def members(self):\n        members = self._shape_model.get('members', self.MAP_TYPE())\n        # The members dict looks like:\n        #    'members': {\n        #        'MemberName': {'shape': 'shapeName'},\n        #        'MemberName2': {'shape': 'shapeName'},\n        #    }\n        # We return a dict of member name to Shape object.\n        shape_members = self.MAP_TYPE()\n        for name, shape_ref in members.items():\n            shape_members[name] = self._resolve_shape_ref(shape_ref)\n        return shape_members\n\n    @CachedProperty\n    def event_stream_name(self):\n        for member_name, member in self.members.items():\n            if member.serialization.get('eventstream'):\n                return member_name\n        return None\n\n    @CachedProperty\n    def error_code(self):\n        if not self.metadata.get('exception', False):\n            return None\n        error_metadata = self.metadata.get(\"error\", {})\n        code = error_metadata.get(\"code\")\n        if code:\n            return code\n        # Use the exception name if there is no explicit code modeled\n        return self.name\n\n    @CachedProperty\n    def is_document_type(self):\n        return self.metadata.get('document', False)\n\n    @CachedProperty\n    def is_tagged_union(self):\n        return self.metadata.get('union', False)\n\n\nclass ListShape(Shape):\n    @CachedProperty\n    def member(self):\n        return self._resolve_shape_ref(self._shape_model['member'])\n\n\nclass MapShape(Shape):\n    @CachedProperty\n    def key(self):\n        return self._resolve_shape_ref(self._shape_model['key'])\n\n    @CachedProperty\n    def value(self):\n        return self._resolve_shape_ref(self._shape_model['value'])\n\n\nclass StringShape(Shape):\n    @CachedProperty\n    def enum(self):\n        return self.metadata.get('enum', [])\n\n\nclass StaticContextParameter(NamedTuple):\n    name: str\n    value: Union[bool, str]\n\n\nclass ContextParameter(NamedTuple):\n    name: str\n    member_name: str\n\n\nclass ClientContextParameter(NamedTuple):\n    name: str\n    type: str\n    documentation: str\n\n\nclass ServiceModel:\n    \"\"\"\n\n    :ivar service_description: The parsed service description dictionary.\n\n    \"\"\"\n\n    def __init__(self, service_description, service_name=None):\n        \"\"\"\n\n        :type service_description: dict\n        :param service_description: The service description model.  This value\n            is obtained from a botocore.loader.Loader, or from directly loading\n            the file yourself::\n\n                service_description = json.load(\n                    open('/path/to/service-description-model.json'))\n                model = ServiceModel(service_description)\n\n        :type service_name: str\n        :param service_name: The name of the service.  Normally this is\n            the endpoint prefix defined in the service_description.  However,\n            you can override this value to provide a more convenient name.\n            This is done in a few places in botocore (ses instead of email,\n            emr instead of elasticmapreduce).  If this value is not provided,\n            it will default to the endpointPrefix defined in the model.\n\n        \"\"\"\n        self._service_description = service_description\n        # We want clients to be able to access metadata directly.\n        self.metadata = service_description.get('metadata', {})\n        self._shape_resolver = ShapeResolver(\n            service_description.get('shapes', {})\n        )\n        self._signature_version = NOT_SET\n        self._service_name = service_name\n        self._instance_cache = {}\n\n    def shape_for(self, shape_name, member_traits=None):\n        return self._shape_resolver.get_shape_by_name(\n            shape_name, member_traits\n        )\n\n    def shape_for_error_code(self, error_code):\n        return self._error_code_cache.get(error_code, None)\n\n    @CachedProperty\n    def _error_code_cache(self):\n        error_code_cache = {}\n        for error_shape in self.error_shapes:\n            code = error_shape.error_code\n            error_code_cache[code] = error_shape\n        return error_code_cache\n\n    def resolve_shape_ref(self, shape_ref):\n        return self._shape_resolver.resolve_shape_ref(shape_ref)\n\n    @CachedProperty\n    def shape_names(self):\n        return list(self._service_description.get('shapes', {}))\n\n    @CachedProperty\n    def error_shapes(self):\n        error_shapes = []\n        for shape_name in self.shape_names:\n            error_shape = self.shape_for(shape_name)\n            if error_shape.metadata.get('exception', False):\n                error_shapes.append(error_shape)\n        return error_shapes\n\n    @instance_cache\n    def operation_model(self, operation_name):\n        try:\n            model = self._service_description['operations'][operation_name]\n        except KeyError:\n            raise OperationNotFoundError(operation_name)\n        return OperationModel(model, self, operation_name)\n\n    @CachedProperty\n    def documentation(self):\n        return self._service_description.get('documentation', '')\n\n    @CachedProperty\n    def operation_names(self):\n        return list(self._service_description.get('operations', []))\n\n    @CachedProperty\n    def service_name(self):\n        \"\"\"The name of the service.\n\n        This defaults to the endpointPrefix defined in the service model.\n        However, this value can be overriden when a ``ServiceModel`` is\n        created.  If a service_name was not provided when the ``ServiceModel``\n        was created and if there is no endpointPrefix defined in the\n        service model, then an ``UndefinedModelAttributeError`` exception\n        will be raised.\n\n        \"\"\"\n        if self._service_name is not None:\n            return self._service_name\n        else:\n            return self.endpoint_prefix\n\n    @CachedProperty\n    def service_id(self):\n        try:\n            return ServiceId(self._get_metadata_property('serviceId'))\n        except UndefinedModelAttributeError:\n            raise MissingServiceIdError(service_name=self._service_name)\n\n    @CachedProperty\n    def signing_name(self):\n        \"\"\"The name to use when computing signatures.\n\n        If the model does not define a signing name, this\n        value will be the endpoint prefix defined in the model.\n        \"\"\"\n        signing_name = self.metadata.get('signingName')\n        if signing_name is None:\n            signing_name = self.endpoint_prefix\n        return signing_name\n\n    @CachedProperty\n    def api_version(self):\n        return self._get_metadata_property('apiVersion')\n\n    @CachedProperty\n    def protocol(self):\n        return self._get_metadata_property('protocol')\n\n    @CachedProperty\n    def endpoint_prefix(self):\n        return self._get_metadata_property('endpointPrefix')\n\n    @CachedProperty\n    def endpoint_discovery_operation(self):\n        for operation in self.operation_names:\n            model = self.operation_model(operation)\n            if model.is_endpoint_discovery_operation:\n                return model\n\n    @CachedProperty\n    def endpoint_discovery_required(self):\n        for operation in self.operation_names:\n            model = self.operation_model(operation)\n            if (\n                model.endpoint_discovery is not None\n                and model.endpoint_discovery.get('required')\n            ):\n                return True\n        return False\n\n    @CachedProperty\n    def client_context_parameters(self):\n        params = self._service_description.get('clientContextParams', {})\n        return [\n            ClientContextParameter(\n                name=param_name,\n                type=param_val['type'],\n                documentation=param_val['documentation'],\n            )\n            for param_name, param_val in params.items()\n        ]\n\n    def _get_metadata_property(self, name):\n        try:\n            return self.metadata[name]\n        except KeyError:\n            raise UndefinedModelAttributeError(\n                f'\"{name}\" not defined in the metadata of the model: {self}'\n            )\n\n    # Signature version is one of the rare properties\n    # that can be modified so a CachedProperty is not used here.\n\n    @property\n    def signature_version(self):\n        if self._signature_version is NOT_SET:\n            signature_version = self.metadata.get('signatureVersion')\n            self._signature_version = signature_version\n        return self._signature_version\n\n    @signature_version.setter\n    def signature_version(self, value):\n        self._signature_version = value\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.service_name})'\n\n\nclass OperationModel:\n    def __init__(self, operation_model, service_model, name=None):\n        \"\"\"\n\n        :type operation_model: dict\n        :param operation_model: The operation model.  This comes from the\n            service model, and is the value associated with the operation\n            name in the service model (i.e ``model['operations'][op_name]``).\n\n        :type service_model: botocore.model.ServiceModel\n        :param service_model: The service model associated with the operation.\n\n        :type name: string\n        :param name: The operation name.  This is the operation name exposed to\n            the users of this model.  This can potentially be different from\n            the \"wire_name\", which is the operation name that *must* by\n            provided over the wire.  For example, given::\n\n               \"CreateCloudFrontOriginAccessIdentity\":{\n                 \"name\":\"CreateCloudFrontOriginAccessIdentity2014_11_06\",\n                  ...\n              }\n\n           The ``name`` would be ``CreateCloudFrontOriginAccessIdentity``,\n           but the ``self.wire_name`` would be\n           ``CreateCloudFrontOriginAccessIdentity2014_11_06``, which is the\n           value we must send in the corresponding HTTP request.\n\n        \"\"\"\n        self._operation_model = operation_model\n        self._service_model = service_model\n        self._api_name = name\n        # Clients can access '.name' to get the operation name\n        # and '.metadata' to get the top level metdata of the service.\n        self._wire_name = operation_model.get('name')\n        self.metadata = service_model.metadata\n        self.http = operation_model.get('http', {})\n\n    @CachedProperty\n    def name(self):\n        if self._api_name is not None:\n            return self._api_name\n        else:\n            return self.wire_name\n\n    @property\n    def wire_name(self):\n        \"\"\"The wire name of the operation.\n\n        In many situations this is the same value as the\n        ``name``, value, but in some services, the operation name\n        exposed to the user is different from the operation name\n        we send across the wire (e.g cloudfront).\n\n        Any serialization code should use ``wire_name``.\n\n        \"\"\"\n        return self._operation_model.get('name')\n\n    @property\n    def service_model(self):\n        return self._service_model\n\n    @CachedProperty\n    def documentation(self):\n        return self._operation_model.get('documentation', '')\n\n    @CachedProperty\n    def deprecated(self):\n        return self._operation_model.get('deprecated', False)\n\n    @CachedProperty\n    def endpoint_discovery(self):\n        # Explicit None default. An empty dictionary for this trait means it is\n        # enabled but not required to be used.\n        return self._operation_model.get('endpointdiscovery', None)\n\n    @CachedProperty\n    def is_endpoint_discovery_operation(self):\n        return self._operation_model.get('endpointoperation', False)\n\n    @CachedProperty\n    def input_shape(self):\n        if 'input' not in self._operation_model:\n            # Some operations do not accept any input and do not define an\n            # input shape.\n            return None\n        return self._service_model.resolve_shape_ref(\n            self._operation_model['input']\n        )\n\n    @CachedProperty\n    def output_shape(self):\n        if 'output' not in self._operation_model:\n            # Some operations do not define an output shape,\n            # in which case we return None to indicate the\n            # operation has no expected output.\n            return None\n        return self._service_model.resolve_shape_ref(\n            self._operation_model['output']\n        )\n\n    @CachedProperty\n    def idempotent_members(self):\n        input_shape = self.input_shape\n        if not input_shape:\n            return []\n\n        return [\n            name\n            for (name, shape) in input_shape.members.items()\n            if 'idempotencyToken' in shape.metadata\n            and shape.metadata['idempotencyToken']\n        ]\n\n    @CachedProperty\n    def static_context_parameters(self):\n        params = self._operation_model.get('staticContextParams', {})\n        return [\n            StaticContextParameter(name=name, value=props.get('value'))\n            for name, props in params.items()\n        ]\n\n    @CachedProperty\n    def context_parameters(self):\n        if not self.input_shape:\n            return []\n\n        return [\n            ContextParameter(\n                name=shape.metadata['contextParam']['name'],\n                member_name=name,\n            )\n            for name, shape in self.input_shape.members.items()\n            if 'contextParam' in shape.metadata\n            and 'name' in shape.metadata['contextParam']\n        ]\n\n    @CachedProperty\n    def request_compression(self):\n        return self._operation_model.get('requestcompression')\n\n    @CachedProperty\n    def auth_type(self):\n        return self._operation_model.get('authtype')\n\n    @CachedProperty\n    def error_shapes(self):\n        shapes = self._operation_model.get(\"errors\", [])\n        return list(self._service_model.resolve_shape_ref(s) for s in shapes)\n\n    @CachedProperty\n    def endpoint(self):\n        return self._operation_model.get('endpoint')\n\n    @CachedProperty\n    def http_checksum_required(self):\n        return self._operation_model.get('httpChecksumRequired', False)\n\n    @CachedProperty\n    def http_checksum(self):\n        return self._operation_model.get('httpChecksum', {})\n\n    @CachedProperty\n    def has_event_stream_input(self):\n        return self.get_event_stream_input() is not None\n\n    @CachedProperty\n    def has_event_stream_output(self):\n        return self.get_event_stream_output() is not None\n\n    def get_event_stream_input(self):\n        return self._get_event_stream(self.input_shape)\n\n    def get_event_stream_output(self):\n        return self._get_event_stream(self.output_shape)\n\n    def _get_event_stream(self, shape):\n        \"\"\"Returns the event stream member's shape if any or None otherwise.\"\"\"\n        if shape is None:\n            return None\n        event_name = shape.event_stream_name\n        if event_name:\n            return shape.members[event_name]\n        return None\n\n    @CachedProperty\n    def has_streaming_input(self):\n        return self.get_streaming_input() is not None\n\n    @CachedProperty\n    def has_streaming_output(self):\n        return self.get_streaming_output() is not None\n\n    def get_streaming_input(self):\n        return self._get_streaming_body(self.input_shape)\n\n    def get_streaming_output(self):\n        return self._get_streaming_body(self.output_shape)\n\n    def _get_streaming_body(self, shape):\n        \"\"\"Returns the streaming member's shape if any; or None otherwise.\"\"\"\n        if shape is None:\n            return None\n        payload = shape.serialization.get('payload')\n        if payload is not None:\n            payload_shape = shape.members[payload]\n            if payload_shape.type_name == 'blob':\n                return payload_shape\n        return None\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(name={self.name})'\n\n\nclass ShapeResolver:\n    \"\"\"Resolves shape references.\"\"\"\n\n    # Any type not in this mapping will default to the Shape class.\n    SHAPE_CLASSES = {\n        'structure': StructureShape,\n        'list': ListShape,\n        'map': MapShape,\n        'string': StringShape,\n    }\n\n    def __init__(self, shape_map):\n        self._shape_map = shape_map\n        self._shape_cache = {}\n\n    def get_shape_by_name(self, shape_name, member_traits=None):\n        try:\n            shape_model = self._shape_map[shape_name]\n        except KeyError:\n            raise NoShapeFoundError(shape_name)\n        try:\n            shape_cls = self.SHAPE_CLASSES.get(shape_model['type'], Shape)\n        except KeyError:\n            raise InvalidShapeError(\n                f\"Shape is missing required key 'type': {shape_model}\"\n            )\n        if member_traits:\n            shape_model = shape_model.copy()\n            shape_model.update(member_traits)\n        result = shape_cls(shape_name, shape_model, self)\n        return result\n\n    def resolve_shape_ref(self, shape_ref):\n        # A shape_ref is a dict that has a 'shape' key that\n        # refers to a shape name as well as any additional\n        # member traits that are then merged over the shape\n        # definition.  For example:\n        # {\"shape\": \"StringType\", \"locationName\": \"Foobar\"}\n        if len(shape_ref) == 1 and 'shape' in shape_ref:\n            # It's just a shape ref with no member traits, we can avoid\n            # a .copy().  This is the common case so it's specifically\n            # called out here.\n            return self.get_shape_by_name(shape_ref['shape'])\n        else:\n            member_traits = shape_ref.copy()\n            try:\n                shape_name = member_traits.pop('shape')\n            except KeyError:\n                raise InvalidShapeReferenceError(\n                    f\"Invalid model, missing shape reference: {shape_ref}\"\n                )\n            return self.get_shape_by_name(shape_name, member_traits)\n\n\nclass UnresolvableShapeMap:\n    \"\"\"A ShapeResolver that will throw ValueErrors when shapes are resolved.\"\"\"\n\n    def get_shape_by_name(self, shape_name, member_traits=None):\n        raise ValueError(\n            f\"Attempted to lookup shape '{shape_name}', but no shape map was provided.\"\n        )\n\n    def resolve_shape_ref(self, shape_ref):\n        raise ValueError(\n            f\"Attempted to resolve shape '{shape_ref}', but no shape \"\n            f\"map was provided.\"\n        )\n\n\nclass DenormalizedStructureBuilder:\n    \"\"\"Build a StructureShape from a denormalized model.\n\n    This is a convenience builder class that makes it easy to construct\n    ``StructureShape``s based on a denormalized model.\n\n    It will handle the details of creating unique shape names and creating\n    the appropriate shape map needed by the ``StructureShape`` class.\n\n    Example usage::\n\n        builder = DenormalizedStructureBuilder()\n        shape = builder.with_members({\n            'A': {\n                'type': 'structure',\n                'members': {\n                    'B': {\n                        'type': 'structure',\n                        'members': {\n                            'C': {\n                                'type': 'string',\n                            }\n                        }\n                    }\n                }\n            }\n        }).build_model()\n        # ``shape`` is now an instance of botocore.model.StructureShape\n\n    :type dict_type: class\n    :param dict_type: The dictionary type to use, allowing you to opt-in\n                      to using OrderedDict or another dict type. This can\n                      be particularly useful for testing when order\n                      matters, such as for documentation.\n\n    \"\"\"\n\n    SCALAR_TYPES = (\n        'string',\n        'integer',\n        'boolean',\n        'blob',\n        'float',\n        'timestamp',\n        'long',\n        'double',\n        'char',\n    )\n\n    def __init__(self, name=None):\n        self.members = OrderedDict()\n        self._name_generator = ShapeNameGenerator()\n        if name is None:\n            self.name = self._name_generator.new_shape_name('structure')\n\n    def with_members(self, members):\n        \"\"\"\n\n        :type members: dict\n        :param members: The denormalized members.\n\n        :return: self\n\n        \"\"\"\n        self._members = members\n        return self\n\n    def build_model(self):\n        \"\"\"Build the model based on the provided members.\n\n        :rtype: botocore.model.StructureShape\n        :return: The built StructureShape object.\n\n        \"\"\"\n        shapes = OrderedDict()\n        denormalized = {\n            'type': 'structure',\n            'members': self._members,\n        }\n        self._build_model(denormalized, shapes, self.name)\n        resolver = ShapeResolver(shape_map=shapes)\n        return StructureShape(\n            shape_name=self.name,\n            shape_model=shapes[self.name],\n            shape_resolver=resolver,\n        )\n\n    def _build_model(self, model, shapes, shape_name):\n        if model['type'] == 'structure':\n            shapes[shape_name] = self._build_structure(model, shapes)\n        elif model['type'] == 'list':\n            shapes[shape_name] = self._build_list(model, shapes)\n        elif model['type'] == 'map':\n            shapes[shape_name] = self._build_map(model, shapes)\n        elif model['type'] in self.SCALAR_TYPES:\n            shapes[shape_name] = self._build_scalar(model)\n        else:\n            raise InvalidShapeError(f\"Unknown shape type: {model['type']}\")\n\n    def _build_structure(self, model, shapes):\n        members = OrderedDict()\n        shape = self._build_initial_shape(model)\n        shape['members'] = members\n\n        for name, member_model in model.get('members', OrderedDict()).items():\n            member_shape_name = self._get_shape_name(member_model)\n            members[name] = {'shape': member_shape_name}\n            self._build_model(member_model, shapes, member_shape_name)\n        return shape\n\n    def _build_list(self, model, shapes):\n        member_shape_name = self._get_shape_name(model)\n        shape = self._build_initial_shape(model)\n        shape['member'] = {'shape': member_shape_name}\n        self._build_model(model['member'], shapes, member_shape_name)\n        return shape\n\n    def _build_map(self, model, shapes):\n        key_shape_name = self._get_shape_name(model['key'])\n        value_shape_name = self._get_shape_name(model['value'])\n        shape = self._build_initial_shape(model)\n        shape['key'] = {'shape': key_shape_name}\n        shape['value'] = {'shape': value_shape_name}\n        self._build_model(model['key'], shapes, key_shape_name)\n        self._build_model(model['value'], shapes, value_shape_name)\n        return shape\n\n    def _build_initial_shape(self, model):\n        shape = {\n            'type': model['type'],\n        }\n        if 'documentation' in model:\n            shape['documentation'] = model['documentation']\n        for attr in Shape.METADATA_ATTRS:\n            if attr in model:\n                shape[attr] = model[attr]\n        return shape\n\n    def _build_scalar(self, model):\n        return self._build_initial_shape(model)\n\n    def _get_shape_name(self, model):\n        if 'shape_name' in model:\n            return model['shape_name']\n        else:\n            return self._name_generator.new_shape_name(model['type'])\n\n\nclass ShapeNameGenerator:\n    \"\"\"Generate unique shape names for a type.\n\n    This class can be used in conjunction with the DenormalizedStructureBuilder\n    to generate unique shape names for a given type.\n\n    \"\"\"\n\n    def __init__(self):\n        self._name_cache = defaultdict(int)\n\n    def new_shape_name(self, type_name):\n        \"\"\"Generate a unique shape name.\n\n        This method will guarantee a unique shape name each time it is\n        called with the same type.\n\n        ::\n\n            >>> s = ShapeNameGenerator()\n            >>> s.new_shape_name('structure')\n            'StructureType1'\n            >>> s.new_shape_name('structure')\n            'StructureType2'\n            >>> s.new_shape_name('list')\n            'ListType1'\n            >>> s.new_shape_name('list')\n            'ListType2'\n\n\n        :type type_name: string\n        :param type_name: The type name (structure, list, map, string, etc.)\n\n        :rtype: string\n        :return: A unique shape name for the given type\n\n        \"\"\"\n        self._name_cache[type_name] += 1\n        current_index = self._name_cache[type_name]\n        return f'{type_name.capitalize()}Type{current_index}'\n", "botocore/awsrequest.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport functools\nimport logging\nfrom collections.abc import Mapping\n\nimport urllib3.util\nfrom urllib3.connection import HTTPConnection, VerifiedHTTPSConnection\nfrom urllib3.connectionpool import HTTPConnectionPool, HTTPSConnectionPool\n\nimport botocore.utils\nfrom botocore.compat import (\n    HTTPHeaders,\n    HTTPResponse,\n    MutableMapping,\n    urlencode,\n    urlparse,\n    urlsplit,\n    urlunsplit,\n)\nfrom botocore.exceptions import UnseekableStreamError\n\nlogger = logging.getLogger(__name__)\n\n\nclass AWSHTTPResponse(HTTPResponse):\n    # The *args, **kwargs is used because the args are slightly\n    # different in py2.6 than in py2.7/py3.\n    def __init__(self, *args, **kwargs):\n        self._status_tuple = kwargs.pop('status_tuple')\n        HTTPResponse.__init__(self, *args, **kwargs)\n\n    def _read_status(self):\n        if self._status_tuple is not None:\n            status_tuple = self._status_tuple\n            self._status_tuple = None\n            return status_tuple\n        else:\n            return HTTPResponse._read_status(self)\n\n\nclass AWSConnection:\n    \"\"\"Mixin for HTTPConnection that supports Expect 100-continue.\n\n    This when mixed with a subclass of httplib.HTTPConnection (though\n    technically we subclass from urllib3, which subclasses\n    httplib.HTTPConnection) and we only override this class to support Expect\n    100-continue, which we need for S3.  As far as I can tell, this is\n    general purpose enough to not be specific to S3, but I'm being\n    tentative and keeping it in botocore because I've only tested\n    this against AWS services.\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._original_response_cls = self.response_class\n        # This variable is set when we receive an early response from the\n        # server. If this value is set to True, any calls to send() are noops.\n        # This value is reset to false every time _send_request is called.\n        # This is to workaround changes in urllib3 2.0 which uses separate\n        # send() calls in request() instead of delegating to endheaders(),\n        # which is where the body is sent in CPython's HTTPConnection.\n        self._response_received = False\n        self._expect_header_set = False\n        self._send_called = False\n\n    def close(self):\n        super().close()\n        # Reset all of our instance state we were tracking.\n        self._response_received = False\n        self._expect_header_set = False\n        self._send_called = False\n        self.response_class = self._original_response_cls\n\n    def request(self, method, url, body=None, headers=None, *args, **kwargs):\n        if headers is None:\n            headers = {}\n        self._response_received = False\n        if headers.get('Expect', b'') == b'100-continue':\n            self._expect_header_set = True\n        else:\n            self._expect_header_set = False\n            self.response_class = self._original_response_cls\n        rval = super().request(method, url, body, headers, *args, **kwargs)\n        self._expect_header_set = False\n        return rval\n\n    def _convert_to_bytes(self, mixed_buffer):\n        # Take a list of mixed str/bytes and convert it\n        # all into a single bytestring.\n        # Any str will be encoded as utf-8.\n        bytes_buffer = []\n        for chunk in mixed_buffer:\n            if isinstance(chunk, str):\n                bytes_buffer.append(chunk.encode('utf-8'))\n            else:\n                bytes_buffer.append(chunk)\n        msg = b\"\\r\\n\".join(bytes_buffer)\n        return msg\n\n    def _send_output(self, message_body=None, *args, **kwargs):\n        self._buffer.extend((b\"\", b\"\"))\n        msg = self._convert_to_bytes(self._buffer)\n        del self._buffer[:]\n        # If msg and message_body are sent in a single send() call,\n        # it will avoid performance problems caused by the interaction\n        # between delayed ack and the Nagle algorithm.\n        if isinstance(message_body, bytes):\n            msg += message_body\n            message_body = None\n        self.send(msg)\n        if self._expect_header_set:\n            # This is our custom behavior.  If the Expect header was\n            # set, it will trigger this custom behavior.\n            logger.debug(\"Waiting for 100 Continue response.\")\n            # Wait for 1 second for the server to send a response.\n            if urllib3.util.wait_for_read(self.sock, 1):\n                self._handle_expect_response(message_body)\n                return\n            else:\n                # From the RFC:\n                # Because of the presence of older implementations, the\n                # protocol allows ambiguous situations in which a client may\n                # send \"Expect: 100-continue\" without receiving either a 417\n                # (Expectation Failed) status or a 100 (Continue) status.\n                # Therefore, when a client sends this header field to an origin\n                # server (possibly via a proxy) from which it has never seen a\n                # 100 (Continue) status, the client SHOULD NOT wait for an\n                # indefinite period before sending the request body.\n                logger.debug(\n                    \"No response seen from server, continuing to \"\n                    \"send the response body.\"\n                )\n        if message_body is not None:\n            # message_body was not a string (i.e. it is a file), and\n            # we must run the risk of Nagle.\n            self.send(message_body)\n\n    def _consume_headers(self, fp):\n        # Most servers (including S3) will just return\n        # the CLRF after the 100 continue response.  However,\n        # some servers (I've specifically seen this for squid when\n        # used as a straight HTTP proxy) will also inject a\n        # Connection: keep-alive header.  To account for this\n        # we'll read until we read '\\r\\n', and ignore any headers\n        # that come immediately after the 100 continue response.\n        current = None\n        while current != b'\\r\\n':\n            current = fp.readline()\n\n    def _handle_expect_response(self, message_body):\n        # This is called when we sent the request headers containing\n        # an Expect: 100-continue header and received a response.\n        # We now need to figure out what to do.\n        fp = self.sock.makefile('rb', 0)\n        try:\n            maybe_status_line = fp.readline()\n            parts = maybe_status_line.split(None, 2)\n            if self._is_100_continue_status(maybe_status_line):\n                self._consume_headers(fp)\n                logger.debug(\n                    \"100 Continue response seen, now sending request body.\"\n                )\n                self._send_message_body(message_body)\n            elif len(parts) == 3 and parts[0].startswith(b'HTTP/'):\n                # From the RFC:\n                # Requirements for HTTP/1.1 origin servers:\n                #\n                # - Upon receiving a request which includes an Expect\n                #   request-header field with the \"100-continue\"\n                #   expectation, an origin server MUST either respond with\n                #   100 (Continue) status and continue to read from the\n                #   input stream, or respond with a final status code.\n                #\n                # So if we don't get a 100 Continue response, then\n                # whatever the server has sent back is the final response\n                # and don't send the message_body.\n                logger.debug(\n                    \"Received a non 100 Continue response \"\n                    \"from the server, NOT sending request body.\"\n                )\n                status_tuple = (\n                    parts[0].decode('ascii'),\n                    int(parts[1]),\n                    parts[2].decode('ascii'),\n                )\n                response_class = functools.partial(\n                    AWSHTTPResponse, status_tuple=status_tuple\n                )\n                self.response_class = response_class\n                self._response_received = True\n        finally:\n            fp.close()\n\n    def _send_message_body(self, message_body):\n        if message_body is not None:\n            self.send(message_body)\n\n    def send(self, str):\n        if self._response_received:\n            if not self._send_called:\n                # urllib3 2.0 chunks and calls send potentially\n                # thousands of times inside `request` unlike the\n                # standard library. Only log this once for sanity.\n                logger.debug(\n                    \"send() called, but response already received. \"\n                    \"Not sending data.\"\n                )\n            self._send_called = True\n            return\n        return super().send(str)\n\n    def _is_100_continue_status(self, maybe_status_line):\n        parts = maybe_status_line.split(None, 2)\n        # Check for HTTP/<version> 100 Continue\\r\\n\n        return (\n            len(parts) >= 3\n            and parts[0].startswith(b'HTTP/')\n            and parts[1] == b'100'\n        )\n\n\nclass AWSHTTPConnection(AWSConnection, HTTPConnection):\n    \"\"\"An HTTPConnection that supports 100 Continue behavior.\"\"\"\n\n\nclass AWSHTTPSConnection(AWSConnection, VerifiedHTTPSConnection):\n    \"\"\"An HTTPSConnection that supports 100 Continue behavior.\"\"\"\n\n\nclass AWSHTTPConnectionPool(HTTPConnectionPool):\n    ConnectionCls = AWSHTTPConnection\n\n\nclass AWSHTTPSConnectionPool(HTTPSConnectionPool):\n    ConnectionCls = AWSHTTPSConnection\n\n\ndef prepare_request_dict(\n    request_dict, endpoint_url, context=None, user_agent=None\n):\n    \"\"\"\n    This method prepares a request dict to be created into an\n    AWSRequestObject. This prepares the request dict by adding the\n    url and the user agent to the request dict.\n\n    :type request_dict: dict\n    :param request_dict:  The request dict (created from the\n        ``serialize`` module).\n\n    :type user_agent: string\n    :param user_agent: The user agent to use for this request.\n\n    :type endpoint_url: string\n    :param endpoint_url: The full endpoint url, which contains at least\n        the scheme, the hostname, and optionally any path components.\n    \"\"\"\n    r = request_dict\n    if user_agent is not None:\n        headers = r['headers']\n        headers['User-Agent'] = user_agent\n    host_prefix = r.get('host_prefix')\n    url = _urljoin(endpoint_url, r['url_path'], host_prefix)\n    if r['query_string']:\n        # NOTE: This is to avoid circular import with utils. This is being\n        # done to avoid moving classes to different modules as to not cause\n        # breaking chainges.\n        percent_encode_sequence = botocore.utils.percent_encode_sequence\n        encoded_query_string = percent_encode_sequence(r['query_string'])\n        if '?' not in url:\n            url += '?%s' % encoded_query_string\n        else:\n            url += '&%s' % encoded_query_string\n    r['url'] = url\n    r['context'] = context\n    if context is None:\n        r['context'] = {}\n\n\ndef create_request_object(request_dict):\n    \"\"\"\n    This method takes a request dict and creates an AWSRequest object\n    from it.\n\n    :type request_dict: dict\n    :param request_dict:  The request dict (created from the\n        ``prepare_request_dict`` method).\n\n    :rtype: ``botocore.awsrequest.AWSRequest``\n    :return: An AWSRequest object based on the request_dict.\n\n    \"\"\"\n    r = request_dict\n    request_object = AWSRequest(\n        method=r['method'],\n        url=r['url'],\n        data=r['body'],\n        headers=r['headers'],\n        auth_path=r.get('auth_path'),\n    )\n    request_object.context = r['context']\n    return request_object\n\n\ndef _urljoin(endpoint_url, url_path, host_prefix):\n    p = urlsplit(endpoint_url)\n    # <part>   - <index>\n    # scheme   - p[0]\n    # netloc   - p[1]\n    # path     - p[2]\n    # query    - p[3]\n    # fragment - p[4]\n    if not url_path or url_path == '/':\n        # If there's no path component, ensure the URL ends with\n        # a '/' for backwards compatibility.\n        if not p[2]:\n            new_path = '/'\n        else:\n            new_path = p[2]\n    elif p[2].endswith('/') and url_path.startswith('/'):\n        new_path = p[2][:-1] + url_path\n    else:\n        new_path = p[2] + url_path\n\n    new_netloc = p[1]\n    if host_prefix is not None:\n        new_netloc = host_prefix + new_netloc\n\n    reconstructed = urlunsplit((p[0], new_netloc, new_path, p[3], p[4]))\n    return reconstructed\n\n\nclass AWSRequestPreparer:\n    \"\"\"\n    This class performs preparation on AWSRequest objects similar to that of\n    the PreparedRequest class does in the requests library. However, the logic\n    has been boiled down to meet the specific use cases in botocore. Of note\n    there are the following differences:\n        This class does not heavily prepare the URL. Requests performed many\n        validations and corrections to ensure the URL is properly formatted.\n        Botocore either performs these validations elsewhere or otherwise\n        consistently provides well formatted URLs.\n\n        This class does not heavily prepare the body. Body preperation is\n        simple and supports only the cases that we document: bytes and\n        file-like objects to determine the content-length. This will also\n        additionally prepare a body that is a dict to be url encoded params\n        string as some signers rely on this. Finally, this class does not\n        support multipart file uploads.\n\n        This class does not prepare the method, auth or cookies.\n    \"\"\"\n\n    def prepare(self, original):\n        method = original.method\n        url = self._prepare_url(original)\n        body = self._prepare_body(original)\n        headers = self._prepare_headers(original, body)\n        stream_output = original.stream_output\n\n        return AWSPreparedRequest(method, url, headers, body, stream_output)\n\n    def _prepare_url(self, original):\n        url = original.url\n        if original.params:\n            url_parts = urlparse(url)\n            delim = '&' if url_parts.query else '?'\n            if isinstance(original.params, Mapping):\n                params_to_encode = list(original.params.items())\n            else:\n                params_to_encode = original.params\n            params = urlencode(params_to_encode, doseq=True)\n            url = delim.join((url, params))\n        return url\n\n    def _prepare_headers(self, original, prepared_body=None):\n        headers = HeadersDict(original.headers.items())\n\n        # If the transfer encoding or content length is already set, use that\n        if 'Transfer-Encoding' in headers or 'Content-Length' in headers:\n            return headers\n\n        # Ensure we set the content length when it is expected\n        if original.method not in ('GET', 'HEAD', 'OPTIONS'):\n            length = self._determine_content_length(prepared_body)\n            if length is not None:\n                headers['Content-Length'] = str(length)\n            else:\n                # Failed to determine content length, using chunked\n                # NOTE: This shouldn't ever happen in practice\n                body_type = type(prepared_body)\n                logger.debug('Failed to determine length of %s', body_type)\n                headers['Transfer-Encoding'] = 'chunked'\n\n        return headers\n\n    def _to_utf8(self, item):\n        key, value = item\n        if isinstance(key, str):\n            key = key.encode('utf-8')\n        if isinstance(value, str):\n            value = value.encode('utf-8')\n        return key, value\n\n    def _prepare_body(self, original):\n        \"\"\"Prepares the given HTTP body data.\"\"\"\n        body = original.data\n        if body == b'':\n            body = None\n\n        if isinstance(body, dict):\n            params = [self._to_utf8(item) for item in body.items()]\n            body = urlencode(params, doseq=True)\n\n        return body\n\n    def _determine_content_length(self, body):\n        return botocore.utils.determine_content_length(body)\n\n\nclass AWSRequest:\n    \"\"\"Represents the elements of an HTTP request.\n\n    This class was originally inspired by requests.models.Request, but has been\n    boiled down to meet the specific use cases in botocore. That being said this\n    class (even in requests) is effectively a named-tuple.\n    \"\"\"\n\n    _REQUEST_PREPARER_CLS = AWSRequestPreparer\n\n    def __init__(\n        self,\n        method=None,\n        url=None,\n        headers=None,\n        data=None,\n        params=None,\n        auth_path=None,\n        stream_output=False,\n    ):\n        self._request_preparer = self._REQUEST_PREPARER_CLS()\n\n        # Default empty dicts for dict params.\n        params = {} if params is None else params\n\n        self.method = method\n        self.url = url\n        self.headers = HTTPHeaders()\n        self.data = data\n        self.params = params\n        self.auth_path = auth_path\n        self.stream_output = stream_output\n\n        if headers is not None:\n            for key, value in headers.items():\n                self.headers[key] = value\n\n        # This is a dictionary to hold information that is used when\n        # processing the request. What is inside of ``context`` is open-ended.\n        # For example, it may have a timestamp key that is used for holding\n        # what the timestamp is when signing the request. Note that none\n        # of the information that is inside of ``context`` is directly\n        # sent over the wire; the information is only used to assist in\n        # creating what is sent over the wire.\n        self.context = {}\n\n    def prepare(self):\n        \"\"\"Constructs a :class:`AWSPreparedRequest <AWSPreparedRequest>`.\"\"\"\n        return self._request_preparer.prepare(self)\n\n    @property\n    def body(self):\n        body = self.prepare().body\n        if isinstance(body, str):\n            body = body.encode('utf-8')\n        return body\n\n\nclass AWSPreparedRequest:\n    \"\"\"A data class representing a finalized request to be sent over the wire.\n\n    Requests at this stage should be treated as final, and the properties of\n    the request should not be modified.\n\n    :ivar method: The HTTP Method\n    :ivar url: The full url\n    :ivar headers: The HTTP headers to send.\n    :ivar body: The HTTP body.\n    :ivar stream_output: If the response for this request should be streamed.\n    \"\"\"\n\n    def __init__(self, method, url, headers, body, stream_output):\n        self.method = method\n        self.url = url\n        self.headers = headers\n        self.body = body\n        self.stream_output = stream_output\n\n    def __repr__(self):\n        fmt = (\n            '<AWSPreparedRequest stream_output=%s, method=%s, url=%s, '\n            'headers=%s>'\n        )\n        return fmt % (self.stream_output, self.method, self.url, self.headers)\n\n    def reset_stream(self):\n        \"\"\"Resets the streaming body to it's initial position.\n\n        If the request contains a streaming body (a streamable file-like object)\n        seek to the object's initial position to ensure the entire contents of\n        the object is sent. This is a no-op for static bytes-like body types.\n        \"\"\"\n        # Trying to reset a stream when there is a no stream will\n        # just immediately return.  It's not an error, it will produce\n        # the same result as if we had actually reset the stream (we'll send\n        # the entire body contents again if we need to).\n        # Same case if the body is a string/bytes/bytearray type.\n\n        non_seekable_types = (bytes, str, bytearray)\n        if self.body is None or isinstance(self.body, non_seekable_types):\n            return\n        try:\n            logger.debug(\"Rewinding stream: %s\", self.body)\n            self.body.seek(0)\n        except Exception as e:\n            logger.debug(\"Unable to rewind stream: %s\", e)\n            raise UnseekableStreamError(stream_object=self.body)\n\n\nclass AWSResponse:\n    \"\"\"A data class representing an HTTP response.\n\n    This class was originally inspired by requests.models.Response, but has\n    been boiled down to meet the specific use cases in botocore. This has\n    effectively been reduced to a named tuple.\n\n    :ivar url: The full url.\n    :ivar status_code: The status code of the HTTP response.\n    :ivar headers: The HTTP headers received.\n    :ivar body: The HTTP response body.\n    \"\"\"\n\n    def __init__(self, url, status_code, headers, raw):\n        self.url = url\n        self.status_code = status_code\n        self.headers = HeadersDict(headers)\n        self.raw = raw\n\n        self._content = None\n\n    @property\n    def content(self):\n        \"\"\"Content of the response as bytes.\"\"\"\n\n        if self._content is None:\n            # Read the contents.\n            # NOTE: requests would attempt to call stream and fall back\n            # to a custom generator that would call read in a loop, but\n            # we don't rely on this behavior\n            self._content = b''.join(self.raw.stream()) or b''\n\n        return self._content\n\n    @property\n    def text(self):\n        \"\"\"Content of the response as a proper text type.\n\n        Uses the encoding type provided in the reponse headers to decode the\n        response content into a proper text type. If the encoding is not\n        present in the headers, UTF-8 is used as a default.\n        \"\"\"\n        encoding = botocore.utils.get_encoding_from_headers(self.headers)\n        if encoding:\n            return self.content.decode(encoding)\n        else:\n            return self.content.decode('utf-8')\n\n\nclass _HeaderKey:\n    def __init__(self, key):\n        self._key = key\n        self._lower = key.lower()\n\n    def __hash__(self):\n        return hash(self._lower)\n\n    def __eq__(self, other):\n        return isinstance(other, _HeaderKey) and self._lower == other._lower\n\n    def __str__(self):\n        return self._key\n\n    def __repr__(self):\n        return repr(self._key)\n\n\nclass HeadersDict(MutableMapping):\n    \"\"\"A case-insenseitive dictionary to represent HTTP headers.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._dict = {}\n        self.update(*args, **kwargs)\n\n    def __setitem__(self, key, value):\n        self._dict[_HeaderKey(key)] = value\n\n    def __getitem__(self, key):\n        return self._dict[_HeaderKey(key)]\n\n    def __delitem__(self, key):\n        del self._dict[_HeaderKey(key)]\n\n    def __iter__(self):\n        return (str(key) for key in self._dict)\n\n    def __len__(self):\n        return len(self._dict)\n\n    def __repr__(self):\n        return repr(self._dict)\n\n    def copy(self):\n        return HeadersDict(self.items())\n", "botocore/translate.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\n\nfrom botocore.utils import merge_dicts\n\n\ndef build_retry_config(\n    endpoint_prefix, retry_model, definitions, client_retry_config=None\n):\n    service_config = retry_model.get(endpoint_prefix, {})\n    resolve_references(service_config, definitions)\n    # We want to merge the global defaults with the service specific\n    # defaults, with the service specific defaults taking precedence.\n    # So we use the global defaults as the base.\n    #\n    # A deepcopy is done on the retry defaults because it ensures the\n    # retry model has no chance of getting mutated when the service specific\n    # configuration or client retry config is merged in.\n    final_retry_config = {\n        '__default__': copy.deepcopy(retry_model.get('__default__', {}))\n    }\n    resolve_references(final_retry_config, definitions)\n    # The merge the service specific config on top.\n    merge_dicts(final_retry_config, service_config)\n    if client_retry_config is not None:\n        _merge_client_retry_config(final_retry_config, client_retry_config)\n    return final_retry_config\n\n\ndef _merge_client_retry_config(retry_config, client_retry_config):\n    max_retry_attempts_override = client_retry_config.get('max_attempts')\n    if max_retry_attempts_override is not None:\n        # In the retry config, the max_attempts refers to the maximum number\n        # of requests in general will be made. However, for the client's\n        # retry config it refers to how many retry attempts will be made at\n        # most. So to translate this number from the client config, one is\n        # added to convert it to the maximum number request that will be made\n        # by including the initial request.\n        #\n        # It is also important to note that if we ever support per operation\n        # configuration in the retry model via the client, we will need to\n        # revisit this logic to make sure max_attempts gets applied\n        # per operation.\n        retry_config['__default__']['max_attempts'] = (\n            max_retry_attempts_override + 1\n        )\n\n\ndef resolve_references(config, definitions):\n    \"\"\"Recursively replace $ref keys.\n\n    To cut down on duplication, common definitions can be declared\n    (and passed in via the ``definitions`` attribute) and then\n    references as {\"$ref\": \"name\"}, when this happens the reference\n    dict is placed with the value from the ``definition`` dict.\n\n    This is recursively done.\n\n    \"\"\"\n    for key, value in config.items():\n        if isinstance(value, dict):\n            if len(value) == 1 and list(value.keys())[0] == '$ref':\n                # Then we need to resolve this reference.\n                config[key] = definitions[list(value.values())[0]]\n            else:\n                resolve_references(value, definitions)\n", "botocore/config.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\n\nfrom botocore.compat import OrderedDict\nfrom botocore.endpoint import DEFAULT_TIMEOUT, MAX_POOL_CONNECTIONS\nfrom botocore.exceptions import (\n    InvalidMaxRetryAttemptsError,\n    InvalidRetryConfigurationError,\n    InvalidRetryModeError,\n    InvalidS3AddressingStyleError,\n)\n\n\nclass Config:\n    \"\"\"Advanced configuration for Botocore clients.\n\n    :type region_name: str\n    :param region_name: The region to use in instantiating the client\n\n    :type signature_version: str\n    :param signature_version: The signature version when signing requests.\n\n    :type user_agent: str\n    :param user_agent: The value to use in the User-Agent header.\n\n    :type user_agent_extra: str\n    :param user_agent_extra: The value to append to the current User-Agent\n        header value.\n\n    :type user_agent_appid: str\n    :param user_agent_appid: A value that gets included in the User-Agent\n        string in the format \"app/<user_agent_appid>\". Allowed characters are\n        ASCII alphanumerics and ``!$%&'*+-.^_`|~``. All other characters will\n        be replaced by a ``-``.\n\n    :type connect_timeout: float or int\n    :param connect_timeout: The time in seconds till a timeout exception is\n        thrown when attempting to make a connection. The default is 60\n        seconds.\n\n    :type read_timeout: float or int\n    :param read_timeout: The time in seconds till a timeout exception is\n        thrown when attempting to read from a connection. The default is\n        60 seconds.\n\n    :type parameter_validation: bool\n    :param parameter_validation: Whether parameter validation should occur\n        when serializing requests. The default is True.  You can disable\n        parameter validation for performance reasons.  Otherwise, it's\n        recommended to leave parameter validation enabled.\n\n    :type max_pool_connections: int\n    :param max_pool_connections: The maximum number of connections to\n        keep in a connection pool.  If this value is not set, the default\n        value of 10 is used.\n\n    :type proxies: dict\n    :param proxies: A dictionary of proxy servers to use by protocol or\n        endpoint, e.g.:\n        ``{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}``.\n        The proxies are used on each request.\n\n    :type proxies_config: dict\n    :param proxies_config: A dictionary of additional proxy configurations.\n        Valid keys are:\n\n        * ``proxy_ca_bundle`` -- The path to a custom certificate bundle to use\n          when establishing SSL/TLS connections with proxy.\n\n        * ``proxy_client_cert`` -- The path to a certificate for proxy\n          TLS client authentication.\n\n          When a string is provided it is treated as a path to a proxy client\n          certificate. When a two element tuple is provided, it will be\n          interpreted as the path to the client certificate, and the path\n          to the certificate key.\n\n        * ``proxy_use_forwarding_for_https`` -- For HTTPS proxies,\n          forward your requests to HTTPS destinations with an absolute\n          URI. We strongly recommend you only use this option with\n          trusted or corporate proxies. Value must be boolean.\n\n    :type s3: dict\n    :param s3: A dictionary of S3 specific configurations.\n        Valid keys are:\n\n        * ``use_accelerate_endpoint`` -- Refers to whether to use the S3\n          Accelerate endpoint. The value must be a boolean. If True, the\n          client will use the S3 Accelerate endpoint. If the S3 Accelerate\n          endpoint is being used then the addressing style will always\n          be virtual.\n\n        * ``payload_signing_enabled`` -- Refers to whether or not to SHA256\n          sign sigv4 payloads. By default, this is disabled for streaming\n          uploads (UploadPart and PutObject).\n\n        * ``addressing_style`` -- Refers to the style in which to address\n          s3 endpoints. Values must be a string that equals one of:\n\n          * ``auto`` -- Addressing style is chosen for user. Depending\n            on the configuration of client, the endpoint may be addressed in\n            the virtual or the path style. Note that this is the default\n            behavior if no style is specified.\n\n          * ``virtual`` -- Addressing style is always virtual. The name of the\n            bucket must be DNS compatible or an exception will be thrown.\n            Endpoints will be addressed as such: ``mybucket.s3.amazonaws.com``\n\n          * ``path`` -- Addressing style is always by path. Endpoints will be\n            addressed as such: ``s3.amazonaws.com/mybucket``\n\n        * ``us_east_1_regional_endpoint`` -- Refers to what S3 endpoint to use\n          when the region is configured to be us-east-1. Values must be a\n          string that equals:\n\n          * ``regional`` -- Use the us-east-1.amazonaws.com endpoint if the\n            client is configured to use the us-east-1 region.\n\n          * ``legacy`` -- Use the s3.amazonaws.com endpoint if the client is\n            configured to use the us-east-1 region. This is the default if\n            the configuration option is not specified.\n\n\n    :type retries: dict\n    :param retries: A dictionary for configuration related to retry behavior.\n        Valid keys are:\n\n        * ``total_max_attempts`` -- An integer representing the maximum number of\n          total attempts that will be made on a single request.  This includes\n          the initial request, so a value of 1 indicates that no requests\n          will be retried.  If ``total_max_attempts`` and ``max_attempts``\n          are both provided, ``total_max_attempts`` takes precedence.\n          ``total_max_attempts`` is preferred over ``max_attempts`` because\n          it maps to the ``AWS_MAX_ATTEMPTS`` environment variable and\n          the ``max_attempts`` config file value.\n        * ``max_attempts`` -- An integer representing the maximum number of\n          retry attempts that will be made on a single request. For\n          example, setting this value to 2 will result in the request\n          being retried at most two times after the initial request. Setting\n          this value to 0 will result in no retries ever being attempted after\n          the initial request. If not provided, the number of retries will\n          default to the value specified in the service model, which is\n          typically four retries.\n        * ``mode`` -- A string representing the type of retry mode botocore\n          should use.  Valid values are:\n\n          * ``legacy`` - The pre-existing retry behavior.\n\n          * ``standard`` - The standardized set of retry rules. This will also\n            default to 3 max attempts unless overridden.\n\n          * ``adaptive`` - Retries with additional client side throttling.\n\n    :type client_cert: str, (str, str)\n    :param client_cert: The path to a certificate for TLS client authentication.\n\n        When a string is provided it is treated as a path to a client\n        certificate to be used when creating a TLS connection.\n\n        If a client key is to be provided alongside the client certificate the\n        client_cert should be set to a tuple of length two where the first\n        element is the path to the client certificate and the second element is\n        the path to the certificate key.\n\n    :type inject_host_prefix: bool\n    :param inject_host_prefix: Whether host prefix injection should occur.\n\n        Defaults to True.\n\n        Setting this to False disables the injection of operation parameters\n        into the prefix of the hostname. This is useful for clients providing\n        custom endpoints that should not have their host prefix modified.\n\n    :type use_dualstack_endpoint: bool\n    :param use_dualstack_endpoint: Setting to True enables dualstack\n        endpoint resolution.\n\n        Defaults to None.\n\n    :type use_fips_endpoint: bool\n    :param use_fips_endpoint: Setting to True enables fips\n        endpoint resolution.\n\n        Defaults to None.\n\n    :type ignore_configured_endpoint_urls: bool\n    :param ignore_configured_endpoint_urls: Setting to True disables use\n        of endpoint URLs provided via environment variables and\n        the shared configuration file.\n\n        Defaults to None.\n\n    :type tcp_keepalive: bool\n    :param tcp_keepalive: Enables the TCP Keep-Alive socket option used when\n        creating new connections if set to True.\n\n        Defaults to False.\n\n    :type request_min_compression_size_bytes: int\n    :param request_min_compression_size_bytes: The minimum size in bytes that a\n        request body should be to trigger compression. All requests with\n        streaming input that don't contain the ``requiresLength`` trait will be\n        compressed regardless of this setting.\n\n        Defaults to None.\n\n    :type disable_request_compression: bool\n    :param disable_request_compression: Disables request body compression if\n        set to True.\n\n        Defaults to None.\n\n    :type client_context_params: dict\n    :param client_context_params: A dictionary of parameters specific to\n        individual services. If available, valid parameters can be found in\n        the ``Client Context Parameters`` section of the service client's\n        documentation. Invalid parameters or ones that are not used by the\n        specified service will be ignored.\n\n        Defaults to None.\n    \"\"\"\n\n    OPTION_DEFAULTS = OrderedDict(\n        [\n            ('region_name', None),\n            ('signature_version', None),\n            ('user_agent', None),\n            ('user_agent_extra', None),\n            ('user_agent_appid', None),\n            ('connect_timeout', DEFAULT_TIMEOUT),\n            ('read_timeout', DEFAULT_TIMEOUT),\n            ('parameter_validation', True),\n            ('max_pool_connections', MAX_POOL_CONNECTIONS),\n            ('proxies', None),\n            ('proxies_config', None),\n            ('s3', None),\n            ('retries', None),\n            ('client_cert', None),\n            ('inject_host_prefix', True),\n            ('endpoint_discovery_enabled', None),\n            ('use_dualstack_endpoint', None),\n            ('use_fips_endpoint', None),\n            ('ignore_configured_endpoint_urls', None),\n            ('defaults_mode', None),\n            ('tcp_keepalive', None),\n            ('request_min_compression_size_bytes', None),\n            ('disable_request_compression', None),\n            ('client_context_params', None),\n        ]\n    )\n\n    NON_LEGACY_OPTION_DEFAULTS = {\n        'connect_timeout': None,\n    }\n\n    def __init__(self, *args, **kwargs):\n        self._user_provided_options = self._record_user_provided_options(\n            args, kwargs\n        )\n\n        # Merge the user_provided options onto the default options\n        config_vars = copy.copy(self.OPTION_DEFAULTS)\n        defaults_mode = self._user_provided_options.get(\n            'defaults_mode', 'legacy'\n        )\n        if defaults_mode != 'legacy':\n            config_vars.update(self.NON_LEGACY_OPTION_DEFAULTS)\n        config_vars.update(self._user_provided_options)\n\n        # Set the attributes based on the config_vars\n        for key, value in config_vars.items():\n            setattr(self, key, value)\n\n        # Validate the s3 options\n        self._validate_s3_configuration(self.s3)\n\n        self._validate_retry_configuration(self.retries)\n\n    def _record_user_provided_options(self, args, kwargs):\n        option_order = list(self.OPTION_DEFAULTS)\n        user_provided_options = {}\n\n        # Iterate through the kwargs passed through to the constructor and\n        # map valid keys to the dictionary\n        for key, value in kwargs.items():\n            if key in self.OPTION_DEFAULTS:\n                user_provided_options[key] = value\n            # The key must exist in the available options\n            else:\n                raise TypeError(f\"Got unexpected keyword argument '{key}'\")\n\n        # The number of args should not be longer than the allowed\n        # options\n        if len(args) > len(option_order):\n            raise TypeError(\n                f\"Takes at most {len(option_order)} arguments ({len(args)} given)\"\n            )\n\n        # Iterate through the args passed through to the constructor and map\n        # them to appropriate keys.\n        for i, arg in enumerate(args):\n            # If a kwarg was specified for the arg, then error out\n            if option_order[i] in user_provided_options:\n                raise TypeError(\n                    f\"Got multiple values for keyword argument '{option_order[i]}'\"\n                )\n            user_provided_options[option_order[i]] = arg\n\n        return user_provided_options\n\n    def _validate_s3_configuration(self, s3):\n        if s3 is not None:\n            addressing_style = s3.get('addressing_style')\n            if addressing_style not in ['virtual', 'auto', 'path', None]:\n                raise InvalidS3AddressingStyleError(\n                    s3_addressing_style=addressing_style\n                )\n\n    def _validate_retry_configuration(self, retries):\n        valid_options = ('max_attempts', 'mode', 'total_max_attempts')\n        valid_modes = ('legacy', 'standard', 'adaptive')\n        if retries is not None:\n            for key, value in retries.items():\n                if key not in valid_options:\n                    raise InvalidRetryConfigurationError(\n                        retry_config_option=key,\n                        valid_options=valid_options,\n                    )\n                if key == 'max_attempts' and value < 0:\n                    raise InvalidMaxRetryAttemptsError(\n                        provided_max_attempts=value,\n                        min_value=0,\n                    )\n                if key == 'total_max_attempts' and value < 1:\n                    raise InvalidMaxRetryAttemptsError(\n                        provided_max_attempts=value,\n                        min_value=1,\n                    )\n                if key == 'mode' and value not in valid_modes:\n                    raise InvalidRetryModeError(\n                        provided_retry_mode=value,\n                        valid_modes=valid_modes,\n                    )\n\n    def merge(self, other_config):\n        \"\"\"Merges the config object with another config object\n\n        This will merge in all non-default values from the provided config\n        and return a new config object\n\n        :type other_config: botocore.config.Config\n        :param other config: Another config object to merge with. The values\n            in the provided config object will take precedence in the merging\n\n        :returns: A config object built from the merged values of both\n            config objects.\n        \"\"\"\n        # Make a copy of the current attributes in the config object.\n        config_options = copy.copy(self._user_provided_options)\n\n        # Merge in the user provided options from the other config\n        config_options.update(other_config._user_provided_options)\n\n        # Return a new config object with the merged properties.\n        return Config(**config_options)\n", "botocore/compress.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nNOTE: All functions in this module are considered private and are\nsubject to abrupt breaking changes. Please do not use them directly.\n\n\"\"\"\n\nimport io\nimport logging\nfrom gzip import GzipFile\nfrom gzip import compress as gzip_compress\n\nfrom botocore.compat import urlencode\nfrom botocore.utils import determine_content_length\n\nlogger = logging.getLogger(__name__)\n\n\ndef maybe_compress_request(config, request_dict, operation_model):\n    \"\"\"Attempt to compress the request body using the modeled encodings.\"\"\"\n    if _should_compress_request(config, request_dict, operation_model):\n        for encoding in operation_model.request_compression['encodings']:\n            encoder = COMPRESSION_MAPPING.get(encoding)\n            if encoder is not None:\n                logger.debug('Compressing request with %s encoding.', encoding)\n                request_dict['body'] = encoder(request_dict['body'])\n                _set_compression_header(request_dict['headers'], encoding)\n                return\n            else:\n                logger.debug('Unsupported compression encoding: %s', encoding)\n\n\ndef _should_compress_request(config, request_dict, operation_model):\n    if (\n        config.disable_request_compression is not True\n        and config.signature_version != 'v2'\n        and operation_model.request_compression is not None\n    ):\n        if not _is_compressible_type(request_dict):\n            body_type = type(request_dict['body'])\n            log_msg = 'Body type %s does not support compression.'\n            logger.debug(log_msg, body_type)\n            return False\n\n        if operation_model.has_streaming_input:\n            streaming_input = operation_model.get_streaming_input()\n            streaming_metadata = streaming_input.metadata\n            return 'requiresLength' not in streaming_metadata\n\n        body_size = _get_body_size(request_dict['body'])\n        min_size = config.request_min_compression_size_bytes\n        return min_size <= body_size\n\n    return False\n\n\ndef _is_compressible_type(request_dict):\n    body = request_dict['body']\n    # Coerce dict to a format compatible with compression.\n    if isinstance(body, dict):\n        body = urlencode(body, doseq=True, encoding='utf-8').encode('utf-8')\n        request_dict['body'] = body\n    is_supported_type = isinstance(body, (str, bytes, bytearray))\n    return is_supported_type or hasattr(body, 'read')\n\n\ndef _get_body_size(body):\n    size = determine_content_length(body)\n    if size is None:\n        logger.debug(\n            'Unable to get length of the request body: %s. '\n            'Skipping compression.',\n            body,\n        )\n        size = 0\n    return size\n\n\ndef _gzip_compress_body(body):\n    if isinstance(body, str):\n        return gzip_compress(body.encode('utf-8'))\n    elif isinstance(body, (bytes, bytearray)):\n        return gzip_compress(body)\n    elif hasattr(body, 'read'):\n        if hasattr(body, 'seek') and hasattr(body, 'tell'):\n            current_position = body.tell()\n            compressed_obj = _gzip_compress_fileobj(body)\n            body.seek(current_position)\n            return compressed_obj\n        return _gzip_compress_fileobj(body)\n\n\ndef _gzip_compress_fileobj(body):\n    compressed_obj = io.BytesIO()\n    with GzipFile(fileobj=compressed_obj, mode='wb') as gz:\n        while True:\n            chunk = body.read(8192)\n            if not chunk:\n                break\n            if isinstance(chunk, str):\n                chunk = chunk.encode('utf-8')\n            gz.write(chunk)\n    compressed_obj.seek(0)\n    return compressed_obj\n\n\ndef _set_compression_header(headers, encoding):\n    ce_header = headers.get('Content-Encoding')\n    if ce_header is None:\n        headers['Content-Encoding'] = encoding\n    else:\n        headers['Content-Encoding'] = f'{ce_header},{encoding}'\n\n\nCOMPRESSION_MAPPING = {'gzip': _gzip_compress_body}\n", "botocore/endpoint.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport datetime\nimport logging\nimport os\nimport threading\nimport time\nimport uuid\n\nfrom botocore import parsers\nfrom botocore.awsrequest import create_request_object\nfrom botocore.exceptions import HTTPClientError\nfrom botocore.history import get_global_history_recorder\nfrom botocore.hooks import first_non_none_response\nfrom botocore.httpchecksum import handle_checksum_body\nfrom botocore.httpsession import URLLib3Session\nfrom botocore.response import StreamingBody\nfrom botocore.utils import (\n    get_environ_proxies,\n    is_valid_endpoint_url,\n    is_valid_ipv6_endpoint_url,\n)\n\nlogger = logging.getLogger(__name__)\nhistory_recorder = get_global_history_recorder()\nDEFAULT_TIMEOUT = 60\nMAX_POOL_CONNECTIONS = 10\n\n\ndef convert_to_response_dict(http_response, operation_model):\n    \"\"\"Convert an HTTP response object to a request dict.\n\n    This converts the requests library's HTTP response object to\n    a dictionary.\n\n    :type http_response: botocore.vendored.requests.model.Response\n    :param http_response: The HTTP response from an AWS service request.\n\n    :rtype: dict\n    :return: A response dictionary which will contain the following keys:\n        * headers (dict)\n        * status_code (int)\n        * body (string or file-like object)\n\n    \"\"\"\n    response_dict = {\n        'headers': http_response.headers,\n        'status_code': http_response.status_code,\n        'context': {\n            'operation_name': operation_model.name,\n        },\n    }\n    if response_dict['status_code'] >= 300:\n        response_dict['body'] = http_response.content\n    elif operation_model.has_event_stream_output:\n        response_dict['body'] = http_response.raw\n    elif operation_model.has_streaming_output:\n        length = response_dict['headers'].get('content-length')\n        response_dict['body'] = StreamingBody(http_response.raw, length)\n    else:\n        response_dict['body'] = http_response.content\n    return response_dict\n\n\nclass Endpoint:\n    \"\"\"\n    Represents an endpoint for a particular service in a specific\n    region.  Only an endpoint can make requests.\n\n    :ivar service: The Service object that describes this endpoints\n        service.\n    :ivar host: The fully qualified endpoint hostname.\n    :ivar session: The session object.\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        endpoint_prefix,\n        event_emitter,\n        response_parser_factory=None,\n        http_session=None,\n    ):\n        self._endpoint_prefix = endpoint_prefix\n        self._event_emitter = event_emitter\n        self.host = host\n        self._lock = threading.Lock()\n        if response_parser_factory is None:\n            response_parser_factory = parsers.ResponseParserFactory()\n        self._response_parser_factory = response_parser_factory\n        self.http_session = http_session\n        if self.http_session is None:\n            self.http_session = URLLib3Session()\n\n    def __repr__(self):\n        return f'{self._endpoint_prefix}({self.host})'\n\n    def close(self):\n        self.http_session.close()\n\n    def make_request(self, operation_model, request_dict):\n        logger.debug(\n            \"Making request for %s with params: %s\",\n            operation_model,\n            request_dict,\n        )\n        return self._send_request(request_dict, operation_model)\n\n    def create_request(self, params, operation_model=None):\n        request = create_request_object(params)\n        if operation_model:\n            request.stream_output = any(\n                [\n                    operation_model.has_streaming_output,\n                    operation_model.has_event_stream_output,\n                ]\n            )\n            service_id = operation_model.service_model.service_id.hyphenize()\n            event_name = 'request-created.{service_id}.{op_name}'.format(\n                service_id=service_id, op_name=operation_model.name\n            )\n            self._event_emitter.emit(\n                event_name,\n                request=request,\n                operation_name=operation_model.name,\n            )\n        prepared_request = self.prepare_request(request)\n        return prepared_request\n\n    def _encode_headers(self, headers):\n        # In place encoding of headers to utf-8 if they are unicode.\n        for key, value in headers.items():\n            if isinstance(value, str):\n                headers[key] = value.encode('utf-8')\n\n    def prepare_request(self, request):\n        self._encode_headers(request.headers)\n        return request.prepare()\n\n    def _calculate_ttl(\n        self, response_received_timestamp, date_header, read_timeout\n    ):\n        local_timestamp = datetime.datetime.utcnow()\n        date_conversion = datetime.datetime.strptime(\n            date_header, \"%a, %d %b %Y %H:%M:%S %Z\"\n        )\n        estimated_skew = date_conversion - response_received_timestamp\n        ttl = (\n            local_timestamp\n            + datetime.timedelta(seconds=read_timeout)\n            + estimated_skew\n        )\n        return ttl.strftime('%Y%m%dT%H%M%SZ')\n\n    def _set_ttl(self, retries_context, read_timeout, success_response):\n        response_date_header = success_response[0].headers.get('Date')\n        has_streaming_input = retries_context.get('has_streaming_input')\n        if response_date_header and not has_streaming_input:\n            try:\n                response_received_timestamp = datetime.datetime.utcnow()\n                retries_context['ttl'] = self._calculate_ttl(\n                    response_received_timestamp,\n                    response_date_header,\n                    read_timeout,\n                )\n            except Exception:\n                logger.debug(\n                    \"Exception received when updating retries context with TTL\",\n                    exc_info=True,\n                )\n\n    def _update_retries_context(self, context, attempt, success_response=None):\n        retries_context = context.setdefault('retries', {})\n        retries_context['attempt'] = attempt\n        if 'invocation-id' not in retries_context:\n            retries_context['invocation-id'] = str(uuid.uuid4())\n\n        if success_response:\n            read_timeout = context['client_config'].read_timeout\n            self._set_ttl(retries_context, read_timeout, success_response)\n\n    def _send_request(self, request_dict, operation_model):\n        attempts = 1\n        context = request_dict['context']\n        self._update_retries_context(context, attempts)\n        request = self.create_request(request_dict, operation_model)\n        success_response, exception = self._get_response(\n            request, operation_model, context\n        )\n        while self._needs_retry(\n            attempts,\n            operation_model,\n            request_dict,\n            success_response,\n            exception,\n        ):\n            attempts += 1\n            self._update_retries_context(context, attempts, success_response)\n            # If there is a stream associated with the request, we need\n            # to reset it before attempting to send the request again.\n            # This will ensure that we resend the entire contents of the\n            # body.\n            request.reset_stream()\n            # Create a new request when retried (including a new signature).\n            request = self.create_request(request_dict, operation_model)\n            success_response, exception = self._get_response(\n                request, operation_model, context\n            )\n        if (\n            success_response is not None\n            and 'ResponseMetadata' in success_response[1]\n        ):\n            # We want to share num retries, not num attempts.\n            total_retries = attempts - 1\n            success_response[1]['ResponseMetadata'][\n                'RetryAttempts'\n            ] = total_retries\n        if exception is not None:\n            raise exception\n        else:\n            return success_response\n\n    def _get_response(self, request, operation_model, context):\n        # This will return a tuple of (success_response, exception)\n        # and success_response is itself a tuple of\n        # (http_response, parsed_dict).\n        # If an exception occurs then the success_response is None.\n        # If no exception occurs then exception is None.\n        success_response, exception = self._do_get_response(\n            request, operation_model, context\n        )\n        kwargs_to_emit = {\n            'response_dict': None,\n            'parsed_response': None,\n            'context': context,\n            'exception': exception,\n        }\n        if success_response is not None:\n            http_response, parsed_response = success_response\n            kwargs_to_emit['parsed_response'] = parsed_response\n            kwargs_to_emit['response_dict'] = convert_to_response_dict(\n                http_response, operation_model\n            )\n        service_id = operation_model.service_model.service_id.hyphenize()\n        self._event_emitter.emit(\n            f\"response-received.{service_id}.{operation_model.name}\",\n            **kwargs_to_emit,\n        )\n        return success_response, exception\n\n    def _do_get_response(self, request, operation_model, context):\n        try:\n            logger.debug(\"Sending http request: %s\", request)\n            history_recorder.record(\n                'HTTP_REQUEST',\n                {\n                    'method': request.method,\n                    'headers': request.headers,\n                    'streaming': operation_model.has_streaming_input,\n                    'url': request.url,\n                    'body': request.body,\n                },\n            )\n            service_id = operation_model.service_model.service_id.hyphenize()\n            event_name = f\"before-send.{service_id}.{operation_model.name}\"\n            responses = self._event_emitter.emit(event_name, request=request)\n            http_response = first_non_none_response(responses)\n            if http_response is None:\n                http_response = self._send(request)\n        except HTTPClientError as e:\n            return (None, e)\n        except Exception as e:\n            logger.debug(\n                \"Exception received when sending HTTP request.\", exc_info=True\n            )\n            return (None, e)\n        # This returns the http_response and the parsed_data.\n        response_dict = convert_to_response_dict(\n            http_response, operation_model\n        )\n        handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n\n        http_response_record_dict = response_dict.copy()\n        http_response_record_dict[\n            'streaming'\n        ] = operation_model.has_streaming_output\n        history_recorder.record('HTTP_RESPONSE', http_response_record_dict)\n\n        protocol = operation_model.metadata['protocol']\n        parser = self._response_parser_factory.create_parser(protocol)\n        parsed_response = parser.parse(\n            response_dict, operation_model.output_shape\n        )\n        # Do a second parsing pass to pick up on any modeled error fields\n        # NOTE: Ideally, we would push this down into the parser classes but\n        # they currently have no reference to the operation or service model\n        # The parsers should probably take the operation model instead of\n        # output shape but we can't change that now\n        if http_response.status_code >= 300:\n            self._add_modeled_error_fields(\n                response_dict,\n                parsed_response,\n                operation_model,\n                parser,\n            )\n        history_recorder.record('PARSED_RESPONSE', parsed_response)\n        return (http_response, parsed_response), None\n\n    def _add_modeled_error_fields(\n        self,\n        response_dict,\n        parsed_response,\n        operation_model,\n        parser,\n    ):\n        error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n        if error_code is None:\n            return\n        service_model = operation_model.service_model\n        error_shape = service_model.shape_for_error_code(error_code)\n        if error_shape is None:\n            return\n        modeled_parse = parser.parse(response_dict, error_shape)\n        # TODO: avoid naming conflicts with ResponseMetadata and Error\n        parsed_response.update(modeled_parse)\n\n    def _needs_retry(\n        self,\n        attempts,\n        operation_model,\n        request_dict,\n        response=None,\n        caught_exception=None,\n    ):\n        service_id = operation_model.service_model.service_id.hyphenize()\n        event_name = f\"needs-retry.{service_id}.{operation_model.name}\"\n        responses = self._event_emitter.emit(\n            event_name,\n            response=response,\n            endpoint=self,\n            operation=operation_model,\n            attempts=attempts,\n            caught_exception=caught_exception,\n            request_dict=request_dict,\n        )\n        handler_response = first_non_none_response(responses)\n        if handler_response is None:\n            return False\n        else:\n            # Request needs to be retried, and we need to sleep\n            # for the specified number of times.\n            logger.debug(\n                \"Response received to retry, sleeping for %s seconds\",\n                handler_response,\n            )\n            time.sleep(handler_response)\n            return True\n\n    def _send(self, request):\n        return self.http_session.send(request)\n\n\nclass EndpointCreator:\n    def __init__(self, event_emitter):\n        self._event_emitter = event_emitter\n\n    def create_endpoint(\n        self,\n        service_model,\n        region_name,\n        endpoint_url,\n        verify=None,\n        response_parser_factory=None,\n        timeout=DEFAULT_TIMEOUT,\n        max_pool_connections=MAX_POOL_CONNECTIONS,\n        http_session_cls=URLLib3Session,\n        proxies=None,\n        socket_options=None,\n        client_cert=None,\n        proxies_config=None,\n    ):\n        if not is_valid_endpoint_url(\n            endpoint_url\n        ) and not is_valid_ipv6_endpoint_url(endpoint_url):\n            raise ValueError(\"Invalid endpoint: %s\" % endpoint_url)\n\n        if proxies is None:\n            proxies = self._get_proxies(endpoint_url)\n        endpoint_prefix = service_model.endpoint_prefix\n\n        logger.debug('Setting %s timeout as %s', endpoint_prefix, timeout)\n        http_session = http_session_cls(\n            timeout=timeout,\n            proxies=proxies,\n            verify=self._get_verify_value(verify),\n            max_pool_connections=max_pool_connections,\n            socket_options=socket_options,\n            client_cert=client_cert,\n            proxies_config=proxies_config,\n        )\n\n        return Endpoint(\n            endpoint_url,\n            endpoint_prefix=endpoint_prefix,\n            event_emitter=self._event_emitter,\n            response_parser_factory=response_parser_factory,\n            http_session=http_session,\n        )\n\n    def _get_proxies(self, url):\n        # We could also support getting proxies from a config file,\n        # but for now proxy support is taken from the environment.\n        return get_environ_proxies(url)\n\n    def _get_verify_value(self, verify):\n        # This is to account for:\n        # https://github.com/kennethreitz/requests/issues/1436\n        # where we need to honor REQUESTS_CA_BUNDLE because we're creating our\n        # own request objects.\n        # First, if verify is not None, then the user explicitly specified\n        # a value so this automatically wins.\n        if verify is not None:\n            return verify\n        # Otherwise use the value from REQUESTS_CA_BUNDLE, or default to\n        # True if the env var does not exist.\n        return os.environ.get('REQUESTS_CA_BUNDLE', True)\n", "botocore/auth.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport calendar\nimport datetime\nimport functools\nimport hmac\nimport json\nimport logging\nimport time\nfrom collections.abc import Mapping\nfrom email.utils import formatdate\nfrom hashlib import sha1, sha256\nfrom operator import itemgetter\n\nfrom botocore.compat import (\n    HAS_CRT,\n    HTTPHeaders,\n    encodebytes,\n    ensure_unicode,\n    parse_qs,\n    quote,\n    unquote,\n    urlsplit,\n    urlunsplit,\n)\nfrom botocore.exceptions import NoAuthTokenError, NoCredentialsError\nfrom botocore.utils import (\n    is_valid_ipv6_endpoint_url,\n    normalize_url_path,\n    percent_encode_sequence,\n)\n\n# Imports for backwards compatibility\nfrom botocore.compat import MD5_AVAILABLE  # noqa\n\n\nlogger = logging.getLogger(__name__)\n\n\nEMPTY_SHA256_HASH = (\n    'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n)\n# This is the buffer size used when calculating sha256 checksums.\n# Experimenting with various buffer sizes showed that this value generally\n# gave the best result (in terms of performance).\nPAYLOAD_BUFFER = 1024 * 1024\nISO8601 = '%Y-%m-%dT%H:%M:%SZ'\nSIGV4_TIMESTAMP = '%Y%m%dT%H%M%SZ'\nSIGNED_HEADERS_BLACKLIST = [\n    'expect',\n    'user-agent',\n    'x-amzn-trace-id',\n]\nUNSIGNED_PAYLOAD = 'UNSIGNED-PAYLOAD'\nSTREAMING_UNSIGNED_PAYLOAD_TRAILER = 'STREAMING-UNSIGNED-PAYLOAD-TRAILER'\n\n\ndef _host_from_url(url):\n    # Given URL, derive value for host header. Ensure that value:\n    # 1) is lowercase\n    # 2) excludes port, if it was the default port\n    # 3) excludes userinfo\n    url_parts = urlsplit(url)\n    host = url_parts.hostname  # urlsplit's hostname is always lowercase\n    if is_valid_ipv6_endpoint_url(url):\n        host = f'[{host}]'\n    default_ports = {\n        'http': 80,\n        'https': 443,\n    }\n    if url_parts.port is not None:\n        if url_parts.port != default_ports.get(url_parts.scheme):\n            host = '%s:%d' % (host, url_parts.port)\n    return host\n\n\ndef _get_body_as_dict(request):\n    # For query services, request.data is form-encoded and is already a\n    # dict, but for other services such as rest-json it could be a json\n    # string or bytes. In those cases we attempt to load the data as a\n    # dict.\n    data = request.data\n    if isinstance(data, bytes):\n        data = json.loads(data.decode('utf-8'))\n    elif isinstance(data, str):\n        data = json.loads(data)\n    return data\n\n\nclass BaseSigner:\n    REQUIRES_REGION = False\n    REQUIRES_TOKEN = False\n\n    def add_auth(self, request):\n        raise NotImplementedError(\"add_auth\")\n\n\nclass TokenSigner(BaseSigner):\n    REQUIRES_TOKEN = True\n    \"\"\"\n    Signers that expect an authorization token to perform the authorization\n    \"\"\"\n\n    def __init__(self, auth_token):\n        self.auth_token = auth_token\n\n\nclass SigV2Auth(BaseSigner):\n    \"\"\"\n    Sign a request with Signature V2.\n    \"\"\"\n\n    def __init__(self, credentials):\n        self.credentials = credentials\n\n    def calc_signature(self, request, params):\n        logger.debug(\"Calculating signature using v2 auth.\")\n        split = urlsplit(request.url)\n        path = split.path\n        if len(path) == 0:\n            path = '/'\n        string_to_sign = f\"{request.method}\\n{split.netloc}\\n{path}\\n\"\n        lhmac = hmac.new(\n            self.credentials.secret_key.encode(\"utf-8\"), digestmod=sha256\n        )\n        pairs = []\n        for key in sorted(params):\n            # Any previous signature should not be a part of this\n            # one, so we skip that particular key. This prevents\n            # issues during retries.\n            if key == 'Signature':\n                continue\n            value = str(params[key])\n            quoted_key = quote(key.encode('utf-8'), safe='')\n            quoted_value = quote(value.encode('utf-8'), safe='-_~')\n            pairs.append(f'{quoted_key}={quoted_value}')\n        qs = '&'.join(pairs)\n        string_to_sign += qs\n        logger.debug('String to sign: %s', string_to_sign)\n        lhmac.update(string_to_sign.encode('utf-8'))\n        b64 = base64.b64encode(lhmac.digest()).strip().decode('utf-8')\n        return (qs, b64)\n\n    def add_auth(self, request):\n        # The auth handler is the last thing called in the\n        # preparation phase of a prepared request.\n        # Because of this we have to parse the query params\n        # from the request body so we can update them with\n        # the sigv2 auth params.\n        if self.credentials is None:\n            raise NoCredentialsError()\n        if request.data:\n            # POST\n            params = request.data\n        else:\n            # GET\n            params = request.params\n        params['AWSAccessKeyId'] = self.credentials.access_key\n        params['SignatureVersion'] = '2'\n        params['SignatureMethod'] = 'HmacSHA256'\n        params['Timestamp'] = time.strftime(ISO8601, time.gmtime())\n        if self.credentials.token:\n            params['SecurityToken'] = self.credentials.token\n        qs, signature = self.calc_signature(request, params)\n        params['Signature'] = signature\n        return request\n\n\nclass SigV3Auth(BaseSigner):\n    def __init__(self, credentials):\n        self.credentials = credentials\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError()\n        if 'Date' in request.headers:\n            del request.headers['Date']\n        request.headers['Date'] = formatdate(usegmt=True)\n        if self.credentials.token:\n            if 'X-Amz-Security-Token' in request.headers:\n                del request.headers['X-Amz-Security-Token']\n            request.headers['X-Amz-Security-Token'] = self.credentials.token\n        new_hmac = hmac.new(\n            self.credentials.secret_key.encode('utf-8'), digestmod=sha256\n        )\n        new_hmac.update(request.headers['Date'].encode('utf-8'))\n        encoded_signature = encodebytes(new_hmac.digest()).strip()\n        signature = (\n            f\"AWS3-HTTPS AWSAccessKeyId={self.credentials.access_key},\"\n            f\"Algorithm=HmacSHA256,Signature={encoded_signature.decode('utf-8')}\"\n        )\n        if 'X-Amzn-Authorization' in request.headers:\n            del request.headers['X-Amzn-Authorization']\n        request.headers['X-Amzn-Authorization'] = signature\n\n\nclass SigV4Auth(BaseSigner):\n    \"\"\"\n    Sign a request with Signature V4.\n    \"\"\"\n\n    REQUIRES_REGION = True\n\n    def __init__(self, credentials, service_name, region_name):\n        self.credentials = credentials\n        # We initialize these value here so the unit tests can have\n        # valid values.  But these will get overriden in ``add_auth``\n        # later for real requests.\n        self._region_name = region_name\n        self._service_name = service_name\n\n    def _sign(self, key, msg, hex=False):\n        if hex:\n            sig = hmac.new(key, msg.encode('utf-8'), sha256).hexdigest()\n        else:\n            sig = hmac.new(key, msg.encode('utf-8'), sha256).digest()\n        return sig\n\n    def headers_to_sign(self, request):\n        \"\"\"\n        Select the headers from the request that need to be included\n        in the StringToSign.\n        \"\"\"\n        header_map = HTTPHeaders()\n        for name, value in request.headers.items():\n            lname = name.lower()\n            if lname not in SIGNED_HEADERS_BLACKLIST:\n                header_map[lname] = value\n        if 'host' not in header_map:\n            # TODO: We should set the host ourselves, instead of relying on our\n            # HTTP client to set it for us.\n            header_map['host'] = _host_from_url(request.url)\n        return header_map\n\n    def canonical_query_string(self, request):\n        # The query string can come from two parts.  One is the\n        # params attribute of the request.  The other is from the request\n        # url (in which case we have to re-split the url into its components\n        # and parse out the query string component).\n        if request.params:\n            return self._canonical_query_string_params(request.params)\n        else:\n            return self._canonical_query_string_url(urlsplit(request.url))\n\n    def _canonical_query_string_params(self, params):\n        # [(key, value), (key2, value2)]\n        key_val_pairs = []\n        if isinstance(params, Mapping):\n            params = params.items()\n        for key, value in params:\n            key_val_pairs.append(\n                (quote(key, safe='-_.~'), quote(str(value), safe='-_.~'))\n            )\n        sorted_key_vals = []\n        # Sort by the URI-encoded key names, and in the case of\n        # repeated keys, then sort by the value.\n        for key, value in sorted(key_val_pairs):\n            sorted_key_vals.append(f'{key}={value}')\n        canonical_query_string = '&'.join(sorted_key_vals)\n        return canonical_query_string\n\n    def _canonical_query_string_url(self, parts):\n        canonical_query_string = ''\n        if parts.query:\n            # [(key, value), (key2, value2)]\n            key_val_pairs = []\n            for pair in parts.query.split('&'):\n                key, _, value = pair.partition('=')\n                key_val_pairs.append((key, value))\n            sorted_key_vals = []\n            # Sort by the URI-encoded key names, and in the case of\n            # repeated keys, then sort by the value.\n            for key, value in sorted(key_val_pairs):\n                sorted_key_vals.append(f'{key}={value}')\n            canonical_query_string = '&'.join(sorted_key_vals)\n        return canonical_query_string\n\n    def canonical_headers(self, headers_to_sign):\n        \"\"\"\n        Return the headers that need to be included in the StringToSign\n        in their canonical form by converting all header keys to lower\n        case, sorting them in alphabetical order and then joining\n        them into a string, separated by newlines.\n        \"\"\"\n        headers = []\n        sorted_header_names = sorted(set(headers_to_sign))\n        for key in sorted_header_names:\n            value = ','.join(\n                self._header_value(v) for v in headers_to_sign.get_all(key)\n            )\n            headers.append(f'{key}:{ensure_unicode(value)}')\n        return '\\n'.join(headers)\n\n    def _header_value(self, value):\n        # From the sigv4 docs:\n        # Lowercase(HeaderName) + ':' + Trimall(HeaderValue)\n        #\n        # The Trimall function removes excess white space before and after\n        # values, and converts sequential spaces to a single space.\n        return ' '.join(value.split())\n\n    def signed_headers(self, headers_to_sign):\n        headers = sorted(n.lower().strip() for n in set(headers_to_sign))\n        return ';'.join(headers)\n\n    def _is_streaming_checksum_payload(self, request):\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        return isinstance(algorithm, dict) and algorithm.get('in') == 'trailer'\n\n    def payload(self, request):\n        if self._is_streaming_checksum_payload(request):\n            return STREAMING_UNSIGNED_PAYLOAD_TRAILER\n        elif not self._should_sha256_sign_payload(request):\n            # When payload signing is disabled, we use this static string in\n            # place of the payload checksum.\n            return UNSIGNED_PAYLOAD\n        request_body = request.body\n        if request_body and hasattr(request_body, 'seek'):\n            position = request_body.tell()\n            read_chunksize = functools.partial(\n                request_body.read, PAYLOAD_BUFFER\n            )\n            checksum = sha256()\n            for chunk in iter(read_chunksize, b''):\n                checksum.update(chunk)\n            hex_checksum = checksum.hexdigest()\n            request_body.seek(position)\n            return hex_checksum\n        elif request_body:\n            # The request serialization has ensured that\n            # request.body is a bytes() type.\n            return sha256(request_body).hexdigest()\n        else:\n            return EMPTY_SHA256_HASH\n\n    def _should_sha256_sign_payload(self, request):\n        # Payloads will always be signed over insecure connections.\n        if not request.url.startswith('https'):\n            return True\n\n        # Certain operations may have payload signing disabled by default.\n        # Since we don't have access to the operation model, we pass in this\n        # bit of metadata through the request context.\n        return request.context.get('payload_signing_enabled', True)\n\n    def canonical_request(self, request):\n        cr = [request.method.upper()]\n        path = self._normalize_url_path(urlsplit(request.url).path)\n        cr.append(path)\n        cr.append(self.canonical_query_string(request))\n        headers_to_sign = self.headers_to_sign(request)\n        cr.append(self.canonical_headers(headers_to_sign) + '\\n')\n        cr.append(self.signed_headers(headers_to_sign))\n        if 'X-Amz-Content-SHA256' in request.headers:\n            body_checksum = request.headers['X-Amz-Content-SHA256']\n        else:\n            body_checksum = self.payload(request)\n        cr.append(body_checksum)\n        return '\\n'.join(cr)\n\n    def _normalize_url_path(self, path):\n        normalized_path = quote(normalize_url_path(path), safe='/~')\n        return normalized_path\n\n    def scope(self, request):\n        scope = [self.credentials.access_key]\n        scope.append(request.context['timestamp'][0:8])\n        scope.append(self._region_name)\n        scope.append(self._service_name)\n        scope.append('aws4_request')\n        return '/'.join(scope)\n\n    def credential_scope(self, request):\n        scope = []\n        scope.append(request.context['timestamp'][0:8])\n        scope.append(self._region_name)\n        scope.append(self._service_name)\n        scope.append('aws4_request')\n        return '/'.join(scope)\n\n    def string_to_sign(self, request, canonical_request):\n        \"\"\"\n        Return the canonical StringToSign as well as a dict\n        containing the original version of all headers that\n        were included in the StringToSign.\n        \"\"\"\n        sts = ['AWS4-HMAC-SHA256']\n        sts.append(request.context['timestamp'])\n        sts.append(self.credential_scope(request))\n        sts.append(sha256(canonical_request.encode('utf-8')).hexdigest())\n        return '\\n'.join(sts)\n\n    def signature(self, string_to_sign, request):\n        key = self.credentials.secret_key\n        k_date = self._sign(\n            (f\"AWS4{key}\").encode(), request.context[\"timestamp\"][0:8]\n        )\n        k_region = self._sign(k_date, self._region_name)\n        k_service = self._sign(k_region, self._service_name)\n        k_signing = self._sign(k_service, 'aws4_request')\n        return self._sign(k_signing, string_to_sign, hex=True)\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError()\n        datetime_now = datetime.datetime.utcnow()\n        request.context['timestamp'] = datetime_now.strftime(SIGV4_TIMESTAMP)\n        # This could be a retry.  Make sure the previous\n        # authorization header is removed first.\n        self._modify_request_before_signing(request)\n        canonical_request = self.canonical_request(request)\n        logger.debug(\"Calculating signature using v4 auth.\")\n        logger.debug('CanonicalRequest:\\n%s', canonical_request)\n        string_to_sign = self.string_to_sign(request, canonical_request)\n        logger.debug('StringToSign:\\n%s', string_to_sign)\n        signature = self.signature(string_to_sign, request)\n        logger.debug('Signature:\\n%s', signature)\n\n        self._inject_signature_to_request(request, signature)\n\n    def _inject_signature_to_request(self, request, signature):\n        auth_str = ['AWS4-HMAC-SHA256 Credential=%s' % self.scope(request)]\n        headers_to_sign = self.headers_to_sign(request)\n        auth_str.append(\n            f\"SignedHeaders={self.signed_headers(headers_to_sign)}\"\n        )\n        auth_str.append('Signature=%s' % signature)\n        request.headers['Authorization'] = ', '.join(auth_str)\n        return request\n\n    def _modify_request_before_signing(self, request):\n        if 'Authorization' in request.headers:\n            del request.headers['Authorization']\n        self._set_necessary_date_headers(request)\n        if self.credentials.token:\n            if 'X-Amz-Security-Token' in request.headers:\n                del request.headers['X-Amz-Security-Token']\n            request.headers['X-Amz-Security-Token'] = self.credentials.token\n\n        if not request.context.get('payload_signing_enabled', True):\n            if 'X-Amz-Content-SHA256' in request.headers:\n                del request.headers['X-Amz-Content-SHA256']\n            request.headers['X-Amz-Content-SHA256'] = UNSIGNED_PAYLOAD\n\n    def _set_necessary_date_headers(self, request):\n        # The spec allows for either the Date _or_ the X-Amz-Date value to be\n        # used so we check both.  If there's a Date header, we use the date\n        # header.  Otherwise we use the X-Amz-Date header.\n        if 'Date' in request.headers:\n            del request.headers['Date']\n            datetime_timestamp = datetime.datetime.strptime(\n                request.context['timestamp'], SIGV4_TIMESTAMP\n            )\n            request.headers['Date'] = formatdate(\n                int(calendar.timegm(datetime_timestamp.timetuple()))\n            )\n            if 'X-Amz-Date' in request.headers:\n                del request.headers['X-Amz-Date']\n        else:\n            if 'X-Amz-Date' in request.headers:\n                del request.headers['X-Amz-Date']\n            request.headers['X-Amz-Date'] = request.context['timestamp']\n\n\nclass S3SigV4Auth(SigV4Auth):\n    def _modify_request_before_signing(self, request):\n        super()._modify_request_before_signing(request)\n        if 'X-Amz-Content-SHA256' in request.headers:\n            del request.headers['X-Amz-Content-SHA256']\n\n        request.headers['X-Amz-Content-SHA256'] = self.payload(request)\n\n    def _should_sha256_sign_payload(self, request):\n        # S3 allows optional body signing, so to minimize the performance\n        # impact, we opt to not SHA256 sign the body on streaming uploads,\n        # provided that we're on https.\n        client_config = request.context.get('client_config')\n        s3_config = getattr(client_config, 's3', None)\n\n        # The config could be None if it isn't set, or if the customer sets it\n        # to None.\n        if s3_config is None:\n            s3_config = {}\n\n        # The explicit configuration takes precedence over any implicit\n        # configuration.\n        sign_payload = s3_config.get('payload_signing_enabled', None)\n        if sign_payload is not None:\n            return sign_payload\n\n        # We require that both a checksum be present and https be enabled\n        # to implicitly disable body signing. The combination of TLS and\n        # a checksum is sufficiently secure and durable for us to be\n        # confident in the request without body signing.\n        checksum_header = 'Content-MD5'\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        if isinstance(algorithm, dict) and algorithm.get('in') == 'header':\n            checksum_header = algorithm['name']\n        if (\n            not request.url.startswith(\"https\")\n            or checksum_header not in request.headers\n        ):\n            return True\n\n        # If the input is streaming we disable body signing by default.\n        if request.context.get('has_streaming_input', False):\n            return False\n\n        # If the S3-specific checks had no results, delegate to the generic\n        # checks.\n        return super()._should_sha256_sign_payload(request)\n\n    def _normalize_url_path(self, path):\n        # For S3, we do not normalize the path.\n        return path\n\n\nclass S3ExpressAuth(S3SigV4Auth):\n    REQUIRES_IDENTITY_CACHE = True\n\n    def __init__(\n        self, credentials, service_name, region_name, *, identity_cache\n    ):\n        super().__init__(credentials, service_name, region_name)\n        self._identity_cache = identity_cache\n\n    def add_auth(self, request):\n        super().add_auth(request)\n\n    def _modify_request_before_signing(self, request):\n        super()._modify_request_before_signing(request)\n        if 'x-amz-s3session-token' not in request.headers:\n            request.headers['x-amz-s3session-token'] = self.credentials.token\n        # S3Express does not support STS' X-Amz-Security-Token\n        if 'X-Amz-Security-Token' in request.headers:\n            del request.headers['X-Amz-Security-Token']\n\n\nclass S3ExpressPostAuth(S3ExpressAuth):\n    REQUIRES_IDENTITY_CACHE = True\n\n    def add_auth(self, request):\n        datetime_now = datetime.datetime.utcnow()\n        request.context['timestamp'] = datetime_now.strftime(SIGV4_TIMESTAMP)\n\n        fields = {}\n        if request.context.get('s3-presign-post-fields', None) is not None:\n            fields = request.context['s3-presign-post-fields']\n\n        policy = {}\n        conditions = []\n        if request.context.get('s3-presign-post-policy', None) is not None:\n            policy = request.context['s3-presign-post-policy']\n            if policy.get('conditions', None) is not None:\n                conditions = policy['conditions']\n\n        policy['conditions'] = conditions\n\n        fields['x-amz-algorithm'] = 'AWS4-HMAC-SHA256'\n        fields['x-amz-credential'] = self.scope(request)\n        fields['x-amz-date'] = request.context['timestamp']\n\n        conditions.append({'x-amz-algorithm': 'AWS4-HMAC-SHA256'})\n        conditions.append({'x-amz-credential': self.scope(request)})\n        conditions.append({'x-amz-date': request.context['timestamp']})\n\n        if self.credentials.token is not None:\n            fields['X-Amz-S3session-Token'] = self.credentials.token\n            conditions.append(\n                {'X-Amz-S3session-Token': self.credentials.token}\n            )\n\n        # Dump the base64 encoded policy into the fields dictionary.\n        fields['policy'] = base64.b64encode(\n            json.dumps(policy).encode('utf-8')\n        ).decode('utf-8')\n\n        fields['x-amz-signature'] = self.signature(fields['policy'], request)\n\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n\nclass S3ExpressQueryAuth(S3ExpressAuth):\n    DEFAULT_EXPIRES = 300\n    REQUIRES_IDENTITY_CACHE = True\n\n    def __init__(\n        self,\n        credentials,\n        service_name,\n        region_name,\n        *,\n        identity_cache,\n        expires=DEFAULT_EXPIRES,\n    ):\n        super().__init__(\n            credentials,\n            service_name,\n            region_name,\n            identity_cache=identity_cache,\n        )\n        self._expires = expires\n\n    def _modify_request_before_signing(self, request):\n        # We automatically set this header, so if it's the auto-set value we\n        # want to get rid of it since it doesn't make sense for presigned urls.\n        content_type = request.headers.get('content-type')\n        blocklisted_content_type = (\n            'application/x-www-form-urlencoded; charset=utf-8'\n        )\n        if content_type == blocklisted_content_type:\n            del request.headers['content-type']\n\n        # Note that we're not including X-Amz-Signature.\n        # From the docs: \"The Canonical Query String must include all the query\n        # parameters from the preceding table except for X-Amz-Signature.\n        signed_headers = self.signed_headers(self.headers_to_sign(request))\n\n        auth_params = {\n            'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n            'X-Amz-Credential': self.scope(request),\n            'X-Amz-Date': request.context['timestamp'],\n            'X-Amz-Expires': self._expires,\n            'X-Amz-SignedHeaders': signed_headers,\n        }\n        if self.credentials.token is not None:\n            auth_params['X-Amz-S3session-Token'] = self.credentials.token\n        # Now parse the original query string to a dict, inject our new query\n        # params, and serialize back to a query string.\n        url_parts = urlsplit(request.url)\n        # parse_qs makes each value a list, but in our case we know we won't\n        # have repeated keys so we know we have single element lists which we\n        # can convert back to scalar values.\n        query_string_parts = parse_qs(url_parts.query, keep_blank_values=True)\n        query_dict = {k: v[0] for k, v in query_string_parts.items()}\n\n        if request.params:\n            query_dict.update(request.params)\n            request.params = {}\n        # The spec is particular about this.  It *has* to be:\n        # https://<endpoint>?<operation params>&<auth params>\n        # You can't mix the two types of params together, i.e just keep doing\n        # new_query_params.update(op_params)\n        # new_query_params.update(auth_params)\n        # percent_encode_sequence(new_query_params)\n        operation_params = ''\n        if request.data:\n            # We also need to move the body params into the query string. To\n            # do this, we first have to convert it to a dict.\n            query_dict.update(_get_body_as_dict(request))\n            request.data = ''\n        if query_dict:\n            operation_params = percent_encode_sequence(query_dict) + '&'\n        new_query_string = (\n            f\"{operation_params}{percent_encode_sequence(auth_params)}\"\n        )\n        # url_parts is a tuple (and therefore immutable) so we need to create\n        # a new url_parts with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        p = url_parts\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n    def _inject_signature_to_request(self, request, signature):\n        # Rather than calculating an \"Authorization\" header, for the query\n        # param quth, we just append an 'X-Amz-Signature' param to the end\n        # of the query string.\n        request.url += '&X-Amz-Signature=%s' % signature\n\n    def _normalize_url_path(self, path):\n        # For S3, we do not normalize the path.\n        return path\n\n    def payload(self, request):\n        # From the doc link above:\n        # \"You don't include a payload hash in the Canonical Request, because\n        # when you create a presigned URL, you don't know anything about the\n        # payload. Instead, you use a constant string \"UNSIGNED-PAYLOAD\".\n        return UNSIGNED_PAYLOAD\n\n\nclass SigV4QueryAuth(SigV4Auth):\n    DEFAULT_EXPIRES = 3600\n\n    def __init__(\n        self, credentials, service_name, region_name, expires=DEFAULT_EXPIRES\n    ):\n        super().__init__(credentials, service_name, region_name)\n        self._expires = expires\n\n    def _modify_request_before_signing(self, request):\n        # We automatically set this header, so if it's the auto-set value we\n        # want to get rid of it since it doesn't make sense for presigned urls.\n        content_type = request.headers.get('content-type')\n        blacklisted_content_type = (\n            'application/x-www-form-urlencoded; charset=utf-8'\n        )\n        if content_type == blacklisted_content_type:\n            del request.headers['content-type']\n\n        # Note that we're not including X-Amz-Signature.\n        # From the docs: \"The Canonical Query String must include all the query\n        # parameters from the preceding table except for X-Amz-Signature.\n        signed_headers = self.signed_headers(self.headers_to_sign(request))\n\n        auth_params = {\n            'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n            'X-Amz-Credential': self.scope(request),\n            'X-Amz-Date': request.context['timestamp'],\n            'X-Amz-Expires': self._expires,\n            'X-Amz-SignedHeaders': signed_headers,\n        }\n        if self.credentials.token is not None:\n            auth_params['X-Amz-Security-Token'] = self.credentials.token\n        # Now parse the original query string to a dict, inject our new query\n        # params, and serialize back to a query string.\n        url_parts = urlsplit(request.url)\n        # parse_qs makes each value a list, but in our case we know we won't\n        # have repeated keys so we know we have single element lists which we\n        # can convert back to scalar values.\n        query_string_parts = parse_qs(url_parts.query, keep_blank_values=True)\n        query_dict = {k: v[0] for k, v in query_string_parts.items()}\n\n        if request.params:\n            query_dict.update(request.params)\n            request.params = {}\n        # The spec is particular about this.  It *has* to be:\n        # https://<endpoint>?<operation params>&<auth params>\n        # You can't mix the two types of params together, i.e just keep doing\n        # new_query_params.update(op_params)\n        # new_query_params.update(auth_params)\n        # percent_encode_sequence(new_query_params)\n        operation_params = ''\n        if request.data:\n            # We also need to move the body params into the query string. To\n            # do this, we first have to convert it to a dict.\n            query_dict.update(_get_body_as_dict(request))\n            request.data = ''\n        if query_dict:\n            operation_params = percent_encode_sequence(query_dict) + '&'\n        new_query_string = (\n            f\"{operation_params}{percent_encode_sequence(auth_params)}\"\n        )\n        # url_parts is a tuple (and therefore immutable) so we need to create\n        # a new url_parts with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        p = url_parts\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n    def _inject_signature_to_request(self, request, signature):\n        # Rather than calculating an \"Authorization\" header, for the query\n        # param quth, we just append an 'X-Amz-Signature' param to the end\n        # of the query string.\n        request.url += '&X-Amz-Signature=%s' % signature\n\n\nclass S3SigV4QueryAuth(SigV4QueryAuth):\n    \"\"\"S3 SigV4 auth using query parameters.\n\n    This signer will sign a request using query parameters and signature\n    version 4, i.e a \"presigned url\" signer.\n\n    Based off of:\n\n    http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html\n\n    \"\"\"\n\n    def _normalize_url_path(self, path):\n        # For S3, we do not normalize the path.\n        return path\n\n    def payload(self, request):\n        # From the doc link above:\n        # \"You don't include a payload hash in the Canonical Request, because\n        # when you create a presigned URL, you don't know anything about the\n        # payload. Instead, you use a constant string \"UNSIGNED-PAYLOAD\".\n        return UNSIGNED_PAYLOAD\n\n\nclass S3SigV4PostAuth(SigV4Auth):\n    \"\"\"\n    Presigns a s3 post\n\n    Implementation doc here:\n    http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html\n    \"\"\"\n\n    def add_auth(self, request):\n        datetime_now = datetime.datetime.utcnow()\n        request.context['timestamp'] = datetime_now.strftime(SIGV4_TIMESTAMP)\n\n        fields = {}\n        if request.context.get('s3-presign-post-fields', None) is not None:\n            fields = request.context['s3-presign-post-fields']\n\n        policy = {}\n        conditions = []\n        if request.context.get('s3-presign-post-policy', None) is not None:\n            policy = request.context['s3-presign-post-policy']\n            if policy.get('conditions', None) is not None:\n                conditions = policy['conditions']\n\n        policy['conditions'] = conditions\n\n        fields['x-amz-algorithm'] = 'AWS4-HMAC-SHA256'\n        fields['x-amz-credential'] = self.scope(request)\n        fields['x-amz-date'] = request.context['timestamp']\n\n        conditions.append({'x-amz-algorithm': 'AWS4-HMAC-SHA256'})\n        conditions.append({'x-amz-credential': self.scope(request)})\n        conditions.append({'x-amz-date': request.context['timestamp']})\n\n        if self.credentials.token is not None:\n            fields['x-amz-security-token'] = self.credentials.token\n            conditions.append({'x-amz-security-token': self.credentials.token})\n\n        # Dump the base64 encoded policy into the fields dictionary.\n        fields['policy'] = base64.b64encode(\n            json.dumps(policy).encode('utf-8')\n        ).decode('utf-8')\n\n        fields['x-amz-signature'] = self.signature(fields['policy'], request)\n\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n\nclass HmacV1Auth(BaseSigner):\n    # List of Query String Arguments of Interest\n    QSAOfInterest = [\n        'accelerate',\n        'acl',\n        'cors',\n        'defaultObjectAcl',\n        'location',\n        'logging',\n        'partNumber',\n        'policy',\n        'requestPayment',\n        'torrent',\n        'versioning',\n        'versionId',\n        'versions',\n        'website',\n        'uploads',\n        'uploadId',\n        'response-content-type',\n        'response-content-language',\n        'response-expires',\n        'response-cache-control',\n        'response-content-disposition',\n        'response-content-encoding',\n        'delete',\n        'lifecycle',\n        'tagging',\n        'restore',\n        'storageClass',\n        'notification',\n        'replication',\n        'requestPayment',\n        'analytics',\n        'metrics',\n        'inventory',\n        'select',\n        'select-type',\n        'object-lock',\n    ]\n\n    def __init__(self, credentials, service_name=None, region_name=None):\n        self.credentials = credentials\n\n    def sign_string(self, string_to_sign):\n        new_hmac = hmac.new(\n            self.credentials.secret_key.encode('utf-8'), digestmod=sha1\n        )\n        new_hmac.update(string_to_sign.encode('utf-8'))\n        return encodebytes(new_hmac.digest()).strip().decode('utf-8')\n\n    def canonical_standard_headers(self, headers):\n        interesting_headers = ['content-md5', 'content-type', 'date']\n        hoi = []\n        if 'Date' in headers:\n            del headers['Date']\n        headers['Date'] = self._get_date()\n        for ih in interesting_headers:\n            found = False\n            for key in headers:\n                lk = key.lower()\n                if headers[key] is not None and lk == ih:\n                    hoi.append(headers[key].strip())\n                    found = True\n            if not found:\n                hoi.append('')\n        return '\\n'.join(hoi)\n\n    def canonical_custom_headers(self, headers):\n        hoi = []\n        custom_headers = {}\n        for key in headers:\n            lk = key.lower()\n            if headers[key] is not None:\n                if lk.startswith('x-amz-'):\n                    custom_headers[lk] = ','.join(\n                        v.strip() for v in headers.get_all(key)\n                    )\n        sorted_header_keys = sorted(custom_headers.keys())\n        for key in sorted_header_keys:\n            hoi.append(f\"{key}:{custom_headers[key]}\")\n        return '\\n'.join(hoi)\n\n    def unquote_v(self, nv):\n        \"\"\"\n        TODO: Do we need this?\n        \"\"\"\n        if len(nv) == 1:\n            return nv\n        else:\n            return (nv[0], unquote(nv[1]))\n\n    def canonical_resource(self, split, auth_path=None):\n        # don't include anything after the first ? in the resource...\n        # unless it is one of the QSA of interest, defined above\n        # NOTE:\n        # The path in the canonical resource should always be the\n        # full path including the bucket name, even for virtual-hosting\n        # style addressing.  The ``auth_path`` keeps track of the full\n        # path for the canonical resource and would be passed in if\n        # the client was using virtual-hosting style.\n        if auth_path is not None:\n            buf = auth_path\n        else:\n            buf = split.path\n        if split.query:\n            qsa = split.query.split('&')\n            qsa = [a.split('=', 1) for a in qsa]\n            qsa = [\n                self.unquote_v(a) for a in qsa if a[0] in self.QSAOfInterest\n            ]\n            if len(qsa) > 0:\n                qsa.sort(key=itemgetter(0))\n                qsa = ['='.join(a) for a in qsa]\n                buf += '?'\n                buf += '&'.join(qsa)\n        return buf\n\n    def canonical_string(\n        self, method, split, headers, expires=None, auth_path=None\n    ):\n        cs = method.upper() + '\\n'\n        cs += self.canonical_standard_headers(headers) + '\\n'\n        custom_headers = self.canonical_custom_headers(headers)\n        if custom_headers:\n            cs += custom_headers + '\\n'\n        cs += self.canonical_resource(split, auth_path=auth_path)\n        return cs\n\n    def get_signature(\n        self, method, split, headers, expires=None, auth_path=None\n    ):\n        if self.credentials.token:\n            del headers['x-amz-security-token']\n            headers['x-amz-security-token'] = self.credentials.token\n        string_to_sign = self.canonical_string(\n            method, split, headers, auth_path=auth_path\n        )\n        logger.debug('StringToSign:\\n%s', string_to_sign)\n        return self.sign_string(string_to_sign)\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError\n        logger.debug(\"Calculating signature using hmacv1 auth.\")\n        split = urlsplit(request.url)\n        logger.debug('HTTP request method: %s', request.method)\n        signature = self.get_signature(\n            request.method, split, request.headers, auth_path=request.auth_path\n        )\n        self._inject_signature(request, signature)\n\n    def _get_date(self):\n        return formatdate(usegmt=True)\n\n    def _inject_signature(self, request, signature):\n        if 'Authorization' in request.headers:\n            # We have to do this because request.headers is not\n            # normal dictionary.  It has the (unintuitive) behavior\n            # of aggregating repeated setattr calls for the same\n            # key value.  For example:\n            # headers['foo'] = 'a'; headers['foo'] = 'b'\n            # list(headers) will print ['foo', 'foo'].\n            del request.headers['Authorization']\n\n        auth_header = f\"AWS {self.credentials.access_key}:{signature}\"\n        request.headers['Authorization'] = auth_header\n\n\nclass HmacV1QueryAuth(HmacV1Auth):\n    \"\"\"\n    Generates a presigned request for s3.\n\n    Spec from this document:\n\n    http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html\n    #RESTAuthenticationQueryStringAuth\n\n    \"\"\"\n\n    DEFAULT_EXPIRES = 3600\n\n    def __init__(self, credentials, expires=DEFAULT_EXPIRES):\n        self.credentials = credentials\n        self._expires = expires\n\n    def _get_date(self):\n        return str(int(time.time() + int(self._expires)))\n\n    def _inject_signature(self, request, signature):\n        query_dict = {}\n        query_dict['AWSAccessKeyId'] = self.credentials.access_key\n        query_dict['Signature'] = signature\n\n        for header_key in request.headers:\n            lk = header_key.lower()\n            # For query string requests, Expires is used instead of the\n            # Date header.\n            if header_key == 'Date':\n                query_dict['Expires'] = request.headers['Date']\n            # We only want to include relevant headers in the query string.\n            # These can be anything that starts with x-amz, is Content-MD5,\n            # or is Content-Type.\n            elif lk.startswith('x-amz-') or lk in (\n                'content-md5',\n                'content-type',\n            ):\n                query_dict[lk] = request.headers[lk]\n        # Combine all of the identified headers into an encoded\n        # query string\n        new_query_string = percent_encode_sequence(query_dict)\n\n        # Create a new url with the presigned url.\n        p = urlsplit(request.url)\n        if p[3]:\n            # If there was a pre-existing query string, we should\n            # add that back before injecting the new query string.\n            new_query_string = f'{p[3]}&{new_query_string}'\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n\nclass HmacV1PostAuth(HmacV1Auth):\n    \"\"\"\n    Generates a presigned post for s3.\n\n    Spec from this document:\n\n    http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html\n    \"\"\"\n\n    def add_auth(self, request):\n        fields = {}\n        if request.context.get('s3-presign-post-fields', None) is not None:\n            fields = request.context['s3-presign-post-fields']\n\n        policy = {}\n        conditions = []\n        if request.context.get('s3-presign-post-policy', None) is not None:\n            policy = request.context['s3-presign-post-policy']\n            if policy.get('conditions', None) is not None:\n                conditions = policy['conditions']\n\n        policy['conditions'] = conditions\n\n        fields['AWSAccessKeyId'] = self.credentials.access_key\n\n        if self.credentials.token is not None:\n            fields['x-amz-security-token'] = self.credentials.token\n            conditions.append({'x-amz-security-token': self.credentials.token})\n\n        # Dump the base64 encoded policy into the fields dictionary.\n        fields['policy'] = base64.b64encode(\n            json.dumps(policy).encode('utf-8')\n        ).decode('utf-8')\n\n        fields['signature'] = self.sign_string(fields['policy'])\n\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n\nclass BearerAuth(TokenSigner):\n    \"\"\"\n    Performs bearer token authorization by placing the bearer token in the\n    Authorization header as specified by Section 2.1 of RFC 6750.\n\n    https://datatracker.ietf.org/doc/html/rfc6750#section-2.1\n    \"\"\"\n\n    def add_auth(self, request):\n        if self.auth_token is None:\n            raise NoAuthTokenError()\n\n        auth_header = f'Bearer {self.auth_token.token}'\n        if 'Authorization' in request.headers:\n            del request.headers['Authorization']\n        request.headers['Authorization'] = auth_header\n\n\nAUTH_TYPE_MAPS = {\n    'v2': SigV2Auth,\n    'v3': SigV3Auth,\n    'v3https': SigV3Auth,\n    's3': HmacV1Auth,\n    's3-query': HmacV1QueryAuth,\n    's3-presign-post': HmacV1PostAuth,\n    's3v4-presign-post': S3SigV4PostAuth,\n    'v4-s3express': S3ExpressAuth,\n    'v4-s3express-query': S3ExpressQueryAuth,\n    'v4-s3express-presign-post': S3ExpressPostAuth,\n    'bearer': BearerAuth,\n}\n\n# Define v4 signers depending on if CRT is present\nif HAS_CRT:\n    from botocore.crt.auth import CRT_AUTH_TYPE_MAPS\n\n    AUTH_TYPE_MAPS.update(CRT_AUTH_TYPE_MAPS)\nelse:\n    AUTH_TYPE_MAPS.update(\n        {\n            'v4': SigV4Auth,\n            'v4-query': SigV4QueryAuth,\n            's3v4': S3SigV4Auth,\n            's3v4-query': S3SigV4QueryAuth,\n        }\n    )\n", "botocore/serialize.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Protocol input serializes.\n\nThis module contains classes that implement input serialization\nfor the various AWS protocol types.\n\nThese classes essentially take user input, a model object that\nrepresents what the expected input should look like, and it returns\na dictionary that contains the various parts of a request.  A few\nhigh level design decisions:\n\n\n* Each protocol type maps to a separate class, all inherit from\n  ``Serializer``.\n* The return value for ``serialize_to_request`` (the main entry\n  point) returns a dictionary that represents a request.  This\n  will have keys like ``url_path``, ``query_string``, etc.  This\n  is done so that it's a) easy to test and b) not tied to a\n  particular HTTP library.  See the ``serialize_to_request`` docstring\n  for more details.\n\nUnicode\n-------\n\nThe input to the serializers should be text (str/unicode), not bytes,\nwith the exception of blob types.  Those are assumed to be binary,\nand if a str/unicode type is passed in, it will be encoded as utf-8.\n\"\"\"\nimport base64\nimport calendar\nimport datetime\nimport json\nimport re\nfrom xml.etree import ElementTree\n\nfrom botocore import validate\nfrom botocore.compat import formatdate\nfrom botocore.exceptions import ParamValidationError\nfrom botocore.utils import (\n    has_header,\n    is_json_value_header,\n    parse_to_aware_datetime,\n    percent_encode,\n)\n\n# From the spec, the default timestamp format if not specified is iso8601.\nDEFAULT_TIMESTAMP_FORMAT = 'iso8601'\nISO8601 = '%Y-%m-%dT%H:%M:%SZ'\n# Same as ISO8601, but with microsecond precision.\nISO8601_MICRO = '%Y-%m-%dT%H:%M:%S.%fZ'\nHOST_PREFIX_RE = re.compile(r\"^[A-Za-z0-9\\.\\-]+$\")\n\n\ndef create_serializer(protocol_name, include_validation=True):\n    # TODO: Unknown protocols.\n    serializer = SERIALIZERS[protocol_name]()\n    if include_validation:\n        validator = validate.ParamValidator()\n        serializer = validate.ParamValidationDecorator(validator, serializer)\n    return serializer\n\n\nclass Serializer:\n    DEFAULT_METHOD = 'POST'\n    # Clients can change this to a different MutableMapping\n    # (i.e OrderedDict) if they want.  This is used in the\n    # compliance test to match the hash ordering used in the\n    # tests.\n    MAP_TYPE = dict\n    DEFAULT_ENCODING = 'utf-8'\n\n    def serialize_to_request(self, parameters, operation_model):\n        \"\"\"Serialize parameters into an HTTP request.\n\n        This method takes user provided parameters and a shape\n        model and serializes the parameters to an HTTP request.\n        More specifically, this method returns information about\n        parts of the HTTP request, it does not enforce a particular\n        interface or standard for an HTTP request.  It instead returns\n        a dictionary of:\n\n            * 'url_path'\n            * 'host_prefix'\n            * 'query_string'\n            * 'headers'\n            * 'body'\n            * 'method'\n\n        It is then up to consumers to decide how to map this to a Request\n        object of their HTTP library of choice.  Below is an example\n        return value::\n\n            {'body': {'Action': 'OperationName',\n                      'Bar': 'val2',\n                      'Foo': 'val1',\n                      'Version': '2014-01-01'},\n             'headers': {},\n             'method': 'POST',\n             'query_string': '',\n             'host_prefix': 'value.',\n             'url_path': '/'}\n\n        :param parameters: The dictionary input parameters for the\n            operation (i.e the user input).\n        :param operation_model: The OperationModel object that describes\n            the operation.\n        \"\"\"\n        raise NotImplementedError(\"serialize_to_request\")\n\n    def _create_default_request(self):\n        # Creates a boilerplate default request dict that subclasses\n        # can use as a starting point.\n        serialized = {\n            'url_path': '/',\n            'query_string': '',\n            'method': self.DEFAULT_METHOD,\n            'headers': {},\n            # An empty body is represented as an empty byte string.\n            'body': b'',\n        }\n        return serialized\n\n    # Some extra utility methods subclasses can use.\n\n    def _timestamp_iso8601(self, value):\n        if value.microsecond > 0:\n            timestamp_format = ISO8601_MICRO\n        else:\n            timestamp_format = ISO8601\n        return value.strftime(timestamp_format)\n\n    def _timestamp_unixtimestamp(self, value):\n        return int(calendar.timegm(value.timetuple()))\n\n    def _timestamp_rfc822(self, value):\n        if isinstance(value, datetime.datetime):\n            value = self._timestamp_unixtimestamp(value)\n        return formatdate(value, usegmt=True)\n\n    def _convert_timestamp_to_str(self, value, timestamp_format=None):\n        if timestamp_format is None:\n            timestamp_format = self.TIMESTAMP_FORMAT\n        timestamp_format = timestamp_format.lower()\n        datetime_obj = parse_to_aware_datetime(value)\n        converter = getattr(self, f'_timestamp_{timestamp_format}')\n        final_value = converter(datetime_obj)\n        return final_value\n\n    def _get_serialized_name(self, shape, default_name):\n        # Returns the serialized name for the shape if it exists.\n        # Otherwise it will return the passed in default_name.\n        return shape.serialization.get('name', default_name)\n\n    def _get_base64(self, value):\n        # Returns the base64-encoded version of value, handling\n        # both strings and bytes. The returned value is a string\n        # via the default encoding.\n        if isinstance(value, str):\n            value = value.encode(self.DEFAULT_ENCODING)\n        return base64.b64encode(value).strip().decode(self.DEFAULT_ENCODING)\n\n    def _expand_host_prefix(self, parameters, operation_model):\n        operation_endpoint = operation_model.endpoint\n        if (\n            operation_endpoint is None\n            or 'hostPrefix' not in operation_endpoint\n        ):\n            return None\n\n        host_prefix_expression = operation_endpoint['hostPrefix']\n        input_members = operation_model.input_shape.members\n        host_labels = [\n            member\n            for member, shape in input_members.items()\n            if shape.serialization.get('hostLabel')\n        ]\n        format_kwargs = {}\n        bad_labels = []\n        for name in host_labels:\n            param = parameters[name]\n            if not HOST_PREFIX_RE.match(param):\n                bad_labels.append(name)\n            format_kwargs[name] = param\n        if bad_labels:\n            raise ParamValidationError(\n                report=(\n                    f\"Invalid value for parameter(s): {', '.join(bad_labels)}. \"\n                    \"Must contain only alphanumeric characters, hyphen, \"\n                    \"or period.\"\n                )\n            )\n        return host_prefix_expression.format(**format_kwargs)\n\n\nclass QuerySerializer(Serializer):\n    TIMESTAMP_FORMAT = 'iso8601'\n\n    def serialize_to_request(self, parameters, operation_model):\n        shape = operation_model.input_shape\n        serialized = self._create_default_request()\n        serialized['method'] = operation_model.http.get(\n            'method', self.DEFAULT_METHOD\n        )\n        serialized['headers'] = {\n            'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8'\n        }\n        # The query serializer only deals with body params so\n        # that's what we hand off the _serialize_* methods.\n        body_params = self.MAP_TYPE()\n        body_params['Action'] = operation_model.name\n        body_params['Version'] = operation_model.metadata['apiVersion']\n        if shape is not None:\n            self._serialize(body_params, parameters, shape)\n        serialized['body'] = body_params\n\n        host_prefix = self._expand_host_prefix(parameters, operation_model)\n        if host_prefix is not None:\n            serialized['host_prefix'] = host_prefix\n\n        return serialized\n\n    def _serialize(self, serialized, value, shape, prefix=''):\n        # serialized: The dict that is incrementally added to with the\n        #             final serialized parameters.\n        # value: The current user input value.\n        # shape: The shape object that describes the structure of the\n        #        input.\n        # prefix: The incrementally built up prefix for the serialized\n        #         key (i.e Foo.bar.members.1).\n        method = getattr(\n            self,\n            f'_serialize_type_{shape.type_name}',\n            self._default_serialize,\n        )\n        method(serialized, value, shape, prefix=prefix)\n\n    def _serialize_type_structure(self, serialized, value, shape, prefix=''):\n        members = shape.members\n        for key, value in value.items():\n            member_shape = members[key]\n            member_prefix = self._get_serialized_name(member_shape, key)\n            if prefix:\n                member_prefix = f'{prefix}.{member_prefix}'\n            self._serialize(serialized, value, member_shape, member_prefix)\n\n    def _serialize_type_list(self, serialized, value, shape, prefix=''):\n        if not value:\n            # The query protocol serializes empty lists.\n            serialized[prefix] = ''\n            return\n        if self._is_shape_flattened(shape):\n            list_prefix = prefix\n            if shape.member.serialization.get('name'):\n                name = self._get_serialized_name(shape.member, default_name='')\n                # Replace '.Original' with '.{name}'.\n                list_prefix = '.'.join(prefix.split('.')[:-1] + [name])\n        else:\n            list_name = shape.member.serialization.get('name', 'member')\n            list_prefix = f'{prefix}.{list_name}'\n        for i, element in enumerate(value, 1):\n            element_prefix = f'{list_prefix}.{i}'\n            element_shape = shape.member\n            self._serialize(serialized, element, element_shape, element_prefix)\n\n    def _serialize_type_map(self, serialized, value, shape, prefix=''):\n        if self._is_shape_flattened(shape):\n            full_prefix = prefix\n        else:\n            full_prefix = '%s.entry' % prefix\n        template = full_prefix + '.{i}.{suffix}'\n        key_shape = shape.key\n        value_shape = shape.value\n        key_suffix = self._get_serialized_name(key_shape, default_name='key')\n        value_suffix = self._get_serialized_name(value_shape, 'value')\n        for i, key in enumerate(value, 1):\n            key_prefix = template.format(i=i, suffix=key_suffix)\n            value_prefix = template.format(i=i, suffix=value_suffix)\n            self._serialize(serialized, key, key_shape, key_prefix)\n            self._serialize(serialized, value[key], value_shape, value_prefix)\n\n    def _serialize_type_blob(self, serialized, value, shape, prefix=''):\n        # Blob args must be base64 encoded.\n        serialized[prefix] = self._get_base64(value)\n\n    def _serialize_type_timestamp(self, serialized, value, shape, prefix=''):\n        serialized[prefix] = self._convert_timestamp_to_str(\n            value, shape.serialization.get('timestampFormat')\n        )\n\n    def _serialize_type_boolean(self, serialized, value, shape, prefix=''):\n        if value:\n            serialized[prefix] = 'true'\n        else:\n            serialized[prefix] = 'false'\n\n    def _default_serialize(self, serialized, value, shape, prefix=''):\n        serialized[prefix] = value\n\n    def _is_shape_flattened(self, shape):\n        return shape.serialization.get('flattened')\n\n\nclass EC2Serializer(QuerySerializer):\n    \"\"\"EC2 specific customizations to the query protocol serializers.\n\n    The EC2 model is almost, but not exactly, similar to the query protocol\n    serializer.  This class encapsulates those differences.  The model\n    will have be marked with a ``protocol`` of ``ec2``, so you don't need\n    to worry about wiring this class up correctly.\n\n    \"\"\"\n\n    def _get_serialized_name(self, shape, default_name):\n        # Returns the serialized name for the shape if it exists.\n        # Otherwise it will return the passed in default_name.\n        if 'queryName' in shape.serialization:\n            return shape.serialization['queryName']\n        elif 'name' in shape.serialization:\n            # A locationName is always capitalized\n            # on input for the ec2 protocol.\n            name = shape.serialization['name']\n            return name[0].upper() + name[1:]\n        else:\n            return default_name\n\n    def _serialize_type_list(self, serialized, value, shape, prefix=''):\n        for i, element in enumerate(value, 1):\n            element_prefix = f'{prefix}.{i}'\n            element_shape = shape.member\n            self._serialize(serialized, element, element_shape, element_prefix)\n\n\nclass JSONSerializer(Serializer):\n    TIMESTAMP_FORMAT = 'unixtimestamp'\n\n    def serialize_to_request(self, parameters, operation_model):\n        target = '{}.{}'.format(\n            operation_model.metadata['targetPrefix'],\n            operation_model.name,\n        )\n        json_version = operation_model.metadata['jsonVersion']\n        serialized = self._create_default_request()\n        serialized['method'] = operation_model.http.get(\n            'method', self.DEFAULT_METHOD\n        )\n        serialized['headers'] = {\n            'X-Amz-Target': target,\n            'Content-Type': 'application/x-amz-json-%s' % json_version,\n        }\n        body = self.MAP_TYPE()\n        input_shape = operation_model.input_shape\n        if input_shape is not None:\n            self._serialize(body, parameters, input_shape)\n        serialized['body'] = json.dumps(body).encode(self.DEFAULT_ENCODING)\n\n        host_prefix = self._expand_host_prefix(parameters, operation_model)\n        if host_prefix is not None:\n            serialized['host_prefix'] = host_prefix\n\n        return serialized\n\n    def _serialize(self, serialized, value, shape, key=None):\n        method = getattr(\n            self,\n            '_serialize_type_%s' % shape.type_name,\n            self._default_serialize,\n        )\n        method(serialized, value, shape, key)\n\n    def _serialize_type_structure(self, serialized, value, shape, key):\n        if shape.is_document_type:\n            serialized[key] = value\n        else:\n            if key is not None:\n                # If a key is provided, this is a result of a recursive\n                # call so we need to add a new child dict as the value\n                # of the passed in serialized dict.  We'll then add\n                # all the structure members as key/vals in the new serialized\n                # dictionary we just created.\n                new_serialized = self.MAP_TYPE()\n                serialized[key] = new_serialized\n                serialized = new_serialized\n            members = shape.members\n            for member_key, member_value in value.items():\n                member_shape = members[member_key]\n                if 'name' in member_shape.serialization:\n                    member_key = member_shape.serialization['name']\n                self._serialize(\n                    serialized, member_value, member_shape, member_key\n                )\n\n    def _serialize_type_map(self, serialized, value, shape, key):\n        map_obj = self.MAP_TYPE()\n        serialized[key] = map_obj\n        for sub_key, sub_value in value.items():\n            self._serialize(map_obj, sub_value, shape.value, sub_key)\n\n    def _serialize_type_list(self, serialized, value, shape, key):\n        list_obj = []\n        serialized[key] = list_obj\n        for list_item in value:\n            wrapper = {}\n            # The JSON list serialization is the only case where we aren't\n            # setting a key on a dict.  We handle this by using\n            # a __current__ key on a wrapper dict to serialize each\n            # list item before appending it to the serialized list.\n            self._serialize(wrapper, list_item, shape.member, \"__current__\")\n            list_obj.append(wrapper[\"__current__\"])\n\n    def _default_serialize(self, serialized, value, shape, key):\n        serialized[key] = value\n\n    def _serialize_type_timestamp(self, serialized, value, shape, key):\n        serialized[key] = self._convert_timestamp_to_str(\n            value, shape.serialization.get('timestampFormat')\n        )\n\n    def _serialize_type_blob(self, serialized, value, shape, key):\n        serialized[key] = self._get_base64(value)\n\n\nclass BaseRestSerializer(Serializer):\n    \"\"\"Base class for rest protocols.\n\n    The only variance between the various rest protocols is the\n    way that the body is serialized.  All other aspects (headers, uri, etc.)\n    are the same and logic for serializing those aspects lives here.\n\n    Subclasses must implement the ``_serialize_body_params`` method.\n\n    \"\"\"\n\n    QUERY_STRING_TIMESTAMP_FORMAT = 'iso8601'\n    HEADER_TIMESTAMP_FORMAT = 'rfc822'\n    # This is a list of known values for the \"location\" key in the\n    # serialization dict.  The location key tells us where on the request\n    # to put the serialized value.\n    KNOWN_LOCATIONS = ['uri', 'querystring', 'header', 'headers']\n\n    def serialize_to_request(self, parameters, operation_model):\n        serialized = self._create_default_request()\n        serialized['method'] = operation_model.http.get(\n            'method', self.DEFAULT_METHOD\n        )\n        shape = operation_model.input_shape\n        if shape is None:\n            serialized['url_path'] = operation_model.http['requestUri']\n            return serialized\n        shape_members = shape.members\n        # While the ``serialized`` key holds the final serialized request\n        # data, we need interim dicts for the various locations of the\n        # request.  We need this for the uri_path_kwargs and the\n        # query_string_kwargs because they are templated, so we need\n        # to gather all the needed data for the string template,\n        # then we render the template.  The body_kwargs is needed\n        # because once we've collected them all, we run them through\n        # _serialize_body_params, which for rest-json, creates JSON,\n        # and for rest-xml, will create XML.  This is what the\n        # ``partitioned`` dict below is for.\n        partitioned = {\n            'uri_path_kwargs': self.MAP_TYPE(),\n            'query_string_kwargs': self.MAP_TYPE(),\n            'body_kwargs': self.MAP_TYPE(),\n            'headers': self.MAP_TYPE(),\n        }\n        for param_name, param_value in parameters.items():\n            if param_value is None:\n                # Don't serialize any parameter with a None value.\n                continue\n            self._partition_parameters(\n                partitioned, param_name, param_value, shape_members\n            )\n        serialized['url_path'] = self._render_uri_template(\n            operation_model.http['requestUri'], partitioned['uri_path_kwargs']\n        )\n\n        if 'authPath' in operation_model.http:\n            serialized['auth_path'] = self._render_uri_template(\n                operation_model.http['authPath'],\n                partitioned['uri_path_kwargs'],\n            )\n        # Note that we lean on the http implementation to handle the case\n        # where the requestUri path already has query parameters.\n        # The bundled http client, requests, already supports this.\n        serialized['query_string'] = partitioned['query_string_kwargs']\n        if partitioned['headers']:\n            serialized['headers'] = partitioned['headers']\n        self._serialize_payload(\n            partitioned, parameters, serialized, shape, shape_members\n        )\n        self._serialize_content_type(serialized, shape, shape_members)\n\n        host_prefix = self._expand_host_prefix(parameters, operation_model)\n        if host_prefix is not None:\n            serialized['host_prefix'] = host_prefix\n\n        return serialized\n\n    def _render_uri_template(self, uri_template, params):\n        # We need to handle two cases::\n        #\n        # /{Bucket}/foo\n        # /{Key+}/bar\n        # A label ending with '+' is greedy.  There can only\n        # be one greedy key.\n        encoded_params = {}\n        for template_param in re.findall(r'{(.*?)}', uri_template):\n            if template_param.endswith('+'):\n                encoded_params[template_param] = percent_encode(\n                    params[template_param[:-1]], safe='/~'\n                )\n            else:\n                encoded_params[template_param] = percent_encode(\n                    params[template_param]\n                )\n        return uri_template.format(**encoded_params)\n\n    def _serialize_payload(\n        self, partitioned, parameters, serialized, shape, shape_members\n    ):\n        # partitioned - The user input params partitioned by location.\n        # parameters - The user input params.\n        # serialized - The final serialized request dict.\n        # shape - Describes the expected input shape\n        # shape_members - The members of the input struct shape\n        payload_member = shape.serialization.get('payload')\n        if self._has_streaming_payload(payload_member, shape_members):\n            # If it's streaming, then the body is just the\n            # value of the payload.\n            body_payload = parameters.get(payload_member, b'')\n            body_payload = self._encode_payload(body_payload)\n            serialized['body'] = body_payload\n        elif payload_member is not None:\n            # If there's a payload member, we serialized that\n            # member to they body.\n            body_params = parameters.get(payload_member)\n            if body_params is not None:\n                serialized['body'] = self._serialize_body_params(\n                    body_params, shape_members[payload_member]\n                )\n            else:\n                serialized['body'] = self._serialize_empty_body()\n        elif partitioned['body_kwargs']:\n            serialized['body'] = self._serialize_body_params(\n                partitioned['body_kwargs'], shape\n            )\n        elif self._requires_empty_body(shape):\n            serialized['body'] = self._serialize_empty_body()\n\n    def _serialize_empty_body(self):\n        return b''\n\n    def _serialize_content_type(self, serialized, shape, shape_members):\n        \"\"\"\n        Some protocols require varied Content-Type headers\n        depending on user input. This allows subclasses to apply\n        this conditionally.\n        \"\"\"\n        pass\n\n    def _requires_empty_body(self, shape):\n        \"\"\"\n        Some protocols require a specific body to represent an empty\n        payload. This allows subclasses to apply this conditionally.\n        \"\"\"\n        return False\n\n    def _has_streaming_payload(self, payload, shape_members):\n        \"\"\"Determine if payload is streaming (a blob or string).\"\"\"\n        return payload is not None and shape_members[payload].type_name in (\n            'blob',\n            'string',\n        )\n\n    def _encode_payload(self, body):\n        if isinstance(body, str):\n            return body.encode(self.DEFAULT_ENCODING)\n        return body\n\n    def _partition_parameters(\n        self, partitioned, param_name, param_value, shape_members\n    ):\n        # This takes the user provided input parameter (``param``)\n        # and figures out where they go in the request dict.\n        # Some params are HTTP headers, some are used in the URI, some\n        # are in the request body.  This method deals with this.\n        member = shape_members[param_name]\n        location = member.serialization.get('location')\n        key_name = member.serialization.get('name', param_name)\n        if location == 'uri':\n            partitioned['uri_path_kwargs'][key_name] = param_value\n        elif location == 'querystring':\n            if isinstance(param_value, dict):\n                partitioned['query_string_kwargs'].update(param_value)\n            elif isinstance(param_value, bool):\n                bool_str = str(param_value).lower()\n                partitioned['query_string_kwargs'][key_name] = bool_str\n            elif member.type_name == 'timestamp':\n                timestamp_format = member.serialization.get(\n                    'timestampFormat', self.QUERY_STRING_TIMESTAMP_FORMAT\n                )\n                timestamp = self._convert_timestamp_to_str(\n                    param_value, timestamp_format\n                )\n                partitioned['query_string_kwargs'][key_name] = timestamp\n            else:\n                partitioned['query_string_kwargs'][key_name] = param_value\n        elif location == 'header':\n            shape = shape_members[param_name]\n            if not param_value and shape.type_name == 'list':\n                # Empty lists should not be set on the headers\n                return\n            value = self._convert_header_value(shape, param_value)\n            partitioned['headers'][key_name] = str(value)\n        elif location == 'headers':\n            # 'headers' is a bit of an oddball.  The ``key_name``\n            # is actually really a prefix for the header names:\n            header_prefix = key_name\n            # The value provided by the user is a dict so we'll be\n            # creating multiple header key/val pairs.  The key\n            # name to use for each header is the header_prefix (``key_name``)\n            # plus the key provided by the user.\n            self._do_serialize_header_map(\n                header_prefix, partitioned['headers'], param_value\n            )\n        else:\n            partitioned['body_kwargs'][param_name] = param_value\n\n    def _do_serialize_header_map(self, header_prefix, headers, user_input):\n        for key, val in user_input.items():\n            full_key = header_prefix + key\n            headers[full_key] = val\n\n    def _serialize_body_params(self, params, shape):\n        raise NotImplementedError('_serialize_body_params')\n\n    def _convert_header_value(self, shape, value):\n        if shape.type_name == 'timestamp':\n            datetime_obj = parse_to_aware_datetime(value)\n            timestamp = calendar.timegm(datetime_obj.utctimetuple())\n            timestamp_format = shape.serialization.get(\n                'timestampFormat', self.HEADER_TIMESTAMP_FORMAT\n            )\n            return self._convert_timestamp_to_str(timestamp, timestamp_format)\n        elif shape.type_name == 'list':\n            converted_value = [\n                self._convert_header_value(shape.member, v)\n                for v in value\n                if v is not None\n            ]\n            return \",\".join(converted_value)\n        elif is_json_value_header(shape):\n            # Serialize with no spaces after separators to save space in\n            # the header.\n            return self._get_base64(json.dumps(value, separators=(',', ':')))\n        else:\n            return value\n\n\nclass RestJSONSerializer(BaseRestSerializer, JSONSerializer):\n    def _serialize_empty_body(self):\n        return b'{}'\n\n    def _requires_empty_body(self, shape):\n        \"\"\"\n        Serialize an empty JSON object whenever the shape has\n        members not targeting a location.\n        \"\"\"\n        for member, val in shape.members.items():\n            if 'location' not in val.serialization:\n                return True\n        return False\n\n    def _serialize_content_type(self, serialized, shape, shape_members):\n        \"\"\"Set Content-Type to application/json for all structured bodies.\"\"\"\n        payload = shape.serialization.get('payload')\n        if self._has_streaming_payload(payload, shape_members):\n            # Don't apply content-type to streaming bodies\n            return\n\n        has_body = serialized['body'] != b''\n        has_content_type = has_header('Content-Type', serialized['headers'])\n        if has_body and not has_content_type:\n            serialized['headers']['Content-Type'] = 'application/json'\n\n    def _serialize_body_params(self, params, shape):\n        serialized_body = self.MAP_TYPE()\n        self._serialize(serialized_body, params, shape)\n        return json.dumps(serialized_body).encode(self.DEFAULT_ENCODING)\n\n\nclass RestXMLSerializer(BaseRestSerializer):\n    TIMESTAMP_FORMAT = 'iso8601'\n\n    def _serialize_body_params(self, params, shape):\n        root_name = shape.serialization['name']\n        pseudo_root = ElementTree.Element('')\n        self._serialize(shape, params, pseudo_root, root_name)\n        real_root = list(pseudo_root)[0]\n        return ElementTree.tostring(real_root, encoding=self.DEFAULT_ENCODING)\n\n    def _serialize(self, shape, params, xmlnode, name):\n        method = getattr(\n            self,\n            '_serialize_type_%s' % shape.type_name,\n            self._default_serialize,\n        )\n        method(xmlnode, params, shape, name)\n\n    def _serialize_type_structure(self, xmlnode, params, shape, name):\n        structure_node = ElementTree.SubElement(xmlnode, name)\n\n        if 'xmlNamespace' in shape.serialization:\n            namespace_metadata = shape.serialization['xmlNamespace']\n            attribute_name = 'xmlns'\n            if namespace_metadata.get('prefix'):\n                attribute_name += ':%s' % namespace_metadata['prefix']\n            structure_node.attrib[attribute_name] = namespace_metadata['uri']\n        for key, value in params.items():\n            member_shape = shape.members[key]\n            member_name = member_shape.serialization.get('name', key)\n            # We need to special case member shapes that are marked as an\n            # xmlAttribute.  Rather than serializing into an XML child node,\n            # we instead serialize the shape to an XML attribute of the\n            # *current* node.\n            if value is None:\n                # Don't serialize any param whose value is None.\n                return\n            if member_shape.serialization.get('xmlAttribute'):\n                # xmlAttributes must have a serialization name.\n                xml_attribute_name = member_shape.serialization['name']\n                structure_node.attrib[xml_attribute_name] = value\n                continue\n            self._serialize(member_shape, value, structure_node, member_name)\n\n    def _serialize_type_list(self, xmlnode, params, shape, name):\n        member_shape = shape.member\n        if shape.serialization.get('flattened'):\n            element_name = name\n            list_node = xmlnode\n        else:\n            element_name = member_shape.serialization.get('name', 'member')\n            list_node = ElementTree.SubElement(xmlnode, name)\n        for item in params:\n            self._serialize(member_shape, item, list_node, element_name)\n\n    def _serialize_type_map(self, xmlnode, params, shape, name):\n        # Given the ``name`` of MyMap, and input of {\"key1\": \"val1\"}\n        # we serialize this as:\n        #   <MyMap>\n        #     <entry>\n        #       <key>key1</key>\n        #       <value>val1</value>\n        #     </entry>\n        #  </MyMap>\n        node = ElementTree.SubElement(xmlnode, name)\n        # TODO: handle flattened maps.\n        for key, value in params.items():\n            entry_node = ElementTree.SubElement(node, 'entry')\n            key_name = self._get_serialized_name(shape.key, default_name='key')\n            val_name = self._get_serialized_name(\n                shape.value, default_name='value'\n            )\n            self._serialize(shape.key, key, entry_node, key_name)\n            self._serialize(shape.value, value, entry_node, val_name)\n\n    def _serialize_type_boolean(self, xmlnode, params, shape, name):\n        # For scalar types, the 'params' attr is actually just a scalar\n        # value representing the data we need to serialize as a boolean.\n        # It will either be 'true' or 'false'\n        node = ElementTree.SubElement(xmlnode, name)\n        if params:\n            str_value = 'true'\n        else:\n            str_value = 'false'\n        node.text = str_value\n\n    def _serialize_type_blob(self, xmlnode, params, shape, name):\n        node = ElementTree.SubElement(xmlnode, name)\n        node.text = self._get_base64(params)\n\n    def _serialize_type_timestamp(self, xmlnode, params, shape, name):\n        node = ElementTree.SubElement(xmlnode, name)\n        node.text = self._convert_timestamp_to_str(\n            params, shape.serialization.get('timestampFormat')\n        )\n\n    def _default_serialize(self, xmlnode, params, shape, name):\n        node = ElementTree.SubElement(xmlnode, name)\n        node.text = str(params)\n\n\nSERIALIZERS = {\n    'ec2': EC2Serializer,\n    'query': QuerySerializer,\n    'json': JSONSerializer,\n    'rest-json': RestJSONSerializer,\n    'rest-xml': RestXMLSerializer,\n}\n", "botocore/regions.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Resolves regions and endpoints.\n\nThis module implements endpoint resolution, including resolving endpoints for a\ngiven service and region and resolving the available endpoints for a service\nin a specific AWS partition.\n\"\"\"\nimport copy\nimport logging\nimport re\nfrom enum import Enum\n\nfrom botocore import UNSIGNED, xform_name\nfrom botocore.auth import AUTH_TYPE_MAPS, HAS_CRT\nfrom botocore.crt import CRT_SUPPORTED_AUTH_TYPES\nfrom botocore.endpoint_provider import EndpointProvider\nfrom botocore.exceptions import (\n    EndpointProviderError,\n    EndpointVariantError,\n    InvalidEndpointConfigurationError,\n    InvalidHostLabelError,\n    MissingDependencyException,\n    NoRegionError,\n    ParamValidationError,\n    UnknownEndpointResolutionBuiltInName,\n    UnknownRegionError,\n    UnknownSignatureVersionError,\n    UnsupportedS3AccesspointConfigurationError,\n    UnsupportedS3ConfigurationError,\n    UnsupportedS3ControlArnError,\n    UnsupportedS3ControlConfigurationError,\n)\nfrom botocore.utils import ensure_boolean, instance_cache\n\nLOG = logging.getLogger(__name__)\nDEFAULT_URI_TEMPLATE = '{service}.{region}.{dnsSuffix}'  # noqa\nDEFAULT_SERVICE_DATA = {'endpoints': {}}\n\n\nclass BaseEndpointResolver:\n    \"\"\"Resolves regions and endpoints. Must be subclassed.\"\"\"\n\n    def construct_endpoint(self, service_name, region_name=None):\n        \"\"\"Resolves an endpoint for a service and region combination.\n\n        :type service_name: string\n        :param service_name: Name of the service to resolve an endpoint for\n            (e.g., s3)\n\n        :type region_name: string\n        :param region_name: Region/endpoint name to resolve (e.g., us-east-1)\n            if no region is provided, the first found partition-wide endpoint\n            will be used if available.\n\n        :rtype: dict\n        :return: Returns a dict containing the following keys:\n            - partition: (string, required) Resolved partition name\n            - endpointName: (string, required) Resolved endpoint name\n            - hostname: (string, required) Hostname to use for this endpoint\n            - sslCommonName: (string) sslCommonName to use for this endpoint.\n            - credentialScope: (dict) Signature version 4 credential scope\n              - region: (string) region name override when signing.\n              - service: (string) service name override when signing.\n            - signatureVersions: (list<string>) A list of possible signature\n              versions, including s3, v4, v2, and s3v4\n            - protocols: (list<string>) A list of supported protocols\n              (e.g., http, https)\n            - ...: Other keys may be included as well based on the metadata\n        \"\"\"\n        raise NotImplementedError\n\n    def get_available_partitions(self):\n        \"\"\"Lists the partitions available to the endpoint resolver.\n\n        :return: Returns a list of partition names (e.g., [\"aws\", \"aws-cn\"]).\n        \"\"\"\n        raise NotImplementedError\n\n    def get_available_endpoints(\n        self, service_name, partition_name='aws', allow_non_regional=False\n    ):\n        \"\"\"Lists the endpoint names of a particular partition.\n\n        :type service_name: string\n        :param service_name: Name of a service to list endpoint for (e.g., s3)\n\n        :type partition_name: string\n        :param partition_name: Name of the partition to limit endpoints to.\n            (e.g., aws for the public AWS endpoints, aws-cn for AWS China\n            endpoints, aws-us-gov for AWS GovCloud (US) Endpoints, etc.\n\n        :type allow_non_regional: bool\n        :param allow_non_regional: Set to True to include endpoints that are\n             not regional endpoints (e.g., s3-external-1,\n             fips-us-gov-west-1, etc).\n        :return: Returns a list of endpoint names (e.g., [\"us-east-1\"]).\n        \"\"\"\n        raise NotImplementedError\n\n\nclass EndpointResolver(BaseEndpointResolver):\n    \"\"\"Resolves endpoints based on partition endpoint metadata\"\"\"\n\n    _UNSUPPORTED_DUALSTACK_PARTITIONS = ['aws-iso', 'aws-iso-b']\n\n    def __init__(self, endpoint_data, uses_builtin_data=False):\n        \"\"\"\n        :type endpoint_data: dict\n        :param endpoint_data: A dict of partition data.\n\n        :type uses_builtin_data: boolean\n        :param uses_builtin_data: Whether the endpoint data originates in the\n            package's data directory.\n        \"\"\"\n        if 'partitions' not in endpoint_data:\n            raise ValueError('Missing \"partitions\" in endpoint data')\n        self._endpoint_data = endpoint_data\n        self.uses_builtin_data = uses_builtin_data\n\n    def get_service_endpoints_data(self, service_name, partition_name='aws'):\n        for partition in self._endpoint_data['partitions']:\n            if partition['partition'] != partition_name:\n                continue\n            services = partition['services']\n            if service_name not in services:\n                continue\n            return services[service_name]['endpoints']\n\n    def get_available_partitions(self):\n        result = []\n        for partition in self._endpoint_data['partitions']:\n            result.append(partition['partition'])\n        return result\n\n    def get_available_endpoints(\n        self,\n        service_name,\n        partition_name='aws',\n        allow_non_regional=False,\n        endpoint_variant_tags=None,\n    ):\n        result = []\n        for partition in self._endpoint_data['partitions']:\n            if partition['partition'] != partition_name:\n                continue\n            services = partition['services']\n            if service_name not in services:\n                continue\n            service_endpoints = services[service_name]['endpoints']\n            for endpoint_name in service_endpoints:\n                is_regional_endpoint = endpoint_name in partition['regions']\n                # Only regional endpoints can be modeled with variants\n                if endpoint_variant_tags and is_regional_endpoint:\n                    variant_data = self._retrieve_variant_data(\n                        service_endpoints[endpoint_name], endpoint_variant_tags\n                    )\n                    if variant_data:\n                        result.append(endpoint_name)\n                elif allow_non_regional or is_regional_endpoint:\n                    result.append(endpoint_name)\n        return result\n\n    def get_partition_dns_suffix(\n        self, partition_name, endpoint_variant_tags=None\n    ):\n        for partition in self._endpoint_data['partitions']:\n            if partition['partition'] == partition_name:\n                if endpoint_variant_tags:\n                    variant = self._retrieve_variant_data(\n                        partition.get('defaults'), endpoint_variant_tags\n                    )\n                    if variant and 'dnsSuffix' in variant:\n                        return variant['dnsSuffix']\n                else:\n                    return partition['dnsSuffix']\n        return None\n\n    def construct_endpoint(\n        self,\n        service_name,\n        region_name=None,\n        partition_name=None,\n        use_dualstack_endpoint=False,\n        use_fips_endpoint=False,\n    ):\n        if (\n            service_name == 's3'\n            and use_dualstack_endpoint\n            and region_name is None\n        ):\n            region_name = 'us-east-1'\n\n        if partition_name is not None:\n            valid_partition = None\n            for partition in self._endpoint_data['partitions']:\n                if partition['partition'] == partition_name:\n                    valid_partition = partition\n\n            if valid_partition is not None:\n                result = self._endpoint_for_partition(\n                    valid_partition,\n                    service_name,\n                    region_name,\n                    use_dualstack_endpoint,\n                    use_fips_endpoint,\n                    True,\n                )\n                return result\n            return None\n\n        # Iterate over each partition until a match is found.\n        for partition in self._endpoint_data['partitions']:\n            if use_dualstack_endpoint and (\n                partition['partition']\n                in self._UNSUPPORTED_DUALSTACK_PARTITIONS\n            ):\n                continue\n            result = self._endpoint_for_partition(\n                partition,\n                service_name,\n                region_name,\n                use_dualstack_endpoint,\n                use_fips_endpoint,\n            )\n            if result:\n                return result\n\n    def get_partition_for_region(self, region_name):\n        for partition in self._endpoint_data['partitions']:\n            if self._region_match(partition, region_name):\n                return partition['partition']\n        raise UnknownRegionError(\n            region_name=region_name,\n            error_msg='No partition found for provided region_name.',\n        )\n\n    def _endpoint_for_partition(\n        self,\n        partition,\n        service_name,\n        region_name,\n        use_dualstack_endpoint,\n        use_fips_endpoint,\n        force_partition=False,\n    ):\n        partition_name = partition[\"partition\"]\n        if (\n            use_dualstack_endpoint\n            and partition_name in self._UNSUPPORTED_DUALSTACK_PARTITIONS\n        ):\n            error_msg = (\n                \"Dualstack endpoints are currently not supported\"\n                \" for %s partition\" % partition_name\n            )\n            raise EndpointVariantError(tags=['dualstack'], error_msg=error_msg)\n\n        # Get the service from the partition, or an empty template.\n        service_data = partition['services'].get(\n            service_name, DEFAULT_SERVICE_DATA\n        )\n        # Use the partition endpoint if no region is supplied.\n        if region_name is None:\n            if 'partitionEndpoint' in service_data:\n                region_name = service_data['partitionEndpoint']\n            else:\n                raise NoRegionError()\n\n        resolve_kwargs = {\n            'partition': partition,\n            'service_name': service_name,\n            'service_data': service_data,\n            'endpoint_name': region_name,\n            'use_dualstack_endpoint': use_dualstack_endpoint,\n            'use_fips_endpoint': use_fips_endpoint,\n        }\n\n        # Attempt to resolve the exact region for this partition.\n        if region_name in service_data['endpoints']:\n            return self._resolve(**resolve_kwargs)\n\n        # Check to see if the endpoint provided is valid for the partition.\n        if self._region_match(partition, region_name) or force_partition:\n            # Use the partition endpoint if set and not regionalized.\n            partition_endpoint = service_data.get('partitionEndpoint')\n            is_regionalized = service_data.get('isRegionalized', True)\n            if partition_endpoint and not is_regionalized:\n                LOG.debug(\n                    'Using partition endpoint for %s, %s: %s',\n                    service_name,\n                    region_name,\n                    partition_endpoint,\n                )\n                resolve_kwargs['endpoint_name'] = partition_endpoint\n                return self._resolve(**resolve_kwargs)\n            LOG.debug(\n                'Creating a regex based endpoint for %s, %s',\n                service_name,\n                region_name,\n            )\n            return self._resolve(**resolve_kwargs)\n\n    def _region_match(self, partition, region_name):\n        if region_name in partition['regions']:\n            return True\n        if 'regionRegex' in partition:\n            return re.compile(partition['regionRegex']).match(region_name)\n        return False\n\n    def _retrieve_variant_data(self, endpoint_data, tags):\n        variants = endpoint_data.get('variants', [])\n        for variant in variants:\n            if set(variant['tags']) == set(tags):\n                result = variant.copy()\n                return result\n\n    def _create_tag_list(self, use_dualstack_endpoint, use_fips_endpoint):\n        tags = []\n        if use_dualstack_endpoint:\n            tags.append('dualstack')\n        if use_fips_endpoint:\n            tags.append('fips')\n        return tags\n\n    def _resolve_variant(\n        self, tags, endpoint_data, service_defaults, partition_defaults\n    ):\n        result = {}\n        for variants in [endpoint_data, service_defaults, partition_defaults]:\n            variant = self._retrieve_variant_data(variants, tags)\n            if variant:\n                self._merge_keys(variant, result)\n        return result\n\n    def _resolve(\n        self,\n        partition,\n        service_name,\n        service_data,\n        endpoint_name,\n        use_dualstack_endpoint,\n        use_fips_endpoint,\n    ):\n        endpoint_data = service_data.get('endpoints', {}).get(\n            endpoint_name, {}\n        )\n\n        if endpoint_data.get('deprecated'):\n            LOG.warning(\n                'Client is configured with the deprecated endpoint: %s'\n                % (endpoint_name)\n            )\n\n        service_defaults = service_data.get('defaults', {})\n        partition_defaults = partition.get('defaults', {})\n        tags = self._create_tag_list(use_dualstack_endpoint, use_fips_endpoint)\n\n        if tags:\n            result = self._resolve_variant(\n                tags, endpoint_data, service_defaults, partition_defaults\n            )\n            if result == {}:\n                error_msg = (\n                    f\"Endpoint does not exist for {service_name} \"\n                    f\"in region {endpoint_name}\"\n                )\n                raise EndpointVariantError(tags=tags, error_msg=error_msg)\n            self._merge_keys(endpoint_data, result)\n        else:\n            result = endpoint_data\n\n        # If dnsSuffix has not already been consumed from a variant definition\n        if 'dnsSuffix' not in result:\n            result['dnsSuffix'] = partition['dnsSuffix']\n\n        result['partition'] = partition['partition']\n        result['endpointName'] = endpoint_name\n\n        # Merge in the service defaults then the partition defaults.\n        self._merge_keys(service_defaults, result)\n        self._merge_keys(partition_defaults, result)\n\n        result['hostname'] = self._expand_template(\n            partition,\n            result['hostname'],\n            service_name,\n            endpoint_name,\n            result['dnsSuffix'],\n        )\n        if 'sslCommonName' in result:\n            result['sslCommonName'] = self._expand_template(\n                partition,\n                result['sslCommonName'],\n                service_name,\n                endpoint_name,\n                result['dnsSuffix'],\n            )\n\n        return result\n\n    def _merge_keys(self, from_data, result):\n        for key in from_data:\n            if key not in result:\n                result[key] = from_data[key]\n\n    def _expand_template(\n        self, partition, template, service_name, endpoint_name, dnsSuffix\n    ):\n        return template.format(\n            service=service_name, region=endpoint_name, dnsSuffix=dnsSuffix\n        )\n\n\nclass EndpointResolverBuiltins(str, Enum):\n    # The AWS Region configured for the SDK client (str)\n    AWS_REGION = \"AWS::Region\"\n    # Whether the UseFIPSEndpoint configuration option has been enabled for\n    # the SDK client (bool)\n    AWS_USE_FIPS = \"AWS::UseFIPS\"\n    # Whether the UseDualStackEndpoint configuration option has been enabled\n    # for the SDK client (bool)\n    AWS_USE_DUALSTACK = \"AWS::UseDualStack\"\n    # Whether the global endpoint should be used with STS, rather the the\n    # regional endpoint for us-east-1 (bool)\n    AWS_STS_USE_GLOBAL_ENDPOINT = \"AWS::STS::UseGlobalEndpoint\"\n    # Whether the global endpoint should be used with S3, rather then the\n    # regional endpoint for us-east-1 (bool)\n    AWS_S3_USE_GLOBAL_ENDPOINT = \"AWS::S3::UseGlobalEndpoint\"\n    # Whether S3 Transfer Acceleration has been requested (bool)\n    AWS_S3_ACCELERATE = \"AWS::S3::Accelerate\"\n    # Whether S3 Force Path Style has been enabled (bool)\n    AWS_S3_FORCE_PATH_STYLE = \"AWS::S3::ForcePathStyle\"\n    # Whether to use the ARN region or raise an error when ARN and client\n    # region differ (for s3 service only, bool)\n    AWS_S3_USE_ARN_REGION = \"AWS::S3::UseArnRegion\"\n    # Whether to use the ARN region or raise an error when ARN and client\n    # region differ (for s3-control service only, bool)\n    AWS_S3CONTROL_USE_ARN_REGION = 'AWS::S3Control::UseArnRegion'\n    # Whether multi-region access points (MRAP) should be disabled (bool)\n    AWS_S3_DISABLE_MRAP = \"AWS::S3::DisableMultiRegionAccessPoints\"\n    # Whether a custom endpoint has been configured (str)\n    SDK_ENDPOINT = \"SDK::Endpoint\"\n\n\nclass EndpointRulesetResolver:\n    \"\"\"Resolves endpoints using a service's endpoint ruleset\"\"\"\n\n    def __init__(\n        self,\n        endpoint_ruleset_data,\n        partition_data,\n        service_model,\n        builtins,\n        client_context,\n        event_emitter,\n        use_ssl=True,\n        requested_auth_scheme=None,\n    ):\n        self._provider = EndpointProvider(\n            ruleset_data=endpoint_ruleset_data,\n            partition_data=partition_data,\n        )\n        self._param_definitions = self._provider.ruleset.parameters\n        self._service_model = service_model\n        self._builtins = builtins\n        self._client_context = client_context\n        self._event_emitter = event_emitter\n        self._use_ssl = use_ssl\n        self._requested_auth_scheme = requested_auth_scheme\n        self._instance_cache = {}\n\n    def construct_endpoint(\n        self,\n        operation_model,\n        call_args,\n        request_context,\n    ):\n        \"\"\"Invokes the provider with params defined in the service's ruleset\"\"\"\n        if call_args is None:\n            call_args = {}\n\n        if request_context is None:\n            request_context = {}\n\n        provider_params = self._get_provider_params(\n            operation_model, call_args, request_context\n        )\n        LOG.debug(\n            'Calling endpoint provider with parameters: %s' % provider_params\n        )\n        try:\n            provider_result = self._provider.resolve_endpoint(\n                **provider_params\n            )\n        except EndpointProviderError as ex:\n            botocore_exception = self.ruleset_error_to_botocore_exception(\n                ex, provider_params\n            )\n            if botocore_exception is None:\n                raise\n            else:\n                raise botocore_exception from ex\n        LOG.debug('Endpoint provider result: %s' % provider_result.url)\n\n        # The endpoint provider does not support non-secure transport.\n        if not self._use_ssl and provider_result.url.startswith('https://'):\n            provider_result = provider_result._replace(\n                url=f'http://{provider_result.url[8:]}'\n            )\n\n        # Multi-valued headers are not supported in botocore. Replace the list\n        # of values returned for each header with just its first entry,\n        # dropping any additionally entries.\n        provider_result = provider_result._replace(\n            headers={\n                key: val[0] for key, val in provider_result.headers.items()\n            }\n        )\n\n        return provider_result\n\n    def _get_provider_params(\n        self, operation_model, call_args, request_context\n    ):\n        \"\"\"Resolve a value for each parameter defined in the service's ruleset\n\n        The resolution order for parameter values is:\n        1. Operation-specific static context values from the service definition\n        2. Operation-specific dynamic context values from API parameters\n        3. Client-specific context parameters\n        4. Built-in values such as region, FIPS usage, ...\n        \"\"\"\n        provider_params = {}\n        # Builtin values can be customized for each operation by hooks\n        # subscribing to the ``before-endpoint-resolution.*`` event.\n        customized_builtins = self._get_customized_builtins(\n            operation_model, call_args, request_context\n        )\n        for param_name, param_def in self._param_definitions.items():\n            param_val = self._resolve_param_from_context(\n                param_name=param_name,\n                operation_model=operation_model,\n                call_args=call_args,\n            )\n            if param_val is None and param_def.builtin is not None:\n                param_val = self._resolve_param_as_builtin(\n                    builtin_name=param_def.builtin,\n                    builtins=customized_builtins,\n                )\n            if param_val is not None:\n                provider_params[param_name] = param_val\n\n        return provider_params\n\n    def _resolve_param_from_context(\n        self, param_name, operation_model, call_args\n    ):\n        static = self._resolve_param_as_static_context_param(\n            param_name, operation_model\n        )\n        if static is not None:\n            return static\n        dynamic = self._resolve_param_as_dynamic_context_param(\n            param_name, operation_model, call_args\n        )\n        if dynamic is not None:\n            return dynamic\n        return self._resolve_param_as_client_context_param(param_name)\n\n    def _resolve_param_as_static_context_param(\n        self, param_name, operation_model\n    ):\n        static_ctx_params = self._get_static_context_params(operation_model)\n        return static_ctx_params.get(param_name)\n\n    def _resolve_param_as_dynamic_context_param(\n        self, param_name, operation_model, call_args\n    ):\n        dynamic_ctx_params = self._get_dynamic_context_params(operation_model)\n        if param_name in dynamic_ctx_params:\n            member_name = dynamic_ctx_params[param_name]\n            return call_args.get(member_name)\n\n    def _resolve_param_as_client_context_param(self, param_name):\n        client_ctx_params = self._get_client_context_params()\n        if param_name in client_ctx_params:\n            client_ctx_varname = client_ctx_params[param_name]\n            return self._client_context.get(client_ctx_varname)\n\n    def _resolve_param_as_builtin(self, builtin_name, builtins):\n        if builtin_name not in EndpointResolverBuiltins.__members__.values():\n            raise UnknownEndpointResolutionBuiltInName(name=builtin_name)\n        return builtins.get(builtin_name)\n\n    @instance_cache\n    def _get_static_context_params(self, operation_model):\n        \"\"\"Mapping of param names to static param value for an operation\"\"\"\n        return {\n            param.name: param.value\n            for param in operation_model.static_context_parameters\n        }\n\n    @instance_cache\n    def _get_dynamic_context_params(self, operation_model):\n        \"\"\"Mapping of param names to member names for an operation\"\"\"\n        return {\n            param.name: param.member_name\n            for param in operation_model.context_parameters\n        }\n\n    @instance_cache\n    def _get_client_context_params(self):\n        \"\"\"Mapping of param names to client configuration variable\"\"\"\n        return {\n            param.name: xform_name(param.name)\n            for param in self._service_model.client_context_parameters\n        }\n\n    def _get_customized_builtins(\n        self, operation_model, call_args, request_context\n    ):\n        service_id = self._service_model.service_id.hyphenize()\n        customized_builtins = copy.copy(self._builtins)\n        # Handlers are expected to modify the builtins dict in place.\n        self._event_emitter.emit(\n            'before-endpoint-resolution.%s' % service_id,\n            builtins=customized_builtins,\n            model=operation_model,\n            params=call_args,\n            context=request_context,\n        )\n        return customized_builtins\n\n    def auth_schemes_to_signing_ctx(self, auth_schemes):\n        \"\"\"Convert an Endpoint's authSchemes property to a signing_context dict\n\n        :type auth_schemes: list\n        :param auth_schemes: A list of dictionaries taken from the\n            ``authSchemes`` property of an Endpoint object returned by\n            ``EndpointProvider``.\n\n        :rtype: str, dict\n        :return: Tuple of auth type string (to be used in\n            ``request_context['auth_type']``) and signing context dict (for use\n            in ``request_context['signing']``).\n        \"\"\"\n        if not isinstance(auth_schemes, list) or len(auth_schemes) == 0:\n            raise TypeError(\"auth_schemes must be a non-empty list.\")\n\n        LOG.debug(\n            'Selecting from endpoint provider\\'s list of auth schemes: %s. '\n            'User selected auth scheme is: \"%s\"',\n            ', '.join([f'\"{s.get(\"name\")}\"' for s in auth_schemes]),\n            self._requested_auth_scheme,\n        )\n\n        if self._requested_auth_scheme == UNSIGNED:\n            return 'none', {}\n\n        auth_schemes = [\n            {**scheme, 'name': self._strip_sig_prefix(scheme['name'])}\n            for scheme in auth_schemes\n        ]\n        if self._requested_auth_scheme is not None:\n            try:\n                # Use the first scheme that matches the requested scheme,\n                # after accounting for naming differences between botocore and\n                # endpoint rulesets. Keep the requested name.\n                name, scheme = next(\n                    (self._requested_auth_scheme, s)\n                    for s in auth_schemes\n                    if self._does_botocore_authname_match_ruleset_authname(\n                        self._requested_auth_scheme, s['name']\n                    )\n                )\n            except StopIteration:\n                # For legacy signers, no match will be found. Do not raise an\n                # exception, instead default to the logic in botocore\n                # customizations.\n                return None, {}\n        else:\n            try:\n                name, scheme = next(\n                    (s['name'], s)\n                    for s in auth_schemes\n                    if s['name'] in AUTH_TYPE_MAPS\n                )\n            except StopIteration:\n                # If no auth scheme was specifically requested and an\n                # authSchemes list is present in the Endpoint object but none\n                # of the entries are supported, raise an exception.\n                fixable_with_crt = False\n                auth_type_options = [s['name'] for s in auth_schemes]\n                if not HAS_CRT:\n                    fixable_with_crt = any(\n                        scheme in CRT_SUPPORTED_AUTH_TYPES\n                        for scheme in auth_type_options\n                    )\n\n                if fixable_with_crt:\n                    raise MissingDependencyException(\n                        msg='This operation requires an additional dependency.'\n                        ' Use pip install botocore[crt] before proceeding.'\n                    )\n                else:\n                    raise UnknownSignatureVersionError(\n                        signature_version=', '.join(auth_type_options)\n                    )\n\n        signing_context = {}\n        if 'signingRegion' in scheme:\n            signing_context['region'] = scheme['signingRegion']\n        elif 'signingRegionSet' in scheme:\n            if len(scheme['signingRegionSet']) > 0:\n                signing_context['region'] = scheme['signingRegionSet'][0]\n        if 'signingName' in scheme:\n            signing_context.update(signing_name=scheme['signingName'])\n        if 'disableDoubleEncoding' in scheme:\n            signing_context['disableDoubleEncoding'] = ensure_boolean(\n                scheme['disableDoubleEncoding']\n            )\n\n        LOG.debug(\n            'Selected auth type \"%s\" as \"%s\" with signing context params: %s',\n            scheme['name'],  # original name without \"sig\"\n            name,  # chosen name can differ when `signature_version` is set\n            signing_context,\n        )\n        return name, signing_context\n\n    def _strip_sig_prefix(self, auth_name):\n        \"\"\"Normalize auth type names by removing any \"sig\" prefix\"\"\"\n        return auth_name[3:] if auth_name.startswith('sig') else auth_name\n\n    def _does_botocore_authname_match_ruleset_authname(self, botoname, rsname):\n        \"\"\"\n        Whether a valid string provided as signature_version parameter for\n        client construction refers to the same auth methods as a string\n        returned by the endpoint ruleset provider. This accounts for:\n\n        * The ruleset prefixes auth names with \"sig\"\n        * The s3 and s3control rulesets don't distinguish between v4[a] and\n          s3v4[a] signers\n        * The v2, v3, and HMAC v1 based signers (s3, s3-*) are botocore legacy\n          features and do not exist in the rulesets\n        * Only characters up to the first dash are considered\n\n        Example matches:\n        * v4, sigv4\n        * v4, v4\n        * s3v4, sigv4\n        * s3v7, sigv7 (hypothetical example)\n        * s3v4a, sigv4a\n        * s3v4-query, sigv4\n\n        Example mismatches:\n        * v4a, sigv4\n        * s3, sigv4\n        * s3-presign-post, sigv4\n        \"\"\"\n        rsname = self._strip_sig_prefix(rsname)\n        botoname = botoname.split('-')[0]\n        if botoname != 's3' and botoname.startswith('s3'):\n            botoname = botoname[2:]\n        return rsname == botoname\n\n    def ruleset_error_to_botocore_exception(self, ruleset_exception, params):\n        \"\"\"Attempts to translate ruleset errors to pre-existing botocore\n        exception types by string matching exception strings.\n        \"\"\"\n        msg = ruleset_exception.kwargs.get('msg')\n        if msg is None:\n            return\n\n        if msg.startswith('Invalid region in ARN: '):\n            # Example message:\n            # \"Invalid region in ARN: `us-we$t-2` (invalid DNS name)\"\n            try:\n                label = msg.split('`')[1]\n            except IndexError:\n                label = msg\n            return InvalidHostLabelError(label=label)\n\n        service_name = self._service_model.service_name\n        if service_name == 's3':\n            if (\n                msg == 'S3 Object Lambda does not support S3 Accelerate'\n                or msg == 'Accelerate cannot be used with FIPS'\n            ):\n                return UnsupportedS3ConfigurationError(msg=msg)\n            if (\n                msg.startswith('S3 Outposts does not support')\n                or msg.startswith('S3 MRAP does not support')\n                or msg.startswith('S3 Object Lambda does not support')\n                or msg.startswith('Access Points do not support')\n                or msg.startswith('Invalid configuration:')\n                or msg.startswith('Client was configured for partition')\n            ):\n                return UnsupportedS3AccesspointConfigurationError(msg=msg)\n            if msg.lower().startswith('invalid arn:'):\n                return ParamValidationError(report=msg)\n        if service_name == 's3control':\n            if msg.startswith('Invalid ARN:'):\n                arn = params.get('Bucket')\n                return UnsupportedS3ControlArnError(arn=arn, msg=msg)\n            if msg.startswith('Invalid configuration:') or msg.startswith(\n                'Client was configured for partition'\n            ):\n                return UnsupportedS3ControlConfigurationError(msg=msg)\n            if msg == \"AccountId is required but not set\":\n                return ParamValidationError(report=msg)\n        if service_name == 'events':\n            if msg.startswith(\n                'Invalid Configuration: FIPS is not supported with '\n                'EventBridge multi-region endpoints.'\n            ):\n                return InvalidEndpointConfigurationError(msg=msg)\n            if msg == 'EndpointId must be a valid host label.':\n                return InvalidEndpointConfigurationError(msg=msg)\n        return None\n", "botocore/exceptions.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom botocore.vendored import requests\nfrom botocore.vendored.requests.packages import urllib3\n\n\ndef _exception_from_packed_args(exception_cls, args=None, kwargs=None):\n    # This is helpful for reducing Exceptions that only accept kwargs as\n    # only positional arguments can be provided for __reduce__\n    # Ideally, this would also be a class method on the BotoCoreError\n    # but instance methods cannot be pickled.\n    if args is None:\n        args = ()\n    if kwargs is None:\n        kwargs = {}\n    return exception_cls(*args, **kwargs)\n\n\nclass BotoCoreError(Exception):\n    \"\"\"\n    The base exception class for BotoCore exceptions.\n\n    :ivar msg: The descriptive message associated with the error.\n    \"\"\"\n\n    fmt = 'An unspecified error occurred'\n\n    def __init__(self, **kwargs):\n        msg = self.fmt.format(**kwargs)\n        Exception.__init__(self, msg)\n        self.kwargs = kwargs\n\n    def __reduce__(self):\n        return _exception_from_packed_args, (self.__class__, None, self.kwargs)\n\n\nclass DataNotFoundError(BotoCoreError):\n    \"\"\"\n    The data associated with a particular path could not be loaded.\n\n    :ivar data_path: The data path that the user attempted to load.\n    \"\"\"\n\n    fmt = 'Unable to load data for: {data_path}'\n\n\nclass UnknownServiceError(DataNotFoundError):\n    \"\"\"Raised when trying to load data for an unknown service.\n\n    :ivar service_name: The name of the unknown service.\n\n    \"\"\"\n\n    fmt = (\n        \"Unknown service: '{service_name}'. Valid service names are: \"\n        \"{known_service_names}\"\n    )\n\n\nclass UnknownRegionError(BotoCoreError):\n    \"\"\"Raised when trying to load data for an unknown region.\n\n    :ivar region_name: The name of the unknown region.\n\n    \"\"\"\n\n    fmt = \"Unknown region: '{region_name}'. {error_msg}\"\n\n\nclass ApiVersionNotFoundError(BotoCoreError):\n    \"\"\"\n    The data associated with either the API version or a compatible one\n    could not be loaded.\n\n    :ivar data_path: The data path that the user attempted to load.\n    :ivar api_version: The API version that the user attempted to load.\n    \"\"\"\n\n    fmt = 'Unable to load data {data_path} for: {api_version}'\n\n\nclass HTTPClientError(BotoCoreError):\n    fmt = 'An HTTP Client raised an unhandled exception: {error}'\n\n    def __init__(self, request=None, response=None, **kwargs):\n        self.request = request\n        self.response = response\n        super().__init__(**kwargs)\n\n    def __reduce__(self):\n        return _exception_from_packed_args, (\n            self.__class__,\n            (self.request, self.response),\n            self.kwargs,\n        )\n\n\nclass ConnectionError(BotoCoreError):\n    fmt = 'An HTTP Client failed to establish a connection: {error}'\n\n\nclass InvalidIMDSEndpointError(BotoCoreError):\n    fmt = 'Invalid endpoint EC2 Instance Metadata endpoint: {endpoint}'\n\n\nclass InvalidIMDSEndpointModeError(BotoCoreError):\n    fmt = (\n        'Invalid EC2 Instance Metadata endpoint mode: {mode}'\n        ' Valid endpoint modes (case-insensitive): {valid_modes}.'\n    )\n\n\nclass EndpointConnectionError(ConnectionError):\n    fmt = 'Could not connect to the endpoint URL: \"{endpoint_url}\"'\n\n\nclass SSLError(ConnectionError, requests.exceptions.SSLError):\n    fmt = 'SSL validation failed for {endpoint_url} {error}'\n\n\nclass ConnectionClosedError(HTTPClientError):\n    fmt = (\n        'Connection was closed before we received a valid response '\n        'from endpoint URL: \"{endpoint_url}\".'\n    )\n\n\nclass ReadTimeoutError(\n    HTTPClientError,\n    requests.exceptions.ReadTimeout,\n    urllib3.exceptions.ReadTimeoutError,\n):\n    fmt = 'Read timeout on endpoint URL: \"{endpoint_url}\"'\n\n\nclass ConnectTimeoutError(ConnectionError, requests.exceptions.ConnectTimeout):\n    fmt = 'Connect timeout on endpoint URL: \"{endpoint_url}\"'\n\n\nclass ProxyConnectionError(ConnectionError, requests.exceptions.ProxyError):\n    fmt = 'Failed to connect to proxy URL: \"{proxy_url}\"'\n\n\nclass ResponseStreamingError(HTTPClientError):\n    fmt = 'An error occurred while reading from response stream: {error}'\n\n\nclass NoCredentialsError(BotoCoreError):\n    \"\"\"\n    No credentials could be found.\n    \"\"\"\n\n    fmt = 'Unable to locate credentials'\n\n\nclass NoAuthTokenError(BotoCoreError):\n    \"\"\"\n    No authorization token could be found.\n    \"\"\"\n\n    fmt = 'Unable to locate authorization token'\n\n\nclass TokenRetrievalError(BotoCoreError):\n    \"\"\"\n    Error attempting to retrieve a token from a remote source.\n\n    :ivar provider: The name of the token provider.\n    :ivar error_msg: The msg explaining why the token could not be retrieved.\n\n    \"\"\"\n\n    fmt = 'Error when retrieving token from {provider}: {error_msg}'\n\n\nclass PartialCredentialsError(BotoCoreError):\n    \"\"\"\n    Only partial credentials were found.\n\n    :ivar cred_var: The missing credential variable name.\n\n    \"\"\"\n\n    fmt = 'Partial credentials found in {provider}, missing: {cred_var}'\n\n\nclass CredentialRetrievalError(BotoCoreError):\n    \"\"\"\n    Error attempting to retrieve credentials from a remote source.\n\n    :ivar provider: The name of the credential provider.\n    :ivar error_msg: The msg explaining why credentials could not be\n        retrieved.\n\n    \"\"\"\n\n    fmt = 'Error when retrieving credentials from {provider}: {error_msg}'\n\n\nclass UnknownSignatureVersionError(BotoCoreError):\n    \"\"\"\n    Requested Signature Version is not known.\n\n    :ivar signature_version: The name of the requested signature version.\n    \"\"\"\n\n    fmt = 'Unknown Signature Version: {signature_version}.'\n\n\nclass ServiceNotInRegionError(BotoCoreError):\n    \"\"\"\n    The service is not available in requested region.\n\n    :ivar service_name: The name of the service.\n    :ivar region_name: The name of the region.\n    \"\"\"\n\n    fmt = 'Service {service_name} not available in region {region_name}'\n\n\nclass BaseEndpointResolverError(BotoCoreError):\n    \"\"\"Base error for endpoint resolving errors.\n\n    Should never be raised directly, but clients can catch\n    this exception if they want to generically handle any errors\n    during the endpoint resolution process.\n\n    \"\"\"\n\n\nclass NoRegionError(BaseEndpointResolverError):\n    \"\"\"No region was specified.\"\"\"\n\n    fmt = 'You must specify a region.'\n\n\nclass EndpointVariantError(BaseEndpointResolverError):\n    \"\"\"\n    Could not construct modeled endpoint variant.\n\n    :ivar error_msg: The message explaining why the modeled endpoint variant\n        is unable to be constructed.\n\n    \"\"\"\n\n    fmt = (\n        'Unable to construct a modeled endpoint with the following '\n        'variant(s) {tags}: '\n    )\n\n\nclass UnknownEndpointError(BaseEndpointResolverError, ValueError):\n    \"\"\"\n    Could not construct an endpoint.\n\n    :ivar service_name: The name of the service.\n    :ivar region_name: The name of the region.\n    \"\"\"\n\n    fmt = (\n        'Unable to construct an endpoint for '\n        '{service_name} in region {region_name}'\n    )\n\n\nclass UnknownFIPSEndpointError(BaseEndpointResolverError):\n    \"\"\"\n    Could not construct a FIPS endpoint.\n\n    :ivar service_name: The name of the service.\n    :ivar region_name: The name of the region.\n    \"\"\"\n\n    fmt = (\n        'The provided FIPS pseudo-region \"{region_name}\" is not known for '\n        'the service \"{service_name}\". A FIPS compliant endpoint cannot be '\n        'constructed.'\n    )\n\n\nclass ProfileNotFound(BotoCoreError):\n    \"\"\"\n    The specified configuration profile was not found in the\n    configuration file.\n\n    :ivar profile: The name of the profile the user attempted to load.\n    \"\"\"\n\n    fmt = 'The config profile ({profile}) could not be found'\n\n\nclass ConfigParseError(BotoCoreError):\n    \"\"\"\n    The configuration file could not be parsed.\n\n    :ivar path: The path to the configuration file.\n    \"\"\"\n\n    fmt = 'Unable to parse config file: {path}'\n\n\nclass ConfigNotFound(BotoCoreError):\n    \"\"\"\n    The specified configuration file could not be found.\n\n    :ivar path: The path to the configuration file.\n    \"\"\"\n\n    fmt = 'The specified config file ({path}) could not be found.'\n\n\nclass MissingParametersError(BotoCoreError):\n    \"\"\"\n    One or more required parameters were not supplied.\n\n    :ivar object: The object that has missing parameters.\n        This can be an operation or a parameter (in the\n        case of inner params).  The str() of this object\n        will be used so it doesn't need to implement anything\n        other than str().\n    :ivar missing: The names of the missing parameters.\n    \"\"\"\n\n    fmt = (\n        'The following required parameters are missing for '\n        '{object_name}: {missing}'\n    )\n\n\nclass ValidationError(BotoCoreError):\n    \"\"\"\n    An exception occurred validating parameters.\n\n    Subclasses must accept a ``value`` and ``param``\n    argument in their ``__init__``.\n\n    :ivar value: The value that was being validated.\n    :ivar param: The parameter that failed validation.\n    :ivar type_name: The name of the underlying type.\n    \"\"\"\n\n    fmt = \"Invalid value ('{value}') for param {param} \" \"of type {type_name} \"\n\n\nclass ParamValidationError(BotoCoreError):\n    fmt = 'Parameter validation failed:\\n{report}'\n\n\n# These exceptions subclass from ValidationError so that code\n# can just 'except ValidationError' to catch any possibly validation\n# error.\nclass UnknownKeyError(ValidationError):\n    \"\"\"\n    Unknown key in a struct parameter.\n\n    :ivar value: The value that was being checked.\n    :ivar param: The name of the parameter.\n    :ivar choices: The valid choices the value can be.\n    \"\"\"\n\n    fmt = (\n        \"Unknown key '{value}' for param '{param}'.  Must be one \"\n        \"of: {choices}\"\n    )\n\n\nclass RangeError(ValidationError):\n    \"\"\"\n    A parameter value was out of the valid range.\n\n    :ivar value: The value that was being checked.\n    :ivar param: The parameter that failed validation.\n    :ivar min_value: The specified minimum value.\n    :ivar max_value: The specified maximum value.\n    \"\"\"\n\n    fmt = (\n        'Value out of range for param {param}: '\n        '{min_value} <= {value} <= {max_value}'\n    )\n\n\nclass UnknownParameterError(ValidationError):\n    \"\"\"\n    Unknown top level parameter.\n\n    :ivar name: The name of the unknown parameter.\n    :ivar operation: The name of the operation.\n    :ivar choices: The valid choices the parameter name can be.\n    \"\"\"\n\n    fmt = (\n        \"Unknown parameter '{name}' for operation {operation}.  Must be one \"\n        \"of: {choices}\"\n    )\n\n\nclass InvalidRegionError(ValidationError, ValueError):\n    \"\"\"\n    Invalid region_name provided to client or resource.\n\n    :ivar region_name: region_name that was being validated.\n    \"\"\"\n\n    fmt = \"Provided region_name '{region_name}' doesn't match a supported format.\"\n\n\nclass AliasConflictParameterError(ValidationError):\n    \"\"\"\n    Error when an alias is provided for a parameter as well as the original.\n\n    :ivar original: The name of the original parameter.\n    :ivar alias: The name of the alias\n    :ivar operation: The name of the operation.\n    \"\"\"\n\n    fmt = (\n        \"Parameter '{original}' and its alias '{alias}' were provided \"\n        \"for operation {operation}.  Only one of them may be used.\"\n    )\n\n\nclass UnknownServiceStyle(BotoCoreError):\n    \"\"\"\n    Unknown style of service invocation.\n\n    :ivar service_style: The style requested.\n    \"\"\"\n\n    fmt = 'The service style ({service_style}) is not understood.'\n\n\nclass PaginationError(BotoCoreError):\n    fmt = 'Error during pagination: {message}'\n\n\nclass OperationNotPageableError(BotoCoreError):\n    fmt = 'Operation cannot be paginated: {operation_name}'\n\n\nclass ChecksumError(BotoCoreError):\n    \"\"\"The expected checksum did not match the calculated checksum.\"\"\"\n\n    fmt = (\n        'Checksum {checksum_type} failed, expected checksum '\n        '{expected_checksum} did not match calculated checksum '\n        '{actual_checksum}.'\n    )\n\n\nclass UnseekableStreamError(BotoCoreError):\n    \"\"\"Need to seek a stream, but stream does not support seeking.\"\"\"\n\n    fmt = (\n        'Need to rewind the stream {stream_object}, but stream '\n        'is not seekable.'\n    )\n\n\nclass WaiterError(BotoCoreError):\n    \"\"\"Waiter failed to reach desired state.\"\"\"\n\n    fmt = 'Waiter {name} failed: {reason}'\n\n    def __init__(self, name, reason, last_response):\n        super().__init__(name=name, reason=reason)\n        self.last_response = last_response\n\n\nclass IncompleteReadError(BotoCoreError):\n    \"\"\"HTTP response did not return expected number of bytes.\"\"\"\n\n    fmt = (\n        '{actual_bytes} read, but total bytes ' 'expected is {expected_bytes}.'\n    )\n\n\nclass InvalidExpressionError(BotoCoreError):\n    \"\"\"Expression is either invalid or too complex.\"\"\"\n\n    fmt = 'Invalid expression {expression}: Only dotted lookups are supported.'\n\n\nclass UnknownCredentialError(BotoCoreError):\n    \"\"\"Tried to insert before/after an unregistered credential type.\"\"\"\n\n    fmt = 'Credential named {name} not found.'\n\n\nclass WaiterConfigError(BotoCoreError):\n    \"\"\"Error when processing waiter configuration.\"\"\"\n\n    fmt = 'Error processing waiter config: {error_msg}'\n\n\nclass UnknownClientMethodError(BotoCoreError):\n    \"\"\"Error when trying to access a method on a client that does not exist.\"\"\"\n\n    fmt = 'Client does not have method: {method_name}'\n\n\nclass UnsupportedSignatureVersionError(BotoCoreError):\n    \"\"\"Error when trying to use an unsupported Signature Version.\"\"\"\n\n    fmt = 'Signature version is not supported: {signature_version}'\n\n\nclass ClientError(Exception):\n    MSG_TEMPLATE = (\n        'An error occurred ({error_code}) when calling the {operation_name} '\n        'operation{retry_info}: {error_message}'\n    )\n\n    def __init__(self, error_response, operation_name):\n        retry_info = self._get_retry_info(error_response)\n        error = error_response.get('Error', {})\n        msg = self.MSG_TEMPLATE.format(\n            error_code=error.get('Code', 'Unknown'),\n            error_message=error.get('Message', 'Unknown'),\n            operation_name=operation_name,\n            retry_info=retry_info,\n        )\n        super().__init__(msg)\n        self.response = error_response\n        self.operation_name = operation_name\n\n    def _get_retry_info(self, response):\n        retry_info = ''\n        if 'ResponseMetadata' in response:\n            metadata = response['ResponseMetadata']\n            if metadata.get('MaxAttemptsReached', False):\n                if 'RetryAttempts' in metadata:\n                    retry_info = (\n                        f\" (reached max retries: {metadata['RetryAttempts']})\"\n                    )\n        return retry_info\n\n    def __reduce__(self):\n        # Subclasses of ClientError's are dynamically generated and\n        # cannot be pickled unless they are attributes of a\n        # module. So at the very least return a ClientError back.\n        return ClientError, (self.response, self.operation_name)\n\n\nclass EventStreamError(ClientError):\n    pass\n\n\nclass UnsupportedTLSVersionWarning(Warning):\n    \"\"\"Warn when an openssl version that uses TLS 1.2 is required\"\"\"\n\n    pass\n\n\nclass ImminentRemovalWarning(Warning):\n    pass\n\n\nclass InvalidDNSNameError(BotoCoreError):\n    \"\"\"Error when virtual host path is forced on a non-DNS compatible bucket\"\"\"\n\n    fmt = (\n        'Bucket named {bucket_name} is not DNS compatible. Virtual '\n        'hosted-style addressing cannot be used. The addressing style '\n        'can be configured by removing the addressing_style value '\n        'or setting that value to \\'path\\' or \\'auto\\' in the AWS Config '\n        'file or in the botocore.client.Config object.'\n    )\n\n\nclass InvalidS3AddressingStyleError(BotoCoreError):\n    \"\"\"Error when an invalid path style is specified\"\"\"\n\n    fmt = (\n        'S3 addressing style {s3_addressing_style} is invalid. Valid options '\n        'are: \\'auto\\', \\'virtual\\', and \\'path\\''\n    )\n\n\nclass UnsupportedS3ArnError(BotoCoreError):\n    \"\"\"Error when S3 ARN provided to Bucket parameter is not supported\"\"\"\n\n    fmt = (\n        'S3 ARN {arn} provided to \"Bucket\" parameter is invalid. Only '\n        'ARNs for S3 access-points are supported.'\n    )\n\n\nclass UnsupportedS3ControlArnError(BotoCoreError):\n    \"\"\"Error when S3 ARN provided to S3 control parameter is not supported\"\"\"\n\n    fmt = 'S3 ARN \"{arn}\" provided is invalid for this operation. {msg}'\n\n\nclass InvalidHostLabelError(BotoCoreError):\n    \"\"\"Error when an invalid host label would be bound to an endpoint\"\"\"\n\n    fmt = (\n        'Invalid host label to be bound to the hostname of the endpoint: '\n        '\"{label}\".'\n    )\n\n\nclass UnsupportedOutpostResourceError(BotoCoreError):\n    \"\"\"Error when S3 Outpost ARN provided to Bucket parameter is incomplete\"\"\"\n\n    fmt = (\n        'S3 Outpost ARN resource \"{resource_name}\" provided to \"Bucket\" '\n        'parameter is invalid. Only ARNs for S3 Outpost arns with an '\n        'access-point sub-resource are supported.'\n    )\n\n\nclass UnsupportedS3ConfigurationError(BotoCoreError):\n    \"\"\"Error when an unsupported configuration is used with access-points\"\"\"\n\n    fmt = 'Unsupported configuration when using S3: {msg}'\n\n\nclass UnsupportedS3AccesspointConfigurationError(BotoCoreError):\n    \"\"\"Error when an unsupported configuration is used with access-points\"\"\"\n\n    fmt = 'Unsupported configuration when using S3 access-points: {msg}'\n\n\nclass InvalidEndpointDiscoveryConfigurationError(BotoCoreError):\n    \"\"\"Error when invalid value supplied for endpoint_discovery_enabled\"\"\"\n\n    fmt = (\n        'Unsupported configuration value for endpoint_discovery_enabled. '\n        'Expected one of (\"true\", \"false\", \"auto\") but got {config_value}.'\n    )\n\n\nclass UnsupportedS3ControlConfigurationError(BotoCoreError):\n    \"\"\"Error when an unsupported configuration is used with S3 Control\"\"\"\n\n    fmt = 'Unsupported configuration when using S3 Control: {msg}'\n\n\nclass InvalidRetryConfigurationError(BotoCoreError):\n    \"\"\"Error when invalid retry configuration is specified\"\"\"\n\n    fmt = (\n        'Cannot provide retry configuration for \"{retry_config_option}\". '\n        'Valid retry configuration options are: {valid_options}'\n    )\n\n\nclass InvalidMaxRetryAttemptsError(InvalidRetryConfigurationError):\n    \"\"\"Error when invalid retry configuration is specified\"\"\"\n\n    fmt = (\n        'Value provided to \"max_attempts\": {provided_max_attempts} must '\n        'be an integer greater than or equal to {min_value}.'\n    )\n\n\nclass InvalidRetryModeError(InvalidRetryConfigurationError):\n    \"\"\"Error when invalid retry mode configuration is specified\"\"\"\n\n    fmt = (\n        'Invalid value provided to \"mode\": \"{provided_retry_mode}\" must '\n        'be one of: {valid_modes}'\n    )\n\n\nclass InvalidS3UsEast1RegionalEndpointConfigError(BotoCoreError):\n    \"\"\"Error for invalid s3 us-east-1 regional endpoints configuration\"\"\"\n\n    fmt = (\n        'S3 us-east-1 regional endpoint option '\n        '{s3_us_east_1_regional_endpoint_config} is '\n        'invalid. Valid options are: \"legacy\", \"regional\"'\n    )\n\n\nclass InvalidSTSRegionalEndpointsConfigError(BotoCoreError):\n    \"\"\"Error when invalid sts regional endpoints configuration is specified\"\"\"\n\n    fmt = (\n        'STS regional endpoints option {sts_regional_endpoints_config} is '\n        'invalid. Valid options are: \"legacy\", \"regional\"'\n    )\n\n\nclass StubResponseError(BotoCoreError):\n    fmt = (\n        'Error getting response stub for operation {operation_name}: {reason}'\n    )\n\n\nclass StubAssertionError(StubResponseError, AssertionError):\n    pass\n\n\nclass UnStubbedResponseError(StubResponseError):\n    pass\n\n\nclass InvalidConfigError(BotoCoreError):\n    fmt = '{error_msg}'\n\n\nclass InfiniteLoopConfigError(InvalidConfigError):\n    fmt = (\n        'Infinite loop in credential configuration detected. Attempting to '\n        'load from profile {source_profile} which has already been visited. '\n        'Visited profiles: {visited_profiles}'\n    )\n\n\nclass RefreshWithMFAUnsupportedError(BotoCoreError):\n    fmt = 'Cannot refresh credentials: MFA token required.'\n\n\nclass MD5UnavailableError(BotoCoreError):\n    fmt = \"This system does not support MD5 generation.\"\n\n\nclass MissingDependencyException(BotoCoreError):\n    fmt = \"Missing Dependency: {msg}\"\n\n\nclass MetadataRetrievalError(BotoCoreError):\n    fmt = \"Error retrieving metadata: {error_msg}\"\n\n\nclass UndefinedModelAttributeError(Exception):\n    pass\n\n\nclass MissingServiceIdError(UndefinedModelAttributeError):\n    fmt = (\n        \"The model being used for the service {service_name} is missing the \"\n        \"serviceId metadata property, which is required.\"\n    )\n\n    def __init__(self, **kwargs):\n        msg = self.fmt.format(**kwargs)\n        Exception.__init__(self, msg)\n        self.kwargs = kwargs\n\n\nclass SSOError(BotoCoreError):\n    fmt = (\n        \"An unspecified error happened when resolving AWS credentials or an \"\n        \"access token from SSO.\"\n    )\n\n\nclass SSOTokenLoadError(SSOError):\n    fmt = \"Error loading SSO Token: {error_msg}\"\n\n\nclass UnauthorizedSSOTokenError(SSOError):\n    fmt = (\n        \"The SSO session associated with this profile has expired or is \"\n        \"otherwise invalid. To refresh this SSO session run aws sso login \"\n        \"with the corresponding profile.\"\n    )\n\n\nclass CapacityNotAvailableError(BotoCoreError):\n    fmt = 'Insufficient request capacity available.'\n\n\nclass InvalidProxiesConfigError(BotoCoreError):\n    fmt = 'Invalid configuration value(s) provided for proxies_config.'\n\n\nclass InvalidDefaultsMode(BotoCoreError):\n    fmt = (\n        'Client configured with invalid defaults mode: {mode}. '\n        'Valid defaults modes include: {valid_modes}.'\n    )\n\n\nclass AwsChunkedWrapperError(BotoCoreError):\n    fmt = '{error_msg}'\n\n\nclass FlexibleChecksumError(BotoCoreError):\n    fmt = '{error_msg}'\n\n\nclass InvalidEndpointConfigurationError(BotoCoreError):\n    fmt = 'Invalid endpoint configuration: {msg}'\n\n\nclass EndpointProviderError(BotoCoreError):\n    \"\"\"Base error for the EndpointProvider class\"\"\"\n\n    fmt = '{msg}'\n\n\nclass EndpointResolutionError(EndpointProviderError):\n    \"\"\"Error when input parameters resolve to an error rule\"\"\"\n\n    fmt = '{msg}'\n\n\nclass UnknownEndpointResolutionBuiltInName(EndpointProviderError):\n    fmt = 'Unknown builtin variable name: {name}'\n", "botocore/retryhandler.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport functools\nimport logging\nimport random\nfrom binascii import crc32\n\nfrom botocore.exceptions import (\n    ChecksumError,\n    ConnectionClosedError,\n    ConnectionError,\n    EndpointConnectionError,\n    ReadTimeoutError,\n)\n\nlogger = logging.getLogger(__name__)\n# The only supported error for now is GENERAL_CONNECTION_ERROR\n# which maps to requests generic ConnectionError.  If we're able\n# to get more specific exceptions from requests we can update\n# this mapping with more specific exceptions.\nEXCEPTION_MAP = {\n    'GENERAL_CONNECTION_ERROR': [\n        ConnectionError,\n        ConnectionClosedError,\n        ReadTimeoutError,\n        EndpointConnectionError,\n    ],\n}\n\n\ndef delay_exponential(base, growth_factor, attempts):\n    \"\"\"Calculate time to sleep based on exponential function.\n\n    The format is::\n\n        base * growth_factor ^ (attempts - 1)\n\n    If ``base`` is set to 'rand' then a random number between\n    0 and 1 will be used as the base.\n    Base must be greater than 0, otherwise a ValueError will be\n    raised.\n\n    \"\"\"\n    if base == 'rand':\n        base = random.random()\n    elif base <= 0:\n        raise ValueError(\n            f\"The 'base' param must be greater than 0, got: {base}\"\n        )\n    time_to_sleep = base * (growth_factor ** (attempts - 1))\n    return time_to_sleep\n\n\ndef create_exponential_delay_function(base, growth_factor):\n    \"\"\"Create an exponential delay function based on the attempts.\n\n    This is used so that you only have to pass it the attempts\n    parameter to calculate the delay.\n\n    \"\"\"\n    return functools.partial(\n        delay_exponential, base=base, growth_factor=growth_factor\n    )\n\n\ndef create_retry_handler(config, operation_name=None):\n    checker = create_checker_from_retry_config(\n        config, operation_name=operation_name\n    )\n    action = create_retry_action_from_config(\n        config, operation_name=operation_name\n    )\n    return RetryHandler(checker=checker, action=action)\n\n\ndef create_retry_action_from_config(config, operation_name=None):\n    # The spec has the possibility of supporting per policy\n    # actions, but right now, we assume this comes from the\n    # default section, which means that delay functions apply\n    # for every policy in the retry config (per service).\n    delay_config = config['__default__']['delay']\n    if delay_config['type'] == 'exponential':\n        return create_exponential_delay_function(\n            base=delay_config['base'],\n            growth_factor=delay_config['growth_factor'],\n        )\n\n\ndef create_checker_from_retry_config(config, operation_name=None):\n    checkers = []\n    max_attempts = None\n    retryable_exceptions = []\n    if '__default__' in config:\n        policies = config['__default__'].get('policies', [])\n        max_attempts = config['__default__']['max_attempts']\n        for key in policies:\n            current_config = policies[key]\n            checkers.append(_create_single_checker(current_config))\n            retry_exception = _extract_retryable_exception(current_config)\n            if retry_exception is not None:\n                retryable_exceptions.extend(retry_exception)\n    if operation_name is not None and config.get(operation_name) is not None:\n        operation_policies = config[operation_name]['policies']\n        for key in operation_policies:\n            checkers.append(_create_single_checker(operation_policies[key]))\n            retry_exception = _extract_retryable_exception(\n                operation_policies[key]\n            )\n            if retry_exception is not None:\n                retryable_exceptions.extend(retry_exception)\n    if len(checkers) == 1:\n        # Don't need to use a MultiChecker\n        return MaxAttemptsDecorator(checkers[0], max_attempts=max_attempts)\n    else:\n        multi_checker = MultiChecker(checkers)\n        return MaxAttemptsDecorator(\n            multi_checker,\n            max_attempts=max_attempts,\n            retryable_exceptions=tuple(retryable_exceptions),\n        )\n\n\ndef _create_single_checker(config):\n    if 'response' in config['applies_when']:\n        return _create_single_response_checker(\n            config['applies_when']['response']\n        )\n    elif 'socket_errors' in config['applies_when']:\n        return ExceptionRaiser()\n\n\ndef _create_single_response_checker(response):\n    if 'service_error_code' in response:\n        checker = ServiceErrorCodeChecker(\n            status_code=response['http_status_code'],\n            error_code=response['service_error_code'],\n        )\n    elif 'http_status_code' in response:\n        checker = HTTPStatusCodeChecker(\n            status_code=response['http_status_code']\n        )\n    elif 'crc32body' in response:\n        checker = CRC32Checker(header=response['crc32body'])\n    else:\n        # TODO: send a signal.\n        raise ValueError(\"Unknown retry policy\")\n    return checker\n\n\ndef _extract_retryable_exception(config):\n    applies_when = config['applies_when']\n    if 'crc32body' in applies_when.get('response', {}):\n        return [ChecksumError]\n    elif 'socket_errors' in applies_when:\n        exceptions = []\n        for name in applies_when['socket_errors']:\n            exceptions.extend(EXCEPTION_MAP[name])\n        return exceptions\n\n\nclass RetryHandler:\n    \"\"\"Retry handler.\n\n    The retry handler takes two params, ``checker`` object\n    and an ``action`` object.\n\n    The ``checker`` object must be a callable object and based on a response\n    and an attempt number, determines whether or not sufficient criteria for\n    a retry has been met.  If this is the case then the ``action`` object\n    (which also is a callable) determines what needs to happen in the event\n    of a retry.\n\n    \"\"\"\n\n    def __init__(self, checker, action):\n        self._checker = checker\n        self._action = action\n\n    def __call__(self, attempts, response, caught_exception, **kwargs):\n        \"\"\"Handler for a retry.\n\n        Intended to be hooked up to an event handler (hence the **kwargs),\n        this will process retries appropriately.\n\n        \"\"\"\n        checker_kwargs = {\n            'attempt_number': attempts,\n            'response': response,\n            'caught_exception': caught_exception,\n        }\n        if isinstance(self._checker, MaxAttemptsDecorator):\n            retries_context = kwargs['request_dict']['context'].get('retries')\n            checker_kwargs.update({'retries_context': retries_context})\n\n        if self._checker(**checker_kwargs):\n            result = self._action(attempts=attempts)\n            logger.debug(\"Retry needed, action of: %s\", result)\n            return result\n        logger.debug(\"No retry needed.\")\n\n\nclass BaseChecker:\n    \"\"\"Base class for retry checkers.\n\n    Each class is responsible for checking a single criteria that determines\n    whether or not a retry should not happen.\n\n    \"\"\"\n\n    def __call__(self, attempt_number, response, caught_exception):\n        \"\"\"Determine if retry criteria matches.\n\n        Note that either ``response`` is not None and ``caught_exception`` is\n        None or ``response`` is None and ``caught_exception`` is not None.\n\n        :type attempt_number: int\n        :param attempt_number: The total number of times we've attempted\n            to send the request.\n\n        :param response: The HTTP response (if one was received).\n\n        :type caught_exception: Exception\n        :param caught_exception: Any exception that was caught while trying to\n            send the HTTP response.\n\n        :return: True, if the retry criteria matches (and therefore a retry\n            should occur.  False if the criteria does not match.\n\n        \"\"\"\n        # The default implementation allows subclasses to not have to check\n        # whether or not response is None or not.\n        if response is not None:\n            return self._check_response(attempt_number, response)\n        elif caught_exception is not None:\n            return self._check_caught_exception(\n                attempt_number, caught_exception\n            )\n        else:\n            raise ValueError(\"Both response and caught_exception are None.\")\n\n    def _check_response(self, attempt_number, response):\n        pass\n\n    def _check_caught_exception(self, attempt_number, caught_exception):\n        pass\n\n\nclass MaxAttemptsDecorator(BaseChecker):\n    \"\"\"Allow retries up to a maximum number of attempts.\n\n    This will pass through calls to the decorated retry checker, provided\n    that the number of attempts does not exceed max_attempts.  It will\n    also catch any retryable_exceptions passed in.  Once max_attempts has\n    been exceeded, then False will be returned or the retryable_exceptions\n    that was previously being caught will be raised.\n\n    \"\"\"\n\n    def __init__(self, checker, max_attempts, retryable_exceptions=None):\n        self._checker = checker\n        self._max_attempts = max_attempts\n        self._retryable_exceptions = retryable_exceptions\n\n    def __call__(\n        self, attempt_number, response, caught_exception, retries_context\n    ):\n        if retries_context:\n            retries_context['max'] = max(\n                retries_context.get('max', 0), self._max_attempts\n            )\n\n        should_retry = self._should_retry(\n            attempt_number, response, caught_exception\n        )\n        if should_retry:\n            if attempt_number >= self._max_attempts:\n                # explicitly set MaxAttemptsReached\n                if response is not None and 'ResponseMetadata' in response[1]:\n                    response[1]['ResponseMetadata'][\n                        'MaxAttemptsReached'\n                    ] = True\n                logger.debug(\n                    \"Reached the maximum number of retry attempts: %s\",\n                    attempt_number,\n                )\n                return False\n            else:\n                return should_retry\n        else:\n            return False\n\n    def _should_retry(self, attempt_number, response, caught_exception):\n        if self._retryable_exceptions and attempt_number < self._max_attempts:\n            try:\n                return self._checker(\n                    attempt_number, response, caught_exception\n                )\n            except self._retryable_exceptions as e:\n                logger.debug(\n                    \"retry needed, retryable exception caught: %s\",\n                    e,\n                    exc_info=True,\n                )\n                return True\n        else:\n            # If we've exceeded the max attempts we just let the exception\n            # propagate if one has occurred.\n            return self._checker(attempt_number, response, caught_exception)\n\n\nclass HTTPStatusCodeChecker(BaseChecker):\n    def __init__(self, status_code):\n        self._status_code = status_code\n\n    def _check_response(self, attempt_number, response):\n        if response[0].status_code == self._status_code:\n            logger.debug(\n                \"retry needed: retryable HTTP status code received: %s\",\n                self._status_code,\n            )\n            return True\n        else:\n            return False\n\n\nclass ServiceErrorCodeChecker(BaseChecker):\n    def __init__(self, status_code, error_code):\n        self._status_code = status_code\n        self._error_code = error_code\n\n    def _check_response(self, attempt_number, response):\n        if response[0].status_code == self._status_code:\n            actual_error_code = response[1].get('Error', {}).get('Code')\n            if actual_error_code == self._error_code:\n                logger.debug(\n                    \"retry needed: matching HTTP status and error code seen: \"\n                    \"%s, %s\",\n                    self._status_code,\n                    self._error_code,\n                )\n                return True\n        return False\n\n\nclass MultiChecker(BaseChecker):\n    def __init__(self, checkers):\n        self._checkers = checkers\n\n    def __call__(self, attempt_number, response, caught_exception):\n        for checker in self._checkers:\n            checker_response = checker(\n                attempt_number, response, caught_exception\n            )\n            if checker_response:\n                return checker_response\n        return False\n\n\nclass CRC32Checker(BaseChecker):\n    def __init__(self, header):\n        # The header where the expected crc32 is located.\n        self._header_name = header\n\n    def _check_response(self, attempt_number, response):\n        http_response = response[0]\n        expected_crc = http_response.headers.get(self._header_name)\n        if expected_crc is None:\n            logger.debug(\n                \"crc32 check skipped, the %s header is not \"\n                \"in the http response.\",\n                self._header_name,\n            )\n        else:\n            actual_crc32 = crc32(response[0].content) & 0xFFFFFFFF\n            if not actual_crc32 == int(expected_crc):\n                logger.debug(\n                    \"retry needed: crc32 check failed, expected != actual: \"\n                    \"%s != %s\",\n                    int(expected_crc),\n                    actual_crc32,\n                )\n                raise ChecksumError(\n                    checksum_type='crc32',\n                    expected_checksum=int(expected_crc),\n                    actual_checksum=actual_crc32,\n                )\n\n\nclass ExceptionRaiser(BaseChecker):\n    \"\"\"Raise any caught exceptions.\n\n    This class will raise any non None ``caught_exception``.\n\n    \"\"\"\n\n    def _check_caught_exception(self, attempt_number, caught_exception):\n        # This is implementation specific, but this class is useful by\n        # coordinating with the MaxAttemptsDecorator.\n        # The MaxAttemptsDecorator has a list of exceptions it should catch\n        # and retry, but something needs to come along and actually raise the\n        # caught_exception.  That's what this class is being used for.  If\n        # the MaxAttemptsDecorator is not interested in retrying the exception\n        # then this exception just propagates out past the retry code.\n        raise caught_exception\n", "botocore/httpchecksum.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\"\"\" The interfaces in this module are not intended for public use.\n\nThis module defines interfaces for applying checksums to HTTP requests within\nthe context of botocore. This involves both resolving the checksum to be used\nbased on client configuration and environment, as well as application of the\nchecksum to the request.\n\"\"\"\nimport base64\nimport io\nimport logging\nfrom binascii import crc32\nfrom hashlib import sha1, sha256\n\nfrom botocore.compat import HAS_CRT\nfrom botocore.exceptions import (\n    AwsChunkedWrapperError,\n    FlexibleChecksumError,\n    MissingDependencyException,\n)\nfrom botocore.response import StreamingBody\nfrom botocore.utils import (\n    conditionally_calculate_md5,\n    determine_content_length,\n)\n\nif HAS_CRT:\n    from awscrt import checksums as crt_checksums\nelse:\n    crt_checksums = None\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseChecksum:\n    _CHUNK_SIZE = 1024 * 1024\n\n    def update(self, chunk):\n        pass\n\n    def digest(self):\n        pass\n\n    def b64digest(self):\n        bs = self.digest()\n        return base64.b64encode(bs).decode(\"ascii\")\n\n    def _handle_fileobj(self, fileobj):\n        start_position = fileobj.tell()\n        for chunk in iter(lambda: fileobj.read(self._CHUNK_SIZE), b\"\"):\n            self.update(chunk)\n        fileobj.seek(start_position)\n\n    def handle(self, body):\n        if isinstance(body, (bytes, bytearray)):\n            self.update(body)\n        else:\n            self._handle_fileobj(body)\n        return self.b64digest()\n\n\nclass Crc32Checksum(BaseChecksum):\n    def __init__(self):\n        self._int_crc32 = 0\n\n    def update(self, chunk):\n        self._int_crc32 = crc32(chunk, self._int_crc32) & 0xFFFFFFFF\n\n    def digest(self):\n        return self._int_crc32.to_bytes(4, byteorder=\"big\")\n\n\nclass CrtCrc32Checksum(BaseChecksum):\n    # Note: This class is only used if the CRT is available\n    def __init__(self):\n        self._int_crc32 = 0\n\n    def update(self, chunk):\n        new_checksum = crt_checksums.crc32(chunk, self._int_crc32)\n        self._int_crc32 = new_checksum & 0xFFFFFFFF\n\n    def digest(self):\n        return self._int_crc32.to_bytes(4, byteorder=\"big\")\n\n\nclass CrtCrc32cChecksum(BaseChecksum):\n    # Note: This class is only used if the CRT is available\n    def __init__(self):\n        self._int_crc32c = 0\n\n    def update(self, chunk):\n        new_checksum = crt_checksums.crc32c(chunk, self._int_crc32c)\n        self._int_crc32c = new_checksum & 0xFFFFFFFF\n\n    def digest(self):\n        return self._int_crc32c.to_bytes(4, byteorder=\"big\")\n\n\nclass Sha1Checksum(BaseChecksum):\n    def __init__(self):\n        self._checksum = sha1()\n\n    def update(self, chunk):\n        self._checksum.update(chunk)\n\n    def digest(self):\n        return self._checksum.digest()\n\n\nclass Sha256Checksum(BaseChecksum):\n    def __init__(self):\n        self._checksum = sha256()\n\n    def update(self, chunk):\n        self._checksum.update(chunk)\n\n    def digest(self):\n        return self._checksum.digest()\n\n\nclass AwsChunkedWrapper:\n    _DEFAULT_CHUNK_SIZE = 1024 * 1024\n\n    def __init__(\n        self,\n        raw,\n        checksum_cls=None,\n        checksum_name=\"x-amz-checksum\",\n        chunk_size=None,\n    ):\n        self._raw = raw\n        self._checksum_name = checksum_name\n        self._checksum_cls = checksum_cls\n        self._reset()\n\n        if chunk_size is None:\n            chunk_size = self._DEFAULT_CHUNK_SIZE\n        self._chunk_size = chunk_size\n\n    def _reset(self):\n        self._remaining = b\"\"\n        self._complete = False\n        self._checksum = None\n        if self._checksum_cls:\n            self._checksum = self._checksum_cls()\n\n    def seek(self, offset, whence=0):\n        if offset != 0 or whence != 0:\n            raise AwsChunkedWrapperError(\n                error_msg=\"Can only seek to start of stream\"\n            )\n        self._reset()\n        self._raw.seek(0)\n\n    def read(self, size=None):\n        # Normalize \"read all\" size values to None\n        if size is not None and size <= 0:\n            size = None\n\n        # If the underlying body is done and we have nothing left then\n        # end the stream\n        if self._complete and not self._remaining:\n            return b\"\"\n\n        # While we're not done and want more bytes\n        want_more_bytes = size is None or size > len(self._remaining)\n        while not self._complete and want_more_bytes:\n            self._remaining += self._make_chunk()\n            want_more_bytes = size is None or size > len(self._remaining)\n\n        # If size was None, we want to return everything\n        if size is None:\n            size = len(self._remaining)\n\n        # Return a chunk up to the size asked for\n        to_return = self._remaining[:size]\n        self._remaining = self._remaining[size:]\n        return to_return\n\n    def _make_chunk(self):\n        # NOTE: Chunk size is not deterministic as read could return less. This\n        # means we cannot know the content length of the encoded aws-chunked\n        # stream ahead of time without ensuring a consistent chunk size\n        raw_chunk = self._raw.read(self._chunk_size)\n        hex_len = hex(len(raw_chunk))[2:].encode(\"ascii\")\n        self._complete = not raw_chunk\n\n        if self._checksum:\n            self._checksum.update(raw_chunk)\n\n        if self._checksum and self._complete:\n            name = self._checksum_name.encode(\"ascii\")\n            checksum = self._checksum.b64digest().encode(\"ascii\")\n            return b\"0\\r\\n%s:%s\\r\\n\\r\\n\" % (name, checksum)\n\n        return b\"%s\\r\\n%s\\r\\n\" % (hex_len, raw_chunk)\n\n    def __iter__(self):\n        while not self._complete:\n            yield self._make_chunk()\n\n\nclass StreamingChecksumBody(StreamingBody):\n    def __init__(self, raw_stream, content_length, checksum, expected):\n        super().__init__(raw_stream, content_length)\n        self._checksum = checksum\n        self._expected = expected\n\n    def read(self, amt=None):\n        chunk = super().read(amt=amt)\n        self._checksum.update(chunk)\n        if amt is None or (not chunk and amt > 0):\n            self._validate_checksum()\n        return chunk\n\n    def _validate_checksum(self):\n        if self._checksum.digest() != base64.b64decode(self._expected):\n            error_msg = (\n                f\"Expected checksum {self._expected} did not match calculated \"\n                f\"checksum: {self._checksum.b64digest()}\"\n            )\n            raise FlexibleChecksumError(error_msg=error_msg)\n\n\ndef resolve_checksum_context(request, operation_model, params):\n    resolve_request_checksum_algorithm(request, operation_model, params)\n    resolve_response_checksum_algorithms(request, operation_model, params)\n\n\ndef resolve_request_checksum_algorithm(\n    request,\n    operation_model,\n    params,\n    supported_algorithms=None,\n):\n    http_checksum = operation_model.http_checksum\n    algorithm_member = http_checksum.get(\"requestAlgorithmMember\")\n    if algorithm_member and algorithm_member in params:\n        # If the client has opted into using flexible checksums and the\n        # request supports it, use that instead of checksum required\n        if supported_algorithms is None:\n            supported_algorithms = _SUPPORTED_CHECKSUM_ALGORITHMS\n\n        algorithm_name = params[algorithm_member].lower()\n        if algorithm_name not in supported_algorithms:\n            if not HAS_CRT and algorithm_name in _CRT_CHECKSUM_ALGORITHMS:\n                raise MissingDependencyException(\n                    msg=(\n                        f\"Using {algorithm_name.upper()} requires an \"\n                        \"additional dependency. You will need to pip install \"\n                        \"botocore[crt] before proceeding.\"\n                    )\n                )\n            raise FlexibleChecksumError(\n                error_msg=\"Unsupported checksum algorithm: %s\" % algorithm_name\n            )\n\n        location_type = \"header\"\n        if operation_model.has_streaming_input:\n            # Operations with streaming input must support trailers.\n            if request[\"url\"].startswith(\"https:\"):\n                # We only support unsigned trailer checksums currently. As this\n                # disables payload signing we'll only use trailers over TLS.\n                location_type = \"trailer\"\n\n        algorithm = {\n            \"algorithm\": algorithm_name,\n            \"in\": location_type,\n            \"name\": \"x-amz-checksum-%s\" % algorithm_name,\n        }\n\n        if algorithm[\"name\"] in request[\"headers\"]:\n            # If the header is already set by the customer, skip calculation\n            return\n\n        checksum_context = request[\"context\"].get(\"checksum\", {})\n        checksum_context[\"request_algorithm\"] = algorithm\n        request[\"context\"][\"checksum\"] = checksum_context\n    elif operation_model.http_checksum_required or http_checksum.get(\n        \"requestChecksumRequired\"\n    ):\n        # Otherwise apply the old http checksum behavior via Content-MD5\n        checksum_context = request[\"context\"].get(\"checksum\", {})\n        checksum_context[\"request_algorithm\"] = \"conditional-md5\"\n        request[\"context\"][\"checksum\"] = checksum_context\n\n\ndef apply_request_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n\n    if not algorithm:\n        return\n\n    if algorithm == \"conditional-md5\":\n        # Special case to handle the http checksum required trait\n        conditionally_calculate_md5(request)\n    elif algorithm[\"in\"] == \"header\":\n        _apply_request_header_checksum(request)\n    elif algorithm[\"in\"] == \"trailer\":\n        _apply_request_trailer_checksum(request)\n    else:\n        raise FlexibleChecksumError(\n            error_msg=\"Unknown checksum variant: %s\" % algorithm[\"in\"]\n        )\n\n\ndef _apply_request_header_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n    location_name = algorithm[\"name\"]\n    if location_name in request[\"headers\"]:\n        # If the header is already set by the customer, skip calculation\n        return\n    checksum_cls = _CHECKSUM_CLS.get(algorithm[\"algorithm\"])\n    digest = checksum_cls().handle(request[\"body\"])\n    request[\"headers\"][location_name] = digest\n\n\ndef _apply_request_trailer_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n    location_name = algorithm[\"name\"]\n    checksum_cls = _CHECKSUM_CLS.get(algorithm[\"algorithm\"])\n\n    headers = request[\"headers\"]\n    body = request[\"body\"]\n\n    if location_name in headers:\n        # If the header is already set by the customer, skip calculation\n        return\n\n    headers[\"Transfer-Encoding\"] = \"chunked\"\n    if \"Content-Encoding\" in headers:\n        # We need to preserve the existing content encoding and add\n        # aws-chunked as a new content encoding.\n        headers[\"Content-Encoding\"] += \",aws-chunked\"\n    else:\n        headers[\"Content-Encoding\"] = \"aws-chunked\"\n    headers[\"X-Amz-Trailer\"] = location_name\n\n    content_length = determine_content_length(body)\n    if content_length is not None:\n        # Send the decoded content length if we can determine it. Some\n        # services such as S3 may require the decoded content length\n        headers[\"X-Amz-Decoded-Content-Length\"] = str(content_length)\n\n    if isinstance(body, (bytes, bytearray)):\n        body = io.BytesIO(body)\n\n    request[\"body\"] = AwsChunkedWrapper(\n        body,\n        checksum_cls=checksum_cls,\n        checksum_name=location_name,\n    )\n\n\ndef resolve_response_checksum_algorithms(\n    request, operation_model, params, supported_algorithms=None\n):\n    http_checksum = operation_model.http_checksum\n    mode_member = http_checksum.get(\"requestValidationModeMember\")\n    if mode_member and mode_member in params:\n        if supported_algorithms is None:\n            supported_algorithms = _SUPPORTED_CHECKSUM_ALGORITHMS\n        response_algorithms = {\n            a.lower() for a in http_checksum.get(\"responseAlgorithms\", [])\n        }\n\n        usable_algorithms = []\n        for algorithm in _ALGORITHMS_PRIORITY_LIST:\n            if algorithm not in response_algorithms:\n                continue\n            if algorithm in supported_algorithms:\n                usable_algorithms.append(algorithm)\n\n        checksum_context = request[\"context\"].get(\"checksum\", {})\n        checksum_context[\"response_algorithms\"] = usable_algorithms\n        request[\"context\"][\"checksum\"] = checksum_context\n\n\ndef handle_checksum_body(http_response, response, context, operation_model):\n    headers = response[\"headers\"]\n    checksum_context = context.get(\"checksum\", {})\n    algorithms = checksum_context.get(\"response_algorithms\")\n\n    if not algorithms:\n        return\n\n    for algorithm in algorithms:\n        header_name = \"x-amz-checksum-%s\" % algorithm\n        # If the header is not found, check the next algorithm\n        if header_name not in headers:\n            continue\n\n        # If a - is in the checksum this is not valid Base64. S3 returns\n        # checksums that include a -# suffix to indicate a checksum derived\n        # from the hash of all part checksums. We cannot wrap this response\n        if \"-\" in headers[header_name]:\n            continue\n\n        if operation_model.has_streaming_output:\n            response[\"body\"] = _handle_streaming_response(\n                http_response, response, algorithm\n            )\n        else:\n            response[\"body\"] = _handle_bytes_response(\n                http_response, response, algorithm\n            )\n\n        # Expose metadata that the checksum check actually occurred\n        checksum_context = response[\"context\"].get(\"checksum\", {})\n        checksum_context[\"response_algorithm\"] = algorithm\n        response[\"context\"][\"checksum\"] = checksum_context\n        return\n\n    logger.info(\n        f'Skipping checksum validation. Response did not contain one of the '\n        f'following algorithms: {algorithms}.'\n    )\n\n\ndef _handle_streaming_response(http_response, response, algorithm):\n    checksum_cls = _CHECKSUM_CLS.get(algorithm)\n    header_name = \"x-amz-checksum-%s\" % algorithm\n    return StreamingChecksumBody(\n        http_response.raw,\n        response[\"headers\"].get(\"content-length\"),\n        checksum_cls(),\n        response[\"headers\"][header_name],\n    )\n\n\ndef _handle_bytes_response(http_response, response, algorithm):\n    body = http_response.content\n    header_name = \"x-amz-checksum-%s\" % algorithm\n    checksum_cls = _CHECKSUM_CLS.get(algorithm)\n    checksum = checksum_cls()\n    checksum.update(body)\n    expected = response[\"headers\"][header_name]\n    if checksum.digest() != base64.b64decode(expected):\n        error_msg = (\n            \"Expected checksum %s did not match calculated checksum: %s\"\n            % (\n                expected,\n                checksum.b64digest(),\n            )\n        )\n        raise FlexibleChecksumError(error_msg=error_msg)\n    return body\n\n\n_CHECKSUM_CLS = {\n    \"crc32\": Crc32Checksum,\n    \"sha1\": Sha1Checksum,\n    \"sha256\": Sha256Checksum,\n}\n_CRT_CHECKSUM_ALGORITHMS = [\"crc32\", \"crc32c\"]\nif HAS_CRT:\n    # Use CRT checksum implementations if available\n    _CRT_CHECKSUM_CLS = {\n        \"crc32\": CrtCrc32Checksum,\n        \"crc32c\": CrtCrc32cChecksum,\n    }\n    _CHECKSUM_CLS.update(_CRT_CHECKSUM_CLS)\n    # Validate this list isn't out of sync with _CRT_CHECKSUM_CLS keys\n    assert all(\n        name in _CRT_CHECKSUM_ALGORITHMS for name in _CRT_CHECKSUM_CLS.keys()\n    )\n_SUPPORTED_CHECKSUM_ALGORITHMS = list(_CHECKSUM_CLS.keys())\n_ALGORITHMS_PRIORITY_LIST = ['crc32c', 'crc32', 'sha1', 'sha256']\n", "botocore/utils.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport binascii\nimport datetime\nimport email.message\nimport functools\nimport hashlib\nimport io\nimport logging\nimport os\nimport random\nimport re\nimport socket\nimport time\nimport warnings\nimport weakref\nfrom datetime import datetime as _DatetimeClass\nfrom ipaddress import ip_address\nfrom pathlib import Path\nfrom urllib.request import getproxies, proxy_bypass\n\nimport dateutil.parser\nfrom dateutil.tz import tzutc\nfrom urllib3.exceptions import LocationParseError\n\nimport botocore\nimport botocore.awsrequest\nimport botocore.httpsession\n\n# IP Regexes retained for backwards compatibility\nfrom botocore.compat import HEX_PAT  # noqa: F401\nfrom botocore.compat import IPV4_PAT  # noqa: F401\nfrom botocore.compat import IPV6_ADDRZ_PAT  # noqa: F401\nfrom botocore.compat import IPV6_PAT  # noqa: F401\nfrom botocore.compat import LS32_PAT  # noqa: F401\nfrom botocore.compat import UNRESERVED_PAT  # noqa: F401\nfrom botocore.compat import ZONE_ID_PAT  # noqa: F401\nfrom botocore.compat import (\n    HAS_CRT,\n    IPV4_RE,\n    IPV6_ADDRZ_RE,\n    MD5_AVAILABLE,\n    UNSAFE_URL_CHARS,\n    OrderedDict,\n    get_md5,\n    get_tzinfo_options,\n    json,\n    quote,\n    urlparse,\n    urlsplit,\n    urlunsplit,\n    zip_longest,\n)\nfrom botocore.exceptions import (\n    ClientError,\n    ConfigNotFound,\n    ConnectionClosedError,\n    ConnectTimeoutError,\n    EndpointConnectionError,\n    HTTPClientError,\n    InvalidDNSNameError,\n    InvalidEndpointConfigurationError,\n    InvalidExpressionError,\n    InvalidHostLabelError,\n    InvalidIMDSEndpointError,\n    InvalidIMDSEndpointModeError,\n    InvalidRegionError,\n    MetadataRetrievalError,\n    MissingDependencyException,\n    ReadTimeoutError,\n    SSOTokenLoadError,\n    UnsupportedOutpostResourceError,\n    UnsupportedS3AccesspointConfigurationError,\n    UnsupportedS3ArnError,\n    UnsupportedS3ConfigurationError,\n    UnsupportedS3ControlArnError,\n    UnsupportedS3ControlConfigurationError,\n)\n\nlogger = logging.getLogger(__name__)\nDEFAULT_METADATA_SERVICE_TIMEOUT = 1\nMETADATA_BASE_URL = 'http://169.254.169.254/'\nMETADATA_BASE_URL_IPv6 = 'http://[fd00:ec2::254]/'\nMETADATA_ENDPOINT_MODES = ('ipv4', 'ipv6')\n\n# These are chars that do not need to be urlencoded.\n# Based on rfc2986, section 2.3\nSAFE_CHARS = '-._~'\nLABEL_RE = re.compile(r'[a-z0-9][a-z0-9\\-]*[a-z0-9]')\nRETRYABLE_HTTP_ERRORS = (\n    ReadTimeoutError,\n    EndpointConnectionError,\n    ConnectionClosedError,\n    ConnectTimeoutError,\n)\nS3_ACCELERATE_WHITELIST = ['dualstack']\n# In switching events from using service name / endpoint prefix to service\n# id, we have to preserve compatibility. This maps the instances where either\n# is different than the transformed service id.\nEVENT_ALIASES = {\n    \"api.mediatailor\": \"mediatailor\",\n    \"api.pricing\": \"pricing\",\n    \"api.sagemaker\": \"sagemaker\",\n    \"apigateway\": \"api-gateway\",\n    \"application-autoscaling\": \"application-auto-scaling\",\n    \"appstream2\": \"appstream\",\n    \"autoscaling\": \"auto-scaling\",\n    \"autoscaling-plans\": \"auto-scaling-plans\",\n    \"ce\": \"cost-explorer\",\n    \"cloudhsmv2\": \"cloudhsm-v2\",\n    \"cloudsearchdomain\": \"cloudsearch-domain\",\n    \"cognito-idp\": \"cognito-identity-provider\",\n    \"config\": \"config-service\",\n    \"cur\": \"cost-and-usage-report-service\",\n    \"data.iot\": \"iot-data-plane\",\n    \"data.jobs.iot\": \"iot-jobs-data-plane\",\n    \"data.mediastore\": \"mediastore-data\",\n    \"datapipeline\": \"data-pipeline\",\n    \"devicefarm\": \"device-farm\",\n    \"devices.iot1click\": \"iot-1click-devices-service\",\n    \"directconnect\": \"direct-connect\",\n    \"discovery\": \"application-discovery-service\",\n    \"dms\": \"database-migration-service\",\n    \"ds\": \"directory-service\",\n    \"dynamodbstreams\": \"dynamodb-streams\",\n    \"elasticbeanstalk\": \"elastic-beanstalk\",\n    \"elasticfilesystem\": \"efs\",\n    \"elasticloadbalancing\": \"elastic-load-balancing\",\n    \"elasticmapreduce\": \"emr\",\n    \"elastictranscoder\": \"elastic-transcoder\",\n    \"elb\": \"elastic-load-balancing\",\n    \"elbv2\": \"elastic-load-balancing-v2\",\n    \"email\": \"ses\",\n    \"entitlement.marketplace\": \"marketplace-entitlement-service\",\n    \"es\": \"elasticsearch-service\",\n    \"events\": \"eventbridge\",\n    \"cloudwatch-events\": \"eventbridge\",\n    \"iot-data\": \"iot-data-plane\",\n    \"iot-jobs-data\": \"iot-jobs-data-plane\",\n    \"iot1click-devices\": \"iot-1click-devices-service\",\n    \"iot1click-projects\": \"iot-1click-projects\",\n    \"kinesisanalytics\": \"kinesis-analytics\",\n    \"kinesisvideo\": \"kinesis-video\",\n    \"lex-models\": \"lex-model-building-service\",\n    \"lex-runtime\": \"lex-runtime-service\",\n    \"logs\": \"cloudwatch-logs\",\n    \"machinelearning\": \"machine-learning\",\n    \"marketplace-entitlement\": \"marketplace-entitlement-service\",\n    \"marketplacecommerceanalytics\": \"marketplace-commerce-analytics\",\n    \"metering.marketplace\": \"marketplace-metering\",\n    \"meteringmarketplace\": \"marketplace-metering\",\n    \"mgh\": \"migration-hub\",\n    \"models.lex\": \"lex-model-building-service\",\n    \"monitoring\": \"cloudwatch\",\n    \"mturk-requester\": \"mturk\",\n    \"opsworks-cm\": \"opsworkscm\",\n    \"projects.iot1click\": \"iot-1click-projects\",\n    \"resourcegroupstaggingapi\": \"resource-groups-tagging-api\",\n    \"route53\": \"route-53\",\n    \"route53domains\": \"route-53-domains\",\n    \"runtime.lex\": \"lex-runtime-service\",\n    \"runtime.sagemaker\": \"sagemaker-runtime\",\n    \"sdb\": \"simpledb\",\n    \"secretsmanager\": \"secrets-manager\",\n    \"serverlessrepo\": \"serverlessapplicationrepository\",\n    \"servicecatalog\": \"service-catalog\",\n    \"states\": \"sfn\",\n    \"stepfunctions\": \"sfn\",\n    \"storagegateway\": \"storage-gateway\",\n    \"streams.dynamodb\": \"dynamodb-streams\",\n    \"tagging\": \"resource-groups-tagging-api\",\n}\n\n\n# This pattern can be used to detect if a header is a flexible checksum header\nCHECKSUM_HEADER_PATTERN = re.compile(\n    r'^X-Amz-Checksum-([a-z0-9]*)$',\n    flags=re.IGNORECASE,\n)\n\n\ndef ensure_boolean(val):\n    \"\"\"Ensures a boolean value if a string or boolean is provided\n\n    For strings, the value for True/False is case insensitive\n    \"\"\"\n    if isinstance(val, bool):\n        return val\n    elif isinstance(val, str):\n        return val.lower() == 'true'\n    else:\n        return False\n\n\ndef resolve_imds_endpoint_mode(session):\n    \"\"\"Resolving IMDS endpoint mode to either IPv6 or IPv4.\n\n    ec2_metadata_service_endpoint_mode takes precedence over imds_use_ipv6.\n    \"\"\"\n    endpoint_mode = session.get_config_variable(\n        'ec2_metadata_service_endpoint_mode'\n    )\n    if endpoint_mode is not None:\n        lendpoint_mode = endpoint_mode.lower()\n        if lendpoint_mode not in METADATA_ENDPOINT_MODES:\n            error_msg_kwargs = {\n                'mode': endpoint_mode,\n                'valid_modes': METADATA_ENDPOINT_MODES,\n            }\n            raise InvalidIMDSEndpointModeError(**error_msg_kwargs)\n        return lendpoint_mode\n    elif session.get_config_variable('imds_use_ipv6'):\n        return 'ipv6'\n    return 'ipv4'\n\n\ndef is_json_value_header(shape):\n    \"\"\"Determines if the provided shape is the special header type jsonvalue.\n\n    :type shape: botocore.shape\n    :param shape: Shape to be inspected for the jsonvalue trait.\n\n    :return: True if this type is a jsonvalue, False otherwise\n    :rtype: Bool\n    \"\"\"\n    return (\n        hasattr(shape, 'serialization')\n        and shape.serialization.get('jsonvalue', False)\n        and shape.serialization.get('location') == 'header'\n        and shape.type_name == 'string'\n    )\n\n\ndef has_header(header_name, headers):\n    \"\"\"Case-insensitive check for header key.\"\"\"\n    if header_name is None:\n        return False\n    elif isinstance(headers, botocore.awsrequest.HeadersDict):\n        return header_name in headers\n    else:\n        return header_name.lower() in [key.lower() for key in headers.keys()]\n\n\ndef get_service_module_name(service_model):\n    \"\"\"Returns the module name for a service\n\n    This is the value used in both the documentation and client class name\n    \"\"\"\n    name = service_model.metadata.get(\n        'serviceAbbreviation',\n        service_model.metadata.get(\n            'serviceFullName', service_model.service_name\n        ),\n    )\n    name = name.replace('Amazon', '')\n    name = name.replace('AWS', '')\n    name = re.sub(r'\\W+', '', name)\n    return name\n\n\ndef normalize_url_path(path):\n    if not path:\n        return '/'\n    return remove_dot_segments(path)\n\n\ndef normalize_boolean(val):\n    \"\"\"Returns None if val is None, otherwise ensure value\n    converted to boolean\"\"\"\n    if val is None:\n        return val\n    else:\n        return ensure_boolean(val)\n\n\ndef remove_dot_segments(url):\n    # RFC 3986, section 5.2.4 \"Remove Dot Segments\"\n    # Also, AWS services require consecutive slashes to be removed,\n    # so that's done here as well\n    if not url:\n        return ''\n    input_url = url.split('/')\n    output_list = []\n    for x in input_url:\n        if x and x != '.':\n            if x == '..':\n                if output_list:\n                    output_list.pop()\n            else:\n                output_list.append(x)\n\n    if url[0] == '/':\n        first = '/'\n    else:\n        first = ''\n    if url[-1] == '/' and output_list:\n        last = '/'\n    else:\n        last = ''\n    return first + '/'.join(output_list) + last\n\n\ndef validate_jmespath_for_set(expression):\n    # Validates a limited jmespath expression to determine if we can set a\n    # value based on it. Only works with dotted paths.\n    if not expression or expression == '.':\n        raise InvalidExpressionError(expression=expression)\n\n    for invalid in ['[', ']', '*']:\n        if invalid in expression:\n            raise InvalidExpressionError(expression=expression)\n\n\ndef set_value_from_jmespath(source, expression, value, is_first=True):\n    # This takes a (limited) jmespath-like expression & can set a value based\n    # on it.\n    # Limitations:\n    # * Only handles dotted lookups\n    # * No offsets/wildcards/slices/etc.\n    if is_first:\n        validate_jmespath_for_set(expression)\n\n    bits = expression.split('.', 1)\n    current_key, remainder = bits[0], bits[1] if len(bits) > 1 else ''\n\n    if not current_key:\n        raise InvalidExpressionError(expression=expression)\n\n    if remainder:\n        if current_key not in source:\n            # We've got something in the expression that's not present in the\n            # source (new key). If there's any more bits, we'll set the key\n            # with an empty dictionary.\n            source[current_key] = {}\n\n        return set_value_from_jmespath(\n            source[current_key], remainder, value, is_first=False\n        )\n\n    # If we're down to a single key, set it.\n    source[current_key] = value\n\n\ndef is_global_accesspoint(context):\n    \"\"\"Determine if request is intended for an MRAP accesspoint.\"\"\"\n    s3_accesspoint = context.get('s3_accesspoint', {})\n    is_global = s3_accesspoint.get('region') == ''\n    return is_global\n\n\nclass _RetriesExceededError(Exception):\n    \"\"\"Internal exception used when the number of retries are exceeded.\"\"\"\n\n    pass\n\n\nclass BadIMDSRequestError(Exception):\n    def __init__(self, request):\n        self.request = request\n\n\nclass IMDSFetcher:\n    _RETRIES_EXCEEDED_ERROR_CLS = _RetriesExceededError\n    _TOKEN_PATH = 'latest/api/token'\n    _TOKEN_TTL = '21600'\n\n    def __init__(\n        self,\n        timeout=DEFAULT_METADATA_SERVICE_TIMEOUT,\n        num_attempts=1,\n        base_url=METADATA_BASE_URL,\n        env=None,\n        user_agent=None,\n        config=None,\n    ):\n        self._timeout = timeout\n        self._num_attempts = num_attempts\n        if config is None:\n            config = {}\n        self._base_url = self._select_base_url(base_url, config)\n        self._config = config\n\n        if env is None:\n            env = os.environ.copy()\n        self._disabled = (\n            env.get('AWS_EC2_METADATA_DISABLED', 'false').lower() == 'true'\n        )\n        self._imds_v1_disabled = config.get('ec2_metadata_v1_disabled')\n        self._user_agent = user_agent\n        self._session = botocore.httpsession.URLLib3Session(\n            timeout=self._timeout,\n            proxies=get_environ_proxies(self._base_url),\n        )\n\n    def get_base_url(self):\n        return self._base_url\n\n    def _select_base_url(self, base_url, config):\n        if config is None:\n            config = {}\n\n        requires_ipv6 = (\n            config.get('ec2_metadata_service_endpoint_mode') == 'ipv6'\n        )\n        custom_metadata_endpoint = config.get('ec2_metadata_service_endpoint')\n\n        if requires_ipv6 and custom_metadata_endpoint:\n            logger.warning(\n                \"Custom endpoint and IMDS_USE_IPV6 are both set. Using custom endpoint.\"\n            )\n\n        chosen_base_url = None\n\n        if base_url != METADATA_BASE_URL:\n            chosen_base_url = base_url\n        elif custom_metadata_endpoint:\n            chosen_base_url = custom_metadata_endpoint\n        elif requires_ipv6:\n            chosen_base_url = METADATA_BASE_URL_IPv6\n        else:\n            chosen_base_url = METADATA_BASE_URL\n\n        logger.debug(\"IMDS ENDPOINT: %s\" % chosen_base_url)\n        if not is_valid_uri(chosen_base_url):\n            raise InvalidIMDSEndpointError(endpoint=chosen_base_url)\n\n        return chosen_base_url\n\n    def _construct_url(self, path):\n        sep = ''\n        if self._base_url and not self._base_url.endswith('/'):\n            sep = '/'\n        return f'{self._base_url}{sep}{path}'\n\n    def _fetch_metadata_token(self):\n        self._assert_enabled()\n        url = self._construct_url(self._TOKEN_PATH)\n        headers = {\n            'x-aws-ec2-metadata-token-ttl-seconds': self._TOKEN_TTL,\n        }\n        self._add_user_agent(headers)\n        request = botocore.awsrequest.AWSRequest(\n            method='PUT', url=url, headers=headers\n        )\n        for i in range(self._num_attempts):\n            try:\n                response = self._session.send(request.prepare())\n                if response.status_code == 200:\n                    return response.text\n                elif response.status_code in (404, 403, 405):\n                    return None\n                elif response.status_code in (400,):\n                    raise BadIMDSRequestError(request)\n            except ReadTimeoutError:\n                return None\n            except RETRYABLE_HTTP_ERRORS as e:\n                logger.debug(\n                    \"Caught retryable HTTP exception while making metadata \"\n                    \"service request to %s: %s\",\n                    url,\n                    e,\n                    exc_info=True,\n                )\n            except HTTPClientError as e:\n                if isinstance(e.kwargs.get('error'), LocationParseError):\n                    raise InvalidIMDSEndpointError(endpoint=url, error=e)\n                else:\n                    raise\n        return None\n\n    def _get_request(self, url_path, retry_func, token=None):\n        \"\"\"Make a get request to the Instance Metadata Service.\n\n        :type url_path: str\n        :param url_path: The path component of the URL to make a get request.\n            This arg is appended to the base_url that was provided in the\n            initializer.\n\n        :type retry_func: callable\n        :param retry_func: A function that takes the response as an argument\n             and determines if it needs to retry. By default empty and non\n             200 OK responses are retried.\n\n        :type token: str\n        :param token: Metadata token to send along with GET requests to IMDS.\n        \"\"\"\n        self._assert_enabled()\n        if not token:\n            self._assert_v1_enabled()\n        if retry_func is None:\n            retry_func = self._default_retry\n        url = self._construct_url(url_path)\n        headers = {}\n        if token is not None:\n            headers['x-aws-ec2-metadata-token'] = token\n        self._add_user_agent(headers)\n        for i in range(self._num_attempts):\n            try:\n                request = botocore.awsrequest.AWSRequest(\n                    method='GET', url=url, headers=headers\n                )\n                response = self._session.send(request.prepare())\n                if not retry_func(response):\n                    return response\n            except RETRYABLE_HTTP_ERRORS as e:\n                logger.debug(\n                    \"Caught retryable HTTP exception while making metadata \"\n                    \"service request to %s: %s\",\n                    url,\n                    e,\n                    exc_info=True,\n                )\n        raise self._RETRIES_EXCEEDED_ERROR_CLS()\n\n    def _add_user_agent(self, headers):\n        if self._user_agent is not None:\n            headers['User-Agent'] = self._user_agent\n\n    def _assert_enabled(self):\n        if self._disabled:\n            logger.debug(\"Access to EC2 metadata has been disabled.\")\n            raise self._RETRIES_EXCEEDED_ERROR_CLS()\n\n    def _assert_v1_enabled(self):\n        if self._imds_v1_disabled:\n            raise MetadataRetrievalError(\n                error_msg=\"Unable to retrieve token for use in IMDSv2 call and IMDSv1 has been disabled\"\n            )\n\n    def _default_retry(self, response):\n        return self._is_non_ok_response(response) or self._is_empty(response)\n\n    def _is_non_ok_response(self, response):\n        if response.status_code != 200:\n            self._log_imds_response(response, 'non-200', log_body=True)\n            return True\n        return False\n\n    def _is_empty(self, response):\n        if not response.content:\n            self._log_imds_response(response, 'no body', log_body=True)\n            return True\n        return False\n\n    def _log_imds_response(self, response, reason_to_log, log_body=False):\n        statement = (\n            \"Metadata service returned %s response \"\n            \"with status code of %s for url: %s\"\n        )\n        logger_args = [reason_to_log, response.status_code, response.url]\n        if log_body:\n            statement += \", content body: %s\"\n            logger_args.append(response.content)\n        logger.debug(statement, *logger_args)\n\n\nclass InstanceMetadataFetcher(IMDSFetcher):\n    _URL_PATH = 'latest/meta-data/iam/security-credentials/'\n    _REQUIRED_CREDENTIAL_FIELDS = [\n        'AccessKeyId',\n        'SecretAccessKey',\n        'Token',\n        'Expiration',\n    ]\n\n    def retrieve_iam_role_credentials(self):\n        try:\n            token = self._fetch_metadata_token()\n            role_name = self._get_iam_role(token)\n            credentials = self._get_credentials(role_name, token)\n            if self._contains_all_credential_fields(credentials):\n                credentials = {\n                    'role_name': role_name,\n                    'access_key': credentials['AccessKeyId'],\n                    'secret_key': credentials['SecretAccessKey'],\n                    'token': credentials['Token'],\n                    'expiry_time': credentials['Expiration'],\n                }\n                self._evaluate_expiration(credentials)\n                return credentials\n            else:\n                # IMDS can return a 200 response that has a JSON formatted\n                # error message (i.e. if ec2 is not trusted entity for the\n                # attached role). We do not necessarily want to retry for\n                # these and we also do not necessarily want to raise a key\n                # error. So at least log the problematic response and return\n                # an empty dictionary to signal that it was not able to\n                # retrieve credentials. These error will contain both a\n                # Code and Message key.\n                if 'Code' in credentials and 'Message' in credentials:\n                    logger.debug(\n                        'Error response received when retrieving'\n                        'credentials: %s.',\n                        credentials,\n                    )\n                return {}\n        except self._RETRIES_EXCEEDED_ERROR_CLS:\n            logger.debug(\n                \"Max number of attempts exceeded (%s) when \"\n                \"attempting to retrieve data from metadata service.\",\n                self._num_attempts,\n            )\n        except BadIMDSRequestError as e:\n            logger.debug(\"Bad IMDS request: %s\", e.request)\n        return {}\n\n    def _get_iam_role(self, token=None):\n        return self._get_request(\n            url_path=self._URL_PATH,\n            retry_func=self._needs_retry_for_role_name,\n            token=token,\n        ).text\n\n    def _get_credentials(self, role_name, token=None):\n        r = self._get_request(\n            url_path=self._URL_PATH + role_name,\n            retry_func=self._needs_retry_for_credentials,\n            token=token,\n        )\n        return json.loads(r.text)\n\n    def _is_invalid_json(self, response):\n        try:\n            json.loads(response.text)\n            return False\n        except ValueError:\n            self._log_imds_response(response, 'invalid json')\n            return True\n\n    def _needs_retry_for_role_name(self, response):\n        return self._is_non_ok_response(response) or self._is_empty(response)\n\n    def _needs_retry_for_credentials(self, response):\n        return (\n            self._is_non_ok_response(response)\n            or self._is_empty(response)\n            or self._is_invalid_json(response)\n        )\n\n    def _contains_all_credential_fields(self, credentials):\n        for field in self._REQUIRED_CREDENTIAL_FIELDS:\n            if field not in credentials:\n                logger.debug(\n                    'Retrieved credentials is missing required field: %s',\n                    field,\n                )\n                return False\n        return True\n\n    def _evaluate_expiration(self, credentials):\n        expiration = credentials.get(\"expiry_time\")\n        if expiration is None:\n            return\n        try:\n            expiration = datetime.datetime.strptime(\n                expiration, \"%Y-%m-%dT%H:%M:%SZ\"\n            )\n            refresh_interval = self._config.get(\n                \"ec2_credential_refresh_window\", 60 * 10\n            )\n            jitter = random.randint(120, 600)  # Between 2 to 10 minutes\n            refresh_interval_with_jitter = refresh_interval + jitter\n            current_time = datetime.datetime.utcnow()\n            refresh_offset = datetime.timedelta(\n                seconds=refresh_interval_with_jitter\n            )\n            extension_time = expiration - refresh_offset\n            if current_time >= extension_time:\n                new_time = current_time + refresh_offset\n                credentials[\"expiry_time\"] = new_time.strftime(\n                    \"%Y-%m-%dT%H:%M:%SZ\"\n                )\n                logger.info(\n                    f\"Attempting credential expiration extension due to a \"\n                    f\"credential service availability issue. A refresh of \"\n                    f\"these credentials will be attempted again within \"\n                    f\"the next {refresh_interval_with_jitter/60:.0f} minutes.\"\n                )\n        except ValueError:\n            logger.debug(\n                f\"Unable to parse expiry_time in {credentials['expiry_time']}\"\n            )\n\n\nclass IMDSRegionProvider:\n    def __init__(self, session, environ=None, fetcher=None):\n        \"\"\"Initialize IMDSRegionProvider.\n        :type session: :class:`botocore.session.Session`\n        :param session: The session is needed to look up configuration for\n            how to contact the instance metadata service. Specifically the\n            whether or not it should use the IMDS region at all, and if so how\n            to configure the timeout and number of attempts to reach the\n            service.\n        :type environ: None or dict\n        :param environ: A dictionary of environment variables to use. If\n            ``None`` is the argument then ``os.environ`` will be used by\n            default.\n        :type fecther: :class:`botocore.utils.InstanceMetadataRegionFetcher`\n        :param fetcher: The class to actually handle the fetching of the region\n            from the IMDS. If not provided a default one will be created.\n        \"\"\"\n        self._session = session\n        if environ is None:\n            environ = os.environ\n        self._environ = environ\n        self._fetcher = fetcher\n\n    def provide(self):\n        \"\"\"Provide the region value from IMDS.\"\"\"\n        instance_region = self._get_instance_metadata_region()\n        return instance_region\n\n    def _get_instance_metadata_region(self):\n        fetcher = self._get_fetcher()\n        region = fetcher.retrieve_region()\n        return region\n\n    def _get_fetcher(self):\n        if self._fetcher is None:\n            self._fetcher = self._create_fetcher()\n        return self._fetcher\n\n    def _create_fetcher(self):\n        metadata_timeout = self._session.get_config_variable(\n            'metadata_service_timeout'\n        )\n        metadata_num_attempts = self._session.get_config_variable(\n            'metadata_service_num_attempts'\n        )\n        imds_config = {\n            'ec2_metadata_service_endpoint': self._session.get_config_variable(\n                'ec2_metadata_service_endpoint'\n            ),\n            'ec2_metadata_service_endpoint_mode': resolve_imds_endpoint_mode(\n                self._session\n            ),\n            'ec2_metadata_v1_disabled': self._session.get_config_variable(\n                'ec2_metadata_v1_disabled'\n            ),\n        }\n        fetcher = InstanceMetadataRegionFetcher(\n            timeout=metadata_timeout,\n            num_attempts=metadata_num_attempts,\n            env=self._environ,\n            user_agent=self._session.user_agent(),\n            config=imds_config,\n        )\n        return fetcher\n\n\nclass InstanceMetadataRegionFetcher(IMDSFetcher):\n    _URL_PATH = 'latest/meta-data/placement/availability-zone/'\n\n    def retrieve_region(self):\n        \"\"\"Get the current region from the instance metadata service.\n        :rvalue: str\n        :returns: The region the current instance is running in or None\n            if the instance metadata service cannot be contacted or does not\n            give a valid response.\n        :rtype: None or str\n        :returns: Returns the region as a string if it is configured to use\n            IMDS as a region source. Otherwise returns ``None``. It will also\n            return ``None`` if it fails to get the region from IMDS due to\n            exhausting its retries or not being able to connect.\n        \"\"\"\n        try:\n            region = self._get_region()\n            return region\n        except self._RETRIES_EXCEEDED_ERROR_CLS:\n            logger.debug(\n                \"Max number of attempts exceeded (%s) when \"\n                \"attempting to retrieve data from metadata service.\",\n                self._num_attempts,\n            )\n        return None\n\n    def _get_region(self):\n        token = self._fetch_metadata_token()\n        response = self._get_request(\n            url_path=self._URL_PATH,\n            retry_func=self._default_retry,\n            token=token,\n        )\n        availability_zone = response.text\n        region = availability_zone[:-1]\n        return region\n\n\ndef merge_dicts(dict1, dict2, append_lists=False):\n    \"\"\"Given two dict, merge the second dict into the first.\n\n    The dicts can have arbitrary nesting.\n\n    :param append_lists: If true, instead of clobbering a list with the new\n        value, append all of the new values onto the original list.\n    \"\"\"\n    for key in dict2:\n        if isinstance(dict2[key], dict):\n            if key in dict1 and key in dict2:\n                merge_dicts(dict1[key], dict2[key])\n            else:\n                dict1[key] = dict2[key]\n        # If the value is a list and the ``append_lists`` flag is set,\n        # append the new values onto the original list\n        elif isinstance(dict2[key], list) and append_lists:\n            # The value in dict1 must be a list in order to append new\n            # values onto it.\n            if key in dict1 and isinstance(dict1[key], list):\n                dict1[key].extend(dict2[key])\n            else:\n                dict1[key] = dict2[key]\n        else:\n            # At scalar types, we iterate and merge the\n            # current dict that we're on.\n            dict1[key] = dict2[key]\n\n\ndef lowercase_dict(original):\n    \"\"\"Copies the given dictionary ensuring all keys are lowercase strings.\"\"\"\n    copy = {}\n    for key in original:\n        copy[key.lower()] = original[key]\n    return copy\n\n\ndef parse_key_val_file(filename, _open=open):\n    try:\n        with _open(filename) as f:\n            contents = f.read()\n            return parse_key_val_file_contents(contents)\n    except OSError:\n        raise ConfigNotFound(path=filename)\n\n\ndef parse_key_val_file_contents(contents):\n    # This was originally extracted from the EC2 credential provider, which was\n    # fairly lenient in its parsing.  We only try to parse key/val pairs if\n    # there's a '=' in the line.\n    final = {}\n    for line in contents.splitlines():\n        if '=' not in line:\n            continue\n        key, val = line.split('=', 1)\n        key = key.strip()\n        val = val.strip()\n        final[key] = val\n    return final\n\n\ndef percent_encode_sequence(mapping, safe=SAFE_CHARS):\n    \"\"\"Urlencode a dict or list into a string.\n\n    This is similar to urllib.urlencode except that:\n\n    * It uses quote, and not quote_plus\n    * It has a default list of safe chars that don't need\n      to be encoded, which matches what AWS services expect.\n\n    If any value in the input ``mapping`` is a list type,\n    then each list element wil be serialized.  This is the equivalent\n    to ``urlencode``'s ``doseq=True`` argument.\n\n    This function should be preferred over the stdlib\n    ``urlencode()`` function.\n\n    :param mapping: Either a dict to urlencode or a list of\n        ``(key, value)`` pairs.\n\n    \"\"\"\n    encoded_pairs = []\n    if hasattr(mapping, 'items'):\n        pairs = mapping.items()\n    else:\n        pairs = mapping\n    for key, value in pairs:\n        if isinstance(value, list):\n            for element in value:\n                encoded_pairs.append(\n                    f'{percent_encode(key)}={percent_encode(element)}'\n                )\n        else:\n            encoded_pairs.append(\n                f'{percent_encode(key)}={percent_encode(value)}'\n            )\n    return '&'.join(encoded_pairs)\n\n\ndef percent_encode(input_str, safe=SAFE_CHARS):\n    \"\"\"Urlencodes a string.\n\n    Whereas percent_encode_sequence handles taking a dict/sequence and\n    producing a percent encoded string, this function deals only with\n    taking a string (not a dict/sequence) and percent encoding it.\n\n    If given the binary type, will simply URL encode it. If given the\n    text type, will produce the binary type by UTF-8 encoding the\n    text. If given something else, will convert it to the text type\n    first.\n    \"\"\"\n    # If its not a binary or text string, make it a text string.\n    if not isinstance(input_str, (bytes, str)):\n        input_str = str(input_str)\n    # If it's not bytes, make it bytes by UTF-8 encoding it.\n    if not isinstance(input_str, bytes):\n        input_str = input_str.encode('utf-8')\n    return quote(input_str, safe=safe)\n\n\ndef _epoch_seconds_to_datetime(value, tzinfo):\n    \"\"\"Parse numerical epoch timestamps (seconds since 1970) into a\n    ``datetime.datetime`` in UTC using ``datetime.timedelta``. This is intended\n    as fallback when ``fromtimestamp`` raises ``OverflowError`` or ``OSError``.\n\n    :type value: float or int\n    :param value: The Unix timestamps as number.\n\n    :type tzinfo: callable\n    :param tzinfo: A ``datetime.tzinfo`` class or compatible callable.\n    \"\"\"\n    epoch_zero = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc())\n    epoch_zero_localized = epoch_zero.astimezone(tzinfo())\n    return epoch_zero_localized + datetime.timedelta(seconds=value)\n\n\ndef _parse_timestamp_with_tzinfo(value, tzinfo):\n    \"\"\"Parse timestamp with pluggable tzinfo options.\"\"\"\n    if isinstance(value, (int, float)):\n        # Possibly an epoch time.\n        return datetime.datetime.fromtimestamp(value, tzinfo())\n    else:\n        try:\n            return datetime.datetime.fromtimestamp(float(value), tzinfo())\n        except (TypeError, ValueError):\n            pass\n    try:\n        # In certain cases, a timestamp marked with GMT can be parsed into a\n        # different time zone, so here we provide a context which will\n        # enforce that GMT == UTC.\n        return dateutil.parser.parse(value, tzinfos={'GMT': tzutc()})\n    except (TypeError, ValueError) as e:\n        raise ValueError(f'Invalid timestamp \"{value}\": {e}')\n\n\ndef parse_timestamp(value):\n    \"\"\"Parse a timestamp into a datetime object.\n\n    Supported formats:\n\n        * iso8601\n        * rfc822\n        * epoch (value is an integer)\n\n    This will return a ``datetime.datetime`` object.\n\n    \"\"\"\n    tzinfo_options = get_tzinfo_options()\n    for tzinfo in tzinfo_options:\n        try:\n            return _parse_timestamp_with_tzinfo(value, tzinfo)\n        except (OSError, OverflowError) as e:\n            logger.debug(\n                'Unable to parse timestamp with \"%s\" timezone info.',\n                tzinfo.__name__,\n                exc_info=e,\n            )\n    # For numeric values attempt fallback to using fromtimestamp-free method.\n    # From Python's ``datetime.datetime.fromtimestamp`` documentation: \"This\n    # may raise ``OverflowError``, if the timestamp is out of the range of\n    # values supported by the platform C localtime() function, and ``OSError``\n    # on localtime() failure. It's common for this to be restricted to years\n    # from 1970 through 2038.\"\n    try:\n        numeric_value = float(value)\n    except (TypeError, ValueError):\n        pass\n    else:\n        try:\n            for tzinfo in tzinfo_options:\n                return _epoch_seconds_to_datetime(numeric_value, tzinfo=tzinfo)\n        except (OSError, OverflowError) as e:\n            logger.debug(\n                'Unable to parse timestamp using fallback method with \"%s\" '\n                'timezone info.',\n                tzinfo.__name__,\n                exc_info=e,\n            )\n    raise RuntimeError(\n        'Unable to calculate correct timezone offset for \"%s\"' % value\n    )\n\n\ndef parse_to_aware_datetime(value):\n    \"\"\"Converted the passed in value to a datetime object with tzinfo.\n\n    This function can be used to normalize all timestamp inputs.  This\n    function accepts a number of different types of inputs, but\n    will always return a datetime.datetime object with time zone\n    information.\n\n    The input param ``value`` can be one of several types:\n\n        * A datetime object (both naive and aware)\n        * An integer representing the epoch time (can also be a string\n          of the integer, i.e '0', instead of 0).  The epoch time is\n          considered to be UTC.\n        * An iso8601 formatted timestamp.  This does not need to be\n          a complete timestamp, it can contain just the date portion\n          without the time component.\n\n    The returned value will be a datetime object that will have tzinfo.\n    If no timezone info was provided in the input value, then UTC is\n    assumed, not local time.\n\n    \"\"\"\n    # This is a general purpose method that handles several cases of\n    # converting the provided value to a string timestamp suitable to be\n    # serialized to an http request. It can handle:\n    # 1) A datetime.datetime object.\n    if isinstance(value, _DatetimeClass):\n        datetime_obj = value\n    else:\n        # 2) A string object that's formatted as a timestamp.\n        #    We document this as being an iso8601 timestamp, although\n        #    parse_timestamp is a bit more flexible.\n        datetime_obj = parse_timestamp(value)\n    if datetime_obj.tzinfo is None:\n        # I think a case would be made that if no time zone is provided,\n        # we should use the local time.  However, to restore backwards\n        # compat, the previous behavior was to assume UTC, which is\n        # what we're going to do here.\n        datetime_obj = datetime_obj.replace(tzinfo=tzutc())\n    else:\n        datetime_obj = datetime_obj.astimezone(tzutc())\n    return datetime_obj\n\n\ndef datetime2timestamp(dt, default_timezone=None):\n    \"\"\"Calculate the timestamp based on the given datetime instance.\n\n    :type dt: datetime\n    :param dt: A datetime object to be converted into timestamp\n    :type default_timezone: tzinfo\n    :param default_timezone: If it is provided as None, we treat it as tzutc().\n                             But it is only used when dt is a naive datetime.\n    :returns: The timestamp\n    \"\"\"\n    epoch = datetime.datetime(1970, 1, 1)\n    if dt.tzinfo is None:\n        if default_timezone is None:\n            default_timezone = tzutc()\n        dt = dt.replace(tzinfo=default_timezone)\n    d = dt.replace(tzinfo=None) - dt.utcoffset() - epoch\n    return d.total_seconds()\n\n\ndef calculate_sha256(body, as_hex=False):\n    \"\"\"Calculate a sha256 checksum.\n\n    This method will calculate the sha256 checksum of a file like\n    object.  Note that this method will iterate through the entire\n    file contents.  The caller is responsible for ensuring the proper\n    starting position of the file and ``seek()``'ing the file back\n    to its starting location if other consumers need to read from\n    the file like object.\n\n    :param body: Any file like object.  The file must be opened\n        in binary mode such that a ``.read()`` call returns bytes.\n    :param as_hex: If True, then the hex digest is returned.\n        If False, then the digest (as binary bytes) is returned.\n\n    :returns: The sha256 checksum\n\n    \"\"\"\n    checksum = hashlib.sha256()\n    for chunk in iter(lambda: body.read(1024 * 1024), b''):\n        checksum.update(chunk)\n    if as_hex:\n        return checksum.hexdigest()\n    else:\n        return checksum.digest()\n\n\ndef calculate_tree_hash(body):\n    \"\"\"Calculate a tree hash checksum.\n\n    For more information see:\n\n    http://docs.aws.amazon.com/amazonglacier/latest/dev/checksum-calculations.html\n\n    :param body: Any file like object.  This has the same constraints as\n        the ``body`` param in calculate_sha256\n\n    :rtype: str\n    :returns: The hex version of the calculated tree hash\n\n    \"\"\"\n    chunks = []\n    required_chunk_size = 1024 * 1024\n    sha256 = hashlib.sha256\n    for chunk in iter(lambda: body.read(required_chunk_size), b''):\n        chunks.append(sha256(chunk).digest())\n    if not chunks:\n        return sha256(b'').hexdigest()\n    while len(chunks) > 1:\n        new_chunks = []\n        for first, second in _in_pairs(chunks):\n            if second is not None:\n                new_chunks.append(sha256(first + second).digest())\n            else:\n                # We're at the end of the list and there's no pair left.\n                new_chunks.append(first)\n        chunks = new_chunks\n    return binascii.hexlify(chunks[0]).decode('ascii')\n\n\ndef _in_pairs(iterable):\n    # Creates iterator that iterates over the list in pairs:\n    # for a, b in _in_pairs([0, 1, 2, 3, 4]):\n    #     print(a, b)\n    #\n    # will print:\n    # 0, 1\n    # 2, 3\n    # 4, None\n    shared_iter = iter(iterable)\n    # Note that zip_longest is a compat import that uses\n    # the itertools izip_longest.  This creates an iterator,\n    # this call below does _not_ immediately create the list\n    # of pairs.\n    return zip_longest(shared_iter, shared_iter)\n\n\nclass CachedProperty:\n    \"\"\"A read only property that caches the initially computed value.\n\n    This descriptor will only call the provided ``fget`` function once.\n    Subsequent access to this property will return the cached value.\n\n    \"\"\"\n\n    def __init__(self, fget):\n        self._fget = fget\n\n    def __get__(self, obj, cls):\n        if obj is None:\n            return self\n        else:\n            computed_value = self._fget(obj)\n            obj.__dict__[self._fget.__name__] = computed_value\n            return computed_value\n\n\nclass ArgumentGenerator:\n    \"\"\"Generate sample input based on a shape model.\n\n    This class contains a ``generate_skeleton`` method that will take\n    an input/output shape (created from ``botocore.model``) and generate\n    a sample dictionary corresponding to the input/output shape.\n\n    The specific values used are place holder values. For strings either an\n    empty string or the member name can be used, for numbers 0 or 0.0 is used.\n    The intended usage of this class is to generate the *shape* of the input\n    structure.\n\n    This can be useful for operations that have complex input shapes.\n    This allows a user to just fill in the necessary data instead of\n    worrying about the specific structure of the input arguments.\n\n    Example usage::\n\n        s = botocore.session.get_session()\n        ddb = s.get_service_model('dynamodb')\n        arg_gen = ArgumentGenerator()\n        sample_input = arg_gen.generate_skeleton(\n            ddb.operation_model('CreateTable').input_shape)\n        print(\"Sample input for dynamodb.CreateTable: %s\" % sample_input)\n\n    \"\"\"\n\n    def __init__(self, use_member_names=False):\n        self._use_member_names = use_member_names\n\n    def generate_skeleton(self, shape):\n        \"\"\"Generate a sample input.\n\n        :type shape: ``botocore.model.Shape``\n        :param shape: The input shape.\n\n        :return: The generated skeleton input corresponding to the\n            provided input shape.\n\n        \"\"\"\n        stack = []\n        return self._generate_skeleton(shape, stack)\n\n    def _generate_skeleton(self, shape, stack, name=''):\n        stack.append(shape.name)\n        try:\n            if shape.type_name == 'structure':\n                return self._generate_type_structure(shape, stack)\n            elif shape.type_name == 'list':\n                return self._generate_type_list(shape, stack)\n            elif shape.type_name == 'map':\n                return self._generate_type_map(shape, stack)\n            elif shape.type_name == 'string':\n                if self._use_member_names:\n                    return name\n                if shape.enum:\n                    return random.choice(shape.enum)\n                return ''\n            elif shape.type_name in ['integer', 'long']:\n                return 0\n            elif shape.type_name in ['float', 'double']:\n                return 0.0\n            elif shape.type_name == 'boolean':\n                return True\n            elif shape.type_name == 'timestamp':\n                return datetime.datetime(1970, 1, 1, 0, 0, 0)\n        finally:\n            stack.pop()\n\n    def _generate_type_structure(self, shape, stack):\n        if stack.count(shape.name) > 1:\n            return {}\n        skeleton = OrderedDict()\n        for member_name, member_shape in shape.members.items():\n            skeleton[member_name] = self._generate_skeleton(\n                member_shape, stack, name=member_name\n            )\n        return skeleton\n\n    def _generate_type_list(self, shape, stack):\n        # For list elements we've arbitrarily decided to\n        # return two elements for the skeleton list.\n        name = ''\n        if self._use_member_names:\n            name = shape.member.name\n        return [\n            self._generate_skeleton(shape.member, stack, name),\n        ]\n\n    def _generate_type_map(self, shape, stack):\n        key_shape = shape.key\n        value_shape = shape.value\n        assert key_shape.type_name == 'string'\n        return OrderedDict(\n            [\n                ('KeyName', self._generate_skeleton(value_shape, stack)),\n            ]\n        )\n\n\ndef is_valid_ipv6_endpoint_url(endpoint_url):\n    if UNSAFE_URL_CHARS.intersection(endpoint_url):\n        return False\n    hostname = f'[{urlparse(endpoint_url).hostname}]'\n    return IPV6_ADDRZ_RE.match(hostname) is not None\n\n\ndef is_valid_ipv4_endpoint_url(endpoint_url):\n    hostname = urlparse(endpoint_url).hostname\n    return IPV4_RE.match(hostname) is not None\n\n\ndef is_valid_endpoint_url(endpoint_url):\n    \"\"\"Verify the endpoint_url is valid.\n\n    :type endpoint_url: string\n    :param endpoint_url: An endpoint_url.  Must have at least a scheme\n        and a hostname.\n\n    :return: True if the endpoint url is valid. False otherwise.\n\n    \"\"\"\n    # post-bpo-43882 urlsplit() strips unsafe characters from URL, causing\n    # it to pass hostname validation below.  Detect them early to fix that.\n    if UNSAFE_URL_CHARS.intersection(endpoint_url):\n        return False\n    parts = urlsplit(endpoint_url)\n    hostname = parts.hostname\n    if hostname is None:\n        return False\n    if len(hostname) > 255:\n        return False\n    if hostname[-1] == \".\":\n        hostname = hostname[:-1]\n    allowed = re.compile(\n        r\"^((?!-)[A-Z\\d-]{1,63}(?<!-)\\.)*((?!-)[A-Z\\d-]{1,63}(?<!-))$\",\n        re.IGNORECASE,\n    )\n    return allowed.match(hostname)\n\n\ndef is_valid_uri(endpoint_url):\n    return is_valid_endpoint_url(endpoint_url) or is_valid_ipv6_endpoint_url(\n        endpoint_url\n    )\n\n\ndef validate_region_name(region_name):\n    \"\"\"Provided region_name must be a valid host label.\"\"\"\n    if region_name is None:\n        return\n    valid_host_label = re.compile(r'^(?![0-9]+$)(?!-)[a-zA-Z0-9-]{,63}(?<!-)$')\n    valid = valid_host_label.match(region_name)\n    if not valid:\n        raise InvalidRegionError(region_name=region_name)\n\n\ndef check_dns_name(bucket_name):\n    \"\"\"\n    Check to see if the ``bucket_name`` complies with the\n    restricted DNS naming conventions necessary to allow\n    access via virtual-hosting style.\n\n    Even though \".\" characters are perfectly valid in this DNS\n    naming scheme, we are going to punt on any name containing a\n    \".\" character because these will cause SSL cert validation\n    problems if we try to use virtual-hosting style addressing.\n    \"\"\"\n    if '.' in bucket_name:\n        return False\n    n = len(bucket_name)\n    if n < 3 or n > 63:\n        # Wrong length\n        return False\n    match = LABEL_RE.match(bucket_name)\n    if match is None or match.end() != len(bucket_name):\n        return False\n    return True\n\n\ndef fix_s3_host(\n    request,\n    signature_version,\n    region_name,\n    default_endpoint_url=None,\n    **kwargs,\n):\n    \"\"\"\n    This handler looks at S3 requests just before they are signed.\n    If there is a bucket name on the path (true for everything except\n    ListAllBuckets) it checks to see if that bucket name conforms to\n    the DNS naming conventions.  If it does, it alters the request to\n    use ``virtual hosting`` style addressing rather than ``path-style``\n    addressing.\n\n    \"\"\"\n    if request.context.get('use_global_endpoint', False):\n        default_endpoint_url = 's3.amazonaws.com'\n    try:\n        switch_to_virtual_host_style(\n            request, signature_version, default_endpoint_url\n        )\n    except InvalidDNSNameError as e:\n        bucket_name = e.kwargs['bucket_name']\n        logger.debug(\n            'Not changing URI, bucket is not DNS compatible: %s', bucket_name\n        )\n\n\ndef switch_to_virtual_host_style(\n    request, signature_version, default_endpoint_url=None, **kwargs\n):\n    \"\"\"\n    This is a handler to force virtual host style s3 addressing no matter\n    the signature version (which is taken in consideration for the default\n    case). If the bucket is not DNS compatible an InvalidDNSName is thrown.\n\n    :param request: A AWSRequest object that is about to be sent.\n    :param signature_version: The signature version to sign with\n    :param default_endpoint_url: The endpoint to use when switching to a\n        virtual style. If None is supplied, the virtual host will be\n        constructed from the url of the request.\n    \"\"\"\n    if request.auth_path is not None:\n        # The auth_path has already been applied (this may be a\n        # retried request).  We don't need to perform this\n        # customization again.\n        return\n    elif _is_get_bucket_location_request(request):\n        # For the GetBucketLocation response, we should not be using\n        # the virtual host style addressing so we can avoid any sigv4\n        # issues.\n        logger.debug(\n            \"Request is GetBucketLocation operation, not checking \"\n            \"for DNS compatibility.\"\n        )\n        return\n    parts = urlsplit(request.url)\n    request.auth_path = parts.path\n    path_parts = parts.path.split('/')\n\n    # Retrieve what the endpoint we will be prepending the bucket name to.\n    if default_endpoint_url is None:\n        default_endpoint_url = parts.netloc\n\n    if len(path_parts) > 1:\n        bucket_name = path_parts[1]\n        if not bucket_name:\n            # If the bucket name is empty we should not be checking for\n            # dns compatibility.\n            return\n        logger.debug('Checking for DNS compatible bucket for: %s', request.url)\n        if check_dns_name(bucket_name):\n            # If the operation is on a bucket, the auth_path must be\n            # terminated with a '/' character.\n            if len(path_parts) == 2:\n                if request.auth_path[-1] != '/':\n                    request.auth_path += '/'\n            path_parts.remove(bucket_name)\n            # At the very least the path must be a '/', such as with the\n            # CreateBucket operation when DNS style is being used. If this\n            # is not used you will get an empty path which is incorrect.\n            path = '/'.join(path_parts) or '/'\n            global_endpoint = default_endpoint_url\n            host = bucket_name + '.' + global_endpoint\n            new_tuple = (parts.scheme, host, path, parts.query, '')\n            new_uri = urlunsplit(new_tuple)\n            request.url = new_uri\n            logger.debug('URI updated to: %s', new_uri)\n        else:\n            raise InvalidDNSNameError(bucket_name=bucket_name)\n\n\ndef _is_get_bucket_location_request(request):\n    return request.url.endswith('?location')\n\n\ndef instance_cache(func):\n    \"\"\"Method decorator for caching method calls to a single instance.\n\n    **This is not a general purpose caching decorator.**\n\n    In order to use this, you *must* provide an ``_instance_cache``\n    attribute on the instance.\n\n    This decorator is used to cache method calls.  The cache is only\n    scoped to a single instance though such that multiple instances\n    will maintain their own cache.  In order to keep things simple,\n    this decorator requires that you provide an ``_instance_cache``\n    attribute on your instance.\n\n    \"\"\"\n    func_name = func.__name__\n\n    @functools.wraps(func)\n    def _cache_guard(self, *args, **kwargs):\n        cache_key = (func_name, args)\n        if kwargs:\n            kwarg_items = tuple(sorted(kwargs.items()))\n            cache_key = (func_name, args, kwarg_items)\n        result = self._instance_cache.get(cache_key)\n        if result is not None:\n            return result\n        result = func(self, *args, **kwargs)\n        self._instance_cache[cache_key] = result\n        return result\n\n    return _cache_guard\n\n\ndef lru_cache_weakref(*cache_args, **cache_kwargs):\n    \"\"\"\n    Version of functools.lru_cache that stores a weak reference to ``self``.\n\n    Serves the same purpose as :py:func:`instance_cache` but uses Python's\n    functools implementation which offers ``max_size`` and ``typed`` properties.\n\n    lru_cache is a global cache even when used on a method. The cache's\n    reference to ``self`` will prevent garbace collection of the object. This\n    wrapper around functools.lru_cache replaces the reference to ``self`` with\n    a weak reference to not interfere with garbage collection.\n    \"\"\"\n\n    def wrapper(func):\n        @functools.lru_cache(*cache_args, **cache_kwargs)\n        def func_with_weakref(weakref_to_self, *args, **kwargs):\n            return func(weakref_to_self(), *args, **kwargs)\n\n        @functools.wraps(func)\n        def inner(self, *args, **kwargs):\n            return func_with_weakref(weakref.ref(self), *args, **kwargs)\n\n        inner.cache_info = func_with_weakref.cache_info\n        return inner\n\n    return wrapper\n\n\ndef switch_host_s3_accelerate(request, operation_name, **kwargs):\n    \"\"\"Switches the current s3 endpoint with an S3 Accelerate endpoint\"\"\"\n\n    # Note that when registered the switching of the s3 host happens\n    # before it gets changed to virtual. So we are not concerned with ensuring\n    # that the bucket name is translated to the virtual style here and we\n    # can hard code the Accelerate endpoint.\n    parts = urlsplit(request.url).netloc.split('.')\n    parts = [p for p in parts if p in S3_ACCELERATE_WHITELIST]\n    endpoint = 'https://s3-accelerate.'\n    if len(parts) > 0:\n        endpoint += '.'.join(parts) + '.'\n    endpoint += 'amazonaws.com'\n\n    if operation_name in ['ListBuckets', 'CreateBucket', 'DeleteBucket']:\n        return\n    _switch_hosts(request, endpoint, use_new_scheme=False)\n\n\ndef switch_host_with_param(request, param_name):\n    \"\"\"Switches the host using a parameter value from a JSON request body\"\"\"\n    request_json = json.loads(request.data.decode('utf-8'))\n    if request_json.get(param_name):\n        new_endpoint = request_json[param_name]\n        _switch_hosts(request, new_endpoint)\n\n\ndef _switch_hosts(request, new_endpoint, use_new_scheme=True):\n    final_endpoint = _get_new_endpoint(\n        request.url, new_endpoint, use_new_scheme\n    )\n    request.url = final_endpoint\n\n\ndef _get_new_endpoint(original_endpoint, new_endpoint, use_new_scheme=True):\n    new_endpoint_components = urlsplit(new_endpoint)\n    original_endpoint_components = urlsplit(original_endpoint)\n    scheme = original_endpoint_components.scheme\n    if use_new_scheme:\n        scheme = new_endpoint_components.scheme\n    final_endpoint_components = (\n        scheme,\n        new_endpoint_components.netloc,\n        original_endpoint_components.path,\n        original_endpoint_components.query,\n        '',\n    )\n    final_endpoint = urlunsplit(final_endpoint_components)\n    logger.debug(f'Updating URI from {original_endpoint} to {final_endpoint}')\n    return final_endpoint\n\n\ndef deep_merge(base, extra):\n    \"\"\"Deeply two dictionaries, overriding existing keys in the base.\n\n    :param base: The base dictionary which will be merged into.\n    :param extra: The dictionary to merge into the base. Keys from this\n        dictionary will take precedence.\n    \"\"\"\n    for key in extra:\n        # If the key represents a dict on both given dicts, merge the sub-dicts\n        if (\n            key in base\n            and isinstance(base[key], dict)\n            and isinstance(extra[key], dict)\n        ):\n            deep_merge(base[key], extra[key])\n            continue\n\n        # Otherwise, set the key on the base to be the value of the extra.\n        base[key] = extra[key]\n\n\ndef hyphenize_service_id(service_id):\n    \"\"\"Translate the form used for event emitters.\n\n    :param service_id: The service_id to convert.\n    \"\"\"\n    return service_id.replace(' ', '-').lower()\n\n\nclass IdentityCache:\n    \"\"\"Base IdentityCache implementation for storing and retrieving\n    highly accessed credentials.\n\n    This class is not intended to be instantiated in user code.\n    \"\"\"\n\n    METHOD = \"base_identity_cache\"\n\n    def __init__(self, client, credential_cls):\n        self._client = client\n        self._credential_cls = credential_cls\n\n    def get_credentials(self, **kwargs):\n        callback = self.build_refresh_callback(**kwargs)\n        metadata = callback()\n        credential_entry = self._credential_cls.create_from_metadata(\n            metadata=metadata,\n            refresh_using=callback,\n            method=self.METHOD,\n            advisory_timeout=45,\n            mandatory_timeout=10,\n        )\n        return credential_entry\n\n    def build_refresh_callback(**kwargs):\n        \"\"\"Callback to be implemented by subclasses.\n\n        Returns a set of metadata to be converted into a new\n        credential instance.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass S3ExpressIdentityCache(IdentityCache):\n    \"\"\"S3Express IdentityCache for retrieving and storing\n    credentials from CreateSession calls.\n\n    This class is not intended to be instantiated in user code.\n    \"\"\"\n\n    METHOD = \"s3express\"\n\n    def __init__(self, client, credential_cls):\n        self._client = client\n        self._credential_cls = credential_cls\n\n    @functools.lru_cache(maxsize=100)\n    def get_credentials(self, bucket):\n        return super().get_credentials(bucket=bucket)\n\n    def build_refresh_callback(self, bucket):\n        def refresher():\n            response = self._client.create_session(Bucket=bucket)\n            creds = response['Credentials']\n            expiration = self._serialize_if_needed(\n                creds['Expiration'], iso=True\n            )\n            return {\n                \"access_key\": creds['AccessKeyId'],\n                \"secret_key\": creds['SecretAccessKey'],\n                \"token\": creds['SessionToken'],\n                \"expiry_time\": expiration,\n            }\n\n        return refresher\n\n    def _serialize_if_needed(self, value, iso=False):\n        if isinstance(value, _DatetimeClass):\n            if iso:\n                return value.isoformat()\n            return value.strftime('%Y-%m-%dT%H:%M:%S%Z')\n        return value\n\n\nclass S3ExpressIdentityResolver:\n    def __init__(self, client, credential_cls, cache=None):\n        self._client = weakref.proxy(client)\n\n        if cache is None:\n            cache = S3ExpressIdentityCache(self._client, credential_cls)\n        self._cache = cache\n\n    def register(self, event_emitter=None):\n        logger.debug('Registering S3Express Identity Resolver')\n        emitter = event_emitter or self._client.meta.events\n        emitter.register('before-call.s3', self.apply_signing_cache_key)\n        emitter.register('before-sign.s3', self.resolve_s3express_identity)\n\n    def apply_signing_cache_key(self, params, context, **kwargs):\n        endpoint_properties = context.get('endpoint_properties', {})\n        backend = endpoint_properties.get('backend', None)\n\n        # Add cache key if Bucket supplied for s3express request\n        bucket_name = context.get('input_params', {}).get('Bucket')\n        if backend == 'S3Express' and bucket_name is not None:\n            context.setdefault('signing', {})\n            context['signing']['cache_key'] = bucket_name\n\n    def resolve_s3express_identity(\n        self,\n        request,\n        signing_name,\n        region_name,\n        signature_version,\n        request_signer,\n        operation_name,\n        **kwargs,\n    ):\n        signing_context = request.context.get('signing', {})\n        signing_name = signing_context.get('signing_name')\n        if signing_name == 's3express' and signature_version.startswith(\n            'v4-s3express'\n        ):\n            signing_context['identity_cache'] = self._cache\n            if 'cache_key' not in signing_context:\n                signing_context['cache_key'] = (\n                    request.context.get('s3_redirect', {})\n                    .get('params', {})\n                    .get('Bucket')\n                )\n\n\nclass S3RegionRedirectorv2:\n    \"\"\"Updated version of S3RegionRedirector for use when\n    EndpointRulesetResolver is in use for endpoint resolution.\n\n    This class is considered private and subject to abrupt breaking changes or\n    removal without prior announcement. Please do not use it directly.\n    \"\"\"\n\n    def __init__(self, endpoint_bridge, client, cache=None):\n        self._cache = cache or {}\n        self._client = weakref.proxy(client)\n\n    def register(self, event_emitter=None):\n        logger.debug('Registering S3 region redirector handler')\n        emitter = event_emitter or self._client.meta.events\n        emitter.register('needs-retry.s3', self.redirect_from_error)\n        emitter.register(\n            'before-parameter-build.s3', self.annotate_request_context\n        )\n        emitter.register(\n            'before-endpoint-resolution.s3', self.redirect_from_cache\n        )\n\n    def redirect_from_error(self, request_dict, response, operation, **kwargs):\n        \"\"\"\n        An S3 request sent to the wrong region will return an error that\n        contains the endpoint the request should be sent to. This handler\n        will add the redirect information to the signing context and then\n        redirect the request.\n        \"\"\"\n        if response is None:\n            # This could be none if there was a ConnectionError or other\n            # transport error.\n            return\n\n        redirect_ctx = request_dict.get('context', {}).get('s3_redirect', {})\n        if ArnParser.is_arn(redirect_ctx.get('bucket')):\n            logger.debug(\n                'S3 request was previously for an Accesspoint ARN, not '\n                'redirecting.'\n            )\n            return\n\n        if redirect_ctx.get('redirected'):\n            logger.debug(\n                'S3 request was previously redirected, not redirecting.'\n            )\n            return\n\n        error = response[1].get('Error', {})\n        error_code = error.get('Code')\n        response_metadata = response[1].get('ResponseMetadata', {})\n\n        # We have to account for 400 responses because\n        # if we sign a Head* request with the wrong region,\n        # we'll get a 400 Bad Request but we won't get a\n        # body saying it's an \"AuthorizationHeaderMalformed\".\n        is_special_head_object = (\n            error_code in ('301', '400') and operation.name == 'HeadObject'\n        )\n        is_special_head_bucket = (\n            error_code in ('301', '400')\n            and operation.name == 'HeadBucket'\n            and 'x-amz-bucket-region'\n            in response_metadata.get('HTTPHeaders', {})\n        )\n        is_wrong_signing_region = (\n            error_code == 'AuthorizationHeaderMalformed' and 'Region' in error\n        )\n        is_redirect_status = response[0] is not None and response[\n            0\n        ].status_code in (301, 302, 307)\n        is_permanent_redirect = error_code == 'PermanentRedirect'\n        if not any(\n            [\n                is_special_head_object,\n                is_wrong_signing_region,\n                is_permanent_redirect,\n                is_special_head_bucket,\n                is_redirect_status,\n            ]\n        ):\n            return\n\n        bucket = request_dict['context']['s3_redirect']['bucket']\n        client_region = request_dict['context'].get('client_region')\n        new_region = self.get_bucket_region(bucket, response)\n\n        if new_region is None:\n            logger.debug(\n                \"S3 client configured for region %s but the bucket %s is not \"\n                \"in that region and the proper region could not be \"\n                \"automatically determined.\" % (client_region, bucket)\n            )\n            return\n\n        logger.debug(\n            \"S3 client configured for region %s but the bucket %s is in region\"\n            \" %s; Please configure the proper region to avoid multiple \"\n            \"unnecessary redirects and signing attempts.\"\n            % (client_region, bucket, new_region)\n        )\n        # Adding the new region to _cache will make construct_endpoint() to\n        # use the new region as value for the AWS::Region builtin parameter.\n        self._cache[bucket] = new_region\n\n        # Re-resolve endpoint with new region and modify request_dict with\n        # the new URL, auth scheme, and signing context.\n        ep_resolver = self._client._ruleset_resolver\n        ep_info = ep_resolver.construct_endpoint(\n            operation_model=operation,\n            call_args=request_dict['context']['s3_redirect']['params'],\n            request_context=request_dict['context'],\n        )\n        request_dict['url'] = self.set_request_url(\n            request_dict['url'], ep_info.url\n        )\n        request_dict['context']['s3_redirect']['redirected'] = True\n        auth_schemes = ep_info.properties.get('authSchemes')\n        if auth_schemes is not None:\n            auth_info = ep_resolver.auth_schemes_to_signing_ctx(auth_schemes)\n            auth_type, signing_context = auth_info\n            request_dict['context']['auth_type'] = auth_type\n            request_dict['context']['signing'] = {\n                **request_dict['context'].get('signing', {}),\n                **signing_context,\n            }\n\n        # Return 0 so it doesn't wait to retry\n        return 0\n\n    def get_bucket_region(self, bucket, response):\n        \"\"\"\n        There are multiple potential sources for the new region to redirect to,\n        but they aren't all universally available for use. This will try to\n        find region from response elements, but will fall back to calling\n        HEAD on the bucket if all else fails.\n\n        :param bucket: The bucket to find the region for. This is necessary if\n            the region is not available in the error response.\n        :param response: A response representing a service request that failed\n            due to incorrect region configuration.\n        \"\"\"\n        # First try to source the region from the headers.\n        service_response = response[1]\n        response_headers = service_response['ResponseMetadata']['HTTPHeaders']\n        if 'x-amz-bucket-region' in response_headers:\n            return response_headers['x-amz-bucket-region']\n\n        # Next, check the error body\n        region = service_response.get('Error', {}).get('Region', None)\n        if region is not None:\n            return region\n\n        # Finally, HEAD the bucket. No other choice sadly.\n        try:\n            response = self._client.head_bucket(Bucket=bucket)\n            headers = response['ResponseMetadata']['HTTPHeaders']\n        except ClientError as e:\n            headers = e.response['ResponseMetadata']['HTTPHeaders']\n\n        region = headers.get('x-amz-bucket-region', None)\n        return region\n\n    def set_request_url(self, old_url, new_endpoint, **kwargs):\n        \"\"\"\n        Splice a new endpoint into an existing URL. Note that some endpoints\n        from the the endpoint provider have a path component which will be\n        discarded by this function.\n        \"\"\"\n        return _get_new_endpoint(old_url, new_endpoint, False)\n\n    def redirect_from_cache(self, builtins, params, **kwargs):\n        \"\"\"\n        If a bucket name has been redirected before, it is in the cache. This\n        handler will update the AWS::Region endpoint resolver builtin param\n        to use the region from cache instead of the client region to avoid the\n        redirect.\n        \"\"\"\n        bucket = params.get('Bucket')\n        if bucket is not None and bucket in self._cache:\n            new_region = self._cache.get(bucket)\n            builtins['AWS::Region'] = new_region\n\n    def annotate_request_context(self, params, context, **kwargs):\n        \"\"\"Store the bucket name in context for later use when redirecting.\n        The bucket name may be an access point ARN or alias.\n        \"\"\"\n        bucket = params.get('Bucket')\n        context['s3_redirect'] = {\n            'redirected': False,\n            'bucket': bucket,\n            'params': params,\n        }\n\n\nclass S3RegionRedirector:\n    \"\"\"This handler has been replaced by S3RegionRedirectorv2. The original\n    version remains in place for any third-party libraries that import it.\n    \"\"\"\n\n    def __init__(self, endpoint_bridge, client, cache=None):\n        self._endpoint_resolver = endpoint_bridge\n        self._cache = cache\n        if self._cache is None:\n            self._cache = {}\n\n        # This needs to be a weak ref in order to prevent memory leaks on\n        # python 2.6\n        self._client = weakref.proxy(client)\n\n        warnings.warn(\n            'The S3RegionRedirector class has been deprecated for a new '\n            'internal replacement. A future version of botocore may remove '\n            'this class.',\n            category=FutureWarning,\n        )\n\n    def register(self, event_emitter=None):\n        emitter = event_emitter or self._client.meta.events\n        emitter.register('needs-retry.s3', self.redirect_from_error)\n        emitter.register('before-call.s3', self.set_request_url)\n        emitter.register('before-parameter-build.s3', self.redirect_from_cache)\n\n    def redirect_from_error(self, request_dict, response, operation, **kwargs):\n        \"\"\"\n        An S3 request sent to the wrong region will return an error that\n        contains the endpoint the request should be sent to. This handler\n        will add the redirect information to the signing context and then\n        redirect the request.\n        \"\"\"\n        if response is None:\n            # This could be none if there was a ConnectionError or other\n            # transport error.\n            return\n\n        if self._is_s3_accesspoint(request_dict.get('context', {})):\n            logger.debug(\n                'S3 request was previously to an accesspoint, not redirecting.'\n            )\n            return\n\n        if request_dict.get('context', {}).get('s3_redirected'):\n            logger.debug(\n                'S3 request was previously redirected, not redirecting.'\n            )\n            return\n\n        error = response[1].get('Error', {})\n        error_code = error.get('Code')\n        response_metadata = response[1].get('ResponseMetadata', {})\n\n        # We have to account for 400 responses because\n        # if we sign a Head* request with the wrong region,\n        # we'll get a 400 Bad Request but we won't get a\n        # body saying it's an \"AuthorizationHeaderMalformed\".\n        is_special_head_object = (\n            error_code in ('301', '400') and operation.name == 'HeadObject'\n        )\n        is_special_head_bucket = (\n            error_code in ('301', '400')\n            and operation.name == 'HeadBucket'\n            and 'x-amz-bucket-region'\n            in response_metadata.get('HTTPHeaders', {})\n        )\n        is_wrong_signing_region = (\n            error_code == 'AuthorizationHeaderMalformed' and 'Region' in error\n        )\n        is_redirect_status = response[0] is not None and response[\n            0\n        ].status_code in (301, 302, 307)\n        is_permanent_redirect = error_code == 'PermanentRedirect'\n        if not any(\n            [\n                is_special_head_object,\n                is_wrong_signing_region,\n                is_permanent_redirect,\n                is_special_head_bucket,\n                is_redirect_status,\n            ]\n        ):\n            return\n\n        bucket = request_dict['context']['signing']['bucket']\n        client_region = request_dict['context'].get('client_region')\n        new_region = self.get_bucket_region(bucket, response)\n\n        if new_region is None:\n            logger.debug(\n                \"S3 client configured for region %s but the bucket %s is not \"\n                \"in that region and the proper region could not be \"\n                \"automatically determined.\" % (client_region, bucket)\n            )\n            return\n\n        logger.debug(\n            \"S3 client configured for region %s but the bucket %s is in region\"\n            \" %s; Please configure the proper region to avoid multiple \"\n            \"unnecessary redirects and signing attempts.\"\n            % (client_region, bucket, new_region)\n        )\n        endpoint = self._endpoint_resolver.resolve('s3', new_region)\n        endpoint = endpoint['endpoint_url']\n\n        signing_context = {\n            'region': new_region,\n            'bucket': bucket,\n            'endpoint': endpoint,\n        }\n        request_dict['context']['signing'] = signing_context\n\n        self._cache[bucket] = signing_context\n        self.set_request_url(request_dict, request_dict['context'])\n\n        request_dict['context']['s3_redirected'] = True\n\n        # Return 0 so it doesn't wait to retry\n        return 0\n\n    def get_bucket_region(self, bucket, response):\n        \"\"\"\n        There are multiple potential sources for the new region to redirect to,\n        but they aren't all universally available for use. This will try to\n        find region from response elements, but will fall back to calling\n        HEAD on the bucket if all else fails.\n\n        :param bucket: The bucket to find the region for. This is necessary if\n            the region is not available in the error response.\n        :param response: A response representing a service request that failed\n            due to incorrect region configuration.\n        \"\"\"\n        # First try to source the region from the headers.\n        service_response = response[1]\n        response_headers = service_response['ResponseMetadata']['HTTPHeaders']\n        if 'x-amz-bucket-region' in response_headers:\n            return response_headers['x-amz-bucket-region']\n\n        # Next, check the error body\n        region = service_response.get('Error', {}).get('Region', None)\n        if region is not None:\n            return region\n\n        # Finally, HEAD the bucket. No other choice sadly.\n        try:\n            response = self._client.head_bucket(Bucket=bucket)\n            headers = response['ResponseMetadata']['HTTPHeaders']\n        except ClientError as e:\n            headers = e.response['ResponseMetadata']['HTTPHeaders']\n\n        region = headers.get('x-amz-bucket-region', None)\n        return region\n\n    def set_request_url(self, params, context, **kwargs):\n        endpoint = context.get('signing', {}).get('endpoint', None)\n        if endpoint is not None:\n            params['url'] = _get_new_endpoint(params['url'], endpoint, False)\n\n    def redirect_from_cache(self, params, context, **kwargs):\n        \"\"\"\n        This handler retrieves a given bucket's signing context from the cache\n        and adds it into the request context.\n        \"\"\"\n        if self._is_s3_accesspoint(context):\n            return\n        bucket = params.get('Bucket')\n        signing_context = self._cache.get(bucket)\n        if signing_context is not None:\n            context['signing'] = signing_context\n        else:\n            context['signing'] = {'bucket': bucket}\n\n    def _is_s3_accesspoint(self, context):\n        return 's3_accesspoint' in context\n\n\nclass InvalidArnException(ValueError):\n    pass\n\n\nclass ArnParser:\n    def parse_arn(self, arn):\n        arn_parts = arn.split(':', 5)\n        if len(arn_parts) < 6:\n            raise InvalidArnException(\n                'Provided ARN: %s must be of the format: '\n                'arn:partition:service:region:account:resource' % arn\n            )\n        return {\n            'partition': arn_parts[1],\n            'service': arn_parts[2],\n            'region': arn_parts[3],\n            'account': arn_parts[4],\n            'resource': arn_parts[5],\n        }\n\n    @staticmethod\n    def is_arn(value):\n        if not isinstance(value, str) or not value.startswith('arn:'):\n            return False\n        arn_parser = ArnParser()\n        try:\n            arn_parser.parse_arn(value)\n            return True\n        except InvalidArnException:\n            return False\n\n\nclass S3ArnParamHandler:\n    _RESOURCE_REGEX = re.compile(\n        r'^(?P<resource_type>accesspoint|outpost)[/:](?P<resource_name>.+)$'\n    )\n    _OUTPOST_RESOURCE_REGEX = re.compile(\n        r'^(?P<outpost_name>[a-zA-Z0-9\\-]{1,63})[/:]accesspoint[/:]'\n        r'(?P<accesspoint_name>[a-zA-Z0-9\\-]{1,63}$)'\n    )\n    _BLACKLISTED_OPERATIONS = ['CreateBucket']\n\n    def __init__(self, arn_parser=None):\n        self._arn_parser = arn_parser\n        if arn_parser is None:\n            self._arn_parser = ArnParser()\n\n    def register(self, event_emitter):\n        event_emitter.register('before-parameter-build.s3', self.handle_arn)\n\n    def handle_arn(self, params, model, context, **kwargs):\n        if model.name in self._BLACKLISTED_OPERATIONS:\n            return\n        arn_details = self._get_arn_details_from_bucket_param(params)\n        if arn_details is None:\n            return\n        if arn_details['resource_type'] == 'accesspoint':\n            self._store_accesspoint(params, context, arn_details)\n        elif arn_details['resource_type'] == 'outpost':\n            self._store_outpost(params, context, arn_details)\n\n    def _get_arn_details_from_bucket_param(self, params):\n        if 'Bucket' in params:\n            try:\n                arn = params['Bucket']\n                arn_details = self._arn_parser.parse_arn(arn)\n                self._add_resource_type_and_name(arn, arn_details)\n                return arn_details\n            except InvalidArnException:\n                pass\n        return None\n\n    def _add_resource_type_and_name(self, arn, arn_details):\n        match = self._RESOURCE_REGEX.match(arn_details['resource'])\n        if match:\n            arn_details['resource_type'] = match.group('resource_type')\n            arn_details['resource_name'] = match.group('resource_name')\n        else:\n            raise UnsupportedS3ArnError(arn=arn)\n\n    def _store_accesspoint(self, params, context, arn_details):\n        # Ideally the access-point would be stored as a parameter in the\n        # request where the serializer would then know how to serialize it,\n        # but access-points are not modeled in S3 operations so it would fail\n        # validation. Instead, we set the access-point to the bucket parameter\n        # to have some value set when serializing the request and additional\n        # information on the context from the arn to use in forming the\n        # access-point endpoint.\n        params['Bucket'] = arn_details['resource_name']\n        context['s3_accesspoint'] = {\n            'name': arn_details['resource_name'],\n            'account': arn_details['account'],\n            'partition': arn_details['partition'],\n            'region': arn_details['region'],\n            'service': arn_details['service'],\n        }\n\n    def _store_outpost(self, params, context, arn_details):\n        resource_name = arn_details['resource_name']\n        match = self._OUTPOST_RESOURCE_REGEX.match(resource_name)\n        if not match:\n            raise UnsupportedOutpostResourceError(resource_name=resource_name)\n        # Because we need to set the bucket name to something to pass\n        # validation we're going to use the access point name to be consistent\n        # with normal access point arns.\n        accesspoint_name = match.group('accesspoint_name')\n        params['Bucket'] = accesspoint_name\n        context['s3_accesspoint'] = {\n            'outpost_name': match.group('outpost_name'),\n            'name': accesspoint_name,\n            'account': arn_details['account'],\n            'partition': arn_details['partition'],\n            'region': arn_details['region'],\n            'service': arn_details['service'],\n        }\n\n\nclass S3EndpointSetter:\n    _DEFAULT_PARTITION = 'aws'\n    _DEFAULT_DNS_SUFFIX = 'amazonaws.com'\n\n    def __init__(\n        self,\n        endpoint_resolver,\n        region=None,\n        s3_config=None,\n        endpoint_url=None,\n        partition=None,\n        use_fips_endpoint=False,\n    ):\n        # This is calling the endpoint_resolver in regions.py\n        self._endpoint_resolver = endpoint_resolver\n        self._region = region\n        self._s3_config = s3_config\n        self._use_fips_endpoint = use_fips_endpoint\n        if s3_config is None:\n            self._s3_config = {}\n        self._endpoint_url = endpoint_url\n        self._partition = partition\n        if partition is None:\n            self._partition = self._DEFAULT_PARTITION\n\n    def register(self, event_emitter):\n        event_emitter.register('before-sign.s3', self.set_endpoint)\n        event_emitter.register('choose-signer.s3', self.set_signer)\n        event_emitter.register(\n            'before-call.s3.WriteGetObjectResponse',\n            self.update_endpoint_to_s3_object_lambda,\n        )\n\n    def update_endpoint_to_s3_object_lambda(self, params, context, **kwargs):\n        if self._use_accelerate_endpoint:\n            raise UnsupportedS3ConfigurationError(\n                msg='S3 client does not support accelerate endpoints for S3 Object Lambda operations',\n            )\n\n        self._override_signing_name(context, 's3-object-lambda')\n        if self._endpoint_url:\n            # Only update the url if an explicit url was not provided\n            return\n\n        resolver = self._endpoint_resolver\n        # Constructing endpoints as s3-object-lambda as region\n        resolved = resolver.construct_endpoint(\n            's3-object-lambda', self._region\n        )\n\n        # Ideally we would be able to replace the endpoint before\n        # serialization but there's no event to do that currently\n        # host_prefix is all the arn/bucket specs\n        new_endpoint = 'https://{host_prefix}{hostname}'.format(\n            host_prefix=params['host_prefix'],\n            hostname=resolved['hostname'],\n        )\n\n        params['url'] = _get_new_endpoint(params['url'], new_endpoint, False)\n\n    def set_endpoint(self, request, **kwargs):\n        if self._use_accesspoint_endpoint(request):\n            self._validate_accesspoint_supported(request)\n            self._validate_fips_supported(request)\n            self._validate_global_regions(request)\n            region_name = self._resolve_region_for_accesspoint_endpoint(\n                request\n            )\n            self._resolve_signing_name_for_accesspoint_endpoint(request)\n            self._switch_to_accesspoint_endpoint(request, region_name)\n            return\n        if self._use_accelerate_endpoint:\n            if self._use_fips_endpoint:\n                raise UnsupportedS3ConfigurationError(\n                    msg=(\n                        'Client is configured to use the FIPS psuedo region '\n                        'for \"%s\", but S3 Accelerate does not have any FIPS '\n                        'compatible endpoints.' % (self._region)\n                    )\n                )\n            switch_host_s3_accelerate(request=request, **kwargs)\n        if self._s3_addressing_handler:\n            self._s3_addressing_handler(request=request, **kwargs)\n\n    def _use_accesspoint_endpoint(self, request):\n        return 's3_accesspoint' in request.context\n\n    def _validate_fips_supported(self, request):\n        if not self._use_fips_endpoint:\n            return\n        if 'fips' in request.context['s3_accesspoint']['region']:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg={'Invalid ARN, FIPS region not allowed in ARN.'}\n            )\n        if 'outpost_name' in request.context['s3_accesspoint']:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client is configured to use the FIPS psuedo-region \"%s\", '\n                    'but outpost ARNs do not support FIPS endpoints.'\n                    % (self._region)\n                )\n            )\n        # Transforming psuedo region to actual region\n        accesspoint_region = request.context['s3_accesspoint']['region']\n        if accesspoint_region != self._region:\n            if not self._s3_config.get('use_arn_region', True):\n                # TODO: Update message to reflect use_arn_region\n                # is not set\n                raise UnsupportedS3AccesspointConfigurationError(\n                    msg=(\n                        'Client is configured to use the FIPS psuedo-region '\n                        'for \"%s\", but the access-point ARN provided is for '\n                        'the \"%s\" region. For clients using a FIPS '\n                        'psuedo-region calls to access-point ARNs in another '\n                        'region are not allowed.'\n                        % (self._region, accesspoint_region)\n                    )\n                )\n\n    def _validate_global_regions(self, request):\n        if self._s3_config.get('use_arn_region', True):\n            return\n        if self._region in ['aws-global', 's3-external-1']:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client is configured to use the global psuedo-region '\n                    '\"%s\". When providing access-point ARNs a regional '\n                    'endpoint must be specified.' % self._region\n                )\n            )\n\n    def _validate_accesspoint_supported(self, request):\n        if self._use_accelerate_endpoint:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client does not support s3 accelerate configuration '\n                    'when an access-point ARN is specified.'\n                )\n            )\n        request_partition = request.context['s3_accesspoint']['partition']\n        if request_partition != self._partition:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client is configured for \"%s\" partition, but access-point'\n                    ' ARN provided is for \"%s\" partition. The client and '\n                    ' access-point partition must be the same.'\n                    % (self._partition, request_partition)\n                )\n            )\n        s3_service = request.context['s3_accesspoint'].get('service')\n        if s3_service == 's3-object-lambda' and self._s3_config.get(\n            'use_dualstack_endpoint'\n        ):\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client does not support s3 dualstack configuration '\n                    'when an S3 Object Lambda access point ARN is specified.'\n                )\n            )\n        outpost_name = request.context['s3_accesspoint'].get('outpost_name')\n        if outpost_name and self._s3_config.get('use_dualstack_endpoint'):\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client does not support s3 dualstack configuration '\n                    'when an outpost ARN is specified.'\n                )\n            )\n        self._validate_mrap_s3_config(request)\n\n    def _validate_mrap_s3_config(self, request):\n        if not is_global_accesspoint(request.context):\n            return\n        if self._s3_config.get('s3_disable_multiregion_access_points'):\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Invalid configuration, Multi-Region Access Point '\n                    'ARNs are disabled.'\n                )\n            )\n        elif self._s3_config.get('use_dualstack_endpoint'):\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client does not support s3 dualstack configuration '\n                    'when a Multi-Region Access Point ARN is specified.'\n                )\n            )\n\n    def _resolve_region_for_accesspoint_endpoint(self, request):\n        if is_global_accesspoint(request.context):\n            # Requests going to MRAP endpoints MUST be set to any (*) region.\n            self._override_signing_region(request, '*')\n        elif self._s3_config.get('use_arn_region', True):\n            accesspoint_region = request.context['s3_accesspoint']['region']\n            # If we are using the region from the access point,\n            # we will also want to make sure that we set it as the\n            # signing region as well\n            self._override_signing_region(request, accesspoint_region)\n            return accesspoint_region\n        return self._region\n\n    def set_signer(self, context, **kwargs):\n        if is_global_accesspoint(context):\n            if HAS_CRT:\n                return 's3v4a'\n            else:\n                raise MissingDependencyException(\n                    msg=\"Using S3 with an MRAP arn requires an additional \"\n                    \"dependency. You will need to pip install \"\n                    \"botocore[crt] before proceeding.\"\n                )\n\n    def _resolve_signing_name_for_accesspoint_endpoint(self, request):\n        accesspoint_service = request.context['s3_accesspoint']['service']\n        self._override_signing_name(request.context, accesspoint_service)\n\n    def _switch_to_accesspoint_endpoint(self, request, region_name):\n        original_components = urlsplit(request.url)\n        accesspoint_endpoint = urlunsplit(\n            (\n                original_components.scheme,\n                self._get_netloc(request.context, region_name),\n                self._get_accesspoint_path(\n                    original_components.path, request.context\n                ),\n                original_components.query,\n                '',\n            )\n        )\n        logger.debug(\n            f'Updating URI from {request.url} to {accesspoint_endpoint}'\n        )\n        request.url = accesspoint_endpoint\n\n    def _get_netloc(self, request_context, region_name):\n        if is_global_accesspoint(request_context):\n            return self._get_mrap_netloc(request_context)\n        else:\n            return self._get_accesspoint_netloc(request_context, region_name)\n\n    def _get_mrap_netloc(self, request_context):\n        s3_accesspoint = request_context['s3_accesspoint']\n        region_name = 's3-global'\n        mrap_netloc_components = [s3_accesspoint['name']]\n        if self._endpoint_url:\n            endpoint_url_netloc = urlsplit(self._endpoint_url).netloc\n            mrap_netloc_components.append(endpoint_url_netloc)\n        else:\n            partition = s3_accesspoint['partition']\n            mrap_netloc_components.extend(\n                [\n                    'accesspoint',\n                    region_name,\n                    self._get_partition_dns_suffix(partition),\n                ]\n            )\n        return '.'.join(mrap_netloc_components)\n\n    def _get_accesspoint_netloc(self, request_context, region_name):\n        s3_accesspoint = request_context['s3_accesspoint']\n        accesspoint_netloc_components = [\n            '{}-{}'.format(s3_accesspoint['name'], s3_accesspoint['account']),\n        ]\n        outpost_name = s3_accesspoint.get('outpost_name')\n        if self._endpoint_url:\n            if outpost_name:\n                accesspoint_netloc_components.append(outpost_name)\n            endpoint_url_netloc = urlsplit(self._endpoint_url).netloc\n            accesspoint_netloc_components.append(endpoint_url_netloc)\n        else:\n            if outpost_name:\n                outpost_host = [outpost_name, 's3-outposts']\n                accesspoint_netloc_components.extend(outpost_host)\n            elif s3_accesspoint['service'] == 's3-object-lambda':\n                component = self._inject_fips_if_needed(\n                    's3-object-lambda', request_context\n                )\n                accesspoint_netloc_components.append(component)\n            else:\n                component = self._inject_fips_if_needed(\n                    's3-accesspoint', request_context\n                )\n                accesspoint_netloc_components.append(component)\n            if self._s3_config.get('use_dualstack_endpoint'):\n                accesspoint_netloc_components.append('dualstack')\n            accesspoint_netloc_components.extend(\n                [region_name, self._get_dns_suffix(region_name)]\n            )\n        return '.'.join(accesspoint_netloc_components)\n\n    def _inject_fips_if_needed(self, component, request_context):\n        if self._use_fips_endpoint:\n            return '%s-fips' % component\n        return component\n\n    def _get_accesspoint_path(self, original_path, request_context):\n        # The Bucket parameter was substituted with the access-point name as\n        # some value was required in serializing the bucket name. Now that\n        # we are making the request directly to the access point, we will\n        # want to remove that access-point name from the path.\n        name = request_context['s3_accesspoint']['name']\n        # All S3 operations require at least a / in their path.\n        return original_path.replace('/' + name, '', 1) or '/'\n\n    def _get_partition_dns_suffix(self, partition_name):\n        dns_suffix = self._endpoint_resolver.get_partition_dns_suffix(\n            partition_name\n        )\n        if dns_suffix is None:\n            dns_suffix = self._DEFAULT_DNS_SUFFIX\n        return dns_suffix\n\n    def _get_dns_suffix(self, region_name):\n        resolved = self._endpoint_resolver.construct_endpoint(\n            's3', region_name\n        )\n        dns_suffix = self._DEFAULT_DNS_SUFFIX\n        if resolved and 'dnsSuffix' in resolved:\n            dns_suffix = resolved['dnsSuffix']\n        return dns_suffix\n\n    def _override_signing_region(self, request, region_name):\n        signing_context = request.context.get('signing', {})\n        # S3SigV4Auth will use the context['signing']['region'] value to\n        # sign with if present. This is used by the Bucket redirector\n        # as well but we should be fine because the redirector is never\n        # used in combination with the accesspoint setting logic.\n        signing_context['region'] = region_name\n        request.context['signing'] = signing_context\n\n    def _override_signing_name(self, context, signing_name):\n        signing_context = context.get('signing', {})\n        # S3SigV4Auth will use the context['signing']['signing_name'] value to\n        # sign with if present. This is used by the Bucket redirector\n        # as well but we should be fine because the redirector is never\n        # used in combination with the accesspoint setting logic.\n        signing_context['signing_name'] = signing_name\n        context['signing'] = signing_context\n\n    @CachedProperty\n    def _use_accelerate_endpoint(self):\n        # Enable accelerate if the configuration is set to to true or the\n        # endpoint being used matches one of the accelerate endpoints.\n\n        # Accelerate has been explicitly configured.\n        if self._s3_config.get('use_accelerate_endpoint'):\n            return True\n\n        # Accelerate mode is turned on automatically if an endpoint url is\n        # provided that matches the accelerate scheme.\n        if self._endpoint_url is None:\n            return False\n\n        # Accelerate is only valid for Amazon endpoints.\n        netloc = urlsplit(self._endpoint_url).netloc\n        if not netloc.endswith('amazonaws.com'):\n            return False\n\n        # The first part of the url should always be s3-accelerate.\n        parts = netloc.split('.')\n        if parts[0] != 's3-accelerate':\n            return False\n\n        # Url parts between 's3-accelerate' and 'amazonaws.com' which\n        # represent different url features.\n        feature_parts = parts[1:-2]\n\n        # There should be no duplicate url parts.\n        if len(feature_parts) != len(set(feature_parts)):\n            return False\n\n        # Remaining parts must all be in the whitelist.\n        return all(p in S3_ACCELERATE_WHITELIST for p in feature_parts)\n\n    @CachedProperty\n    def _addressing_style(self):\n        # Use virtual host style addressing if accelerate is enabled or if\n        # the given endpoint url is an accelerate endpoint.\n        if self._use_accelerate_endpoint:\n            return 'virtual'\n\n        # If a particular addressing style is configured, use it.\n        configured_addressing_style = self._s3_config.get('addressing_style')\n        if configured_addressing_style:\n            return configured_addressing_style\n\n    @CachedProperty\n    def _s3_addressing_handler(self):\n        # If virtual host style was configured, use it regardless of whether\n        # or not the bucket looks dns compatible.\n        if self._addressing_style == 'virtual':\n            logger.debug(\"Using S3 virtual host style addressing.\")\n            return switch_to_virtual_host_style\n\n        # If path style is configured, no additional steps are needed. If\n        # endpoint_url was specified, don't default to virtual. We could\n        # potentially default provided endpoint urls to virtual hosted\n        # style, but for now it is avoided.\n        if self._addressing_style == 'path' or self._endpoint_url is not None:\n            logger.debug(\"Using S3 path style addressing.\")\n            return None\n\n        logger.debug(\n            \"Defaulting to S3 virtual host style addressing with \"\n            \"path style addressing fallback.\"\n        )\n\n        # By default, try to use virtual style with path fallback.\n        return fix_s3_host\n\n\nclass S3ControlEndpointSetter:\n    _DEFAULT_PARTITION = 'aws'\n    _DEFAULT_DNS_SUFFIX = 'amazonaws.com'\n    _HOST_LABEL_REGEX = re.compile(r'^[a-zA-Z0-9\\-]{1,63}$')\n\n    def __init__(\n        self,\n        endpoint_resolver,\n        region=None,\n        s3_config=None,\n        endpoint_url=None,\n        partition=None,\n        use_fips_endpoint=False,\n    ):\n        self._endpoint_resolver = endpoint_resolver\n        self._region = region\n        self._s3_config = s3_config\n        self._use_fips_endpoint = use_fips_endpoint\n        if s3_config is None:\n            self._s3_config = {}\n        self._endpoint_url = endpoint_url\n        self._partition = partition\n        if partition is None:\n            self._partition = self._DEFAULT_PARTITION\n\n    def register(self, event_emitter):\n        event_emitter.register('before-sign.s3-control', self.set_endpoint)\n\n    def set_endpoint(self, request, **kwargs):\n        if self._use_endpoint_from_arn_details(request):\n            self._validate_endpoint_from_arn_details_supported(request)\n            region_name = self._resolve_region_from_arn_details(request)\n            self._resolve_signing_name_from_arn_details(request)\n            self._resolve_endpoint_from_arn_details(request, region_name)\n            self._add_headers_from_arn_details(request)\n        elif self._use_endpoint_from_outpost_id(request):\n            self._validate_outpost_redirection_valid(request)\n            self._override_signing_name(request, 's3-outposts')\n            new_netloc = self._construct_outpost_endpoint(self._region)\n            self._update_request_netloc(request, new_netloc)\n\n    def _use_endpoint_from_arn_details(self, request):\n        return 'arn_details' in request.context\n\n    def _use_endpoint_from_outpost_id(self, request):\n        return 'outpost_id' in request.context\n\n    def _validate_endpoint_from_arn_details_supported(self, request):\n        if 'fips' in request.context['arn_details']['region']:\n            raise UnsupportedS3ControlArnError(\n                arn=request.context['arn_details']['original'],\n                msg='Invalid ARN, FIPS region not allowed in ARN.',\n            )\n        if not self._s3_config.get('use_arn_region', False):\n            arn_region = request.context['arn_details']['region']\n            if arn_region != self._region:\n                error_msg = (\n                    'The use_arn_region configuration is disabled but '\n                    'received arn for \"%s\" when the client is configured '\n                    'to use \"%s\"'\n                ) % (arn_region, self._region)\n                raise UnsupportedS3ControlConfigurationError(msg=error_msg)\n        request_partion = request.context['arn_details']['partition']\n        if request_partion != self._partition:\n            raise UnsupportedS3ControlConfigurationError(\n                msg=(\n                    'Client is configured for \"%s\" partition, but arn '\n                    'provided is for \"%s\" partition. The client and '\n                    'arn partition must be the same.'\n                    % (self._partition, request_partion)\n                )\n            )\n        if self._s3_config.get('use_accelerate_endpoint'):\n            raise UnsupportedS3ControlConfigurationError(\n                msg='S3 control client does not support accelerate endpoints',\n            )\n        if 'outpost_name' in request.context['arn_details']:\n            self._validate_outpost_redirection_valid(request)\n\n    def _validate_outpost_redirection_valid(self, request):\n        if self._s3_config.get('use_dualstack_endpoint'):\n            raise UnsupportedS3ControlConfigurationError(\n                msg=(\n                    'Client does not support s3 dualstack configuration '\n                    'when an outpost is specified.'\n                )\n            )\n\n    def _resolve_region_from_arn_details(self, request):\n        if self._s3_config.get('use_arn_region', False):\n            arn_region = request.context['arn_details']['region']\n            # If we are using the region from the expanded arn, we will also\n            # want to make sure that we set it as the signing region as well\n            self._override_signing_region(request, arn_region)\n            return arn_region\n        return self._region\n\n    def _resolve_signing_name_from_arn_details(self, request):\n        arn_service = request.context['arn_details']['service']\n        self._override_signing_name(request, arn_service)\n        return arn_service\n\n    def _resolve_endpoint_from_arn_details(self, request, region_name):\n        new_netloc = self._resolve_netloc_from_arn_details(\n            request, region_name\n        )\n        self._update_request_netloc(request, new_netloc)\n\n    def _update_request_netloc(self, request, new_netloc):\n        original_components = urlsplit(request.url)\n        arn_details_endpoint = urlunsplit(\n            (\n                original_components.scheme,\n                new_netloc,\n                original_components.path,\n                original_components.query,\n                '',\n            )\n        )\n        logger.debug(\n            f'Updating URI from {request.url} to {arn_details_endpoint}'\n        )\n        request.url = arn_details_endpoint\n\n    def _resolve_netloc_from_arn_details(self, request, region_name):\n        arn_details = request.context['arn_details']\n        if 'outpost_name' in arn_details:\n            return self._construct_outpost_endpoint(region_name)\n        account = arn_details['account']\n        return self._construct_s3_control_endpoint(region_name, account)\n\n    def _is_valid_host_label(self, label):\n        return self._HOST_LABEL_REGEX.match(label)\n\n    def _validate_host_labels(self, *labels):\n        for label in labels:\n            if not self._is_valid_host_label(label):\n                raise InvalidHostLabelError(label=label)\n\n    def _construct_s3_control_endpoint(self, region_name, account):\n        self._validate_host_labels(region_name, account)\n        if self._endpoint_url:\n            endpoint_url_netloc = urlsplit(self._endpoint_url).netloc\n            netloc = [account, endpoint_url_netloc]\n        else:\n            netloc = [\n                account,\n                's3-control',\n            ]\n            self._add_dualstack(netloc)\n            dns_suffix = self._get_dns_suffix(region_name)\n            netloc.extend([region_name, dns_suffix])\n        return self._construct_netloc(netloc)\n\n    def _construct_outpost_endpoint(self, region_name):\n        self._validate_host_labels(region_name)\n        if self._endpoint_url:\n            return urlsplit(self._endpoint_url).netloc\n        else:\n            netloc = [\n                's3-outposts',\n                region_name,\n                self._get_dns_suffix(region_name),\n            ]\n            self._add_fips(netloc)\n        return self._construct_netloc(netloc)\n\n    def _construct_netloc(self, netloc):\n        return '.'.join(netloc)\n\n    def _add_fips(self, netloc):\n        if self._use_fips_endpoint:\n            netloc[0] = netloc[0] + '-fips'\n\n    def _add_dualstack(self, netloc):\n        if self._s3_config.get('use_dualstack_endpoint'):\n            netloc.append('dualstack')\n\n    def _get_dns_suffix(self, region_name):\n        resolved = self._endpoint_resolver.construct_endpoint(\n            's3', region_name\n        )\n        dns_suffix = self._DEFAULT_DNS_SUFFIX\n        if resolved and 'dnsSuffix' in resolved:\n            dns_suffix = resolved['dnsSuffix']\n        return dns_suffix\n\n    def _override_signing_region(self, request, region_name):\n        signing_context = request.context.get('signing', {})\n        # S3SigV4Auth will use the context['signing']['region'] value to\n        # sign with if present. This is used by the Bucket redirector\n        # as well but we should be fine because the redirector is never\n        # used in combination with the accesspoint setting logic.\n        signing_context['region'] = region_name\n        request.context['signing'] = signing_context\n\n    def _override_signing_name(self, request, signing_name):\n        signing_context = request.context.get('signing', {})\n        # S3SigV4Auth will use the context['signing']['signing_name'] value to\n        # sign with if present. This is used by the Bucket redirector\n        # as well but we should be fine because the redirector is never\n        # used in combination with the accesspoint setting logic.\n        signing_context['signing_name'] = signing_name\n        request.context['signing'] = signing_context\n\n    def _add_headers_from_arn_details(self, request):\n        arn_details = request.context['arn_details']\n        outpost_name = arn_details.get('outpost_name')\n        if outpost_name:\n            self._add_outpost_id_header(request, outpost_name)\n\n    def _add_outpost_id_header(self, request, outpost_name):\n        request.headers['x-amz-outpost-id'] = outpost_name\n\n\nclass S3ControlArnParamHandler:\n    \"\"\"This handler has been replaced by S3ControlArnParamHandlerv2. The\n    original version remains in place for any third-party importers.\n    \"\"\"\n\n    _RESOURCE_SPLIT_REGEX = re.compile(r'[/:]')\n\n    def __init__(self, arn_parser=None):\n        self._arn_parser = arn_parser\n        if arn_parser is None:\n            self._arn_parser = ArnParser()\n        warnings.warn(\n            'The S3ControlArnParamHandler class has been deprecated for a new '\n            'internal replacement. A future version of botocore may remove '\n            'this class.',\n            category=FutureWarning,\n        )\n\n    def register(self, event_emitter):\n        event_emitter.register(\n            'before-parameter-build.s3-control',\n            self.handle_arn,\n        )\n\n    def handle_arn(self, params, model, context, **kwargs):\n        if model.name in ('CreateBucket', 'ListRegionalBuckets'):\n            # CreateBucket and ListRegionalBuckets are special cases that do\n            # not obey ARN based redirection but will redirect based off of the\n            # presence of the OutpostId parameter\n            self._handle_outpost_id_param(params, model, context)\n        else:\n            self._handle_name_param(params, model, context)\n            self._handle_bucket_param(params, model, context)\n\n    def _get_arn_details_from_param(self, params, param_name):\n        if param_name not in params:\n            return None\n        try:\n            arn = params[param_name]\n            arn_details = self._arn_parser.parse_arn(arn)\n            arn_details['original'] = arn\n            arn_details['resources'] = self._split_resource(arn_details)\n            return arn_details\n        except InvalidArnException:\n            return None\n\n    def _split_resource(self, arn_details):\n        return self._RESOURCE_SPLIT_REGEX.split(arn_details['resource'])\n\n    def _override_account_id_param(self, params, arn_details):\n        account_id = arn_details['account']\n        if 'AccountId' in params and params['AccountId'] != account_id:\n            error_msg = (\n                'Account ID in arn does not match the AccountId parameter '\n                'provided: \"%s\"'\n            ) % params['AccountId']\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n        params['AccountId'] = account_id\n\n    def _handle_outpost_id_param(self, params, model, context):\n        if 'OutpostId' not in params:\n            return\n        context['outpost_id'] = params['OutpostId']\n\n    def _handle_name_param(self, params, model, context):\n        # CreateAccessPoint is a special case that does not expand Name\n        if model.name == 'CreateAccessPoint':\n            return\n        arn_details = self._get_arn_details_from_param(params, 'Name')\n        if arn_details is None:\n            return\n        if self._is_outpost_accesspoint(arn_details):\n            self._store_outpost_accesspoint(params, context, arn_details)\n        else:\n            error_msg = 'The Name parameter does not support the provided ARN'\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n\n    def _is_outpost_accesspoint(self, arn_details):\n        if arn_details['service'] != 's3-outposts':\n            return False\n        resources = arn_details['resources']\n        if len(resources) != 4:\n            return False\n        # Resource must be of the form outpost/op-123/accesspoint/name\n        return resources[0] == 'outpost' and resources[2] == 'accesspoint'\n\n    def _store_outpost_accesspoint(self, params, context, arn_details):\n        self._override_account_id_param(params, arn_details)\n        accesspoint_name = arn_details['resources'][3]\n        params['Name'] = accesspoint_name\n        arn_details['accesspoint_name'] = accesspoint_name\n        arn_details['outpost_name'] = arn_details['resources'][1]\n        context['arn_details'] = arn_details\n\n    def _handle_bucket_param(self, params, model, context):\n        arn_details = self._get_arn_details_from_param(params, 'Bucket')\n        if arn_details is None:\n            return\n        if self._is_outpost_bucket(arn_details):\n            self._store_outpost_bucket(params, context, arn_details)\n        else:\n            error_msg = (\n                'The Bucket parameter does not support the provided ARN'\n            )\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n\n    def _is_outpost_bucket(self, arn_details):\n        if arn_details['service'] != 's3-outposts':\n            return False\n        resources = arn_details['resources']\n        if len(resources) != 4:\n            return False\n        # Resource must be of the form outpost/op-123/bucket/name\n        return resources[0] == 'outpost' and resources[2] == 'bucket'\n\n    def _store_outpost_bucket(self, params, context, arn_details):\n        self._override_account_id_param(params, arn_details)\n        bucket_name = arn_details['resources'][3]\n        params['Bucket'] = bucket_name\n        arn_details['bucket_name'] = bucket_name\n        arn_details['outpost_name'] = arn_details['resources'][1]\n        context['arn_details'] = arn_details\n\n\nclass S3ControlArnParamHandlerv2(S3ControlArnParamHandler):\n    \"\"\"Updated version of S3ControlArnParamHandler for use when\n    EndpointRulesetResolver is in use for endpoint resolution.\n\n    This class is considered private and subject to abrupt breaking changes or\n    removal without prior announcement. Please do not use it directly.\n    \"\"\"\n\n    def __init__(self, arn_parser=None):\n        self._arn_parser = arn_parser\n        if arn_parser is None:\n            self._arn_parser = ArnParser()\n\n    def register(self, event_emitter):\n        event_emitter.register(\n            'before-endpoint-resolution.s3-control',\n            self.handle_arn,\n        )\n\n    def _handle_name_param(self, params, model, context):\n        # CreateAccessPoint is a special case that does not expand Name\n        if model.name == 'CreateAccessPoint':\n            return\n        arn_details = self._get_arn_details_from_param(params, 'Name')\n        if arn_details is None:\n            return\n        self._raise_for_fips_pseudo_region(arn_details)\n        self._raise_for_accelerate_endpoint(context)\n        if self._is_outpost_accesspoint(arn_details):\n            self._store_outpost_accesspoint(params, context, arn_details)\n        else:\n            error_msg = 'The Name parameter does not support the provided ARN'\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n\n    def _store_outpost_accesspoint(self, params, context, arn_details):\n        self._override_account_id_param(params, arn_details)\n\n    def _handle_bucket_param(self, params, model, context):\n        arn_details = self._get_arn_details_from_param(params, 'Bucket')\n        if arn_details is None:\n            return\n        self._raise_for_fips_pseudo_region(arn_details)\n        self._raise_for_accelerate_endpoint(context)\n        if self._is_outpost_bucket(arn_details):\n            self._store_outpost_bucket(params, context, arn_details)\n        else:\n            error_msg = (\n                'The Bucket parameter does not support the provided ARN'\n            )\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n\n    def _store_outpost_bucket(self, params, context, arn_details):\n        self._override_account_id_param(params, arn_details)\n\n    def _raise_for_fips_pseudo_region(self, arn_details):\n        # FIPS pseudo region names cannot be used in ARNs\n        arn_region = arn_details['region']\n        if arn_region.startswith('fips-') or arn_region.endswith('fips-'):\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg='Invalid ARN, FIPS region not allowed in ARN.',\n            )\n\n    def _raise_for_accelerate_endpoint(self, context):\n        s3_config = context['client_config'].s3 or {}\n        if s3_config.get('use_accelerate_endpoint'):\n            raise UnsupportedS3ControlConfigurationError(\n                msg='S3 control client does not support accelerate endpoints',\n            )\n\n\nclass ContainerMetadataFetcher:\n    TIMEOUT_SECONDS = 2\n    RETRY_ATTEMPTS = 3\n    SLEEP_TIME = 1\n    IP_ADDRESS = '169.254.170.2'\n    _ALLOWED_HOSTS = [\n        IP_ADDRESS,\n        '169.254.170.23',\n        'fd00:ec2::23',\n        'localhost',\n    ]\n\n    def __init__(self, session=None, sleep=time.sleep):\n        if session is None:\n            session = botocore.httpsession.URLLib3Session(\n                timeout=self.TIMEOUT_SECONDS\n            )\n        self._session = session\n        self._sleep = sleep\n\n    def retrieve_full_uri(self, full_url, headers=None):\n        \"\"\"Retrieve JSON metadata from container metadata.\n\n        :type full_url: str\n        :param full_url: The full URL of the metadata service.\n            This should include the scheme as well, e.g\n            \"http://localhost:123/foo\"\n\n        \"\"\"\n        self._validate_allowed_url(full_url)\n        return self._retrieve_credentials(full_url, headers)\n\n    def _validate_allowed_url(self, full_url):\n        parsed = botocore.compat.urlparse(full_url)\n        if self._is_loopback_address(parsed.hostname):\n            return\n        is_whitelisted_host = self._check_if_whitelisted_host(parsed.hostname)\n        if not is_whitelisted_host:\n            raise ValueError(\n                f\"Unsupported host '{parsed.hostname}'.  Can only retrieve metadata \"\n                f\"from a loopback address or one of these hosts: {', '.join(self._ALLOWED_HOSTS)}\"\n            )\n\n    def _is_loopback_address(self, hostname):\n        try:\n            ip = ip_address(hostname)\n            return ip.is_loopback\n        except ValueError:\n            return False\n\n    def _check_if_whitelisted_host(self, host):\n        if host in self._ALLOWED_HOSTS:\n            return True\n        return False\n\n    def retrieve_uri(self, relative_uri):\n        \"\"\"Retrieve JSON metadata from container metadata.\n\n        :type relative_uri: str\n        :param relative_uri: A relative URI, e.g \"/foo/bar?id=123\"\n\n        :return: The parsed JSON response.\n\n        \"\"\"\n        full_url = self.full_url(relative_uri)\n        return self._retrieve_credentials(full_url)\n\n    def _retrieve_credentials(self, full_url, extra_headers=None):\n        headers = {'Accept': 'application/json'}\n        if extra_headers is not None:\n            headers.update(extra_headers)\n        attempts = 0\n        while True:\n            try:\n                return self._get_response(\n                    full_url, headers, self.TIMEOUT_SECONDS\n                )\n            except MetadataRetrievalError as e:\n                logger.debug(\n                    \"Received error when attempting to retrieve \"\n                    \"container metadata: %s\",\n                    e,\n                    exc_info=True,\n                )\n                self._sleep(self.SLEEP_TIME)\n                attempts += 1\n                if attempts >= self.RETRY_ATTEMPTS:\n                    raise\n\n    def _get_response(self, full_url, headers, timeout):\n        try:\n            AWSRequest = botocore.awsrequest.AWSRequest\n            request = AWSRequest(method='GET', url=full_url, headers=headers)\n            response = self._session.send(request.prepare())\n            response_text = response.content.decode('utf-8')\n            if response.status_code != 200:\n                raise MetadataRetrievalError(\n                    error_msg=(\n                        f\"Received non 200 response {response.status_code} \"\n                        f\"from container metadata: {response_text}\"\n                    )\n                )\n            try:\n                return json.loads(response_text)\n            except ValueError:\n                error_msg = \"Unable to parse JSON returned from container metadata services\"\n                logger.debug('%s:%s', error_msg, response_text)\n                raise MetadataRetrievalError(error_msg=error_msg)\n        except RETRYABLE_HTTP_ERRORS as e:\n            error_msg = (\n                \"Received error when attempting to retrieve \"\n                f\"container metadata: {e}\"\n            )\n            raise MetadataRetrievalError(error_msg=error_msg)\n\n    def full_url(self, relative_uri):\n        return f'http://{self.IP_ADDRESS}{relative_uri}'\n\n\ndef get_environ_proxies(url):\n    if should_bypass_proxies(url):\n        return {}\n    else:\n        return getproxies()\n\n\ndef should_bypass_proxies(url):\n    \"\"\"\n    Returns whether we should bypass proxies or not.\n    \"\"\"\n    # NOTE: requests allowed for ip/cidr entries in no_proxy env that we don't\n    # support current as urllib only checks DNS suffix\n    # If the system proxy settings indicate that this URL should be bypassed,\n    # don't proxy.\n    # The proxy_bypass function is incredibly buggy on OS X in early versions\n    # of Python 2.6, so allow this call to fail. Only catch the specific\n    # exceptions we've seen, though: this call failing in other ways can reveal\n    # legitimate problems.\n    try:\n        if proxy_bypass(urlparse(url).netloc):\n            return True\n    except (TypeError, socket.gaierror):\n        pass\n\n    return False\n\n\ndef determine_content_length(body):\n    # No body, content length of 0\n    if not body:\n        return 0\n\n    # Try asking the body for it's length\n    try:\n        return len(body)\n    except (AttributeError, TypeError):\n        pass\n\n    # Try getting the length from a seekable stream\n    if hasattr(body, 'seek') and hasattr(body, 'tell'):\n        try:\n            orig_pos = body.tell()\n            body.seek(0, 2)\n            end_file_pos = body.tell()\n            body.seek(orig_pos)\n            return end_file_pos - orig_pos\n        except io.UnsupportedOperation:\n            # in case when body is, for example, io.BufferedIOBase object\n            # it has \"seek\" method which throws \"UnsupportedOperation\"\n            # exception in such case we want to fall back to \"chunked\"\n            # encoding\n            pass\n    # Failed to determine the length\n    return None\n\n\ndef get_encoding_from_headers(headers, default='ISO-8859-1'):\n    \"\"\"Returns encodings from given HTTP Header Dict.\n\n    :param headers: dictionary to extract encoding from.\n    :param default: default encoding if the content-type is text\n    \"\"\"\n\n    content_type = headers.get('content-type')\n\n    if not content_type:\n        return None\n\n    message = email.message.Message()\n    message['content-type'] = content_type\n    charset = message.get_param(\"charset\")\n\n    if charset is not None:\n        return charset\n\n    if 'text' in content_type:\n        return default\n\n\ndef calculate_md5(body, **kwargs):\n    if isinstance(body, (bytes, bytearray)):\n        binary_md5 = _calculate_md5_from_bytes(body)\n    else:\n        binary_md5 = _calculate_md5_from_file(body)\n    return base64.b64encode(binary_md5).decode('ascii')\n\n\ndef _calculate_md5_from_bytes(body_bytes):\n    md5 = get_md5(body_bytes)\n    return md5.digest()\n\n\ndef _calculate_md5_from_file(fileobj):\n    start_position = fileobj.tell()\n    md5 = get_md5()\n    for chunk in iter(lambda: fileobj.read(1024 * 1024), b''):\n        md5.update(chunk)\n    fileobj.seek(start_position)\n    return md5.digest()\n\n\ndef _is_s3express_request(params):\n    endpoint_properties = params.get('context', {}).get(\n        'endpoint_properties', {}\n    )\n    return endpoint_properties.get('backend') == 'S3Express'\n\n\ndef _has_checksum_header(params):\n    headers = params['headers']\n    # If a user provided Content-MD5 is present,\n    # don't try to compute a new one.\n    if 'Content-MD5' in headers:\n        return True\n\n    # If a header matching the x-amz-checksum-* pattern is present, we\n    # assume a checksum has already been provided and an md5 is not needed\n    for header in headers:\n        if CHECKSUM_HEADER_PATTERN.match(header):\n            return True\n\n    return False\n\n\ndef conditionally_calculate_checksum(params, **kwargs):\n    if not _has_checksum_header(params):\n        conditionally_calculate_md5(params, **kwargs)\n        conditionally_enable_crc32(params, **kwargs)\n\n\ndef conditionally_enable_crc32(params, **kwargs):\n    checksum_context = params.get('context', {}).get('checksum', {})\n    checksum_algorithm = checksum_context.get('request_algorithm')\n    if (\n        _is_s3express_request(params)\n        and params['body'] is not None\n        and checksum_algorithm in (None, \"conditional-md5\")\n    ):\n        params['context']['checksum'] = {\n            'request_algorithm': {\n                'algorithm': 'crc32',\n                'in': 'header',\n                'name': 'x-amz-checksum-crc32',\n            }\n        }\n\n\ndef conditionally_calculate_md5(params, **kwargs):\n    \"\"\"Only add a Content-MD5 if the system supports it.\"\"\"\n    body = params['body']\n    checksum_context = params.get('context', {}).get('checksum', {})\n    checksum_algorithm = checksum_context.get('request_algorithm')\n    if checksum_algorithm and checksum_algorithm != 'conditional-md5':\n        # Skip for requests that will have a flexible checksum applied\n        return\n\n    if _has_checksum_header(params):\n        # Don't add a new header if one is already available.\n        return\n\n    if _is_s3express_request(params):\n        # S3Express doesn't support MD5\n        return\n\n    if MD5_AVAILABLE and body is not None:\n        md5_digest = calculate_md5(body, **kwargs)\n        params['headers']['Content-MD5'] = md5_digest\n\n\nclass FileWebIdentityTokenLoader:\n    def __init__(self, web_identity_token_path, _open=open):\n        self._web_identity_token_path = web_identity_token_path\n        self._open = _open\n\n    def __call__(self):\n        with self._open(self._web_identity_token_path) as token_file:\n            return token_file.read()\n\n\nclass SSOTokenLoader:\n    def __init__(self, cache=None):\n        if cache is None:\n            cache = {}\n        self._cache = cache\n\n    def _generate_cache_key(self, start_url, session_name):\n        input_str = start_url\n        if session_name is not None:\n            input_str = session_name\n        return hashlib.sha1(input_str.encode('utf-8')).hexdigest()\n\n    def save_token(self, start_url, token, session_name=None):\n        cache_key = self._generate_cache_key(start_url, session_name)\n        self._cache[cache_key] = token\n\n    def __call__(self, start_url, session_name=None):\n        cache_key = self._generate_cache_key(start_url, session_name)\n        logger.debug(f'Checking for cached token at: {cache_key}')\n        if cache_key not in self._cache:\n            name = start_url\n            if session_name is not None:\n                name = session_name\n            error_msg = f'Token for {name} does not exist'\n            raise SSOTokenLoadError(error_msg=error_msg)\n\n        token = self._cache[cache_key]\n        if 'accessToken' not in token or 'expiresAt' not in token:\n            error_msg = f'Token for {start_url} is invalid'\n            raise SSOTokenLoadError(error_msg=error_msg)\n        return token\n\n\nclass EventbridgeSignerSetter:\n    _DEFAULT_PARTITION = 'aws'\n    _DEFAULT_DNS_SUFFIX = 'amazonaws.com'\n\n    def __init__(self, endpoint_resolver, region=None, endpoint_url=None):\n        self._endpoint_resolver = endpoint_resolver\n        self._region = region\n        self._endpoint_url = endpoint_url\n\n    def register(self, event_emitter):\n        event_emitter.register(\n            'before-parameter-build.events.PutEvents',\n            self.check_for_global_endpoint,\n        )\n        event_emitter.register(\n            'before-call.events.PutEvents', self.set_endpoint_url\n        )\n\n    def set_endpoint_url(self, params, context, **kwargs):\n        if 'eventbridge_endpoint' in context:\n            endpoint = context['eventbridge_endpoint']\n            logger.debug(f\"Rewriting URL from {params['url']} to {endpoint}\")\n            params['url'] = endpoint\n\n    def check_for_global_endpoint(self, params, context, **kwargs):\n        endpoint = params.get('EndpointId')\n        if endpoint is None:\n            return\n\n        if len(endpoint) == 0:\n            raise InvalidEndpointConfigurationError(\n                msg='EndpointId must not be a zero length string'\n            )\n\n        if not HAS_CRT:\n            raise MissingDependencyException(\n                msg=\"Using EndpointId requires an additional \"\n                \"dependency. You will need to pip install \"\n                \"botocore[crt] before proceeding.\"\n            )\n\n        config = context.get('client_config')\n        endpoint_variant_tags = None\n        if config is not None:\n            if config.use_fips_endpoint:\n                raise InvalidEndpointConfigurationError(\n                    msg=\"FIPS is not supported with EventBridge \"\n                    \"multi-region endpoints.\"\n                )\n            if config.use_dualstack_endpoint:\n                endpoint_variant_tags = ['dualstack']\n\n        if self._endpoint_url is None:\n            # Validate endpoint is a valid hostname component\n            parts = urlparse(f'https://{endpoint}')\n            if parts.hostname != endpoint:\n                raise InvalidEndpointConfigurationError(\n                    msg='EndpointId is not a valid hostname component.'\n                )\n            resolved_endpoint = self._get_global_endpoint(\n                endpoint, endpoint_variant_tags=endpoint_variant_tags\n            )\n        else:\n            resolved_endpoint = self._endpoint_url\n\n        context['eventbridge_endpoint'] = resolved_endpoint\n        context['auth_type'] = 'v4a'\n\n    def _get_global_endpoint(self, endpoint, endpoint_variant_tags=None):\n        resolver = self._endpoint_resolver\n\n        partition = resolver.get_partition_for_region(self._region)\n        if partition is None:\n            partition = self._DEFAULT_PARTITION\n        dns_suffix = resolver.get_partition_dns_suffix(\n            partition, endpoint_variant_tags=endpoint_variant_tags\n        )\n        if dns_suffix is None:\n            dns_suffix = self._DEFAULT_DNS_SUFFIX\n\n        return f\"https://{endpoint}.endpoint.events.{dns_suffix}/\"\n\n\ndef is_s3_accelerate_url(url):\n    \"\"\"Does the URL match the S3 Accelerate endpoint scheme?\n\n    Virtual host naming style with bucket names in the netloc part of the URL\n    are not allowed by this function.\n    \"\"\"\n    if url is None:\n        return False\n\n    # Accelerate is only valid for Amazon endpoints.\n    url_parts = urlsplit(url)\n    if not url_parts.netloc.endswith(\n        'amazonaws.com'\n    ) or url_parts.scheme not in ['https', 'http']:\n        return False\n\n    # The first part of the URL must be s3-accelerate.\n    parts = url_parts.netloc.split('.')\n    if parts[0] != 's3-accelerate':\n        return False\n\n    # Url parts between 's3-accelerate' and 'amazonaws.com' which\n    # represent different url features.\n    feature_parts = parts[1:-2]\n\n    # There should be no duplicate URL parts.\n    if len(feature_parts) != len(set(feature_parts)):\n        return False\n\n    # Remaining parts must all be in the whitelist.\n    return all(p in S3_ACCELERATE_WHITELIST for p in feature_parts)\n\n\nclass JSONFileCache:\n    \"\"\"JSON file cache.\n    This provides a dict like interface that stores JSON serializable\n    objects.\n    The objects are serialized to JSON and stored in a file.  These\n    values can be retrieved at a later time.\n    \"\"\"\n\n    CACHE_DIR = os.path.expanduser(os.path.join('~', '.aws', 'boto', 'cache'))\n\n    def __init__(self, working_dir=CACHE_DIR, dumps_func=None):\n        self._working_dir = working_dir\n        if dumps_func is None:\n            dumps_func = self._default_dumps\n        self._dumps = dumps_func\n\n    def _default_dumps(self, obj):\n        return json.dumps(obj, default=self._serialize_if_needed)\n\n    def __contains__(self, cache_key):\n        actual_key = self._convert_cache_key(cache_key)\n        return os.path.isfile(actual_key)\n\n    def __getitem__(self, cache_key):\n        \"\"\"Retrieve value from a cache key.\"\"\"\n        actual_key = self._convert_cache_key(cache_key)\n        try:\n            with open(actual_key) as f:\n                return json.load(f)\n        except (OSError, ValueError):\n            raise KeyError(cache_key)\n\n    def __delitem__(self, cache_key):\n        actual_key = self._convert_cache_key(cache_key)\n        try:\n            key_path = Path(actual_key)\n            key_path.unlink()\n        except FileNotFoundError:\n            raise KeyError(cache_key)\n\n    def __setitem__(self, cache_key, value):\n        full_key = self._convert_cache_key(cache_key)\n        try:\n            file_content = self._dumps(value)\n        except (TypeError, ValueError):\n            raise ValueError(\n                f\"Value cannot be cached, must be \"\n                f\"JSON serializable: {value}\"\n            )\n        if not os.path.isdir(self._working_dir):\n            os.makedirs(self._working_dir)\n        with os.fdopen(\n            os.open(full_key, os.O_WRONLY | os.O_CREAT, 0o600), 'w'\n        ) as f:\n            f.truncate()\n            f.write(file_content)\n\n    def _convert_cache_key(self, cache_key):\n        full_path = os.path.join(self._working_dir, cache_key + '.json')\n        return full_path\n\n    def _serialize_if_needed(self, value, iso=False):\n        if isinstance(value, _DatetimeClass):\n            if iso:\n                return value.isoformat()\n            return value.strftime('%Y-%m-%dT%H:%M:%S%Z')\n        return value\n\n\ndef is_s3express_bucket(bucket):\n    if bucket is None:\n        return False\n    return bucket.endswith('--x-s3')\n\n\n# This parameter is not part of the public interface and is subject to abrupt\n# breaking changes or removal without prior announcement.\n# Mapping of services that have been renamed for backwards compatibility reasons.\n# Keys are the previous name that should be allowed, values are the documented\n# and preferred client name.\nSERVICE_NAME_ALIASES = {'runtime.sagemaker': 'sagemaker-runtime'}\n\n\n# This parameter is not part of the public interface and is subject to abrupt\n# breaking changes or removal without prior announcement.\n# Mapping to determine the service ID for services that do not use it as the\n# model data directory name. The keys are the data directory name and the\n# values are the transformed service IDs (lower case and hyphenated).\nCLIENT_NAME_TO_HYPHENIZED_SERVICE_ID_OVERRIDES = {\n    # Actual service name we use -> Allowed computed service name.\n    'apigateway': 'api-gateway',\n    'application-autoscaling': 'application-auto-scaling',\n    'appmesh': 'app-mesh',\n    'autoscaling': 'auto-scaling',\n    'autoscaling-plans': 'auto-scaling-plans',\n    'ce': 'cost-explorer',\n    'cloudhsmv2': 'cloudhsm-v2',\n    'cloudsearchdomain': 'cloudsearch-domain',\n    'cognito-idp': 'cognito-identity-provider',\n    'config': 'config-service',\n    'cur': 'cost-and-usage-report-service',\n    'datapipeline': 'data-pipeline',\n    'directconnect': 'direct-connect',\n    'devicefarm': 'device-farm',\n    'discovery': 'application-discovery-service',\n    'dms': 'database-migration-service',\n    'ds': 'directory-service',\n    'dynamodbstreams': 'dynamodb-streams',\n    'elasticbeanstalk': 'elastic-beanstalk',\n    'elastictranscoder': 'elastic-transcoder',\n    'elb': 'elastic-load-balancing',\n    'elbv2': 'elastic-load-balancing-v2',\n    'es': 'elasticsearch-service',\n    'events': 'eventbridge',\n    'globalaccelerator': 'global-accelerator',\n    'iot-data': 'iot-data-plane',\n    'iot-jobs-data': 'iot-jobs-data-plane',\n    'iot1click-devices': 'iot-1click-devices-service',\n    'iot1click-projects': 'iot-1click-projects',\n    'iotevents-data': 'iot-events-data',\n    'iotevents': 'iot-events',\n    'iotwireless': 'iot-wireless',\n    'kinesisanalytics': 'kinesis-analytics',\n    'kinesisanalyticsv2': 'kinesis-analytics-v2',\n    'kinesisvideo': 'kinesis-video',\n    'lex-models': 'lex-model-building-service',\n    'lexv2-models': 'lex-models-v2',\n    'lex-runtime': 'lex-runtime-service',\n    'lexv2-runtime': 'lex-runtime-v2',\n    'logs': 'cloudwatch-logs',\n    'machinelearning': 'machine-learning',\n    'marketplacecommerceanalytics': 'marketplace-commerce-analytics',\n    'marketplace-entitlement': 'marketplace-entitlement-service',\n    'meteringmarketplace': 'marketplace-metering',\n    'mgh': 'migration-hub',\n    'sms-voice': 'pinpoint-sms-voice',\n    'resourcegroupstaggingapi': 'resource-groups-tagging-api',\n    'route53': 'route-53',\n    'route53domains': 'route-53-domains',\n    's3control': 's3-control',\n    'sdb': 'simpledb',\n    'secretsmanager': 'secrets-manager',\n    'serverlessrepo': 'serverlessapplicationrepository',\n    'servicecatalog': 'service-catalog',\n    'servicecatalog-appregistry': 'service-catalog-appregistry',\n    'stepfunctions': 'sfn',\n    'storagegateway': 'storage-gateway',\n}\n", "botocore/signers.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport datetime\nimport json\nimport weakref\n\nimport botocore\nimport botocore.auth\nfrom botocore.awsrequest import create_request_object, prepare_request_dict\nfrom botocore.compat import OrderedDict\nfrom botocore.exceptions import (\n    UnknownClientMethodError,\n    UnknownSignatureVersionError,\n    UnsupportedSignatureVersionError,\n)\nfrom botocore.utils import ArnParser, datetime2timestamp\n\n# Keep these imported.  There's pre-existing code that uses them.\nfrom botocore.utils import fix_s3_host  # noqa\n\n\nclass RequestSigner:\n    \"\"\"\n    An object to sign requests before they go out over the wire using\n    one of the authentication mechanisms defined in ``auth.py``. This\n    class fires two events scoped to a service and operation name:\n\n    * choose-signer: Allows overriding the auth signer name.\n    * before-sign: Allows mutating the request before signing.\n\n    Together these events allow for customization of the request\n    signing pipeline, including overrides, request path manipulation,\n    and disabling signing per operation.\n\n\n    :type service_id: botocore.model.ServiceId\n    :param service_id: The service id for the service, e.g. ``S3``\n\n    :type region_name: string\n    :param region_name: Name of the service region, e.g. ``us-east-1``\n\n    :type signing_name: string\n    :param signing_name: Service signing name. This is usually the\n                         same as the service name, but can differ. E.g.\n                         ``emr`` vs. ``elasticmapreduce``.\n\n    :type signature_version: string\n    :param signature_version: Signature name like ``v4``.\n\n    :type credentials: :py:class:`~botocore.credentials.Credentials`\n    :param credentials: User credentials with which to sign requests.\n\n    :type event_emitter: :py:class:`~botocore.hooks.BaseEventHooks`\n    :param event_emitter: Extension mechanism to fire events.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_id,\n        region_name,\n        signing_name,\n        signature_version,\n        credentials,\n        event_emitter,\n        auth_token=None,\n    ):\n        self._region_name = region_name\n        self._signing_name = signing_name\n        self._signature_version = signature_version\n        self._credentials = credentials\n        self._auth_token = auth_token\n        self._service_id = service_id\n\n        # We need weakref to prevent leaking memory in Python 2.6 on Linux 2.6\n        self._event_emitter = weakref.proxy(event_emitter)\n\n    @property\n    def region_name(self):\n        return self._region_name\n\n    @property\n    def signature_version(self):\n        return self._signature_version\n\n    @property\n    def signing_name(self):\n        return self._signing_name\n\n    def handler(self, operation_name=None, request=None, **kwargs):\n        # This is typically hooked up to the \"request-created\" event\n        # from a client's event emitter.  When a new request is created\n        # this method is invoked to sign the request.\n        # Don't call this method directly.\n        return self.sign(operation_name, request)\n\n    def sign(\n        self,\n        operation_name,\n        request,\n        region_name=None,\n        signing_type='standard',\n        expires_in=None,\n        signing_name=None,\n    ):\n        \"\"\"Sign a request before it goes out over the wire.\n\n        :type operation_name: string\n        :param operation_name: The name of the current operation, e.g.\n                               ``ListBuckets``.\n        :type request: AWSRequest\n        :param request: The request object to be sent over the wire.\n\n        :type region_name: str\n        :param region_name: The region to sign the request for.\n\n        :type signing_type: str\n        :param signing_type: The type of signing to perform. This can be one of\n            three possible values:\n\n            * 'standard'     - This should be used for most requests.\n            * 'presign-url'  - This should be used when pre-signing a request.\n            * 'presign-post' - This should be used when pre-signing an S3 post.\n\n        :type expires_in: int\n        :param expires_in: The number of seconds the presigned url is valid\n            for. This parameter is only valid for signing type 'presign-url'.\n\n        :type signing_name: str\n        :param signing_name: The name to use for the service when signing.\n        \"\"\"\n        explicit_region_name = region_name\n        if region_name is None:\n            region_name = self._region_name\n\n        if signing_name is None:\n            signing_name = self._signing_name\n\n        signature_version = self._choose_signer(\n            operation_name, signing_type, request.context\n        )\n\n        # Allow mutating request before signing\n        self._event_emitter.emit(\n            'before-sign.{}.{}'.format(\n                self._service_id.hyphenize(), operation_name\n            ),\n            request=request,\n            signing_name=signing_name,\n            region_name=self._region_name,\n            signature_version=signature_version,\n            request_signer=self,\n            operation_name=operation_name,\n        )\n\n        if signature_version != botocore.UNSIGNED:\n            kwargs = {\n                'signing_name': signing_name,\n                'region_name': region_name,\n                'signature_version': signature_version,\n            }\n            if expires_in is not None:\n                kwargs['expires'] = expires_in\n            signing_context = request.context.get('signing', {})\n            if not explicit_region_name and signing_context.get('region'):\n                kwargs['region_name'] = signing_context['region']\n            if signing_context.get('signing_name'):\n                kwargs['signing_name'] = signing_context['signing_name']\n            if signing_context.get('request_credentials'):\n                kwargs['request_credentials'] = signing_context[\n                    'request_credentials'\n                ]\n            if signing_context.get('identity_cache') is not None:\n                self._resolve_identity_cache(\n                    kwargs,\n                    signing_context['identity_cache'],\n                    signing_context['cache_key'],\n                )\n            try:\n                auth = self.get_auth_instance(**kwargs)\n            except UnknownSignatureVersionError as e:\n                if signing_type != 'standard':\n                    raise UnsupportedSignatureVersionError(\n                        signature_version=signature_version\n                    )\n                else:\n                    raise e\n\n            auth.add_auth(request)\n\n    def _resolve_identity_cache(self, kwargs, cache, cache_key):\n        kwargs['identity_cache'] = cache\n        kwargs['cache_key'] = cache_key\n\n    def _choose_signer(self, operation_name, signing_type, context):\n        \"\"\"\n        Allow setting the signature version via the choose-signer event.\n        A value of `botocore.UNSIGNED` means no signing will be performed.\n\n        :param operation_name: The operation to sign.\n        :param signing_type: The type of signing that the signer is to be used\n            for.\n        :return: The signature version to sign with.\n        \"\"\"\n        signing_type_suffix_map = {\n            'presign-post': '-presign-post',\n            'presign-url': '-query',\n        }\n        suffix = signing_type_suffix_map.get(signing_type, '')\n\n        # operation specific signing context takes precedent over client-level\n        # defaults\n        signature_version = context.get('auth_type') or self._signature_version\n        signing = context.get('signing', {})\n        signing_name = signing.get('signing_name', self._signing_name)\n        region_name = signing.get('region', self._region_name)\n        if (\n            signature_version is not botocore.UNSIGNED\n            and not signature_version.endswith(suffix)\n        ):\n            signature_version += suffix\n\n        handler, response = self._event_emitter.emit_until_response(\n            'choose-signer.{}.{}'.format(\n                self._service_id.hyphenize(), operation_name\n            ),\n            signing_name=signing_name,\n            region_name=region_name,\n            signature_version=signature_version,\n            context=context,\n        )\n\n        if response is not None:\n            signature_version = response\n            # The suffix needs to be checked again in case we get an improper\n            # signature version from choose-signer.\n            if (\n                signature_version is not botocore.UNSIGNED\n                and not signature_version.endswith(suffix)\n            ):\n                signature_version += suffix\n\n        return signature_version\n\n    def get_auth_instance(\n        self,\n        signing_name,\n        region_name,\n        signature_version=None,\n        request_credentials=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Get an auth instance which can be used to sign a request\n        using the given signature version.\n\n        :type signing_name: string\n        :param signing_name: Service signing name. This is usually the\n                             same as the service name, but can differ. E.g.\n                             ``emr`` vs. ``elasticmapreduce``.\n\n        :type region_name: string\n        :param region_name: Name of the service region, e.g. ``us-east-1``\n\n        :type signature_version: string\n        :param signature_version: Signature name like ``v4``.\n\n        :rtype: :py:class:`~botocore.auth.BaseSigner`\n        :return: Auth instance to sign a request.\n        \"\"\"\n        if signature_version is None:\n            signature_version = self._signature_version\n\n        cls = botocore.auth.AUTH_TYPE_MAPS.get(signature_version)\n        if cls is None:\n            raise UnknownSignatureVersionError(\n                signature_version=signature_version\n            )\n\n        if cls.REQUIRES_TOKEN is True:\n            frozen_token = None\n            if self._auth_token is not None:\n                frozen_token = self._auth_token.get_frozen_token()\n            auth = cls(frozen_token)\n            return auth\n\n        credentials = request_credentials or self._credentials\n        if getattr(cls, \"REQUIRES_IDENTITY_CACHE\", None) is True:\n            cache = kwargs[\"identity_cache\"]\n            key = kwargs[\"cache_key\"]\n            credentials = cache.get_credentials(key)\n            del kwargs[\"cache_key\"]\n\n        # If there's no credentials provided (i.e credentials is None),\n        # then we'll pass a value of \"None\" over to the auth classes,\n        # which already handle the cases where no credentials have\n        # been provided.\n        frozen_credentials = None\n        if credentials is not None:\n            frozen_credentials = credentials.get_frozen_credentials()\n        kwargs['credentials'] = frozen_credentials\n        if cls.REQUIRES_REGION:\n            if self._region_name is None:\n                raise botocore.exceptions.NoRegionError()\n            kwargs['region_name'] = region_name\n            kwargs['service_name'] = signing_name\n        auth = cls(**kwargs)\n        return auth\n\n    # Alias get_auth for backwards compatibility.\n    get_auth = get_auth_instance\n\n    def generate_presigned_url(\n        self,\n        request_dict,\n        operation_name,\n        expires_in=3600,\n        region_name=None,\n        signing_name=None,\n    ):\n        \"\"\"Generates a presigned url\n\n        :type request_dict: dict\n        :param request_dict: The prepared request dictionary returned by\n            ``botocore.awsrequest.prepare_request_dict()``\n\n        :type operation_name: str\n        :param operation_name: The operation being signed.\n\n        :type expires_in: int\n        :param expires_in: The number of seconds the presigned url is valid\n            for. By default it expires in an hour (3600 seconds)\n\n        :type region_name: string\n        :param region_name: The region name to sign the presigned url.\n\n        :type signing_name: str\n        :param signing_name: The name to use for the service when signing.\n\n        :returns: The presigned url\n        \"\"\"\n        request = create_request_object(request_dict)\n        self.sign(\n            operation_name,\n            request,\n            region_name,\n            'presign-url',\n            expires_in,\n            signing_name,\n        )\n\n        request.prepare()\n        return request.url\n\n\nclass CloudFrontSigner:\n    '''A signer to create a signed CloudFront URL.\n\n    First you create a cloudfront signer based on a normalized RSA signer::\n\n        import rsa\n        def rsa_signer(message):\n            private_key = open('private_key.pem', 'r').read()\n            return rsa.sign(\n                message,\n                rsa.PrivateKey.load_pkcs1(private_key.encode('utf8')),\n                'SHA-1')  # CloudFront requires SHA-1 hash\n        cf_signer = CloudFrontSigner(key_id, rsa_signer)\n\n    To sign with a canned policy::\n\n        signed_url = cf_signer.generate_signed_url(\n            url, date_less_than=datetime(2015, 12, 1))\n\n    To sign with a custom policy::\n\n        signed_url = cf_signer.generate_signed_url(url, policy=my_policy)\n    '''\n\n    def __init__(self, key_id, rsa_signer):\n        \"\"\"Create a CloudFrontSigner.\n\n        :type key_id: str\n        :param key_id: The CloudFront Key Pair ID\n\n        :type rsa_signer: callable\n        :param rsa_signer: An RSA signer.\n               Its only input parameter will be the message to be signed,\n               and its output will be the signed content as a binary string.\n               The hash algorithm needed by CloudFront is SHA-1.\n        \"\"\"\n        self.key_id = key_id\n        self.rsa_signer = rsa_signer\n\n    def generate_presigned_url(self, url, date_less_than=None, policy=None):\n        \"\"\"Creates a signed CloudFront URL based on given parameters.\n\n        :type url: str\n        :param url: The URL of the protected object\n\n        :type date_less_than: datetime\n        :param date_less_than: The URL will expire after that date and time\n\n        :type policy: str\n        :param policy: The custom policy, possibly built by self.build_policy()\n\n        :rtype: str\n        :return: The signed URL.\n        \"\"\"\n        both_args_supplied = date_less_than is not None and policy is not None\n        neither_arg_supplied = date_less_than is None and policy is None\n        if both_args_supplied or neither_arg_supplied:\n            e = 'Need to provide either date_less_than or policy, but not both'\n            raise ValueError(e)\n        if date_less_than is not None:\n            # We still need to build a canned policy for signing purpose\n            policy = self.build_policy(url, date_less_than)\n        if isinstance(policy, str):\n            policy = policy.encode('utf8')\n        if date_less_than is not None:\n            params = ['Expires=%s' % int(datetime2timestamp(date_less_than))]\n        else:\n            params = ['Policy=%s' % self._url_b64encode(policy).decode('utf8')]\n        signature = self.rsa_signer(policy)\n        params.extend(\n            [\n                f\"Signature={self._url_b64encode(signature).decode('utf8')}\",\n                f\"Key-Pair-Id={self.key_id}\",\n            ]\n        )\n        return self._build_url(url, params)\n\n    def _build_url(self, base_url, extra_params):\n        separator = '&' if '?' in base_url else '?'\n        return base_url + separator + '&'.join(extra_params)\n\n    def build_policy(\n        self, resource, date_less_than, date_greater_than=None, ip_address=None\n    ):\n        \"\"\"A helper to build policy.\n\n        :type resource: str\n        :param resource: The URL or the stream filename of the protected object\n\n        :type date_less_than: datetime\n        :param date_less_than: The URL will expire after the time has passed\n\n        :type date_greater_than: datetime\n        :param date_greater_than: The URL will not be valid until this time\n\n        :type ip_address: str\n        :param ip_address: Use 'x.x.x.x' for an IP, or 'x.x.x.x/x' for a subnet\n\n        :rtype: str\n        :return: The policy in a compact string.\n        \"\"\"\n        # Note:\n        # 1. Order in canned policy is significant. Special care has been taken\n        #    to ensure the output will match the order defined by the document.\n        #    There is also a test case to ensure that order.\n        #    SEE: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-creating-signed-url-canned-policy.html#private-content-canned-policy-creating-policy-statement\n        # 2. Albeit the order in custom policy is not required by CloudFront,\n        #    we still use OrderedDict internally to ensure the result is stable\n        #    and also matches canned policy requirement.\n        #    SEE: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-creating-signed-url-custom-policy.html\n        moment = int(datetime2timestamp(date_less_than))\n        condition = OrderedDict({\"DateLessThan\": {\"AWS:EpochTime\": moment}})\n        if ip_address:\n            if '/' not in ip_address:\n                ip_address += '/32'\n            condition[\"IpAddress\"] = {\"AWS:SourceIp\": ip_address}\n        if date_greater_than:\n            moment = int(datetime2timestamp(date_greater_than))\n            condition[\"DateGreaterThan\"] = {\"AWS:EpochTime\": moment}\n        ordered_payload = [('Resource', resource), ('Condition', condition)]\n        custom_policy = {\"Statement\": [OrderedDict(ordered_payload)]}\n        return json.dumps(custom_policy, separators=(',', ':'))\n\n    def _url_b64encode(self, data):\n        # Required by CloudFront. See also:\n        # http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-linux-openssl.html\n        return (\n            base64.b64encode(data)\n            .replace(b'+', b'-')\n            .replace(b'=', b'_')\n            .replace(b'/', b'~')\n        )\n\n\ndef add_generate_db_auth_token(class_attributes, **kwargs):\n    class_attributes['generate_db_auth_token'] = generate_db_auth_token\n\n\ndef generate_db_auth_token(self, DBHostname, Port, DBUsername, Region=None):\n    \"\"\"Generates an auth token used to connect to a db with IAM credentials.\n\n    :type DBHostname: str\n    :param DBHostname: The hostname of the database to connect to.\n\n    :type Port: int\n    :param Port: The port number the database is listening on.\n\n    :type DBUsername: str\n    :param DBUsername: The username to log in as.\n\n    :type Region: str\n    :param Region: The region the database is in. If None, the client\n        region will be used.\n\n    :return: A presigned url which can be used as an auth token.\n    \"\"\"\n    region = Region\n    if region is None:\n        region = self.meta.region_name\n\n    params = {\n        'Action': 'connect',\n        'DBUser': DBUsername,\n    }\n\n    request_dict = {\n        'url_path': '/',\n        'query_string': '',\n        'headers': {},\n        'body': params,\n        'method': 'GET',\n    }\n\n    # RDS requires that the scheme not be set when sent over. This can cause\n    # issues when signing because the Python url parsing libraries follow\n    # RFC 1808 closely, which states that a netloc must be introduced by `//`.\n    # Otherwise the url is presumed to be relative, and thus the whole\n    # netloc would be treated as a path component. To work around this we\n    # introduce https here and remove it once we're done processing it.\n    scheme = 'https://'\n    endpoint_url = f'{scheme}{DBHostname}:{Port}'\n    prepare_request_dict(request_dict, endpoint_url)\n    presigned_url = self._request_signer.generate_presigned_url(\n        operation_name='connect',\n        request_dict=request_dict,\n        region_name=region,\n        expires_in=900,\n        signing_name='rds-db',\n    )\n    return presigned_url[len(scheme) :]\n\n\nclass S3PostPresigner:\n    def __init__(self, request_signer):\n        self._request_signer = request_signer\n\n    def generate_presigned_post(\n        self,\n        request_dict,\n        fields=None,\n        conditions=None,\n        expires_in=3600,\n        region_name=None,\n    ):\n        \"\"\"Generates the url and the form fields used for a presigned s3 post\n\n        :type request_dict: dict\n        :param request_dict: The prepared request dictionary returned by\n            ``botocore.awsrequest.prepare_request_dict()``\n\n        :type fields: dict\n        :param fields: A dictionary of prefilled form fields to build on top\n            of.\n\n        :type conditions: list\n        :param conditions: A list of conditions to include in the policy. Each\n            element can be either a list or a structure. For example:\n            [\n             {\"acl\": \"public-read\"},\n             {\"bucket\": \"mybucket\"},\n             [\"starts-with\", \"$key\", \"mykey\"]\n            ]\n\n        :type expires_in: int\n        :param expires_in: The number of seconds the presigned post is valid\n            for.\n\n        :type region_name: string\n        :param region_name: The region name to sign the presigned post to.\n\n        :rtype: dict\n        :returns: A dictionary with two elements: ``url`` and ``fields``.\n            Url is the url to post to. Fields is a dictionary filled with\n            the form fields and respective values to use when submitting the\n            post. For example:\n\n            {'url': 'https://mybucket.s3.amazonaws.com\n             'fields': {'acl': 'public-read',\n                        'key': 'mykey',\n                        'signature': 'mysignature',\n                        'policy': 'mybase64 encoded policy'}\n            }\n        \"\"\"\n        if fields is None:\n            fields = {}\n\n        if conditions is None:\n            conditions = []\n\n        # Create the policy for the post.\n        policy = {}\n\n        # Create an expiration date for the policy\n        datetime_now = datetime.datetime.utcnow()\n        expire_date = datetime_now + datetime.timedelta(seconds=expires_in)\n        policy['expiration'] = expire_date.strftime(botocore.auth.ISO8601)\n\n        # Append all of the conditions that the user supplied.\n        policy['conditions'] = []\n        for condition in conditions:\n            policy['conditions'].append(condition)\n\n        # Store the policy and the fields in the request for signing\n        request = create_request_object(request_dict)\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n        self._request_signer.sign(\n            'PutObject', request, region_name, 'presign-post'\n        )\n        # Return the url and the fields for th form to post.\n        return {'url': request.url, 'fields': fields}\n\n\ndef add_generate_presigned_url(class_attributes, **kwargs):\n    class_attributes['generate_presigned_url'] = generate_presigned_url\n\n\ndef generate_presigned_url(\n    self, ClientMethod, Params=None, ExpiresIn=3600, HttpMethod=None\n):\n    \"\"\"Generate a presigned url given a client, its method, and arguments\n\n    :type ClientMethod: string\n    :param ClientMethod: The client method to presign for\n\n    :type Params: dict\n    :param Params: The parameters normally passed to\n        ``ClientMethod``.\n\n    :type ExpiresIn: int\n    :param ExpiresIn: The number of seconds the presigned url is valid\n        for. By default it expires in an hour (3600 seconds)\n\n    :type HttpMethod: string\n    :param HttpMethod: The http method to use on the generated url. By\n        default, the http method is whatever is used in the method's model.\n\n    :returns: The presigned url\n    \"\"\"\n    client_method = ClientMethod\n    params = Params\n    if params is None:\n        params = {}\n    expires_in = ExpiresIn\n    http_method = HttpMethod\n    context = {\n        'is_presign_request': True,\n        'use_global_endpoint': _should_use_global_endpoint(self),\n    }\n\n    request_signer = self._request_signer\n\n    try:\n        operation_name = self._PY_TO_OP_NAME[client_method]\n    except KeyError:\n        raise UnknownClientMethodError(method_name=client_method)\n\n    operation_model = self.meta.service_model.operation_model(operation_name)\n    params = self._emit_api_params(\n        api_params=params,\n        operation_model=operation_model,\n        context=context,\n    )\n    bucket_is_arn = ArnParser.is_arn(params.get('Bucket', ''))\n    (\n        endpoint_url,\n        additional_headers,\n        properties,\n    ) = self._resolve_endpoint_ruleset(\n        operation_model,\n        params,\n        context,\n        ignore_signing_region=(not bucket_is_arn),\n    )\n\n    request_dict = self._convert_to_request_dict(\n        api_params=params,\n        operation_model=operation_model,\n        endpoint_url=endpoint_url,\n        context=context,\n        headers=additional_headers,\n        set_user_agent_header=False,\n    )\n\n    # Switch out the http method if user specified it.\n    if http_method is not None:\n        request_dict['method'] = http_method\n\n    # Generate the presigned url.\n    return request_signer.generate_presigned_url(\n        request_dict=request_dict,\n        expires_in=expires_in,\n        operation_name=operation_name,\n    )\n\n\ndef add_generate_presigned_post(class_attributes, **kwargs):\n    class_attributes['generate_presigned_post'] = generate_presigned_post\n\n\ndef generate_presigned_post(\n    self, Bucket, Key, Fields=None, Conditions=None, ExpiresIn=3600\n):\n    \"\"\"Builds the url and the form fields used for a presigned s3 post\n\n    :type Bucket: string\n    :param Bucket: The name of the bucket to presign the post to. Note that\n        bucket related conditions should not be included in the\n        ``conditions`` parameter.\n\n    :type Key: string\n    :param Key: Key name, optionally add ${filename} to the end to\n        attach the submitted filename. Note that key related conditions and\n        fields are filled out for you and should not be included in the\n        ``Fields`` or ``Conditions`` parameter.\n\n    :type Fields: dict\n    :param Fields: A dictionary of prefilled form fields to build on top\n        of. Elements that may be included are acl, Cache-Control,\n        Content-Type, Content-Disposition, Content-Encoding, Expires,\n        success_action_redirect, redirect, success_action_status,\n        and x-amz-meta-.\n\n        Note that if a particular element is included in the fields\n        dictionary it will not be automatically added to the conditions\n        list. You must specify a condition for the element as well.\n\n    :type Conditions: list\n    :param Conditions: A list of conditions to include in the policy. Each\n        element can be either a list or a structure. For example:\n\n        [\n         {\"acl\": \"public-read\"},\n         [\"content-length-range\", 2, 5],\n         [\"starts-with\", \"$success_action_redirect\", \"\"]\n        ]\n\n        Conditions that are included may pertain to acl,\n        content-length-range, Cache-Control, Content-Type,\n        Content-Disposition, Content-Encoding, Expires,\n        success_action_redirect, redirect, success_action_status,\n        and/or x-amz-meta-.\n\n        Note that if you include a condition, you must specify\n        the a valid value in the fields dictionary as well. A value will\n        not be added automatically to the fields dictionary based on the\n        conditions.\n\n    :type ExpiresIn: int\n    :param ExpiresIn: The number of seconds the presigned post\n        is valid for.\n\n    :rtype: dict\n    :returns: A dictionary with two elements: ``url`` and ``fields``.\n        Url is the url to post to. Fields is a dictionary filled with\n        the form fields and respective values to use when submitting the\n        post. For example:\n\n        {'url': 'https://mybucket.s3.amazonaws.com\n         'fields': {'acl': 'public-read',\n                    'key': 'mykey',\n                    'signature': 'mysignature',\n                    'policy': 'mybase64 encoded policy'}\n        }\n    \"\"\"\n    bucket = Bucket\n    key = Key\n    fields = Fields\n    conditions = Conditions\n    expires_in = ExpiresIn\n\n    if fields is None:\n        fields = {}\n    else:\n        fields = fields.copy()\n\n    if conditions is None:\n        conditions = []\n\n    context = {\n        'is_presign_request': True,\n        'use_global_endpoint': _should_use_global_endpoint(self),\n    }\n\n    post_presigner = S3PostPresigner(self._request_signer)\n\n    # We choose the CreateBucket operation model because its url gets\n    # serialized to what a presign post requires.\n    operation_model = self.meta.service_model.operation_model('CreateBucket')\n    params = self._emit_api_params(\n        api_params={'Bucket': bucket},\n        operation_model=operation_model,\n        context=context,\n    )\n    bucket_is_arn = ArnParser.is_arn(params.get('Bucket', ''))\n    (\n        endpoint_url,\n        additional_headers,\n        properties,\n    ) = self._resolve_endpoint_ruleset(\n        operation_model,\n        params,\n        context,\n        ignore_signing_region=(not bucket_is_arn),\n    )\n\n    request_dict = self._convert_to_request_dict(\n        api_params=params,\n        operation_model=operation_model,\n        endpoint_url=endpoint_url,\n        context=context,\n        headers=additional_headers,\n        set_user_agent_header=False,\n    )\n\n    # Append that the bucket name to the list of conditions.\n    conditions.append({'bucket': bucket})\n\n    # If the key ends with filename, the only constraint that can be\n    # imposed is if it starts with the specified prefix.\n    if key.endswith('${filename}'):\n        conditions.append([\"starts-with\", '$key', key[: -len('${filename}')]])\n    else:\n        conditions.append({'key': key})\n\n    # Add the key to the fields.\n    fields['key'] = key\n\n    return post_presigner.generate_presigned_post(\n        request_dict=request_dict,\n        fields=fields,\n        conditions=conditions,\n        expires_in=expires_in,\n    )\n\n\ndef _should_use_global_endpoint(client):\n    if client.meta.partition != 'aws':\n        return False\n    s3_config = client.meta.config.s3\n    if s3_config:\n        if s3_config.get('use_dualstack_endpoint', False):\n            return False\n        if (\n            s3_config.get('us_east_1_regional_endpoint') == 'regional'\n            and client.meta.config.region_name == 'us-east-1'\n        ):\n            return False\n        if s3_config.get('addressing_style') == 'virtual':\n            return False\n    return True\n", "botocore/httpsession.py": "import logging\nimport os\nimport os.path\nimport socket\nimport sys\nimport warnings\nfrom base64 import b64encode\n\nfrom urllib3 import PoolManager, Timeout, proxy_from_url\nfrom urllib3.exceptions import (\n    ConnectTimeoutError as URLLib3ConnectTimeoutError,\n)\nfrom urllib3.exceptions import (\n    LocationParseError,\n    NewConnectionError,\n    ProtocolError,\n    ProxyError,\n)\nfrom urllib3.exceptions import ReadTimeoutError as URLLib3ReadTimeoutError\nfrom urllib3.exceptions import SSLError as URLLib3SSLError\nfrom urllib3.util.retry import Retry\nfrom urllib3.util.ssl_ import (\n    OP_NO_COMPRESSION,\n    PROTOCOL_TLS,\n    OP_NO_SSLv2,\n    OP_NO_SSLv3,\n    is_ipaddress,\n    ssl,\n)\nfrom urllib3.util.url import parse_url\n\ntry:\n    from urllib3.util.ssl_ import OP_NO_TICKET, PROTOCOL_TLS_CLIENT\nexcept ImportError:\n    # Fallback directly to ssl for version of urllib3 before 1.26.\n    # They are available in the standard library starting in Python 3.6.\n    from ssl import OP_NO_TICKET, PROTOCOL_TLS_CLIENT\n\ntry:\n    # pyopenssl will be removed in urllib3 2.0, we'll fall back to ssl_ at that point.\n    # This can be removed once our urllib3 floor is raised to >= 2.0.\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n        # Always import the original SSLContext, even if it has been patched\n        from urllib3.contrib.pyopenssl import (\n            orig_util_SSLContext as SSLContext,\n        )\nexcept ImportError:\n    from urllib3.util.ssl_ import SSLContext\n\ntry:\n    from urllib3.util.ssl_ import DEFAULT_CIPHERS\nexcept ImportError:\n    # Defer to system configuration starting with\n    # urllib3 2.0. This will choose the ciphers provided by\n    # Openssl 1.1.1+ or secure system defaults.\n    DEFAULT_CIPHERS = None\n\nimport botocore.awsrequest\nfrom botocore.compat import (\n    IPV6_ADDRZ_RE,\n    ensure_bytes,\n    filter_ssl_warnings,\n    unquote,\n    urlparse,\n)\nfrom botocore.exceptions import (\n    ConnectionClosedError,\n    ConnectTimeoutError,\n    EndpointConnectionError,\n    HTTPClientError,\n    InvalidProxiesConfigError,\n    ProxyConnectionError,\n    ReadTimeoutError,\n    SSLError,\n)\n\nfilter_ssl_warnings()\nlogger = logging.getLogger(__name__)\nDEFAULT_TIMEOUT = 60\nMAX_POOL_CONNECTIONS = 10\nDEFAULT_CA_BUNDLE = os.path.join(os.path.dirname(__file__), 'cacert.pem')\n\ntry:\n    from certifi import where\nexcept ImportError:\n\n    def where():\n        return DEFAULT_CA_BUNDLE\n\n\ndef get_cert_path(verify):\n    if verify is not True:\n        return verify\n\n    cert_path = where()\n    logger.debug(f\"Certificate path: {cert_path}\")\n\n    return cert_path\n\n\ndef create_urllib3_context(\n    ssl_version=None, cert_reqs=None, options=None, ciphers=None\n):\n    \"\"\"This function is a vendored version of the same function in urllib3\n\n    We vendor this function to ensure that the SSL contexts we construct\n    always use the std lib SSLContext instead of pyopenssl.\n    \"\"\"\n    # PROTOCOL_TLS is deprecated in Python 3.10\n    if not ssl_version or ssl_version == PROTOCOL_TLS:\n        ssl_version = PROTOCOL_TLS_CLIENT\n\n    context = SSLContext(ssl_version)\n\n    if ciphers:\n        context.set_ciphers(ciphers)\n    elif DEFAULT_CIPHERS:\n        context.set_ciphers(DEFAULT_CIPHERS)\n\n    # Setting the default here, as we may have no ssl module on import\n    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs\n\n    if options is None:\n        options = 0\n        # SSLv2 is easily broken and is considered harmful and dangerous\n        options |= OP_NO_SSLv2\n        # SSLv3 has several problems and is now dangerous\n        options |= OP_NO_SSLv3\n        # Disable compression to prevent CRIME attacks for OpenSSL 1.0+\n        # (issue urllib3#309)\n        options |= OP_NO_COMPRESSION\n        # TLSv1.2 only. Unless set explicitly, do not request tickets.\n        # This may save some bandwidth on wire, and although the ticket is encrypted,\n        # there is a risk associated with it being on wire,\n        # if the server is not rotating its ticketing keys properly.\n        options |= OP_NO_TICKET\n\n    context.options |= options\n\n    # Enable post-handshake authentication for TLS 1.3, see GH #1634. PHA is\n    # necessary for conditional client cert authentication with TLS 1.3.\n    # The attribute is None for OpenSSL <= 1.1.0 or does not exist in older\n    # versions of Python.  We only enable on Python 3.7.4+ or if certificate\n    # verification is enabled to work around Python issue #37428\n    # See: https://bugs.python.org/issue37428\n    if (\n        cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)\n    ) and getattr(context, \"post_handshake_auth\", None) is not None:\n        context.post_handshake_auth = True\n\n    def disable_check_hostname():\n        if (\n            getattr(context, \"check_hostname\", None) is not None\n        ):  # Platform-specific: Python 3.2\n            # We do our own verification, including fingerprints and alternative\n            # hostnames. So disable it here\n            context.check_hostname = False\n\n    # The order of the below lines setting verify_mode and check_hostname\n    # matter due to safe-guards SSLContext has to prevent an SSLContext with\n    # check_hostname=True, verify_mode=NONE/OPTIONAL. This is made even more\n    # complex because we don't know whether PROTOCOL_TLS_CLIENT will be used\n    # or not so we don't know the initial state of the freshly created SSLContext.\n    if cert_reqs == ssl.CERT_REQUIRED:\n        context.verify_mode = cert_reqs\n        disable_check_hostname()\n    else:\n        disable_check_hostname()\n        context.verify_mode = cert_reqs\n\n    # Enable logging of TLS session keys via defacto standard environment variable\n    # 'SSLKEYLOGFILE', if the feature is available (Python 3.8+). Skip empty values.\n    if hasattr(context, \"keylog_filename\"):\n        sslkeylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n        if sslkeylogfile and not sys.flags.ignore_environment:\n            context.keylog_filename = sslkeylogfile\n\n    return context\n\n\ndef ensure_boolean(val):\n    \"\"\"Ensures a boolean value if a string or boolean is provided\n\n    For strings, the value for True/False is case insensitive\n    \"\"\"\n    if isinstance(val, bool):\n        return val\n    else:\n        return val.lower() == 'true'\n\n\ndef mask_proxy_url(proxy_url):\n    \"\"\"\n    Mask proxy url credentials.\n\n    :type proxy_url: str\n    :param proxy_url: The proxy url, i.e. https://username:password@proxy.com\n\n    :return: Masked proxy url, i.e. https://***:***@proxy.com\n    \"\"\"\n    mask = '*' * 3\n    parsed_url = urlparse(proxy_url)\n    if parsed_url.username:\n        proxy_url = proxy_url.replace(parsed_url.username, mask, 1)\n    if parsed_url.password:\n        proxy_url = proxy_url.replace(parsed_url.password, mask, 1)\n    return proxy_url\n\n\ndef _is_ipaddress(host):\n    \"\"\"Wrap urllib3's is_ipaddress to support bracketed IPv6 addresses.\"\"\"\n    return is_ipaddress(host) or bool(IPV6_ADDRZ_RE.match(host))\n\n\nclass ProxyConfiguration:\n    \"\"\"Represents a proxy configuration dictionary and additional settings.\n\n    This class represents a proxy configuration dictionary and provides utility\n    functions to retrieve well structured proxy urls and proxy headers from the\n    proxy configuration dictionary.\n    \"\"\"\n\n    def __init__(self, proxies=None, proxies_settings=None):\n        if proxies is None:\n            proxies = {}\n        if proxies_settings is None:\n            proxies_settings = {}\n\n        self._proxies = proxies\n        self._proxies_settings = proxies_settings\n\n    def proxy_url_for(self, url):\n        \"\"\"Retrieves the corresponding proxy url for a given url.\"\"\"\n        parsed_url = urlparse(url)\n        proxy = self._proxies.get(parsed_url.scheme)\n        if proxy:\n            proxy = self._fix_proxy_url(proxy)\n        return proxy\n\n    def proxy_headers_for(self, proxy_url):\n        \"\"\"Retrieves the corresponding proxy headers for a given proxy url.\"\"\"\n        headers = {}\n        username, password = self._get_auth_from_url(proxy_url)\n        if username and password:\n            basic_auth = self._construct_basic_auth(username, password)\n            headers['Proxy-Authorization'] = basic_auth\n        return headers\n\n    @property\n    def settings(self):\n        return self._proxies_settings\n\n    def _fix_proxy_url(self, proxy_url):\n        if proxy_url.startswith('http:') or proxy_url.startswith('https:'):\n            return proxy_url\n        elif proxy_url.startswith('//'):\n            return 'http:' + proxy_url\n        else:\n            return 'http://' + proxy_url\n\n    def _construct_basic_auth(self, username, password):\n        auth_str = f'{username}:{password}'\n        encoded_str = b64encode(auth_str.encode('ascii')).strip().decode()\n        return f'Basic {encoded_str}'\n\n    def _get_auth_from_url(self, url):\n        parsed_url = urlparse(url)\n        try:\n            return unquote(parsed_url.username), unquote(parsed_url.password)\n        except (AttributeError, TypeError):\n            return None, None\n\n\nclass URLLib3Session:\n    \"\"\"A basic HTTP client that supports connection pooling and proxies.\n\n    This class is inspired by requests.adapters.HTTPAdapter, but has been\n    boiled down to meet the use cases needed by botocore. For the most part\n    this classes matches the functionality of HTTPAdapter in requests v2.7.0\n    (the same as our vendored version). The only major difference of note is\n    that we currently do not support sending chunked requests. While requests\n    v2.7.0 implemented this themselves, later version urllib3 support this\n    directly via a flag to urlopen so enabling it if needed should be trivial.\n    \"\"\"\n\n    def __init__(\n        self,\n        verify=True,\n        proxies=None,\n        timeout=None,\n        max_pool_connections=MAX_POOL_CONNECTIONS,\n        socket_options=None,\n        client_cert=None,\n        proxies_config=None,\n    ):\n        self._verify = verify\n        self._proxy_config = ProxyConfiguration(\n            proxies=proxies, proxies_settings=proxies_config\n        )\n        self._pool_classes_by_scheme = {\n            'http': botocore.awsrequest.AWSHTTPConnectionPool,\n            'https': botocore.awsrequest.AWSHTTPSConnectionPool,\n        }\n        if timeout is None:\n            timeout = DEFAULT_TIMEOUT\n        if not isinstance(timeout, (int, float)):\n            timeout = Timeout(connect=timeout[0], read=timeout[1])\n\n        self._cert_file = None\n        self._key_file = None\n        if isinstance(client_cert, str):\n            self._cert_file = client_cert\n        elif isinstance(client_cert, tuple):\n            self._cert_file, self._key_file = client_cert\n\n        self._timeout = timeout\n        self._max_pool_connections = max_pool_connections\n        self._socket_options = socket_options\n        if socket_options is None:\n            self._socket_options = []\n        self._proxy_managers = {}\n        self._manager = PoolManager(**self._get_pool_manager_kwargs())\n        self._manager.pool_classes_by_scheme = self._pool_classes_by_scheme\n\n    def _proxies_kwargs(self, **kwargs):\n        proxies_settings = self._proxy_config.settings\n        proxies_kwargs = {\n            'use_forwarding_for_https': proxies_settings.get(\n                'proxy_use_forwarding_for_https'\n            ),\n            **kwargs,\n        }\n        return {k: v for k, v in proxies_kwargs.items() if v is not None}\n\n    def _get_pool_manager_kwargs(self, **extra_kwargs):\n        pool_manager_kwargs = {\n            'timeout': self._timeout,\n            'maxsize': self._max_pool_connections,\n            'ssl_context': self._get_ssl_context(),\n            'socket_options': self._socket_options,\n            'cert_file': self._cert_file,\n            'key_file': self._key_file,\n        }\n        pool_manager_kwargs.update(**extra_kwargs)\n        return pool_manager_kwargs\n\n    def _get_ssl_context(self):\n        return create_urllib3_context()\n\n    def _get_proxy_manager(self, proxy_url):\n        if proxy_url not in self._proxy_managers:\n            proxy_headers = self._proxy_config.proxy_headers_for(proxy_url)\n            proxy_ssl_context = self._setup_proxy_ssl_context(proxy_url)\n            proxy_manager_kwargs = self._get_pool_manager_kwargs(\n                proxy_headers=proxy_headers\n            )\n            proxy_manager_kwargs.update(\n                self._proxies_kwargs(proxy_ssl_context=proxy_ssl_context)\n            )\n            proxy_manager = proxy_from_url(proxy_url, **proxy_manager_kwargs)\n            proxy_manager.pool_classes_by_scheme = self._pool_classes_by_scheme\n            self._proxy_managers[proxy_url] = proxy_manager\n\n        return self._proxy_managers[proxy_url]\n\n    def _path_url(self, url):\n        parsed_url = urlparse(url)\n        path = parsed_url.path\n        if not path:\n            path = '/'\n        if parsed_url.query:\n            path = path + '?' + parsed_url.query\n        return path\n\n    def _setup_ssl_cert(self, conn, url, verify):\n        if url.lower().startswith('https') and verify:\n            conn.cert_reqs = 'CERT_REQUIRED'\n            conn.ca_certs = get_cert_path(verify)\n        else:\n            conn.cert_reqs = 'CERT_NONE'\n            conn.ca_certs = None\n\n    def _setup_proxy_ssl_context(self, proxy_url):\n        proxies_settings = self._proxy_config.settings\n        proxy_ca_bundle = proxies_settings.get('proxy_ca_bundle')\n        proxy_cert = proxies_settings.get('proxy_client_cert')\n        if proxy_ca_bundle is None and proxy_cert is None:\n            return None\n\n        context = self._get_ssl_context()\n        try:\n            url = parse_url(proxy_url)\n            # urllib3 disables this by default but we need it for proper\n            # proxy tls negotiation when proxy_url is not an IP Address\n            if not _is_ipaddress(url.host):\n                context.check_hostname = True\n            if proxy_ca_bundle is not None:\n                context.load_verify_locations(cafile=proxy_ca_bundle)\n\n            if isinstance(proxy_cert, tuple):\n                context.load_cert_chain(proxy_cert[0], keyfile=proxy_cert[1])\n            elif isinstance(proxy_cert, str):\n                context.load_cert_chain(proxy_cert)\n\n            return context\n        except (OSError, URLLib3SSLError, LocationParseError) as e:\n            raise InvalidProxiesConfigError(error=e)\n\n    def _get_connection_manager(self, url, proxy_url=None):\n        if proxy_url:\n            manager = self._get_proxy_manager(proxy_url)\n        else:\n            manager = self._manager\n        return manager\n\n    def _get_request_target(self, url, proxy_url):\n        has_proxy = proxy_url is not None\n\n        if not has_proxy:\n            return self._path_url(url)\n\n        # HTTP proxies expect the request_target to be the absolute url to know\n        # which host to establish a connection to. urllib3 also supports\n        # forwarding for HTTPS through the 'use_forwarding_for_https' parameter.\n        proxy_scheme = urlparse(proxy_url).scheme\n        using_https_forwarding_proxy = (\n            proxy_scheme == 'https'\n            and self._proxies_kwargs().get('use_forwarding_for_https', False)\n        )\n\n        if using_https_forwarding_proxy or url.startswith('http:'):\n            return url\n        else:\n            return self._path_url(url)\n\n    def _chunked(self, headers):\n        transfer_encoding = headers.get('Transfer-Encoding', b'')\n        transfer_encoding = ensure_bytes(transfer_encoding)\n        return transfer_encoding.lower() == b'chunked'\n\n    def close(self):\n        self._manager.clear()\n        for manager in self._proxy_managers.values():\n            manager.clear()\n\n    def send(self, request):\n        try:\n            proxy_url = self._proxy_config.proxy_url_for(request.url)\n            manager = self._get_connection_manager(request.url, proxy_url)\n            conn = manager.connection_from_url(request.url)\n            self._setup_ssl_cert(conn, request.url, self._verify)\n            if ensure_boolean(\n                os.environ.get('BOTO_EXPERIMENTAL__ADD_PROXY_HOST_HEADER', '')\n            ):\n                # This is currently an \"experimental\" feature which provides\n                # no guarantees of backwards compatibility. It may be subject\n                # to change or removal in any patch version. Anyone opting in\n                # to this feature should strictly pin botocore.\n                host = urlparse(request.url).hostname\n                conn.proxy_headers['host'] = host\n\n            request_target = self._get_request_target(request.url, proxy_url)\n            urllib_response = conn.urlopen(\n                method=request.method,\n                url=request_target,\n                body=request.body,\n                headers=request.headers,\n                retries=Retry(False),\n                assert_same_host=False,\n                preload_content=False,\n                decode_content=False,\n                chunked=self._chunked(request.headers),\n            )\n\n            http_response = botocore.awsrequest.AWSResponse(\n                request.url,\n                urllib_response.status,\n                urllib_response.headers,\n                urllib_response,\n            )\n\n            if not request.stream_output:\n                # Cause the raw stream to be exhausted immediately. We do it\n                # this way instead of using preload_content because\n                # preload_content will never buffer chunked responses\n                http_response.content\n\n            return http_response\n        except URLLib3SSLError as e:\n            raise SSLError(endpoint_url=request.url, error=e)\n        except (NewConnectionError, socket.gaierror) as e:\n            raise EndpointConnectionError(endpoint_url=request.url, error=e)\n        except ProxyError as e:\n            raise ProxyConnectionError(\n                proxy_url=mask_proxy_url(proxy_url), error=e\n            )\n        except URLLib3ConnectTimeoutError as e:\n            raise ConnectTimeoutError(endpoint_url=request.url, error=e)\n        except URLLib3ReadTimeoutError as e:\n            raise ReadTimeoutError(endpoint_url=request.url, error=e)\n        except ProtocolError as e:\n            raise ConnectionClosedError(\n                error=e, request=request, endpoint_url=request.url\n            )\n        except Exception as e:\n            message = 'Exception received when sending urllib3 HTTP request'\n            logger.debug(message, exc_info=True)\n            raise HTTPClientError(error=e)\n", "botocore/errorfactory.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.exceptions import ClientError\nfrom botocore.utils import get_service_module_name\n\n\nclass BaseClientExceptions:\n    ClientError = ClientError\n\n    def __init__(self, code_to_exception):\n        \"\"\"Base class for exceptions object on a client\n\n        :type code_to_exception: dict\n        :param code_to_exception: Mapping of error codes (strings) to exception\n            class that should be raised when encountering a particular\n            error code.\n        \"\"\"\n        self._code_to_exception = code_to_exception\n\n    def from_code(self, error_code):\n        \"\"\"Retrieves the error class based on the error code\n\n        This is helpful for identifying the exception class needing to be\n        caught based on the ClientError.parsed_reponse['Error']['Code'] value\n\n        :type error_code: string\n        :param error_code: The error code associated to a ClientError exception\n\n        :rtype: ClientError or a subclass of ClientError\n        :returns: The appropriate modeled exception class for that error\n            code. If the error code does not match any of the known\n            modeled exceptions then return a generic ClientError.\n        \"\"\"\n        return self._code_to_exception.get(error_code, self.ClientError)\n\n    def __getattr__(self, name):\n        exception_cls_names = [\n            exception_cls.__name__\n            for exception_cls in self._code_to_exception.values()\n        ]\n        raise AttributeError(\n            fr\"{self} object has no attribute {name}. \"\n            fr\"Valid exceptions are: {', '.join(exception_cls_names)}\"\n        )\n\n\nclass ClientExceptionsFactory:\n    def __init__(self):\n        self._client_exceptions_cache = {}\n\n    def create_client_exceptions(self, service_model):\n        \"\"\"Creates a ClientExceptions object for the particular service client\n\n        :type service_model: botocore.model.ServiceModel\n        :param service_model: The service model for the client\n\n        :rtype: object that subclasses from BaseClientExceptions\n        :returns: The exceptions object of a client that can be used\n            to grab the various different modeled exceptions.\n        \"\"\"\n        service_name = service_model.service_name\n        if service_name not in self._client_exceptions_cache:\n            client_exceptions = self._create_client_exceptions(service_model)\n            self._client_exceptions_cache[service_name] = client_exceptions\n        return self._client_exceptions_cache[service_name]\n\n    def _create_client_exceptions(self, service_model):\n        cls_props = {}\n        code_to_exception = {}\n        for error_shape in service_model.error_shapes:\n            exception_name = str(error_shape.name)\n            exception_cls = type(exception_name, (ClientError,), {})\n            cls_props[exception_name] = exception_cls\n            code = str(error_shape.error_code)\n            code_to_exception[code] = exception_cls\n        cls_name = str(get_service_module_name(service_model) + 'Exceptions')\n        client_exceptions_cls = type(\n            cls_name, (BaseClientExceptions,), cls_props\n        )\n        return client_exceptions_cls(code_to_exception)\n", "botocore/eventstream.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Binary Event Stream Decoding \"\"\"\n\nfrom binascii import crc32\nfrom struct import unpack\n\nfrom botocore.exceptions import EventStreamError\n\n# byte length of the prelude (total_length + header_length + prelude_crc)\n_PRELUDE_LENGTH = 12\n_MAX_HEADERS_LENGTH = 128 * 1024  # 128 Kb\n_MAX_PAYLOAD_LENGTH = 16 * 1024**2  # 16 Mb\n\n\nclass ParserError(Exception):\n    \"\"\"Base binary flow encoding parsing exception.\"\"\"\n\n    pass\n\n\nclass DuplicateHeader(ParserError):\n    \"\"\"Duplicate header found in the event.\"\"\"\n\n    def __init__(self, header):\n        message = 'Duplicate header present: \"%s\"' % header\n        super().__init__(message)\n\n\nclass InvalidHeadersLength(ParserError):\n    \"\"\"Headers length is longer than the maximum.\"\"\"\n\n    def __init__(self, length):\n        message = 'Header length of {} exceeded the maximum of {}'.format(\n            length,\n            _MAX_HEADERS_LENGTH,\n        )\n        super().__init__(message)\n\n\nclass InvalidPayloadLength(ParserError):\n    \"\"\"Payload length is longer than the maximum.\"\"\"\n\n    def __init__(self, length):\n        message = 'Payload length of {} exceeded the maximum of {}'.format(\n            length,\n            _MAX_PAYLOAD_LENGTH,\n        )\n        super().__init__(message)\n\n\nclass ChecksumMismatch(ParserError):\n    \"\"\"Calculated checksum did not match the expected checksum.\"\"\"\n\n    def __init__(self, expected, calculated):\n        message = (\n            'Checksum mismatch: expected 0x{:08x}, calculated 0x{:08x}'.format(\n                expected,\n                calculated,\n            )\n        )\n        super().__init__(message)\n\n\nclass NoInitialResponseError(ParserError):\n    \"\"\"An event of type initial-response was not received.\n\n    This exception is raised when the event stream produced no events or\n    the first event in the stream was not of the initial-response type.\n    \"\"\"\n\n    def __init__(self):\n        message = 'First event was not of the initial-response type'\n        super().__init__(message)\n\n\nclass DecodeUtils:\n    \"\"\"Unpacking utility functions used in the decoder.\n\n    All methods on this class take raw bytes and return  a tuple containing\n    the value parsed from the bytes and the number of bytes consumed to parse\n    that value.\n    \"\"\"\n\n    UINT8_BYTE_FORMAT = '!B'\n    UINT16_BYTE_FORMAT = '!H'\n    UINT32_BYTE_FORMAT = '!I'\n    INT8_BYTE_FORMAT = '!b'\n    INT16_BYTE_FORMAT = '!h'\n    INT32_BYTE_FORMAT = '!i'\n    INT64_BYTE_FORMAT = '!q'\n    PRELUDE_BYTE_FORMAT = '!III'\n\n    # uint byte size to unpack format\n    UINT_BYTE_FORMAT = {\n        1: UINT8_BYTE_FORMAT,\n        2: UINT16_BYTE_FORMAT,\n        4: UINT32_BYTE_FORMAT,\n    }\n\n    @staticmethod\n    def unpack_true(data):\n        \"\"\"This method consumes none of the provided bytes and returns True.\n\n        :type data: bytes\n        :param data: The bytes to parse from. This is ignored in this method.\n\n        :rtype: tuple\n        :rtype: (bool, int)\n        :returns: The tuple (True, 0)\n        \"\"\"\n        return True, 0\n\n    @staticmethod\n    def unpack_false(data):\n        \"\"\"This method consumes none of the provided bytes and returns False.\n\n        :type data: bytes\n        :param data: The bytes to parse from. This is ignored in this method.\n\n        :rtype: tuple\n        :rtype: (bool, int)\n        :returns: The tuple (False, 0)\n        \"\"\"\n        return False, 0\n\n    @staticmethod\n    def unpack_uint8(data):\n        \"\"\"Parse an unsigned 8-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.UINT8_BYTE_FORMAT, data[:1])[0]\n        return value, 1\n\n    @staticmethod\n    def unpack_uint32(data):\n        \"\"\"Parse an unsigned 32-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.UINT32_BYTE_FORMAT, data[:4])[0]\n        return value, 4\n\n    @staticmethod\n    def unpack_int8(data):\n        \"\"\"Parse a signed 8-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.INT8_BYTE_FORMAT, data[:1])[0]\n        return value, 1\n\n    @staticmethod\n    def unpack_int16(data):\n        \"\"\"Parse a signed 16-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: tuple\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.INT16_BYTE_FORMAT, data[:2])[0]\n        return value, 2\n\n    @staticmethod\n    def unpack_int32(data):\n        \"\"\"Parse a signed 32-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: tuple\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.INT32_BYTE_FORMAT, data[:4])[0]\n        return value, 4\n\n    @staticmethod\n    def unpack_int64(data):\n        \"\"\"Parse a signed 64-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: tuple\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.INT64_BYTE_FORMAT, data[:8])[0]\n        return value, 8\n\n    @staticmethod\n    def unpack_byte_array(data, length_byte_size=2):\n        \"\"\"Parse a variable length byte array from the bytes.\n\n        The bytes are expected to be in the following format:\n            [ length ][0 ... length bytes]\n        where length is an unsigned integer represented in the smallest number\n        of bytes to hold the maximum length of the array.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :type length_byte_size: int\n        :param length_byte_size: The byte size of the preceding integer that\n        represents the length of the array. Supported values are 1, 2, and 4.\n\n        :rtype: (bytes, int)\n        :returns: A tuple containing the (parsed byte array, bytes consumed).\n        \"\"\"\n        uint_byte_format = DecodeUtils.UINT_BYTE_FORMAT[length_byte_size]\n        length = unpack(uint_byte_format, data[:length_byte_size])[0]\n        bytes_end = length + length_byte_size\n        array_bytes = data[length_byte_size:bytes_end]\n        return array_bytes, bytes_end\n\n    @staticmethod\n    def unpack_utf8_string(data, length_byte_size=2):\n        \"\"\"Parse a variable length utf-8 string from the bytes.\n\n        The bytes are expected to be in the following format:\n            [ length ][0 ... length bytes]\n        where length is an unsigned integer represented in the smallest number\n        of bytes to hold the maximum length of the array and the following\n        bytes are a valid utf-8 string.\n\n        :type data: bytes\n        :param bytes: The bytes to parse from.\n\n        :type length_byte_size: int\n        :param length_byte_size: The byte size of the preceding integer that\n        represents the length of the array. Supported values are 1, 2, and 4.\n\n        :rtype: (str, int)\n        :returns: A tuple containing the (utf-8 string, bytes consumed).\n        \"\"\"\n        array_bytes, consumed = DecodeUtils.unpack_byte_array(\n            data, length_byte_size\n        )\n        return array_bytes.decode('utf-8'), consumed\n\n    @staticmethod\n    def unpack_uuid(data):\n        \"\"\"Parse a 16-byte uuid from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: (bytes, int)\n        :returns: A tuple containing the (uuid bytes, bytes consumed).\n        \"\"\"\n        return data[:16], 16\n\n    @staticmethod\n    def unpack_prelude(data):\n        \"\"\"Parse the prelude for an event stream message from the bytes.\n\n        The prelude for an event stream message has the following format:\n            [total_length][header_length][prelude_crc]\n        where each field is an unsigned 32-bit integer.\n\n        :rtype: ((int, int, int), int)\n        :returns: A tuple of ((total_length, headers_length, prelude_crc),\n        consumed)\n        \"\"\"\n        return (unpack(DecodeUtils.PRELUDE_BYTE_FORMAT, data), _PRELUDE_LENGTH)\n\n\ndef _validate_checksum(data, checksum, crc=0):\n    # To generate the same numeric value across all Python versions and\n    # platforms use crc32(data) & 0xffffffff.\n    computed_checksum = crc32(data, crc) & 0xFFFFFFFF\n    if checksum != computed_checksum:\n        raise ChecksumMismatch(checksum, computed_checksum)\n\n\nclass MessagePrelude:\n    \"\"\"Represents the prelude of an event stream message.\"\"\"\n\n    def __init__(self, total_length, headers_length, crc):\n        self.total_length = total_length\n        self.headers_length = headers_length\n        self.crc = crc\n\n    @property\n    def payload_length(self):\n        \"\"\"Calculates the total payload length.\n\n        The extra minus 4 bytes is for the message CRC.\n\n        :rtype: int\n        :returns: The total payload length.\n        \"\"\"\n        return self.total_length - self.headers_length - _PRELUDE_LENGTH - 4\n\n    @property\n    def payload_end(self):\n        \"\"\"Calculates the byte offset for the end of the message payload.\n\n        The extra minus 4 bytes is for the message CRC.\n\n        :rtype: int\n        :returns: The byte offset from the beginning of the event stream\n        message to the end of the payload.\n        \"\"\"\n        return self.total_length - 4\n\n    @property\n    def headers_end(self):\n        \"\"\"Calculates the byte offset for the end of the message headers.\n\n        :rtype: int\n        :returns: The byte offset from the beginning of the event stream\n        message to the end of the headers.\n        \"\"\"\n        return _PRELUDE_LENGTH + self.headers_length\n\n\nclass EventStreamMessage:\n    \"\"\"Represents an event stream message.\"\"\"\n\n    def __init__(self, prelude, headers, payload, crc):\n        self.prelude = prelude\n        self.headers = headers\n        self.payload = payload\n        self.crc = crc\n\n    def to_response_dict(self, status_code=200):\n        message_type = self.headers.get(':message-type')\n        if message_type == 'error' or message_type == 'exception':\n            status_code = 400\n        return {\n            'status_code': status_code,\n            'headers': self.headers,\n            'body': self.payload,\n        }\n\n\nclass EventStreamHeaderParser:\n    \"\"\"Parses the event headers from an event stream message.\n\n    Expects all of the header data upfront and creates a dictionary of headers\n    to return. This object can be reused multiple times to parse the headers\n    from multiple event stream messages.\n    \"\"\"\n\n    # Maps header type to appropriate unpacking function\n    # These unpacking functions return the value and the amount unpacked\n    _HEADER_TYPE_MAP = {\n        # boolean_true\n        0: DecodeUtils.unpack_true,\n        # boolean_false\n        1: DecodeUtils.unpack_false,\n        # byte\n        2: DecodeUtils.unpack_int8,\n        # short\n        3: DecodeUtils.unpack_int16,\n        # integer\n        4: DecodeUtils.unpack_int32,\n        # long\n        5: DecodeUtils.unpack_int64,\n        # byte_array\n        6: DecodeUtils.unpack_byte_array,\n        # string\n        7: DecodeUtils.unpack_utf8_string,\n        # timestamp\n        8: DecodeUtils.unpack_int64,\n        # uuid\n        9: DecodeUtils.unpack_uuid,\n    }\n\n    def __init__(self):\n        self._data = None\n\n    def parse(self, data):\n        \"\"\"Parses the event stream headers from an event stream message.\n\n        :type data: bytes\n        :param data: The bytes that correspond to the headers section of an\n        event stream message.\n\n        :rtype: dict\n        :returns: A dictionary of header key, value pairs.\n        \"\"\"\n        self._data = data\n        return self._parse_headers()\n\n    def _parse_headers(self):\n        headers = {}\n        while self._data:\n            name, value = self._parse_header()\n            if name in headers:\n                raise DuplicateHeader(name)\n            headers[name] = value\n        return headers\n\n    def _parse_header(self):\n        name = self._parse_name()\n        value = self._parse_value()\n        return name, value\n\n    def _parse_name(self):\n        name, consumed = DecodeUtils.unpack_utf8_string(self._data, 1)\n        self._advance_data(consumed)\n        return name\n\n    def _parse_type(self):\n        type, consumed = DecodeUtils.unpack_uint8(self._data)\n        self._advance_data(consumed)\n        return type\n\n    def _parse_value(self):\n        header_type = self._parse_type()\n        value_unpacker = self._HEADER_TYPE_MAP[header_type]\n        value, consumed = value_unpacker(self._data)\n        self._advance_data(consumed)\n        return value\n\n    def _advance_data(self, consumed):\n        self._data = self._data[consumed:]\n\n\nclass EventStreamBuffer:\n    \"\"\"Streaming based event stream buffer\n\n    A buffer class that wraps bytes from an event stream providing parsed\n    messages as they become available via an iterable interface.\n    \"\"\"\n\n    def __init__(self):\n        self._data = b''\n        self._prelude = None\n        self._header_parser = EventStreamHeaderParser()\n\n    def add_data(self, data):\n        \"\"\"Add data to the buffer.\n\n        :type data: bytes\n        :param data: The bytes to add to the buffer to be used when parsing\n        \"\"\"\n        self._data += data\n\n    def _validate_prelude(self, prelude):\n        if prelude.headers_length > _MAX_HEADERS_LENGTH:\n            raise InvalidHeadersLength(prelude.headers_length)\n\n        if prelude.payload_length > _MAX_PAYLOAD_LENGTH:\n            raise InvalidPayloadLength(prelude.payload_length)\n\n    def _parse_prelude(self):\n        prelude_bytes = self._data[:_PRELUDE_LENGTH]\n        raw_prelude, _ = DecodeUtils.unpack_prelude(prelude_bytes)\n        prelude = MessagePrelude(*raw_prelude)\n        self._validate_prelude(prelude)\n        # The minus 4 removes the prelude crc from the bytes to be checked\n        _validate_checksum(prelude_bytes[: _PRELUDE_LENGTH - 4], prelude.crc)\n        return prelude\n\n    def _parse_headers(self):\n        header_bytes = self._data[_PRELUDE_LENGTH : self._prelude.headers_end]\n        return self._header_parser.parse(header_bytes)\n\n    def _parse_payload(self):\n        prelude = self._prelude\n        payload_bytes = self._data[prelude.headers_end : prelude.payload_end]\n        return payload_bytes\n\n    def _parse_message_crc(self):\n        prelude = self._prelude\n        crc_bytes = self._data[prelude.payload_end : prelude.total_length]\n        message_crc, _ = DecodeUtils.unpack_uint32(crc_bytes)\n        return message_crc\n\n    def _parse_message_bytes(self):\n        # The minus 4 includes the prelude crc to the bytes to be checked\n        message_bytes = self._data[\n            _PRELUDE_LENGTH - 4 : self._prelude.payload_end\n        ]\n        return message_bytes\n\n    def _validate_message_crc(self):\n        message_crc = self._parse_message_crc()\n        message_bytes = self._parse_message_bytes()\n        _validate_checksum(message_bytes, message_crc, crc=self._prelude.crc)\n        return message_crc\n\n    def _parse_message(self):\n        crc = self._validate_message_crc()\n        headers = self._parse_headers()\n        payload = self._parse_payload()\n        message = EventStreamMessage(self._prelude, headers, payload, crc)\n        self._prepare_for_next_message()\n        return message\n\n    def _prepare_for_next_message(self):\n        # Advance the data and reset the current prelude\n        self._data = self._data[self._prelude.total_length :]\n        self._prelude = None\n\n    def next(self):\n        \"\"\"Provides the next available message parsed from the stream\n\n        :rtype: EventStreamMessage\n        :returns: The next event stream message\n        \"\"\"\n        if len(self._data) < _PRELUDE_LENGTH:\n            raise StopIteration()\n\n        if self._prelude is None:\n            self._prelude = self._parse_prelude()\n\n        if len(self._data) < self._prelude.total_length:\n            raise StopIteration()\n\n        return self._parse_message()\n\n    def __next__(self):\n        return self.next()\n\n    def __iter__(self):\n        return self\n\n\nclass EventStream:\n    \"\"\"Wrapper class for an event stream body.\n\n    This wraps the underlying streaming body, parsing it for individual events\n    and yielding them as they come available through the iterator interface.\n\n    The following example uses the S3 select API to get structured data out of\n    an object stored in S3 using an event stream.\n\n    **Example:**\n    ::\n        from botocore.session import Session\n\n        s3 = Session().create_client('s3')\n        response = s3.select_object_content(\n            Bucket='bucketname',\n            Key='keyname',\n            ExpressionType='SQL',\n            RequestProgress={'Enabled': True},\n            Expression=\"SELECT * FROM S3Object s\",\n            InputSerialization={'CSV': {}},\n            OutputSerialization={'CSV': {}},\n        )\n        # This is the event stream in the response\n        event_stream = response['Payload']\n        end_event_received = False\n        with open('output', 'wb') as f:\n            # Iterate over events in the event stream as they come\n            for event in event_stream:\n                # If we received a records event, write the data to a file\n                if 'Records' in event:\n                    data = event['Records']['Payload']\n                    f.write(data)\n                # If we received a progress event, print the details\n                elif 'Progress' in event:\n                    print(event['Progress']['Details'])\n                # End event indicates that the request finished successfully\n                elif 'End' in event:\n                    print('Result is complete')\n                    end_event_received = True\n        if not end_event_received:\n            raise Exception(\"End event not received, request incomplete.\")\n    \"\"\"\n\n    def __init__(self, raw_stream, output_shape, parser, operation_name):\n        self._raw_stream = raw_stream\n        self._output_shape = output_shape\n        self._operation_name = operation_name\n        self._parser = parser\n        self._event_generator = self._create_raw_event_generator()\n\n    def __iter__(self):\n        for event in self._event_generator:\n            parsed_event = self._parse_event(event)\n            if parsed_event:\n                yield parsed_event\n\n    def _create_raw_event_generator(self):\n        event_stream_buffer = EventStreamBuffer()\n        for chunk in self._raw_stream.stream():\n            event_stream_buffer.add_data(chunk)\n            yield from event_stream_buffer\n\n    def _parse_event(self, event):\n        response_dict = event.to_response_dict()\n        parsed_response = self._parser.parse(response_dict, self._output_shape)\n        if response_dict['status_code'] == 200:\n            return parsed_response\n        else:\n            raise EventStreamError(parsed_response, self._operation_name)\n\n    def get_initial_response(self):\n        try:\n            initial_event = next(self._event_generator)\n            event_type = initial_event.headers.get(':event-type')\n            if event_type == 'initial-response':\n                return initial_event\n        except StopIteration:\n            pass\n        raise NoInitialResponseError()\n\n    def close(self):\n        \"\"\"Closes the underlying streaming body.\"\"\"\n        self._raw_stream.close()\n", "botocore/waiter.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport time\n\nimport jmespath\n\nfrom botocore.docs.docstring import WaiterDocstring\nfrom botocore.utils import get_service_module_name\n\nfrom . import xform_name\nfrom .exceptions import ClientError, WaiterConfigError, WaiterError\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_waiter_with_client(waiter_name, waiter_model, client):\n    \"\"\"\n\n    :type waiter_name: str\n    :param waiter_name: The name of the waiter.  The name should match\n        the name (including the casing) of the key name in the waiter\n        model file (typically this is CamelCasing).\n\n    :type waiter_model: botocore.waiter.WaiterModel\n    :param waiter_model: The model for the waiter configuration.\n\n    :type client: botocore.client.BaseClient\n    :param client: The botocore client associated with the service.\n\n    :rtype: botocore.waiter.Waiter\n    :return: The waiter object.\n\n    \"\"\"\n    single_waiter_config = waiter_model.get_waiter(waiter_name)\n    operation_name = xform_name(single_waiter_config.operation)\n    operation_method = NormalizedOperationMethod(\n        getattr(client, operation_name)\n    )\n\n    # Create a new wait method that will serve as a proxy to the underlying\n    # Waiter.wait method. This is needed to attach a docstring to the\n    # method.\n    def wait(self, **kwargs):\n        Waiter.wait(self, **kwargs)\n\n    wait.__doc__ = WaiterDocstring(\n        waiter_name=waiter_name,\n        event_emitter=client.meta.events,\n        service_model=client.meta.service_model,\n        service_waiter_model=waiter_model,\n        include_signature=False,\n    )\n\n    # Rename the waiter class based on the type of waiter.\n    waiter_class_name = str(\n        '%s.Waiter.%s'\n        % (get_service_module_name(client.meta.service_model), waiter_name)\n    )\n\n    # Create the new waiter class\n    documented_waiter_cls = type(waiter_class_name, (Waiter,), {'wait': wait})\n\n    # Return an instance of the new waiter class.\n    return documented_waiter_cls(\n        waiter_name, single_waiter_config, operation_method\n    )\n\n\ndef is_valid_waiter_error(response):\n    error = response.get('Error')\n    if isinstance(error, dict) and 'Code' in error:\n        return True\n    return False\n\n\nclass NormalizedOperationMethod:\n    def __init__(self, client_method):\n        self._client_method = client_method\n\n    def __call__(self, **kwargs):\n        try:\n            return self._client_method(**kwargs)\n        except ClientError as e:\n            return e.response\n\n\nclass WaiterModel:\n    SUPPORTED_VERSION = 2\n\n    def __init__(self, waiter_config):\n        \"\"\"\n\n        Note that the WaiterModel takes ownership of the waiter_config.\n        It may or may not mutate the waiter_config.  If this is a concern,\n        it is best to make a copy of the waiter config before passing it to\n        the WaiterModel.\n\n        :type waiter_config: dict\n        :param waiter_config: The loaded waiter config\n            from the <service>*.waiters.json file.  This can be\n            obtained from a botocore Loader object as well.\n\n        \"\"\"\n        self._waiter_config = waiter_config['waiters']\n\n        # These are part of the public API.  Changing these\n        # will result in having to update the consuming code,\n        # so don't change unless you really need to.\n        version = waiter_config.get('version', 'unknown')\n        self._verify_supported_version(version)\n        self.version = version\n        self.waiter_names = list(sorted(waiter_config['waiters'].keys()))\n\n    def _verify_supported_version(self, version):\n        if version != self.SUPPORTED_VERSION:\n            raise WaiterConfigError(\n                error_msg=(\n                    \"Unsupported waiter version, supported version \"\n                    \"must be: %s, but version of waiter config \"\n                    \"is: %s\" % (self.SUPPORTED_VERSION, version)\n                )\n            )\n\n    def get_waiter(self, waiter_name):\n        try:\n            single_waiter_config = self._waiter_config[waiter_name]\n        except KeyError:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n        return SingleWaiterConfig(single_waiter_config)\n\n\nclass SingleWaiterConfig:\n    \"\"\"Represents the waiter configuration for a single waiter.\n\n    A single waiter is considered the configuration for a single\n    value associated with a named waiter (i.e TableExists).\n\n    \"\"\"\n\n    def __init__(self, single_waiter_config):\n        self._config = single_waiter_config\n\n        # These attributes are part of the public API.\n        self.description = single_waiter_config.get('description', '')\n        # Per the spec, these three fields are required.\n        self.operation = single_waiter_config['operation']\n        self.delay = single_waiter_config['delay']\n        self.max_attempts = single_waiter_config['maxAttempts']\n\n    @property\n    def acceptors(self):\n        acceptors = []\n        for acceptor_config in self._config['acceptors']:\n            acceptor = AcceptorConfig(acceptor_config)\n            acceptors.append(acceptor)\n        return acceptors\n\n\nclass AcceptorConfig:\n    def __init__(self, config):\n        self.state = config['state']\n        self.matcher = config['matcher']\n        self.expected = config['expected']\n        self.argument = config.get('argument')\n        self.matcher_func = self._create_matcher_func()\n\n    @property\n    def explanation(self):\n        if self.matcher == 'path':\n            return 'For expression \"{}\" we matched expected path: \"{}\"'.format(\n                self.argument,\n                self.expected,\n            )\n        elif self.matcher == 'pathAll':\n            return (\n                'For expression \"%s\" all members matched excepted path: \"%s\"'\n                % (self.argument, self.expected)\n            )\n        elif self.matcher == 'pathAny':\n            return (\n                'For expression \"%s\" we matched expected path: \"%s\" at least once'\n                % (self.argument, self.expected)\n            )\n        elif self.matcher == 'status':\n            return 'Matched expected HTTP status code: %s' % self.expected\n        elif self.matcher == 'error':\n            return 'Matched expected service error code: %s' % self.expected\n        else:\n            return (\n                'No explanation for unknown waiter type: \"%s\"' % self.matcher\n            )\n\n    def _create_matcher_func(self):\n        # An acceptor function is a callable that takes a single value.  The\n        # parsed AWS response.  Note that the parsed error response is also\n        # provided in the case of errors, so it's entirely possible to\n        # handle all the available matcher capabilities in the future.\n        # There's only three supported matchers, so for now, this is all\n        # contained to a single method.  If this grows, we can expand this\n        # out to separate methods or even objects.\n\n        if self.matcher == 'path':\n            return self._create_path_matcher()\n        elif self.matcher == 'pathAll':\n            return self._create_path_all_matcher()\n        elif self.matcher == 'pathAny':\n            return self._create_path_any_matcher()\n        elif self.matcher == 'status':\n            return self._create_status_matcher()\n        elif self.matcher == 'error':\n            return self._create_error_matcher()\n        else:\n            raise WaiterConfigError(\n                error_msg=\"Unknown acceptor: %s\" % self.matcher\n            )\n\n    def _create_path_matcher(self):\n        expression = jmespath.compile(self.argument)\n        expected = self.expected\n\n        def acceptor_matches(response):\n            if is_valid_waiter_error(response):\n                return\n            return expression.search(response) == expected\n\n        return acceptor_matches\n\n    def _create_path_all_matcher(self):\n        expression = jmespath.compile(self.argument)\n        expected = self.expected\n\n        def acceptor_matches(response):\n            if is_valid_waiter_error(response):\n                return\n            result = expression.search(response)\n            if not isinstance(result, list) or not result:\n                # pathAll matcher must result in a list.\n                # Also we require at least one element in the list,\n                # that is, an empty list should not result in this\n                # acceptor match.\n                return False\n            for element in result:\n                if element != expected:\n                    return False\n            return True\n\n        return acceptor_matches\n\n    def _create_path_any_matcher(self):\n        expression = jmespath.compile(self.argument)\n        expected = self.expected\n\n        def acceptor_matches(response):\n            if is_valid_waiter_error(response):\n                return\n            result = expression.search(response)\n            if not isinstance(result, list) or not result:\n                # pathAny matcher must result in a list.\n                # Also we require at least one element in the list,\n                # that is, an empty list should not result in this\n                # acceptor match.\n                return False\n            for element in result:\n                if element == expected:\n                    return True\n            return False\n\n        return acceptor_matches\n\n    def _create_status_matcher(self):\n        expected = self.expected\n\n        def acceptor_matches(response):\n            # We don't have any requirements on the expected incoming data\n            # other than it is a dict, so we don't assume there's\n            # a ResponseMetadata.HTTPStatusCode.\n            status_code = response.get('ResponseMetadata', {}).get(\n                'HTTPStatusCode'\n            )\n            return status_code == expected\n\n        return acceptor_matches\n\n    def _create_error_matcher(self):\n        expected = self.expected\n\n        def acceptor_matches(response):\n            # When the client encounters an error, it will normally raise\n            # an exception.  However, the waiter implementation will catch\n            # this exception, and instead send us the parsed error\n            # response.  So response is still a dictionary, and in the case\n            # of an error response will contain the \"Error\" and\n            # \"ResponseMetadata\" key.\n            return response.get(\"Error\", {}).get(\"Code\", \"\") == expected\n\n        return acceptor_matches\n\n\nclass Waiter:\n    def __init__(self, name, config, operation_method):\n        \"\"\"\n\n        :type name: string\n        :param name: The name of the waiter\n\n        :type config: botocore.waiter.SingleWaiterConfig\n        :param config: The configuration for the waiter.\n\n        :type operation_method: callable\n        :param operation_method: A callable that accepts **kwargs\n            and returns a response.  For example, this can be\n            a method from a botocore client.\n\n        \"\"\"\n        self._operation_method = operation_method\n        # The two attributes are exposed to allow for introspection\n        # and documentation.\n        self.name = name\n        self.config = config\n\n    def wait(self, **kwargs):\n        acceptors = list(self.config.acceptors)\n        current_state = 'waiting'\n        # pop the invocation specific config\n        config = kwargs.pop('WaiterConfig', {})\n        sleep_amount = config.get('Delay', self.config.delay)\n        max_attempts = config.get('MaxAttempts', self.config.max_attempts)\n        last_matched_acceptor = None\n        num_attempts = 0\n\n        while True:\n            response = self._operation_method(**kwargs)\n            num_attempts += 1\n            for acceptor in acceptors:\n                if acceptor.matcher_func(response):\n                    last_matched_acceptor = acceptor\n                    current_state = acceptor.state\n                    break\n            else:\n                # If none of the acceptors matched, we should\n                # transition to the failure state if an error\n                # response was received.\n                if is_valid_waiter_error(response):\n                    # Transition to a failure state, which we\n                    # can just handle here by raising an exception.\n                    raise WaiterError(\n                        name=self.name,\n                        reason='An error occurred (%s): %s'\n                        % (\n                            response['Error'].get('Code', 'Unknown'),\n                            response['Error'].get('Message', 'Unknown'),\n                        ),\n                        last_response=response,\n                    )\n            if current_state == 'success':\n                logger.debug(\n                    \"Waiting complete, waiter matched the \" \"success state.\"\n                )\n                return\n            if current_state == 'failure':\n                reason = 'Waiter encountered a terminal failure state: %s' % (\n                    acceptor.explanation\n                )\n                raise WaiterError(\n                    name=self.name,\n                    reason=reason,\n                    last_response=response,\n                )\n            if num_attempts >= max_attempts:\n                if last_matched_acceptor is None:\n                    reason = 'Max attempts exceeded'\n                else:\n                    reason = (\n                        'Max attempts exceeded. Previously accepted state: %s'\n                        % (acceptor.explanation)\n                    )\n                raise WaiterError(\n                    name=self.name,\n                    reason=reason,\n                    last_response=response,\n                )\n            time.sleep(sleep_amount)\n", "botocore/credentials.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\nimport getpass\nimport json\nimport logging\nimport os\nimport subprocess\nimport threading\nimport time\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom hashlib import sha1\n\nfrom dateutil.parser import parse\nfrom dateutil.tz import tzlocal, tzutc\n\nimport botocore.compat\nimport botocore.configloader\nfrom botocore import UNSIGNED\nfrom botocore.compat import compat_shell_split, total_seconds\nfrom botocore.config import Config\nfrom botocore.exceptions import (\n    ConfigNotFound,\n    CredentialRetrievalError,\n    InfiniteLoopConfigError,\n    InvalidConfigError,\n    MetadataRetrievalError,\n    PartialCredentialsError,\n    RefreshWithMFAUnsupportedError,\n    UnauthorizedSSOTokenError,\n    UnknownCredentialError,\n)\nfrom botocore.tokens import SSOTokenProvider\nfrom botocore.utils import (\n    ContainerMetadataFetcher,\n    FileWebIdentityTokenLoader,\n    InstanceMetadataFetcher,\n    JSONFileCache,\n    SSOTokenLoader,\n    parse_key_val_file,\n    resolve_imds_endpoint_mode,\n)\n\nlogger = logging.getLogger(__name__)\nReadOnlyCredentials = namedtuple(\n    'ReadOnlyCredentials', ['access_key', 'secret_key', 'token']\n)\n\n_DEFAULT_MANDATORY_REFRESH_TIMEOUT = 10 * 60  # 10 min\n_DEFAULT_ADVISORY_REFRESH_TIMEOUT = 15 * 60  # 15 min\n\n\ndef create_credential_resolver(session, cache=None, region_name=None):\n    \"\"\"Create a default credential resolver.\n\n    This creates a pre-configured credential resolver\n    that includes the default lookup chain for\n    credentials.\n\n    \"\"\"\n    profile_name = session.get_config_variable('profile') or 'default'\n    metadata_timeout = session.get_config_variable('metadata_service_timeout')\n    num_attempts = session.get_config_variable('metadata_service_num_attempts')\n    disable_env_vars = session.instance_variables().get('profile') is not None\n\n    imds_config = {\n        'ec2_metadata_service_endpoint': session.get_config_variable(\n            'ec2_metadata_service_endpoint'\n        ),\n        'ec2_metadata_service_endpoint_mode': resolve_imds_endpoint_mode(\n            session\n        ),\n        'ec2_credential_refresh_window': _DEFAULT_ADVISORY_REFRESH_TIMEOUT,\n        'ec2_metadata_v1_disabled': session.get_config_variable(\n            'ec2_metadata_v1_disabled'\n        ),\n    }\n\n    if cache is None:\n        cache = {}\n\n    env_provider = EnvProvider()\n    container_provider = ContainerProvider()\n    instance_metadata_provider = InstanceMetadataProvider(\n        iam_role_fetcher=InstanceMetadataFetcher(\n            timeout=metadata_timeout,\n            num_attempts=num_attempts,\n            user_agent=session.user_agent(),\n            config=imds_config,\n        )\n    )\n\n    profile_provider_builder = ProfileProviderBuilder(\n        session, cache=cache, region_name=region_name\n    )\n    assume_role_provider = AssumeRoleProvider(\n        load_config=lambda: session.full_config,\n        client_creator=_get_client_creator(session, region_name),\n        cache=cache,\n        profile_name=profile_name,\n        credential_sourcer=CanonicalNameCredentialSourcer(\n            [env_provider, container_provider, instance_metadata_provider]\n        ),\n        profile_provider_builder=profile_provider_builder,\n    )\n\n    pre_profile = [\n        env_provider,\n        assume_role_provider,\n    ]\n    profile_providers = profile_provider_builder.providers(\n        profile_name=profile_name,\n        disable_env_vars=disable_env_vars,\n    )\n    post_profile = [\n        OriginalEC2Provider(),\n        BotoProvider(),\n        container_provider,\n        instance_metadata_provider,\n    ]\n    providers = pre_profile + profile_providers + post_profile\n\n    if disable_env_vars:\n        # An explicitly provided profile will negate an EnvProvider.\n        # We will defer to providers that understand the \"profile\"\n        # concept to retrieve credentials.\n        # The one edge case if is all three values are provided via\n        # env vars:\n        # export AWS_ACCESS_KEY_ID=foo\n        # export AWS_SECRET_ACCESS_KEY=bar\n        # export AWS_PROFILE=baz\n        # Then, just like our client() calls, the explicit credentials\n        # will take precedence.\n        #\n        # This precedence is enforced by leaving the EnvProvider in the chain.\n        # This means that the only way a \"profile\" would win is if the\n        # EnvProvider does not return credentials, which is what we want\n        # in this scenario.\n        providers.remove(env_provider)\n        logger.debug(\n            'Skipping environment variable credential check'\n            ' because profile name was explicitly set.'\n        )\n\n    resolver = CredentialResolver(providers=providers)\n    return resolver\n\n\nclass ProfileProviderBuilder:\n    \"\"\"This class handles the creation of profile based providers.\n\n    NOTE: This class is only intended for internal use.\n\n    This class handles the creation and ordering of the various credential\n    providers that primarly source their configuration from the shared config.\n    This is needed to enable sharing between the default credential chain and\n    the source profile chain created by the assume role provider.\n    \"\"\"\n\n    def __init__(\n        self, session, cache=None, region_name=None, sso_token_cache=None\n    ):\n        self._session = session\n        self._cache = cache\n        self._region_name = region_name\n        self._sso_token_cache = sso_token_cache\n\n    def providers(self, profile_name, disable_env_vars=False):\n        return [\n            self._create_web_identity_provider(\n                profile_name,\n                disable_env_vars,\n            ),\n            self._create_sso_provider(profile_name),\n            self._create_shared_credential_provider(profile_name),\n            self._create_process_provider(profile_name),\n            self._create_config_provider(profile_name),\n        ]\n\n    def _create_process_provider(self, profile_name):\n        return ProcessProvider(\n            profile_name=profile_name,\n            load_config=lambda: self._session.full_config,\n        )\n\n    def _create_shared_credential_provider(self, profile_name):\n        credential_file = self._session.get_config_variable('credentials_file')\n        return SharedCredentialProvider(\n            profile_name=profile_name,\n            creds_filename=credential_file,\n        )\n\n    def _create_config_provider(self, profile_name):\n        config_file = self._session.get_config_variable('config_file')\n        return ConfigProvider(\n            profile_name=profile_name,\n            config_filename=config_file,\n        )\n\n    def _create_web_identity_provider(self, profile_name, disable_env_vars):\n        return AssumeRoleWithWebIdentityProvider(\n            load_config=lambda: self._session.full_config,\n            client_creator=_get_client_creator(\n                self._session, self._region_name\n            ),\n            cache=self._cache,\n            profile_name=profile_name,\n            disable_env_vars=disable_env_vars,\n        )\n\n    def _create_sso_provider(self, profile_name):\n        return SSOProvider(\n            load_config=lambda: self._session.full_config,\n            client_creator=self._session.create_client,\n            profile_name=profile_name,\n            cache=self._cache,\n            token_cache=self._sso_token_cache,\n            token_provider=SSOTokenProvider(\n                self._session,\n                cache=self._sso_token_cache,\n                profile_name=profile_name,\n            ),\n        )\n\n\ndef get_credentials(session):\n    resolver = create_credential_resolver(session)\n    return resolver.load_credentials()\n\n\ndef _local_now():\n    return datetime.datetime.now(tzlocal())\n\n\ndef _parse_if_needed(value):\n    if isinstance(value, datetime.datetime):\n        return value\n    return parse(value)\n\n\ndef _serialize_if_needed(value, iso=False):\n    if isinstance(value, datetime.datetime):\n        if iso:\n            return value.isoformat()\n        return value.strftime('%Y-%m-%dT%H:%M:%S%Z')\n    return value\n\n\ndef _get_client_creator(session, region_name):\n    def client_creator(service_name, **kwargs):\n        create_client_kwargs = {'region_name': region_name}\n        create_client_kwargs.update(**kwargs)\n        return session.create_client(service_name, **create_client_kwargs)\n\n    return client_creator\n\n\ndef create_assume_role_refresher(client, params):\n    def refresh():\n        response = client.assume_role(**params)\n        credentials = response['Credentials']\n        # We need to normalize the credential names to\n        # the values expected by the refresh creds.\n        return {\n            'access_key': credentials['AccessKeyId'],\n            'secret_key': credentials['SecretAccessKey'],\n            'token': credentials['SessionToken'],\n            'expiry_time': _serialize_if_needed(credentials['Expiration']),\n        }\n\n    return refresh\n\n\ndef create_mfa_serial_refresher(actual_refresh):\n    class _Refresher:\n        def __init__(self, refresh):\n            self._refresh = refresh\n            self._has_been_called = False\n\n        def __call__(self):\n            if self._has_been_called:\n                # We can explore an option in the future to support\n                # reprompting for MFA, but for now we just error out\n                # when the temp creds expire.\n                raise RefreshWithMFAUnsupportedError()\n            self._has_been_called = True\n            return self._refresh()\n\n    return _Refresher(actual_refresh)\n\n\nclass Credentials:\n    \"\"\"\n    Holds the credentials needed to authenticate requests.\n\n    :param str access_key: The access key part of the credentials.\n    :param str secret_key: The secret key part of the credentials.\n    :param str token: The security token, valid only for session credentials.\n    :param str method: A string which identifies where the credentials\n        were found.\n    \"\"\"\n\n    def __init__(self, access_key, secret_key, token=None, method=None):\n        self.access_key = access_key\n        self.secret_key = secret_key\n        self.token = token\n\n        if method is None:\n            method = 'explicit'\n        self.method = method\n\n        self._normalize()\n\n    def _normalize(self):\n        # Keys would sometimes (accidentally) contain non-ascii characters.\n        # It would cause a confusing UnicodeDecodeError in Python 2.\n        # We explicitly convert them into unicode to avoid such error.\n        #\n        # Eventually the service will decide whether to accept the credential.\n        # This also complies with the behavior in Python 3.\n        self.access_key = botocore.compat.ensure_unicode(self.access_key)\n        self.secret_key = botocore.compat.ensure_unicode(self.secret_key)\n\n    def get_frozen_credentials(self):\n        return ReadOnlyCredentials(\n            self.access_key, self.secret_key, self.token\n        )\n\n\nclass RefreshableCredentials(Credentials):\n    \"\"\"\n    Holds the credentials needed to authenticate requests. In addition, it\n    knows how to refresh itself.\n\n    :param str access_key: The access key part of the credentials.\n    :param str secret_key: The secret key part of the credentials.\n    :param str token: The security token, valid only for session credentials.\n    :param datetime expiry_time: The expiration time of the credentials.\n    :param function refresh_using: Callback function to refresh the credentials.\n    :param str method: A string which identifies where the credentials\n        were found.\n    :param function time_fetcher: Callback function to retrieve current time.\n    \"\"\"\n\n    # The time at which we'll attempt to refresh, but not\n    # block if someone else is refreshing.\n    _advisory_refresh_timeout = _DEFAULT_ADVISORY_REFRESH_TIMEOUT\n    # The time at which all threads will block waiting for\n    # refreshed credentials.\n    _mandatory_refresh_timeout = _DEFAULT_MANDATORY_REFRESH_TIMEOUT\n\n    def __init__(\n        self,\n        access_key,\n        secret_key,\n        token,\n        expiry_time,\n        refresh_using,\n        method,\n        time_fetcher=_local_now,\n        advisory_timeout=None,\n        mandatory_timeout=None,\n    ):\n        self._refresh_using = refresh_using\n        self._access_key = access_key\n        self._secret_key = secret_key\n        self._token = token\n        self._expiry_time = expiry_time\n        self._time_fetcher = time_fetcher\n        self._refresh_lock = threading.Lock()\n        self.method = method\n        self._frozen_credentials = ReadOnlyCredentials(\n            access_key, secret_key, token\n        )\n        self._normalize()\n        if advisory_timeout is not None:\n            self._advisory_refresh_timeout = advisory_timeout\n        if mandatory_timeout is not None:\n            self._mandatory_refresh_timeout = mandatory_timeout\n\n    def _normalize(self):\n        self._access_key = botocore.compat.ensure_unicode(self._access_key)\n        self._secret_key = botocore.compat.ensure_unicode(self._secret_key)\n\n    @classmethod\n    def create_from_metadata(\n        cls,\n        metadata,\n        refresh_using,\n        method,\n        advisory_timeout=None,\n        mandatory_timeout=None,\n    ):\n        kwargs = {}\n        if advisory_timeout is not None:\n            kwargs['advisory_timeout'] = advisory_timeout\n        if mandatory_timeout is not None:\n            kwargs['mandatory_timeout'] = mandatory_timeout\n\n        instance = cls(\n            access_key=metadata['access_key'],\n            secret_key=metadata['secret_key'],\n            token=metadata['token'],\n            expiry_time=cls._expiry_datetime(metadata['expiry_time']),\n            method=method,\n            refresh_using=refresh_using,\n            **kwargs,\n        )\n        return instance\n\n    @property\n    def access_key(self):\n        \"\"\"Warning: Using this property can lead to race conditions if you\n        access another property subsequently along the refresh boundary.\n        Please use get_frozen_credentials instead.\n        \"\"\"\n        self._refresh()\n        return self._access_key\n\n    @access_key.setter\n    def access_key(self, value):\n        self._access_key = value\n\n    @property\n    def secret_key(self):\n        \"\"\"Warning: Using this property can lead to race conditions if you\n        access another property subsequently along the refresh boundary.\n        Please use get_frozen_credentials instead.\n        \"\"\"\n        self._refresh()\n        return self._secret_key\n\n    @secret_key.setter\n    def secret_key(self, value):\n        self._secret_key = value\n\n    @property\n    def token(self):\n        \"\"\"Warning: Using this property can lead to race conditions if you\n        access another property subsequently along the refresh boundary.\n        Please use get_frozen_credentials instead.\n        \"\"\"\n        self._refresh()\n        return self._token\n\n    @token.setter\n    def token(self, value):\n        self._token = value\n\n    def _seconds_remaining(self):\n        delta = self._expiry_time - self._time_fetcher()\n        return total_seconds(delta)\n\n    def refresh_needed(self, refresh_in=None):\n        \"\"\"Check if a refresh is needed.\n\n        A refresh is needed if the expiry time associated\n        with the temporary credentials is less than the\n        provided ``refresh_in``.  If ``time_delta`` is not\n        provided, ``self.advisory_refresh_needed`` will be used.\n\n        For example, if your temporary credentials expire\n        in 10 minutes and the provided ``refresh_in`` is\n        ``15 * 60``, then this function will return ``True``.\n\n        :type refresh_in: int\n        :param refresh_in: The number of seconds before the\n            credentials expire in which refresh attempts should\n            be made.\n\n        :return: True if refresh needed, False otherwise.\n\n        \"\"\"\n        if self._expiry_time is None:\n            # No expiration, so assume we don't need to refresh.\n            return False\n\n        if refresh_in is None:\n            refresh_in = self._advisory_refresh_timeout\n        # The credentials should be refreshed if they're going to expire\n        # in less than 5 minutes.\n        if self._seconds_remaining() >= refresh_in:\n            # There's enough time left. Don't refresh.\n            return False\n        logger.debug(\"Credentials need to be refreshed.\")\n        return True\n\n    def _is_expired(self):\n        # Checks if the current credentials are expired.\n        return self.refresh_needed(refresh_in=0)\n\n    def _refresh(self):\n        # In the common case where we don't need a refresh, we\n        # can immediately exit and not require acquiring the\n        # refresh lock.\n        if not self.refresh_needed(self._advisory_refresh_timeout):\n            return\n\n        # acquire() doesn't accept kwargs, but False is indicating\n        # that we should not block if we can't acquire the lock.\n        # If we aren't able to acquire the lock, we'll trigger\n        # the else clause.\n        if self._refresh_lock.acquire(False):\n            try:\n                if not self.refresh_needed(self._advisory_refresh_timeout):\n                    return\n                is_mandatory_refresh = self.refresh_needed(\n                    self._mandatory_refresh_timeout\n                )\n                self._protected_refresh(is_mandatory=is_mandatory_refresh)\n                return\n            finally:\n                self._refresh_lock.release()\n        elif self.refresh_needed(self._mandatory_refresh_timeout):\n            # If we're within the mandatory refresh window,\n            # we must block until we get refreshed credentials.\n            with self._refresh_lock:\n                if not self.refresh_needed(self._mandatory_refresh_timeout):\n                    return\n                self._protected_refresh(is_mandatory=True)\n\n    def _protected_refresh(self, is_mandatory):\n        # precondition: this method should only be called if you've acquired\n        # the self._refresh_lock.\n        try:\n            metadata = self._refresh_using()\n        except Exception:\n            period_name = 'mandatory' if is_mandatory else 'advisory'\n            logger.warning(\n                \"Refreshing temporary credentials failed \"\n                \"during %s refresh period.\",\n                period_name,\n                exc_info=True,\n            )\n            if is_mandatory:\n                # If this is a mandatory refresh, then\n                # all errors that occur when we attempt to refresh\n                # credentials are propagated back to the user.\n                raise\n            # Otherwise we'll just return.\n            # The end result will be that we'll use the current\n            # set of temporary credentials we have.\n            return\n        self._set_from_data(metadata)\n        self._frozen_credentials = ReadOnlyCredentials(\n            self._access_key, self._secret_key, self._token\n        )\n        if self._is_expired():\n            # We successfully refreshed credentials but for whatever\n            # reason, our refreshing function returned credentials\n            # that are still expired.  In this scenario, the only\n            # thing we can do is let the user know and raise\n            # an exception.\n            msg = (\n                \"Credentials were refreshed, but the \"\n                \"refreshed credentials are still expired.\"\n            )\n            logger.warning(msg)\n            raise RuntimeError(msg)\n\n    @staticmethod\n    def _expiry_datetime(time_str):\n        return parse(time_str)\n\n    def _set_from_data(self, data):\n        expected_keys = ['access_key', 'secret_key', 'token', 'expiry_time']\n        if not data:\n            missing_keys = expected_keys\n        else:\n            missing_keys = [k for k in expected_keys if k not in data]\n\n        if missing_keys:\n            message = \"Credential refresh failed, response did not contain: %s\"\n            raise CredentialRetrievalError(\n                provider=self.method,\n                error_msg=message % ', '.join(missing_keys),\n            )\n\n        self.access_key = data['access_key']\n        self.secret_key = data['secret_key']\n        self.token = data['token']\n        self._expiry_time = parse(data['expiry_time'])\n        logger.debug(\n            \"Retrieved credentials will expire at: %s\", self._expiry_time\n        )\n        self._normalize()\n\n    def get_frozen_credentials(self):\n        \"\"\"Return immutable credentials.\n\n        The ``access_key``, ``secret_key``, and ``token`` properties\n        on this class will always check and refresh credentials if\n        needed before returning the particular credentials.\n\n        This has an edge case where you can get inconsistent\n        credentials.  Imagine this:\n\n            # Current creds are \"t1\"\n            tmp.access_key  ---> expired? no, so return t1.access_key\n            # ---- time is now expired, creds need refreshing to \"t2\" ----\n            tmp.secret_key  ---> expired? yes, refresh and return t2.secret_key\n\n        This means we're using the access key from t1 with the secret key\n        from t2.  To fix this issue, you can request a frozen credential object\n        which is guaranteed not to change.\n\n        The frozen credentials returned from this method should be used\n        immediately and then discarded.  The typical usage pattern would\n        be::\n\n            creds = RefreshableCredentials(...)\n            some_code = SomeSignerObject()\n            # I'm about to sign the request.\n            # The frozen credentials are only used for the\n            # duration of generate_presigned_url and will be\n            # immediately thrown away.\n            request = some_code.sign_some_request(\n                with_credentials=creds.get_frozen_credentials())\n            print(\"Signed request:\", request)\n\n        \"\"\"\n        self._refresh()\n        return self._frozen_credentials\n\n\nclass DeferredRefreshableCredentials(RefreshableCredentials):\n    \"\"\"Refreshable credentials that don't require initial credentials.\n\n    refresh_using will be called upon first access.\n    \"\"\"\n\n    def __init__(self, refresh_using, method, time_fetcher=_local_now):\n        self._refresh_using = refresh_using\n        self._access_key = None\n        self._secret_key = None\n        self._token = None\n        self._expiry_time = None\n        self._time_fetcher = time_fetcher\n        self._refresh_lock = threading.Lock()\n        self.method = method\n        self._frozen_credentials = None\n\n    def refresh_needed(self, refresh_in=None):\n        if self._frozen_credentials is None:\n            return True\n        return super().refresh_needed(refresh_in)\n\n\nclass CachedCredentialFetcher:\n    DEFAULT_EXPIRY_WINDOW_SECONDS = 60 * 15\n\n    def __init__(self, cache=None, expiry_window_seconds=None):\n        if cache is None:\n            cache = {}\n        self._cache = cache\n        self._cache_key = self._create_cache_key()\n        if expiry_window_seconds is None:\n            expiry_window_seconds = self.DEFAULT_EXPIRY_WINDOW_SECONDS\n        self._expiry_window_seconds = expiry_window_seconds\n\n    def _create_cache_key(self):\n        raise NotImplementedError('_create_cache_key()')\n\n    def _make_file_safe(self, filename):\n        # Replace :, path sep, and / to make it the string filename safe.\n        filename = filename.replace(':', '_').replace(os.sep, '_')\n        return filename.replace('/', '_')\n\n    def _get_credentials(self):\n        raise NotImplementedError('_get_credentials()')\n\n    def fetch_credentials(self):\n        return self._get_cached_credentials()\n\n    def _get_cached_credentials(self):\n        \"\"\"Get up-to-date credentials.\n\n        This will check the cache for up-to-date credentials, calling assume\n        role if none are available.\n        \"\"\"\n        response = self._load_from_cache()\n        if response is None:\n            response = self._get_credentials()\n            self._write_to_cache(response)\n        else:\n            logger.debug(\"Credentials for role retrieved from cache.\")\n\n        creds = response['Credentials']\n        expiration = _serialize_if_needed(creds['Expiration'], iso=True)\n        return {\n            'access_key': creds['AccessKeyId'],\n            'secret_key': creds['SecretAccessKey'],\n            'token': creds['SessionToken'],\n            'expiry_time': expiration,\n        }\n\n    def _load_from_cache(self):\n        if self._cache_key in self._cache:\n            creds = deepcopy(self._cache[self._cache_key])\n            if not self._is_expired(creds):\n                return creds\n            else:\n                logger.debug(\n                    \"Credentials were found in cache, but they are expired.\"\n                )\n        return None\n\n    def _write_to_cache(self, response):\n        self._cache[self._cache_key] = deepcopy(response)\n\n    def _is_expired(self, credentials):\n        \"\"\"Check if credentials are expired.\"\"\"\n        end_time = _parse_if_needed(credentials['Credentials']['Expiration'])\n        seconds = total_seconds(end_time - _local_now())\n        return seconds < self._expiry_window_seconds\n\n\nclass BaseAssumeRoleCredentialFetcher(CachedCredentialFetcher):\n    def __init__(\n        self,\n        client_creator,\n        role_arn,\n        extra_args=None,\n        cache=None,\n        expiry_window_seconds=None,\n    ):\n        self._client_creator = client_creator\n        self._role_arn = role_arn\n\n        if extra_args is None:\n            self._assume_kwargs = {}\n        else:\n            self._assume_kwargs = deepcopy(extra_args)\n        self._assume_kwargs['RoleArn'] = self._role_arn\n\n        self._role_session_name = self._assume_kwargs.get('RoleSessionName')\n        self._using_default_session_name = False\n        if not self._role_session_name:\n            self._generate_assume_role_name()\n\n        super().__init__(cache, expiry_window_seconds)\n\n    def _generate_assume_role_name(self):\n        self._role_session_name = 'botocore-session-%s' % (int(time.time()))\n        self._assume_kwargs['RoleSessionName'] = self._role_session_name\n        self._using_default_session_name = True\n\n    def _create_cache_key(self):\n        \"\"\"Create a predictable cache key for the current configuration.\n\n        The cache key is intended to be compatible with file names.\n        \"\"\"\n        args = deepcopy(self._assume_kwargs)\n\n        # The role session name gets randomly generated, so we don't want it\n        # in the hash.\n        if self._using_default_session_name:\n            del args['RoleSessionName']\n\n        if 'Policy' in args:\n            # To have a predictable hash, the keys of the policy must be\n            # sorted, so we have to load it here to make sure it gets sorted\n            # later on.\n            args['Policy'] = json.loads(args['Policy'])\n\n        args = json.dumps(args, sort_keys=True)\n        argument_hash = sha1(args.encode('utf-8')).hexdigest()\n        return self._make_file_safe(argument_hash)\n\n\nclass AssumeRoleCredentialFetcher(BaseAssumeRoleCredentialFetcher):\n    def __init__(\n        self,\n        client_creator,\n        source_credentials,\n        role_arn,\n        extra_args=None,\n        mfa_prompter=None,\n        cache=None,\n        expiry_window_seconds=None,\n    ):\n        \"\"\"\n        :type client_creator: callable\n        :param client_creator: A callable that creates a client taking\n            arguments like ``Session.create_client``.\n\n        :type source_credentials: Credentials\n        :param source_credentials: The credentials to use to create the\n            client for the call to AssumeRole.\n\n        :type role_arn: str\n        :param role_arn: The ARN of the role to be assumed.\n\n        :type extra_args: dict\n        :param extra_args: Any additional arguments to add to the assume\n            role request using the format of the botocore operation.\n            Possible keys include, but may not be limited to,\n            DurationSeconds, Policy, SerialNumber, ExternalId and\n            RoleSessionName.\n\n        :type mfa_prompter: callable\n        :param mfa_prompter: A callable that returns input provided by the\n            user (i.e raw_input, getpass.getpass, etc.).\n\n        :type cache: dict\n        :param cache: An object that supports ``__getitem__``,\n            ``__setitem__``, and ``__contains__``.  An example of this is\n            the ``JSONFileCache`` class in aws-cli.\n\n        :type expiry_window_seconds: int\n        :param expiry_window_seconds: The amount of time, in seconds,\n        \"\"\"\n        self._source_credentials = source_credentials\n        self._mfa_prompter = mfa_prompter\n        if self._mfa_prompter is None:\n            self._mfa_prompter = getpass.getpass\n\n        super().__init__(\n            client_creator,\n            role_arn,\n            extra_args=extra_args,\n            cache=cache,\n            expiry_window_seconds=expiry_window_seconds,\n        )\n\n    def _get_credentials(self):\n        \"\"\"Get credentials by calling assume role.\"\"\"\n        kwargs = self._assume_role_kwargs()\n        client = self._create_client()\n        return client.assume_role(**kwargs)\n\n    def _assume_role_kwargs(self):\n        \"\"\"Get the arguments for assume role based on current configuration.\"\"\"\n        assume_role_kwargs = deepcopy(self._assume_kwargs)\n\n        mfa_serial = assume_role_kwargs.get('SerialNumber')\n\n        if mfa_serial is not None:\n            prompt = 'Enter MFA code for %s: ' % mfa_serial\n            token_code = self._mfa_prompter(prompt)\n            assume_role_kwargs['TokenCode'] = token_code\n\n        duration_seconds = assume_role_kwargs.get('DurationSeconds')\n\n        if duration_seconds is not None:\n            assume_role_kwargs['DurationSeconds'] = duration_seconds\n\n        return assume_role_kwargs\n\n    def _create_client(self):\n        \"\"\"Create an STS client using the source credentials.\"\"\"\n        frozen_credentials = self._source_credentials.get_frozen_credentials()\n        return self._client_creator(\n            'sts',\n            aws_access_key_id=frozen_credentials.access_key,\n            aws_secret_access_key=frozen_credentials.secret_key,\n            aws_session_token=frozen_credentials.token,\n        )\n\n\nclass AssumeRoleWithWebIdentityCredentialFetcher(\n    BaseAssumeRoleCredentialFetcher\n):\n    def __init__(\n        self,\n        client_creator,\n        web_identity_token_loader,\n        role_arn,\n        extra_args=None,\n        cache=None,\n        expiry_window_seconds=None,\n    ):\n        \"\"\"\n        :type client_creator: callable\n        :param client_creator: A callable that creates a client taking\n            arguments like ``Session.create_client``.\n\n        :type web_identity_token_loader: callable\n        :param web_identity_token_loader: A callable that takes no arguments\n        and returns a web identity token str.\n\n        :type role_arn: str\n        :param role_arn: The ARN of the role to be assumed.\n\n        :type extra_args: dict\n        :param extra_args: Any additional arguments to add to the assume\n            role request using the format of the botocore operation.\n            Possible keys include, but may not be limited to,\n            DurationSeconds, Policy, SerialNumber, ExternalId and\n            RoleSessionName.\n\n        :type cache: dict\n        :param cache: An object that supports ``__getitem__``,\n            ``__setitem__``, and ``__contains__``.  An example of this is\n            the ``JSONFileCache`` class in aws-cli.\n\n        :type expiry_window_seconds: int\n        :param expiry_window_seconds: The amount of time, in seconds,\n        \"\"\"\n        self._web_identity_token_loader = web_identity_token_loader\n\n        super().__init__(\n            client_creator,\n            role_arn,\n            extra_args=extra_args,\n            cache=cache,\n            expiry_window_seconds=expiry_window_seconds,\n        )\n\n    def _get_credentials(self):\n        \"\"\"Get credentials by calling assume role.\"\"\"\n        kwargs = self._assume_role_kwargs()\n        # Assume role with web identity does not require credentials other than\n        # the token, explicitly configure the client to not sign requests.\n        config = Config(signature_version=UNSIGNED)\n        client = self._client_creator('sts', config=config)\n        return client.assume_role_with_web_identity(**kwargs)\n\n    def _assume_role_kwargs(self):\n        \"\"\"Get the arguments for assume role based on current configuration.\"\"\"\n        assume_role_kwargs = deepcopy(self._assume_kwargs)\n        identity_token = self._web_identity_token_loader()\n        assume_role_kwargs['WebIdentityToken'] = identity_token\n\n        return assume_role_kwargs\n\n\nclass CredentialProvider:\n    # A short name to identify the provider within botocore.\n    METHOD = None\n\n    # A name to identify the provider for use in cross-sdk features like\n    # assume role's `credential_source` configuration option. These names\n    # are to be treated in a case-insensitive way. NOTE: any providers not\n    # implemented in botocore MUST prefix their canonical names with\n    # 'custom' or we DO NOT guarantee that it will work with any features\n    # that this provides.\n    CANONICAL_NAME = None\n\n    def __init__(self, session=None):\n        self.session = session\n\n    def load(self):\n        \"\"\"\n        Loads the credentials from their source & sets them on the object.\n\n        Subclasses should implement this method (by reading from disk, the\n        environment, the network or wherever), returning ``True`` if they were\n        found & loaded.\n\n        If not found, this method should return ``False``, indictating that the\n        ``CredentialResolver`` should fall back to the next available method.\n\n        The default implementation does nothing, assuming the user has set the\n        ``access_key/secret_key/token`` themselves.\n\n        :returns: Whether credentials were found & set\n        :rtype: Credentials\n        \"\"\"\n        return True\n\n    def _extract_creds_from_mapping(self, mapping, *key_names):\n        found = []\n        for key_name in key_names:\n            try:\n                found.append(mapping[key_name])\n            except KeyError:\n                raise PartialCredentialsError(\n                    provider=self.METHOD, cred_var=key_name\n                )\n        return found\n\n\nclass ProcessProvider(CredentialProvider):\n    METHOD = 'custom-process'\n\n    def __init__(self, profile_name, load_config, popen=subprocess.Popen):\n        self._profile_name = profile_name\n        self._load_config = load_config\n        self._loaded_config = None\n        self._popen = popen\n\n    def load(self):\n        credential_process = self._credential_process\n        if credential_process is None:\n            return\n\n        creds_dict = self._retrieve_credentials_using(credential_process)\n        if creds_dict.get('expiry_time') is not None:\n            return RefreshableCredentials.create_from_metadata(\n                creds_dict,\n                lambda: self._retrieve_credentials_using(credential_process),\n                self.METHOD,\n            )\n\n        return Credentials(\n            access_key=creds_dict['access_key'],\n            secret_key=creds_dict['secret_key'],\n            token=creds_dict.get('token'),\n            method=self.METHOD,\n        )\n\n    def _retrieve_credentials_using(self, credential_process):\n        # We're not using shell=True, so we need to pass the\n        # command and all arguments as a list.\n        process_list = compat_shell_split(credential_process)\n        p = self._popen(\n            process_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        stdout, stderr = p.communicate()\n        if p.returncode != 0:\n            raise CredentialRetrievalError(\n                provider=self.METHOD, error_msg=stderr.decode('utf-8')\n            )\n        parsed = botocore.compat.json.loads(stdout.decode('utf-8'))\n        version = parsed.get('Version', '<Version key not provided>')\n        if version != 1:\n            raise CredentialRetrievalError(\n                provider=self.METHOD,\n                error_msg=(\n                    f\"Unsupported version '{version}' for credential process \"\n                    f\"provider, supported versions: 1\"\n                ),\n            )\n        try:\n            return {\n                'access_key': parsed['AccessKeyId'],\n                'secret_key': parsed['SecretAccessKey'],\n                'token': parsed.get('SessionToken'),\n                'expiry_time': parsed.get('Expiration'),\n            }\n        except KeyError as e:\n            raise CredentialRetrievalError(\n                provider=self.METHOD,\n                error_msg=f\"Missing required key in response: {e}\",\n            )\n\n    @property\n    def _credential_process(self):\n        if self._loaded_config is None:\n            self._loaded_config = self._load_config()\n        profile_config = self._loaded_config.get('profiles', {}).get(\n            self._profile_name, {}\n        )\n        return profile_config.get('credential_process')\n\n\nclass InstanceMetadataProvider(CredentialProvider):\n    METHOD = 'iam-role'\n    CANONICAL_NAME = 'Ec2InstanceMetadata'\n\n    def __init__(self, iam_role_fetcher):\n        self._role_fetcher = iam_role_fetcher\n\n    def load(self):\n        fetcher = self._role_fetcher\n        # We do the first request, to see if we get useful data back.\n        # If not, we'll pass & move on to whatever's next in the credential\n        # chain.\n        metadata = fetcher.retrieve_iam_role_credentials()\n        if not metadata:\n            return None\n        logger.info(\n            'Found credentials from IAM Role: %s', metadata['role_name']\n        )\n        # We manually set the data here, since we already made the request &\n        # have it. When the expiry is hit, the credentials will auto-refresh\n        # themselves.\n        creds = RefreshableCredentials.create_from_metadata(\n            metadata,\n            method=self.METHOD,\n            refresh_using=fetcher.retrieve_iam_role_credentials,\n        )\n        return creds\n\n\nclass EnvProvider(CredentialProvider):\n    METHOD = 'env'\n    CANONICAL_NAME = 'Environment'\n    ACCESS_KEY = 'AWS_ACCESS_KEY_ID'\n    SECRET_KEY = 'AWS_SECRET_ACCESS_KEY'\n    # The token can come from either of these env var.\n    # AWS_SESSION_TOKEN is what other AWS SDKs have standardized on.\n    TOKENS = ['AWS_SECURITY_TOKEN', 'AWS_SESSION_TOKEN']\n    EXPIRY_TIME = 'AWS_CREDENTIAL_EXPIRATION'\n\n    def __init__(self, environ=None, mapping=None):\n        \"\"\"\n\n        :param environ: The environment variables (defaults to\n            ``os.environ`` if no value is provided).\n        :param mapping: An optional mapping of variable names to\n            environment variable names.  Use this if you want to\n            change the mapping of access_key->AWS_ACCESS_KEY_ID, etc.\n            The dict can have up to 3 keys: ``access_key``, ``secret_key``,\n            ``session_token``.\n        \"\"\"\n        if environ is None:\n            environ = os.environ\n        self.environ = environ\n        self._mapping = self._build_mapping(mapping)\n\n    def _build_mapping(self, mapping):\n        # Mapping of variable name to env var name.\n        var_mapping = {}\n        if mapping is None:\n            # Use the class var default.\n            var_mapping['access_key'] = self.ACCESS_KEY\n            var_mapping['secret_key'] = self.SECRET_KEY\n            var_mapping['token'] = self.TOKENS\n            var_mapping['expiry_time'] = self.EXPIRY_TIME\n        else:\n            var_mapping['access_key'] = mapping.get(\n                'access_key', self.ACCESS_KEY\n            )\n            var_mapping['secret_key'] = mapping.get(\n                'secret_key', self.SECRET_KEY\n            )\n            var_mapping['token'] = mapping.get('token', self.TOKENS)\n            if not isinstance(var_mapping['token'], list):\n                var_mapping['token'] = [var_mapping['token']]\n            var_mapping['expiry_time'] = mapping.get(\n                'expiry_time', self.EXPIRY_TIME\n            )\n        return var_mapping\n\n    def load(self):\n        \"\"\"\n        Search for credentials in explicit environment variables.\n        \"\"\"\n\n        access_key = self.environ.get(self._mapping['access_key'], '')\n\n        if access_key:\n            logger.info('Found credentials in environment variables.')\n            fetcher = self._create_credentials_fetcher()\n            credentials = fetcher(require_expiry=False)\n\n            expiry_time = credentials['expiry_time']\n            if expiry_time is not None:\n                expiry_time = parse(expiry_time)\n                return RefreshableCredentials(\n                    credentials['access_key'],\n                    credentials['secret_key'],\n                    credentials['token'],\n                    expiry_time,\n                    refresh_using=fetcher,\n                    method=self.METHOD,\n                )\n\n            return Credentials(\n                credentials['access_key'],\n                credentials['secret_key'],\n                credentials['token'],\n                method=self.METHOD,\n            )\n        else:\n            return None\n\n    def _create_credentials_fetcher(self):\n        mapping = self._mapping\n        method = self.METHOD\n        environ = self.environ\n\n        def fetch_credentials(require_expiry=True):\n            credentials = {}\n\n            access_key = environ.get(mapping['access_key'], '')\n            if not access_key:\n                raise PartialCredentialsError(\n                    provider=method, cred_var=mapping['access_key']\n                )\n            credentials['access_key'] = access_key\n\n            secret_key = environ.get(mapping['secret_key'], '')\n            if not secret_key:\n                raise PartialCredentialsError(\n                    provider=method, cred_var=mapping['secret_key']\n                )\n            credentials['secret_key'] = secret_key\n\n            credentials['token'] = None\n            for token_env_var in mapping['token']:\n                token = environ.get(token_env_var, '')\n                if token:\n                    credentials['token'] = token\n                    break\n\n            credentials['expiry_time'] = None\n            expiry_time = environ.get(mapping['expiry_time'], '')\n            if expiry_time:\n                credentials['expiry_time'] = expiry_time\n            if require_expiry and not expiry_time:\n                raise PartialCredentialsError(\n                    provider=method, cred_var=mapping['expiry_time']\n                )\n\n            return credentials\n\n        return fetch_credentials\n\n\nclass OriginalEC2Provider(CredentialProvider):\n    METHOD = 'ec2-credentials-file'\n    CANONICAL_NAME = 'Ec2Config'\n\n    CRED_FILE_ENV = 'AWS_CREDENTIAL_FILE'\n    ACCESS_KEY = 'AWSAccessKeyId'\n    SECRET_KEY = 'AWSSecretKey'\n\n    def __init__(self, environ=None, parser=None):\n        if environ is None:\n            environ = os.environ\n        if parser is None:\n            parser = parse_key_val_file\n        self._environ = environ\n        self._parser = parser\n\n    def load(self):\n        \"\"\"\n        Search for a credential file used by original EC2 CLI tools.\n        \"\"\"\n        if 'AWS_CREDENTIAL_FILE' in self._environ:\n            full_path = os.path.expanduser(\n                self._environ['AWS_CREDENTIAL_FILE']\n            )\n            creds = self._parser(full_path)\n            if self.ACCESS_KEY in creds:\n                logger.info('Found credentials in AWS_CREDENTIAL_FILE.')\n                access_key = creds[self.ACCESS_KEY]\n                secret_key = creds[self.SECRET_KEY]\n                # EC2 creds file doesn't support session tokens.\n                return Credentials(access_key, secret_key, method=self.METHOD)\n        else:\n            return None\n\n\nclass SharedCredentialProvider(CredentialProvider):\n    METHOD = 'shared-credentials-file'\n    CANONICAL_NAME = 'SharedCredentials'\n\n    ACCESS_KEY = 'aws_access_key_id'\n    SECRET_KEY = 'aws_secret_access_key'\n    # Same deal as the EnvProvider above.  Botocore originally supported\n    # aws_security_token, but the SDKs are standardizing on aws_session_token\n    # so we support both.\n    TOKENS = ['aws_security_token', 'aws_session_token']\n\n    def __init__(self, creds_filename, profile_name=None, ini_parser=None):\n        self._creds_filename = creds_filename\n        if profile_name is None:\n            profile_name = 'default'\n        self._profile_name = profile_name\n        if ini_parser is None:\n            ini_parser = botocore.configloader.raw_config_parse\n        self._ini_parser = ini_parser\n\n    def load(self):\n        try:\n            available_creds = self._ini_parser(self._creds_filename)\n        except ConfigNotFound:\n            return None\n        if self._profile_name in available_creds:\n            config = available_creds[self._profile_name]\n            if self.ACCESS_KEY in config:\n                logger.info(\n                    \"Found credentials in shared credentials file: %s\",\n                    self._creds_filename,\n                )\n                access_key, secret_key = self._extract_creds_from_mapping(\n                    config, self.ACCESS_KEY, self.SECRET_KEY\n                )\n                token = self._get_session_token(config)\n                return Credentials(\n                    access_key, secret_key, token, method=self.METHOD\n                )\n\n    def _get_session_token(self, config):\n        for token_envvar in self.TOKENS:\n            if token_envvar in config:\n                return config[token_envvar]\n\n\nclass ConfigProvider(CredentialProvider):\n    \"\"\"INI based config provider with profile sections.\"\"\"\n\n    METHOD = 'config-file'\n    CANONICAL_NAME = 'SharedConfig'\n\n    ACCESS_KEY = 'aws_access_key_id'\n    SECRET_KEY = 'aws_secret_access_key'\n    # Same deal as the EnvProvider above.  Botocore originally supported\n    # aws_security_token, but the SDKs are standardizing on aws_session_token\n    # so we support both.\n    TOKENS = ['aws_security_token', 'aws_session_token']\n\n    def __init__(self, config_filename, profile_name, config_parser=None):\n        \"\"\"\n\n        :param config_filename: The session configuration scoped to the current\n            profile.  This is available via ``session.config``.\n        :param profile_name: The name of the current profile.\n        :param config_parser: A config parser callable.\n\n        \"\"\"\n        self._config_filename = config_filename\n        self._profile_name = profile_name\n        if config_parser is None:\n            config_parser = botocore.configloader.load_config\n        self._config_parser = config_parser\n\n    def load(self):\n        \"\"\"\n        If there is are credentials in the configuration associated with\n        the session, use those.\n        \"\"\"\n        try:\n            full_config = self._config_parser(self._config_filename)\n        except ConfigNotFound:\n            return None\n        if self._profile_name in full_config['profiles']:\n            profile_config = full_config['profiles'][self._profile_name]\n            if self.ACCESS_KEY in profile_config:\n                logger.info(\n                    \"Credentials found in config file: %s\",\n                    self._config_filename,\n                )\n                access_key, secret_key = self._extract_creds_from_mapping(\n                    profile_config, self.ACCESS_KEY, self.SECRET_KEY\n                )\n                token = self._get_session_token(profile_config)\n                return Credentials(\n                    access_key, secret_key, token, method=self.METHOD\n                )\n        else:\n            return None\n\n    def _get_session_token(self, profile_config):\n        for token_name in self.TOKENS:\n            if token_name in profile_config:\n                return profile_config[token_name]\n\n\nclass BotoProvider(CredentialProvider):\n    METHOD = 'boto-config'\n    CANONICAL_NAME = 'Boto2Config'\n\n    BOTO_CONFIG_ENV = 'BOTO_CONFIG'\n    DEFAULT_CONFIG_FILENAMES = ['/etc/boto.cfg', '~/.boto']\n    ACCESS_KEY = 'aws_access_key_id'\n    SECRET_KEY = 'aws_secret_access_key'\n\n    def __init__(self, environ=None, ini_parser=None):\n        if environ is None:\n            environ = os.environ\n        if ini_parser is None:\n            ini_parser = botocore.configloader.raw_config_parse\n        self._environ = environ\n        self._ini_parser = ini_parser\n\n    def load(self):\n        \"\"\"\n        Look for credentials in boto config file.\n        \"\"\"\n        if self.BOTO_CONFIG_ENV in self._environ:\n            potential_locations = [self._environ[self.BOTO_CONFIG_ENV]]\n        else:\n            potential_locations = self.DEFAULT_CONFIG_FILENAMES\n        for filename in potential_locations:\n            try:\n                config = self._ini_parser(filename)\n            except ConfigNotFound:\n                # Move on to the next potential config file name.\n                continue\n            if 'Credentials' in config:\n                credentials = config['Credentials']\n                if self.ACCESS_KEY in credentials:\n                    logger.info(\n                        \"Found credentials in boto config file: %s\", filename\n                    )\n                    access_key, secret_key = self._extract_creds_from_mapping(\n                        credentials, self.ACCESS_KEY, self.SECRET_KEY\n                    )\n                    return Credentials(\n                        access_key, secret_key, method=self.METHOD\n                    )\n\n\nclass AssumeRoleProvider(CredentialProvider):\n    METHOD = 'assume-role'\n    # The AssumeRole provider is logically part of the SharedConfig and\n    # SharedCredentials providers. Since the purpose of the canonical name\n    # is to provide cross-sdk compatibility, calling code will need to be\n    # aware that either of those providers should be tied to the AssumeRole\n    # provider as much as possible.\n    CANONICAL_NAME = None\n    ROLE_CONFIG_VAR = 'role_arn'\n    WEB_IDENTITY_TOKE_FILE_VAR = 'web_identity_token_file'\n    # Credentials are considered expired (and will be refreshed) once the total\n    # remaining time left until the credentials expires is less than the\n    # EXPIRY_WINDOW.\n    EXPIRY_WINDOW_SECONDS = 60 * 15\n\n    def __init__(\n        self,\n        load_config,\n        client_creator,\n        cache,\n        profile_name,\n        prompter=getpass.getpass,\n        credential_sourcer=None,\n        profile_provider_builder=None,\n    ):\n        \"\"\"\n        :type load_config: callable\n        :param load_config: A function that accepts no arguments, and\n            when called, will return the full configuration dictionary\n            for the session (``session.full_config``).\n\n        :type client_creator: callable\n        :param client_creator: A factory function that will create\n            a client when called.  Has the same interface as\n            ``botocore.session.Session.create_client``.\n\n        :type cache: dict\n        :param cache: An object that supports ``__getitem__``,\n            ``__setitem__``, and ``__contains__``.  An example\n            of this is the ``JSONFileCache`` class in the CLI.\n\n        :type profile_name: str\n        :param profile_name: The name of the profile.\n\n        :type prompter: callable\n        :param prompter: A callable that returns input provided\n            by the user (i.e raw_input, getpass.getpass, etc.).\n\n        :type credential_sourcer: CanonicalNameCredentialSourcer\n        :param credential_sourcer: A credential provider that takes a\n            configuration, which is used to provide the source credentials\n            for the STS call.\n        \"\"\"\n        #: The cache used to first check for assumed credentials.\n        #: This is checked before making the AssumeRole API\n        #: calls and can be useful if you have short lived\n        #: scripts and you'd like to avoid calling AssumeRole\n        #: until the credentials are expired.\n        self.cache = cache\n        self._load_config = load_config\n        # client_creator is a callable that creates function.\n        # It's basically session.create_client\n        self._client_creator = client_creator\n        self._profile_name = profile_name\n        self._prompter = prompter\n        # The _loaded_config attribute will be populated from the\n        # load_config() function once the configuration is actually\n        # loaded.  The reason we go through all this instead of just\n        # requiring that the loaded_config be passed to us is to that\n        # we can defer configuration loaded until we actually try\n        # to load credentials (as opposed to when the object is\n        # instantiated).\n        self._loaded_config = {}\n        self._credential_sourcer = credential_sourcer\n        self._profile_provider_builder = profile_provider_builder\n        self._visited_profiles = [self._profile_name]\n\n    def load(self):\n        self._loaded_config = self._load_config()\n        profiles = self._loaded_config.get('profiles', {})\n        profile = profiles.get(self._profile_name, {})\n        if self._has_assume_role_config_vars(profile):\n            return self._load_creds_via_assume_role(self._profile_name)\n\n    def _has_assume_role_config_vars(self, profile):\n        return (\n            self.ROLE_CONFIG_VAR in profile\n            and\n            # We need to ensure this provider doesn't look at a profile when\n            # the profile has configuration for web identity. Simply relying on\n            # the order in the credential chain is insufficient as it doesn't\n            # prevent the case when we're doing an assume role chain.\n            self.WEB_IDENTITY_TOKE_FILE_VAR not in profile\n        )\n\n    def _load_creds_via_assume_role(self, profile_name):\n        role_config = self._get_role_config(profile_name)\n        source_credentials = self._resolve_source_credentials(\n            role_config, profile_name\n        )\n\n        extra_args = {}\n        role_session_name = role_config.get('role_session_name')\n        if role_session_name is not None:\n            extra_args['RoleSessionName'] = role_session_name\n\n        external_id = role_config.get('external_id')\n        if external_id is not None:\n            extra_args['ExternalId'] = external_id\n\n        mfa_serial = role_config.get('mfa_serial')\n        if mfa_serial is not None:\n            extra_args['SerialNumber'] = mfa_serial\n\n        duration_seconds = role_config.get('duration_seconds')\n        if duration_seconds is not None:\n            extra_args['DurationSeconds'] = duration_seconds\n\n        fetcher = AssumeRoleCredentialFetcher(\n            client_creator=self._client_creator,\n            source_credentials=source_credentials,\n            role_arn=role_config['role_arn'],\n            extra_args=extra_args,\n            mfa_prompter=self._prompter,\n            cache=self.cache,\n        )\n        refresher = fetcher.fetch_credentials\n        if mfa_serial is not None:\n            refresher = create_mfa_serial_refresher(refresher)\n\n        # The initial credentials are empty and the expiration time is set\n        # to now so that we can delay the call to assume role until it is\n        # strictly needed.\n        return DeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=refresher,\n            time_fetcher=_local_now,\n        )\n\n    def _get_role_config(self, profile_name):\n        \"\"\"Retrieves and validates the role configuration for the profile.\"\"\"\n        profiles = self._loaded_config.get('profiles', {})\n\n        profile = profiles[profile_name]\n        source_profile = profile.get('source_profile')\n        role_arn = profile['role_arn']\n        credential_source = profile.get('credential_source')\n        mfa_serial = profile.get('mfa_serial')\n        external_id = profile.get('external_id')\n        role_session_name = profile.get('role_session_name')\n        duration_seconds = profile.get('duration_seconds')\n\n        role_config = {\n            'role_arn': role_arn,\n            'external_id': external_id,\n            'mfa_serial': mfa_serial,\n            'role_session_name': role_session_name,\n            'source_profile': source_profile,\n            'credential_source': credential_source,\n        }\n\n        if duration_seconds is not None:\n            try:\n                role_config['duration_seconds'] = int(duration_seconds)\n            except ValueError:\n                pass\n\n        # Either the credential source or the source profile must be\n        # specified, but not both.\n        if credential_source is not None and source_profile is not None:\n            raise InvalidConfigError(\n                error_msg=(\n                    'The profile \"%s\" contains both source_profile and '\n                    'credential_source.' % profile_name\n                )\n            )\n        elif credential_source is None and source_profile is None:\n            raise PartialCredentialsError(\n                provider=self.METHOD,\n                cred_var='source_profile or credential_source',\n            )\n        elif credential_source is not None:\n            self._validate_credential_source(profile_name, credential_source)\n        else:\n            self._validate_source_profile(profile_name, source_profile)\n\n        return role_config\n\n    def _validate_credential_source(self, parent_profile, credential_source):\n        if self._credential_sourcer is None:\n            raise InvalidConfigError(\n                error_msg=(\n                    f\"The credential_source \\\"{credential_source}\\\" is specified \"\n                    f\"in profile \\\"{parent_profile}\\\", \"\n                    f\"but no source provider was configured.\"\n                )\n            )\n        if not self._credential_sourcer.is_supported(credential_source):\n            raise InvalidConfigError(\n                error_msg=(\n                    f\"The credential source \\\"{credential_source}\\\" referenced \"\n                    f\"in profile \\\"{parent_profile}\\\" is not valid.\"\n                )\n            )\n\n    def _source_profile_has_credentials(self, profile):\n        return any(\n            [\n                self._has_static_credentials(profile),\n                self._has_assume_role_config_vars(profile),\n            ]\n        )\n\n    def _validate_source_profile(\n        self, parent_profile_name, source_profile_name\n    ):\n        profiles = self._loaded_config.get('profiles', {})\n        if source_profile_name not in profiles:\n            raise InvalidConfigError(\n                error_msg=(\n                    f\"The source_profile \\\"{source_profile_name}\\\" referenced in \"\n                    f\"the profile \\\"{parent_profile_name}\\\" does not exist.\"\n                )\n            )\n\n        source_profile = profiles[source_profile_name]\n\n        # Make sure we aren't going into an infinite loop. If we haven't\n        # visited the profile yet, we're good.\n        if source_profile_name not in self._visited_profiles:\n            return\n\n        # If we have visited the profile and the profile isn't simply\n        # referencing itself, that's an infinite loop.\n        if source_profile_name != parent_profile_name:\n            raise InfiniteLoopConfigError(\n                source_profile=source_profile_name,\n                visited_profiles=self._visited_profiles,\n            )\n\n        # A profile is allowed to reference itself so that it can source\n        # static credentials and have configuration all in the same\n        # profile. This will only ever work for the top level assume\n        # role because the static credentials will otherwise take\n        # precedence.\n        if not self._has_static_credentials(source_profile):\n            raise InfiniteLoopConfigError(\n                source_profile=source_profile_name,\n                visited_profiles=self._visited_profiles,\n            )\n\n    def _has_static_credentials(self, profile):\n        static_keys = ['aws_secret_access_key', 'aws_access_key_id']\n        return any(static_key in profile for static_key in static_keys)\n\n    def _resolve_source_credentials(self, role_config, profile_name):\n        credential_source = role_config.get('credential_source')\n        if credential_source is not None:\n            return self._resolve_credentials_from_source(\n                credential_source, profile_name\n            )\n\n        source_profile = role_config['source_profile']\n        self._visited_profiles.append(source_profile)\n        return self._resolve_credentials_from_profile(source_profile)\n\n    def _resolve_credentials_from_profile(self, profile_name):\n        profiles = self._loaded_config.get('profiles', {})\n        profile = profiles[profile_name]\n\n        if (\n            self._has_static_credentials(profile)\n            and not self._profile_provider_builder\n        ):\n            # This is only here for backwards compatibility. If this provider\n            # isn't given a profile provider builder we still want to be able\n            # handle the basic static credential case as we would before the\n            # provile provider builder parameter was added.\n            return self._resolve_static_credentials_from_profile(profile)\n        elif self._has_static_credentials(\n            profile\n        ) or not self._has_assume_role_config_vars(profile):\n            profile_providers = self._profile_provider_builder.providers(\n                profile_name=profile_name,\n                disable_env_vars=True,\n            )\n            profile_chain = CredentialResolver(profile_providers)\n            credentials = profile_chain.load_credentials()\n            if credentials is None:\n                error_message = (\n                    'The source profile \"%s\" must have credentials.'\n                )\n                raise InvalidConfigError(\n                    error_msg=error_message % profile_name,\n                )\n            return credentials\n\n        return self._load_creds_via_assume_role(profile_name)\n\n    def _resolve_static_credentials_from_profile(self, profile):\n        try:\n            return Credentials(\n                access_key=profile['aws_access_key_id'],\n                secret_key=profile['aws_secret_access_key'],\n                token=profile.get('aws_session_token'),\n            )\n        except KeyError as e:\n            raise PartialCredentialsError(\n                provider=self.METHOD, cred_var=str(e)\n            )\n\n    def _resolve_credentials_from_source(\n        self, credential_source, profile_name\n    ):\n        credentials = self._credential_sourcer.source_credentials(\n            credential_source\n        )\n        if credentials is None:\n            raise CredentialRetrievalError(\n                provider=credential_source,\n                error_msg=(\n                    'No credentials found in credential_source referenced '\n                    'in profile %s' % profile_name\n                ),\n            )\n        return credentials\n\n\nclass AssumeRoleWithWebIdentityProvider(CredentialProvider):\n    METHOD = 'assume-role-with-web-identity'\n    CANONICAL_NAME = None\n    _CONFIG_TO_ENV_VAR = {\n        'web_identity_token_file': 'AWS_WEB_IDENTITY_TOKEN_FILE',\n        'role_session_name': 'AWS_ROLE_SESSION_NAME',\n        'role_arn': 'AWS_ROLE_ARN',\n    }\n\n    def __init__(\n        self,\n        load_config,\n        client_creator,\n        profile_name,\n        cache=None,\n        disable_env_vars=False,\n        token_loader_cls=None,\n    ):\n        self.cache = cache\n        self._load_config = load_config\n        self._client_creator = client_creator\n        self._profile_name = profile_name\n        self._profile_config = None\n        self._disable_env_vars = disable_env_vars\n        if token_loader_cls is None:\n            token_loader_cls = FileWebIdentityTokenLoader\n        self._token_loader_cls = token_loader_cls\n\n    def load(self):\n        return self._assume_role_with_web_identity()\n\n    def _get_profile_config(self, key):\n        if self._profile_config is None:\n            loaded_config = self._load_config()\n            profiles = loaded_config.get('profiles', {})\n            self._profile_config = profiles.get(self._profile_name, {})\n        return self._profile_config.get(key)\n\n    def _get_env_config(self, key):\n        if self._disable_env_vars:\n            return None\n        env_key = self._CONFIG_TO_ENV_VAR.get(key)\n        if env_key and env_key in os.environ:\n            return os.environ[env_key]\n        return None\n\n    def _get_config(self, key):\n        env_value = self._get_env_config(key)\n        if env_value is not None:\n            return env_value\n        return self._get_profile_config(key)\n\n    def _assume_role_with_web_identity(self):\n        token_path = self._get_config('web_identity_token_file')\n        if not token_path:\n            return None\n        token_loader = self._token_loader_cls(token_path)\n\n        role_arn = self._get_config('role_arn')\n        if not role_arn:\n            error_msg = (\n                'The provided profile or the current environment is '\n                'configured to assume role with web identity but has no '\n                'role ARN configured. Ensure that the profile has the role_arn'\n                'configuration set or the AWS_ROLE_ARN env var is set.'\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        extra_args = {}\n        role_session_name = self._get_config('role_session_name')\n        if role_session_name is not None:\n            extra_args['RoleSessionName'] = role_session_name\n\n        fetcher = AssumeRoleWithWebIdentityCredentialFetcher(\n            client_creator=self._client_creator,\n            web_identity_token_loader=token_loader,\n            role_arn=role_arn,\n            extra_args=extra_args,\n            cache=self.cache,\n        )\n        # The initial credentials are empty and the expiration time is set\n        # to now so that we can delay the call to assume role until it is\n        # strictly needed.\n        return DeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=fetcher.fetch_credentials,\n        )\n\n\nclass CanonicalNameCredentialSourcer:\n    def __init__(self, providers):\n        self._providers = providers\n\n    def is_supported(self, source_name):\n        \"\"\"Validates a given source name.\n\n        :type source_name: str\n        :param source_name: The value of credential_source in the config\n            file. This is the canonical name of the credential provider.\n\n        :rtype: bool\n        :returns: True if the credential provider is supported,\n            False otherwise.\n        \"\"\"\n        return source_name in [p.CANONICAL_NAME for p in self._providers]\n\n    def source_credentials(self, source_name):\n        \"\"\"Loads source credentials based on the provided configuration.\n\n        :type source_name: str\n        :param source_name: The value of credential_source in the config\n            file. This is the canonical name of the credential provider.\n\n        :rtype: Credentials\n        \"\"\"\n        source = self._get_provider(source_name)\n        if isinstance(source, CredentialResolver):\n            return source.load_credentials()\n        return source.load()\n\n    def _get_provider(self, canonical_name):\n        \"\"\"Return a credential provider by its canonical name.\n\n        :type canonical_name: str\n        :param canonical_name: The canonical name of the provider.\n\n        :raises UnknownCredentialError: Raised if no\n            credential provider by the provided name\n            is found.\n        \"\"\"\n        provider = self._get_provider_by_canonical_name(canonical_name)\n\n        # The AssumeRole provider should really be part of the SharedConfig\n        # provider rather than being its own thing, but it is not. It is\n        # effectively part of both the SharedConfig provider and the\n        # SharedCredentials provider now due to the way it behaves.\n        # Therefore if we want either of those providers we should return\n        # the AssumeRole provider with it.\n        if canonical_name.lower() in ['sharedconfig', 'sharedcredentials']:\n            assume_role_provider = self._get_provider_by_method('assume-role')\n            if assume_role_provider is not None:\n                # The SharedConfig or SharedCredentials provider may not be\n                # present if it was removed for some reason, but the\n                # AssumeRole provider could still be present. In that case,\n                # return the assume role provider by itself.\n                if provider is None:\n                    return assume_role_provider\n\n                # If both are present, return them both as a\n                # CredentialResolver so that calling code can treat them as\n                # a single entity.\n                return CredentialResolver([assume_role_provider, provider])\n\n        if provider is None:\n            raise UnknownCredentialError(name=canonical_name)\n\n        return provider\n\n    def _get_provider_by_canonical_name(self, canonical_name):\n        \"\"\"Return a credential provider by its canonical name.\n\n        This function is strict, it does not attempt to address\n        compatibility issues.\n        \"\"\"\n        for provider in self._providers:\n            name = provider.CANONICAL_NAME\n            # Canonical names are case-insensitive\n            if name and name.lower() == canonical_name.lower():\n                return provider\n\n    def _get_provider_by_method(self, method):\n        \"\"\"Return a credential provider by its METHOD name.\"\"\"\n        for provider in self._providers:\n            if provider.METHOD == method:\n                return provider\n\n\nclass ContainerProvider(CredentialProvider):\n    METHOD = 'container-role'\n    CANONICAL_NAME = 'EcsContainer'\n    ENV_VAR = 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI'\n    ENV_VAR_FULL = 'AWS_CONTAINER_CREDENTIALS_FULL_URI'\n    ENV_VAR_AUTH_TOKEN = 'AWS_CONTAINER_AUTHORIZATION_TOKEN'\n    ENV_VAR_AUTH_TOKEN_FILE = 'AWS_CONTAINER_AUTHORIZATION_TOKEN_FILE'\n\n    def __init__(self, environ=None, fetcher=None):\n        if environ is None:\n            environ = os.environ\n        if fetcher is None:\n            fetcher = ContainerMetadataFetcher()\n        self._environ = environ\n        self._fetcher = fetcher\n\n    def load(self):\n        # This cred provider is only triggered if the self.ENV_VAR is set,\n        # which only happens if you opt into this feature.\n        if self.ENV_VAR in self._environ or self.ENV_VAR_FULL in self._environ:\n            return self._retrieve_or_fail()\n\n    def _retrieve_or_fail(self):\n        if self._provided_relative_uri():\n            full_uri = self._fetcher.full_url(self._environ[self.ENV_VAR])\n        else:\n            full_uri = self._environ[self.ENV_VAR_FULL]\n        fetcher = self._create_fetcher(full_uri)\n        creds = fetcher()\n        return RefreshableCredentials(\n            access_key=creds['access_key'],\n            secret_key=creds['secret_key'],\n            token=creds['token'],\n            method=self.METHOD,\n            expiry_time=_parse_if_needed(creds['expiry_time']),\n            refresh_using=fetcher,\n        )\n\n    def _build_headers(self):\n        auth_token = None\n        if self.ENV_VAR_AUTH_TOKEN_FILE in self._environ:\n            auth_token_file_path = self._environ[self.ENV_VAR_AUTH_TOKEN_FILE]\n            with open(auth_token_file_path) as token_file:\n                auth_token = token_file.read()\n        elif self.ENV_VAR_AUTH_TOKEN in self._environ:\n            auth_token = self._environ[self.ENV_VAR_AUTH_TOKEN]\n        if auth_token is not None:\n            self._validate_auth_token(auth_token)\n            return {'Authorization': auth_token}\n\n    def _validate_auth_token(self, auth_token):\n        if \"\\r\" in auth_token or \"\\n\" in auth_token:\n            raise ValueError(\"Auth token value is not a legal header value\")\n\n    def _create_fetcher(self, full_uri, *args, **kwargs):\n        def fetch_creds():\n            try:\n                headers = self._build_headers()\n                response = self._fetcher.retrieve_full_uri(\n                    full_uri, headers=headers\n                )\n            except MetadataRetrievalError as e:\n                logger.debug(\n                    \"Error retrieving container metadata: %s\", e, exc_info=True\n                )\n                raise CredentialRetrievalError(\n                    provider=self.METHOD, error_msg=str(e)\n                )\n            return {\n                'access_key': response['AccessKeyId'],\n                'secret_key': response['SecretAccessKey'],\n                'token': response['Token'],\n                'expiry_time': response['Expiration'],\n            }\n\n        return fetch_creds\n\n    def _provided_relative_uri(self):\n        return self.ENV_VAR in self._environ\n\n\nclass CredentialResolver:\n    def __init__(self, providers):\n        \"\"\"\n\n        :param providers: A list of ``CredentialProvider`` instances.\n\n        \"\"\"\n        self.providers = providers\n\n    def insert_before(self, name, credential_provider):\n        \"\"\"\n        Inserts a new instance of ``CredentialProvider`` into the chain that\n        will be tried before an existing one.\n\n        :param name: The short name of the credentials you'd like to insert the\n            new credentials before. (ex. ``env`` or ``config``). Existing names\n            & ordering can be discovered via ``self.available_methods``.\n        :type name: string\n\n        :param cred_instance: An instance of the new ``Credentials`` object\n            you'd like to add to the chain.\n        :type cred_instance: A subclass of ``Credentials``\n        \"\"\"\n        try:\n            offset = [p.METHOD for p in self.providers].index(name)\n        except ValueError:\n            raise UnknownCredentialError(name=name)\n        self.providers.insert(offset, credential_provider)\n\n    def insert_after(self, name, credential_provider):\n        \"\"\"\n        Inserts a new type of ``Credentials`` instance into the chain that will\n        be tried after an existing one.\n\n        :param name: The short name of the credentials you'd like to insert the\n            new credentials after. (ex. ``env`` or ``config``). Existing names\n            & ordering can be discovered via ``self.available_methods``.\n        :type name: string\n\n        :param cred_instance: An instance of the new ``Credentials`` object\n            you'd like to add to the chain.\n        :type cred_instance: A subclass of ``Credentials``\n        \"\"\"\n        offset = self._get_provider_offset(name)\n        self.providers.insert(offset + 1, credential_provider)\n\n    def remove(self, name):\n        \"\"\"\n        Removes a given ``Credentials`` instance from the chain.\n\n        :param name: The short name of the credentials instance to remove.\n        :type name: string\n        \"\"\"\n        available_methods = [p.METHOD for p in self.providers]\n        if name not in available_methods:\n            # It's not present. Fail silently.\n            return\n\n        offset = available_methods.index(name)\n        self.providers.pop(offset)\n\n    def get_provider(self, name):\n        \"\"\"Return a credential provider by name.\n\n        :type name: str\n        :param name: The name of the provider.\n\n        :raises UnknownCredentialError: Raised if no\n            credential provider by the provided name\n            is found.\n        \"\"\"\n        return self.providers[self._get_provider_offset(name)]\n\n    def _get_provider_offset(self, name):\n        try:\n            return [p.METHOD for p in self.providers].index(name)\n        except ValueError:\n            raise UnknownCredentialError(name=name)\n\n    def load_credentials(self):\n        \"\"\"\n        Goes through the credentials chain, returning the first ``Credentials``\n        that could be loaded.\n        \"\"\"\n        # First provider to return a non-None response wins.\n        for provider in self.providers:\n            logger.debug(\"Looking for credentials via: %s\", provider.METHOD)\n            creds = provider.load()\n            if creds is not None:\n                return creds\n\n        # If we got here, no credentials could be found.\n        # This feels like it should be an exception, but historically, ``None``\n        # is returned.\n        #\n        # +1\n        # -js\n        return None\n\n\nclass SSOCredentialFetcher(CachedCredentialFetcher):\n    _UTC_DATE_FORMAT = '%Y-%m-%dT%H:%M:%SZ'\n\n    def __init__(\n        self,\n        start_url,\n        sso_region,\n        role_name,\n        account_id,\n        client_creator,\n        token_loader=None,\n        cache=None,\n        expiry_window_seconds=None,\n        token_provider=None,\n        sso_session_name=None,\n    ):\n        self._client_creator = client_creator\n        self._sso_region = sso_region\n        self._role_name = role_name\n        self._account_id = account_id\n        self._start_url = start_url\n        self._token_loader = token_loader\n        self._token_provider = token_provider\n        self._sso_session_name = sso_session_name\n        super().__init__(cache, expiry_window_seconds)\n\n    def _create_cache_key(self):\n        \"\"\"Create a predictable cache key for the current configuration.\n\n        The cache key is intended to be compatible with file names.\n        \"\"\"\n        args = {\n            'roleName': self._role_name,\n            'accountId': self._account_id,\n        }\n        if self._sso_session_name:\n            args['sessionName'] = self._sso_session_name\n        else:\n            args['startUrl'] = self._start_url\n        # NOTE: It would be good to hoist this cache key construction logic\n        # into the CachedCredentialFetcher class as we should be consistent.\n        # Unfortunately, the current assume role fetchers that sub class don't\n        # pass separators resulting in non-minified JSON. In the long term,\n        # all fetchers should use the below caching scheme.\n        args = json.dumps(args, sort_keys=True, separators=(',', ':'))\n        argument_hash = sha1(args.encode('utf-8')).hexdigest()\n        return self._make_file_safe(argument_hash)\n\n    def _parse_timestamp(self, timestamp_ms):\n        # fromtimestamp expects seconds so: milliseconds / 1000 = seconds\n        timestamp_seconds = timestamp_ms / 1000.0\n        timestamp = datetime.datetime.fromtimestamp(timestamp_seconds, tzutc())\n        return timestamp.strftime(self._UTC_DATE_FORMAT)\n\n    def _get_credentials(self):\n        \"\"\"Get credentials by calling SSO get role credentials.\"\"\"\n        config = Config(\n            signature_version=UNSIGNED,\n            region_name=self._sso_region,\n        )\n        client = self._client_creator('sso', config=config)\n        if self._token_provider:\n            initial_token_data = self._token_provider.load_token()\n            token = initial_token_data.get_frozen_token().token\n        else:\n            token = self._token_loader(self._start_url)['accessToken']\n\n        kwargs = {\n            'roleName': self._role_name,\n            'accountId': self._account_id,\n            'accessToken': token,\n        }\n        try:\n            response = client.get_role_credentials(**kwargs)\n        except client.exceptions.UnauthorizedException:\n            raise UnauthorizedSSOTokenError()\n        credentials = response['roleCredentials']\n\n        credentials = {\n            'ProviderType': 'sso',\n            'Credentials': {\n                'AccessKeyId': credentials['accessKeyId'],\n                'SecretAccessKey': credentials['secretAccessKey'],\n                'SessionToken': credentials['sessionToken'],\n                'Expiration': self._parse_timestamp(credentials['expiration']),\n            },\n        }\n        return credentials\n\n\nclass SSOProvider(CredentialProvider):\n    METHOD = 'sso'\n\n    _SSO_TOKEN_CACHE_DIR = os.path.expanduser(\n        os.path.join('~', '.aws', 'sso', 'cache')\n    )\n    _PROFILE_REQUIRED_CONFIG_VARS = (\n        'sso_role_name',\n        'sso_account_id',\n    )\n    _SSO_REQUIRED_CONFIG_VARS = (\n        'sso_start_url',\n        'sso_region',\n    )\n    _ALL_REQUIRED_CONFIG_VARS = (\n        _PROFILE_REQUIRED_CONFIG_VARS + _SSO_REQUIRED_CONFIG_VARS\n    )\n\n    def __init__(\n        self,\n        load_config,\n        client_creator,\n        profile_name,\n        cache=None,\n        token_cache=None,\n        token_provider=None,\n    ):\n        if token_cache is None:\n            token_cache = JSONFileCache(self._SSO_TOKEN_CACHE_DIR)\n        self._token_cache = token_cache\n        self._token_provider = token_provider\n        if cache is None:\n            cache = {}\n        self.cache = cache\n        self._load_config = load_config\n        self._client_creator = client_creator\n        self._profile_name = profile_name\n\n    def _load_sso_config(self):\n        loaded_config = self._load_config()\n        profiles = loaded_config.get('profiles', {})\n        profile_name = self._profile_name\n        profile_config = profiles.get(self._profile_name, {})\n        sso_sessions = loaded_config.get('sso_sessions', {})\n\n        # Role name & Account ID indicate the cred provider should be used\n        if all(\n            c not in profile_config for c in self._PROFILE_REQUIRED_CONFIG_VARS\n        ):\n            return None\n\n        resolved_config, extra_reqs = self._resolve_sso_session_reference(\n            profile_config, sso_sessions\n        )\n\n        config = {}\n        missing_config_vars = []\n        all_required_configs = self._ALL_REQUIRED_CONFIG_VARS + extra_reqs\n        for config_var in all_required_configs:\n            if config_var in resolved_config:\n                config[config_var] = resolved_config[config_var]\n            else:\n                missing_config_vars.append(config_var)\n\n        if missing_config_vars:\n            missing = ', '.join(missing_config_vars)\n            raise InvalidConfigError(\n                error_msg=(\n                    'The profile \"%s\" is configured to use SSO but is missing '\n                    'required configuration: %s' % (profile_name, missing)\n                )\n            )\n        return config\n\n    def _resolve_sso_session_reference(self, profile_config, sso_sessions):\n        sso_session_name = profile_config.get('sso_session')\n        if sso_session_name is None:\n            # No reference to resolve, proceed with legacy flow\n            return profile_config, ()\n\n        if sso_session_name not in sso_sessions:\n            error_msg = f'The specified sso-session does not exist: \"{sso_session_name}\"'\n            raise InvalidConfigError(error_msg=error_msg)\n\n        config = profile_config.copy()\n        session = sso_sessions[sso_session_name]\n        for config_var, val in session.items():\n            # Validate any keys referenced in both profile and sso_session match\n            if config.get(config_var, val) != val:\n                error_msg = (\n                    f\"The value for {config_var} is inconsistent between \"\n                    f\"profile ({config[config_var]}) and sso-session ({val}).\"\n                )\n                raise InvalidConfigError(error_msg=error_msg)\n            config[config_var] = val\n        return config, ('sso_session',)\n\n    def load(self):\n        sso_config = self._load_sso_config()\n        if not sso_config:\n            return None\n\n        fetcher_kwargs = {\n            'start_url': sso_config['sso_start_url'],\n            'sso_region': sso_config['sso_region'],\n            'role_name': sso_config['sso_role_name'],\n            'account_id': sso_config['sso_account_id'],\n            'client_creator': self._client_creator,\n            'token_loader': SSOTokenLoader(cache=self._token_cache),\n            'cache': self.cache,\n        }\n        if 'sso_session' in sso_config:\n            fetcher_kwargs['sso_session_name'] = sso_config['sso_session']\n            fetcher_kwargs['token_provider'] = self._token_provider\n\n        sso_fetcher = SSOCredentialFetcher(**fetcher_kwargs)\n\n        return DeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=sso_fetcher.fetch_credentials,\n        )\n", "botocore/monitoring.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport logging\nimport re\nimport time\n\nfrom botocore.compat import ensure_bytes, ensure_unicode, urlparse\nfrom botocore.retryhandler import EXCEPTION_MAP as RETRYABLE_EXCEPTIONS\n\nlogger = logging.getLogger(__name__)\n\n\nclass Monitor:\n    _EVENTS_TO_REGISTER = [\n        'before-parameter-build',\n        'request-created',\n        'response-received',\n        'after-call',\n        'after-call-error',\n    ]\n\n    def __init__(self, adapter, publisher):\n        \"\"\"Abstraction for monitoring clients API calls\n\n        :param adapter: An adapter that takes event emitter events\n            and produces monitor events\n\n        :param publisher: A publisher for generated monitor events\n        \"\"\"\n        self._adapter = adapter\n        self._publisher = publisher\n\n    def register(self, event_emitter):\n        \"\"\"Register an event emitter to the monitor\"\"\"\n        for event_to_register in self._EVENTS_TO_REGISTER:\n            event_emitter.register_last(event_to_register, self.capture)\n\n    def capture(self, event_name, **payload):\n        \"\"\"Captures an incoming event from the event emitter\n\n        It will feed an event emitter event to the monitor's adaptor to create\n        a monitor event and then publish that event to the monitor's publisher.\n        \"\"\"\n        try:\n            monitor_event = self._adapter.feed(event_name, payload)\n            if monitor_event:\n                self._publisher.publish(monitor_event)\n        except Exception as e:\n            logger.debug(\n                'Exception %s raised by client monitor in handling event %s',\n                e,\n                event_name,\n                exc_info=True,\n            )\n\n\nclass MonitorEventAdapter:\n    def __init__(self, time=time.time):\n        \"\"\"Adapts event emitter events to produce monitor events\n\n        :type time: callable\n        :param time: A callable that produces the current time\n        \"\"\"\n        self._time = time\n\n    def feed(self, emitter_event_name, emitter_payload):\n        \"\"\"Feed an event emitter event to generate a monitor event\n\n        :type emitter_event_name: str\n        :param emitter_event_name: The name of the event emitted\n\n        :type emitter_payload: dict\n        :param emitter_payload: The payload to associated to the event\n            emitted\n\n        :rtype: BaseMonitorEvent\n        :returns: A monitor event based on the event emitter events\n            fired\n        \"\"\"\n        return self._get_handler(emitter_event_name)(**emitter_payload)\n\n    def _get_handler(self, event_name):\n        return getattr(\n            self, '_handle_' + event_name.split('.')[0].replace('-', '_')\n        )\n\n    def _handle_before_parameter_build(self, model, context, **kwargs):\n        context['current_api_call_event'] = APICallEvent(\n            service=model.service_model.service_id,\n            operation=model.wire_name,\n            timestamp=self._get_current_time(),\n        )\n\n    def _handle_request_created(self, request, **kwargs):\n        context = request.context\n        new_attempt_event = context[\n            'current_api_call_event'\n        ].new_api_call_attempt(timestamp=self._get_current_time())\n        new_attempt_event.request_headers = request.headers\n        new_attempt_event.url = request.url\n        context['current_api_call_attempt_event'] = new_attempt_event\n\n    def _handle_response_received(\n        self, parsed_response, context, exception, **kwargs\n    ):\n        attempt_event = context.pop('current_api_call_attempt_event')\n        attempt_event.latency = self._get_latency(attempt_event)\n        if parsed_response is not None:\n            attempt_event.http_status_code = parsed_response[\n                'ResponseMetadata'\n            ]['HTTPStatusCode']\n            attempt_event.response_headers = parsed_response[\n                'ResponseMetadata'\n            ]['HTTPHeaders']\n            attempt_event.parsed_error = parsed_response.get('Error')\n        else:\n            attempt_event.wire_exception = exception\n        return attempt_event\n\n    def _handle_after_call(self, context, parsed, **kwargs):\n        context['current_api_call_event'].retries_exceeded = parsed[\n            'ResponseMetadata'\n        ].get('MaxAttemptsReached', False)\n        return self._complete_api_call(context)\n\n    def _handle_after_call_error(self, context, exception, **kwargs):\n        # If the after-call-error was emitted and the error being raised\n        # was a retryable connection error, then the retries must have exceeded\n        # for that exception as this event gets emitted **after** retries\n        # happen.\n        context[\n            'current_api_call_event'\n        ].retries_exceeded = self._is_retryable_exception(exception)\n        return self._complete_api_call(context)\n\n    def _is_retryable_exception(self, exception):\n        return isinstance(\n            exception, tuple(RETRYABLE_EXCEPTIONS['GENERAL_CONNECTION_ERROR'])\n        )\n\n    def _complete_api_call(self, context):\n        call_event = context.pop('current_api_call_event')\n        call_event.latency = self._get_latency(call_event)\n        return call_event\n\n    def _get_latency(self, event):\n        return self._get_current_time() - event.timestamp\n\n    def _get_current_time(self):\n        return int(self._time() * 1000)\n\n\nclass BaseMonitorEvent:\n    def __init__(self, service, operation, timestamp):\n        \"\"\"Base monitor event\n\n        :type service: str\n        :param service: A string identifying the service associated to\n            the event\n\n        :type operation: str\n        :param operation: A string identifying the operation of service\n            associated to the event\n\n        :type timestamp: int\n        :param timestamp: Epoch time in milliseconds from when the event began\n        \"\"\"\n        self.service = service\n        self.operation = operation\n        self.timestamp = timestamp\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.__dict__!r})'\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.__dict__ == other.__dict__\n        return False\n\n\nclass APICallEvent(BaseMonitorEvent):\n    def __init__(\n        self,\n        service,\n        operation,\n        timestamp,\n        latency=None,\n        attempts=None,\n        retries_exceeded=False,\n    ):\n        \"\"\"Monitor event for a single API call\n\n        This event corresponds to a single client method call, which includes\n        every HTTP requests attempt made in order to complete the client call\n\n        :type service: str\n        :param service: A string identifying the service associated to\n            the event\n\n        :type operation: str\n        :param operation: A string identifying the operation of service\n            associated to the event\n\n        :type timestamp: int\n        :param timestamp: Epoch time in milliseconds from when the event began\n\n        :type latency: int\n        :param latency: The time in milliseconds to complete the client call\n\n        :type attempts: list\n        :param attempts: The list of APICallAttempts associated to the\n            APICall\n\n        :type retries_exceeded: bool\n        :param retries_exceeded: True if API call exceeded retries. False\n            otherwise\n        \"\"\"\n        super().__init__(\n            service=service, operation=operation, timestamp=timestamp\n        )\n        self.latency = latency\n        self.attempts = attempts\n        if attempts is None:\n            self.attempts = []\n        self.retries_exceeded = retries_exceeded\n\n    def new_api_call_attempt(self, timestamp):\n        \"\"\"Instantiates APICallAttemptEvent associated to the APICallEvent\n\n        :type timestamp: int\n        :param timestamp: Epoch time in milliseconds to associate to the\n            APICallAttemptEvent\n        \"\"\"\n        attempt_event = APICallAttemptEvent(\n            service=self.service, operation=self.operation, timestamp=timestamp\n        )\n        self.attempts.append(attempt_event)\n        return attempt_event\n\n\nclass APICallAttemptEvent(BaseMonitorEvent):\n    def __init__(\n        self,\n        service,\n        operation,\n        timestamp,\n        latency=None,\n        url=None,\n        http_status_code=None,\n        request_headers=None,\n        response_headers=None,\n        parsed_error=None,\n        wire_exception=None,\n    ):\n        \"\"\"Monitor event for a single API call attempt\n\n        This event corresponds to a single HTTP request attempt in completing\n        the entire client method call.\n\n        :type service: str\n        :param service: A string identifying the service associated to\n            the event\n\n        :type operation: str\n        :param operation: A string identifying the operation of service\n            associated to the event\n\n        :type timestamp: int\n        :param timestamp: Epoch time in milliseconds from when the HTTP request\n            started\n\n        :type latency: int\n        :param latency: The time in milliseconds to complete the HTTP request\n            whether it succeeded or failed\n\n        :type url: str\n        :param url: The URL the attempt was sent to\n\n        :type http_status_code: int\n        :param http_status_code: The HTTP status code of the HTTP response\n            if there was a response\n\n        :type request_headers: dict\n        :param request_headers: The HTTP headers sent in making the HTTP\n            request\n\n        :type response_headers: dict\n        :param response_headers: The HTTP headers returned in the HTTP response\n            if there was a response\n\n        :type parsed_error: dict\n        :param parsed_error: The error parsed if the service returned an\n            error back\n\n        :type wire_exception: Exception\n        :param wire_exception: The exception raised in sending the HTTP\n            request (i.e. ConnectionError)\n        \"\"\"\n        super().__init__(\n            service=service, operation=operation, timestamp=timestamp\n        )\n        self.latency = latency\n        self.url = url\n        self.http_status_code = http_status_code\n        self.request_headers = request_headers\n        self.response_headers = response_headers\n        self.parsed_error = parsed_error\n        self.wire_exception = wire_exception\n\n\nclass CSMSerializer:\n    _MAX_CLIENT_ID_LENGTH = 255\n    _MAX_EXCEPTION_CLASS_LENGTH = 128\n    _MAX_ERROR_CODE_LENGTH = 128\n    _MAX_USER_AGENT_LENGTH = 256\n    _MAX_MESSAGE_LENGTH = 512\n    _RESPONSE_HEADERS_TO_EVENT_ENTRIES = {\n        'x-amzn-requestid': 'XAmznRequestId',\n        'x-amz-request-id': 'XAmzRequestId',\n        'x-amz-id-2': 'XAmzId2',\n    }\n    _AUTH_REGEXS = {\n        'v4': re.compile(\n            r'AWS4-HMAC-SHA256 '\n            r'Credential=(?P<access_key>\\w+)/\\d+/'\n            r'(?P<signing_region>[a-z0-9-]+)/'\n        ),\n        's3': re.compile(r'AWS (?P<access_key>\\w+):'),\n    }\n    _SERIALIZEABLE_EVENT_PROPERTIES = [\n        'service',\n        'operation',\n        'timestamp',\n        'attempts',\n        'latency',\n        'retries_exceeded',\n        'url',\n        'request_headers',\n        'http_status_code',\n        'response_headers',\n        'parsed_error',\n        'wire_exception',\n    ]\n\n    def __init__(self, csm_client_id):\n        \"\"\"Serializes monitor events to CSM (Client Side Monitoring) format\n\n        :type csm_client_id: str\n        :param csm_client_id: The application identifier to associate\n            to the serialized events\n        \"\"\"\n        self._validate_client_id(csm_client_id)\n        self.csm_client_id = csm_client_id\n\n    def _validate_client_id(self, csm_client_id):\n        if len(csm_client_id) > self._MAX_CLIENT_ID_LENGTH:\n            raise ValueError(\n                f'The value provided for csm_client_id: {csm_client_id} exceeds '\n                f'the maximum length of {self._MAX_CLIENT_ID_LENGTH} characters'\n            )\n\n    def serialize(self, event):\n        \"\"\"Serializes a monitor event to the CSM format\n\n        :type event: BaseMonitorEvent\n        :param event: The event to serialize to bytes\n\n        :rtype: bytes\n        :returns: The CSM serialized form of the event\n        \"\"\"\n        event_dict = self._get_base_event_dict(event)\n        event_type = self._get_event_type(event)\n        event_dict['Type'] = event_type\n        for attr in self._SERIALIZEABLE_EVENT_PROPERTIES:\n            value = getattr(event, attr, None)\n            if value is not None:\n                getattr(self, '_serialize_' + attr)(\n                    value, event_dict, event_type=event_type\n                )\n        return ensure_bytes(json.dumps(event_dict, separators=(',', ':')))\n\n    def _get_base_event_dict(self, event):\n        return {\n            'Version': 1,\n            'ClientId': self.csm_client_id,\n        }\n\n    def _serialize_service(self, service, event_dict, **kwargs):\n        event_dict['Service'] = service\n\n    def _serialize_operation(self, operation, event_dict, **kwargs):\n        event_dict['Api'] = operation\n\n    def _serialize_timestamp(self, timestamp, event_dict, **kwargs):\n        event_dict['Timestamp'] = timestamp\n\n    def _serialize_attempts(self, attempts, event_dict, **kwargs):\n        event_dict['AttemptCount'] = len(attempts)\n        if attempts:\n            self._add_fields_from_last_attempt(event_dict, attempts[-1])\n\n    def _add_fields_from_last_attempt(self, event_dict, last_attempt):\n        if last_attempt.request_headers:\n            # It does not matter which attempt to use to grab the region\n            # for the ApiCall event, but SDKs typically do the last one.\n            region = self._get_region(last_attempt.request_headers)\n            if region is not None:\n                event_dict['Region'] = region\n            event_dict['UserAgent'] = self._get_user_agent(\n                last_attempt.request_headers\n            )\n        if last_attempt.http_status_code is not None:\n            event_dict['FinalHttpStatusCode'] = last_attempt.http_status_code\n        if last_attempt.parsed_error is not None:\n            self._serialize_parsed_error(\n                last_attempt.parsed_error, event_dict, 'ApiCall'\n            )\n        if last_attempt.wire_exception is not None:\n            self._serialize_wire_exception(\n                last_attempt.wire_exception, event_dict, 'ApiCall'\n            )\n\n    def _serialize_latency(self, latency, event_dict, event_type):\n        if event_type == 'ApiCall':\n            event_dict['Latency'] = latency\n        elif event_type == 'ApiCallAttempt':\n            event_dict['AttemptLatency'] = latency\n\n    def _serialize_retries_exceeded(\n        self, retries_exceeded, event_dict, **kwargs\n    ):\n        event_dict['MaxRetriesExceeded'] = 1 if retries_exceeded else 0\n\n    def _serialize_url(self, url, event_dict, **kwargs):\n        event_dict['Fqdn'] = urlparse(url).netloc\n\n    def _serialize_request_headers(\n        self, request_headers, event_dict, **kwargs\n    ):\n        event_dict['UserAgent'] = self._get_user_agent(request_headers)\n        if self._is_signed(request_headers):\n            event_dict['AccessKey'] = self._get_access_key(request_headers)\n        region = self._get_region(request_headers)\n        if region is not None:\n            event_dict['Region'] = region\n        if 'X-Amz-Security-Token' in request_headers:\n            event_dict['SessionToken'] = request_headers[\n                'X-Amz-Security-Token'\n            ]\n\n    def _serialize_http_status_code(\n        self, http_status_code, event_dict, **kwargs\n    ):\n        event_dict['HttpStatusCode'] = http_status_code\n\n    def _serialize_response_headers(\n        self, response_headers, event_dict, **kwargs\n    ):\n        for header, entry in self._RESPONSE_HEADERS_TO_EVENT_ENTRIES.items():\n            if header in response_headers:\n                event_dict[entry] = response_headers[header]\n\n    def _serialize_parsed_error(\n        self, parsed_error, event_dict, event_type, **kwargs\n    ):\n        field_prefix = 'Final' if event_type == 'ApiCall' else ''\n        event_dict[field_prefix + 'AwsException'] = self._truncate(\n            parsed_error['Code'], self._MAX_ERROR_CODE_LENGTH\n        )\n        event_dict[field_prefix + 'AwsExceptionMessage'] = self._truncate(\n            parsed_error['Message'], self._MAX_MESSAGE_LENGTH\n        )\n\n    def _serialize_wire_exception(\n        self, wire_exception, event_dict, event_type, **kwargs\n    ):\n        field_prefix = 'Final' if event_type == 'ApiCall' else ''\n        event_dict[field_prefix + 'SdkException'] = self._truncate(\n            wire_exception.__class__.__name__, self._MAX_EXCEPTION_CLASS_LENGTH\n        )\n        event_dict[field_prefix + 'SdkExceptionMessage'] = self._truncate(\n            str(wire_exception), self._MAX_MESSAGE_LENGTH\n        )\n\n    def _get_event_type(self, event):\n        if isinstance(event, APICallEvent):\n            return 'ApiCall'\n        elif isinstance(event, APICallAttemptEvent):\n            return 'ApiCallAttempt'\n\n    def _get_access_key(self, request_headers):\n        auth_val = self._get_auth_value(request_headers)\n        _, auth_match = self._get_auth_match(auth_val)\n        return auth_match.group('access_key')\n\n    def _get_region(self, request_headers):\n        if not self._is_signed(request_headers):\n            return None\n        auth_val = self._get_auth_value(request_headers)\n        signature_version, auth_match = self._get_auth_match(auth_val)\n        if signature_version != 'v4':\n            return None\n        return auth_match.group('signing_region')\n\n    def _get_user_agent(self, request_headers):\n        return self._truncate(\n            ensure_unicode(request_headers.get('User-Agent', '')),\n            self._MAX_USER_AGENT_LENGTH,\n        )\n\n    def _is_signed(self, request_headers):\n        return 'Authorization' in request_headers\n\n    def _get_auth_value(self, request_headers):\n        return ensure_unicode(request_headers['Authorization'])\n\n    def _get_auth_match(self, auth_val):\n        for signature_version, regex in self._AUTH_REGEXS.items():\n            match = regex.match(auth_val)\n            if match:\n                return signature_version, match\n        return None, None\n\n    def _truncate(self, text, max_length):\n        if len(text) > max_length:\n            logger.debug(\n                'Truncating following value to maximum length of ' '%s: %s',\n                text,\n                max_length,\n            )\n            return text[:max_length]\n        return text\n\n\nclass SocketPublisher:\n    _MAX_MONITOR_EVENT_LENGTH = 8 * 1024\n\n    def __init__(self, socket, host, port, serializer):\n        \"\"\"Publishes monitor events to a socket\n\n        :type socket: socket.socket\n        :param socket: The socket object to use to publish events\n\n        :type host: string\n        :param host: The host to send events to\n\n        :type port: integer\n        :param port: The port on the host to send events to\n\n        :param serializer: The serializer to use to serialize the event\n            to a form that can be published to the socket. This must\n            have a `serialize()` method that accepts a monitor event\n            and return bytes\n        \"\"\"\n        self._socket = socket\n        self._address = (host, port)\n        self._serializer = serializer\n\n    def publish(self, event):\n        \"\"\"Publishes a specified monitor event\n\n        :type event: BaseMonitorEvent\n        :param event: The monitor event to be sent\n            over the publisher's socket to the desired address.\n        \"\"\"\n        serialized_event = self._serializer.serialize(event)\n        if len(serialized_event) > self._MAX_MONITOR_EVENT_LENGTH:\n            logger.debug(\n                'Serialized event of size %s exceeds the maximum length '\n                'allowed: %s. Not sending event to socket.',\n                len(serialized_event),\n                self._MAX_MONITOR_EVENT_LENGTH,\n            )\n            return\n        self._socket.sendto(serialized_event, self._address)\n", "botocore/configprovider.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"This module contains the interface for controlling how configuration\nis loaded.\n\"\"\"\nimport copy\nimport logging\nimport os\n\nfrom botocore import utils\nfrom botocore.exceptions import InvalidConfigError\n\nlogger = logging.getLogger(__name__)\n\n\n#: A default dictionary that maps the logical names for session variables\n#: to the specific environment variables and configuration file names\n#: that contain the values for these variables.\n#: When creating a new Session object, you can pass in your own dictionary\n#: to remap the logical names or to add new logical names.  You can then\n#: get the current value for these variables by using the\n#: ``get_config_variable`` method of the :class:`botocore.session.Session`\n#: class.\n#: These form the keys of the dictionary.  The values in the dictionary\n#: are tuples of (<config_name>, <environment variable>, <default value>,\n#: <conversion func>).\n#: The conversion func is a function that takes the configuration value\n#: as an argument and returns the converted value.  If this value is\n#: None, then the configuration value is returned unmodified.  This\n#: conversion function can be used to type convert config values to\n#: values other than the default values of strings.\n#: The ``profile`` and ``config_file`` variables should always have a\n#: None value for the first entry in the tuple because it doesn't make\n#: sense to look inside the config file for the location of the config\n#: file or for the default profile to use.\n#: The ``config_name`` is the name to look for in the configuration file,\n#: the ``env var`` is the OS environment variable (``os.environ``) to\n#: use, and ``default_value`` is the value to use if no value is otherwise\n#: found.\n#: NOTE: Fixing the spelling of this variable would be a breaking change.\n#: Please leave as is.\nBOTOCORE_DEFAUT_SESSION_VARIABLES = {\n    # logical:  config_file, env_var,        default_value, conversion_func\n    'profile': (None, ['AWS_DEFAULT_PROFILE', 'AWS_PROFILE'], None, None),\n    'region': ('region', 'AWS_DEFAULT_REGION', None, None),\n    'data_path': ('data_path', 'AWS_DATA_PATH', None, None),\n    'config_file': (None, 'AWS_CONFIG_FILE', '~/.aws/config', None),\n    'ca_bundle': ('ca_bundle', 'AWS_CA_BUNDLE', None, None),\n    'api_versions': ('api_versions', None, {}, None),\n    # This is the shared credentials file amongst sdks.\n    'credentials_file': (\n        None,\n        'AWS_SHARED_CREDENTIALS_FILE',\n        '~/.aws/credentials',\n        None,\n    ),\n    # These variables only exist in the config file.\n    # This is the number of seconds until we time out a request to\n    # the instance metadata service.\n    'metadata_service_timeout': (\n        'metadata_service_timeout',\n        'AWS_METADATA_SERVICE_TIMEOUT',\n        1,\n        int,\n    ),\n    # This is the number of request attempts we make until we give\n    # up trying to retrieve data from the instance metadata service.\n    'metadata_service_num_attempts': (\n        'metadata_service_num_attempts',\n        'AWS_METADATA_SERVICE_NUM_ATTEMPTS',\n        1,\n        int,\n    ),\n    'ec2_metadata_service_endpoint': (\n        'ec2_metadata_service_endpoint',\n        'AWS_EC2_METADATA_SERVICE_ENDPOINT',\n        None,\n        None,\n    ),\n    'ec2_metadata_service_endpoint_mode': (\n        'ec2_metadata_service_endpoint_mode',\n        'AWS_EC2_METADATA_SERVICE_ENDPOINT_MODE',\n        None,\n        None,\n    ),\n    'ec2_metadata_v1_disabled': (\n        'ec2_metadata_v1_disabled',\n        'AWS_EC2_METADATA_V1_DISABLED',\n        False,\n        utils.ensure_boolean,\n    ),\n    'imds_use_ipv6': (\n        'imds_use_ipv6',\n        'AWS_IMDS_USE_IPV6',\n        False,\n        utils.ensure_boolean,\n    ),\n    'use_dualstack_endpoint': (\n        'use_dualstack_endpoint',\n        'AWS_USE_DUALSTACK_ENDPOINT',\n        None,\n        utils.ensure_boolean,\n    ),\n    'use_fips_endpoint': (\n        'use_fips_endpoint',\n        'AWS_USE_FIPS_ENDPOINT',\n        None,\n        utils.ensure_boolean,\n    ),\n    'ignore_configured_endpoint_urls': (\n        'ignore_configured_endpoint_urls',\n        'AWS_IGNORE_CONFIGURED_ENDPOINT_URLS',\n        None,\n        utils.ensure_boolean,\n    ),\n    'parameter_validation': ('parameter_validation', None, True, None),\n    # Client side monitoring configurations.\n    # Note: These configurations are considered internal to botocore.\n    # Do not use them until publicly documented.\n    'csm_enabled': (\n        'csm_enabled',\n        'AWS_CSM_ENABLED',\n        False,\n        utils.ensure_boolean,\n    ),\n    'csm_host': ('csm_host', 'AWS_CSM_HOST', '127.0.0.1', None),\n    'csm_port': ('csm_port', 'AWS_CSM_PORT', 31000, int),\n    'csm_client_id': ('csm_client_id', 'AWS_CSM_CLIENT_ID', '', None),\n    # Endpoint discovery configuration\n    'endpoint_discovery_enabled': (\n        'endpoint_discovery_enabled',\n        'AWS_ENDPOINT_DISCOVERY_ENABLED',\n        'auto',\n        None,\n    ),\n    'sts_regional_endpoints': (\n        'sts_regional_endpoints',\n        'AWS_STS_REGIONAL_ENDPOINTS',\n        'legacy',\n        None,\n    ),\n    'retry_mode': ('retry_mode', 'AWS_RETRY_MODE', 'legacy', None),\n    'defaults_mode': ('defaults_mode', 'AWS_DEFAULTS_MODE', 'legacy', None),\n    # We can't have a default here for v1 because we need to defer to\n    # whatever the defaults are in _retry.json.\n    'max_attempts': ('max_attempts', 'AWS_MAX_ATTEMPTS', None, int),\n    'user_agent_appid': ('sdk_ua_app_id', 'AWS_SDK_UA_APP_ID', None, None),\n    'request_min_compression_size_bytes': (\n        'request_min_compression_size_bytes',\n        'AWS_REQUEST_MIN_COMPRESSION_SIZE_BYTES',\n        10240,\n        None,\n    ),\n    'disable_request_compression': (\n        'disable_request_compression',\n        'AWS_DISABLE_REQUEST_COMPRESSION',\n        False,\n        utils.ensure_boolean,\n    ),\n}\n# A mapping for the s3 specific configuration vars. These are the configuration\n# vars that typically go in the s3 section of the config file. This mapping\n# follows the same schema as the previous session variable mapping.\nDEFAULT_S3_CONFIG_VARS = {\n    'addressing_style': (('s3', 'addressing_style'), None, None, None),\n    'use_accelerate_endpoint': (\n        ('s3', 'use_accelerate_endpoint'),\n        None,\n        None,\n        utils.ensure_boolean,\n    ),\n    'use_dualstack_endpoint': (\n        ('s3', 'use_dualstack_endpoint'),\n        None,\n        None,\n        utils.ensure_boolean,\n    ),\n    'payload_signing_enabled': (\n        ('s3', 'payload_signing_enabled'),\n        None,\n        None,\n        utils.ensure_boolean,\n    ),\n    'use_arn_region': (\n        ['s3_use_arn_region', ('s3', 'use_arn_region')],\n        'AWS_S3_USE_ARN_REGION',\n        None,\n        utils.ensure_boolean,\n    ),\n    'us_east_1_regional_endpoint': (\n        [\n            's3_us_east_1_regional_endpoint',\n            ('s3', 'us_east_1_regional_endpoint'),\n        ],\n        'AWS_S3_US_EAST_1_REGIONAL_ENDPOINT',\n        None,\n        None,\n    ),\n    's3_disable_multiregion_access_points': (\n        ('s3', 's3_disable_multiregion_access_points'),\n        'AWS_S3_DISABLE_MULTIREGION_ACCESS_POINTS',\n        None,\n        utils.ensure_boolean,\n    ),\n}\n# A mapping for the proxy specific configuration vars. These are\n# used to configure how botocore interacts with proxy setups while\n# sending requests.\nDEFAULT_PROXIES_CONFIG_VARS = {\n    'proxy_ca_bundle': ('proxy_ca_bundle', None, None, None),\n    'proxy_client_cert': ('proxy_client_cert', None, None, None),\n    'proxy_use_forwarding_for_https': (\n        'proxy_use_forwarding_for_https',\n        None,\n        None,\n        utils.normalize_boolean,\n    ),\n}\n\n\ndef create_botocore_default_config_mapping(session):\n    chain_builder = ConfigChainFactory(session=session)\n    config_mapping = _create_config_chain_mapping(\n        chain_builder, BOTOCORE_DEFAUT_SESSION_VARIABLES\n    )\n    config_mapping['s3'] = SectionConfigProvider(\n        's3',\n        session,\n        _create_config_chain_mapping(chain_builder, DEFAULT_S3_CONFIG_VARS),\n    )\n    config_mapping['proxies_config'] = SectionConfigProvider(\n        'proxies_config',\n        session,\n        _create_config_chain_mapping(\n            chain_builder, DEFAULT_PROXIES_CONFIG_VARS\n        ),\n    )\n    return config_mapping\n\n\ndef _create_config_chain_mapping(chain_builder, config_variables):\n    mapping = {}\n    for logical_name, config in config_variables.items():\n        mapping[logical_name] = chain_builder.create_config_chain(\n            instance_name=logical_name,\n            env_var_names=config[1],\n            config_property_names=config[0],\n            default=config[2],\n            conversion_func=config[3],\n        )\n    return mapping\n\n\nclass DefaultConfigResolver:\n    def __init__(self, default_config_data):\n        self._base_default_config = default_config_data['base']\n        self._modes = default_config_data['modes']\n        self._resolved_default_configurations = {}\n\n    def _resolve_default_values_by_mode(self, mode):\n        default_config = self._base_default_config.copy()\n        modifications = self._modes.get(mode)\n\n        for config_var in modifications:\n            default_value = default_config[config_var]\n            modification_dict = modifications[config_var]\n            modification = list(modification_dict.keys())[0]\n            modification_value = modification_dict[modification]\n            if modification == 'multiply':\n                default_value *= modification_value\n            elif modification == 'add':\n                default_value += modification_value\n            elif modification == 'override':\n                default_value = modification_value\n            default_config[config_var] = default_value\n        return default_config\n\n    def get_default_modes(self):\n        default_modes = ['legacy', 'auto']\n        default_modes.extend(self._modes.keys())\n        return default_modes\n\n    def get_default_config_values(self, mode):\n        if mode not in self._resolved_default_configurations:\n            defaults = self._resolve_default_values_by_mode(mode)\n            self._resolved_default_configurations[mode] = defaults\n        return self._resolved_default_configurations[mode]\n\n\nclass ConfigChainFactory:\n    \"\"\"Factory class to create our most common configuration chain case.\n\n    This is a convenience class to construct configuration chains that follow\n    our most common pattern. This is to prevent ordering them incorrectly,\n    and to make the config chain construction more readable.\n    \"\"\"\n\n    def __init__(self, session, environ=None):\n        \"\"\"Initialize a ConfigChainFactory.\n\n        :type session: :class:`botocore.session.Session`\n        :param session: This is the session that should be used to look up\n            values from the config file.\n\n        :type environ: dict\n        :param environ: A mapping to use for environment variables. If this\n            is not provided it will default to use os.environ.\n        \"\"\"\n        self._session = session\n        if environ is None:\n            environ = os.environ\n        self._environ = environ\n\n    def create_config_chain(\n        self,\n        instance_name=None,\n        env_var_names=None,\n        config_property_names=None,\n        default=None,\n        conversion_func=None,\n    ):\n        \"\"\"Build a config chain following the standard botocore pattern.\n\n        In botocore most of our config chains follow the the precendence:\n        session_instance_variables, environment, config_file, default_value.\n\n        This is a convenience function for creating a chain that follow\n        that precendence.\n\n        :type instance_name: str\n        :param instance_name: This indicates what session instance variable\n            corresponds to this config value. If it is None it will not be\n            added to the chain.\n\n        :type env_var_names: str or list of str or None\n        :param env_var_names: One or more environment variable names to\n            search for this value. They are searched in order. If it is None\n            it will not be added to the chain.\n\n        :type config_property_names: str/tuple or list of str/tuple or None\n        :param config_property_names: One of more strings or tuples\n            representing the name of the key in the config file for this\n            config option. They are searched in order. If it is None it will\n            not be added to the chain.\n\n        :type default: Any\n        :param default: Any constant value to be returned.\n\n        :type conversion_func: None or callable\n        :param conversion_func: If this value is None then it has no effect on\n            the return type. Otherwise, it is treated as a function that will\n            conversion_func our provided type.\n\n        :rvalue: ConfigChain\n        :returns: A ConfigChain that resolves in the order env_var_names ->\n            config_property_name -> default. Any values that were none are\n            omitted form the chain.\n        \"\"\"\n        providers = []\n        if instance_name is not None:\n            providers.append(\n                InstanceVarProvider(\n                    instance_var=instance_name, session=self._session\n                )\n            )\n        if env_var_names is not None:\n            providers.extend(self._get_env_providers(env_var_names))\n        if config_property_names is not None:\n            providers.extend(\n                self._get_scoped_config_providers(config_property_names)\n            )\n        if default is not None:\n            providers.append(ConstantProvider(value=default))\n\n        return ChainProvider(\n            providers=providers,\n            conversion_func=conversion_func,\n        )\n\n    def _get_env_providers(self, env_var_names):\n        env_var_providers = []\n        if not isinstance(env_var_names, list):\n            env_var_names = [env_var_names]\n        for env_var_name in env_var_names:\n            env_var_providers.append(\n                EnvironmentProvider(name=env_var_name, env=self._environ)\n            )\n        return env_var_providers\n\n    def _get_scoped_config_providers(self, config_property_names):\n        scoped_config_providers = []\n        if not isinstance(config_property_names, list):\n            config_property_names = [config_property_names]\n        for config_property_name in config_property_names:\n            scoped_config_providers.append(\n                ScopedConfigProvider(\n                    config_var_name=config_property_name,\n                    session=self._session,\n                )\n            )\n        return scoped_config_providers\n\n\nclass ConfigValueStore:\n    \"\"\"The ConfigValueStore object stores configuration values.\"\"\"\n\n    def __init__(self, mapping=None):\n        \"\"\"Initialize a ConfigValueStore.\n\n        :type mapping: dict\n        :param mapping: The mapping parameter is a map of string to a subclass\n            of BaseProvider. When a config variable is asked for via the\n            get_config_variable method, the corresponding provider will be\n            invoked to load the value.\n        \"\"\"\n        self._overrides = {}\n        self._mapping = {}\n        if mapping is not None:\n            for logical_name, provider in mapping.items():\n                self.set_config_provider(logical_name, provider)\n\n    def __deepcopy__(self, memo):\n        config_store = ConfigValueStore(copy.deepcopy(self._mapping, memo))\n        for logical_name, override_value in self._overrides.items():\n            config_store.set_config_variable(logical_name, override_value)\n\n        return config_store\n\n    def __copy__(self):\n        config_store = ConfigValueStore(copy.copy(self._mapping))\n        for logical_name, override_value in self._overrides.items():\n            config_store.set_config_variable(logical_name, override_value)\n\n        return config_store\n\n    def get_config_variable(self, logical_name):\n        \"\"\"\n        Retrieve the value associeated with the specified logical_name\n        from the corresponding provider. If no value is found None will\n        be returned.\n\n        :type logical_name: str\n        :param logical_name: The logical name of the session variable\n            you want to retrieve.  This name will be mapped to the\n            appropriate environment variable name for this session as\n            well as the appropriate config file entry.\n\n        :returns: value of variable or None if not defined.\n        \"\"\"\n        if logical_name in self._overrides:\n            return self._overrides[logical_name]\n        if logical_name not in self._mapping:\n            return None\n        provider = self._mapping[logical_name]\n        return provider.provide()\n\n    def get_config_provider(self, logical_name):\n        \"\"\"\n        Retrieve the provider associated with the specified logical_name.\n        If no provider is found None will be returned.\n\n        :type logical_name: str\n        :param logical_name: The logical name of the session variable\n            you want to retrieve.  This name will be mapped to the\n            appropriate environment variable name for this session as\n            well as the appropriate config file entry.\n\n        :returns: configuration provider or None if not defined.\n        \"\"\"\n        if (\n            logical_name in self._overrides\n            or logical_name not in self._mapping\n        ):\n            return None\n        provider = self._mapping[logical_name]\n        return provider\n\n    def set_config_variable(self, logical_name, value):\n        \"\"\"Set a configuration variable to a specific value.\n\n        By using this method, you can override the normal lookup\n        process used in ``get_config_variable`` by explicitly setting\n        a value.  Subsequent calls to ``get_config_variable`` will\n        use the ``value``.  This gives you per-session specific\n        configuration values.\n\n        ::\n            >>> # Assume logical name 'foo' maps to env var 'FOO'\n            >>> os.environ['FOO'] = 'myvalue'\n            >>> s.get_config_variable('foo')\n            'myvalue'\n            >>> s.set_config_variable('foo', 'othervalue')\n            >>> s.get_config_variable('foo')\n            'othervalue'\n\n        :type logical_name: str\n        :param logical_name: The logical name of the session variable\n            you want to set.  These are the keys in ``SESSION_VARIABLES``.\n\n        :param value: The value to associate with the config variable.\n        \"\"\"\n        self._overrides[logical_name] = value\n\n    def clear_config_variable(self, logical_name):\n        \"\"\"Remove an override config variable from the session.\n\n        :type logical_name: str\n        :param logical_name: The name of the parameter to clear the override\n            value from.\n        \"\"\"\n        self._overrides.pop(logical_name, None)\n\n    def set_config_provider(self, logical_name, provider):\n        \"\"\"Set the provider for a config value.\n\n        This provides control over how a particular configuration value is\n        loaded. This replaces the provider for ``logical_name`` with the new\n        ``provider``.\n\n        :type logical_name: str\n        :param logical_name: The name of the config value to change the config\n            provider for.\n\n        :type provider: :class:`botocore.configprovider.BaseProvider`\n        :param provider: The new provider that should be responsible for\n            providing a value for the config named ``logical_name``.\n        \"\"\"\n        self._mapping[logical_name] = provider\n\n\nclass SmartDefaultsConfigStoreFactory:\n    def __init__(self, default_config_resolver, imds_region_provider):\n        self._default_config_resolver = default_config_resolver\n        self._imds_region_provider = imds_region_provider\n        # Initializing _instance_metadata_region as None so we\n        # can fetch region in a lazy fashion only when needed.\n        self._instance_metadata_region = None\n\n    def merge_smart_defaults(self, config_store, mode, region_name):\n        if mode == 'auto':\n            mode = self.resolve_auto_mode(region_name)\n        default_configs = (\n            self._default_config_resolver.get_default_config_values(mode)\n        )\n        for config_var in default_configs:\n            config_value = default_configs[config_var]\n            method = getattr(self, f'_set_{config_var}', None)\n            if method:\n                method(config_store, config_value)\n\n    def resolve_auto_mode(self, region_name):\n        current_region = None\n        if os.environ.get('AWS_EXECUTION_ENV'):\n            default_region = os.environ.get('AWS_DEFAULT_REGION')\n            current_region = os.environ.get('AWS_REGION', default_region)\n        if not current_region:\n            if self._instance_metadata_region:\n                current_region = self._instance_metadata_region\n            else:\n                try:\n                    current_region = self._imds_region_provider.provide()\n                    self._instance_metadata_region = current_region\n                except Exception:\n                    pass\n\n        if current_region:\n            if region_name == current_region:\n                return 'in-region'\n            else:\n                return 'cross-region'\n        return 'standard'\n\n    def _update_provider(self, config_store, variable, value):\n        original_provider = config_store.get_config_provider(variable)\n        default_provider = ConstantProvider(value)\n        if isinstance(original_provider, ChainProvider):\n            chain_provider_copy = copy.deepcopy(original_provider)\n            chain_provider_copy.set_default_provider(default_provider)\n            default_provider = chain_provider_copy\n        elif isinstance(original_provider, BaseProvider):\n            default_provider = ChainProvider(\n                providers=[original_provider, default_provider]\n            )\n        config_store.set_config_provider(variable, default_provider)\n\n    def _update_section_provider(\n        self, config_store, section_name, variable, value\n    ):\n        section_provider_copy = copy.deepcopy(\n            config_store.get_config_provider(section_name)\n        )\n        section_provider_copy.set_default_provider(\n            variable, ConstantProvider(value)\n        )\n        config_store.set_config_provider(section_name, section_provider_copy)\n\n    def _set_retryMode(self, config_store, value):\n        self._update_provider(config_store, 'retry_mode', value)\n\n    def _set_stsRegionalEndpoints(self, config_store, value):\n        self._update_provider(config_store, 'sts_regional_endpoints', value)\n\n    def _set_s3UsEast1RegionalEndpoints(self, config_store, value):\n        self._update_section_provider(\n            config_store, 's3', 'us_east_1_regional_endpoint', value\n        )\n\n    def _set_connectTimeoutInMillis(self, config_store, value):\n        self._update_provider(config_store, 'connect_timeout', value / 1000)\n\n\nclass BaseProvider:\n    \"\"\"Base class for configuration value providers.\n\n    A configuration provider has some method of providing a configuration\n    value.\n    \"\"\"\n\n    def provide(self):\n        \"\"\"Provide a config value.\"\"\"\n        raise NotImplementedError('provide')\n\n\nclass ChainProvider(BaseProvider):\n    \"\"\"This provider wraps one or more other providers.\n\n    Each provider in the chain is called, the first one returning a non-None\n    value is then returned.\n    \"\"\"\n\n    def __init__(self, providers=None, conversion_func=None):\n        \"\"\"Initalize a ChainProvider.\n\n        :type providers: list\n        :param providers: The initial list of providers to check for values\n            when invoked.\n\n        :type conversion_func: None or callable\n        :param conversion_func: If this value is None then it has no affect on\n            the return type. Otherwise, it is treated as a function that will\n            transform provided value.\n        \"\"\"\n        if providers is None:\n            providers = []\n        self._providers = providers\n        self._conversion_func = conversion_func\n\n    def __deepcopy__(self, memo):\n        return ChainProvider(\n            copy.deepcopy(self._providers, memo), self._conversion_func\n        )\n\n    def provide(self):\n        \"\"\"Provide the value from the first provider to return non-None.\n\n        Each provider in the chain has its provide method called. The first\n        one in the chain to return a non-None value is the returned from the\n        ChainProvider. When no non-None value is found, None is returned.\n        \"\"\"\n        for provider in self._providers:\n            value = provider.provide()\n            if value is not None:\n                return self._convert_type(value)\n        return None\n\n    def set_default_provider(self, default_provider):\n        if self._providers and isinstance(\n            self._providers[-1], ConstantProvider\n        ):\n            self._providers[-1] = default_provider\n        else:\n            self._providers.append(default_provider)\n\n        num_of_constants = sum(\n            isinstance(provider, ConstantProvider)\n            for provider in self._providers\n        )\n        if num_of_constants > 1:\n            logger.info(\n                'ChainProvider object contains multiple '\n                'instances of ConstantProvider objects'\n            )\n\n    def _convert_type(self, value):\n        if self._conversion_func is not None:\n            return self._conversion_func(value)\n        return value\n\n    def __repr__(self):\n        return '[%s]' % ', '.join([str(p) for p in self._providers])\n\n\nclass InstanceVarProvider(BaseProvider):\n    \"\"\"This class loads config values from the session instance vars.\"\"\"\n\n    def __init__(self, instance_var, session):\n        \"\"\"Initialize InstanceVarProvider.\n\n        :type instance_var: str\n        :param instance_var: The instance variable to load from the session.\n\n        :type session: :class:`botocore.session.Session`\n        :param session: The botocore session to get the loaded configuration\n            file variables from.\n        \"\"\"\n        self._instance_var = instance_var\n        self._session = session\n\n    def __deepcopy__(self, memo):\n        return InstanceVarProvider(\n            copy.deepcopy(self._instance_var, memo), self._session\n        )\n\n    def provide(self):\n        \"\"\"Provide a config value from the session instance vars.\"\"\"\n        instance_vars = self._session.instance_variables()\n        value = instance_vars.get(self._instance_var)\n        return value\n\n    def __repr__(self):\n        return 'InstanceVarProvider(instance_var={}, session={})'.format(\n            self._instance_var,\n            self._session,\n        )\n\n\nclass ScopedConfigProvider(BaseProvider):\n    def __init__(self, config_var_name, session):\n        \"\"\"Initialize ScopedConfigProvider.\n\n        :type config_var_name: str or tuple\n        :param config_var_name: The name of the config variable to load from\n            the configuration file. If the value is a tuple, it must only\n            consist of two items, where the first item represents the section\n            and the second item represents the config var name in the section.\n\n        :type session: :class:`botocore.session.Session`\n        :param session: The botocore session to get the loaded configuration\n            file variables from.\n        \"\"\"\n        self._config_var_name = config_var_name\n        self._session = session\n\n    def __deepcopy__(self, memo):\n        return ScopedConfigProvider(\n            copy.deepcopy(self._config_var_name, memo), self._session\n        )\n\n    def provide(self):\n        \"\"\"Provide a value from a config file property.\"\"\"\n        scoped_config = self._session.get_scoped_config()\n        if isinstance(self._config_var_name, tuple):\n            section_config = scoped_config.get(self._config_var_name[0])\n            if not isinstance(section_config, dict):\n                return None\n            return section_config.get(self._config_var_name[1])\n        return scoped_config.get(self._config_var_name)\n\n    def __repr__(self):\n        return 'ScopedConfigProvider(config_var_name={}, session={})'.format(\n            self._config_var_name,\n            self._session,\n        )\n\n\nclass EnvironmentProvider(BaseProvider):\n    \"\"\"This class loads config values from environment variables.\"\"\"\n\n    def __init__(self, name, env):\n        \"\"\"Initialize with the keys in the dictionary to check.\n\n        :type name: str\n        :param name: The key with that name will be loaded and returned.\n\n        :type env: dict\n        :param env: Environment variables dictionary to get variables from.\n        \"\"\"\n        self._name = name\n        self._env = env\n\n    def __deepcopy__(self, memo):\n        return EnvironmentProvider(\n            copy.deepcopy(self._name, memo), copy.deepcopy(self._env, memo)\n        )\n\n    def provide(self):\n        \"\"\"Provide a config value from a source dictionary.\"\"\"\n        if self._name in self._env:\n            return self._env[self._name]\n        return None\n\n    def __repr__(self):\n        return f'EnvironmentProvider(name={self._name}, env={self._env})'\n\n\nclass SectionConfigProvider(BaseProvider):\n    \"\"\"Provides a dictionary from a section in the scoped config\n\n    This is useful for retrieving scoped config variables (i.e. s3) that have\n    their own set of config variables and resolving logic.\n    \"\"\"\n\n    def __init__(self, section_name, session, override_providers=None):\n        self._section_name = section_name\n        self._session = session\n        self._scoped_config_provider = ScopedConfigProvider(\n            self._section_name, self._session\n        )\n        self._override_providers = override_providers\n        if self._override_providers is None:\n            self._override_providers = {}\n\n    def __deepcopy__(self, memo):\n        return SectionConfigProvider(\n            copy.deepcopy(self._section_name, memo),\n            self._session,\n            copy.deepcopy(self._override_providers, memo),\n        )\n\n    def provide(self):\n        section_config = self._scoped_config_provider.provide()\n        if section_config and not isinstance(section_config, dict):\n            logger.debug(\n                \"The %s config key is not a dictionary type, \"\n                \"ignoring its value of: %s\",\n                self._section_name,\n                section_config,\n            )\n            return None\n        for section_config_var, provider in self._override_providers.items():\n            provider_val = provider.provide()\n            if provider_val is not None:\n                if section_config is None:\n                    section_config = {}\n                section_config[section_config_var] = provider_val\n        return section_config\n\n    def set_default_provider(self, key, default_provider):\n        provider = self._override_providers.get(key)\n        if isinstance(provider, ChainProvider):\n            provider.set_default_provider(default_provider)\n            return\n        elif isinstance(provider, BaseProvider):\n            default_provider = ChainProvider(\n                providers=[provider, default_provider]\n            )\n        self._override_providers[key] = default_provider\n\n    def __repr__(self):\n        return (\n            f'SectionConfigProvider(section_name={self._section_name}, '\n            f'session={self._session}, '\n            f'override_providers={self._override_providers})'\n        )\n\n\nclass ConstantProvider(BaseProvider):\n    \"\"\"This provider provides a constant value.\"\"\"\n\n    def __init__(self, value):\n        self._value = value\n\n    def __deepcopy__(self, memo):\n        return ConstantProvider(copy.deepcopy(self._value, memo))\n\n    def provide(self):\n        \"\"\"Provide the constant value given during initialization.\"\"\"\n        return self._value\n\n    def __repr__(self):\n        return 'ConstantProvider(value=%s)' % self._value\n\n\nclass ConfiguredEndpointProvider(BaseProvider):\n    \"\"\"Lookup an endpoint URL from environment variable or shared config file.\n\n    NOTE: This class is considered private and is subject to abrupt breaking\n    changes or removal without prior announcement. Please do not use it\n    directly.\n    \"\"\"\n\n    _ENDPOINT_URL_LOOKUP_ORDER = [\n        'environment_service',\n        'environment_global',\n        'config_service',\n        'config_global',\n    ]\n\n    def __init__(\n        self,\n        full_config,\n        scoped_config,\n        client_name,\n        environ=None,\n    ):\n        \"\"\"Initialize a ConfiguredEndpointProviderChain.\n\n        :type full_config: dict\n        :param full_config: This is the dict representing the full\n            configuration file.\n\n        :type scoped_config: dict\n        :param scoped_config: This is the dict representing the configuration\n            for the current profile for the session.\n\n        :type client_name: str\n        :param client_name: The name used to instantiate a client using\n            botocore.session.Session.create_client.\n\n        :type environ: dict\n        :param environ: A mapping to use for environment variables. If this\n            is not provided it will default to use os.environ.\n        \"\"\"\n        self._full_config = full_config\n        self._scoped_config = scoped_config\n        self._client_name = client_name\n        self._transformed_service_id = self._get_snake_case_service_id(\n            self._client_name\n        )\n        if environ is None:\n            environ = os.environ\n        self._environ = environ\n\n    def provide(self):\n        \"\"\"Lookup the configured endpoint URL.\n\n        The order is:\n\n        1. The value provided by a service-specific environment variable.\n        2. The value provided by the global endpoint environment variable\n           (AWS_ENDPOINT_URL).\n        3. The value provided by a service-specific parameter from a services\n           definition section in the shared configuration file.\n        4. The value provided by the global parameter from a services\n           definition section in the shared configuration file.\n        \"\"\"\n        for location in self._ENDPOINT_URL_LOOKUP_ORDER:\n            logger.debug(\n                'Looking for endpoint for %s via: %s',\n                self._client_name,\n                location,\n            )\n\n            endpoint_url = getattr(self, f'_get_endpoint_url_{location}')()\n\n            if endpoint_url:\n                logger.info(\n                    'Found endpoint for %s via: %s.',\n                    self._client_name,\n                    location,\n                )\n                return endpoint_url\n\n        logger.debug('No configured endpoint found.')\n        return None\n\n    def _get_snake_case_service_id(self, client_name):\n        # Get the service ID without loading the service data file, accounting\n        # for any aliases and standardizing the names with hyphens.\n        client_name = utils.SERVICE_NAME_ALIASES.get(client_name, client_name)\n        hyphenized_service_id = (\n            utils.CLIENT_NAME_TO_HYPHENIZED_SERVICE_ID_OVERRIDES.get(\n                client_name, client_name\n            )\n        )\n        return hyphenized_service_id.replace('-', '_')\n\n    def _get_service_env_var_name(self):\n        transformed_service_id_env = self._transformed_service_id.upper()\n        return f'AWS_ENDPOINT_URL_{transformed_service_id_env}'\n\n    def _get_services_config(self):\n        if 'services' not in self._scoped_config:\n            return {}\n\n        section_name = self._scoped_config['services']\n        services_section = self._full_config.get('services', {}).get(\n            section_name\n        )\n\n        if not services_section:\n            error_msg = (\n                f'The profile is configured to use the services '\n                f'section but the \"{section_name}\" services '\n                f'configuration does not exist.'\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        return services_section\n\n    def _get_endpoint_url_config_service(self):\n        snakecase_service_id = self._transformed_service_id.lower()\n        return (\n            self._get_services_config()\n            .get(snakecase_service_id, {})\n            .get('endpoint_url')\n        )\n\n    def _get_endpoint_url_config_global(self):\n        return self._scoped_config.get('endpoint_url')\n\n    def _get_endpoint_url_environment_service(self):\n        return EnvironmentProvider(\n            name=self._get_service_env_var_name(), env=self._environ\n        ).provide()\n\n    def _get_endpoint_url_environment_global(self):\n        return EnvironmentProvider(\n            name='AWS_ENDPOINT_URL', env=self._environ\n        ).provide()\n", "botocore/client.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nfrom botocore import waiter, xform_name\nfrom botocore.args import ClientArgsCreator\nfrom botocore.auth import AUTH_TYPE_MAPS\nfrom botocore.awsrequest import prepare_request_dict\nfrom botocore.compress import maybe_compress_request\nfrom botocore.config import Config\nfrom botocore.credentials import RefreshableCredentials\nfrom botocore.discovery import (\n    EndpointDiscoveryHandler,\n    EndpointDiscoveryManager,\n    block_endpoint_discovery_required_operations,\n)\nfrom botocore.docs.docstring import ClientMethodDocstring, PaginatorDocstring\nfrom botocore.exceptions import (\n    DataNotFoundError,\n    InvalidEndpointDiscoveryConfigurationError,\n    OperationNotPageableError,\n    UnknownServiceError,\n    UnknownSignatureVersionError,\n)\nfrom botocore.history import get_global_history_recorder\nfrom botocore.hooks import first_non_none_response\nfrom botocore.httpchecksum import (\n    apply_request_checksum,\n    resolve_checksum_context,\n)\nfrom botocore.model import ServiceModel\nfrom botocore.paginate import Paginator\nfrom botocore.retries import adaptive, standard\nfrom botocore.useragent import UserAgentString\nfrom botocore.utils import (\n    CachedProperty,\n    EventbridgeSignerSetter,\n    S3ControlArnParamHandlerv2,\n    S3ExpressIdentityResolver,\n    S3RegionRedirectorv2,\n    ensure_boolean,\n    get_service_module_name,\n)\n\n# Keep these imported.  There's pre-existing code that uses:\n# \"from botocore.client import UNSIGNED\"\n# \"from botocore.client import ClientError\"\n# etc.\nfrom botocore.exceptions import ClientError  # noqa\nfrom botocore.utils import S3ArnParamHandler  # noqa\nfrom botocore.utils import S3ControlArnParamHandler  # noqa\nfrom botocore.utils import S3ControlEndpointSetter  # noqa\nfrom botocore.utils import S3EndpointSetter  # noqa\nfrom botocore.utils import S3RegionRedirector  # noqa\nfrom botocore import UNSIGNED  # noqa\n\n\n_LEGACY_SIGNATURE_VERSIONS = frozenset(\n    (\n        'v2',\n        'v3',\n        'v3https',\n        'v4',\n        's3',\n        's3v4',\n    )\n)\n\n\nlogger = logging.getLogger(__name__)\nhistory_recorder = get_global_history_recorder()\n\n\nclass ClientCreator:\n    \"\"\"Creates client objects for a service.\"\"\"\n\n    def __init__(\n        self,\n        loader,\n        endpoint_resolver,\n        user_agent,\n        event_emitter,\n        retry_handler_factory,\n        retry_config_translator,\n        response_parser_factory=None,\n        exceptions_factory=None,\n        config_store=None,\n        user_agent_creator=None,\n    ):\n        self._loader = loader\n        self._endpoint_resolver = endpoint_resolver\n        self._user_agent = user_agent\n        self._event_emitter = event_emitter\n        self._retry_handler_factory = retry_handler_factory\n        self._retry_config_translator = retry_config_translator\n        self._response_parser_factory = response_parser_factory\n        self._exceptions_factory = exceptions_factory\n        # TODO: Migrate things away from scoped_config in favor of the\n        # config_store.  The config store can pull things from both the scoped\n        # config and environment variables (and potentially more in the\n        # future).\n        self._config_store = config_store\n        self._user_agent_creator = user_agent_creator\n\n    def create_client(\n        self,\n        service_name,\n        region_name,\n        is_secure=True,\n        endpoint_url=None,\n        verify=None,\n        credentials=None,\n        scoped_config=None,\n        api_version=None,\n        client_config=None,\n        auth_token=None,\n    ):\n        responses = self._event_emitter.emit(\n            'choose-service-name', service_name=service_name\n        )\n        service_name = first_non_none_response(responses, default=service_name)\n        service_model = self._load_service_model(service_name, api_version)\n        try:\n            endpoints_ruleset_data = self._load_service_endpoints_ruleset(\n                service_name, api_version\n            )\n            partition_data = self._loader.load_data('partitions')\n        except UnknownServiceError:\n            endpoints_ruleset_data = None\n            partition_data = None\n            logger.info(\n                'No endpoints ruleset found for service %s, falling back to '\n                'legacy endpoint routing.',\n                service_name,\n            )\n\n        cls = self._create_client_class(service_name, service_model)\n        region_name, client_config = self._normalize_fips_region(\n            region_name, client_config\n        )\n        endpoint_bridge = ClientEndpointBridge(\n            self._endpoint_resolver,\n            scoped_config,\n            client_config,\n            service_signing_name=service_model.metadata.get('signingName'),\n            config_store=self._config_store,\n            service_signature_version=service_model.metadata.get(\n                'signatureVersion'\n            ),\n        )\n        client_args = self._get_client_args(\n            service_model,\n            region_name,\n            is_secure,\n            endpoint_url,\n            verify,\n            credentials,\n            scoped_config,\n            client_config,\n            endpoint_bridge,\n            auth_token,\n            endpoints_ruleset_data,\n            partition_data,\n        )\n        service_client = cls(**client_args)\n        self._register_retries(service_client)\n        self._register_s3_events(\n            client=service_client,\n            endpoint_bridge=None,\n            endpoint_url=None,\n            client_config=client_config,\n            scoped_config=scoped_config,\n        )\n        self._register_s3express_events(client=service_client)\n        self._register_s3_control_events(client=service_client)\n        self._register_endpoint_discovery(\n            service_client, endpoint_url, client_config\n        )\n        return service_client\n\n    def create_client_class(self, service_name, api_version=None):\n        service_model = self._load_service_model(service_name, api_version)\n        return self._create_client_class(service_name, service_model)\n\n    def _create_client_class(self, service_name, service_model):\n        class_attributes = self._create_methods(service_model)\n        py_name_to_operation_name = self._create_name_mapping(service_model)\n        class_attributes['_PY_TO_OP_NAME'] = py_name_to_operation_name\n        bases = [BaseClient]\n        service_id = service_model.service_id.hyphenize()\n        self._event_emitter.emit(\n            'creating-client-class.%s' % service_id,\n            class_attributes=class_attributes,\n            base_classes=bases,\n        )\n        class_name = get_service_module_name(service_model)\n        cls = type(str(class_name), tuple(bases), class_attributes)\n        return cls\n\n    def _normalize_fips_region(self, region_name, client_config):\n        if region_name is not None:\n            normalized_region_name = region_name.replace('fips-', '').replace(\n                '-fips', ''\n            )\n            # If region has been transformed then set flag\n            if normalized_region_name != region_name:\n                config_use_fips_endpoint = Config(use_fips_endpoint=True)\n                if client_config:\n                    # Keeping endpoint setting client specific\n                    client_config = client_config.merge(\n                        config_use_fips_endpoint\n                    )\n                else:\n                    client_config = config_use_fips_endpoint\n                logger.warning(\n                    'transforming region from %s to %s and setting '\n                    'use_fips_endpoint to true. client should not '\n                    'be configured with a fips psuedo region.'\n                    % (region_name, normalized_region_name)\n                )\n                region_name = normalized_region_name\n        return region_name, client_config\n\n    def _load_service_model(self, service_name, api_version=None):\n        json_model = self._loader.load_service_model(\n            service_name, 'service-2', api_version=api_version\n        )\n        service_model = ServiceModel(json_model, service_name=service_name)\n        return service_model\n\n    def _load_service_endpoints_ruleset(self, service_name, api_version=None):\n        return self._loader.load_service_model(\n            service_name, 'endpoint-rule-set-1', api_version=api_version\n        )\n\n    def _register_retries(self, client):\n        retry_mode = client.meta.config.retries['mode']\n        if retry_mode == 'standard':\n            self._register_v2_standard_retries(client)\n        elif retry_mode == 'adaptive':\n            self._register_v2_standard_retries(client)\n            self._register_v2_adaptive_retries(client)\n        elif retry_mode == 'legacy':\n            self._register_legacy_retries(client)\n\n    def _register_v2_standard_retries(self, client):\n        max_attempts = client.meta.config.retries.get('total_max_attempts')\n        kwargs = {'client': client}\n        if max_attempts is not None:\n            kwargs['max_attempts'] = max_attempts\n        standard.register_retry_handler(**kwargs)\n\n    def _register_v2_adaptive_retries(self, client):\n        adaptive.register_retry_handler(client)\n\n    def _register_legacy_retries(self, client):\n        endpoint_prefix = client.meta.service_model.endpoint_prefix\n        service_id = client.meta.service_model.service_id\n        service_event_name = service_id.hyphenize()\n\n        # First, we load the entire retry config for all services,\n        # then pull out just the information we need.\n        original_config = self._loader.load_data('_retry')\n        if not original_config:\n            return\n\n        retries = self._transform_legacy_retries(client.meta.config.retries)\n        retry_config = self._retry_config_translator.build_retry_config(\n            endpoint_prefix,\n            original_config.get('retry', {}),\n            original_config.get('definitions', {}),\n            retries,\n        )\n\n        logger.debug(\n            \"Registering retry handlers for service: %s\",\n            client.meta.service_model.service_name,\n        )\n        handler = self._retry_handler_factory.create_retry_handler(\n            retry_config, endpoint_prefix\n        )\n        unique_id = 'retry-config-%s' % service_event_name\n        client.meta.events.register(\n            f\"needs-retry.{service_event_name}\", handler, unique_id=unique_id\n        )\n\n    def _transform_legacy_retries(self, retries):\n        if retries is None:\n            return\n        copied_args = retries.copy()\n        if 'total_max_attempts' in retries:\n            copied_args = retries.copy()\n            copied_args['max_attempts'] = (\n                copied_args.pop('total_max_attempts') - 1\n            )\n        return copied_args\n\n    def _get_retry_mode(self, client, config_store):\n        client_retries = client.meta.config.retries\n        if (\n            client_retries is not None\n            and client_retries.get('mode') is not None\n        ):\n            return client_retries['mode']\n        return config_store.get_config_variable('retry_mode') or 'legacy'\n\n    def _register_endpoint_discovery(self, client, endpoint_url, config):\n        if endpoint_url is not None:\n            # Don't register any handlers in the case of a custom endpoint url\n            return\n        # Only attach handlers if the service supports discovery\n        if client.meta.service_model.endpoint_discovery_operation is None:\n            return\n        events = client.meta.events\n        service_id = client.meta.service_model.service_id.hyphenize()\n        enabled = False\n        if config and config.endpoint_discovery_enabled is not None:\n            enabled = config.endpoint_discovery_enabled\n        elif self._config_store:\n            enabled = self._config_store.get_config_variable(\n                'endpoint_discovery_enabled'\n            )\n\n        enabled = self._normalize_endpoint_discovery_config(enabled)\n        if enabled and self._requires_endpoint_discovery(client, enabled):\n            discover = enabled is True\n            manager = EndpointDiscoveryManager(\n                client, always_discover=discover\n            )\n            handler = EndpointDiscoveryHandler(manager)\n            handler.register(events, service_id)\n        else:\n            events.register(\n                'before-parameter-build',\n                block_endpoint_discovery_required_operations,\n            )\n\n    def _normalize_endpoint_discovery_config(self, enabled):\n        \"\"\"Config must either be a boolean-string or string-literal 'auto'\"\"\"\n        if isinstance(enabled, str):\n            enabled = enabled.lower().strip()\n            if enabled == 'auto':\n                return enabled\n            elif enabled in ('true', 'false'):\n                return ensure_boolean(enabled)\n        elif isinstance(enabled, bool):\n            return enabled\n\n        raise InvalidEndpointDiscoveryConfigurationError(config_value=enabled)\n\n    def _requires_endpoint_discovery(self, client, enabled):\n        if enabled == \"auto\":\n            return client.meta.service_model.endpoint_discovery_required\n        return enabled\n\n    def _register_eventbridge_events(\n        self, client, endpoint_bridge, endpoint_url\n    ):\n        if client.meta.service_model.service_name != 'events':\n            return\n        EventbridgeSignerSetter(\n            endpoint_resolver=self._endpoint_resolver,\n            region=client.meta.region_name,\n            endpoint_url=endpoint_url,\n        ).register(client.meta.events)\n\n    def _register_s3express_events(\n        self,\n        client,\n        endpoint_bridge=None,\n        endpoint_url=None,\n        client_config=None,\n        scoped_config=None,\n    ):\n        if client.meta.service_model.service_name != 's3':\n            return\n        S3ExpressIdentityResolver(client, RefreshableCredentials).register()\n\n    def _register_s3_events(\n        self,\n        client,\n        endpoint_bridge,\n        endpoint_url,\n        client_config,\n        scoped_config,\n    ):\n        if client.meta.service_model.service_name != 's3':\n            return\n        S3RegionRedirectorv2(None, client).register()\n        self._set_s3_presign_signature_version(\n            client.meta, client_config, scoped_config\n        )\n        client.meta.events.register(\n            'before-parameter-build.s3', self._inject_s3_input_parameters\n        )\n\n    def _register_s3_control_events(\n        self,\n        client,\n        endpoint_bridge=None,\n        endpoint_url=None,\n        client_config=None,\n        scoped_config=None,\n    ):\n        if client.meta.service_model.service_name != 's3control':\n            return\n        S3ControlArnParamHandlerv2().register(client.meta.events)\n\n    def _set_s3_presign_signature_version(\n        self, client_meta, client_config, scoped_config\n    ):\n        # This will return the manually configured signature version, or None\n        # if none was manually set. If a customer manually sets the signature\n        # version, we always want to use what they set.\n        provided_signature_version = _get_configured_signature_version(\n            's3', client_config, scoped_config\n        )\n        if provided_signature_version is not None:\n            return\n\n        # Check to see if the region is a region that we know about. If we\n        # don't know about a region, then we can safely assume it's a new\n        # region that is sigv4 only, since all new S3 regions only allow sigv4.\n        # The only exception is aws-global. This is a pseudo-region for the\n        # global endpoint, we should respect the signature versions it\n        # supports, which includes v2.\n        regions = self._endpoint_resolver.get_available_endpoints(\n            's3', client_meta.partition\n        )\n        if (\n            client_meta.region_name != 'aws-global'\n            and client_meta.region_name not in regions\n        ):\n            return\n\n        # If it is a region we know about, we want to default to sigv2, so here\n        # we check to see if it is available.\n        endpoint = self._endpoint_resolver.construct_endpoint(\n            's3', client_meta.region_name\n        )\n        signature_versions = endpoint['signatureVersions']\n        if 's3' not in signature_versions:\n            return\n\n        # We now know that we're in a known region that supports sigv2 and\n        # the customer hasn't set a signature version so we default the\n        # signature version to sigv2.\n        client_meta.events.register(\n            'choose-signer.s3', self._default_s3_presign_to_sigv2\n        )\n\n    def _inject_s3_input_parameters(self, params, context, **kwargs):\n        context['input_params'] = {}\n        inject_parameters = ('Bucket', 'Delete', 'Key', 'Prefix')\n        for inject_parameter in inject_parameters:\n            if inject_parameter in params:\n                context['input_params'][inject_parameter] = params[\n                    inject_parameter\n                ]\n\n    def _default_s3_presign_to_sigv2(self, signature_version, **kwargs):\n        \"\"\"\n        Returns the 's3' (sigv2) signer if presigning an s3 request. This is\n        intended to be used to set the default signature version for the signer\n        to sigv2. Situations where an asymmetric signature is required are the\n        exception, for example MRAP needs v4a.\n\n        :type signature_version: str\n        :param signature_version: The current client signature version.\n\n        :type signing_name: str\n        :param signing_name: The signing name of the service.\n\n        :return: 's3' if the request is an s3 presign request, None otherwise\n        \"\"\"\n        if signature_version.startswith('v4a'):\n            return\n\n        if signature_version.startswith('v4-s3express'):\n            return f'{signature_version}'\n\n        for suffix in ['-query', '-presign-post']:\n            if signature_version.endswith(suffix):\n                return f's3{suffix}'\n\n    def _get_client_args(\n        self,\n        service_model,\n        region_name,\n        is_secure,\n        endpoint_url,\n        verify,\n        credentials,\n        scoped_config,\n        client_config,\n        endpoint_bridge,\n        auth_token,\n        endpoints_ruleset_data,\n        partition_data,\n    ):\n        args_creator = ClientArgsCreator(\n            self._event_emitter,\n            self._user_agent,\n            self._response_parser_factory,\n            self._loader,\n            self._exceptions_factory,\n            config_store=self._config_store,\n            user_agent_creator=self._user_agent_creator,\n        )\n        return args_creator.get_client_args(\n            service_model,\n            region_name,\n            is_secure,\n            endpoint_url,\n            verify,\n            credentials,\n            scoped_config,\n            client_config,\n            endpoint_bridge,\n            auth_token,\n            endpoints_ruleset_data,\n            partition_data,\n        )\n\n    def _create_methods(self, service_model):\n        op_dict = {}\n        for operation_name in service_model.operation_names:\n            py_operation_name = xform_name(operation_name)\n            op_dict[py_operation_name] = self._create_api_method(\n                py_operation_name, operation_name, service_model\n            )\n        return op_dict\n\n    def _create_name_mapping(self, service_model):\n        # py_name -> OperationName, for every operation available\n        # for a service.\n        mapping = {}\n        for operation_name in service_model.operation_names:\n            py_operation_name = xform_name(operation_name)\n            mapping[py_operation_name] = operation_name\n        return mapping\n\n    def _create_api_method(\n        self, py_operation_name, operation_name, service_model\n    ):\n        def _api_call(self, *args, **kwargs):\n            # We're accepting *args so that we can give a more helpful\n            # error message than TypeError: _api_call takes exactly\n            # 1 argument.\n            if args:\n                raise TypeError(\n                    f\"{py_operation_name}() only accepts keyword arguments.\"\n                )\n            # The \"self\" in this scope is referring to the BaseClient.\n            return self._make_api_call(operation_name, kwargs)\n\n        _api_call.__name__ = str(py_operation_name)\n\n        # Add the docstring to the client method\n        operation_model = service_model.operation_model(operation_name)\n        docstring = ClientMethodDocstring(\n            operation_model=operation_model,\n            method_name=operation_name,\n            event_emitter=self._event_emitter,\n            method_description=operation_model.documentation,\n            example_prefix='response = client.%s' % py_operation_name,\n            include_signature=False,\n        )\n        _api_call.__doc__ = docstring\n        return _api_call\n\n\nclass ClientEndpointBridge:\n    \"\"\"Bridges endpoint data and client creation\n\n    This class handles taking out the relevant arguments from the endpoint\n    resolver and determining which values to use, taking into account any\n    client configuration options and scope configuration options.\n\n    This class also handles determining what, if any, region to use if no\n    explicit region setting is provided. For example, Amazon S3 client will\n    utilize \"us-east-1\" by default if no region can be resolved.\"\"\"\n\n    DEFAULT_ENDPOINT = '{service}.{region}.amazonaws.com'\n    _DUALSTACK_CUSTOMIZED_SERVICES = ['s3', 's3-control']\n\n    def __init__(\n        self,\n        endpoint_resolver,\n        scoped_config=None,\n        client_config=None,\n        default_endpoint=None,\n        service_signing_name=None,\n        config_store=None,\n        service_signature_version=None,\n    ):\n        self.service_signing_name = service_signing_name\n        self.endpoint_resolver = endpoint_resolver\n        self.scoped_config = scoped_config\n        self.client_config = client_config\n        self.default_endpoint = default_endpoint or self.DEFAULT_ENDPOINT\n        self.config_store = config_store\n        self.service_signature_version = service_signature_version\n\n    def resolve(\n        self, service_name, region_name=None, endpoint_url=None, is_secure=True\n    ):\n        region_name = self._check_default_region(service_name, region_name)\n        use_dualstack_endpoint = self._resolve_use_dualstack_endpoint(\n            service_name\n        )\n        use_fips_endpoint = self._resolve_endpoint_variant_config_var(\n            'use_fips_endpoint'\n        )\n        resolved = self.endpoint_resolver.construct_endpoint(\n            service_name,\n            region_name,\n            use_dualstack_endpoint=use_dualstack_endpoint,\n            use_fips_endpoint=use_fips_endpoint,\n        )\n\n        # If we can't resolve the region, we'll attempt to get a global\n        # endpoint for non-regionalized services (iam, route53, etc)\n        if not resolved:\n            # TODO: fallback partition_name should be configurable in the\n            # future for users to define as needed.\n            resolved = self.endpoint_resolver.construct_endpoint(\n                service_name,\n                region_name,\n                partition_name='aws',\n                use_dualstack_endpoint=use_dualstack_endpoint,\n                use_fips_endpoint=use_fips_endpoint,\n            )\n\n        if resolved:\n            return self._create_endpoint(\n                resolved, service_name, region_name, endpoint_url, is_secure\n            )\n        else:\n            return self._assume_endpoint(\n                service_name, region_name, endpoint_url, is_secure\n            )\n\n    def resolver_uses_builtin_data(self):\n        return self.endpoint_resolver.uses_builtin_data\n\n    def _check_default_region(self, service_name, region_name):\n        if region_name is not None:\n            return region_name\n        # Use the client_config region if no explicit region was provided.\n        if self.client_config and self.client_config.region_name is not None:\n            return self.client_config.region_name\n\n    def _create_endpoint(\n        self, resolved, service_name, region_name, endpoint_url, is_secure\n    ):\n        region_name, signing_region = self._pick_region_values(\n            resolved, region_name, endpoint_url\n        )\n        if endpoint_url is None:\n            endpoint_url = self._make_url(\n                resolved.get('hostname'),\n                is_secure,\n                resolved.get('protocols', []),\n            )\n        signature_version = self._resolve_signature_version(\n            service_name, resolved\n        )\n        signing_name = self._resolve_signing_name(service_name, resolved)\n        return self._create_result(\n            service_name=service_name,\n            region_name=region_name,\n            signing_region=signing_region,\n            signing_name=signing_name,\n            endpoint_url=endpoint_url,\n            metadata=resolved,\n            signature_version=signature_version,\n        )\n\n    def _resolve_endpoint_variant_config_var(self, config_var):\n        client_config = self.client_config\n        config_val = False\n\n        # Client configuration arg has precedence\n        if client_config and getattr(client_config, config_var) is not None:\n            return getattr(client_config, config_var)\n        elif self.config_store is not None:\n            # Check config store\n            config_val = self.config_store.get_config_variable(config_var)\n        return config_val\n\n    def _resolve_use_dualstack_endpoint(self, service_name):\n        s3_dualstack_mode = self._is_s3_dualstack_mode(service_name)\n        if s3_dualstack_mode is not None:\n            return s3_dualstack_mode\n        return self._resolve_endpoint_variant_config_var(\n            'use_dualstack_endpoint'\n        )\n\n    def _is_s3_dualstack_mode(self, service_name):\n        if service_name not in self._DUALSTACK_CUSTOMIZED_SERVICES:\n            return None\n        # TODO: This normalization logic is duplicated from the\n        # ClientArgsCreator class.  Consolidate everything to\n        # ClientArgsCreator.  _resolve_signature_version also has similarly\n        # duplicated logic.\n        client_config = self.client_config\n        if (\n            client_config is not None\n            and client_config.s3 is not None\n            and 'use_dualstack_endpoint' in client_config.s3\n        ):\n            # Client config trumps scoped config.\n            return client_config.s3['use_dualstack_endpoint']\n        if self.scoped_config is not None:\n            enabled = self.scoped_config.get('s3', {}).get(\n                'use_dualstack_endpoint'\n            )\n            if enabled in [True, 'True', 'true']:\n                return True\n\n    def _assume_endpoint(\n        self, service_name, region_name, endpoint_url, is_secure\n    ):\n        if endpoint_url is None:\n            # Expand the default hostname URI template.\n            hostname = self.default_endpoint.format(\n                service=service_name, region=region_name\n            )\n            endpoint_url = self._make_url(\n                hostname, is_secure, ['http', 'https']\n            )\n        logger.debug(\n            f'Assuming an endpoint for {service_name}, {region_name}: {endpoint_url}'\n        )\n        # We still want to allow the user to provide an explicit version.\n        signature_version = self._resolve_signature_version(\n            service_name, {'signatureVersions': ['v4']}\n        )\n        signing_name = self._resolve_signing_name(service_name, resolved={})\n        return self._create_result(\n            service_name=service_name,\n            region_name=region_name,\n            signing_region=region_name,\n            signing_name=signing_name,\n            signature_version=signature_version,\n            endpoint_url=endpoint_url,\n            metadata={},\n        )\n\n    def _create_result(\n        self,\n        service_name,\n        region_name,\n        signing_region,\n        signing_name,\n        endpoint_url,\n        signature_version,\n        metadata,\n    ):\n        return {\n            'service_name': service_name,\n            'region_name': region_name,\n            'signing_region': signing_region,\n            'signing_name': signing_name,\n            'endpoint_url': endpoint_url,\n            'signature_version': signature_version,\n            'metadata': metadata,\n        }\n\n    def _make_url(self, hostname, is_secure, supported_protocols):\n        if is_secure and 'https' in supported_protocols:\n            scheme = 'https'\n        else:\n            scheme = 'http'\n        return f'{scheme}://{hostname}'\n\n    def _resolve_signing_name(self, service_name, resolved):\n        # CredentialScope overrides everything else.\n        if (\n            'credentialScope' in resolved\n            and 'service' in resolved['credentialScope']\n        ):\n            return resolved['credentialScope']['service']\n        # Use the signingName from the model if present.\n        if self.service_signing_name:\n            return self.service_signing_name\n        # Just assume is the same as the service name.\n        return service_name\n\n    def _pick_region_values(self, resolved, region_name, endpoint_url):\n        signing_region = region_name\n        if endpoint_url is None:\n            # Do not use the region name or signing name from the resolved\n            # endpoint if the user explicitly provides an endpoint_url. This\n            # would happen if we resolve to an endpoint where the service has\n            # a \"defaults\" section that overrides all endpoint with a single\n            # hostname and credentialScope. This has been the case historically\n            # for how STS has worked. The only way to resolve an STS endpoint\n            # was to provide a region_name and an endpoint_url. In that case,\n            # we would still resolve an endpoint, but we would not use the\n            # resolved endpointName or signingRegion because we want to allow\n            # custom endpoints.\n            region_name = resolved['endpointName']\n            signing_region = region_name\n            if (\n                'credentialScope' in resolved\n                and 'region' in resolved['credentialScope']\n            ):\n                signing_region = resolved['credentialScope']['region']\n        return region_name, signing_region\n\n    def _resolve_signature_version(self, service_name, resolved):\n        configured_version = _get_configured_signature_version(\n            service_name, self.client_config, self.scoped_config\n        )\n        if configured_version is not None:\n            return configured_version\n\n        potential_versions = resolved.get('signatureVersions', [])\n        if (\n            self.service_signature_version is not None\n            and self.service_signature_version\n            not in _LEGACY_SIGNATURE_VERSIONS\n        ):\n            # Prefer the service model as most specific\n            # source of truth for new signature versions.\n            potential_versions = [self.service_signature_version]\n\n        # Pick a signature version from the endpoint metadata if present.\n        if 'signatureVersions' in resolved:\n            if service_name == 's3':\n                return 's3v4'\n            if 'v4' in potential_versions:\n                return 'v4'\n            # Now just iterate over the signature versions in order until we\n            # find the first one that is known to Botocore.\n            for known in potential_versions:\n                if known in AUTH_TYPE_MAPS:\n                    return known\n        raise UnknownSignatureVersionError(\n            signature_version=potential_versions\n        )\n\n\nclass BaseClient:\n    # This is actually reassigned with the py->op_name mapping\n    # when the client creator creates the subclass.  This value is used\n    # because calls such as client.get_paginator('list_objects') use the\n    # snake_case name, but we need to know the ListObjects form.\n    # xform_name() does the ListObjects->list_objects conversion, but\n    # we need the reverse mapping here.\n    _PY_TO_OP_NAME = {}\n\n    def __init__(\n        self,\n        serializer,\n        endpoint,\n        response_parser,\n        event_emitter,\n        request_signer,\n        service_model,\n        loader,\n        client_config,\n        partition,\n        exceptions_factory,\n        endpoint_ruleset_resolver=None,\n        user_agent_creator=None,\n    ):\n        self._serializer = serializer\n        self._endpoint = endpoint\n        self._ruleset_resolver = endpoint_ruleset_resolver\n        self._response_parser = response_parser\n        self._request_signer = request_signer\n        self._cache = {}\n        self._loader = loader\n        self._client_config = client_config\n        self.meta = ClientMeta(\n            event_emitter,\n            self._client_config,\n            endpoint.host,\n            service_model,\n            self._PY_TO_OP_NAME,\n            partition,\n        )\n        self._exceptions_factory = exceptions_factory\n        self._exceptions = None\n        self._user_agent_creator = user_agent_creator\n        if self._user_agent_creator is None:\n            self._user_agent_creator = (\n                UserAgentString.from_environment().with_client_config(\n                    self._client_config\n                )\n            )\n        self._register_handlers()\n\n    def __getattr__(self, item):\n        service_id = self._service_model.service_id.hyphenize()\n        event_name = f'getattr.{service_id}.{item}'\n\n        handler, event_response = self.meta.events.emit_until_response(\n            event_name, client=self\n        )\n\n        if event_response is not None:\n            return event_response\n\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{item}'\"\n        )\n\n    def close(self):\n        \"\"\"Closes underlying endpoint connections.\"\"\"\n        self._endpoint.close()\n\n    def _register_handlers(self):\n        # Register the handler required to sign requests.\n        service_id = self.meta.service_model.service_id.hyphenize()\n        self.meta.events.register(\n            f\"request-created.{service_id}\", self._request_signer.handler\n        )\n\n    @property\n    def _service_model(self):\n        return self.meta.service_model\n\n    def _make_api_call(self, operation_name, api_params):\n        operation_model = self._service_model.operation_model(operation_name)\n        service_name = self._service_model.service_name\n        history_recorder.record(\n            'API_CALL',\n            {\n                'service': service_name,\n                'operation': operation_name,\n                'params': api_params,\n            },\n        )\n        if operation_model.deprecated:\n            logger.debug(\n                'Warning: %s.%s() is deprecated', service_name, operation_name\n            )\n        request_context = {\n            'client_region': self.meta.region_name,\n            'client_config': self.meta.config,\n            'has_streaming_input': operation_model.has_streaming_input,\n            'auth_type': operation_model.auth_type,\n        }\n        api_params = self._emit_api_params(\n            api_params=api_params,\n            operation_model=operation_model,\n            context=request_context,\n        )\n        (\n            endpoint_url,\n            additional_headers,\n            properties,\n        ) = self._resolve_endpoint_ruleset(\n            operation_model, api_params, request_context\n        )\n        if properties:\n            # Pass arbitrary endpoint info with the Request\n            # for use during construction.\n            request_context['endpoint_properties'] = properties\n        request_dict = self._convert_to_request_dict(\n            api_params=api_params,\n            operation_model=operation_model,\n            endpoint_url=endpoint_url,\n            context=request_context,\n            headers=additional_headers,\n        )\n        resolve_checksum_context(request_dict, operation_model, api_params)\n\n        service_id = self._service_model.service_id.hyphenize()\n        handler, event_response = self.meta.events.emit_until_response(\n            'before-call.{service_id}.{operation_name}'.format(\n                service_id=service_id, operation_name=operation_name\n            ),\n            model=operation_model,\n            params=request_dict,\n            request_signer=self._request_signer,\n            context=request_context,\n        )\n\n        if event_response is not None:\n            http, parsed_response = event_response\n        else:\n            maybe_compress_request(\n                self.meta.config, request_dict, operation_model\n            )\n            apply_request_checksum(request_dict)\n            http, parsed_response = self._make_request(\n                operation_model, request_dict, request_context\n            )\n\n        self.meta.events.emit(\n            'after-call.{service_id}.{operation_name}'.format(\n                service_id=service_id, operation_name=operation_name\n            ),\n            http_response=http,\n            parsed=parsed_response,\n            model=operation_model,\n            context=request_context,\n        )\n\n        if http.status_code >= 300:\n            error_info = parsed_response.get(\"Error\", {})\n            error_code = error_info.get(\"QueryErrorCode\") or error_info.get(\n                \"Code\"\n            )\n            error_class = self.exceptions.from_code(error_code)\n            raise error_class(parsed_response, operation_name)\n        else:\n            return parsed_response\n\n    def _make_request(self, operation_model, request_dict, request_context):\n        try:\n            return self._endpoint.make_request(operation_model, request_dict)\n        except Exception as e:\n            self.meta.events.emit(\n                'after-call-error.{service_id}.{operation_name}'.format(\n                    service_id=self._service_model.service_id.hyphenize(),\n                    operation_name=operation_model.name,\n                ),\n                exception=e,\n                context=request_context,\n            )\n            raise\n\n    def _convert_to_request_dict(\n        self,\n        api_params,\n        operation_model,\n        endpoint_url,\n        context=None,\n        headers=None,\n        set_user_agent_header=True,\n    ):\n        request_dict = self._serializer.serialize_to_request(\n            api_params, operation_model\n        )\n        if not self._client_config.inject_host_prefix:\n            request_dict.pop('host_prefix', None)\n        if headers is not None:\n            request_dict['headers'].update(headers)\n        if set_user_agent_header:\n            user_agent = self._user_agent_creator.to_string()\n        else:\n            user_agent = None\n        prepare_request_dict(\n            request_dict,\n            endpoint_url=endpoint_url,\n            user_agent=user_agent,\n            context=context,\n        )\n        return request_dict\n\n    def _emit_api_params(self, api_params, operation_model, context):\n        # Given the API params provided by the user and the operation_model\n        # we can serialize the request to a request_dict.\n        operation_name = operation_model.name\n\n        # Emit an event that allows users to modify the parameters at the\n        # beginning of the method. It allows handlers to modify existing\n        # parameters or return a new set of parameters to use.\n        service_id = self._service_model.service_id.hyphenize()\n        responses = self.meta.events.emit(\n            f'provide-client-params.{service_id}.{operation_name}',\n            params=api_params,\n            model=operation_model,\n            context=context,\n        )\n        api_params = first_non_none_response(responses, default=api_params)\n\n        self.meta.events.emit(\n            f'before-parameter-build.{service_id}.{operation_name}',\n            params=api_params,\n            model=operation_model,\n            context=context,\n        )\n        return api_params\n\n    def _resolve_endpoint_ruleset(\n        self,\n        operation_model,\n        params,\n        request_context,\n        ignore_signing_region=False,\n    ):\n        \"\"\"Returns endpoint URL and list of additional headers returned from\n        EndpointRulesetResolver for the given operation and params. If the\n        ruleset resolver is not available, for example because the service has\n        no endpoints ruleset file, the legacy endpoint resolver's value is\n        returned.\n\n        Use ignore_signing_region for generating presigned URLs or any other\n        situation where the signing region information from the ruleset\n        resolver should be ignored.\n\n        Returns tuple of URL and headers dictionary. Additionally, the\n        request_context dict is modified in place with any signing information\n        returned from the ruleset resolver.\n        \"\"\"\n        if self._ruleset_resolver is None:\n            endpoint_url = self.meta.endpoint_url\n            additional_headers = {}\n            endpoint_properties = {}\n        else:\n            endpoint_info = self._ruleset_resolver.construct_endpoint(\n                operation_model=operation_model,\n                call_args=params,\n                request_context=request_context,\n            )\n            endpoint_url = endpoint_info.url\n            additional_headers = endpoint_info.headers\n            endpoint_properties = endpoint_info.properties\n            # If authSchemes is present, overwrite default auth type and\n            # signing context derived from service model.\n            auth_schemes = endpoint_info.properties.get('authSchemes')\n            if auth_schemes is not None:\n                auth_info = self._ruleset_resolver.auth_schemes_to_signing_ctx(\n                    auth_schemes\n                )\n                auth_type, signing_context = auth_info\n                request_context['auth_type'] = auth_type\n                if 'region' in signing_context and ignore_signing_region:\n                    del signing_context['region']\n                if 'signing' in request_context:\n                    request_context['signing'].update(signing_context)\n                else:\n                    request_context['signing'] = signing_context\n\n        return endpoint_url, additional_headers, endpoint_properties\n\n    def get_paginator(self, operation_name):\n        \"\"\"Create a paginator for an operation.\n\n        :type operation_name: string\n        :param operation_name: The operation name.  This is the same name\n            as the method name on the client.  For example, if the\n            method name is ``create_foo``, and you'd normally invoke the\n            operation as ``client.create_foo(**kwargs)``, if the\n            ``create_foo`` operation can be paginated, you can use the\n            call ``client.get_paginator(\"create_foo\")``.\n\n        :raise OperationNotPageableError: Raised if the operation is not\n            pageable.  You can use the ``client.can_paginate`` method to\n            check if an operation is pageable.\n\n        :rtype: ``botocore.paginate.Paginator``\n        :return: A paginator object.\n\n        \"\"\"\n        if not self.can_paginate(operation_name):\n            raise OperationNotPageableError(operation_name=operation_name)\n        else:\n            actual_operation_name = self._PY_TO_OP_NAME[operation_name]\n\n            # Create a new paginate method that will serve as a proxy to\n            # the underlying Paginator.paginate method. This is needed to\n            # attach a docstring to the method.\n            def paginate(self, **kwargs):\n                return Paginator.paginate(self, **kwargs)\n\n            paginator_config = self._cache['page_config'][\n                actual_operation_name\n            ]\n            # Add the docstring for the paginate method.\n            paginate.__doc__ = PaginatorDocstring(\n                paginator_name=actual_operation_name,\n                event_emitter=self.meta.events,\n                service_model=self.meta.service_model,\n                paginator_config=paginator_config,\n                include_signature=False,\n            )\n\n            # Rename the paginator class based on the type of paginator.\n            service_module_name = get_service_module_name(\n                self.meta.service_model\n            )\n            paginator_class_name = (\n                f\"{service_module_name}.Paginator.{actual_operation_name}\"\n            )\n\n            # Create the new paginator class\n            documented_paginator_cls = type(\n                paginator_class_name, (Paginator,), {'paginate': paginate}\n            )\n\n            operation_model = self._service_model.operation_model(\n                actual_operation_name\n            )\n            paginator = documented_paginator_cls(\n                getattr(self, operation_name),\n                paginator_config,\n                operation_model,\n            )\n            return paginator\n\n    def can_paginate(self, operation_name):\n        \"\"\"Check if an operation can be paginated.\n\n        :type operation_name: string\n        :param operation_name: The operation name.  This is the same name\n            as the method name on the client.  For example, if the\n            method name is ``create_foo``, and you'd normally invoke the\n            operation as ``client.create_foo(**kwargs)``, if the\n            ``create_foo`` operation can be paginated, you can use the\n            call ``client.get_paginator(\"create_foo\")``.\n\n        :return: ``True`` if the operation can be paginated,\n            ``False`` otherwise.\n\n        \"\"\"\n        if 'page_config' not in self._cache:\n            try:\n                page_config = self._loader.load_service_model(\n                    self._service_model.service_name,\n                    'paginators-1',\n                    self._service_model.api_version,\n                )['pagination']\n                self._cache['page_config'] = page_config\n            except DataNotFoundError:\n                self._cache['page_config'] = {}\n        actual_operation_name = self._PY_TO_OP_NAME[operation_name]\n        return actual_operation_name in self._cache['page_config']\n\n    def _get_waiter_config(self):\n        if 'waiter_config' not in self._cache:\n            try:\n                waiter_config = self._loader.load_service_model(\n                    self._service_model.service_name,\n                    'waiters-2',\n                    self._service_model.api_version,\n                )\n                self._cache['waiter_config'] = waiter_config\n            except DataNotFoundError:\n                self._cache['waiter_config'] = {}\n        return self._cache['waiter_config']\n\n    def get_waiter(self, waiter_name):\n        \"\"\"Returns an object that can wait for some condition.\n\n        :type waiter_name: str\n        :param waiter_name: The name of the waiter to get. See the waiters\n            section of the service docs for a list of available waiters.\n\n        :returns: The specified waiter object.\n        :rtype: ``botocore.waiter.Waiter``\n        \"\"\"\n        config = self._get_waiter_config()\n        if not config:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n        model = waiter.WaiterModel(config)\n        mapping = {}\n        for name in model.waiter_names:\n            mapping[xform_name(name)] = name\n        if waiter_name not in mapping:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n\n        return waiter.create_waiter_with_client(\n            mapping[waiter_name], model, self\n        )\n\n    @CachedProperty\n    def waiter_names(self):\n        \"\"\"Returns a list of all available waiters.\"\"\"\n        config = self._get_waiter_config()\n        if not config:\n            return []\n        model = waiter.WaiterModel(config)\n        # Waiter configs is a dict, we just want the waiter names\n        # which are the keys in the dict.\n        return [xform_name(name) for name in model.waiter_names]\n\n    @property\n    def exceptions(self):\n        if self._exceptions is None:\n            self._exceptions = self._load_exceptions()\n        return self._exceptions\n\n    def _load_exceptions(self):\n        return self._exceptions_factory.create_client_exceptions(\n            self._service_model\n        )\n\n    def _get_credentials(self):\n        \"\"\"\n        This private interface is subject to abrupt breaking changes, including\n        removal, in any botocore release.\n        \"\"\"\n        return self._request_signer._credentials\n\n\nclass ClientMeta:\n    \"\"\"Holds additional client methods.\n\n    This class holds additional information for clients.  It exists for\n    two reasons:\n\n        * To give advanced functionality to clients\n        * To namespace additional client attributes from the operation\n          names which are mapped to methods at runtime.  This avoids\n          ever running into collisions with operation names.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        events,\n        client_config,\n        endpoint_url,\n        service_model,\n        method_to_api_mapping,\n        partition,\n    ):\n        self.events = events\n        self._client_config = client_config\n        self._endpoint_url = endpoint_url\n        self._service_model = service_model\n        self._method_to_api_mapping = method_to_api_mapping\n        self._partition = partition\n\n    @property\n    def service_model(self):\n        return self._service_model\n\n    @property\n    def region_name(self):\n        return self._client_config.region_name\n\n    @property\n    def endpoint_url(self):\n        return self._endpoint_url\n\n    @property\n    def config(self):\n        return self._client_config\n\n    @property\n    def method_to_api_mapping(self):\n        return self._method_to_api_mapping\n\n    @property\n    def partition(self):\n        return self._partition\n\n\ndef _get_configured_signature_version(\n    service_name, client_config, scoped_config\n):\n    \"\"\"\n    Gets the manually configured signature version.\n\n    :returns: the customer configured signature version, or None if no\n        signature version was configured.\n    \"\"\"\n    # Client config overrides everything.\n    if client_config and client_config.signature_version is not None:\n        return client_config.signature_version\n\n    # Scoped config overrides picking from the endpoint metadata.\n    if scoped_config is not None:\n        # A given service may have service specific configuration in the\n        # config file, so we need to check there as well.\n        service_config = scoped_config.get(service_name)\n        if service_config is not None and isinstance(service_config, dict):\n            version = service_config.get('signature_version')\n            if version:\n                logger.debug(\n                    \"Switching signature version for service %s \"\n                    \"to version %s based on config file override.\",\n                    service_name,\n                    version,\n                )\n                return version\n    return None\n", "botocore/handlers.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\"\"\"Builtin event handlers.\n\nThis module contains builtin handlers for events emitted by botocore.\n\"\"\"\n\nimport base64\nimport copy\nimport logging\nimport os\nimport re\nimport uuid\nimport warnings\nfrom io import BytesIO\n\nimport botocore\nimport botocore.auth\nfrom botocore import utils\nfrom botocore.compat import (\n    ETree,\n    OrderedDict,\n    XMLParseError,\n    ensure_bytes,\n    get_md5,\n    json,\n    quote,\n    unquote,\n    unquote_str,\n    urlsplit,\n    urlunsplit,\n)\nfrom botocore.docs.utils import (\n    AppendParamDocumentation,\n    AutoPopulatedParam,\n    HideParamFromOperations,\n)\nfrom botocore.endpoint_provider import VALID_HOST_LABEL_RE\nfrom botocore.exceptions import (\n    AliasConflictParameterError,\n    ParamValidationError,\n    UnsupportedTLSVersionWarning,\n)\nfrom botocore.regions import EndpointResolverBuiltins\nfrom botocore.signers import (\n    add_generate_db_auth_token,\n    add_generate_presigned_post,\n    add_generate_presigned_url,\n)\nfrom botocore.utils import (\n    SAFE_CHARS,\n    ArnParser,\n    conditionally_calculate_checksum,\n    conditionally_calculate_md5,\n    percent_encode,\n    switch_host_with_param,\n)\n\n# Keep these imported.  There's pre-existing code that uses them.\nfrom botocore import retryhandler  # noqa\nfrom botocore import translate  # noqa\nfrom botocore.compat import MD5_AVAILABLE  # noqa\nfrom botocore.exceptions import MissingServiceIdError  # noqa\nfrom botocore.utils import hyphenize_service_id  # noqa\nfrom botocore.utils import is_global_accesspoint  # noqa\nfrom botocore.utils import SERVICE_NAME_ALIASES  # noqa\n\n\nlogger = logging.getLogger(__name__)\n\nREGISTER_FIRST = object()\nREGISTER_LAST = object()\n# From the S3 docs:\n# The rules for bucket names in the US Standard region allow bucket names\n# to be as long as 255 characters, and bucket names can contain any\n# combination of uppercase letters, lowercase letters, numbers, periods\n# (.), hyphens (-), and underscores (_).\nVALID_BUCKET = re.compile(r'^[a-zA-Z0-9.\\-_]{1,255}$')\n_ACCESSPOINT_ARN = (\n    r'^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:]'\n    r'[a-zA-Z0-9\\-.]{1,63}$'\n)\n_OUTPOST_ARN = (\n    r'^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:]'\n    r'[a-zA-Z0-9\\-]{1,63}[/:]accesspoint[/:][a-zA-Z0-9\\-]{1,63}$'\n)\nVALID_S3_ARN = re.compile('|'.join([_ACCESSPOINT_ARN, _OUTPOST_ARN]))\n# signing names used for the services s3 and s3-control, for example in\n# botocore/data/s3/2006-03-01/endpoints-rule-set-1.json\nS3_SIGNING_NAMES = ('s3', 's3-outposts', 's3-object-lambda', 's3express')\nVERSION_ID_SUFFIX = re.compile(r'\\?versionId=[^\\s]+$')\n\n\ndef handle_service_name_alias(service_name, **kwargs):\n    return SERVICE_NAME_ALIASES.get(service_name, service_name)\n\n\ndef add_recursion_detection_header(params, **kwargs):\n    has_lambda_name = 'AWS_LAMBDA_FUNCTION_NAME' in os.environ\n    trace_id = os.environ.get('_X_AMZN_TRACE_ID')\n    if has_lambda_name and trace_id:\n        headers = params['headers']\n        if 'X-Amzn-Trace-Id' not in headers:\n            headers['X-Amzn-Trace-Id'] = quote(trace_id, safe='-=;:+&[]{}\"\\',')\n\n\ndef escape_xml_payload(params, **kwargs):\n    # Replace \\r and \\n with the escaped sequence over the whole XML document\n    # to avoid linebreak normalization modifying customer input when the\n    # document is parsed. Ideally, we would do this in ElementTree.tostring,\n    # but it doesn't allow us to override entity escaping for text fields. For\n    # this operation \\r and \\n can only appear in the XML document if they were\n    # passed as part of the customer input.\n    body = params['body']\n    if b'\\r' in body:\n        body = body.replace(b'\\r', b'&#xD;')\n    if b'\\n' in body:\n        body = body.replace(b'\\n', b'&#xA;')\n\n    params['body'] = body\n\n\ndef check_for_200_error(response, **kwargs):\n    # From: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html\n    # There are two opportunities for a copy request to return an error. One\n    # can occur when Amazon S3 receives the copy request and the other can\n    # occur while Amazon S3 is copying the files. If the error occurs before\n    # the copy operation starts, you receive a standard Amazon S3 error. If the\n    # error occurs during the copy operation, the error response is embedded in\n    # the 200 OK response. This means that a 200 OK response can contain either\n    # a success or an error. Make sure to design your application to parse the\n    # contents of the response and handle it appropriately.\n    #\n    # So this handler checks for this case.  Even though the server sends a\n    # 200 response, conceptually this should be handled exactly like a\n    # 500 response (with respect to raising exceptions, retries, etc.)\n    # We're connected *before* all the other retry logic handlers, so as long\n    # as we switch the error code to 500, we'll retry the error as expected.\n    if response is None:\n        # A None response can happen if an exception is raised while\n        # trying to retrieve the response.  See Endpoint._get_response().\n        return\n    http_response, parsed = response\n    if _looks_like_special_case_error(http_response):\n        logger.debug(\n            \"Error found for response with 200 status code, \"\n            \"errors: %s, changing status code to \"\n            \"500.\",\n            parsed,\n        )\n        http_response.status_code = 500\n\n\ndef _looks_like_special_case_error(http_response):\n    if http_response.status_code == 200:\n        try:\n            parser = ETree.XMLParser(\n                target=ETree.TreeBuilder(), encoding='utf-8'\n            )\n            parser.feed(http_response.content)\n            root = parser.close()\n        except XMLParseError:\n            # In cases of network disruptions, we may end up with a partial\n            # streamed response from S3. We need to treat these cases as\n            # 500 Service Errors and try again.\n            return True\n        if root.tag == 'Error':\n            return True\n    return False\n\n\ndef set_operation_specific_signer(context, signing_name, **kwargs):\n    \"\"\"Choose the operation-specific signer.\n\n    Individual operations may have a different auth type than the service as a\n    whole. This will most often manifest as operations that should not be\n    authenticated at all, but can include other auth modes such as sigv4\n    without body signing.\n    \"\"\"\n    auth_type = context.get('auth_type')\n\n    # Auth type will be None if the operation doesn't have a configured auth\n    # type.\n    if not auth_type:\n        return\n\n    # Auth type will be the string value 'none' if the operation should not\n    # be signed at all.\n    if auth_type == 'none':\n        return botocore.UNSIGNED\n\n    if auth_type == 'bearer':\n        return 'bearer'\n\n    if auth_type.startswith('v4'):\n        if auth_type == 'v4-s3express':\n            return auth_type\n\n        if auth_type == 'v4a':\n            # If sigv4a is chosen, we must add additional signing config for\n            # global signature.\n            signing = {'region': '*', 'signing_name': signing_name}\n            if 'signing' in context:\n                context['signing'].update(signing)\n            else:\n                context['signing'] = signing\n            signature_version = 'v4a'\n        else:\n            signature_version = 'v4'\n\n        # If the operation needs an unsigned body, we set additional context\n        # allowing the signer to be aware of this.\n        if auth_type == 'v4-unsigned-body':\n            context['payload_signing_enabled'] = False\n\n        # Signing names used by s3 and s3-control use customized signers \"s3v4\"\n        # and \"s3v4a\".\n        if signing_name in S3_SIGNING_NAMES:\n            signature_version = f's3{signature_version}'\n\n        return signature_version\n\n\ndef decode_console_output(parsed, **kwargs):\n    if 'Output' in parsed:\n        try:\n            # We're using 'replace' for errors because it is\n            # possible that console output contains non string\n            # chars we can't utf-8 decode.\n            value = base64.b64decode(\n                bytes(parsed['Output'], 'latin-1')\n            ).decode('utf-8', 'replace')\n            parsed['Output'] = value\n        except (ValueError, TypeError, AttributeError):\n            logger.debug('Error decoding base64', exc_info=True)\n\n\ndef generate_idempotent_uuid(params, model, **kwargs):\n    for name in model.idempotent_members:\n        if name not in params:\n            params[name] = str(uuid.uuid4())\n            logger.debug(\n                \"injecting idempotency token (%s) into param '%s'.\"\n                % (params[name], name)\n            )\n\n\ndef decode_quoted_jsondoc(value):\n    try:\n        value = json.loads(unquote(value))\n    except (ValueError, TypeError):\n        logger.debug('Error loading quoted JSON', exc_info=True)\n    return value\n\n\ndef json_decode_template_body(parsed, **kwargs):\n    if 'TemplateBody' in parsed:\n        try:\n            value = json.loads(\n                parsed['TemplateBody'], object_pairs_hook=OrderedDict\n            )\n            parsed['TemplateBody'] = value\n        except (ValueError, TypeError):\n            logger.debug('error loading JSON', exc_info=True)\n\n\ndef validate_bucket_name(params, **kwargs):\n    if 'Bucket' not in params:\n        return\n    bucket = params['Bucket']\n    if not VALID_BUCKET.search(bucket) and not VALID_S3_ARN.search(bucket):\n        error_msg = (\n            f'Invalid bucket name \"{bucket}\": Bucket name must match '\n            f'the regex \"{VALID_BUCKET.pattern}\" or be an ARN matching '\n            f'the regex \"{VALID_S3_ARN.pattern}\"'\n        )\n        raise ParamValidationError(report=error_msg)\n\n\ndef sse_md5(params, **kwargs):\n    \"\"\"\n    S3 server-side encryption requires the encryption key to be sent to the\n    server base64 encoded, as well as a base64-encoded MD5 hash of the\n    encryption key. This handler does both if the MD5 has not been set by\n    the caller.\n    \"\"\"\n    _sse_md5(params, 'SSECustomer')\n\n\ndef copy_source_sse_md5(params, **kwargs):\n    \"\"\"\n    S3 server-side encryption requires the encryption key to be sent to the\n    server base64 encoded, as well as a base64-encoded MD5 hash of the\n    encryption key. This handler does both if the MD5 has not been set by\n    the caller specifically if the parameter is for the copy-source sse-c key.\n    \"\"\"\n    _sse_md5(params, 'CopySourceSSECustomer')\n\n\ndef _sse_md5(params, sse_member_prefix='SSECustomer'):\n    if not _needs_s3_sse_customization(params, sse_member_prefix):\n        return\n\n    sse_key_member = sse_member_prefix + 'Key'\n    sse_md5_member = sse_member_prefix + 'KeyMD5'\n    key_as_bytes = params[sse_key_member]\n    if isinstance(key_as_bytes, str):\n        key_as_bytes = key_as_bytes.encode('utf-8')\n    key_md5_str = base64.b64encode(get_md5(key_as_bytes).digest()).decode(\n        'utf-8'\n    )\n    key_b64_encoded = base64.b64encode(key_as_bytes).decode('utf-8')\n    params[sse_key_member] = key_b64_encoded\n    params[sse_md5_member] = key_md5_str\n\n\ndef _needs_s3_sse_customization(params, sse_member_prefix):\n    return (\n        params.get(sse_member_prefix + 'Key') is not None\n        and sse_member_prefix + 'KeyMD5' not in params\n    )\n\n\ndef disable_signing(**kwargs):\n    \"\"\"\n    This handler disables request signing by setting the signer\n    name to a special sentinel value.\n    \"\"\"\n    return botocore.UNSIGNED\n\n\ndef add_expect_header(model, params, **kwargs):\n    if model.http.get('method', '') not in ['PUT', 'POST']:\n        return\n    if 'body' in params:\n        body = params['body']\n        if hasattr(body, 'read'):\n            check_body = utils.ensure_boolean(\n                os.environ.get(\n                    'BOTO_EXPERIMENTAL__NO_EMPTY_CONTINUE',\n                    False,\n                )\n            )\n            if check_body and utils.determine_content_length(body) == 0:\n                return\n            # Any file like object will use an expect 100-continue\n            # header regardless of size.\n            logger.debug(\"Adding expect 100 continue header to request.\")\n            params['headers']['Expect'] = '100-continue'\n\n\nclass DeprecatedServiceDocumenter:\n    def __init__(self, replacement_service_name):\n        self._replacement_service_name = replacement_service_name\n\n    def inject_deprecation_notice(self, section, event_name, **kwargs):\n        section.style.start_important()\n        section.write('This service client is deprecated. Please use ')\n        section.style.ref(\n            self._replacement_service_name,\n            self._replacement_service_name,\n        )\n        section.write(' instead.')\n        section.style.end_important()\n\n\ndef document_copy_source_form(section, event_name, **kwargs):\n    if 'request-example' in event_name:\n        parent = section.get_section('structure-value')\n        param_line = parent.get_section('CopySource')\n        value_portion = param_line.get_section('member-value')\n        value_portion.clear_text()\n        value_portion.write(\n            \"'string' or {'Bucket': 'string', \"\n            \"'Key': 'string', 'VersionId': 'string'}\"\n        )\n    elif 'request-params' in event_name:\n        param_section = section.get_section('CopySource')\n        type_section = param_section.get_section('param-type')\n        type_section.clear_text()\n        type_section.write(':type CopySource: str or dict')\n        doc_section = param_section.get_section('param-documentation')\n        doc_section.clear_text()\n        doc_section.write(\n            \"The name of the source bucket, key name of the source object, \"\n            \"and optional version ID of the source object.  You can either \"\n            \"provide this value as a string or a dictionary.  The \"\n            \"string form is {bucket}/{key} or \"\n            \"{bucket}/{key}?versionId={versionId} if you want to copy a \"\n            \"specific version.  You can also provide this value as a \"\n            \"dictionary.  The dictionary format is recommended over \"\n            \"the string format because it is more explicit.  The dictionary \"\n            \"format is: {'Bucket': 'bucket', 'Key': 'key', 'VersionId': 'id'}.\"\n            \"  Note that the VersionId key is optional and may be omitted.\"\n            \" To specify an S3 access point, provide the access point\"\n            \" ARN for the ``Bucket`` key in the copy source dictionary. If you\"\n            \" want to provide the copy source for an S3 access point as a\"\n            \" string instead of a dictionary, the ARN provided must be the\"\n            \" full S3 access point object ARN\"\n            \" (i.e. {accesspoint_arn}/object/{key})\"\n        )\n\n\ndef handle_copy_source_param(params, **kwargs):\n    \"\"\"Convert CopySource param for CopyObject/UploadPartCopy.\n\n    This handler will deal with two cases:\n\n        * CopySource provided as a string.  We'll make a best effort\n          to URL encode the key name as required.  This will require\n          parsing the bucket and version id from the CopySource value\n          and only encoding the key.\n        * CopySource provided as a dict.  In this case we're\n          explicitly given the Bucket, Key, and VersionId so we're\n          able to encode the key and ensure this value is serialized\n          and correctly sent to S3.\n\n    \"\"\"\n    source = params.get('CopySource')\n    if source is None:\n        # The call will eventually fail but we'll let the\n        # param validator take care of this.  It will\n        # give a better error message.\n        return\n    if isinstance(source, str):\n        params['CopySource'] = _quote_source_header(source)\n    elif isinstance(source, dict):\n        params['CopySource'] = _quote_source_header_from_dict(source)\n\n\ndef _quote_source_header_from_dict(source_dict):\n    try:\n        bucket = source_dict['Bucket']\n        key = source_dict['Key']\n        version_id = source_dict.get('VersionId')\n        if VALID_S3_ARN.search(bucket):\n            final = f'{bucket}/object/{key}'\n        else:\n            final = f'{bucket}/{key}'\n    except KeyError as e:\n        raise ParamValidationError(\n            report=f'Missing required parameter: {str(e)}'\n        )\n    final = percent_encode(final, safe=SAFE_CHARS + '/')\n    if version_id is not None:\n        final += '?versionId=%s' % version_id\n    return final\n\n\ndef _quote_source_header(value):\n    result = VERSION_ID_SUFFIX.search(value)\n    if result is None:\n        return percent_encode(value, safe=SAFE_CHARS + '/')\n    else:\n        first, version_id = value[: result.start()], value[result.start() :]\n        return percent_encode(first, safe=SAFE_CHARS + '/') + version_id\n\n\ndef _get_cross_region_presigned_url(\n    request_signer, request_dict, model, source_region, destination_region\n):\n    # The better way to do this is to actually get the\n    # endpoint_resolver and get the endpoint_url given the\n    # source region.  In this specific case, we know that\n    # we can safely replace the dest region with the source\n    # region because of the supported EC2 regions, but in\n    # general this is not a safe assumption to make.\n    # I think eventually we should try to plumb through something\n    # that allows us to resolve endpoints from regions.\n    request_dict_copy = copy.deepcopy(request_dict)\n    request_dict_copy['body']['DestinationRegion'] = destination_region\n    request_dict_copy['url'] = request_dict['url'].replace(\n        destination_region, source_region\n    )\n    request_dict_copy['method'] = 'GET'\n    request_dict_copy['headers'] = {}\n    return request_signer.generate_presigned_url(\n        request_dict_copy, region_name=source_region, operation_name=model.name\n    )\n\n\ndef _get_presigned_url_source_and_destination_regions(request_signer, params):\n    # Gets the source and destination regions to be used\n    destination_region = request_signer._region_name\n    source_region = params.get('SourceRegion')\n    return source_region, destination_region\n\n\ndef inject_presigned_url_ec2(params, request_signer, model, **kwargs):\n    # The customer can still provide this, so we should pass if they do.\n    if 'PresignedUrl' in params['body']:\n        return\n    src, dest = _get_presigned_url_source_and_destination_regions(\n        request_signer, params['body']\n    )\n    url = _get_cross_region_presigned_url(\n        request_signer, params, model, src, dest\n    )\n    params['body']['PresignedUrl'] = url\n    # EC2 Requires that the destination region be sent over the wire in\n    # addition to the source region.\n    params['body']['DestinationRegion'] = dest\n\n\ndef inject_presigned_url_rds(params, request_signer, model, **kwargs):\n    # SourceRegion is not required for RDS operations, so it's possible that\n    # it isn't set. In that case it's probably a local copy so we don't need\n    # to do anything else.\n    if 'SourceRegion' not in params['body']:\n        return\n\n    src, dest = _get_presigned_url_source_and_destination_regions(\n        request_signer, params['body']\n    )\n\n    # Since SourceRegion isn't actually modeled for RDS, it needs to be\n    # removed from the request params before we send the actual request.\n    del params['body']['SourceRegion']\n\n    if 'PreSignedUrl' in params['body']:\n        return\n\n    url = _get_cross_region_presigned_url(\n        request_signer, params, model, src, dest\n    )\n    params['body']['PreSignedUrl'] = url\n\n\ndef json_decode_policies(parsed, model, **kwargs):\n    # Any time an IAM operation returns a policy document\n    # it is a string that is json that has been urlencoded,\n    # i.e urlencode(json.dumps(policy_document)).\n    # To give users something more useful, we will urldecode\n    # this value and json.loads() the result so that they have\n    # the policy document as a dictionary.\n    output_shape = model.output_shape\n    if output_shape is not None:\n        _decode_policy_types(parsed, model.output_shape)\n\n\ndef _decode_policy_types(parsed, shape):\n    # IAM consistently uses the policyDocumentType shape to indicate\n    # strings that have policy documents.\n    shape_name = 'policyDocumentType'\n    if shape.type_name == 'structure':\n        for member_name, member_shape in shape.members.items():\n            if (\n                member_shape.type_name == 'string'\n                and member_shape.name == shape_name\n                and member_name in parsed\n            ):\n                parsed[member_name] = decode_quoted_jsondoc(\n                    parsed[member_name]\n                )\n            elif member_name in parsed:\n                _decode_policy_types(parsed[member_name], member_shape)\n    if shape.type_name == 'list':\n        shape_member = shape.member\n        for item in parsed:\n            _decode_policy_types(item, shape_member)\n\n\ndef parse_get_bucket_location(parsed, http_response, **kwargs):\n    # s3.GetBucketLocation cannot be modeled properly.  To\n    # account for this we just manually parse the XML document.\n    # The \"parsed\" passed in only has the ResponseMetadata\n    # filled out.  This handler will fill in the LocationConstraint\n    # value.\n    if http_response.raw is None:\n        return\n    response_body = http_response.content\n    parser = ETree.XMLParser(target=ETree.TreeBuilder(), encoding='utf-8')\n    parser.feed(response_body)\n    root = parser.close()\n    region = root.text\n    parsed['LocationConstraint'] = region\n\n\ndef base64_encode_user_data(params, **kwargs):\n    if 'UserData' in params:\n        if isinstance(params['UserData'], str):\n            # Encode it to bytes if it is text.\n            params['UserData'] = params['UserData'].encode('utf-8')\n        params['UserData'] = base64.b64encode(params['UserData']).decode(\n            'utf-8'\n        )\n\n\ndef document_base64_encoding(param):\n    description = (\n        '**This value will be base64 encoded automatically. Do '\n        'not base64 encode this value prior to performing the '\n        'operation.**'\n    )\n    append = AppendParamDocumentation(param, description)\n    return append.append_documentation\n\n\ndef validate_ascii_metadata(params, **kwargs):\n    \"\"\"Verify S3 Metadata only contains ascii characters.\n\n    From: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html\n\n    \"Amazon S3 stores user-defined metadata in lowercase. Each name, value pair\n    must conform to US-ASCII when using REST and UTF-8 when using SOAP or\n    browser-based uploads via POST.\"\n\n    \"\"\"\n    metadata = params.get('Metadata')\n    if not metadata or not isinstance(metadata, dict):\n        # We have to at least type check the metadata as a dict type\n        # because this handler is called before param validation.\n        # We'll go ahead and return because the param validator will\n        # give a descriptive error message for us.\n        # We might need a post-param validation event.\n        return\n    for key, value in metadata.items():\n        try:\n            key.encode('ascii')\n            value.encode('ascii')\n        except UnicodeEncodeError:\n            error_msg = (\n                'Non ascii characters found in S3 metadata '\n                'for key \"%s\", value: \"%s\".  \\nS3 metadata can only '\n                'contain ASCII characters. ' % (key, value)\n            )\n            raise ParamValidationError(report=error_msg)\n\n\ndef fix_route53_ids(params, model, **kwargs):\n    \"\"\"\n    Check for and split apart Route53 resource IDs, setting\n    only the last piece. This allows the output of one operation\n    (e.g. ``'foo/1234'``) to be used as input in another\n    operation (e.g. it expects just ``'1234'``).\n    \"\"\"\n    input_shape = model.input_shape\n    if not input_shape or not hasattr(input_shape, 'members'):\n        return\n\n    members = [\n        name\n        for (name, shape) in input_shape.members.items()\n        if shape.name in ['ResourceId', 'DelegationSetId', 'ChangeId']\n    ]\n\n    for name in members:\n        if name in params:\n            orig_value = params[name]\n            params[name] = orig_value.split('/')[-1]\n            logger.debug('%s %s -> %s', name, orig_value, params[name])\n\n\ndef inject_account_id(params, **kwargs):\n    if params.get('accountId') is None:\n        # Glacier requires accountId, but allows you\n        # to specify '-' for the current owners account.\n        # We add this default value if the user does not\n        # provide the accountId as a convenience.\n        params['accountId'] = '-'\n\n\ndef add_glacier_version(model, params, **kwargs):\n    request_dict = params\n    request_dict['headers']['x-amz-glacier-version'] = model.metadata[\n        'apiVersion'\n    ]\n\n\ndef add_accept_header(model, params, **kwargs):\n    if params['headers'].get('Accept', None) is None:\n        request_dict = params\n        request_dict['headers']['Accept'] = 'application/json'\n\n\ndef add_glacier_checksums(params, **kwargs):\n    \"\"\"Add glacier checksums to the http request.\n\n    This will add two headers to the http request:\n\n        * x-amz-content-sha256\n        * x-amz-sha256-tree-hash\n\n    These values will only be added if they are not present\n    in the HTTP request.\n\n    \"\"\"\n    request_dict = params\n    headers = request_dict['headers']\n    body = request_dict['body']\n    if isinstance(body, bytes):\n        # If the user provided a bytes type instead of a file\n        # like object, we're temporarily create a BytesIO object\n        # so we can use the util functions to calculate the\n        # checksums which assume file like objects.  Note that\n        # we're not actually changing the body in the request_dict.\n        body = BytesIO(body)\n    starting_position = body.tell()\n    if 'x-amz-content-sha256' not in headers:\n        headers['x-amz-content-sha256'] = utils.calculate_sha256(\n            body, as_hex=True\n        )\n    body.seek(starting_position)\n    if 'x-amz-sha256-tree-hash' not in headers:\n        headers['x-amz-sha256-tree-hash'] = utils.calculate_tree_hash(body)\n    body.seek(starting_position)\n\n\ndef document_glacier_tree_hash_checksum():\n    doc = '''\n        This is a required field.\n\n        Ideally you will want to compute this value with checksums from\n        previous uploaded parts, using the algorithm described in\n        `Glacier documentation <http://docs.aws.amazon.com/amazonglacier/latest/dev/checksum-calculations.html>`_.\n\n        But if you prefer, you can also use botocore.utils.calculate_tree_hash()\n        to compute it from raw file by::\n\n            checksum = calculate_tree_hash(open('your_file.txt', 'rb'))\n\n        '''\n    return AppendParamDocumentation('checksum', doc).append_documentation\n\n\ndef document_cloudformation_get_template_return_type(\n    section, event_name, **kwargs\n):\n    if 'response-params' in event_name:\n        template_body_section = section.get_section('TemplateBody')\n        type_section = template_body_section.get_section('param-type')\n        type_section.clear_text()\n        type_section.write('(*dict*) --')\n    elif 'response-example' in event_name:\n        parent = section.get_section('structure-value')\n        param_line = parent.get_section('TemplateBody')\n        value_portion = param_line.get_section('member-value')\n        value_portion.clear_text()\n        value_portion.write('{}')\n\n\ndef switch_host_machinelearning(request, **kwargs):\n    switch_host_with_param(request, 'PredictEndpoint')\n\n\ndef check_openssl_supports_tls_version_1_2(**kwargs):\n    import ssl\n\n    try:\n        openssl_version_tuple = ssl.OPENSSL_VERSION_INFO\n        if openssl_version_tuple < (1, 0, 1):\n            warnings.warn(\n                'Currently installed openssl version: %s does not '\n                'support TLS 1.2, which is required for use of iot-data. '\n                'Please use python installed with openssl version 1.0.1 or '\n                'higher.' % (ssl.OPENSSL_VERSION),\n                UnsupportedTLSVersionWarning,\n            )\n    # We cannot check the openssl version on python2.6, so we should just\n    # pass on this conveniency check.\n    except AttributeError:\n        pass\n\n\ndef change_get_to_post(request, **kwargs):\n    # This is useful when we need to change a potentially large GET request\n    # into a POST with x-www-form-urlencoded encoding.\n    if request.method == 'GET' and '?' in request.url:\n        request.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n        request.method = 'POST'\n        request.url, request.data = request.url.split('?', 1)\n\n\ndef set_list_objects_encoding_type_url(params, context, **kwargs):\n    if 'EncodingType' not in params:\n        # We set this context so that we know it wasn't the customer that\n        # requested the encoding.\n        context['encoding_type_auto_set'] = True\n        params['EncodingType'] = 'url'\n\n\ndef decode_list_object(parsed, context, **kwargs):\n    # This is needed because we are passing url as the encoding type. Since the\n    # paginator is based on the key, we need to handle it before it can be\n    # round tripped.\n    #\n    # From the documentation: If you specify encoding-type request parameter,\n    # Amazon S3 includes this element in the response, and returns encoded key\n    # name values in the following response elements:\n    # Delimiter, Marker, Prefix, NextMarker, Key.\n    _decode_list_object(\n        top_level_keys=['Delimiter', 'Marker', 'NextMarker'],\n        nested_keys=[('Contents', 'Key'), ('CommonPrefixes', 'Prefix')],\n        parsed=parsed,\n        context=context,\n    )\n\n\ndef decode_list_object_v2(parsed, context, **kwargs):\n    # From the documentation: If you specify encoding-type request parameter,\n    # Amazon S3 includes this element in the response, and returns encoded key\n    # name values in the following response elements:\n    # Delimiter, Prefix, ContinuationToken, Key, and StartAfter.\n    _decode_list_object(\n        top_level_keys=['Delimiter', 'Prefix', 'StartAfter'],\n        nested_keys=[('Contents', 'Key'), ('CommonPrefixes', 'Prefix')],\n        parsed=parsed,\n        context=context,\n    )\n\n\ndef decode_list_object_versions(parsed, context, **kwargs):\n    # From the documentation: If you specify encoding-type request parameter,\n    # Amazon S3 includes this element in the response, and returns encoded key\n    # name values in the following response elements:\n    # KeyMarker, NextKeyMarker, Prefix, Key, and Delimiter.\n    _decode_list_object(\n        top_level_keys=[\n            'KeyMarker',\n            'NextKeyMarker',\n            'Prefix',\n            'Delimiter',\n        ],\n        nested_keys=[\n            ('Versions', 'Key'),\n            ('DeleteMarkers', 'Key'),\n            ('CommonPrefixes', 'Prefix'),\n        ],\n        parsed=parsed,\n        context=context,\n    )\n\n\ndef _decode_list_object(top_level_keys, nested_keys, parsed, context):\n    if parsed.get('EncodingType') == 'url' and context.get(\n        'encoding_type_auto_set'\n    ):\n        # URL decode top-level keys in the response if present.\n        for key in top_level_keys:\n            if key in parsed:\n                parsed[key] = unquote_str(parsed[key])\n        # URL decode nested keys from the response if present.\n        for top_key, child_key in nested_keys:\n            if top_key in parsed:\n                for member in parsed[top_key]:\n                    member[child_key] = unquote_str(member[child_key])\n\n\ndef convert_body_to_file_like_object(params, **kwargs):\n    if 'Body' in params:\n        if isinstance(params['Body'], str):\n            params['Body'] = BytesIO(ensure_bytes(params['Body']))\n        elif isinstance(params['Body'], bytes):\n            params['Body'] = BytesIO(params['Body'])\n\n\ndef _add_parameter_aliases(handler_list):\n    # Mapping of original parameter to parameter alias.\n    # The key is <service>.<operation>.parameter\n    # The first part of the key is used for event registration.\n    # The last part is the original parameter name and the value is the\n    # alias to expose in documentation.\n    aliases = {\n        'ec2.*.Filter': 'Filters',\n        'logs.CreateExportTask.from': 'fromTime',\n        'cloudsearchdomain.Search.return': 'returnFields',\n    }\n\n    for original, new_name in aliases.items():\n        event_portion, original_name = original.rsplit('.', 1)\n        parameter_alias = ParameterAlias(original_name, new_name)\n\n        # Add the handlers to the list of handlers.\n        # One handler is to handle when users provide the alias.\n        # The other handler is to update the documentation to show only\n        # the alias.\n        parameter_build_event_handler_tuple = (\n            'before-parameter-build.' + event_portion,\n            parameter_alias.alias_parameter_in_call,\n            REGISTER_FIRST,\n        )\n        docs_event_handler_tuple = (\n            'docs.*.' + event_portion + '.complete-section',\n            parameter_alias.alias_parameter_in_documentation,\n        )\n        handler_list.append(parameter_build_event_handler_tuple)\n        handler_list.append(docs_event_handler_tuple)\n\n\nclass ParameterAlias:\n    def __init__(self, original_name, alias_name):\n        self._original_name = original_name\n        self._alias_name = alias_name\n\n    def alias_parameter_in_call(self, params, model, **kwargs):\n        if model.input_shape:\n            # Only consider accepting the alias if it is modeled in the\n            # input shape.\n            if self._original_name in model.input_shape.members:\n                if self._alias_name in params:\n                    if self._original_name in params:\n                        raise AliasConflictParameterError(\n                            original=self._original_name,\n                            alias=self._alias_name,\n                            operation=model.name,\n                        )\n                    # Remove the alias parameter value and use the old name\n                    # instead.\n                    params[self._original_name] = params.pop(self._alias_name)\n\n    def alias_parameter_in_documentation(self, event_name, section, **kwargs):\n        if event_name.startswith('docs.request-params'):\n            if self._original_name not in section.available_sections:\n                return\n            # Replace the name for parameter type\n            param_section = section.get_section(self._original_name)\n            param_type_section = param_section.get_section('param-type')\n            self._replace_content(param_type_section)\n\n            # Replace the name for the parameter description\n            param_name_section = param_section.get_section('param-name')\n            self._replace_content(param_name_section)\n        elif event_name.startswith('docs.request-example'):\n            section = section.get_section('structure-value')\n            if self._original_name not in section.available_sections:\n                return\n            # Replace the name for the example\n            param_section = section.get_section(self._original_name)\n            self._replace_content(param_section)\n\n    def _replace_content(self, section):\n        content = section.getvalue().decode('utf-8')\n        updated_content = content.replace(\n            self._original_name, self._alias_name\n        )\n        section.clear_text()\n        section.write(updated_content)\n\n\nclass ClientMethodAlias:\n    def __init__(self, actual_name):\n        \"\"\"Aliases a non-extant method to an existing method.\n\n        :param actual_name: The name of the method that actually exists on\n            the client.\n        \"\"\"\n        self._actual = actual_name\n\n    def __call__(self, client, **kwargs):\n        return getattr(client, self._actual)\n\n\n# TODO: Remove this class as it is no longer used\nclass HeaderToHostHoister:\n    \"\"\"Takes a header and moves it to the front of the hoststring.\"\"\"\n\n    _VALID_HOSTNAME = re.compile(r'(?!-)[a-z\\d-]{1,63}(?<!-)$', re.IGNORECASE)\n\n    def __init__(self, header_name):\n        self._header_name = header_name\n\n    def hoist(self, params, **kwargs):\n        \"\"\"Hoist a header to the hostname.\n\n        Hoist a header to the beginning of the hostname with a suffix \".\" after\n        it. The original header should be removed from the header map. This\n        method is intended to be used as a target for the before-call event.\n        \"\"\"\n        if self._header_name not in params['headers']:\n            return\n        header_value = params['headers'][self._header_name]\n        self._ensure_header_is_valid_host(header_value)\n        original_url = params['url']\n        new_url = self._prepend_to_host(original_url, header_value)\n        params['url'] = new_url\n\n    def _ensure_header_is_valid_host(self, header):\n        match = self._VALID_HOSTNAME.match(header)\n        if not match:\n            raise ParamValidationError(\n                report=(\n                    'Hostnames must contain only - and alphanumeric characters, '\n                    'and between 1 and 63 characters long.'\n                )\n            )\n\n    def _prepend_to_host(self, url, prefix):\n        url_components = urlsplit(url)\n        parts = url_components.netloc.split('.')\n        parts = [prefix] + parts\n        new_netloc = '.'.join(parts)\n        new_components = (\n            url_components.scheme,\n            new_netloc,\n            url_components.path,\n            url_components.query,\n            '',\n        )\n        new_url = urlunsplit(new_components)\n        return new_url\n\n\ndef inject_api_version_header_if_needed(model, params, **kwargs):\n    if not model.is_endpoint_discovery_operation:\n        return\n    params['headers']['x-amz-api-version'] = model.service_model.api_version\n\n\ndef remove_lex_v2_start_conversation(class_attributes, **kwargs):\n    \"\"\"Operation requires h2 which is currently unsupported in Python\"\"\"\n    if 'start_conversation' in class_attributes:\n        del class_attributes['start_conversation']\n\n\ndef remove_qbusiness_chat(class_attributes, **kwargs):\n    \"\"\"Operation requires h2 which is currently unsupported in Python\"\"\"\n    if 'chat' in class_attributes:\n        del class_attributes['chat']\n\n\ndef add_retry_headers(request, **kwargs):\n    retries_context = request.context.get('retries')\n    if not retries_context:\n        return\n    headers = request.headers\n    headers['amz-sdk-invocation-id'] = retries_context['invocation-id']\n    sdk_retry_keys = ('ttl', 'attempt', 'max')\n    sdk_request_headers = [\n        f'{key}={retries_context[key]}'\n        for key in sdk_retry_keys\n        if key in retries_context\n    ]\n    headers['amz-sdk-request'] = '; '.join(sdk_request_headers)\n\n\ndef remove_bucket_from_url_paths_from_model(params, model, context, **kwargs):\n    \"\"\"Strips leading `{Bucket}/` from any operations that have it.\n\n    The original value is retained in a separate \"authPath\" field. This is\n    used in the HmacV1Auth signer. See HmacV1Auth.canonical_resource in\n    botocore/auth.py for details.\n\n    This change is applied to the operation model during the first time the\n    operation is invoked and then stays in effect for the lifetime of the\n    client object.\n\n    When the ruleset based endpoint resolver is in effect, both the endpoint\n    ruleset AND the service model place the bucket name in the final URL.\n    The result is an invalid URL. This handler modifies the operation model to\n    no longer place the bucket name. Previous versions of botocore fixed the\n    URL after the fact when necessary. Since the introduction of ruleset based\n    endpoint resolution, the problem exists in ALL URLs that contain a bucket\n    name and can therefore be addressed before the URL gets assembled.\n    \"\"\"\n    req_uri = model.http['requestUri']\n    bucket_path = '/{Bucket}'\n    if req_uri.startswith(bucket_path):\n        model.http['requestUri'] = req_uri[len(bucket_path) :]\n        # Strip query off the requestUri before using as authPath. The\n        # HmacV1Auth signer will append query params to the authPath during\n        # signing.\n        req_uri = req_uri.split('?')[0]\n        # If the request URI is ONLY a bucket, the auth_path must be\n        # terminated with a '/' character to generate a signature that the\n        # server will accept.\n        needs_slash = req_uri == bucket_path\n        model.http['authPath'] = f'{req_uri}/' if needs_slash else req_uri\n\n\ndef remove_accid_host_prefix_from_model(params, model, context, **kwargs):\n    \"\"\"Removes the `{AccountId}.` prefix from the operation model.\n\n    This change is applied to the operation model during the first time the\n    operation is invoked and then stays in effect for the lifetime of the\n    client object.\n\n    When the ruleset based endpoint resolver is in effect, both the endpoint\n    ruleset AND the service model place the {AccountId}. prefix in the URL.\n    The result is an invalid endpoint. This handler modifies the operation\n    model to remove the `endpoint.hostPrefix` field while leaving the\n    `RequiresAccountId` static context parameter in place.\n    \"\"\"\n    has_ctx_param = any(\n        ctx_param.name == 'RequiresAccountId' and ctx_param.value is True\n        for ctx_param in model.static_context_parameters\n    )\n    if (\n        model.endpoint is not None\n        and model.endpoint.get('hostPrefix') == '{AccountId}.'\n        and has_ctx_param\n    ):\n        del model.endpoint['hostPrefix']\n\n\ndef remove_arn_from_signing_path(request, **kwargs):\n    auth_path = request.auth_path\n    if isinstance(auth_path, str) and auth_path.startswith('/arn%3A'):\n        auth_path_parts = auth_path.split('/')\n        if len(auth_path_parts) > 1 and ArnParser.is_arn(\n            unquote(auth_path_parts[1])\n        ):\n            request.auth_path = '/'.join(['', *auth_path_parts[2:]])\n\n\ndef customize_endpoint_resolver_builtins(\n    builtins, model, params, context, **kwargs\n):\n    \"\"\"Modify builtin parameter values for endpoint resolver\n\n    Modifies the builtins dict in place. Changes are in effect for one call.\n    The corresponding event is emitted only if at least one builtin parameter\n    value is required for endpoint resolution for the operation.\n    \"\"\"\n    bucket_name = params.get('Bucket')\n    bucket_is_arn = bucket_name is not None and ArnParser.is_arn(bucket_name)\n    # In some situations the host will return AuthorizationHeaderMalformed\n    # when the signing region of a sigv4 request is not the bucket's\n    # region (which is likely unknown by the user of GetBucketLocation).\n    # Avoid this by always using path-style addressing.\n    if model.name == 'GetBucketLocation':\n        builtins[EndpointResolverBuiltins.AWS_S3_FORCE_PATH_STYLE] = True\n    # All situations where the bucket name is an ARN are not compatible\n    # with path style addressing.\n    elif bucket_is_arn:\n        builtins[EndpointResolverBuiltins.AWS_S3_FORCE_PATH_STYLE] = False\n\n    # Bucket names that are invalid host labels require path-style addressing.\n    # If path-style addressing was specifically requested, the default builtin\n    # value is already set.\n    path_style_required = (\n        bucket_name is not None and not VALID_HOST_LABEL_RE.match(bucket_name)\n    )\n    path_style_requested = builtins[\n        EndpointResolverBuiltins.AWS_S3_FORCE_PATH_STYLE\n    ]\n\n    # Path-style addressing is incompatible with the global endpoint for\n    # presigned URLs. If the bucket name is an ARN, the ARN's region should be\n    # used in the endpoint.\n    if (\n        context.get('use_global_endpoint')\n        and not path_style_required\n        and not path_style_requested\n        and not bucket_is_arn\n        and not utils.is_s3express_bucket(bucket_name)\n    ):\n        builtins[EndpointResolverBuiltins.AWS_REGION] = 'aws-global'\n        builtins[EndpointResolverBuiltins.AWS_S3_USE_GLOBAL_ENDPOINT] = True\n\n\ndef remove_content_type_header_for_presigning(request, **kwargs):\n    if (\n        request.context.get('is_presign_request') is True\n        and 'Content-Type' in request.headers\n    ):\n        del request.headers['Content-Type']\n\n\n# This is a list of (event_name, handler).\n# When a Session is created, everything in this list will be\n# automatically registered with that Session.\n\nBUILTIN_HANDLERS = [\n    ('choose-service-name', handle_service_name_alias),\n    (\n        'getattr.mturk.list_hi_ts_for_qualification_type',\n        ClientMethodAlias('list_hits_for_qualification_type'),\n    ),\n    (\n        'before-parameter-build.s3.UploadPart',\n        convert_body_to_file_like_object,\n        REGISTER_LAST,\n    ),\n    (\n        'before-parameter-build.s3.PutObject',\n        convert_body_to_file_like_object,\n        REGISTER_LAST,\n    ),\n    ('creating-client-class', add_generate_presigned_url),\n    ('creating-client-class.s3', add_generate_presigned_post),\n    ('creating-client-class.iot-data', check_openssl_supports_tls_version_1_2),\n    ('creating-client-class.lex-runtime-v2', remove_lex_v2_start_conversation),\n    ('creating-client-class.qbusiness', remove_qbusiness_chat),\n    ('after-call.iam', json_decode_policies),\n    ('after-call.ec2.GetConsoleOutput', decode_console_output),\n    ('after-call.cloudformation.GetTemplate', json_decode_template_body),\n    ('after-call.s3.GetBucketLocation', parse_get_bucket_location),\n    ('before-parameter-build', generate_idempotent_uuid),\n    ('before-parameter-build.s3', validate_bucket_name),\n    ('before-parameter-build.s3', remove_bucket_from_url_paths_from_model),\n    (\n        'before-parameter-build.s3.ListObjects',\n        set_list_objects_encoding_type_url,\n    ),\n    (\n        'before-parameter-build.s3.ListObjectsV2',\n        set_list_objects_encoding_type_url,\n    ),\n    (\n        'before-parameter-build.s3.ListObjectVersions',\n        set_list_objects_encoding_type_url,\n    ),\n    ('before-parameter-build.s3.CopyObject', handle_copy_source_param),\n    ('before-parameter-build.s3.UploadPartCopy', handle_copy_source_param),\n    ('before-parameter-build.s3.CopyObject', validate_ascii_metadata),\n    ('before-parameter-build.s3.PutObject', validate_ascii_metadata),\n    (\n        'before-parameter-build.s3.CreateMultipartUpload',\n        validate_ascii_metadata,\n    ),\n    ('before-parameter-build.s3-control', remove_accid_host_prefix_from_model),\n    ('docs.*.s3.CopyObject.complete-section', document_copy_source_form),\n    ('docs.*.s3.UploadPartCopy.complete-section', document_copy_source_form),\n    ('before-endpoint-resolution.s3', customize_endpoint_resolver_builtins),\n    ('before-call', add_recursion_detection_header),\n    ('before-call.s3', add_expect_header),\n    ('before-call.glacier', add_glacier_version),\n    ('before-call.apigateway', add_accept_header),\n    ('before-call.s3.PutObject', conditionally_calculate_checksum),\n    ('before-call.s3.UploadPart', conditionally_calculate_md5),\n    ('before-call.s3.DeleteObjects', escape_xml_payload),\n    ('before-call.s3.DeleteObjects', conditionally_calculate_checksum),\n    ('before-call.s3.PutBucketLifecycleConfiguration', escape_xml_payload),\n    ('before-call.glacier.UploadArchive', add_glacier_checksums),\n    ('before-call.glacier.UploadMultipartPart', add_glacier_checksums),\n    ('before-call.ec2.CopySnapshot', inject_presigned_url_ec2),\n    ('request-created', add_retry_headers),\n    ('request-created.machinelearning.Predict', switch_host_machinelearning),\n    ('needs-retry.s3.UploadPartCopy', check_for_200_error, REGISTER_FIRST),\n    ('needs-retry.s3.CopyObject', check_for_200_error, REGISTER_FIRST),\n    (\n        'needs-retry.s3.CompleteMultipartUpload',\n        check_for_200_error,\n        REGISTER_FIRST,\n    ),\n    ('choose-signer.cognito-identity.GetId', disable_signing),\n    ('choose-signer.cognito-identity.GetOpenIdToken', disable_signing),\n    ('choose-signer.cognito-identity.UnlinkIdentity', disable_signing),\n    (\n        'choose-signer.cognito-identity.GetCredentialsForIdentity',\n        disable_signing,\n    ),\n    ('choose-signer.sts.AssumeRoleWithSAML', disable_signing),\n    ('choose-signer.sts.AssumeRoleWithWebIdentity', disable_signing),\n    ('choose-signer', set_operation_specific_signer),\n    ('before-parameter-build.s3.HeadObject', sse_md5),\n    ('before-parameter-build.s3.GetObject', sse_md5),\n    ('before-parameter-build.s3.PutObject', sse_md5),\n    ('before-parameter-build.s3.CopyObject', sse_md5),\n    ('before-parameter-build.s3.CopyObject', copy_source_sse_md5),\n    ('before-parameter-build.s3.CreateMultipartUpload', sse_md5),\n    ('before-parameter-build.s3.UploadPart', sse_md5),\n    ('before-parameter-build.s3.UploadPartCopy', sse_md5),\n    ('before-parameter-build.s3.UploadPartCopy', copy_source_sse_md5),\n    ('before-parameter-build.s3.CompleteMultipartUpload', sse_md5),\n    ('before-parameter-build.s3.SelectObjectContent', sse_md5),\n    ('before-parameter-build.ec2.RunInstances', base64_encode_user_data),\n    (\n        'before-parameter-build.autoscaling.CreateLaunchConfiguration',\n        base64_encode_user_data,\n    ),\n    ('before-parameter-build.route53', fix_route53_ids),\n    ('before-parameter-build.glacier', inject_account_id),\n    ('before-sign.s3', remove_arn_from_signing_path),\n    (\n        'before-sign.polly.SynthesizeSpeech',\n        remove_content_type_header_for_presigning,\n    ),\n    ('after-call.s3.ListObjects', decode_list_object),\n    ('after-call.s3.ListObjectsV2', decode_list_object_v2),\n    ('after-call.s3.ListObjectVersions', decode_list_object_versions),\n    # Cloudsearchdomain search operation will be sent by HTTP POST\n    ('request-created.cloudsearchdomain.Search', change_get_to_post),\n    # Glacier documentation customizations\n    (\n        'docs.*.glacier.*.complete-section',\n        AutoPopulatedParam(\n            'accountId',\n            'Note: this parameter is set to \"-\" by'\n            'default if no value is not specified.',\n        ).document_auto_populated_param,\n    ),\n    (\n        'docs.*.glacier.UploadArchive.complete-section',\n        AutoPopulatedParam('checksum').document_auto_populated_param,\n    ),\n    (\n        'docs.*.glacier.UploadMultipartPart.complete-section',\n        AutoPopulatedParam('checksum').document_auto_populated_param,\n    ),\n    (\n        'docs.request-params.glacier.CompleteMultipartUpload.complete-section',\n        document_glacier_tree_hash_checksum(),\n    ),\n    # Cloudformation documentation customizations\n    (\n        'docs.*.cloudformation.GetTemplate.complete-section',\n        document_cloudformation_get_template_return_type,\n    ),\n    # UserData base64 encoding documentation customizations\n    (\n        'docs.*.ec2.RunInstances.complete-section',\n        document_base64_encoding('UserData'),\n    ),\n    (\n        'docs.*.autoscaling.CreateLaunchConfiguration.complete-section',\n        document_base64_encoding('UserData'),\n    ),\n    # EC2 CopySnapshot documentation customizations\n    (\n        'docs.*.ec2.CopySnapshot.complete-section',\n        AutoPopulatedParam('PresignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.ec2.CopySnapshot.complete-section',\n        AutoPopulatedParam('DestinationRegion').document_auto_populated_param,\n    ),\n    # S3 SSE documentation modifications\n    (\n        'docs.*.s3.*.complete-section',\n        AutoPopulatedParam('SSECustomerKeyMD5').document_auto_populated_param,\n    ),\n    # S3 SSE Copy Source documentation modifications\n    (\n        'docs.*.s3.*.complete-section',\n        AutoPopulatedParam(\n            'CopySourceSSECustomerKeyMD5'\n        ).document_auto_populated_param,\n    ),\n    # Add base64 information to Lambda\n    (\n        'docs.*.lambda.UpdateFunctionCode.complete-section',\n        document_base64_encoding('ZipFile'),\n    ),\n    # The following S3 operations cannot actually accept a ContentMD5\n    (\n        'docs.*.s3.*.complete-section',\n        HideParamFromOperations(\n            's3',\n            'ContentMD5',\n            [\n                'DeleteObjects',\n                'PutBucketAcl',\n                'PutBucketCors',\n                'PutBucketLifecycle',\n                'PutBucketLogging',\n                'PutBucketNotification',\n                'PutBucketPolicy',\n                'PutBucketReplication',\n                'PutBucketRequestPayment',\n                'PutBucketTagging',\n                'PutBucketVersioning',\n                'PutBucketWebsite',\n                'PutObjectAcl',\n            ],\n        ).hide_param,\n    ),\n    #############\n    # RDS\n    #############\n    ('creating-client-class.rds', add_generate_db_auth_token),\n    ('before-call.rds.CopyDBClusterSnapshot', inject_presigned_url_rds),\n    ('before-call.rds.CreateDBCluster', inject_presigned_url_rds),\n    ('before-call.rds.CopyDBSnapshot', inject_presigned_url_rds),\n    ('before-call.rds.CreateDBInstanceReadReplica', inject_presigned_url_rds),\n    (\n        'before-call.rds.StartDBInstanceAutomatedBackupsReplication',\n        inject_presigned_url_rds,\n    ),\n    # RDS PresignedUrl documentation customizations\n    (\n        'docs.*.rds.CopyDBClusterSnapshot.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.rds.CreateDBCluster.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.rds.CopyDBSnapshot.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.rds.CreateDBInstanceReadReplica.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.rds.StartDBInstanceAutomatedBackupsReplication.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    #############\n    # Neptune\n    #############\n    ('before-call.neptune.CopyDBClusterSnapshot', inject_presigned_url_rds),\n    ('before-call.neptune.CreateDBCluster', inject_presigned_url_rds),\n    # Neptune PresignedUrl documentation customizations\n    (\n        'docs.*.neptune.CopyDBClusterSnapshot.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.neptune.CreateDBCluster.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    #############\n    # DocDB\n    #############\n    ('before-call.docdb.CopyDBClusterSnapshot', inject_presigned_url_rds),\n    ('before-call.docdb.CreateDBCluster', inject_presigned_url_rds),\n    # DocDB PresignedUrl documentation customizations\n    (\n        'docs.*.docdb.CopyDBClusterSnapshot.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.docdb.CreateDBCluster.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    ('before-call', inject_api_version_header_if_needed),\n]\n_add_parameter_aliases(BUILTIN_HANDLERS)\n", "botocore/args.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Internal module to help with normalizing botocore client args.\n\nThis module (and all function/classes within this module) should be\nconsidered internal, and *not* a public API.\n\n\"\"\"\nimport copy\nimport logging\nimport socket\n\nimport botocore.exceptions\nimport botocore.parsers\nimport botocore.serialize\nfrom botocore.config import Config\nfrom botocore.endpoint import EndpointCreator\nfrom botocore.regions import EndpointResolverBuiltins as EPRBuiltins\nfrom botocore.regions import EndpointRulesetResolver\nfrom botocore.signers import RequestSigner\nfrom botocore.useragent import UserAgentString\nfrom botocore.utils import ensure_boolean, is_s3_accelerate_url\n\nlogger = logging.getLogger(__name__)\n\n\nVALID_REGIONAL_ENDPOINTS_CONFIG = [\n    'legacy',\n    'regional',\n]\nLEGACY_GLOBAL_STS_REGIONS = [\n    'ap-northeast-1',\n    'ap-south-1',\n    'ap-southeast-1',\n    'ap-southeast-2',\n    'aws-global',\n    'ca-central-1',\n    'eu-central-1',\n    'eu-north-1',\n    'eu-west-1',\n    'eu-west-2',\n    'eu-west-3',\n    'sa-east-1',\n    'us-east-1',\n    'us-east-2',\n    'us-west-1',\n    'us-west-2',\n]\n# Maximum allowed length of the ``user_agent_appid`` config field. Longer\n# values result in a warning-level log message.\nUSERAGENT_APPID_MAXLEN = 50\n\n\nclass ClientArgsCreator:\n    def __init__(\n        self,\n        event_emitter,\n        user_agent,\n        response_parser_factory,\n        loader,\n        exceptions_factory,\n        config_store,\n        user_agent_creator=None,\n    ):\n        self._event_emitter = event_emitter\n        self._response_parser_factory = response_parser_factory\n        self._loader = loader\n        self._exceptions_factory = exceptions_factory\n        self._config_store = config_store\n        if user_agent_creator is None:\n            self._session_ua_creator = UserAgentString.from_environment()\n        else:\n            self._session_ua_creator = user_agent_creator\n\n    def get_client_args(\n        self,\n        service_model,\n        region_name,\n        is_secure,\n        endpoint_url,\n        verify,\n        credentials,\n        scoped_config,\n        client_config,\n        endpoint_bridge,\n        auth_token=None,\n        endpoints_ruleset_data=None,\n        partition_data=None,\n    ):\n        final_args = self.compute_client_args(\n            service_model,\n            client_config,\n            endpoint_bridge,\n            region_name,\n            endpoint_url,\n            is_secure,\n            scoped_config,\n        )\n\n        service_name = final_args['service_name']  # noqa\n        parameter_validation = final_args['parameter_validation']\n        endpoint_config = final_args['endpoint_config']\n        protocol = final_args['protocol']\n        config_kwargs = final_args['config_kwargs']\n        s3_config = final_args['s3_config']\n        partition = endpoint_config['metadata'].get('partition', None)\n        socket_options = final_args['socket_options']\n        configured_endpoint_url = final_args['configured_endpoint_url']\n        signing_region = endpoint_config['signing_region']\n        endpoint_region_name = endpoint_config['region_name']\n\n        event_emitter = copy.copy(self._event_emitter)\n        signer = RequestSigner(\n            service_model.service_id,\n            signing_region,\n            endpoint_config['signing_name'],\n            endpoint_config['signature_version'],\n            credentials,\n            event_emitter,\n            auth_token,\n        )\n\n        config_kwargs['s3'] = s3_config\n        new_config = Config(**config_kwargs)\n        endpoint_creator = EndpointCreator(event_emitter)\n\n        endpoint = endpoint_creator.create_endpoint(\n            service_model,\n            region_name=endpoint_region_name,\n            endpoint_url=endpoint_config['endpoint_url'],\n            verify=verify,\n            response_parser_factory=self._response_parser_factory,\n            max_pool_connections=new_config.max_pool_connections,\n            proxies=new_config.proxies,\n            timeout=(new_config.connect_timeout, new_config.read_timeout),\n            socket_options=socket_options,\n            client_cert=new_config.client_cert,\n            proxies_config=new_config.proxies_config,\n        )\n\n        serializer = botocore.serialize.create_serializer(\n            protocol, parameter_validation\n        )\n        response_parser = botocore.parsers.create_parser(protocol)\n\n        ruleset_resolver = self._build_endpoint_resolver(\n            endpoints_ruleset_data,\n            partition_data,\n            client_config,\n            service_model,\n            endpoint_region_name,\n            region_name,\n            configured_endpoint_url,\n            endpoint,\n            is_secure,\n            endpoint_bridge,\n            event_emitter,\n        )\n\n        # Copy the session's user agent factory and adds client configuration.\n        client_ua_creator = self._session_ua_creator.with_client_config(\n            new_config\n        )\n        supplied_ua = client_config.user_agent if client_config else None\n        new_config._supplied_user_agent = supplied_ua\n\n        return {\n            'serializer': serializer,\n            'endpoint': endpoint,\n            'response_parser': response_parser,\n            'event_emitter': event_emitter,\n            'request_signer': signer,\n            'service_model': service_model,\n            'loader': self._loader,\n            'client_config': new_config,\n            'partition': partition,\n            'exceptions_factory': self._exceptions_factory,\n            'endpoint_ruleset_resolver': ruleset_resolver,\n            'user_agent_creator': client_ua_creator,\n        }\n\n    def compute_client_args(\n        self,\n        service_model,\n        client_config,\n        endpoint_bridge,\n        region_name,\n        endpoint_url,\n        is_secure,\n        scoped_config,\n    ):\n        service_name = service_model.endpoint_prefix\n        protocol = service_model.metadata['protocol']\n        parameter_validation = True\n        if client_config and not client_config.parameter_validation:\n            parameter_validation = False\n        elif scoped_config:\n            raw_value = scoped_config.get('parameter_validation')\n            if raw_value is not None:\n                parameter_validation = ensure_boolean(raw_value)\n\n        s3_config = self.compute_s3_config(client_config)\n\n        configured_endpoint_url = self._compute_configured_endpoint_url(\n            client_config=client_config,\n            endpoint_url=endpoint_url,\n        )\n\n        endpoint_config = self._compute_endpoint_config(\n            service_name=service_name,\n            region_name=region_name,\n            endpoint_url=configured_endpoint_url,\n            is_secure=is_secure,\n            endpoint_bridge=endpoint_bridge,\n            s3_config=s3_config,\n        )\n        endpoint_variant_tags = endpoint_config['metadata'].get('tags', [])\n\n        # Some third-party libraries expect the final user-agent string in\n        # ``client.meta.config.user_agent``. To maintain backwards\n        # compatibility, the preliminary user-agent string (before any Config\n        # object modifications and without request-specific user-agent\n        # components) is stored in the new Config object's ``user_agent``\n        # property but not used by Botocore itself.\n        preliminary_ua_string = self._session_ua_creator.with_client_config(\n            client_config\n        ).to_string()\n        # Create a new client config to be passed to the client based\n        # on the final values. We do not want the user to be able\n        # to try to modify an existing client with a client config.\n        config_kwargs = dict(\n            region_name=endpoint_config['region_name'],\n            signature_version=endpoint_config['signature_version'],\n            user_agent=preliminary_ua_string,\n        )\n        if 'dualstack' in endpoint_variant_tags:\n            config_kwargs.update(use_dualstack_endpoint=True)\n        if 'fips' in endpoint_variant_tags:\n            config_kwargs.update(use_fips_endpoint=True)\n        if client_config is not None:\n            config_kwargs.update(\n                connect_timeout=client_config.connect_timeout,\n                read_timeout=client_config.read_timeout,\n                max_pool_connections=client_config.max_pool_connections,\n                proxies=client_config.proxies,\n                proxies_config=client_config.proxies_config,\n                retries=client_config.retries,\n                client_cert=client_config.client_cert,\n                inject_host_prefix=client_config.inject_host_prefix,\n                tcp_keepalive=client_config.tcp_keepalive,\n                user_agent_extra=client_config.user_agent_extra,\n                user_agent_appid=client_config.user_agent_appid,\n                request_min_compression_size_bytes=(\n                    client_config.request_min_compression_size_bytes\n                ),\n                disable_request_compression=(\n                    client_config.disable_request_compression\n                ),\n                client_context_params=client_config.client_context_params,\n            )\n        self._compute_retry_config(config_kwargs)\n        self._compute_connect_timeout(config_kwargs)\n        self._compute_user_agent_appid_config(config_kwargs)\n        self._compute_request_compression_config(config_kwargs)\n        s3_config = self.compute_s3_config(client_config)\n\n        is_s3_service = self._is_s3_service(service_name)\n\n        if is_s3_service and 'dualstack' in endpoint_variant_tags:\n            if s3_config is None:\n                s3_config = {}\n            s3_config['use_dualstack_endpoint'] = True\n\n        return {\n            'service_name': service_name,\n            'parameter_validation': parameter_validation,\n            'configured_endpoint_url': configured_endpoint_url,\n            'endpoint_config': endpoint_config,\n            'protocol': protocol,\n            'config_kwargs': config_kwargs,\n            's3_config': s3_config,\n            'socket_options': self._compute_socket_options(\n                scoped_config, client_config\n            ),\n        }\n\n    def _compute_configured_endpoint_url(self, client_config, endpoint_url):\n        if endpoint_url is not None:\n            return endpoint_url\n\n        if self._ignore_configured_endpoint_urls(client_config):\n            logger.debug(\"Ignoring configured endpoint URLs.\")\n            return endpoint_url\n\n        return self._config_store.get_config_variable('endpoint_url')\n\n    def _ignore_configured_endpoint_urls(self, client_config):\n        if (\n            client_config\n            and client_config.ignore_configured_endpoint_urls is not None\n        ):\n            return client_config.ignore_configured_endpoint_urls\n\n        return self._config_store.get_config_variable(\n            'ignore_configured_endpoint_urls'\n        )\n\n    def compute_s3_config(self, client_config):\n        s3_configuration = self._config_store.get_config_variable('s3')\n\n        # Next specific client config values takes precedence over\n        # specific values in the scoped config.\n        if client_config is not None:\n            if client_config.s3 is not None:\n                if s3_configuration is None:\n                    s3_configuration = client_config.s3\n                else:\n                    # The current s3_configuration dictionary may be\n                    # from a source that only should be read from so\n                    # we want to be safe and just make a copy of it to modify\n                    # before it actually gets updated.\n                    s3_configuration = s3_configuration.copy()\n                    s3_configuration.update(client_config.s3)\n\n        return s3_configuration\n\n    def _is_s3_service(self, service_name):\n        \"\"\"Whether the service is S3 or S3 Control.\n\n        Note that throughout this class, service_name refers to the endpoint\n        prefix, not the folder name of the service in botocore/data. For\n        S3 Control, the folder name is 's3control' but the endpoint prefix is\n        's3-control'.\n        \"\"\"\n        return service_name in ['s3', 's3-control']\n\n    def _compute_endpoint_config(\n        self,\n        service_name,\n        region_name,\n        endpoint_url,\n        is_secure,\n        endpoint_bridge,\n        s3_config,\n    ):\n        resolve_endpoint_kwargs = {\n            'service_name': service_name,\n            'region_name': region_name,\n            'endpoint_url': endpoint_url,\n            'is_secure': is_secure,\n            'endpoint_bridge': endpoint_bridge,\n        }\n        if service_name == 's3':\n            return self._compute_s3_endpoint_config(\n                s3_config=s3_config, **resolve_endpoint_kwargs\n            )\n        if service_name == 'sts':\n            return self._compute_sts_endpoint_config(**resolve_endpoint_kwargs)\n        return self._resolve_endpoint(**resolve_endpoint_kwargs)\n\n    def _compute_s3_endpoint_config(\n        self, s3_config, **resolve_endpoint_kwargs\n    ):\n        force_s3_global = self._should_force_s3_global(\n            resolve_endpoint_kwargs['region_name'], s3_config\n        )\n        if force_s3_global:\n            resolve_endpoint_kwargs['region_name'] = None\n        endpoint_config = self._resolve_endpoint(**resolve_endpoint_kwargs)\n        self._set_region_if_custom_s3_endpoint(\n            endpoint_config, resolve_endpoint_kwargs['endpoint_bridge']\n        )\n        # For backwards compatibility reasons, we want to make sure the\n        # client.meta.region_name will remain us-east-1 if we forced the\n        # endpoint to be the global region. Specifically, if this value\n        # changes to aws-global, it breaks logic where a user is checking\n        # for us-east-1 as the global endpoint such as in creating buckets.\n        if force_s3_global and endpoint_config['region_name'] == 'aws-global':\n            endpoint_config['region_name'] = 'us-east-1'\n        return endpoint_config\n\n    def _should_force_s3_global(self, region_name, s3_config):\n        s3_regional_config = 'legacy'\n        if s3_config and 'us_east_1_regional_endpoint' in s3_config:\n            s3_regional_config = s3_config['us_east_1_regional_endpoint']\n            self._validate_s3_regional_config(s3_regional_config)\n\n        is_global_region = region_name in ('us-east-1', None)\n        return s3_regional_config == 'legacy' and is_global_region\n\n    def _validate_s3_regional_config(self, config_val):\n        if config_val not in VALID_REGIONAL_ENDPOINTS_CONFIG:\n            raise botocore.exceptions.InvalidS3UsEast1RegionalEndpointConfigError(\n                s3_us_east_1_regional_endpoint_config=config_val\n            )\n\n    def _set_region_if_custom_s3_endpoint(\n        self, endpoint_config, endpoint_bridge\n    ):\n        # If a user is providing a custom URL, the endpoint resolver will\n        # refuse to infer a signing region. If we want to default to s3v4,\n        # we have to account for this.\n        if (\n            endpoint_config['signing_region'] is None\n            and endpoint_config['region_name'] is None\n        ):\n            endpoint = endpoint_bridge.resolve('s3')\n            endpoint_config['signing_region'] = endpoint['signing_region']\n            endpoint_config['region_name'] = endpoint['region_name']\n\n    def _compute_sts_endpoint_config(self, **resolve_endpoint_kwargs):\n        endpoint_config = self._resolve_endpoint(**resolve_endpoint_kwargs)\n        if self._should_set_global_sts_endpoint(\n            resolve_endpoint_kwargs['region_name'],\n            resolve_endpoint_kwargs['endpoint_url'],\n            endpoint_config,\n        ):\n            self._set_global_sts_endpoint(\n                endpoint_config, resolve_endpoint_kwargs['is_secure']\n            )\n        return endpoint_config\n\n    def _should_set_global_sts_endpoint(\n        self, region_name, endpoint_url, endpoint_config\n    ):\n        has_variant_tags = endpoint_config and endpoint_config.get(\n            'metadata', {}\n        ).get('tags')\n        if endpoint_url or has_variant_tags:\n            return False\n        return (\n            self._get_sts_regional_endpoints_config() == 'legacy'\n            and region_name in LEGACY_GLOBAL_STS_REGIONS\n        )\n\n    def _get_sts_regional_endpoints_config(self):\n        sts_regional_endpoints_config = self._config_store.get_config_variable(\n            'sts_regional_endpoints'\n        )\n        if not sts_regional_endpoints_config:\n            sts_regional_endpoints_config = 'legacy'\n        if (\n            sts_regional_endpoints_config\n            not in VALID_REGIONAL_ENDPOINTS_CONFIG\n        ):\n            raise botocore.exceptions.InvalidSTSRegionalEndpointsConfigError(\n                sts_regional_endpoints_config=sts_regional_endpoints_config\n            )\n        return sts_regional_endpoints_config\n\n    def _set_global_sts_endpoint(self, endpoint_config, is_secure):\n        scheme = 'https' if is_secure else 'http'\n        endpoint_config['endpoint_url'] = '%s://sts.amazonaws.com' % scheme\n        endpoint_config['signing_region'] = 'us-east-1'\n\n    def _resolve_endpoint(\n        self,\n        service_name,\n        region_name,\n        endpoint_url,\n        is_secure,\n        endpoint_bridge,\n    ):\n        return endpoint_bridge.resolve(\n            service_name, region_name, endpoint_url, is_secure\n        )\n\n    def _compute_socket_options(self, scoped_config, client_config=None):\n        # This disables Nagle's algorithm and is the default socket options\n        # in urllib3.\n        socket_options = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]\n        client_keepalive = client_config and client_config.tcp_keepalive\n        scoped_keepalive = scoped_config and self._ensure_boolean(\n            scoped_config.get(\"tcp_keepalive\", False)\n        )\n        # Enables TCP Keepalive if specified in client config object or shared config file.\n        if client_keepalive or scoped_keepalive:\n            socket_options.append((socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1))\n        return socket_options\n\n    def _compute_retry_config(self, config_kwargs):\n        self._compute_retry_max_attempts(config_kwargs)\n        self._compute_retry_mode(config_kwargs)\n\n    def _compute_retry_max_attempts(self, config_kwargs):\n        # There's a pre-existing max_attempts client config value that actually\n        # means max *retry* attempts.  There's also a `max_attempts` we pull\n        # from the config store that means *total attempts*, which includes the\n        # intitial request.  We can't change what `max_attempts` means in\n        # client config so we try to normalize everything to a new\n        # \"total_max_attempts\" variable.  We ensure that after this, the only\n        # configuration for \"max attempts\" is the 'total_max_attempts' key.\n        # An explicitly provided max_attempts in the client config\n        # overrides everything.\n        retries = config_kwargs.get('retries')\n        if retries is not None:\n            if 'total_max_attempts' in retries:\n                retries.pop('max_attempts', None)\n                return\n            if 'max_attempts' in retries:\n                value = retries.pop('max_attempts')\n                # client config max_attempts means total retries so we\n                # have to add one for 'total_max_attempts' to account\n                # for the initial request.\n                retries['total_max_attempts'] = value + 1\n                return\n        # Otherwise we'll check the config store which checks env vars,\n        # config files, etc.  There is no default value for max_attempts\n        # so if this returns None and we don't set a default value here.\n        max_attempts = self._config_store.get_config_variable('max_attempts')\n        if max_attempts is not None:\n            if retries is None:\n                retries = {}\n                config_kwargs['retries'] = retries\n            retries['total_max_attempts'] = max_attempts\n\n    def _compute_retry_mode(self, config_kwargs):\n        retries = config_kwargs.get('retries')\n        if retries is None:\n            retries = {}\n            config_kwargs['retries'] = retries\n        elif 'mode' in retries:\n            # If there's a retry mode explicitly set in the client config\n            # that overrides everything.\n            return\n        retry_mode = self._config_store.get_config_variable('retry_mode')\n        if retry_mode is None:\n            retry_mode = 'legacy'\n        retries['mode'] = retry_mode\n\n    def _compute_connect_timeout(self, config_kwargs):\n        # Checking if connect_timeout is set on the client config.\n        # If it is not, we check the config_store in case a\n        # non legacy default mode has been configured.\n        connect_timeout = config_kwargs.get('connect_timeout')\n        if connect_timeout is not None:\n            return\n        connect_timeout = self._config_store.get_config_variable(\n            'connect_timeout'\n        )\n        if connect_timeout:\n            config_kwargs['connect_timeout'] = connect_timeout\n\n    def _compute_request_compression_config(self, config_kwargs):\n        min_size = config_kwargs.get('request_min_compression_size_bytes')\n        disabled = config_kwargs.get('disable_request_compression')\n        if min_size is None:\n            min_size = self._config_store.get_config_variable(\n                'request_min_compression_size_bytes'\n            )\n        # conversion func is skipped so input validation must be done here\n        # regardless if the value is coming from the config store or the\n        # config object\n        min_size = self._validate_min_compression_size(min_size)\n        config_kwargs['request_min_compression_size_bytes'] = min_size\n\n        if disabled is None:\n            disabled = self._config_store.get_config_variable(\n                'disable_request_compression'\n            )\n        else:\n            # if the user provided a value we must check if it's a boolean\n            disabled = ensure_boolean(disabled)\n        config_kwargs['disable_request_compression'] = disabled\n\n    def _validate_min_compression_size(self, min_size):\n        min_allowed_min_size = 1\n        max_allowed_min_size = 1048576\n        if min_size is not None:\n            error_msg_base = (\n                f'Invalid value \"{min_size}\" for '\n                'request_min_compression_size_bytes.'\n            )\n            try:\n                min_size = int(min_size)\n            except (ValueError, TypeError):\n                msg = (\n                    f'{error_msg_base} Value must be an integer. '\n                    f'Received {type(min_size)} instead.'\n                )\n                raise botocore.exceptions.InvalidConfigError(error_msg=msg)\n            if not min_allowed_min_size <= min_size <= max_allowed_min_size:\n                msg = (\n                    f'{error_msg_base} Value must be between '\n                    f'{min_allowed_min_size} and {max_allowed_min_size}.'\n                )\n                raise botocore.exceptions.InvalidConfigError(error_msg=msg)\n\n        return min_size\n\n    def _ensure_boolean(self, val):\n        if isinstance(val, bool):\n            return val\n        else:\n            return val.lower() == 'true'\n\n    def _build_endpoint_resolver(\n        self,\n        endpoints_ruleset_data,\n        partition_data,\n        client_config,\n        service_model,\n        endpoint_region_name,\n        region_name,\n        endpoint_url,\n        endpoint,\n        is_secure,\n        endpoint_bridge,\n        event_emitter,\n    ):\n        if endpoints_ruleset_data is None:\n            return None\n\n        # The legacy EndpointResolver is global to the session, but\n        # EndpointRulesetResolver is service-specific. Builtins for\n        # EndpointRulesetResolver must not be derived from the legacy\n        # endpoint resolver's output, including final_args, s3_config,\n        # etc.\n        s3_config_raw = self.compute_s3_config(client_config) or {}\n        service_name_raw = service_model.endpoint_prefix\n        # Maintain complex logic for s3 and sts endpoints for backwards\n        # compatibility.\n        if service_name_raw in ['s3', 'sts'] or region_name is None:\n            eprv2_region_name = endpoint_region_name\n        else:\n            eprv2_region_name = region_name\n        resolver_builtins = self.compute_endpoint_resolver_builtin_defaults(\n            region_name=eprv2_region_name,\n            service_name=service_name_raw,\n            s3_config=s3_config_raw,\n            endpoint_bridge=endpoint_bridge,\n            client_endpoint_url=endpoint_url,\n            legacy_endpoint_url=endpoint.host,\n        )\n        # Client context params for s3 conflict with the available settings\n        # in the `s3` parameter on the `Config` object. If the same parameter\n        # is set in both places, the value in the `s3` parameter takes priority.\n        if client_config is not None:\n            client_context = client_config.client_context_params or {}\n        else:\n            client_context = {}\n        if self._is_s3_service(service_name_raw):\n            client_context.update(s3_config_raw)\n\n        sig_version = (\n            client_config.signature_version\n            if client_config is not None\n            else None\n        )\n        return EndpointRulesetResolver(\n            endpoint_ruleset_data=endpoints_ruleset_data,\n            partition_data=partition_data,\n            service_model=service_model,\n            builtins=resolver_builtins,\n            client_context=client_context,\n            event_emitter=event_emitter,\n            use_ssl=is_secure,\n            requested_auth_scheme=sig_version,\n        )\n\n    def compute_endpoint_resolver_builtin_defaults(\n        self,\n        region_name,\n        service_name,\n        s3_config,\n        endpoint_bridge,\n        client_endpoint_url,\n        legacy_endpoint_url,\n    ):\n        # EndpointRulesetResolver rulesets may accept an \"SDK::Endpoint\" as\n        # input. If the endpoint_url argument of create_client() is set, it\n        # always takes priority.\n        if client_endpoint_url:\n            given_endpoint = client_endpoint_url\n        # If an endpoints.json data file other than the one bundled within\n        # the botocore/data directory is used, the output of legacy\n        # endpoint resolution is provided to EndpointRulesetResolver.\n        elif not endpoint_bridge.resolver_uses_builtin_data():\n            given_endpoint = legacy_endpoint_url\n        else:\n            given_endpoint = None\n\n        # The endpoint rulesets differ from legacy botocore behavior in whether\n        # forcing path style addressing in incompatible situations raises an\n        # exception or silently ignores the config setting. The\n        # AWS_S3_FORCE_PATH_STYLE parameter is adjusted both here and for each\n        # operation so that the ruleset behavior is backwards compatible.\n        if s3_config.get('use_accelerate_endpoint', False):\n            force_path_style = False\n        elif client_endpoint_url is not None and not is_s3_accelerate_url(\n            client_endpoint_url\n        ):\n            force_path_style = s3_config.get('addressing_style') != 'virtual'\n        else:\n            force_path_style = s3_config.get('addressing_style') == 'path'\n\n        return {\n            EPRBuiltins.AWS_REGION: region_name,\n            EPRBuiltins.AWS_USE_FIPS: (\n                # SDK_ENDPOINT cannot be combined with AWS_USE_FIPS\n                given_endpoint is None\n                # use legacy resolver's _resolve_endpoint_variant_config_var()\n                # or default to False if it returns None\n                and endpoint_bridge._resolve_endpoint_variant_config_var(\n                    'use_fips_endpoint'\n                )\n                or False\n            ),\n            EPRBuiltins.AWS_USE_DUALSTACK: (\n                # SDK_ENDPOINT cannot be combined with AWS_USE_DUALSTACK\n                given_endpoint is None\n                # use legacy resolver's _resolve_use_dualstack_endpoint() and\n                # or default to False if it returns None\n                and endpoint_bridge._resolve_use_dualstack_endpoint(\n                    service_name\n                )\n                or False\n            ),\n            EPRBuiltins.AWS_STS_USE_GLOBAL_ENDPOINT: (\n                self._should_set_global_sts_endpoint(\n                    region_name=region_name,\n                    endpoint_url=None,\n                    endpoint_config=None,\n                )\n            ),\n            EPRBuiltins.AWS_S3_USE_GLOBAL_ENDPOINT: (\n                self._should_force_s3_global(region_name, s3_config)\n            ),\n            EPRBuiltins.AWS_S3_ACCELERATE: s3_config.get(\n                'use_accelerate_endpoint', False\n            ),\n            EPRBuiltins.AWS_S3_FORCE_PATH_STYLE: force_path_style,\n            EPRBuiltins.AWS_S3_USE_ARN_REGION: s3_config.get(\n                'use_arn_region', True\n            ),\n            EPRBuiltins.AWS_S3CONTROL_USE_ARN_REGION: s3_config.get(\n                'use_arn_region', False\n            ),\n            EPRBuiltins.AWS_S3_DISABLE_MRAP: s3_config.get(\n                's3_disable_multiregion_access_points', False\n            ),\n            EPRBuiltins.SDK_ENDPOINT: given_endpoint,\n        }\n\n    def _compute_user_agent_appid_config(self, config_kwargs):\n        user_agent_appid = config_kwargs.get('user_agent_appid')\n        if user_agent_appid is None:\n            user_agent_appid = self._config_store.get_config_variable(\n                'user_agent_appid'\n            )\n        if (\n            user_agent_appid is not None\n            and len(user_agent_appid) > USERAGENT_APPID_MAXLEN\n        ):\n            logger.warning(\n                'The configured value for user_agent_appid exceeds the '\n                f'maximum length of {USERAGENT_APPID_MAXLEN} characters.'\n            )\n        config_kwargs['user_agent_appid'] = user_agent_appid\n", "botocore/__init__.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\nimport os\nimport re\n\n__version__ = '1.34.132'\n\n\nclass NullHandler(logging.Handler):\n    def emit(self, record):\n        pass\n\n\n# Configure default logger to do nothing\nlog = logging.getLogger('botocore')\nlog.addHandler(NullHandler())\n\n_INITIALIZERS = []\n\n_first_cap_regex = re.compile('(.)([A-Z][a-z]+)')\n_end_cap_regex = re.compile('([a-z0-9])([A-Z])')\n# The regex below handles the special case where some acronym\n# name is pluralized, e.g GatewayARNs, ListWebACLs, SomeCNAMEs.\n_special_case_transform = re.compile('[A-Z]{2,}s$')\n# Prepopulate the cache with special cases that don't match\n# our regular transformation.\n_xform_cache = {\n    ('CreateCachediSCSIVolume', '_'): 'create_cached_iscsi_volume',\n    ('CreateCachediSCSIVolume', '-'): 'create-cached-iscsi-volume',\n    ('DescribeCachediSCSIVolumes', '_'): 'describe_cached_iscsi_volumes',\n    ('DescribeCachediSCSIVolumes', '-'): 'describe-cached-iscsi-volumes',\n    ('DescribeStorediSCSIVolumes', '_'): 'describe_stored_iscsi_volumes',\n    ('DescribeStorediSCSIVolumes', '-'): 'describe-stored-iscsi-volumes',\n    ('CreateStorediSCSIVolume', '_'): 'create_stored_iscsi_volume',\n    ('CreateStorediSCSIVolume', '-'): 'create-stored-iscsi-volume',\n    ('ListHITsForQualificationType', '_'): 'list_hits_for_qualification_type',\n    ('ListHITsForQualificationType', '-'): 'list-hits-for-qualification-type',\n    ('ExecutePartiQLStatement', '_'): 'execute_partiql_statement',\n    ('ExecutePartiQLStatement', '-'): 'execute-partiql-statement',\n    ('ExecutePartiQLTransaction', '_'): 'execute_partiql_transaction',\n    ('ExecutePartiQLTransaction', '-'): 'execute-partiql-transaction',\n    ('ExecutePartiQLBatch', '_'): 'execute_partiql_batch',\n    ('ExecutePartiQLBatch', '-'): 'execute-partiql-batch',\n}\n# The items in this dict represent partial renames to apply globally to all\n# services which might have a matching argument or operation. This way a\n# common mis-translation can be fixed without having to call out each\n# individual case.\nScalarTypes = ('string', 'integer', 'boolean', 'timestamp', 'float', 'double')\n\nBOTOCORE_ROOT = os.path.dirname(os.path.abspath(__file__))\n\n\n# Used to specify anonymous (unsigned) request signature\nclass UNSIGNED:\n    def __copy__(self):\n        return self\n\n    def __deepcopy__(self, memodict):\n        return self\n\n\nUNSIGNED = UNSIGNED()\n\n\ndef xform_name(name, sep='_', _xform_cache=_xform_cache):\n    \"\"\"Convert camel case to a \"pythonic\" name.\n\n    If the name contains the ``sep`` character, then it is\n    returned unchanged.\n\n    \"\"\"\n    if sep in name:\n        # If the sep is in the name, assume that it's already\n        # transformed and return the string unchanged.\n        return name\n    key = (name, sep)\n    if key not in _xform_cache:\n        if _special_case_transform.search(name) is not None:\n            is_special = _special_case_transform.search(name)\n            matched = is_special.group()\n            # Replace something like ARNs, ACLs with _arns, _acls.\n            name = f\"{name[: -len(matched)]}{sep}{matched.lower()}\"\n        s1 = _first_cap_regex.sub(r'\\1' + sep + r'\\2', name)\n        transformed = _end_cap_regex.sub(r'\\1' + sep + r'\\2', s1).lower()\n        _xform_cache[key] = transformed\n    return _xform_cache[key]\n\n\ndef register_initializer(callback):\n    \"\"\"Register an initializer function for session creation.\n\n    This initializer function will be invoked whenever a new\n    `botocore.session.Session` is instantiated.\n\n    :type callback: callable\n    :param callback: A callable that accepts a single argument\n        of type `botocore.session.Session`.\n\n    \"\"\"\n    _INITIALIZERS.append(callback)\n\n\ndef unregister_initializer(callback):\n    \"\"\"Unregister an initializer function.\n\n    :type callback: callable\n    :param callback: A callable that was previously registered\n        with `botocore.register_initializer`.\n\n    :raises ValueError: If a callback is provided that is not currently\n        registered as an initializer.\n\n    \"\"\"\n    _INITIALIZERS.remove(callback)\n\n\ndef invoke_initializers(session):\n    \"\"\"Invoke all initializers for a session.\n\n    :type session: botocore.session.Session\n    :param session: The session to initialize.\n\n    \"\"\"\n    for initializer in _INITIALIZERS:\n        initializer(session)\n", "botocore/useragent.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nNOTE: All classes and functions in this module are considered private and are\nsubject to abrupt breaking changes. Please do not use them directly.\n\nTo modify the User-Agent header sent by botocore, use one of these\nconfiguration options:\n* The ``AWS_SDK_UA_APP_ID`` environment variable.\n* The ``sdk_ua_app_id`` setting in the shared AWS config file.\n* The ``user_agent_appid`` field in the :py:class:`botocore.config.Config`.\n* The ``user_agent_extra`` field in the :py:class:`botocore.config.Config`.\n\n\"\"\"\nimport os\nimport platform\nfrom copy import copy\nfrom string import ascii_letters, digits\nfrom typing import NamedTuple, Optional\n\nfrom botocore import __version__ as botocore_version\nfrom botocore.compat import HAS_CRT\n\n_USERAGENT_ALLOWED_CHARACTERS = ascii_letters + digits + \"!$%&'*+-.^_`|~\"\n_USERAGENT_ALLOWED_OS_NAMES = (\n    'windows',\n    'linux',\n    'macos',\n    'android',\n    'ios',\n    'watchos',\n    'tvos',\n    'other',\n)\n_USERAGENT_PLATFORM_NAME_MAPPINGS = {'darwin': 'macos'}\n# The name by which botocore is identified in the User-Agent header. While most\n# AWS SDKs follow a naming pattern of \"aws-sdk-*\", botocore and boto3 continue\n# using their existing values. Uses uppercase \"B\" with all other characters\n# lowercase.\n_USERAGENT_SDK_NAME = 'Botocore'\n\n\ndef sanitize_user_agent_string_component(raw_str, allow_hash):\n    \"\"\"Replaces all not allowed characters in the string with a dash (\"-\").\n\n    Allowed characters are ASCII alphanumerics and ``!$%&'*+-.^_`|~``. If\n    ``allow_hash`` is ``True``, \"#\"``\" is also allowed.\n\n    :type raw_str: str\n    :param raw_str: The input string to be sanitized.\n\n    :type allow_hash: bool\n    :param allow_hash: Whether \"#\" is considered an allowed character.\n    \"\"\"\n    return ''.join(\n        c\n        if c in _USERAGENT_ALLOWED_CHARACTERS or (allow_hash and c == '#')\n        else '-'\n        for c in raw_str\n    )\n\n\nclass UserAgentComponent(NamedTuple):\n    \"\"\"\n    Component of a Botocore User-Agent header string in the standard format.\n\n    Each component consists of a prefix, a name, and a value. In the string\n    representation these are combined in the format ``prefix/name#value``.\n\n    This class is considered private and is subject to abrupt breaking changes.\n    \"\"\"\n\n    prefix: str\n    name: str\n    value: Optional[str] = None\n\n    def to_string(self):\n        \"\"\"Create string like 'prefix/name#value' from a UserAgentComponent.\"\"\"\n        clean_prefix = sanitize_user_agent_string_component(\n            self.prefix, allow_hash=True\n        )\n        clean_name = sanitize_user_agent_string_component(\n            self.name, allow_hash=False\n        )\n        if self.value is None or self.value == '':\n            return f'{clean_prefix}/{clean_name}'\n        clean_value = sanitize_user_agent_string_component(\n            self.value, allow_hash=True\n        )\n        return f'{clean_prefix}/{clean_name}#{clean_value}'\n\n\nclass RawStringUserAgentComponent:\n    \"\"\"\n    UserAgentComponent interface wrapper around ``str``.\n\n    Use for User-Agent header components that are not constructed from\n    prefix+name+value but instead are provided as strings. No sanitization is\n    performed.\n    \"\"\"\n\n    def __init__(self, value):\n        self._value = value\n\n    def to_string(self):\n        return self._value\n\n\n# This is not a public interface and is subject to abrupt breaking changes.\n# Any usage is not advised or supported in external code bases.\ntry:\n    from botocore.customizations.useragent import modify_components\nexcept ImportError:\n    # Default implementation that returns unmodified User-Agent components.\n    def modify_components(components):\n        return components\n\n\nclass UserAgentString:\n    \"\"\"\n    Generator for AWS SDK User-Agent header strings.\n\n    The User-Agent header format contains information from session, client, and\n    request context. ``UserAgentString`` provides methods for collecting the\n    information and ``to_string`` for assembling it into the standardized\n    string format.\n\n    Example usage:\n\n        ua_session = UserAgentString.from_environment()\n        ua_session.set_session_config(...)\n        ua_client = ua_session.with_client_config(Config(...))\n        ua_string = ua_request.to_string()\n\n    For testing or when information from all sources is available at the same\n    time, the methods can be chained:\n\n        ua_string = (\n            UserAgentString\n            .from_environment()\n            .set_session_config(...)\n            .with_client_config(Config(...))\n            .to_string()\n        )\n\n    \"\"\"\n\n    def __init__(\n        self,\n        platform_name,\n        platform_version,\n        platform_machine,\n        python_version,\n        python_implementation,\n        execution_env,\n        crt_version=None,\n    ):\n        \"\"\"\n        :type platform_name: str\n        :param platform_name: Name of the operating system or equivalent\n            platform name. Should be sourced from :py:meth:`platform.system`.\n        :type platform_version: str\n        :param platform_version: Version of the operating system or equivalent\n            platform name. Should be sourced from :py:meth:`platform.version`.\n        :type platform_machine: str\n        :param platform_version: Processor architecture or machine type. For\n        example \"x86_64\". Should be sourced from :py:meth:`platform.machine`.\n        :type python_version: str\n        :param python_version: Version of the python implementation as str.\n            Should be sourced from :py:meth:`platform.python_version`.\n        :type python_implementation: str\n        :param python_implementation: Name of the python implementation.\n            Should be sourced from :py:meth:`platform.python_implementation`.\n        :type execution_env: str\n        :param execution_env: The value of the AWS execution environment.\n            Should be sourced from the ``AWS_EXECUTION_ENV` environment\n            variable.\n        :type crt_version: str\n        :param crt_version: Version string of awscrt package, if installed.\n        \"\"\"\n        self._platform_name = platform_name\n        self._platform_version = platform_version\n        self._platform_machine = platform_machine\n        self._python_version = python_version\n        self._python_implementation = python_implementation\n        self._execution_env = execution_env\n        self._crt_version = crt_version\n\n        # Components that can be added with ``set_session_config()``\n        self._session_user_agent_name = None\n        self._session_user_agent_version = None\n        self._session_user_agent_extra = None\n\n        self._client_config = None\n        self._uses_paginator = None\n        self._uses_waiter = None\n        self._uses_resource = None\n\n    @classmethod\n    def from_environment(cls):\n        crt_version = None\n        if HAS_CRT:\n            crt_version = _get_crt_version() or 'Unknown'\n        return cls(\n            platform_name=platform.system(),\n            platform_version=platform.release(),\n            platform_machine=platform.machine(),\n            python_version=platform.python_version(),\n            python_implementation=platform.python_implementation(),\n            execution_env=os.environ.get('AWS_EXECUTION_ENV'),\n            crt_version=crt_version,\n        )\n\n    def set_session_config(\n        self,\n        session_user_agent_name,\n        session_user_agent_version,\n        session_user_agent_extra,\n    ):\n        \"\"\"\n        Set the user agent configuration values that apply at session level.\n\n        :param user_agent_name: The user agent name configured in the\n            :py:class:`botocore.session.Session` object. For backwards\n            compatibility, this will always be at the beginning of the\n            User-Agent string, together with ``user_agent_version``.\n        :param user_agent_version: The user agent version configured in the\n            :py:class:`botocore.session.Session` object.\n        :param user_agent_extra: The user agent \"extra\" configured in the\n            :py:class:`botocore.session.Session` object.\n        \"\"\"\n        self._session_user_agent_name = session_user_agent_name\n        self._session_user_agent_version = session_user_agent_version\n        self._session_user_agent_extra = session_user_agent_extra\n        return self\n\n    def with_client_config(self, client_config):\n        \"\"\"\n        Create a copy with all original values and client-specific values.\n\n        :type client_config: botocore.config.Config\n        :param client_config: The client configuration object.\n        \"\"\"\n        cp = copy(self)\n        cp._client_config = client_config\n        return cp\n\n    def to_string(self):\n        \"\"\"\n        Build User-Agent header string from the object's properties.\n        \"\"\"\n        config_ua_override = None\n        if self._client_config:\n            if hasattr(self._client_config, '_supplied_user_agent'):\n                config_ua_override = self._client_config._supplied_user_agent\n            else:\n                config_ua_override = self._client_config.user_agent\n\n        if config_ua_override is not None:\n            return self._build_legacy_ua_string(config_ua_override)\n\n        components = [\n            *self._build_sdk_metadata(),\n            RawStringUserAgentComponent('ua/2.0'),\n            *self._build_os_metadata(),\n            *self._build_architecture_metadata(),\n            *self._build_language_metadata(),\n            *self._build_execution_env_metadata(),\n            *self._build_feature_metadata(),\n            *self._build_config_metadata(),\n            *self._build_app_id(),\n            *self._build_extra(),\n        ]\n\n        components = modify_components(components)\n\n        return ' '.join([comp.to_string() for comp in components])\n\n    def _build_sdk_metadata(self):\n        \"\"\"\n        Build the SDK name and version component of the User-Agent header.\n\n        For backwards-compatibility both session-level and client-level config\n        of custom tool names are honored. If this removes the Botocore\n        information from the start of the string, Botocore's name and version\n        are included as a separate field with \"md\" prefix.\n        \"\"\"\n        sdk_md = []\n        if (\n            self._session_user_agent_name\n            and self._session_user_agent_version\n            and (\n                self._session_user_agent_name != _USERAGENT_SDK_NAME\n                or self._session_user_agent_version != botocore_version\n            )\n        ):\n            sdk_md.extend(\n                [\n                    UserAgentComponent(\n                        self._session_user_agent_name,\n                        self._session_user_agent_version,\n                    ),\n                    UserAgentComponent(\n                        'md', _USERAGENT_SDK_NAME, botocore_version\n                    ),\n                ]\n            )\n        else:\n            sdk_md.append(\n                UserAgentComponent(_USERAGENT_SDK_NAME, botocore_version)\n            )\n\n        if self._crt_version is not None:\n            sdk_md.append(\n                UserAgentComponent('md', 'awscrt', self._crt_version)\n            )\n\n        return sdk_md\n\n    def _build_os_metadata(self):\n        \"\"\"\n        Build the OS/platform components of the User-Agent header string.\n\n        For recognized platform names that match or map to an entry in the list\n        of standardized OS names, a single component with prefix \"os\" is\n        returned. Otherwise, one component \"os/other\" is returned and a second\n        with prefix \"md\" and the raw platform name.\n\n        String representations of example return values:\n         * ``os/macos#10.13.6``\n         * ``os/linux``\n         * ``os/other``\n         * ``os/other md/foobar#1.2.3``\n        \"\"\"\n        if self._platform_name is None:\n            return [UserAgentComponent('os', 'other')]\n\n        plt_name_lower = self._platform_name.lower()\n        if plt_name_lower in _USERAGENT_ALLOWED_OS_NAMES:\n            os_family = plt_name_lower\n        elif plt_name_lower in _USERAGENT_PLATFORM_NAME_MAPPINGS:\n            os_family = _USERAGENT_PLATFORM_NAME_MAPPINGS[plt_name_lower]\n        else:\n            os_family = None\n\n        if os_family is not None:\n            return [\n                UserAgentComponent('os', os_family, self._platform_version)\n            ]\n        else:\n            return [\n                UserAgentComponent('os', 'other'),\n                UserAgentComponent(\n                    'md', self._platform_name, self._platform_version\n                ),\n            ]\n\n    def _build_architecture_metadata(self):\n        \"\"\"\n        Build architecture component of the User-Agent header string.\n\n        Returns the machine type with prefix \"md\" and name \"arch\", if one is\n        available. Common values include \"x86_64\", \"arm64\", \"i386\".\n        \"\"\"\n        if self._platform_machine:\n            return [\n                UserAgentComponent(\n                    'md', 'arch', self._platform_machine.lower()\n                )\n            ]\n        return []\n\n    def _build_language_metadata(self):\n        \"\"\"\n        Build the language components of the User-Agent header string.\n\n        Returns the Python version in a component with prefix \"lang\" and name\n        \"python\". The Python implementation (e.g. CPython, PyPy) is returned as\n        separate metadata component with prefix \"md\" and name \"pyimpl\".\n\n        String representation of an example return value:\n        ``lang/python#3.10.4 md/pyimpl#CPython``\n        \"\"\"\n        lang_md = [\n            UserAgentComponent('lang', 'python', self._python_version),\n        ]\n        if self._python_implementation:\n            lang_md.append(\n                UserAgentComponent('md', 'pyimpl', self._python_implementation)\n            )\n        return lang_md\n\n    def _build_execution_env_metadata(self):\n        \"\"\"\n        Build the execution environment component of the User-Agent header.\n\n        Returns a single component prefixed with \"exec-env\", usually sourced\n        from the environment variable AWS_EXECUTION_ENV.\n        \"\"\"\n        if self._execution_env:\n            return [UserAgentComponent('exec-env', self._execution_env)]\n        else:\n            return []\n\n    def _build_feature_metadata(self):\n        \"\"\"\n        Build the features components of the User-Agent header string.\n\n        Botocore currently does not report any features. This may change in a\n        future version.\n        \"\"\"\n        return []\n\n    def _build_config_metadata(self):\n        \"\"\"\n        Build the configuration components of the User-Agent header string.\n\n        Returns a list of components with prefix \"cfg\" followed by the config\n        setting name and its value. Tracked configuration settings may be\n        added or removed in future versions.\n        \"\"\"\n        if not self._client_config or not self._client_config.retries:\n            return []\n        retry_mode = self._client_config.retries.get('mode')\n        cfg_md = [UserAgentComponent('cfg', 'retry-mode', retry_mode)]\n        if self._client_config.endpoint_discovery_enabled:\n            cfg_md.append(UserAgentComponent('cfg', 'endpoint-discovery'))\n        return cfg_md\n\n    def _build_app_id(self):\n        \"\"\"\n        Build app component of the User-Agent header string.\n\n        Returns a single component with prefix \"app\" and value sourced from the\n        ``user_agent_appid`` field in :py:class:`botocore.config.Config` or\n        the ``sdk_ua_app_id`` setting in the shared configuration file, or the\n        ``AWS_SDK_UA_APP_ID`` environment variable. These are the recommended\n        ways for apps built with Botocore to insert their identifer into the\n        User-Agent header.\n        \"\"\"\n        if self._client_config and self._client_config.user_agent_appid:\n            return [\n                UserAgentComponent('app', self._client_config.user_agent_appid)\n            ]\n        else:\n            return []\n\n    def _build_extra(self):\n        \"\"\"User agent string components based on legacy \"extra\" settings.\n\n        Creates components from the session-level and client-level\n        ``user_agent_extra`` setting, if present. Both are passed through\n        verbatim and should be appended at the end of the string.\n\n        Preferred ways to inject application-specific information into\n        botocore's User-Agent header string are the ``user_agent_appid` field\n        in :py:class:`botocore.config.Config`. The ``AWS_SDK_UA_APP_ID``\n        environment variable and the ``sdk_ua_app_id`` configuration file\n        setting are alternative ways to set the ``user_agent_appid`` config.\n        \"\"\"\n        extra = []\n        if self._session_user_agent_extra:\n            extra.append(\n                RawStringUserAgentComponent(self._session_user_agent_extra)\n            )\n        if self._client_config and self._client_config.user_agent_extra:\n            extra.append(\n                RawStringUserAgentComponent(\n                    self._client_config.user_agent_extra\n                )\n            )\n        return extra\n\n    def _build_legacy_ua_string(self, config_ua_override):\n        components = [config_ua_override]\n        if self._session_user_agent_extra:\n            components.append(self._session_user_agent_extra)\n        if self._client_config.user_agent_extra:\n            components.append(self._client_config.user_agent_extra)\n        return ' '.join(components)\n\n\ndef _get_crt_version():\n    \"\"\"\n    This function is considered private and is subject to abrupt breaking\n    changes.\n    \"\"\"\n    try:\n        import awscrt\n\n        return awscrt.__version__\n    except AttributeError:\n        return None\n", "botocore/compat.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport copy\nimport datetime\nimport sys\nimport inspect\nimport warnings\nimport hashlib\nfrom http.client import HTTPMessage\nimport logging\nimport shlex\nimport re\nimport os\nfrom collections import OrderedDict\nfrom collections.abc import MutableMapping\nfrom math import floor\n\nfrom botocore.vendored import six\nfrom botocore.exceptions import MD5UnavailableError\nfrom dateutil.tz import tzlocal\nfrom urllib3 import exceptions\n\nlogger = logging.getLogger(__name__)\n\n\nclass HTTPHeaders(HTTPMessage):\n    pass\n\nfrom urllib.parse import (\n    quote,\n    urlencode,\n    unquote,\n    unquote_plus,\n    urlparse,\n    urlsplit,\n    urlunsplit,\n    urljoin,\n    parse_qsl,\n    parse_qs,\n)\nfrom http.client import HTTPResponse\nfrom io import IOBase as _IOBase\nfrom base64 import encodebytes\nfrom email.utils import formatdate\nfrom itertools import zip_longest\nfile_type = _IOBase\nzip = zip\n\n# In python3, unquote takes a str() object, url decodes it,\n# then takes the bytestring and decodes it to utf-8.\nunquote_str = unquote_plus\n\ndef set_socket_timeout(http_response, timeout):\n    \"\"\"Set the timeout of the socket from an HTTPResponse.\n\n    :param http_response: An instance of ``httplib.HTTPResponse``\n\n    \"\"\"\n    http_response._fp.fp.raw._sock.settimeout(timeout)\n\ndef accepts_kwargs(func):\n    # In python3.4.1, there's backwards incompatible\n    # changes when using getargspec with functools.partials.\n    return inspect.getfullargspec(func)[2]\n\ndef ensure_unicode(s, encoding=None, errors=None):\n    # NOOP in Python 3, because every string is already unicode\n    return s\n\ndef ensure_bytes(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    if isinstance(s, bytes):\n        return s\n    raise ValueError(f\"Expected str or bytes, received {type(s)}.\")\n\n\ntry:\n    import xml.etree.cElementTree as ETree\nexcept ImportError:\n    # cElementTree does not exist from Python3.9+\n    import xml.etree.ElementTree as ETree\nXMLParseError = ETree.ParseError\nimport json\n\n\ndef filter_ssl_warnings():\n    # Ignore warnings related to SNI as it is not being used in validations.\n    warnings.filterwarnings(\n        'ignore',\n        message=\"A true SSLContext object is not available.*\",\n        category=exceptions.InsecurePlatformWarning,\n        module=r\".*urllib3\\.util\\.ssl_\",\n    )\n\n\n@classmethod\ndef from_dict(cls, d):\n    new_instance = cls()\n    for key, value in d.items():\n        new_instance[key] = value\n    return new_instance\n\n\n@classmethod\ndef from_pairs(cls, pairs):\n    new_instance = cls()\n    for key, value in pairs:\n        new_instance[key] = value\n    return new_instance\n\n\nHTTPHeaders.from_dict = from_dict\nHTTPHeaders.from_pairs = from_pairs\n\n\ndef copy_kwargs(kwargs):\n    \"\"\"\n    This used to be a compat shim for 2.6 but is now just an alias.\n    \"\"\"\n    copy_kwargs = copy.copy(kwargs)\n    return copy_kwargs\n\n\ndef total_seconds(delta):\n    \"\"\"\n    Returns the total seconds in a ``datetime.timedelta``.\n\n    This used to be a compat shim for 2.6 but is now just an alias.\n\n    :param delta: The timedelta object\n    :type delta: ``datetime.timedelta``\n    \"\"\"\n    return delta.total_seconds()\n\n\n# Checks to see if md5 is available on this system. A given system might not\n# have access to it for various reasons, such as FIPS mode being enabled.\ntry:\n    hashlib.md5()\n    MD5_AVAILABLE = True\nexcept ValueError:\n    MD5_AVAILABLE = False\n\n\ndef get_md5(*args, **kwargs):\n    \"\"\"\n    Attempts to get an md5 hashing object.\n\n    :param args: Args to pass to the MD5 constructor\n    :param kwargs: Key word arguments to pass to the MD5 constructor\n    :return: An MD5 hashing object if available. If it is unavailable, None\n        is returned if raise_error_if_unavailable is set to False.\n    \"\"\"\n    if MD5_AVAILABLE:\n        return hashlib.md5(*args, **kwargs)\n    else:\n        raise MD5UnavailableError()\n\n\ndef compat_shell_split(s, platform=None):\n    if platform is None:\n        platform = sys.platform\n\n    if platform == \"win32\":\n        return _windows_shell_split(s)\n    else:\n        return shlex.split(s)\n\n\ndef _windows_shell_split(s):\n    \"\"\"Splits up a windows command as the built-in command parser would.\n\n    Windows has potentially bizarre rules depending on where you look. When\n    spawning a process via the Windows C runtime (which is what python does\n    when you call popen) the rules are as follows:\n\n    https://docs.microsoft.com/en-us/cpp/cpp/parsing-cpp-command-line-arguments\n\n    To summarize:\n\n    * Only space and tab are valid delimiters\n    * Double quotes are the only valid quotes\n    * Backslash is interpreted literally unless it is part of a chain that\n      leads up to a double quote. Then the backslashes escape the backslashes,\n      and if there is an odd number the final backslash escapes the quote.\n\n    :param s: The command string to split up into parts.\n    :return: A list of command components.\n    \"\"\"\n    if not s:\n        return []\n\n    components = []\n    buff = []\n    is_quoted = False\n    num_backslashes = 0\n    for character in s:\n        if character == '\\\\':\n            # We can't simply append backslashes because we don't know if\n            # they are being used as escape characters or not. Instead we\n            # keep track of how many we've encountered and handle them when\n            # we encounter a different character.\n            num_backslashes += 1\n        elif character == '\"':\n            if num_backslashes > 0:\n                # The backslashes are in a chain leading up to a double\n                # quote, so they are escaping each other.\n                buff.append('\\\\' * int(floor(num_backslashes / 2)))\n                remainder = num_backslashes % 2\n                num_backslashes = 0\n                if remainder == 1:\n                    # The number of backslashes is uneven, so they are also\n                    # escaping the double quote, so it needs to be added to\n                    # the current component buffer.\n                    buff.append('\"')\n                    continue\n\n            # We've encountered a double quote that is not escaped,\n            # so we toggle is_quoted.\n            is_quoted = not is_quoted\n\n            # If there are quotes, then we may want an empty string. To be\n            # safe, we add an empty string to the buffer so that we make\n            # sure it sticks around if there's nothing else between quotes.\n            # If there is other stuff between quotes, the empty string will\n            # disappear during the joining process.\n            buff.append('')\n        elif character in [' ', '\\t'] and not is_quoted:\n            # Since the backslashes aren't leading up to a quote, we put in\n            # the exact number of backslashes.\n            if num_backslashes > 0:\n                buff.append('\\\\' * num_backslashes)\n                num_backslashes = 0\n\n            # Excess whitespace is ignored, so only add the components list\n            # if there is anything in the buffer.\n            if buff:\n                components.append(''.join(buff))\n                buff = []\n        else:\n            # Since the backslashes aren't leading up to a quote, we put in\n            # the exact number of backslashes.\n            if num_backslashes > 0:\n                buff.append('\\\\' * num_backslashes)\n                num_backslashes = 0\n            buff.append(character)\n\n    # Quotes must be terminated.\n    if is_quoted:\n        raise ValueError(f\"No closing quotation in string: {s}\")\n\n    # There may be some leftover backslashes, so we need to add them in.\n    # There's no quote so we add the exact number.\n    if num_backslashes > 0:\n        buff.append('\\\\' * num_backslashes)\n\n    # Add the final component in if there is anything in the buffer.\n    if buff:\n        components.append(''.join(buff))\n\n    return components\n\n\ndef get_tzinfo_options():\n    # Due to dateutil/dateutil#197, Windows may fail to parse times in the past\n    # with the system clock. We can alternatively fallback to tzwininfo when\n    # this happens, which will get time info from the Windows registry.\n    if sys.platform == 'win32':\n        from dateutil.tz import tzwinlocal\n\n        return (tzlocal, tzwinlocal)\n    else:\n        return (tzlocal,)\n\n\n# Detect if CRT is available for use\ntry:\n    import awscrt.auth\n\n    # Allow user opt-out if needed\n    disabled = os.environ.get('BOTO_DISABLE_CRT', \"false\")\n    HAS_CRT = not disabled.lower() == 'true'\nexcept ImportError:\n    HAS_CRT = False\n\n\n########################################################\n#              urllib3 compat backports                #\n########################################################\n\n# Vendoring IPv6 validation regex patterns from urllib3\n# https://github.com/urllib3/urllib3/blob/7e856c0/src/urllib3/util/url.py\nIPV4_PAT = r\"(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\"\nIPV4_RE = re.compile(\"^\" + IPV4_PAT + \"$\")\nHEX_PAT = \"[0-9A-Fa-f]{1,4}\"\nLS32_PAT = \"(?:{hex}:{hex}|{ipv4})\".format(hex=HEX_PAT, ipv4=IPV4_PAT)\n_subs = {\"hex\": HEX_PAT, \"ls32\": LS32_PAT}\n_variations = [\n    #                            6( h16 \":\" ) ls32\n    \"(?:%(hex)s:){6}%(ls32)s\",\n    #                       \"::\" 5( h16 \":\" ) ls32\n    \"::(?:%(hex)s:){5}%(ls32)s\",\n    # [               h16 ] \"::\" 4( h16 \":\" ) ls32\n    \"(?:%(hex)s)?::(?:%(hex)s:){4}%(ls32)s\",\n    # [ *1( h16 \":\" ) h16 ] \"::\" 3( h16 \":\" ) ls32\n    \"(?:(?:%(hex)s:)?%(hex)s)?::(?:%(hex)s:){3}%(ls32)s\",\n    # [ *2( h16 \":\" ) h16 ] \"::\" 2( h16 \":\" ) ls32\n    \"(?:(?:%(hex)s:){0,2}%(hex)s)?::(?:%(hex)s:){2}%(ls32)s\",\n    # [ *3( h16 \":\" ) h16 ] \"::\"    h16 \":\"   ls32\n    \"(?:(?:%(hex)s:){0,3}%(hex)s)?::%(hex)s:%(ls32)s\",\n    # [ *4( h16 \":\" ) h16 ] \"::\"              ls32\n    \"(?:(?:%(hex)s:){0,4}%(hex)s)?::%(ls32)s\",\n    # [ *5( h16 \":\" ) h16 ] \"::\"              h16\n    \"(?:(?:%(hex)s:){0,5}%(hex)s)?::%(hex)s\",\n    # [ *6( h16 \":\" ) h16 ] \"::\"\n    \"(?:(?:%(hex)s:){0,6}%(hex)s)?::\",\n]\n\nUNRESERVED_PAT = (\n    r\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._!\\-~\"\n)\nIPV6_PAT = \"(?:\" + \"|\".join([x % _subs for x in _variations]) + \")\"\nZONE_ID_PAT = \"(?:%25|%)(?:[\" + UNRESERVED_PAT + \"]|%[a-fA-F0-9]{2})+\"\nIPV6_ADDRZ_PAT = r\"\\[\" + IPV6_PAT + r\"(?:\" + ZONE_ID_PAT + r\")?\\]\"\nIPV6_ADDRZ_RE = re.compile(\"^\" + IPV6_ADDRZ_PAT + \"$\")\n\n# These are the characters that are stripped by post-bpo-43882 urlparse().\nUNSAFE_URL_CHARS = frozenset('\\t\\r\\n')\n\n# Detect if gzip is available for use\ntry:\n    import gzip\n    HAS_GZIP = True\nexcept ImportError:\n    HAS_GZIP = False\n", "botocore/stub.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nfrom collections import deque\nfrom pprint import pformat\n\nfrom botocore.awsrequest import AWSResponse\nfrom botocore.exceptions import (\n    ParamValidationError,\n    StubAssertionError,\n    StubResponseError,\n    UnStubbedResponseError,\n)\nfrom botocore.validate import validate_parameters\n\n\nclass _ANY:\n    \"\"\"\n    A helper object that compares equal to everything. Copied from\n    unittest.mock\n    \"\"\"\n\n    def __eq__(self, other):\n        return True\n\n    def __ne__(self, other):\n        return False\n\n    def __repr__(self):\n        return '<ANY>'\n\n\nANY = _ANY()\n\n\nclass Stubber:\n    \"\"\"\n    This class will allow you to stub out requests so you don't have to hit\n    an endpoint to write tests. Responses are returned first in, first out.\n    If operations are called out of order, or are called with no remaining\n    queued responses, an error will be raised.\n\n    **Example:**\n    ::\n        import datetime\n        import botocore.session\n        from botocore.stub import Stubber\n\n\n        s3 = botocore.session.get_session().create_client('s3')\n        stubber = Stubber(s3)\n\n        response = {\n            'IsTruncated': False,\n            'Name': 'test-bucket',\n            'MaxKeys': 1000, 'Prefix': '',\n            'Contents': [{\n                'Key': 'test.txt',\n                'ETag': '\"abc123\"',\n                'StorageClass': 'STANDARD',\n                'LastModified': datetime.datetime(2016, 1, 20, 22, 9),\n                'Owner': {'ID': 'abc123', 'DisplayName': 'myname'},\n                'Size': 14814\n            }],\n            'EncodingType': 'url',\n            'ResponseMetadata': {\n                'RequestId': 'abc123',\n                'HTTPStatusCode': 200,\n                'HostId': 'abc123'\n            },\n            'Marker': ''\n        }\n\n        expected_params = {'Bucket': 'test-bucket'}\n\n        stubber.add_response('list_objects', response, expected_params)\n        stubber.activate()\n\n        service_response = s3.list_objects(Bucket='test-bucket')\n        assert service_response == response\n\n\n    This class can also be called as a context manager, which will handle\n    activation / deactivation for you.\n\n    **Example:**\n    ::\n        import datetime\n        import botocore.session\n        from botocore.stub import Stubber\n\n\n        s3 = botocore.session.get_session().create_client('s3')\n\n        response = {\n            \"Owner\": {\n                \"ID\": \"foo\",\n                \"DisplayName\": \"bar\"\n            },\n            \"Buckets\": [{\n                \"CreationDate\": datetime.datetime(2016, 1, 20, 22, 9),\n                \"Name\": \"baz\"\n            }]\n        }\n\n\n        with Stubber(s3) as stubber:\n            stubber.add_response('list_buckets', response, {})\n            service_response = s3.list_buckets()\n\n        assert service_response == response\n\n\n    If you have an input parameter that is a randomly generated value, or you\n    otherwise don't care about its value, you can use ``stub.ANY`` to ignore\n    it in validation.\n\n    **Example:**\n    ::\n        import datetime\n        import botocore.session\n        from botocore.stub import Stubber, ANY\n\n\n        s3 = botocore.session.get_session().create_client('s3')\n        stubber = Stubber(s3)\n\n        response = {\n            'IsTruncated': False,\n            'Name': 'test-bucket',\n            'MaxKeys': 1000, 'Prefix': '',\n            'Contents': [{\n                'Key': 'test.txt',\n                'ETag': '\"abc123\"',\n                'StorageClass': 'STANDARD',\n                'LastModified': datetime.datetime(2016, 1, 20, 22, 9),\n                'Owner': {'ID': 'abc123', 'DisplayName': 'myname'},\n                'Size': 14814\n            }],\n            'EncodingType': 'url',\n            'ResponseMetadata': {\n                'RequestId': 'abc123',\n                'HTTPStatusCode': 200,\n                'HostId': 'abc123'\n            },\n            'Marker': ''\n        }\n\n        expected_params = {'Bucket': ANY}\n        stubber.add_response('list_objects', response, expected_params)\n\n        with stubber:\n            service_response = s3.list_objects(Bucket='test-bucket')\n\n        assert service_response == response\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        :param client: The client to add your stubs to.\n        \"\"\"\n        self.client = client\n        self._event_id = 'boto_stubber'\n        self._expected_params_event_id = 'boto_stubber_expected_params'\n        self._queue = deque()\n\n    def __enter__(self):\n        self.activate()\n        return self\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        self.deactivate()\n\n    def activate(self):\n        \"\"\"\n        Activates the stubber on the client\n        \"\"\"\n        self.client.meta.events.register_first(\n            'before-parameter-build.*.*',\n            self._assert_expected_params,\n            unique_id=self._expected_params_event_id,\n        )\n        self.client.meta.events.register(\n            'before-call.*.*',\n            self._get_response_handler,\n            unique_id=self._event_id,\n        )\n\n    def deactivate(self):\n        \"\"\"\n        Deactivates the stubber on the client\n        \"\"\"\n        self.client.meta.events.unregister(\n            'before-parameter-build.*.*',\n            self._assert_expected_params,\n            unique_id=self._expected_params_event_id,\n        )\n        self.client.meta.events.unregister(\n            'before-call.*.*',\n            self._get_response_handler,\n            unique_id=self._event_id,\n        )\n\n    def add_response(self, method, service_response, expected_params=None):\n        \"\"\"\n        Adds a service response to the response queue. This will be validated\n        against the service model to ensure correctness. It should be noted,\n        however, that while missing attributes are often considered correct,\n        your code may not function properly if you leave them out. Therefore\n        you should always fill in every value you see in a typical response for\n        your particular request.\n\n        :param method: The name of the client method to stub.\n        :type method: str\n\n        :param service_response: A dict response stub. Provided parameters will\n            be validated against the service model.\n        :type service_response: dict\n\n        :param expected_params: A dictionary of the expected parameters to\n            be called for the provided service response. The parameters match\n            the names of keyword arguments passed to that client call. If\n            any of the parameters differ a ``StubResponseError`` is thrown.\n            You can use stub.ANY to indicate a particular parameter to ignore\n            in validation. stub.ANY is only valid for top level params.\n        \"\"\"\n        self._add_response(method, service_response, expected_params)\n\n    def _add_response(self, method, service_response, expected_params):\n        if not hasattr(self.client, method):\n            raise ValueError(\n                \"Client %s does not have method: %s\"\n                % (self.client.meta.service_model.service_name, method)\n            )\n\n        # Create a successful http response\n        http_response = AWSResponse(None, 200, {}, None)\n\n        operation_name = self.client.meta.method_to_api_mapping.get(method)\n        self._validate_operation_response(operation_name, service_response)\n\n        # Add the service_response to the queue for returning responses\n        response = {\n            'operation_name': operation_name,\n            'response': (http_response, service_response),\n            'expected_params': expected_params,\n        }\n        self._queue.append(response)\n\n    def add_client_error(\n        self,\n        method,\n        service_error_code='',\n        service_message='',\n        http_status_code=400,\n        service_error_meta=None,\n        expected_params=None,\n        response_meta=None,\n        modeled_fields=None,\n    ):\n        \"\"\"\n        Adds a ``ClientError`` to the response queue.\n\n        :param method: The name of the service method to return the error on.\n        :type method: str\n\n        :param service_error_code: The service error code to return,\n                                   e.g. ``NoSuchBucket``\n        :type service_error_code: str\n\n        :param service_message: The service message to return, e.g.\n                        'The specified bucket does not exist.'\n        :type service_message: str\n\n        :param http_status_code: The HTTP status code to return, e.g. 404, etc\n        :type http_status_code: int\n\n        :param service_error_meta: Additional keys to be added to the\n            service Error\n        :type service_error_meta: dict\n\n        :param expected_params: A dictionary of the expected parameters to\n            be called for the provided service response. The parameters match\n            the names of keyword arguments passed to that client call. If\n            any of the parameters differ a ``StubResponseError`` is thrown.\n            You can use stub.ANY to indicate a particular parameter to ignore\n            in validation.\n\n        :param response_meta: Additional keys to be added to the\n            response's ResponseMetadata\n        :type response_meta: dict\n\n        :param modeled_fields: Additional keys to be added to the response\n            based on fields that are modeled for the particular error code.\n            These keys will be validated against the particular error shape\n            designated by the error code.\n        :type modeled_fields: dict\n\n        \"\"\"\n        http_response = AWSResponse(None, http_status_code, {}, None)\n\n        # We don't look to the model to build this because the caller would\n        # need to know the details of what the HTTP body would need to\n        # look like.\n        parsed_response = {\n            'ResponseMetadata': {'HTTPStatusCode': http_status_code},\n            'Error': {'Message': service_message, 'Code': service_error_code},\n        }\n\n        if service_error_meta is not None:\n            parsed_response['Error'].update(service_error_meta)\n\n        if response_meta is not None:\n            parsed_response['ResponseMetadata'].update(response_meta)\n\n        if modeled_fields is not None:\n            service_model = self.client.meta.service_model\n            shape = service_model.shape_for_error_code(service_error_code)\n            self._validate_response(shape, modeled_fields)\n            parsed_response.update(modeled_fields)\n\n        operation_name = self.client.meta.method_to_api_mapping.get(method)\n        # Note that we do not allow for expected_params while\n        # adding errors into the queue yet.\n        response = {\n            'operation_name': operation_name,\n            'response': (http_response, parsed_response),\n            'expected_params': expected_params,\n        }\n        self._queue.append(response)\n\n    def assert_no_pending_responses(self):\n        \"\"\"\n        Asserts that all expected calls were made.\n        \"\"\"\n        remaining = len(self._queue)\n        if remaining != 0:\n            raise AssertionError(f\"{remaining} responses remaining in queue.\")\n\n    def _assert_expected_call_order(self, model, params):\n        if not self._queue:\n            raise UnStubbedResponseError(\n                operation_name=model.name,\n                reason=(\n                    'Unexpected API Call: A call was made but no additional '\n                    'calls expected. Either the API Call was not stubbed or '\n                    'it was called multiple times.'\n                ),\n            )\n\n        name = self._queue[0]['operation_name']\n        if name != model.name:\n            raise StubResponseError(\n                operation_name=model.name,\n                reason=f'Operation mismatch: found response for {name}.',\n            )\n\n    def _get_response_handler(self, model, params, context, **kwargs):\n        self._assert_expected_call_order(model, params)\n        # Pop off the entire response once everything has been validated\n        return self._queue.popleft()['response']\n\n    def _assert_expected_params(self, model, params, context, **kwargs):\n        if self._should_not_stub(context):\n            return\n        self._assert_expected_call_order(model, params)\n        expected_params = self._queue[0]['expected_params']\n        if expected_params is None:\n            return\n\n        # Validate the parameters are equal\n        for param, value in expected_params.items():\n            if param not in params or expected_params[param] != params[param]:\n                raise StubAssertionError(\n                    operation_name=model.name,\n                    reason='Expected parameters:\\n%s,\\nbut received:\\n%s'\n                    % (pformat(expected_params), pformat(params)),\n                )\n\n        # Ensure there are no extra params hanging around\n        if sorted(expected_params.keys()) != sorted(params.keys()):\n            raise StubAssertionError(\n                operation_name=model.name,\n                reason='Expected parameters:\\n%s,\\nbut received:\\n%s'\n                % (pformat(expected_params), pformat(params)),\n            )\n\n    def _should_not_stub(self, context):\n        # Do not include presign requests when processing stubbed client calls\n        # as a presign request will never have an HTTP request sent over the\n        # wire for it and therefore not receive a response back.\n        if context and context.get('is_presign_request'):\n            return True\n\n    def _validate_operation_response(self, operation_name, service_response):\n        service_model = self.client.meta.service_model\n        operation_model = service_model.operation_model(operation_name)\n        output_shape = operation_model.output_shape\n\n        # Remove ResponseMetadata so that the validator doesn't attempt to\n        # perform validation on it.\n        response = service_response\n        if 'ResponseMetadata' in response:\n            response = copy.copy(service_response)\n            del response['ResponseMetadata']\n\n        self._validate_response(output_shape, response)\n\n    def _validate_response(self, shape, response):\n        if shape is not None:\n            validate_parameters(response, shape)\n        elif response:\n            # If the output shape is None, that means the response should be\n            # empty apart from ResponseMetadata\n            raise ParamValidationError(\n                report=(\n                    \"Service response should only contain ResponseMetadata.\"\n                )\n            )\n", "botocore/session.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nThis module contains the main interface to the botocore package, the\nSession object.\n\"\"\"\n\nimport copy\nimport logging\nimport os\nimport platform\nimport socket\nimport warnings\n\nimport botocore.client\nimport botocore.configloader\nimport botocore.credentials\nimport botocore.tokens\nfrom botocore import (\n    UNSIGNED,\n    __version__,\n    handlers,\n    invoke_initializers,\n    monitoring,\n    paginate,\n    retryhandler,\n    translate,\n    waiter,\n)\nfrom botocore.compat import HAS_CRT, MutableMapping\nfrom botocore.configprovider import (\n    BOTOCORE_DEFAUT_SESSION_VARIABLES,\n    ConfigChainFactory,\n    ConfiguredEndpointProvider,\n    ConfigValueStore,\n    DefaultConfigResolver,\n    SmartDefaultsConfigStoreFactory,\n    create_botocore_default_config_mapping,\n)\nfrom botocore.errorfactory import ClientExceptionsFactory\nfrom botocore.exceptions import (\n    ConfigNotFound,\n    InvalidDefaultsMode,\n    PartialCredentialsError,\n    ProfileNotFound,\n    UnknownServiceError,\n)\nfrom botocore.hooks import (\n    EventAliaser,\n    HierarchicalEmitter,\n    first_non_none_response,\n)\nfrom botocore.loaders import create_loader\nfrom botocore.model import ServiceModel\nfrom botocore.parsers import ResponseParserFactory\nfrom botocore.regions import EndpointResolver\nfrom botocore.useragent import UserAgentString\nfrom botocore.utils import (\n    EVENT_ALIASES,\n    IMDSRegionProvider,\n    validate_region_name,\n)\n\nfrom botocore.compat import HAS_CRT  # noqa\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Session:\n    \"\"\"\n    The Session object collects together useful functionality\n    from `botocore` as well as important data such as configuration\n    information and credentials into a single, easy-to-use object.\n\n    :ivar available_profiles: A list of profiles defined in the config\n        file associated with this session.\n    :ivar profile: The current profile.\n    \"\"\"\n\n    SESSION_VARIABLES = copy.copy(BOTOCORE_DEFAUT_SESSION_VARIABLES)\n\n    #: The default format string to use when configuring the botocore logger.\n    LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n\n    def __init__(\n        self,\n        session_vars=None,\n        event_hooks=None,\n        include_builtin_handlers=True,\n        profile=None,\n    ):\n        \"\"\"\n        Create a new Session object.\n\n        :type session_vars: dict\n        :param session_vars: A dictionary that is used to override some or all\n            of the environment variables associated with this session.  The\n            key/value pairs defined in this dictionary will override the\n            corresponding variables defined in ``SESSION_VARIABLES``.\n\n        :type event_hooks: BaseEventHooks\n        :param event_hooks: The event hooks object to use. If one is not\n            provided, an event hooks object will be automatically created\n            for you.\n\n        :type include_builtin_handlers: bool\n        :param include_builtin_handlers: Indicates whether or not to\n            automatically register builtin handlers.\n\n        :type profile: str\n        :param profile: The name of the profile to use for this\n            session.  Note that the profile can only be set when\n            the session is created.\n\n        \"\"\"\n        if event_hooks is None:\n            self._original_handler = HierarchicalEmitter()\n        else:\n            self._original_handler = event_hooks\n        self._events = EventAliaser(self._original_handler)\n        if include_builtin_handlers:\n            self._register_builtin_handlers(self._events)\n        self.user_agent_name = 'Botocore'\n        self.user_agent_version = __version__\n        self.user_agent_extra = ''\n        # The _profile attribute is just used to cache the value\n        # of the current profile to avoid going through the normal\n        # config lookup process each access time.\n        self._profile = None\n        self._config = None\n        self._credentials = None\n        self._auth_token = None\n        self._profile_map = None\n        # This is a dict that stores per session specific config variable\n        # overrides via set_config_variable().\n        self._session_instance_vars = {}\n        if profile is not None:\n            self._session_instance_vars['profile'] = profile\n        self._client_config = None\n        self._last_client_region_used = None\n        self._components = ComponentLocator()\n        self._internal_components = ComponentLocator()\n        self._register_components()\n        self.session_var_map = SessionVarDict(self, self.SESSION_VARIABLES)\n        if session_vars is not None:\n            self.session_var_map.update(session_vars)\n        invoke_initializers(self)\n\n    def _register_components(self):\n        self._register_credential_provider()\n        self._register_token_provider()\n        self._register_data_loader()\n        self._register_endpoint_resolver()\n        self._register_event_emitter()\n        self._register_response_parser_factory()\n        self._register_exceptions_factory()\n        self._register_config_store()\n        self._register_monitor()\n        self._register_default_config_resolver()\n        self._register_smart_defaults_factory()\n        self._register_user_agent_creator()\n\n    def _register_event_emitter(self):\n        self._components.register_component('event_emitter', self._events)\n\n    def _register_token_provider(self):\n        self._components.lazy_register_component(\n            'token_provider', self._create_token_resolver\n        )\n\n    def _create_token_resolver(self):\n        return botocore.tokens.create_token_resolver(self)\n\n    def _register_credential_provider(self):\n        self._components.lazy_register_component(\n            'credential_provider', self._create_credential_resolver\n        )\n\n    def _create_credential_resolver(self):\n        return botocore.credentials.create_credential_resolver(\n            self, region_name=self._last_client_region_used\n        )\n\n    def _register_data_loader(self):\n        self._components.lazy_register_component(\n            'data_loader',\n            lambda: create_loader(self.get_config_variable('data_path')),\n        )\n\n    def _register_endpoint_resolver(self):\n        def create_default_resolver():\n            loader = self.get_component('data_loader')\n            endpoints, path = loader.load_data_with_path('endpoints')\n            uses_builtin = loader.is_builtin_path(path)\n            return EndpointResolver(endpoints, uses_builtin_data=uses_builtin)\n\n        self._internal_components.lazy_register_component(\n            'endpoint_resolver', create_default_resolver\n        )\n\n    def _register_default_config_resolver(self):\n        def create_default_config_resolver():\n            loader = self.get_component('data_loader')\n            defaults = loader.load_data('sdk-default-configuration')\n            return DefaultConfigResolver(defaults)\n\n        self._internal_components.lazy_register_component(\n            'default_config_resolver', create_default_config_resolver\n        )\n\n    def _register_smart_defaults_factory(self):\n        def create_smart_defaults_factory():\n            default_config_resolver = self._get_internal_component(\n                'default_config_resolver'\n            )\n            imds_region_provider = IMDSRegionProvider(session=self)\n            return SmartDefaultsConfigStoreFactory(\n                default_config_resolver, imds_region_provider\n            )\n\n        self._internal_components.lazy_register_component(\n            'smart_defaults_factory', create_smart_defaults_factory\n        )\n\n    def _register_response_parser_factory(self):\n        self._components.register_component(\n            'response_parser_factory', ResponseParserFactory()\n        )\n\n    def _register_exceptions_factory(self):\n        self._internal_components.register_component(\n            'exceptions_factory', ClientExceptionsFactory()\n        )\n\n    def _register_builtin_handlers(self, events):\n        for spec in handlers.BUILTIN_HANDLERS:\n            if len(spec) == 2:\n                event_name, handler = spec\n                self.register(event_name, handler)\n            else:\n                event_name, handler, register_type = spec\n                if register_type is handlers.REGISTER_FIRST:\n                    self._events.register_first(event_name, handler)\n                elif register_type is handlers.REGISTER_LAST:\n                    self._events.register_last(event_name, handler)\n\n    def _register_config_store(self):\n        config_store_component = ConfigValueStore(\n            mapping=create_botocore_default_config_mapping(self)\n        )\n        self._components.register_component(\n            'config_store', config_store_component\n        )\n\n    def _register_monitor(self):\n        self._internal_components.lazy_register_component(\n            'monitor', self._create_csm_monitor\n        )\n\n    def _register_user_agent_creator(self):\n        uas = UserAgentString.from_environment()\n        self._components.register_component('user_agent_creator', uas)\n\n    def _create_csm_monitor(self):\n        if self.get_config_variable('csm_enabled'):\n            client_id = self.get_config_variable('csm_client_id')\n            host = self.get_config_variable('csm_host')\n            port = self.get_config_variable('csm_port')\n            handler = monitoring.Monitor(\n                adapter=monitoring.MonitorEventAdapter(),\n                publisher=monitoring.SocketPublisher(\n                    socket=socket.socket(socket.AF_INET, socket.SOCK_DGRAM),\n                    host=host,\n                    port=port,\n                    serializer=monitoring.CSMSerializer(\n                        csm_client_id=client_id\n                    ),\n                ),\n            )\n            return handler\n        return None\n\n    def _get_crt_version(self):\n        user_agent_creator = self.get_component('user_agent_creator')\n        return user_agent_creator._crt_version or 'Unknown'\n\n    @property\n    def available_profiles(self):\n        return list(self._build_profile_map().keys())\n\n    def _build_profile_map(self):\n        # This will build the profile map if it has not been created,\n        # otherwise it will return the cached value.  The profile map\n        # is a list of profile names, to the config values for the profile.\n        if self._profile_map is None:\n            self._profile_map = self.full_config['profiles']\n        return self._profile_map\n\n    @property\n    def profile(self):\n        if self._profile is None:\n            profile = self.get_config_variable('profile')\n            self._profile = profile\n        return self._profile\n\n    def get_config_variable(self, logical_name, methods=None):\n        if methods is not None:\n            return self._get_config_variable_with_custom_methods(\n                logical_name, methods\n            )\n        return self.get_component('config_store').get_config_variable(\n            logical_name\n        )\n\n    def _get_config_variable_with_custom_methods(self, logical_name, methods):\n        # If a custom list of methods was supplied we need to perserve the\n        # behavior with the new system. To do so a new chain that is a copy of\n        # the old one will be constructed, but only with the supplied methods\n        # being added to the chain. This chain will be consulted for a value\n        # and then thrown out. This is not efficient, nor is the methods arg\n        # used in botocore, this is just for backwards compatibility.\n        chain_builder = SubsetChainConfigFactory(session=self, methods=methods)\n        mapping = create_botocore_default_config_mapping(self)\n        for name, config_options in self.session_var_map.items():\n            config_name, env_vars, default, typecast = config_options\n            build_chain_config_args = {\n                'conversion_func': typecast,\n                'default': default,\n            }\n            if 'instance' in methods:\n                build_chain_config_args['instance_name'] = name\n            if 'env' in methods:\n                build_chain_config_args['env_var_names'] = env_vars\n            if 'config' in methods:\n                build_chain_config_args['config_property_name'] = config_name\n            mapping[name] = chain_builder.create_config_chain(\n                **build_chain_config_args\n            )\n        config_store_component = ConfigValueStore(mapping=mapping)\n        value = config_store_component.get_config_variable(logical_name)\n        return value\n\n    def set_config_variable(self, logical_name, value):\n        \"\"\"Set a configuration variable to a specific value.\n\n        By using this method, you can override the normal lookup\n        process used in ``get_config_variable`` by explicitly setting\n        a value.  Subsequent calls to ``get_config_variable`` will\n        use the ``value``.  This gives you per-session specific\n        configuration values.\n\n        ::\n            >>> # Assume logical name 'foo' maps to env var 'FOO'\n            >>> os.environ['FOO'] = 'myvalue'\n            >>> s.get_config_variable('foo')\n            'myvalue'\n            >>> s.set_config_variable('foo', 'othervalue')\n            >>> s.get_config_variable('foo')\n            'othervalue'\n\n        :type logical_name: str\n        :param logical_name: The logical name of the session variable\n            you want to set.  These are the keys in ``SESSION_VARIABLES``.\n        :param value: The value to associate with the config variable.\n\n        \"\"\"\n        logger.debug(\n            \"Setting config variable for %s to %r\",\n            logical_name,\n            value,\n        )\n        self._session_instance_vars[logical_name] = value\n\n    def instance_variables(self):\n        return copy.copy(self._session_instance_vars)\n\n    def get_scoped_config(self):\n        \"\"\"\n        Returns the config values from the config file scoped to the current\n        profile.\n\n        The configuration data is loaded **only** from the config file.\n        It does not resolve variables based on different locations\n        (e.g. first from the session instance, then from environment\n        variables, then from the config file).  If you want this lookup\n        behavior, use the ``get_config_variable`` method instead.\n\n        Note that this configuration is specific to a single profile (the\n        ``profile`` session variable).\n\n        If the ``profile`` session variable is set and the profile does\n        not exist in the config file, a ``ProfileNotFound`` exception\n        will be raised.\n\n        :raises: ConfigNotFound, ConfigParseError, ProfileNotFound\n        :rtype: dict\n\n        \"\"\"\n        profile_name = self.get_config_variable('profile')\n        profile_map = self._build_profile_map()\n        # If a profile is not explicitly set return the default\n        # profile config or an empty config dict if we don't have\n        # a default profile.\n        if profile_name is None:\n            return profile_map.get('default', {})\n        elif profile_name not in profile_map:\n            # Otherwise if they specified a profile, it has to\n            # exist (even if it's the default profile) otherwise\n            # we complain.\n            raise ProfileNotFound(profile=profile_name)\n        else:\n            return profile_map[profile_name]\n\n    @property\n    def full_config(self):\n        \"\"\"Return the parsed config file.\n\n        The ``get_config`` method returns the config associated with the\n        specified profile.  This property returns the contents of the\n        **entire** config file.\n\n        :rtype: dict\n        \"\"\"\n        if self._config is None:\n            try:\n                config_file = self.get_config_variable('config_file')\n                self._config = botocore.configloader.load_config(config_file)\n            except ConfigNotFound:\n                self._config = {'profiles': {}}\n            try:\n                # Now we need to inject the profiles from the\n                # credentials file.  We don't actually need the values\n                # in the creds file, only the profile names so that we\n                # can validate the user is not referring to a nonexistent\n                # profile.\n                cred_file = self.get_config_variable('credentials_file')\n                cred_profiles = botocore.configloader.raw_config_parse(\n                    cred_file\n                )\n                for profile in cred_profiles:\n                    cred_vars = cred_profiles[profile]\n                    if profile not in self._config['profiles']:\n                        self._config['profiles'][profile] = cred_vars\n                    else:\n                        self._config['profiles'][profile].update(cred_vars)\n            except ConfigNotFound:\n                pass\n        return self._config\n\n    def get_default_client_config(self):\n        \"\"\"Retrieves the default config for creating clients\n\n        :rtype: botocore.client.Config\n        :returns: The default client config object when creating clients. If\n            the value is ``None`` then there is no default config object\n            attached to the session.\n        \"\"\"\n        return self._client_config\n\n    def set_default_client_config(self, client_config):\n        \"\"\"Sets the default config for creating clients\n\n        :type client_config: botocore.client.Config\n        :param client_config: The default client config object when creating\n            clients. If the value is ``None`` then there is no default config\n            object attached to the session.\n        \"\"\"\n        self._client_config = client_config\n\n    def set_credentials(self, access_key, secret_key, token=None):\n        \"\"\"\n        Manually create credentials for this session.  If you would\n        prefer to use botocore without a config file, environment variables,\n        or IAM roles, you can pass explicit credentials into this\n        method to establish credentials for this session.\n\n        :type access_key: str\n        :param access_key: The access key part of the credentials.\n\n        :type secret_key: str\n        :param secret_key: The secret key part of the credentials.\n\n        :type token: str\n        :param token: An option session token used by STS session\n            credentials.\n        \"\"\"\n        self._credentials = botocore.credentials.Credentials(\n            access_key, secret_key, token\n        )\n\n    def get_credentials(self):\n        \"\"\"\n        Return the :class:`botocore.credential.Credential` object\n        associated with this session.  If the credentials have not\n        yet been loaded, this will attempt to load them.  If they\n        have already been loaded, this will return the cached\n        credentials.\n\n        \"\"\"\n        if self._credentials is None:\n            self._credentials = self._components.get_component(\n                'credential_provider'\n            ).load_credentials()\n        return self._credentials\n\n    def get_auth_token(self):\n        \"\"\"\n        Return the :class:`botocore.tokens.AuthToken` object associated with\n        this session. If the authorization token has not yet been loaded, this\n        will attempt to load it. If it has already been loaded, this will\n        return the cached authorization token.\n\n        \"\"\"\n        if self._auth_token is None:\n            provider = self._components.get_component('token_provider')\n            self._auth_token = provider.load_token()\n        return self._auth_token\n\n    def user_agent(self):\n        \"\"\"\n        Return a string suitable for use as a User-Agent header.\n        The string will be of the form:\n\n        <agent_name>/<agent_version> Python/<py_ver> <plat_name>/<plat_ver> <exec_env>\n\n        Where:\n\n         - agent_name is the value of the `user_agent_name` attribute\n           of the session object (`Botocore` by default).\n         - agent_version is the value of the `user_agent_version`\n           attribute of the session object (the botocore version by default).\n           by default.\n         - py_ver is the version of the Python interpreter beng used.\n         - plat_name is the name of the platform (e.g. Darwin)\n         - plat_ver is the version of the platform\n         - exec_env is exec-env/$AWS_EXECUTION_ENV\n\n        If ``user_agent_extra`` is not empty, then this value will be\n        appended to the end of the user agent string.\n\n        \"\"\"\n        base = (\n            f'{self.user_agent_name}/{self.user_agent_version} '\n            f'Python/{platform.python_version()} '\n            f'{platform.system()}/{platform.release()}'\n        )\n        if HAS_CRT:\n            base += ' awscrt/%s' % self._get_crt_version()\n        if os.environ.get('AWS_EXECUTION_ENV') is not None:\n            base += ' exec-env/%s' % os.environ.get('AWS_EXECUTION_ENV')\n        if self.user_agent_extra:\n            base += ' %s' % self.user_agent_extra\n\n        return base\n\n    def get_data(self, data_path):\n        \"\"\"\n        Retrieve the data associated with `data_path`.\n\n        :type data_path: str\n        :param data_path: The path to the data you wish to retrieve.\n        \"\"\"\n        return self.get_component('data_loader').load_data(data_path)\n\n    def get_service_model(self, service_name, api_version=None):\n        \"\"\"Get the service model object.\n\n        :type service_name: string\n        :param service_name: The service name\n\n        :type api_version: string\n        :param api_version: The API version of the service.  If none is\n            provided, then the latest API version will be used.\n\n        :rtype: L{botocore.model.ServiceModel}\n        :return: The botocore service model for the service.\n\n        \"\"\"\n        service_description = self.get_service_data(service_name, api_version)\n        return ServiceModel(service_description, service_name=service_name)\n\n    def get_waiter_model(self, service_name, api_version=None):\n        loader = self.get_component('data_loader')\n        waiter_config = loader.load_service_model(\n            service_name, 'waiters-2', api_version\n        )\n        return waiter.WaiterModel(waiter_config)\n\n    def get_paginator_model(self, service_name, api_version=None):\n        loader = self.get_component('data_loader')\n        paginator_config = loader.load_service_model(\n            service_name, 'paginators-1', api_version\n        )\n        return paginate.PaginatorModel(paginator_config)\n\n    def get_service_data(self, service_name, api_version=None):\n        \"\"\"\n        Retrieve the fully merged data associated with a service.\n        \"\"\"\n        data_path = service_name\n        service_data = self.get_component('data_loader').load_service_model(\n            data_path, type_name='service-2', api_version=api_version\n        )\n        service_id = EVENT_ALIASES.get(service_name, service_name)\n        self._events.emit(\n            'service-data-loaded.%s' % service_id,\n            service_data=service_data,\n            service_name=service_name,\n            session=self,\n        )\n        return service_data\n\n    def get_available_services(self):\n        \"\"\"\n        Return a list of names of available services.\n        \"\"\"\n        return self.get_component('data_loader').list_available_services(\n            type_name='service-2'\n        )\n\n    def set_debug_logger(self, logger_name='botocore'):\n        \"\"\"\n        Convenience function to quickly configure full debug output\n        to go to the console.\n        \"\"\"\n        self.set_stream_logger(logger_name, logging.DEBUG)\n\n    def set_stream_logger(\n        self, logger_name, log_level, stream=None, format_string=None\n    ):\n        \"\"\"\n        Convenience method to configure a stream logger.\n\n        :type logger_name: str\n        :param logger_name: The name of the logger to configure\n\n        :type log_level: str\n        :param log_level: The log level to set for the logger.  This\n            is any param supported by the ``.setLevel()`` method of\n            a ``Log`` object.\n\n        :type stream: file\n        :param stream: A file like object to log to.  If none is provided\n            then sys.stderr will be used.\n\n        :type format_string: str\n        :param format_string: The format string to use for the log\n            formatter.  If none is provided this will default to\n            ``self.LOG_FORMAT``.\n\n        \"\"\"\n        log = logging.getLogger(logger_name)\n        log.setLevel(logging.DEBUG)\n\n        ch = logging.StreamHandler(stream)\n        ch.setLevel(log_level)\n\n        # create formatter\n        if format_string is None:\n            format_string = self.LOG_FORMAT\n        formatter = logging.Formatter(format_string)\n\n        # add formatter to ch\n        ch.setFormatter(formatter)\n\n        # add ch to logger\n        log.addHandler(ch)\n\n    def set_file_logger(self, log_level, path, logger_name='botocore'):\n        \"\"\"\n        Convenience function to quickly configure any level of logging\n        to a file.\n\n        :type log_level: int\n        :param log_level: A log level as specified in the `logging` module\n\n        :type path: string\n        :param path: Path to the log file.  The file will be created\n            if it doesn't already exist.\n        \"\"\"\n        log = logging.getLogger(logger_name)\n        log.setLevel(logging.DEBUG)\n\n        # create console handler and set level to debug\n        ch = logging.FileHandler(path)\n        ch.setLevel(log_level)\n\n        # create formatter\n        formatter = logging.Formatter(self.LOG_FORMAT)\n\n        # add formatter to ch\n        ch.setFormatter(formatter)\n\n        # add ch to logger\n        log.addHandler(ch)\n\n    def register(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        \"\"\"Register a handler with an event.\n\n        :type event_name: str\n        :param event_name: The name of the event.\n\n        :type handler: callable\n        :param handler: The callback to invoke when the event\n            is emitted.  This object must be callable, and must\n            accept ``**kwargs``.  If either of these preconditions are\n            not met, a ``ValueError`` will be raised.\n\n        :type unique_id: str\n        :param unique_id: An optional identifier to associate with the\n            registration.  A unique_id can only be used once for\n            the entire session registration (unless it is unregistered).\n            This can be used to prevent an event handler from being\n            registered twice.\n\n        :param unique_id_uses_count: boolean\n        :param unique_id_uses_count: Specifies if the event should maintain\n            a count when a ``unique_id`` is registered and unregisted. The\n            event can only be completely unregistered once every register call\n            using the unique id has been matched by an ``unregister`` call.\n            If ``unique_id`` is specified, subsequent ``register``\n            calls must use the same value for  ``unique_id_uses_count``\n            as the ``register`` call that first registered the event.\n\n        :raises ValueError: If the call to ``register`` uses ``unique_id``\n            but the value for ``unique_id_uses_count`` differs from the\n            ``unique_id_uses_count`` value declared by the very first\n            ``register`` call for that ``unique_id``.\n        \"\"\"\n        self._events.register(\n            event_name,\n            handler,\n            unique_id,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def unregister(\n        self,\n        event_name,\n        handler=None,\n        unique_id=None,\n        unique_id_uses_count=False,\n    ):\n        \"\"\"Unregister a handler with an event.\n\n        :type event_name: str\n        :param event_name: The name of the event.\n\n        :type handler: callable\n        :param handler: The callback to unregister.\n\n        :type unique_id: str\n        :param unique_id: A unique identifier identifying the callback\n            to unregister.  You can provide either the handler or the\n            unique_id, you do not have to provide both.\n\n        :param unique_id_uses_count: boolean\n        :param unique_id_uses_count: Specifies if the event should maintain\n            a count when a ``unique_id`` is registered and unregisted. The\n            event can only be completely unregistered once every ``register``\n            call using the ``unique_id`` has been matched by an ``unregister``\n            call. If the ``unique_id`` is specified, subsequent\n            ``unregister`` calls must use the same value for\n            ``unique_id_uses_count`` as the ``register`` call that first\n            registered the event.\n\n        :raises ValueError: If the call to ``unregister`` uses ``unique_id``\n            but the value for ``unique_id_uses_count`` differs from the\n            ``unique_id_uses_count`` value declared by the very first\n            ``register`` call for that ``unique_id``.\n        \"\"\"\n        self._events.unregister(\n            event_name,\n            handler=handler,\n            unique_id=unique_id,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def emit(self, event_name, **kwargs):\n        return self._events.emit(event_name, **kwargs)\n\n    def emit_first_non_none_response(self, event_name, **kwargs):\n        responses = self._events.emit(event_name, **kwargs)\n        return first_non_none_response(responses)\n\n    def get_component(self, name):\n        try:\n            return self._components.get_component(name)\n        except ValueError:\n            if name in ['endpoint_resolver', 'exceptions_factory']:\n                warnings.warn(\n                    'Fetching the %s component with the get_component() '\n                    'method is deprecated as the component has always been '\n                    'considered an internal interface of botocore' % name,\n                    DeprecationWarning,\n                )\n                return self._internal_components.get_component(name)\n            raise\n\n    def _get_internal_component(self, name):\n        # While this method may be called by botocore classes outside of the\n        # Session, this method should **never** be used by a class that lives\n        # outside of botocore.\n        return self._internal_components.get_component(name)\n\n    def _register_internal_component(self, name, component):\n        # While this method may be called by botocore classes outside of the\n        # Session, this method should **never** be used by a class that lives\n        # outside of botocore.\n        return self._internal_components.register_component(name, component)\n\n    def register_component(self, name, component):\n        self._components.register_component(name, component)\n\n    def lazy_register_component(self, name, component):\n        self._components.lazy_register_component(name, component)\n\n    def create_client(\n        self,\n        service_name,\n        region_name=None,\n        api_version=None,\n        use_ssl=True,\n        verify=None,\n        endpoint_url=None,\n        aws_access_key_id=None,\n        aws_secret_access_key=None,\n        aws_session_token=None,\n        config=None,\n    ):\n        \"\"\"Create a botocore client.\n\n        :type service_name: string\n        :param service_name: The name of the service for which a client will\n            be created.  You can use the ``Session.get_available_services()``\n            method to get a list of all available service names.\n\n        :type region_name: string\n        :param region_name: The name of the region associated with the client.\n            A client is associated with a single region.\n\n        :type api_version: string\n        :param api_version: The API version to use.  By default, botocore will\n            use the latest API version when creating a client.  You only need\n            to specify this parameter if you want to use a previous API version\n            of the client.\n\n        :type use_ssl: boolean\n        :param use_ssl: Whether or not to use SSL.  By default, SSL is used.\n            Note that not all services support non-ssl connections.\n\n        :type verify: boolean/string\n        :param verify: Whether or not to verify SSL certificates.\n            By default SSL certificates are verified.  You can provide the\n            following values:\n\n            * False - do not validate SSL certificates.  SSL will still be\n              used (unless use_ssl is False), but SSL certificates\n              will not be verified.\n            * path/to/cert/bundle.pem - A filename of the CA cert bundle to\n              uses.  You can specify this argument if you want to use a\n              different CA cert bundle than the one used by botocore.\n\n        :type endpoint_url: string\n        :param endpoint_url: The complete URL to use for the constructed\n            client.  Normally, botocore will automatically construct the\n            appropriate URL to use when communicating with a service.  You can\n            specify a complete URL (including the \"http/https\" scheme) to\n            override this behavior.  If this value is provided, then\n            ``use_ssl`` is ignored.\n\n        :type aws_access_key_id: string\n        :param aws_access_key_id: The access key to use when creating\n            the client.  This is entirely optional, and if not provided,\n            the credentials configured for the session will automatically\n            be used.  You only need to provide this argument if you want\n            to override the credentials used for this specific client.\n\n        :type aws_secret_access_key: string\n        :param aws_secret_access_key: The secret key to use when creating\n            the client.  Same semantics as aws_access_key_id above.\n\n        :type aws_session_token: string\n        :param aws_session_token: The session token to use when creating\n            the client.  Same semantics as aws_access_key_id above.\n\n        :type config: botocore.client.Config\n        :param config: Advanced client configuration options. If a value\n            is specified in the client config, its value will take precedence\n            over environment variables and configuration values, but not over\n            a value passed explicitly to the method. If a default config\n            object is set on the session, the config object used when creating\n            the client will be the result of calling ``merge()`` on the\n            default config with the config provided to this call.\n\n        :rtype: botocore.client.BaseClient\n        :return: A botocore client instance\n\n        \"\"\"\n        default_client_config = self.get_default_client_config()\n        # If a config is provided and a default config is set, then\n        # use the config resulting from merging the two.\n        if config is not None and default_client_config is not None:\n            config = default_client_config.merge(config)\n        # If a config was not provided then use the default\n        # client config from the session\n        elif default_client_config is not None:\n            config = default_client_config\n\n        region_name = self._resolve_region_name(region_name, config)\n\n        # Figure out the verify value base on the various\n        # configuration options.\n        if verify is None:\n            verify = self.get_config_variable('ca_bundle')\n\n        if api_version is None:\n            api_version = self.get_config_variable('api_versions').get(\n                service_name, None\n            )\n\n        loader = self.get_component('data_loader')\n        event_emitter = self.get_component('event_emitter')\n        response_parser_factory = self.get_component('response_parser_factory')\n        if config is not None and config.signature_version is UNSIGNED:\n            credentials = None\n        elif (\n            aws_access_key_id is not None and aws_secret_access_key is not None\n        ):\n            credentials = botocore.credentials.Credentials(\n                access_key=aws_access_key_id,\n                secret_key=aws_secret_access_key,\n                token=aws_session_token,\n            )\n        elif self._missing_cred_vars(aws_access_key_id, aws_secret_access_key):\n            raise PartialCredentialsError(\n                provider='explicit',\n                cred_var=self._missing_cred_vars(\n                    aws_access_key_id, aws_secret_access_key\n                ),\n            )\n        else:\n            credentials = self.get_credentials()\n        auth_token = self.get_auth_token()\n        endpoint_resolver = self._get_internal_component('endpoint_resolver')\n        exceptions_factory = self._get_internal_component('exceptions_factory')\n        config_store = copy.copy(self.get_component('config_store'))\n        user_agent_creator = self.get_component('user_agent_creator')\n        # Session configuration values for the user agent string are applied\n        # just before each client creation because they may have been modified\n        # at any time between session creation and client creation.\n        user_agent_creator.set_session_config(\n            session_user_agent_name=self.user_agent_name,\n            session_user_agent_version=self.user_agent_version,\n            session_user_agent_extra=self.user_agent_extra,\n        )\n        defaults_mode = self._resolve_defaults_mode(config, config_store)\n        if defaults_mode != 'legacy':\n            smart_defaults_factory = self._get_internal_component(\n                'smart_defaults_factory'\n            )\n            smart_defaults_factory.merge_smart_defaults(\n                config_store, defaults_mode, region_name\n            )\n\n        self._add_configured_endpoint_provider(\n            client_name=service_name,\n            config_store=config_store,\n        )\n\n        client_creator = botocore.client.ClientCreator(\n            loader,\n            endpoint_resolver,\n            self.user_agent(),\n            event_emitter,\n            retryhandler,\n            translate,\n            response_parser_factory,\n            exceptions_factory,\n            config_store,\n            user_agent_creator=user_agent_creator,\n        )\n        client = client_creator.create_client(\n            service_name=service_name,\n            region_name=region_name,\n            is_secure=use_ssl,\n            endpoint_url=endpoint_url,\n            verify=verify,\n            credentials=credentials,\n            scoped_config=self.get_scoped_config(),\n            client_config=config,\n            api_version=api_version,\n            auth_token=auth_token,\n        )\n        monitor = self._get_internal_component('monitor')\n        if monitor is not None:\n            monitor.register(client.meta.events)\n        return client\n\n    def _resolve_region_name(self, region_name, config):\n        # Figure out the user-provided region based on the various\n        # configuration options.\n        if region_name is None:\n            if config and config.region_name is not None:\n                region_name = config.region_name\n            else:\n                region_name = self.get_config_variable('region')\n\n        validate_region_name(region_name)\n        # For any client that we create in retrieving credentials\n        # we want to create it using the same region as specified in\n        # creating this client. It is important to note though that the\n        # credentials client is only created once per session. So if a new\n        # client is created with a different region, its credential resolver\n        # will use the region of the first client. However, that is not an\n        # issue as of now because the credential resolver uses only STS and\n        # the credentials returned at regional endpoints are valid across\n        # all regions in the partition.\n        self._last_client_region_used = region_name\n        return region_name\n\n    def _resolve_defaults_mode(self, client_config, config_store):\n        mode = config_store.get_config_variable('defaults_mode')\n\n        if client_config and client_config.defaults_mode:\n            mode = client_config.defaults_mode\n\n        default_config_resolver = self._get_internal_component(\n            'default_config_resolver'\n        )\n        default_modes = default_config_resolver.get_default_modes()\n        lmode = mode.lower()\n        if lmode not in default_modes:\n            raise InvalidDefaultsMode(\n                mode=mode, valid_modes=', '.join(default_modes)\n            )\n\n        return lmode\n\n    def _add_configured_endpoint_provider(self, client_name, config_store):\n        chain = ConfiguredEndpointProvider(\n            full_config=self.full_config,\n            scoped_config=self.get_scoped_config(),\n            client_name=client_name,\n        )\n        config_store.set_config_provider(\n            logical_name='endpoint_url',\n            provider=chain,\n        )\n\n    def _missing_cred_vars(self, access_key, secret_key):\n        if access_key is not None and secret_key is None:\n            return 'aws_secret_access_key'\n        if secret_key is not None and access_key is None:\n            return 'aws_access_key_id'\n        return None\n\n    def get_available_partitions(self):\n        \"\"\"Lists the available partitions found on disk\n\n        :rtype: list\n        :return: Returns a list of partition names (e.g., [\"aws\", \"aws-cn\"])\n        \"\"\"\n        resolver = self._get_internal_component('endpoint_resolver')\n        return resolver.get_available_partitions()\n\n    def get_partition_for_region(self, region_name):\n        \"\"\"Lists the partition name of a particular region.\n\n        :type region_name: string\n        :param region_name: Name of the region to list partition for (e.g.,\n             us-east-1).\n\n        :rtype: string\n        :return: Returns the respective partition name (e.g., aws).\n        \"\"\"\n        resolver = self._get_internal_component('endpoint_resolver')\n        return resolver.get_partition_for_region(region_name)\n\n    def get_available_regions(\n        self, service_name, partition_name='aws', allow_non_regional=False\n    ):\n        \"\"\"Lists the region and endpoint names of a particular partition.\n\n        :type service_name: string\n        :param service_name: Name of a service to list endpoint for (e.g., s3).\n            This parameter accepts a service name (e.g., \"elb\") or endpoint\n            prefix (e.g., \"elasticloadbalancing\").\n\n        :type partition_name: string\n        :param partition_name: Name of the partition to limit endpoints to.\n            (e.g., aws for the public AWS endpoints, aws-cn for AWS China\n            endpoints, aws-us-gov for AWS GovCloud (US) Endpoints, etc.\n\n        :type allow_non_regional: bool\n        :param allow_non_regional: Set to True to include endpoints that are\n             not regional endpoints (e.g., s3-external-1,\n             fips-us-gov-west-1, etc).\n        :return: Returns a list of endpoint names (e.g., [\"us-east-1\"]).\n        \"\"\"\n        resolver = self._get_internal_component('endpoint_resolver')\n        results = []\n        try:\n            service_data = self.get_service_data(service_name)\n            endpoint_prefix = service_data['metadata'].get(\n                'endpointPrefix', service_name\n            )\n            results = resolver.get_available_endpoints(\n                endpoint_prefix, partition_name, allow_non_regional\n            )\n        except UnknownServiceError:\n            pass\n        return results\n\n\nclass ComponentLocator:\n    \"\"\"Service locator for session components.\"\"\"\n\n    def __init__(self):\n        self._components = {}\n        self._deferred = {}\n\n    def get_component(self, name):\n        if name in self._deferred:\n            factory = self._deferred[name]\n            self._components[name] = factory()\n            # Only delete the component from the deferred dict after\n            # successfully creating the object from the factory as well as\n            # injecting the instantiated value into the _components dict.\n            try:\n                del self._deferred[name]\n            except KeyError:\n                # If we get here, it's likely that get_component was called\n                # concurrently from multiple threads, and another thread\n                # already deleted the entry. This means the factory was\n                # probably called twice, but cleaning up the deferred entry\n                # should not crash outright.\n                pass\n        try:\n            return self._components[name]\n        except KeyError:\n            raise ValueError(\"Unknown component: %s\" % name)\n\n    def register_component(self, name, component):\n        self._components[name] = component\n        try:\n            del self._deferred[name]\n        except KeyError:\n            pass\n\n    def lazy_register_component(self, name, no_arg_factory):\n        self._deferred[name] = no_arg_factory\n        try:\n            del self._components[name]\n        except KeyError:\n            pass\n\n\nclass SessionVarDict(MutableMapping):\n    def __init__(self, session, session_vars):\n        self._session = session\n        self._store = copy.copy(session_vars)\n\n    def __getitem__(self, key):\n        return self._store[key]\n\n    def __setitem__(self, key, value):\n        self._store[key] = value\n        self._update_config_store_from_session_vars(key, value)\n\n    def __delitem__(self, key):\n        del self._store[key]\n\n    def __iter__(self):\n        return iter(self._store)\n\n    def __len__(self):\n        return len(self._store)\n\n    def _update_config_store_from_session_vars(\n        self, logical_name, config_options\n    ):\n        # This is for backwards compatibility. The new preferred way to\n        # modify configuration logic is to use the component system to get\n        # the config_store component from the session, and then update\n        # a key with a custom config provider(s).\n        # This backwards compatibility method takes the old session_vars\n        # list of tuples and and transforms that into a set of updates to\n        # the config_store component.\n        config_chain_builder = ConfigChainFactory(session=self._session)\n        config_name, env_vars, default, typecast = config_options\n        config_store = self._session.get_component('config_store')\n        config_store.set_config_provider(\n            logical_name,\n            config_chain_builder.create_config_chain(\n                instance_name=logical_name,\n                env_var_names=env_vars,\n                config_property_names=config_name,\n                default=default,\n                conversion_func=typecast,\n            ),\n        )\n\n\nclass SubsetChainConfigFactory:\n    \"\"\"A class for creating backwards compatible configuration chains.\n\n    This class can be used instead of\n    :class:`botocore.configprovider.ConfigChainFactory` to make it honor the\n    methods argument to get_config_variable. This class can be used to filter\n    out providers that are not in the methods tuple when creating a new config\n    chain.\n    \"\"\"\n\n    def __init__(self, session, methods, environ=None):\n        self._factory = ConfigChainFactory(session, environ)\n        self._supported_methods = methods\n\n    def create_config_chain(\n        self,\n        instance_name=None,\n        env_var_names=None,\n        config_property_name=None,\n        default=None,\n        conversion_func=None,\n    ):\n        \"\"\"Build a config chain following the standard botocore pattern.\n\n        This config chain factory will omit any providers not in the methods\n        tuple provided at initialization. For example if given the tuple\n        ('instance', 'config',) it will not inject the environment provider\n        into the standard config chain. This lets the botocore session support\n        the custom ``methods`` argument for all the default botocore config\n        variables when calling ``get_config_variable``.\n        \"\"\"\n        if 'instance' not in self._supported_methods:\n            instance_name = None\n        if 'env' not in self._supported_methods:\n            env_var_names = None\n        if 'config' not in self._supported_methods:\n            config_property_name = None\n        return self._factory.create_config_chain(\n            instance_name=instance_name,\n            env_var_names=env_var_names,\n            config_property_names=config_property_name,\n            default=default,\n            conversion_func=conversion_func,\n        )\n\n\ndef get_session(env_vars=None):\n    \"\"\"\n    Return a new session object.\n    \"\"\"\n    return Session(env_vars)\n", "botocore/history.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nHISTORY_RECORDER = None\nlogger = logging.getLogger(__name__)\n\n\nclass BaseHistoryHandler:\n    def emit(self, event_type, payload, source):\n        raise NotImplementedError('emit()')\n\n\nclass HistoryRecorder:\n    def __init__(self):\n        self._enabled = False\n        self._handlers = []\n\n    def enable(self):\n        self._enabled = True\n\n    def disable(self):\n        self._enabled = False\n\n    def add_handler(self, handler):\n        self._handlers.append(handler)\n\n    def record(self, event_type, payload, source='BOTOCORE'):\n        if self._enabled and self._handlers:\n            for handler in self._handlers:\n                try:\n                    handler.emit(event_type, payload, source)\n                except Exception:\n                    # Never let the process die because we had a failure in\n                    # a record collection handler.\n                    logger.debug(\n                        \"Exception raised in %s.\", handler, exc_info=True\n                    )\n\n\ndef get_global_history_recorder():\n    global HISTORY_RECORDER\n    if HISTORY_RECORDER is None:\n        HISTORY_RECORDER = HistoryRecorder()\n    return HISTORY_RECORDER\n", "botocore/endpoint_provider.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\"\"\"\nNOTE: All classes and functions in this module are considered private and are\nsubject to abrupt breaking changes. Please do not use them directly.\n\nTo view the raw JSON that the objects in this module represent, please\ngo to any `endpoint-rule-set.json` file in /botocore/data/<service>/<api version>/\nor you can look at the test files in /tests/unit/data/endpoints/valid-rules/\n\"\"\"\n\n\nimport logging\nimport re\nfrom enum import Enum\nfrom string import Formatter\nfrom typing import NamedTuple\n\nfrom botocore import xform_name\nfrom botocore.compat import IPV4_RE, quote, urlparse\nfrom botocore.exceptions import EndpointResolutionError\nfrom botocore.utils import (\n    ArnParser,\n    InvalidArnException,\n    is_valid_ipv4_endpoint_url,\n    is_valid_ipv6_endpoint_url,\n    lru_cache_weakref,\n    normalize_url_path,\n    percent_encode,\n)\n\nlogger = logging.getLogger(__name__)\n\nTEMPLATE_STRING_RE = re.compile(r\"\\{[a-zA-Z#]+\\}\")\nGET_ATTR_RE = re.compile(r\"(\\w+)\\[(\\d+)\\]\")\nVALID_HOST_LABEL_RE = re.compile(\n    r\"^(?!-)[a-zA-Z\\d-]{1,63}(?<!-)$\",\n)\nCACHE_SIZE = 100\nARN_PARSER = ArnParser()\nSTRING_FORMATTER = Formatter()\n\n\nclass RuleSetStandardLibrary:\n    \"\"\"Rule actions to be performed by the EndpointProvider.\"\"\"\n\n    def __init__(self, partitions_data):\n        self.partitions_data = partitions_data\n\n    def is_func(self, argument):\n        \"\"\"Determine if an object is a function object.\n\n        :type argument: Any\n        :rtype: bool\n        \"\"\"\n        return isinstance(argument, dict) and \"fn\" in argument\n\n    def is_ref(self, argument):\n        \"\"\"Determine if an object is a reference object.\n\n        :type argument: Any\n        :rtype: bool\n        \"\"\"\n        return isinstance(argument, dict) and \"ref\" in argument\n\n    def is_template(self, argument):\n        \"\"\"Determine if an object contains a template string.\n\n        :type argument: Any\n        :rtpe: bool\n        \"\"\"\n        return (\n            isinstance(argument, str)\n            and TEMPLATE_STRING_RE.search(argument) is not None\n        )\n\n    def resolve_template_string(self, value, scope_vars):\n        \"\"\"Resolve and inject values into a template string.\n\n        :type value: str\n        :type scope_vars: dict\n        :rtype: str\n        \"\"\"\n        result = \"\"\n        for literal, reference, _, _ in STRING_FORMATTER.parse(value):\n            if reference is not None:\n                template_value = scope_vars\n                template_params = reference.split(\"#\")\n                for param in template_params:\n                    template_value = template_value[param]\n                result += f\"{literal}{template_value}\"\n            else:\n                result += literal\n        return result\n\n    def resolve_value(self, value, scope_vars):\n        \"\"\"Return evaluated value based on type.\n\n        :type value: Any\n        :type scope_vars: dict\n        :rtype: Any\n        \"\"\"\n        if self.is_func(value):\n            return self.call_function(value, scope_vars)\n        elif self.is_ref(value):\n            return scope_vars.get(value[\"ref\"])\n        elif self.is_template(value):\n            return self.resolve_template_string(value, scope_vars)\n\n        return value\n\n    def convert_func_name(self, value):\n        \"\"\"Normalize function names.\n\n        :type value: str\n        :rtype: str\n        \"\"\"\n        normalized_name = f\"{xform_name(value)}\"\n        if normalized_name == \"not\":\n            normalized_name = f\"_{normalized_name}\"\n        return normalized_name.replace(\".\", \"_\")\n\n    def call_function(self, func_signature, scope_vars):\n        \"\"\"Call the function with the resolved arguments and assign to `scope_vars`\n        when applicable.\n\n        :type func_signature: dict\n        :type scope_vars: dict\n        :rtype: Any\n        \"\"\"\n        func_args = [\n            self.resolve_value(arg, scope_vars)\n            for arg in func_signature[\"argv\"]\n        ]\n        func_name = self.convert_func_name(func_signature[\"fn\"])\n        func = getattr(self, func_name)\n        result = func(*func_args)\n        if \"assign\" in func_signature:\n            assign = func_signature[\"assign\"]\n            if assign in scope_vars:\n                raise EndpointResolutionError(\n                    msg=f\"Assignment {assign} already exists in \"\n                    \"scoped variables and cannot be overwritten\"\n                )\n            scope_vars[assign] = result\n        return result\n\n    def is_set(self, value):\n        \"\"\"Evaluates whether a value is set.\n\n        :type value: Any\n        :rytpe: bool\n        \"\"\"\n        return value is not None\n\n    def get_attr(self, value, path):\n        \"\"\"Find an attribute within a value given a path string. The path can contain\n        the name of the attribute and an index in brackets. A period separating attribute\n        names indicates the one to the right is nested. The index will always occur at\n        the end of the path.\n\n        :type value: dict or list\n        :type path: str\n        :rtype: Any\n        \"\"\"\n        for part in path.split(\".\"):\n            match = GET_ATTR_RE.search(part)\n            if match is not None:\n                name, index = match.groups()\n                index = int(index)\n                value = value.get(name)\n                if value is None or index >= len(value):\n                    return None\n                return value[index]\n            else:\n                value = value[part]\n        return value\n\n    def format_partition_output(self, partition):\n        output = partition[\"outputs\"]\n        output[\"name\"] = partition[\"id\"]\n        return output\n\n    def is_partition_match(self, region, partition):\n        matches_regex = re.match(partition[\"regionRegex\"], region) is not None\n        return region in partition[\"regions\"] or matches_regex\n\n    def aws_partition(self, value):\n        \"\"\"Match a region string to an AWS partition.\n\n        :type value: str\n        :rtype: dict\n        \"\"\"\n        partitions = self.partitions_data['partitions']\n\n        if value is not None:\n            for partition in partitions:\n                if self.is_partition_match(value, partition):\n                    return self.format_partition_output(partition)\n\n        # return the default partition if no matches were found\n        aws_partition = partitions[0]\n        return self.format_partition_output(aws_partition)\n\n    def aws_parse_arn(self, value):\n        \"\"\"Parse and validate string for ARN components.\n\n        :type value: str\n        :rtype: dict\n        \"\"\"\n        if value is None or not value.startswith(\"arn:\"):\n            return None\n\n        try:\n            arn_dict = ARN_PARSER.parse_arn(value)\n        except InvalidArnException:\n            return None\n\n        # partition, resource, and service are required\n        if not all(\n            (arn_dict[\"partition\"], arn_dict[\"service\"], arn_dict[\"resource\"])\n        ):\n            return None\n\n        arn_dict[\"accountId\"] = arn_dict.pop(\"account\")\n\n        resource = arn_dict.pop(\"resource\")\n        arn_dict[\"resourceId\"] = resource.replace(\":\", \"/\").split(\"/\")\n\n        return arn_dict\n\n    def is_valid_host_label(self, value, allow_subdomains):\n        \"\"\"Evaluates whether a value is a valid host label per\n        RFC 1123. If allow_subdomains is True, split on `.` and validate\n        each component separately.\n\n        :type value: str\n        :type allow_subdomains: bool\n        :rtype: bool\n        \"\"\"\n        if value is None or allow_subdomains is False and value.count(\".\") > 0:\n            return False\n\n        if allow_subdomains is True:\n            return all(\n                self.is_valid_host_label(label, False)\n                for label in value.split(\".\")\n            )\n\n        return VALID_HOST_LABEL_RE.match(value) is not None\n\n    def string_equals(self, value1, value2):\n        \"\"\"Evaluates two string values for equality.\n\n        :type value1: str\n        :type value2: str\n        :rtype: bool\n        \"\"\"\n        if not all(isinstance(val, str) for val in (value1, value2)):\n            msg = f\"Both values must be strings, not {type(value1)} and {type(value2)}.\"\n            raise EndpointResolutionError(msg=msg)\n        return value1 == value2\n\n    def uri_encode(self, value):\n        \"\"\"Perform percent-encoding on an input string.\n\n        :type value: str\n        :rytpe: str\n        \"\"\"\n        if value is None:\n            return None\n\n        return percent_encode(value)\n\n    def parse_url(self, value):\n        \"\"\"Parse a URL string into components.\n\n        :type value: str\n        :rtype: dict\n        \"\"\"\n        if value is None:\n            return None\n\n        url_components = urlparse(value)\n        try:\n            # url_parse may assign non-integer values to\n            # `port` and will fail when accessed.\n            url_components.port\n        except ValueError:\n            return None\n\n        scheme = url_components.scheme\n        query = url_components.query\n        # URLs with queries are not supported\n        if scheme not in (\"https\", \"http\") or len(query) > 0:\n            return None\n\n        path = url_components.path\n        normalized_path = quote(normalize_url_path(path))\n        if not normalized_path.endswith(\"/\"):\n            normalized_path = f\"{normalized_path}/\"\n\n        return {\n            \"scheme\": scheme,\n            \"authority\": url_components.netloc,\n            \"path\": path,\n            \"normalizedPath\": normalized_path,\n            \"isIp\": is_valid_ipv4_endpoint_url(value)\n            or is_valid_ipv6_endpoint_url(value),\n        }\n\n    def boolean_equals(self, value1, value2):\n        \"\"\"Evaluates two boolean values for equality.\n\n        :type value1: bool\n        :type value2: bool\n        :rtype: bool\n        \"\"\"\n        if not all(isinstance(val, bool) for val in (value1, value2)):\n            msg = f\"Both arguments must be bools, not {type(value1)} and {type(value2)}.\"\n            raise EndpointResolutionError(msg=msg)\n        return value1 is value2\n\n    def is_ascii(self, value):\n        \"\"\"Evaluates if a string only contains ASCII characters.\n\n        :type value: str\n        :rtype: bool\n        \"\"\"\n        try:\n            value.encode(\"ascii\")\n            return True\n        except UnicodeEncodeError:\n            return False\n\n    def substring(self, value, start, stop, reverse):\n        \"\"\"Computes a substring given the start index and end index. If `reverse` is\n        True, slice the string from the end instead.\n\n        :type value: str\n        :type start: int\n        :type end: int\n        :type reverse: bool\n        :rtype: str\n        \"\"\"\n        if not isinstance(value, str):\n            msg = f\"Input must be a string, not {type(value)}.\"\n            raise EndpointResolutionError(msg=msg)\n        if start >= stop or len(value) < stop or not self.is_ascii(value):\n            return None\n\n        if reverse is True:\n            r_start = len(value) - stop\n            r_stop = len(value) - start\n            return value[r_start:r_stop]\n\n        return value[start:stop]\n\n    def _not(self, value):\n        \"\"\"A function implementation of the logical operator `not`.\n\n        :type value: Any\n        :rtype: bool\n        \"\"\"\n        return not value\n\n    def aws_is_virtual_hostable_s3_bucket(self, value, allow_subdomains):\n        \"\"\"Evaluates whether a value is a valid bucket name for virtual host\n        style bucket URLs. To pass, the value must meet the following criteria:\n        1. is_valid_host_label(value) is True\n        2. length between 3 and 63 characters (inclusive)\n        3. does not contain uppercase characters\n        4. is not formatted as an IP address\n\n        If allow_subdomains is True, split on `.` and validate\n        each component separately.\n\n        :type value: str\n        :type allow_subdomains: bool\n        :rtype: bool\n        \"\"\"\n        if (\n            value is None\n            or len(value) < 3\n            or value.lower() != value\n            or IPV4_RE.match(value) is not None\n        ):\n            return False\n\n        return self.is_valid_host_label(\n            value, allow_subdomains=allow_subdomains\n        )\n\n\n# maintains backwards compatibility as `Library` was misspelled\n# in earlier versions\nRuleSetStandardLibary = RuleSetStandardLibrary\n\n\nclass BaseRule:\n    \"\"\"Base interface for individual endpoint rules.\"\"\"\n\n    def __init__(self, conditions, documentation=None):\n        self.conditions = conditions\n        self.documentation = documentation\n\n    def evaluate(self, scope_vars, rule_lib):\n        raise NotImplementedError()\n\n    def evaluate_conditions(self, scope_vars, rule_lib):\n        \"\"\"Determine if all conditions in a rule are met.\n\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: bool\n        \"\"\"\n        for func_signature in self.conditions:\n            result = rule_lib.call_function(func_signature, scope_vars)\n            if result is False or result is None:\n                return False\n        return True\n\n\nclass RuleSetEndpoint(NamedTuple):\n    \"\"\"A resolved endpoint object returned by a rule.\"\"\"\n\n    url: str\n    properties: dict\n    headers: dict\n\n\nclass EndpointRule(BaseRule):\n    def __init__(self, endpoint, **kwargs):\n        super().__init__(**kwargs)\n        self.endpoint = endpoint\n\n    def evaluate(self, scope_vars, rule_lib):\n        \"\"\"Determine if conditions are met to provide a valid endpoint.\n\n        :type scope_vars: dict\n        :rtype: RuleSetEndpoint\n        \"\"\"\n        if self.evaluate_conditions(scope_vars, rule_lib):\n            url = rule_lib.resolve_value(self.endpoint[\"url\"], scope_vars)\n            properties = self.resolve_properties(\n                self.endpoint.get(\"properties\", {}),\n                scope_vars,\n                rule_lib,\n            )\n            headers = self.resolve_headers(scope_vars, rule_lib)\n            return RuleSetEndpoint(\n                url=url, properties=properties, headers=headers\n            )\n\n        return None\n\n    def resolve_properties(self, properties, scope_vars, rule_lib):\n        \"\"\"Traverse `properties` attribute, resolving any template strings.\n\n        :type properties: dict/list/str\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: dict\n        \"\"\"\n        if isinstance(properties, list):\n            return [\n                self.resolve_properties(prop, scope_vars, rule_lib)\n                for prop in properties\n            ]\n        elif isinstance(properties, dict):\n            return {\n                key: self.resolve_properties(value, scope_vars, rule_lib)\n                for key, value in properties.items()\n            }\n        elif rule_lib.is_template(properties):\n            return rule_lib.resolve_template_string(properties, scope_vars)\n\n        return properties\n\n    def resolve_headers(self, scope_vars, rule_lib):\n        \"\"\"Iterate through headers attribute resolving all values.\n\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: dict\n        \"\"\"\n        resolved_headers = {}\n        headers = self.endpoint.get(\"headers\", {})\n\n        for header, values in headers.items():\n            resolved_headers[header] = [\n                rule_lib.resolve_value(item, scope_vars) for item in values\n            ]\n        return resolved_headers\n\n\nclass ErrorRule(BaseRule):\n    def __init__(self, error, **kwargs):\n        super().__init__(**kwargs)\n        self.error = error\n\n    def evaluate(self, scope_vars, rule_lib):\n        \"\"\"If an error rule's conditions are met, raise an error rule.\n\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: EndpointResolutionError\n        \"\"\"\n        if self.evaluate_conditions(scope_vars, rule_lib):\n            error = rule_lib.resolve_value(self.error, scope_vars)\n            raise EndpointResolutionError(msg=error)\n        return None\n\n\nclass TreeRule(BaseRule):\n    \"\"\"A tree rule is non-terminal meaning it will never be returned to a provider.\n    Additionally this means it has no attributes that need to be resolved.\n    \"\"\"\n\n    def __init__(self, rules, **kwargs):\n        super().__init__(**kwargs)\n        self.rules = [RuleCreator.create(**rule) for rule in rules]\n\n    def evaluate(self, scope_vars, rule_lib):\n        \"\"\"If a tree rule's conditions are met, iterate its sub-rules\n        and return first result found.\n\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: RuleSetEndpoint/EndpointResolutionError\n        \"\"\"\n        if self.evaluate_conditions(scope_vars, rule_lib):\n            for rule in self.rules:\n                # don't share scope_vars between rules\n                rule_result = rule.evaluate(scope_vars.copy(), rule_lib)\n                if rule_result:\n                    return rule_result\n        return None\n\n\nclass RuleCreator:\n    endpoint = EndpointRule\n    error = ErrorRule\n    tree = TreeRule\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Create a rule instance from metadata.\n\n        :rtype: TreeRule/EndpointRule/ErrorRule\n        \"\"\"\n        rule_type = kwargs.pop(\"type\")\n        try:\n            rule_class = getattr(cls, rule_type)\n        except AttributeError:\n            raise EndpointResolutionError(\n                msg=f\"Unknown rule type: {rule_type}. A rule must \"\n                \"be of type tree, endpoint or error.\"\n            )\n        else:\n            return rule_class(**kwargs)\n\n\nclass ParameterType(Enum):\n    \"\"\"Translation from `type` attribute to native Python type.\"\"\"\n\n    string = str\n    boolean = bool\n\n\nclass ParameterDefinition:\n    \"\"\"The spec of an individual parameter defined in a RuleSet.\"\"\"\n\n    def __init__(\n        self,\n        name,\n        parameter_type,\n        documentation=None,\n        builtIn=None,\n        default=None,\n        required=None,\n        deprecated=None,\n    ):\n        self.name = name\n        try:\n            self.parameter_type = getattr(\n                ParameterType, parameter_type.lower()\n            ).value\n        except AttributeError:\n            raise EndpointResolutionError(\n                msg=f\"Unknown parameter type: {parameter_type}. \"\n                \"A parameter must be of type string or boolean.\"\n            )\n        self.documentation = documentation\n        self.builtin = builtIn\n        self.default = default\n        self.required = required\n        self.deprecated = deprecated\n\n    def validate_input(self, value):\n        \"\"\"Perform base validation on parameter input.\n\n        :type value: Any\n        :raises: EndpointParametersError\n        \"\"\"\n\n        if not isinstance(value, self.parameter_type):\n            raise EndpointResolutionError(\n                msg=f\"Value ({self.name}) is the wrong \"\n                f\"type. Must be {self.parameter_type}.\"\n            )\n        if self.deprecated is not None:\n            depr_str = f\"{self.name} has been deprecated.\"\n            msg = self.deprecated.get(\"message\")\n            since = self.deprecated.get(\"since\")\n            if msg:\n                depr_str += f\"\\n{msg}\"\n            if since:\n                depr_str += f\"\\nDeprecated since {since}.\"\n            logger.info(depr_str)\n\n        return None\n\n    def process_input(self, value):\n        \"\"\"Process input against spec, applying default if value is None.\"\"\"\n        if value is None:\n            if self.default is not None:\n                return self.default\n            if self.required:\n                raise EndpointResolutionError(\n                    f\"Cannot find value for required parameter {self.name}\"\n                )\n            # in all other cases, the parameter will keep the value None\n        else:\n            self.validate_input(value)\n        return value\n\n\nclass RuleSet:\n    \"\"\"Collection of rules to derive a routable service endpoint.\"\"\"\n\n    def __init__(\n        self, version, parameters, rules, partitions, documentation=None\n    ):\n        self.version = version\n        self.parameters = self._ingest_parameter_spec(parameters)\n        self.rules = [RuleCreator.create(**rule) for rule in rules]\n        self.rule_lib = RuleSetStandardLibrary(partitions)\n        self.documentation = documentation\n\n    def _ingest_parameter_spec(self, parameters):\n        return {\n            name: ParameterDefinition(\n                name,\n                spec[\"type\"],\n                spec.get(\"documentation\"),\n                spec.get(\"builtIn\"),\n                spec.get(\"default\"),\n                spec.get(\"required\"),\n                spec.get(\"deprecated\"),\n            )\n            for name, spec in parameters.items()\n        }\n\n    def process_input_parameters(self, input_params):\n        \"\"\"Process each input parameter against its spec.\n\n        :type input_params: dict\n        \"\"\"\n        for name, spec in self.parameters.items():\n            value = spec.process_input(input_params.get(name))\n            if value is not None:\n                input_params[name] = value\n        return None\n\n    def evaluate(self, input_parameters):\n        \"\"\"Evaluate input parameters against rules returning first match.\n\n        :type input_parameters: dict\n        \"\"\"\n        self.process_input_parameters(input_parameters)\n        for rule in self.rules:\n            evaluation = rule.evaluate(input_parameters.copy(), self.rule_lib)\n            if evaluation is not None:\n                return evaluation\n        return None\n\n\nclass EndpointProvider:\n    \"\"\"Derives endpoints from a RuleSet for given input parameters.\"\"\"\n\n    def __init__(self, ruleset_data, partition_data):\n        self.ruleset = RuleSet(**ruleset_data, partitions=partition_data)\n\n    @lru_cache_weakref(maxsize=CACHE_SIZE)\n    def resolve_endpoint(self, **input_parameters):\n        \"\"\"Match input parameters to a rule.\n\n        :type input_parameters: dict\n        :rtype: RuleSetEndpoint\n        \"\"\"\n        params_for_error = input_parameters.copy()\n        endpoint = self.ruleset.evaluate(input_parameters)\n        if endpoint is None:\n            param_string = \"\\n\".join(\n                [f\"{key}: {value}\" for key, value in params_for_error.items()]\n            )\n            raise EndpointResolutionError(\n                msg=f\"No endpoint found for parameters:\\n{param_string}\"\n            )\n        return endpoint\n", "botocore/hooks.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport logging\nfrom collections import deque, namedtuple\n\nfrom botocore.compat import accepts_kwargs\nfrom botocore.utils import EVENT_ALIASES\n\nlogger = logging.getLogger(__name__)\n\n\n_NodeList = namedtuple('NodeList', ['first', 'middle', 'last'])\n_FIRST = 0\n_MIDDLE = 1\n_LAST = 2\n\n\nclass NodeList(_NodeList):\n    def __copy__(self):\n        first_copy = copy.copy(self.first)\n        middle_copy = copy.copy(self.middle)\n        last_copy = copy.copy(self.last)\n        copied = NodeList(first_copy, middle_copy, last_copy)\n        return copied\n\n\ndef first_non_none_response(responses, default=None):\n    \"\"\"Find first non None response in a list of tuples.\n\n    This function can be used to find the first non None response from\n    handlers connected to an event.  This is useful if you are interested\n    in the returned responses from event handlers. Example usage::\n\n        print(first_non_none_response([(func1, None), (func2, 'foo'),\n                                       (func3, 'bar')]))\n        # This will print 'foo'\n\n    :type responses: list of tuples\n    :param responses: The responses from the ``EventHooks.emit`` method.\n        This is a list of tuples, and each tuple is\n        (handler, handler_response).\n\n    :param default: If no non-None responses are found, then this default\n        value will be returned.\n\n    :return: The first non-None response in the list of tuples.\n\n    \"\"\"\n    for response in responses:\n        if response[1] is not None:\n            return response[1]\n    return default\n\n\nclass BaseEventHooks:\n    def emit(self, event_name, **kwargs):\n        \"\"\"Call all handlers subscribed to an event.\n\n        :type event_name: str\n        :param event_name: The name of the event to emit.\n\n        :type **kwargs: dict\n        :param **kwargs: Arbitrary kwargs to pass through to the\n            subscribed handlers.  The ``event_name`` will be injected\n            into the kwargs so it's not necessary to add this to **kwargs.\n\n        :rtype: list of tuples\n        :return: A list of ``(handler_func, handler_func_return_value)``\n\n        \"\"\"\n        return []\n\n    def register(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        \"\"\"Register an event handler for a given event.\n\n        If a ``unique_id`` is given, the handler will not be registered\n        if a handler with the ``unique_id`` has already been registered.\n\n        Handlers are called in the order they have been registered.\n        Note handlers can also be registered with ``register_first()``\n        and ``register_last()``.  All handlers registered with\n        ``register_first()`` are called before handlers registered\n        with ``register()`` which are called before handlers registered\n        with ``register_last()``.\n\n        \"\"\"\n        self._verify_and_register(\n            event_name,\n            handler,\n            unique_id,\n            register_method=self._register,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def register_first(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        \"\"\"Register an event handler to be called first for an event.\n\n        All event handlers registered with ``register_first()`` will\n        be called before handlers registered with ``register()`` and\n        ``register_last()``.\n\n        \"\"\"\n        self._verify_and_register(\n            event_name,\n            handler,\n            unique_id,\n            register_method=self._register_first,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def register_last(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        \"\"\"Register an event handler to be called last for an event.\n\n        All event handlers registered with ``register_last()`` will be called\n        after handlers registered with ``register_first()`` and ``register()``.\n\n        \"\"\"\n        self._verify_and_register(\n            event_name,\n            handler,\n            unique_id,\n            register_method=self._register_last,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def _verify_and_register(\n        self,\n        event_name,\n        handler,\n        unique_id,\n        register_method,\n        unique_id_uses_count,\n    ):\n        self._verify_is_callable(handler)\n        self._verify_accept_kwargs(handler)\n        register_method(event_name, handler, unique_id, unique_id_uses_count)\n\n    def unregister(\n        self,\n        event_name,\n        handler=None,\n        unique_id=None,\n        unique_id_uses_count=False,\n    ):\n        \"\"\"Unregister an event handler for a given event.\n\n        If no ``unique_id`` was given during registration, then the\n        first instance of the event handler is removed (if the event\n        handler has been registered multiple times).\n\n        \"\"\"\n        pass\n\n    def _verify_is_callable(self, func):\n        if not callable(func):\n            raise ValueError(\"Event handler %s must be callable.\" % func)\n\n    def _verify_accept_kwargs(self, func):\n        \"\"\"Verifies a callable accepts kwargs\n\n        :type func: callable\n        :param func: A callable object.\n\n        :returns: True, if ``func`` accepts kwargs, otherwise False.\n\n        \"\"\"\n        try:\n            if not accepts_kwargs(func):\n                raise ValueError(\n                    f\"Event handler {func} must accept keyword \"\n                    f\"arguments (**kwargs)\"\n                )\n        except TypeError:\n            return False\n\n\nclass HierarchicalEmitter(BaseEventHooks):\n    def __init__(self):\n        # We keep a reference to the handlers for quick\n        # read only access (we never modify self._handlers).\n        # A cache of event name to handler list.\n        self._lookup_cache = {}\n        self._handlers = _PrefixTrie()\n        # This is used to ensure that unique_id's are only\n        # registered once.\n        self._unique_id_handlers = {}\n\n    def _emit(self, event_name, kwargs, stop_on_response=False):\n        \"\"\"\n        Emit an event with optional keyword arguments.\n\n        :type event_name: string\n        :param event_name: Name of the event\n        :type kwargs: dict\n        :param kwargs: Arguments to be passed to the handler functions.\n        :type stop_on_response: boolean\n        :param stop_on_response: Whether to stop on the first non-None\n                                response. If False, then all handlers\n                                will be called. This is especially useful\n                                to handlers which mutate data and then\n                                want to stop propagation of the event.\n        :rtype: list\n        :return: List of (handler, response) tuples from all processed\n                 handlers.\n        \"\"\"\n        responses = []\n        # Invoke the event handlers from most specific\n        # to least specific, each time stripping off a dot.\n        handlers_to_call = self._lookup_cache.get(event_name)\n        if handlers_to_call is None:\n            handlers_to_call = self._handlers.prefix_search(event_name)\n            self._lookup_cache[event_name] = handlers_to_call\n        elif not handlers_to_call:\n            # Short circuit and return an empty response is we have\n            # no handlers to call.  This is the common case where\n            # for the majority of signals, nothing is listening.\n            return []\n        kwargs['event_name'] = event_name\n        responses = []\n        for handler in handlers_to_call:\n            logger.debug('Event %s: calling handler %s', event_name, handler)\n            response = handler(**kwargs)\n            responses.append((handler, response))\n            if stop_on_response and response is not None:\n                return responses\n        return responses\n\n    def emit(self, event_name, **kwargs):\n        \"\"\"\n        Emit an event by name with arguments passed as keyword args.\n\n            >>> responses = emitter.emit(\n            ...     'my-event.service.operation', arg1='one', arg2='two')\n\n        :rtype: list\n        :return: List of (handler, response) tuples from all processed\n                 handlers.\n        \"\"\"\n        return self._emit(event_name, kwargs)\n\n    def emit_until_response(self, event_name, **kwargs):\n        \"\"\"\n        Emit an event by name with arguments passed as keyword args,\n        until the first non-``None`` response is received. This\n        method prevents subsequent handlers from being invoked.\n\n            >>> handler, response = emitter.emit_until_response(\n                'my-event.service.operation', arg1='one', arg2='two')\n\n        :rtype: tuple\n        :return: The first (handler, response) tuple where the response\n                 is not ``None``, otherwise (``None``, ``None``).\n        \"\"\"\n        responses = self._emit(event_name, kwargs, stop_on_response=True)\n        if responses:\n            return responses[-1]\n        else:\n            return (None, None)\n\n    def _register(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        self._register_section(\n            event_name,\n            handler,\n            unique_id,\n            unique_id_uses_count,\n            section=_MIDDLE,\n        )\n\n    def _register_first(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        self._register_section(\n            event_name,\n            handler,\n            unique_id,\n            unique_id_uses_count,\n            section=_FIRST,\n        )\n\n    def _register_last(\n        self, event_name, handler, unique_id, unique_id_uses_count=False\n    ):\n        self._register_section(\n            event_name, handler, unique_id, unique_id_uses_count, section=_LAST\n        )\n\n    def _register_section(\n        self, event_name, handler, unique_id, unique_id_uses_count, section\n    ):\n        if unique_id is not None:\n            if unique_id in self._unique_id_handlers:\n                # We've already registered a handler using this unique_id\n                # so we don't need to register it again.\n                count = self._unique_id_handlers[unique_id].get('count', None)\n                if unique_id_uses_count:\n                    if not count:\n                        raise ValueError(\n                            \"Initial registration of  unique id %s was \"\n                            \"specified to use a counter. Subsequent register \"\n                            \"calls to unique id must specify use of a counter \"\n                            \"as well.\" % unique_id\n                        )\n                    else:\n                        self._unique_id_handlers[unique_id]['count'] += 1\n                else:\n                    if count:\n                        raise ValueError(\n                            \"Initial registration of unique id %s was \"\n                            \"specified to not use a counter. Subsequent \"\n                            \"register calls to unique id must specify not to \"\n                            \"use a counter as well.\" % unique_id\n                        )\n                return\n            else:\n                # Note that the trie knows nothing about the unique\n                # id.  We track uniqueness in this class via the\n                # _unique_id_handlers.\n                self._handlers.append_item(\n                    event_name, handler, section=section\n                )\n                unique_id_handler_item = {'handler': handler}\n                if unique_id_uses_count:\n                    unique_id_handler_item['count'] = 1\n                self._unique_id_handlers[unique_id] = unique_id_handler_item\n        else:\n            self._handlers.append_item(event_name, handler, section=section)\n        # Super simple caching strategy for now, if we change the registrations\n        # clear the cache.  This has the opportunity for smarter invalidations.\n        self._lookup_cache = {}\n\n    def unregister(\n        self,\n        event_name,\n        handler=None,\n        unique_id=None,\n        unique_id_uses_count=False,\n    ):\n        if unique_id is not None:\n            try:\n                count = self._unique_id_handlers[unique_id].get('count', None)\n            except KeyError:\n                # There's no handler matching that unique_id so we have\n                # nothing to unregister.\n                return\n            if unique_id_uses_count:\n                if count is None:\n                    raise ValueError(\n                        \"Initial registration of unique id %s was specified to \"\n                        \"use a counter. Subsequent unregister calls to unique \"\n                        \"id must specify use of a counter as well.\" % unique_id\n                    )\n                elif count == 1:\n                    handler = self._unique_id_handlers.pop(unique_id)[\n                        'handler'\n                    ]\n                else:\n                    self._unique_id_handlers[unique_id]['count'] -= 1\n                    return\n            else:\n                if count:\n                    raise ValueError(\n                        \"Initial registration of unique id %s was specified \"\n                        \"to not use a counter. Subsequent unregister calls \"\n                        \"to unique id must specify not to use a counter as \"\n                        \"well.\" % unique_id\n                    )\n                handler = self._unique_id_handlers.pop(unique_id)['handler']\n        try:\n            self._handlers.remove_item(event_name, handler)\n            self._lookup_cache = {}\n        except ValueError:\n            pass\n\n    def __copy__(self):\n        new_instance = self.__class__()\n        new_state = self.__dict__.copy()\n        new_state['_handlers'] = copy.copy(self._handlers)\n        new_state['_unique_id_handlers'] = copy.copy(self._unique_id_handlers)\n        new_instance.__dict__ = new_state\n        return new_instance\n\n\nclass EventAliaser(BaseEventHooks):\n    def __init__(self, event_emitter, event_aliases=None):\n        self._event_aliases = event_aliases\n        if event_aliases is None:\n            self._event_aliases = EVENT_ALIASES\n        self._alias_name_cache = {}\n        self._emitter = event_emitter\n\n    def emit(self, event_name, **kwargs):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.emit(aliased_event_name, **kwargs)\n\n    def emit_until_response(self, event_name, **kwargs):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.emit_until_response(aliased_event_name, **kwargs)\n\n    def register(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.register(\n            aliased_event_name, handler, unique_id, unique_id_uses_count\n        )\n\n    def register_first(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.register_first(\n            aliased_event_name, handler, unique_id, unique_id_uses_count\n        )\n\n    def register_last(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.register_last(\n            aliased_event_name, handler, unique_id, unique_id_uses_count\n        )\n\n    def unregister(\n        self,\n        event_name,\n        handler=None,\n        unique_id=None,\n        unique_id_uses_count=False,\n    ):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.unregister(\n            aliased_event_name, handler, unique_id, unique_id_uses_count\n        )\n\n    def _alias_event_name(self, event_name):\n        if event_name in self._alias_name_cache:\n            return self._alias_name_cache[event_name]\n\n        for old_part, new_part in self._event_aliases.items():\n            # We can't simply do a string replace for everything, otherwise we\n            # might end up translating substrings that we never intended to\n            # translate. When there aren't any dots in the old event name\n            # part, then we can quickly replace the item in the list if it's\n            # there.\n            event_parts = event_name.split('.')\n            if '.' not in old_part:\n                try:\n                    # Theoretically a given event name could have the same part\n                    # repeated, but in practice this doesn't happen\n                    event_parts[event_parts.index(old_part)] = new_part\n                except ValueError:\n                    continue\n\n            # If there's dots in the name, it gets more complicated. Now we\n            # have to replace multiple sections of the original event.\n            elif old_part in event_name:\n                old_parts = old_part.split('.')\n                self._replace_subsection(event_parts, old_parts, new_part)\n            else:\n                continue\n\n            new_name = '.'.join(event_parts)\n            logger.debug(\n                f\"Changing event name from {event_name} to {new_name}\"\n            )\n            self._alias_name_cache[event_name] = new_name\n            return new_name\n\n        self._alias_name_cache[event_name] = event_name\n        return event_name\n\n    def _replace_subsection(self, sections, old_parts, new_part):\n        for i in range(len(sections)):\n            if (\n                sections[i] == old_parts[0]\n                and sections[i : i + len(old_parts)] == old_parts\n            ):\n                sections[i : i + len(old_parts)] = [new_part]\n                return\n\n    def __copy__(self):\n        return self.__class__(\n            copy.copy(self._emitter), copy.copy(self._event_aliases)\n        )\n\n\nclass _PrefixTrie:\n    \"\"\"Specialized prefix trie that handles wildcards.\n\n    The prefixes in this case are based on dot separated\n    names so 'foo.bar.baz' is::\n\n        foo -> bar -> baz\n\n    Wildcard support just means that having a key such as 'foo.bar.*.baz' will\n    be matched with a call to ``get_items(key='foo.bar.ANYTHING.baz')``.\n\n    You can think of this prefix trie as the equivalent as defaultdict(list),\n    except that it can do prefix searches:\n\n        foo.bar.baz -> A\n        foo.bar -> B\n        foo -> C\n\n    Calling ``get_items('foo.bar.baz')`` will return [A + B + C], from\n    most specific to least specific.\n\n    \"\"\"\n\n    def __init__(self):\n        # Each dictionary can be though of as a node, where a node\n        # has values associated with the node, and children is a link\n        # to more nodes.  So 'foo.bar' would have a 'foo' node with\n        # a 'bar' node as a child of foo.\n        # {'foo': {'children': {'bar': {...}}}}.\n        self._root = {'chunk': None, 'children': {}, 'values': None}\n\n    def append_item(self, key, value, section=_MIDDLE):\n        \"\"\"Add an item to a key.\n\n        If a value is already associated with that key, the new\n        value is appended to the list for the key.\n        \"\"\"\n        key_parts = key.split('.')\n        current = self._root\n        for part in key_parts:\n            if part not in current['children']:\n                new_child = {'chunk': part, 'values': None, 'children': {}}\n                current['children'][part] = new_child\n                current = new_child\n            else:\n                current = current['children'][part]\n        if current['values'] is None:\n            current['values'] = NodeList([], [], [])\n        current['values'][section].append(value)\n\n    def prefix_search(self, key):\n        \"\"\"Collect all items that are prefixes of key.\n\n        Prefix in this case are delineated by '.' characters so\n        'foo.bar.baz' is a 3 chunk sequence of 3 \"prefixes\" (\n        \"foo\", \"bar\", and \"baz\").\n\n        \"\"\"\n        collected = deque()\n        key_parts = key.split('.')\n        current = self._root\n        self._get_items(current, key_parts, collected, 0)\n        return collected\n\n    def _get_items(self, starting_node, key_parts, collected, starting_index):\n        stack = [(starting_node, starting_index)]\n        key_parts_len = len(key_parts)\n        # Traverse down the nodes, where at each level we add the\n        # next part from key_parts as well as the wildcard element '*'.\n        # This means for each node we see we potentially add two more\n        # elements to our stack.\n        while stack:\n            current_node, index = stack.pop()\n            if current_node['values']:\n                # We're using extendleft because we want\n                # the values associated with the node furthest\n                # from the root to come before nodes closer\n                # to the root.  extendleft() also adds its items\n                # in right-left order so .extendleft([1, 2, 3])\n                # will result in final_list = [3, 2, 1], which is\n                # why we reverse the lists.\n                node_list = current_node['values']\n                complete_order = (\n                    node_list.first + node_list.middle + node_list.last\n                )\n                collected.extendleft(reversed(complete_order))\n            if not index == key_parts_len:\n                children = current_node['children']\n                directs = children.get(key_parts[index])\n                wildcard = children.get('*')\n                next_index = index + 1\n                if wildcard is not None:\n                    stack.append((wildcard, next_index))\n                if directs is not None:\n                    stack.append((directs, next_index))\n\n    def remove_item(self, key, value):\n        \"\"\"Remove an item associated with a key.\n\n        If the value is not associated with the key a ``ValueError``\n        will be raised.  If the key does not exist in the trie, a\n        ``ValueError`` will be raised.\n\n        \"\"\"\n        key_parts = key.split('.')\n        current = self._root\n        self._remove_item(current, key_parts, value, index=0)\n\n    def _remove_item(self, current_node, key_parts, value, index):\n        if current_node is None:\n            return\n        elif index < len(key_parts):\n            next_node = current_node['children'].get(key_parts[index])\n            if next_node is not None:\n                self._remove_item(next_node, key_parts, value, index + 1)\n                if index == len(key_parts) - 1:\n                    node_list = next_node['values']\n                    if value in node_list.first:\n                        node_list.first.remove(value)\n                    elif value in node_list.middle:\n                        node_list.middle.remove(value)\n                    elif value in node_list.last:\n                        node_list.last.remove(value)\n                if not next_node['children'] and not next_node['values']:\n                    # Then this is a leaf node with no values so\n                    # we can just delete this link from the parent node.\n                    # This makes subsequent search faster in the case\n                    # where a key does not exist.\n                    del current_node['children'][key_parts[index]]\n            else:\n                raise ValueError(f\"key is not in trie: {'.'.join(key_parts)}\")\n\n    def __copy__(self):\n        # The fact that we're using a nested dict under the covers\n        # is an implementation detail, and the user shouldn't have\n        # to know that they'd normally need a deepcopy so we expose\n        # __copy__ instead of __deepcopy__.\n        new_copy = self.__class__()\n        copied_attrs = self._recursive_copy(self.__dict__)\n        new_copy.__dict__ = copied_attrs\n        return new_copy\n\n    def _recursive_copy(self, node):\n        # We can't use copy.deepcopy because we actually only want to copy\n        # the structure of the trie, not the handlers themselves.\n        # Each node has a chunk, children, and values.\n        copied_node = {}\n        for key, value in node.items():\n            if isinstance(value, NodeList):\n                copied_node[key] = copy.copy(value)\n            elif isinstance(value, dict):\n                copied_node[key] = self._recursive_copy(value)\n            else:\n                copied_node[key] = value\n        return copied_node\n", "botocore/validate.py": "\"\"\"User input parameter validation.\n\nThis module handles user input parameter validation\nagainst a provided input model.\n\nNote that the objects in this module do *not* mutate any\narguments.  No type version happens here.  It is up to another\nlayer to properly convert arguments to any required types.\n\nValidation Errors\n-----------------\n\n\n\"\"\"\n\nimport decimal\nimport json\nfrom datetime import datetime\n\nfrom botocore.exceptions import ParamValidationError\nfrom botocore.utils import is_json_value_header, parse_to_aware_datetime\n\n\ndef validate_parameters(params, shape):\n    \"\"\"Validates input parameters against a schema.\n\n    This is a convenience function that validates parameters against a schema.\n    You can also instantiate and use the ParamValidator class directly if you\n    want more control.\n\n    If there are any validation errors then a ParamValidationError\n    will be raised.  If there are no validation errors than no exception\n    is raised and a value of None is returned.\n\n    :param params: The user provided input parameters.\n\n    :type shape: botocore.model.Shape\n    :param shape: The schema which the input parameters should\n        adhere to.\n\n    :raise: ParamValidationError\n\n    \"\"\"\n    validator = ParamValidator()\n    report = validator.validate(params, shape)\n    if report.has_errors():\n        raise ParamValidationError(report=report.generate_report())\n\n\ndef type_check(valid_types):\n    def _create_type_check_guard(func):\n        def _on_passes_type_check(self, param, shape, errors, name):\n            if _type_check(param, errors, name):\n                return func(self, param, shape, errors, name)\n\n        def _type_check(param, errors, name):\n            if not isinstance(param, valid_types):\n                valid_type_names = [str(t) for t in valid_types]\n                errors.report(\n                    name,\n                    'invalid type',\n                    param=param,\n                    valid_types=valid_type_names,\n                )\n                return False\n            return True\n\n        return _on_passes_type_check\n\n    return _create_type_check_guard\n\n\ndef range_check(name, value, shape, error_type, errors):\n    failed = False\n    min_allowed = float('-inf')\n    if 'min' in shape.metadata:\n        min_allowed = shape.metadata['min']\n        if value < min_allowed:\n            failed = True\n    elif hasattr(shape, 'serialization'):\n        # Members that can be bound to the host have an implicit min of 1\n        if shape.serialization.get('hostLabel'):\n            min_allowed = 1\n            if value < min_allowed:\n                failed = True\n    if failed:\n        errors.report(name, error_type, param=value, min_allowed=min_allowed)\n\n\nclass ValidationErrors:\n    def __init__(self):\n        self._errors = []\n\n    def has_errors(self):\n        if self._errors:\n            return True\n        return False\n\n    def generate_report(self):\n        error_messages = []\n        for error in self._errors:\n            error_messages.append(self._format_error(error))\n        return '\\n'.join(error_messages)\n\n    def _format_error(self, error):\n        error_type, name, additional = error\n        name = self._get_name(name)\n        if error_type == 'missing required field':\n            return (\n                f\"Missing required parameter in {name}: \"\n                f\"\\\"{additional['required_name']}\\\"\"\n            )\n        elif error_type == 'unknown field':\n            unknown_param = additional['unknown_param']\n            valid_names = ', '.join(additional['valid_names'])\n            return (\n                f'Unknown parameter in {name}: \"{unknown_param}\", '\n                f'must be one of: {valid_names}'\n            )\n        elif error_type == 'invalid type':\n            param = additional['param']\n            param_type = type(param)\n            valid_types = ', '.join(additional['valid_types'])\n            return (\n                f'Invalid type for parameter {name}, value: {param}, '\n                f'type: {param_type}, valid types: {valid_types}'\n            )\n        elif error_type == 'invalid range':\n            param = additional['param']\n            min_allowed = additional['min_allowed']\n            return (\n                f'Invalid value for parameter {name}, value: {param}, '\n                f'valid min value: {min_allowed}'\n            )\n        elif error_type == 'invalid length':\n            param = additional['param']\n            min_allowed = additional['min_allowed']\n            return (\n                f'Invalid length for parameter {name}, value: {param}, '\n                f'valid min length: {min_allowed}'\n            )\n        elif error_type == 'unable to encode to json':\n            return 'Invalid parameter {} must be json serializable: {}'.format(\n                name,\n                additional['type_error'],\n            )\n        elif error_type == 'invalid type for document':\n            param = additional['param']\n            param_type = type(param)\n            valid_types = ', '.join(additional['valid_types'])\n            return (\n                f'Invalid type for document parameter {name}, value: {param}, '\n                f'type: {param_type}, valid types: {valid_types}'\n            )\n        elif error_type == 'more than one input':\n            members = ', '.join(additional['members'])\n            return (\n                f'Invalid number of parameters set for tagged union structure '\n                f'{name}. Can only set one of the following keys: '\n                f'{members}.'\n            )\n        elif error_type == 'empty input':\n            members = ', '.join(additional['members'])\n            return (\n                f'Must set one of the following keys for tagged union'\n                f'structure {name}: {members}.'\n            )\n\n    def _get_name(self, name):\n        if not name:\n            return 'input'\n        elif name.startswith('.'):\n            return name[1:]\n        else:\n            return name\n\n    def report(self, name, reason, **kwargs):\n        self._errors.append((reason, name, kwargs))\n\n\nclass ParamValidator:\n    \"\"\"Validates parameters against a shape model.\"\"\"\n\n    def validate(self, params, shape):\n        \"\"\"Validate parameters against a shape model.\n\n        This method will validate the parameters against a provided shape model.\n        All errors will be collected before returning to the caller.  This means\n        that this method will not stop at the first error, it will return all\n        possible errors.\n\n        :param params: User provided dict of parameters\n        :param shape: A shape model describing the expected input.\n\n        :return: A list of errors.\n\n        \"\"\"\n        errors = ValidationErrors()\n        self._validate(params, shape, errors, name='')\n        return errors\n\n    def _check_special_validation_cases(self, shape):\n        if is_json_value_header(shape):\n            return self._validate_jsonvalue_string\n        if shape.type_name == 'structure' and shape.is_document_type:\n            return self._validate_document\n\n    def _validate(self, params, shape, errors, name):\n        special_validator = self._check_special_validation_cases(shape)\n        if special_validator:\n            special_validator(params, shape, errors, name)\n        else:\n            getattr(self, '_validate_%s' % shape.type_name)(\n                params, shape, errors, name\n            )\n\n    def _validate_jsonvalue_string(self, params, shape, errors, name):\n        # Check to see if a value marked as a jsonvalue can be dumped to\n        # a json string.\n        try:\n            json.dumps(params)\n        except (ValueError, TypeError) as e:\n            errors.report(name, 'unable to encode to json', type_error=e)\n\n    def _validate_document(self, params, shape, errors, name):\n        if params is None:\n            return\n\n        if isinstance(params, dict):\n            for key in params:\n                self._validate_document(params[key], shape, errors, key)\n        elif isinstance(params, list):\n            for index, entity in enumerate(params):\n                self._validate_document(\n                    entity, shape, errors, '%s[%d]' % (name, index)\n                )\n        elif not isinstance(params, ((str,), int, bool, float)):\n            valid_types = (str, int, bool, float, list, dict)\n            valid_type_names = [str(t) for t in valid_types]\n            errors.report(\n                name,\n                'invalid type for document',\n                param=params,\n                param_type=type(params),\n                valid_types=valid_type_names,\n            )\n\n    @type_check(valid_types=(dict,))\n    def _validate_structure(self, params, shape, errors, name):\n        if shape.is_tagged_union:\n            if len(params) == 0:\n                errors.report(name, 'empty input', members=shape.members)\n            elif len(params) > 1:\n                errors.report(\n                    name, 'more than one input', members=shape.members\n                )\n\n        # Validate required fields.\n        for required_member in shape.metadata.get('required', []):\n            if required_member not in params:\n                errors.report(\n                    name,\n                    'missing required field',\n                    required_name=required_member,\n                    user_params=params,\n                )\n        members = shape.members\n        known_params = []\n        # Validate known params.\n        for param in params:\n            if param not in members:\n                errors.report(\n                    name,\n                    'unknown field',\n                    unknown_param=param,\n                    valid_names=list(members),\n                )\n            else:\n                known_params.append(param)\n        # Validate structure members.\n        for param in known_params:\n            self._validate(\n                params[param],\n                shape.members[param],\n                errors,\n                f'{name}.{param}',\n            )\n\n    @type_check(valid_types=(str,))\n    def _validate_string(self, param, shape, errors, name):\n        # Validate range.  For a string, the min/max constraints\n        # are of the string length.\n        # Looks like:\n        # \"WorkflowId\":{\n        #   \"type\":\"string\",\n        #   \"min\":1,\n        #   \"max\":256\n        #  }\n        range_check(name, len(param), shape, 'invalid length', errors)\n\n    @type_check(valid_types=(list, tuple))\n    def _validate_list(self, param, shape, errors, name):\n        member_shape = shape.member\n        range_check(name, len(param), shape, 'invalid length', errors)\n        for i, item in enumerate(param):\n            self._validate(item, member_shape, errors, f'{name}[{i}]')\n\n    @type_check(valid_types=(dict,))\n    def _validate_map(self, param, shape, errors, name):\n        key_shape = shape.key\n        value_shape = shape.value\n        for key, value in param.items():\n            self._validate(key, key_shape, errors, f\"{name} (key: {key})\")\n            self._validate(value, value_shape, errors, f'{name}.{key}')\n\n    @type_check(valid_types=(int,))\n    def _validate_integer(self, param, shape, errors, name):\n        range_check(name, param, shape, 'invalid range', errors)\n\n    def _validate_blob(self, param, shape, errors, name):\n        if isinstance(param, (bytes, bytearray, str)):\n            return\n        elif hasattr(param, 'read'):\n            # File like objects are also allowed for blob types.\n            return\n        else:\n            errors.report(\n                name,\n                'invalid type',\n                param=param,\n                valid_types=[str(bytes), str(bytearray), 'file-like object'],\n            )\n\n    @type_check(valid_types=(bool,))\n    def _validate_boolean(self, param, shape, errors, name):\n        pass\n\n    @type_check(valid_types=(float, decimal.Decimal) + (int,))\n    def _validate_double(self, param, shape, errors, name):\n        range_check(name, param, shape, 'invalid range', errors)\n\n    _validate_float = _validate_double\n\n    @type_check(valid_types=(int,))\n    def _validate_long(self, param, shape, errors, name):\n        range_check(name, param, shape, 'invalid range', errors)\n\n    def _validate_timestamp(self, param, shape, errors, name):\n        # We don't use @type_check because datetimes are a bit\n        # more flexible.  You can either provide a datetime\n        # object, or a string that parses to a datetime.\n        is_valid_type = self._type_check_datetime(param)\n        if not is_valid_type:\n            valid_type_names = [str(datetime), 'timestamp-string']\n            errors.report(\n                name, 'invalid type', param=param, valid_types=valid_type_names\n            )\n\n    def _type_check_datetime(self, value):\n        try:\n            parse_to_aware_datetime(value)\n            return True\n        except (TypeError, ValueError, AttributeError):\n            # Yes, dateutil can sometimes raise an AttributeError\n            # when parsing timestamps.\n            return False\n\n\nclass ParamValidationDecorator:\n    def __init__(self, param_validator, serializer):\n        self._param_validator = param_validator\n        self._serializer = serializer\n\n    def serialize_to_request(self, parameters, operation_model):\n        input_shape = operation_model.input_shape\n        if input_shape is not None:\n            report = self._param_validator.validate(\n                parameters, operation_model.input_shape\n            )\n            if report.has_errors():\n                raise ParamValidationError(report=report.generate_report())\n        return self._serializer.serialize_to_request(\n            parameters, operation_model\n        )\n", "botocore/loaders.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Module for loading various model files.\n\nThis module provides the classes that are used to load models used\nby botocore.  This can include:\n\n    * Service models (e.g. the model for EC2, S3, DynamoDB, etc.)\n    * Service model extras which customize the service models\n    * Other models associated with a service (pagination, waiters)\n    * Non service-specific config (Endpoint data, retry config)\n\nLoading a module is broken down into several steps:\n\n    * Determining the path to load\n    * Search the data_path for files to load\n    * The mechanics of loading the file\n    * Searching for extras and applying them to the loaded file\n\nThe last item is used so that other faster loading mechanism\nbesides the default JSON loader can be used.\n\nThe Search Path\n===============\n\nSimilar to how the PATH environment variable is to finding executables\nand the PYTHONPATH environment variable is to finding python modules\nto import, the botocore loaders have the concept of a data path exposed\nthrough AWS_DATA_PATH.\n\nThis enables end users to provide additional search paths where we\nwill attempt to load models outside of the models we ship with\nbotocore.  When you create a ``Loader``, there are two paths\nautomatically added to the model search path:\n\n    * <botocore root>/data/\n    * ~/.aws/models\n\nThe first value is the path where all the model files shipped with\nbotocore are located.\n\nThe second path is so that users can just drop new model files in\n``~/.aws/models`` without having to mess around with the AWS_DATA_PATH.\n\nThe AWS_DATA_PATH using the platform specific path separator to\nseparate entries (typically ``:`` on linux and ``;`` on windows).\n\n\nDirectory Layout\n================\n\nThe Loader expects a particular directory layout.  In order for any\ndirectory specified in AWS_DATA_PATH to be considered, it must have\nthis structure for service models::\n\n    <root>\n      |\n      |-- servicename1\n      |   |-- 2012-10-25\n      |       |-- service-2.json\n      |-- ec2\n      |   |-- 2014-01-01\n      |   |   |-- paginators-1.json\n      |   |   |-- service-2.json\n      |   |   |-- waiters-2.json\n      |   |-- 2015-03-01\n      |       |-- paginators-1.json\n      |       |-- service-2.json\n      |       |-- waiters-2.json\n      |       |-- service-2.sdk-extras.json\n\n\nThat is:\n\n    * The root directory contains sub directories that are the name\n      of the services.\n    * Within each service directory, there's a sub directory for each\n      available API version.\n    * Within each API version, there are model specific files, including\n      (but not limited to): service-2.json, waiters-2.json, paginators-1.json\n\nThe ``-1`` and ``-2`` suffix at the end of the model files denote which version\nschema is used within the model.  Even though this information is available in\nthe ``version`` key within the model, this version is also part of the filename\nso that code does not need to load the JSON model in order to determine which\nversion to use.\n\nThe ``sdk-extras`` and similar files represent extra data that needs to be\napplied to the model after it is loaded. Data in these files might represent\ninformation that doesn't quite fit in the original models, but is still needed\nfor the sdk. For instance, additional operation parameters might be added here\nwhich don't represent the actual service api.\n\"\"\"\nimport logging\nimport os\n\nfrom botocore import BOTOCORE_ROOT\nfrom botocore.compat import HAS_GZIP, OrderedDict, json\nfrom botocore.exceptions import DataNotFoundError, UnknownServiceError\nfrom botocore.utils import deep_merge\n\n_JSON_OPEN_METHODS = {\n    '.json': open,\n}\n\n\nif HAS_GZIP:\n    from gzip import open as gzip_open\n\n    _JSON_OPEN_METHODS['.json.gz'] = gzip_open\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef instance_cache(func):\n    \"\"\"Cache the result of a method on a per instance basis.\n\n    This is not a general purpose caching decorator.  In order\n    for this to be used, it must be used on methods on an\n    instance, and that instance *must* provide a\n    ``self._cache`` dictionary.\n\n    \"\"\"\n\n    def _wrapper(self, *args, **kwargs):\n        key = (func.__name__,) + args\n        for pair in sorted(kwargs.items()):\n            key += pair\n        if key in self._cache:\n            return self._cache[key]\n        data = func(self, *args, **kwargs)\n        self._cache[key] = data\n        return data\n\n    return _wrapper\n\n\nclass JSONFileLoader:\n    \"\"\"Loader JSON files.\n\n    This class can load the default format of models, which is a JSON file.\n\n    \"\"\"\n\n    def exists(self, file_path):\n        \"\"\"Checks if the file exists.\n\n        :type file_path: str\n        :param file_path: The full path to the file to load without\n            the '.json' extension.\n\n        :return: True if file path exists, False otherwise.\n\n        \"\"\"\n        for ext in _JSON_OPEN_METHODS:\n            if os.path.isfile(file_path + ext):\n                return True\n        return False\n\n    def _load_file(self, full_path, open_method):\n        if not os.path.isfile(full_path):\n            return\n\n        # By default the file will be opened with locale encoding on Python 3.\n        # We specify \"utf8\" here to ensure the correct behavior.\n        with open_method(full_path, 'rb') as fp:\n            payload = fp.read().decode('utf-8')\n\n        logger.debug(\"Loading JSON file: %s\", full_path)\n        return json.loads(payload, object_pairs_hook=OrderedDict)\n\n    def load_file(self, file_path):\n        \"\"\"Attempt to load the file path.\n\n        :type file_path: str\n        :param file_path: The full path to the file to load without\n            the '.json' extension.\n\n        :return: The loaded data if it exists, otherwise None.\n\n        \"\"\"\n        for ext, open_method in _JSON_OPEN_METHODS.items():\n            data = self._load_file(file_path + ext, open_method)\n            if data is not None:\n                return data\n        return None\n\n\ndef create_loader(search_path_string=None):\n    \"\"\"Create a Loader class.\n\n    This factory function creates a loader given a search string path.\n\n    :type search_string_path: str\n    :param search_string_path: The AWS_DATA_PATH value.  A string\n        of data path values separated by the ``os.path.pathsep`` value,\n        which is typically ``:`` on POSIX platforms and ``;`` on\n        windows.\n\n    :return: A ``Loader`` instance.\n\n    \"\"\"\n    if search_path_string is None:\n        return Loader()\n    paths = []\n    extra_paths = search_path_string.split(os.pathsep)\n    for path in extra_paths:\n        path = os.path.expanduser(os.path.expandvars(path))\n        paths.append(path)\n    return Loader(extra_search_paths=paths)\n\n\nclass Loader:\n    \"\"\"Find and load data models.\n\n    This class will handle searching for and loading data models.\n\n    The main method used here is ``load_service_model``, which is a\n    convenience method over ``load_data`` and ``determine_latest_version``.\n\n    \"\"\"\n\n    FILE_LOADER_CLASS = JSONFileLoader\n    # The included models in botocore/data/ that we ship with botocore.\n    BUILTIN_DATA_PATH = os.path.join(BOTOCORE_ROOT, 'data')\n    # For convenience we automatically add ~/.aws/models to the data path.\n    CUSTOMER_DATA_PATH = os.path.join(\n        os.path.expanduser('~'), '.aws', 'models'\n    )\n    BUILTIN_EXTRAS_TYPES = ['sdk']\n\n    def __init__(\n        self,\n        extra_search_paths=None,\n        file_loader=None,\n        cache=None,\n        include_default_search_paths=True,\n        include_default_extras=True,\n    ):\n        self._cache = {}\n        if file_loader is None:\n            file_loader = self.FILE_LOADER_CLASS()\n        self.file_loader = file_loader\n        if extra_search_paths is not None:\n            self._search_paths = extra_search_paths\n        else:\n            self._search_paths = []\n        if include_default_search_paths:\n            self._search_paths.extend(\n                [self.CUSTOMER_DATA_PATH, self.BUILTIN_DATA_PATH]\n            )\n\n        self._extras_types = []\n        if include_default_extras:\n            self._extras_types.extend(self.BUILTIN_EXTRAS_TYPES)\n\n        self._extras_processor = ExtrasProcessor()\n\n    @property\n    def search_paths(self):\n        return self._search_paths\n\n    @property\n    def extras_types(self):\n        return self._extras_types\n\n    @instance_cache\n    def list_available_services(self, type_name):\n        \"\"\"List all known services.\n\n        This will traverse the search path and look for all known\n        services.\n\n        :type type_name: str\n        :param type_name: The type of the service (service-2,\n            paginators-1, waiters-2, etc).  This is needed because\n            the list of available services depends on the service\n            type.  For example, the latest API version available for\n            a resource-1.json file may not be the latest API version\n            available for a services-2.json file.\n\n        :return: A list of all services.  The list of services will\n            be sorted.\n\n        \"\"\"\n        services = set()\n        for possible_path in self._potential_locations():\n            # Any directory in the search path is potentially a service.\n            # We'll collect any initial list of potential services,\n            # but we'll then need to further process these directories\n            # by searching for the corresponding type_name in each\n            # potential directory.\n            possible_services = [\n                d\n                for d in os.listdir(possible_path)\n                if os.path.isdir(os.path.join(possible_path, d))\n            ]\n            for service_name in possible_services:\n                full_dirname = os.path.join(possible_path, service_name)\n                api_versions = os.listdir(full_dirname)\n                for api_version in api_versions:\n                    full_load_path = os.path.join(\n                        full_dirname, api_version, type_name\n                    )\n                    if self.file_loader.exists(full_load_path):\n                        services.add(service_name)\n                        break\n        return sorted(services)\n\n    @instance_cache\n    def determine_latest_version(self, service_name, type_name):\n        \"\"\"Find the latest API version available for a service.\n\n        :type service_name: str\n        :param service_name: The name of the service.\n\n        :type type_name: str\n        :param type_name: The type of the service (service-2,\n            paginators-1, waiters-2, etc).  This is needed because\n            the latest API version available can depend on the service\n            type.  For example, the latest API version available for\n            a resource-1.json file may not be the latest API version\n            available for a services-2.json file.\n\n        :rtype: str\n        :return: The latest API version.  If the service does not exist\n            or does not have any available API data, then a\n            ``DataNotFoundError`` exception will be raised.\n\n        \"\"\"\n        return max(self.list_api_versions(service_name, type_name))\n\n    @instance_cache\n    def list_api_versions(self, service_name, type_name):\n        \"\"\"List all API versions available for a particular service type\n\n        :type service_name: str\n        :param service_name: The name of the service\n\n        :type type_name: str\n        :param type_name: The type name for the service (i.e service-2,\n            paginators-1, etc.)\n\n        :rtype: list\n        :return: A list of API version strings in sorted order.\n\n        \"\"\"\n        known_api_versions = set()\n        for possible_path in self._potential_locations(\n            service_name, must_exist=True, is_dir=True\n        ):\n            for dirname in os.listdir(possible_path):\n                full_path = os.path.join(possible_path, dirname, type_name)\n                # Only add to the known_api_versions if the directory\n                # contains a service-2, paginators-1, etc. file corresponding\n                # to the type_name passed in.\n                if self.file_loader.exists(full_path):\n                    known_api_versions.add(dirname)\n        if not known_api_versions:\n            raise DataNotFoundError(data_path=service_name)\n        return sorted(known_api_versions)\n\n    @instance_cache\n    def load_service_model(self, service_name, type_name, api_version=None):\n        \"\"\"Load a botocore service model\n\n        This is the main method for loading botocore models (e.g. a service\n        model, pagination configs, waiter configs, etc.).\n\n        :type service_name: str\n        :param service_name: The name of the service (e.g ``ec2``, ``s3``).\n\n        :type type_name: str\n        :param type_name: The model type.  Valid types include, but are not\n            limited to: ``service-2``, ``paginators-1``, ``waiters-2``.\n\n        :type api_version: str\n        :param api_version: The API version to load.  If this is not\n            provided, then the latest API version will be used.\n\n        :type load_extras: bool\n        :param load_extras: Whether or not to load the tool extras which\n            contain additional data to be added to the model.\n\n        :raises: UnknownServiceError if there is no known service with\n            the provided service_name.\n\n        :raises: DataNotFoundError if no data could be found for the\n            service_name/type_name/api_version.\n\n        :return: The loaded data, as a python type (e.g. dict, list, etc).\n        \"\"\"\n        # Wrapper around the load_data.  This will calculate the path\n        # to call load_data with.\n        known_services = self.list_available_services(type_name)\n        if service_name not in known_services:\n            raise UnknownServiceError(\n                service_name=service_name,\n                known_service_names=', '.join(sorted(known_services)),\n            )\n        if api_version is None:\n            api_version = self.determine_latest_version(\n                service_name, type_name\n            )\n        full_path = os.path.join(service_name, api_version, type_name)\n        model = self.load_data(full_path)\n\n        # Load in all the extras\n        extras_data = self._find_extras(service_name, type_name, api_version)\n        self._extras_processor.process(model, extras_data)\n\n        return model\n\n    def _find_extras(self, service_name, type_name, api_version):\n        \"\"\"Creates an iterator over all the extras data.\"\"\"\n        for extras_type in self.extras_types:\n            extras_name = f'{type_name}.{extras_type}-extras'\n            full_path = os.path.join(service_name, api_version, extras_name)\n\n            try:\n                yield self.load_data(full_path)\n            except DataNotFoundError:\n                pass\n\n    @instance_cache\n    def load_data_with_path(self, name):\n        \"\"\"Same as ``load_data`` but returns file path as second return value.\n\n        :type name: str\n        :param name: The data path, i.e ``ec2/2015-03-01/service-2``.\n\n        :return: Tuple of the loaded data and the path to the data file\n            where the data was loaded from. If no data could be found then a\n            DataNotFoundError is raised.\n        \"\"\"\n        for possible_path in self._potential_locations(name):\n            found = self.file_loader.load_file(possible_path)\n            if found is not None:\n                return found, possible_path\n\n        # We didn't find anything that matched on any path.\n        raise DataNotFoundError(data_path=name)\n\n    def load_data(self, name):\n        \"\"\"Load data given a data path.\n\n        This is a low level method that will search through the various\n        search paths until it's able to load a value.  This is typically\n        only needed to load *non* model files (such as _endpoints and\n        _retry).  If you need to load model files, you should prefer\n        ``load_service_model``.  Use ``load_data_with_path`` to get the\n        data path of the data file as second return value.\n\n        :type name: str\n        :param name: The data path, i.e ``ec2/2015-03-01/service-2``.\n\n        :return: The loaded data. If no data could be found then\n            a DataNotFoundError is raised.\n        \"\"\"\n        data, _ = self.load_data_with_path(name)\n        return data\n\n    def _potential_locations(self, name=None, must_exist=False, is_dir=False):\n        # Will give an iterator over the full path of potential locations\n        # according to the search path.\n        for path in self.search_paths:\n            if os.path.isdir(path):\n                full_path = path\n                if name is not None:\n                    full_path = os.path.join(path, name)\n                if not must_exist:\n                    yield full_path\n                else:\n                    if is_dir and os.path.isdir(full_path):\n                        yield full_path\n                    elif os.path.exists(full_path):\n                        yield full_path\n\n    def is_builtin_path(self, path):\n        \"\"\"Whether a given path is within the package's data directory.\n\n        This method can be used together with load_data_with_path(name)\n        to determine if data has been loaded from a file bundled with the\n        package, as opposed to a file in a separate location.\n\n        :type path: str\n        :param path: The file path to check.\n\n        :return: Whether the given path is within the package's data directory.\n        \"\"\"\n        path = os.path.expanduser(os.path.expandvars(path))\n        return path.startswith(self.BUILTIN_DATA_PATH)\n\n\nclass ExtrasProcessor:\n    \"\"\"Processes data from extras files into service models.\"\"\"\n\n    def process(self, original_model, extra_models):\n        \"\"\"Processes data from a list of loaded extras files into a model\n\n        :type original_model: dict\n        :param original_model: The service model to load all the extras into.\n\n        :type extra_models: iterable of dict\n        :param extra_models: A list of loaded extras models.\n        \"\"\"\n        for extras in extra_models:\n            self._process(original_model, extras)\n\n    def _process(self, model, extra_model):\n        \"\"\"Process a single extras model into a service model.\"\"\"\n        if 'merge' in extra_model:\n            deep_merge(model, extra_model['merge'])\n", "botocore/paginate.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport base64\nimport json\nimport logging\nfrom itertools import tee\n\nimport jmespath\n\nfrom botocore.exceptions import PaginationError\nfrom botocore.utils import merge_dicts, set_value_from_jmespath\n\nlog = logging.getLogger(__name__)\n\n\nclass TokenEncoder:\n    \"\"\"Encodes dictionaries into opaque strings.\n\n    This for the most part json dumps + base64 encoding, but also supports\n    having bytes in the dictionary in addition to the types that json can\n    handle by default.\n\n    This is intended for use in encoding pagination tokens, which in some\n    cases can be complex structures and / or contain bytes.\n    \"\"\"\n\n    def encode(self, token):\n        \"\"\"Encodes a dictionary to an opaque string.\n\n        :type token: dict\n        :param token: A dictionary containing pagination information,\n            particularly the service pagination token(s) but also other boto\n            metadata.\n\n        :rtype: str\n        :returns: An opaque string\n        \"\"\"\n        try:\n            # Try just using json dumps first to avoid having to traverse\n            # and encode the dict. In 99.9999% of cases this will work.\n            json_string = json.dumps(token)\n        except (TypeError, UnicodeDecodeError):\n            # If normal dumping failed, go through and base64 encode all bytes.\n            encoded_token, encoded_keys = self._encode(token, [])\n\n            # Save the list of all the encoded key paths. We can safely\n            # assume that no service will ever use this key.\n            encoded_token['boto_encoded_keys'] = encoded_keys\n\n            # Now that the bytes are all encoded, dump the json.\n            json_string = json.dumps(encoded_token)\n\n        # base64 encode the json string to produce an opaque token string.\n        return base64.b64encode(json_string.encode('utf-8')).decode('utf-8')\n\n    def _encode(self, data, path):\n        \"\"\"Encode bytes in given data, keeping track of the path traversed.\"\"\"\n        if isinstance(data, dict):\n            return self._encode_dict(data, path)\n        elif isinstance(data, list):\n            return self._encode_list(data, path)\n        elif isinstance(data, bytes):\n            return self._encode_bytes(data, path)\n        else:\n            return data, []\n\n    def _encode_list(self, data, path):\n        \"\"\"Encode any bytes in a list, noting the index of what is encoded.\"\"\"\n        new_data = []\n        encoded = []\n        for i, value in enumerate(data):\n            new_path = path + [i]\n            new_value, new_encoded = self._encode(value, new_path)\n            new_data.append(new_value)\n            encoded.extend(new_encoded)\n        return new_data, encoded\n\n    def _encode_dict(self, data, path):\n        \"\"\"Encode any bytes in a dict, noting the index of what is encoded.\"\"\"\n        new_data = {}\n        encoded = []\n        for key, value in data.items():\n            new_path = path + [key]\n            new_value, new_encoded = self._encode(value, new_path)\n            new_data[key] = new_value\n            encoded.extend(new_encoded)\n        return new_data, encoded\n\n    def _encode_bytes(self, data, path):\n        \"\"\"Base64 encode a byte string.\"\"\"\n        return base64.b64encode(data).decode('utf-8'), [path]\n\n\nclass TokenDecoder:\n    \"\"\"Decodes token strings back into dictionaries.\n\n    This performs the inverse operation to the TokenEncoder, accepting\n    opaque strings and decoding them into a useable form.\n    \"\"\"\n\n    def decode(self, token):\n        \"\"\"Decodes an opaque string to a dictionary.\n\n        :type token: str\n        :param token: A token string given by the botocore pagination\n            interface.\n\n        :rtype: dict\n        :returns: A dictionary containing pagination information,\n            particularly the service pagination token(s) but also other boto\n            metadata.\n        \"\"\"\n        json_string = base64.b64decode(token.encode('utf-8')).decode('utf-8')\n        decoded_token = json.loads(json_string)\n\n        # Remove the encoding metadata as it is read since it will no longer\n        # be needed.\n        encoded_keys = decoded_token.pop('boto_encoded_keys', None)\n        if encoded_keys is None:\n            return decoded_token\n        else:\n            return self._decode(decoded_token, encoded_keys)\n\n    def _decode(self, token, encoded_keys):\n        \"\"\"Find each encoded value and decode it.\"\"\"\n        for key in encoded_keys:\n            encoded = self._path_get(token, key)\n            decoded = base64.b64decode(encoded.encode('utf-8'))\n            self._path_set(token, key, decoded)\n        return token\n\n    def _path_get(self, data, path):\n        \"\"\"Return the nested data at the given path.\n\n        For instance:\n            data = {'foo': ['bar', 'baz']}\n            path = ['foo', 0]\n            ==> 'bar'\n        \"\"\"\n        # jmespath isn't used here because it would be difficult to actually\n        # create the jmespath query when taking all of the unknowns of key\n        # structure into account. Gross though this is, it is simple and not\n        # very error prone.\n        d = data\n        for step in path:\n            d = d[step]\n        return d\n\n    def _path_set(self, data, path, value):\n        \"\"\"Set the value of a key in the given data.\n\n        Example:\n            data = {'foo': ['bar', 'baz']}\n            path = ['foo', 1]\n            value = 'bin'\n            ==> data = {'foo': ['bar', 'bin']}\n        \"\"\"\n        container = self._path_get(data, path[:-1])\n        container[path[-1]] = value\n\n\nclass PaginatorModel:\n    def __init__(self, paginator_config):\n        self._paginator_config = paginator_config['pagination']\n\n    def get_paginator(self, operation_name):\n        try:\n            single_paginator_config = self._paginator_config[operation_name]\n        except KeyError:\n            raise ValueError(\n                \"Paginator for operation does not exist: %s\" % operation_name\n            )\n        return single_paginator_config\n\n\nclass PageIterator:\n    \"\"\"An iterable object to paginate API results.\n    Please note it is NOT a python iterator.\n    Use ``iter`` to wrap this as a generator.\n    \"\"\"\n\n    def __init__(\n        self,\n        method,\n        input_token,\n        output_token,\n        more_results,\n        result_keys,\n        non_aggregate_keys,\n        limit_key,\n        max_items,\n        starting_token,\n        page_size,\n        op_kwargs,\n    ):\n        self._method = method\n        self._input_token = input_token\n        self._output_token = output_token\n        self._more_results = more_results\n        self._result_keys = result_keys\n        self._max_items = max_items\n        self._limit_key = limit_key\n        self._starting_token = starting_token\n        self._page_size = page_size\n        self._op_kwargs = op_kwargs\n        self._resume_token = None\n        self._non_aggregate_key_exprs = non_aggregate_keys\n        self._non_aggregate_part = {}\n        self._token_encoder = TokenEncoder()\n        self._token_decoder = TokenDecoder()\n\n    @property\n    def result_keys(self):\n        return self._result_keys\n\n    @property\n    def resume_token(self):\n        \"\"\"Token to specify to resume pagination.\"\"\"\n        return self._resume_token\n\n    @resume_token.setter\n    def resume_token(self, value):\n        if not isinstance(value, dict):\n            raise ValueError(\"Bad starting token: %s\" % value)\n\n        if 'boto_truncate_amount' in value:\n            token_keys = sorted(self._input_token + ['boto_truncate_amount'])\n        else:\n            token_keys = sorted(self._input_token)\n        dict_keys = sorted(value.keys())\n\n        if token_keys == dict_keys:\n            self._resume_token = self._token_encoder.encode(value)\n        else:\n            raise ValueError(\"Bad starting token: %s\" % value)\n\n    @property\n    def non_aggregate_part(self):\n        return self._non_aggregate_part\n\n    def __iter__(self):\n        current_kwargs = self._op_kwargs\n        previous_next_token = None\n        next_token = {key: None for key in self._input_token}\n        if self._starting_token is not None:\n            # If the starting token exists, populate the next_token with the\n            # values inside it. This ensures that we have the service's\n            # pagination token on hand if we need to truncate after the\n            # first response.\n            next_token = self._parse_starting_token()[0]\n        # The number of items from result_key we've seen so far.\n        total_items = 0\n        first_request = True\n        primary_result_key = self.result_keys[0]\n        starting_truncation = 0\n        self._inject_starting_params(current_kwargs)\n        while True:\n            response = self._make_request(current_kwargs)\n            parsed = self._extract_parsed_response(response)\n            if first_request:\n                # The first request is handled differently.  We could\n                # possibly have a resume/starting token that tells us where\n                # to index into the retrieved page.\n                if self._starting_token is not None:\n                    starting_truncation = self._handle_first_request(\n                        parsed, primary_result_key, starting_truncation\n                    )\n                first_request = False\n                self._record_non_aggregate_key_values(parsed)\n            else:\n                # If this isn't the first request, we have already sliced into\n                # the first request and had to make additional requests after.\n                # We no longer need to add this to truncation.\n                starting_truncation = 0\n            current_response = primary_result_key.search(parsed)\n            if current_response is None:\n                current_response = []\n            num_current_response = len(current_response)\n            truncate_amount = 0\n            if self._max_items is not None:\n                truncate_amount = (\n                    total_items + num_current_response - self._max_items\n                )\n            if truncate_amount > 0:\n                self._truncate_response(\n                    parsed,\n                    primary_result_key,\n                    truncate_amount,\n                    starting_truncation,\n                    next_token,\n                )\n                yield response\n                break\n            else:\n                yield response\n                total_items += num_current_response\n                next_token = self._get_next_token(parsed)\n                if all(t is None for t in next_token.values()):\n                    break\n                if (\n                    self._max_items is not None\n                    and total_items == self._max_items\n                ):\n                    # We're on a page boundary so we can set the current\n                    # next token to be the resume token.\n                    self.resume_token = next_token\n                    break\n                if (\n                    previous_next_token is not None\n                    and previous_next_token == next_token\n                ):\n                    message = (\n                        f\"The same next token was received \"\n                        f\"twice: {next_token}\"\n                    )\n                    raise PaginationError(message=message)\n                self._inject_token_into_kwargs(current_kwargs, next_token)\n                previous_next_token = next_token\n\n    def search(self, expression):\n        \"\"\"Applies a JMESPath expression to a paginator\n\n        Each page of results is searched using the provided JMESPath\n        expression. If the result is not a list, it is yielded\n        directly. If the result is a list, each element in the result\n        is yielded individually (essentially implementing a flatmap in\n        which the JMESPath search is the mapping function).\n\n        :type expression: str\n        :param expression: JMESPath expression to apply to each page.\n\n        :return: Returns an iterator that yields the individual\n            elements of applying a JMESPath expression to each page of\n            results.\n        \"\"\"\n        compiled = jmespath.compile(expression)\n        for page in self:\n            results = compiled.search(page)\n            if isinstance(results, list):\n                yield from results\n            else:\n                # Yield result directly if it is not a list.\n                yield results\n\n    def _make_request(self, current_kwargs):\n        return self._method(**current_kwargs)\n\n    def _extract_parsed_response(self, response):\n        return response\n\n    def _record_non_aggregate_key_values(self, response):\n        non_aggregate_keys = {}\n        for expression in self._non_aggregate_key_exprs:\n            result = expression.search(response)\n            set_value_from_jmespath(\n                non_aggregate_keys, expression.expression, result\n            )\n        self._non_aggregate_part = non_aggregate_keys\n\n    def _inject_starting_params(self, op_kwargs):\n        # If the user has specified a starting token we need to\n        # inject that into the operation's kwargs.\n        if self._starting_token is not None:\n            # Don't need to do anything special if there is no starting\n            # token specified.\n            next_token = self._parse_starting_token()[0]\n            self._inject_token_into_kwargs(op_kwargs, next_token)\n        if self._page_size is not None:\n            # Pass the page size as the parameter name for limiting\n            # page size, also known as the limit_key.\n            op_kwargs[self._limit_key] = self._page_size\n\n    def _inject_token_into_kwargs(self, op_kwargs, next_token):\n        for name, token in next_token.items():\n            if (token is not None) and (token != 'None'):\n                op_kwargs[name] = token\n            elif name in op_kwargs:\n                del op_kwargs[name]\n\n    def _handle_first_request(\n        self, parsed, primary_result_key, starting_truncation\n    ):\n        # If the payload is an array or string, we need to slice into it\n        # and only return the truncated amount.\n        starting_truncation = self._parse_starting_token()[1]\n        all_data = primary_result_key.search(parsed)\n        if isinstance(all_data, (list, str)):\n            data = all_data[starting_truncation:]\n        else:\n            data = None\n        set_value_from_jmespath(parsed, primary_result_key.expression, data)\n        # We also need to truncate any secondary result keys\n        # because they were not truncated in the previous last\n        # response.\n        for token in self.result_keys:\n            if token == primary_result_key:\n                continue\n            sample = token.search(parsed)\n            if isinstance(sample, list):\n                empty_value = []\n            elif isinstance(sample, str):\n                empty_value = ''\n            elif isinstance(sample, (int, float)):\n                empty_value = 0\n            else:\n                empty_value = None\n            set_value_from_jmespath(parsed, token.expression, empty_value)\n        return starting_truncation\n\n    def _truncate_response(\n        self,\n        parsed,\n        primary_result_key,\n        truncate_amount,\n        starting_truncation,\n        next_token,\n    ):\n        original = primary_result_key.search(parsed)\n        if original is None:\n            original = []\n        amount_to_keep = len(original) - truncate_amount\n        truncated = original[:amount_to_keep]\n        set_value_from_jmespath(\n            parsed, primary_result_key.expression, truncated\n        )\n        # The issue here is that even though we know how much we've truncated\n        # we need to account for this globally including any starting\n        # left truncation. For example:\n        # Raw response: [0,1,2,3]\n        # Starting index: 1\n        # Max items: 1\n        # Starting left truncation: [1, 2, 3]\n        # End right truncation for max items: [1]\n        # However, even though we only kept 1, this is post\n        # left truncation so the next starting index should be 2, not 1\n        # (left_truncation + amount_to_keep).\n        next_token['boto_truncate_amount'] = (\n            amount_to_keep + starting_truncation\n        )\n        self.resume_token = next_token\n\n    def _get_next_token(self, parsed):\n        if self._more_results is not None:\n            if not self._more_results.search(parsed):\n                return {}\n        next_tokens = {}\n        for output_token, input_key in zip(\n            self._output_token, self._input_token\n        ):\n            next_token = output_token.search(parsed)\n            # We do not want to include any empty strings as actual tokens.\n            # Treat them as None.\n            if next_token:\n                next_tokens[input_key] = next_token\n            else:\n                next_tokens[input_key] = None\n        return next_tokens\n\n    def result_key_iters(self):\n        teed_results = tee(self, len(self.result_keys))\n        return [\n            ResultKeyIterator(i, result_key)\n            for i, result_key in zip(teed_results, self.result_keys)\n        ]\n\n    def build_full_result(self):\n        complete_result = {}\n        for response in self:\n            page = response\n            # We want to try to catch operation object pagination\n            # and format correctly for those. They come in the form\n            # of a tuple of two elements: (http_response, parsed_responsed).\n            # We want the parsed_response as that is what the page iterator\n            # uses. We can remove it though once operation objects are removed.\n            if isinstance(response, tuple) and len(response) == 2:\n                page = response[1]\n            # We're incrementally building the full response page\n            # by page.  For each page in the response we need to\n            # inject the necessary components from the page\n            # into the complete_result.\n            for result_expression in self.result_keys:\n                # In order to incrementally update a result key\n                # we need to search the existing value from complete_result,\n                # then we need to search the _current_ page for the\n                # current result key value.  Then we append the current\n                # value onto the existing value, and re-set that value\n                # as the new value.\n                result_value = result_expression.search(page)\n                if result_value is None:\n                    continue\n                existing_value = result_expression.search(complete_result)\n                if existing_value is None:\n                    # Set the initial result\n                    set_value_from_jmespath(\n                        complete_result,\n                        result_expression.expression,\n                        result_value,\n                    )\n                    continue\n                # Now both result_value and existing_value contain something\n                if isinstance(result_value, list):\n                    existing_value.extend(result_value)\n                elif isinstance(result_value, (int, float, str)):\n                    # Modify the existing result with the sum or concatenation\n                    set_value_from_jmespath(\n                        complete_result,\n                        result_expression.expression,\n                        existing_value + result_value,\n                    )\n        merge_dicts(complete_result, self.non_aggregate_part)\n        if self.resume_token is not None:\n            complete_result['NextToken'] = self.resume_token\n        return complete_result\n\n    def _parse_starting_token(self):\n        if self._starting_token is None:\n            return None\n\n        # The starting token is a dict passed as a base64 encoded string.\n        next_token = self._starting_token\n        try:\n            next_token = self._token_decoder.decode(next_token)\n            index = 0\n            if 'boto_truncate_amount' in next_token:\n                index = next_token.get('boto_truncate_amount')\n                del next_token['boto_truncate_amount']\n        except (ValueError, TypeError):\n            next_token, index = self._parse_starting_token_deprecated()\n        return next_token, index\n\n    def _parse_starting_token_deprecated(self):\n        \"\"\"\n        This handles parsing of old style starting tokens, and attempts to\n        coerce them into the new style.\n        \"\"\"\n        log.debug(\n            \"Attempting to fall back to old starting token parser. For \"\n            \"token: %s\" % self._starting_token\n        )\n        if self._starting_token is None:\n            return None\n\n        parts = self._starting_token.split('___')\n        next_token = []\n        index = 0\n        if len(parts) == len(self._input_token) + 1:\n            try:\n                index = int(parts.pop())\n            except ValueError:\n                # This doesn't look like a valid old-style token, so we're\n                # passing it along as an opaque service token.\n                parts = [self._starting_token]\n\n        for part in parts:\n            if part == 'None':\n                next_token.append(None)\n            else:\n                next_token.append(part)\n        return self._convert_deprecated_starting_token(next_token), index\n\n    def _convert_deprecated_starting_token(self, deprecated_token):\n        \"\"\"\n        This attempts to convert a deprecated starting token into the new\n        style.\n        \"\"\"\n        len_deprecated_token = len(deprecated_token)\n        len_input_token = len(self._input_token)\n        if len_deprecated_token > len_input_token:\n            raise ValueError(\"Bad starting token: %s\" % self._starting_token)\n        elif len_deprecated_token < len_input_token:\n            log.debug(\n                \"Old format starting token does not contain all input \"\n                \"tokens. Setting the rest, in order, as None.\"\n            )\n            for i in range(len_input_token - len_deprecated_token):\n                deprecated_token.append(None)\n        return dict(zip(self._input_token, deprecated_token))\n\n\nclass Paginator:\n    PAGE_ITERATOR_CLS = PageIterator\n\n    def __init__(self, method, pagination_config, model):\n        self._model = model\n        self._method = method\n        self._pagination_cfg = pagination_config\n        self._output_token = self._get_output_tokens(self._pagination_cfg)\n        self._input_token = self._get_input_tokens(self._pagination_cfg)\n        self._more_results = self._get_more_results_token(self._pagination_cfg)\n        self._non_aggregate_keys = self._get_non_aggregate_keys(\n            self._pagination_cfg\n        )\n        self._result_keys = self._get_result_keys(self._pagination_cfg)\n        self._limit_key = self._get_limit_key(self._pagination_cfg)\n\n    @property\n    def result_keys(self):\n        return self._result_keys\n\n    def _get_non_aggregate_keys(self, config):\n        keys = []\n        for key in config.get('non_aggregate_keys', []):\n            keys.append(jmespath.compile(key))\n        return keys\n\n    def _get_output_tokens(self, config):\n        output = []\n        output_token = config['output_token']\n        if not isinstance(output_token, list):\n            output_token = [output_token]\n        for config in output_token:\n            output.append(jmespath.compile(config))\n        return output\n\n    def _get_input_tokens(self, config):\n        input_token = self._pagination_cfg['input_token']\n        if not isinstance(input_token, list):\n            input_token = [input_token]\n        return input_token\n\n    def _get_more_results_token(self, config):\n        more_results = config.get('more_results')\n        if more_results is not None:\n            return jmespath.compile(more_results)\n\n    def _get_result_keys(self, config):\n        result_key = config.get('result_key')\n        if result_key is not None:\n            if not isinstance(result_key, list):\n                result_key = [result_key]\n            result_key = [jmespath.compile(rk) for rk in result_key]\n            return result_key\n\n    def _get_limit_key(self, config):\n        return config.get('limit_key')\n\n    def paginate(self, **kwargs):\n        \"\"\"Create paginator object for an operation.\n\n        This returns an iterable object.  Iterating over\n        this object will yield a single page of a response\n        at a time.\n\n        \"\"\"\n        page_params = self._extract_paging_params(kwargs)\n        return self.PAGE_ITERATOR_CLS(\n            self._method,\n            self._input_token,\n            self._output_token,\n            self._more_results,\n            self._result_keys,\n            self._non_aggregate_keys,\n            self._limit_key,\n            page_params['MaxItems'],\n            page_params['StartingToken'],\n            page_params['PageSize'],\n            kwargs,\n        )\n\n    def _extract_paging_params(self, kwargs):\n        pagination_config = kwargs.pop('PaginationConfig', {})\n        max_items = pagination_config.get('MaxItems', None)\n        if max_items is not None:\n            max_items = int(max_items)\n        page_size = pagination_config.get('PageSize', None)\n        if page_size is not None:\n            if self._limit_key is None:\n                raise PaginationError(\n                    message=\"PageSize parameter is not supported for the \"\n                    \"pagination interface for this operation.\"\n                )\n            input_members = self._model.input_shape.members\n            limit_key_shape = input_members.get(self._limit_key)\n            if limit_key_shape.type_name == 'string':\n                if not isinstance(page_size, str):\n                    page_size = str(page_size)\n            else:\n                page_size = int(page_size)\n        return {\n            'MaxItems': max_items,\n            'StartingToken': pagination_config.get('StartingToken', None),\n            'PageSize': page_size,\n        }\n\n\nclass ResultKeyIterator:\n    \"\"\"Iterates over the results of paginated responses.\n\n    Each iterator is associated with a single result key.\n    Iterating over this object will give you each element in\n    the result key list.\n\n    :param pages_iterator: An iterator that will give you\n        pages of results (a ``PageIterator`` class).\n    :param result_key: The JMESPath expression representing\n        the result key.\n\n    \"\"\"\n\n    def __init__(self, pages_iterator, result_key):\n        self._pages_iterator = pages_iterator\n        self.result_key = result_key\n\n    def __iter__(self):\n        for page in self._pages_iterator:\n            results = self.result_key.search(page)\n            if results is None:\n                results = []\n            yield from results\n", "botocore/docs/sharedexample.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport numbers\nimport re\n\nfrom botocore.docs.utils import escape_controls\nfrom botocore.utils import parse_timestamp\n\n\nclass SharedExampleDocumenter:\n    def document_shared_example(\n        self, example, prefix, section, operation_model\n    ):\n        \"\"\"Documents a single shared example based on its definition.\n\n        :param example: The model of the example\n\n        :param prefix: The prefix to use in the method example.\n\n        :param section: The section to write to.\n\n        :param operation_model: The model of the operation used in the example\n        \"\"\"\n        section.style.new_paragraph()\n        section.write(example.get('description'))\n        section.style.new_line()\n        self.document_input(\n            section, example, prefix, operation_model.input_shape\n        )\n        self.document_output(section, example, operation_model.output_shape)\n\n    def document_input(self, section, example, prefix, shape):\n        input_section = section.add_new_section('input')\n        input_section.style.start_codeblock()\n        if prefix is not None:\n            input_section.write(prefix)\n        params = example.get('input', {})\n        comments = example.get('comments')\n        if comments:\n            comments = comments.get('input')\n        param_section = input_section.add_new_section('parameters')\n        self._document_params(param_section, params, comments, [], shape)\n        closing_section = input_section.add_new_section('input-close')\n        closing_section.style.new_line()\n        closing_section.style.new_line()\n        closing_section.write('print(response)')\n        closing_section.style.end_codeblock()\n\n    def document_output(self, section, example, shape):\n        output_section = section.add_new_section('output')\n        output_section.style.new_line()\n        output_section.write('Expected Output:')\n        output_section.style.new_line()\n        output_section.style.start_codeblock()\n        params = example.get('output', {})\n\n        # There might not be an output, but we will return metadata anyway\n        params['ResponseMetadata'] = {\"...\": \"...\"}\n        comments = example.get('comments')\n        if comments:\n            comments = comments.get('output')\n        self._document_dict(output_section, params, comments, [], shape, True)\n        closing_section = output_section.add_new_section('output-close')\n        closing_section.style.end_codeblock()\n\n    def _document(self, section, value, comments, path, shape):\n        \"\"\"\n        :param section: The section to add the docs to.\n\n        :param value: The input / output values representing the parameters that\n                      are included in the example.\n\n        :param comments: The dictionary containing all the comments to be\n                         applied to the example.\n\n        :param path: A list describing where the documenter is in traversing the\n                     parameters. This is used to find the equivalent location\n                     in the comments dictionary.\n        \"\"\"\n        if isinstance(value, dict):\n            self._document_dict(section, value, comments, path, shape)\n        elif isinstance(value, list):\n            self._document_list(section, value, comments, path, shape)\n        elif isinstance(value, numbers.Number):\n            self._document_number(section, value, path)\n        elif shape and shape.type_name == 'timestamp':\n            self._document_datetime(section, value, path)\n        else:\n            self._document_str(section, value, path)\n\n    def _document_dict(\n        self, section, value, comments, path, shape, top_level=False\n    ):\n        dict_section = section.add_new_section('dict-value')\n        self._start_nested_value(dict_section, '{')\n        for key, val in value.items():\n            path.append('.%s' % key)\n            item_section = dict_section.add_new_section(key)\n            item_section.style.new_line()\n            item_comment = self._get_comment(path, comments)\n            if item_comment:\n                item_section.write(item_comment)\n                item_section.style.new_line()\n            item_section.write(\"'%s': \" % key)\n\n            # Shape could be none if there is no output besides ResponseMetadata\n            item_shape = None\n            if shape:\n                if shape.type_name == 'structure':\n                    item_shape = shape.members.get(key)\n                elif shape.type_name == 'map':\n                    item_shape = shape.value\n            self._document(item_section, val, comments, path, item_shape)\n            path.pop()\n        dict_section_end = dict_section.add_new_section('ending-brace')\n        self._end_nested_value(dict_section_end, '}')\n        if not top_level:\n            dict_section_end.write(',')\n\n    def _document_params(self, section, value, comments, path, shape):\n        param_section = section.add_new_section('param-values')\n        self._start_nested_value(param_section, '(')\n        for key, val in value.items():\n            path.append('.%s' % key)\n            item_section = param_section.add_new_section(key)\n            item_section.style.new_line()\n            item_comment = self._get_comment(path, comments)\n            if item_comment:\n                item_section.write(item_comment)\n                item_section.style.new_line()\n            item_section.write(key + '=')\n\n            # Shape could be none if there are no input parameters\n            item_shape = None\n            if shape:\n                item_shape = shape.members.get(key)\n            self._document(item_section, val, comments, path, item_shape)\n            path.pop()\n        param_section_end = param_section.add_new_section('ending-parenthesis')\n        self._end_nested_value(param_section_end, ')')\n\n    def _document_list(self, section, value, comments, path, shape):\n        list_section = section.add_new_section('list-section')\n        self._start_nested_value(list_section, '[')\n        item_shape = shape.member\n        for index, val in enumerate(value):\n            item_section = list_section.add_new_section(index)\n            item_section.style.new_line()\n            path.append('[%s]' % index)\n            item_comment = self._get_comment(path, comments)\n            if item_comment:\n                item_section.write(item_comment)\n                item_section.style.new_line()\n            self._document(item_section, val, comments, path, item_shape)\n            path.pop()\n        list_section_end = list_section.add_new_section('ending-bracket')\n        self._end_nested_value(list_section_end, '],')\n\n    def _document_str(self, section, value, path):\n        # We do the string conversion because this might accept a type that\n        # we don't specifically address.\n        safe_value = escape_controls(value)\n        section.write(f\"'{safe_value}',\")\n\n    def _document_number(self, section, value, path):\n        section.write(\"%s,\" % str(value))\n\n    def _document_datetime(self, section, value, path):\n        datetime_tuple = parse_timestamp(value).timetuple()\n        datetime_str = str(datetime_tuple[0])\n        for i in range(1, len(datetime_tuple)):\n            datetime_str += \", \" + str(datetime_tuple[i])\n        section.write(\"datetime(%s),\" % datetime_str)\n\n    def _get_comment(self, path, comments):\n        key = re.sub(r'^\\.', '', ''.join(path))\n        if comments and key in comments:\n            return '# ' + comments[key]\n        else:\n            return ''\n\n    def _start_nested_value(self, section, start):\n        section.write(start)\n        section.style.indent()\n        section.style.indent()\n\n    def _end_nested_value(self, section, end):\n        section.style.dedent()\n        section.style.dedent()\n        section.style.new_line()\n        section.write(end)\n\n\ndef document_shared_examples(\n    section, operation_model, example_prefix, shared_examples\n):\n    \"\"\"Documents the shared examples\n\n    :param section: The section to write to.\n\n    :param operation_model: The model of the operation.\n\n    :param example_prefix: The prefix to use in the method example.\n\n    :param shared_examples: The shared JSON examples from the model.\n    \"\"\"\n    container_section = section.add_new_section('shared-examples')\n    container_section.style.new_paragraph()\n    container_section.style.bold('Examples')\n    documenter = SharedExampleDocumenter()\n    for example in shared_examples:\n        documenter.document_shared_example(\n            example=example,\n            section=container_section.add_new_section(example['id']),\n            prefix=example_prefix,\n            operation_model=operation_model,\n        )\n", "botocore/docs/paginator.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nfrom botocore import xform_name\nfrom botocore.compat import OrderedDict\nfrom botocore.docs.bcdoc.restdoc import DocumentStructure\nfrom botocore.docs.method import document_model_driven_method\nfrom botocore.docs.utils import DocumentedShape\nfrom botocore.utils import get_service_module_name\n\n\nclass PaginatorDocumenter:\n    def __init__(self, client, service_paginator_model, root_docs_path):\n        self._client = client\n        self._client_class_name = self._client.__class__.__name__\n        self._service_name = self._client.meta.service_model.service_name\n        self._service_paginator_model = service_paginator_model\n        self._root_docs_path = root_docs_path\n        self._USER_GUIDE_LINK = (\n            'https://boto3.amazonaws.com/'\n            'v1/documentation/api/latest/guide/paginators.html'\n        )\n\n    def document_paginators(self, section):\n        \"\"\"Documents the various paginators for a service\n\n        param section: The section to write to.\n        \"\"\"\n        section.style.h2('Paginators')\n        self._add_overview(section)\n        section.style.new_line()\n        section.writeln('The available paginators are:')\n        section.style.toctree()\n\n        paginator_names = sorted(\n            self._service_paginator_model._paginator_config\n        )\n\n        # List the available paginators and then document each paginator.\n        for paginator_name in paginator_names:\n            section.style.tocitem(\n                f'{self._service_name}/paginator/{paginator_name}'\n            )\n            # Create a new DocumentStructure for each paginator and add contents.\n            paginator_doc_structure = DocumentStructure(\n                paginator_name, target='html'\n            )\n            self._add_paginator(paginator_doc_structure, paginator_name)\n            # Write paginators in individual/nested files.\n            # Path: <root>/reference/services/<service>/paginator/<paginator_name>.rst\n            paginator_dir_path = os.path.join(\n                self._root_docs_path, self._service_name, 'paginator'\n            )\n            paginator_doc_structure.write_to_file(\n                paginator_dir_path, paginator_name\n            )\n\n    def _add_paginator(self, section, paginator_name):\n        breadcrumb_section = section.add_new_section('breadcrumb')\n        breadcrumb_section.style.ref(\n            self._client_class_name, f'../../{self._service_name}'\n        )\n        breadcrumb_section.write(f' / Paginator / {paginator_name}')\n        section.add_title_section(paginator_name)\n\n        # Docment the paginator class\n        paginator_section = section.add_new_section(paginator_name)\n        paginator_section.style.start_sphinx_py_class(\n            class_name=(\n                f'{self._client_class_name}.Paginator.{paginator_name}'\n            )\n        )\n        paginator_section.style.start_codeblock()\n        paginator_section.style.new_line()\n\n        # Document how to instantiate the paginator.\n        paginator_section.write(\n            f\"paginator = client.get_paginator('{xform_name(paginator_name)}')\"\n        )\n        paginator_section.style.end_codeblock()\n        paginator_section.style.new_line()\n        # Get the pagination model for the particular paginator.\n        paginator_config = self._service_paginator_model.get_paginator(\n            paginator_name\n        )\n        document_paginate_method(\n            section=paginator_section,\n            paginator_name=paginator_name,\n            event_emitter=self._client.meta.events,\n            service_model=self._client.meta.service_model,\n            paginator_config=paginator_config,\n        )\n\n    def _add_overview(self, section):\n        section.style.new_line()\n        section.write(\n            'Paginators are available on a client instance '\n            'via the ``get_paginator`` method. For more detailed instructions '\n            'and examples on the usage of paginators, see the '\n            'paginators '\n        )\n        section.style.external_link(\n            title='user guide',\n            link=self._USER_GUIDE_LINK,\n        )\n        section.write('.')\n        section.style.new_line()\n\n\ndef document_paginate_method(\n    section,\n    paginator_name,\n    event_emitter,\n    service_model,\n    paginator_config,\n    include_signature=True,\n):\n    \"\"\"Documents the paginate method of a paginator\n\n    :param section: The section to write to\n\n    :param paginator_name: The name of the paginator. It is snake cased.\n\n    :param event_emitter: The event emitter to use to emit events\n\n    :param service_model: The service model\n\n    :param paginator_config: The paginator config associated to a particular\n        paginator.\n\n    :param include_signature: Whether or not to include the signature.\n        It is useful for generating docstrings.\n    \"\"\"\n    # Retrieve the operation model of the underlying operation.\n    operation_model = service_model.operation_model(paginator_name)\n\n    # Add representations of the request and response parameters\n    # we want to include in the description of the paginate method.\n    # These are parameters we expose via the botocore interface.\n    pagination_config_members = OrderedDict()\n\n    pagination_config_members['MaxItems'] = DocumentedShape(\n        name='MaxItems',\n        type_name='integer',\n        documentation=(\n            '<p>The total number of items to return. If the total '\n            'number of items available is more than the value '\n            'specified in max-items then a <code>NextToken</code> '\n            'will be provided in the output that you can use to '\n            'resume pagination.</p>'\n        ),\n    )\n\n    if paginator_config.get('limit_key', None):\n        pagination_config_members['PageSize'] = DocumentedShape(\n            name='PageSize',\n            type_name='integer',\n            documentation='<p>The size of each page.<p>',\n        )\n\n    pagination_config_members['StartingToken'] = DocumentedShape(\n        name='StartingToken',\n        type_name='string',\n        documentation=(\n            '<p>A token to specify where to start paginating. '\n            'This is the <code>NextToken</code> from a previous '\n            'response.</p>'\n        ),\n    )\n\n    botocore_pagination_params = [\n        DocumentedShape(\n            name='PaginationConfig',\n            type_name='structure',\n            documentation=(\n                '<p>A dictionary that provides parameters to control '\n                'pagination.</p>'\n            ),\n            members=pagination_config_members,\n        )\n    ]\n\n    botocore_pagination_response_params = [\n        DocumentedShape(\n            name='NextToken',\n            type_name='string',\n            documentation=('<p>A token to resume pagination.</p>'),\n        )\n    ]\n\n    service_pagination_params = []\n\n    # Add the normal input token of the method to a list\n    # of input paramters that we wish to hide since we expose our own.\n    if isinstance(paginator_config['input_token'], list):\n        service_pagination_params += paginator_config['input_token']\n    else:\n        service_pagination_params.append(paginator_config['input_token'])\n\n    # Hide the limit key in the documentation.\n    if paginator_config.get('limit_key', None):\n        service_pagination_params.append(paginator_config['limit_key'])\n\n    # Hide the output tokens in the documentation.\n    service_pagination_response_params = []\n    if isinstance(paginator_config['output_token'], list):\n        service_pagination_response_params += paginator_config['output_token']\n    else:\n        service_pagination_response_params.append(\n            paginator_config['output_token']\n        )\n\n    paginate_description = (\n        'Creates an iterator that will paginate through responses '\n        'from :py:meth:`{}.Client.{}`.'.format(\n            get_service_module_name(service_model), xform_name(paginator_name)\n        )\n    )\n\n    document_model_driven_method(\n        section,\n        'paginate',\n        operation_model,\n        event_emitter=event_emitter,\n        method_description=paginate_description,\n        example_prefix='response_iterator = paginator.paginate',\n        include_input=botocore_pagination_params,\n        include_output=botocore_pagination_response_params,\n        exclude_input=service_pagination_params,\n        exclude_output=service_pagination_response_params,\n        include_signature=include_signature,\n    )\n", "botocore/docs/utils.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport re\nfrom collections import namedtuple\n\n\ndef py_type_name(type_name):\n    \"\"\"Get the Python type name for a given model type.\n\n        >>> py_type_name('list')\n        'list'\n        >>> py_type_name('structure')\n        'dict'\n\n    :rtype: string\n    \"\"\"\n    return {\n        'blob': 'bytes',\n        'character': 'string',\n        'double': 'float',\n        'long': 'integer',\n        'map': 'dict',\n        'structure': 'dict',\n        'timestamp': 'datetime',\n    }.get(type_name, type_name)\n\n\ndef py_default(type_name):\n    \"\"\"Get the Python default value for a given model type.\n\n        >>> py_default('string')\n        '\\'string\\''\n        >>> py_default('list')\n        '[...]'\n        >>> py_default('unknown')\n        '...'\n\n    :rtype: string\n    \"\"\"\n    return {\n        'double': '123.0',\n        'long': '123',\n        'integer': '123',\n        'string': \"'string'\",\n        'blob': \"b'bytes'\",\n        'boolean': 'True|False',\n        'list': '[...]',\n        'map': '{...}',\n        'structure': '{...}',\n        'timestamp': 'datetime(2015, 1, 1)',\n    }.get(type_name, '...')\n\n\ndef get_official_service_name(service_model):\n    \"\"\"Generate the official name of an AWS Service\n\n    :param service_model: The service model representing the service\n    \"\"\"\n    official_name = service_model.metadata.get('serviceFullName')\n    short_name = service_model.metadata.get('serviceAbbreviation', '')\n    if short_name.startswith('Amazon'):\n        short_name = short_name[7:]\n    if short_name.startswith('AWS'):\n        short_name = short_name[4:]\n    if short_name and short_name.lower() not in official_name.lower():\n        official_name += f' ({short_name})'\n    return official_name\n\n\n_DocumentedShape = namedtuple(\n    'DocumentedShape',\n    [\n        'name',\n        'type_name',\n        'documentation',\n        'metadata',\n        'members',\n        'required_members',\n    ],\n)\n\n\nclass DocumentedShape(_DocumentedShape):\n    \"\"\"Use this class to inject new shapes into a model for documentation\"\"\"\n\n    def __new__(\n        cls,\n        name,\n        type_name,\n        documentation,\n        metadata=None,\n        members=None,\n        required_members=None,\n    ):\n        if metadata is None:\n            metadata = []\n        if members is None:\n            members = []\n        if required_members is None:\n            required_members = []\n        return super().__new__(\n            cls,\n            name,\n            type_name,\n            documentation,\n            metadata,\n            members,\n            required_members,\n        )\n\n\nclass AutoPopulatedParam:\n    def __init__(self, name, param_description=None):\n        self.name = name\n        self.param_description = param_description\n        if param_description is None:\n            self.param_description = (\n                'Please note that this parameter is automatically populated '\n                'if it is not provided. Including this parameter is not '\n                'required\\n'\n            )\n\n    def document_auto_populated_param(self, event_name, section, **kwargs):\n        \"\"\"Documents auto populated parameters\n\n        It will remove any required marks for the parameter, remove the\n        parameter from the example, and add a snippet about the parameter\n        being autopopulated in the description.\n        \"\"\"\n        if event_name.startswith('docs.request-params'):\n            if self.name in section.available_sections:\n                section = section.get_section(self.name)\n                if 'is-required' in section.available_sections:\n                    section.delete_section('is-required')\n                description_section = section.get_section(\n                    'param-documentation'\n                )\n                description_section.writeln(self.param_description)\n        elif event_name.startswith('docs.request-example'):\n            section = section.get_section('structure-value')\n            if self.name in section.available_sections:\n                section.delete_section(self.name)\n\n\nclass HideParamFromOperations:\n    \"\"\"Hides a single parameter from multiple operations.\n\n    This method will remove a parameter from documentation and from\n    examples. This method is typically used for things that are\n    automatically populated because a user would be unable to provide\n    a value (e.g., a checksum of a serialized XML request body).\"\"\"\n\n    def __init__(self, service_name, parameter_name, operation_names):\n        \"\"\"\n        :type service_name: str\n        :param service_name: Name of the service to modify.\n\n        :type parameter_name: str\n        :param parameter_name: Name of the parameter to modify.\n\n        :type operation_names: list\n        :param operation_names: Operation names to modify.\n        \"\"\"\n        self._parameter_name = parameter_name\n        self._params_events = set()\n        self._example_events = set()\n        # Build up the sets of relevant event names.\n        param_template = 'docs.request-params.%s.%s.complete-section'\n        example_template = 'docs.request-example.%s.%s.complete-section'\n        for name in operation_names:\n            self._params_events.add(param_template % (service_name, name))\n            self._example_events.add(example_template % (service_name, name))\n\n    def hide_param(self, event_name, section, **kwargs):\n        if event_name in self._example_events:\n            # Modify the structure value for example events.\n            section = section.get_section('structure-value')\n        elif event_name not in self._params_events:\n            return\n        if self._parameter_name in section.available_sections:\n            section.delete_section(self._parameter_name)\n\n\nclass AppendParamDocumentation:\n    \"\"\"Appends documentation to a specific parameter\"\"\"\n\n    def __init__(self, parameter_name, doc_string):\n        self._parameter_name = parameter_name\n        self._doc_string = doc_string\n\n    def append_documentation(self, event_name, section, **kwargs):\n        if self._parameter_name in section.available_sections:\n            section = section.get_section(self._parameter_name)\n            description_section = section.get_section('param-documentation')\n            description_section.writeln(self._doc_string)\n\n\n_CONTROLS = {\n    '\\n': '\\\\n',\n    '\\r': '\\\\r',\n    '\\t': '\\\\t',\n    '\\b': '\\\\b',\n    '\\f': '\\\\f',\n}\n# Combines all CONTROLS keys into a big or regular expression\n_ESCAPE_CONTROLS_RE = re.compile('|'.join(map(re.escape, _CONTROLS)))\n# Based on the match get the appropriate replacement from CONTROLS\n_CONTROLS_MATCH_HANDLER = lambda match: _CONTROLS[match.group(0)]\n\n\ndef escape_controls(value):\n    return _ESCAPE_CONTROLS_RE.sub(_CONTROLS_MATCH_HANDLER, value)\n", "botocore/docs/params.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.shape import ShapeDocumenter\nfrom botocore.docs.utils import py_type_name\n\n\nclass BaseParamsDocumenter(ShapeDocumenter):\n    def document_params(self, section, shape, include=None, exclude=None):\n        \"\"\"Fills out the documentation for a section given a model shape.\n\n        :param section: The section to write the documentation to.\n\n        :param shape: The shape of the operation.\n\n        :type include: Dictionary where keys are parameter names and\n            values are the shapes of the parameter names.\n        :param include: The parameter shapes to include in the documentation.\n\n        :type exclude: List of the names of the parameters to exclude.\n        :param exclude: The names of the parameters to exclude from\n            documentation.\n        \"\"\"\n        history = []\n        self.traverse_and_document_shape(\n            section=section,\n            shape=shape,\n            history=history,\n            name=None,\n            include=include,\n            exclude=exclude,\n        )\n\n    def document_recursive_shape(self, section, shape, **kwargs):\n        self._add_member_documentation(section, shape, **kwargs)\n\n    def document_shape_default(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        self._add_member_documentation(section, shape, **kwargs)\n\n    def document_shape_type_list(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        self._add_member_documentation(section, shape, **kwargs)\n        param_shape = shape.member\n        param_section = section.add_new_section(\n            param_shape.name, context={'shape': shape.member.name}\n        )\n        self._start_nested_param(param_section)\n        self.traverse_and_document_shape(\n            section=param_section,\n            shape=param_shape,\n            history=history,\n            name=None,\n        )\n        section = section.add_new_section('end-list')\n        self._end_nested_param(section)\n\n    def document_shape_type_map(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        self._add_member_documentation(section, shape, **kwargs)\n\n        key_section = section.add_new_section(\n            'key', context={'shape': shape.key.name}\n        )\n        self._start_nested_param(key_section)\n        self._add_member_documentation(key_section, shape.key)\n\n        param_section = section.add_new_section(\n            shape.value.name, context={'shape': shape.value.name}\n        )\n        param_section.style.indent()\n        self._start_nested_param(param_section)\n        self.traverse_and_document_shape(\n            section=param_section,\n            shape=shape.value,\n            history=history,\n            name=None,\n        )\n\n        end_section = section.add_new_section('end-map')\n        self._end_nested_param(end_section)\n        self._end_nested_param(end_section)\n\n    def document_shape_type_structure(\n        self,\n        section,\n        shape,\n        history,\n        include=None,\n        exclude=None,\n        name=None,\n        **kwargs,\n    ):\n        members = self._add_members_to_shape(shape.members, include)\n        self._add_member_documentation(section, shape, name=name)\n        for param in members:\n            if exclude and param in exclude:\n                continue\n            param_shape = members[param]\n            param_section = section.add_new_section(\n                param, context={'shape': param_shape.name}\n            )\n            self._start_nested_param(param_section)\n            self.traverse_and_document_shape(\n                section=param_section,\n                shape=param_shape,\n                history=history,\n                name=param,\n            )\n        section = section.add_new_section('end-structure')\n        self._end_nested_param(section)\n\n    def _add_member_documentation(self, section, shape, **kwargs):\n        pass\n\n    def _add_members_to_shape(self, members, include):\n        if include:\n            members = members.copy()\n            for param in include:\n                members[param.name] = param\n        return members\n\n    def _document_non_top_level_param_type(self, type_section, shape):\n        special_py_type = self._get_special_py_type_name(shape)\n        py_type = py_type_name(shape.type_name)\n\n        type_format = '(%s) --'\n        if special_py_type is not None:\n            # Special type can reference a linked class.\n            # Italicizing it blows away the link.\n            type_section.write(type_format % special_py_type)\n        else:\n            type_section.style.italics(type_format % py_type)\n        type_section.write(' ')\n\n    def _start_nested_param(self, section):\n        section.style.indent()\n        section.style.new_line()\n\n    def _end_nested_param(self, section):\n        section.style.dedent()\n        section.style.new_line()\n\n\nclass ResponseParamsDocumenter(BaseParamsDocumenter):\n    \"\"\"Generates the description for the response parameters\"\"\"\n\n    EVENT_NAME = 'response-params'\n\n    def _add_member_documentation(self, section, shape, name=None, **kwargs):\n        name_section = section.add_new_section('param-name')\n        name_section.write('- ')\n        if name is not None:\n            name_section.style.bold('%s' % name)\n            name_section.write(' ')\n        type_section = section.add_new_section('param-type')\n        self._document_non_top_level_param_type(type_section, shape)\n\n        documentation_section = section.add_new_section('param-documentation')\n        if shape.documentation:\n            documentation_section.style.indent()\n            if getattr(shape, 'is_tagged_union', False):\n                tagged_union_docs = section.add_new_section(\n                    'param-tagged-union-docs'\n                )\n                note = (\n                    '.. note::'\n                    '    This is a Tagged Union structure. Only one of the '\n                    '    following top level keys will be set: %s. '\n                    '    If a client receives an unknown member it will '\n                    '    set ``SDK_UNKNOWN_MEMBER`` as the top level key, '\n                    '    which maps to the name or tag of the unknown '\n                    '    member. The structure of ``SDK_UNKNOWN_MEMBER`` is '\n                    '    as follows'\n                )\n                tagged_union_members_str = ', '.join(\n                    ['``%s``' % key for key in shape.members.keys()]\n                )\n                unknown_code_example = (\n                    '\\'SDK_UNKNOWN_MEMBER\\': '\n                    '{\\'name\\': \\'UnknownMemberName\\'}'\n                )\n                tagged_union_docs.write(note % (tagged_union_members_str))\n                example = section.add_new_section('param-unknown-example')\n                example.style.codeblock(unknown_code_example)\n            documentation_section.include_doc_string(shape.documentation)\n        section.style.new_paragraph()\n\n    def document_shape_type_event_stream(\n        self, section, shape, history, **kwargs\n    ):\n        self.document_shape_type_structure(section, shape, history, **kwargs)\n\n\nclass RequestParamsDocumenter(BaseParamsDocumenter):\n    \"\"\"Generates the description for the request parameters\"\"\"\n\n    EVENT_NAME = 'request-params'\n\n    def document_shape_type_structure(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        if len(history) > 1:\n            self._add_member_documentation(section, shape, **kwargs)\n            section.style.indent()\n        members = self._add_members_to_shape(shape.members, include)\n        for i, param in enumerate(members):\n            if exclude and param in exclude:\n                continue\n            param_shape = members[param]\n            param_section = section.add_new_section(\n                param, context={'shape': param_shape.name}\n            )\n            param_section.style.new_line()\n            is_required = param in shape.required_members\n            self.traverse_and_document_shape(\n                section=param_section,\n                shape=param_shape,\n                history=history,\n                name=param,\n                is_required=is_required,\n            )\n        section = section.add_new_section('end-structure')\n        if len(history) > 1:\n            section.style.dedent()\n        section.style.new_line()\n\n    def _add_member_documentation(\n        self,\n        section,\n        shape,\n        name=None,\n        is_top_level_param=False,\n        is_required=False,\n        **kwargs,\n    ):\n        py_type = self._get_special_py_type_name(shape)\n        if py_type is None:\n            py_type = py_type_name(shape.type_name)\n        if is_top_level_param:\n            type_section = section.add_new_section('param-type')\n            type_section.write(f':type {name}: {py_type}')\n            end_type_section = type_section.add_new_section('end-param-type')\n            end_type_section.style.new_line()\n            name_section = section.add_new_section('param-name')\n            name_section.write(':param %s: ' % name)\n\n        else:\n            name_section = section.add_new_section('param-name')\n            name_section.write('- ')\n            if name is not None:\n                name_section.style.bold('%s' % name)\n                name_section.write(' ')\n            type_section = section.add_new_section('param-type')\n            self._document_non_top_level_param_type(type_section, shape)\n\n        if is_required:\n            is_required_section = section.add_new_section('is-required')\n            is_required_section.style.indent()\n            is_required_section.style.bold('[REQUIRED]')\n            is_required_section.write(' ')\n        if shape.documentation:\n            documentation_section = section.add_new_section(\n                'param-documentation'\n            )\n            documentation_section.style.indent()\n            if getattr(shape, 'is_tagged_union', False):\n                tagged_union_docs = section.add_new_section(\n                    'param-tagged-union-docs'\n                )\n                note = (\n                    '.. note::'\n                    '    This is a Tagged Union structure. Only one of the '\n                    '    following top level keys can be set: %s. '\n                )\n                tagged_union_members_str = ', '.join(\n                    ['``%s``' % key for key in shape.members.keys()]\n                )\n                tagged_union_docs.write(note % (tagged_union_members_str))\n            documentation_section.include_doc_string(shape.documentation)\n            self._add_special_trait_documentation(documentation_section, shape)\n        end_param_section = section.add_new_section('end-param')\n        end_param_section.style.new_paragraph()\n\n    def _add_special_trait_documentation(self, section, shape):\n        if 'idempotencyToken' in shape.metadata:\n            self._append_idempotency_documentation(section)\n\n    def _append_idempotency_documentation(self, section):\n        docstring = 'This field is autopopulated if not provided.'\n        section.write(docstring)\n", "botocore/docs/waiter.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nfrom botocore import xform_name\nfrom botocore.compat import OrderedDict\nfrom botocore.docs.bcdoc.restdoc import DocumentStructure\nfrom botocore.docs.method import document_model_driven_method\nfrom botocore.docs.utils import DocumentedShape\nfrom botocore.utils import get_service_module_name\n\n\nclass WaiterDocumenter:\n    def __init__(self, client, service_waiter_model, root_docs_path):\n        self._client = client\n        self._client_class_name = self._client.__class__.__name__\n        self._service_name = self._client.meta.service_model.service_name\n        self._service_waiter_model = service_waiter_model\n        self._root_docs_path = root_docs_path\n        self._USER_GUIDE_LINK = (\n            'https://boto3.amazonaws.com/'\n            'v1/documentation/api/latest/guide/clients.html#waiters'\n        )\n\n    def document_waiters(self, section):\n        \"\"\"Documents the various waiters for a service.\n\n        :param section: The section to write to.\n        \"\"\"\n        section.style.h2('Waiters')\n        self._add_overview(section)\n        section.style.new_line()\n        section.writeln('The available waiters are:')\n        section.style.toctree()\n        for waiter_name in self._service_waiter_model.waiter_names:\n            section.style.tocitem(f'{self._service_name}/waiter/{waiter_name}')\n            # Create a new DocumentStructure for each waiter and add contents.\n            waiter_doc_structure = DocumentStructure(\n                waiter_name, target='html'\n            )\n            self._add_single_waiter(waiter_doc_structure, waiter_name)\n            # Write waiters in individual/nested files.\n            # Path: <root>/reference/services/<service>/waiter/<waiter_name>.rst\n            waiter_dir_path = os.path.join(\n                self._root_docs_path, self._service_name, 'waiter'\n            )\n            waiter_doc_structure.write_to_file(waiter_dir_path, waiter_name)\n\n    def _add_single_waiter(self, section, waiter_name):\n        breadcrumb_section = section.add_new_section('breadcrumb')\n        breadcrumb_section.style.ref(\n            self._client_class_name, f'../../{self._service_name}'\n        )\n        breadcrumb_section.write(f' / Waiter / {waiter_name}')\n        section.add_title_section(waiter_name)\n        waiter_section = section.add_new_section(waiter_name)\n        waiter_section.style.start_sphinx_py_class(\n            class_name=f\"{self._client_class_name}.Waiter.{waiter_name}\"\n        )\n\n        # Add example on how to instantiate waiter.\n        waiter_section.style.start_codeblock()\n        waiter_section.style.new_line()\n        waiter_section.write(\n            'waiter = client.get_waiter(\\'%s\\')' % xform_name(waiter_name)\n        )\n        waiter_section.style.end_codeblock()\n\n        # Add information on the wait() method\n        waiter_section.style.new_line()\n        document_wait_method(\n            section=waiter_section,\n            waiter_name=waiter_name,\n            event_emitter=self._client.meta.events,\n            service_model=self._client.meta.service_model,\n            service_waiter_model=self._service_waiter_model,\n        )\n\n    def _add_overview(self, section):\n        section.style.new_line()\n        section.write(\n            'Waiters are available on a client instance '\n            'via the ``get_waiter`` method. For more detailed instructions '\n            'and examples on the usage or waiters, see the '\n            'waiters '\n        )\n        section.style.external_link(\n            title='user guide',\n            link=self._USER_GUIDE_LINK,\n        )\n        section.write('.')\n        section.style.new_line()\n\n\ndef document_wait_method(\n    section,\n    waiter_name,\n    event_emitter,\n    service_model,\n    service_waiter_model,\n    include_signature=True,\n):\n    \"\"\"Documents a the wait method of a waiter\n\n    :param section: The section to write to\n\n    :param waiter_name: The name of the waiter\n\n    :param event_emitter: The event emitter to use to emit events\n\n    :param service_model: The service model\n\n    :param service_waiter_model: The waiter model associated to the service\n\n    :param include_signature: Whether or not to include the signature.\n        It is useful for generating docstrings.\n    \"\"\"\n    waiter_model = service_waiter_model.get_waiter(waiter_name)\n    operation_model = service_model.operation_model(waiter_model.operation)\n\n    waiter_config_members = OrderedDict()\n\n    waiter_config_members['Delay'] = DocumentedShape(\n        name='Delay',\n        type_name='integer',\n        documentation=(\n            '<p>The amount of time in seconds to wait between '\n            'attempts. Default: {}</p>'.format(waiter_model.delay)\n        ),\n    )\n\n    waiter_config_members['MaxAttempts'] = DocumentedShape(\n        name='MaxAttempts',\n        type_name='integer',\n        documentation=(\n            '<p>The maximum number of attempts to be made. '\n            'Default: {}</p>'.format(waiter_model.max_attempts)\n        ),\n    )\n\n    botocore_waiter_params = [\n        DocumentedShape(\n            name='WaiterConfig',\n            type_name='structure',\n            documentation=(\n                '<p>A dictionary that provides parameters to control '\n                'waiting behavior.</p>'\n            ),\n            members=waiter_config_members,\n        )\n    ]\n\n    wait_description = (\n        'Polls :py:meth:`{}.Client.{}` every {} '\n        'seconds until a successful state is reached. An error is '\n        'returned after {} failed checks.'.format(\n            get_service_module_name(service_model),\n            xform_name(waiter_model.operation),\n            waiter_model.delay,\n            waiter_model.max_attempts,\n        )\n    )\n\n    document_model_driven_method(\n        section,\n        'wait',\n        operation_model,\n        event_emitter=event_emitter,\n        method_description=wait_description,\n        example_prefix='waiter.wait',\n        include_input=botocore_waiter_params,\n        document_output=False,\n        include_signature=include_signature,\n    )\n", "botocore/docs/method.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport inspect\nimport types\n\nfrom botocore.docs.example import (\n    RequestExampleDocumenter,\n    ResponseExampleDocumenter,\n)\nfrom botocore.docs.params import (\n    RequestParamsDocumenter,\n    ResponseParamsDocumenter,\n)\n\nAWS_DOC_BASE = 'https://docs.aws.amazon.com/goto/WebAPI'\n\n\ndef get_instance_public_methods(instance):\n    \"\"\"Retrieves an objects public methods\n\n    :param instance: The instance of the class to inspect\n    :rtype: dict\n    :returns: A dictionary that represents an instance's methods where\n        the keys are the name of the methods and the\n        values are the handler to the method.\n    \"\"\"\n    instance_members = inspect.getmembers(instance)\n    instance_methods = {}\n    for name, member in instance_members:\n        if not name.startswith('_'):\n            if inspect.ismethod(member):\n                instance_methods[name] = member\n    return instance_methods\n\n\ndef document_model_driven_signature(\n    section, name, operation_model, include=None, exclude=None\n):\n    \"\"\"Documents the signature of a model-driven method\n\n    :param section: The section to write the documentation to.\n\n    :param name: The name of the method\n\n    :param operation_model: The operation model for the method\n\n    :type include: Dictionary where keys are parameter names and\n        values are the shapes of the parameter names.\n    :param include: The parameter shapes to include in the documentation.\n\n    :type exclude: List of the names of the parameters to exclude.\n    :param exclude: The names of the parameters to exclude from\n        documentation.\n    \"\"\"\n    params = {}\n    if operation_model.input_shape:\n        params = operation_model.input_shape.members\n\n    parameter_names = list(params.keys())\n\n    if include is not None:\n        for member in include:\n            parameter_names.append(member.name)\n\n    if exclude is not None:\n        for member in exclude:\n            if member in parameter_names:\n                parameter_names.remove(member)\n\n    signature_params = ''\n    if parameter_names:\n        signature_params = '**kwargs'\n    section.style.start_sphinx_py_method(name, signature_params)\n\n\ndef document_custom_signature(\n    section, name, method, include=None, exclude=None\n):\n    \"\"\"Documents the signature of a custom method\n\n    :param section: The section to write the documentation to.\n\n    :param name: The name of the method\n\n    :param method: The handle to the method being documented\n\n    :type include: Dictionary where keys are parameter names and\n        values are the shapes of the parameter names.\n    :param include: The parameter shapes to include in the documentation.\n\n    :type exclude: List of the names of the parameters to exclude.\n    :param exclude: The names of the parameters to exclude from\n        documentation.\n    \"\"\"\n    signature = inspect.signature(method)\n    # \"raw\" class methods are FunctionType and they include \"self\" param\n    # object methods are MethodType and they skip the \"self\" param\n    if isinstance(method, types.FunctionType):\n        self_param = next(iter(signature.parameters))\n        self_kind = signature.parameters[self_param].kind\n        # safety check that we got the right parameter\n        assert self_kind == inspect.Parameter.POSITIONAL_OR_KEYWORD\n        new_params = signature.parameters.copy()\n        del new_params[self_param]\n        signature = signature.replace(parameters=new_params.values())\n    signature_params = str(signature).lstrip('(')\n    signature_params = signature_params.rstrip(')')\n    section.style.start_sphinx_py_method(name, signature_params)\n\n\ndef document_custom_method(section, method_name, method):\n    \"\"\"Documents a non-data driven method\n\n    :param section: The section to write the documentation to.\n\n    :param method_name: The name of the method\n\n    :param method: The handle to the method being documented\n    \"\"\"\n    full_method_name = f\"{section.context.get('qualifier', '')}{method_name}\"\n    document_custom_signature(section, full_method_name, method)\n    method_intro_section = section.add_new_section('method-intro')\n    method_intro_section.writeln('')\n    doc_string = inspect.getdoc(method)\n    if doc_string is not None:\n        method_intro_section.style.write_py_doc_string(doc_string)\n\n\ndef document_model_driven_method(\n    section,\n    method_name,\n    operation_model,\n    event_emitter,\n    method_description=None,\n    example_prefix=None,\n    include_input=None,\n    include_output=None,\n    exclude_input=None,\n    exclude_output=None,\n    document_output=True,\n    include_signature=True,\n):\n    \"\"\"Documents an individual method\n\n    :param section: The section to write to\n\n    :param method_name: The name of the method\n\n    :param operation_model: The model of the operation\n\n    :param event_emitter: The event emitter to use to emit events\n\n    :param example_prefix: The prefix to use in the method example.\n\n    :type include_input: Dictionary where keys are parameter names and\n        values are the shapes of the parameter names.\n    :param include_input: The parameter shapes to include in the\n        input documentation.\n\n    :type include_output: Dictionary where keys are parameter names and\n        values are the shapes of the parameter names.\n    :param include_input: The parameter shapes to include in the\n        output documentation.\n\n    :type exclude_input: List of the names of the parameters to exclude.\n    :param exclude_input: The names of the parameters to exclude from\n        input documentation.\n\n    :type exclude_output: List of the names of the parameters to exclude.\n    :param exclude_input: The names of the parameters to exclude from\n        output documentation.\n\n    :param document_output: A boolean flag to indicate whether to\n        document the output.\n\n    :param include_signature: Whether or not to include the signature.\n        It is useful for generating docstrings.\n    \"\"\"\n    # Add the signature if specified.\n    if include_signature:\n        document_model_driven_signature(\n            section,\n            method_name,\n            operation_model,\n            include=include_input,\n            exclude=exclude_input,\n        )\n\n    # Add the description for the method.\n    method_intro_section = section.add_new_section('method-intro')\n    method_intro_section.include_doc_string(method_description)\n    if operation_model.deprecated:\n        method_intro_section.style.start_danger()\n        method_intro_section.writeln(\n            'This operation is deprecated and may not function as '\n            'expected. This operation should not be used going forward '\n            'and is only kept for the purpose of backwards compatiblity.'\n        )\n        method_intro_section.style.end_danger()\n    service_uid = operation_model.service_model.metadata.get('uid')\n    if service_uid is not None:\n        method_intro_section.style.new_paragraph()\n        method_intro_section.write(\"See also: \")\n        link = f\"{AWS_DOC_BASE}/{service_uid}/{operation_model.name}\"\n        method_intro_section.style.external_link(\n            title=\"AWS API Documentation\", link=link\n        )\n        method_intro_section.writeln('')\n\n    # Add the example section.\n    example_section = section.add_new_section('request-example')\n    example_section.style.new_paragraph()\n    example_section.style.bold('Request Syntax')\n\n    context = {\n        'special_shape_types': {\n            'streaming_input_shape': operation_model.get_streaming_input(),\n            'streaming_output_shape': operation_model.get_streaming_output(),\n            'eventstream_output_shape': operation_model.get_event_stream_output(),\n        },\n    }\n\n    if operation_model.input_shape:\n        RequestExampleDocumenter(\n            service_name=operation_model.service_model.service_name,\n            operation_name=operation_model.name,\n            event_emitter=event_emitter,\n            context=context,\n        ).document_example(\n            example_section,\n            operation_model.input_shape,\n            prefix=example_prefix,\n            include=include_input,\n            exclude=exclude_input,\n        )\n    else:\n        example_section.style.new_paragraph()\n        example_section.style.start_codeblock()\n        example_section.write(example_prefix + '()')\n\n    # Add the request parameter documentation.\n    request_params_section = section.add_new_section('request-params')\n    if operation_model.input_shape:\n        RequestParamsDocumenter(\n            service_name=operation_model.service_model.service_name,\n            operation_name=operation_model.name,\n            event_emitter=event_emitter,\n            context=context,\n        ).document_params(\n            request_params_section,\n            operation_model.input_shape,\n            include=include_input,\n            exclude=exclude_input,\n        )\n\n    # Add the return value documentation\n    return_section = section.add_new_section('return')\n    return_section.style.new_line()\n    if operation_model.output_shape is not None and document_output:\n        return_section.write(':rtype: dict')\n        return_section.style.new_line()\n        return_section.write(':returns: ')\n        return_section.style.indent()\n        return_section.style.new_line()\n\n        # If the operation is an event stream, describe the tagged union\n        event_stream_output = operation_model.get_event_stream_output()\n        if event_stream_output:\n            event_section = return_section.add_new_section('event-stream')\n            event_section.style.new_paragraph()\n            event_section.write(\n                'The response of this operation contains an '\n                ':class:`.EventStream` member. When iterated the '\n                ':class:`.EventStream` will yield events based on the '\n                'structure below, where only one of the top level keys '\n                'will be present for any given event.'\n            )\n            event_section.style.new_line()\n\n        # Add an example return value\n        return_example_section = return_section.add_new_section(\n            'response-example'\n        )\n        return_example_section.style.new_line()\n        return_example_section.style.bold('Response Syntax')\n        return_example_section.style.new_paragraph()\n        ResponseExampleDocumenter(\n            service_name=operation_model.service_model.service_name,\n            operation_name=operation_model.name,\n            event_emitter=event_emitter,\n            context=context,\n        ).document_example(\n            return_example_section,\n            operation_model.output_shape,\n            include=include_output,\n            exclude=exclude_output,\n        )\n\n        # Add a description for the return value\n        return_description_section = return_section.add_new_section(\n            'description'\n        )\n        return_description_section.style.new_line()\n        return_description_section.style.bold('Response Structure')\n        return_description_section.style.new_paragraph()\n        ResponseParamsDocumenter(\n            service_name=operation_model.service_model.service_name,\n            operation_name=operation_model.name,\n            event_emitter=event_emitter,\n            context=context,\n        ).document_params(\n            return_description_section,\n            operation_model.output_shape,\n            include=include_output,\n            exclude=exclude_output,\n        )\n    else:\n        return_section.write(':returns: None')\n", "botocore/docs/client.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nfrom botocore import xform_name\nfrom botocore.compat import OrderedDict\nfrom botocore.docs.bcdoc.restdoc import DocumentStructure\nfrom botocore.docs.example import ResponseExampleDocumenter\nfrom botocore.docs.method import (\n    document_custom_method,\n    document_model_driven_method,\n    get_instance_public_methods,\n)\nfrom botocore.docs.params import ResponseParamsDocumenter\nfrom botocore.docs.sharedexample import document_shared_examples\nfrom botocore.docs.utils import DocumentedShape, get_official_service_name\n\n\ndef _allowlist_generate_presigned_url(method_name, service_name, **kwargs):\n    if method_name != 'generate_presigned_url':\n        return None\n    return service_name in ['s3']\n\n\nclass ClientDocumenter:\n    _CLIENT_METHODS_FILTERS = [\n        _allowlist_generate_presigned_url,\n    ]\n\n    def __init__(self, client, root_docs_path, shared_examples=None):\n        self._client = client\n        self._client_class_name = self._client.__class__.__name__\n        self._root_docs_path = root_docs_path\n        self._shared_examples = shared_examples\n        if self._shared_examples is None:\n            self._shared_examples = {}\n        self._service_name = self._client.meta.service_model.service_name\n\n    def document_client(self, section):\n        \"\"\"Documents a client and its methods\n\n        :param section: The section to write to.\n        \"\"\"\n        self._add_title(section)\n        self._add_class_signature(section)\n        client_methods = self._get_client_methods()\n        self._add_client_intro(section, client_methods)\n        self._add_client_methods(client_methods)\n\n    def _get_client_methods(self):\n        client_methods = get_instance_public_methods(self._client)\n        return self._filter_client_methods(client_methods)\n\n    def _filter_client_methods(self, client_methods):\n        filtered_methods = {}\n        for method_name, method in client_methods.items():\n            include = self._filter_client_method(\n                method=method,\n                method_name=method_name,\n                service_name=self._service_name,\n            )\n            if include:\n                filtered_methods[method_name] = method\n        return filtered_methods\n\n    def _filter_client_method(self, **kwargs):\n        # Apply each filter to the method\n        for filter in self._CLIENT_METHODS_FILTERS:\n            filter_include = filter(**kwargs)\n            # Use the first non-None value returned by any of the filters\n            if filter_include is not None:\n                return filter_include\n        # Otherwise default to including it\n        return True\n\n    def _add_title(self, section):\n        section.style.h2('Client')\n\n    def _add_client_intro(self, section, client_methods):\n        section = section.add_new_section('intro')\n        # Write out the top level description for the client.\n        official_service_name = get_official_service_name(\n            self._client.meta.service_model\n        )\n        section.write(\n            f\"A low-level client representing {official_service_name}\"\n        )\n        section.style.new_line()\n        section.include_doc_string(\n            self._client.meta.service_model.documentation\n        )\n\n        # Write out the client example instantiation.\n        self._add_client_creation_example(section)\n\n        # List out all of the possible client methods.\n        section.style.dedent()\n        section.style.new_paragraph()\n        section.writeln('These are the available methods:')\n        section.style.toctree()\n        for method_name in sorted(client_methods):\n            section.style.tocitem(f'{self._service_name}/client/{method_name}')\n\n    def _add_class_signature(self, section):\n        section.style.start_sphinx_py_class(\n            class_name=f'{self._client_class_name}.Client'\n        )\n\n    def _add_client_creation_example(self, section):\n        section.style.start_codeblock()\n        section.style.new_line()\n        section.write(\n            'client = session.create_client(\\'{service}\\')'.format(\n                service=self._service_name\n            )\n        )\n        section.style.end_codeblock()\n\n    def _add_client_methods(self, client_methods):\n        for method_name in sorted(client_methods):\n            # Create a new DocumentStructure for each client method and add contents.\n            method_doc_structure = DocumentStructure(\n                method_name, target='html'\n            )\n            self._add_client_method(\n                method_doc_structure, method_name, client_methods[method_name]\n            )\n            # Write client methods in individual/nested files.\n            # Path: <root>/reference/services/<service>/client/<method_name>.rst\n            client_dir_path = os.path.join(\n                self._root_docs_path, self._service_name, 'client'\n            )\n            method_doc_structure.write_to_file(client_dir_path, method_name)\n\n    def _add_client_method(self, section, method_name, method):\n        breadcrumb_section = section.add_new_section('breadcrumb')\n        breadcrumb_section.style.ref(\n            self._client_class_name, f'../../{self._service_name}'\n        )\n        breadcrumb_section.write(f' / Client / {method_name}')\n        section.add_title_section(method_name)\n        method_section = section.add_new_section(\n            method_name,\n            context={'qualifier': f'{self._client_class_name}.Client.'},\n        )\n        if self._is_custom_method(method_name):\n            self._add_custom_method(\n                method_section,\n                method_name,\n                method,\n            )\n        else:\n            self._add_model_driven_method(method_section, method_name)\n\n    def _is_custom_method(self, method_name):\n        return method_name not in self._client.meta.method_to_api_mapping\n\n    def _add_custom_method(self, section, method_name, method):\n        document_custom_method(section, method_name, method)\n\n    def _add_method_exceptions_list(self, section, operation_model):\n        error_section = section.add_new_section('exceptions')\n        error_section.style.new_line()\n        error_section.style.bold('Exceptions')\n        error_section.style.new_line()\n        for error in operation_model.error_shapes:\n            class_name = (\n                f'{self._client_class_name}.Client.exceptions.{error.name}'\n            )\n            error_section.style.li(':py:class:`%s`' % class_name)\n\n    def _add_model_driven_method(self, section, method_name):\n        service_model = self._client.meta.service_model\n        operation_name = self._client.meta.method_to_api_mapping[method_name]\n        operation_model = service_model.operation_model(operation_name)\n\n        example_prefix = 'response = client.%s' % method_name\n        full_method_name = (\n            f\"{section.context.get('qualifier', '')}{method_name}\"\n        )\n        document_model_driven_method(\n            section,\n            full_method_name,\n            operation_model,\n            event_emitter=self._client.meta.events,\n            method_description=operation_model.documentation,\n            example_prefix=example_prefix,\n        )\n\n        # Add any modeled exceptions\n        if operation_model.error_shapes:\n            self._add_method_exceptions_list(section, operation_model)\n\n        # Add the shared examples\n        shared_examples = self._shared_examples.get(operation_name)\n        if shared_examples:\n            document_shared_examples(\n                section, operation_model, example_prefix, shared_examples\n            )\n\n\nclass ClientExceptionsDocumenter:\n    _USER_GUIDE_LINK = (\n        'https://boto3.amazonaws.com/'\n        'v1/documentation/api/latest/guide/error-handling.html'\n    )\n    _GENERIC_ERROR_SHAPE = DocumentedShape(\n        name='Error',\n        type_name='structure',\n        documentation=('Normalized access to common exception attributes.'),\n        members=OrderedDict(\n            [\n                (\n                    'Code',\n                    DocumentedShape(\n                        name='Code',\n                        type_name='string',\n                        documentation=(\n                            'An identifier specifying the exception type.'\n                        ),\n                    ),\n                ),\n                (\n                    'Message',\n                    DocumentedShape(\n                        name='Message',\n                        type_name='string',\n                        documentation=(\n                            'A descriptive message explaining why the exception '\n                            'occured.'\n                        ),\n                    ),\n                ),\n            ]\n        ),\n    )\n\n    def __init__(self, client, root_docs_path):\n        self._client = client\n        self._client_class_name = self._client.__class__.__name__\n        self._service_name = self._client.meta.service_model.service_name\n        self._root_docs_path = root_docs_path\n\n    def document_exceptions(self, section):\n        self._add_title(section)\n        self._add_overview(section)\n        self._add_exceptions_list(section)\n        self._add_exception_classes()\n\n    def _add_title(self, section):\n        section.style.h2('Client Exceptions')\n\n    def _add_overview(self, section):\n        section.style.new_line()\n        section.write(\n            'Client exceptions are available on a client instance '\n            'via the ``exceptions`` property. For more detailed instructions '\n            'and examples on the exact usage of client exceptions, see the '\n            'error handling '\n        )\n        section.style.external_link(\n            title='user guide',\n            link=self._USER_GUIDE_LINK,\n        )\n        section.write('.')\n        section.style.new_line()\n\n    def _exception_class_name(self, shape):\n        return f'{self._client_class_name}.Client.exceptions.{shape.name}'\n\n    def _add_exceptions_list(self, section):\n        error_shapes = self._client.meta.service_model.error_shapes\n        if not error_shapes:\n            section.style.new_line()\n            section.write('This client has no modeled exception classes.')\n            section.style.new_line()\n            return\n        section.style.new_line()\n        section.writeln('The available client exceptions are:')\n        section.style.toctree()\n        for shape in error_shapes:\n            section.style.tocitem(\n                f'{self._service_name}/client/exceptions/{shape.name}'\n            )\n\n    def _add_exception_classes(self):\n        for shape in self._client.meta.service_model.error_shapes:\n            # Create a new DocumentStructure for each exception method and add contents.\n            exception_doc_structure = DocumentStructure(\n                shape.name, target='html'\n            )\n            self._add_exception_class(exception_doc_structure, shape)\n            # Write exceptions in individual/nested files.\n            # Path: <root>/reference/services/<service>/client/exceptions/<exception_name>.rst\n            exception_dir_path = os.path.join(\n                self._root_docs_path,\n                self._service_name,\n                'client',\n                'exceptions',\n            )\n            exception_doc_structure.write_to_file(\n                exception_dir_path, shape.name\n            )\n\n    def _add_exception_class(self, section, shape):\n        breadcrumb_section = section.add_new_section('breadcrumb')\n        breadcrumb_section.style.ref(\n            self._client_class_name, f'../../../{self._service_name}'\n        )\n        breadcrumb_section.write(f' / Client / exceptions / {shape.name}')\n        section.add_title_section(shape.name)\n        class_section = section.add_new_section(shape.name)\n        class_name = self._exception_class_name(shape)\n        class_section.style.start_sphinx_py_class(class_name=class_name)\n        self._add_top_level_documentation(class_section, shape)\n        self._add_exception_catch_example(class_section, shape)\n        self._add_response_attr(class_section, shape)\n        class_section.style.end_sphinx_py_class()\n\n    def _add_top_level_documentation(self, section, shape):\n        if shape.documentation:\n            section.style.new_line()\n            section.include_doc_string(shape.documentation)\n            section.style.new_line()\n\n    def _add_exception_catch_example(self, section, shape):\n        section.style.new_line()\n        section.style.bold('Example')\n        section.style.new_paragraph()\n        section.style.start_codeblock()\n        section.write('try:')\n        section.style.indent()\n        section.style.new_line()\n        section.write('...')\n        section.style.dedent()\n        section.style.new_line()\n        section.write('except client.exceptions.%s as e:' % shape.name)\n        section.style.indent()\n        section.style.new_line()\n        section.write('print(e.response)')\n        section.style.dedent()\n        section.style.end_codeblock()\n\n    def _add_response_attr(self, section, shape):\n        response_section = section.add_new_section('response')\n        response_section.style.start_sphinx_py_attr('response')\n        self._add_response_attr_description(response_section)\n        self._add_response_example(response_section, shape)\n        self._add_response_params(response_section, shape)\n        response_section.style.end_sphinx_py_attr()\n\n    def _add_response_attr_description(self, section):\n        section.style.new_line()\n        section.include_doc_string(\n            'The parsed error response. All exceptions have a top level '\n            '``Error`` key that provides normalized access to common '\n            'exception atrributes. All other keys are specific to this '\n            'service or exception class.'\n        )\n        section.style.new_line()\n\n    def _add_response_example(self, section, shape):\n        example_section = section.add_new_section('syntax')\n        example_section.style.new_line()\n        example_section.style.bold('Syntax')\n        example_section.style.new_paragraph()\n        documenter = ResponseExampleDocumenter(\n            service_name=self._service_name,\n            operation_name=None,\n            event_emitter=self._client.meta.events,\n        )\n        documenter.document_example(\n            example_section,\n            shape,\n            include=[self._GENERIC_ERROR_SHAPE],\n        )\n\n    def _add_response_params(self, section, shape):\n        params_section = section.add_new_section('Structure')\n        params_section.style.new_line()\n        params_section.style.bold('Structure')\n        params_section.style.new_paragraph()\n        documenter = ResponseParamsDocumenter(\n            service_name=self._service_name,\n            operation_name=None,\n            event_emitter=self._client.meta.events,\n        )\n        documenter.document_params(\n            params_section,\n            shape,\n            include=[self._GENERIC_ERROR_SHAPE],\n        )\n\n\nclass ClientContextParamsDocumenter:\n    _CONFIG_GUIDE_LINK = (\n        'https://boto3.amazonaws.com/'\n        'v1/documentation/api/latest/guide/configuration.html'\n    )\n\n    OMITTED_CONTEXT_PARAMS = {\n        's3': (\n            'Accelerate',\n            'DisableMultiRegionAccessPoints',\n            'ForcePathStyle',\n            'UseArnRegion',\n        ),\n        's3control': ('UseArnRegion',),\n    }\n\n    def __init__(self, service_name, context_params):\n        self._service_name = service_name\n        self._context_params = context_params\n\n    def document_context_params(self, section):\n        self._add_title(section)\n        self._add_overview(section)\n        self._add_context_params_list(section)\n\n    def _add_title(self, section):\n        section.style.h2('Client Context Parameters')\n\n    def _add_overview(self, section):\n        section.style.new_line()\n        section.write(\n            'Client context parameters are configurable on a client '\n            'instance via the ``client_context_params`` parameter in the '\n            '``Config`` object. For more detailed instructions and examples '\n            'on the exact usage of context params see the '\n        )\n        section.style.external_link(\n            title='configuration guide',\n            link=self._CONFIG_GUIDE_LINK,\n        )\n        section.write('.')\n        section.style.new_line()\n\n    def _add_context_params_list(self, section):\n        section.style.new_line()\n        sn = f'``{self._service_name}``'\n        section.writeln(f'The available {sn} client context params are:')\n        for param in self._context_params:\n            section.style.new_line()\n            name = f'``{xform_name(param.name)}``'\n            section.write(f'* {name} ({param.type}) - {param.documentation}')\n", "botocore/docs/shape.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\n# NOTE: This class should not be instantiated and its\n# ``traverse_and_document_shape`` method called directly. It should be\n# inherited from a Documenter class with the appropriate methods\n# and attributes.\nfrom botocore.utils import is_json_value_header\n\n\nclass ShapeDocumenter:\n    EVENT_NAME = ''\n\n    def __init__(\n        self, service_name, operation_name, event_emitter, context=None\n    ):\n        self._service_name = service_name\n        self._operation_name = operation_name\n        self._event_emitter = event_emitter\n        self._context = context\n        if context is None:\n            self._context = {'special_shape_types': {}}\n\n    def traverse_and_document_shape(\n        self,\n        section,\n        shape,\n        history,\n        include=None,\n        exclude=None,\n        name=None,\n        is_required=False,\n    ):\n        \"\"\"Traverses and documents a shape\n\n        Will take a self class and call its appropriate methods as a shape\n        is traversed.\n\n        :param section: The section to document.\n\n        :param history: A list of the names of the shapes that have been\n            traversed.\n\n        :type include: Dictionary where keys are parameter names and\n            values are the shapes of the parameter names.\n        :param include: The parameter shapes to include in the documentation.\n\n        :type exclude: List of the names of the parameters to exclude.\n        :param exclude: The names of the parameters to exclude from\n            documentation.\n\n        :param name: The name of the shape.\n\n        :param is_required: If the shape is a required member.\n        \"\"\"\n        param_type = shape.type_name\n        if getattr(shape, 'serialization', {}).get('eventstream'):\n            param_type = 'event_stream'\n        if shape.name in history:\n            self.document_recursive_shape(section, shape, name=name)\n        else:\n            history.append(shape.name)\n            is_top_level_param = len(history) == 2\n            if hasattr(shape, 'is_document_type') and shape.is_document_type:\n                param_type = 'document'\n            getattr(\n                self,\n                f\"document_shape_type_{param_type}\",\n                self.document_shape_default,\n            )(\n                section,\n                shape,\n                history=history,\n                name=name,\n                include=include,\n                exclude=exclude,\n                is_top_level_param=is_top_level_param,\n                is_required=is_required,\n            )\n            if is_top_level_param:\n                self._event_emitter.emit(\n                    f\"docs.{self.EVENT_NAME}.{self._service_name}.{self._operation_name}.{name}\",\n                    section=section,\n                )\n            at_overlying_method_section = len(history) == 1\n            if at_overlying_method_section:\n                self._event_emitter.emit(\n                    f\"docs.{self.EVENT_NAME}.{self._service_name}.{self._operation_name}.complete-section\",\n                    section=section,\n                )\n            history.pop()\n\n    def _get_special_py_default(self, shape):\n        special_defaults = {\n            'document_type': '{...}|[...]|123|123.4|\\'string\\'|True|None',\n            'jsonvalue_header': '{...}|[...]|123|123.4|\\'string\\'|True|None',\n            'streaming_input_shape': 'b\\'bytes\\'|file',\n            'streaming_output_shape': 'StreamingBody()',\n            'eventstream_output_shape': 'EventStream()',\n        }\n        return self._get_value_for_special_type(shape, special_defaults)\n\n    def _get_special_py_type_name(self, shape):\n        special_type_names = {\n            'document_type': ':ref:`document<document>`',\n            'jsonvalue_header': 'JSON serializable',\n            'streaming_input_shape': 'bytes or seekable file-like object',\n            'streaming_output_shape': ':class:`.StreamingBody`',\n            'eventstream_output_shape': ':class:`.EventStream`',\n        }\n        return self._get_value_for_special_type(shape, special_type_names)\n\n    def _get_value_for_special_type(self, shape, special_type_map):\n        if is_json_value_header(shape):\n            return special_type_map['jsonvalue_header']\n        if hasattr(shape, 'is_document_type') and shape.is_document_type:\n            return special_type_map['document_type']\n        for special_type, marked_shape in self._context[\n            'special_shape_types'\n        ].items():\n            if special_type in special_type_map:\n                if shape == marked_shape:\n                    return special_type_map[special_type]\n        return None\n", "botocore/docs/translator.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom docutils import nodes\nfrom sphinx.locale import admonitionlabels\nfrom sphinx.writers.html5 import HTML5Translator as SphinxHTML5Translator\n\n\nclass BotoHTML5Translator(SphinxHTML5Translator):\n    \"\"\"Extension of Sphinx's ``HTML5Translator`` for Botocore documentation.\"\"\"\n\n    IGNORE_IMPLICIT_HEADINGS = [\n        '[REQUIRED]',\n    ]\n\n    def visit_admonition(self, node, name=\"\"):\n        \"\"\"Uses the h3 tag for admonition titles instead of the p tag.\"\"\"\n        self.body.append(\n            self.starttag(node, \"div\", CLASS=(\"admonition \" + name))\n        )\n        if name:\n            title = (\n                f\"<h3 class='admonition-title'> {admonitionlabels[name]}</h3>\"\n            )\n            self.body.append(title)\n\n    def is_implicit_heading(self, node):\n        \"\"\"Determines if a node is an implicit heading.\n\n        An implicit heading is represented by a paragraph node whose only\n        child is a strong node with text that isnt in `IGNORE_IMPLICIT_HEADINGS`.\n        \"\"\"\n        return (\n            len(node) == 1\n            and isinstance(node[0], nodes.strong)\n            and len(node[0]) == 1\n            and isinstance(node[0][0], nodes.Text)\n            and node[0][0].astext() not in self.IGNORE_IMPLICIT_HEADINGS\n        )\n\n    def visit_paragraph(self, node):\n        \"\"\"Visit a paragraph HTML element.\n\n        Replaces implicit headings with an h3 tag and defers to default\n        behavior for normal paragraph elements.\n        \"\"\"\n        if self.is_implicit_heading(node):\n            text = node[0][0]\n            self.body.append(f'<h3>{text}</h3>\\n')\n            # Do not visit the current nodes children or call its depart method.\n            raise nodes.SkipNode\n        else:\n            super().visit_paragraph(node)\n", "botocore/docs/service.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.bcdoc.restdoc import DocumentStructure\nfrom botocore.docs.client import (\n    ClientContextParamsDocumenter,\n    ClientDocumenter,\n    ClientExceptionsDocumenter,\n)\nfrom botocore.docs.paginator import PaginatorDocumenter\nfrom botocore.docs.waiter import WaiterDocumenter\nfrom botocore.exceptions import DataNotFoundError\n\n\nclass ServiceDocumenter:\n    def __init__(self, service_name, session, root_docs_path):\n        self._session = session\n        self._service_name = service_name\n        self._root_docs_path = root_docs_path\n\n        self._client = self._session.create_client(\n            service_name,\n            region_name='us-east-1',\n            aws_access_key_id='foo',\n            aws_secret_access_key='bar',\n        )\n        self._event_emitter = self._client.meta.events\n\n        self.sections = [\n            'title',\n            'client-api',\n            'client-exceptions',\n            'paginator-api',\n            'waiter-api',\n            'client-context-params',\n        ]\n\n    def document_service(self):\n        \"\"\"Documents an entire service.\n\n        :returns: The reStructured text of the documented service.\n        \"\"\"\n        doc_structure = DocumentStructure(\n            self._service_name, section_names=self.sections, target='html'\n        )\n        self.title(doc_structure.get_section('title'))\n        self.client_api(doc_structure.get_section('client-api'))\n        self.client_exceptions(doc_structure.get_section('client-exceptions'))\n        self.paginator_api(doc_structure.get_section('paginator-api'))\n        self.waiter_api(doc_structure.get_section('waiter-api'))\n        context_params_section = doc_structure.get_section(\n            'client-context-params'\n        )\n        self.client_context_params(context_params_section)\n        return doc_structure.flush_structure()\n\n    def title(self, section):\n        section.style.h1(self._client.__class__.__name__)\n        self._event_emitter.emit(\n            f\"docs.title.{self._service_name}\", section=section\n        )\n\n    def table_of_contents(self, section):\n        section.style.table_of_contents(title='Table of Contents', depth=2)\n\n    def client_api(self, section):\n        examples = None\n        try:\n            examples = self.get_examples(self._service_name)\n        except DataNotFoundError:\n            pass\n\n        ClientDocumenter(\n            self._client, self._root_docs_path, examples\n        ).document_client(section)\n\n    def client_exceptions(self, section):\n        ClientExceptionsDocumenter(\n            self._client, self._root_docs_path\n        ).document_exceptions(section)\n\n    def paginator_api(self, section):\n        try:\n            service_paginator_model = self._session.get_paginator_model(\n                self._service_name\n            )\n        except DataNotFoundError:\n            return\n        if service_paginator_model._paginator_config:\n            paginator_documenter = PaginatorDocumenter(\n                self._client, service_paginator_model, self._root_docs_path\n            )\n            paginator_documenter.document_paginators(section)\n\n    def waiter_api(self, section):\n        if self._client.waiter_names:\n            service_waiter_model = self._session.get_waiter_model(\n                self._service_name\n            )\n            waiter_documenter = WaiterDocumenter(\n                self._client, service_waiter_model, self._root_docs_path\n            )\n            waiter_documenter.document_waiters(section)\n\n    def get_examples(self, service_name, api_version=None):\n        loader = self._session.get_component('data_loader')\n        examples = loader.load_service_model(\n            service_name, 'examples-1', api_version\n        )\n        return examples['examples']\n\n    def client_context_params(self, section):\n        omitted_params = ClientContextParamsDocumenter.OMITTED_CONTEXT_PARAMS\n        params_to_omit = omitted_params.get(self._service_name, [])\n        service_model = self._client.meta.service_model\n        raw_context_params = service_model.client_context_parameters\n        context_params = [\n            p for p in raw_context_params if p.name not in params_to_omit\n        ]\n        if context_params:\n            context_param_documenter = ClientContextParamsDocumenter(\n                self._service_name, context_params\n            )\n            context_param_documenter.document_context_params(section)\n", "botocore/docs/__init__.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\n\nfrom botocore.docs.service import ServiceDocumenter\n\nDEPRECATED_SERVICE_NAMES = {'sms-voice'}\n\n\ndef generate_docs(root_dir, session):\n    \"\"\"Generates the reference documentation for botocore\n\n    This will go through every available AWS service and output ReSTructured\n    text files documenting each service.\n\n    :param root_dir: The directory to write the reference files to. Each\n        service's reference documentation is loacated at\n        root_dir/reference/services/service-name.rst\n    \"\"\"\n    # Create the root directory where all service docs live.\n    services_dir_path = os.path.join(root_dir, 'reference', 'services')\n    if not os.path.exists(services_dir_path):\n        os.makedirs(services_dir_path)\n\n    # Prevents deprecated service names from being generated in docs.\n    available_services = [\n        service\n        for service in session.get_available_services()\n        if service not in DEPRECATED_SERVICE_NAMES\n    ]\n\n    # Generate reference docs and write them out.\n    for service_name in available_services:\n        docs = ServiceDocumenter(\n            service_name, session, services_dir_path\n        ).document_service()\n\n        # Write the main service documentation page.\n        # Path: <root>/reference/services/<service>/index.rst\n        service_file_path = os.path.join(\n            services_dir_path, f'{service_name}.rst'\n        )\n        with open(service_file_path, 'wb') as f:\n            f.write(docs)\n", "botocore/docs/example.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.shape import ShapeDocumenter\nfrom botocore.docs.utils import py_default\n\n\nclass BaseExampleDocumenter(ShapeDocumenter):\n    def document_example(\n        self, section, shape, prefix=None, include=None, exclude=None\n    ):\n        \"\"\"Generates an example based on a shape\n\n        :param section: The section to write the documentation to.\n\n        :param shape: The shape of the operation.\n\n        :param prefix: Anything to be included before the example\n\n        :type include: Dictionary where keys are parameter names and\n            values are the shapes of the parameter names.\n        :param include: The parameter shapes to include in the documentation.\n\n        :type exclude: List of the names of the parameters to exclude.\n        :param exclude: The names of the parameters to exclude from\n            documentation.\n        \"\"\"\n        history = []\n        section.style.new_line()\n        section.style.start_codeblock()\n        if prefix is not None:\n            section.write(prefix)\n        self.traverse_and_document_shape(\n            section=section,\n            shape=shape,\n            history=history,\n            include=include,\n            exclude=exclude,\n        )\n        final_blank_line_section = section.add_new_section('final-blank-line')\n        final_blank_line_section.style.new_line()\n\n    def document_recursive_shape(self, section, shape, **kwargs):\n        section.write('{\\'... recursive ...\\'}')\n\n    def document_shape_default(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        py_type = self._get_special_py_default(shape)\n        if py_type is None:\n            py_type = py_default(shape.type_name)\n\n        if self._context.get('streaming_shape') == shape:\n            py_type = 'StreamingBody()'\n        section.write(py_type)\n\n    def document_shape_type_string(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        if 'enum' in shape.metadata:\n            for i, enum in enumerate(shape.metadata['enum']):\n                section.write('\\'%s\\'' % enum)\n                if i < len(shape.metadata['enum']) - 1:\n                    section.write('|')\n        else:\n            self.document_shape_default(section, shape, history)\n\n    def document_shape_type_list(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        param_shape = shape.member\n        list_section = section.add_new_section('list-value')\n        self._start_nested_param(list_section, '[')\n        param_section = list_section.add_new_section(\n            'member', context={'shape': param_shape.name}\n        )\n        self.traverse_and_document_shape(\n            section=param_section, shape=param_shape, history=history\n        )\n        ending_comma_section = list_section.add_new_section('ending-comma')\n        ending_comma_section.write(',')\n        ending_bracket_section = list_section.add_new_section('ending-bracket')\n        self._end_nested_param(ending_bracket_section, ']')\n\n    def document_shape_type_structure(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        if not shape.members:\n            section.write('{}')\n            return\n\n        section = section.add_new_section('structure-value')\n        self._start_nested_param(section, '{')\n\n        input_members = self._add_members_to_shape(shape.members, include)\n\n        for i, param in enumerate(input_members):\n            if exclude and param in exclude:\n                continue\n            param_section = section.add_new_section(param)\n            param_section.write('\\'%s\\': ' % param)\n            param_shape = input_members[param]\n            param_value_section = param_section.add_new_section(\n                'member-value', context={'shape': param_shape.name}\n            )\n            self.traverse_and_document_shape(\n                section=param_value_section,\n                shape=param_shape,\n                history=history,\n                name=param,\n            )\n            if i < len(input_members) - 1:\n                ending_comma_section = param_section.add_new_section(\n                    'ending-comma'\n                )\n                ending_comma_section.write(',')\n                ending_comma_section.style.new_line()\n        self._end_structure(section, '{', '}')\n\n    def document_shape_type_map(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        map_section = section.add_new_section('map-value')\n        self._start_nested_param(map_section, '{')\n        value_shape = shape.value\n        key_section = map_section.add_new_section(\n            'key', context={'shape': shape.key.name}\n        )\n        key_section.write('\\'string\\': ')\n        value_section = map_section.add_new_section(\n            'value', context={'shape': value_shape.name}\n        )\n        self.traverse_and_document_shape(\n            section=value_section, shape=value_shape, history=history\n        )\n        end_bracket_section = map_section.add_new_section('ending-bracket')\n        self._end_nested_param(end_bracket_section, '}')\n\n    def _add_members_to_shape(self, members, include):\n        if include:\n            members = members.copy()\n            for param in include:\n                members[param.name] = param\n        return members\n\n    def _start_nested_param(self, section, start=None):\n        if start is not None:\n            section.write(start)\n        section.style.indent()\n        section.style.indent()\n        section.style.new_line()\n\n    def _end_nested_param(self, section, end=None):\n        section.style.dedent()\n        section.style.dedent()\n        section.style.new_line()\n        if end is not None:\n            section.write(end)\n\n    def _end_structure(self, section, start, end):\n        # If there are no members in the strucuture, then make sure the\n        # start and the end bracket are on the same line, by removing all\n        # previous text and writing the start and end.\n        if not section.available_sections:\n            section.clear_text()\n            section.write(start + end)\n            self._end_nested_param(section)\n        else:\n            end_bracket_section = section.add_new_section('ending-bracket')\n            self._end_nested_param(end_bracket_section, end)\n\n\nclass ResponseExampleDocumenter(BaseExampleDocumenter):\n    EVENT_NAME = 'response-example'\n\n    def document_shape_type_event_stream(\n        self, section, shape, history, **kwargs\n    ):\n        section.write('EventStream(')\n        self.document_shape_type_structure(section, shape, history, **kwargs)\n        end_section = section.add_new_section('event-stream-end')\n        end_section.write(')')\n\n\nclass RequestExampleDocumenter(BaseExampleDocumenter):\n    EVENT_NAME = 'request-example'\n\n    def document_shape_type_structure(\n        self, section, shape, history, include=None, exclude=None, **kwargs\n    ):\n        param_format = '\\'%s\\''\n        operator = ': '\n        start = '{'\n        end = '}'\n\n        if len(history) <= 1:\n            operator = '='\n            start = '('\n            end = ')'\n            param_format = '%s'\n        section = section.add_new_section('structure-value')\n        self._start_nested_param(section, start)\n        input_members = self._add_members_to_shape(shape.members, include)\n\n        for i, param in enumerate(input_members):\n            if exclude and param in exclude:\n                continue\n            param_section = section.add_new_section(param)\n            param_section.write(param_format % param)\n            param_section.write(operator)\n            param_shape = input_members[param]\n            param_value_section = param_section.add_new_section(\n                'member-value', context={'shape': param_shape.name}\n            )\n            self.traverse_and_document_shape(\n                section=param_value_section,\n                shape=param_shape,\n                history=history,\n                name=param,\n            )\n            if i < len(input_members) - 1:\n                ending_comma_section = param_section.add_new_section(\n                    'ending-comma'\n                )\n                ending_comma_section.write(',')\n                ending_comma_section.style.new_line()\n        self._end_structure(section, start, end)\n", "botocore/docs/docstring.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.docs.bcdoc.restdoc import DocumentStructure\nfrom botocore.docs.method import document_model_driven_method\nfrom botocore.docs.paginator import document_paginate_method\nfrom botocore.docs.waiter import document_wait_method\n\n\nclass LazyLoadedDocstring(str):\n    \"\"\"Used for lazily loading docstrings\n\n    You can instantiate this class and assign it to a __doc__ value.\n    The docstring will not be generated till accessed via __doc__ or\n    help(). Note that all docstring classes **must** subclass from\n    this class. It cannot be used directly as a docstring.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        The args and kwargs are the same as the underlying document\n        generation function. These just get proxied to the underlying\n        function.\n        \"\"\"\n        super().__init__()\n        self._gen_args = args\n        self._gen_kwargs = kwargs\n        self._docstring = None\n\n    def __new__(cls, *args, **kwargs):\n        # Needed in order to sub class from str with args and kwargs\n        return super().__new__(cls)\n\n    def _write_docstring(self, *args, **kwargs):\n        raise NotImplementedError(\n            '_write_docstring is not implemented. Please subclass from '\n            'this class and provide your own _write_docstring method'\n        )\n\n    def expandtabs(self, tabsize=8):\n        \"\"\"Expands tabs to spaces\n\n        So this is a big hack in order to get lazy loaded docstring work\n        for the ``help()``. In the ``help()`` function, ``pydoc`` and\n        ``inspect`` are used. At some point the ``inspect.cleandoc``\n        method is called. To clean the docs ``expandtabs`` is called\n        and that is where we override the method to generate and return the\n        docstrings.\n        \"\"\"\n        if self._docstring is None:\n            self._generate()\n        return self._docstring.expandtabs(tabsize)\n\n    def __str__(self):\n        return self._generate()\n\n    # __doc__ of target will use either __repr__ or __str__ of this class.\n    __repr__ = __str__\n\n    def _generate(self):\n        # Generate the docstring if it is not already cached.\n        if self._docstring is None:\n            self._docstring = self._create_docstring()\n        return self._docstring\n\n    def _create_docstring(self):\n        docstring_structure = DocumentStructure('docstring', target='html')\n        # Call the document method function with the args and kwargs\n        # passed to the class.\n        self._write_docstring(\n            docstring_structure, *self._gen_args, **self._gen_kwargs\n        )\n        return docstring_structure.flush_structure().decode('utf-8')\n\n\nclass ClientMethodDocstring(LazyLoadedDocstring):\n    def _write_docstring(self, *args, **kwargs):\n        document_model_driven_method(*args, **kwargs)\n\n\nclass WaiterDocstring(LazyLoadedDocstring):\n    def _write_docstring(self, *args, **kwargs):\n        document_wait_method(*args, **kwargs)\n\n\nclass PaginatorDocstring(LazyLoadedDocstring):\n    def _write_docstring(self, *args, **kwargs):\n        document_paginate_method(*args, **kwargs)\n", "botocore/docs/bcdoc/restdoc.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport os\nimport re\n\nfrom botocore.compat import OrderedDict\nfrom botocore.docs.bcdoc.docstringparser import DocStringParser\nfrom botocore.docs.bcdoc.style import ReSTStyle\n\nDEFAULT_AWS_DOCS_LINK = 'https://docs.aws.amazon.com/index.html'\nDOCUMENTATION_LINK_REGEX = re.compile(\n    r'`AWS API Documentation '\n    r'<https://docs.aws.amazon.com/goto/WebAPI/[a-z0-9-.]*/[a-zA-Z]*>`_'\n)\nLARGE_SECTION_MESSAGE = \"\"\"\n\n    **{}**\n    ::\n\n        # This section is too large to render.\n        # Please see the AWS API Documentation linked below.\n\n    {}\n    \"\"\"\nLOG = logging.getLogger('bcdocs')\nSECTION_LINE_LIMIT_CONFIG = {\n    'response-example': {'name': 'Response Syntax', 'line_limit': 1500},\n    'description': {'name': 'Response Structure', 'line_limit': 5000},\n    'request-example': {'name': 'Request Syntax', 'line_limit': 1500},\n    'request-params': {'name': 'Parameters', 'line_limit': 5000},\n}\nSECTION_METHOD_PATH_DEPTH = {\n    'client-api': 4,\n    'paginator-api': 3,\n    'waiter-api': 3,\n}\n\n\nclass ReSTDocument:\n    def __init__(self, target='man'):\n        self.style = ReSTStyle(self)\n        self.target = target\n        self.parser = DocStringParser(self)\n        self.keep_data = True\n        self.do_translation = False\n        self.translation_map = {}\n        self.hrefs = {}\n        self._writes = []\n        self._last_doc_string = None\n\n    def _write(self, s):\n        if self.keep_data and s is not None:\n            self._writes.append(s)\n\n    def write(self, content):\n        \"\"\"\n        Write content into the document.\n        \"\"\"\n        self._write(content)\n\n    def writeln(self, content):\n        \"\"\"\n        Write content on a newline.\n        \"\"\"\n        self._write(f'{self.style.spaces()}{content}\\n')\n\n    def peek_write(self):\n        \"\"\"\n        Returns the last content written to the document without\n        removing it from the stack.\n        \"\"\"\n        return self._writes[-1]\n\n    def pop_write(self):\n        \"\"\"\n        Removes and returns the last content written to the stack.\n        \"\"\"\n        return self._writes.pop() if len(self._writes) > 0 else None\n\n    def push_write(self, s):\n        \"\"\"\n        Places new content on the stack.\n        \"\"\"\n        self._writes.append(s)\n\n    def getvalue(self):\n        \"\"\"\n        Returns the current content of the document as a string.\n        \"\"\"\n        if self.hrefs:\n            self.style.new_paragraph()\n            for refname, link in self.hrefs.items():\n                self.style.link_target_definition(refname, link)\n        return ''.join(self._writes).encode('utf-8')\n\n    def translate_words(self, words):\n        return [self.translation_map.get(w, w) for w in words]\n\n    def handle_data(self, data):\n        if data and self.keep_data:\n            self._write(data)\n\n    def include_doc_string(self, doc_string):\n        if doc_string:\n            try:\n                start = len(self._writes)\n                self.parser.feed(doc_string)\n                self.parser.close()\n                end = len(self._writes)\n                self._last_doc_string = (start, end)\n            except Exception:\n                LOG.debug('Error parsing doc string', exc_info=True)\n                LOG.debug(doc_string)\n\n    def remove_last_doc_string(self):\n        # Removes all writes inserted by last doc string\n        if self._last_doc_string is not None:\n            start, end = self._last_doc_string\n            del self._writes[start:end]\n\n\nclass DocumentStructure(ReSTDocument):\n    def __init__(self, name, section_names=None, target='man', context=None):\n        \"\"\"Provides a Hierarichial structure to a ReSTDocument\n\n        You can write to it similiar to as you can to a ReSTDocument but\n        has an innate structure for more orginaztion and abstraction.\n\n        :param name: The name of the document\n        :param section_names: A list of sections to be included\n            in the document.\n        :param target: The target documentation of the Document structure\n        :param context: A dictionary of data to store with the strucuture. These\n            are only stored per section not the entire structure.\n        \"\"\"\n        super().__init__(target=target)\n        self._name = name\n        self._structure = OrderedDict()\n        self._path = [self._name]\n        self._context = {}\n        if context is not None:\n            self._context = context\n        if section_names is not None:\n            self._generate_structure(section_names)\n\n    @property\n    def name(self):\n        \"\"\"The name of the document structure\"\"\"\n        return self._name\n\n    @property\n    def path(self):\n        \"\"\"\n        A list of where to find a particular document structure in the\n        overlying document structure.\n        \"\"\"\n        return self._path\n\n    @path.setter\n    def path(self, value):\n        self._path = value\n\n    @property\n    def available_sections(self):\n        return list(self._structure)\n\n    @property\n    def context(self):\n        return self._context\n\n    def _generate_structure(self, section_names):\n        for section_name in section_names:\n            self.add_new_section(section_name)\n\n    def add_new_section(self, name, context=None):\n        \"\"\"Adds a new section to the current document structure\n\n        This document structure will be considered a section to the\n        current document structure but will in itself be an entirely\n        new document structure that can be written to and have sections\n        as well\n\n        :param name: The name of the section.\n        :param context: A dictionary of data to store with the strucuture. These\n            are only stored per section not the entire structure.\n        :rtype: DocumentStructure\n        :returns: A new document structure to add to but lives as a section\n            to the document structure it was instantiated from.\n        \"\"\"\n        # Add a new section\n        section = self.__class__(\n            name=name, target=self.target, context=context\n        )\n        section.path = self.path + [name]\n        # Indent the section apporpriately as well\n        section.style.indentation = self.style.indentation\n        section.translation_map = self.translation_map\n        section.hrefs = self.hrefs\n        self._structure[name] = section\n        return section\n\n    def get_section(self, name):\n        \"\"\"Retrieve a section\"\"\"\n        return self._structure[name]\n\n    def delete_section(self, name):\n        \"\"\"Delete a section\"\"\"\n        del self._structure[name]\n\n    def flush_structure(self, docs_link=None):\n        \"\"\"Flushes a doc structure to a ReSTructed string\n\n        The document is flushed out in a DFS style where sections and their\n        subsections' values are added to the string as they are visited.\n        \"\"\"\n        # We are at the root flush the links at the beginning of the\n        # document\n        path_length = len(self.path)\n        if path_length == 1:\n            if self.hrefs:\n                self.style.new_paragraph()\n                for refname, link in self.hrefs.items():\n                    self.style.link_target_definition(refname, link)\n        # Clear docs_link at the correct depth to prevent passing a non-related link.\n        elif path_length == SECTION_METHOD_PATH_DEPTH.get(self.path[1]):\n            docs_link = None\n        value = self.getvalue()\n        for name, section in self._structure.items():\n            # Checks is the AWS API Documentation link has been generated.\n            # If it has been generated, it gets passed as a the doc_link parameter.\n            match = DOCUMENTATION_LINK_REGEX.search(value.decode())\n            docs_link = (\n                f'{match.group(0)}\\n\\n'.encode() if match else docs_link\n            )\n            value += section.flush_structure(docs_link)\n\n        # Replace response/request sections if the line number exceeds our limit.\n        # The section is replaced with a message linking to AWS API Documentation.\n        line_count = len(value.splitlines())\n        section_config = SECTION_LINE_LIMIT_CONFIG.get(self.name)\n        aws_docs_link = (\n            docs_link.decode()\n            if docs_link is not None\n            else DEFAULT_AWS_DOCS_LINK\n        )\n        if section_config and line_count > section_config['line_limit']:\n            value = LARGE_SECTION_MESSAGE.format(\n                section_config['name'], aws_docs_link\n            ).encode()\n        return value\n\n    def getvalue(self):\n        return ''.join(self._writes).encode('utf-8')\n\n    def remove_all_sections(self):\n        self._structure = OrderedDict()\n\n    def clear_text(self):\n        self._writes = []\n\n    def add_title_section(self, title):\n        title_section = self.add_new_section('title')\n        title_section.style.h1(title)\n        return title_section\n\n    def write_to_file(self, full_path, file_name):\n        if not os.path.exists(full_path):\n            os.makedirs(full_path)\n        sub_resource_file_path = os.path.join(full_path, f'{file_name}.rst')\n        with open(sub_resource_file_path, 'wb') as f:\n            f.write(self.flush_structure())\n", "botocore/docs/bcdoc/docstringparser.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom html.parser import HTMLParser\nfrom itertools import zip_longest\n\nPRIORITY_PARENT_TAGS = ('code', 'a')\nOMIT_NESTED_TAGS = ('span', 'i', 'code', 'a')\nOMIT_SELF_TAGS = ('i', 'b')\nHTML_BLOCK_DISPLAY_TAGS = ('p', 'note', 'ul', 'li')\n\n\nclass DocStringParser(HTMLParser):\n    \"\"\"\n    A simple HTML parser.  Focused on converting the subset of HTML\n    that appears in the documentation strings of the JSON models into\n    simple ReST format.\n    \"\"\"\n\n    def __init__(self, doc):\n        self.tree = None\n        self.doc = doc\n        super().__init__()\n\n    def reset(self):\n        HTMLParser.reset(self)\n        self.tree = HTMLTree(self.doc)\n\n    def feed(self, data):\n        super().feed(data)\n        self.tree.write()\n        self.tree = HTMLTree(self.doc)\n\n    def close(self):\n        super().close()\n        # Write if there is anything remaining.\n        self.tree.write()\n        self.tree = HTMLTree(self.doc)\n\n    def handle_starttag(self, tag, attrs):\n        self.tree.add_tag(tag, attrs=attrs)\n\n    def handle_endtag(self, tag):\n        self.tree.add_tag(tag, is_start=False)\n\n    def handle_data(self, data):\n        self.tree.add_data(data)\n\n\nclass HTMLTree:\n    \"\"\"\n    A tree which handles HTML nodes. Designed to work with a python HTML parser,\n    meaning that the current_node will be the most recently opened tag. When\n    a tag is closed, the current_node moves up to the parent node.\n    \"\"\"\n\n    def __init__(self, doc):\n        self.doc = doc\n        self.head = StemNode()\n        self.current_node = self.head\n        self.unhandled_tags = []\n\n    def add_tag(self, tag, attrs=None, is_start=True):\n        if not self._doc_has_handler(tag, is_start):\n            self.unhandled_tags.append(tag)\n            return\n\n        if is_start:\n            node = TagNode(tag, attrs)\n            self.current_node.add_child(node)\n            self.current_node = node\n        else:\n            self.current_node = self.current_node.parent\n\n    def _doc_has_handler(self, tag, is_start):\n        if is_start:\n            handler_name = 'start_%s' % tag\n        else:\n            handler_name = 'end_%s' % tag\n\n        return hasattr(self.doc.style, handler_name)\n\n    def add_data(self, data):\n        self.current_node.add_child(DataNode(data))\n\n    def write(self):\n        self.head.write(self.doc)\n\n\nclass Node:\n    def __init__(self, parent=None):\n        self.parent = parent\n\n    def write(self, doc):\n        raise NotImplementedError\n\n\nclass StemNode(Node):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.children = []\n\n    def add_child(self, child):\n        child.parent = self\n        self.children.append(child)\n\n    def write(self, doc):\n        self.collapse_whitespace()\n        self._write_children(doc)\n\n    def _write_children(self, doc):\n        for child, next_child in zip_longest(self.children, self.children[1:]):\n            if isinstance(child, TagNode) and next_child is not None:\n                child.write(doc, next_child)\n            else:\n                child.write(doc)\n\n    def is_whitespace(self):\n        return all(child.is_whitespace() for child in self.children)\n\n    def startswith_whitespace(self):\n        return self.children and self.children[0].startswith_whitespace()\n\n    def endswith_whitespace(self):\n        return self.children and self.children[-1].endswith_whitespace()\n\n    def lstrip(self):\n        while self.children and self.children[0].is_whitespace():\n            self.children = self.children[1:]\n        if self.children:\n            self.children[0].lstrip()\n\n    def rstrip(self):\n        while self.children and self.children[-1].is_whitespace():\n            self.children = self.children[:-1]\n        if self.children:\n            self.children[-1].rstrip()\n\n    def collapse_whitespace(self):\n        \"\"\"Remove collapsible white-space from HTML.\n\n        HTML in docstrings often contains extraneous white-space around tags,\n        for readability. Browsers would collapse this white-space before\n        rendering. If not removed before conversion to RST where white-space is\n        part of the syntax, for example for indentation, it can result in\n        incorrect output.\n        \"\"\"\n        self.lstrip()\n        self.rstrip()\n        for child in self.children:\n            child.collapse_whitespace()\n\n\nclass TagNode(StemNode):\n    \"\"\"\n    A generic Tag node. It will verify that handlers exist before writing.\n    \"\"\"\n\n    def __init__(self, tag, attrs=None, parent=None):\n        super().__init__(parent)\n        self.attrs = attrs\n        self.tag = tag\n\n    def _has_nested_tags(self):\n        # Returns True if any children are TagNodes and False otherwise.\n        return any(isinstance(child, TagNode) for child in self.children)\n\n    def write(self, doc, next_child=None):\n        prioritize_nested_tags = (\n            self.tag in OMIT_SELF_TAGS and self._has_nested_tags()\n        )\n        prioritize_parent_tag = (\n            isinstance(self.parent, TagNode)\n            and self.parent.tag in PRIORITY_PARENT_TAGS\n            and self.tag in OMIT_NESTED_TAGS\n        )\n        if prioritize_nested_tags or prioritize_parent_tag:\n            self._write_children(doc)\n            return\n\n        self._write_start(doc)\n        self._write_children(doc)\n        self._write_end(doc, next_child)\n\n    def collapse_whitespace(self):\n        \"\"\"Remove collapsible white-space.\n\n        All tags collapse internal whitespace. Block-display HTML tags also\n        strip all leading and trailing whitespace.\n\n        Approximately follows the specification used in browsers:\n        https://www.w3.org/TR/css-text-3/#white-space-rules\n        https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Whitespace\n        \"\"\"\n        if self.tag in HTML_BLOCK_DISPLAY_TAGS:\n            self.lstrip()\n            self.rstrip()\n        # Collapse whitespace in situations like ``</b> <i> foo</i>`` into\n        # ``</b><i> foo</i>``.\n        for prev, cur in zip(self.children[:-1], self.children[1:]):\n            if (\n                isinstance(prev, DataNode)\n                and prev.endswith_whitespace()\n                and cur.startswith_whitespace()\n            ):\n                cur.lstrip()\n        # Same logic, but for situations like ``<b>bar </b> <i>``:\n        for cur, nxt in zip(self.children[:-1], self.children[1:]):\n            if (\n                isinstance(nxt, DataNode)\n                and cur.endswith_whitespace()\n                and nxt.startswith_whitespace()\n            ):\n                cur.rstrip()\n        # Recurse into children\n        for child in self.children:\n            child.collapse_whitespace()\n\n    def _write_start(self, doc):\n        handler_name = 'start_%s' % self.tag\n        if hasattr(doc.style, handler_name):\n            getattr(doc.style, handler_name)(self.attrs)\n\n    def _write_end(self, doc, next_child):\n        handler_name = 'end_%s' % self.tag\n        if hasattr(doc.style, handler_name):\n            if handler_name == 'end_a':\n                # We use lookahead to determine if a space is needed after a link node\n                getattr(doc.style, handler_name)(next_child)\n            else:\n                getattr(doc.style, handler_name)()\n\n\nclass DataNode(Node):\n    \"\"\"\n    A Node that contains only string data.\n    \"\"\"\n\n    def __init__(self, data, parent=None):\n        super().__init__(parent)\n        if not isinstance(data, str):\n            raise ValueError(\"Expecting string type, %s given.\" % type(data))\n        self._leading_whitespace = ''\n        self._trailing_whitespace = ''\n        self._stripped_data = ''\n        if data == '':\n            return\n        if data.isspace():\n            self._trailing_whitespace = data\n            return\n        first_non_space = next(\n            idx for idx, ch in enumerate(data) if not ch.isspace()\n        )\n        last_non_space = len(data) - next(\n            idx for idx, ch in enumerate(reversed(data)) if not ch.isspace()\n        )\n        self._leading_whitespace = data[:first_non_space]\n        self._trailing_whitespace = data[last_non_space:]\n        self._stripped_data = data[first_non_space:last_non_space]\n\n    @property\n    def data(self):\n        return (\n            f'{self._leading_whitespace}{self._stripped_data}'\n            f'{self._trailing_whitespace}'\n        )\n\n    def is_whitespace(self):\n        return self._stripped_data == '' and (\n            self._leading_whitespace != '' or self._trailing_whitespace != ''\n        )\n\n    def startswith_whitespace(self):\n        return self._leading_whitespace != '' or (\n            self._stripped_data == '' and self._trailing_whitespace != ''\n        )\n\n    def endswith_whitespace(self):\n        return self._trailing_whitespace != '' or (\n            self._stripped_data == '' and self._leading_whitespace != ''\n        )\n\n    def lstrip(self):\n        if self._leading_whitespace != '':\n            self._leading_whitespace = ''\n        elif self._stripped_data == '':\n            self.rstrip()\n\n    def rstrip(self):\n        if self._trailing_whitespace != '':\n            self._trailing_whitespace = ''\n        elif self._stripped_data == '':\n            self.lstrip()\n\n    def collapse_whitespace(self):\n        \"\"\"Noop, ``DataNode.write`` always collapses whitespace\"\"\"\n        return\n\n    def write(self, doc):\n        words = doc.translate_words(self._stripped_data.split())\n        str_data = (\n            f'{self._leading_whitespace}{\" \".join(words)}'\n            f'{self._trailing_whitespace}'\n        )\n        if str_data != '':\n            doc.handle_data(str_data)\n", "botocore/docs/bcdoc/style.py": "# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\n\nlogger = logging.getLogger('bcdocs')\n# Terminal punctuation where a space is not needed before.\nPUNCTUATION_CHARACTERS = ('.', ',', '?', '!', ':', ';')\n\n\nclass BaseStyle:\n    def __init__(self, doc, indent_width=2):\n        self.doc = doc\n        self.indent_width = indent_width\n        self._indent = 0\n        self.keep_data = True\n\n    @property\n    def indentation(self):\n        return self._indent\n\n    @indentation.setter\n    def indentation(self, value):\n        self._indent = value\n\n    def new_paragraph(self):\n        return '\\n%s' % self.spaces()\n\n    def indent(self):\n        self._indent += 1\n\n    def dedent(self):\n        if self._indent > 0:\n            self._indent -= 1\n\n    def spaces(self):\n        return ' ' * (self._indent * self.indent_width)\n\n    def bold(self, s):\n        return s\n\n    def ref(self, link, title=None):\n        return link\n\n    def h2(self, s):\n        return s\n\n    def h3(self, s):\n        return s\n\n    def underline(self, s):\n        return s\n\n    def italics(self, s):\n        return s\n\n    def add_trailing_space_to_previous_write(self):\n        # Adds a trailing space if none exists. This is mainly used for\n        # ensuring inline code and links are separated from surrounding text.\n        last_write = self.doc.pop_write()\n        if last_write is None:\n            last_write = ''\n        if last_write != '' and last_write[-1] != ' ':\n            last_write += ' '\n        self.doc.push_write(last_write)\n\n\nclass ReSTStyle(BaseStyle):\n    def __init__(self, doc, indent_width=2):\n        BaseStyle.__init__(self, doc, indent_width)\n        self.do_p = True\n        self.a_href = None\n        self.list_depth = 0\n\n    def new_paragraph(self):\n        self.doc.write('\\n\\n%s' % self.spaces())\n\n    def new_line(self):\n        self.doc.write('\\n%s' % self.spaces())\n\n    def _start_inline(self, markup):\n        # Insert space between any directly adjacent bold and italic inlines to\n        # avoid situations like ``**abc***def*``.\n        try:\n            last_write = self.doc.peek_write()\n        except IndexError:\n            pass\n        else:\n            if last_write in ('*', '**') and markup in ('*', '**'):\n                self.doc.write(' ')\n        self.doc.write(markup)\n\n    def _end_inline(self, markup):\n        # Remove empty and self-closing tags like ``<b></b>`` and ``<b/>``.\n        # If we simply translate that directly then we end up with something\n        # like ****, which rst will assume is a heading instead of an empty\n        # bold.\n        last_write = self.doc.pop_write()\n        if last_write == markup:\n            return\n        self.doc.push_write(last_write)\n        self.doc.write(markup)\n\n    def start_bold(self, attrs=None):\n        self._start_inline('**')\n\n    def end_bold(self):\n        self._end_inline('**')\n\n    def start_b(self, attrs=None):\n        self.doc.do_translation = True\n        self.start_bold(attrs)\n\n    def end_b(self):\n        self.doc.do_translation = False\n        self.end_bold()\n\n    def bold(self, s):\n        if s:\n            self.start_bold()\n            self.doc.write(s)\n            self.end_bold()\n\n    def ref(self, title, link=None):\n        if link is None:\n            link = title\n        self.doc.write(f':doc:`{title} <{link}>`')\n\n    def _heading(self, s, border_char):\n        border = border_char * len(s)\n        self.new_paragraph()\n        self.doc.write(f'{border}\\n{s}\\n{border}')\n        self.new_paragraph()\n\n    def h1(self, s):\n        self._heading(s, '*')\n\n    def h2(self, s):\n        self._heading(s, '=')\n\n    def h3(self, s):\n        self._heading(s, '-')\n\n    def start_italics(self, attrs=None):\n        self._start_inline('*')\n\n    def end_italics(self):\n        self._end_inline('*')\n\n    def italics(self, s):\n        if s:\n            self.start_italics()\n            self.doc.write(s)\n            self.end_italics()\n\n    def start_p(self, attrs=None):\n        if self.do_p:\n            self.doc.write('\\n\\n%s' % self.spaces())\n\n    def end_p(self):\n        if self.do_p:\n            self.doc.write('\\n\\n%s' % self.spaces())\n\n    def start_code(self, attrs=None):\n        self.doc.do_translation = True\n        self.add_trailing_space_to_previous_write()\n        self._start_inline('``')\n\n    def end_code(self):\n        self.doc.do_translation = False\n        self._end_inline('``')\n\n    def code(self, s):\n        if s:\n            self.start_code()\n            self.doc.write(s)\n            self.end_code()\n\n    def start_note(self, attrs=None):\n        self.new_paragraph()\n        self.doc.write('.. note::')\n        self.indent()\n        self.new_paragraph()\n\n    def end_note(self):\n        self.dedent()\n        self.new_paragraph()\n\n    def start_important(self, attrs=None):\n        self.new_paragraph()\n        self.doc.write('.. warning::')\n        self.indent()\n        self.new_paragraph()\n\n    def end_important(self):\n        self.dedent()\n        self.new_paragraph()\n\n    def start_danger(self, attrs=None):\n        self.new_paragraph()\n        self.doc.write('.. danger::')\n        self.indent()\n        self.new_paragraph()\n\n    def end_danger(self):\n        self.dedent()\n        self.new_paragraph()\n\n    def start_a(self, attrs=None):\n        # Write an empty space to guard against zero whitespace\n        # before an \"a\" tag. Example: hi<a>Example</a>\n        self.add_trailing_space_to_previous_write()\n        if attrs:\n            for attr_key, attr_value in attrs:\n                if attr_key == 'href':\n                    # Removes unnecessary whitespace around the href link.\n                    # Example: <a href=\" http://example.com \">Example</a>\n                    self.a_href = attr_value.strip()\n                    self.doc.write('`')\n        else:\n            # There are some model documentation that\n            # looks like this: <a>DescribeInstances</a>.\n            # In this case we just write out an empty\n            # string.\n            self.doc.write(' ')\n        self.doc.do_translation = True\n\n    def link_target_definition(self, refname, link):\n        self.doc.writeln(f'.. _{refname}: {link}')\n\n    def sphinx_reference_label(self, label, text=None):\n        if text is None:\n            text = label\n        if self.doc.target == 'html':\n            self.doc.write(f':ref:`{text} <{label}>`')\n        else:\n            self.doc.write(text)\n\n    def _clean_link_text(self):\n        doc = self.doc\n        # Pop till we reach the link start character to retrieve link text.\n        last_write = doc.pop_write()\n        while not last_write.startswith('`'):\n            last_write = doc.pop_write() + last_write\n        if last_write != '':\n            # Remove whitespace from the start of link text.\n            if last_write.startswith('` '):\n                last_write = f'`{last_write[1:].lstrip(\" \")}'\n            doc.push_write(last_write)\n\n    def end_a(self, next_child=None):\n        self.doc.do_translation = False\n        if self.a_href:\n            self._clean_link_text()\n            last_write = self.doc.pop_write()\n            last_write = last_write.rstrip(' ')\n            if last_write and last_write != '`':\n                if ':' in last_write:\n                    last_write = last_write.replace(':', r'\\:')\n                self.doc.push_write(last_write)\n                self.doc.push_write(' <%s>`__' % self.a_href)\n            elif last_write == '`':\n                # Look at start_a().  It will do a self.doc.write('`')\n                # which is the start of the link title.  If that is the\n                # case then there was no link text.  We should just\n                # use an inline link.  The syntax of this is\n                # `<http://url>`_\n                self.doc.push_write('`<%s>`__' % self.a_href)\n            else:\n                self.doc.push_write(self.a_href)\n                self.doc.hrefs[self.a_href] = self.a_href\n                self.doc.write('`__')\n            self.a_href = None\n\n    def start_i(self, attrs=None):\n        self.doc.do_translation = True\n        self.start_italics()\n\n    def end_i(self):\n        self.doc.do_translation = False\n        self.end_italics()\n\n    def start_li(self, attrs=None):\n        self.new_line()\n        self.do_p = False\n        self.doc.write('* ')\n\n    def end_li(self):\n        self.do_p = True\n        self.new_line()\n\n    def li(self, s):\n        if s:\n            self.start_li()\n            self.doc.writeln(s)\n            self.end_li()\n\n    def start_ul(self, attrs=None):\n        if self.list_depth != 0:\n            self.indent()\n        self.list_depth += 1\n        self.new_paragraph()\n\n    def end_ul(self):\n        self.list_depth -= 1\n        if self.list_depth != 0:\n            self.dedent()\n        self.new_paragraph()\n\n    def start_ol(self, attrs=None):\n        # TODO: Need to control the bullets used for LI items\n        if self.list_depth != 0:\n            self.indent()\n        self.list_depth += 1\n        self.new_paragraph()\n\n    def end_ol(self):\n        self.list_depth -= 1\n        if self.list_depth != 0:\n            self.dedent()\n        self.new_paragraph()\n\n    def start_examples(self, attrs=None):\n        self.doc.keep_data = False\n\n    def end_examples(self):\n        self.doc.keep_data = True\n\n    def start_fullname(self, attrs=None):\n        self.doc.keep_data = False\n\n    def end_fullname(self):\n        self.doc.keep_data = True\n\n    def start_codeblock(self, attrs=None):\n        self.doc.write('::')\n        self.indent()\n        self.new_paragraph()\n\n    def end_codeblock(self):\n        self.dedent()\n        self.new_paragraph()\n\n    def codeblock(self, code):\n        \"\"\"\n        Literal code blocks are introduced by ending a paragraph with\n        the special marker ::.  The literal block must be indented\n        (and, like all paragraphs, separated from the surrounding\n        ones by blank lines).\n        \"\"\"\n        self.start_codeblock()\n        self.doc.writeln(code)\n        self.end_codeblock()\n\n    def toctree(self):\n        if self.doc.target == 'html':\n            self.doc.write('\\n.. toctree::\\n')\n            self.doc.write('  :maxdepth: 1\\n')\n            self.doc.write('  :titlesonly:\\n\\n')\n        else:\n            self.start_ul()\n\n    def tocitem(self, item, file_name=None):\n        if self.doc.target == 'man':\n            self.li(item)\n        else:\n            if file_name:\n                self.doc.writeln('  %s' % file_name)\n            else:\n                self.doc.writeln('  %s' % item)\n\n    def hidden_toctree(self):\n        if self.doc.target == 'html':\n            self.doc.write('\\n.. toctree::\\n')\n            self.doc.write('  :maxdepth: 1\\n')\n            self.doc.write('  :hidden:\\n\\n')\n\n    def hidden_tocitem(self, item):\n        if self.doc.target == 'html':\n            self.tocitem(item)\n\n    def table_of_contents(self, title=None, depth=None):\n        self.doc.write('.. contents:: ')\n        if title is not None:\n            self.doc.writeln(title)\n        if depth is not None:\n            self.doc.writeln('   :depth: %s' % depth)\n\n    def start_sphinx_py_class(self, class_name):\n        self.new_paragraph()\n        self.doc.write('.. py:class:: %s' % class_name)\n        self.indent()\n        self.new_paragraph()\n\n    def end_sphinx_py_class(self):\n        self.dedent()\n        self.new_paragraph()\n\n    def start_sphinx_py_method(self, method_name, parameters=None):\n        self.new_paragraph()\n        content = '.. py:method:: %s' % method_name\n        if parameters is not None:\n            content += '(%s)' % parameters\n        self.doc.write(content)\n        self.indent()\n        self.new_paragraph()\n\n    def end_sphinx_py_method(self):\n        self.dedent()\n        self.new_paragraph()\n\n    def start_sphinx_py_attr(self, attr_name):\n        self.new_paragraph()\n        self.doc.write('.. py:attribute:: %s' % attr_name)\n        self.indent()\n        self.new_paragraph()\n\n    def end_sphinx_py_attr(self):\n        self.dedent()\n        self.new_paragraph()\n\n    def write_py_doc_string(self, docstring):\n        docstring_lines = docstring.splitlines()\n        for docstring_line in docstring_lines:\n            self.doc.writeln(docstring_line)\n\n    def external_link(self, title, link):\n        if self.doc.target == 'html':\n            self.doc.write(f'`{title} <{link}>`_')\n        else:\n            self.doc.write(title)\n\n    def internal_link(self, title, page):\n        if self.doc.target == 'html':\n            self.doc.write(f':doc:`{title} <{page}>`')\n        else:\n            self.doc.write(title)\n", "botocore/docs/bcdoc/__init__.py": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n__version__ = '0.16.0'\n", "botocore/crt/auth.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport datetime\nfrom io import BytesIO\n\nfrom botocore.auth import (\n    SIGNED_HEADERS_BLACKLIST,\n    STREAMING_UNSIGNED_PAYLOAD_TRAILER,\n    UNSIGNED_PAYLOAD,\n    BaseSigner,\n    _get_body_as_dict,\n    _host_from_url,\n)\nfrom botocore.compat import HTTPHeaders, awscrt, parse_qs, urlsplit, urlunsplit\nfrom botocore.exceptions import NoCredentialsError\nfrom botocore.utils import percent_encode_sequence\n\n\nclass CrtSigV4Auth(BaseSigner):\n    REQUIRES_REGION = True\n    _PRESIGNED_HEADERS_BLOCKLIST = [\n        'Authorization',\n        'X-Amz-Date',\n        'X-Amz-Content-SHA256',\n        'X-Amz-Security-Token',\n    ]\n    _SIGNATURE_TYPE = awscrt.auth.AwsSignatureType.HTTP_REQUEST_HEADERS\n    _USE_DOUBLE_URI_ENCODE = True\n    _SHOULD_NORMALIZE_URI_PATH = True\n\n    def __init__(self, credentials, service_name, region_name):\n        self.credentials = credentials\n        self._service_name = service_name\n        self._region_name = region_name\n        self._expiration_in_seconds = None\n\n    def _is_streaming_checksum_payload(self, request):\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        return isinstance(algorithm, dict) and algorithm.get('in') == 'trailer'\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError()\n\n        # Use utcnow() because that's what gets mocked by tests, but set\n        # timezone because CRT assumes naive datetime is local time.\n        datetime_now = datetime.datetime.utcnow().replace(\n            tzinfo=datetime.timezone.utc\n        )\n\n        # Use existing 'X-Amz-Content-SHA256' header if able\n        existing_sha256 = self._get_existing_sha256(request)\n\n        self._modify_request_before_signing(request)\n\n        credentials_provider = awscrt.auth.AwsCredentialsProvider.new_static(\n            access_key_id=self.credentials.access_key,\n            secret_access_key=self.credentials.secret_key,\n            session_token=self.credentials.token,\n        )\n\n        if self._is_streaming_checksum_payload(request):\n            explicit_payload = STREAMING_UNSIGNED_PAYLOAD_TRAILER\n        elif self._should_sha256_sign_payload(request):\n            if existing_sha256:\n                explicit_payload = existing_sha256\n            else:\n                explicit_payload = None  # to be calculated during signing\n        else:\n            explicit_payload = UNSIGNED_PAYLOAD\n\n        if self._should_add_content_sha256_header(explicit_payload):\n            body_header = (\n                awscrt.auth.AwsSignedBodyHeaderType.X_AMZ_CONTENT_SHA_256\n            )\n        else:\n            body_header = awscrt.auth.AwsSignedBodyHeaderType.NONE\n\n        signing_config = awscrt.auth.AwsSigningConfig(\n            algorithm=awscrt.auth.AwsSigningAlgorithm.V4,\n            signature_type=self._SIGNATURE_TYPE,\n            credentials_provider=credentials_provider,\n            region=self._region_name,\n            service=self._service_name,\n            date=datetime_now,\n            should_sign_header=self._should_sign_header,\n            use_double_uri_encode=self._USE_DOUBLE_URI_ENCODE,\n            should_normalize_uri_path=self._SHOULD_NORMALIZE_URI_PATH,\n            signed_body_value=explicit_payload,\n            signed_body_header_type=body_header,\n            expiration_in_seconds=self._expiration_in_seconds,\n        )\n        crt_request = self._crt_request_from_aws_request(request)\n        future = awscrt.auth.aws_sign_request(crt_request, signing_config)\n        future.result()\n        self._apply_signing_changes(request, crt_request)\n\n    def _crt_request_from_aws_request(self, aws_request):\n        url_parts = urlsplit(aws_request.url)\n        crt_path = url_parts.path if url_parts.path else '/'\n        if aws_request.params:\n            array = []\n            for param, value in aws_request.params.items():\n                value = str(value)\n                array.append(f'{param}={value}')\n            crt_path = crt_path + '?' + '&'.join(array)\n        elif url_parts.query:\n            crt_path = f'{crt_path}?{url_parts.query}'\n\n        crt_headers = awscrt.http.HttpHeaders(aws_request.headers.items())\n\n        # CRT requires body (if it exists) to be an I/O stream.\n        crt_body_stream = None\n        if aws_request.body:\n            if hasattr(aws_request.body, 'seek'):\n                crt_body_stream = aws_request.body\n            else:\n                crt_body_stream = BytesIO(aws_request.body)\n\n        crt_request = awscrt.http.HttpRequest(\n            method=aws_request.method,\n            path=crt_path,\n            headers=crt_headers,\n            body_stream=crt_body_stream,\n        )\n        return crt_request\n\n    def _apply_signing_changes(self, aws_request, signed_crt_request):\n        # Apply changes from signed CRT request to the AWSRequest\n        aws_request.headers = HTTPHeaders.from_pairs(\n            list(signed_crt_request.headers)\n        )\n\n    def _should_sign_header(self, name, **kwargs):\n        return name.lower() not in SIGNED_HEADERS_BLACKLIST\n\n    def _modify_request_before_signing(self, request):\n        # This could be a retry. Make sure the previous\n        # authorization headers are removed first.\n        for h in self._PRESIGNED_HEADERS_BLOCKLIST:\n            if h in request.headers:\n                del request.headers[h]\n        # If necessary, add the host header\n        if 'host' not in request.headers:\n            request.headers['host'] = _host_from_url(request.url)\n\n    def _get_existing_sha256(self, request):\n        return request.headers.get('X-Amz-Content-SHA256')\n\n    def _should_sha256_sign_payload(self, request):\n        # Payloads will always be signed over insecure connections.\n        if not request.url.startswith('https'):\n            return True\n\n        # Certain operations may have payload signing disabled by default.\n        # Since we don't have access to the operation model, we pass in this\n        # bit of metadata through the request context.\n        return request.context.get('payload_signing_enabled', True)\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # only add X-Amz-Content-SHA256 header if payload is explicitly set\n        return explicit_payload is not None\n\n\nclass CrtS3SigV4Auth(CrtSigV4Auth):\n    # For S3, we do not normalize the path.\n    _USE_DOUBLE_URI_ENCODE = False\n    _SHOULD_NORMALIZE_URI_PATH = False\n\n    def _get_existing_sha256(self, request):\n        # always recalculate\n        return None\n\n    def _should_sha256_sign_payload(self, request):\n        # S3 allows optional body signing, so to minimize the performance\n        # impact, we opt to not SHA256 sign the body on streaming uploads,\n        # provided that we're on https.\n        client_config = request.context.get('client_config')\n        s3_config = getattr(client_config, 's3', None)\n\n        # The config could be None if it isn't set, or if the customer sets it\n        # to None.\n        if s3_config is None:\n            s3_config = {}\n\n        # The explicit configuration takes precedence over any implicit\n        # configuration.\n        sign_payload = s3_config.get('payload_signing_enabled', None)\n        if sign_payload is not None:\n            return sign_payload\n\n        # We require that both a checksum be present and https be enabled\n        # to implicitly disable body signing. The combination of TLS and\n        # a checksum is sufficiently secure and durable for us to be\n        # confident in the request without body signing.\n        checksum_header = 'Content-MD5'\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        if isinstance(algorithm, dict) and algorithm.get('in') == 'header':\n            checksum_header = algorithm['name']\n        if (\n            not request.url.startswith('https')\n            or checksum_header not in request.headers\n        ):\n            return True\n\n        # If the input is streaming we disable body signing by default.\n        if request.context.get('has_streaming_input', False):\n            return False\n\n        # If the S3-specific checks had no results, delegate to the generic\n        # checks.\n        return super()._should_sha256_sign_payload(request)\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # Always add X-Amz-Content-SHA256 header\n        return True\n\n\nclass CrtSigV4AsymAuth(BaseSigner):\n    REQUIRES_REGION = True\n    _PRESIGNED_HEADERS_BLOCKLIST = [\n        'Authorization',\n        'X-Amz-Date',\n        'X-Amz-Content-SHA256',\n        'X-Amz-Security-Token',\n    ]\n    _SIGNATURE_TYPE = awscrt.auth.AwsSignatureType.HTTP_REQUEST_HEADERS\n    _USE_DOUBLE_URI_ENCODE = True\n    _SHOULD_NORMALIZE_URI_PATH = True\n\n    def __init__(self, credentials, service_name, region_name):\n        self.credentials = credentials\n        self._service_name = service_name\n        self._region_name = region_name\n        self._expiration_in_seconds = None\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError()\n\n        # Use utcnow() because that's what gets mocked by tests, but set\n        # timezone because CRT assumes naive datetime is local time.\n        datetime_now = datetime.datetime.utcnow().replace(\n            tzinfo=datetime.timezone.utc\n        )\n\n        # Use existing 'X-Amz-Content-SHA256' header if able\n        existing_sha256 = self._get_existing_sha256(request)\n\n        self._modify_request_before_signing(request)\n\n        credentials_provider = awscrt.auth.AwsCredentialsProvider.new_static(\n            access_key_id=self.credentials.access_key,\n            secret_access_key=self.credentials.secret_key,\n            session_token=self.credentials.token,\n        )\n\n        if self._is_streaming_checksum_payload(request):\n            explicit_payload = STREAMING_UNSIGNED_PAYLOAD_TRAILER\n        elif self._should_sha256_sign_payload(request):\n            if existing_sha256:\n                explicit_payload = existing_sha256\n            else:\n                explicit_payload = None  # to be calculated during signing\n        else:\n            explicit_payload = UNSIGNED_PAYLOAD\n\n        if self._should_add_content_sha256_header(explicit_payload):\n            body_header = (\n                awscrt.auth.AwsSignedBodyHeaderType.X_AMZ_CONTENT_SHA_256\n            )\n        else:\n            body_header = awscrt.auth.AwsSignedBodyHeaderType.NONE\n\n        signing_config = awscrt.auth.AwsSigningConfig(\n            algorithm=awscrt.auth.AwsSigningAlgorithm.V4_ASYMMETRIC,\n            signature_type=self._SIGNATURE_TYPE,\n            credentials_provider=credentials_provider,\n            region=self._region_name,\n            service=self._service_name,\n            date=datetime_now,\n            should_sign_header=self._should_sign_header,\n            use_double_uri_encode=self._USE_DOUBLE_URI_ENCODE,\n            should_normalize_uri_path=self._SHOULD_NORMALIZE_URI_PATH,\n            signed_body_value=explicit_payload,\n            signed_body_header_type=body_header,\n            expiration_in_seconds=self._expiration_in_seconds,\n        )\n        crt_request = self._crt_request_from_aws_request(request)\n        future = awscrt.auth.aws_sign_request(crt_request, signing_config)\n        future.result()\n        self._apply_signing_changes(request, crt_request)\n\n    def _crt_request_from_aws_request(self, aws_request):\n        url_parts = urlsplit(aws_request.url)\n        crt_path = url_parts.path if url_parts.path else '/'\n        if aws_request.params:\n            array = []\n            for param, value in aws_request.params.items():\n                value = str(value)\n                array.append(f'{param}={value}')\n            crt_path = crt_path + '?' + '&'.join(array)\n        elif url_parts.query:\n            crt_path = f'{crt_path}?{url_parts.query}'\n\n        crt_headers = awscrt.http.HttpHeaders(aws_request.headers.items())\n\n        # CRT requires body (if it exists) to be an I/O stream.\n        crt_body_stream = None\n        if aws_request.body:\n            if hasattr(aws_request.body, 'seek'):\n                crt_body_stream = aws_request.body\n            else:\n                crt_body_stream = BytesIO(aws_request.body)\n\n        crt_request = awscrt.http.HttpRequest(\n            method=aws_request.method,\n            path=crt_path,\n            headers=crt_headers,\n            body_stream=crt_body_stream,\n        )\n        return crt_request\n\n    def _apply_signing_changes(self, aws_request, signed_crt_request):\n        # Apply changes from signed CRT request to the AWSRequest\n        aws_request.headers = HTTPHeaders.from_pairs(\n            list(signed_crt_request.headers)\n        )\n\n    def _should_sign_header(self, name, **kwargs):\n        return name.lower() not in SIGNED_HEADERS_BLACKLIST\n\n    def _modify_request_before_signing(self, request):\n        # This could be a retry. Make sure the previous\n        # authorization headers are removed first.\n        for h in self._PRESIGNED_HEADERS_BLOCKLIST:\n            if h in request.headers:\n                del request.headers[h]\n        # If necessary, add the host header\n        if 'host' not in request.headers:\n            request.headers['host'] = _host_from_url(request.url)\n\n    def _get_existing_sha256(self, request):\n        return request.headers.get('X-Amz-Content-SHA256')\n\n    def _is_streaming_checksum_payload(self, request):\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        return isinstance(algorithm, dict) and algorithm.get('in') == 'trailer'\n\n    def _should_sha256_sign_payload(self, request):\n        # Payloads will always be signed over insecure connections.\n        if not request.url.startswith('https'):\n            return True\n\n        # Certain operations may have payload signing disabled by default.\n        # Since we don't have access to the operation model, we pass in this\n        # bit of metadata through the request context.\n        return request.context.get('payload_signing_enabled', True)\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # only add X-Amz-Content-SHA256 header if payload is explicitly set\n        return explicit_payload is not None\n\n\nclass CrtS3SigV4AsymAuth(CrtSigV4AsymAuth):\n    # For S3, we do not normalize the path.\n    _USE_DOUBLE_URI_ENCODE = False\n    _SHOULD_NORMALIZE_URI_PATH = False\n\n    def _get_existing_sha256(self, request):\n        # always recalculate\n        return None\n\n    def _should_sha256_sign_payload(self, request):\n        # S3 allows optional body signing, so to minimize the performance\n        # impact, we opt to not SHA256 sign the body on streaming uploads,\n        # provided that we're on https.\n        client_config = request.context.get('client_config')\n        s3_config = getattr(client_config, 's3', None)\n\n        # The config could be None if it isn't set, or if the customer sets it\n        # to None.\n        if s3_config is None:\n            s3_config = {}\n\n        # The explicit configuration takes precedence over any implicit\n        # configuration.\n        sign_payload = s3_config.get('payload_signing_enabled', None)\n        if sign_payload is not None:\n            return sign_payload\n\n        # We require that both content-md5 be present and https be enabled\n        # to implicitly disable body signing. The combination of TLS and\n        # content-md5 is sufficiently secure and durable for us to be\n        # confident in the request without body signing.\n        if (\n            not request.url.startswith('https')\n            or 'Content-MD5' not in request.headers\n        ):\n            return True\n\n        # If the input is streaming we disable body signing by default.\n        if request.context.get('has_streaming_input', False):\n            return False\n\n        # If the S3-specific checks had no results, delegate to the generic\n        # checks.\n        return super()._should_sha256_sign_payload(request)\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # Always add X-Amz-Content-SHA256 header\n        return True\n\n\nclass CrtSigV4AsymQueryAuth(CrtSigV4AsymAuth):\n    DEFAULT_EXPIRES = 3600\n    _SIGNATURE_TYPE = awscrt.auth.AwsSignatureType.HTTP_REQUEST_QUERY_PARAMS\n\n    def __init__(\n        self, credentials, service_name, region_name, expires=DEFAULT_EXPIRES\n    ):\n        super().__init__(credentials, service_name, region_name)\n        self._expiration_in_seconds = expires\n\n    def _modify_request_before_signing(self, request):\n        super()._modify_request_before_signing(request)\n\n        # We automatically set this header, so if it's the auto-set value we\n        # want to get rid of it since it doesn't make sense for presigned urls.\n        content_type = request.headers.get('content-type')\n        if content_type == 'application/x-www-form-urlencoded; charset=utf-8':\n            del request.headers['content-type']\n\n        # Now parse the original query string to a dict, inject our new query\n        # params, and serialize back to a query string.\n        url_parts = urlsplit(request.url)\n        # parse_qs makes each value a list, but in our case we know we won't\n        # have repeated keys so we know we have single element lists which we\n        # can convert back to scalar values.\n        query_string_parts = parse_qs(url_parts.query, keep_blank_values=True)\n        query_dict = {k: v[0] for k, v in query_string_parts.items()}\n\n        # The spec is particular about this.  It *has* to be:\n        # https://<endpoint>?<operation params>&<auth params>\n        # You can't mix the two types of params together, i.e just keep doing\n        # new_query_params.update(op_params)\n        # new_query_params.update(auth_params)\n        # percent_encode_sequence(new_query_params)\n        if request.data:\n            # We also need to move the body params into the query string. To\n            # do this, we first have to convert it to a dict.\n            query_dict.update(_get_body_as_dict(request))\n            request.data = ''\n        new_query_string = percent_encode_sequence(query_dict)\n        # url_parts is a tuple (and therefore immutable) so we need to create\n        # a new url_parts with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        p = url_parts\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n    def _apply_signing_changes(self, aws_request, signed_crt_request):\n        # Apply changes from signed CRT request to the AWSRequest\n        super()._apply_signing_changes(aws_request, signed_crt_request)\n\n        signed_query = urlsplit(signed_crt_request.path).query\n        p = urlsplit(aws_request.url)\n        # urlsplit() returns a tuple (and therefore immutable) so we\n        # need to create new url with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        aws_request.url = urlunsplit((p[0], p[1], p[2], signed_query, p[4]))\n\n\nclass CrtS3SigV4AsymQueryAuth(CrtSigV4AsymQueryAuth):\n    \"\"\"S3 SigV4A auth using query parameters.\n    This signer will sign a request using query parameters and signature\n    version 4A, i.e a \"presigned url\" signer.\n    \"\"\"\n\n    # For S3, we do not normalize the path.\n    _USE_DOUBLE_URI_ENCODE = False\n    _SHOULD_NORMALIZE_URI_PATH = False\n\n    def _should_sha256_sign_payload(self, request):\n        # From the doc link above:\n        # \"You don't include a payload hash in the Canonical Request, because\n        # when you create a presigned URL, you don't know anything about the\n        # payload. Instead, you use a constant string \"UNSIGNED-PAYLOAD\".\n        return False\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # Never add X-Amz-Content-SHA256 header\n        return False\n\n\nclass CrtSigV4QueryAuth(CrtSigV4Auth):\n    DEFAULT_EXPIRES = 3600\n    _SIGNATURE_TYPE = awscrt.auth.AwsSignatureType.HTTP_REQUEST_QUERY_PARAMS\n\n    def __init__(\n        self, credentials, service_name, region_name, expires=DEFAULT_EXPIRES\n    ):\n        super().__init__(credentials, service_name, region_name)\n        self._expiration_in_seconds = expires\n\n    def _modify_request_before_signing(self, request):\n        super()._modify_request_before_signing(request)\n\n        # We automatically set this header, so if it's the auto-set value we\n        # want to get rid of it since it doesn't make sense for presigned urls.\n        content_type = request.headers.get('content-type')\n        if content_type == 'application/x-www-form-urlencoded; charset=utf-8':\n            del request.headers['content-type']\n\n        # Now parse the original query string to a dict, inject our new query\n        # params, and serialize back to a query string.\n        url_parts = urlsplit(request.url)\n        # parse_qs makes each value a list, but in our case we know we won't\n        # have repeated keys so we know we have single element lists which we\n        # can convert back to scalar values.\n        query_dict = {\n            k: v[0]\n            for k, v in parse_qs(\n                url_parts.query, keep_blank_values=True\n            ).items()\n        }\n        if request.params:\n            query_dict.update(request.params)\n            request.params = {}\n        # The spec is particular about this.  It *has* to be:\n        # https://<endpoint>?<operation params>&<auth params>\n        # You can't mix the two types of params together, i.e just keep doing\n        # new_query_params.update(op_params)\n        # new_query_params.update(auth_params)\n        # percent_encode_sequence(new_query_params)\n        if request.data:\n            # We also need to move the body params into the query string. To\n            # do this, we first have to convert it to a dict.\n            query_dict.update(_get_body_as_dict(request))\n            request.data = ''\n        new_query_string = percent_encode_sequence(query_dict)\n        # url_parts is a tuple (and therefore immutable) so we need to create\n        # a new url_parts with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        p = url_parts\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n    def _apply_signing_changes(self, aws_request, signed_crt_request):\n        # Apply changes from signed CRT request to the AWSRequest\n        super()._apply_signing_changes(aws_request, signed_crt_request)\n\n        signed_query = urlsplit(signed_crt_request.path).query\n        p = urlsplit(aws_request.url)\n        # urlsplit() returns a tuple (and therefore immutable) so we\n        # need to create new url with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        aws_request.url = urlunsplit((p[0], p[1], p[2], signed_query, p[4]))\n\n\nclass CrtS3SigV4QueryAuth(CrtSigV4QueryAuth):\n    \"\"\"S3 SigV4 auth using query parameters.\n    This signer will sign a request using query parameters and signature\n    version 4, i.e a \"presigned url\" signer.\n    Based off of:\n    http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html\n    \"\"\"\n\n    # For S3, we do not normalize the path.\n    _USE_DOUBLE_URI_ENCODE = False\n    _SHOULD_NORMALIZE_URI_PATH = False\n\n    def _should_sha256_sign_payload(self, request):\n        # From the doc link above:\n        # \"You don't include a payload hash in the Canonical Request, because\n        # when you create a presigned URL, you don't know anything about the\n        # payload. Instead, you use a constant string \"UNSIGNED-PAYLOAD\".\n        return False\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # Never add X-Amz-Content-SHA256 header\n        return False\n\n\n# Defined at the bottom of module to ensure all Auth\n# classes are defined.\nCRT_AUTH_TYPE_MAPS = {\n    'v4': CrtSigV4Auth,\n    'v4-query': CrtSigV4QueryAuth,\n    'v4a': CrtSigV4AsymAuth,\n    's3v4': CrtS3SigV4Auth,\n    's3v4-query': CrtS3SigV4QueryAuth,\n    's3v4a': CrtS3SigV4AsymAuth,\n    's3v4a-query': CrtS3SigV4AsymQueryAuth,\n}\n", "botocore/crt/__init__.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n# A list of auth types supported by the signers in botocore/crt/auth.py. This\n# should always match the keys of botocore.crt.auth.CRT_AUTH_TYPE_MAPS. The\n# information is duplicated here so that it can be accessed in environments\n# where `awscrt` is not present and any import from botocore.crt.auth would\n# fail.\nCRT_SUPPORTED_AUTH_TYPES = (\n    'v4',\n    'v4-query',\n    'v4a',\n    's3v4',\n    's3v4-query',\n    's3v4a',\n    's3v4a-query',\n)\n", "botocore/retries/special.py": "\"\"\"Special cased retries.\n\nThese are additional retry cases we still have to handle from the legacy\nretry handler.  They don't make sense as part of the standard mode retry\nmodule.  Ideally we should be able to remove this module.\n\n\"\"\"\nimport logging\nfrom binascii import crc32\n\nfrom botocore.retries.base import BaseRetryableChecker\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: This is an ideal candidate for the retryable trait once that's\n# available.\nclass RetryIDPCommunicationError(BaseRetryableChecker):\n    _SERVICE_NAME = 'sts'\n\n    def is_retryable(self, context):\n        service_name = context.operation_model.service_model.service_name\n        if service_name != self._SERVICE_NAME:\n            return False\n        error_code = context.get_error_code()\n        return error_code == 'IDPCommunicationError'\n\n\nclass RetryDDBChecksumError(BaseRetryableChecker):\n    _CHECKSUM_HEADER = 'x-amz-crc32'\n    _SERVICE_NAME = 'dynamodb'\n\n    def is_retryable(self, context):\n        service_name = context.operation_model.service_model.service_name\n        if service_name != self._SERVICE_NAME:\n            return False\n        if context.http_response is None:\n            return False\n        checksum = context.http_response.headers.get(self._CHECKSUM_HEADER)\n        if checksum is None:\n            return False\n        actual_crc32 = crc32(context.http_response.content) & 0xFFFFFFFF\n        if actual_crc32 != int(checksum):\n            logger.debug(\n                \"DynamoDB crc32 checksum does not match, \"\n                \"expected: %s, actual: %s\",\n                checksum,\n                actual_crc32,\n            )\n            return True\n", "botocore/retries/quota.py": "\"\"\"Retry quota implementation.\n\n\n\"\"\"\nimport threading\n\n\nclass RetryQuota:\n    INITIAL_CAPACITY = 500\n\n    def __init__(self, initial_capacity=INITIAL_CAPACITY, lock=None):\n        self._max_capacity = initial_capacity\n        self._available_capacity = initial_capacity\n        if lock is None:\n            lock = threading.Lock()\n        self._lock = lock\n\n    def acquire(self, capacity_amount):\n        \"\"\"Attempt to aquire a certain amount of capacity.\n\n        If there's not sufficient amount of capacity available, ``False``\n        is returned.  Otherwise, ``True`` is returned, which indicates that\n        capacity was successfully allocated.\n\n        \"\"\"\n        # The acquire() is only called when we encounter a retryable\n        # response so we aren't worried about locking the entire method.\n        with self._lock:\n            if capacity_amount > self._available_capacity:\n                return False\n            self._available_capacity -= capacity_amount\n            return True\n\n    def release(self, capacity_amount):\n        \"\"\"Release capacity back to the retry quota.\n\n        The capacity being released will be truncated if necessary\n        to ensure the max capacity is never exceeded.\n\n        \"\"\"\n        # Implementation note:  The release() method is called as part\n        # of the \"after-call\" event, which means it gets invoked for\n        # every API call.  In the common case where the request is\n        # successful and we're at full capacity, we can avoid locking.\n        # We can't exceed max capacity so there's no work we have to do.\n        if self._max_capacity == self._available_capacity:\n            return\n        with self._lock:\n            amount = min(\n                self._max_capacity - self._available_capacity, capacity_amount\n            )\n            self._available_capacity += amount\n\n    @property\n    def available_capacity(self):\n        return self._available_capacity\n", "botocore/retries/base.py": "class BaseRetryBackoff:\n    def delay_amount(self, context):\n        \"\"\"Calculate how long we should delay before retrying.\n\n        :type context: RetryContext\n\n        \"\"\"\n        raise NotImplementedError(\"delay_amount\")\n\n\nclass BaseRetryableChecker:\n    \"\"\"Base class for determining if a retry should happen.\n\n    This base class checks for specific retryable conditions.\n    A single retryable checker doesn't necessarily indicate a retry\n    will happen.  It's up to the ``RetryPolicy`` to use its\n    ``BaseRetryableCheckers`` to make the final decision on whether a retry\n    should happen.\n    \"\"\"\n\n    def is_retryable(self, context):\n        \"\"\"Returns True if retryable, False if not.\n\n        :type context: RetryContext\n        \"\"\"\n        raise NotImplementedError(\"is_retryable\")\n", "botocore/retries/bucket.py": "\"\"\"This module implements token buckets used for client side throttling.\"\"\"\nimport threading\nimport time\n\nfrom botocore.exceptions import CapacityNotAvailableError\n\n\nclass Clock:\n    def __init__(self):\n        pass\n\n    def sleep(self, amount):\n        time.sleep(amount)\n\n    def current_time(self):\n        return time.time()\n\n\nclass TokenBucket:\n    _MIN_RATE = 0.5\n\n    def __init__(self, max_rate, clock, min_rate=_MIN_RATE):\n        self._fill_rate = None\n        self._max_capacity = None\n        self._current_capacity = 0\n        self._clock = clock\n        self._last_timestamp = None\n        self._min_rate = min_rate\n        self._lock = threading.Lock()\n        self._new_fill_rate_condition = threading.Condition(self._lock)\n        self.max_rate = max_rate\n\n    @property\n    def max_rate(self):\n        return self._fill_rate\n\n    @max_rate.setter\n    def max_rate(self, value):\n        with self._new_fill_rate_condition:\n            # Before we can change the rate we need to fill any pending\n            # tokens we might have based on the current rate.  If we don't\n            # do this it means everything since the last recorded timestamp\n            # will accumulate at the rate we're about to set which isn't\n            # correct.\n            self._refill()\n            self._fill_rate = max(value, self._min_rate)\n            if value >= 1:\n                self._max_capacity = value\n            else:\n                self._max_capacity = 1\n            # If we're scaling down, we also can't have a capacity that's\n            # more than our max_capacity.\n            self._current_capacity = min(\n                self._current_capacity, self._max_capacity\n            )\n            self._new_fill_rate_condition.notify()\n\n    @property\n    def max_capacity(self):\n        return self._max_capacity\n\n    @property\n    def available_capacity(self):\n        return self._current_capacity\n\n    def acquire(self, amount=1, block=True):\n        \"\"\"Acquire token or return amount of time until next token available.\n\n        If block is True, then this method will block until there's sufficient\n        capacity to acquire the desired amount.\n\n        If block is False, then this method will return True is capacity\n        was successfully acquired, False otherwise.\n\n        \"\"\"\n        with self._new_fill_rate_condition:\n            return self._acquire(amount=amount, block=block)\n\n    def _acquire(self, amount, block):\n        self._refill()\n        if amount <= self._current_capacity:\n            self._current_capacity -= amount\n            return True\n        else:\n            if not block:\n                raise CapacityNotAvailableError()\n            # Not enough capacity.\n            sleep_amount = self._sleep_amount(amount)\n            while sleep_amount > 0:\n                # Until python3.2, wait() always returned None so we can't\n                # tell if a timeout occurred waiting on the cond var.\n                # Because of this we'll unconditionally call _refill().\n                # The downside to this is that we were waken up via\n                # a notify(), we're calling unnecessarily calling _refill() an\n                # extra time.\n                self._new_fill_rate_condition.wait(sleep_amount)\n                self._refill()\n                sleep_amount = self._sleep_amount(amount)\n            self._current_capacity -= amount\n            return True\n\n    def _sleep_amount(self, amount):\n        return (amount - self._current_capacity) / self._fill_rate\n\n    def _refill(self):\n        timestamp = self._clock.current_time()\n        if self._last_timestamp is None:\n            self._last_timestamp = timestamp\n            return\n        current_capacity = self._current_capacity\n        fill_amount = (timestamp - self._last_timestamp) * self._fill_rate\n        new_capacity = min(self._max_capacity, current_capacity + fill_amount)\n        self._current_capacity = new_capacity\n        self._last_timestamp = timestamp\n", "botocore/retries/throttling.py": "from collections import namedtuple\n\nCubicParams = namedtuple('CubicParams', ['w_max', 'k', 'last_fail'])\n\n\nclass CubicCalculator:\n    _SCALE_CONSTANT = 0.4\n    _BETA = 0.7\n\n    def __init__(\n        self,\n        starting_max_rate,\n        start_time,\n        scale_constant=_SCALE_CONSTANT,\n        beta=_BETA,\n    ):\n        self._w_max = starting_max_rate\n        self._scale_constant = scale_constant\n        self._beta = beta\n        self._k = self._calculate_zero_point()\n        self._last_fail = start_time\n\n    def _calculate_zero_point(self):\n        scaled_value = (self._w_max * (1 - self._beta)) / self._scale_constant\n        k = scaled_value ** (1 / 3.0)\n        return k\n\n    def success_received(self, timestamp):\n        dt = timestamp - self._last_fail\n        new_rate = self._scale_constant * (dt - self._k) ** 3 + self._w_max\n        return new_rate\n\n    def error_received(self, current_rate, timestamp):\n        # Consider not having this be the current measured rate.\n\n        # We have a new max rate, which is the current rate we were sending\n        # at when we received an error response.\n        self._w_max = current_rate\n        self._k = self._calculate_zero_point()\n        self._last_fail = timestamp\n        return current_rate * self._beta\n\n    def get_params_snapshot(self):\n        \"\"\"Return a read-only object of the current cubic parameters.\n\n        These parameters are intended to be used for debug/troubleshooting\n        purposes.  These object is a read-only snapshot and cannot be used\n        to modify the behavior of the CUBIC calculations.\n\n        New parameters may be added to this object in the future.\n\n        \"\"\"\n        return CubicParams(\n            w_max=self._w_max, k=self._k, last_fail=self._last_fail\n        )\n", "botocore/retries/adaptive.py": "import logging\nimport math\nimport threading\n\nfrom botocore.retries import bucket, standard, throttling\n\nlogger = logging.getLogger(__name__)\n\n\ndef register_retry_handler(client):\n    clock = bucket.Clock()\n    rate_adjustor = throttling.CubicCalculator(\n        starting_max_rate=0, start_time=clock.current_time()\n    )\n    token_bucket = bucket.TokenBucket(max_rate=1, clock=clock)\n    rate_clocker = RateClocker(clock)\n    throttling_detector = standard.ThrottlingErrorDetector(\n        retry_event_adapter=standard.RetryEventAdapter(),\n    )\n    limiter = ClientRateLimiter(\n        rate_adjustor=rate_adjustor,\n        rate_clocker=rate_clocker,\n        token_bucket=token_bucket,\n        throttling_detector=throttling_detector,\n        clock=clock,\n    )\n    client.meta.events.register(\n        'before-send',\n        limiter.on_sending_request,\n    )\n    client.meta.events.register(\n        'needs-retry',\n        limiter.on_receiving_response,\n    )\n    return limiter\n\n\nclass ClientRateLimiter:\n    _MAX_RATE_ADJUST_SCALE = 2.0\n\n    def __init__(\n        self,\n        rate_adjustor,\n        rate_clocker,\n        token_bucket,\n        throttling_detector,\n        clock,\n    ):\n        self._rate_adjustor = rate_adjustor\n        self._rate_clocker = rate_clocker\n        self._token_bucket = token_bucket\n        self._throttling_detector = throttling_detector\n        self._clock = clock\n        self._enabled = False\n        self._lock = threading.Lock()\n\n    def on_sending_request(self, request, **kwargs):\n        if self._enabled:\n            self._token_bucket.acquire()\n\n    # Hooked up to needs-retry.\n    def on_receiving_response(self, **kwargs):\n        measured_rate = self._rate_clocker.record()\n        timestamp = self._clock.current_time()\n        with self._lock:\n            if not self._throttling_detector.is_throttling_error(**kwargs):\n                new_rate = self._rate_adjustor.success_received(timestamp)\n            else:\n                if not self._enabled:\n                    rate_to_use = measured_rate\n                else:\n                    rate_to_use = min(\n                        measured_rate, self._token_bucket.max_rate\n                    )\n                new_rate = self._rate_adjustor.error_received(\n                    rate_to_use, timestamp\n                )\n                logger.debug(\n                    \"Throttling response received, new send rate: %s \"\n                    \"measured rate: %s, token bucket capacity \"\n                    \"available: %s\",\n                    new_rate,\n                    measured_rate,\n                    self._token_bucket.available_capacity,\n                )\n                self._enabled = True\n            self._token_bucket.max_rate = min(\n                new_rate, self._MAX_RATE_ADJUST_SCALE * measured_rate\n            )\n\n\nclass RateClocker:\n    \"\"\"Tracks the rate at which a client is sending a request.\"\"\"\n\n    _DEFAULT_SMOOTHING = 0.8\n    # Update the rate every _TIME_BUCKET_RANGE seconds.\n    _TIME_BUCKET_RANGE = 0.5\n\n    def __init__(\n        self,\n        clock,\n        smoothing=_DEFAULT_SMOOTHING,\n        time_bucket_range=_TIME_BUCKET_RANGE,\n    ):\n        self._clock = clock\n        self._measured_rate = 0\n        self._smoothing = smoothing\n        self._last_bucket = math.floor(self._clock.current_time())\n        self._time_bucket_scale = 1 / self._TIME_BUCKET_RANGE\n        self._count = 0\n        self._lock = threading.Lock()\n\n    def record(self, amount=1):\n        with self._lock:\n            t = self._clock.current_time()\n            bucket = (\n                math.floor(t * self._time_bucket_scale)\n                / self._time_bucket_scale\n            )\n            self._count += amount\n            if bucket > self._last_bucket:\n                current_rate = self._count / float(bucket - self._last_bucket)\n                self._measured_rate = (current_rate * self._smoothing) + (\n                    self._measured_rate * (1 - self._smoothing)\n                )\n                self._count = 0\n                self._last_bucket = bucket\n            return self._measured_rate\n\n    @property\n    def measured_rate(self):\n        return self._measured_rate\n", "botocore/retries/__init__.py": "\"\"\"New retry v2 handlers.\n\nThis package obsoletes the botocore/retryhandler.py module and contains\nnew retry logic.\n\n\"\"\"\n", "botocore/retries/standard.py": "\"\"\"Standard retry behavior.\n\nThis contains the default standard retry behavior.\nIt provides consistent behavior with other AWS SDKs.\n\nThe key base classes uses for retries:\n\n    * ``BaseRetryableChecker`` - Use to check a specific condition that\n    indicates a retry should happen.  This can include things like\n    max attempts, HTTP status code checks, error code checks etc.\n    * ``RetryBackoff`` - Use to determine how long we should backoff until\n    we retry a request.  This is the class that will implement delay such\n    as exponential backoff.\n    * ``RetryPolicy`` - Main class that determines if a retry should\n    happen.  It can combine data from a various BaseRetryableCheckers\n    to make a final call as to whether or not a retry should happen.\n    It then uses a ``BaseRetryBackoff`` to determine how long to delay.\n    * ``RetryHandler`` - The bridge between botocore's event system\n    used by endpoint.py to manage retries and the interfaces defined\n    in this module.\n\nThis allows us to define an API that has minimal coupling to the event\nbased API used by botocore.\n\n\"\"\"\nimport logging\nimport random\n\nfrom botocore.exceptions import (\n    ConnectionError,\n    ConnectTimeoutError,\n    HTTPClientError,\n    ReadTimeoutError,\n)\nfrom botocore.retries import quota, special\nfrom botocore.retries.base import BaseRetryableChecker, BaseRetryBackoff\n\nDEFAULT_MAX_ATTEMPTS = 3\nlogger = logging.getLogger(__name__)\n\n\ndef register_retry_handler(client, max_attempts=DEFAULT_MAX_ATTEMPTS):\n    retry_quota = RetryQuotaChecker(quota.RetryQuota())\n\n    service_id = client.meta.service_model.service_id\n    service_event_name = service_id.hyphenize()\n    client.meta.events.register(\n        f'after-call.{service_event_name}', retry_quota.release_retry_quota\n    )\n\n    handler = RetryHandler(\n        retry_policy=RetryPolicy(\n            retry_checker=StandardRetryConditions(max_attempts=max_attempts),\n            retry_backoff=ExponentialBackoff(),\n        ),\n        retry_event_adapter=RetryEventAdapter(),\n        retry_quota=retry_quota,\n    )\n\n    unique_id = 'retry-config-%s' % service_event_name\n    client.meta.events.register(\n        'needs-retry.%s' % service_event_name,\n        handler.needs_retry,\n        unique_id=unique_id,\n    )\n    return handler\n\n\nclass RetryHandler:\n    \"\"\"Bridge between botocore's event system and this module.\n\n    This class is intended to be hooked to botocore's event system\n    as an event handler.\n    \"\"\"\n\n    def __init__(self, retry_policy, retry_event_adapter, retry_quota):\n        self._retry_policy = retry_policy\n        self._retry_event_adapter = retry_event_adapter\n        self._retry_quota = retry_quota\n\n    def needs_retry(self, **kwargs):\n        \"\"\"Connect as a handler to the needs-retry event.\"\"\"\n        retry_delay = None\n        context = self._retry_event_adapter.create_retry_context(**kwargs)\n        if self._retry_policy.should_retry(context):\n            # Before we can retry we need to ensure we have sufficient\n            # capacity in our retry quota.\n            if self._retry_quota.acquire_retry_quota(context):\n                retry_delay = self._retry_policy.compute_retry_delay(context)\n                logger.debug(\n                    \"Retry needed, retrying request after delay of: %s\",\n                    retry_delay,\n                )\n            else:\n                logger.debug(\n                    \"Retry needed but retry quota reached, \"\n                    \"not retrying request.\"\n                )\n        else:\n            logger.debug(\"Not retrying request.\")\n        self._retry_event_adapter.adapt_retry_response_from_context(context)\n        return retry_delay\n\n\nclass RetryEventAdapter:\n    \"\"\"Adapter to existing retry interface used in the endpoints layer.\n\n    This existing interface for determining if a retry needs to happen\n    is event based and used in ``botocore.endpoint``.  The interface has\n    grown organically over the years and could use some cleanup.  This\n    adapter converts that interface into the interface used by the\n    new retry strategies.\n\n    \"\"\"\n\n    def create_retry_context(self, **kwargs):\n        \"\"\"Create context based on needs-retry kwargs.\"\"\"\n        response = kwargs['response']\n        if response is None:\n            # If response is None it means that an exception was raised\n            # because we never received a response from the service.  This\n            # could be something like a ConnectionError we get from our\n            # http layer.\n            http_response = None\n            parsed_response = None\n        else:\n            http_response, parsed_response = response\n        # This provides isolation between the kwargs emitted in the\n        # needs-retry event, and what this module uses to check for\n        # retries.\n        context = RetryContext(\n            attempt_number=kwargs['attempts'],\n            operation_model=kwargs['operation'],\n            http_response=http_response,\n            parsed_response=parsed_response,\n            caught_exception=kwargs['caught_exception'],\n            request_context=kwargs['request_dict']['context'],\n        )\n        return context\n\n    def adapt_retry_response_from_context(self, context):\n        \"\"\"Modify response back to user back from context.\"\"\"\n        # This will mutate attributes that are returned back to the end\n        # user.  We do it this way so that all the various retry classes\n        # don't mutate any input parameters from the needs-retry event.\n        metadata = context.get_retry_metadata()\n        if context.parsed_response is not None:\n            context.parsed_response.setdefault('ResponseMetadata', {}).update(\n                metadata\n            )\n\n\n# Implementation note: this is meant to encapsulate all the misc stuff\n# that gets sent in the needs-retry event.  This is mapped so that params\n# are more clear and explicit.\nclass RetryContext:\n    \"\"\"Normalize a response that we use to check if a retry should occur.\n\n    This class smoothes over the different types of responses we may get\n    from a service including:\n\n        * A modeled error response from the service that contains a service\n          code and error message.\n        * A raw HTTP response that doesn't contain service protocol specific\n          error keys.\n        * An exception received while attempting to retrieve a response.\n          This could be a ConnectionError we receive from our HTTP layer which\n          could represent that we weren't able to receive a response from\n          the service.\n\n    This class guarantees that at least one of the above attributes will be\n    non None.\n\n    This class is meant to provide a read-only view into the properties\n    associated with a possible retryable response.  None of the properties\n    are meant to be modified directly.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        attempt_number,\n        operation_model=None,\n        parsed_response=None,\n        http_response=None,\n        caught_exception=None,\n        request_context=None,\n    ):\n        # 1-based attempt number.\n        self.attempt_number = attempt_number\n        self.operation_model = operation_model\n        # This is the parsed response dictionary we get from parsing\n        # the HTTP response from the service.\n        self.parsed_response = parsed_response\n        # This is an instance of botocore.awsrequest.AWSResponse.\n        self.http_response = http_response\n        # This is a subclass of Exception that will be non None if\n        # an exception was raised when retrying to retrieve a response.\n        self.caught_exception = caught_exception\n        # This is the request context dictionary that's added to the\n        # request dict.  This is used to story any additional state\n        # about the request.  We use this for storing retry quota\n        # capacity.\n        if request_context is None:\n            request_context = {}\n        self.request_context = request_context\n        self._retry_metadata = {}\n\n    # These are misc helper methods to avoid duplication in the various\n    # checkers.\n    def get_error_code(self):\n        \"\"\"Check if there was a parsed response with an error code.\n\n        If we could not find any error codes, ``None`` is returned.\n\n        \"\"\"\n        if self.parsed_response is None:\n            return\n        error = self.parsed_response.get('Error', {})\n        if not isinstance(error, dict):\n            return\n        return error.get('Code')\n\n    def add_retry_metadata(self, **kwargs):\n        \"\"\"Add key/value pairs to the retry metadata.\n\n        This allows any objects during the retry process to add\n        metadata about any checks/validations that happened.\n\n        This gets added to the response metadata in the retry handler.\n\n        \"\"\"\n        self._retry_metadata.update(**kwargs)\n\n    def get_retry_metadata(self):\n        return self._retry_metadata.copy()\n\n\nclass RetryPolicy:\n    def __init__(self, retry_checker, retry_backoff):\n        self._retry_checker = retry_checker\n        self._retry_backoff = retry_backoff\n\n    def should_retry(self, context):\n        return self._retry_checker.is_retryable(context)\n\n    def compute_retry_delay(self, context):\n        return self._retry_backoff.delay_amount(context)\n\n\nclass ExponentialBackoff(BaseRetryBackoff):\n    _BASE = 2\n    _MAX_BACKOFF = 20\n\n    def __init__(self, max_backoff=20, random=random.random):\n        self._base = self._BASE\n        self._max_backoff = max_backoff\n        self._random = random\n\n    def delay_amount(self, context):\n        \"\"\"Calculates delay based on exponential backoff.\n\n        This class implements truncated binary exponential backoff\n        with jitter::\n\n            t_i = rand(0, 1) * min(2 ** attempt, MAX_BACKOFF)\n\n        where ``i`` is the request attempt (0 based).\n\n        \"\"\"\n        # The context.attempt_number is a 1-based value, but we have\n        # to calculate the delay based on i based a 0-based value.  We\n        # want the first delay to just be ``rand(0, 1)``.\n        return self._random() * min(\n            (self._base ** (context.attempt_number - 1)),\n            self._max_backoff,\n        )\n\n\nclass MaxAttemptsChecker(BaseRetryableChecker):\n    def __init__(self, max_attempts):\n        self._max_attempts = max_attempts\n\n    def is_retryable(self, context):\n        under_max_attempts = context.attempt_number < self._max_attempts\n        retries_context = context.request_context.get('retries')\n        if retries_context:\n            retries_context['max'] = max(\n                retries_context.get('max', 0), self._max_attempts\n            )\n        if not under_max_attempts:\n            logger.debug(\"Max attempts of %s reached.\", self._max_attempts)\n            context.add_retry_metadata(MaxAttemptsReached=True)\n        return under_max_attempts\n\n\nclass TransientRetryableChecker(BaseRetryableChecker):\n    _TRANSIENT_ERROR_CODES = [\n        'RequestTimeout',\n        'RequestTimeoutException',\n        'PriorRequestNotComplete',\n    ]\n    _TRANSIENT_STATUS_CODES = [500, 502, 503, 504]\n    _TRANSIENT_EXCEPTION_CLS = (\n        ConnectionError,\n        HTTPClientError,\n    )\n\n    def __init__(\n        self,\n        transient_error_codes=None,\n        transient_status_codes=None,\n        transient_exception_cls=None,\n    ):\n        if transient_error_codes is None:\n            transient_error_codes = self._TRANSIENT_ERROR_CODES[:]\n        if transient_status_codes is None:\n            transient_status_codes = self._TRANSIENT_STATUS_CODES[:]\n        if transient_exception_cls is None:\n            transient_exception_cls = self._TRANSIENT_EXCEPTION_CLS\n        self._transient_error_codes = transient_error_codes\n        self._transient_status_codes = transient_status_codes\n        self._transient_exception_cls = transient_exception_cls\n\n    def is_retryable(self, context):\n        if context.get_error_code() in self._transient_error_codes:\n            return True\n        if context.http_response is not None:\n            if (\n                context.http_response.status_code\n                in self._transient_status_codes\n            ):\n                return True\n        if context.caught_exception is not None:\n            return isinstance(\n                context.caught_exception, self._transient_exception_cls\n            )\n        return False\n\n\nclass ThrottledRetryableChecker(BaseRetryableChecker):\n    # This is the union of all error codes we've seen that represent\n    # a throttled error.\n    _THROTTLED_ERROR_CODES = [\n        'Throttling',\n        'ThrottlingException',\n        'ThrottledException',\n        'RequestThrottledException',\n        'TooManyRequestsException',\n        'ProvisionedThroughputExceededException',\n        'TransactionInProgressException',\n        'RequestLimitExceeded',\n        'BandwidthLimitExceeded',\n        'LimitExceededException',\n        'RequestThrottled',\n        'SlowDown',\n        'PriorRequestNotComplete',\n        'EC2ThrottledException',\n    ]\n\n    def __init__(self, throttled_error_codes=None):\n        if throttled_error_codes is None:\n            throttled_error_codes = self._THROTTLED_ERROR_CODES[:]\n        self._throttled_error_codes = throttled_error_codes\n\n    def is_retryable(self, context):\n        # Only the error code from a parsed service response is used\n        # to determine if the response is a throttled response.\n        return context.get_error_code() in self._throttled_error_codes\n\n\nclass ModeledRetryableChecker(BaseRetryableChecker):\n    \"\"\"Check if an error has been modeled as retryable.\"\"\"\n\n    def __init__(self):\n        self._error_detector = ModeledRetryErrorDetector()\n\n    def is_retryable(self, context):\n        error_code = context.get_error_code()\n        if error_code is None:\n            return False\n        return self._error_detector.detect_error_type(context) is not None\n\n\nclass ModeledRetryErrorDetector:\n    \"\"\"Checks whether or not an error is a modeled retryable error.\"\"\"\n\n    # There are return values from the detect_error_type() method.\n    TRANSIENT_ERROR = 'TRANSIENT_ERROR'\n    THROTTLING_ERROR = 'THROTTLING_ERROR'\n    # This class is lower level than ModeledRetryableChecker, which\n    # implements BaseRetryableChecker.  This object allows you to distinguish\n    # between the various types of retryable errors.\n\n    def detect_error_type(self, context):\n        \"\"\"Detect the error type associated with an error code and model.\n\n        This will either return:\n\n            * ``self.TRANSIENT_ERROR`` - If the error is a transient error\n            * ``self.THROTTLING_ERROR`` - If the error is a throttling error\n            * ``None`` - If the error is neither type of error.\n\n        \"\"\"\n        error_code = context.get_error_code()\n        op_model = context.operation_model\n        if op_model is None or not op_model.error_shapes:\n            return\n        for shape in op_model.error_shapes:\n            if shape.metadata.get('retryable') is not None:\n                # Check if this error code matches the shape.  This can\n                # be either by name or by a modeled error code.\n                error_code_to_check = (\n                    shape.metadata.get('error', {}).get('code') or shape.name\n                )\n                if error_code == error_code_to_check:\n                    if shape.metadata['retryable'].get('throttling'):\n                        return self.THROTTLING_ERROR\n                    return self.TRANSIENT_ERROR\n\n\nclass ThrottlingErrorDetector:\n    def __init__(self, retry_event_adapter):\n        self._modeled_error_detector = ModeledRetryErrorDetector()\n        self._fixed_error_code_detector = ThrottledRetryableChecker()\n        self._retry_event_adapter = retry_event_adapter\n\n    # This expects the kwargs from needs-retry to be passed through.\n    def is_throttling_error(self, **kwargs):\n        context = self._retry_event_adapter.create_retry_context(**kwargs)\n        if self._fixed_error_code_detector.is_retryable(context):\n            return True\n        error_type = self._modeled_error_detector.detect_error_type(context)\n        return error_type == self._modeled_error_detector.THROTTLING_ERROR\n\n\nclass StandardRetryConditions(BaseRetryableChecker):\n    \"\"\"Concrete class that implements the standard retry policy checks.\n\n    Specifically:\n\n        not max_attempts and (transient or throttled or modeled_retry)\n\n    \"\"\"\n\n    def __init__(self, max_attempts=DEFAULT_MAX_ATTEMPTS):\n        # Note: This class is for convenience so you can have the\n        # standard retry condition in a single class.\n        self._max_attempts_checker = MaxAttemptsChecker(max_attempts)\n        self._additional_checkers = OrRetryChecker(\n            [\n                TransientRetryableChecker(),\n                ThrottledRetryableChecker(),\n                ModeledRetryableChecker(),\n                OrRetryChecker(\n                    [\n                        special.RetryIDPCommunicationError(),\n                        special.RetryDDBChecksumError(),\n                    ]\n                ),\n            ]\n        )\n\n    def is_retryable(self, context):\n        return self._max_attempts_checker.is_retryable(\n            context\n        ) and self._additional_checkers.is_retryable(context)\n\n\nclass OrRetryChecker(BaseRetryableChecker):\n    def __init__(self, checkers):\n        self._checkers = checkers\n\n    def is_retryable(self, context):\n        return any(checker.is_retryable(context) for checker in self._checkers)\n\n\nclass RetryQuotaChecker:\n    _RETRY_COST = 5\n    _NO_RETRY_INCREMENT = 1\n    _TIMEOUT_RETRY_REQUEST = 10\n    _TIMEOUT_EXCEPTIONS = (ConnectTimeoutError, ReadTimeoutError)\n\n    # Implementation note:  We're not making this a BaseRetryableChecker\n    # because this isn't just a check if we can retry.  This also changes\n    # state so we have to careful when/how we call this.  Making it\n    # a BaseRetryableChecker implies you can call .is_retryable(context)\n    # as many times as you want and not affect anything.\n\n    def __init__(self, quota):\n        self._quota = quota\n        # This tracks the last amount\n        self._last_amount_acquired = None\n\n    def acquire_retry_quota(self, context):\n        if self._is_timeout_error(context):\n            capacity_amount = self._TIMEOUT_RETRY_REQUEST\n        else:\n            capacity_amount = self._RETRY_COST\n        success = self._quota.acquire(capacity_amount)\n        if success:\n            # We add the capacity amount to the request context so we know\n            # how much to release later.  The capacity amount can vary based\n            # on the error.\n            context.request_context['retry_quota_capacity'] = capacity_amount\n            return True\n        context.add_retry_metadata(RetryQuotaReached=True)\n        return False\n\n    def _is_timeout_error(self, context):\n        return isinstance(context.caught_exception, self._TIMEOUT_EXCEPTIONS)\n\n    # This is intended to be hooked up to ``after-call``.\n    def release_retry_quota(self, context, http_response, **kwargs):\n        # There's three possible options.\n        # 1. The HTTP response did not have a 2xx response.  In that case we\n        #    give no quota back.\n        # 2. The HTTP request was successful and was never retried.  In\n        #    that case we give _NO_RETRY_INCREMENT back.\n        # 3. The API call had retries, and we eventually receive an HTTP\n        #    response with a 2xx status code.  In that case we give back\n        #    whatever quota was associated with the last acquisition.\n        if http_response is None:\n            return\n        status_code = http_response.status_code\n        if 200 <= status_code < 300:\n            if 'retry_quota_capacity' not in context:\n                self._quota.release(self._NO_RETRY_INCREMENT)\n            else:\n                capacity_amount = context['retry_quota_capacity']\n                self._quota.release(capacity_amount)\n", "botocore/vendored/six.py": "# Copyright (c) 2010-2020 Benjamin Peterson\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\n\nfrom __future__ import absolute_import\n\nimport functools\nimport itertools\nimport operator\nimport sys\nimport types\n\n__author__ = \"Benjamin Peterson <benjamin@python.org>\"\n__version__ = \"1.16.0\"\n\n\n# Useful for very coarse version differentiation.\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\nPY34 = sys.version_info[0:2] >= (3, 4)\n\nif PY3:\n    string_types = str,\n    integer_types = int,\n    class_types = type,\n    text_type = str\n    binary_type = bytes\n\n    MAXSIZE = sys.maxsize\nelse:\n    string_types = basestring,\n    integer_types = (int, long)\n    class_types = (type, types.ClassType)\n    text_type = unicode\n    binary_type = str\n\n    if sys.platform.startswith(\"java\"):\n        # Jython always uses 32 bits.\n        MAXSIZE = int((1 << 31) - 1)\n    else:\n        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).\n        class X(object):\n\n            def __len__(self):\n                return 1 << 31\n        try:\n            len(X())\n        except OverflowError:\n            # 32-bit\n            MAXSIZE = int((1 << 31) - 1)\n        else:\n            # 64-bit\n            MAXSIZE = int((1 << 63) - 1)\n        del X\n\nif PY34:\n    from importlib.util import spec_from_loader\nelse:\n    spec_from_loader = None\n\n\ndef _add_doc(func, doc):\n    \"\"\"Add documentation to a function.\"\"\"\n    func.__doc__ = doc\n\n\ndef _import_module(name):\n    \"\"\"Import module, returning the module after the last dot.\"\"\"\n    __import__(name)\n    return sys.modules[name]\n\n\nclass _LazyDescr(object):\n\n    def __init__(self, name):\n        self.name = name\n\n    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes __set__.\n        try:\n            # This is a bit ugly, but it avoids running this again by\n            # removing this descriptor.\n            delattr(obj.__class__, self.name)\n        except AttributeError:\n            pass\n        return result\n\n\nclass MovedModule(_LazyDescr):\n\n    def __init__(self, name, old, new=None):\n        super(MovedModule, self).__init__(name)\n        if PY3:\n            if new is None:\n                new = name\n            self.mod = new\n        else:\n            self.mod = old\n\n    def _resolve(self):\n        return _import_module(self.mod)\n\n    def __getattr__(self, attr):\n        _module = self._resolve()\n        value = getattr(_module, attr)\n        setattr(self, attr, value)\n        return value\n\n\nclass _LazyModule(types.ModuleType):\n\n    def __init__(self, name):\n        super(_LazyModule, self).__init__(name)\n        self.__doc__ = self.__class__.__doc__\n\n    def __dir__(self):\n        attrs = [\"__doc__\", \"__name__\"]\n        attrs += [attr.name for attr in self._moved_attributes]\n        return attrs\n\n    # Subclasses should override this\n    _moved_attributes = []\n\n\nclass MovedAttribute(_LazyDescr):\n\n    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\n        super(MovedAttribute, self).__init__(name)\n        if PY3:\n            if new_mod is None:\n                new_mod = name\n            self.mod = new_mod\n            if new_attr is None:\n                if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n            self.mod = old_mod\n            if old_attr is None:\n                old_attr = name\n            self.attr = old_attr\n\n    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)\n\n\nclass _SixMetaPathImporter(object):\n\n    \"\"\"\n    A meta path importer to import six.moves and its submodules.\n\n    This class implements a PEP302 finder and loader. It should be compatible\n    with Python 2.5 and all existing versions of Python3\n    \"\"\"\n\n    def __init__(self, six_module_name):\n        self.name = six_module_name\n        self.known_modules = {}\n\n    def _add_module(self, mod, *fullnames):\n        for fullname in fullnames:\n            self.known_modules[self.name + \".\" + fullname] = mod\n\n    def _get_module(self, fullname):\n        return self.known_modules[self.name + \".\" + fullname]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.known_modules:\n            return self\n        return None\n\n    def find_spec(self, fullname, path, target=None):\n        if fullname in self.known_modules:\n            return spec_from_loader(fullname, self)\n        return None\n\n    def __get_module(self, fullname):\n        try:\n            return self.known_modules[fullname]\n        except KeyError:\n            raise ImportError(\"This loader does not know module \" + fullname)\n\n    def load_module(self, fullname):\n        try:\n            # in case of a reload\n            return sys.modules[fullname]\n        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n        sys.modules[fullname] = mod\n        return mod\n\n    def is_package(self, fullname):\n        \"\"\"\n        Return true, if the named module is a package.\n\n        We need this method to get correct spec objects with\n        Python 3.4 (see PEP451)\n        \"\"\"\n        return hasattr(self.__get_module(fullname), \"__path__\")\n\n    def get_code(self, fullname):\n        \"\"\"Return None\n\n        Required, if is_package is implemented\"\"\"\n        self.__get_module(fullname)  # eventually raises ImportError\n        return None\n    get_source = get_code  # same as get_code\n\n    def create_module(self, spec):\n        return self.load_module(spec.name)\n\n    def exec_module(self, module):\n        pass\n\n_importer = _SixMetaPathImporter(__name__)\n\n\nclass _MovedItems(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects\"\"\"\n    __path__ = []  # mark as package\n\n\n_moved_attributes = [\n    MovedAttribute(\"cStringIO\", \"cStringIO\", \"io\", \"StringIO\"),\n    MovedAttribute(\"filter\", \"itertools\", \"builtins\", \"ifilter\", \"filter\"),\n    MovedAttribute(\"filterfalse\", \"itertools\", \"itertools\", \"ifilterfalse\", \"filterfalse\"),\n    MovedAttribute(\"input\", \"__builtin__\", \"builtins\", \"raw_input\", \"input\"),\n    MovedAttribute(\"intern\", \"__builtin__\", \"sys\"),\n    MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\n    MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\n    MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\n    MovedAttribute(\"getoutput\", \"commands\", \"subprocess\"),\n    MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n    MovedAttribute(\"reduce\", \"__builtin__\", \"functools\"),\n    MovedAttribute(\"shlex_quote\", \"pipes\", \"shlex\", \"quote\"),\n    MovedAttribute(\"StringIO\", \"StringIO\", \"io\"),\n    MovedAttribute(\"UserDict\", \"UserDict\", \"collections\"),\n    MovedAttribute(\"UserList\", \"UserList\", \"collections\"),\n    MovedAttribute(\"UserString\", \"UserString\", \"collections\"),\n    MovedAttribute(\"xrange\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"zip\", \"itertools\", \"builtins\", \"izip\", \"zip\"),\n    MovedAttribute(\"zip_longest\", \"itertools\", \"itertools\", \"izip_longest\", \"zip_longest\"),\n    MovedModule(\"builtins\", \"__builtin__\"),\n    MovedModule(\"configparser\", \"ConfigParser\"),\n    MovedModule(\"collections_abc\", \"collections\", \"collections.abc\" if sys.version_info >= (3, 3) else \"collections\"),\n    MovedModule(\"copyreg\", \"copy_reg\"),\n    MovedModule(\"dbm_gnu\", \"gdbm\", \"dbm.gnu\"),\n    MovedModule(\"dbm_ndbm\", \"dbm\", \"dbm.ndbm\"),\n    MovedModule(\"_dummy_thread\", \"dummy_thread\", \"_dummy_thread\" if sys.version_info < (3, 9) else \"_thread\"),\n    MovedModule(\"http_cookiejar\", \"cookielib\", \"http.cookiejar\"),\n    MovedModule(\"http_cookies\", \"Cookie\", \"http.cookies\"),\n    MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\n    MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\n    MovedModule(\"http_client\", \"httplib\", \"http.client\"),\n    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\n    MovedModule(\"email_mime_image\", \"email.MIMEImage\", \"email.mime.image\"),\n    MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\n    MovedModule(\"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"),\n    MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\n    MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\n    MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\n    MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\n    MovedModule(\"cPickle\", \"cPickle\", \"pickle\"),\n    MovedModule(\"queue\", \"Queue\"),\n    MovedModule(\"reprlib\", \"repr\"),\n    MovedModule(\"socketserver\", \"SocketServer\"),\n    MovedModule(\"_thread\", \"thread\", \"_thread\"),\n    MovedModule(\"tkinter\", \"Tkinter\"),\n    MovedModule(\"tkinter_dialog\", \"Dialog\", \"tkinter.dialog\"),\n    MovedModule(\"tkinter_filedialog\", \"FileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_scrolledtext\", \"ScrolledText\", \"tkinter.scrolledtext\"),\n    MovedModule(\"tkinter_simpledialog\", \"SimpleDialog\", \"tkinter.simpledialog\"),\n    MovedModule(\"tkinter_tix\", \"Tix\", \"tkinter.tix\"),\n    MovedModule(\"tkinter_ttk\", \"ttk\", \"tkinter.ttk\"),\n    MovedModule(\"tkinter_constants\", \"Tkconstants\", \"tkinter.constants\"),\n    MovedModule(\"tkinter_dnd\", \"Tkdnd\", \"tkinter.dnd\"),\n    MovedModule(\"tkinter_colorchooser\", \"tkColorChooser\",\n                \"tkinter.colorchooser\"),\n    MovedModule(\"tkinter_commondialog\", \"tkCommonDialog\",\n                \"tkinter.commondialog\"),\n    MovedModule(\"tkinter_tkfiledialog\", \"tkFileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_font\", \"tkFont\", \"tkinter.font\"),\n    MovedModule(\"tkinter_messagebox\", \"tkMessageBox\", \"tkinter.messagebox\"),\n    MovedModule(\"tkinter_tksimpledialog\", \"tkSimpleDialog\",\n                \"tkinter.simpledialog\"),\n    MovedModule(\"urllib_parse\", __name__ + \".moves.urllib_parse\", \"urllib.parse\"),\n    MovedModule(\"urllib_error\", __name__ + \".moves.urllib_error\", \"urllib.error\"),\n    MovedModule(\"urllib\", __name__ + \".moves.urllib\", __name__ + \".moves.urllib\"),\n    MovedModule(\"urllib_robotparser\", \"robotparser\", \"urllib.robotparser\"),\n    MovedModule(\"xmlrpc_client\", \"xmlrpclib\", \"xmlrpc.client\"),\n    MovedModule(\"xmlrpc_server\", \"SimpleXMLRPCServer\", \"xmlrpc.server\"),\n]\n# Add windows specific modules.\nif sys.platform == \"win32\":\n    _moved_attributes += [\n        MovedModule(\"winreg\", \"_winreg\"),\n    ]\n\nfor attr in _moved_attributes:\n    setattr(_MovedItems, attr.name, attr)\n    if isinstance(attr, MovedModule):\n        _importer._add_module(attr, \"moves.\" + attr.name)\ndel attr\n\n_MovedItems._moved_attributes = _moved_attributes\n\nmoves = _MovedItems(__name__ + \".moves\")\n_importer._add_module(moves, \"moves\")\n\n\nclass Module_six_moves_urllib_parse(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_parse\"\"\"\n\n\n_urllib_parse_moved_attributes = [\n    MovedAttribute(\"ParseResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"SplitResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qs\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qsl\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urldefrag\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urljoin\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"quote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_to_bytes\", \"urllib\", \"urllib.parse\", \"unquote\", \"unquote_to_bytes\"),\n    MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitvalue\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_query\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_relative\", \"urlparse\", \"urllib.parse\"),\n]\nfor attr in _urllib_parse_moved_attributes:\n    setattr(Module_six_moves_urllib_parse, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n                      \"moves.urllib_parse\", \"moves.urllib.parse\")\n\n\nclass Module_six_moves_urllib_error(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_error\"\"\"\n\n\n_urllib_error_moved_attributes = [\n    MovedAttribute(\"URLError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"HTTPError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"ContentTooShortError\", \"urllib\", \"urllib.error\"),\n]\nfor attr in _urllib_error_moved_attributes:\n    setattr(Module_six_moves_urllib_error, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n                      \"moves.urllib_error\", \"moves.urllib.error\")\n\n\nclass Module_six_moves_urllib_request(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_request\"\"\"\n\n\n_urllib_request_moved_attributes = [\n    MovedAttribute(\"urlopen\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"install_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"build_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"pathname2url\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"url2pathname\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"getproxies\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"Request\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"OpenerDirector\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDefaultErrorHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPRedirectHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPCookieProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"BaseHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgr\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgrWithDefaultRealm\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPSHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FileHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"CacheFTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"UnknownHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPErrorProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"urlretrieve\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"urlcleanup\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"parse_http_list\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"parse_keqv_list\", \"urllib2\", \"urllib.request\"),\n]\nfor attr in _urllib_request_moved_attributes:\n    setattr(Module_six_moves_urllib_request, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n                      \"moves.urllib_request\", \"moves.urllib.request\")\n\n\nclass Module_six_moves_urllib_response(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_response\"\"\"\n\n\n_urllib_response_moved_attributes = [\n    MovedAttribute(\"addbase\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addclosehook\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfo\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfourl\", \"urllib\", \"urllib.response\"),\n]\nfor attr in _urllib_response_moved_attributes:\n    setattr(Module_six_moves_urllib_response, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n                      \"moves.urllib_response\", \"moves.urllib.response\")\n\n\nclass Module_six_moves_urllib_robotparser(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_robotparser\"\"\"\n\n\n_urllib_robotparser_moved_attributes = [\n    MovedAttribute(\"RobotFileParser\", \"robotparser\", \"urllib.robotparser\"),\n]\nfor attr in _urllib_robotparser_moved_attributes:\n    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n                      \"moves.urllib_robotparser\", \"moves.urllib.robotparser\")\n\n\nclass Module_six_moves_urllib(types.ModuleType):\n\n    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', 'error', 'request', 'response', 'robotparser']\n\n_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n                      \"moves.urllib\")\n\n\ndef add_move(move):\n    \"\"\"Add an item to six.moves.\"\"\"\n    setattr(_MovedItems, move.name, move)\n\n\ndef remove_move(name):\n    \"\"\"Remove item from six.moves.\"\"\"\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError(\"no such move, %r\" % (name,))\n\n\nif PY3:\n    _meth_func = \"__func__\"\n    _meth_self = \"__self__\"\n\n    _func_closure = \"__closure__\"\n    _func_code = \"__code__\"\n    _func_defaults = \"__defaults__\"\n    _func_globals = \"__globals__\"\nelse:\n    _meth_func = \"im_func\"\n    _meth_self = \"im_self\"\n\n    _func_closure = \"func_closure\"\n    _func_code = \"func_code\"\n    _func_defaults = \"func_defaults\"\n    _func_globals = \"func_globals\"\n\n\ntry:\n    advance_iterator = next\nexcept NameError:\n    def advance_iterator(it):\n        return it.next()\nnext = advance_iterator\n\n\ntry:\n    callable = callable\nexcept NameError:\n    def callable(obj):\n        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n\n\nif PY3:\n    def get_unbound_function(unbound):\n        return unbound\n\n    create_bound_method = types.MethodType\n\n    def create_unbound_method(func, cls):\n        return func\n\n    Iterator = object\nelse:\n    def get_unbound_function(unbound):\n        return unbound.im_func\n\n    def create_bound_method(func, obj):\n        return types.MethodType(func, obj, obj.__class__)\n\n    def create_unbound_method(func, cls):\n        return types.MethodType(func, None, cls)\n\n    class Iterator(object):\n\n        def next(self):\n            return type(self).__next__(self)\n\n    callable = callable\n_add_doc(get_unbound_function,\n         \"\"\"Get the function out of a possibly unbound function\"\"\")\n\n\nget_method_function = operator.attrgetter(_meth_func)\nget_method_self = operator.attrgetter(_meth_self)\nget_function_closure = operator.attrgetter(_func_closure)\nget_function_code = operator.attrgetter(_func_code)\nget_function_defaults = operator.attrgetter(_func_defaults)\nget_function_globals = operator.attrgetter(_func_globals)\n\n\nif PY3:\n    def iterkeys(d, **kw):\n        return iter(d.keys(**kw))\n\n    def itervalues(d, **kw):\n        return iter(d.values(**kw))\n\n    def iteritems(d, **kw):\n        return iter(d.items(**kw))\n\n    def iterlists(d, **kw):\n        return iter(d.lists(**kw))\n\n    viewkeys = operator.methodcaller(\"keys\")\n\n    viewvalues = operator.methodcaller(\"values\")\n\n    viewitems = operator.methodcaller(\"items\")\nelse:\n    def iterkeys(d, **kw):\n        return d.iterkeys(**kw)\n\n    def itervalues(d, **kw):\n        return d.itervalues(**kw)\n\n    def iteritems(d, **kw):\n        return d.iteritems(**kw)\n\n    def iterlists(d, **kw):\n        return d.iterlists(**kw)\n\n    viewkeys = operator.methodcaller(\"viewkeys\")\n\n    viewvalues = operator.methodcaller(\"viewvalues\")\n\n    viewitems = operator.methodcaller(\"viewitems\")\n\n_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n_add_doc(iteritems,\n         \"Return an iterator over the (key, value) pairs of a dictionary.\")\n_add_doc(iterlists,\n         \"Return an iterator over the (key, [values]) pairs of a dictionary.\")\n\n\nif PY3:\n    def b(s):\n        return s.encode(\"latin-1\")\n\n    def u(s):\n        return s\n    unichr = chr\n    import struct\n    int2byte = struct.Struct(\">B\").pack\n    del struct\n    byte2int = operator.itemgetter(0)\n    indexbytes = operator.getitem\n    iterbytes = iter\n    import io\n    StringIO = io.StringIO\n    BytesIO = io.BytesIO\n    del io\n    _assertCountEqual = \"assertCountEqual\"\n    if sys.version_info[1] <= 1:\n        _assertRaisesRegex = \"assertRaisesRegexp\"\n        _assertRegex = \"assertRegexpMatches\"\n        _assertNotRegex = \"assertNotRegexpMatches\"\n    else:\n        _assertRaisesRegex = \"assertRaisesRegex\"\n        _assertRegex = \"assertRegex\"\n        _assertNotRegex = \"assertNotRegex\"\nelse:\n    def b(s):\n        return s\n    # Workaround for standalone backslash\n\n    def u(s):\n        return unicode(s.replace(r'\\\\', r'\\\\\\\\'), \"unicode_escape\")\n    unichr = unichr\n    int2byte = chr\n\n    def byte2int(bs):\n        return ord(bs[0])\n\n    def indexbytes(buf, i):\n        return ord(buf[i])\n    iterbytes = functools.partial(itertools.imap, ord)\n    import StringIO\n    StringIO = BytesIO = StringIO.StringIO\n    _assertCountEqual = \"assertItemsEqual\"\n    _assertRaisesRegex = \"assertRaisesRegexp\"\n    _assertRegex = \"assertRegexpMatches\"\n    _assertNotRegex = \"assertNotRegexpMatches\"\n_add_doc(b, \"\"\"Byte literal\"\"\")\n_add_doc(u, \"\"\"Text literal\"\"\")\n\n\ndef assertCountEqual(self, *args, **kwargs):\n    return getattr(self, _assertCountEqual)(*args, **kwargs)\n\n\ndef assertRaisesRegex(self, *args, **kwargs):\n    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n\n\ndef assertRegex(self, *args, **kwargs):\n    return getattr(self, _assertRegex)(*args, **kwargs)\n\n\ndef assertNotRegex(self, *args, **kwargs):\n    return getattr(self, _assertNotRegex)(*args, **kwargs)\n\n\nif PY3:\n    exec_ = getattr(moves.builtins, \"exec\")\n\n    def reraise(tp, value, tb=None):\n        try:\n            if value is None:\n                value = tp()\n            if value.__traceback__ is not tb:\n                raise value.with_traceback(tb)\n            raise value\n        finally:\n            value = None\n            tb = None\n\nelse:\n    def exec_(_code_, _globs_=None, _locs_=None):\n        \"\"\"Execute code in a namespace.\"\"\"\n        if _globs_ is None:\n            frame = sys._getframe(1)\n            _globs_ = frame.f_globals\n            if _locs_ is None:\n                _locs_ = frame.f_locals\n            del frame\n        elif _locs_ is None:\n            _locs_ = _globs_\n        exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")\n\n    exec_(\"\"\"def reraise(tp, value, tb=None):\n    try:\n        raise tp, value, tb\n    finally:\n        tb = None\n\"\"\")\n\n\nif sys.version_info[:2] > (3,):\n    exec_(\"\"\"def raise_from(value, from_value):\n    try:\n        raise value from from_value\n    finally:\n        value = None\n\"\"\")\nelse:\n    def raise_from(value, from_value):\n        raise value\n\n\nprint_ = getattr(moves.builtins, \"print\", None)\nif print_ is None:\n    def print_(*args, **kwargs):\n        \"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n        fp = kwargs.pop(\"file\", sys.stdout)\n        if fp is None:\n            return\n\n        def write(data):\n            if not isinstance(data, basestring):\n                data = str(data)\n            # If the file has an encoding, encode unicode with it.\n            if (isinstance(fp, file) and\n                    isinstance(data, unicode) and\n                    fp.encoding is not None):\n                errors = getattr(fp, \"errors\", None)\n                if errors is None:\n                    errors = \"strict\"\n                data = data.encode(fp.encoding, errors)\n            fp.write(data)\n        want_unicode = False\n        sep = kwargs.pop(\"sep\", None)\n        if sep is not None:\n            if isinstance(sep, unicode):\n                want_unicode = True\n            elif not isinstance(sep, str):\n                raise TypeError(\"sep must be None or a string\")\n        end = kwargs.pop(\"end\", None)\n        if end is not None:\n            if isinstance(end, unicode):\n                want_unicode = True\n            elif not isinstance(end, str):\n                raise TypeError(\"end must be None or a string\")\n        if kwargs:\n            raise TypeError(\"invalid keyword arguments to print()\")\n        if not want_unicode:\n            for arg in args:\n                if isinstance(arg, unicode):\n                    want_unicode = True\n                    break\n        if want_unicode:\n            newline = unicode(\"\\n\")\n            space = unicode(\" \")\n        else:\n            newline = \"\\n\"\n            space = \" \"\n        if sep is None:\n            sep = space\n        if end is None:\n            end = newline\n        for i, arg in enumerate(args):\n            if i:\n                write(sep)\n            write(arg)\n        write(end)\nif sys.version_info[:2] < (3, 3):\n    _print = print_\n\n    def print_(*args, **kwargs):\n        fp = kwargs.get(\"file\", sys.stdout)\n        flush = kwargs.pop(\"flush\", False)\n        _print(*args, **kwargs)\n        if flush and fp is not None:\n            fp.flush()\n\n_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n\nif sys.version_info[0:2] < (3, 4):\n    # This does exactly the same what the :func:`py3:functools.update_wrapper`\n    # function does on Python versions after 3.2. It sets the ``__wrapped__``\n    # attribute on ``wrapper`` object and it doesn't raise an error if any of\n    # the attributes mentioned in ``assigned`` and ``updated`` are missing on\n    # ``wrapped`` object.\n    def _update_wrapper(wrapper, wrapped,\n                        assigned=functools.WRAPPER_ASSIGNMENTS,\n                        updated=functools.WRAPPER_UPDATES):\n        for attr in assigned:\n            try:\n                value = getattr(wrapped, attr)\n            except AttributeError:\n                continue\n            else:\n                setattr(wrapper, attr, value)\n        for attr in updated:\n            getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n        wrapper.__wrapped__ = wrapped\n        return wrapper\n    _update_wrapper.__doc__ = functools.update_wrapper.__doc__\n\n    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,\n              updated=functools.WRAPPER_UPDATES):\n        return functools.partial(_update_wrapper, wrapped=wrapped,\n                                 assigned=assigned, updated=updated)\n    wraps.__doc__ = functools.wraps.__doc__\n\nelse:\n    wraps = functools.wraps\n\n\ndef with_metaclass(meta, *bases):\n    \"\"\"Create a base class with a metaclass.\"\"\"\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            if sys.version_info[:2] >= (3, 7):\n                # This version introduced PEP 560 that requires a bit\n                # of extra care (we mimic what is done by __build_class__).\n                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})\n\n\ndef add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        if hasattr(cls, '__qualname__'):\n            orig_vars['__qualname__'] = cls.__qualname__\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper\n\n\ndef ensure_binary(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce **s** to six.binary_type.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> encoded to `bytes`\n      - `bytes` -> `bytes`\n    \"\"\"\n    if isinstance(s, binary_type):\n        return s\n    if isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    raise TypeError(\"not expecting type '%s'\" % type(s))\n\n\ndef ensure_str(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce *s* to `str`.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    # Optimization: Fast return for the common case.\n    if type(s) is str:\n        return s\n    if PY2 and isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    return s\n\n\ndef ensure_text(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce *s* to six.text_type.\n\n    For Python 2:\n      - `unicode` -> `unicode`\n      - `str` -> `unicode`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n\n\ndef python_2_unicode_compatible(klass):\n    \"\"\"\n    A class decorator that defines __unicode__ and __str__ methods under Python 2.\n    Under Python 3 it does nothing.\n\n    To support Python 2 and 3 with a single code base, define a __str__ method\n    returning text and apply this decorator to the class.\n    \"\"\"\n    if PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"@python_2_unicode_compatible cannot be applied \"\n                             \"to %s because it doesn't define __str__().\" %\n                             klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return klass\n\n\n# Complete the moves implementation.\n# This code is at the end of this module to speed up module loading.\n# Turn this module into a package.\n__path__ = []  # required for PEP 302 and PEP 451\n__package__ = __name__  # see PEP 366 @ReservedAssignment\nif globals().get(\"__spec__\") is not None:\n    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable\n# Remove other six meta path importers, since they cause problems. This can\n# happen if six is removed from sys.modules and then reloaded. (Setuptools does\n# this for some reason.)\nif sys.meta_path:\n    for i, importer in enumerate(sys.meta_path):\n        # Here's some real nastiness: Another \"instance\" of the six module might\n        # be floating around. Therefore, we can't use isinstance() to check for\n        # the six meta path importer, since the other six instance will have\n        # inserted an importer with different class.\n        if (type(importer).__name__ == \"_SixMetaPathImporter\" and\n                importer.name == __name__):\n            del sys.meta_path[i]\n            break\n    del i, importer\n# Finally, add the importer to the meta path import hook.\nsys.meta_path.append(_importer)\n", "botocore/vendored/__init__.py": "", "botocore/vendored/requests/exceptions.py": "# -*- coding: utf-8 -*-\n\n\"\"\"\nrequests.exceptions\n~~~~~~~~~~~~~~~~~~~\n\nThis module contains the set of Requests' exceptions.\n\n\"\"\"\nfrom .packages.urllib3.exceptions import HTTPError as BaseHTTPError\n\n\nclass RequestException(IOError):\n    \"\"\"There was an ambiguous exception that occurred while handling your\n    request.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize RequestException with `request` and `response` objects.\n        \"\"\"\n        response = kwargs.pop('response', None)\n        self.response = response\n        self.request = kwargs.pop('request', None)\n        if (response is not None and not self.request and\n                hasattr(response, 'request')):\n            self.request = self.response.request\n        super(RequestException, self).__init__(*args, **kwargs)\n\n\nclass HTTPError(RequestException):\n    \"\"\"An HTTP error occurred.\"\"\"\n\n\nclass ConnectionError(RequestException):\n    \"\"\"A Connection error occurred.\"\"\"\n\n\nclass ProxyError(ConnectionError):\n    \"\"\"A proxy error occurred.\"\"\"\n\n\nclass SSLError(ConnectionError):\n    \"\"\"An SSL error occurred.\"\"\"\n\n\nclass Timeout(RequestException):\n    \"\"\"The request timed out.\n\n    Catching this error will catch both\n    :exc:`~requests.exceptions.ConnectTimeout` and\n    :exc:`~requests.exceptions.ReadTimeout` errors.\n    \"\"\"\n\n\nclass ConnectTimeout(ConnectionError, Timeout):\n    \"\"\"The request timed out while trying to connect to the remote server.\n\n    Requests that produced this error are safe to retry.\n    \"\"\"\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"The server did not send any data in the allotted amount of time.\"\"\"\n\n\nclass URLRequired(RequestException):\n    \"\"\"A valid URL is required to make a request.\"\"\"\n\n\nclass TooManyRedirects(RequestException):\n    \"\"\"Too many redirects.\"\"\"\n\n\nclass MissingSchema(RequestException, ValueError):\n    \"\"\"The URL schema (e.g. http or https) is missing.\"\"\"\n\n\nclass InvalidSchema(RequestException, ValueError):\n    \"\"\"See defaults.py for valid schemas.\"\"\"\n\n\nclass InvalidURL(RequestException, ValueError):\n    \"\"\" The URL provided was somehow invalid. \"\"\"\n\n\nclass ChunkedEncodingError(RequestException):\n    \"\"\"The server declared chunked encoding but sent an invalid chunk.\"\"\"\n\n\nclass ContentDecodingError(RequestException, BaseHTTPError):\n    \"\"\"Failed to decode response content\"\"\"\n\n\nclass StreamConsumedError(RequestException, TypeError):\n    \"\"\"The content for this response was already consumed\"\"\"\n\n\nclass RetryError(RequestException):\n    \"\"\"Custom retries logic failed\"\"\"\n", "botocore/vendored/requests/__init__.py": "# -*- coding: utf-8 -*-\n\n#   __\n#  /__)  _  _     _   _ _/   _\n# / (   (- (/ (/ (- _)  /  _)\n#          /\nfrom .exceptions import (\n    RequestException, Timeout, URLRequired,\n    TooManyRedirects, HTTPError, ConnectionError\n)\n", "botocore/vendored/requests/packages/__init__.py": "from __future__ import absolute_import\n\nfrom . import urllib3\n", "botocore/vendored/requests/packages/urllib3/exceptions.py": "\n## Base Exceptions\n\nclass HTTPError(Exception):\n    \"Base exception used by this module.\"\n    pass\n\nclass HTTPWarning(Warning):\n    \"Base warning used by this module.\"\n    pass\n\n\n\nclass PoolError(HTTPError):\n    \"Base exception for errors caused within a pool.\"\n    def __init__(self, pool, message):\n        self.pool = pool\n        HTTPError.__init__(self, \"%s: %s\" % (pool, message))\n\n    def __reduce__(self):\n        # For pickling purposes.\n        return self.__class__, (None, None)\n\n\nclass RequestError(PoolError):\n    \"Base exception for PoolErrors that have associated URLs.\"\n    def __init__(self, pool, url, message):\n        self.url = url\n        PoolError.__init__(self, pool, message)\n\n    def __reduce__(self):\n        # For pickling purposes.\n        return self.__class__, (None, self.url, None)\n\n\nclass SSLError(HTTPError):\n    \"Raised when SSL certificate fails in an HTTPS connection.\"\n    pass\n\n\nclass ProxyError(HTTPError):\n    \"Raised when the connection to a proxy fails.\"\n    pass\n\n\nclass DecodeError(HTTPError):\n    \"Raised when automatic decoding based on Content-Type fails.\"\n    pass\n\n\nclass ProtocolError(HTTPError):\n    \"Raised when something unexpected happens mid-request/response.\"\n    pass\n\n\n#: Renamed to ProtocolError but aliased for backwards compatibility.\nConnectionError = ProtocolError\n\n\n## Leaf Exceptions\n\nclass MaxRetryError(RequestError):\n    \"\"\"Raised when the maximum number of retries is exceeded.\n\n    :param pool: The connection pool\n    :type pool: :class:`~urllib3.connectionpool.HTTPConnectionPool`\n    :param string url: The requested Url\n    :param exceptions.Exception reason: The underlying error\n\n    \"\"\"\n\n    def __init__(self, pool, url, reason=None):\n        self.reason = reason\n\n        message = \"Max retries exceeded with url: %s (Caused by %r)\" % (\n            url, reason)\n\n        RequestError.__init__(self, pool, url, message)\n\n\nclass HostChangedError(RequestError):\n    \"Raised when an existing pool gets a request for a foreign host.\"\n\n    def __init__(self, pool, url, retries=3):\n        message = \"Tried to open a foreign host with url: %s\" % url\n        RequestError.__init__(self, pool, url, message)\n        self.retries = retries\n\n\nclass TimeoutStateError(HTTPError):\n    \"\"\" Raised when passing an invalid state to a timeout \"\"\"\n    pass\n\n\nclass TimeoutError(HTTPError):\n    \"\"\" Raised when a socket timeout error occurs.\n\n    Catching this error will catch both :exc:`ReadTimeoutErrors\n    <ReadTimeoutError>` and :exc:`ConnectTimeoutErrors <ConnectTimeoutError>`.\n    \"\"\"\n    pass\n\n\nclass ReadTimeoutError(TimeoutError, RequestError):\n    \"Raised when a socket timeout occurs while receiving data from a server\"\n    pass\n\n\n# This timeout error does not have a URL attached and needs to inherit from the\n# base HTTPError\nclass ConnectTimeoutError(TimeoutError):\n    \"Raised when a socket timeout occurs while connecting to a server\"\n    pass\n\n\nclass EmptyPoolError(PoolError):\n    \"Raised when a pool runs out of connections and no more are allowed.\"\n    pass\n\n\nclass ClosedPoolError(PoolError):\n    \"Raised when a request enters a pool after the pool has been closed.\"\n    pass\n\n\nclass LocationValueError(ValueError, HTTPError):\n    \"Raised when there is something wrong with a given URL input.\"\n    pass\n\n\nclass LocationParseError(LocationValueError):\n    \"Raised when get_host or similar fails to parse the URL input.\"\n\n    def __init__(self, location):\n        message = \"Failed to parse: %s\" % location\n        HTTPError.__init__(self, message)\n\n        self.location = location\n\n\nclass ResponseError(HTTPError):\n    \"Used as a container for an error reason supplied in a MaxRetryError.\"\n    GENERIC_ERROR = 'too many error responses'\n    SPECIFIC_ERROR = 'too many {status_code} error responses'\n\n\nclass SecurityWarning(HTTPWarning):\n    \"Warned when perfoming security reducing actions\"\n    pass\n\n\nclass InsecureRequestWarning(SecurityWarning):\n    \"Warned when making an unverified HTTPS request.\"\n    pass\n\n\nclass SystemTimeWarning(SecurityWarning):\n    \"Warned when system time is suspected to be wrong\"\n    pass\n\n\nclass InsecurePlatformWarning(SecurityWarning):\n    \"Warned when certain SSL configuration is not available on a platform.\"\n    pass\n\n\nclass ResponseNotChunked(ProtocolError, ValueError):\n    \"Response needs to be chunked in order to read it as chunks.\"\n    pass\n", "botocore/vendored/requests/packages/urllib3/__init__.py": "\"\"\"\nurllib3 - Thread-safe connection pooling and re-using.\n\"\"\"\n\n__author__ = 'Andrey Petrov (andrey.petrov@shazow.net)'\n__license__ = 'MIT'\n__version__ = ''\n\n\nfrom . import exceptions\n"}